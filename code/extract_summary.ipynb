{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\duyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract word vectors\n",
    "word_embeddings = {}\n",
    "file = open('../../glove.6B.200d.txt', encoding='utf-8')\n",
    "for line in file:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "file.close()\n",
    "len(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove stopwords\n",
    "def remove_stopwords(sen):\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "    return sen_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_vector_func (sentences_cleaned) : \n",
    "    sentence_vector = []\n",
    "    for i in sentences_cleaned:\n",
    "        if len(i) != 0:\n",
    "            v = sum([word_embeddings.get(w, np.zeros((200,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "        else:\n",
    "            v = np.zeros((200,))\n",
    "        sentence_vector.append(v)\n",
    "    \n",
    "    return (sentence_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_from_numpy_array(similarity_matrix):\n",
    "    num_nodes = similarity_matrix.shape[0]\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(num_nodes))\n",
    "\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(i+1, num_nodes):\n",
    "            weight = similarity_matrix[i, j]\n",
    "            G.add_edge(i, j, weight=weight)\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_pagerank(graph, alpha=0.85, max_iter=10, tol=1.0e-6):\n",
    "    num_nodes = graph.number_of_nodes()\n",
    "    pagerank = {node: 1 / num_nodes for node in graph.nodes()}\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        new_pagerank = {node: (1 - alpha) / num_nodes for node in graph.nodes()}\n",
    "\n",
    "        for node in graph.nodes():\n",
    "            degree = graph.degree(node, weight='weight')\n",
    "            if degree != 0:\n",
    "                for neighbor in graph.neighbors(node):\n",
    "                    new_pagerank[neighbor] += alpha * pagerank[node] / degree\n",
    "\n",
    "        residual = np.linalg.norm(np.array(list(new_pagerank.values())) - np.array(list(pagerank.values())))\n",
    "        pagerank = new_pagerank\n",
    "\n",
    "        if residual < tol:\n",
    "            break\n",
    "\n",
    "    return pagerank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_text(test_text, n=5):\n",
    "    sentences = []\n",
    "\n",
    "    # Token hóa văn bản\n",
    "    sentences.extend(sent_tokenize(test_text))\n",
    "    sentences = [y for x in sentences for y in x]  # Flatten list\n",
    "\n",
    "    # Loại bỏ dấu câu, số và ký tự đặc biệt\n",
    "    clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z0-9]\", \" \")\n",
    "\n",
    "    # Chuyển đổi chữ in thành chữ thường\n",
    "    clean_sentences = [s.lower() for s in clean_sentences]\n",
    "\n",
    "    # Loại bỏ stopwords từ các câu\n",
    "    clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "\n",
    "    # Hàm không được cung cấp, bạn cần thay thế bằng hàm tạo vectơ câu của mình\n",
    "    sentence_vectors = sentence_vector_func(clean_sentences)\n",
    "\n",
    "    # Ma trận tương đồng\n",
    "    sim_mat = cosine_similarity(sentence_vectors, sentence_vectors)\n",
    "\n",
    "    # Biểu diễn đồ thị từ ma trận tương đồng\n",
    "    custom_graph = custom_from_numpy_array(sim_mat)\n",
    "\n",
    "    # Áp dụng thuật toán Pagerank tùy chỉnh để tính điểm quan trọng\n",
    "    pagerank_values = custom_pagerank(custom_graph)\n",
    "\n",
    "    # Sắp xếp câu dựa trên điểm quan trọng\n",
    "    ranked_sentences = sorted(((pagerank_values[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
    "\n",
    "    # Trích xuất câu để tạo tóm tắt\n",
    "    summarised_string = ''\n",
    "    for i in range(min(n, len(ranked_sentences))):\n",
    "        summarised_string = summarised_string + str(ranked_sentences[i][1])\n",
    "\n",
    "    return summarised_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yyy\n"
     ]
    }
   ],
   "source": [
    "text = '''The quality, type, and density of information conveyed via text varies from source to source. Textbooks tend to be low in density but high in quality, while academic articles are high in both quality and density. On the other hand, news articles can vary significantly from source to source. Regardless of where the text comes from the goal here is to minimize the time you spend reading. Thus, we will build a tool that can easily be adapted to any number of sources.'''\n",
    "\n",
    "summary = summary_text(text, 3)\n",
    "\n",
    "summary_sentences = summary.split('. ')\n",
    "formatted_summary = '.\\n'.join(summary_sentences)\n",
    "\n",
    "print(formatted_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text, per):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc= nlp(text)\n",
    "    # tạo từ điển để lưu lại tần số các từ\n",
    "    word_frequencies={}\n",
    "    for word in doc:\n",
    "        if word.text.lower() not in list(STOP_WORDS):\n",
    "            if word.text.lower() not in punctuation:\n",
    "                if word.text not in word_frequencies.keys():\n",
    "                    word_frequencies[word.text] = 1\n",
    "                else:\n",
    "                    word_frequencies[word.text] += 1\n",
    "    #chuẩn hóa từ bằng cách chia tần suất max\n",
    "    max_frequency=max(word_frequencies.values())\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word]=word_frequencies[word]/max_frequency\n",
    "    sentence_tokens= [sent for sent in doc.sents]\n",
    "    # tính điểm = tổng tần suất từ trong câu\n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_tokens:\n",
    "        for word in sent:\n",
    "            if word.text.lower() in word_frequencies.keys():\n",
    "                if sent not in sentence_scores.keys():                            \n",
    "                    sentence_scores[sent]=word_frequencies[word.text.lower()]\n",
    "                else:\n",
    "                    sentence_scores[sent]+=word_frequencies[word.text.lower()]\n",
    "    # xác định số câu và in ra các câu có số điểm từ cao nhất\n",
    "    select_length=int(len(sentence_tokens)*per)\n",
    "    summary=nlargest(select_length, sentence_scores,key=sentence_scores.get)\n",
    "    final_summary=[word.text for word in summary]\n",
    "    summary=' '.join(final_summary)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary :\n",
      "Textbooks tend to be low in density but high in quality, while academic articles are high in both quality and density.\n",
      "The quality, type, and density of information conveyed via text varies from source to source.\n"
     ]
    }
   ],
   "source": [
    "text = '''The quality, type, and density of information conveyed via text varies from source to source. Textbooks tend to be low in density but high in quality, while academic articles are high in both quality and density. On the other hand, news articles can vary significantly from source to source. Regardless of where the text comes from the goal here is to minimize the time you spend reading. Thus, we will build a tool that can easily be adapted to any number of sources.'''\n",
    "\n",
    "summary = summarize(text, 0.4)\n",
    "\n",
    "summary_sentences = summary.split('. ')\n",
    "formatted_summary = '.\\n'.join(summary_sentences)\n",
    "\n",
    "print(\"summary :\")\n",
    "print(formatted_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "464"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "468"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
