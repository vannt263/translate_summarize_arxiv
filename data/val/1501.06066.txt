{
  "article_text": [
    "the support vector machine ( svm ) @xcite is a widely used modern classification method . in the standard binary classification problem ,",
    "training dataset consists of @xmath0 pairs , @xmath1 , where @xmath2 and @xmath3 .",
    "the linear svm seeks a hyperplane @xmath4 which maximizes the smallest margin of all data points : @xmath5 where @xmath6 is defined as the _ margin _ of the @xmath7th data point , @xmath8 s are slack variables introduced to ensure all margins non - negative , and @xmath9 is a tuning parameter controlling the overlap . by using a kernel trick , the svm can also produce nonlinear decision boundaries by fitting an optimal separating hyperplane in the extended kernel feature space .",
    "the readers are referred to @xcite for a more detailed explanation of the svm .",
    "@xcite noticed that when the svm is applied on some data with @xmath10 , many data points lie on two hyperplanes parallel to the decision boundary .",
    "@xcite referred to this phenomenon as _ data pilling _ and claimed that the data pilling can  affect the generalization performance of svm \" . to overcome this issue , @xcite proposed a new method called the distance weighted discrimination ( dwd ) , which finds a separating hyperplane minimizing the sum of the inverse margins of all data points : @xmath11 the initial version of @xcite also mentioned the sum of the inverse margins @xmath12 could be also replaced by @xmath13 , the @xmath14th power of the inverse margins , and this generalized version was used as the definition of the dwd in @xcite .",
    "@xcite asserted the dwd can avoid the data piling and thereby improve the generalizability .",
    "one example [ see the group 2 of figure 3 in @xcite ] shows that the dwd has about 5% prediction error whereas the svm does 15% .",
    "enhancement of the dwd over the svm can also be exemplified in @xcite through a novel geometric view . as for the computation of the dwd",
    ", @xcite observed that the dwd is an application of the second - order cone programming and thus can be solved by the primal - dual interior - point methods .",
    "the algorithm has been implemented in both matlab code http://www.unc.edu/~marron/marron_software.html and an r package ` dwd ` @xcite .    in this paper",
    "we focus on classification with high - dimensional data where the number of covariates is much larger than the sample size .",
    "the standard svm and dwd are not suitable tools for high - dimensional classification for two reasons .",
    "first , based on the scientific hypothesis that only a few important variables affect the outcome , a good classifier for high - dimensional classification should have the ability to select important variables and discard irrelevant ones .",
    "however , the standard svm and dwd use all variables and do not conduct variable selection .",
    "second , because these two classifiers use all variables , they may have very poor classification performance .",
    "as explained in @xcite , the bad performance is caused by the error accumulation when estimating too many noise variables in the classifier .",
    "owing to these two considerations , sparse classifiers are generally preferred for high - dimensional classification . in the literature ,",
    "some penalties have been applied to the svm to produce sparse svms such as the @xmath15 svm @xcite , the scad svm @xcite , and the elastic - net penalized svm @xcite .    in this work",
    "we consider sparse penalized dwd for high dimensional classification .",
    "the standard dwd uses the @xmath16 penalty and can be solved by the second - order cone programming .",
    "however , the sparse dwd is computationally more challenging and requires a different computing algorithm . to cope with the computational challenges associated with the sparse penalty and high - dimensionality",
    ", we derive an efficient algorithm to solve the sparse dwd by combining majorization - minimization principle and coordinate - descent .",
    "we have implemented the algorithm in an r package ` sdwd ` . to give a quick demonstration here",
    ", we use the prostate cancer data [ @xcite , 102 observations and 6033 genes ] as an example .",
    "the left panel of figure  [ fig : solnpaths ] depicts the solution paths of the elastic - net penalized dwd , and ` sdwd ` only took 0.453 second to compute the whole solution path . as comparison",
    ", we also used the code in @xcite to compute the solution path of the elastic - net penalized svm .",
    "we observed that the timing of the sparse svm was about 290 times larger than that of the sparse dwd .    , @xmath17 ) using the elastic - net dwd and the elastic - net svm . in every method , @xmath18 is fixed to be 1 .",
    "the dashed vertical lines indicate the @xmath19 selected by the five - folder cross validation .",
    "both timings are averaged over 10 runs . ]    , @xmath17 ) using the elastic - net dwd and the elastic - net svm . in every method , @xmath18 is fixed to be 1 .",
    "the dashed vertical lines indicate the @xmath19 selected by the five - folder cross validation .",
    "both timings are averaged over 10 runs . ]",
    "in this section we present several sparse penalized dwds .",
    "our formulation follows the @xmath15 svm @xcite .",
    "thus , we first review the derivation process of the @xmath15 svm .",
    "the standard svm is often rephrased as the following quadratic programming problem @xcite : @xmath20 moreover , the above constrained minimization problem has an equivalent _ loss+penalty _",
    "formulation @xcite : @xmath21_{+ } + \\dfrac{\\lambda_2}{2}||{\\boldsymbol}{\\beta}||^2_2 .",
    "\\label{eq : l2svmloss}\\ ] ] the loss function @xmath22_{+}=\\max(1-t,\\ 0)$ ] is the so - called hinge loss in the literature . for the high - dimensional setting , the standard svm uses all variables because of the @xmath16 norm penalty used therein . as a result",
    ", its performance can be very poor .",
    "@xcite proposed the @xmath15-norm svm to fix this issue : @xmath21_{+ } + \\lambda_1||{\\boldsymbol}{\\beta}||_1 .",
    "\\label{eq : l1svmloss}\\ ] ]    similarly , we can propose the @xmath15 penalized dwd .",
    "it has been shown that the standard dwd also has a _",
    "formulation @xcite : @xmath23 where the loss function is given by @xmath24 similar to the @xmath15 svm , we replace the @xmath16 norm penalty with the @xmath15 norm penalty in order to achieve sparsity in the dwd classifier .",
    "hence , the @xmath15 dwd is defined by @xmath25 the lasso penalized dwd classification rule is @xmath26 .",
    "besides the @xmath15 norm penalty , we also consider the elastic - net penalty @xcite .",
    "it is now well - known that the elastic - net often outperforms the lasso ( @xmath15 norm penalty ) in prediction .",
    "@xcite studied the elastic - net penalized svm ( drsvm ) and showed that the drsvm performs better than the @xmath15 norm svm .",
    "similarly , we propose the elastic - net penalized dwd : @xmath27 where @xmath28 the elastic - net penalized dwd classification rule is @xmath29 .",
    "both @xmath19 and @xmath18 are important tuning parameters for regularization . in practice , @xmath19 and @xmath18",
    "are chosen from finite grids by validation or cross - validation",
    ".    a further refinement of the elastic - net penalty is the adaptive elastic - net penalty @xcite where we replace the @xmath15 ( lasso ) penalty with the adaptive @xmath15 ( lasso ) penalty @xcite .",
    "the adaptive lasso penalty produces estimators with the oracle properties .",
    "the adaptive elastic - net enjoys the benefits of elastic - net and adaptive lasso . after fitting the elastic - net penalized dwd",
    ", we further consider the adaptive elastic - net penalized dwd : @xmath30 and the adaptive weights are computed by @xmath31 where @xmath32 is the solution of @xmath33 in .",
    "the adaptive elastic - net penalized dwd classification rule is @xmath34 .",
    "the @xmath16 dwd was solved based on the second - order - cone programming ; nevertheless , it is not trivial to generalize the algorithm to the @xmath15 dwd , and even more difficult to handle the elastic - net and the adaptive elastic - net penalties . in this section ,",
    "we propose a completely different algorithm .",
    "we solve the solution paths of the sparse dwd by using the generalized coordinate descent ( gcd ) algorithm proposed by @xcite .",
    "we introduce the gcd algorithm in section  [ sec : gcd1 ] , the implementation in section  [ sec : gcd3 ] , and the strict descent property in section [ sec : gcd2 ] . the same algorithm solves all the @xmath15 , the elastic - net , and adaptive elastic - net penalized dwds , while only the elastic - net is focused in the discussion for the sake of presentation .      without loss of generality , we assume that the variables @xmath35 are standardized : @xmath36 , for @xmath37 .",
    "we fix @xmath19 and @xmath18 and let @xmath38 .",
    "we focus on @xmath33 s first . for each @xmath33",
    ", we define the coordinate - wise update function : @xmath39 then the standard coordinate descent algorithm suggests cyclically updating @xmath40 for each @xmath41 . however , does not have a closed - form solution .",
    "the gcd algorithm solves this issue by adopting the mm principle @xcite .",
    "we approximate the @xmath42 function by a quadratic function @xmath43 then we update @xmath44 by @xmath45 , the closed - form minimizer of : @xmath46 where @xmath47 is the soft - thresholding operator @xcite and @xmath48 is the positive part of @xmath49 . with",
    "the intercept similarly updated , algorithm  [ algo1 ] summarizes the details of the gcd algorithm .",
    "1 .   initialize @xmath50 .",
    "cyclic coordinate descent , for @xmath51 : 1 .",
    "compute @xmath52 .",
    "2 .   compute @xmath53 3 .",
    "set @xmath54 .",
    "3 .   update the intercept term : 1 .",
    "compute @xmath55 .",
    "2 .   compute @xmath56 3 .",
    "set @xmath57 .",
    "repeat steps 2 - 3 until convergence of @xmath50 .",
    "we have implemented algorithm 1 in an r package ` sdwd ` .",
    "we exploit the warm - start , the strong rule , and the active set trick to increase the algorithm speeding . in our implementation",
    ", @xmath18 is pre - chosen and we compute the solution path as @xmath19 varies .",
    "first , we adopt the warm - start to lead to a faster and more stable algorithm @xcite .",
    "we compute the solutions at a grid of @xmath58 decreasing @xmath19 values , starting at the smallest @xmath19 value such that @xmath59 .",
    "denote these grid points by @xmath60 } , \\ldots , \\lambda_1^{[k]}$ ] . with the warm - start trick",
    ", we can use the solution at @xmath61}$ ] as the initial value ( the warm - start ) to compute the solution at @xmath62}$ ] .    specifically , to find @xmath60}$ ]",
    ", we fit a model with a sufficiently large @xmath19 and thus @xmath59 .",
    "let @xmath63 be the estimate of the intercept . by the kkt conditions , @xmath64 , so we can choose @xmath65 } = \\frac{1}{n } \\max_j \\left|\\sum_{i=1}^n v'(\\hat{\\beta}_0 ) y_i x_{ij})\\right|.\\ ] ] generally , we use @xmath66 , and @xmath67}=\\epsilon \\lambda_1^{[1]}$ ] , where @xmath68 when @xmath69 and @xmath70 otherwise .",
    "all the other grid points are placed to uniformly distribute on a log scale .",
    "second , we follow the strong rule @xcite to improve the computational speed .",
    "suppose @xmath71}$ ] and @xmath72}$ ] are the solutions at @xmath61}$ ] . after we solve @xmath71}$ ] and @xmath72}$ ] , the strong rule claims that any @xmath73 satisfying @xmath74 } + x_i^t \\boldsymbol{\\hat{\\beta}}^{[k]}))y_i x_{ij}\\right| < 2 \\lambda_1^{[k+1 ] } - \\lambda_1^{[k ] } \\label{eq : alg21}\\ ] ] is likely to be inactive at @xmath62}$ ] , i.e. , @xmath75}=0 $ ] .",
    "let @xmath76 be the collection of @xmath77 which satisfies , and its compliment @xmath78 .",
    "we call @xmath79 the survival set . if the strong rule guesses correctly , the variables contained in @xmath76 are discarded , and we only apply algorithm  [ algo1 ] to repeat the coordinate descent in the survival set @xmath79 . after computing the solution @xmath80 and @xmath81 ,",
    "we need to check whether some variables are incorrectly discarded .",
    "we check this by the kkt condition , @xmath82 if no @xmath83 violates , @xmath80 and @xmath81 are the solutions at @xmath62}$ ] .",
    "we rephrase them as @xmath84}$ ] and @xmath85}$ ] .",
    "otherwise , any incorrectly discarded variable should be added to the survival set @xmath79 .",
    "we update @xmath76 by @xmath86 where @xmath87 after each update of @xmath76 , some incorrectly discarded variables are added back to the survival set .",
    "third , the active set is also used to boost the algorithm speed .",
    "after we apply algorithm  [ algo1 ] on the survival set @xmath79 , we only apply the coordinate descent on a subset @xmath88 of @xmath79 till convergence , where @xmath89 .",
    "then another cycle of coordinate descent is run on @xmath79 to investigate if the active set @xmath88 changes .",
    "we finish the algorithm if no changes in @xmath88 ; otherwise , we update the active set @xmath88 and repeat the process .    in algorithm  [ algo1 ] ,",
    "the margin @xmath90 can be updated conveniently : if @xmath33 is updated by @xmath91 , we update @xmath90 by @xmath92 .",
    "last , the default convergence rule in ` sdwd ` is @xmath93 for all @xmath94 .",
    "@xcite showed the gcd algorithm enjoys descent property . in this section ,",
    "we also show the gcd algorithm has a stronger statement , the strict descent property , when the gcd is used to solve the sparse dwd .",
    "we first elaborate the following majorization result , whose proof is deferred in the appendix .",
    "@xmath95 is the coordinate - wise update function defined in , and @xmath96 is the surrogate function defined in .",
    "we have and :    @xmath97    [ lemma : p5 ]    given @xmath98 , and assuming @xmath99 , and imply the strict descent property of the gcd algorithm : @xmath100 .",
    "it is because @xmath101 .",
    "note that the original gcd paper only showed @xmath102 .",
    "the arguments above prove that the objective function @xmath42 strictly decreases after updating all variables in a cycle , unless the solution does not change after each update .",
    "if this is the case , the algorithm stops .",
    "we show that the algorithm must stop at the right answer .",
    "assuming @xmath54 for all @xmath77 , implies : @xmath103 a straightforward algebra can show that for all @xmath77 , @xmath104 which is exactly the kkt conditions of the original objective function . in conclusion , if the objective function does not change after a cycle , the algorithm necessarily converges to the correct solution satisfying the kkt condition .",
    "the simulation in this section aims to support the following three points : ( 1 ) the sparse dwd has highly competitive prediction accuracy with the sparse svm and the sparse logistic regression ; ( 2 ) the adaptive elastic - net penalized dwd performs the best in variable selection ; ( 3 ) for the prediction accuracy , no single method among the @xmath15 , the elastic - net , and the adaptive elastic - net penalized dwds dominate the others in all situations .    in this section ,",
    "the response variables of all the data are binary .",
    "the dimension @xmath105 of the variables @xmath106 is always 3000 . within each example , our simulated data consist of a training set , an independent validation set , and an independent test set .",
    "the training set contains 50 observations : 25 of them are from the positive class and the other 25 from the negative class .",
    "models are fitted on the training data only , and we use an independent validation set of 50 observations to select the tuning parameters : @xmath18 is selected from @xmath107 , @xmath108 , @xmath109 , @xmath110 , 1 , 5 , and 10 ; @xmath19 is searched along the solution paths .",
    "we compared the prediction accuracy ( in percentage ) on another independent test data set of 20,000 observations .",
    "we followed @xcite to generate the first two examples .",
    "in example 1 , the positive class is a random sample from @xmath111 , where @xmath112 is the @xmath105 by @xmath105 identity matrix and @xmath113 has all zeros except for @xmath114 at the first dimension ; the negative class is from @xmath115 with @xmath116 .",
    "in example 2 , @xmath117 of the data are generated from the same distributions as example 1 ; for the other @xmath118 of the data , the positive class is drawn from @xmath111 and negative class @xmath119 where @xmath120 .",
    "we obtained the other three examples following @xcite .",
    "in example 3 , the positive class has a normal distribution with mean @xmath113 and covariance @xmath121 , where @xmath113 has 0.7 in the first five covariates and 0 in others ; the negative class has the same distribution except for a different mean @xmath122 . in example 4 and 5 , we consider the cases where the relevant variables are correlated .",
    "two classes have the same distributions except for the covariance , @xmath123 in example 4 , the diagonal elements of @xmath124 are 1 and the off - diagonal elements are all equal to @xmath125 . in example 5",
    ", the @xmath126th element of @xmath124 equals @xmath127 .",
    "we compared the sparse dwd with the sparse svm and the sparse logistic regression .",
    "both the dwd and the logistic regression use the @xmath15 , the elastic - net and the adaptive elastic - net penalties .",
    "we used r packages ` sdwd ` and ` gcdnet ` @xcite to compute the sparse dwds and the sparse logistic regressions respectively .",
    "the @xmath15 and the elastic - net svms were solved by using the code from @xcite which does not handle the adaptive elastic - net penalty .",
    "table  [ tab : simu1 ] presents the prediction accuracy results . in the first two examples , the @xmath15 dwd and the @xmath15 logistic regression",
    "perform the best .",
    "we attribute this good performance to the only one nonzero variable in the data , despite @xmath118 of outliers in example 2 . in example 3 , 4 , and 5 , we increase the number of nonzero variables to five . for all models ,",
    "the elastic - net and the adaptive elastic - net penalties have similar performance , and both of them dominate the @xmath15 penalties .",
    "the elastic - net dwd produces the least prediction error in example 4 and 5 .",
    "table 3 compares the variable selection . in all cases ,",
    "the adaptive elastic - net penalties address all relevant variables with relatively few mistakes .",
    "the @xmath15 penalties share similar performance in the first two examples .",
    "in this section we analyze four benchmark data .",
    "the data arcene was obtained from @xcite , the breast cancer data from @xcite , the lsvt data from @xcite , and the prostate cancer was from @xcite .",
    "we randomly split each data with a ratio 1:1 into a training set and a test set . on the training set",
    ", we fit the sparse dwd with imposing the elastic - net and the adaptive elastic - net penalties . with the same tuning",
    "parameter candidates in the simulation , we used a five folder cross validation to find the best pair of @xmath128 incurring the least mis - classification rate .",
    "then we investigated the prediction accuracy of the selected model on the test set .",
    "as comparisons , we considered the sparse svm and the sparse logistic regression .",
    "every method was trained and tuned in the same way as the sparse dwd .",
    "all numerical experiments were carried out on an intel core i7 - 3770 ( 3.40 ghz ) processor .    in table  [ tab : realdata ] , we reported the average mis - classification percentage on the test set from 200 independent splits .",
    "we observe that the classifiers achieving the least error in these four datasets are the adaptive elastic - net logistic regression , the elastic - net svm , the elastic - net and the adaptive elastic - net dwds .",
    "we also find all the differences are not quite large .",
    "for the sparse dwd , we get the same message as @xcite concluded for the standard dwd :  it very often is competitive with the best of the others and sometimes is better . \"",
    "we also notice that the computation of the sparse dwd is the fastest in almost all cases .",
    "the timing of the svm is much longer than other methods .",
    "a possible explanation is that the svm uses the non - differentiable hinge loss function which makes the gcd algorithm not suitable for solving the sparse svm .",
    "so far , the best algorithm for the sparse svm is a lars type algorithm @xcite , which is very different from the gcd algorithm for the sparse dwd and logistic regression .",
    "it has been observed that coordinate descent may be faster than the lars algorithm for solving the lasso penalized least squares @xcite .",
    "in this article , we have proposed the sparse dwd for high - dimensional classification and developed an efficient algorithm to compute its solution path .",
    "we have shown that the sparse dwd has competitive prediction performance with the sparse svm and the sparse logistic regression and is often faster to compute with the help of our algorithm .",
    "thus , the sparse dwd is a valuable addition to the toolbox for high - dimensional classification .",
    "the generalized dwd defined in @xcite minimizes the @xmath14th power of the inverse margins .",
    "when @xmath129 , it reduces to the usual dwd . for computation considerations , @xcite choose to fix @xmath129 , because it leads to a second order cone programming problem",
    "we have found that our algorithm can be readily used to solve the sparse generalized dwd with any positive @xmath14 . in our numerical study",
    "we tried the generalized dwd with @xmath130 and also tried to use cross - validation to select a data - driven @xmath14 value .",
    "our numeric results indicated that using different @xmath14 values does not lead to significant differences in performance .",
    "we opt to leave those results to the technical report version of this paper .",
    "the authors thank the editor , the associate editor , and two referees for their helpful comments and suggestions .",
    "[ [ proof - of - lemmalemmap5 ] ] proof of lemma  [ lemma : p5 ] + + + + + + + + + + + + + + + + + + + + + + + + +    is trivial . to prove , it suffices to show for any @xmath131 , @xmath132 first , it is not hard to check that the first - order derivative @xmath133 is lipschitz continuous , i.e. , for any @xmath134 , @xmath135 let @xmath136 , then shows @xmath137 is strictly increasing . therefore @xmath138 is a strictly convex function , and its first - order condition leads to directly .",
    "bradley , p. and mangasarian , o. ( 1998 ) , `` feature selection via concave minimization and support vector machines , '' in _ machine learning proceedings of the fifteenth international conference ( icml98 ) _ , citeseer , 8290 .",
    "graham , k. , de  las morenas , a. , tripathi , a. , king , c. , kavanah , m. , mendez , j. , stone , m. , slama , j. , miller , m. , antoine , g. , willers , h. , sebastiani , p. , and rosenberg , c. ( 2010 ) , `` gene expression in histologically normal epithelium from breast cancer patients and from cancer - free lemmahylactic mastectomy patients shares a similar profile , '' _ british journal of cancer _ , 102(8 ) , 12841293 .",
    "singh , d. , febbo , p. , ross , k. , jackson , d. , manola , j. , ladd , c. , tamayo , p. , renshaw , a. , damico , a. , richie , j. ( 2002 ) , `` gene expression correlates of clinical prostate cancer behavior , '' _ cancer cell _ , 1(2 ) , 203209 .",
    "tibshirani , r. , bien , j. , friedman , j. hastie , t. , simon , n. , taylor , j. , and tibshirani , r. j. ( 2010 ) , `` strong rules for discarding predictors in lasso - type problems , '' _ journal of the royal statistical society , series b _ , 74(2 ) , 245266 .",
    "tsanas , a. , little , m. , fox , c. , and ramig , l. ( 2014 ) , `` objective automatic assessment of rehabilitative speech treatment in parkinson s disease , '' _ ieee transactions on neural systmes and rehabilitation engineering _ , 22(1 ) , 181190 ."
  ],
  "abstract_text": [
    "<S> distance weighted discrimination ( dwd ) was originally proposed to handle the data piling issue in the support vector machine . in this paper , we consider the sparse penalized dwd for high - dimensional classification . </S>",
    "<S> the state - of - the - art algorithm for solving the standard dwd is based on second - order cone programming , however such an algorithm does not work well for the sparse penalized dwd with high - dimensional data . in order to overcome the challenging computation difficulty , we develop a very efficient algorithm to compute the solution path of the sparse dwd at a given fine grid of regularization parameters . </S>",
    "<S> we implement the algorithm in a publicly available r package ` sdwd ` . </S>",
    "<S> we conduct extensive numerical experiments to demonstrate the computational efficiency and classification performance of our method . + * key words : * high - dimensional classification , svm , dwd . </S>"
  ]
}