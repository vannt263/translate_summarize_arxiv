{
  "article_text": [
    "swarm optimization ( pso ) is a typical swarm intelligence optimization algorithm inspired by animal social behaviors , such as bird flocking and fish schooling @xcite .",
    "a group of particles in pso fly in the search space , aiming to find the optimum cooperatively .",
    "each particle exchanges information with others and learns useful information to improve its performance .",
    "due to its ease to implement and outstanding performance , pso has been widely used to solve real - world schedule or engineering problems such as antennas@xcite , system control@xcite , electronics and electromagnetics @xcite .    in the original pso @xcite",
    ", each particle learns from the best historical experience of the whole population .",
    "the concepts of structure and neighbors in pso were first introduced in @xcite , where each particle learns from the best historical experience of its neighbors .",
    "however , in both versions the particles learn from the best individual , hence some useful information of other individuals is neglected @xcite . to take the advantage of full information , the fully informed particle swarm optimization ( fipso )",
    "was proposed @xcite where all neighbors are information sources .",
    "though fipso can rapidly converge , it may miss some promising regions in the search space of the optimization problem .",
    "most previous works treated all individuals as the same , neglecting the individual heterogeneity .",
    "actually , the individual heterogeneity plays an important role in swarm intelligence and has been verified to be able to significantly improve the performance of pso @xcite . here , we propose a heterogeneous strategy particle swarm optimization ( hspso ) , in which a proportion of particles are single informed , while others are fully informed .",
    "our experimental results show that hspso obtains satisfactory solutions and outperforms fipso and canonical singly informed pso ( sipso ) , because in hspso fully - informed particles can adequately utilize the global information and guide the swarm while singly - informed particles can maintain the diversity .",
    "the rest of the paper is organized as follows .",
    "section ii introduces hspso in detail and shows its relation to sipso and fipso .",
    "section iii compares the results of three psos .",
    "section iv employs hspso to solve the problem of 2-dimensional recursive filter design .",
    "section v makes a conclusion .",
    "in hspso , @xmath0 particles fly in a @xmath1-dimensional space to search the optimum .",
    "the @xmath2th particle updates its velocity and position of @xmath3th dimension by @xmath4\\\\ & v_{i}^d:=\\chi[v_{i}^d+\\frac{\\varphi}{k_i}\\sum_{m=1}^{k_i}r_{m}(p_{i_m}^d - x_{i}^d)]\\end{aligned}\\ ] ]    where ( 2 ) is for singly - informed ( si ) particles while ( 3 ) is for fully - informed ( fi ) particles , @xmath5=4.1 and @xmath6=0.729 according to common practices @xcite , @xmath7 $ ] denotes the historical best position of particle @xmath2 , @xmath8 $ ] denotes the historical best position in all neighbors of particle @xmath2 , @xmath9 is the number of the @xmath2th particle s neighbors , @xmath10 is the @xmath11th neighbor of the particle @xmath2 , @xmath12 $ ] is the historical best position of @xmath10 , @xmath13 , @xmath14 in ( 2 ) and all of @xmath15 in ( 3 ) are independent random numbers in range @xmath16 $ ] .",
    "note that there are two strategies of updating the velocity , i.e. fi and si .",
    "each particle employs the alternative velocity formula according to its property .",
    "here we use a parameter @xmath17 $ ] to divide the swarm into two groups .",
    "a group of particles , with size @xmath18 , are randomly selected as fully - informed ( fi ) particles , and the rest are singly - informed ( si ) ones .",
    "[ f1 ] illustrates this feature , where a widely used ring structure with average degree @xmath19=4 is employed for instance .",
    "one can see that each particle is influenced by the best one of @xmath20 neighbors in sipso fig .",
    "[ f1](a ) , and by all of 4 in fipso fig .",
    "[ f1](b ) , while both types exist in hspso fig .",
    "[ f1](c ) with a certain proportion ( @xmath21=0.3 in this example ) of individuals .",
    "the @xmath21 is a key parameter in hspso algorithm to balance the effect of fi and si particles , because superfluous fi particles could provide too much redundant information while an excess of si particles may result in information loss . specifically , when @xmath21=1 , all particles are fi particles , then hspso degrades to fipso .",
    "when @xmath21=0 , hspso becomes sipso .",
    "to evaluate the performance of hspso , we employ six widely - used benchmark functions @xcite .",
    "the formulas and the details of these functions are listed in table [ t1 ] . among these functions ,",
    "@xmath22(sphere ) , @xmath23(rosenbrock ) and @xmath24(quartic noise ) are unimodal function , yet @xmath23 is sometimes treated as multimodal function when @xmath1 is large , and @xmath24 includes a stochastic term .",
    "the other @xmath25 functions are multimodal , where @xmath26(ackley ) is the simplest one , while the landscape of @xmath27(rastrigin ) is more complex with many deep local optima , and @xmath28(griewank ) are asymmetrical .",
    "the dimension of all these benchmark functions are set as @xmath29 . with such diverse characteristics , these functions could help test the performance of hspso in a comprehensive way .",
    "the rest experiments adopt the following parameter setting : the population size @xmath30 , each run stops at @xmath31 iterations and each data is averaged by @xmath32 times .",
    "@xmath33 & @xmath34^d$ ] + @xmath35 & @xmath36^d$ ] + @xmath37 & @xmath38^d$ ] + @xmath39 & @xmath40^d$ ] + @xmath41 & @xmath42^d$ ] + @xmath43 & @xmath44^d$ ] +    [ t1 ]      we compare the performance of hspso to that of sipso and fipso , i.e. hspso with @xmath45 and with @xmath46 under the criteria of solution quality @xmath47 ( the final optimized fitness value ) , which is the most important criteria .",
    "firstly , we investigate @xmath47 of the algorithm under the ring structure with @xmath19=4 , where @xmath21 varies from @xmath48 to @xmath49 . as is shown in fig .",
    "[ f2 ] , hspso with an appropriate @xmath21 outperforms both canonical pso and fipso on almost all of test functions .",
    "moreover , @xmath21 is various with different functions .",
    "it reveals that the cooperation of fi particles and si particles helps to improve the optimization process under an appropriate proportion of fi particles .     with variation of @xmath21.,scaledwidth=45.0% ]    to investigate the optimization process in more details , we examine the variation of fitness value during the evolution . as shown in fig .",
    "[ f3 ] , hspso with a larger @xmath21 , especially fipso , converges faster than hspso with small @xmath21 at the beginning of the evolution .",
    "however , the premature convergence will make the swarm stagnate , not finding more promising solutions .",
    "hence the @xmath47 value of fipso is usually unsatisfactory .",
    "canonical pso is rarely troubled by premature , yet it converges quite slowly .",
    "hspso is outstanding because fi particles could ensure an appropriate convergence speed , while si particles maintain the diversity of the swarm .",
    "therefore , hspso with an appropriate @xmath21 could converge faster than canonical pso and avoid premature meanwhile .",
    "a key advance in understanding complex networks over the last decade has been how powerfully network topology affects many network properties and dynamical processes @xcite-@xcite .",
    "though the idea of hspso is mainly about learning strategy , the topology is also an important factor . as a network - based information system , pso s performance",
    "is greatly influenced by the network sparsity .",
    "a dense network makes information spread fast . yet",
    "a network with a small average degree impedes the information spreading , in which particles could preferably maintain the diversity .",
    "thus , we further investigate the impact of topology sparsity .",
    "as shown in fig .",
    "[ f4 ] , the optimal @xmath21 , inducing best @xmath47 , decreases with the increase of @xmath19 . in a dense network ,",
    "fi particles speed up the process of spreading information due to the abundant neighbors , which may lead to premature convergence .",
    "plenty of fi particles which absorb information without discrimination will weaken valuable information , even mislead each other , while the mechanism of si particle could discriminate information effectively . therefore , to avoid confusion , the better choice is to employ much fewer fi particles than si ones in a dense network .     of hspso with different network sparsity and variation of @xmath21 to solve @xmath22.,scaledwidth=45.0% ]    to further uncover the underlying mechanism of the optimization process , we examine the exploring ability of fi particles . in fig .",
    "[ f5 ] , @xmath50 is denoted as the percentage that fi particles discover better solutions .",
    "interestingly , the optimal @xmath21 in fig . [ f4 ] is well consistent with the maximal @xmath50 in fig .",
    "[ f5 ] , indicating that the performances of fi particles are evidently relevant to the solution quality of hspso .",
    "in other words , fi particles are more likely to act as guiders in the swarm due to the fi learning strategy . furthermore , as @xmath19 increases , the appropriate @xmath21 for the maximum of @xmath50 decreases , implying that fewer guiders are needed to lead the swarm in more densely - connected networks .",
    "when @xmath21 is small , the minority fi particles are powerless while si particles which are adept at maintaining the diversity can not use information effectively .",
    "if @xmath21 is too large , on the contrary , the redundant information will mislead fi particles , thus si particles will play an effective role to pull the swarm out of a local optimum .",
    "that fi particles discover better solutions during the whole evolution iterations .",
    "@xmath51 , where @xmath52 is the number that fi particles find better solutions and @xmath53 is the total number that all particles find better solutions.,scaledwidth=45.0% ]    we also investigate other networks , such as scale free network @xcite ( in fig .",
    "[ f6](a ) ) and small world network @xcite ( in fig . [ f6](b ) ) . in consideration of the appropriateness of network sparsity , @xmath19 of these networks are set no more than 10 . as expected , in fig .",
    "[ f6 ] , hspso with these topologies show similar results to fig .",
    "[ f4 ] , demonstrating the robustness of our algorithm .",
    "futhermore , some relatively novel network structures such as in @xcite and @xcite will be investigated in our future work .",
    "vs @xmath21 when hspso employs a scale - free network ( a ) or a small - world network ( b).,scaledwidth=45.0% ]",
    "to demonstrate the applicability of hspso we use it to solve a design problem of iir digital filters , which attracted considerable attentions during past decades @xcite-@xcite .",
    "the transfer function of @xmath54-d recursive digital filters can be described by @xmath55 where @xmath0 is the dimension of the filter , @xmath56 and @xmath57 , and @xmath58 , @xmath59 are the frequencies in range @xmath60 $ ] . the task of filter designing is to adjust the coefficients of @xmath61 to approximate the desired amplitude response of the @xmath54-d filter @xmath62 . in this brief , the desired amplitude response @xmath62 follows @xcite as @xmath63 hence , the design of @xmath54-d filter can be formalized as an optimization problem of minimizing the cost function @xmath64^p } } \\end{aligned}\\ ] ] s.t .",
    "@xmath65 where @xmath66 , and @xmath67 .",
    "the cost function describes the difference of @xmath68 and @xmath62 in @xmath69 points .          as @xmath70",
    "is the function of @xmath71 , @xmath72 , @xmath73 and @xmath74 , we construct a vector @xmath75=[@xmath76 , @xmath77 , @xmath78 , @xmath79 , @xmath80 , @xmath81 , @xmath82 , @xmath83 , @xmath84 , @xmath85 , @xmath86 , @xmath87 , @xmath88 , @xmath89 , @xmath74 ] for hspso .",
    "the parameter of hspso is set as follow : population size @xmath0 is set as @xmath91 , all variables in vector @xmath75 are in the range of @xmath92 $ ] @xcite , the evolution lasts for @xmath93 iterations .",
    "table [ t2 ] lists the parameters optimized by hspso and other competitors , including genetic algorithm ( ga)@xcite , neural network ( nn)@xcite , sipso ( hspso with @xmath45 , @xmath94 ) and fipso ( hspso with @xmath46 , @xmath94 ) .",
    "[ f7 ] shows the frequency response of the required filter and the designed filters with the parameters in table [ t2 ] .",
    "one can see that hspso performs better than ga and nn methods .",
    "note that the high frequency region of filters designed by sipso and fipso are flat , yet the low frequency region are not satisfactory , due to its elliptical transverse section rather than a circle .",
    "therefore , hspso outperforms both sipso and fipso due to the cooperation of singly- informed particles and fully - informed particles .",
    ".the results of optimized filter coefficients [ cols=\"^,^,^,^,^,^\",options=\"header \" , ]     [ t2 ]",
    "in this brief we propose hspso , a swarm optimization algorithm composed of two types of particles with different learning strategies .",
    "we test the performance of hspso on six widely - used benchmark functions .",
    "our results show that hspso is superior to canonical pso and fipso .",
    "our investigation on the impact of network topology and the underlying mechanism of hspso reveals that the heterogeneity of the swarm results in the division and cooperation between different particles , leading to a more effective optimization process .",
    "the successful application of hspso to @xmath54-d filter design problem demonstrates its applicability in solving real - world optimization problems .",
    "this paper is supported by the national natural science foundation of china ( grant nos .",
    "61425014 , 61521091 ) , national key research and development program of china ( grant no .",
    "2016yfb1200100 ) , and national key technology r&d program of china ( grant no .",
    "2015bag15b01 ) .      j. kennedy and r. c. eberhart ,  particle swarm optimization , \" _ in proc .",
    "neural netw .",
    "4 , pp . 1942 - 1948 , 1995 .",
    "y. kim , l. hao ,  equivalent circuit modeling of broadband antennas using vector fitting and particle swarm optimization , \" _ antennas and propagation society international symposium _ , pp .",
    "3555 - 3558 , 2006 . y. l. abdel - magid , m. a. abido ,  agc tuning of interconnected reheat thermal systems with particle swarm optimization , \" _ in proc .",
    "of the 2003 10th ieee inter .",
    "conf . on electronics , circuits and systems _ ,",
    "376 - 379 , 2003 .",
    "m. carpenter , r. morgan , p. rochat ,  particle swarm optimization for the design of frequency selective surfaces , \" _ ieee antennas wireless propagation letters _",
    ", vol . 5 , no .",
    "277 - 279 , 2006 .",
    "m. clerc , j. kennedy ,  the particle swarm - explosion , stability , and convergence in a multidimensional complex space , \" _ ieee trans .",
    "_ , vol . 6 , no . 1 , pp .",
    "58 - 73 , 2002 .",
    "r. mendes , j. kennedy , j. neves ,  the fully informed particle swarm : simpler , maybe better , \" _ ieee trans .",
    "_ , vol . 8 , no .",
    "204 - 210 , 2004 . c. liu , w. b. du , w. x. wang ,  particle swarm optimization with scale - free interactions , \" _",
    "plos one _ , vol . 9 , e97822 , 2014 .",
    "y. gao , w. b. du , g. yan ,  selectively - informed particle swarm optimization , \" _ sci .",
    "_ , vol . 5 , 9295 , 2015 .",
    "w. b. du , y. gao , c. liu , z. zheng , z. wang ,  adequate is better : particle swarm optimization with limited - information , \" _ appl .",
    "comput . _ , vol .",
    "832 - 838 , 2015 .",
    "x. yao , y. liu , g. m. lin .",
    " evolutionary programming made faster , \" _ ieee comput .",
    "82 - 102 , 1999 .",
    "x. li , x. f. wang , g. r. chen .",
    " pinning a complex dynamical network to its equilibrium , \" _ ieee trans .",
    "circuits syst .",
    "i , regu . paper _ , vol .",
    "2074 - 2087 , 2004",
    ". q. j. zhang , j. n. lu , c. k. tse .",
    " adaptive feedback synchronization of a general complex dynamical network with delayed nodes , \" _ ieee trans .",
    "circuits syst .",
    "ii , exp . brief _ ,",
    "183 - 187 , 2008 .",
    "z. h. rong , z. x. wu , g. r. chen .",
    " coevolution of strategy - selection time scale and cooperation in spatial prisoner s dilemma game , \" _ epl _ , vol .",
    "102 , pp . 68005 , 2013 . h. y. liu , y. x. xia .",
    " optimal resource allocation in complex communication networks , \" _ ieee trans . circuits syst .",
    "ii , exp . brief _ ,",
    "706 - 710 , 2015 .",
    "a. l. barabsi , r. albert .",
    " emergence of scaling in random networks , \" _ science _ , vol .",
    "509 - 512 , 1999 .",
    "d. j. watts , s. h. strogatz . ",
    "collective dynamics of ` small - world ' networks , \" _ nature _ , vol .",
    "440 - 442 , 1998 .",
    "s. jespersen , a. blumen .",
    " small - world networks : links with long - tailed distributions , \" _ phys .",
    "e _ , vol .",
    "6270 - 6274 , 2000 .",
    "t. jia , r. v. kulkarni .",
    " on the structural properties of small - world networks with range - limited shortcut links , \" _ phys .",
    "mech . & its appl .",
    "6118 - 6124 , 2013",
    ". k. m. tsui , s. c. chan , h. k. kwan .  a new method for designing causal stable iir variable fractional delay digital filters , \" _ ieee trans .",
    "circuits syst .",
    "999 - 1003 , 2007 .",
    "x. p. lai , z. p. lin , h. k. kwan .",
    " a sequential minimization procedure for minimax design of iir filters based on second - order factor updates , \" _ ieee trans .",
    "circuits syst .",
    "ii , exp . brief _ , vol .",
    "51 - 55 , 2011 .",
    "n. piyachaiyakul , c. charoenlarpnopparut .",
    " nonseparable three - dimensional iir notch filter design using outer product expansion , \" _ ieee trans .",
    "circuits syst .",
    "ii , exp . brief _ , vol .",
    "605 - 609 , 2011 .",
    " fast design of iir digital filters with a general chebyshev characteristic , \" _ ieee trans .",
    "circuits syst .",
    "ii , exp . brief _",
    "962 - 966 , 2014 .",
    "n. e. mastorakis , i. f. gonos , m. n. s swamy ,  design of two - dimensional recursive filters using genetic algorithms , \" _ ieee trans .",
    "circuits syst .",
    "i fundam theory appl .",
    "634 - 639 , 2003 . v.",
    "m. mladenov , n. e. mastorakis ,  design of two - dimensional recursive filters by using neural networks , \" _ ieee trans .",
    "neural netw .",
    "585 - 590 , 2001 ."
  ],
  "abstract_text": [
    "<S> pso is a widely recognized optimization algorithm inspired by social swarm . in this </S>",
    "<S> brief we present a heterogeneous strategy particle swarm optimization ( hspso ) , in which a proportion of particles adopt a fully informed strategy to enhance the converging speed while the rest are singly informed to maintain the diversity . </S>",
    "<S> our extensive numerical experiments show that hspso algorithm is able to obtain satisfactory solutions , outperforming both pso and the fully informed pso . </S>",
    "<S> the evolution process is examined from both structural and microscopic points of view . </S>",
    "<S> we find that the cooperation between two types of particles can facilitate a good balance between exploration and exploitation , yielding better performance . </S>",
    "<S> we demonstrate the applicability of hspso on the filter design problem .    </S>",
    "<S> ying : heterogeneous strategy particle swarm optimization    optimization , complex networks , filter design , pso . </S>"
  ]
}