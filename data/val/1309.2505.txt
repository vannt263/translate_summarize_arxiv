{
  "article_text": [
    "compressed sensing @xcite is one of the most exciting topics of present - day signal processing .",
    "signal reconstruction from its low - dimensional representation becomes a possibility , given the sparse nature of the signal and , incoherence and/or restricted isometry property ( rip ) @xcite of the sensing / measurement process . in this regard ,",
    "a number of approaches can be used , e.g. , basis pursuit ( bp ) @xcite , least absolute shrinkage and selection operator ( lasso ) @xcite and greedy algorithms @xcite . in order to exploit the structure of the signal being sensed , a number of variants of lasso have become popular , e.g. , group lasso ( g - lasso ) @xcite , sparse group lasso ( sg - lasso ) @xcite and fused lasso ( f - lasso ) @xcite , etc . in this letter",
    "we propose new lasso formulations to handle block sparse smooth signals .",
    "smooth signals are often encountered in a wide range of engineering and biological fields . in engineering ,",
    "signals observed in image processing , control systems and environment monitoring are often smooth or piece - wise smooth . in biology , a similar structure",
    "is observed , e.g. , in protein mass spectroscopy @xcite .",
    "the goal is to recover such structured signals from noisy and/or under - sampled measurements .",
    "a related topic is signal smoothing which deals with removing random outliers . apart from being smooth , such signals",
    "can often be represented as sparse in some basis .",
    "this sparsity pattern normally varies in terms of the location and block size of the sparse coefficients .",
    "the challenge for signal reconstruction is to exploit the block sparsity with varying block sizes , while keeping smoothness intact and using fewer measurements , but all at low complexity . in the cs domain ,",
    "signal smoothness has been handled by using a fusion constraint in @xcite .",
    "the fusion is also known as total variation ( tv ) in the image processing literature .",
    "apart from fusion , @xcite also proposed an @xmath0-norm penalty to cater for signal sparsity .",
    "however , since most of the signals are block sparse , @xcite can not give efficient results . to cater for the block sparsity",
    ", one can replace the @xmath0-norm penalty with a group penalty .",
    "although this approach can handle the block sparsity very well , it only offers fixed group sizes and causes complete groups to be zero or nonzero . to avoid elimination of small sets of nonzero elements ,",
    "a very small group size is opted but that can make the algorithm inefficient in eliminating large blocks of zero elements . in this regard",
    "we propose to use a moderate group size along with an @xmath0-norm penalty over the signal , to create sparsity within the groups .",
    "thus by using fusion in combination with @xmath0-norm penalty and a moderate group size , a smooth signal can be reconstructed with high accuracy .",
    "the problem of varying group sizes can also be handled by using the concept of latent groups , see @xcite and references therein .",
    "these are basically overlapping groups , with recurring signal elements in possibly multiple groups .",
    "thus , an element lost in one group may resurface through another group after reconstruction .",
    "so we also propose to use such latents groups in combination with a fusion constraint to recover block sparse smooth signals with varying block sizes .",
    "note that a work on using overlapping groups over the fusion function instead of the signal structure has appeared in @xcite , which however requires complete signal samples .",
    "instead , we propose overlapping groups and fusion penalties over the actual signal for the under - determined systems .",
    "thus , we exploit the actual structure of the signal rather than the difference of elements .",
    "further , in order to solve the proposed formulations we derive low - complexity algorithms based on the alternating direction method of multipliers ( admm ) @xcite .",
    "the reason for using this version of the augmented lagrangian methods is primarily the non - separability of the fusion penalty in terms of the elements of the signal .",
    "thus , the general convergence properties of admm can be used to guarantee optimal results for our proposed algorithms . * _ notations_*. matrices are in upper case bold while column vectors are in lower case bold , @xmath1_{i , j}$ ] is the @xmath2th entry of the matrix @xmath3 , @xmath4_{i}$ ] is the @xmath5th entry of the vector @xmath6 , @xmath7 is the identity matrix of size @xmath8 , @xmath9 is transpose , @xmath10 is the estimate of @xmath6 , @xmath11 defines an entity , @xmath12_{i}\\right|^p)^{1/p}}$ ] is the the @xmath13 norm of @xmath6 , @xmath14 is the sign function which takes values @xmath15 and @xmath16 depending on the polarity of the element @xmath17 , whereas the function @xmath18 if and only if @xmath19 otherwise @xmath20 .",
    "let @xmath6 be the @xmath21 recoverable signal .",
    "given @xmath22 measurements , the sensed signal can be written as = + [ eq : sens_y ] where @xmath23 is an @xmath24 measurement vector , @xmath25 is an @xmath26 measurement matrix ( @xmath27 ) with compression ratio @xmath28 and @xmath29 is an @xmath24 zero - mean additive white gaussian noise vector with variance @xmath30 . to recover the signal from the compressed measurements",
    "while keeping the signal structure in tact , we propose below , two lasso formulations .      through sparse group fused lasso ( sgf - lasso ) , we can resolve the issue of signal smoothness , as well as , that of fixed group sizes .",
    "the optimization problem can be formulated as @xmath31_j-[\\xbf]_{j-1}\\|_1 ^ 1 \\label{eq : sglf_x}\\end{aligned}\\ ] ] where @xmath32 is an @xmath33 sub - vector of @xmath6 , representing one of @xmath34 groups over the elements of @xmath6 , i.e. , @xmath35^t$ ] .",
    "we can see from ( [ eq : sglf_x ] ) that @xmath36 accounts for group sparsity , @xmath37 for element - wise sparsity and @xmath38_j-[\\xbf]_{j-1}\\|_1 ^ 1 $ ] accounts for fusion within the elements of @xmath6 , such that the effect of each penalty becomes severe with increasing penalty parameters , i.e. , @xmath39 , @xmath40 and @xmath41 , respectively . for a moderate value of @xmath34",
    ", the proposed formulation can tackle the varying group size problem by creating sparsity within the group along with fusing consecutive elements .",
    "note that , for @xmath42 , ( [ eq : sglf_x ] ) reduces to the standard lasso problem , for @xmath43 , ( [ eq : sglf_x ] ) reduces to sg - lasso , for @xmath44 , ( [ eq : sglf_x ] ) takes the shape of g - lasso and for @xmath45 , ( [ eq : sglf_x ] ) becomes f - lasso .      in order to solve the sgf - lasso problem via admm",
    ", we introduce two auxiliary variables @xmath46 and @xmath47 of size @xmath21 .",
    "thus , ( [ eq : sglf_x ] ) can be written as @xmath48 = \\mathop{\\arg \\min}\\limits_{\\small \\xbf , \\ubf , \\zbf }   & \\frac{1}{2 } \\|\\ybf-\\phibf\\xbf\\|_2 ^ 2 + \\lambda_e\\|\\ubf\\|_1 ^ 1 \\notag \\\\ & + \\lambda_g \\sum_{i=0}^{g-1 } \\|\\ubf_i\\|_2 ^ 1 + \\lambda_f \\|\\zbf\\|_1 ^ 1 \\notag \\\\ \\text{s.t . } \\;\\;\\ ; & \\ubf_i=\\xbf_i,\\;{0\\le i\\le g-1 } , \\;\\ ; \\zbf = \\dbf\\xbf \\label{eq : glf_admm}\\end{aligned}\\ ] ] where @xmath49 is an @xmath50 sub - vector of @xmath46 , i.e. , @xmath51^t$ ] , and @xmath52 is the difference matrix with @xmath53_{j , j}=-1 $ ] , @xmath53_{j , j+1}=1 $ ] , for @xmath54 and @xmath53_{n-1,n-1}=1 $ ] , such that @xmath55 equals the element - wise fusion . from ( [ eq : glf_admm ] ) , the lagrangian problem can be written as @xmath56 where @xmath57 ( with sub - vectors @xmath58 , for @xmath59 ) and @xmath60 are lagrange multipliers and , @xmath61 and @xmath62 are positive constants .",
    "the solution of ( [ eq : glf_admm ] ) is generated by the following successive approximations @xmath63 } & = \\mathop{\\arg \\min}\\limits_{\\small \\xbf } \\call\\left ( \\xbf,\\ubf^{[n-1]},\\zbf^{[n-1]},\\rhobf_u^{[n-1]},\\rhobf_z^{[n-1 ] } \\right ) \\label{eq : glf_admm_op_x } \\\\",
    "\\ubf^{[n ] } & = \\mathop{\\arg \\min}\\limits_{\\small \\ubf } \\call\\left ( \\xbf^{[n-1]},\\ubf,\\rhobf_u^{[n-1 ] } \\right ) \\label{eq : glf_admm_op_u } \\\\ \\zbf^{[n ] } & = \\mathop{\\arg \\min}\\limits_{\\small \\zbf } \\call\\left ( \\xbf^{[n-1]},\\zbf,\\rhobf_z^{[n-1 ] } \\right ) \\label{eq : glf_admm_op_z } \\end{aligned}\\ ] ] and the multipliers are updated as @xmath64 } & = \\rhobf_u^{[n-1 ] } + c_u(\\xbf^{[n]}-\\ubf^{[n ] } ) \\label{eq : rho_u } \\\\",
    "\\rhobf_z^{[n ] } & = \\rhobf_z^{[n-1 ] } + c_z(\\dbf\\xbf^{[n]}-\\zbf^{[n ] } ) .",
    "\\label{eq : rho_z}\\end{aligned}\\ ] ] the closed - form solution for ( [ eq : glf_admm_op_x ] ) at the @xmath65th iteration can be derived to be @xmath63 } & = \\left ( \\phibf^t\\phibf + c_z\\dbf^t\\dbf + c_u\\ibf_n \\right)^{-1 } \\notag \\\\ & \\times \\left ( \\phibf^t\\ybf - \\dbf^t\\rhobf_z^{[n-1 ] } + c_z\\dbf^t\\zbf^{[n-1 ] } - \\rhobf_u^{[n-1 ] } + c_u \\ubf^{[n-1 ] } \\right ) .",
    "\\label{eq : glf_admm_x_cf}\\end{aligned}\\ ] ] we can see from ( [ eq : glf_admm_x_cf ] ) that the matrix inversion part does not change during the iterations so that it can be performed off - line , resulting in reduced complexity . note that the matrix inversion lemma can be used to further ease the computations involved in the inversion operation .    for @xmath46 ,",
    "note that the optimization involves two penalties , i.e. , apart from penalizing each element of @xmath46 for sparsity , we need to optimize on each of its sub - groups as well .",
    "since both penalties are non - differentiable , we utilize the fact that soft thresholding generates a minimizer for the cost function involving @xmath66 @xcite , and for the cost function involving @xmath67 , the minimizer is @xmath68 in case @xmath69 and the minimizer is a vector @xmath70 such that @xmath71 in case @xmath72 @xcite .",
    "thus the closed - form solution of ( [ eq : glf_admm_op_u ] ) for the @xmath5th subgroup at the @xmath65th iteration can be written as @xmath73 } = & \\left ( \\| \\cals \\left(\\xbf_i^{[n-1 ] } + \\frac{\\rhobf_{u_i}^{[n-1]}}{c_u } , \\frac{\\lambda_e}{c_u}\\right ) \\|_2 ^ 2 - \\frac{\\lambda_g}{c_u }   \\right)_+ \\notag \\\\ & \\times \\dfrac{\\cals\\left(\\xbf_i^{[n-1 ] } + \\frac{\\rhobf_{u_i}^{[n-1]}}{c_u } , \\frac{\\lambda_e}{c_u}\\right ) } { \\| \\cals \\left(\\xbf_i^{[n-1 ] } + \\frac{\\rhobf_{u_i}^{[n-1]}}{c_u } , \\frac{\\lambda_e}{c_u}\\right ) \\|_2 ^ 2 } \\label{eq : glf_admm_ui_cf}\\end{aligned}\\ ] ] for @xmath59 , where @xmath74 is the soft thresholding operator .",
    "thus the estimate of @xmath46 can be obtained as ^[n ] = [ _ 0^[n]t,_1^[n]t,,_g-1^[n]t]^t [ eq : glf_admm_u_cf ] which along with @xmath75}$ ] is used to update @xmath76}$ ] in ( [ eq : rho_u ] ) .    now from ( [ eq : glf_admm_op_z ] ) , the closed - form expression for the estimate of @xmath47 at the @xmath65th iteration can be derived as ^[n ] = ( ^[n-1 ] + , ) [ eq : glf_admm_u_cf ] which subsequently updates @xmath77}$ ] in ( [ eq : rho_z ] ) .      for",
    "the latent group fused lasso ( lgf - lasso ) , the signal is segmented into many overlapping groups of certain sizes .",
    "in contrast to the disjoints groups , overlapping groups can reselect the elements from other groups .",
    "we create @xmath78 overlapping groups through an @xmath79 sub - selection matrices @xmath80 which select @xmath81 rows from an identity matrix @xmath7 .",
    "an overlapping group can then be obtained by the relation , @xmath82 , for @xmath83 , where @xmath80 is such that @xmath84^t$ ] . each sub - selection matrix @xmath80 repeats @xmath85 rows of @xmath86 , where @xmath85 is the overlapping factor and @xmath87 .",
    "figure  [ fig : ogl_model ] schematically shows the difference between disjoint ( @xmath88 ) and overlapping groups ( for @xmath89 ) .",
    "we can see that the overlapping groups can solve the problem of the fixed group size but the price to be paid is in terms of computational complexity which increases excessively with the factor @xmath85 due to the related increase in @xmath78 .",
    "= 3.25 in    now , the optimization problem for lgf - lasso can be formulated as = _",
    "-_2 ^ 2 + _ g _ i=0 ^ -1 _",
    "i_2 ^ 1 + _ f _ 1 ^ 1 [ eq : oglf_x ] which does not contain an element - wise sparsity term as required in ( [ eq : sglf_x ] ) .",
    "to solve the lgf - lasso problem , we again turn to admm . by introducing a new auxiliary variable @xmath90 of size @xmath91 , ( [ eq : oglf_x ] )",
    "can be written as @xmath92 = \\mathop{\\arg \\min}\\limits_{\\small \\xbf , \\tilde{\\ubf } , \\zbf }   & \\frac{1}{2 } \\|\\ybf-\\phibf\\xbf\\|_2 ^ 2 + \\lambda_g \\sum_{i=0}^{\\tilde{g}-1 } \\|\\tilde{\\ubf}_i\\|_2 ^ 1 + \\lambda_f \\|\\zbf\\|_1 ^ 1 \\notag \\\\ \\text{s.t . } \\;\\;\\ ; \\tilde{\\ubf}_i&=\\wbf_i\\xbf,\\;{0\\le i\\le \\tilde{g}-1 } , \\;\\ ; \\zbf = \\dbf\\xbf \\label{eq : olf_admm}\\end{aligned}\\ ] ] where @xmath93 is an @xmath50 sub - vector of @xmath90 , i.e. , @xmath94^t$ ] . now",
    "the lagrangian for ( [ eq : olf_admm ] ) can be written as @xmath95 where @xmath96 collects the lagrangian multipliers with sub - vectors @xmath97 for @xmath83 .",
    "now the successive approximations for the solution of ( [ eq : glf_admm_lagr ] ) w.r.t .",
    "@xmath6 , @xmath90 and @xmath96 can be written as @xmath63 } & = \\mathop{\\arg \\min}\\limits_{\\small \\xbf } \\call\\left ( \\xbf,\\tilde{\\ubf}^{[n-1]},\\zbf^{[n-1]},\\rhobf_{\\tilde{u}}^{[n-1]},\\rhobf_z^{[n-1 ] } \\right ) \\label{eq : oglf_admm_op_x } \\\\",
    "\\tilde{\\ubf}^{[n ] } & = \\mathop{\\arg \\min}\\limits_{\\small \\ubf } \\call\\left ( \\xbf^{[n-1]},\\tilde{\\ubf},\\rhobf_{\\tilde{u}}^{[n-1 ] } \\right ) \\label{eq : oglf_admm_op_u } \\\\ \\rhobf_{\\tilde{u}}^{[n ] } & = \\rhobf_{\\tilde{u}}^{[n-1 ] } + c_u(\\xbf^{[n]}-\\tilde{\\ubf}^{[n ] } ) \\label{eq : rho_u_til } \\end{aligned}\\ ] ] whereas , the expressions for the estimates of @xmath47 and @xmath60 are the same as in ( [ eq : glf_admm_op_z ] ) and ( [ eq : rho_z ] ) , respectively .    from ( [ eq : oglf_admm_op_x ] ) , the closed - form expression for the estimate of @xmath6 at the @xmath65th iteration can be derived as @xmath98 } = \\left ( \\phibf^t\\phibf + c_z\\dbf^t\\dbf + c_u\\wbf^t\\wbf \\right)^{-1 } \\notag \\\\ & \\times \\left ( \\phibf^t\\ybf - \\dbf^t\\rhobf_z^{[n-1 ] } + c_z\\dbf^t\\zbf^{[n-1 ] } - \\wbf^t(\\rhobf_u^{[n-1 ] } - c_u \\ubf^{[n-1 ] } ) \\right ) .",
    "\\label{eq : oglf_admm_x_cf}\\end{aligned}\\ ] ] where we can see that as @xmath99 is already known , the matrix inversion part of the estimate can again be obtained off - line and does not need to be estimated for each iteration .    from ( [ eq : oglf_admm_op_u ] ) , the closed - form expression for the estimate of @xmath93 , for @xmath100 , at the @xmath65th iteration can be derived as @xmath101 } = & \\left ( \\| \\wbf_i\\xbf^{[n-1 ] } + \\frac{\\rhobf_{\\tilde{u}_i}^{[n-1]}}{c_u } \\|_2 ^ 2 - \\frac{\\lambda_g}{c_u }   \\right)_+ \\notag \\\\ & \\times \\dfrac{\\wbf_i\\xbf^{[n-1 ] } + \\frac{\\rhobf_{\\tilde{u}_i}^{[n-1]}}{c_u } } { \\| \\wbf_i\\xbf^{[n-1 ] } + \\frac{\\rhobf_{\\tilde{u}_i}^{[n-1]}}{c_u } \\|_2 ^ 2 } \\label{eq : oglf_admm_ui_cf}\\end{aligned}\\ ] ] and the estimate for @xmath102}$ ] can be obtained as ^[n ] = [ _ 0^[n]t,_1^[n]t,,_g-1^[n]t]^t [ eq : oglf_admm_u_cf ] which is then used to update the multipliers @xmath96 in ( [ eq : rho_u_til ] ) .",
    "in this section , we present some simulation results to compare the performance of our proposed algorithms . we compare the performance of sgf - lasso , lgf - lasso and g - lasso .",
    "we consider a random test signal of length @xmath103 , which is composed of a couple of blocks of exponentially decaying elements , a step signal block and a lone small group of nonzero elements , along with multiple zero blocks .",
    "such a mixture is mostly expected in smooth signals .",
    "the noise variance has been considered as , @xmath104 .",
    "the signal is sensed through the measurement matrix @xmath25 , which has been drawn from a zero - mean gaussian distribution with variance @xmath105 .",
    "we have further orthogonalized the rows of matrix @xmath25 .",
    "the penalty parameters for the simulations have been considered as @xmath106 , @xmath107 and @xmath108 . in general , these parameters can be selected from a given range in a cross - validation manner , by varying one of the parameters and keeping others fixed @xcite .",
    "further , since all of these parameters are sparsity promoting , and can possibly affect each other , it is expected that the search of the optimal set of parameters would be restricted to a smaller range .",
    "the parameters @xmath61 and @xmath62 are positive numbers and may affect the convergence rate . we take them as @xmath109 . as initial conditions , the vectors @xmath110}$ ] , @xmath111}$ ] , @xmath112}$ ] , @xmath113}$ ] , @xmath114}$ ] , @xmath115}$ ] and @xmath116}$ ] , have all been considered as zero vectors , respectively .",
    "note that , a least - squares solution of @xmath6 , can also be considered as a warm - start to speed up the convergence rate .",
    "the group size for sgf - lasso , lgf - lasso and g - lasso has been taken as @xmath117 . therefore , the number of groups in sgf - lasso and g - lasso are the same , i.e. , @xmath118",
    ". for lgf - lasso , an overlapping factor of @xmath119 has been used , and therefore the number of overlapping groups of size @xmath117 are @xmath120 .",
    "we use a maximum of @xmath121 iterations for each algorithm .",
    "we have observed that a tolerance level of @xmath122 between consecutive updates is reached much earlier than this limit , and therefore we stop the algorithm at this stage .",
    "figure  [ fig : glf_sigrecon_comp_01 ] shows the reconstruction performance of sgf - lasso , lgf - lasso and g - lasso when the signal was sensed with a compression ratio @xmath123 .",
    "we can see that the performance of sgf - lasso and lgf - lasso is very close to each other and both are able to recover the smooth transitions of the original signal .",
    "sgf - lasso has an edge over lgf - lasso , as it better reconstructs even a very small group of nonzero elements .",
    "on the other hand , the performance of g - lasso deteriorates both on the front of smoothness as well as block size .",
    "note that in contrast to sgf - lasso and lgf - lasso , @xmath39 is the only sparsity creating parameter for g - lasso .",
    "therefore , we increase its value to @xmath124 , which is the minimum to recreate the actual zero blocks .    figure  [ fig : glf_sigrecon_mse_01 ] shows the performance comparison of the proposed algorithms through the mean squared error ( mse ) metric against varying compression ratios , where @xmath125 .",
    "we can see that the performance improves in general with increasing value of @xmath126 , for @xmath127 .",
    "nonetheless , the difference in performance follows the previously observed pattern .",
    "sgf - lasso keeps an edge over lgf - lasso , whereas g - lasso remains quite far away .",
    "here we would like to mention that sgf - lasso has an edge over lgf - lasso with a lower number of groups .",
    "lgf - lasso can have improved performance by increasing the overlapping factor but that would cause a subsequent increase in the computational complexity .",
    "= 3.5 in    = 3.5 in",
    "in this letter , we have proposed two new lasso formulations , namely , sparse group fused lasso and latent group fused lasso . the former uses element - wise sparsity , group sparsity ( over disjoint groups ) and fusion penalties , whereas the latter combines the fusion penalty with a latent group penalty .",
    "both formulations can be used to reconstruct smooth signals from their compressed measurements .",
    "we also provide low - complexity solvers for the proposed formulations , based on the alternating direction method of multipliers .",
    "we compared the performance of our proposed algorithms with standard group lasso over a smooth test signal .",
    "the simulation results confirm the better performance of the proposed algorithms for signal reconstruction against group lasso .",
    "similar results were obtained for the mean squared error metric , for varying compression ratios .",
    "e. cands , j. romberg and t. tao , `` robust uncertainty principles : exact signal reconstruction from highly incomplete frequency information , '' _ ieee transaction on information theory _ , vol .",
    "489 - 509 , february 2006 ."
  ],
  "abstract_text": [
    "<S> we present reconstruction algorithms for smooth signals with block sparsity from their compressed measurements . </S>",
    "<S> we tackle the issue of varying group size via group - sparse least absolute shrinkage selection operator ( lasso ) as well as via latent group lasso regularizations . </S>",
    "<S> we achieve smoothness in the signal via fusion . </S>",
    "<S> we develop low - complexity solvers for our proposed formulations through the alternating direction method of multipliers .    </S>",
    "<S> compressed sensing , block sparsity , smoothness , signal reconstruction </S>"
  ]
}