{
  "article_text": [
    "since the early work of sobol @xcite , global sensitivity analysis ( gsa ) has received a lot of attention in the computer code experiments community .",
    "these days variance - based sensitivity indices are common tools in the analysis of complex physical phenomena .",
    "several statistical estimators have been proposed @xcite and their asymptotic properties are now well understood @xcite .",
    "in addition , the case of computationally expensive codes has been investigated thoroughly with the introduction of several dedicated surrogate models @xcite .",
    "however , even if they are extremely popular and informative importance measures , variance - based indices suffer from theoretical and practical limitations .",
    "first , by definition they only study the impact of the input parameters on the variance of the output .",
    "since this is a restricted summary of the output distribution , this measure happens to be inadequate for many case studies .",
    "alternative approaches include for example density - based indices @xcite , derivative - based measures @xcite , or goal - oriented dedicated indices @xcite .",
    "second , variance - based indices do not generalize easily to the case of a multivariate output @xcite .",
    "unfortunately , computer code outputs often consist of several scalars or even time - dependent curves , which limits severely the practical use of standard indices . finally for high - dimensional problems ,",
    "a preliminary screening procedure is usually mandatory before the analysis of the computer code or the modeling with a surrogate .",
    "the computational cost of gsa is in general too high to envision its use for screening purposes and more qualitative approaches are thus needed , e.g. the morris method @xcite or group screening @xcite .    in this paper",
    ", we propose a completely original point of view in order to overcome these limitations .",
    "starting from the general framework of gsa and the concept of dissimilarity measure , we introduce a new class of sensitivity indices which comprises as a special case the density - based index of @xcite .",
    "we propose an estimation procedure relying on density ratio estimation and show that it gives access to several different indices for the same computational cost .",
    "more importantly , we highlight that other special cases lead to well - known dependence measures , including the mutual information .",
    "this link motivates us to investigate the potential of recent state - of - the - art dependence measures as new sensitivity indices , such as the distance correlation @xcite or the hilbert - schmidt independence criterion @xcite .",
    "an appealing property of such measures is that they can handle multivariate random variables very easily .",
    "we also discuss how feature selection methods based on these measures can effectively replace standard screening procedures .",
    "the structure of the paper is as follows . in section [ dissim ]",
    ", we introduce the general gsa framework based on dissimilarity measures and discuss the use of csiszr f - divergences .",
    "we also emphasize the link with mutual information and propose an estimation procedure . in section [ indep ] ,",
    "we give a review of some dependence measures and show how they can be used as new sensitivity indices .",
    "we also provide examples of feature selection techniques in which they are involved .",
    "screening will then be seen as an equivalent to feature selection in machine learning .",
    "finally , several numerical experiments are conducted in section [ secexp ] on both analytical and industrial applications . in particular , we illustrate the potential of dependence measures for gsa .",
    "denote @xmath0 the computer code output which is a function of the @xmath1 input random variables @xmath2 , @xmath3 where @xmath4 is assumed to be continuous . in standard global sensitivity analysis",
    ", it is further assumed that the @xmath2 have a known distribution and are independent . as pointed out by @xcite , a natural way of defining the impact of a given input @xmath2 on @xmath5",
    "is to consider a function which measures the similarity between the distribution of @xmath5 and that of @xmath6 .",
    "more precisely , the impact of @xmath2 on @xmath5 is given by @xmath7 where @xmath8 denotes a dissimilarity measure between two random variables .",
    "the advantage of such a formulation is that many choices for @xmath9 are available , and we will see in what follows that some natural dissimilarity measures yield sensitivity indices related to well known quantities . however before going further , let us note that the naive dissimilarity measure @xmath10 where random variables are compared only through their mean values produces the unnormalized sobol first - order sensivity index @xmath11 .      assuming all input random variables have an absolutely continuous distribution with respect to the lebesgue measure on @xmath12 , the f - divergence @xcite between @xmath5 and @xmath6 is given by @xmath13 where @xmath14 is a convex function such that @xmath15 and @xmath16 and @xmath17 are the probability distribution functions of @xmath5 and @xmath6 , respectively .",
    "standard choices for function @xmath14 include for example    * kullback - leibler divergence : @xmath18 or @xmath19 ; * hellinger distance : @xmath20 ; * total variation distance : @xmath21 ; * pearson @xmath22 divergence : @xmath23 or @xmath24 ; * neyman @xmath22 divergence : @xmath25 or @xmath26 . +    plugging this dissimilarity measure in ( [ si ] ) yields the following sensitivity index : @xmath27 where @xmath28 and @xmath29 are the probability distribution functions of @xmath2 and @xmath30 , respectively .",
    "first of all , note that inequalities on csiszr f - divergences imply that such sensitivity indices are positive and equal zero when @xmath5 and @xmath31 are independent .",
    "also , it is important to note that given the form of @xmath32 , it is invariant under any smooth and uniquely invertible transformation of the variables @xmath2 and @xmath5 , see the proof for mutual information in @xcite .",
    "this is a major advantage over variance - based sobol sensitivity indices , which are only invariant under linear transformations .",
    "+ it is easy to see that the total variation distance with @xmath21 gives a sensivity index equal to the one proposed by @xcite : @xmath33 in addition , the kullback - leibler divergence with @xmath18 yields @xmath34 that is the mutual information @xmath35 between @xmath2 and @xmath5 .",
    "a normalized version of this sensitivity index was studied by @xcite .",
    "similarly , the neyman @xmath22 divergence with @xmath26 leads to @xmath36 which is the so - called squared - loss mutual information between @xmath2 and @xmath5 ( or mean square contingency ) .",
    "these results show that some previously proposed sensitivity indices are actually special cases of more general indices defined through csiszr f - divergences . to the best of our knowledge ,",
    "this is the first work in which this link is highlighted .",
    "moreover , the specific structure of equation ( [ sif ] ) makes it possible to envision more efficient tools for the estimation of these sensitivity indices .",
    "indeed , it only involves approximating a density ratio rather than full densities .",
    "this point is investigated in the next subsection . but more importantly",
    ", we see that special choices for @xmath14 define sensivity indices that are actually well - known dependence measures such as the mutual information .",
    "this paves the way for completely new sensitivity indices based on recent state - of - the - art dependence measures , see section [ indep ] .      coming back to equation ( [ sif ] )",
    ", the goal is to estimate @xmath37 where @xmath38 is the ratio between the joint density of @xmath30 and the marginals .",
    "of course , straightforward estimation is possible if one estimates the densities @xmath39 , @xmath40 and @xmath41 with e.g. kernel density estimators .",
    "however , it is well known that density estimation suffers from the curse of dimensionality .",
    "this limits the possible multivariate extensions we discuss in the next subsection .",
    "besides , since only the ratio function @xmath42 is needed , we expect more robust estimates by focusing only on it .",
    "let us assume now that we have a sample @xmath43 of @xmath30 , the idea is to build first an estimate @xmath44 of the ratio .",
    "the final estimator @xmath45 of @xmath32 will then be given by @xmath46 powerful estimating methods for ratios include e.g. maximum - likelihood estimation @xcite , unconstrained least - squares importance fitting @xcite , among others ( see * ? ? ?",
    "a k - nearest neighbors strategy dedicated to mutual information is also discussed in @xcite .",
    "given our approach focusing only on densities , it is straightforward to extend the definition of the sensitivity index in equation ( [ sif ] ) to any number of input and output variables .",
    "we can then study the impact of a given group of input variables @xmath47 where @xmath48 is a subset of @xmath49 on a multivariate output @xmath50 with the sensitivity index given by @xmath51 this favorable generalization was already mentioned for the special cases of the total - variation distance and mutual information by @xcite and @xcite , respectively .",
    "however , in the high - dimensional setting , estimation of such sensitivity indices is infeasible since even the ratio trick detailed above fails .",
    "this is thus a severe limitation for screening purposes .",
    "we examine efficient alternatives in section [ indep ] .    moreover , note that extending the naive dissimilarity measure ( [ naive ] ) to the multivariate output case naturally leads to consider @xmath52 .",
    "straightforward calculations reveal that the corresponding sensitivity index is then the sum of sobol first - order sensitivity indices on each output .",
    "@xcite showed that this multivariate index is the only one possessing desired invariance properties in the variance - based index family .",
    "we focused above on csiszr f - divergences but other dissimilarity measures exist to compare probability distributions .",
    "in particular , integral probability metrics ( ipm , * ? ? ?",
    "* ) are a popular family of distance measures on probabilities given by @xmath53 for two probability measures @xmath54 and @xmath55 and where @xmath56 is a class of real - valued bounded measurable functions on @xmath57 .",
    "just as the choice of function @xmath14 in csiszr f - divergences gives rise to different measures , the choice of @xmath56 generates different ipms , e.g. the wasserstein distance , the dudley metric or the total variation distance .",
    "it is interesting to note that csiszr f - divergences and ipms are very distinct classes of measures , since they only intersect at the total variation distance @xcite .",
    "unfortunately , plugging the general expression ( [ ipm ] ) of an ipm in equation ( [ si ] ) no longer yields a closed - form expression for a sensitivity index .",
    "however , we plan to study such indices in a future work since estimation of ipms appears to be easier than for csiszr f - divergences and is independent of the dimensionality of the random variables @xcite .",
    "finally , let us mention the recent work of @xcite on goal - oriented measures , where they introduce a new class of sensitivity indices @xmath58 where @xmath59 is the contrast function associated to the features of interest @xmath60 and @xmath61 of @xmath5 and @xmath5 conditionally to @xmath62 , respectively ( note that we only give here the unnormalized version of the index ) .",
    "it is easy to check that ( [ contrast ] ) is a special case of ( [ si ] ) .",
    "given two random vectors @xmath63 in @xmath64 and @xmath5 in @xmath65 , dependence measures aim at quantifying the dependence between @xmath63 and @xmath5 in arbitrary dimension , with the property that the measure equals zero if and only if @xmath63 and @xmath5 are independent .",
    "in particular , they are useful when one wants to design a statistical test for independence . here , we focus on the long - known mutual information criterion , as well as on the novel distance correlation measure @xcite .",
    "recently , @xcite showed that it shares deep links with distances between embeddings of distributions to reproducing kernel hilbert spaces ( rkhs ) and especially the hilbert - schmidt independence criterion ( hsic , * ? ? ? * ) which will also be discussed .",
    "finally , we will review feature selection techniques introduced in machine learning which make use of these dependence measures .      mutual information ( mi , * ? ? ?",
    "* ) is a symmetric measure of dependence which is related to the entropy . assuming @xmath63 and @xmath5 are two random vectors which are absolutely continuous with respect to the lebesgue measure on @xmath64 and @xmath65 with density functions @xmath66 and @xmath41 , respectively , one can define their marginal entropy : @xmath67 and @xmath68 similarly . denoting @xmath69 their joint density function",
    ", the joint entropy between @xmath63 and @xmath5 writes @xmath70 mi is then formally defined as @xmath71 interestingly , mi equals zero if and only if @xmath63 and @xmath5 are independent .",
    "this implies that mi is able to detect nonlinear dependencies between random variables , unlike the correlation coefficient .",
    "it is also easy to check that @xmath72 with jensen s inequality .",
    "further note that it is not a distance since it does not obey the triangle inequality .",
    "a simple modified version yielding a distance , the variation of information ( vi ) , is given by @xmath73 another variant is the squared - loss mutual information ( smi , * ? ? ?",
    "* ) : @xmath74 which is again a dependence measure verifying @xmath75 with equality if and only if @xmath63 and @xmath5 are independent .",
    "applications of mi , vi and smi include independent component analysis @xcite , image registration @xcite and hierarchical clustering @xcite , among many others .",
    "+ in the context of global sensitivity analysis , we have seen in section [ fdiv ] that mi and smi arise as sensitivity indices when specific csiszr f - divergences are chosen to evaluate the dissimilarity between the output @xmath5 and the conditional output @xmath6 where @xmath2 is one of the input variables .",
    "we will then study the two following sensitivity indices : @xmath76 and @xmath77 a normalized version of @xmath78 given by @xmath79 has already been proposed by @xcite and compared to sobol sensitivity indices by @xcite .      the distance correlation was introduced by @xcite to address the problem of testing dependence between two random vectors @xmath63 in @xmath64 and @xmath5 in @xmath65 .",
    "it is based on the concept of distance covariance which measures the distance between the joint characteristic function of @xmath80 and the product of the marginal characteristic functions .",
    "more precisely , denote @xmath81 and @xmath82 the characteristic function of @xmath63 and @xmath5 , respectively , and @xmath83 their joint characteristic function . for a complex - valued function @xmath84",
    ", we also denote @xmath85 the complex conjugate of @xmath86 and @xmath87 .",
    "the distance covariance ( dcov ) @xmath88 between @xmath63 and @xmath5 with finite first moment is then defined as a weighted @xmath89-distance between @xmath83 and @xmath90 given by @xmath91 where the weight function @xmath92 with constants @xmath93 for @xmath94 is chosen to ensure invariance properties , see @xcite .",
    "the distance correlation ( dcor ) @xmath95 between @xmath63 and @xmath5 is then naturally defined as @xmath96 if @xmath97 and equals @xmath98 otherwise .",
    "important properties of the distance correlation introduced in ( [ dcor ] ) include that @xmath99 and @xmath100 if and only if @xmath63 and @xmath5 are independent .",
    "interestingly , the distance covariance in ( [ dcov ] ) can be computed in terms of expectations of pairwise euclidean distances , namely @xmath101 \\label{dcove}\\end{aligned}\\ ] ] where @xmath102 is an i.i.d .",
    "copy of @xmath80 .",
    "concerning estimation , let @xmath103 be a sample of the random vector @xmath80 .",
    "following equation ( [ dcove ] ) , an estimator @xmath104 of @xmath105 is then given by @xmath106 .",
    "\\label{dcovemp}\\end{aligned}\\ ] ] denoting @xmath107 , @xmath108 , @xmath109 , @xmath110 , @xmath111 and similarly @xmath112 for @xmath113 , @xcite show that equation ( [ dcovemp ] ) can be written as @xmath114 and is also equal to equation ( [ dcov ] ) if one uses the empirical characteristic functions computed on the sample @xmath103 .",
    "the empirical distance correlation @xmath115 is then @xmath116 and satisfies @xmath117 .",
    "although @xmath104 is a consistent estimator of @xmath105 , it is easy to see that it is biased . @xcite",
    "propose an unbiased version of @xmath104 and a specific correction for the high - dimensional case @xmath118 is investigated in @xcite .",
    "further note that @xcite also study @xmath119 defined as @xmath120 \\label{dcova}\\end{aligned}\\ ] ] with the new weight function @xmath121 and constants @xmath122 as soon as @xmath123 and @xmath124 .",
    "distance covariance is retrieved for @xmath125 .",
    "the very general case of @xmath63 and @xmath5 living in metric spaces has been examined by @xcite .",
    "more precisely , let @xmath126 and @xmath127 be metric spaces of negative type ( see * ? ? ?",
    "* ) , the generalized distance covariance @xmath128 \\label{dcovmetric}\\end{aligned}\\ ] ] characterizes independence between @xmath129 and @xmath130 .",
    "+ coming back to sensitivity analysis , just like we defined a new index based on mutual information , we can finally introduce an index based on distance correlation , i.e. @xmath131 which will measure the dependence between an input variable @xmath2 and the output @xmath5 . since distance correlation is designed to detect nonlinear relationships , we except this index to quantify effectively the impact of @xmath2 on @xmath5 .",
    "besides , considering that distance covariance is defined in arbitrary dimension , this index generalizes easily to the multivariate case : @xmath132 for evaluating the impact of a group of inputs @xmath133 on a multivariate output @xmath5 .",
    "[ rempf ] the limiting case @xmath134 in ( [ dcova ] ) interestingly leads to @xmath135 , see @xcite .",
    "this turns out to be another original way for defining a new sensitivity index .",
    "indeed , recall that sobol first - oder sensitivity index actually equals @xmath136 where @xmath137 is an independent copy of @xmath5 obtained by fixing @xmath2 , see @xcite .",
    "the idea is then to replace the covariance ( obtained with @xmath134 ) by dcov ( with @xmath125 ) : @xmath138 where pf stands for pick - and - freeze , since this index generalizes the pick - and - freeze estimator proposed by @xcite and is able to detect nonlinear dependencies , unlike the correlation coefficient .",
    "the hilbert - schmidt independence criterion proposed by @xcite builds upon kernel - based approaches for detecting dependence , and more particularly on cross - covariance operators in rkhss . here , we only give a brief summary and introduction on this topic and refer the reader to @xcite for details .",
    "let the random vector @xmath129 have distribution @xmath139 and consider a rkhs @xmath56 of functions @xmath140 with kernel @xmath141 and dot product @xmath142 .",
    "similarly , we can also define a second rkhs @xmath143 of functions @xmath144 with kernel @xmath145 and dot product @xmath146 associated to the random vector @xmath130 with distribution @xmath147 . by definition ,",
    "the cross - covariance operator @xmath148 associated to the joint distribution @xmath149 of @xmath80 is the linear operator @xmath150 defined for every @xmath151 and @xmath152 as @xmath153 -\\mathbb{e}_xf(x)\\mathbb{e}_yg(y).\\ ] ] in a nutshell , the cross - covariance operator generalizes the covariance matrix by representing higher order correlations between @xmath63 and @xmath5 through nonlinear kernels . for every linear operator @xmath154 and provided the sum converges , the hilbert - schmidt norm of @xmath155 is given by @xmath156 where @xmath157 and @xmath158 are orthonormal bases of @xmath56 and @xmath143 , respectively .",
    "this is simply the generalization of the frobenius norm on matrices . the hsic criterion is then defined as the hilbert - schmidt norm of the cross - covariance operator : @xmath159 \\label{hsic}\\end{aligned}\\ ] ] where the last equality in terms of kernels is proven in @xcite .",
    "an important property of @xmath160 is that it equals @xmath98 if and only if @xmath63 and @xmath5 are independent , as long as the associated rkhss @xmath56 and @xmath143 are universal , i.e. they are dense in the space of continuous functions with respect to the infinity norm @xcite",
    ". examples of kernels generating universal rkhss are e.g. the gaussian and the laplace kernels @xcite .",
    "it is interesting to note the similarity between the generalized distance covariance of equation ( [ dcovmetric ] ) and the hsic criterion ( [ hsic ] ) .",
    "actually , @xcite recently studied the deep connection between these approaches and show that @xmath161 if the kernels @xmath141 and @xmath145 generate the metrics @xmath162 and @xmath163 , respectively ( see * ? ? ?",
    "in particular , the standard distance covariance ( [ dcove ] ) is retrieved with the ( universal ) kernel @xmath164 which generates the metric @xmath165 .    assume now that @xmath103 is a sample of the random vector @xmath80 and denote @xmath166 and @xmath167 the gram matrices with entries @xmath168 and @xmath169 .",
    "@xcite propose the following consistent estimator for @xmath160 : @xmath170 where @xmath171 is the centering matrix such that @xmath172 .",
    "besides , it is easy to check that @xmath173 can be expressed just like the empirical distance covariance ( [ dcovemp ] ) : @xmath174.\\end{aligned}\\ ] ] an unbiased estimator is also introduced by @xcite .",
    "+ we can finally propose a sensitivity index generalizing ( [ sdcor ] ) : @xmath175 where the kernel - based distance correlation is given by @xmath176 and the kernels inducing @xmath56 and @xmath143 have to be chosen within the class of universal kernels .",
    "the multivariate extension of @xmath177 is straightforward .",
    "the impact of the choice of kernels has previously been studied by @xcite in the context of independence hypothesis tests .    instead of working with the cross - covariance operator @xmath148 , @xcite work with the normalized cross - covariance operator ( nocco ) @xmath178 defined as @xmath179 ,",
    "see @xcite for the existence of this representation .",
    "just as the hsic criterion , the associated measure of dependence is given by @xmath180 .",
    "interestingly , @xmath181 is independent of the choice of kernels and is actually equal to the squared - loss mutual information ( [ smi ] ) under some assumptions , see @xcite . despite the advantage of being kernel - free , using @xmath182 in practice unfortunately requires to work with an estimator with a regularization parameter , which has to be selected @xcite . nevertheless , it is still interesting to use this approach for approximating smi efficiently , since dimensionality limitations related to density function estimation no longer apply .",
    "the pick - and - freeze estimator defined in remark [ rempf ] can be readily generalized with kernels : @xmath183 where this time only the kernel acting on @xmath184 needs to be specified .",
    "the kernel point of view in hsic also provides an elegant and powerful framework for dealing with categorical inputs and outputs , as well as functional ones .",
    "the categorical case is common practice in feature selection , since the target output is often represented as labels .",
    "appropriate kernels include for example @xmath186 where @xmath187 is the number of samples with label @xmath188 , see e.g. @xcite . from a gsa perspective",
    ", this implies that we can evaluate the impact of the inputs on level sets of the output by a simple change of variable @xmath189 for a given threshold @xmath190 .",
    "we can note the resemblance with the approach of @xcite if one uses a contrast function adapted to exceedance probabilities .    as a matter of fact , it is also possible to design dedicated semi - metrics for functional data which can be incorporated in the definition of the kernels , see e.g. @xcite .",
    "for example , let @xmath191 be such a semi - metric defined on @xmath192 when the output variable is of functional type .",
    "the kernel associated to @xmath184 is then given by @xmath193 where @xmath194 is a kernel acting on @xmath12 .",
    "the same scheme applies to functional inputs as well , see @xcite for an illustration in the context of surrogate modeling where the semi - metric is a cheap and simplified computer code .",
    "however , a theoretical shortcoming lies in our current inability to check if such semi - metric kernels are universal , which implies that we can not claim that independence can be detected . despite this deficiency ,",
    "we show in section [ secexp ] that from a practical perspective , the use of a semi - metric based on principal components can efficiently deal with a functional output given as a 2d map .      in machine learning ,",
    "feature selection aims at identifying relevant features ( among a large set ) with respect to a prediction task .",
    "the goal is to detect irrelevant or redundant features which may increase the prediction variance without reducing its bias . as a matter of fact",
    ", this closely resembles the objective of factor screening in gsa .",
    "the main difference is that in gsa , input variables are usually assumed to be independent , whereas in feature selection redundant features , i.e. highly dependent factors , precisely have to be filtered out .",
    "this apparently naive distinction actually makes feature selection an interesting alternative to screening when some input variables are correlated .",
    "but it is important to note that it is also a powerful option even in the independent case .",
    "we do not intend here to give an exhaustive review of feature selection techniques , but rather detail some approaches which make use of the dependence measures we recapped above .",
    "we hope that it will illustrate how they can be used as new screening procedures in high dimensional problems .",
    "+ literature on feature selection is abundant and entails many approaches . in the high dimensional",
    "setting , model - based techniques include for example the lasso @xcite or sparse additive models @xcite , see @xcite for a selective overview .",
    "generalizations for the ultra - high dimensional case usually replace penalty - based techniques to focus on marginal regression , where an underlying model is still assumed ( e.g. linear @xcite or non - parametric additive @xcite ) .",
    "another line of work for the ultra - high dimensional setting are model - free methods , where only dependence measures are used to identify relevant features . except for the very specific hsic lasso technique @xcite ,",
    "here we only focus on pure dependence - based approaches .",
    "let us first introduce the concept of max - dependency @xcite .",
    "denote @xmath195 the set of available features , @xmath5 the target output to predict and @xmath196 any measure quantifying the dependence between two random vectors .",
    "the max - dependency scheme for feature selection involves finding @xmath197 features @xmath198 which jointly have the largest dependency with @xmath5 , i.e. one has to solve the following optimization problem @xmath199 solving ( [ maxd ] ) is however computationally infeasible when @xmath197 and @xmath1 are large for cardinality reasons .",
    "near - optimal solutions are then usually found by iterative procedures , where features are added one at a time in the subset @xmath198 ( forward selection ) . on the other hand , the dependence measure @xmath196 must also be robust to dimensionality , which is hard to achieve in practice when the number of samples is less than @xmath197 .",
    "consequently , marginal computations which only involve @xmath200 terms are usually preferred .",
    "the max - relevance criterion @xcite serves in this context as a proxy to max - dependency , where the optimization problem writes @xmath201 but when the features are dependent , it is likely that this criterion will select redundant features . to limit this effect",
    ", one can add a condition of min - redudancy expressed as @xmath202 the final scheme combining ( [ maxr ] ) and ( [ minr ] ) , called minimal - redundancy - maximal - relevance ( mrmr ) , is given by @xmath203 forward and backward procedures for mrmr are investigated by @xcite where @xmath196 is chosen as the mutual information .",
    "similarly , forward and backward approaches where mi is replaced with hsic is introduced by @xcite .",
    "a purely marginal point of view is studied by @xcite where the authors propose the dcor criterion ( [ dcor ] ) . in a nutshell",
    ", the dcor measure is computed between @xmath5 and each factor @xmath2 , @xmath3 and only the features with dcor above a certain threshold are retained .",
    "a sure screening property of this approach is also proven .",
    "@xcite extend this work by considering a modified version of the hsic dependence measure ( supremum of hsic over a family of universal kernels , denoted sup - hsic ) .",
    "even if the sure screening procedure of this generalized method is proven , the authors mention that every feature selection technique based on marginal computations fails at detecting features that may be marginally uncorrelated with the output but are in fact jointly correlated with it . as a result",
    ", they propose the following iterative approach :    1 .",
    "compute the marginal sup - hsic measures between @xmath5 and each feature @xmath2 , @xmath3 and select the inputs with a measure above a given threshold .",
    "let @xmath204 be the subset of selected features .",
    "2 .   compute sup - hsic between @xmath5 and @xmath205 for each @xmath206 .",
    "augment @xmath48 with features having a measure greater than the sup - hsic criterion between @xmath5 and @xmath207 .",
    "repeat until the subset of selected features stabilizes or when its cardinality reaches a given maximum value .    as pointed out previously",
    ", another drawback of marginal computations which is not taken care of by the above scheme is that redundant variables are not eliminated . but",
    "@xcite design another iterative procedure to deal with this case . finally , let us note that in the examples of section [ secexp ] , we will only study the above iterative technique since we focus on independent input factors .",
    "we plan to investigate in particular the full mrmr approach for problems with correlated inputs in a future work .",
    "instead of working with forward and backward approaches , @xcite propose a combination of the lasso and the hsic dependence measure .",
    "denote @xmath208 for @xmath3 and @xmath209 the centered gram matrices computed from a sample @xmath210 of @xmath211 following the notations of section [ sechsic ] .",
    "the hsic lasso solves the following optimization problem @xmath212 with constraints @xmath213 and where @xmath214 stands for the frobenius norm and @xmath215 is a regularization parameter .",
    "interestingly , the first term of equation ( [ hsiclasso ] ) expands as @xmath216 using that @xmath217 , @xmath218 are symmetric and @xmath171 is idempotent , which highlights the strong correspondence with the mrmr criterion ( [ mrmr ] ) . the authors show that ( [ hsiclasso ] ) can be recast as a standard lasso program and propose a dual augmented lagrangian algorithm to solve the optimization problem .",
    "they also discuss a variant based on the @xmath182 dependence measure .",
    "we mentioned before that feature selection techniques based on dependence measures have been particularly designed for the ultra - high dimensional case , which is not the common setting of screening problems in gsa .",
    "nevertheless , we illustrate in section [ secexp ] that they perform remarkably well on complex benchmark functions , while requiring very few samples of the output variable .",
    "this reveals their high potential for preliminary screening of expensive computer codes .",
    "in this section , we finally assess the performance of all the new sensitivity indices introduced before on a series of benchmark analytical functions and two industrial applications .",
    "all benchmark functions can be found in the virtual library of simulation experiments available at http://www.sfu.ca/~ssurjano/index.html . for easier comparison",
    ", we first summarize the proposed indices in table [ tabsi ] ( si stands for sensitivity index and see @xcite for rbd - fast ) .",
    "[ [ standard - gsa . ] ] standard gsa . + + + + + + + + + + + + +    for the first experiments , we focus on gsa problems where the dimensionality is not too large ( less than @xmath219 input variables ) .",
    "the objective is to compare the information given by the new indices with sobol first - order and total indices .",
    "* @xcite decreasing function @xmath220 with @xmath221 , @xmath222 .",
    "we compute the sensitivity indices based on csiszr f - divergences , dcor , pick - and - freeze dcor , hsic and pick - and - freeze hsic ( gaussian kernels ) with a sample of size @xmath223 and we repeat this calculation 100 times . here",
    "we use a simple kernel density estimator since we only study first - order indices .",
    "results are given in figure [ figlink ] .",
    ", @xmath224 , @xmath225 , @xmath226 and @xmath227 for function @xmath228 , @xmath223 , 100 replicates.,scaledwidth=90.0% ]    analytical first - order sis are @xmath229 for @xmath222 , which is coherent with the estimates at the top left .",
    "as expected , indices given by csiszr f - divergences , @xmath224 , @xmath225 , @xmath226 and @xmath227 provide the same information as variance - based ones in this simple case of a linear model . however , let us note that dcor and hsic detect non - influential factors very easily and robustly .",
    "* @xcite function @xmath230 with @xmath221 , @xmath222 ( the original function has constraint @xmath231 but we do not consider it here ) .",
    "conclusions are similar for the @xcite function , where only the first three inputs have a large impact on the output with very small interactions ( total sis almost equal first - order sis ) , see figure [ figloep ] .",
    "note that @xmath225 and @xmath227 recover @xmath232 since interactions are small .",
    "again , dcor and hsic clearly identify inputs which are independent of the output .",
    ", @xmath224 , @xmath225 , @xmath226 and @xmath227 for function @xmath233 , @xmath223 , 100 replicates.,scaledwidth=90.0% ]    * ishigami function @xcite @xmath234 with @xmath235 , @xmath236 and constants taken from @xcite .",
    "this time we also compute @xmath237 since @xmath238 encompasses a strong iteration term .",
    "we use a sample of size @xmath239 for computing @xmath232 , @xmath240 , @xmath224 and @xmath225 , but now we also need a sample of size @xmath241 for @xmath237 with rbd - fast .",
    "estimates obtained with 100 replications are reported in figure [ figishi1 ] .",
    "while first - order sis indicate that @xmath242 has a negligible impact , it actually influences the output through an interaction term which is naturally accounted for by the total index .",
    "is is interesting to note that all other indices detect the impact of @xmath242 , as was pointed out by @xcite for the total - variation index .",
    "however , one can observe the striking adequacy between @xmath237 and @xmath224 ( unlike @xmath243 ) .",
    "this clearly shows that distance correlation has the potential to detect any interaction effect since it is specifically designed for nonlinear dependence .",
    "an additional appealing property is that its estimation does not depend on the number of inputs , unlike @xmath237 .",
    "this is a major advantage for expensive computer codes .",
    "finally , @xmath225 tends to bring the same information as @xmath243 . but recall that it has the same limitation as @xmath237 concerning computational cost due to the pick - and - freeze technique .",
    "the same comments apply to @xmath226 and @xmath227 .    , @xmath224 , @xmath225 , @xmath226 and @xmath227 for function @xmath238 , @xmath239 , 100 replicates.,scaledwidth=90.0% ]    we also investigate hsic on level sets of the ishigami function to compare our results with @xcite .",
    "more precisely , we use a categorical kernel and use the change of variable @xmath244 .",
    "figure [ figishi2 ] shows that we can recover the fact that input factor @xmath242 is more important than @xmath245 and @xmath246 for this level set function , as was observed by @xcite .     and",
    "@xmath227 for function @xmath247 , @xmath239 , 100 replicates.,scaledwidth=90.0% ]    [ [ screening . ] ] screening .",
    "+ + + + + + + + + +    we now propose to study the performance of feature selection as an alternative to screening for problems where the number of input variables is large ( more than @xmath248 ) .",
    "we will deliberately limit the number of samples in order to be as close as possible to a real test case on an expensive code .",
    "* @xcite function @xmath249 where @xmath250 , @xmath251 , @xmath221 , @xmath252 and @xmath253 is an integer controlling the number of influential inputs .",
    "we select @xmath254 , @xmath255 and compute @xmath232 , @xmath243 , @xmath224 and @xmath226 since all other indices are too expensive to compute in this setting ( recall that @xmath256 ) .",
    "first remark that first - order sis identify the influential inputs in mean , but there are many replicates for which they are confounded with non - influential ones . on the contrary",
    ", @xmath243 completely fails ate detecting them : it will then be excluded from the other tests on screening .",
    "notably , dcor and hsic perfectly discriminate the first five factors and identify the remaining ones as independent from the output .",
    "we also use the hsic lasso ( [ hsiclasso ] ) , where for each replicate we use a bootstrap procedure to evaluate the probability of selection of each input factor . here , hsic lasso performs very well since it selects the first 5 inputs factors almost every time .    , @xmath224 , @xmath226 and hsic lasso for function @xmath257 , @xmath254 , 100 replicates.,scaledwidth=90.0% ]    * @xcite function @xmath258 with @xmath221 , @xmath259 and @xmath260 are taken from @xcite .",
    "only the first eight factors are influential .    here",
    "we also illustrate the feature selection method based on the iterative hsic scheme detailed in section [ secfs ] .",
    "results for @xmath254 are given in figure [ figsoblev1 ] .",
    "as expected , @xmath232 is unable to detect correctly the impact of the inputs . on the other hand , dcor and hsic",
    "accurately estimate higher dependence for the first input factors than for the remaining ones .",
    "similarly , hsic lasso never selects the last inputs as influential ones .",
    "the iterative feature selection based on hsic performs well but tends to select more inputs than necessary .    ,",
    "@xmath224 , @xmath226 and hsic lasso for function @xmath261 , @xmath254 , 100 replicates.,scaledwidth=90.0% ]    to go further and to compare our results with the ones obtained by @xcite , we repeat this experiment with @xmath262 ( approximately the sample size used by @xcite ) .",
    "this time first - order sis slightly detect the influential inputs , but the more interesting fact is that dcor , hsic and hsic lasso give even better results and almost perfectly identifies them .",
    "finally , the iterative hsic scheme now almost always discards the non - influential inputs .    , @xmath224 , @xmath226 and hsic lasso for function @xmath261 , @xmath262 , 100 replicates.,scaledwidth=90.0% ]      [ [ acquisition - strategy - for - reservoir - characterization .",
    "] ] acquisition strategy for reservoir characterization .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in the petroleum industry , reservoir characterization aims at reducing the uncertainty on some unknown physical parameters of an oil reservoir by using all the data collected on the field , e.g. well logs , seismic images or dynamic data at the wells ( pressures , ... ) .",
    "basically , engineers solve a bayesian inverse problem where an initial prior distribution assumed on the parameters is updated by incorporating all field observations to produce a posterior distribution . in the end , this posterior distribution is used to predict the expected oil recovery of the reservoir in the future .",
    "however , it may be expensive to collect data and usually one wants to gather relevant observations only .",
    "this principle is at the core of so - called data acquisition strategies .",
    "for example , given the prior distribution , a natural idea is to get data which , when incorporated in the bayesian procedure , will reduce the most the uncertainty of the obtained posterior distribution .",
    "it is easy to see that this idea actually corresponds to performing a sensitivity analysis of the parameters when data varies , where the difference between the prior and the posterior distribution is given by the measure of uncertainty reduction one chooses , i.e. the dissimilarity function @xmath8 in equation ( [ si ] ) .",
    "since the number of both uncertain parameters and observations can be large , we can greatly capitalize on the advantages of dcor and hsic measures in arbitrary dimension to perform this task .",
    "our example here makes use of the punq reservoir test case , which is an oil reservoir model derived from real field data @xcite . in this simplified model ,",
    "seven variables which are characteristic of media , rocks , fluids or aquifer activity , are considered as uncertain ( permeability multipliers , residual oil saturations , ... ) and are assigned a uniform prior distribution . for illustration purposes",
    ", we assume that collectable data only consist of gas - oil ratios measurements at a given well .",
    "we generate a sample of size @xmath262 of the prior distribution , and propagate them through a fluid - flow simulator to get a sample of the simulated gas - oil ratios at the well over 10 years .",
    "they are given in figure [ figpunq ] , top .",
    "for each day in these 10 years , we compute the dcor measure between the parameters and the simulated ratio , see figure [ figpunq ] , bottom .",
    "this information makes it possible to pick up the days where measurements should be collected in order to reduce as much as possible the uncertainty on the parameters : at the beginning of the reservoir production ( before 500 days ) or around 3 years after .",
    "obviously , this procedure generalizes to any number of measurements and can be performed sequentially on many observations thanks to the properties of dcor .     of the collectable data ( top ) and dcor measure between the parameters and the data at each time step ( bottom ) for the punq test case.,scaledwidth=90.0% ]    [ [ screening - for - contamination - migration - in - waste - storage - site . ] ] screening for contamination migration in waste storage site .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the marthe test case investigated here concerns prediction of the transport of strontium 90 in a porous water - saturated medium for evaluating the contamination of an aquifer in a temporary storage of radioactive waste @xcite .",
    "twenty input parameters mainly representative of the geological uncertainty are considered as random , and a set of 300 simulations is available at http://www.gdr-mascotnum.fr/benchmarks.html .",
    "accessible outputs are strontium 90 concentrations simulated at ten different wells , as well as the concentrations on a complete 2d map of the area ( discretized on @xmath263 pixels ) .",
    "we place ourselves in a screening setting where we use only @xmath254 simulations to identify the influential inputs . to estimate the variability of our results , we pick at random these 50 samples among the 300 available and repeat the procedure 100 times .",
    "we use the hsic dependence measure with a gaussian kernel first in its standard form by considering the vector of concentrations at the 10 observation wells .",
    "but we also take advantage of a kernel designed for the 2d maps as was mentioned in section [ secfunc ] .",
    "namely we use the pca semi - metric @xcite and vary the number of principal components ( 1 , 5 and 20 explaining @xmath264 , @xmath265 and @xmath266 of the total variance , respectively ) .",
    "results are given in figure [ figmarthe ] .",
    "first note that they are coherent with the ones obtained by @xcite where the authors used the 300 simulations to build a surrogate model . here",
    ", we then get the same detection of influential inputs but with only 50 simulations ( parameters i3 , kd1 , kd2 ) .",
    "in addition , the pca kernel leads to more discriminating indices as soon as the explained variance is sufficient ( 5 pcs ) .",
    "this clearly illustrates the potential of hsic for functional data .",
    "in this paper , we introduced a new class of sensitivity indices based on dependence measures which overcomes the insufficiencies of variance - based methods in gsa .",
    "we demonstrated that when the output distribution is compared with its conditional counterpart through csiszr f - divergences , sensitivity indices arise as well - known dependence measures between random variables .",
    "we then extended these indices by using recent state - of - the - art dependence measures , such as distance correlation and the hilbert - schmidt independence criterion .",
    "we also emphasized the potential of feature selection techniques relying on such dependence measures as alternatives to screening in high dimension .",
    "interestingly , these new sensitivity indices are very robust to dimensionality , have low computational cost and can be elegantly extended to functional and categorical output or input variables .",
    "this opens the door to new and powerful tools for gsa and factors screening for high dimensional and expensive computer codes .",
    "balasubramanian , k. , sriperumbudur , b.  lebanon , g. 2013 , ultrahigh dimensional feature screening via rkhs embeddings , _ in _ ` proceedings of the sixteenth international conference on artificial intelligence and statistics ' , pp .  126134 .",
    "cukier , r.  i. , fortuin , c.  m. , shuler , k.  e. , petschek , a.  g. schaibly , j.  h. 1973 , ` study of the sensitivity of coupled reaction systems to uncertainties in rate coefficients .",
    "i theory ' , _ the journal of chemical physics _ * 59 * ,  38733878 .",
    "gretton , a. , bousquet , o. , smola , a.  schlkopf , b. 2005a , measuring statistical dependence with hilbert - schmidt norms , _ in _",
    "s.  jain , h.  simon e.  tomita , eds , ` algorithmic learning theory ' , vol .",
    "3734 of _ lecture notes in computer science _ , springer berlin heidelberg , pp .",
    "6377 .",
    "ishigami , t.  homma , t. 1990 , an importance quantification technique in uncertainty analysis for computer models , _ in _ ` first international symposium on uncertainty modeling and analysis ' , ieee , pp .",
    "398403 .",
    "manceau , e. , mezghani , m. , zabalza - mezghani , i.  roggero , f. 2001 , combination of experimental design and joint modeling methods for quantifying the risk associated with deterministic and stochastic uncertainties - an integrated test study , _ in _ ` 2001 spe annual technical conference and exhibition , new orleans , 30 september-3 october ' .",
    "paper spe 71620 .",
    "morris , m.  d. , moore , l.  m.  mckay , m.  d. 2006 , ` sampling plans based on balanced incomplete block designs for evaluating the importance of computer model inputs ' , _ journal of statistical planning and inference _ * 136*(9 ) ,  32033220 .",
    "peng , h. , long , f.  ding , c. 2005 , ` feature selection based on mutual information criteria of max - dependency , max - relevance , and min - redundancy ' , _ pattern analysis and machine intelligence , ieee transactions on _ * 27*(8 ) ,  12261238 .",
    "saltelli , a. , annoni , p. , azzini , i. , campolongo , f. , ratto , m. tarantola , s. 2010 , ` variance based sensitivity analysis of model output .",
    "design and estimator for the total sensitivity index ' , _ computer physics communications _ * 181*(2 ) ,  259270 .",
    "sriperumbudur , b.  k. , fukumizu , k. , gretton , a. , lanckriet , g.  r. schlkopf , b. 2009 , kernel choice and classifiability for rkhs embeddings of probability distributions , _ in _ ` advances in neural information processing systems ' , pp .  17501758 .",
    "suzuki , t. , sugiyama , m. , sese , j.  kanamori , t. 2008 , ` approximating mutual information by maximum likelihood density ratio estimation ' , _ journal of machine learning research - proceedings track _ * 4 * ,  520 .",
    "volkova , e. , iooss , b.  van  dorpe , f. 2008 , ` global sensitivity analysis for a numerical model of radionuclide migration from the rrc `` kurchatov institute '' radwaste disposal site ' , _ stochastic environmental research and risk assessment _ * 22*(1 ) ,  1731 ."
  ],
  "abstract_text": [
    "<S> global sensitivity analysis with variance - based measures suffers from several theoretical and practical limitations , since they focus only on the variance of the output and handle multivariate variables in a limited way . in this paper , we introduce a new class of sensitivity indices based on dependence measures which overcomes these insufficiencies . </S>",
    "<S> our approach originates from the idea to compare the output distribution with its conditional counterpart when one of the input variables is fixed . </S>",
    "<S> we establish that this comparison yields previously proposed indices when it is performed with csiszr f - divergences , as well as sensitivity indices which are well - known dependence measures between random variables . </S>",
    "<S> this leads us to investigate completely new sensitivity indices based on recent state - of - the - art dependence measures , such as distance correlation and the hilbert - schmidt independence criterion . </S>",
    "<S> we also emphasize the potential of feature selection techniques relying on such dependence measures as alternatives to screening in high dimension . </S>"
  ]
}