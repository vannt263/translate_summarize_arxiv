{
  "article_text": [
    "error exponent analysis has been an active area of research for quite a few decades .",
    "the vast literature in this area can be categorized based on ( i ) whether the channel is memoryless or with memory ; ( ii ) whether there is or is not channel output feedback to the transmitter ; ( iii ) whether the employed coding is fixed - length or variable - length ; and ( iv ) whether upper ( converse ) or lower ( achievable ) bounds are analyzed .    in the case of memoryless channels with noiseless feedabck schalkwijk and kailath",
    "@xcite proposed a transmission scheme for the additive white gaussian noise ( awgn ) channel with infinite error exponent . on the other hand ,",
    "dobrushin  @xcite and later harutunian  @xcite , by deriving an error upper bound for discrete memoryless channels ( dmcs ) showed that at least for symmetric channels there is no gain to be expected through feedback when fixed - length codes are employed .",
    "this was a strong negative result since it suggested that for dmc channels , noiseless feedback can neither improve capacity ( as was well known ) nor can it improve the error exponent when fixed - length codes are used .",
    "a remarkable result was derived by burnashev in  @xcite , where matching upper and lower bounds were derived for dmcs with feedback and variable - length codes .",
    "the error exponent has a simple form @xmath0 , where @xmath1 is the average rate , @xmath2 is the channel capacity and @xmath3 is the maximum divergence that can be obtained in the channel for a binary hypothesis testing problem .",
    "several variable - length transmission schemes have been proposed in the literature for dmcs and their error exponents have been analyzed  @xcite .    in the case of channels with memory and feedback ,",
    "the capacity was studied in  @xcite , and a number of capacity - achievable schemes have been recently studied in the literature  @xcite . the only work that studies error exponents for variable - length codes for channels with memory and feedback is  @xcite where the authors consider a finite state channels with channel state known causally to both the transmitter and the receiver .    in this work",
    ", we consider channels with memory and feedback , and derive a straight - line upper bound on the error - exponent for variable - length codes .",
    "we specifically look at unifilar channels since for this family , the capacity has been characterized in an elegant way through the use of markov decision processes ( mdps )  @xcite .",
    "our technique is motivated by that of  @xcite , i.e. , studying the rate of decay of the posterior message entropy @xmath4 using martingale theory in two distinct regimes : large and small message entropies . a major difference between this work as compared to  @xcite is that we analyze the multi - step drift behavior of the communication system instead of the one - step drift that is analyzed for dmcs .",
    "this is necessitated by the fact that one - step analysis can not capture the memory inherent in the channel and thus results in extremely loose bounds .",
    "it is not surprising that the parameter @xmath3 in our case also relates to the maximum discrimination that can be achieved in this channel in a binary hypothesis testing problem . in order to evaluate this quantity ,",
    "we formulate two mdps with decreasing degree of complexity , the solutions of which are upper bounds on the the quantity @xmath3 , with the former being tighter than the latter .",
    "the tightness of the bounds is argued based on the fact that asymptotically this is the expected performance of the best system , and by achievability results presented in the companion paper  @xcite .",
    "an additional contribution of this work is a complete reworking of some of the more opaque proofs of  @xcite resulting in significant simplification of the exposition .",
    "we finally provide some numerical results for a number of interesting unifilar channels including the trapdoor , chemical , and other two - input / output / state unifilar channels . the main difference between our work and that in  @xcite is that for unfilar channels , the channel state is not observed at the receiver .",
    "this complicates the analysis considerably as is evidenced by the different approaches in evaluating the constant @xmath3 in these two works .",
    "furthermore , our results indicate that optimal policies for achieving the maximum divergence are very different when the receiver knows or does not know the channel state .",
    "the remaining part of this paper is organized as follows . in section  [ sec :",
    "model ] , we describe the channel model for the unifilar channel and the class of encoding and decoding strategies . in section  [ sec : bound ] , we analyze the drifts of the posterior message entropy in the large- and small - entropy regime . in section  [ sec : c_1 ] , we formulate two mdps in order to study the problem of one - bit transmission over this channel .",
    "section  [ sec : example ] presents numerical results for several unifilar channels .",
    "final conclusions are given in section  [ sec : conclusions ]",
    "consider a family of finite - state point - to - point channels with inputs @xmath5 , output @xmath6 and state @xmath7 at time @xmath8 , with all alphabets being finite and initial state @xmath9 known to both the transmitter and the receiver .",
    "the channel conditional probability is @xmath10 for a given stochastic kernel @xmath11 and deterministic function @xmath12 , where @xmath13 denotes the space of all probability measure on @xmath14 , and @xmath15 is the kronecker delta function .",
    "this family of channels is referred to as unifilar channels  @xcite .",
    "the authors in  @xcite have derived the capacity @xmath2 under certain conditions in the form of @xmath16 in this paper , we restrict our attention to such channels with strictly positive @xmath17 for any @xmath18 and ergodic behavior so that the above limit indeed exists .",
    "let @xmath19 be the message to be transmitted .",
    "in this system , the transmitter receives perfect feedback of the output with unit delay and decides the input @xmath20 based on @xmath21 at time @xmath8 .",
    "the transmitter can adopt randomized encoding strategies , where @xmath22 with a collection of distributions @xmath23 .",
    "the decoding policy consists of a stopping time @xmath24 w.r.t .",
    "filtration @xmath25 and estimated message @xmath26 at every time @xmath8 .",
    "the average rate @xmath1 and error probability @xmath27 of this scheme are defined as @xmath28}$ ] and @xmath29 .",
    "the channel reliability function ( highest achievable error exponent ) is defined as @xmath30}$ ] . since transmission schemes with @xmath31 result in the trivial error exponent @xmath32}=0 $ ]",
    ", we restrict attention to those schemes that have a.s . finite decision times .",
    "our methodology is inspired by the analysis in  @xcite for dmcs .",
    "the analysis involves lower - bounding the rate of decrease of the posterior message entropy which , through a generalization of fano s lemma , provides lower bounds on the error probability .",
    "entropy can decrease no faster than the channel capacity .",
    "however this bound becomes trivial at low values of entropy which necessitates switching to lower bounding the corresponding log drift .",
    "the log drift analysis is quite involved in  @xcite even for the dmc .",
    "the fundamental difference in our work compared to dmc , is the presence of memory in unifilar channels .",
    "a single - step drift analysis would nt be able to capture this memory resulting in loose bounds .",
    "for this reason we analyze multi - step drifts ; in fact we consider the asymptotic behavior as the step size becomes larger and larger .",
    "the outline of the analysis is as follows .",
    "lemma  [ lemma : driftentropy ] and lemma  [ lemma : driftlogentropy ] describe the overall decreasing rate of the entropy induced by the posterior belief on the message in terms of drifts in the linear and logarithmic regime , respectively .",
    "the former relates the drift to capacity , @xmath2 , while the latter relates it to a quantity @xmath3 which can be interpreted as the largest discrimination that can be achieved in this channel for a binary hypothesis testing problem , as elegantly explained in  @xcite .",
    "the result presented in lemma  [ lemma : newsubmartingale ] shows that based on a general random process that satisfies the two above mentioned drift conditions one can create an appropriate submartingale .",
    "these three results are then combined together in proposition  [ th : main ] to provide a lower bound on the stopping time of an arbitrary system employing variable - length coding , and equivalently an upper bound on the error exponent .",
    "let us define the following random processes @xmath33 from the generalized fano s lemma ( * ? ? ?",
    "* lemma 1 ) , the expectation of the posterior entropy at stopping time @xmath24 is upper bounded by @xmath34 .",
    "thus in order to estimate the rate of decrease of @xmath27 , we study the corresponding rate for @xmath35 .",
    "the next lemma gives a first estimate of the drift of @xmath36 .",
    "[ lemma : driftentropy ] for any @xmath37 and @xmath38 , there exists an @xmath39 such that @xmath40 \\geqslant -n(c+\\epsilon ) \\qquad a.s.\\ ] ]    please see appendix  [ app : lemma1 ] .    since for small values of @xmath35",
    "the above result does not give any information , we now analyze the drifts of the process @xmath41 .",
    "[ lemma : driftlogentropy ] for any given @xmath38 , there exists an @xmath39 such that if @xmath42 @xmath43 \\geqslant    -n(c_1+\\epsilon ) \\qquad a.s.\\ ] ] where the constant @xmath3 is given by @xmath44    please see appendix  [ app : lemma2 ]",
    ".    we comment at this point that the proof of this result is significantly simpler than the corresponding one in  ( * ? ? ? * lemma  3 ) .",
    "the reason is that we develop the proof directly in the asymptotic regime and thus there is no need for complex convexity arguments as the ones derived in  ( * ? ? ?",
    "* lemma  7 , and eq .",
    "( a8)-(a12 ) ) .",
    "at this point one can bound the quantity in   by @xmath45 using convexity .",
    "such a bound , however , can be very loose since it does not account for channel memory . in section",
    "[ sec : c_1 ] , we will discuss how to evaluate @xmath3 . before we continue with the second stage of the analysis",
    ", we also note that @xmath46 is bounded above by a positive number @xmath47 almost surely due to the fact that kernel @xmath48 is strictly positive .",
    "the proof is similar to that in  ( * ? ? ?",
    "* lemma  4 ) .    in the following lemma",
    ", we propose a submartingale that connects drift analysis and the stopping time in the proof our main result .",
    "[ lemma : newsubmartingale ] suppose a random process @xmath36 has the following properties    @xmath49 & \\geqslant -k_1 \\label{ineq : entropy}\\\\ e[\\log h_{t+1}-\\log h_{t}|\\mathcal{f}_t ] & \\geqslant -k_2 \\label{ineq : logentropysmall }   \\qquad \\text{if } h_t < h^ * \\\\     almost surely for some positive numbers @xmath50 where @xmath51 .",
    "define a process @xmath52 by @xmath53 where @xmath54 is defined by @xmath55 with a positive constant @xmath56 . then , for sufficiently small @xmath56 , @xmath52 is a submartingale w.r.t . @xmath57 .",
    "please see appendix  [ app : lemma3 ] .",
    "two comments are in place regarding the proof of this result .",
    "first , the main difficulty in proving such results is to take care of what happens in the `` transition '' range ( around @xmath58 ) where @xmath35 and @xmath59 are not both above or below the threshold .",
    "the choice of the function @xmath60 is what makes the proof work .",
    "the proof offered here is quite concise compared to the one employed in  @xcite ( which consists of lemma  5 and an approximation argument given in theorem  1 ) .",
    "the reason for that is the specific definition of the @xmath61 process and in particular the choice of the @xmath60 function which simplifies considerably the proof .",
    "the second , and related , comment is that this lemma is not a straightforward extension of  ( * ? ? ?",
    "* lemma in p.  50 ) since there , the purpose was to bound from below a positive rate of increase of a process . in our case , the proof hinges on the additional constraint   we impose on the choice of the @xmath60 function .",
    "we are now ready to present our main result .",
    "[ th : main ] any transmission scheme with @xmath62 messages and error probability @xmath27 satisfies @xmath63 } \\leqslant c_1(1-\\frac{\\overline{r}}{c})+ u(\\epsilon , k,\\overline{r},c , c_1,c_2),\\ ] ] for any @xmath38 .",
    "furthermore , @xmath64",
    ".    please see appendix  [ app : proposition ] .",
    "in this section we evaluate the constant @xmath3 .",
    "as noted in @xcite@xcite , the quantity @xmath3 relates to a binary hypothesis testing problem .",
    "when the posterior entropy is small , the receiver has very high confidence in a certain message .",
    "in this situation , the transmitter is essentially trying to inform the receiver whether or not this candidate message is the true one . since the unifilar channel has memory , it is not surprising that the constant @xmath3 is connected to a markov decision process related to the aforementioned binary hypothesis testing problem , as was the case in  @xcite . recall that @xmath3 is defined as @xmath65",
    "we now look into the quantities @xmath66 and @xmath67 .",
    "let us define @xmath68 and @xmath69 which are the input and the state at time @xmath8 , respectively conditioning on @xmath70 .",
    "@xmath71 and @xmath72 where @xmath73 and @xmath74 are given by @xmath75 moreover , @xmath76 can be updated by @xmath77 which we can concisely express as @xmath78 . with the above derivation",
    ", the divergence in   can be expressed as @xmath79 \\nonumber \\\\ & = \\sum_{i = t+1}^{t+n}e [ e[\\log \\frac{q(y_i|s^k_i , x^k_i ) } { \\sum_{x , s } q(y_{i}|x , s ) x^{\\overline{k}}_i(x|s ) b_{i-1}(s)}|s^k_i , b_{i-1},x^k_i , x^{\\overline{k}}_i , y^t , s_1,w = k ] |y^t , s_1,w = k]\\nonumber \\\\ & = \\sum_{i = t+1}^{t+n}e [ r(s^k_i , b_{i-1},x^k_i , x^{\\overline{k}}_i)|y^t , s_1,w = k],\\end{aligned}\\ ] ] where the function @xmath80 is given by @xmath81 this inspires us to define a controlled markov process with state @xmath82 , action @xmath83 , instantaneous reward @xmath84 at time @xmath8 and transition kernel @xmath85 that this is indeed a controlled markov process can be readily established .",
    "note that at time @xmath86 the process starts with initial state @xmath87 .",
    "let @xmath88 be the ( average ) reward in @xmath89 steps of this process @xmath90,\\end{aligned}\\ ] ] and denote by @xmath91 the corresponding @xmath92 , i.e. , @xmath93 .",
    "then , the constant @xmath3 is given by @xmath94      the mdp defined above has uncountably infinite state and action spaces . in this section , we propose an alternative upper bound on @xmath3 and formulate an mdp with finite state and action spaces to evaluate it .",
    "this provides a looser but more computational efficient upper bound .",
    "consider again the divergence term @xmath95 where ( a ) is due to convexity .",
    "consider deterministic policies and look into the first distribution in the divergence , @xmath96 where @xmath97 and @xmath98 represent the input and the state at time @xmath99 conditioning on @xmath70 , respectively .",
    "then we have @xmath100|y^t , s_1,w = k ] \\nonumber \\\\   & = \\sum_{i=1}^{n } e[\\tilde{r}(s^k_{t+i},s^j_{t+i},x^k_{t+i},x^j_{t+i})|y^t , s_1,w = k],\\end{aligned}\\ ] ] where @xmath101 is defined by @xmath102 similar to the previous development , we define a controlled markov chain with state @xmath103 , action @xmath104 , instantaneous reward @xmath105 at time @xmath8 and transition kernel @xmath106 let @xmath107 denote the average @xmath89-stage reward for this mdp , i.e. , @xmath108,\\end{aligned}\\ ] ] combine the above with the definition of @xmath3 , we have @xmath109 which gives a simple upper bound on @xmath3 .",
    "in this section , we provide numerical results for the expressions @xmath110 and @xmath111 for some binary input / output / state unifilar channels .",
    "we consider the trapdoor channel , chemical channels , symmetric unifilar channels , and asymmetric unifilar channels .",
    "all of these channels have @xmath112 and kernel @xmath113 characterized as shown in table  [ t : q ] .",
    ".kernel definition for binary unifilar channels [ cols=\"^,^,^,^,^\",options=\"header \" , ]     [ t : r ]    it is not surprising that the trapdoor and chemical channels have infinite upper bounds .",
    "this is also true for the z channel in the dmc case and it is related to the fact that the transition kernel has a zero entry",
    ". intuitively , discrimination of the two hypotheses can be perfect by transmitting always @xmath114 under @xmath115 and @xmath116 under @xmath117 hypothesis : with high probability , that does not depend on the message size or the target error rate , the receiver under the @xmath115 hypothesis will receive the output @xmath118 which is impossible under @xmath115 hypothesis and thus will make a perfect decision .    for each mdp the rewards do not seem to depend on the initial state , or",
    "the results are within the accuracy of our calculations .",
    "similarly , the results comparing the first and second mdps , the results are within the accuracy of our calculations and so we can not make a conclusive statement regarding the difference between the two mdp solutions .",
    "there is a strong indication , however , that they both result in the same average reward asymptotically .",
    "for all channels considered above , the optimal policy for the second mdp is @xmath119,x_1[s_0,s_1])=(1\\oplus s_1,s_1)$ ] which is exactly what we would do in the z channel .",
    "in this paper , we derive an upper bound on the error - exponent of unifilar channels with noiseless feedback and variable length codes .",
    "we generalize burnashev s techniques by performing multi - step drift analysis and deriving a lower bound on the stopping time together with a proposed submartingale . the constant @xmath3 which is the zero rate exponent",
    "is evaluated through an mdp and furher upper bounded through a more computationally tractable mdp .",
    "numerical results show that for some unifilar channels the two mdps give different results .",
    "a future research direction is the analytical solution of these mdps .",
    "in addition , the presented analysis can be easily generalized to channels with finite state and inter - symbol interference ( isi ) with the state known only to the receiver .",
    "given any @xmath120 and @xmath121 , @xmath122",
    "\\nonumber\\\\   & = -i(w;y_{t+1}|y^{t}=y^{t},s_1=s_1)\\nonumber \\\\   & = -h(y_{t+1}|y^{t}=y^{t},s_1=s_1 ) +   h(y_{t+1}|y^{t}=y^{t},s_1=s_1,w ) \\nonumber \\\\   & \\overset{(a)}{\\geqslant } -h(y_{t+1}|y^{t}=y^{t},s_1=s_1 ) +   h(y_{t+1}|y^{t}=y^{t},s_1=s_1,w , s_{t+1},x_{t+1 } ) \\nonumber \\\\   & \\overset{(b)}{= } -h(y_{t+1}|y^{t}=y^{t},s_1=s_1 ) +   h(y_{t+1}|y^{t}=y^{t},s_1=s_1,s_{t+1},x_{t+1 } ) \\nonumber \\\\   & = -i(x_{t+1},s_{t+1};y_{t+1}|y^{t}=y^{t},s_1=s_1),\\end{aligned}\\ ] ] where ( a ) is due to conditioning reduces entropy , and ( c ) is due to the channel properties . note that the last term is the mutual information between @xmath123 and @xmath124 conditioning on @xmath125 , which is different from conditional mutual information @xmath126 .",
    "now the @xmath89-step drift becomes @xmath127 \\nonumber \\\\   & = -e[\\sum_{k = t}^{t+n-1}\\sum_{y^k_{t+1 } } p(y^k_{t+1}=y^k_{t+1}|y^t = y^t , s_1=s_1)e[h_{k+1 } - h_{k}|y^k = y^k , s_1=s_1]|y^t = y^t , s_1=s_1 ] \\nonumber \\\\   & \\overset{(a)}{\\geqslant } -e[\\sum_{k = t}^{t+n-1 } \\sum_{y^k_{t+1 } } p(y^k_{t+1}=y^k_{t+1}|y^t = y^t , s_1=s_1 ) i(x_{k+1},s_{k+1 } ; y_{k+1}|y^{k}=y^k , s_1=s_1)|y^t = y^t , s_1=s_1]\\nonumber \\\\   & = -e[\\sum_{k = t}^{t+n-1 } i(x_{k+1},s_{k+1 } ; y_{k+1}|y^{k}_{t+1},y^{t}=y^t , s_1=s_1)|y^t = y^t , s_1=s_1]\\nonumber \\\\   & \\overset{(b)}{\\geqslant } -n(c+\\epsilon),\\end{aligned}\\ ] ] where ( a ) is due to   and ( b ) is due to  .",
    "given any @xmath128 and @xmath129 , @xmath130 = \\nonumber \\\\ & e[\\log\\frac{-\\sum_{i}p(w = i|y^{t+n}_{t+1},y^t , s_1 )   \\log p(w = i|y^{t+n}_{t+1},y^t , s_1)}{-\\sum_{i}p(w = i|y^{t},s_1 )   \\log p(w = i|y^t , s_1)}|y^t = y^t , s_1=s1].\\end{aligned}\\ ] ] for convenience , we define the following quantities    @xmath131    since @xmath132 , there exits a @xmath133 such that @xmath134 while @xmath135 for @xmath136 .",
    "we further define @xmath137 for @xmath136 .",
    "the following approximations are valid for @xmath138 close to 1 .",
    "@xmath139    substituting these approximate expressions back to the drift expression we have @xmath140 \\nonumber \\\\   & = \\sum_{y^{t+n}_{t+1 } } p(y^{t+n}_{t+1}|y^t , s_1 ) \\log \\frac{\\sum_i f_i(y^{t+n}_{t+1})\\log f_i(y^{t+n}_{t+1})}{\\sum_i f_i\\log f_i }   \\nonumber \\\\   & = \\sum_{y^{t+n}_{t+1 } } \\hat{q}(y^{t+n}_{t+1}|k ) \\log \\frac{(1-f_k ) ( \\log(1-f_k ) + o(\\log(1-f_k)))\\sum_{j\\neq k}\\frac{\\hat{f}_j\\hat{q}(y^{t+n}_{t+1}|j)}{\\hat{q}(y^{t+n}_{t+1}|k)}}{(1-f_k)(\\log ( 1-f_k ) + o(\\log(1-f_k ) ) }    \\nonumber \\\\   & = -\\sum_{y^{t+n}_{t+1}}\\hat{q}(y^{t+n}_{t+1}|k ) \\log \\frac{\\hat{q}(y^{t+n}_{t+1}|k)}{\\sum_{j\\neq k}\\hat{f}_j\\hat{q}(y^{t+n}_{t+1}|j ) } + o(1)\\nonumber \\\\   & \\geqslant -n(c_1 + \\epsilon),\\end{aligned}\\ ] ] where the last inequality is due to the definition of @xmath3 .",
    "we can always choose a sufficiently small positive @xmath56 such that    @xmath141    we first consider the case @xmath142 .",
    "@xmath143 where ( a ) is due to .",
    "therefore we have @xmath144 & = e[z_{t+1}1_{\\{h_{t } > h^*\\}}-z_t1_{\\{h_{t } > h^*\\}}|\\mathcal{f}_t ] \\nonumber\\\\ & \\geqslant e[(\\frac{h_{t+1}-h^*}{k_1}+t+1)1_{\\{h_{t } > h^*\\}}-(\\frac{h_{t}-h^*}{k_1}+t)1_{\\{h_{t } > h^*\\}}|\\mathcal{f}_t ] \\nonumber \\\\ & \\geqslant 0 , \\label{ineq : subhtbig}\\end{aligned}\\ ] ] where the last equation is due to .",
    "similarly , for the case @xmath145 , from we have @xmath146 and therefore @xmath147 \\nonumber \\\\   & \\geqslant e[\\frac{\\log \\frac{h_{t+1}}{h^*}}{k_2}+t+1+f(\\log \\frac{h_{t+1}}{h^*})-\\frac{\\log \\frac{h_{t}}{h^*}}{k_2}-t - f(\\log \\frac{h_{t}}{h^*})|\\mathcal{f}_t]\\nonumber \\\\ & \\overset{(a)}{= } e[(\\frac{1}{k_2}+f'(\\log\\frac{h_{t+1}}{h^*}))(\\log\\frac{h_{t+1}}{h^*}-\\log\\frac{h_{t}}{h^*})+1+\\frac{f''(z(h_{t+1},h_t))}{2}(\\log\\frac{h_{t+1}}{h^*}-\\log\\frac{h_{t}}{h^*})^2|\\mathcal{f}_t]\\nonumber \\\\ & \\overset{(b)}{\\geqslant } e[-k_2f'(\\log\\frac{h_{t+1}}{h^*})+\\frac{f''(z(h_{t+1},h_t))}{2}(\\log\\frac{h_{t+1}}{h^*}-\\log\\frac{h_{t}}{h^*})^2|\\mathcal{f}_t ]   \\nonumber \\\\ & = e[e^{\\lambda\\log\\frac{h_{t+1}}{h^*}}+\\frac{-\\lambda e^{\\lambda ( z(h_{t+1},h_t)-\\log\\frac{h_{t+1}}{h^*}+\\log\\frac{h_{t+1}}{h^*})}}{2k_2}(\\log\\frac{h_{t+1}}{h^*}-\\log\\frac{h_{t}}{h^*})^2|\\mathcal{f}_t ] \\nonumber \\\\ & \\overset{(c)}{\\geqslant }   e[e^{\\lambda\\log\\frac{h_{t+1}}{h^*}}-\\frac{\\lambda e^{\\lambda ( k_3+\\log\\frac{h_{t+1}}{h^*})}}{2k_2}(\\log\\frac{h_{t+1}}{h^*}-\\log\\frac{h_{t}}{h^*})^2|\\mathcal{f}_t ] \\nonumber \\\\ & \\overset{(d)}{\\geqslant } ( 1-\\frac{\\lambda e^{\\lambda k_3}}{2k_2}k_3 ^ 2 ) e[e^{\\lambda\\log\\frac{h_{t+1}}{h^*}}|\\mathcal{f}_t ] \\nonumber \\\\ & \\overset{(e)}{\\geqslant}\\ 0,\\label{ineq : subhtsmall}\\end{aligned}\\ ] ] where ( a ) is from the second - order taylor s expansion of @xmath148 at @xmath149 , ( b ) is due to and , ( c ) is due to that @xmath150 is between @xmath149 and @xmath151 , ( d ) is due to , and ( e ) is due to . from and , we have @xmath152\\geqslant 0 $ ] and thus @xmath52 is a submartingale .",
    "the proof essentially applies lemma  [ lemma : newsubmartingale ] to the `` block '' submartingale .",
    "given any @xmath38 , there exists an @xmath39 such that by lemma  [ lemma : driftentropy ] and lemma  [ lemma : driftlogentropy ] ,       define @xmath154 , where @xmath61 is defined in  , and filtration @xmath155 .",
    "@xmath156 is a submartingale w.r.t .",
    "@xmath157 by lemma  [ lemma : newsubmartingale ] .",
    "notice that the quantity @xmath158 here indicates the order of the block of @xmath89 consecutive transmissions .",
    "furthermore , define the stopping time @xmath159 w.r.t .",
    "@xmath157 by @xmath160 . by definition of @xmath159 , we have @xmath161 now we essentially apply the optional sampling theorem on the submartingale @xmath156 as follows @xmath162\\nonumber \\\\   & = e[(\\frac{\\log h_{n\\hat{t}}-\\log \\epsilon}{n ( c_1 + \\epsilon ) } + f(\\frac{\\log h_{n\\hat{t}}}{\\log \\epsilon}))1_{h_{n\\hat{t}}\\leqslant\\epsilon } ] + e [ ( \\frac{h_{n\\hat{t}}-\\epsilon}{n(c+\\epsilon ) } ) 1_{h_{n\\hat{t}}>\\epsilon } ] + e[\\hat{t } ] \\nonumber \\\\   & \\leqslant e[\\frac{\\log h_{n\\hat{t}}+|\\log \\epsilon|}{n ( c_1 + \\epsilon ) } + f(\\frac{\\log h_{n\\hat{t}}}{\\log \\epsilon } ) ] + e [ \\frac{h_{n\\hat{t}}+\\epsilon}{n(c+\\epsilon ) } ] + e[\\hat{t } ] \\nonumber \\\\   & \\overset{(a)}{= } e[\\frac{\\log h_{t}+|\\log \\epsilon|}{n ( c_1 + \\epsilon ) } + f(\\frac{\\log h_{t}}{\\log \\epsilon } ) ] + e [ \\frac{h_{t}+\\epsilon}{n(c+\\epsilon ) } ] + e[\\hat{t } ]   \\nonumber \\\\   & \\overset{(b)}{\\leqslant } \\frac{\\log e[h_{t}]+|\\log \\epsilon|}{n ( c_1 + \\epsilon ) } + e[f(\\frac{\\log h_{t}}{\\log \\epsilon } ) ] +   \\frac{e[h_{t}]+\\epsilon}{n(c+\\epsilon ) }   + e[\\hat{t } ] \\nonumber \\\\   & \\overset{(c)}{\\leqslant }   \\frac{\\log e[h_{t}]+|\\log \\epsilon|}{n ( c_1 + \\epsilon ) }   + e[f(\\frac{\\log h_{n\\hat{t}}}{\\log \\epsilon } ) ] + \\frac{e[h_{t}]+\\epsilon}{n(c+\\epsilon)}+\\frac{e[t]}{n}+1   \\nonumber \\\\   & \\overset{(d)}{\\leqslant }   \\frac{\\log e[h_{t}]+|\\log \\epsilon|}{n ( c_1 + \\epsilon ) } + \\frac{e[h_{t}]+\\epsilon}{n(c+\\epsilon)}+ \\frac{e[t]}{n}+1 + \\frac{1}{\\lambda n c_1 } \\nonumber \\\\   & \\overset{(e)}{\\leqslant }   \\frac{\\log pe + \\log(k-\\log pe)+|\\log \\epsilon | } { n ( c_1 + \\epsilon ) } + \\frac{-pe\\log pe-(1-pe)\\log(1-pe ) + pek+\\epsilon}{n(c+\\epsilon ) } + \\frac{e[t]}{n}+1 + \\frac{1}{\\lambda nc_1},\\end{aligned}\\ ] ] where ( a ) is due to that the receiver no longer performs actions after time @xmath24 , ( b ) is due the the concavity of @xmath163 , ( c ) is due to , ( d ) is due to that @xmath148 is upper - bounded by @xmath164 , and ( e ) is due to the fano s lemma .",
    "multiplying @xmath89 on the both sides the above inequality , we get @xmath165 } & \\leqslant c_1(1-\\frac{\\overline{r}}{c } ) + \\frac { \\log(k-\\log pe)+ |\\log \\epsilon|}{k/\\overline{r } }   + \\epsilon ( 1+\\frac{|c_1-c|}{c(c+\\epsilon)}\\overline{r})\\nonumber \\\\ & \\qquad + \\frac{c_1+\\epsilon}{k/\\overline{r } } ( n(\\epsilon)+\\frac{1}{\\lambda c_1}+\\frac{-pe\\log pe -(1-pe)\\log(1-pe)+pek+2\\epsilon}{c+\\epsilon } ) , \\label{ineq : result}\\end{aligned}\\ ] ] which proves the result ."
  ],
  "abstract_text": [
    "<S> the reliability function of memoryless channels with noiseless feedback and variable - length coding has been found to be a linear function of the average rate in the classic work of burnashev . in this work </S>",
    "<S> we consider unifilar channels with noiseless feedback and study upper bounds for the channel reliability function with variable length codes . in unifilar channels the channel state is known to the transmitter but is unknown to the receiver . </S>",
    "<S> we generalize burnashev s analysis and derive a similar expression which is linear in average rate and depends on the channel capacity , as well as an additional parameter which relates to a sequential binary hypothesis testing problem over this channel . </S>",
    "<S> this parameter is evaluated by setting up an appropriate markov decision process ( mdp ) . </S>",
    "<S> furthermore , an upper bound for this parameter is derived using a simplified mdp . </S>",
    "<S> numerical evaluation of the parameter for several binary input / state / output unifilar channels hints at the optimal transmission strategies . </S>",
    "<S> such strategies are studied in a companion paper to provide lower ( achievable ) bounds on the channel reliability function . </S>"
  ]
}