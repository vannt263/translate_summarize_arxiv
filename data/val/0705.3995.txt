{
  "article_text": [
    "_ random coding _ is an extremely powerful technique to show the existence of a code satisfying certain properties .",
    "it has been used for proving the direct part ( achievability ) of many types of coding theorems .",
    "recently , the idea of random coding has also come to be regarded as important from a practical point of view .",
    "an ldpc ( low - density parity - check ) code can be constructed by choosing a parity check matrix from an ensemble of sparse matrices . thus , there is a growing interest in randomly generated codes .",
    "one of the main difficulties associated with the use of randomly generated codes is the difficulty in evaluating the properties or performance of such codes .",
    "for example , it is difficult to evaluate minimum distance , weight distribution , ml decoding performance , etc . for these codes .",
    "to overcome this problem , we can take a _",
    "probabilistic approach_. in such an approach , we consider an ensemble of parity check matrices : i.e. , probability is assigned to each matrix in the ensemble .",
    "a property of a matrix ( e.g. , minimum distance , weight distributions ) can then be regarded as a random variable .",
    "it is natural to consider statistics of the random variable such as mean , variance , higher moments and covariance . in some cases",
    ", we can show that a property is strongly concentrated around its expectation .",
    "such a concentration result justifies the use of the probabilistic approach .",
    "recent advances in the analysis of the average weight distributions of ldpc codes , such as those described by litsyn and shevelev @xcite@xcite , burshtein and miller @xcite , richardson and urbanke @xcite , show that the probabilistic approach is a useful technique for investigating typical properties of codes and matrices , which are not easy to obtain .",
    "furthermore , the second moment analysis of the weight distribution of ldpc codes @xcite@xcite can be utilized to prove concentration results for weight distributions .",
    "the evaluation of the error detection probability of a given code ( or given parity check matrix ) is a classical problem in coding theory @xcite , @xcite and some results on this topic have been derived from the view point of a probabilistic approach .",
    "for example , for a linear code ensemble the inequality , @xmath1 has long been known where @xmath2 is the undetected error probability and @xmath3 is the number of rows of a parity check matrix .",
    "since the undetected error probability can be expressed as a linear combination of the weight distribution of a code , there is a natural connection between the expectation of the weight distribution and the expectation of the undetected error probability .    in this paper ,",
    "an analysis of the undetected error probability of ensembles of binary matrices of size @xmath0 is presented .",
    "an error detection scheme is a crucial part of a feedback error correction scheme such as arq(automatic repeat request ) .",
    "detailed knowledge of the error detection performance of a matrix ensemble would be useful for assessing the performance of a feedback error correction scheme .",
    "for a given @xmath4 binary parity check matrix @xmath5 , let @xmath6 be the binary linear code of length @xmath7 defined by @xmath5 , namely , @xmath8 where @xmath9 is the galois field with two elements @xmath10 ( the addition over @xmath9 is denoted by @xmath11 ) .",
    "the notation @xmath12 denotes the zero vector of length @xmath3 . in this paper , a boldface letter , such as @xmath13 for example ,",
    "denotes a binary row vector .    throughout the paper , a binary symmetric channel ( bsc ) with crossover probability @xmath14 ( @xmath15 ) is assumed .",
    "we assume the conventional scenario for error detection : a transmitter sends a codeword @xmath16 to a receiver via a bsc with crossover probability @xmath14 .",
    "the receiver obtains a received word @xmath17 , where @xmath18 denotes an error vector .",
    "the receiver firstly computes the syndrome @xmath19 and then checks whether @xmath20 holds or not .",
    "an undetected error event occurs when @xmath21 and @xmath22 .",
    "this means that the error vector @xmath23 causes an undetected error event .",
    "thus , the undetected error probability @xmath24 can be expressed as @xmath25 where @xmath26 denotes the hamming weight of vector @xmath13 .",
    "the above equation can be rewritten as @xmath27 where @xmath28 is defined by @xmath29.\\ ] ] the set @xmath30 is usually called the _ weight distribution _ of @xmath6 .",
    "the notation @xmath31 denotes the set of @xmath7-tuples with weight @xmath32 .",
    "the notation @xmath33 $ ] is the indicator function such that @xmath33 = 1 $ ] if @xmath34 is true ; otherwise , it evaluates to 0 .",
    "suppose that @xmath35 is a set of binary @xmath36 matrices @xmath37 .",
    "note that @xmath35 may contain some matrices with all elements identical .",
    "such matrices should be distinguished as distinct matrices .",
    "a probability @xmath38 is associated with each matrix @xmath5 in @xmath35 .",
    "thus , @xmath35 can be considered as an _ ensemble _ of binary matrices .",
    "let @xmath39 be a real - valued function which depends on @xmath40 .",
    "the expectation of @xmath39 with respect to the ensemble @xmath35 is defined by @xmath41 { \\stackrel{\\triangle}{=}}\\sum_{h \\in { { \\cal g } } } p(h ) f(h).\\ ] ] the average weight distribution of a given ensemble @xmath35 is given by @xmath42 .",
    "$ ] this quantity is very useful for analyzing the performance of binary linear codes , including analysis of the undetected error probability .      in this paper",
    ", we will focus on a parameterized ensemble @xmath43 which is called the _ bernoulli ensemble _ because the bernoulli ensemble is amenable to ensemble analysis .",
    "the bernoulli ensemble @xmath43 contains all the binary @xmath0 matrices ( @xmath44 ) , whose elements are regarded as i.i.d . binary random variables such that an element takes the value 1 with probability @xmath45 .",
    "the parameter @xmath46 is a positive real number which represents the average number of ones for each row . in other words ,",
    "a matrix @xmath47 can be considered as an output from the bernoulli source such that symbol 1 occurs with probability @xmath48 .    from the above definition ,",
    "it is clear that a matrix @xmath47 is associated with the probability @xmath49 where @xmath50 is the number of ones in @xmath5 ( i.e. , hamming weight of @xmath5 ) .",
    "the average weight distribution of the bernoulli ensemble is given by @xmath51   =   \\left(\\frac{1+z^w}{2 } \\right)^m { n \\choose w}\\ ] ] for @xmath52 $ ] where @xmath53 .",
    "the notation @xmath54 $ ] denotes the set of consecutive integers from @xmath55 to @xmath56 .",
    "the average weight distribution of this ensemble was first discussed by litsyn and shevelev @xcite .",
    "if @xmath57 is a constant ( i.e. , not a function of @xmath7 ) , this ensemble can be considered as an ensemble of sparse matrices . in the spacial case",
    "where @xmath58 , equal probability @xmath59 is assigned to every matrix in the bernoulli ensemble . as a simplified notation , we will denote @xmath60 where @xmath61 is called the _ random ensemble_. since a typical instance of @xmath61 contains @xmath62 ones , the ensemble can be regarded as an ensemble of dense matrices .",
    "for a given @xmath0 matrix @xmath5 , the evaluation of the undetected error probability @xmath24 is in general computationally difficult because we need to know the weight distribution of @xmath6 for such evaluation . on the other hand , in some cases",
    ", we can evaluate the average of @xmath24 for a given ensemble .",
    "such an average probability is useful for the estimation of the undetected error probability of a matrix which belongs to the ensemble .",
    "taking the ensemble average of the undetected error probability over a given ensemble @xmath35 , we have @xmath63 & = & e_{{\\cal g}}\\left[\\sum_{w = 1}^n",
    "a_w(h ) \\epsilon^w ( 1-\\epsilon)^{n - w } \\right ] \\\\",
    "\\label{avew } & = & \\sum_{w = 1}^n e_{{\\cal g}}[a_w(h ) ] \\epsilon^w ( 1-\\epsilon)^{n - w } .\\end{aligned}\\ ] ] in the above equations , @xmath5 can be regarded as a random variable . from this equation , it is evident that the average of @xmath24 can be evaluated if we know the average weight distribution of the ensemble .",
    "for example , in the case of the random ensemble @xmath61 , the average undetected error probability has a simple closed form .    [ averandom ] the average undetected error probability of the random ensemble @xmath61 is given by @xmath64 = 2^{-m } ( 1-(1-\\epsilon)^n).\\ ] ] ( proof ) by using ( [ avew ] ) , we have @xmath65 & = & \\sum_{w = 1}^n e_{{{\\cal r}}_{m , n}}[a_w(h ) ] \\epsilon^w ( 1-\\epsilon)^{n - w }   \\\\ \\nonumber & = & \\sum_{w = 1}^n 2^{-m } { n",
    "\\choose w } \\epsilon^w ( 1-\\epsilon)^{n - w }   \\\\ & = & 2^{-m } ( 1-(1-\\epsilon)^n).\\end{aligned}\\ ] ] the second equality is based on the well known result @xcite : @xmath66 = 2^{-m } { n \\choose w}.\\ ] ] the last equality is due to the binomial theorem .      for a given sequence of @xmath67 matrix ensembles @xmath68 ,",
    "the average undetected error probability is usually an exponentially decreasing function of @xmath7 , where @xmath69 is a real number satisfying @xmath70 ( called the _ design rate _ ) .",
    "thus , the exponent of the undetected error probability is of prime importance in understanding the asymptotic behavior of the undetected error probability .",
    "let @xmath71 be a series of ensembles such that @xmath72 consists of @xmath67 binary matrices .",
    "in order to see the asymptotic behavior of the undetected error probability of this sequence of ensembles , it is reasonable to define the error exponent of undetected error probability in the following way :    the asymptotic error exponent of the average undetected error probability for a series of ensembles @xmath71 is defined by @xmath73\\ ] ] if the limit exists .",
    "henceforth we will not explicitly express the dependence of @xmath2 on @xmath5 , writing instead @xmath2 to denote @xmath24 in all cases where there is no fear of confusion .",
    "the following example describes the exponent of the random ensemble .",
    "consider the series of the random ensembles @xmath74 .",
    "it is easy to evaluate @xmath75 : @xmath76 \\\\ \\nonumber & = &   \\lim_{n \\rightarrow \\infty } \\frac 1 n \\log_2 2^{-(1-r)n } ( 1-(1-\\epsilon)^n ) \\\\",
    "\\label{1mr } & = &   -(1-r).\\end{aligned}\\ ] ] this equality implies that the average undetected error probability of the sequence of random ensembles behaves like @xmath77 \\simeq 2^{-n(1-r)}\\ ] ] if @xmath7 is sufficiently large .",
    "note that the exponent @xmath78 is independent from the crossover probability @xmath14 .",
    "the _ asymptotic growth rate _ of the average weight distribution ( for simplicity henceforth abbreviated as the asymptotic growth rate ) , which is the basis of the derivation of the error exponent , is defined as follows .",
    "suppose that a series of ensembles @xmath71 is given .",
    "if @xmath79\\ ] ] exists for @xmath80 , then we define the _ asymptotic growth rate _ @xmath81 by @xmath82.\\ ] ] the parameter @xmath83 is called the _ normalized weight_.    from this definition , it is clear that @xmath84 = 2^{n(f(\\ell ) + o(1))},\\ ] ] where the notation @xmath85 denotes terms which converge to 0 in the limit as @xmath7 goes to infinity .",
    "the asymptotic growth rate of some ensembles of binary matrices can be found in @xcite@xcite@xcite .",
    "the next theorem gives the error exponent of the undetected error probability for a series of ensembles @xmath86 .",
    "[ th1 ] the error exponent of @xmath71 is given by @xmath87,\\ ] ] where @xmath81 is the asymptotic growth rate of @xmath86 .",
    "+ ( proof ) based on the definition of asymptotic growth rate , we can rewrite @xmath88 in the form @xmath89   \\\\ \\nonumber & = & \\hspace{-3 mm } \\lim_{n \\rightarrow \\infty } \\frac 1 n\\log_2\\sum_{w=1}^n e_{{{\\cal g}}_n } [ a_{w } ] \\epsilon^{w } ( 1 - \\epsilon)^{n - w } \\\\ \\nonumber & = & \\hspace{-3 mm } \\lim_{n \\rightarrow \\infty } \\frac 1 n\\log_2\\sum_{w=1}^n   2^{n(f(\\frac{w}{n } ) + k(\\epsilon , n , w)+ o(1 ) ) } , \\end{aligned}\\ ] ] where @xmath90 is defined by @xmath91 using a conventional technique for bounding summation , we have the following upper bound on @xmath92 : @xmath93 \\\\",
    "\\label{supeq } & = & \\hspace{-3 mm } \\sup_{0",
    "< \\ell \\le 1 } \\left[f(\\ell ) + \\ell   \\log_2\\epsilon + ( 1 - \\ell)\\log_2(1 - \\epsilon ) \\right].\\end{aligned}\\ ] ] we can also show that @xmath92 is greater than or equal to the right - hand side of the above inequality ( [ supeq ] ) in a similar manner .",
    "this means that the right - hand side of the inequality is asymptotically tight .",
    "the next example discusses the case of the random ensemble .",
    "let us again consider the series of the random ensembles given by @xmath94 .",
    "these ensembles have the asymptotic growth rate @xmath95 , where the function @xmath96 is the binary entropy function defined by @xmath97 in this case , by using theorem [ th1 ] , we have @xmath98.\\ ] ] let @xmath99 by using @xmath100 , we can rewrite ( [ trmn ] ) as @xmath101.\\ ] ] since @xmath102 can be considered as the kullback - libler divergence between two probability distributions @xmath103 and @xmath104 , @xmath102 is always non - negative and @xmath105 holds if and only if @xmath106 .",
    "thus , we obtain @xmath107 = - ( 1-r),\\ ] ] which is identical to the exponent obtained in expression ( [ 1mr ] ) .",
    "let @xmath108 .",
    "figure [ fig - random ] displays the behavior of @xmath109 when @xmath110 .",
    "this figure confirms the result that the maximum ( @xmath111 ) is attained at @xmath106 .     for random ensembles with @xmath112.,title=\"fig : \" ] +    the curves of @xmath113 correspond to the parameters @xmath114 from left to right",
    "are presented . as a reference , line of @xmath115",
    "is also included in the figure .",
    "the asymptotic growth rate of the bernoulli ensemble @xmath43 with a constant @xmath57 and design rate @xmath69 is given by @xmath116 this formula is presented in @xcite .",
    "the error exponent of this ensemble shows a different behavior from that for random ensembles .",
    "consider the bernoulli ensemble with parameters @xmath110 and @xmath117 .",
    "let @xmath118    figure [ fig - sparse1 ] includes the curves of @xmath119 where @xmath114 .",
    "in contrast to @xmath113 of a random ensemble , we can see that @xmath119 is not a concave function .",
    "the shape of the curve of @xmath119 depends on the crossover probability @xmath14 . for large @xmath14",
    ", @xmath120 takes its largest value around @xmath106 . on the other hand , for small @xmath14 , @xmath119 has the supremum at @xmath121 .",
    "figure [ fig - sparse2 ] presents the error exponent of bernoulli ensembles with parameters @xmath122 and @xmath117 . as an example , consider the exponent for @xmath112 . in the regime where @xmath14 is smaller than ( around ) 0.3 , the error exponent is a monotonically decreasing function of @xmath14 .     for bernoulli ensembles.,title=\"fig : \" ] + the curves of @xmath119 correspond to the parameters @xmath114 are presented .",
    "the parameters @xmath123 are assumed . as a reference , line of @xmath115",
    "is also included in the figure .",
    "+ the curves of @xmath124 correspond to the parameters @xmath125 and @xmath126 .",
    "are presented .",
    "the examples suggest that a sparse ensemble has less powerful error detection performance than that of a dense ensemble ( such as the random ensemble ) in terms of the error exponent .",
    "however , if the crossover probability is sufficiently large , the difference in exponent of sparse and dense ensembles is negligible .",
    "for example , the exponent of the bernoulli ensemble in fig .",
    "[ fig - sparse2 ] is almost equal to that of the random ensemble when @xmath14 is larger than ( around ) 0.3 .",
    "the above properties of the error exponents of the bernoulli ensembles can be explained with reference to their average weight distributions ( or asymptotic growth rate ) .",
    "figure [ fig - agr ] displays the asymptotic growth rates of a random ensemble and a bernoulli ensemble .",
    "+    the weight of typical error vectors is very close to @xmath127 when @xmath7 is sufficiently large . for a large value of @xmath14 , such as @xmath128 , the average weight distribution around @xmath129 , namely @xmath130 $ ] , dominates the undetected error probability .",
    "in such a range , the difference in the average weight distributions corresponding to the random and the bernoulli ensembles is small . on the other hand ,",
    "if the crossover probability is small , weight distributions of low weight become the most influential parameter . the difference in the average weight distributions of small weight results in a difference in the error exponent .",
    "note that the time complexity of the error detection operation ( multiplication of received vector and a parity check matrix ) is @xmath131-time for a typical instance of a random ensemble , and is @xmath132-time for a typical instance of a bernoulli ensemble with constant @xmath57 .",
    "a sparse matrix offers almost same error detection performance of a dense matrix with linear time complexity if @xmath14 is sufficiently large .",
    "in the previous section , we have seen that the average weight distribution plays an important role in the derivation of average undetected error probability .",
    "similarly , we need to examine the _ covariance of weight distribution _ in order to analyze the variance of undetected error probability .",
    "the covariance between two real - valued functions @xmath133 defined on an ensemble @xmath35 is given by @xmath134 { \\stackrel{\\triangle}{=}}e_{{{\\cal g } } } [ f g ] - e_{{{\\cal g } } } [ f ] e_{{{\\cal g } } } [ g].\\ ] ]    the next theorem forms the basis of the derivation of the variance of the undetected error probability for the bernoulli ensemble .",
    "the covariance of the weight distribution for the bernoulli ensemble is given in the following theorem .",
    "[ covsparse ] the covariance of the weight distribution for the bernoulli ensemble @xmath43 is given by @xmath135 for @xmath136 and @xmath137 for @xmath138 where @xmath139 and @xmath140 .",
    "+ ( proof ) see appendix .",
    "when @xmath58 , @xmath43 becomes the random ensemble @xmath61 .",
    "we discuss this case here .",
    "we first assume that @xmath141 .",
    "let @xmath142 ( i.e. , @xmath58 ) .",
    "in such a case , we have @xmath143 .",
    "define @xmath144 by @xmath145 the variable @xmath144 takes the following values : @xmath146 substituting @xmath147 into equation ( [ convformula ] ) and using the identity ( [ commutative ] ) , we get @xmath148 another proof of this formula is presented in @xcite .      the variance of the undetected error probability is a straightforward consequence of theorem [ covsparse ] .",
    "[ spvarthreom ] the variance of the undetected error probability of the bernoulli ensemble , @xmath149 is given by @xmath150 ( proof ) the variance of the undetected error probability @xmath2 is given by @xmath151 \\\\ & = & e_{{{\\cal b}}_{m , n , k}}[p_u^2 ] - e_{{{\\cal b}}_{m , n , k}}[p_u]^2 . \\end{aligned}\\ ] ] we first consider the second moment of the undetected error probability : @xmath152 \\\\ \\nonumber & = & \\hspace{-3mm}e_{{{\\cal b}}_{m , n , k}}\\left [ \\left(\\sum_{w = 1}^n a_w \\epsilon^w ( 1-\\epsilon)^{n - w } \\right)^2 \\right ]   \\\\ \\nonumber & = & \\hspace{-3mm}e_{{{\\cal b}}_{m , n ,",
    "k}}\\left [ \\sum_{w_1 = 1}^n \\sum_{w_2 = 1}^n a_{w_1 } a_{w_2 } \\epsilon^{w_1+w_2 }   ( 1-\\epsilon)^{2n - w_1-w_2 } \\right ] \\\\ & = &   \\hspace{-4mm}\\sum_{w_1 = 1}^n \\sum_{w_2 = 1}^n \\hspace{-1 mm } e_{{{\\cal b}}_{m , n , k}}\\left[a_{w_1 } a_{w_2}\\right ] \\epsilon^{w_1+w_2 }   ( 1-\\epsilon)^{2n - w_1-w_2}\\hspace{-1mm}.\\end{aligned}\\ ] ] the squared average undetected error probability can be expressed as @xmath153 ^ 2&= & \\hspace{-3mm}e_{{{\\cal b}}_{m , n , k}}\\left [ \\left(\\sum_{w = 1}^n a_w \\epsilon^w ( 1-\\epsilon)^{n - w } \\right ) \\right]^2   \\\\ \\nonumber & = &   \\hspace{-4mm}\\sum_{w_1 = 1}^n \\sum_{w_2 = 1}^n \\hspace{-1 mm }   e_{{{\\cal b}}_{m , n , k}}\\left[a_{w_1}\\right ] e_{{{\\cal b}}_{m , n , k}}\\left[a_{w_2 }   \\right ] \\\\ & \\times & \\epsilon^{w_1+w_2 } ( 1-\\epsilon)^{2n - w_1-w_2}\\hspace{-1mm}.\\end{aligned}\\ ] ] combining these equalities and the covariance of the weight distribution , the variance of undetected error probability @xmath154 is obtained .",
    "the covariance of the weight distribution for a given ensemble @xmath43 is useful not only for the evaluation of the variance of @xmath2 .",
    "let @xmath155 be a random variable represented by @xmath156 where @xmath157 is a real - valued function of @xmath32 .",
    "the covariance of the weight distribution is required more generally for the evaluation of the variance of @xmath155 , which is given by @xmath158 a specialized version ( the case where @xmath159 ) of this equation has been derived in the previous corollary .",
    "let us consider the bernoulli ensemble with @xmath160 and @xmath161 .",
    "table [ r12 ] displays the weight distributions and undetected error probabilities for the 4 matrices in @xmath162 .",
    ".weight distributions and undetected error probabilities [ cols=\"^,^,^,^,^\",options=\"header \" , ]     [ r12 ]    from the definition of a bernoulli ensemble , the following probability is assigned to each matrix : @xmath163 combining the undetected error probabilities presented in table [ r12 ] and the above probability assignment , we immediately have the first and second moments : @xmath164 & = & \\frac 2 3 \\epsilon - \\frac 7 8 \\epsilon^2 \\\\ e_{{{\\cal b}}_{1,2,1/2}}[p_u^2 ] & = & \\frac { 21}{8 } \\epsilon^2 - \\frac 3 8 \\epsilon^3 + \\epsilon^4.\\end{aligned}\\ ] ] from these moments , the variance can be derived : @xmath165 -e_{{{\\cal b}}_{1,2,1/2}}[p_u ] ^2 \\\\ \\label{var12 } & = & \\frac 3 8 \\epsilon^2 - \\frac 3 8 \\epsilon^3 + \\frac{15}{64 } \\epsilon^4.\\end{aligned}\\ ] ]    we can also consider another route to derive the variance by using corollary [ spvarthreom ] .",
    "the covariances of @xmath162 are given by @xmath166 from corollary [ spvarthreom ] , we obtain the variance @xmath167 that is identical to expression ( [ var12 ] ) .    in the case of @xmath58 ( i.e. the case of a random ensemble ) , we can derive a closed form expression for the variance .",
    "[ rvartheorem ] for the random ensemble @xmath61 , the variance of the undetected error probability @xmath2 is given by @xmath168 ( proof ) the variance of undetected error probability @xmath169 can be obtained in the following way : @xmath170-e_{{{\\cal r}}_{m , n } } [ p_u]^2 \\\\ \\nonumber & = & \\hspace{-3mm}\\sum_{w_1 = 1}^n \\sum_{w_2 = 1}^n { \\rm cov}_{{{\\cal r}}_{m , n}}\\left[a_{w_1 } , a_{w_2}\\right ] \\epsilon^{w_1+w_2 }   ( 1-\\epsilon)^{2n - w_1-w_2 }    \\\\ \\nonumber & = &   \\hspace{-3mm}\\sum_{w=1}^n(1 - 2^{-m})2^{-m}{n \\choose w } \\epsilon^{2w } ( 1-\\epsilon)^{2n - 2w } .\\end{aligned}\\ ] ] the second equality is due to corollary [ spvarthreom ] .",
    "the last equality are due to eq .",
    "( [ covrandom ] ) .",
    "we can further simplify the expression using the binomial theorem : @xmath171 the last equality is the claim of the theorem .",
    "the next example facilitates an understanding of how the average and the variance of @xmath2 behave .",
    "we consider the random ensemble with @xmath172 , and the bernoulli ensemble with @xmath173 ( labeled `` sparse '' in fig . [ fig - curve1 ] ) .",
    "figure [ fig - curve1 ] depicts the average undetected error probabilities of the two ensembles .",
    "it can be observed that the average undetected error probability of the random ensemble monotonically decreases as @xmath14 decreases .",
    "in contrast , the curve for the bernoulli ensemble has a peak around @xmath174 .",
    "+ random ensemble : @xmath175 , sparse matrix ensemble : @xmath176 .",
    "figure [ fig - curve2 ] shows the variance of @xmath2 for the above two ensembles .",
    "the two curves have a similar shape , but the variance of the sparse ensemble is always larger than that of the random ensemble .",
    "+ random ensemble : @xmath175 , sparse matrix ensemble : @xmath176 .",
    "we here discuss the asymptotic behavior of the covariance of the weight distribution and the variance of @xmath2 for the bernoulli ensemble .",
    "the following corollary explains the asymptotic behavior of the covariance of the weight distribution .",
    "[ asymptcov ] let the asymptotic growth rate of the covariance of the weigh distribution of the bernoulli ensemble be @xmath177 defined by @xmath178 for @xmath179 and @xmath180 .",
    "the asymptotic growth rate is given by @xmath181 for @xmath182 and @xmath183 for @xmath184 where @xmath185 is defined by @xmath186 the function @xmath187 is defined by @xmath188 ( proof ) we here rewrite the covariance formula ( [ convformula ] ) into asymptotic form . by using the binomial theorem , we have @xmath189 by using this identity , the covariance in ( [ convformula ] ) can be rewritten in the following form : @xmath190 where @xmath191 is defined by @xmath192    letting @xmath193 , we have @xmath194 and @xmath195 if @xmath57 is a constant and @xmath80 , then , making use of the identity @xcite @xmath196 we get @xmath197 combining these asymptotic expressions , the claim of the corollary is derived .",
    "the following corollary gives the asymptotic growth rate of the variance of the undetected error probability .",
    "the asymptotic growth rate of the variance of the undetected error is given by @xmath198 where @xmath199 is given by @xmath200 ( proof ) it is evident that @xmath201 holds .",
    "combining this identity and corollaries [ spvarthreom ] and [ asymptcov ] , we immediately have the claim of the corollary .",
    "the second moment of the weight distribution for a given ensemble @xmath35 is given by @xmath202 \\\\",
    "\\nonumber    & = &    e_{{{\\cal g}}}\\left [     \\sum _ { { \\mbox{\\boldmath$x$ } } \\in z^{(n , w_1 ) } } \\sum _ { { \\mbox{\\boldmath$y$ } } \\in z^{(n , w_2 ) } }      i[h { \\mbox{\\boldmath$x$ } } ^t = 0^m ] i[h { \\mbox{\\boldmath$y$ } } ^t = 0^m ] \\right].\\end{aligned}\\ ] ] for @xmath203 . since @xmath204 i[h { \\mbox{\\boldmath$y$ } } ^t = 0^m ] = i[h { \\mbox{\\boldmath$x$ } } ^t = 0^m , h { \\mbox{\\boldmath$y$ } } ^t = 0^m],\\ ] ] we have @xmath202   \\\\",
    "\\nonumber    & = & \\hspace{-3 mm }    e_{{{\\cal g}}}\\left [     \\sum _ { { \\mbox{\\boldmath$x$ } } \\in z^{(n , w_1 ) } } \\sum _ { { \\mbox{\\boldmath$y$ } } \\in z^{(n , w_2 ) } }      i[h { \\mbox{\\boldmath$x$ } } ^t = 0^m , h { \\mbox{\\boldmath$y$ } } ^t = 0^m ] \\right ] \\\\",
    "\\label{secmom }    & = & \\hspace{-7 mm }     \\sum _ { { \\mbox{\\boldmath$x$ } } \\in z^{(n , w_1 ) } } \\sum _ { { \\mbox{\\boldmath$y$ } } \\in z^{(n , w_2 ) } }     e_{{{\\cal g}}}\\left [      i[h { \\mbox{\\boldmath$x$ } } ^t = 0^m , h { \\mbox{\\boldmath$y$ } } ^t = 0^m ] \\right].\\end{aligned}\\ ] ]    we here encounter a problem of evaluating probability of occurrence of both @xmath205 and @xmath206 . in preparation to solve this problem , we will introduce some notation :    for a given pair @xmath207 , the index sets @xmath208 are defined as follows : @xmath209 : x_k = 1 , y_k = 0 \\ } \\\\ i_2 & { \\stackrel{\\triangle}{= } } & \\{k \\in [ 1,n ] : x_k = 1 , y_k = 1 \\ } \\\\ i_3 & { \\stackrel{\\triangle}{= } } & \\{k \\in [ 1,n ] : x_k = 0 , y_k = 1 \\ } \\\\ i_4 & { \\stackrel{\\triangle}{= } } & \\{k \\in [ 1,n ] : x_k = 0 , y_k = 0 \\},\\end{aligned}\\ ] ] where @xmath210 and @xmath211 these regions are illustrated in fig.[fig - regions ] .",
    "the size of each index set is denoted by @xmath212 .",
    "let @xmath213 be a binary @xmath7-tuple .",
    "the partial weight of @xmath214 corresponding to an index set @xmath215 is denoted by @xmath216 , namely @xmath217    .,title=\"fig : \" ] +    since the index sets are mutually exclusive , the equation @xmath218 holds and @xmath219 can take an integer value in the following range : @xmath220 the size of each index set can be expressed as @xmath221 , @xmath222 , @xmath223 .",
    "let @xmath224 and @xmath225 be binary vectors satisfying @xmath226 . in this proof , we first prove the following equality : @xmath227 ]     \\\\ \\label{xyprob } & = & \\left(\\frac{1+z^{w_1 } + z^{w_2 } + z^{w_1 + w_2   - 2 v } } { 4 } \\right)^m\\end{aligned}\\ ] ] where @xmath228 , @xmath139 and @xmath140 .",
    "the support set @xmath229 is defined by @xmath230 : v_i \\ne 0 \\},\\ ] ] where @xmath231 .",
    "we need to consider the following three cases : case ( i ) : @xmath232 ( i.e. , the intersection of @xmath233 and @xmath234 is not empty but @xmath234 does not include @xmath233 ) , case ( ii ) : @xmath235 ( i.e. , the intersection of @xmath233 and @xmath234 is empty ) , case ( iii ) : @xmath236 ( i.e. , @xmath234 includes @xmath233 ) .    we first study case ( i ) .",
    "suppose that a binary @xmath7-tuple @xmath214 is generated from a bernoulli source with @xmath237 = p ( i \\in [ 1,n])$ ] .",
    "recall that @xmath48 is defined by @xmath140 . in this case",
    ", @xmath238 holds if and only if @xmath239 for @xmath240 or @xmath241 for @xmath240 .",
    "it is well known that a binary vector @xmath242 generated from a bernoulli source has even weight with probability @xmath243 , where @xmath244 is the probability that @xmath245)$ ] takes 1 @xcite .",
    "the probability that @xmath242 has an odd weight is given by @xmath246 .",
    "for example , the probability that @xmath247 becomes even is @xmath248 where @xmath249 .",
    "based on the above argument , we can write the probability @xmath250 $ ] as a function of @xmath251 : @xmath252 \\\\",
    "\\nonumber \\hspace{-3 mm } & = & \\hspace{-3 mm } \\frac{(1+z^{i_1})(1+z^{i_2})(1+z^{i_3})+(1 -z^{i_1})(1-z^{i_2})(1-z^{i_3 } ) } { 8 } \\\\ & = & \\hspace{-3mm}\\frac{1+z^{w_1 } + z^{w_2 } + z^{w_1 + w_2   - 2 v } } { 4}.\\end{aligned}\\ ] ] where @xmath253 .",
    "we next consider case ( ii ) .",
    "for this case , @xmath254 is assumed to be zero . in this case",
    ", @xmath238 holds if and only if both @xmath255 and @xmath256 are even .",
    "the probability that @xmath214 satisfies @xmath257 and @xmath258 under the condition @xmath235 is given by @xmath259 \\\\ \\nonumber & = & \\left(\\frac{1+z^{i_1}}{2}\\right ) \\left(\\frac{1 + z^{i_3}}{2 } \\right ) \\\\",
    "\\nonumber & = & \\left(\\frac{1+z^{w_1}}{2}\\right ) \\left(\\frac{1 + z^{w_2}}{2 } \\right ) \\\\ & = & \\frac{1+z^{w_1 } + z^{w_2 } + z^{w_1 + w_2   - 2 v } } { 4}.\\end{aligned}\\ ] ]    finally we consider case ( iii ) .",
    "assume the case @xmath260 .",
    "in this case , @xmath238 holds if and only if both @xmath261 and @xmath256 are even .",
    "the probability @xmath250 $ ] under the condition @xmath262 is thus given by @xmath263   \\\\ \\nonumber & = & \\left(\\frac{1+z^{i_2}}{2}\\right ) \\left(\\frac{1 + z^{i_3}}{2 } \\right ) \\\\",
    "\\nonumber & = & \\frac{1+z^{w_1}+z^{w_2 }   + z^{w_2 - w_1 }    } { 4 } \\\\ & = & \\frac{1+z^{w_1}+z^{w_2 }   + z^{w_1 + w_2 - 2 v }    } { 4}. \\end{aligned}\\ ] ] we next consider the case @xmath264 .",
    "for this case , we also have @xmath263   \\\\ \\nonumber & = & \\frac{1+x^{w_1}}{2 } \\\\ & = & \\frac{1+z^{w_1 } + z^{w_2 } + z^{w_1 + w_2   - 2 v } } { 4}.\\end{aligned}\\ ] ]    in summary , for any cases ( cases ( i ) ,",
    "( ii ) , ( iii ) ) , @xmath265   = \\frac{1+z^{w_1 } + z^{w_2 } + z^{w_1 + w_2   - 2 v } } { 4}\\ ] ] holds . since the rows of parity check matrices in @xmath266 can be independently chosen , we obtain eq .",
    "( [ xyprob ] ) in the following way : @xmath267 ] \\\\",
    "\\nonumber & = & pr[h { \\mbox{\\boldmath$x$ } } ^t = 0 , h { \\mbox{\\boldmath$y$ } } ^t = 0 ]   \\\\ \\nonumber & = & pr [ { \\mbox{\\boldmath$h$ } } { \\mbox{\\boldmath$x$ } } ^t = 0 , { \\mbox{\\boldmath$h$ } } { \\mbox{\\boldmath$y$ } } ^t = 0]^m \\\\   & = & \\left(\\frac{1+z^{w_1 } + z^{w_2 } + z^{w_1 + w_2   - 2 v } } { 4 } \\right)^m.\\end{aligned}\\ ] ]    combining ( [ secmom ] ) and ( [ xyprob ] ) , we have @xmath268   \\\\",
    "\\nonumber   & = & \\hspace{-7 mm }     \\sum _ { { \\mbox{\\boldmath$x$ } } \\in z^{(n , w_1 ) } } \\sum _ { { \\mbox{\\boldmath$y$ } } \\in z^{(n , w_2 ) } }     e_{{{{\\cal b}}}_{n , m , k}}\\left [      i[h { \\mbox{\\boldmath$x$ } } ^t = 0^m , h { \\mbox{\\boldmath$y$ } } ^t = 0^m ] \\right ] \\\\",
    "\\nonumber   & = & \\hspace{-7 mm }     \\sum _ { { \\mbox{\\boldmath$x$ } } \\in z^{(n , w_1 ) } } \\sum _ { { \\mbox{\\boldmath$y$ } } \\in z^{(n , w_2 ) } } \\left(\\frac{1+z^{w_1 } + z^{w_2 } + z^{w_1 + w_2   - 2 v } } { 4 } \\right)^m \\\\ \\nonumber & = & \\hspace{-2mm}\\sum_{v= \\max\\{0,w_1+w_2 - n\\}}^{w_1 } { n \\choose w_1 } { w_1 \\choose v } { n - w_1 \\choose w_2 - v } \\\\",
    "\\label{2nd } & \\times & \\left(\\frac{1+z^{w_1 } + z^{w_2 } + z^{w_1 + w_2   - 2 v } } { 4 } \\right)^m.\\end{aligned}\\ ] ] since @xmath269   = { n \\choose w } \\left(\\frac{1+z^w}{2 } \\right)^m\\ ] ] holds @xcite , we thus have @xmath270 e_{{{{\\cal b}}}_{n , m , k}}\\left[a_{w_2 } \\right ] \\\\   \\nonumber & = & { n \\choose w_1}{n \\choose w_2 } \\left(\\frac{1+z^{w_1}}{2 } \\right)^m \\left(\\frac{1+z^{w_2}}{2 } \\right)^m \\\\ \\nonumber & = & \\sum_{v= \\max\\{0,w_1+w_2 - n\\}}^{w_1 } { n \\choose w_1 } { w_1 \\choose v } { n - w_1 \\choose w_2 - v } \\\\   \\label{exp2 } & \\times & \\left(\\frac{1+z^{w_1 } + z^{w_2 } + z^{w_1 + w_2 } } { 4 } \\right)^m .",
    "\\end{aligned}\\ ] ] the last equality is due to the following combinatorial identity : @xmath271 we are ready to derive the covariance of weight distributions for the case @xmath226 . substituting ( [ 2nd ] ) and ( [ exp2 ] ) into @xmath272 -   e_{{{{\\cal b}}}_{n , m , k}}\\left[a_{w_1 } \\right ] e_{{{{\\cal b}}}_{n , m , k}}\\left[a_{w_2 } \\right],\\end{aligned}\\ ] ] we have ( [ convformula ] ) in the claim part of the theorem .",
    "since the definition of covariance is commutative , @xmath273 holds if @xmath274 .",
    "this work was partly supported by the ministry of education , science , sports and culture , japan , grant - in - aid for scientific research on priority areas ( deepening and expansion of statistical informatics ) 180790091 ."
  ],
  "abstract_text": [
    "<S> in this paper , an analysis of the undetected error probability of ensembles of @xmath0 binary matrices is presented . </S>",
    "<S> the ensemble called the _ bernoulli ensemble _ whose members are considered as matrices generated from i.i.d . </S>",
    "<S> bernoulli source is mainly considered here . </S>",
    "<S> the main contributions of this work are ( i ) derivation of the error exponent of the average undetected error probability and ( ii ) closed form expressions for the variance of the undetected error probability . </S>",
    "<S> it is shown that the behavior of the exponent for a sparse ensemble is somewhat different from that for a dense ensemble . furthermore , as a byproduct of the proof of the variance formula , simple covariance formula of the weight distribution is derived . </S>"
  ]
}