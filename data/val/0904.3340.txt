{
  "article_text": [
    "one of the last major outstanding classical problems of information theory is the development of general - purpose , practical , efficiently implementable lossy compression algorithms .",
    "the corresponding problem for lossless data compression was essentially settled in the late 1970s by the advance of the lempel - ziv ( lz ) family of algorithms @xcite@xcite@xcite and arithmetic coding @xcite@xcite@xcite ; see also the texts @xcite@xcite .",
    "similarly , from the early- to mid-1990s on , efficient channel coding strategies emerged that perform close to capacity , primarily using sparse graph codes , turbo codes , and local message - passing decoding algorithms ; see , e.g. , @xcite@xcite@xcite@xcite , the texts @xcite@xcite@xcite , and the references therein .    for lossy data compression , although there is a rich and varied literature on both theoretical results and practical compression schemes , near - optimal , efficiently implementable algorithms are yet to be discovered . from rate - distortion theory",
    "@xcite@xcite we know that it is possible to achieve a sometimes dramatic improvement in compression performance by allowing for a certain amount of distortion in the reconstructed data .",
    "but the majority of existing algorithms are either compression - suboptimal or they involve exhaustive searches of exponential complexity at the encoder , making them unsuitable for realistic practical implementation .    until the late 1990s ,",
    "most of the research effort was devoted to addressing the issue of universality , see @xcite and the references therein , as well as @xcite@xcite@xcite@xcite@xcite@xcite@xcite@xcite ; algorithms emphasizing more practical aspects have been proposed in @xcite .",
    "in addition to many application - specific families of compression standards ( e.g. , jpeg for images and mpeg for video ) , there is a general theory of algorithm design based on vector quantization ; see @xcite@xcite@xcite@xcite and the references therein .",
    "yet another line of research , closer in spirit to the present work , is on lossy extensions of the celebrated lempel - ziv schemes , based on approximate pattern matching ; see @xcite@xcite@xcite@xcite@xcite@xcite@xcite@xcite@xcite@xcite .",
    "more recently , there has been renewed interest in the compression - complexity trade - off , and in the development of low - complexity compressors that give near - optimal performance , at least for simple sources with known statistics .",
    "the lossy lz algorithm of @xcite is rate - distortion optimal and of polynomial complexity , although , in part due the penalty paid for universality , its convergence is slow . for the uniform bernoulli source , @xcite@xcite@xcite present codes based on sparse graphs , and , although their performance is promising , like earlier approaches they rely on exponential searches at the encoder . in related work ,",
    "@xcite@xcite present sparse - graph compression schemes with much more attractive complexity characteristics , but suboptimal compression performance .",
    "rissanen and tabus @xcite describe a different method which , unlike most of the earlier approaches , is not based on a random ( or otherwise exponentially large ) codebook .",
    "it has linear complexity in the encoder and decoder and , although it appears to be rate - distortion suboptimal , it is an effective practical scheme for bernoulli sources .",
    "sparse - graph codes that are compression - optimal and of subexponential complexity are constructed in @xcite .",
    "a simulation - based iterative algorithm is presented in @xcite and it is shown to be compression - optimal , although its complexity is hard to evaluate precisely as it depends on the convergence of a markov chain monte carlo sampler .",
    "the more recent work @xcite on the lossy compression of discrete markov sources also contains promising results ; it is based on the combination of a viterbi - like optimization algorithm at the encoder followed by universal lossless compression .    the present work is partly motivated by the results reported in @xcite by gupta - verd - weissman ( gvw ) .",
    "their compression schemes are based on the `` divide - and - conquer '' approach , namely the idea that instead of encoding a long message @xmath0 using a classical random codebook of blocklength @xmath1 , it is preferable to break up @xmath2 into shorter sub - blocks of shorter length @xmath3 , say , and encode the sub - blocks separately .",
    "the main results in @xcite state that , with an appropriately chosen sub - block length @xmath3 , it is possible to achieve asymptotically optimal rate - distortion performance with near - linear implementation complexity ( in a sense made precise in section  [ s : hyb ] below ) .",
    "our starting point is the observation that there is a closely related , in a sense dual , point of view . on a conceptual as well as mathematical level ,",
    "the divide - and - conquer approach is very closely related to a pattern - matching scheme with a restricted database . in the divide - and - conquer setting ,",
    "given a target distortion level @xmath4 and an @xmath5 , each sub - block of length @xmath3 in the original message @xmath2 is encoded using a random codebook consisting of @xmath6 codewords , where @xmath7 is the rate - distortion function of the source being compressed ( see the following section for more details and rigorous definitions ) .",
    "to encode each sub - block , the encoder searches all @xmath8 entries of the codebook , in order to find the one which has the smallest distortion with respect to that sub - block .",
    "now suppose that , instead of a random codebook , the encoder and decoder share a random database with length @xmath9 , generated from the same distribution as the shannon - optimal codebook . as in @xcite , the encoder searches for the longest prefix @xmath10 of the message @xmath2 that matches somewhere in the database with distortion @xmath4 or less .",
    "then the prefix @xmath11 is described to the decoder by describing the position and length of the match in the database , and the same process is repeated inductively starting at @xmath12 . although the match - length @xmath13 is random , we know @xcite@xcite that , asymptotically , it behaves like , @xmath14 therefore , because the length @xmath15 of the database was chosen to be @xmath6 , in effect both schemes will individually encode sub - blocks of approximately the same length @xmath3 , and will also have comparable implementation complexity at the encoder .",
    "thus motivated , after reviewing the gvw scheme in section  [ s : gvw ] we introduce a ( non - universal ) version of the lossy lz scheme in @xcite , termed llz , and we compare its performance to that of gvw .",
    "theorem  1 shows that llz is asymptotically optimal in the rate - distortion sense for compressing data from a known discrete memoryless source with respect to a single - letter distortion criterion .",
    "simulation results are also presented , comparing the performance of llz and gvw on a simple bernoulli source .",
    "these results indicate that for blocklengths around 1000 bits , gvw offers better compression than llz at a given distortion level , but it requires significantly more memory for its execution .",
    "[ the same findings are also confirmed in the other simulation examples presented in section  [ s : sim ] . ]    in order to combine the different advantages of the two schemes , in section  [ s : hyb ] we introduce a hybrid algorithm ( hyb ) , which utilizes both the divide - and - conquer idea of gvw and the single - database structure of llz . in theorems  2 and",
    "3 we prove that hyb shares with gvw the exact same rate - distortion performance and implementation complexity , in that it operates in near - linear time at the encoder and linear time at the decoder .",
    "moreover , like llz , the hyb scheme requires much less memory , by an unbounded factor , depending on the choice of parameters in the design of the two algorithms .",
    "experimental results are presented in section  [ s : sim ] , comparing the performance of gvw and hyb .",
    "these confirm the theoretical findings , and indicate that hyb outperforms existing schemes for the compression of some simple discrete sources with respect to the hamming distortion criterion .",
    "the earlier theoretical results stating that hyb s rate - distortion performance is the same as gvw s are confirmed empirically , and it is also shown that , again for blocklengths of approximately 1000 symbols , the hyb scheme requires much less memory , by a factor ranging between 15 and 240 .",
    "after a brief discussion on potential extensions of the present results , some conclusions are collected in section  [ s : con ] . the appendix contains the proofs of the theorems in sections  [ s : gvw ] and  [ s : hyb ] .",
    "after describing the basic setting within which all later results will be developed , in section  [ s : gvw2 ] we recall the divide - and - conquer idea of the gvw scheme , and in section  [ s : gvw3 ] we present a new , non - universal lossy lz algorithm and examine its properties .",
    "let @xmath16 be a memoryless source on some finite alphabet @xmath17 and suppose that its distribution is described by a known probability mass function @xmath18 on @xmath17 .",
    "the objective is to compress @xmath19 with respect to a sequence of single - letter distortion criteria , @xmath20 where @xmath21 is an arbitrary source string to be compressed , @xmath22 is a potential reproduction string taking values in a finite reproduction alphabet @xmath23 , and @xmath24 is an arbitrary distortion measure .",
    "we make the customary assumption that for any source letter @xmath25 there is a reproduction letter @xmath26 with zero distortion , @xmath27    the best achievable rate at which data from the source @xmath19 can be compressed with distortion not exceeding @xmath28 is given by the _ rate - distortion function _ @xcite@xcite@xcite , r(d ) = _ w(y|x):_x , yap(x)w(y|x)(x , y)d i(x;y ) , [ eq : rddefn ] where @xmath29 denotes the mutual information between a random variable @xmath30 with the same distribution @xmath18 as the source and a random variable @xmath31 with conditional distribution @xmath32 given @xmath33 .",
    "let @xmath34 $ ] ; in order to avoid the trivial case where @xmath7 is identically equal to zero , @xmath35 is assumed to be strictly positive .",
    "it is well - known and easy to check that , for all distortion values in the nontrivial range @xmath36 , there is a conditional distribution @xmath37 that achieves the infimum in ( [ eq : rddefn ] ) , and this induces a distribution @xmath38 on @xmath23 via @xmath39 , for all @xmath40 . with a slight abuse of terminology ( as @xmath38 may not be unique )",
    "we refer to @xmath38 as _ the optimal reproduction distribution at distortion level @xmath4_. recall also the analogous definition of the _ distortion - rate function _",
    "@xmath41 of the source ; cf.@xcite@xcite .      the gvw algorithm . ]",
    "is a fixed - rate , variable - distortion code of blocklength @xmath1 and target distortion @xmath42 .",
    "it is described in terms of two parameters ; a `` small '' @xmath43 , and an integer @xmath3 so that @xmath44 .    given the target distortion level @xmath4 , let @xmath45 , and take , = r^-1 ( r(d)+/2 ) = d ( r(d)+/2 ) d. [ eq : dbargvw ] first a fixed - rate code of blocklength @xmath3 and rate @xmath46 is created according to shannon s classical random codebook construction .",
    "letting @xmath38 denote the optimal reproduction distribution at level @xmath47 , the codebook consists of @xmath48 i.i.d .",
    "codewords of length @xmath3 , each generated i.i.d . from @xmath38 .",
    "writing @xmath49 , as the concatenation of @xmath50 sub - blocks , each sub - block is matched to its @xmath51-nearest neighbor in the codebook , and it is described to the decoder using @xmath52 bits to describe the index of that nearest neighbor in the codebook .",
    "this code is used @xmath50 times , once on each of the @xmath50 sub - blocks , to produce corresponding reconstruction strings @xmath53 , for @xmath54 .",
    "the description of @xmath2 is the concatenation of the descriptions of the individual sub - blocks , and the reconstruction string itself is the concatenation of the corresponding reproduction blocks , @xmath55 .",
    "the overall description length of this code is @xmath56 bits , so the ( fixed ) rate of this code is @xmath57 bits / symbol , and its ( variable ) distortion is @xmath58 .",
    "the llz algorithm described here can be seen as a simplified ( in that it is non - universal ) and modified ( to facilitate the comparison below ) version of the algorithm in @xcite .",
    "it is a fixed - distortion , variable - rate code of blocklength @xmath1 , described in terms of three parameters ; an integer blocklength @xmath59 , and `` small '' @xmath60 .",
    "the algorithm will be presented in a setting `` dual '' to that of the gvw algorithm , in the sense that was described in the introduction .",
    "the main difference is that the source sting @xmath2 will be parsed into substrings of _ variable _ length , not of fixed length @xmath3 .    given @xmath1 and a target distortion level @xmath4 , define @xmath45 , take , @xmath61 and let @xmath38 denote the optimal reproduction distribution at level @xmath47 . then generate a single i.i.d .",
    "database @xmath62 of length ,",
    "m = m()=2^r+-1 , [ eq : m ] and make it available to both the encoder and decoder .",
    "the encoding algorithm is as follows : the encoder calculates the length of the longest match ( up to @xmath63-many symbols ) of an initial portion of the message @xmath2 , within distortion @xmath47 , in the database .",
    "let @xmath64 denote the length of this longest match , @xmath65 and let @xmath66 denote the initial phrase of length @xmath64 in @xmath67 then the encoder describes to the decoder :    * the length @xmath64 ; this takes @xmath68 bits ; * the position @xmath69 in the database where the match occurs ; this takes @xmath70 bits .    from @xmath71 and",
    "@xmath72 the decoder can recover the string @xmath73 which is within distortion @xmath47 of @xmath74 .",
    "alternatively , @xmath74 can be described with _",
    "zero _ distortion by first describing its length @xmath64 as before , and then describing @xmath74 itself directly using , l_,1|| [ eq : desc2 ] the encoder uses whichever one of the two descriptions is shorter .",
    "[ note that is not necessary to add a flag to indicate which one was chosen ; the decoder can simply check if @xmath75 is larger or smaller than @xmath70 . ] therefore , from  @xmath71 , @xmath72 , and  ( [ eq : desc2 ] ) the length of the description of @xmath74 is , ( ( 1+))+\\ { m , l_,1|| } .",
    "[ eq : desc4 ] after @xmath74 has been described within distortion @xmath47 , the same process is repeated to encode the rest of the message : the encoder finds the length @xmath76 of the longest string starting at position @xmath77 in @xmath2 that matches within distortion @xmath47 into the database , and describes @xmath78 to the decoder by repeating the above steps .",
    "the algorithm is terminated , in the natural way , when the entire string @xmath2 has been exhausted .",
    "at that point , @xmath2 has been parsed into @xmath79 distinct phrases @xmath80 , each of length @xmath81 , @xmath82 with the possible exception of the last phrase , which may be shorter .",
    "since each substring @xmath80 is described within distortion @xmath47 , also the concatenation of all the reproduction strings , call it @xmath83 , will be within distortion @xmath47 of @xmath2 .",
    "the distortion achieved by this code is @xmath84 , and it is guaranteed to be @xmath85 by construction . regarding the rate , if we write @xmath86 for the overall description length of @xmath2 , then from ( [ eq : desc4 ] ) , ( x_1^n)= _ k=1^ _ [ eq : ell - bound ] and",
    "the rate achieved by this code is @xmath87 bits / symbol",
    ".    * remark .",
    "* as mentioned in the introduction , there are two main differences between the gvw algorithm and the llz scheme .",
    "the first one is that while the gvw is based on a shannon - style random codebook , the llz uses an lz - type random database .",
    "the second is that gvw divides up the message @xmath2 into fixed - length sub - blocks of size @xmath3 , whereas llz parses @xmath2 into variable - length strings of ( random ) lengths @xmath81 .",
    "but there is also an important point of solidarity between the two algorithms .",
    "recall ( * ? ? ?",
    "* theorem  23 ) that , for large @xmath3 , the match length @xmath64 behaves logarithmically in the size of the database ; that is , with high probability , @xmath88 where the second approximation follows by the choice of @xmath89 and of @xmath90 therefore , both algorithms end up parsing the message @xmath2 into sub - blocks of length @xmath91 symbols .",
    "our first result shows that llz is asymptotically optimal in the usual sense established for fixed - database versions of lz - like schemes ; see @xcite@xcite .",
    "specifically , it is shown that by taking @xmath3 large enough and @xmath92 small enough , the llz comes arbitrarily close to any optimal rate - distortion point @xmath93 .",
    "note that @xmath94 is a parameter that simply controls the complexity of the best - match search , and its influence on the rate - distortion performance is asymptotically irrelevant .",
    "* theorem 1 .",
    "* suppose the llz with parameters @xmath95 and @xmath92 is used to compress a memoryless source @xmath19 with rate - distortion function @xmath7 at a target distortion rate @xmath42 .",
    "for any @xmath96 , the parameter @xmath43 can be chosen small enough such that : + ( a ) for any choice of @xmath3 and any blocklength @xmath97 the distortion achieved by llz is no greater than @xmath98 + ( b ) taking @xmath3 large enough , the asymptotic rate of llz achieves the rate - distortion bound , in that , _ _",
    "n e\\{|x_1^n } r ( ) = r(d)-/2 , [ eq : as - optimality ] where the expectation is over all databases . therefore , also , _ _",
    "n e\\ { } r ( ) = r(d)-/2 [ eq : optimality ] with the expectation here being over both the message and the databases .    next ,",
    "the performance of llz is compared with that of gvw on data generated from a bernoulli source with parameter @xmath99 and with respect to hamming distortion .",
    "simulation results at different target distortions are shown in figure  [ f : bern04a ] and table  [ tab:04a ] ; see section  [ s : sim ] for details on the choice of parameter values .",
    "it is clear from these results that , at the same distortion level , the gvw algorithm typically gives a better rate than llz . in terms of implementation complexity , the two algorithms have comparable execution times , but the llz uses significantly less memory . the same pattern  gvw giving better compression but using much more memory than llz ",
    "is also confirmed in the other examples we consider in section  [ s : sim ] .",
    "note that , like for the case of gvw , more can be said about the implementation complexity of llz and how it depends on the exact choice of parameters @xmath95 and @xmath92 .",
    "but since , as we will see next , the performance of both algorithms is dominated by that of a different algorithm ( hyb ) , we do not pursue this direction further .",
    "bits generated from a bernoulli source with parameter @xmath99 .",
    "the solid line is the rate - distortion function , the rate - distortion pairs achieved by llz are shown as red stars and those of gvw as blue diamonds .",
    ", width=345,height=326 ]     & + & @xmath100 & @xmath101 & rate & memory & time + gvw & 0.05 & 0.07143 & 0.70095 & 26 mb & 27m53s + gvw & 0.08 & 0.10286 & 0.59143 & 23 mb & 21m11s + gvw & 0.11 & 0.12667 & 0.50381 & 27 mb & 20m48s + gvw & 0.14 & 0.15714 & 0.41619 & 31 mb & 19m52s + gvw & 0.17 & 0.18857 & 0.32857 & 36 mb & 18m48s + gvw & 0.2 & 0.20571 & 0.26286 & 46 mb & 19m18s + gvw & 0.23 & 0.22857 & 0.21905 & 57 mb & 18m42s + gvw & 0.26 & 0.26381 & 0.15333 & 79 mb & 19m46s + gvw & 0.29 & 0.31429 & 0.10952 & 113 mb & 20m29s + llz & 0.05 & 0.03238 & 1.00029 & 1.5 mb & 4m23s + llz & 0.08 & 0.07524 & 0.79129 & 1.28 mb & 6m15s + llz & 0.11 & 0.10571 & 0.6754 & 1.46 mb & 8m53s + llz & 0.14 & 0.1381 & 0.55171 & 1.69 mb & 11m18s + llz & 0.17 & 0.16952 & 0.41827 & 2.6 mb & 18m15s + llz & 0.2 & 0.2019 & 0.36381 & 3.6 mb & 20m09s + llz & 0.23 & 0.23333 & 0.27975 & 6.2 mb & 41m32s + llz & 0.26 & 0.26571 & 0.23102 & 13 mb & 63m56s + llz & 0.29 & 0.29714 & 0.1741 & 47 mb & 165m54s +    @xmath102",
    "in order to combine the rate - distortion advantage of gvw with the memory advantage of llz , in this section we introduce a hybrid algorithm and examine its performance .",
    "the new algorithm , termed hyb , uses the divide - and - conquer approach of gvw , but based on a random database like the llz instead of a random codebook .",
    "it is a fixed - rate , variable - distortion code of blocklength @xmath1 and target distortion @xmath42 , and it is described in terms of two parameters ; a `` small '' @xmath43 , and an integer @xmath3 so that @xmath44 .",
    "like with the gvw , given a target distortion level @xmath4 , let @xmath45 and take @xmath47 as in ( [ eq : dbargvw ] ) .",
    "now , like for the llz algorithm , let @xmath103 as in ( [ eq : m ] ) , and generate a random database @xmath62 , where the @xmath104 are drawn i.i.d.from the optimal reproduction distribution at level @xmath47 . the database is made available to both the encoder and the decoder , and the message @xmath2 to be compressed is parsed into @xmath105 non - overlapping blocks , @xmath49 .",
    "the first sub - block @xmath106 is matched to its @xmath51-nearest neighbor in the database , where we consider each possible @xmath107 , @xmath108 as a potential reproduction word .",
    "then @xmath106 is described to the decoder by describing the position of its matching reproduction block in the database using @xmath109 bits , and the same process is repeated on each of the @xmath50 sub - blocks , to produce @xmath50 reconstruction strings .",
    "the description of @xmath2 is the concatenation of the descriptions of the individual sub - blocks , and the reconstruction string itself is the concatenation of the corresponding reproduction blocks .",
    "the overall description length of this code is @xmath56 bits .",
    "the following result shows that the hyb algorithm shares the exact same rate - distortion performance , as well as the same implementation complexity characteristics , as the gvw .",
    "let : @xmath110    * theorem 2 .",
    "* consider a memoryless source @xmath19 with rate - distortion function @xmath7 , which is to be compressed at target distortion level @xmath42 .",
    "there exists an @xmath111 such that , for any @xmath112 , the hyb algorithm with parameters @xmath113 and @xmath3 as in ( [ eq : elldefn1 ] ) achieves a rate of @xmath45 bits / symbol , its expected distortion is less than @xmath114 , and moreover :     encoding time per source symbol is proportional to @xmath115 ,     decoding time per symbol is independent of @xmath92 and @xmath116 ,    where @xmath117 and @xmath118 are independent of @xmath116 and @xmath92 .",
    "* remarks .",
    "theorem  2 is an exact analog of theorem  1 proved for gvw in @xcite , the only difference being that we consider average distortion instead of the probability - of - excess distortion criterion .",
    "the reason is that , instead of presenting an existence proof for an algorithm with certain desired properties , here we examine the performance of the hyb algorithm itself .",
    "indeed , the proof of theorem  2 can easily be modified to prove the stronger claim that there exists _ some _ instance of the random database @xmath119 such that , using that particular database , the hyb algorithm also has the additional property that the probability of excess distortion vanishes as @xmath120 .",
    "the same comments apply to theorem 3 below .",
    "\\2 . in @xcite",
    "a similar result is proved with the roles and @xmath116 and @xmath92 interchanged .",
    "in fact , it should be pointed out that the scheme we call `` the '' gvw algorithm here corresponds to the scheme used in the proof of ( * ? ? ? * theorem  1 ) .",
    "a slight variant ( having to do with the choice of parameter values and not with the mechanics of the algorithm itself ) is used to prove ( * ? ? ?",
    "* theorem  2 ) .",
    "having gone over the proofs , it would be obvious to the reader that , once the corresponding changes are made for hyb , an analogous result can be proved for hyb .",
    "the straightforward but tedious details are omitted .",
    "\\3 . in terms of memory ,",
    "the gvw scheme requires @xmath121 reproduction symbols for storing the codebook , while using the same memory parameters the hyb algorithm needs @xmath122 symbols .",
    "the ratio between the two is , @xmath123 so that the gvw needs @xmath91 _ times _ more memory than hyb .",
    "moreover , the closer we require the algorithm to come to achieving an optimal @xmath124 point , the smaller the values of @xmath116 and @xmath125 need to be taken in theorem  2 , and the larger the corresponding value of @xmath3 ; cf .",
    "equation ( [ eq : elldefn1 ] ) .",
    "therefore , not only the difference , but even the ratio of the memory required by gvw compared to hyb , is unbounded .",
    "the next result shows that , choosing the parameters @xmath3 and @xmath92 in hyb appropriately , optimal compression performance can be achieved with linear decoding complexity and near - linear encoding complexity .",
    "it is a parallel result to ( * ? ? ?",
    "* theorem 3 ) .",
    "* theorem 3 .",
    "* for a memoryless source @xmath19 with rate - distortion function @xmath7 , a target distortion level @xmath42 , and an arbitrary increasing and unbounded function @xmath126 , the hyb algorithm with appropriately chosen parameters @xmath127 and @xmath128 , achieves a limiting rate equal to @xmath7 bits / symbol and limiting average distortion @xmath4 .",
    "the encoding and decoding complexities are @xmath129 and @xmath130 respectively .",
    "the actual empirical performance of hyb on simulated data is compared to that of gvw and llz in the following section .",
    "here the empirical performance of the hyb scheme is compared with that of gvw and llz , on three simulated data sets from simple memoryless sources .",
    "the following parameter values were used in all of the experiments . for the gvw and hyb algorithms ,",
    "@xmath3 was chosen as in @xcite to be @xmath131 , where @xmath7 is the rate - distortion function of the source , and @xmath92 was taken equal to @xmath132 . similarly , for llz we took @xmath133 , @xmath134 and @xmath135 .",
    "note that , with this choice of parameters , the complexity of all three algorithms is approximately linear in the message length @xmath1 .",
    "all experiments were performed on a sony vaio laptop running ubuntu linux , under identical conditions .",
    "first we revisit the example of section  [ s : gvw ] ; @xmath136 bits generated by a bernoulli source with parameter @xmath99 , are compressed by all three algorithms at various different distortion levels with respect to hamming distortion .",
    "figure  [ f : bern04 ] shows the rate - distortion pairs achieved .",
    "bits generated from a bernoulli source with parameter @xmath99 .",
    "the solid convex curve is the rate - distortion function ; the rate - distortion pairs achieved by gvw are shown as blue diamonds ; by llz as red stars ; and by hyb as bold green dots . , width=403,height=364 ]    * rate - distortion performance .",
    "* it is evident that the compression performance obtained by gvw and hyb is near - identical , and better than that of llz .",
    "this example was also examined by rissanen and tabus in @xcite , where it was noted that it is quite hard for any implementable scheme to produce rate - distortion pairs below the straight line connecting the end - points @xmath124 of the rate - distortion curve corresponding to @xmath137 and @xmath138 .",
    "as noted in @xcite , the rissanen - tabus scheme produces results slightly below the straight line , and it is one of the best implementable schemes for this problem .     &",
    "+ & @xmath100 & @xmath101 & rate & memory & time + hyb & 0.05 & 0.06952 & 0.70095 & 0.79 mb & 2m45s + hyb & 0.08 & 0.11238 & 0.59143 & 0.6 mb & 3m06s + hyb & 0.11 & 0.12952 & 0.50381 & 0.59 mb & 3m33s + hyb & 0.14 & 0.15714 & 0.41619 & 0.56 mb & 4m06s + hyb & 0.17 & 0.19143 & 0.32857 & 0.52 mb & 4m40s + hyb & 0.2 & 0.22095 & 0.26286 & 0.53 mb & 5m21s + hyb & 0.23 & 0.23905 & 0.21905 & 0.51 mb & 5m26s + hyb & 0.26 & 0.27048 & 0.15333 & 0.53 mb & 6m27s + hyb & 0.29 & 0.29333 & 0.10952 & 0.53 mb & 6m56s +    * memory and complexity . * tables  [ tab:04a ] and  [ tab:04 ] contain a complete listing off all performance parameters obtained in the above experiment , including the execution time required for the encoder and the total amount of memory used . as already observed in section  [ s : gvw ] , the llz scheme requires much less memory that gvw , and so does the hybrid algorithm hyb .",
    "in fact , while gvw and hyb produce essentially identical rate - distortion performance , the hyb algorithm requires between 32 and 213 _ times _ less memory than gvw . [ note that these figures are deterministic ; the memory requirement is fixed by the description of the algorithm and it is not subject to random variations produced by the simulated data . ] in terms of the corresponding execution times , the gvw and hyb share the exact same theoretical complexity in their implementation . nevertheless , because of the vastly different memory requirements , in practice we find that the execution times of hyb were approximately three to ten times faster than gvw .",
    "the second example is again on a bernoulli source with respect to hamming distortion , this time with source parameter @xmath139 .",
    "the corresponding simulation results are displayed in figure  [ f : bern02 ] and table  [ tab:02 ] .",
    "finally , in the third example @xmath19 is taken as a memoryless source uniformly distributed on @xmath140 , to be compressed with respect to hamming distortion .",
    "the empirical results are shown in figure  [ f:4letter ] and table  [ tab:4letter ] .    in both these cases ,",
    "the same qualitative conclusions are drawn .",
    "the rate - distortion performance of the gvw and hyb algorithms is essentially indistinguishable , while the compression achieved by llz is generally somewhat worse , though in several instances not significantly so . in the second example note",
    "that the memory required by hyb is smaller than that of gvw by a factor that ranges between 44 and 242 , while in the third example the corresponding factors are between 16 and 218 . and again , although the theoretical implementation complexity of gvw and hyb is identical , because of their different memory requirements the encoding time of hyb is smaller than that of gvw by a factor ranging between approximately 3 and 9 in the second example , and between 1.25 and 1.5 in the third example .",
    "bits generated from a bernoulli source with parameter @xmath139 .",
    "the solid curve is the rate - distortion function ; the rate - distortion pairs achieved by gvw are shown as blue diamonds ; by llz as red stars ; and by hyb as bold green dots .",
    ", width=403,height=336 ]     & + & @xmath100 & @xmath101 & rate & memory & time + gvw & 0.04 & 0.05429 & 0.50381 & 25 mb & 19m05s + gvw & 0.055 & 0.07048 & 0.4381 & 28 mb & 18m13s + gvw & 0.07 & 0.08857 & 0.37238 & 35 mb & 19m50s + gvw & 0.085 & 0.10476 & 0.32857 & 42 mb & 20m14s + gvw & 0.1 & 0.12762 & 0.28476 & 49 mb & 19m55s + gvw & 0.115 & 0.12381 & 0.21905 & 59 mb & 20m03s + gvw & 0.13 & 0.12857 & 0.17524 & 73 mb & 19m57s + gvw & 0.145 & 0.14571 & 0.15333 & 90 mb & 19m08s + gvw & 0.16 & 0.16286 & 0.10952 & 126 mb & 19m38s + llz & 0.04 & 0.0381 & 0.64495 & 1.36 mb & 3m05s + llz & 0.055 & 0.05048 & 0.59165 & 2.02 mb & 7m45s + llz & 0.07 & 0.06857 & 0.54836 & 1.9 mb & 8m02s + llz & 0.085 & 0.08381 & 0.50616 & 2.4 mb & 13m38s + llz & 0.1 & 0.09714 & 0.42154 & 3.1 mb & 22m18s + llz & 0.115 & 0.11619 & 0.3083 & 5.2 mb & 24m03s + llz & 0.13 & 0.13048 & 0.26809 & 8.3 mb & 58m07s + llz & 0.145 & 0.14857 & 0.20223 & 21 mb & 132m30s + llz & 0.16 & 0.16571 & 0.1472 & 100 mb & 377m10s + hyb & 0.04 & 0.05429 & 0.50381 & 0.56 mb & 2m02s + hyb & 0.055 & 0.07048 & 0.4381 & 0.53 mb & 2m54s + hyb & 0.07 & 0.08952 & 0.37238 & 0.57 mb & 3m32s + hyb & 0.085 & 0.08286 & 0.32857 & 0.58 mb & 3m52s + hyb & 0.1 & 0.12 & 0.28476 & 0.57 mb & 4m46s + hyb & 0.115 & 0.12857 & 0.21905 & 0.56 mb & 5m21s + hyb & 0.13 & 0.13143 & 0.17524 & 0.55 mb & 5m45s + hyb & 0.145 & 0.14286 & 0.15333 & 0.52 mb & 6m30s + hyb & 0.16 & 0.17429 & 0.10952 & 0.52 mb & 7m11s +     symbols generated from the uniform source on @xmath140 .",
    "the solid curve is the rate - distortion function ; the rate - distortion pairs achieved by gvw are shown as blue diamonds ; by llz as red stars ; and by hyb as bold green dots .",
    ", width=403,height=336 ]     & + & @xmath100 & @xmath101 & rate & memory & time + gvw & 0.1 & 0.1419 & 1.41714 & 43 mb & 10m27s + gvw & 0.16 & 0.20095 & 1.16095 & 24 mb & 6m44s + gvw & 0.22 & 0.25238 & 0.92 & 31 mb & 8m19s + gvw & 0.28 & 0.31333 & 0.72286 & 44 mb & 11m12s + gvw & 0.34 & 0.36762 & 0.56952 & 43 mb & 9m45s + gvw & 0.4 & 0.42381 & 0.41619 & 65 mb & 12m29s + gvw & 0.46 & 0.47238 & 0.30667 & 92 mb & 13m59s + gvw & 0.52 & 0.53238 & 0.19714 & 124 mb & 14m12s + gvw & 0.58 & 0.58952 & 0.10952 & 229 mb & 17m30s + llz & 0.1 & 0.06857 & 1.97778 & 3.597 mb & 9m54s + llz & 0.16 & 0.1381 & 1.53794 & 1.79 mb & 7m46s + llz & 0.22 & 0.20381 & 1.25461 & 2.04 mb & 12m50s + llz & 0.28 & 0.26952 & 1.02841 & 2.61 mb & 18m51s + llz & 0.34 & 0.33524 & 0.76228 & 3.445 mb & 28m25s + llz & 0.4 & 0.4019 & 0.5393 & 3.49 mb & 30m37s + llz & 0.46 & 0.46381 & 0.3893 & 5.44 mb & 46m19s + llz & 0.52 & 0.52571 & 0.25807 & 14.6 mb & 105m56s + llz & 0.58 & 0.58571 & 0.17475 & 104 mb & 62m16s + hyb & 0.1 & 0.1419 & 1.41714 & 2.58 mb & 7m49s + hyb & 0.16 & 0.19714 & 1.16095 & 1.22 mb & 5m06s + hyb & 0.22 & 0.25429 & 0.92 & 1.26 mb & 6m37s + hyb & 0.28 & 0.30762 & 0.72286 & 1.39 mb & 8m48s + hyb & 0.34 & 0.37238 & 0.56952 & 1.05 mb & 7m42s + hyb & 0.4 & 0.42095 & 0.41619 & 1.18 mb & 9m39s + hyb & 0.46 & 0.47143 & 0.30667 & 1.15 mb & 10m34s + hyb & 0.52 & 0.52952 & 0.19714 & 1.01 mb & 10m14s + hyb & 0.58 & 0.58476 & 0.10952 & 1.05 mb & 11m43s +    @xmath102    @xmath102",
    "the starting point for this work was the observation that there is a certain duality relationship between the divide - and - conquer compression schemes of gupta - verd - weissman ( gvw ) in @xcite , and certain lossy lempel - ziv schemes based on a fixed - database as in @xcite . to explore this duality , llz ,",
    "a new ( non - universal ) lossy lz algorithm was introduced , and it was shown to be asymptotically rate - distortion optimal . to combine the low - complexity advantage of gvw with the low - memory requirement of llz , a hybrid algorithm , called hyb ,",
    "was then proposed , and its properties were explored both theoretically and empirically .    the main contribution of this short paper is the introduction of memory considerations in the usual compression - complexity trade - off .",
    "building on the success of the gvw algorithm , it was shown that the hyb scheme simultaneously achieves three goals : 1 .",
    "its rate - distortion performance can be made arbitrarily close to the fundamental rate - distortion limit ; 2 .",
    "the encoding complexity can be tuned in a rigorous manner so as to balance the trade - off of encoding complexity vs.  compression redundancy ; and 3 .",
    "the memory required for the execution of the algorithm is much smaller than that required by gvw , a difference which may be made arbitrarily large depending on the choice of parameters .",
    "moreover , empirically , for blocklengths of the order of thousands , the hyb scheme appears to outperform existing schemes for the compression of simple memoryless sources with respect to hamming distortion .",
    "lastly , we briefly mention that the results presented in this paper can be extended in several directions .",
    "first we note that the finite - alphabet assumption was made exclusively for the sake of simplicity of exposition and to avoid cumbersome technicalities .",
    "while keeping the structure of all three algorithms exactly the same , this assumption can easily be relaxed , at the price of longer , more technical proofs , along the lines of arguments , e.g. , in @xcite@xcite@xcite@xcite .",
    "for example , theorem  4 of @xcite which gives precise performance and complexity bounds for the gvw used with general source and reproduction alphabets and with respect to an unbounded distortion measure , can easily be generalized to hyb .",
    "similarly , theorem  5 of @xcite which describes the performance of a universal version of gvw can also be generalized to the corresponding statement for a universal version of hyb ( with obvious modifications ) , although , as noted in @xcite , the utility of that result is purely of theoretical interest .",
    "recall that , under the present assumptions , the rate - distortion function @xmath7 is continuous , differentiable , convex and nonincreasing @xcite@xcite . given @xmath42 and @xmath96 , assume without loss of generality that @xmath141 ; then we can choose @xmath43 according to @xmath142 , so that @xmath143 .",
    "[ as it does not change the asymptotic analysis below , we take @xmath94 fixed and arbitrary . ]",
    "then the distortion part of the theorem is immediate by the construction of the algorithm .    before considering the rate , we record two useful asymptotic results for the match - lengths @xmath81 .",
    "let @xmath45 , and @xmath103 as in ( [ eq : m ] ) .",
    "then ( * ? ? ?",
    "* theorem  23 ) immediately implies that , @xmath144 moreover , for any @xmath145 , the following more precise asymptotic lower bound on @xmath64 holds : as @xmath146 , ( m())\\{l_,1 | x_1^n } 0 [ eq : tails ] the proof of ( [ eq : tails ] ) is a straightforward simplification of the proof of ( * ? ? ?",
    "* corollary  3 ) , and therefore omitted .",
    "now let @xmath145 arbitrary .",
    "the encoder parses the message @xmath147 into @xmath148 distinct words @xmath80 , each of length @xmath81 .",
    "we let @xmath149 and following @xcite we assume , without loss of generality , that @xmath150 is an integer and that the last phrase is complete , i.e. , @xmath151    to bound above the rate obtained by llz , we consider phrases of different lengths separately .",
    "we call a phrase @xmath80 _ long _ if its length satisfies @xmath152 , and we call @xmath80 _ short _ otherwise . recalling ( [ eq : ell - bound ] ) , the total description length of the llz can be broken into two parts as , ( x_1^n ) & & _ k : z^(k ) + & & + _ k : z^(k ) .",
    "[ eq : total - bound ] for the first sum we note that , by the choice of @xmath89 and the definition of a short phrase , each summand is bounded above by a constant times @xmath150 , at least for all @xmath3 large enough ; therefore , the conditional expectation of the whole sum given @xmath147 is bounded by , e\\ { c_1n _",
    "k=1^ _ i_\\{l_m , kn } |x_1^n } c_2 m ( ) n \\{l_m,1    function of an event @xmath153 , and the inequality follows by considering not just all @xmath50 s , but all the possible positions in @xmath147 where a short match can occur .",
    "dividing by @xmath1 and letting @xmath120 , from ( [ eq : tails ] ) we get that this expression converges to zero w.p .",
    "1 , so that the conditional expectation of the first term in ( [ eq : total - bound ] ) also converges to zero , w.p.1 .    for the second and dominant term in ( [ eq : total - bound ] ) , let @xmath154 be the number of long phrases @xmath80 . since each long @xmath80 has length @xmath155 , we must have @xmath156 , so that .",
    "[ eq : bad ] also , by the definition of @xmath89 , for all @xmath3 large enough ( independently of @xmath1 ) , we have , @xmath157 therefore , the second sum in ( [ eq : total - bound ] ) can be bounded above by , @xmath158 combining this with the fact that the first term in ( [ eq : total - bound ] ) vanishes , immediately yields _ _ n e\\{|x_1^n } ( r()+)(1 + ) , and since @xmath145 was arbitrary we get the first claim in the theorem .",
    "finally , the second claim follows from the first and fatou s lemma .",
    "the proof of the theorem is based on lemma  1 below , which plays the same role as ( * ? ? ? * lemma",
    "1 ) in the proof of ( * ? ? ?",
    "* theorem  1 ) .",
    "the rest of of the proof is identical , except for the fact that we do not need to invoke the law of large numbers , since here we do not claim that the probability of excess distortion goes to zero .    before stating the lemma , we define the following auxiliary quantities : @xmath159 , @xmath160 , @xmath161 and , @xmath162    * lemma 1 .",
    "* consider a memoryless source @xmath19 to be compressed at target distortion level @xmath42 .",
    "then for any @xmath112 , the hyb algorithm with parameters @xmath113 and = , [ eq : elldefn1 ] when applied to a single block @xmath163 achieves rate @xmath45 , and its expected distortion is less than @xmath114 .",
    "_ given @xmath145 , choose a positive @xmath164 such that , @xmath165 now follow the proof of ( * ? ? ?",
    "* lemma  1 ) with @xmath166 in place of @xmath116 , until the beginning of the computation of the probability of excess distortion .",
    "the key observation is that , for hyb , this probability can be bounded above by the excess - distortion probability with respect to a random codebook with @xmath167 words , by just considering possible matches starting at positions @xmath168 , making the corresponding potentially matching blocks in the database independent .",
    "therefore , following the same computation , the required probability can be bounded above as before by , 2(2 ^ -c(d)^2)+2 ^ -/4 .",
    "[ eq : prob ] the first term is bounded above by , @xmath169 as before , and in order to show that the expected distortion is less than @xmath116 it suffices to show that the last term satisfies , ( -d)2^(r(d)+)</3 .",
    "[ eq : target ] substituting the choice of @xmath3 from ( [ eq : elldefn1 ] ) , it becomes , @xmath170 and since @xmath92 is restricted to be less than one , this can in turn be bounded above , uniformly in @xmath171 , by its value at @xmath172 .",
    "[ to see that , note that the function @xmath173 is increasing for @xmath174 and decreasing for @xmath175 . by our choice of @xmath176 ,",
    "the maximum above is achieved at the point @xmath177 . ] therefore , noting also that @xmath178 , this term is bounded above by , @xmath179 which , after some algebra , simplifies to , @xmath180 and this is less than @xmath181 by the choice of @xmath166 .",
    "this establishes ( [ eq : target ] ) and completes the proof of the lemma .      taking @xmath182 arbitrary , we let , as in the proof of ( * ? ? ?",
    "* theorem 3 ) , @xmath183 for each @xmath1 we use hyb with the corresponding parameters ; the rate result follows from the construction of the algorithm , which , at blocklength @xmath1 , has rate no larger than , @xmath184 as @xmath120 .",
    "regarding the distortion , equation ( [ eq : prob ] ) in the proof of theorem  2 shows that that the probability of the event that the distortion of the @xmath69th block will exceed @xmath4 is bounded above by , @xmath185 it is easily seen that , for large @xmath1 , this is dominated by the second term , @xmath186 therefore , the distortion of any one @xmath3-block is bounded above by , @xmath187 noting that the excess term goes to zero as @xmath120 , it will still go to zero when averaged out over all @xmath188 sub - blocks , and , therefore , the expected distortion over the whole message @xmath147 will converge to @xmath4 .",
    "finally , the complexity results are straightforward by construction ; see the discussion in ( * ? ? ?",
    "* section  ii - a ) .",
    "we thank sergio verd for sharing with us preprints of @xcite and @xcite .",
    "d.  arnaud and w.  szpankowski . pattern matching image compression with prediction loop : preliminary experimental results . in _ proc .",
    "data compression conf .",
    " dcc 97 _ , los alamitos , california , 1997 .",
    "ieee , ieee computer society press .",
    "d.j.c . mackay and r.m .",
    "good codes based on very sparse matrices . in _ cryptography and coding .",
    "5th i m a conference , number 1025 in lecture notes in computer science _ , pages 100111 .",
    "springer , 1995 .",
    "h.  morita and k.  kobayashi .",
    "an extension of lzw coding algorithm to source coding subject to a fidelity criterion . in _",
    "4th joint swedish - soviet int .",
    "workshop on inform .",
    "theory _ , pages 105109 , gotland , sweden , 1989 .",
    "coding theorems for a discrete source with a fidelity criterion . ,",
    "part  4:142163 , 1959 . reprinted in d.",
    "slepian ( ed . ) , _ key papers in the development of information theory _ , ieee press , 1974 .",
    "wainwright and e.  maneva .",
    "lossy source encoding via message - passing and decimation over generalized codewords of ldgm codes . in _ proc . of the ieee international symposium on inform .",
    "theory _ , pages 14931497 , adelaide , australia , sept ."
  ],
  "abstract_text": [
    "<S> the compression - complexity trade - off of lossy compression algorithms that are based on a random codebook or a random database is examined . </S>",
    "<S> motivated , in part , by recent results of gupta - verd - weissman ( gvw ) and their underlying connections with the pattern - matching scheme of kontoyiannis lossy lempel - ziv algorithm , we introduce a non - universal version of the lossy lempel - ziv method ( termed llz ) . </S>",
    "<S> the optimality of llz for memoryless sources is established , and its performance is compared to that of the gvw divide - and - conquer approach . </S>",
    "<S> experimental results indicate that the gvw approach often yields better compression than llz , but at the price of much higher memory requirements . to combine the advantages of both </S>",
    "<S> , we introduce a hybrid algorithm ( hyb ) that utilizes both the divide - and - conquer idea of gvw and the single - database structure of llz . </S>",
    "<S> it is proved that hyb shares with gvw the exact same rate - distortion performance and implementation complexity , while , like llz , requiring less memory , by a factor which may become unbounded , depending on the choice or the relevant design parameters . </S>",
    "<S> experimental results are also presented , illustrating the performance of all three methods on data generated by simple discrete memoryless sources . in particular , the hyb algorithm is shown to outperform existing schemes for the compression of some simple discrete sources with respect to the hamming distortion criterion .     lossy data compression , rate - distortion theory , pattern matching , lempel - ziv , random codebook , fixed database </S>"
  ]
}