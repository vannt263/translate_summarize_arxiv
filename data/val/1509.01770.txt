{
  "article_text": [
    "a wide range of real - world data takes the format of _ matrices _ and _ tensors _ , e.g. , recommendation @xcite , video sequences @xcite , climates @xcite , genomes @xcite , and neuro - imaging @xcite .",
    "a naive way to learn from such matrix and tensor data is to vectorize them and apply ordinary regression or classification methods designed for vectorial data .",
    "however , such a vectorization approach would lead to loss in structural information of matrices and tensors such as _ low - rankness_.    the objective of this paper is to investigate regression and classification methods that directly handle tensor data without vectorization .",
    "low - rank structure of _ data _ has been successfully utilized in various applications such as missing data imputation @xcite , robust principal component analysis @xcite , and subspace clustering @xcite . in this paper , instead of low - rankness of data itself , we consider its dual_learning coefficients _ of a regressor and a classifier .",
    "low - rankness in learning coefficients means that only a subspace of feature space is used for regression and classification",
    ".    for matrices , regression and classification has been studied in @xcite and @xcite in the context of eeg data analysis .",
    "it was experimentally demonstrated that directly learning matrix data by low - rank regularization can significantly improve the performance compared to learning after vectorization .",
    "another advantage of using low - rank regularization in the context of eeg data analysis is that analyzing _",
    "singular value spectra _ of learning coefficients is useful in understanding activities of brain regions .",
    "more recently , an inductive learning method for tensors has been explored @xcite .",
    "compared to the matrix case , learning with tensors is inherently more complex .",
    "for example , the _ multilinear _ ranks of tensors make it more complicated to find a proper low - rankness of a tensor compared to matrices which has only one rank .",
    "so far , several tensor norms such as the _ overlapped trace norm _ or the _ tensor nuclear norm _ @xcite , the _ latent trace norm _",
    "@xcite , and the _ scaled latent trace norm _ @xcite have been proposed and demonstrated to perform well for various tensor structures .",
    "however , theoretical analysis of tensor learning in inductive learning settings has not been much investigated yet .",
    "another challenge in inductive tensor learning is efficient optimization strategies , since tensor data often has much higher dimensionalities than matrix and vector data .    in this paper",
    ", we theoretically and experimentally investigate tensor - based regression and classification with regularization by the overlapped trace norm , the latent trace norm , and the scaled latent trace norm .",
    "we first provide their dual formulations and propose optimization procedures using the _ alternating direction method of multipliers _",
    "@xcite , which is computationally efficient when the number of data samples is moderate .",
    "we then derive an excess risk bound for each tensor regularization , which allows us to theoretically understand the behavior of tensor norm regularization .",
    "more specifically , we elucidate that the excess risk of the overlapped trace norm is bounded with the average multilinear ranks of each mode , that of the latent trace norm is bounded with the minimum multilinear rank among all modes , and that of the scaled latent trace norm is bounded with the minimum ratio between multilinear ranks and mode dimensions .",
    "finally , for simulated and real tensor data , we experimentally investigate the behavior of tensor - based regression and classification methods .",
    "the experimental results are in concordance with our theoretical findings , and tensor - based learning methods compare favorably with vector- and matrix - based methods .",
    "the remainder of the paper is organized as follows . in section  2",
    ", we formulate the problem of tensor - based supervised learning and review the overlapped trace norm , the latent trace norm , and the scaled latent trace norm . in section  3 , we derive dual optimization algorithms based on the alternating direction method of multipliers . in section  4 , we theoretically give an excess risk bound for each tensor norm . in section  5 ,",
    "we give experimental results on both artificial and real - world data and illustrate the advantage of tensor - based learning methods .",
    "finally , in section  6 , we conclude this paper .      throughout the paper ,",
    "we use standard tensor notation following @xcite .",
    "we represent a @xmath0-way tensor as @xmath1 that consists of @xmath2 elements .",
    "a mode-@xmath3 fiber of @xmath4 is an @xmath5-dimensional vector which can be obtained by fixing all except the @xmath3th index .",
    "the mode-@xmath3 unfolding of tensor @xmath4 is represented as @xmath6 which is obtained by concatenating all the @xmath7 mode-@xmath3 fibers along its columns .",
    "the spectral norm of a matrix @xmath8 is denoted by @xmath9 which is the maximum singular value of @xmath8 .",
    "the operator @xmath10 is the sum of element - wise multiplications of @xmath4 and @xmath11 , i.e. , @xmath12 .",
    "the frobenius norm of a tensor @xmath11 is defined as @xmath13 .",
    "in this section , we put forward inductive tensor learning models with tensor regularization and review different tensor norms used for low - rank regularization .",
    "our focus in this paper is regression and classification of tensor data .",
    "let us consider a data set @xmath14 , where @xmath15 is a covariate tensor and @xmath16 is a target .",
    "@xmath17 for regression , while @xmath18 for classification .",
    "we consider the following learning model for a tensor norm @xmath19 : @xmath20 where @xmath21 is the loss function : the squared loss , @xmath22 is used for regression , and the logistic loss , @xmath23 is used for classification .",
    "@xmath24 is the bias term and @xmath25 is the regularization parameter . if @xmath26 or @xmath27 , then the above problem is equivalent to ordinary vector - based @xmath28- or @xmath29-regularization .    to understand the effect of tensor - based regularization , it is important to investigate the low - rankness of tensors . when considering a matrix @xmath30 , its _ trace norm _",
    "is defined as @xmath31 where @xmath32 is the @xmath33 singular value and @xmath34 is the number of non - zero singular values ( @xmath35 ) .",
    "a matrix is called _ law rank _",
    "if @xmath36 . the matrix trace norm is a convex envelop to the matrix rank and it is commonly used in matrix low - rank approximation @xcite .    as in matrices ,",
    "the rank property is also available for tensors , but it is more complicated due to its multidimensional structure .",
    "the mode-@xmath3 rank @xmath37 of a tensor @xmath38 is defined as the rank of mode-@xmath3 unfolding @xmath39 and the multilinear rank of @xmath4 is given as @xmath40 .",
    "the mode-@xmath41 of a tensor @xmath4 is called low rank if @xmath42 .",
    "one of the earliest definitions of a tensor norm is the _ tensor nuclear norm _",
    "@xcite or the _ overlapped trace norm _",
    "@xcite , which can be represented for a tensor @xmath43 as @xmath44 the overlapped trace norm can be viewed as a direct extension of the matrix trace norm since it unfolds a tensor on each of its mode and computes the sum of trace norms of the unfolded matrices .",
    "regularization with the overlapped trace norm can also be seen as an overlapped group regularization due to the fact that the same tensor is unfolded over different modes and regularized with the trace norm .",
    "one of the popular applications of the overlapped trace norm is _ tensor completion _",
    "@xcite , where missing entries of a tensor are imputed .",
    "another application is _ multilinear multitask learning _",
    "@xcite , where multiple vector - based linear learning tasks with a common feature space are arranged as a tensor feature structure and the multiple tasks are solved together with constraints to minimize the multilinear ranks of the tensor feature .",
    "theoretical analyses on the overlapped norm have been carried out for both tensor completion @xcite and multilinear multitask learning @xcite ; they have shown that the prediction error of overlapped trace norm regularization is bounded by the average mode-@xmath3 ranks which can be large if some modes are close to full rank even if there are low - rank modes .",
    "thus , these studies imply that the overlapped trace norm performs well when the multilinear ranks have small variations , and it may result in a poor performance when the multilinear ranks have high variations .    to overcome the weakness of the overlapped trace norm , recent research in tensor norms has led to new norms such as the _ latent trace norm _ @xcite and the _ scaled latent trace norm _",
    "@xcite .",
    "@xcite proposed the latent trace norm as @xmath45 the latent trace norm takes a mixture of @xmath0 latent tensors which is equal to the number of modes , and regularizes each of them separately .",
    "in contrast to the overlapped trace norm , the latent tensor trace norm regularizes different latent tensors for each unfolded mode and this gives the tendency that the latent tensor trace norm picks the latent tensor with the lowest rank .",
    "in general , the latent trace norm results in a mixture of latent tensors and the content of each latent tensor would depend on the rank of its unfolding . in an extreme case , for a tensor with all its modes full except one mode , regularization with the latent tensor trace norm would result in making the latent tensor with",
    "the lowest mode become prominent while others become zero .",
    "recently , @xcite proposed the scaled latent trace norm as an extension of the latent trace norm : @xmath46 compared to the latent trace norm , the scaled latent trace norm takes the rank relative to the mode dimension .",
    "a major drawback of the latent trace norm is its inability to identify the rank of a mode relative to its dimension .",
    "if a tensor has a mode where its dimension is smaller than other modes yet its relative rank with respect to its mode dimension is high compared to other modes , the latent trace norm could incorrectly pick the smallest mode .",
    "the scaled latent norm has the ability to overcome this problem by its scaling with the mode dimensions such that it is able to work with the relative ranks of the tensor . in the context of multilinear multitask learning",
    ", it has been shown that the scaled latent trace norm works well for tensors with high variations in multilinear ranks and mode dimensions compared to the overlapped trace norm and the latent trace norm @xcite .",
    "the inductive learning setting mentioned in with the overlapped trace norm has been studied previously in @xcite . however",
    ", theoretical analysis and performance comparison with other tensor norms have not been conducted yet . similarly to tensor decomposition @xcite and multilinear multitask learning @xcite , tensor - based regression and classification",
    "may also be improved by regularization methods that can work with high variations in multilinear ranks and mode dimensions .    in the following sections , to make tensor - based learning more practical and to improve the performance , we consider formulation with the overlapped trace norm , the latent trace norm , and the scaled latent trace norm , and give computationally efficient optimization algorithms and excess risk bounds .",
    "in this section , we consider the dual formulation for and propose computationally efficient optimization algorithms . since optimization of with regularization using the overlapped trace norm has already been studied in @xcite , we do not discuss it again here .",
    "our main focus in this section is optimization of with regularization using the latent trace norm and the scaled latent trace norm .",
    "let us consider the formulation for a data set @xmath47 with latent and scaled latent trace norm regularization as follows : @xmath48 where , for @xmath49 and for any given regularization parameter @xmath50 , @xmath51 in the case of the latent trace norm and @xmath52 in the case of the scaled latent trace norm , respectively .",
    "@xmath53 is the unfolding of @xmath54 on its @xmath3th mode .",
    "it is worth noticing that the application of the latent and scaled latent trace norms requires optimizing over @xmath0 latent tensors which contain @xmath55 variables in total . for large @xmath0 and @xmath56 ,",
    "solving the primal problem can be computationally expensive especially in non - linear problems such as logistic regression , since they require computationally expensive optimization methods such as gradient descent or the newton method .",
    "if the number of training samples @xmath57 is @xmath58 , solving the dual problem of could be computationally more efficient .",
    "for this reason , we focus on optimization in the dual below .    the dual formulation of can be written as follows ( its detailed derivation is given in appendix  a ) : @xmath59 @xmath60 @xmath61 where @xmath62 are dual variables corresponding to the training data set @xmath14 , @xmath63 is the _ conjugate loss function _ defined as @xmath64 in the case of regression with the squared loss @xcite , and @xmath65 with constraint @xmath66 in the case of classification with the logistic loss @xcite .",
    "@xmath67 is the indicator function defined as @xmath68 if @xmath69 and @xmath70 otherwise .",
    "the constraint @xmath71 is due to the bias term @xmath72 . here , the auxiliary variables @xmath73 are introduced to remove the coupling between the indicator functions in the objective function ( see appendix  a for details ) .",
    "the _ alternating direction method of multipliers _ ( admm ) @xcite has been previously used to solve primal problems of tensor decomposition @xcite and multilinear multi - task learning @xcite with the overlapped trace norm regularization .",
    "optimization in the dual for tensor decomposition problems with the latent and scaled latent trace norm regularization has been solved using admm in @xcite . here",
    ", we also adopt admm to solve , and describe the formulation and the optimization steps in detail .    with introduction of dual variables @xmath74",
    "( corresponding to the primal variables of ) , @xmath24 , and parameter @xmath75 , the augmented lagrangian function for is defined as follows : @xmath76 this admm formulation is solved for variables @xmath77 , @xmath78 , @xmath79 , and @xmath72 by considering sub - problems for each variable .",
    "below , we give the solution for each variable at iterative step @xmath80 .",
    "the first sub - problem to solve is for @xmath77 at step @xmath80 : @xmath81 where @xmath82 , and @xmath83 are the solutions obtained at step @xmath84 .",
    "depending on the conjugate loss @xmath63 , the solution for @xmath77 differs . in the case of regression with the squared loss",
    ", the augmented lagrangian can be minimized with respect to @xmath77 by solving the following linear equation : @xmath85 where @xmath86 \\in \\mathbb{r}^{m \\times n}$ ] , @xmath87 , @xmath88 , @xmath89 , and @xmath90 is the @xmath57-dimensional vector of all ones .",
    "note that , in the above system of equations , coefficient matrix multiplied with @xmath77 does not change during optimization .",
    "thus , it can be efficiently solved at each iteration by precomputing the cholesky factorization of the matrix .    for classification with the logistic loss",
    ", the newton method is used to find the solution for @xmath91 , which requires the gradient and the hessian of @xmath92 : @xmath93 k\\beta   \\langle \\mathcal{x}_{i } ,   \\mathcal{x}_{j } \\rangle + \\beta & ( i\\neq j ) .",
    "\\end{cases}\\end{aligned}\\ ] ]    next , we update @xmath94 at step @xmath95 by solving the following sub - problem : @xmath96 where @xmath97 and @xmath98 .",
    "finally , we update the dual variables @xmath54 and @xmath72 at step @xmath80 as @xmath99 note that step and step can be combined as @xmath100 where @xmath101 and @xmath98 .",
    "this allows us to avoid computing singular values and the associated singular vectors that are smaller than the threshold @xmath102 in .      as a stopping condition , we use the _ relative duality gap _ @xcite , which can be expressed as @xmath103 where @xmath104 is the primal solution at step @xmath84 of and @xmath105 is a predefined tolerance value",
    "@xmath106 is the dual solution at step @xmath84 of with @xmath107 obtained by multiplying @xmath77 with @xmath108 , where @xmath109 and @xmath110 is the largest singular value of @xmath111 .",
    "in this section , we theoretically analyze the excess risk for regularization with the overlapped trace norm , the latent trace norm , and the scaled latent trace norm .",
    "we consider a loss function @xmath112 which is lipshitz continuous with constant @xmath113 .",
    "note that this condition is true for both the squared loss and logistic loss functions .",
    "let the training data set be given as @xmath114 , where @xmath115 for regression and @xmath116 for classification . in our theoretical analysis",
    ", we assume that elements of @xmath117 independently follow the standard gaussian distribution .    as the standard formulation @xcite",
    ", the empirical risk without the bias term is defined as @xmath118 and the expected risk is defined as @xmath119 where @xmath120 is the probability distribution from which @xmath121 are sampled .    the optimal @xmath122 that minimizes the expected risk is given as @xmath123 where @xmath124 is either the overlapped trace norm , the latent trace norm , or the scaled latent trace norm .",
    "the optimal @xmath125 that minimizes the empirical risk is denoted as @xmath126    the next lemma provides an upper bound of the excess risk for tensor - based learning problems ( see appendix  b for its proof ) , where @xmath127 is the dual norm of @xmath128 for @xmath129 :    for a given @xmath113-lipchitz continuous loss function @xmath112 and for any @xmath130 such that @xmath131 for problems  , the excess risk for a given training data set @xmath132 is bounded with probability at least @xmath133 as @xmath134 where @xmath135 and @xmath136 are rademacher random variables .",
    "the next theorem gives an excess risk bound for overlapped trace norm regularization ( its proof is also included in appendix  b ) , which is based on the inequality @xmath137 given in @xcite :    with probability at least @xmath138 , the excess risk of learning using the overlapped trace norm regularization for any @xmath122 with @xmath139 , multilinear ranks @xmath140 , and estimator @xmath125 with @xmath141 is bounded as @xmath142 where @xmath143 and @xmath144 and @xmath145 are constants .    in the next theorem",
    ", we give an excess risk bound for the latent trace norm ( its proof is also included in appendix  b ) , which uses the inequality @xmath146 given in @xcite :    with probability at least @xmath138 , the excess risk of learning using the latent norm regularization for any @xmath122 with @xmath139 , multilinear ranks @xmath140 , and estimator @xmath125 with @xmath147 is bounded as @xmath148 where @xmath143 and @xmath149 , @xmath150 , and @xmath151 are constants .",
    "the above theorem shows that the excess risk for the latent trace norm is bounded by the minimum multilinear rank .",
    "if @xmath152 , the latent trace norm is always better then the overlapped trace norm in terms of the excess risk bounds because @xmath153 .",
    "if the dimensions @xmath154 are not the same , the overlapped trace norm could be better .    finally , we bound the excess risk for the scaled latent trace norm based on the inequality @xmath155 given in @xcite :    with probability at least @xmath138 , the excess risk of learning using the scaled latent trace norm regularization for any @xmath122 with @xmath139 , multilinear ranks @xmath140 , and estimator @xmath125 with @xmath156 is bounded as @xmath157 where @xmath149 , @xmath150 , and @xmath151 are constants .    note that when @xmath158 and the multilinear ranks @xmath159 are different , the bounds in theorem  2 and theorem  3 are the same .",
    "theorem  3 shows that the excess risk for regularization with the scaled latent trace norm is bounded with the minimum of multilinear ranks relative to their mode dimensions .",
    "similarly to the latent trace norm , the scaled latent trace norm would also perform better than the overlapped norm when the multilinear ranks have large variations .",
    "if we consider a `` flat '' tensor , the modes with small dimensions may have ranks comparable to their dimensions .",
    "although these modes have the lowest mode-@xmath3 rank , they do not impose a low - rank structure .",
    "in such cases , our theory predicts that the scaled latent trace norm performs better because it is sensitive to the mode-@xmath3 rank relative to its dimension .    as a variation",
    ", we can also consider a mode - wise `` scaled '' version of the overlapped trace norm defined as @xmath160 .",
    "it can be easily seen that @xmath161 holds and with the same conditions as in theorem  1 , we can upper - bound the excess risk for the scaled overlapped trace norm regularization as @xmath162 note that when all modes have the same dimensions , @xmath163 coincides with @xmath164 . compared with bound @xmath165 , the scaled latent norm would perform better than the scaled overlapped norm regularization since @xmath166 .",
    "we conducted several experiments using simulated and real - world data to evaluate the performance of tensor - based regression and classification methods with regularizations using different tensor norms .",
    "we discuss simulations for tensor - based regression in section 5.1 , experiments with real - world data for tensor classification in section 5.2 . for all experiments",
    ", we use a matlab@xmath167 environment on a 2.10 ghz ( 2@xmath1688 cores ) intel xeon e5 - 2450 server machine with 128 gb memory .",
    "we report the results of artificial data experiments on tensor - based regression .",
    "we generated three different @xmath169-mode tensors as weight tensors @xmath4 with different multilinear ranks and mode dimensions .",
    "we created two homogenous tensors with equal mode dimensions of @xmath170 with different multilinear ranks @xmath171 and @xmath172 .",
    "the third weight tensor is an inhomogenous case with mode dimensions of @xmath173 , @xmath174 and multilinear ranks @xmath175 . to generate these weight tensors",
    ", we use the _ tucker decomposition _",
    "@xcite of a tensor as @xmath176 , where @xmath177 is the core tensor and @xmath178 are component matrices .",
    "we sample elements of the core tensor @xmath179 from a standard gaussian distribution , choose component matrices @xmath178 to be orthogonal matrices , and generate @xmath4 by mode - wise multiplication of the core tensor and component matrices . to create training samples @xmath180",
    ", we first create the random tensors @xmath117 generated with each element independently sampled from the standard gaussian distribution and obtain @xmath181 , where @xmath182 is noise drawn from the gaussian distribution with mean zero and variance @xmath183 . in our experiments we use cross validation to select the regularization parameter from range @xmath184@xmath185 at intervals of @xmath183 . for the purpose of comparison , we have also simulated matrix regularized regressions for each mode unfolding . also , we experimented with cross validation among matrix regularization on each unfolded matrix to understand whether it can find the correct mode for regularization .",
    "as the baseline vector - based learning method , we use _ ridge regression _",
    "( i.e. , @xmath28-regularized least - squares ) .",
    "figure  1 shows the performance of homogenous tensors with equal mode dimensions @xmath170 and equal multilinear ranks @xmath186 @xmath187 .",
    "we see that the overlapped norm performs the best , while both latent norms perform equally ( since mode dimensions are equal ) but inferior to the overlapped norm .",
    "also , the regression results from all matrix regularizations with individual modes perform better than the latent and the scaled latent norm regularized regression models . due to the equal multilinear ranks and equal mode dimensions",
    ", it results in equal performance with cross validation among each mode - wise unfolded matrix regularization .",
    "figure  2 shows the performances of homogenous tensors with equal mode dimensions @xmath170 and unequal multilinear ranks @xmath188 . in this case",
    ", both the latent and the scaled latent norms also perform equally since tensor dimensions are the same .",
    "the mode-@xmath189 regularized regression models give the best performance since it has the lowest rank and regularization with the latent and scaled latent norms gives the next best performance .",
    "the mode - wise cross validation correctly coincides with the mode-@xmath189 regularization .",
    "the overlapped norm performs poorly compared to the latent and the scaled latent trace norms .",
    "figure  3 shows the performance of inhomogenous tensors with mode dimensions @xmath173 , @xmath174 and multilinear ranks @xmath190 . in this case",
    ", we can see that the scaled latent trace norm outperforms all other tensor norms .",
    "the latent trace norm performs poorly since it fails to find the mode with the lowest rank",
    ". this well agrees with our theoretical analysis : as shown in , the excess risk of the latent trace norm is bounded with the minimum of multilinear ranks , which is on the first mode in the current setup and it is high ranked .",
    "the scaled latent trace norm is able to find the mode with the lowest rank since it takes the relative rank with respect to the mode dimension as in .",
    "if we look at the individual mode regularizations , we see that the best performance is given with the second mode , which has the lowest rank with respect to the mode dimension , and the worst performance is given with the first mode , which is high ranked compared to other modes . here , the mode - wise cross validation is again as good as mode-@xmath191 regularization .     and unequal multilinear rank @xmath172,title=\"fig : \" ] [ fig:1 ]     and unequal multilinear rank @xmath172,title=\"fig : \" ] [ fig:2 ]    , @xmath174 and multilinear rank @xmath186 @xmath192 , title=\"fig : \" ] [ fig:3 ]    , @xmath174 and multilinear rank @xmath186 @xmath192 , title=\"fig : \" ] [ fig:5 ]    it is also worth noticing in all above experiments that ridge regression performed worse than all the tensor regularized learning models .",
    "this highlights the necessity of employing low - rank inducing norms for learning with tensor data without vectorization to get the best performance .",
    "figure  4 shows the computation time for the toy regression experiment with inhomogenous tensors with mode dimensions @xmath173 , @xmath174 and multilinear ranks @xmath186 @xmath192 ( computation time for other setups showed similar tendency and thus we omit the results ) . for each data",
    "set , we measured the computation time of training regression models , cross validation for model selection , and predicting output values for test data .",
    "we can see that methods based on tensor norms and matrix norms are computationally much more expensive compared to ridge regression .",
    "however , as we saw above , they achieves higher accuracy than ridge regression .",
    "it is worth noticing that mode - wise cross validation is computationally more expensive compared to the scaled latent trace norm and other tensor norms .",
    "this computational advantage and comparable performance with respect to the best mode - wise regularization makes the scaled latent trace norm a useful regularization method for tensor - based regression especially for tensors with high variations in its multilinear ranks .",
    "next , we report the results of experiments on tensor classification with the _ cambridge hand gesture data set _ @xcite .",
    "the cambridge hand gesture data set contains image sequences from 9 gesture classes .",
    "these gesture classes include 3 primitive hand shapes of flats , spread , and v - shape , and 3 different hand motions of rightward , leftward , and contrast .",
    "each class has 100 image sequences with different illumination conditions and arbitrary motions of two people .",
    "previously , the _ tensor canonical correlation _",
    "@xcite has been used to classify these hand gestures .    to apply tensor classification ,",
    "first we build action sequences as tensor data by sampling @xmath193 images with equal time intervals from each sequence .",
    "this makes each sequence a tensor of @xmath194 , where the first two modes are down - sampled images as in @xcite and @xmath193 is the number of sampled images . in our experiments , we set @xmath193 at @xmath195 or @xmath196 .",
    "we consider binary classification and we have chosen visually similar sequences of left / flat and left / spread ( figure  5 ) , which we found to be difficult to classify .",
    "we apply standardization of data by mean removal and variance normalization .",
    "we randomly sample data into a training set of @xmath197 data elements , use a validation set of @xmath198 data elements to select the optimal regularization parameter , and finally use a test set of @xmath198 elements to evaluate the learned classifier .",
    "in addition to the tensor regularized learning models , we also trained classifiers with matrix regularization with unfolding on each mode separately .",
    "as a baseline vector - based learning method , we have used the @xmath28-regularized logistic regression .",
    "we also trained mode - wise cross validation with individual mode regularization ( mode - wise cv ) .",
    "we repeated the learning procedure for @xmath196 sample sets for each classifier and the results are shown in table  1 .",
    "[ fig:4 ]    .classification error of experiments with the hand gesture data set .",
    "the boldfaced figures indicate comparable accuracies among classifiers after a t - test with significance of @xmath199 . [",
    "cols=\"^,^,^ \" , ]     the results of the experiment are given in table  [ t:2 ] , which strongly indicate that vector - based logistic regression is clearly outperformed by the overlapped and scaled latent trace norms . also , in most cases , the averaged matrix method performs poorly compared to the optimal tensor structured regularization methods .",
    "mode-1 regularization performs poorly since mode-1 was high ranked compared to the other modes .",
    "similarly , the latent trace norm gives poor performance since it can not properly regularize since it does not consider the rank relative to the mode dimension .",
    "for all subjects , mode-2 and mode-3 unfolded regularizations result in the same performance due to the symmetry of each @xmath200 resulting in same rank along mode-2 and mode-3 unfoldings . for subject _ aa _ , the scaled latent norm , mode-1 , mode-2 , and mode - wise cross validation give the best or comparable performance .",
    "in subject _",
    "al _ , all classifiers except the latent norm and mode-1 regularization gives comparable performance .",
    "for all other subjects except for _ aa _ and _ al _ , the overlapped trace norm gives the best performance .",
    "in contrast to the computation time for regression experiments , in this experiment , we see that the computation time for tensor trace norm regularizations are more expensive compared to the mode - wise regularization . also , the mode - wise cross validation is computationally less expensive than the scaled latent trace norm and other tensor trace norms .",
    "this is a slight drawback with the tensor norms , though they tend to have higher classification accuracy .",
    "in this paper , we have studied tensor - based regression and classification with regularization using the overlapped trace norm , the latent trace norm , and the scaled latent trace norm .",
    "we have provided dual optimization methods , theoretical analysis and experimental evaluations to understand tensor - based inductive learning .",
    "our theoretical analysis on excess risk bounds showed the relationship of excess risks with the multilinear ranks and dimensions of the weight tensor .",
    "our experimental results on both simulated and real data sets further confirmed the validity of our theoretical analyses . from the theoretical and empirical results",
    ", we can conclude that the performance of regularization with tensor norms depends on the multilinear ranks and mode dimensions , where the latent and scaled latent norms are more robust in tensors with large variations of multilinear ranks .",
    "our research opens up many future research directions .",
    "for example , an important direction is on improvement of optimization methods .",
    "optimization over the latent tensors that results in the use of the latent trace norm and the scaled latent trace norm increases the computational cost compared to the vectorized methods .",
    "also , computing multiple singular value decompositions and solving newton optimization sub - problems ( for logistic regression ) at each iterative step are computationally expensive .",
    "this is evident from our experimental results on computation time for regression and classification",
    ". it would be an important direction to develop computationally more efficient methods for learning with tensor data to make it more practical .",
    "regularization with a mixture of norms is common in both vector - based ( e.g. , the _ elastic net _",
    "@xcite ) and matrix - based regularizations @xcite .",
    "it would be an interesting research direction to combine sparse regularization ( the @xmath29-norm ) to existing tensor norms .",
    "there is also a recent research direction to develop new composite norms such the @xmath201-trace norm @xcite .",
    "development of composite tensor norms can be useful for inductive tensor learning to obtain sparse and low - rank solutions .",
    "kw acknowledges the monbukagakusho mext scholarship and kakenhi 23120004 , rt acknowledges xxxxxx , and ms acknowledges the jst crest program .",
    "in this appendix , we derive the dual formulation of the latent trace norms .",
    "let us consider a training data set @xmath14 , where @xmath15 . to derive the dual for the latent trace norms , we rewrite the primal for the regression of as @xmath202 @xmath203 its lagrangian",
    "can be written by introducing variables @xmath204 as @xmath205 let us introduce auxiliary variables @xmath206 to remove the coupling between the indicator functions .",
    "then the above dual solutions can be restated as @xmath207 @xmath208 @xmath209 similarly , we can derive the dual formulation for logistic regression .",
    "in this appendix , we prove the theoretical results in section  4 .    _ proof of lemma 1 _ : by using the same approach as the one given in @xcite , we rewrite @xmath210 + [ \\hat{r}(\\mathcal{\\hat{w } } ) - \\hat{r}(\\mathcal{w}^{0 } ) ]    + [ \\hat{r}(\\mathcal{w}^{0 } ) - r(\\mathcal{w}^{0})].\\end{gathered}\\ ] ] the second term is always negative and based on hoeffding s inequality , with probability @xmath211 , the third term can be bounded as @xmath212 : @xmath213 further applying mcdiarmid s inequality , with probability at least @xmath138 , we get the following following rademacher complexity : @xmath214 where @xmath215 are rademacher variables which leads to @xmath216@xmath217    _ proof of theorem 1 _ : first we bound the data - dependent component of @xmath218 . for this",
    ", we use the following duality relationship borrowed from @xcite : @xmath219 since we can take any @xmath220 to equal @xmath221 , the above norm can be upper bounded as @xmath222 furthermore , the expectation of the minimum of @xmath3 can be upper - bounded by the minimum of the expectation : @xmath223 let @xmath224 be fixed rademacher variables . since each @xmath117 contains elements following the standard gaussian distribution , it makes each element in @xmath221 a sample from @xmath225 .",
    "based on the standard methods used in @xcite , we can express @xmath226 as @xmath227 using gordan s theorem as in @xcite , we have @xmath228 next taking the expectation over @xmath229 , we have @xmath230 combining and with results in @xmath231 finally , the excess loss can be written as @xmath232 @xmath217    _ proof of theorem 2 _ : to bound the data - dependent component , we use the duality result given in @xcite : @xmath233 since @xmath221 consists of elements following the standard gaussian distribution , for each mode @xmath3 unfolding , we can write a tail bound @xcite as @xmath234 using a union bound , we have @xmath235 and this results in @xmath236 where @xmath151 is a constant .",
    "similarly to , taking the expectation over @xmath237 , we arrive at @xmath238 where @xmath151 is constant .",
    "finally , the excess risk is given as @xmath239 @xmath217    _ proof of theorem 3 _ : from @xcite , we have @xmath240 using a similar approach to the latent trace norm with the additional scaling of @xmath241 , we arrive at the following excess bound for the scaled latent trace norm : @xmath242                    ganesh , a. , lin , z. , wright , j. , wu , l. , chen , m. , and ma , y. ( 2009 ) .",
    "fast convex optimization algorithms for exact recovery of a corrupted low - rank matrix . in _ proceedings of 3rd ieee international workshop on computational advances in multi - sensor adaptive processing _ ,",
    "pages 213216 .",
    "karatzoglou , a. , amatriain , x. , baltrunas , l. , and oliver , n. ( 2010 ) .",
    "multiverse recommendation : n - dimensional tensor factorization for context - aware collaborative filtering . in _ proceedings of the fourth acm conference on recommender systems _ ,",
    "pages 7986 .",
    "kim , t .- k . , wong , s .- f . , and cipolla , r. ( 2007 ) .",
    "tensor canonical correlation analysis for action classification . in _ proceedings of the ieee conference on computer vision and pattern recognition _ , pages 18 .",
    "liu , j. , musialski , p. , wonka , p. , and ye , j. ( 2009 ) .",
    "tensor completion for estimating missing values in visual data . in _ proceedings of the ieee international conference on computer vision _ ,",
    "pages 21142121 .",
    "sankaranarayanan , p. , schomay , t.  e. , aiello , k.  a. , and alter , o. ( 2015 ) .",
    "tensor gsvd of patient- and platform - matched tumor and normal dna copy - number profiles uncovers chromosome arm - wide patterns of tumor - exclusive platform - consistent alterations encoding for cell transformation and predicting ovarian cancer survival .",
    ", 10(4):e0121396 .",
    "wimalawarne , k. , sugiyama , m. , and tomioka , r. ( 2014 ) .",
    "multitask learning meets tensor factorization : task imputation via convex optimization . in _ advances in neural information processing systems 27",
    "_ , pages 28252833 ."
  ],
  "abstract_text": [
    "<S> we theoretically and experimentally investigate tensor - based regression and classification . </S>",
    "<S> our focus is regularization with various tensor norms , including the _ overlapped trace norm _ , the _ latent trace norm _ , and the _ scaled latent trace norm_. we first give dual optimization methods using the _ alternating direction method of multipliers _ </S>",
    "<S> , which is computationally efficient when the number of training samples is moderate . </S>",
    "<S> we then theoretically derive an excess risk bound for each tensor norm and clarify their behavior . </S>",
    "<S> finally , we perform extensive experiments using simulated and real data and demonstrate the superiority of tensor - based learning methods over vector- and matrix - based learning methods . </S>"
  ]
}