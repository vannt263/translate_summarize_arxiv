{
  "article_text": [
    "the primary purpose of this paper is to demonstrate the result stated in the  title :    [ thm : main ] the maximum likelihood estimators of gaussian mixture models are transcendental functions .",
    "more precisely , there exist rational samples @xmath0 in @xmath1 whose maximum likelihood parameters for the mixture of two @xmath2-dimensional gaussians are not algebraic numbers over @xmath3 .",
    "the principle of maximum likelihood ( ml ) is central to statistical inference .",
    "most implementations of ml estimation employ iterative hill - climbing methods , such as expectation maximization ( em ) .",
    "these can rarely certify that a globally optimal solution has been reached .",
    "an alternative paradigm , advanced by algebraic statistics  @xcite , is to find the ml estimator ( mle ) by solving the likelihood equations .",
    "this is only feasible for small models , but it has the benefit of being exact and certifiable . an important notion in this approach is the _ ml degree _ , which is defined as the algebraic degree of the mle as a function of the data .",
    "this rests on the premise that the likelihood equations are given by polynomials .",
    "many models used in practice , such as exponential families for discrete or gaussian observations , can be represented by polynomials .",
    "hence , they have an ml degree that serves as an upper bound for the number of isolated local maxima of the likelihood function , independently of the sample size and the data .",
    "the ml degree is an intrinsic invariant of a statistical model , with interesting geometric and topological properties @xcite .",
    "the notion has proven useful for characterization of when mles admit a ` closed form ' @xcite . when the ml degree is moderate , these exact tools are guaranteed to find the optimal solution to the ml problem @xcite .",
    "however , the ml degree of a statistical model is only defined when the mle is an algebraic function of the data .",
    "theorem [ thm : main ] means that there is no ml degree for gaussian mixtures .",
    "it also highlights a fundamental difference between likelihood inference and the method of moments @xcite .",
    "the latter is a computational paradigm within algebraic geometry , that is , it is based on solving polynomial equations .",
    "ml estimation being transcendental means that likelihood inference in gaussian mixtures is outside the scope of algebraic geometry .    the proof of theorem [ thm : main ] will appear in section  [ sec2 ] . in section [ sec3 ] we shed further light on the transcendental nature of gaussian mixture models .",
    "we focus on mixtures of two univariate gaussians , the model given in ( [ eq : pdf - uni ] ) below , and we present a family of data points on the real line such that the number of critical points of the corresponding log - likelihood function ( [ eq : loglik1 ] ) exceeds any bound .",
    "while the mle for gaussian mixtures is transcendental , this does not mean that exact methods are not available .",
    "quite to the contrary .",
    "work of yap and his collaborators in computational geometry @xcite convincingly demonstrates this . using root bounds from transcendental number theory ,",
    "they provide certified answers to geometric optimization problems whose solutions are known to be transcendental .",
    "theorem [ thm : main ] opens up the possibility of transferring these techniques to statistical inference . in our view , gaussian mixtures are an excellent domain of application for certified computation in numerical analytic geometry .",
    "transcendental number theory @xcite is a field that furnishes tools for deciding whether a given real number @xmath4 is a root of a nonzero polynomial in @xmath5 $ ] .",
    "if this holds then @xmath4 is algebraic ; otherwise @xmath4 is transcendental . for instance",
    ", @xmath6 is algebraic , and so are the parameter estimates computed by pearson in his 1894 study of crab data @xcite . by contrast , the famous constants @xmath7 and @xmath8 are transcendental . our proof will be based on the following classical result .",
    "a textbook reference is ( * ? ? ?",
    "* theorem 1.4 ) :    if @xmath9 are distinct algebraic numbers then @xmath10 are linearly independent over the algebraic numbers .    for now , consider the case of @xmath11 , that is , mixtures of two univariate gaussians .",
    "we allow mixtures with arbitrary means and variances .",
    "our model then consists of all probability distributions on the real line @xmath12 with density @xmath13 \\enspace .\\ ] ] it has five unknown parameters , namely , the means @xmath14 , the standard deviations @xmath15 , and the mixture weight @xmath16 $ ] . the aim is to estimate the five model parameters from a collection of data points @xmath17 .",
    "the _ log - likelihood function _ of the model ( [ eq : pdf - uni ] ) is @xmath18 this is a function of the five parameters , while @xmath19 are fixed constants .",
    "the principle of maximum likelihood suggests to find estimates by maximizing the function @xmath20 over the five - dimensional parameter space @xmath21 \\times { \\mathbb{r}}^2 \\times { \\mathbb{r}}^2_{>0}$ ] .",
    "[ rmk : notbounded ] the log - likelihood function @xmath20 in  ( [ eq : loglik1 ] ) is never bounded above . to see this , we argue as in  (",
    "* section  9.2.1 ) .",
    "set @xmath22 , fix arbitrary values @xmath23 $ ] , @xmath24 and @xmath25 , and match the first mean to the first data point @xmath26 .",
    "the remaining function of one unknown @xmath27 equals @xmath28   \\,+\\ , \\text{const } \\enspace .\\ ] ] the lower bound tends to @xmath29 as @xmath30 .",
    "remark [ rmk : notbounded ] means that there is no global solution to the mle problem .",
    "this is remedied by restricting to a subset of the parameter space @xmath31 . in practice ,",
    "maximum likelihood for gaussian mixtures means computing local maxima of the function  @xmath20 .",
    "these are found numerically by a hill climbing method , such as the em algorithm , with particular choices of starting values .",
    "see section  [ sec3 ] .",
    "this method is implemented , for instance , in the r package mclust @xcite . in order for theorem [ thm : main ] to cover such local maxima , we prove the following statement :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ there exist samples @xmath32 such that every non - trivial critical point @xmath33 of the log - likelihood function @xmath20 in the domain @xmath31 has at least one transcendental coordinate . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    here , a critical point is _ non - trivial _ if it yields an honest mixture , i.e.  a distribution that is not gaussian . by the identifiability results of @xcite , this happens if and only if the estimate @xmath33 satisfies @xmath34 and @xmath35 .",
    "the log - likelihood function always has some algebraic critical points , for any @xmath36 .",
    "indeed , if we define the empirical mean and variance as @xmath37 then any point @xmath33 with @xmath38 and @xmath39 is critical .",
    "this gives a gaussian distribution with mean @xmath40 and variance @xmath41 , so it is trivial .",
    "first , we treat the univariate case .",
    "consider the partial derivative of ( [ eq : loglik1 ] ) with respect to the mixture weight @xmath42 :    @xmath43 .",
    "\\end{aligned}\\ ] ]    clearing the common denominator @xmath44 we see that @xmath45 if and only if @xmath46\\\\",
    "\\label{eq : partial - alpha - cleared }      & \\times\\prod_{j\\not = i }",
    "\\biggl [ \\frac{\\alpha}{\\sigma_1 } \\,\\exp        \\bigl(-\\frac{(x_j-\\mu_1)^2 } { 2\\sigma_1 ^ 2 } \\bigr )   \\,+\\ ,   \\frac{1-\\alpha}{\\sigma_2 } \\,\\exp \\bigl(-\\frac{(x_j-\\mu_2)^2        } { 2\\sigma_2 ^ 2 } \\bigr )     \\biggr]\\;=\\ ; 0 \\enspace .",
    "\\end{aligned}\\ ] ] letting @xmath47 and @xmath48 , we may rewrite the left - hand side of  ( [ eq : partial - alpha - cleared ] ) as    @xmath49                     \\prod_{j\\not = i } \\left [ \\sum_{k_j=1}^2                     \\frac{\\alpha_{k_j}}{\\sigma_{k_j } } \\,\\exp         \\bigl(-\\frac{(x_j-\\mu_{k_j})^2 } { 2\\sigma_{k_j}^2 } \\bigr )   \\right ] .     \\end{aligned}\\ ] ]    we expand the products , collect terms , and set @xmath50 . with this",
    ", the partial derivative @xmath51 is zero if and only if the following vanishes :    @xmath52\\\\       & \\ ! \\ ! = \\!\\ !",
    "\\sum_{k\\in\\{1,2\\}^n } \\!\\!\\!\\ !      \\exp \\",
    "! \\left ( \\ ! -\\sum_{j=1}^n \\frac{(x_j-\\mu_{k_j})^2        } { 2\\sigma_{k_j}^2 } \\right ) \\!\\ ! \\left(\\prod_{j=1}^n      \\frac{1}{\\sigma_{k_j } }   \\right )        \\alpha^{n_1(k)-1}(1-\\alpha)^{n - n_1(k)-1}(n_1(k)-n\\alpha ) \\enspace .",
    "\\,\\,\\ ,    \\end{aligned}\\ ] ]    let @xmath53 be a non - trivial isolated critical point of the likelihood function .",
    "this means that @xmath34 and @xmath35 .",
    "this point depends continuously on the choice of the data @xmath0 . by moving the vector with these coordinates along a general line in @xmath54 , the mixture parameter @xmath55 moves continuously in the critical equation @xmath56 above . by the implicit function theorem",
    ", it takes on all values in some open interval of @xmath12 , and we can thus choose our data points @xmath57 general enough so that @xmath58 is not an integer multiple of @xmath59 . we can further ensure that the last sum above is a @xmath60-linear combination of exponentials with nonzero coefficients .",
    "suppose that @xmath61 is algebraic .",
    "the lindemann - weierstrass theorem implies that the arguments of @xmath62 are all the same .",
    "then the @xmath63 numbers @xmath64 are all identical . however , for @xmath65 , and for general choice of data @xmath66 as above , this can only happen if @xmath67 .",
    "this contradicts our hypothesis that the critical point is non - trivial .",
    "we conclude that all non - trivial critical points of the log - likelihood function ( [ eq : loglik1 ] ) are transcendental .    in the multivariate case ,",
    "the model parameters comprise the mixture weight @xmath16 $ ] , mean vectors @xmath68 and positive definite covariance matrices @xmath69 . arguing as above , if a non - trivial critical @xmath70 is algebraic , then the lindemann - weierstrass theorem implies that the numbers @xmath71 are all identical . for @xmath72",
    "sufficiently large and a general choice of @xmath66 in @xmath73 , the @xmath63 numbers are identical only if @xmath74 .",
    "again , this constitutes a contradiction to the hypothesis that @xmath70 is non - trivial .",
    "many variations and specializations of the gaussian mixture model are used in applications . in the case",
    "@xmath11 , the variances are sometimes assumed equal , so @xmath75 for the above two - mixture .",
    "this avoids the issue of an unbounded likelihood function ( as long as @xmath76 ) .",
    "our proof of theorem  [ thm : main ] applies to this setting . in higher dimensions",
    "@xmath77 ) , the covariance matrices are sometimes assumed arbitrary and distinct , sometimes arbitrary and equal , but often also have special structure such as being diagonal .",
    "various default choices are discussed in the paper @xcite that introduces the r package mclust .",
    "our results imply that maximum likelihood estimation is transcendental for all these mclust models .",
    "we illustrate theorem  [ thm : main ] for a specialization of ( [ eq : pdf - uni ] ) obtained by fixing three parameters : @xmath78 and @xmath79 .",
    "the remaining two free parameters are @xmath42 and @xmath80 .",
    "we take only @xmath22 data points , namely @xmath81 and @xmath82 . omitting an additive constant ,",
    "our log - likelihood function equals @xmath83    for a concrete example take @xmath84 .",
    "the graph of ( [ eq : simplest - loglik-0x ] ) for this choice is shown in figure  [ fig : simple ] . by maximizing @xmath85 numerically",
    ", we find the parameter estimates @xmath86 our technique can be applied to prove that @xmath55 and @xmath87 are transcendental over @xmath3 .",
    "we illustrate it for @xmath87 .    for any @xmath88 ,",
    "the function @xmath85 is bounded from above and achieves its maximum on @xmath89\\times \\mathbb{r}$ ] . if @xmath90 is large , then any global maximum @xmath91 of @xmath20 is in the interior of @xmath89\\times \\mathbb{r}$ ] and satisfies @xmath92 .",
    "according to a mathematica computation , the choice @xmath93 suffices for this .",
    "assume that this holds .",
    "setting the two partial derivatives equal to zero and eliminating the unknown @xmath42 in a further mathematica computation , the critical equation for @xmath94 is found to be @xmath95 suppose for contradiction that both @xmath96 and @xmath97 are algebraic numbers over @xmath3 . since @xmath92 , we have @xmath98 .",
    "hence @xmath99 , @xmath100 and @xmath101 are distinct algebraic numbers .",
    "the lindemann - weierstrass theorem implies that @xmath102 and @xmath103 are linearly independent over the field of algebraic numbers .",
    "however , from ( [ eq : transeqn ] ) we know that @xmath104 this is a contradiction .",
    "we conclude that the number @xmath97 is transcendental over  @xmath3 .     and @xmath105 .",
    "theorem [ thm : main ] shows that gaussian mixtures do not admit an ml degree .",
    "this raises the question of how to find any bound for the number of critical points .",
    "does there exist a universal bound on the number of non - trivial critical points for the log - likelihood function of the mixture of two univariate gaussians ? or , can we find a sequence of samples on the real line such that the number of non - trivial critical points increases beyond any bound ?    we shall resolve this problem by answering the second question affirmatively . the idea behind",
    "our solution is to choose a sample consisting of many well - separated clusters of size @xmath106 .",
    "then each cluster gives rise to a distinct non - trivial critical point @xmath107 of the log - likelihood function @xmath20 from ( [ eq : loglik1 ] ) .",
    "we propose one particular choice of data , but many others would work too .",
    "[ thm : manyhills ] fix sample size @xmath108 for @xmath109 , and take the ordered sample @xmath110 .",
    "then , for each @xmath111 , the log - likelihood function @xmath20 from ( [ eq : loglik1 ] ) has a non - trivial critical point with @xmath112 .",
    "hence , there are at least @xmath113 non - trivial critical points .    before turning to the proof ,",
    "we offer a numerical illustration .",
    ".[tab : k7 ] seven critical points of the log - likelihood function in theorem  [ thm : manyhills ] with @xmath114 . [ cols=\"<,<,<,<,<,<,<\",options=\"header \" , ]     for @xmath115 , we have @xmath116 data points in the interval @xmath117 $ ] . running the em algorithm ( as explained in the proof of theorem  [ thm : manyhills ] below ) yields the non - trivial critical points reported in table  [ tab : k7 ] .",
    "their @xmath118 coordinates are seen to be close to the cluster midpoints @xmath119 for all @xmath120 .",
    "the observed symmetry under reversing the order of the rows also holds for all larger @xmath113 .",
    "our proof of theorem [ thm : manyhills ] will be based on the _ em algorithm_. we first recall this algorithm .",
    "let @xmath121 be the mixture density from  ( [ eq : pdf - uni ] ) , and let @xmath122 be the two gaussian component densities .",
    "define @xmath123 which can be interpreted as the conditional probability that data point @xmath57 belongs to the first mixture component .",
    "further , define @xmath124 and @xmath125 , which are expected cluster sizes . following ( * ? ? ?",
    "* section  9.2.2 ) , the likelihood equations for our model can be written in the following fixed - point form : @xmath126 in the present context , the em algorithm amounts to solving these equations iteratively .",
    "more precisely , consider any starting point @xmath127 .",
    "then the e - step ( `` expectation '' ) computes the estimated frequencies @xmath128 via ( [ probs ] ) . in the subsequent m - step ( `` maximization '' ) , one obtains a new parameter vector @xmath127 by evaluating the right - hand sides of the equations ( [ firstlik])-([lastlik ] ) .",
    "the two steps are repeated until a fixed point is reached , up to the desired numerical accuracy .",
    "the updates never decrease the log - likelihood . for our problem",
    "it can be shown that the algorithm will converge to a critical point ; see e.g.  @xcite .",
    "fix @xmath129 .",
    "we choose starting parameter values to suggest that the pair @xmath130 belongs to the first mixture component , while the rest of the sample belongs to the second .",
    "explicitly , we set @xmath131 we shall argue that , when running the em algorithm , the parameters will always stay close to these starting values .",
    "specifically , we claim that throughout all em iterations , the parameter values satisfy the inequalities @xmath132 @xmath133 the starting values proposed above obviously satisfy the inequalities in  ( [ eq : box1 ] ) , and it is not difficult to check that  ( [ eq : box2 ] ) and  ( [ eq : box3 ] ) are satisfied as well .",
    "to prove the theorem , it remains to show that  ( [ eq : box1])-([eq : box3 ] ) continue to hold after an em update .    in the remainder , we assume that @xmath134",
    ". for smaller values of @xmath113 the claim of the theorem can be checked by running the em algorithm . in particular , for @xmath135 , the second standard deviation satisfies the simpler bounds @xmath136",
    "a key property is that the quantity @xmath128 , computed in the e - step , is always very close to zero for @xmath137 . to see why , rewrite  ( [ probs ] ) as @xmath138 since @xmath139",
    ", we have @xmath140 . on the other hand , @xmath141 . using that @xmath134",
    ", their product is thus bounded below by 0.3209 .",
    "turning to the exponential term , the second inequality in  ( [ eq : box1 ] ) implies that @xmath142 for @xmath143 or @xmath144 , which index the data points closest to the @xmath120th pair .",
    "using  ( [ eq : box4 ] ) , we obtain @xmath145 from @xmath146 , we deduce that @xmath147 .",
    "the exponential term becomes only smaller as the considered data point @xmath57 move away from the @xmath120th pair . as @xmath148 increases , @xmath128 decreases and can be bounded above by a geometric progression starting at @xmath149 and with ratio @xmath150 .",
    "this makes @xmath128 with @xmath151 negligible .",
    "indeed , from the limit of geometric series , we have @xmath152 and similarly , @xmath153 satisfies @xmath154 the two sums @xmath155 and @xmath156 are relevant for the m - step .    the probabilities @xmath157 and @xmath158 give the main contribution to the averages that are evaluated in the m - step .",
    "they satisfy @xmath159 .",
    "moreover , we may show that the values of @xmath157 and @xmath158 are similar , namely : @xmath160 which we prove by writing @xmath161 and using @xmath134 to bound @xmath162    bringing it all together , we have @xmath163 using @xmath164 and ( [ series ] ) , as well as the lower bound in ( [ eq : bound : gamma : ratio ] ) , we find @xmath165 using the upper bound in ( [ eq : bound : gamma : ratio ] ) , we also have @xmath166 .",
    "hence , the second inequality in  ( [ eq : box1 ] ) holds .",
    "the inequalities for the other parameters are verified similarly .",
    "for instance , @xmath167 holds for @xmath168 .",
    "therefore , the first inequality in  ( [ eq : box1 ] ) continues to be true .",
    "we conclude that running the em algorithm from the chosen starting values yields a sequence of parameter vectors that satisfy the inequalities  ( [ eq : box1])-([eq : box4 ] ) .",
    "the sequence has at least one limit point , which must be a non - trivial critical point of the log - likelihood function .",
    "therefore , for every @xmath169 , the log - likelihood function has a non - trivial critical point with @xmath170 .",
    "we showed that the maximum likelihood estimator ( mle ) in gaussian mixture models is not an algebraic function of the data , and that the log - likelihood function may have arbitrarily many critical points .",
    "hence , in contrast to the models studied so far in algebraic statistics @xcite , there is no notion of an ml degree for gaussian mixtures .",
    "however , certified likelihood inference may still be possible , via transcendental root separation bounds , as in @xcite .    the _ cauchy - location model",
    "_ , treated in @xcite , is an example where the ml estimation is algebraic but the ml degree , and also the maximum number of local maxima , depends on the sample size and increases beyond any bound .    the ml estimation problem admits a population / infinite - sample version . here",
    "the maximization of the likelihood function is replaced by _",
    "minimization of the kullback - leibler divergence _ between a given data - generating distribution and the distributions in the model .",
    "the question of whether this population problem is subject to local but not global maxima was raised in @xcite  in the context of gaussian mixtures with known and equal variances .",
    "it is known that the kullback - leibler divergence for such gaussian mixtures is not an analytic function @xcite .",
    "readers of japanese should be able to find details in @xcite .",
    "as previously mentioned , theorem [ thm : main ] shows that likelihood inference is in a fundamental way more complicated than the classical _ method of moments _ @xcite .",
    "the latter involves only the solution of polynomial equation systems .",
    "this was recognized also in the computer science literature on learning gaussian mixtures @xcite , where most of the recent progress is based on variants of the method of moments rather than likelihood inference .",
    "we refer to @xcite for a study of the method of moments from an algebraic perspective .",
    "section 3 in that paper illustrates the behavior of pearson s method for the sample used in theorem  [ thm : manyhills ] .",
    "m.  buot , s.  hoten , and d.  richards , _ counting and locating the solutions of polynomial systems of maximum likelihood equations .",
    "ii . the behrens - fisher problem _ , statistica sinica * 17 * ( 2007 ) , no .  4 , 13431354 .      s.w .",
    "choi , s.  pae , h.  park and c.  yap : _ decidability of collision between a helical motion and an algebraic motion _",
    ", in g.  hanrot and p.  zimmermann ( eds ) : 7th conf .  on real numbers and computers , loria , nancy , france , 2006 , pp",
    ".  6982 ."
  ],
  "abstract_text": [
    "<S> gaussian mixture models are central to classical statistics , widely used in the information sciences , and have a rich mathematical structure . </S>",
    "<S> we examine their maximum likelihood estimates through the lens of algebraic statistics . </S>",
    "<S> the mle is not an algebraic function of the data , so there is no notion of ml degree for these models . </S>",
    "<S> the critical points of the likelihood function are transcendental , and there is no bound on their number , even for mixtures of two univariate gaussians . </S>"
  ]
}