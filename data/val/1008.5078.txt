{
  "article_text": [
    "data compression @xcite is an area in the field of information theory that studies ways of obtaining efficient representations of information . given some general data , for instance ,",
    "text , picture , movie , represented in some form of a binary computer file the aim in data compression is to obtain a new shorter binary description of this data .",
    "this is done by assuming or searching for a structure in the data that permits removing unimportant repetitions or redundancy that exists in the original representation of the data .",
    "the development of data compression algorithm can be divided into two phases . the first phase is referred to as modelling . in this phase",
    "we try to remove any redundancy that exists in the data and describe it in the form of a model .",
    "the second phase is coding . in this phase a description of the model and a description of how the data differs from the model are encoded . producing this description",
    "is based on a prediction step .",
    "for instance , in lossless image compression the calic algorithm decides what will be the value of the next pixel in a picture , then encodes the difference between this predicted value and true value using huffman or arithmetic coding .",
    "another example is in text compression : the ppm algorithm computes the probability distribution of the next symbol and uses arithmetic coding to encode the value .    in this paper",
    "we pose the following question : can a compressor be used for prediction ? that is , given some compressor can we use it as a black - box ( i.e. , not changing its internal workings ) to predict the next symbol in a data sequence of symbols ?",
    "the motivation behind this question is mainly theoretical since practically one can design a predictor by tackling the problem directly without the need for data compression .",
    "for instance , by modeling the source that produces the data and doing statistical estimation to learn the model s parameters and then use the estimator to predict the next symbol given the observed part of the sequence .",
    "in contrast to other works @xcite that consider compression as a means of doing inference by expanding on rissanen s minimum description length principle we take a direct approach that uses a ready - made compression algorithm as stage in our inference .",
    "theoretically , the question that we pose is interesting since prediction by compression , if it is possible , would further strengthen a basic principle of pattern recognition and statistical learning theory . the principle is known as occam s razor ( see @xcite ) : it says that given several possible explanations for an observed data one should choose the simplest explanation that is consistent with the data .",
    "as far as prediction is concerned we may interpret this principle as follows : let @xmath0 be a finite alphabet consisting of @xmath1 symbols and denote by @xmath2 the space of all finite sequences of symbols .",
    "given a sequence @xmath3 representing some observed data @xmath4 , @xmath5 , @xmath6 , then according to this principle one should predict the next symbol @xmath7 as one which requires the shortest additional description given @xmath8 . the word shortest is crucial since it is the basis of occam s principle . but how should one obtain the shortest description of a data sequence ?",
    "an obvious approach is to use data compression .",
    "let us denote by @xmath9 $ ] the concatenation of a sequence @xmath8 of symbols from @xmath10 with some symbol @xmath11 .",
    "denote by @xmath12 and let @xmath13 denote the space of all finite binary encoding sequences .",
    "suppose we have a data compression box represented as a mapping @xmath14 from the space of symbol sequences to their unique binary encodings .",
    "consider several candidate sequences @xmath15 $ ] , @xmath16 $ ] , @xmath17 , @xmath18 $ ] , representing all the possible continuations of the data sequence @xmath8 .",
    "compress each one and measure the length @xmath19))$ ] , @xmath20 , of their corresponding binary encodings .",
    "then the idea is to predict the next symbol to be @xmath21 where @xmath22))$ ] .",
    "this is our theoretical interpretation of occam s principle for the prediction paradigm .",
    "the current paper aims to check if this interpretation works in reality .",
    "our setup consists of a source capable of producing random binary sequences .",
    "the source is based on a markov chain with states @xmath23 which we represent as the set of binary vectors @xmath24 , respectively , using @xmath25 bits .",
    "this number of bits is referred to as the _ order _ of the source which we denote by @xmath26 .",
    "we alternatively use the decimal or binary representations of states . given that the source is at a state @xmath27 then it can change into one of only two states @xmath28 or @xmath29 .",
    "we denote the transitions into these states as the @xmath30-transition and @xmath31-transition , respectively . associated with state",
    "@xmath32 there is a conditional probability distribution @xmath33 $ ] which gives the probability of making a @xmath30-transition or a @xmath31-transition .",
    "the source produces a finite binary sequence @xmath8 of length @xmath34 .",
    "it will be convenient to refer to the @xmath35 bit of @xmath8 as the bit value @xmath36 at discrete time @xmath32 .",
    "the prediction problem that we consider is incremental .",
    "we start by having the first @xmath37 bits of @xmath8 as observables and predict the @xmath38 bit . after seeing the true value @xmath39 , we add it to the observables and predict the @xmath40 bit .",
    "this iterative process is repeated until we finish predicting the @xmath41 bit .",
    "let us denote by @xmath42 the binary value decided by the predictor at time @xmath32 , where @xmath43 . in predicting the value of a bit @xmath36",
    ", the predictor makes an error if its decision @xmath42 at time @xmath32 differs from @xmath36 . as a measure of goodness of the predictor",
    ", we estimate the probability that it makes an error in the future as the number of errors that it makes on the @xmath44 bit predictions divided by @xmath44 .",
    "we denote this estimate as @xmath45 .    as discussed in section [",
    "sec : introduction ] we are considering a predictor that bases its decision on a compressor . for compression",
    "we use the ppmd algorithm @xcite .",
    "the ppmd algorithm is based on a predictive model that estimates the probability of the next symbol and encodes it by arithmetic coding .",
    "the specific implementation that we use ( called 7zip ) permits to set the maximal context size @xmath46 of the ppm algorithm up to a value of @xmath47 with a default value of @xmath48 ( this is the number of symbols that it uses as the given observables to estimate the probability of the next yet unseen symbol )",
    ". we will use @xmath46 as one of the parameters that controls the predictor .",
    "another parameter is the history size @xmath49 .",
    "we define a window of length @xmath49 bits . at prediction time",
    "@xmath32 , @xmath43 , instead of using all previous @xmath50 bits as observables we let the predictor see only the last @xmath49 bits of @xmath8 at time instants @xmath51 , @xmath52 , @xmath17 , @xmath50 .",
    "we denote this by @xmath53 and refer to this subsequence as the history for prediction time @xmath32 .",
    "in our tests , we consider different values for the parameters @xmath46 and @xmath49 .",
    "let @xmath54 be the binary string obtained from compressing the binary sequence @xmath8 and @xmath55 the length of @xmath8 .",
    "write @xmath56 for @xmath57 .",
    "we chose a source with @xmath58 and conditional probability distribution @xmath59=\\left[0.3,0.7\\right]$ ] for each of the four states @xmath60 . here",
    "the bayes optimal decision is @xmath30 at every state and the bayes error is @xmath61 . for the compressor we used ppmd without controlling the @xmath46 parameter",
    "( the maximal possible context size is left to be the default size of @xmath48 ) .",
    "our first attempt at a compression - based predictor was to try to predict not just the next single bit of @xmath8 but a whole future subsequence @xmath7 ( of length @xmath62 ) of @xmath8 .",
    "with each prediction time @xmath32 we associated two counters @xmath63 which keep the number of times that we covered bit @xmath32 and the number of times we predicted the value @xmath31 for this time instant @xmath32 , respectively .",
    "we divided the prediction process into two phases . in the first phase initialize all counts to zero then at each prediction time we considered @xmath64 prediction candidates and chose the one with the best score value ( score is specified below ) . once a subsequence @xmath65 was decided at time @xmath32 we incremented by one the counts @xmath66 of all covered time instants @xmath67 and incremented by one the counts @xmath68 corresponding to those time instants in the range @xmath67 whose value @xmath69 .",
    "we left the counts of the remaining time instants unchanged .",
    "this was repeated for all prediction times @xmath70 .",
    "then the second phase started .",
    "for all time @xmath43 , decide the value @xmath71 if @xmath72 else decide @xmath73 .",
    "we let the candidate length @xmath74 in bits be a parameter and together with the parameter @xmath49 ( describe above ) we repeated the prediction over many runs where for each run we set a different parameter value @xmath75 , @xmath76 , in increments of @xmath77 and @xmath78 .",
    "we used the following function to score a prediction candidate subsequence @xmath7 given the history @xmath53 for all times @xmath32 : @xmath79)\\label{eq : s1}\\ ] ] where @xmath80 $ ] denotes the concatenation of the strings @xmath81 and @xmath7 .",
    "this score represents the description length necessary for describing @xmath7 and the observed @xmath81 .",
    "with occam s principle the intuition says that the lower @xmath82 the better the candidate @xmath7 .",
    "however this scheme led to unsatisfactory results as the prediction error was significantly higher than bayes error for all runs .",
    "another score function that we tried is    @xmath83)-\\lambda(y)\\right).\\label{eq : s2}\\ ] ]    we may regard @xmath84)-\\lambda(y)\\equiv\\lambda(x_{h}|y)$ ] as the extra description length needed for describing @xmath81 given the knowledge of @xmath7 ( extra meaning beyond what is needed to describe just @xmath7 ) .",
    "note @xmath85 resembles kolmogorov s combinatorial notion of conditional entropy @xcite ( see also @xcite ) .",
    "@xmath86 resembles kolmogorov s combinatorial information measure .",
    "indeed @xmath86 measures the information that @xmath7 provides about @xmath81since it shows the reduction in description length of @xmath81 once @xmath7 is used as an input for describing @xmath81 .",
    "however still , this did not improve the previous results .    at this point we decided to change the source .",
    "we increased the number of states to @xmath87 still with only two possible transitions outgoing from each state with probability @xmath61 and @xmath88 .",
    "thus the bayes error is @xmath61 .",
    "this made the source more complex , however , unlike before , here we used @xmath87 bit combinations chosen at random ( and not @xmath89 bits ) to represent each of the states and the sequence @xmath8 was now generated based on the state combinations in a non - overlapping manner . this introduced redundancy in the sequence representation .",
    "for instance , suppose the state combinations are the strings @xmath90 , @xmath91 , @xmath17 , @xmath92 and suppose that the transitions are obtained by shifting in a circular manner the @xmath31 bit to the left or right by one position .",
    "then an example of a sequence @xmath8 is : @xmath93@xmath94@xmath93@xmath95 .",
    "such a scheme led to some improvement but the minimal error obtained was still above @xmath96 hence larger than bayes error .      at this point",
    "we had some more insight , mainly , that adding redundancy to the source somewhat improves prediction by compression .",
    "but in general one can not change the source of the data .",
    "so we chose to leave the source as generating the data sequence @xmath8 based on single bit transitions , i.e. , @xmath30-transition and @xmath31-transition , as described in the first paragraph on section [ sub : initial - investigation ] .",
    "the idea at this stage was to extract the information from @xmath8 and form a new sequence based on @xmath97-bit words of @xmath8 where the value of @xmath97 is fixed in advance in the range @xmath98 .",
    "we call this process _",
    "peeling_. let us describe it through an example : suppose @xmath99 and suppose @xmath100 then the extracted words are ( in sequence ) : @xmath101 , @xmath102 , @xmath101 , @xmath103 .",
    "we then complete each such _ @xmath97-codeword _ into an @xmath87-bit word ( a byte ) by inserting @xmath104 zeros in the front .",
    "for this example it yields @xmath105 , @xmath106 , @xmath105 , @xmath107 .",
    "we then create a random injective map from the set @xmath108 to @xmath109 characters whose utf-8 representation fall in the range u+@xmath110 to @xmath111 ( we write @xmath112 to denote a string of @xmath74 zeros ) .",
    "note that for this range of utf-8 each character is represented by a single byte .",
    "this is important since when we compare candidates for prediction we base it on the compressed length of the representation ( in this the representation is utf-8 encoded characters ) .",
    "having chosen to limit to single - byte utf-8 representation ensures that any two candidate sequences made up of the same number of characters will be encoded with the same length encoding sequences .",
    "we map @xmath8 into a sequence of characters , for instance , the above sequence could be mapped into @xmath113 .",
    "each symbol represents a state ( we call them _ perceived states _ since they are from the perspective of the predictor ) .",
    "the number of such states is @xmath114 and it need not necessarily equal the number @xmath115 of source s states since @xmath97 may differ from @xmath26 ( the order of the source ) .",
    "we then convert this character sequence into a sequence @xmath116 specifying the pairwise transitions ( except the last entry which is half full ) and repeating each transition @xmath117 times ( we term it the _ repeat factor _ ) .",
    "we denote this as _ transition - encoding _ scheme . for this example , suppose @xmath118 then it takes the following form    @xmath119    where the symbol @xmath120 has a utf-8 code @xmath121 .",
    "note , the last transition ( the trailer ) is half full , in this example it is @xmath122 , and consists of the state ( in this case @xmath123 ) on the right of the last transition ( @xmath124 ) .",
    "thus @xmath125 is a sequence of bytes that encode perceived - state characters , a comma or the arrow symbol .",
    "clearly , @xmath125 has more redundancy than @xmath8 since its length is @xmath126 times longer than @xmath8 yet it contains the same amount of information .",
    "obviously if the source s order @xmath26 is known to the predictor then a good choice is to let @xmath127 .",
    "if it is not known , this method can still be used by scanning over all values @xmath98 while testing the probability of error ( on some training sequence @xmath8 ) . this way a value for @xmath97 will be selected when",
    "the minimal probability of error is obtained over this range .",
    "can the score function defined in ( [ eq : s2 ] ) be used here ?",
    "instead of @xmath81 we have @xmath128 so we need to represent prediction candidates @xmath7 in the utf-8 symbol sequence format , denoted as @xmath129 , of the from @xmath130 . only after converting @xmath7 into @xmath129 we may attempt to concatenate them to form an utf-8 symbol sequence @xmath131 . in this new representation",
    "we are coding transitions between states rather than absolute states .",
    "prediction candidates @xmath129 should represent a transition starting from the state that corresponds to the symbol on the right of the last pairwise transition of @xmath129 .",
    "for instance in the above example , this last pairwise transition is @xmath124 hence we should only consider the two candidates @xmath129 that code transitions of the from @xmath132 where @xmath133 is a symbol whose @xmath97-bit codeword starts with the first @xmath134 bits of the codeword of @xmath123 and ends either with a @xmath30 or @xmath31 ( denote them as the @xmath135 and @xmath136 candidates ) . but to form the concatenation @xmath137 means that the right symbol of these two candidates must be the left symbol of the first transition of @xmath128 ( in the above example this is the symbol @xmath138 ) .",
    "hence there may be a conflict constraints in determining the two candidates since as in this example the symbol @xmath138 may not be a symbol whose codeword agrees on the first @xmath134 bits with the symbol @xmath123 .",
    "hence the score ( [ eq : s2 ] ) is not suitable .",
    "this led us to consider the following new scoring function ,    @xmath139)-\\lambda(\\chi_{h})\\right)\\label{eq : syx}\\ ] ]    note that we may regard @xmath140)-\\lambda(\\chi_{h})\\equiv\\lambda(\\xi|\\chi_{h})$ ] as the extra description length needed for describing @xmath129 given the knowledge of @xmath128 .",
    "in a similar way as we interpreted @xmath86 above , here @xmath141 is what kolmogorov @xcite calls the information that @xmath128 provides about @xmath129 .",
    "note that to evaluate ( [ eq : syx ] ) we just need to convert @xmath7 into @xmath129 and @xmath81 into @xmath128 ( using the above process ) . unlike the string @xmath137 considered above , now the concatenated string @xmath142 is well - defined since we just look at the utf-8 symbol of the trailer of @xmath128 ( in the above example it is @xmath123 ) and only consider the two candidates @xmath129 that encode transitions of the form @xmath132 where @xmath133 is a symbol whose @xmath97-bit word starts with the last @xmath134 bits of the codeword of @xmath123 and ends either with a @xmath30 or @xmath31 . in the above example , suppose that corresponding to the @xmath30 and @xmath31 candidates the two candidate symbols are @xmath143 and @xmath144 then we will form the candidate sequences@xmath145    note that the candidates sequences depend on the trailer of @xmath128 ( in this example it is the symbol @xmath123 ) . when we form the concatenated sequence @xmath142 we first remove the trailer of @xmath128 and then attach @xmath129 form the right .",
    "hence in the above example we get @xmath146 using this representation scheme we did several runs with a source of order @xmath147 and a predictor with peeling size @xmath148 .",
    "we obtained encouraging results with the prediction error being smaller than in the previous trials but staying around @xmath96 hence not close enough to the bayes error even when the history size @xmath49 is large .",
    "we then observed the second term of the score function ( [ eq : syx ] ) for several sample runs and saw that most of the times both candidates @xmath135 , @xmath136 have the same value for compressed length @xmath149)=\\lambda([\\chi_{h}\\xi_{1}])$ ] and @xmath150 .",
    "this makes them be indistinguishable which leads to random prediction ( since we we predict a @xmath31 with probability @xmath151 when the scores are the same ) .",
    "we realized that this is a result of the compression algorithm imperfectness .",
    "it has some constant minimal file size that it never goes below no matter how short the sequence to be compressed is .",
    "hence the next idea was to consider extending the length of each candidate @xmath129 by repeating it @xmath152 times ( where @xmath117 is the repeat factor for @xmath153 .",
    "for instance , in the above example ( [ eq : xhi ] ) where @xmath118 using @xmath154 , then if @xmath155 its extension is @xmath156 now , the lengths of the compressed version of the two strings @xmath157 , @xmath158 become distinguishable ( in our experiments we used @xmath159 and @xmath160 ) .",
    "doing that led to significant improvement and we can now see the prediction error decreasing towards the bayes error with increasing history size @xmath49 and increasing ppm order @xmath46 .",
    "note that we are still treating the case where the peeling size equals the source order ( which is less - realistic since the predictor may not know the source s order ) .",
    "let us summarize the features of the final version of the predictor :    * _ aim _ : given history @xmath81 at bit time @xmath32 , we want to predict bit @xmath32 , @xmath43 * peel @xmath97-codewords from @xmath81 and map them into utf-8 symbols representing predictor s perceived states * represent the transitions between these states in the form @xmath130 and repeat each @xmath117 times to form the sequence @xmath161 .",
    "* there are two prediction candidates , @xmath135 , @xmath136 corresponding to the two possible binary predictions of the next bit in @xmath81 .",
    "we use the same representation of state transition to represent the two candidates .",
    "we use a repeat factor of @xmath152 to obtain @xmath162 , @xmath163 .",
    "* use the scoring function @xmath141 to score each of the two candidates .",
    "if both have the same value then predict @xmath31 with probability @xmath151 .",
    "* otherwise , predict the next bit @xmath164    the next series of trials were more realistic as we allowed the predictor s peeling size @xmath97 to be different from the source s order .",
    "let us review the results .",
    "in the first test we considered a source of order @xmath147 and peeling size @xmath100 . in this case the number of perceived states @xmath165 is greater than the number of source s states @xmath166 . to see the effect of the compressor s ppm order @xmath46 on the predictor s error @xmath45 we performed several trial runs changing the history size @xmath49 and",
    "ppm order @xmath46 and measuring the prediction error @xmath45 .",
    "figure [ fig:1a ] shows how @xmath45 decreases with respect to @xmath46 .",
    "we plot the error for each value of @xmath46 averaged out over @xmath167 runs of the same history size @xmath49 and where @xmath168 , @xmath169 , @xmath17 , @xmath170 . at this rate of decrease",
    "the prediction error would reach the bayes error of @xmath61 at about @xmath171 .",
    "note that the error starts decreasing from its maximal value only at @xmath172 . at this value",
    "the ppm compression captures the full trailer of @xmath128 .",
    "its context conditional probability distribution for the next character in the sequence contains full information about the perceived state ( in the above example this is the state @xmath123 ) .",
    "this is a necessary condition for ppm to estimate the transition probability correctly .",
    "figures [ fig:1a ] shows that @xmath45 decreases steadily as we increase the ppm order @xmath46 .",
    "figure [ fig:1b ] shows that there is hardly any effect on the error as the history size @xmath49 increases .",
    "what happens when @xmath173 ?",
    "we tested a predictor of peeling size @xmath100 on a sequence produced by a source of order @xmath174 .",
    "figures [ fig:2a ] and [ fig:2a-1 ] show that already from @xmath175 we see an error close to the bayes error .",
    "next , we tested a predictor of peeling size @xmath100 on a sequence produced by a source of order @xmath176 .",
    "figures [ fig:3a ] and [ fig:3b ] show that the predictor is inaccurate since even when the order @xmath46 is high we still see an error which is only slightly better than pure guessing .",
    "next , we tested a predictor of peeling size @xmath177 on a sequence produced by a source of order @xmath176 .",
    "we let the history size reach up to @xmath178 .",
    "figures [ fig:4a ] and [ fig:4b ] show that the predictor s error decreases with increasing ppm order albeit at a slower rate than the case of @xmath100 and @xmath147 ( figures [ fig:1a],[fig:1b ] ) .",
    "this suggests that the error rate not only depends on the difference @xmath179 ( which in both case equals @xmath31 ) but also on @xmath26 itself .    finally , we changed the compressor s algorithm from ppm to lzma which is a recently developed algorithm that incorporates the lempel - ziv dictionary - based compressor with a new predictive component .",
    "we tested the predictor with lzma on the problem of @xmath174 and @xmath100 and instead of ppm order we used the dictionary size as a parameter .",
    "as figure [ fig : source - model - has-1 ] shows , the error initially decreases but remains above the @xmath96 level even as we increase the dictionary size at an exponential rate . on this same problem the ppm predictor performed very well ( figure [ fig:2 ] ) .",
    "we are not sure why the lzma underperforms compared to the ppm approach .",
    "perhaps doing prediction based on dictionary - based compression requires a different transition - encoding scheme .",
    "this requires further investigation .",
    "source model has @xmath180 states ( @xmath174 ) , peeling size is @xmath181 .",
    "the predictor is based on lzma compression , a dictionary - based algorithm .",
    "the prediction error is plotted with respect to @xmath182 , where @xmath183 is size of the dictionary ]",
    "in this paper we raise a new question : can a compression algorithm be used as a black - box to obtain prediction ?",
    "we introduced a new scoring function @xmath141 which measures the amount of information that the history @xmath128 gives about a candidate future prediction @xmath129 .",
    "we showed through a series of empirical results that this scoring function leads to successful prediction by a predictor whose decision for the next bit is obtained by maximizing the score .",
    "prediction - by - compression works and agrees with the intuition that says to choose the candidate future prediction @xmath129 for which the history gives a maximal amount of information ."
  ],
  "abstract_text": [
    "<S> it is well known that text compression can be achieved by predicting the next symbol in the stream of text data based on the history seen up to the current symbol . </S>",
    "<S> the better the prediction the more skewed the conditional probability distribution of the next symbol and the shorter the codeword that needs to be assigned to represent this next symbol . what about the opposite direction ? </S>",
    "<S> suppose we have a black box that can compress text stream . can it be used to predict the next symbol in the stream ? </S>",
    "<S> we introduce a novel criterion based on the length of the compressed data and use it to predict the next symbol . </S>",
    "<S> we examine empirically the prediction error rate and its dependency on some compression parameters . </S>"
  ]
}