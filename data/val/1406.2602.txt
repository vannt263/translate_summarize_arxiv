{
  "article_text": [
    "many unsupervised learning algorithms , such as spectral clustering @xcite , @xcite and low - dimensional embedding via laplacian eigenmaps and diffusion maps @xcite,@xcite , need as input a _ matrix of pairwise similarities _",
    "@xmath0 between the different objects in our data . in some cases",
    ", obtaining the full matrix can be a costly matter .",
    "for example , @xmath1 may be based on some expensive - to - compute metric such as w2d @xcite ; based on some physical measurement ( such as in some computational biology applications ) ; or is given by a human annotator .",
    "in such cases , we would like to have a good approximation of the ( initially unknown ) matrix , while querying only a limited number of entries .",
    "an alternative but equivalent viewpoint is the problem of approximating an unknown weighted undirected graph , by querying a limited number of edges .",
    "this question has received previous attention in works such as @xcite and @xcite , which focus on the task of spectral clustering into two clusters , and assuming two such distinct clusters indeed exist ( i.e. that there is a big gap between the second and third eigenvalues of the laplacian matrix ) . in this work",
    "we consider , both theoretically and algorithmically , the question of query - based graph approximation more generally , obtaining results relevant beyond two clusters and beyond spectral clustering .",
    "when considering graph approximations , the first question is what notion of approximation to consider .",
    "one important notion is _ cut approximation _",
    "@xcite where we wish for every cut in the approximated graph to have weight close to the weight of the cut in the original graph up to a multiplicative factor . many machine learning algorithms ( and many more general algorithms ) such as cut based clustering @xcite , energy minimization @xcite , etc .",
    "@xcite are based on cuts , so this notion of approximation is natural for these uses .",
    "a stronger notion is _ spectral approximation _",
    "@xcite , where we wish to uniformly approximate the quadratic form defined by the laplacian up to a multiplicative factor .",
    "this approximation is important for algorithms such as spectral clustering @xcite , laplacian eigenmaps @xcite , diffusion maps @xcite , etc .",
    "that use the connection between the spectral properties of the laplacian matrix and the graph .",
    "our theoretical analysis focuses on the number of queries needed for such approximations .",
    "we first consider the simple and intuitive strategy of sampling edges uniformly at random , and obtain results for both cut and spectral approximations , under various assumptions .",
    "we note that these results are considerably more general than the theoretical analysis in @xcite , which focuses on the behavior of the 2nd eigenvector of the laplacian matrix , and crucially rely on this large eigengap .",
    "we then consider how to extend these results to adaptive sampling strategies , and design a generic framework as well as a new adaptive sampling algorithm for clustering ( ` clus2k ` ) .",
    "compared to previous approaches , the algorithm is much simpler and avoids doing a costly full eigen - decomposition at each iteration , yet experimentally appears to obtain equal or even better performance on a range of datasets .",
    "our theoretical results build on techniques for graph sparsification ( @xcite , @xcite ) , where the task is to find a sparse approximation to a given graph @xmath2 .",
    "this is somewhat similar to our task , but with two important differences : first and foremost , we do not have access to the full graph , whereas in graph sparsification the graph is given , and this full knowledge is used by algorithms for this task ( e.g. using the sum of edge weights associated with each node ) .",
    "second , our goal is to minimize the number of edge sampled , not the number of edges in the resulting graph ( of course by sampling a smaller number of edges we will get a sparser graph ) .",
    "notice that if we wish to end with a sparse graph , one can always use any graph sparsification technique on our resulting graph and get a graph with guarantees on sparsity .",
    "for simplicity we will consider all graphs as full weighted graphs ( with zero weights at edges that do not exist ) and so any graph will be defined by a set of vertices @xmath3 and a weight matrix @xmath0 .",
    "we will start with a few basic definitions .",
    "let @xmath4 be a weighted graph and @xmath5 a subset of vertices , then the cut defined by @xmath6 , @xmath7 , is the sum of all the weights of edges that have exactly one endpoint in @xmath6 .",
    "let @xmath4 and @xmath8 be two graphs on the same set of vertices .",
    "@xmath9 is an @xmath10-cut approximation of @xmath2 if for any @xmath5 we have @xmath11 let @xmath4 . the graph laplacian @xmath12 is defined as @xmath13 where @xmath14 is a diagonal matrix with values @xmath15 .",
    "the normalized graph laplacian @xmath16 is defined as @xmath17 .",
    "the laplacian holds much information about the graph @xcite .",
    "one of the main connections of the laplacian , and in particular the quadratic form it represents , to the graph is through the equation @xmath18 when @xmath19 this is easily seen to be the value of the cut defined by @xmath20 .",
    "many spectral graph techniques , such as spectral clustering , can be seen as a relaxation of such a discrete problem to @xmath21 .",
    "a graph @xmath9 is an @xmath22spectral approximation of @xmath2 if @xmath23    we note that this is different than requiring @xmath24 ( using the matrix 2-norm ) as we can view it as a multiplicative error vs. an additive error term . in particular , it implies approximation of eigenvectors ( using the min - max theorem @xcite ) , which is relevant to many spectral algorithms , and includes the approximation of the 2nd eigenvector ( the focus of the analysis in @xcite ) as a special case moreover , it implies cut approximation ( via equation [ quadlap ] ) , and is in fact strictly stronger ( see @xcite for a simple example of a cut approximation which is not a spectral approximation ) .",
    "therefore , we will focus on spectral approximation in our theoretical results .",
    "our initial approximation strategy will be to uniformly at random sample a subset @xmath25 of @xmath26 edges , i.e. pick @xmath26 edges without replacement and construct a graph @xmath8 with weights @xmath27 for any @xmath28 and zero otherwise , where @xmath29 is the probability any edge is sampled .",
    "it is easy to see that the @xmath30=w$ ] .",
    "we begin by providing a bound on @xmath26 which ensures an @xmath10-spectral approximation .",
    "it is based on an adaptation of the work in @xcite , in which the author considered picking each edge independently .",
    "this differs from our setting , where we are interested in picking @xmath26 edges without replacement , since in this case the probabilities of picking different edges are no longer independent .",
    "while this seems like a serious complication , it can be fixed using the notion of negative dependence :    the random variables @xmath31 are said to be _ negatively dependent _ if for all disjoint subset @xmath32 $ ] and all nondecreasing functions @xmath33 and @xmath34 , @xmath35\\leq \\mathds{e}[f(x_i , i\\in i)]\\mathds{e}[g(x_j , j\\in j)]\\ ] ]    intuitively , a group of random variables are negatively dependent if when some of them have a high value , the others are more probable to have lower values . if we pick @xmath26 edges uniformly , each edge that has been picked",
    "lowers the chances of the other edges to get picked , so intuitively the probabilities are negatively dependent .",
    "the edge picking probabilities have been shown to be indeed negatively dependent in @xcite .",
    "an important application of negative dependence is that the chernoff - hoeffding bounds , which hold for sums of independent random variables , also hold for negatively dependent variables .",
    "see supplementary material for details .",
    "we can now state the general spectral approximation theorem :    [ spectralbound ] let @xmath2 be a graph with weights @xmath36 $ ] and @xmath9 its approximation after sampling @xmath26 edges uniformly . define @xmath37 as the second smallest eigenvalue of @xmath16 and @xmath38 .",
    "if @xmath39 then the probability that @xmath9 is not an @xmath10-spectral approximation is smaller then @xmath40 .",
    "the proof is based on an adaptation of part of theorem 6.1 from @xcite .",
    "the two main differences are that we use negative dependence instead of independence , and a weighted graph instead of an unweighted graph .",
    "the proof uses the following lemma    [ speilmanlemma ] let @xmath16 be the normalized laplacian of @xmath2 with second eigenvalue @xmath37 . if @xmath41 then @xmath9 is an @xmath42-spectral approximation for @xmath43 .",
    "the next part is to bound @xmath44 using a modified version of the trace method @xcite in order to bound this norm .",
    "see the supplementary material for more details .",
    "stating the result in a simplified form , we have that if @xmath45 , then one gets an @xmath10-approximation guarantee using @xmath46 sampled edges .",
    "the main caveat of theorem [ spectralbound ] is that it only leads to a non - trivial guarantee @xmath47 when @xmath48 and @xmath37 is not too small .",
    "most algorithms , such as spectral clustering , assume that the graph has @xmath49 relatively small eigenvalues , in the ideal case ( more then one connected component ) we even have @xmath50 .",
    "unfortunately , we will now show that this is unavoidable , and that the bound above is essentially optimal ( up to log factors ) for graphs with bounded @xmath51 , i.e. expanders . in the next section",
    ", we will show how a few reasonable assumptions allows us to recover non - trivial guarantees even in the regime of small eigenvalues .",
    "since spectral approximation implies cut approximation , we will use this to find simple bounds on the number of edges needed for both approximations .",
    "we will show that a necessary condition for any approximation is that the minimal cut is not too small , the intuition being that that even finding a single edge ( for connectedness ) on that cut can be hard , and get a lower bound on the number of samples needed . for this",
    "we will need to following simple lemma ( which follows directly from the linearity of expectation )    [ interlemma ] let @xmath52 be a finite set , and @xmath53 .",
    "if we pick a subset @xmath54 of size @xmath26 uniformly at random then @xmath55=\\frac{m\\cdot     we will now use this to prove a lower bound on the number of edges sampled for binary weighted graphs ( i.e. unweighted graph ) @xmath56 .",
    "[ karlower ] let g be an binary weighted graph with minimal cut weight @xmath57@xmath58@xmath59 .",
    "assume @xmath9 was constructed by sampling @xmath26@xmath60@xmath61 edges , then for any @xmath10@xmath60@xmath62 , @xmath63 .",
    "let @xmath64 be all the edges in a minimal cut and let @xmath25 be the edges sampled .",
    "since the weights are binary the weight of this cut in @xmath9 is the number of edges in @xmath65 .",
    "from lemma [ interlemma ] we know that @xmath66=mc / \\tbinom{n}{2}<1-\\delta$ ] . from the markov s inequality",
    "we get that @xmath67 . if @xmath68 then the intersection is empty and we do not have an @xmath10-approximation for any @xmath69 proving @xmath70 .",
    "this theorem proves that in order to get any reasonable approximation with a small budget @xmath26 ( at least with uniform sampling ) the original graphs minimal cut can not be too small and that @xmath71 samples are needed . comparing this to theorem [ spectralbound ] ( noticing @xmath72 ) we see that , for graphs with a lower bound on @xmath73 by sampling within a logarithmic factor of this lower bound is sufficient to ensure a good cut approximation .",
    "clustering algorithms assume a certain structure of the graph , generally they assume @xmath74 strongly connected components , i.e. the clusters , with weak connections between them ( the precise assumptions vary from algorithm to algorithm ) . while this is a bad scenario for approximation , as this normally means a small minimal cut ( and for spectral clustering a small @xmath37 ) , we will show how approximation can be used ( on the inner - cluster graphs ) to obtain useful results .",
    "we give two results , one geared towards spectral approximation , and the other towards cut approximation .",
    "assume a graph @xmath4 consists of @xmath74 clusters , define @xmath75 as the block diagonal matrix consisting of the similarity scores between same - cluster elements .",
    "@xmath76 and @xmath77 the off - diagonal elements .",
    "+    [ assumlambda ] define @xmath78 , the smallest of all the second normalized eigenvalues over all @xmath79 .",
    "assume @xmath80 for some constant @xmath81 .",
    "[ assumd ] @xmath82 where @xmath83 for some @xmath48 .",
    "[ outsmall ] assume that @xmath84 for some @xmath85 .",
    "assumption [ assumlambda ] implies well connected clusters , while assumption [ assumd ] excludes sparse , well - connected graphs , which we have already shown earlier to be hard to approximate .",
    "assumption [ outsmall ] essentially requires the between - cluster connections to be relatively weaker than the within - cluster connections .    under these assumptions ,",
    "the next theorem proves the clusters can be found .",
    "[ specclusterable ] let p be the zero eigenspace of @xmath86 corresponding to the @xmath74 clusters , @xmath87 the laplacian of the graph we get by sampling @xmath88 edges for @xmath89 , and @xmath90 the space spanned by the first @xmath74 eigenvectors of @xmath87 . under previous assumptions , @xmath91 .",
    "we simplified the statement in order not to get overwhelmed by notation .",
    "@xmath92 is a diagonal matrix whose diagonal values correspond to the canonical angles between the subspaces @xmath93 and @xmath90 , and @xmath94 is a common way to measure distance between subspaces .",
    "if @xmath95 , i.e. @xmath90 was spanned by eigenvectors of the full @xmath96 , then the theorem would be true by the sin - theta theorem @xcite using our assumptions .",
    "we need to show that this theorem can be used with @xmath87 .",
    "the sin - theta theorem states that @xmath97 where @xmath98 the `` noise '' factor , and @xmath99 the second eigenvalue of @xmath100 the `` signal '' factor . using theorem [ spectralbound ] and our first two assumptions we can approximate each @xmath101 and use to show that @xmath102 .",
    "we now only need to show @xmath103 .",
    "this can be done using the matrix chernoff inequality @xcite , by applying a result in @xcite that shows how it can be adapted to sampling without replacements .",
    "we note that the result in @xcite is limited to sampling without replacements as negative dependence has no obvious extension to random matrices . for further details see the supplementary material    this gives us a tradeoff between the number of edges sampled and the error .",
    "the theoretical guarantee from the sin - theta theorem for the complete graph is @xmath104 so for @xmath105 we have the same guarantee as if we used the full graph . for @xmath106",
    "large enough one can get @xmath94 as small as desired by using @xmath107 .",
    "cut based clustering , such as @xcite , have a different natural notion of `` clusterable '' .",
    "we will assume nothing on eigenvalues , making this more general than the previous section .",
    "[ asscut1 ] assume @xmath2 can be partitioned into @xmath74 clusters , within which the minimal cut is at least @xmath108 . furthermore , assume that any cut separating between the clusters of @xmath2 ( but does not split same cluster elements ) is smaller then @xmath109 , and that @xmath110 .",
    "these assumptions basically require the inner - cluster connections to be relatively stronger than between - cluster connections .",
    "[ clustering ] let @xmath2 be a graph with weights @xmath36 $ ] and @xmath9 its approximation after observing @xmath26 edges . under previous assumptions if @xmath111 then the cuts separating the clusters are smaller then any cut that cuts into one of the clusters .",
    "we can use cut approximation for the clusters themselves so @xmath112 . using the chernoff bound and union bound for the @xmath113 cuts between clusters",
    ", we get that none of them is greater then @xmath114 .",
    "see the supplementary material for full proof .    in the supplementary material ,",
    "we provide a more in - depth analysis of cut approximation including an analog of theorem [ spectralbound ] .",
    "theorem [ karlower ] states that , with uniform sampling and no prior assumptions of the graph structure , we need at least @xmath115 where @xmath57 is the weight of the smallest cut .",
    "what if we had an adaptive algorithm instead of just uniform sampling ?",
    "it is easy to see that for some graphs the same ( up to a constant ) lower bound holds .",
    "think of a graph with @xmath116 vertices , consisting of two cliques that have c randomly chosen edges connecting them .",
    "let s assume further that some oracle told us which vertex is in which clique , so any sensible algorithm would sample only edges connecting the cliques . as the edges are random , it would take @xmath117 tries just to hit one edge needed for any good approximation .",
    "however , in some cases an adaptive scheme can reduce the number of samples , as we now turn to discuss in the context of clustering .",
    "consider a similar toy problem - we have a graph which is known to consist of two connected components , each a clique of size @xmath106 and we wish to find these clusters .",
    "we can run the uniform sampling algorithm until we have only two connected components and return them .",
    "how many edges do we need to sample until we get only two connected components ?",
    "if we look only at one clique , then basic results in random graph theory @xcite show that with high probability , the number of edges added before we get a connected graph is @xmath118 which lower bounds the number of samples needed . to improve on this",
    "we can use an adaptive algorithm with the following scheme : at each iteration , pick an edge at random connecting the smallest connected component to some other connected component . at each step",
    "we have at least a probability of @xmath119 to connect two connected components .",
    "this is because there are @xmath106 nodes in the wrong cluster , and at least @xmath120 in the right cluster ( since we pick the smallest connected component ) .",
    "therefore with high probability the number of steps needed to decrease the number of connected components from @xmath116 to two is @xmath121 .",
    "this argument leads us to consider adaptive sampling schemes , which iteratively sample edges according to a non - uniform distribution .",
    "intuitively , such a distribution should place more weight on edges which may be more helpful in approximating the structure of the original graph .",
    "we first discuss how we can incorporate _",
    "arbitrary _ non - uniform distributions into our framework .",
    "we then propose a specific non - uniform distribution , motivated by the toy example above , leading to a new algorithm for our setting in the context of clustering .",
    "one approach to incorporate non - uniform distributions is by unbiased sampling , where we re - scale the weights according to the sampling probability .",
    "this means that the weights are unbiased estimates of the actual weights .",
    "unfortunately , this re - scaling is not easy to compute in general when sampling without replacement , as the probability of sampling an edge is a marginal distribution over all the algorithm s possible trajectories .",
    "sampling with replacement is much easier , since it only depends on the sampling probability in the current iteration .",
    "moreover , as long as we sample only a small part of all edges , the risk of re - sampling an already - sampled edge is negligible .",
    "finally , one can show that whatever the non - uniform distribution , a simple modification ( adding with probability half a uniform sample ) suffices for cut approximation to hold .",
    "unfortunately , we found this approach to work poorly in practice , as it was unstable and oscillated between good and bad clustering long after a good clustering is initially found .    due to these issues",
    ", we consider a _ biased _ sampling without replacement approach , where we mix the non - uniform distribution with a uniform distribution ( as proposed earlier ) on unseen edges , but do not attempt to re - scale of weights .",
    "more specifically , consider any adaptive sampling algorithm which picks an unseen edge at step @xmath122 with probability @xmath123 that depends on the graph @xmath124 seen so far .",
    "we will consider a modified distribution that with probability @xmath125 picks an unseen edge uniformly , and with probability @xmath125 picks it according to @xmath123 . while biased sampling can ruin approximation guarantees , in the clustering scenarios one can show similar results to theorem [ clustering ] ( under stronger conditions ) for any adaptive sampling scheme .",
    "the theoretical guarantees for adaptive sampling are in the supplementary material .",
    "we now turn to consider a specific algorithmic instantiation , in the context of clustering .",
    "motivated by the toy example from earlier , we consider a non - uniform distribution which iteratively attempts to connect clusters in the currently - observed graph , by picking edges between them .",
    "these clusters are determined by the clustering algorithm we wish to use on the approximated graph , and are incrementally updated after each iteration .",
    "inspired by the common practice ( in computer vision ) of over - segmentation , we use more clusters than the desired number of clusters @xmath74 ( @xmath126 in our case ) . moreover , as discussed earlier , we mix this distribution with a uniform distribution .",
    "the resulting algorithm , which we denote as ` clus2k ` , appears as algorithm [ alg : example ] below .",
    "budget @xmath127 , number of clusters @xmath74 + @xmath128 , @xmath129 the zero matrix . with",
    "probability 1/2 pick @xmath130 uniformly ; otherwise : @xmath131cluster @xmath129 into @xmath126 clusters ; + pick two distinct clusters @xmath132 and @xmath133 uniformly at random ; + pick @xmath130 connecting @xmath132 and @xmath133 uniformly at random ; + set @xmath134    for the setting of budget - constrained clustering , the two most relevant algorithms we are aware of is the algorithm of @xcite ( hereby denoted as ` s&t ` ) , and the ` iu_red ` algorithm of @xcite .",
    "these algorithms are somewhat similar to our approach , in that they interleave uniform sampling and a non - uniform sampling scheme",
    ". however , the sampling scheme is very different than ours , and focuses on finding the edge to which the derivative of the 2nd laplacian eigenvector is most sensitive .",
    "this has two drawbacks .",
    "first , it is specifically designed for spectral clustering and the case of @xmath135 clusters , which is based on the 2nd laplacian eigenvector .",
    "extending this to more than @xmath136 clusters requires either recursive partitioning ( which can be suboptimal ) , or considering sensitivity w.r.t .",
    "@xmath137 eigenvectors , and it is not clear what is the best way to do so .",
    "second , computing eigenvector derivatives requires a full spectral decomposition at each iteration , which can be quite costly or impractical for large matrices .",
    "in contrast , our algorithm does not compute derivatives .",
    "therefore , when used with spectral clustering methods , which require only the smallest @xmath126 eigenvectors , we have a significant gain .",
    "it is possible to speed up implementation even further , in the context of spectral clustering . since only a single edge",
    "is added per iteration , one can use the previously computed eigenvectors as an initial value for fast iterative eigenvector solvers ( although restarting every couple of steps is advised ) .",
    "another possible option is to pick several edges from this distribution at each step , which makes this process parallelizable .",
    "we tested our ` clus2k ` algorithm on several datasets , and compared it to the ` s&t ` and ` iu_red ` discussed earlier ( other alternatives were tested in @xcite and shown to be inferior ) .",
    "it is important to note that ` s&t ` and ` iu_red ` were designed specifically for @xmath135 and spectral clustering using the unnormalized laplacian @xmath12 , while we also tested for various values of @xmath74 , and using the normalized laplacian @xmath16 as well @xcite .",
    "the ` iu_red ` performed badly ( perhaps because it relies substantially on the @xmath135 assumption ) in these cases while ` s&t ` performed surprisingly well ( yet still inferior to ` clus2k ` ) .",
    "clustering was measured by cluster purity .",
    "the purity of a single cluster is the percent of the most frequent class in the cluster .",
    "the purity of a clustering is a weighted average of its single cluster purity , weighted by the number of elements in each cluster .",
    "the purity shown is averaged over 5 runs .",
    "the synthetic experiments were performed on two datasets - the two half circles dataset , and a dataset comprising of four well separated gaussians , both experiments used unnormalized spectral clustering ( see figure [ synthetic ] ) using a gaussian weight matrix .",
    "the two half circles is a classic clustering dataset with @xmath135 clusters .",
    "the gaussian dataset shows how the various algorithms handle an easy @xmath138 dataset , and ` iu_red ` performs worse than uniform sampling in this case .",
    "we tested on three further datasets - the iris and glass http://archive.ics.uci.edu/ml/#uci[uci  datasets ] ( both with k>2 clusters )   using a gaussian weight matrix , and the caltech-7 dataset , a subset of the caltech-101 images datasets with 7 clusters gathered by @xcite , using the similarity matrix suggested by @xcite .",
    "we tested each dataset using both the normalized and unnormalized laplacian for clustering .",
    "the results are presented in figure [ uci ]    [ uci ]      + overall , the experiments show that the ` clus2k ` algorithm performs as good or better than previous algorithms for budget - constrained clustering , while being significantly computationally cheaper as well as more general .    * *",
    "we will need to hoeffding - chernoff bound for negative dependence :    [ hoeffding ] assume @xmath139 $ ] are negatively dependent variables and define @xmath140 then @xmath141|\\geq\\epsilon\\mathds{e}[x]\\right ) \\leq 2\\exp\\left(-\\frac{\\epsilon^2\\mathds{e}[x]}{3}\\right ) \\ ] ]    see @xcite 1.6 and 3.1 for details .",
    "+ first it is important to note a change in notation from @xcite in order to be consistent with notation used in @xcite .",
    "a @xmath10-spectral approximation in our paper is weaker then a @xmath142-spectral approximation in @xcite .",
    "+ we will now go over the main changes needed to prove theorem 6.1 in @xcite ( disregarding s.2 ) with negatively dependent sampling of edges and weights @xmath143 $ ] . + the proof of claim 6.5 is quite straightforward .",
    "the claim of lemma 6.6 needs to be changed to @xmath144\\leq\\frac{w_{r , t}}{\\gamma^{k+l-1}d_r}$ ] instead of @xmath144\\leq\\frac{1}{\\gamma^{k+l-1}d_r}$ ] .",
    "the changes to the proof are again straightforward ( remembering @xmath143 $ ] ) .",
    "+ the main change is to lemma 6.4 . using the modified lemma 6.6 and substituting negative dependents for independence one can prove @xmath145\\leq\\frac{1}{\\gamma^{k-|t|}}\\sum_{\\sigma\\,valid\\,for\\,t,\\tau}\\prod_{s\\in t}\\frac{w_{v_{s-1},v_s}}{d_{v_{s-1}}}.\\ ] ] instead of equation 10 in the paper .",
    "the last change is to pick @xmath146 proportional to @xmath147 instead of uniformly to prove that    @xmath148    instead of equation 11 .",
    "from there on all changes are straightforward .",
    "let @xmath149 . let @xmath93 be the zeros eigenspace of @xmath150 , which is the same as the zero eigenspace of @xmath151 , if all the @xmath152 are connected .",
    "let @xmath90 be the space spanned by the first @xmath74 eigenvectors of @xmath87 . according to the sin - theta theorem @xcite , @xmath153 where @xmath154 is the spectral norm of @xmath155 and @xmath156 is the second smallest unnormalized eigenvalue of @xmath150 . to prove the theorem we will show that @xmath157 and that @xmath158 .",
    "+ the first claim is through using the first two assumptions and the following lemma    [ nor_unnor ] let @xmath159 and @xmath160 be the second smallest normalized and unnormalized eigenvalues of @xmath96 , and @xmath161 then @xmath162 .    from the min - max theorem we have @xmath163 @xmath164 and the lemma follows from the fact that @xmath165 .",
    "let @xmath166 be the number of edges needed in order to have an @xmath10-spectral approximation of each inner - cluster matrix @xmath167 for @xmath168 with probability @xmath169 , then by theorem [ spectralbound ] we have that @xmath170 .",
    "using this fact , the first two assumptions and lemma [ nor_unnor ] , it is easy to see that @xmath171 .",
    "+ we now need to show that @xmath158",
    ". the main tool would be the matrix chernoff inequlity for sampling matrixes without replacements .",
    "consider a finite sequence of hermitian matrices @xmath172 sampled uniformly without replacements from a finite set of matrices of dimension n. assume that @xmath173 define @xmath174 then @xmath175}||)\\leq n\\cdot\\left(\\frac{e^\\epsilon}{(1+\\epsilon)^{1+\\epsilon}}\\right)^{||\\mathds{e}[y]||/r}\\ ] ]    this is an adaptation of theorem 5.1.1 from @xmath176 replacing the independence requirement to sampling without replacements . in order to adapt the proof we notice that the only place where independence is used in in lemma 3.5.1 ( subadditivity of the matrix cumulant generating functions ) where we need to prove that @xmath177\\leq tr\\left(\\exp\\left(\\sum\\log\\mathds{e}e^{\\theta\\bf x_i}\\right)\\right)\\ ] ] using the result of @xmath178 , if @xmath179 are sampled uniformly at random without replacements for a finite set , and @xmath180 are sampled with the same probability with replacements then @xmath181\\leq \\mathds{e}\\left[tr\\left(\\exp\\left(\\sum\\theta{\\bf y_i}\\right)\\right)\\right]\\ ] ] so we can conclude that @xmath181\\leq\\mathds{e}\\left[tr\\left(\\exp\\left(\\sum\\theta{\\bf y_i}\\right)\\right)\\right]\\leq\\ ] ] @xmath182 where the second inequality is from [ cgfs ] as @xmath183 are independent .",
    "we define for each edge @xmath184 connecting nodes in different clusters the matrix @xmath185 that is equal to zero with probability @xmath186 and is equal to @xmath187 with probability @xmath188 , where @xmath189 is the laplacian of a single edge graph with weight @xmath190 .",
    "then @xmath191 , @xmath192=l^{out}$ ] , @xmath193 and @xmath194 .",
    "if we use the matrix chernoff inequality with @xmath195 then @xmath196 so if @xmath197 we get that @xmath198 for large enough @xmath106 .",
    "we will start by proving an analog of theorem [ spectralbound ] in the paper .",
    "we will use the following lemma from @xcite :    [ comblemma ] let @xmath2 be an undirected graph with @xmath106 vertices and minimal cut @xmath199 .",
    "for all @xmath200 the number of cuts with weight smaller of equal to @xmath201 is less then @xmath202 .",
    "the lemma is proven in @xcite for graphs with integer weights , but the extension to any positive weights is trivial by scaling and rounding .",
    "we can now state and prove the theorem guaranteeing good cut approximations .",
    "[ origkar ] let @xmath2 be a graph with weights @xmath36 $ ] , with minimal cut @xmath199 , and @xmath9 its approximation after sampling @xmath26 edges uniformly . if @xmath203 where @xmath204 , then the probability that @xmath9 is not an @xmath10-cut approximation is smaller then @xmath40 .",
    "this is an adaptation of the proof in @xcite - consider a cut with weight @xmath201 .",
    "let @xmath205 the probability to sample a single edge .",
    "let @xmath206 where @xmath207 is an indicator whether edge @xmath184 on the cut was sampled and @xmath190 its weight .",
    "define @xmath64 the sum of @xmath208 on all the edges along the cut , then by the fact that edges are negatively dependent and theorem [ hoeffding ] , the probability that the cut is not an @xmath10 approximation is smaller then @xmath209}{3}\\right)=2\\exp\\left ( -\\frac{\\epsilon^2\\alpha c p}{3}\\right)\\leq \\\\",
    "\\leq 2\\exp\\left ( -\\left(\\ln\\left(\\frac{1}{\\delta}\\right)+k\\right)\\alpha\\right)\\cdot n^{-2\\alpha } \\end{split}\\ ] ] define @xmath210 and let @xmath211 the number of cuts with value @xmath201 in the original graph .",
    "by the union bound the probability that some cut is not an @xmath10 approximation is less then @xmath212 ( notice that this sum is well defined since @xmath211 is non zero only in a finite number of @xmath213 values ) . defining @xmath214 then by the previous lemma @xmath215 .",
    "let @xmath34 be any measure on @xmath216 such that @xmath217 , then the integral @xmath218 is maximized when @xmath219 .",
    "this is due to the fact that @xmath93 is a monotonically decreasing function , so if the inequality is not tight at some point @xmath220 we could increase the value by picking @xmath221 for some appropriate @xmath222 and @xmath223 ( where @xmath40 is the dirac delta function ) . from this",
    "we can conclude that the probability of some cut not being an @xmath10-approximation is bounded by    @xmath224    a drawback is that the theorem gives a bound that depends on the minimal cut , which we do not know , and unlike the situation in @xcite we can not approximate it using the full graph .",
    "we can prove a bound that uses only known data about the graph .",
    "the following theorem shows we can lower bound @xmath57 .",
    "[ cmin ] let @xmath2 be a graph with weights @xmath36 $ ] , with minimal cut @xmath57 , and @xmath9 its approximation after sampling @xmath26 edges with minimal cut @xmath225 .",
    "define @xmath226 the probability to sample a single edge . also define @xmath227 and @xmath228 .",
    "with probability greater then @xmath229 the following inequality holds - @xmath230 .",
    "let @xmath6 be a subset of vertices such that @xmath231 then from the chernoff - hoeffding inequality ( the one - sided version ) @xmath232 where we multiply by @xmath188 to have all the elements bounded by @xmath62 .",
    "setting @xmath233 we get that with probability greater then @xmath229 that + @xmath234 . by completing the square",
    "we get that @xmath235 which means ( after some simple algebraic manipulation ) that @xmath236    we can combine these to theorems and get    let @xmath2 be a graph with weights @xmath36 $ ] and @xmath9 its approximation after sampling @xmath26 edges with minimal cut @xmath225 .",
    "define @xmath237 and @xmath74 as in previous theorems .",
    "if @xmath238 then the probability that @xmath9 is not an @xmath10-cut approximation is smaller then @xmath40 .    this is just using lemma [ cmin ] with error probability @xmath239 and using that @xmath57 for theorem [ origkar ] with the same error probability and the union bound .",
    "this theorem gives a high probability bound that depends only on observable quantities . while the notation is a bit cumbersome , it is easy to see that if @xmath240 , i.e. the unscaled weight of the smallest cut is not too small , then @xmath241 and we have a bound that is almost as good as if we knew the real @xmath57",
    ". + we will now prove theorem [ clustering ] in the paper .",
    "let @xmath2 be a graph with weights @xmath36 $ ] and @xmath9 its approximation after observing @xmath26 edges .",
    "assume @xmath2 is partitioned into @xmath242 clusters each has minimal cut greater or equal to @xmath108 , and the cuts separating clusters from the others is smaller then @xmath109 .",
    "furthermore assume @xmath243 . if @xmath244 then the cuts separating the clusters are smaller then any cut that cuts into one of the clusters .",
    "after seeing @xmath26 edges , the probability for sampling any edge inside any cluster is @xmath245 . by theorem [ origkar ]",
    "we have that if @xmath246 then the probability of any cut in a single cluster being smaller then @xmath247 is smaller then @xmath248 , with the union bound we have that with probability greater then @xmath249 all cuts in any cluster ( and therefore any cut in @xmath9 that cuts some cluster ) have weights greater or equal to @xmath247 .",
    "+ we now need to show that the cuts separating the clusters are not too large .",
    "consider a cut separating some clusters from the others . if the weight of this cut is @xmath57 we need to show that with probability greater then @xmath250 we have @xmath251 .",
    "this means that we want to show that @xmath252 , i.e. we can use the negatively dependent chernoff - hoeffding inequality ( theorem [ hoeffding ] ) with @xmath253 ( using the fact that @xmath243 ) and get that the @xmath254 .",
    "as @xmath255 we can finish the proof .",
    "while we found that adaptive sampling with replacements did not perform as well as without replacements in practice , for completeness we will present her a proof that it has the same theoretical guarantees as uniform sampling for cut approximation .",
    "let @xmath124 be the graph build at step @xmath256 , an adaptive sampling algorithm is an algorithm who picks an edge at step @xmath122 with probability @xmath123 that depends on @xmath124 . in order to prove that with high probability @xmath257 is a @xmath10-approximation of @xmath2 for @xmath258 we need that @xmath123 is nt too small on any edge",
    "this can be easily done by sampling according to a modified distribution - with probability @xmath125 pick an edge uniformly , and with probability @xmath125 pick it according to @xmath259 .",
    "the new distribution satisfies @xmath260",
    ". + the graphs @xmath124 are by no means independent .",
    "although one can view ( after subtracting the mean ) them as a martingale process , using the method of bounded differences @xcite will not suffice , as it depends on the square of the bounding constant , so we will have a @xmath261 factor that only gives a trivial bound .",
    "we will next show that a high probability bound does exists .",
    "+ consider a cut with weight @xmath57 that contains the edges @xmath262 and consider any bounded adaptive sampling algorithm with replacements with @xmath26 steps .",
    "define @xmath263 with @xmath264 and @xmath265 to be the random variable that has value @xmath266 if the edge @xmath267 was chosen at step @xmath74 and zero otherwise .",
    "define @xmath268 , @xmath269 is the weight added to the cut at step @xmath74 and its expectation is @xmath57 .",
    "[ boundlemma ] if @xmath270 and @xmath271 then @xmath272 \\leq \\exp(c\\rho(e^t-1))\\ ] ]    since at most one of the positive variables @xmath263 is nonzero for a constant @xmath74 then they are negatively dependent when conditioned by @xmath273 .",
    "this implies that @xmath274 \\leq\\prod\\limits_{i=1}\\limits^l \\mathds{e}[\\exp(t\\rho x_{ik})|\\tilde{g}_{k-1 } ] $ ] . by definition of @xmath263",
    "we get that @xmath275 = \\tilde{p}(e_i)\\cdot\\exp\\left(\\frac{t\\rho w(e_i)}{\\tilde{p}(e_i)}\\right)+(1-\\tilde{p}(e_i))\\ ] ] one can easily verify that the right hand side of equation [ temp1 ] decreases monotonically with @xmath276 , so the fact that @xmath277 and @xmath271 implies that @xmath278 \\leq \\rho w(e_i)e^t+(1-\\rho w(e_i))=\\ ] ] @xmath279 where the last inequality is due to the fact that for @xmath280 .",
    "we can finish the proof since @xmath281 \\leq\\prod\\limits_{i=1}\\limits^l \\mathds{e}[\\exp(t\\rho x_{ik})|\\tilde{g}_{k-1 } ] \\\\ & \\leq \\exp(\\rho c ( e^t-1 ) ) .",
    "\\end{split}\\ ] ] as @xmath282 .",
    "we can now prove the concentration of measure bound for a single cut    let @xmath2 be a graph such that @xmath271 and @xmath257 the output of a bounded adaptive sampling algorithm with replacements such that @xmath283 then the probability that a cut with weight @xmath57 in @xmath284 is not a @xmath10-approximation is bounded by @xmath285 .",
    "we need to show that @xmath286 the proof is similar to the proof of the chernoff bound , replacing independence with lemma [ boundlemma ] .",
    "first look at @xmath287 . using the standard trick for all @xmath288 @xmath289 by the markov inequality this",
    "is bounded by @xmath290}{\\exp(t(1+\\epsilon ) \\rho m c)}$ ] . the law of total expectation states that @xmath291=\\mathds{e}\\left[\\mathds{e}\\left[\\exp\\left(t\\rho\\sum\\limits_{k=1}\\limits^m y_k \\right)|\\tilde{g}_{m-1}\\right]\\right]$ ] .",
    "as @xmath292 is a deterministic function of @xmath293 this is equal to @xmath294\\exp\\left(t\\rho\\sum\\limits_{k=1}\\limits^{m-1 } y_k \\right)\\right ] \\\\ & \\leq \\mathds{e}\\left[\\exp\\left(t\\rho\\sum\\limits_{k=1}\\limits^{m-1 } y_k \\right)\\right]\\exp(\\rho c ( e^t-1 ) ) .",
    "\\end{split}\\ ] ] using lemma [ boundlemma ] . by induction",
    "we can conclude that the expectation is smaller then @xmath295 .",
    "we have shown that @xmath296 following the steps as in the standard chernoff bound proof one can show that this is smaller ( for the right @xmath297 ) then @xmath298 .",
    "the proof for this bound on @xmath299 is done in a similar fashion , and using the union bound we finish our proof .",
    "using @xmath300 one can now show similar theorems to what we shown in the previous section with this theorem replacing the ( negatively dependent ) chernoff bound .",
    "for a specific graph one can always design a bad biased sampling scheme .",
    "consider an adversarial scheme that always samples the largest weight edge between two constant clusters , it is easy to see that this can lead to bad cut clustering . to circumvent this",
    "we will consider graphs where the edge weights between the clusters , which we regard as noise , are picked randomly .",
    "let @xmath9 be the graph after sampling @xmath304 edges without replacements ( with probability @xmath305 of sampling uniformly ) with @xmath85 .",
    "let @xmath306 and @xmath307 be the minimal cut weight inside any cluster and the maximal cut weight between clusters , under previous assumptions the probability that @xmath308 is smaller then @xmath40 .    using theorem [ origkar ] on the edges sampled uniformly ( remembering that the biased sampling can only increase the cut weight ) we get that with probability greater then @xmath169 , @xmath309 .",
    "if we consider the weight of any cut between clusters , then the key observation is that because the edges are picked uniformly at random , then whatever the algorithm does is equivalent to running a uniform sampling of a constant edge set .",
    "we then get that the expected minimal cut weight is @xmath310 using lemma [ interlemma ] ( the upper bound is by looking as if all edges where picked from this cut )",
    ". we can now use the markov inequality to show @xmath311 ."
  ],
  "abstract_text": [
    "<S> we consider the problem of learning from a similarity matrix ( such as spectral clustering and low - dimensional embedding ) , when computing pairwise similarities are costly , and only a limited number of entries can be observed . </S>",
    "<S> we provide a theoretical analysis using standard notions of graph approximation , significantly generalizing previous results ( which focused on spectral clustering with two clusters ) . </S>",
    "<S> we also propose a new algorithmic approach based on adaptive sampling , which experimentally matches or improves on previous methods , while being considerably more general and computationally cheaper . </S>"
  ]
}