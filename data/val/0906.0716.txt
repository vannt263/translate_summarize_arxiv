{
  "article_text": [
    "some 75 years ago zipf found that the word frequency of a language has a very particular `` power - law like '' distribution @xcite .",
    "this phenomena is best known as zipf s law and states that the number of occurrences of a word in a long enough written text falls off as @xmath0 where @xmath1 is the occurrence - rank of a word ( the smaller rank , the more occurrences ) @xcite @xcite @xcite @xcite @xcite .",
    "how well is this power law obeyed ?",
    "what is its origin ?",
    "what does it imply from a linguistic and cognitive point of view , if anything ?",
    "simon in ref .",
    "@xcite emphasized that the fact that `` power law '' distributions occur in a wide range of seemingly unrelated phenomena suggests a general underlying stochastic nature .",
    "in particular he devised a general stochastic model for the writing of a text , the _",
    "simon model _ @xcite .",
    "the random element in this model is tied to the actual process of evolving the text and not to a property of the language itself .",
    "simon model _ and its stochastic evolution mechanism has since its first appearance turned up in many disguises such as rich - get - richer models and preferential attachment @xcite .",
    "an alternative view was taken by mandelbrot who proposed that zipf s law of word frequencies could be associated with the collective language itself rather than with the evolution of a particular text @xcite .",
    "in particular he proposed that the `` power - law like '' distribution could be linked to an optimization of a letter - combination information @xcite .",
    "however , miller in ref .",
    "@xcite showed that a power law distribution of words in a collective language does not _",
    "per se _ requer any optimization , which gave rise to the metaphor of a monkey randomly writing on a typewriter @xcite .",
    "all these proposed explanations presumes that the `` power - law like '' distribution says nothing about the syntax , grammar and context correlations of a written text . yet",
    "the word correlations are , of course , essential for the meaning of a text .    in the present paper we focus on the function @xmath2 , the number of distinct words which occur precisely @xmath3 times in a written text .",
    "the correspondence of zipf s word rank power law is for this quantity @xmath4 @xcite .",
    "we here focus on the properties of single novels , each novel written by a single author . in this way",
    "we ensure that both the evolution aspect of the text and the properties of the language always relates to the very same text . from this perspective",
    "a novel can perhaps be regarded as a fingerprint of the author s brain @xcite .",
    "we demonstrate that the text of a novel display certain general features and show that these features are shared with a simple null model which we call the random book .",
    "in section 2 we describe some general characteristic features which the text of a novel display . for clarity",
    "we choose one typical novel as an illustrative example . in appendix",
    "a we include data for a collection of novels in order to illustrate the generality of the conclusions . in section 3",
    "we discuss the random book transformation which describes how the word - frequency distribution changes with the length of the text analyzed .",
    "it is shown that a real novel to good approximation transforms in the same way .",
    "it is also shown that the random book transformation can be used in order to obtain a sharper determination of the word - frequency distribution of a novel .",
    "section 4 contains our summary and concluding remarks .",
    "examples of key characteristics of the word frequencies in a novel are as follows : the most obvious is the word - frequency distribution of the _ complete _ novel .",
    "a word is in this context defined as a group of letters separated by blanks .",
    "if the book contains @xmath5  distinct ( different ) words and a total of @xmath6  words , then @xmath7 is the probability that a word , which you pick randomly in the book , is occurring @xmath3-times in the book .",
    "this means that @xmath8 where @xmath9  is the maximum number of times a distinct word appears in the book and also that @xmath10 , which is the average number of times a word occurs in the book .",
    "the function @xmath11 is the word - frequency distribution and is often very broad and more or less `` power - law like '' , @xmath12 with @xmath13 , over a substantial region .",
    "this is illustrated in fig .",
    "1a with data for the novel _ howards end _ ( he in the following ) by e.  m.  forster taken from ref .",
    "@xcite where circles correspond to the raw data .",
    "the horizontal distribution for the largest @xmath3-values means that only single unique words have the largest number of occurrences .",
    "the triangles corresponds to a @xmath14-binning ( bin @xmath15 has a size of @xmath16 ) of the data and one notes that these data follow a smooth curve .",
    "_ this last fact implies that the data are produced by a stochastic process_. the functional form @xmath17 gives a good fit to the data ( @xmath18 in fig .",
    "the level of `` goodness '' of this fit is discussed in section iii and shown in fig .  [ 4 ] .    instead of analyzing the complete book",
    ", one can analyze a _ section _ containing a total of @xmath19 words .",
    "then one finds that the `` power - law slope '' of the corresponding word - frequency distribution , @xmath20 , in a novel depends on the total number of words @xmath21 .",
    "this is illustrated in fig .",
    "1b , which shows the average word - frequency distribution for @xmath22-parts of howards end .",
    "the total number of words is @xmath23 which means that the @xmath24-part shown in fig .",
    "1b only has @xmath25 words while the @xmath26-part corresponds to @xmath27 words .",
    "the word - frequency distribution for a section of size @xmath21 is obtained as an average over a large amount of sections of the same size and we use periodic boundary conditions in order to avoid reduced statistics due to the boundaries of the book . as will be shown below",
    ", real books display a strong tendency towards having the words close to randomly distributed , allowing for the use of periodic boundary conditions . as seen in fig .",
    "1b the slope of the  power - law like  part of the distribution gets systematically steeper when taking smaller and smaller sections of the book . from a practical point of view",
    "this means that if you attempt to approximate the word frequency distribution with the function @xmath28 then the exponent @xmath29 increases as @xmath30 decreases . _",
    "the change of the shape of _",
    "@xmath31 _ as a function of the total number of words _",
    "@xmath30 _ is a characteristic feature of the word frequency in a book_.    fig .",
    "2a shows the number of distinct words @xmath32 as a function of the total number of words @xmath30 : the first word is always distinct which means that @xmath33 .",
    "as you go further into the book , words tend to be repeated which means that the number of distinct words increases slower than a straight line with slope 1 .",
    "the shape of @xmath34 gives a characteristics of the novel since it reflects the spatial distribution of words within the novel .",
    "note that the function @xmath34 and the distributions @xmath35 are directly related , since the average number of times a distinct word appears is @xmath36 . how would @xmath34 change",
    "if the words were completely randomly distributed in the book , keeping the same frequency distribution ? as seen from fig .",
    "2a , the function for the randomized book ( where all words are placed randomly in the book ) is very close to the raw data of the novel . _",
    "a characteristic feature of a novel is that the distribution @xmath34 is close to the one for the random null model of the novel_. this implies that the real novel and the null model share some overall random features .",
    "the random features are also reflected in the distribution of words belonging to different frequency classes : the frequency class @xmath3 contains all words which appears precisely @xmath3 times in the book .",
    "for example the class @xmath37 contains all the words which only occurs once in the book .",
    "random with respect to frequency classes means that there is no preference for words belonging to a specific frequency class to appear in any particular part in the book .",
    "thus for a random book you should have encountered close to half the words belonging to a frequency class when you have read precisely half of the book .",
    "2b shows the percentage of words belonging to a frequency class @xmath3 encountered after reading half a real book as a function of @xmath3 .",
    "the data is for the real he and the full drawn horizontal line is the expectation value for a randomized he .",
    "the grey shadings mark one and two standard deviations ( using the same binning as for the real novel ) away from the randomized he .",
    "this means that if the data circles in fig .",
    "2b had belonged to a single realization of a random he book then they would with large probability fall inside the grey areas .",
    "the actual circles in fig .",
    "2b give the data for the real novel he .",
    "these data follow the same horizontal trend and are compatible with the random null model over a substantial region of @xmath3 values .",
    "however a real novel is of course a highly purposely structured creation . some noticable deviations in fig .",
    "2b can immediately be associated with such contructive features . the first noticable deviation in fig .",
    "2b is that the value for the frequency class @xmath37 ( words which only occur once in the book ) is only 47% ( an average over the collection of books in appendix a gives 47,3% ) , which is a statistically significant deviation from 50% .",
    "the reason is that an author who writes a book from the beginning to the end will have a slightly decreasing tendency of introducing new rare words towards the end of the book .",
    "another noticable deviations is the two circles higher than 50% for larger @xmath3 ( words occurring very often in the book ) .",
    "these deviations are actually caused by the two specific words _",
    "she _ and _ her _ and are clearly context related features in the novel ( a particular context in chapter four about a third into the book has a very low concentration of _ she _ and _ her _ ) .",
    "nevertheless fig .",
    "2b illustrates that the overall tendency of the data has the same characteristic feature as the null model . for the simon model ,",
    "the distribution of words belonging to different frequency classes are incompatible with the random feature displayed by real novels : the triangles in fig .",
    "2b represent the data for a single simon book of the same length and @xmath38 as he .",
    "the dashed lines give the analytic asymptotic behavior in the small and large @xmath3 limits ( see appendix b ) .",
    "it is clear that rare words tend to appear very late in the simon - book while common words are more densely positioned early in the book .",
    "as explained below , this is because the simon model is a growth model .",
    "another characteristic feature of the null model is that the text is translationally invariant .",
    "this means that if you divide the novel into three consecutive sections and obtain the functions @xmath34 separately for all three sections then these three functions show no systematic trend in there deviations .",
    "2c demonstrates that the same is to very good approximation also true for the real novel he .",
    "appendix a gives data from a variety of novels suggesting that the qualitative agreements between the random null model and real novels given by figs .",
    "2a and c are indeed general features .",
    "real books contain information in the form of a story .",
    "different parts describe different events and surroundings which may creates word correlations .",
    "so , we should expect some fluctuations between curves for different parts of a novel .",
    "but the point is that , in general , no systematic change can be observed between parts of a real novel . _",
    "the translational invariance of the text is a characteristic feature of a novel . _",
    "whereas a real novel is in qualitative agreement with the null model , the _",
    "simon model _ is instead incompatible : fig .",
    "2d shows that the simon model does not obey translational invariance , but instead display a strong systematic trend .",
    "the data is obtained by generating books of the same length as he using the stochastical growth model by simon @xcite .",
    "the books are divided into three consecutive parts of equal size and the average distributions for these three parts are plotted in the figure . as seen the distributions systematically changes with the position in the book in a way that is incompatible with the translational invariance .",
    "this is contrary to the data for a real book ( compare fig .",
    "so what is wrong with the simon model in the context of real novels ? the problem can be traced to the stochastic element ( the dice ) in the model : the ground version of the _ simon model _ goes as follows@xcite : the novel is assumed to be written by adding words in a consecutive order from the start to the end . each time the author adds a word to the text it can either be a word not previously used in the text or an old word .",
    "there is a certain chance to add a new word and a certain chance to use an old one .",
    "the crucial stochastical assumption in the model is that the chance for picking a specific old word is directly proportional to the number of times this word has already been used in the text written so far .",
    "thus the randomness in the simon model is associated with picking words randomly from the text already written . as this text evolves , the reservoir ( the text written so far ) from which the random words are picked also changes .",
    "hence the random element in simon - type models explicitly depends on the growth process of the text .",
    "it means that the stochastic element changes with the position in the book .",
    "this is in contrast to the random null model , where the randomness is independent of the position in the book .",
    "one may also note that the resulting word - frequency distribution , @xmath11 , for the simon model , with a constant growth rate , is independent of the length of the text .",
    "this is in contrast to a real novel where the shape of the distribution changes with the length of the text ( compare fig .",
    "the crucial point is that stochastic text evolution models in general have the same problem as the simon model , including all preferential attachment type models @xcite @xcite @xcite @xcite .",
    "_ growth processes which are based on a stochastic element ( a dice ) which ipso facto depends on the position in the text do not adequately reproduce the statistical distribution of words in a text .",
    "_ we emphasize that this is a fundamental structural feature which can not be remedied within this class of stochastic models .",
    "this implies that the stochastic element in real novels belong to an altogether different stochastic class .",
    "a noteworthy additional characteristic feature is that the word - frequency distribution @xmath20 for an author does to large extent only depend on the number of words @xmath21 written by the author and not on the specific book or short story .",
    "this is illustrated in fig .  3 by comparing a short story by d.  h.  lawrence ( _ the prussian officer _",
    "( po ) , @xmath39 ) with book sections of the corresponding size from two of his full novels . fig .",
    "3a is for _ woman in love _ ( wl ) which has @xmath40 and b ) for _ sons and lovers _ ( sl ) which has @xmath41 . as in the case of fig .",
    "1b , the word frequency distribution for a section is the average over many sections of the same size . in order to obtain a section size of the same length as the short story we use @xmath24-parts in a ) and @xmath42-parts in b ) .",
    "the agreement is very good in both cases except for the data of the very highest @xmath3-values .",
    "this difference is an artifact of comparing a snapshot ( po ) with a curve resulting from averages ( sectioning of wl and sl ) .",
    "we now return to the characteristic size dependence of the word - frequency distribution @xmath20 for a novel described in fig .",
    "1b . in fig .",
    "4a compares this size dependence with the corresponding size dependence of the random null model : first we extract , directly from the raw data , the @xmath43 corresponding to sections @xmath26-parts of the novel he .",
    "this data is represented by squares in fig .",
    "next we randomize the words in he .",
    "note that a randomization leaves the frequency distribution @xmath11 invariant . from a sample of the randomized he - book we extract",
    "@xmath43 corresponding to @xmath26-parts of the randomized he .",
    "this is given by the triangles in fig .",
    "the overlap of the data is near perfect , indicating that the null model transform in very much the same way as the real novel . in case of the random null model",
    "one can straighforwardly obtain the size transformation .",
    "the starting point is the word - frequency distribution @xmath11 for a book with @xmath6 total words and @xmath5 different words .",
    "the question is how @xmath11 relates to the word - frequency distribution , @xmath44 , for a section size @xmath45 of the very same book . for the case",
    "when the words within a frequency class are randomly distributed the relation follows from combinatorics .",
    "the probablility for a word that appears @xmath46 times in the full book to appear @xmath3 times in a smaller section ( @xmath47 ) can be expressed in binomial coefficients @xcite : if we let @xmath11 and @xmath20 be two column matrices with @xmath5 elements numerated by @xmath3 , then    @xmath48    where @xmath49 is the triangular matrix with the elements @xmath50    and @xmath51 is the binomial coefficient .",
    "the coefficient @xmath52 is @xmath53    since @xmath49 is a triangular matrix with only positive definite elements it also has an inverse which is given by    @xmath54    one should note that rbt ( random book transformation ) only hinges on the assumption that words belonging to a frequency class are randomly distributed through out the book .",
    "since this assumption is rather well obeyed by real novels ( compare fig .",
    "2b ) , the near perfect agreement between the randomized null model and the real he in case of the two @xmath26-parts shown in fig .",
    "4a may be interpretated as a confirmation that the real novel and the randomized novel share some basic stochastical features .    in fig .",
    "4b we start from the randomized he and section it into parts with @xmath21 words . from each section size the average number of distinct words @xmath55 is determined so that one obtains the quantity @xmath56 .",
    "an average over many sections of the same size is used .",
    "the result is the full drawn curve in fig .",
    "one should note that this is in fact not a curve but a very dense set of data points ( each point corresponds to a different section size which means that the total number is @xmath57 ) . in this way the raw data for he given by the cirles in fig .",
    "1a are transformed into a very smooth curve for @xmath58 .",
    "the bayesean probabalistic assumption used is that words from different word - frequency classes have no preferential order . as apparent from fig .",
    "2b and fig .",
    "4a this is a very reasonable bayesean assumption .",
    "the point is now that the function @xmath58 through the rbt - transformation uniquely determines @xmath11 and vice versa .",
    "in order to find the corresponding @xmath11 we have used a parametrized ansatz for @xmath11 and determined the parameters so as to reproduce the @xmath58-data as well as possible . in fig .",
    "4b we have tested three different parametrization forms . the first is a pure power law , @xmath59 , ( short dashed curve in fig .",
    "our conclusion is that a power law is incompatible with the data and can be ruled out .",
    "the next try is a power law with an exponential cut off , @xmath60 .",
    "this form gives a very resonable approximation of the data and the function representing the binned data in fig .",
    "1a corresponds to the long dashed curve in fig .",
    "but one can , off course , do a little bit better by adding another parameter .",
    "the augmentet power law with an exponential cut off , @xmath61 , gives an even better fit to the data ( open circles in fig .",
    "4b ) .    as simple quantitative goodness measure",
    ", one can take the maximum absolute difference between the real data and the data obtained from the various parametrizations : the values for the power - law , power - law with exponential cut off and the augmented power - law with exponential cut off are approximately @xmath62 , @xmath63 and @xmath64 , respectively . in fig .",
    "4a we have replotted the binned he - data from fig .  1a together with the best parametrization of @xmath11 obtained from the @xmath58-data in fig .",
    "4b ( circles and dashed curve , respectively ) .",
    "the interesting point here is that our data analysis , which makes use of the rbt - transformation , makes it possible to distinguish between parametrizations of @xmath11 which would otherwise be very hard to distinguish .",
    "this is illustrated in fig .",
    "4c which directly compares the augemented power law with exponential cut off with the straight power law with exponential cut off .",
    "as seen from the fig .",
    "4c , there is almost no discernable difference when @xmath11 is plotted in a log - log scale .",
    "a consequence of the rbt - transformation is that the functional form of @xmath11 changes with the length of the text . the full drawn curve in fig .",
    "4a gives @xmath11 corresponding to @xmath26-parts of he obtained from the parametrization of the form @xmath65 determined from fig  .4b .",
    "it agrees very well with the real data .    in fig .",
    "3 it was demonstrated that the word frequency distribution , associated with @xmath66-part sections of a novel of an author , to good approximation also describes a shorter novel by the same author , provided the shorter novel has the same length as the sections .",
    "one can then extrapolate this idea and imagine that the longer novel also can be described as a section of an even longer novel , and so on . this leads to the suggestion of a `` meta book '' , a giant single `` mother book '' which characterizes the word - frequency distribution of all the writings of an author .",
    "an author would then , when writing a novel , be roughly pulling a section of @xmath21 words from this `` meta book '' resulting in a word - frequency distribution @xmath20 .",
    "this is the same as transforming down the `` meta book '' via the rbt to the size @xmath21 .",
    "the `` meta book''-concept will be further explored in a forthcoming paper.@xcite",
    "we have shown that the words belonging to a frequency - class in a book have a tendency to be randomly distributed thoughout the text .",
    "this randomness is incompatible with text growth models like the simon model@xcite .",
    "this is because these models are based on a stochastic assumption of re - using words already written in the text .",
    "this is true for all growth models , independent on the detail of the growth mechanism .",
    "it was also shown that the word - frequency distribution of a novel has a _ shape _ which systematically depends on the size of the novel .",
    "also this feature is incompatible the simon model @xcite .",
    "instead the properties of a novel were to large extent found to be shared with a random null model .",
    "the size transformation of this model is explicitly given by a random book transformation ( rbt ) and some consequences of this were explored .",
    "we speculate that the word - frequency is consistent with the concept of a `` meta book '' which characterizes the word - frequency distribution of all the writings of an author .",
    "our findings about the statistical properties of the words in a novel seem to be general : it does not matter much which author or book you pick , the overall properties are the same ( at least for the english novels we have so far analyzed ) .",
    "thus it does say something general about the structure of the written language used by a single author .",
    "since language in general is a product of the human evolution , it also means that the statistical properties presumably reflects some evolutionary pressure .",
    "this work was supported by the swedish research council through contract 50412501 . very helpful discussions with seung ki baek",
    "are also gratefully acknowledged .",
    ".list of the books analyzed .",
    "@xmath67 is the total number of words in the book , @xmath68 is the total number of _ different _ words in the book and @xmath69 is the average number of times a word is used .",
    "the initials of the authors stand for : e.m f @xmath70 e.m .",
    "h m @xmath70 herman melville .",
    "g o @xmath70 george orwell .",
    "t h @xmath70 thomas hardy .",
    "l @xmath70 d.h lawrence . [ cols=\"<,<,^,^,^\",options=\"header \" , ]     [ book_values ]    when compared to the simon - book , all the real books seem to have small values of @xmath71 and @xmath72 , indicating a strong resemblance to the null model of the random book .",
    "the values in the second row is also showing that there is no real trend among the real books , except for sl , which has a small negative trend ( compared to the simon - book which has a very strong _ positive _ trend ) .",
    "in the simon model a word is being written at every time step .",
    "with probability @xmath73 a new word , that has never been written in the book so far , is written . and with probability @xmath74 an old word is rewritten , chosen uniformly from the words existing in the book .",
    "this means that the probability for a word to be rewritten is proportional to the number of times it has already been written .",
    "when re - creating a real book the parameter @xmath73 ( @xmath75 ) is usually a small number ( @xmath76 ) and the length of the book ( @xmath77 ) is generally large ( @xmath78 ) .",
    "we want to start by calculating how big a fraction of a book , written by the simon - model , one has to read before having encountered half of all the words that appear only once in the book . to do this we need to calculate the probability that a specific word which is introduced at time @xmath79 is not repeated through out the book with length @xmath80 . at every time @xmath81 the probability for this word not to be rewritten is the sum of the probabilities that another of the words already written is rewritten ( @xmath82 ) and that instead a completely new word is written ( @xmath73 ) . at time @xmath79 , @xmath79 words have been written in total and @xmath83 words are still to be written , so the total probability @xmath84 becomes          since @xmath88 ( except for very small times , which includes only a tiny part of the whole text ) we make a taylor expansion around zero , approximate the sum with an integral and get @xmath89_t^t = \\ln \\rho^{t - t } + \\frac{1}{\\rho}\\ln\\left(\\frac{t}{t}\\right ) \\label{b3}\\end{aligned}\\ ] ]      if we write a book , then @xmath84 is the average number of @xmath37-words one gets from the introduction time @xmath79 , and so @xmath91 where @xmath92 is the total number of @xmath37-words in the book .    @xmath93_{1/t}^1\\nonumber\\\\   & = & \\frac{t}{2-\\alpha}\\left(1-\\underbrace{\\left(\\frac{1}{t}\\right)^{2-\\alpha}}_{\\approx 0 } \\right)\\nonumber\\\\ \\rightarrow w_d(1 ) & \\approx & \\frac{t}{2-\\alpha}\\end{aligned}\\ ] ]    to find the time , @xmath94 , when we have introduced half of all the @xmath37-words , we solve the expression : @xmath95_{1/t}^{t_{1/2}/t}\\nonumber\\\\ & & = \\left(\\frac{t_{1/2}}{t } \\right)^{2-\\alpha}-\\underbrace{\\left(\\frac{1}{t } \\right)^{2-\\alpha}}_{\\approx 0 } \\approx \\left(\\frac{t_{1/2}}{t } \\right)^{2-\\alpha}\\nonumber\\\\ \\rightarrow & & \\frac{t_{1/2}}{t } = \\left(\\frac{1}{2}\\right)^{\\frac{1}{2-\\alpha } } \\label{t_1/2}\\end{aligned}\\ ] ] which is the fraction of the book one has to read before one half of the @xmath37-words have been read . for the simon - book in fig .  2 ( @xmath96 ) this value is @xmath97 .",
    "that is , @xmath98 of the book .",
    "+ equation [ t_1/2 ] can be generalized into @xmath99 where @xmath66 is the fraction of one - degree words .",
    "+ next we want to do the same thing for @xmath100-words .",
    "now we need to calculate the probability that if a word is first introduced at time @xmath101 it will only be repeated once at time @xmath102 .",
    "this probability is given by          again , this quantity gives the average number of @xmath100-words one will get from words that are introduced at time @xmath101 and repeated at time @xmath102 , which means that @xmath106 where we sum over all possible combinations of @xmath101 and @xmath102 where @xmath107 .",
    "this can also be evaluated in a similar way as for the @xmath37-case and we get          the first sum counts all the words where both its appearances happen before @xmath94 and is thus counted twice .",
    "the second sum counts all the words that was introduced before @xmath94 and repeated after @xmath94 and is thus counted as one .",
    "equation [ half_k2 ] can be evaluated into : @xmath111 equation [ t_1/2_k2 ] can not be solved analytically but a numerical solution for the simon - book in fig .  2 ( @xmath96 )",
    "gives the value @xmath112 .",
    "we now have two points ( @xmath37 and @xmath100 ) giving the asymptotic functional form for low k : s . in fig .",
    "2b a straight line was drawn intersecting these two point ( @xmath113 and @xmath114 ) to show this asymptotic behavior .",
    "the derivations for this quantity gets very complicated for larger values of @xmath3 since we are summing over all different words with the same frequency .",
    "but for very large @xmath3:s we have words that are alone in their frequency - group .",
    "that is , they are the only one with that particularly frequency .",
    "this makes the derivation much simpler and we can get the asymptotic behavior for large @xmath3:s . from ref .",
    "@xcite we get the equation      where @xmath116 is the number of occurrences a word will have in a book of length @xmath80 if it was introduced at time @xmath79 .",
    "we want to know at what time we have written half of those words .",
    "this is given by                                          fig 1 : word frequency distribution @xmath11 for the book howards end ( he ) : a ) circles give the raw data .",
    "the horizontal tail reflects that the largest number of occurrences corresponds to single words .",
    "triangles give log - binned data and follow a smooth curve implying a stochastic origin .",
    "the actual data is to good approximation of the form @xmath17 with @xmath18 : b ) @xmath11 changes with the section size of the book .",
    "full curve represent the complete he , long - dashed curve and short - dashed curves represents sections corresponding to a 20th and 200th parts of he , respectively .",
    "the curves represent the log - binned data .",
    "fig 2 : number of distinct words @xmath34 as a function of the total number of words @xmath21 : a ) real and randomized he given by full and dashed curve , respectively .",
    "the close agreement implies that the words are close to randomly distributed throughout the book : b ) curves describing how big a fraction of the book one has to read before having encountered half of all the words with a specific frequancy .",
    "the circles and triangles represent the real he and a simon - book ( same size and @xmath120 as he ) respectively .",
    "the dashed lines are showing the analytic asymptotic behavior of the simon - book ( see appendix b ) .",
    "the full line represents the average result for a randomized book and the gray areas shows one and two standard deviations away from the random book .",
    "c ) @xmath34 for three different starting points within the book ; full , long - dashed and short - dashed curves correspond to the beginning , middle and end of he , respectively .",
    "the close agreement implies that the word distribution in a book is to good approximation translational invariant : d ) the same different starting points as in c ) assuming that the word - distribution was given by the simon text growth model .",
    "the large and systematic differences shows that the simon - type growth models do not describe the randomness of the word distribution in a real text .",
    "fig 3 : the sectioning of two full novels compared to a short story by the same author .",
    "a ) the circles represent the binned data of the full novel _ woman in love_. the triangles show the sectioning ( a 20th - part ) of the same book down to the same size as the short story _ the prussian officer _ , shown with squares .",
    "b ) the same as for a ) but for the full noval _ sons and lovers _ sectioned into an 18th - part .    fig 4 : the random book transformation ( rbt ) .",
    "a ) the data for he ( open circles ) is parametrized ( dashed curve ) .",
    "the dashed curve is transformed to a 200th - part of the book ( full curve ) .",
    "this full curve should correspond to a 200th - part of the randomized he ( open triangles ) .",
    "the agreement is striking .",
    "the distribution corresponding to a 200th part of the real he is given by the open squares .",
    "the close agreement with the triangles shows that the words are to large extent randomly distributed .",
    "b ) the function @xmath121 for he : full curve corresponds to the randomized he and the circles are obtained from the rbt using the parametrization of @xmath11 given in a ) .",
    "the agreement is perfect .",
    "the long - dashed curve corresponds to the data obtained from rbt using the parametrization of @xmath11 given in fig .",
    "1a and the inset , which is an in - zoomed version of the dashed squar , is showing how this curve is deviating from the real data . the short dashed curve in b ) represents a power - law fit to the word - frequancy distribution which clearly fails to represent the data .",
    "c ) is showing how similar the two parametrizations are which means that rbt determines @xmath11 to high accuracy .",
    "fig a1 : complementary figure to fig .",
    "2a and c showing the number of distinct words @xmath34 as a function of the total number of words @xmath21 for seven additional books : first column represents counting from start to finish and the second column represents counting through three consecutive parts of the same size ."
  ],
  "abstract_text": [
    "<S> it is shown that a real novel shares many characteristic features with a null model in which the words are randomly distributed throughout the text . such a common feature is a certain translational invariance of the text . </S>",
    "<S> another is that the functional form of the word - frequency distribution of a novel depends on the length of the text in the same way as the null model . </S>",
    "<S> this means that an approximate power - law tail ascribed to the data will have an exponent which changes with the size of the text - section which is analyzed . </S>",
    "<S> a further consequence is that a novel can not be described by text - evolution models like the simon model . </S>",
    "<S> the size - transformation of a novel is found to be well described by a specific random book transformation . </S>",
    "<S> this size transformation in addition enables a more precise determination of the functional form of the word - frequency distribution . </S>",
    "<S> the implications of the results are discussed . </S>"
  ]
}