{
  "article_text": [
    "one reason why network science has recently attracted significant research attention is that network structure efficiently encodes the complexity of a large variety of systems , from the brain to different techno - social infrastructures @xcite . in the last fifteen years or",
    "so there has been significant progress in characterizing not only universal properties of complex networks , e.g. , scale - free degree distributions or small - world properties , but also their specific features that distinguish one network from another  degree correlations , community structure @xcite , or motif distributions @xcite .",
    "more recently , considerable effort has focused on quantifying network complexity using new network entropy measures @xcite borrowed from information theory , statistical mechanics @xcite , and quantum information @xcite .",
    "the entropy of a network ensemble evaluates the total number of networks belonging to the ensemble @xcite .",
    "the more complex , sophisticated , and unique the network structure , the smaller the number of networks in the ensemble having these peculiar properties , the smaller the entropy .",
    "the entropy measures have proven useful for solving inference problems involving real - world networks @xcite .",
    "the statistical mechanics treatment of network ensembles can be used to characterize the likelihood that a real dataset is generated by a model @xcite .",
    "some real networks have been shown to belong with high likelihood to ensembles of random geometric graphs in hyperbolic spaces , modelling trade - offs between popularity and similarity in network evolution , and casting preferential attachment as an emergent phenomenon @xcite .",
    "more recently , the entropy of multiplex ensembles has been proposed to characterize the complexity of multilayer networks @xcite .    by definition ,",
    "a network ensemble is a set of graphs @xmath2 with probability measure @xmath3 .",
    "it is important to make a distinction between microcanonical and canonical ensembles @xcite . in microcanonical ensembles , some structural network properties",
    "are fixed to given values .",
    "for example , the total number of links in graphs of size @xmath0 can be fixed to @xmath4 , or the degrees of all nodes can be fixed to degree sequence @xmath5 , @xmath6 . in this case , the ensemble consists of all graphs of size @xmath0 , and the probability measure is uniform : if the number of graphs @xmath2 satisfying the constraints is @xmath7 , then for all such graphs , @xmath8 , and @xmath9 for all other graphs that do not satisfy the constraints . in the canonical counterparts of these ensembles ,",
    "the same structural constraints are fixed only on average  the resulting ensembles are maximum - entropy ensembles under the constraints that the expected values of the number of edges or node degrees in the ensemble are @xmath4 or @xmath5 . the probability measure @xmath3 in this case is not uniform  the closer the @xmath2 to satisfying the constraints , the larger the @xmath3 . in random graph ensembles with a fixed exact or expected number of links",
    ", the canonical distribution converges to the microcanonical distribution in the thermodynamic limit @xmath10 . however , as soon as the number of imposed constraints is extensive , the canonical and microcanonical network ensembles are not equal even in the thermodynamic limit , and neither are their entropies .",
    "for example , the entropy of the microcanonical ensemble with a fixed degree sequence is not equal to the entropy of the canonical ensemble where only the expected degree sequence is fixed @xcite .    in network theory",
    "there is usually no question of how precisely we know the node degrees : given a graph , its degree sequence is uniquely defined .",
    "however when a network practitioner works with real network data , she is typically given a collection of network measurements .",
    "does she have to treat the measured degrees of nodes as precisely defined as well , given that measurements are always imprecise and ever - changing , and so is the network itself ?",
    "the answer is usually ` no'the relevant information is not the exact degree sequence , but its statistical properties , such as the distribution of these degrees .",
    "the ensembles of networks with a given exact or even expected degree sequence do not account for possible statistical fluctuations of node degrees in a given dataset , motivating us to consider here ensembles of random networks whose exact or expected degree sequences @xmath5 are independently sampled from a given distribution @xmath11 .",
    "this approach is a way to explore only the statistical properties of networks , and not their specific linking diagrams that might be affected by false or missing links , almost always present in real data .    specifically , we study the distribution of entropy in network ensembles with a given exact or expected degree distribution @xmath11 .",
    "we find that in both cases ( hard / exact and soft / expected ) , if the network ensemble is sparse , the average entropy is well - defined and self - averaging .",
    "we also evaluate the probability that ensemble entropy is equal to a particular value , conditioned on the total number of links in the network , and show that this conditional entropy distribution is always well - behaved . characterizing large entropy deviations in the ensemble with a given degree distribution and average degree , we observe that a condensation phenomena can occur in the network .",
    "this phenomena occurs only if the average degree in the network @xmath12 exceeds the degree distribution average @xmath13 . for @xmath14",
    "there is a symmetry under permutation of the labels of the nodes of the network , meaning that if we fix the average degree @xmath12 of the network , then the degrees of all nodes are @xmath15 , where @xmath0 is the network size . instead ,",
    "if @xmath16 , we observe a spontaneous breaking of this symmetry , with a single node having an @xmath1 degree .",
    "these results hold in both hard and soft ensembles , i.e. , ensembles with fixed exact or expected degree distributions .",
    "we begin with reviewing in section  ii what is known about the entropy of network ensembles with a fixed expected or exact degree sequence .",
    "we then move to section  iii and  iv where we analyze some properties of the entropy distributions in the ensemble with a given expected and exact degree distribution , respectively .",
    "final remarks are in section  v.",
    "a network ensemble is specified once probability @xmath3 is assigned to every network @xmath2 of size @xmath0 .",
    "we denote nodes by @xmath17 .",
    "the set of simple undirected unweighted labeled networks of size @xmath0 is bijective to the set of symmetric boolean @xmath18 adjacency matrices @xmath19 having zeroes on the diagonal . depending on whether nodes @xmath20 and @xmath21 are connected or not in network @xmath2 , element @xmath22 of @xmath2 s adjacency matrix is either @xmath23 or @xmath24 .",
    "we next impose the constraint that the degree @xmath25 of each node @xmath20 is fixed to some @xmath26 .",
    "we can treat this constraint as hard or soft .",
    "if it is hard , we impose it exactly .",
    "the resulting ensemble is a microcanonical network ensemble with given degree sequence @xmath5 , known as the configuration model @xcite . in the soft case",
    ", we relax the constraint , and demand that the degree of each node @xmath20 , averaged over all networks in the ensemble , is @xmath26 , which no longer has to be integer but can be any non - negative real number .",
    "the resulting ensemble is a canonical network ensemble with a given expected degree sequence @xmath5 , belonging to the class of random graphs known as exponential random graphs @xcite . in what follows",
    "we denote by @xmath27 the probability of @xmath2 in the canonical ( @xmath28 ) or micro - canonical ( @xmath29 ) ensembles .",
    "the entropy @xmath30 of these ensemble evaluates the typical number of networks in the ensemble and is given by s=-_gp_e(g|\\{k_i})p_e(g|\\{k_i } ) , [ entropy0 ] where the sum is performed over all networks in the ensemble .",
    "the probability distribution @xmath31 in the canonical ensemble is defined as the distribution that maximizes entropy s(\\{k_i})=-_gp_c(g|\\{k_i})p_c(g|\\{k_i } ) , [ entropy ] subject to the following @xmath0 constraints : @xmath32 by summing over all networks @xmath2 , we sum over their adjacency matrices . introducing lagrangian multipliers",
    "@xmath33 to enforce the conditions in eq .",
    "@xmath34 , and lagrangian multiplier @xmath35 to normalize the probability measure @xmath36 , we solve the system of equations @xmath37=0\\end{aligned}\\ ] ] to find the expression for distribution @xmath31 : @xmath38 \\label{pc}\\ ] ] where the normalization constant z_c = e^-=_gis called the `` partition function . ''",
    "since the probability distribution in eq .",
    "@xmath39 has an exponential form , this ensemble is called _ exponential random graphs_.    in this ensemble , we can relate entropy @xmath30 in eq .",
    "@xmath40 to partition function @xmath41 : @xmath42\\nonumber \\\\ & = & \\sum_{i=1}^n \\lambda_{i } k_i+\\log{z_c}. \\label{sc}\\end{aligned}\\ ] ] we call the entropy @xmath30 of the canonical ensemble the _ shannon entropy_.    the probability @xmath43 of a link between node @xmath20 and node @xmath21 in the ensemble is given by p_ij=&= & + & = & , [ pijc ] + where @xmath44 are called `` hidden variables '' . upon this change of variables , the constraints in eq .",
    "@xmath34 translate to k_i=_jip_ij=_ji .",
    "[ kpij ] this system of equations can be solved for @xmath45 yielding the values of lagrangian multipliers @xmath46 .",
    "using @xmath43 in eq .",
    "@xmath47 , a simpler expression for distribution @xmath31 reads p_c(g|\\{k_i})=_ijp_ij^a_ij(1-p_ij)^1-a_ij .",
    "[ eq : p_c(pij ) ] therefore probability @xmath31 is actually the probability to generate network @xmath2 with hidden variables @xmath45 by connecting node pairs @xmath20 and @xmath21 with probability @xmath43 given by eq .",
    "( [ pijc ] ) , and not connecting them with probability @xmath48 . using these link existence probabilities @xmath43 , the entropy of the ensemble in eq .",
    "@xmath40 can be written as s(\\{k_i})=-_i <",
    "j[p_ijp_ij+(1-p_ij)(1-p_ij ) ] .",
    "[ s ] by inserting the explicit dependence of probabilities @xmath43 on hidden variables @xmath49 , we can extract the leading term @xmath50 of the entropy that depends only on the average degree in the network , and the subleading term @xmath51 that increases linearly with @xmath0 : s(\\{k_i})=s(\\{k_i})-n(\\{k_i } ) , [ s2 ] where @xmath50 and @xmath52 in any sparse network are given by ( \\{k_i})&=&nn , + n(\\{k_i})&=&_i < j[p_ij(n p_ij)+(1-p_ij)(1-p_ij ) ] + & = & .",
    "[ sigma ] if all expected degrees @xmath53 , where @xmath12 is the average expected degree @xmath54 , then hidden variables @xmath55 are proportional to expected degrees @xmath26 , @xmath56 , and we can approximate probabilities @xmath43 in eq .",
    "@xmath47 by p_ij==. which corresponds to the case where links in the network are uncorrelated .    in this case the expression for the extensive entropy term @xmath52 in eq .",
    "@xmath57 simplifies to ( \\{k_i})&=&_i k_i k_i-[1 + ] .      in the microcanonical ensemble ,",
    "all networks satisfy the hard constraint that the degree sequence is @xmath5 exactly .",
    "we assume here that the degree sequence is graphical @xcite , meaning that it can be realized by at least one network .",
    "this condition is obviously satisfied if the degree sequence is read off from a real network .",
    "the probability distribution in the ensemble is uniform  all networks satisfying this constraint have the same probability @xmath58 \\label{pm}\\ ] ] where @xmath59 $ ] stands for the kronecker delta , and where `` partition function '' @xmath60 is given by @xmath61 .",
    "\\label{zm}\\ ] ] this partition function simply counts the number of networks with degree sequence @xmath5 .    the definition of the network ensemble entropy in eq .",
    "@xmath62 applied to the microcanonical distribution in eq .",
    "@xmath63 yields @xmath64 where we call @xmath65 the _ gibbs entropy _ of the network ensemble .",
    "the gibbs entropy @xmath65 of the microcanonical ensemble is related to the shannon entropy @xmath30 of the conjugate canonical ensemble via @xmath66 where @xmath67 is equal to the logarithm of the probability that in the conjugate canonical ensemble the hard constraints @xmath68 are satisfied : @xmath69\\right\\}. \\label{omega}\\ ] ] the relation between entropies in eq .",
    "@xmath70 can be obtained by substituting the canonical distribution @xmath71 given by eq .",
    "@xmath39 into eq .",
    "@xmath72 , yielding @xmath73&=&\\sum_{g } \\frac{1}{z_c}e^{-\\sum_{i=1}^n\\lambda_{i}\\sum_{j\\neq i}a_{ij}}\\nonumber\\\\ & & \\times\\prod_{r=1}^n\\delta\\left[\\sum_{s\\neq r}a_{rs},k_r\\right]\\nonumber \\\\ & = & \\frac{1}{z_c}e^{-\\sum_{i=1}^n \\lambda_{i}k_i}\\nonumber\\\\ & & \\times\\sum_{{g}}\\prod_{r=1}^n\\delta\\left[\\sum_{s\\neq r}a_{rs},k_r\\right]\\nonumber \\\\ & = & \\frac{z_m}{e^{s(\\{k_i\\})}}=\\exp[n\\sigma(\\{k_i\\})-s(\\{k_i\\})],\\nonumber\\end{aligned}\\ ] ] where in the last relation we have used eq .",
    "@xmath74 , eq .",
    "@xmath75 , and eq .  @xmath76 .",
    "the value of function @xmath77 in sparse networks can be calculated by statistical mechanics methods @xcite : ( \\{k_i})&=&_i=1^n .",
    "[ oe ] it does not vanish in the thermodynamic limit @xmath10 . therefore in view of the relation between the microcanonical and canonical entropies in eq .",
    "@xmath70 , the microcanonical and conjugate canonical ensembles are not equivalent even in the large-@xmath0 limit .",
    "in this section we consider the network ensemble in which the expected degree sequence @xmath5 is not fixed but sampled in each network realization from a fixed distribution @xmath11 .",
    "drawing from the field of disordered systems , we make a distinction between _ quenched _ and _ annealed _ disorder . if the disorder is annealed the degree of the nodes are not fixed and they are continuously drawn form a degree distribution @xmath11 .",
    "if the disorder is quenched , then the expected degree sequence @xmath5 in each realization is assumed to be fixed but unknown , and for each expected degree sequence , the ensemble probability distribution is obtained by maximizing the entropy .",
    "below we consider the quenched case only .    for a fixed expected degree sequence @xmath5 , the maximum - entropy distribution @xmath31 is given by eq .",
    "( [ eq : p_c(pij ) ] ) , while the entropy @xmath30 of this distribution is given by eq .",
    "( [ s ] ) .",
    "if this degree sequence @xmath5 has probability @xmath78 in a larger ensemble , then the probability and entropy distributions in this larger ensemble are p_c(g)&= & p(\\{k_i } ) p_c(g|\\{k_i } ) , + p_c(s)&= & p(\\{k_i } ) .",
    "therefore the distribution of the entropy @xmath30 in this ensemble gives a very important indication on how the number of possible network realization with expected degree sequence @xmath5 changes if the sequence realization is drawn randomly from a degree distribution @xmath11 .",
    "if we can not compute the full distribution @xmath79 exactly , we may still characterize its average , variance , and relative error & = & p(\\{k_i } ) s(\\{k_i } ) + & = & _ i=1^ndk_i p(\\{k_i } ) ^2 , + s(\\{k_i})&=&.      we assume that each expected degree sequence @xmath5 has probability @xmath80 , where @xmath11 is the expected degree distribution , and consider the specific case of power - law @xmath81 with @xmath82 .",
    "we first focus on the leading term of entropy @xmath30 given by @xmath83 , where @xmath84 is the sum of expected degrees @xmath85 .",
    "we distinguish between two cases .",
    "* _ case @xmath86 . _ + when @xmath86 , distribution @xmath87 is gaussian , the average of @xmath88 is well defined in the network and its relative error vanishes in the large network limit .",
    "indeed , since @xmath89 , we have ( \\{k_i})n^-1/2 . *",
    "_ case @xmath90 $ ] .",
    "_ + for large @xmath0 and @xmath90 $ ] , due to the structural degree cutoff @xmath91 , we observe that the average of @xmath88 is also well defined in the network and its relative error also vanishes in the large network limit , but with a different exponent . indeed , since @xmath92 we have ( \\{k_i})n^-(-2)/2 .",
    "these results are important because they imply that for every value of @xmath93 the average of the leading entropy term @xmath94 is well defined , with vanishing relative error .",
    "+ since the leading entropy term @xmath95 depends only on the average degree @xmath12 , we can further analyze entropy fluctuations in the ensemble with a fixed @xmath12 .",
    "therefore we next evaluate the conditional probability distribution @xmath96 that depends only on the distribution of the subleading entropy term , since average degree @xmath12 determines uniquely the leading term .      if @xmath78 is the probability of degree sequence @xmath5 , then p(s|)&=&_i dk_i p(\\{k_i})(s , s(\\{k_i } ) ) + & & ( n,_i=1^nk_i ) .",
    "[ ps00 ] since entropy @xmath97 is a function of hidden variables @xmath45 only , @xmath98 , we can perform the following change of variables in the last equation :",
    "p(\\{k_i})_i=1^n dk_i=(\\{h_i})_i=1n dh_i , where @xmath99 is the probability of hidden variables sequence @xmath45 . after this transformation , and expressing delta functions in eq .  ( [ ps00 ] ) via exponentials , we obtain , p(s|)&=&_i dh_i ( \\{h_i})de^i + & & de^i [ ps0 ] we next make the simplifying assumption that the hidden variables are i.i.d",
    ".  distributed , @xmath100 with some distribution @xmath101 . in the large network limit",
    "we can then transform the multiplex integral over @xmath0 variables @xmath102 to a functional integral over density function ( h)=_i=1^n ( h , h_i ) , imposing constraint @xmath103 by lagrangian multiplier @xmath104 . the distribution @xmath96 defined in eq .",
    "( [ ps0 ] ) becomes p(s|)=ddd(h ) e^g ( , , , ) , [ ps1 ] with & & g ( , , , ) = -ndh ( h ) + & & -i n^2dh dh ( h ) ( h ) [ s(h , h)+p(h , h ) ] + & & -indh + in+is , [ g ] where s(h , h)&=&-\\{. + & & .+ } + p(h , h)&=&. [ shhphh ] the integrals in eq .",
    "( [ ps1 ] ) can be evaluated at the saddle point given by s&=&n^2dh dh ( h ) ( h ) ( h , h)[sp ] + & = & n^2dh dh ( h ) ( h ) p(h , h ) + ( h)&=&,where we have performed the wick rotation of parameters @xmath105 and @xmath106 . denoting by @xmath107 the @xmath108-dependent solution of the above saddle point equations , we obtain the following simple expression for distribution @xmath96 : p(s|)&= & e^-nd_kl , + d_kl&= & dh^(h ) is the kullback - leibler distance between distributions @xmath107 and @xmath109 .",
    "therefore conditional distribution @xmath96 is well behaved , and depends only on kl - distance @xmath110 $ ] .",
    "the saddle point eqs .",
    "( [ sp ] ) have a solution only if the average degree @xmath12 is equal to or less than the expected degree @xmath111 of the degree distribution , i.e. only if m = ndh dh ( h)(h ) p(h , h ) .",
    "in fact the lagrangian multipliers @xmath112 must be real and greater than zero to guarantee that @xmath113 given by eq .",
    "( [ sp ] ) is well defined .",
    "following @xcite , to explore large deviation properties of a fat - tailed distribution , we use the following ansatz : n(h)=(n-1)_c(h)+(h , h_c ) .",
    "this ansatz accounts for a spontaneous breaking of permutation symmetry between the @xmath0 hidden variables in the ensemble , and reflects our expectation to detect condensation in some large - deviation realization in the ensemble . with this ansatz ,",
    "probability @xmath96 becomes p(s|)=ddd(h ) e^g(_c , h_c , , , ) , [ ps1b ] with & & g(_c , h_c , , , ) = -ndh _ c(h ) + & & - + & & -i n^2dh dh _",
    "c(h ) [ s(h , h)+p(h , h ) ] + & & -i 2 n^2 dh _",
    "c(h ) [ s(h_c , h)+p(h_c , h ] ) + & & -in dh + in+is , [ gb ] where functions @xmath114 and @xmath115 are as in eqs .",
    "( [ shhphh ] ) .",
    "the problem of minimizing function @xmath116 with respect to all its parameters has a non - trivial solution only if @xmath16 , in which case we have n = mn+2n^2dh _ c(h ) p(h_c , h ) with @xmath117 , and @xmath118 for any @xmath16 . in figure",
    "@xmath119 we show the phase diagram @xmath120 , where @xmath121 is the exponent of the hidden variable distribution @xmath122 , and @xmath123 $ ] for @xmath124 . above the curve @xmath125 , i.e. , in the shaded region of parameter values , we observe condensation .     where we observe the condensation discussed in the text ( shaded region ) .",
    "the figure corresponds to the hidden variable distribution @xmath122 with @xmath126 $ ] and @xmath124.,title=\"fig:\",width=297 ]",
    "the results presented in the previous section concerning the soft ensembles remain qualitatively unchanged if we consider the hard ensembles of networks with a given degree distribution of exact degrees .",
    "note that we are treating here always quenched disorder .",
    "in fact here we consider ensembles of networks of fixed degree sequence , where each degree sequence is drawn randomly from a given degree distribution . as in the soft case , in this hard case",
    "we assume that the disorder is quenched , and that the exact degree sequence @xmath5 is fixed but unknown and drawn from degree distribution @xmath11 .",
    "the probability of degree sequence @xmath5 is thus @xmath127 , and for each @xmath5 we consider the microcanonical ensemble of networks with the fixed sequence of exact degrees @xmath5 . in this ensemble ,",
    "network @xmath2 has probability @xmath128 defined in eq .",
    "( [ pm ] ) , so that the probability distribution @xmath129 in the ensemble is p_m(g)= p(\\{k_i } ) p_m(g|\\{k_i } ) .",
    "given degree sequence @xmath5 , and using eq .",
    "( [ sso ] ) and eq .",
    "( [ s2 ] ) , the gibbs entropy @xmath65 is the sum of three contributions , n(\\{k_i})&=&s(\\{k_i})-n(\\{k_i } ) + & = & s(\\{k_i})-n(\\{k_i})-n(\\{k_i } , where the leading term of @xmath65 is @xmath130 . in what follows",
    "we analyze the entropy distribution @xmath131 in the ensemble p(n)= p(\\{k_i } ) .",
    "if we can not compute the full distribution @xmath131 exactly , we may still characterize its average , variance , and relative error & = & p(\\{k_i } ) n(\\{k_i } ) + & = & _ i=1^ndk_i p(\\{k_i } ) ^2 , + & = & .",
    "we first consider the probability distribution @xmath87 of the leading term @xmath88 of entropy @xmath65 , defined as p(s)= _",
    "i=1^n p(k_i ) , where @xmath132 depends only on the average degree in the network .",
    "according to the generalized central limit theorem @xcite , and similarly to the soft case , we have the following two cases :    * _ case @xmath86 _ : + the distribution @xmath87 converges to a gaussian distribution and the relative error on the average of @xmath88 is vanishing in the large network limit .",
    "in fact we find that ( \\{k_i})n^-1/2 . *",
    "_ case @xmath90 $ ] _ : + due to the structural degree cutoff @xmath91 , the entropy distribution has a vanishing relative error given by ( \\{k_i})n^-(-2)/2 .    in the both cases ,",
    "the average @xmath94 is well defined with a relative error @xmath133 vanishing in the large network limit .",
    "similarly to the soft case , we next show that the large entropy fluctuations are due exclusively to the fluctuations of the total number of links in the ensemble .",
    "we note that these fluctuations are necessarily present since the exact degree sequence in each network in the ensemble is independently sampled from the given distribution . following a similar procedure",
    ", we evaluate the probability of @xmath135 conditioned to a fixed value of the average degree in the network @xmath12 , @xmath134 .",
    "this conditional entropy distribution depends only on the distribution of the subleading contributions to @xmath65 , @xmath136 , because the average degree determines uniquely the leading term @xmath50 .    if @xmath78 is the probability of degree sequence @xmath5 , then p(n|)&=&_i dk_i p(\\{k_i})(n , n(\\{k_i } ) + & & ( n,_i=1^nk_i ) .",
    "[ ps00b ] since entropy @xmath137 is a function of hidden variables @xmath45 only , @xmath138 , we can change variables p(\\{k_i})_i=1^n dk_i=(\\{h_i})_i=1^n dh_i where @xmath99 is the probability of hidden variables sequence @xmath45 , and obtain , p(n|)&=&_i dh_i ( \\{h_i})de^i + & & de^i .",
    "[ ps0b ] assuming next that our hidden variables are i.i.d .",
    "distributed @xmath100 with some distribution @xmath101 , we transform the multiplex integral over @xmath0 variables @xmath102 in the large network limit to a functional integral over density function ( h)=_i=1^n ( h , h_i ) , imposing constraint @xmath139 by lagrangian multiplier @xmath104 . the distribution @xmath134 defined in eq .",
    "( [ ps0b ] ) becomes p(n|)=ddd(h ) e^g ( , , , ) , [ ps1bb ] with & & g ( , , , ) = -ndh ( h ) + & & -i n^2dh dh ( h ) ( h ) [ s(h , h)+p(h , h ) ] + & & + i n dh ( h ) ( ) + & & -indh+in+in , [ gb2 ] where s(h , h)&=&-\\{. + & & .+ } + p(h , h)&= & , + k(h)&=&ndh ( h ) p(h , h ) .",
    "[ shhphhb ] the integrals in eq .",
    "( [ ps1bb ] ) can be evaluated at the saddle point given by n&=&n^2dh dh ( h ) ( h ) ( h , h ) + & & - ndh ( h ) ( ) + k(h)&=&ndh ( h ) p(h , h ) + & = & n^2dh dh ( h ) ( h ) p(h , h ) + ( h)&=&(h)()^ + & & \\{-n dh ( h)p(h , h ) } + & & \\{-n dh ( h ) 2s(h , h)}[spb ] where @xmath140 is a normalization constant and @xmath141 stands for the harmonic number . denoting by @xmath107 the @xmath135-dependent solution of the above saddle point equations , we get the following simple expression for distribution @xmath134 : p(n|)&= & e^-nd_kl . as in the soft case , in this hard case",
    "the entropy distribution conditioned on the average degree in the network is well - behaved and depends only on the kl distance @xmath110 $ ] between distributions @xmath107 and @xmath109 .",
    "the saddle point eqs .",
    "( [ spb ] ) have a solution only if the average degree @xmath12 is equal to or less than the expected degree @xmath111 over the degree distribution , < m = ndh dh ( h)(h ) p(h , h ) .",
    "in fact the lagrangian multipliers @xmath112 must be real and greater than zero to guarantee that @xmath113 given by eq .",
    "( [ spb ] ) is well defined .",
    "therefore , following the same logic as in the soft case , we use the following ansatz : n(h)=(n-1)_c(h)+(h , h_c ) , with this this ansatz , probability @xmath134 becomes p(n|)=ddd(h ) e^g(_c , h_c , , , ) , with & & g(_c , h_c , , , ) = -ndh _ c(h ) + & & - + & & -i n^2dh dh _",
    "c(h ) [ s(h , h)+p(h , h ) ] + & & + i n dh _",
    "c(h ) ( ) + & & -i 2 n^2 dh _",
    "c(h ) [ s(h_c , h)+p(h_c , h ] ) + & & + i n ( ) + & & -indh + in+in , [ gbb ] where functions @xmath114 , @xmath115 , and @xmath142 are as in eqs .",
    "( [ shhphhb ] ) .",
    "the problem of minimizing function @xmath116 with respect to all its parameters , has a non - trivial solution only if @xmath16 , in which case we have n = mn+2n^2dh _ c(h ) p(h_c , h ) with @xmath117 , and @xmath118 for any @xmath16 .",
    "the phase diagram of @xmath120 with @xmath122 and @xmath143 for @xmath124 is the same as in the soft case shown in figure  [ fig1 ] .",
    "if @xmath16 , we can observe condensation  a single node in the network can acquire a degree of the order of @xmath0 .",
    "motivated by the observation that in modeling real networks , the degree distribution is a more reasonable and realistic constraint than the degree sequence , we have studied the entropy distribution in the ensembles of random networks with a given degree distribution .",
    "we found that entropy is self - averaging , thanks to the structural degree cutoff at @xmath144 .",
    "the fluctuations of entropy are mainly determined by the fluctuations of the average degree in the ensembles .",
    "networks with average degree exceeding a certain threshold , @xmath16 , exhibit large deviation or condensation effects  a single node can attract @xmath1 links .",
    "interestingly , this condensation is different from the bose - einstein condensation in complex networks @xcite in that the condensation considered here corresponds only to some `` large deviation configurations . ''",
    "it is not typically expected in the ensemble .",
    "we thank m.  marsili and m.  ostilli for interesting discussions .",
    "this work was supported by nsf grants no .",
    "cns-1344289 , cns-1039646 , and cns-0964236 ; darpa grant no",
    ".  hr0011 - 12 - 1 - 0012 ; and by cisco systems .        , physics reports * 486 * , 75(2010 ) .",
    "r. milo , s. shen - orr , s. itzkovitz , n. kashtan , d. chklovskii , u. alon , science * 298 * , 824 ( 2002 ) .",
    "g. bianconi and a. capocci , phys .",
    "90 , 078701 ( 2003 ) .",
    "g. bianconi and m. marsili , epl * 74 * , 740 ( 2006 ) .",
    "m. ostilli , phys .",
    "e * 89 * , 022807 ( 2014 ) . , 28005 ( 2008 ) .",
    "m. molloy and b. reed , random structures algorithms , * 6 * , 161 ( 1995 ) .",
    "f. chung and l. lu , internet mathematics , * 1 * , 91 ( 2004 ) .",
    "g. caldarelli , a. capocci , p. de los rios , m. a. munz , phys .",
    "89 * , 258702 , ( 2002 ) .",
    "m. boguna , and r. pastor - satorras , phys .",
    "lett . * 68*,036112 ( 2003 ) .    ,",
    "e10012 ( 2010 ) . , 178701 ( 2011 ) . .",
    "bouchaud and m. potters , _ theory of financial risk and derivative pricing : from statistical physics to risk management _ , ( cambridge university press , cambridge , 2003 ) .",
    "g. bianconi and a .-",
    "barabasi , phys .",
    "lett . * 86 * , 5632 ( 2001 ) ."
  ],
  "abstract_text": [
    "<S> the entropy of network ensembles characterizes the amount of information encoded in the network structure , and can be used to quantify network complexity , and the relevance of given structural properties observed in real network datasets with respect to a random hypothesis . in many real networks </S>",
    "<S> the degrees of individual nodes are not fixed but change in time , while their statistical properties , such as the degree distribution , are preserved . here </S>",
    "<S> we characterize the distribution of entropy of random networks with given degree sequences , where each degree sequence is drawn randomly from a given degree distribution . </S>",
    "<S> we show that the leading term of the entropy of scale - free network ensembles depends only on the network size and average degree , and that entropy is self - averaging , meaning that its relative variance vanishes in the thermodynamic limit . </S>",
    "<S> we also characterize large fluctuations of entropy that are fully determined by the average degree in the network . finally , above a certain threshold , </S>",
    "<S> large fluctuations of the average degree in the ensemble can lead to condensation , meaning that a single node in a network of size  @xmath0 can attract @xmath1 links . </S>"
  ]
}