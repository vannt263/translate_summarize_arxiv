{
  "article_text": [
    "the numerical simulation of multiscale phenomena , such as fluid flows in porous media and wave propagation in nano - structures , provides effective ways to solve relevant problems in areas such as energy , climate / weather , and health . in this context",
    ", the use of finite element methods ( fem ) is particularly disseminated in industry and academia to simulate these phenomena . despite the fact that classical fem methods  ( e.g.  the galerkin method ) are appropriate for many different problems , their accuracy may seriously decay when they face problems with highly multiscale features . because of that",
    ", many modern fem techniques have emerged  @xcite . among them , of particular interest to this paper , is the family of multiscale hybrid - mixed ( mhm ) methods originally proposed in  @xcite . from a mathematical viewpoint ,",
    "the mhm methods naturally incorporate multiple scales ( the mhm - levels ) and provide solutions with high - order precision on coarse meshes . from a computational viewpoint , at a lower mhm - level a set of completely independent _ local _ problems",
    "can be easily computed in parallel to provide information to its upper mhm - level ( the so - called _ global _ problem ) . as a result",
    ", the mhm methods are naturally shaped to be used in parallel computing environments . thereby , the mhm methods appear as a highly competitive option to handle realistic multiscale boundary value problems ( bvps ) .",
    "the mhm methods have been numerically validated for problems such as the darcy equation  @xcite , the stokes / brinkman models @xcite , the advective - reactive dominated problems  @xcite , and the linear elasticity problem  @xcite . in this paper , we present and evaluate two simulator prototypes specifically crafted for the mhm methods , which adopt two different implementation approaches :    a multi - programming language approach with dynamic process scheduling ; and    a single - programming language approach with static process scheduling .    specifically , we use c++ for numerical computation of the global and local problems in a modular way . as for process distribution in the simulator",
    ", we employ the erlang concurrent , actor - based language integrated with the c++ modules in the first approach , and a mpi - based c++ application in the second approach .",
    "the aim of exploring these different approaches is twofold :    to allow for the deployment of the simulator both in conventional high - performance computing ( hpc ) environments ( with mpi ) and in cloud computing environments ( with erlang ) ; and    to pave the way for further exploration of quality attributes related with software productivity and fault - tolerance , which are key to exascale systems  @xcite .",
    "we present a preliminary performance evaluation of the two simulator prototypes , taking into account their strong and weak scalability efficiency .",
    "we show that , although not as fast as the mpi version , the erlang version does provide fairly good speedups up to a few hundred cores , even with all node and link failover mechanisms being set up .",
    "besides , despite the lack of formal evidence , it is worth mentioning erlang s case for software productivity , since it took one of the authors 4 months worth of coding in the erlang part , from learning a whole new , actor - based programming language to implementing a fully - fledged application with all node and link failover mechanisms enabled .",
    "the remainder of this paper is structured as follows .",
    "section  [ sec : mhm ] overviews the family of mhm methods .",
    "section  [ sec : primitives ] presents the key computing primitives of the mhm simulator and how they relate to the algorithm implied by the mhm methods .",
    "section  [ sec : impl ] shows the architecture and key design principles that underlie the two different implementations of the simulator .",
    "section  [ sec : eval ] offers a performance evaluation of these two simulator implementations .",
    "finally , section  [ sec : conc ] presents some concluding remarks and perspectives for future work .",
    "to present the mhm methods in a compact way , we consider the bvp to find @xmath0 , @xmath1 , so that @xmath2 in an open - bounded domain @xmath3 with prescribed homogeneous dirichlet boundary condition on the polygonal boundary @xmath4 . hereafter @xmath5 is a bounded linear operator .",
    "some definitions will be needed in the sequel .",
    "first , let @xmath6 be a familiy of regular triangulations of @xmath3 such that each mesh @xmath7 decomposes in a set of globally - indexed elements @xmath8 , with @xmath9 .",
    "this set defines a _ mesh _ with @xmath10 elements .",
    "the label @xmath11 refers to the _ mesh size _ , where @xmath11 is the maximum diameter of any element in the mesh .",
    "each element @xmath12 has a boundary @xmath13 consisting of a set of faces @xmath14 , and some of these faces ( those who are internal to the mesh ) may be shared between two elements .",
    "denote by @xmath15 the set of @xmath13 , and by @xmath16 the set in the triangulation of all globally - indexed faces @xmath17 , with @xmath18 .",
    "@xmath16 defines the _ skeleton _ of a mesh with @xmath19 faces .",
    "figure  [ fig : mesh ] illustrates these concepts for @xmath20 .",
    "we associate each face of @xmath21 to a unitary normal vector @xmath22 , taking care to ensure this is facing outward on @xmath4 .",
    "also , the unitary outward normal vector on a face @xmath14 of element @xmath12 is represented by @xmath23 .    .",
    "]    second , we define some function spaces :    1 .   @xmath24 is the null space ( with basis @xmath25 ) of operator @xmath26 for all @xmath27 ( e.g. , for the darcy equation @xmath24 is formed by piecewise constants in each @xmath12  @xcite , and then @xmath28 . ) ; 2 .",
    "@xmath29 is the orthogonal complement of @xmath24 such that it yields the wellposedness of @xmath26 for all @xmath30 ; 3 .",
    "@xmath31 is the space defined as the direct sum between @xmath24 and @xmath32 ( e.g. , for the darcy equation @xmath33  @xcite ) ; 4 .",
    "@xmath34 is the space containing the lagrange multipliers defined over @xmath15  @xcite .",
    "for these spaces , we also define the @xmath35 inner products @xmath36 and @xmath37 , respectively .",
    "third , we define the following finite - dimensional space needed for approximating the exact field @xmath38 :    * @xmath39 is the space of discontinuous polynomial functions on @xmath40 of degree less than or equal to @xmath41 and defined on a uniform partition of @xmath42 , composed of @xmath43 elements ( @xmath44 ) , with basis @xmath45 .    given these definitions , a bvp can be written within the mhm terminology in terms of a collection of local problems ",
    "each @xmath12 being associated with a different local problem  brought together by a global problem defined on @xmath7 .",
    "specifically , the global formulation is as follows : find @xmath46 such that @xmath47 for all @xmath48 .",
    "@xmath49 is the total number of degrees of freedom ( dofs ) for @xmath50 , and its value depends on the adopted degree @xmath51 of the piecewise polynomials in @xmath52 , the total number of partitions @xmath43 in each face @xmath42 , and the total number of faces @xmath19 in the mesh . in the above formulation",
    ", @xmath53 is given ; @xmath54 and @xmath55 are linear bounded operators driven by the local problems .",
    "figure  [ fig : dofs](a ) illustrates an example of the distribution of the dofs for @xmath56 in the case of the darcy equation and @xmath57 ( with @xmath58 and @xmath59 ) in the mesh of figure  [ fig : mesh ] .",
    "the local operators @xmath54 and @xmath55 in are responsible for the upscaling process . to precise them , let @xmath60 be a finite - dimensional space ( with basis @xmath61 ) defined within an element @xmath62 in the mesh .",
    "at the local level , each element @xmath62 is considered a domain on its own and as such may have an internal triangulation , thus defining a _ submesh _ within @xmath62 .",
    "each of these submeshes may have a different characteristic size @xmath63 , depending on the high - contrast aspects of the coefficients involved in operator @xmath5 for each @xmath62 .",
    "the number of dofs @xmath64 in @xmath65 will therefore depend on the degree of the polynomial functions in @xmath65 and @xmath66 .",
    "for the purposes of this paper , we assume the solution to each local problem is approximated using the continuous galerkin method with a lagrange basis for @xmath67 ( the space of piecewise polynomials of degree less than or equal to @xmath68 ) ; @xmath64 is then determined by the degree @xmath68 of the polynomials and the number of elements in the submesh . as such",
    ", we employ the bilinear form @xmath69 to set up operators @xmath54 and @xmath55 .",
    "it is defined ( as usual ) as the action of @xmath70 on @xmath71 in @xmath62 , with @xmath72 .",
    "figure  [ fig : dofs](b ) illustrates an example of @xmath65 with quadratic interpolation @xmath73 for element @xmath74 in the mesh of figure  [ fig : mesh ] . in the example",
    ", @xmath74 has been internally triangulated generating a submesh with 4 elements .",
    "each local problem comprises a set of local _",
    "subproblems_. one such subproblem defines @xmath55 by computing @xmath75 in each @xmath76 as follows : find @xmath77 such that @xmath78    the other @xmath79 subproblems define @xmath54 by computing @xmath80 from the following local problems : for @xmath81 , find @xmath82 such that @xmath83 where @xmath84 changes its sign accordingly to @xmath85 , and @xmath79 is defined by the number of faces in @xmath62 and by the degree @xmath51 of the polynomials and @xmath43 in @xmath50 .",
    "finally , from the solution of equations  , and , and using that @xmath86 with @xmath87 , the exact field @xmath38 is approximated by @xmath88.\\end{aligned}\\ ] ]",
    "the mhm simulator is built on top of key computing primitives that bear correspondence with the major mathematical building blocks of the mhm method : splitproblem , solvelocalproblem , reducelocalproblems , and computesolution .",
    "these primitives are combined to implement the mhm simulator as depicted abstractly in algorithm  [ alg : main ] .",
    "@xmath89 @xmath90 @xmath91 @xmath92 @xmath93 ( @xmath94 @xmath95    the splitproblem primitive divides the original problem posed over a mesh resulting from a triangulation @xmath7 into a global problem ( the 1^st^ mhm - level ) and a set of local problems ( the 2^nd^ mhm - level ) , one for each element @xmath62 in the mesh .",
    "this primitive returns a set @xmath96 of pairs @xmath97 , where @xmath98 represents the set of all faces of @xmath62 .",
    "the solvelocalproblem computes the contributions @xmath99 and @xmath100 from a specific element @xmath62 in the mesh to be used in the global problem .",
    "functions @xmath99 and @xmath101 , @xmath81 are searched as the solution of and , respectively , in the following form : @xmath102 the coefficients @xmath103 and @xmath104 are the actual numerical results computed by the solvelocalproblem primitive through the solution of a set of linear systems of the form @xmath105 and @xmath106 .",
    "it is worth noting that these systems share the same matrix @xmath107_{i , j \\in \\left\\{0 , \\dots , d_{k_t}-1\\right\\}}$ ] , albeit with different load vectors .",
    "this feature means that @xmath108 needs to be factorized only once ( for linear problems ) , and therefore the increase in the value of degree @xmath51 or @xmath43 in the approximation space @xmath50 only increases the number of matrix - vector multiplications , which can be trivially parallelized even within the context of a single local problem by means of vectorized computations . moreover , if the coefficients associated with the operator @xmath5 are not time - dependent , @xmath108 needs to be assembled and factorized only during the first timestep in a time marching scheme .",
    "the reducelocalproblems primitive gathers the contributions @xmath99 and @xmath100 from the subproblems to compute @xmath56 and @xmath57 from equation .",
    "the underlying linear system associated to reads : @xmath109 with : @xmath110_{i , j \\in \\left\\{0,\\dots , l_l-1\\right\\}},\\\\ \\mathbf{\\underline{b } } : = [ ( \\psi_j , \\upsilon_i)]_{i \\in \\left\\{0,\\dots , n_{v_0}-1\\right\\ } , j \\in \\left\\{0,\\dots , l_l-1\\right\\}},\\\\ \\mathbf{e } : = [ -(\\psi_j , \\eta_t^f)]_{j \\in \\left\\{0,\\dots , l_l-1\\right\\}},\\\\ \\mathbf{f } : = [ ( \\upsilon_j , f)]_{j \\in \\left\\{0,\\dots , n_{v_0}-1\\right\\}}. \\end{array}\\ ] ]    the computed entries @xmath111_{j \\in \\{0,\\dots , l_l-1\\}}$ ] in @xmath112 define the solution to @xmath57 in @xmath50 ; likewise , the computed entries @xmath113_{j \\in \\{0,\\dots , n_{v_0}-1\\}}$ ] in @xmath114 define the solution @xmath56 in @xmath24 through @xmath115 finally , owing to the computesolution primitive , the two - level numerical solution @xmath116 reads @xmath117.\\ ] ] where we used and in .",
    "the mhm simulator is a distributed system . it exploits either erlang s or mpi support for scalable computing and c++ s efficiency for the numerical computations of the global and local problems .",
    "the overall architecture of the mhm simulator is depicted in figures  [ fig : architecture](a ) and [ fig : architecture](b ) for the erlang and mpi versions , respectively .",
    "the following subsections describe the two `` layers '' ( distribution and computation ) of the architecture .        before describing this version of the distribution layer , we give a short introduction to the erlang language and its runtime , including its fault - tolerance mechanisms .    * an erlang prime .",
    "* erlang is a concurrent language with functional programming features .",
    "it is a language that compiles to an intermediate format ( _ byte codes _ ) and runs in virtual machines ( the erlang _ nodes _ ) . typically , each erlang node runs on a different physical machine .",
    "erlang programs are built on functions that can be called from the same process of the calling function or dispatched ( _ spawned _ ) to another process residing in the same or a different erlang node .",
    "processes in erlang run concurrently and communicate with each other by message passing .",
    "they usually follow a common pattern : once spawned they call a tail - recursive function to process and produce data .",
    "this data is referred to as the process _",
    "state_. the tail - recursive function usually takes the following sequence of operations :    receive a message from another process ,    handle the message ,    update the process state , and    pass the updated state to a new invocation of the function through its tail - recursive call .",
    "finally , a special _ stop _ message is usually defined to allow the receiving process to be terminated gracefully .",
    "the erlang runtime provides templates  called _ behaviors_that implement the above pattern for processes acting as servers ( the ` gen_server ` behavior ) and finite state machines ( fsms  the ` gen_fsm ` behavior ) , among others .",
    "building on its messaging mechanism , erlang offers process monitoring capabilities that allow a process to detect whether another process has failed and act accordingly .",
    "this is explored in an specific behavior called ` supervisor ` .",
    "a supervisor is a process whose main task is to manage  spawn , monitor , and eventually re - spawn  other processes in case of failure .",
    "supervisors may manage any type of erlang process , including other supervisors .",
    "this organization creates what is called a _",
    "supervision tree_. finally , an erlang _ application _ is a special kind of behavior that allows an entire supervision tree to be started and stopped as a unit . to provide tolerance to hardware faults",
    ", an erlang application can be controlled in a distributed manner , by employing two or more erlang nodes running in different machines .",
    "this setup configures a _ distributed application_.",
    "if the erlang node where a distributed application goes down ( e.g. because of a network partition or machine failure ) , the application can be restarted at another node .    *",
    "the simulator distributed application*. figure  [ fig : architecture](a ) illustrates the chief processes of the erlang - based simulator distributed application .",
    "the _ main supervisor _ implements the ` supervisor ` behavior .",
    "it manages a _ master _ , a _",
    "regis server _ , and a _ slave supervisor _ , which are all spawned at the application bootstrap .",
    "all these processes run in a single erlang node ; if this node becomes unavailable , they are resumed in another erlang node with their state preserved thanks to the use of the mnesia fault - tolerant database embedded in the erlang runtime .",
    "this is the first key feature that allows the erlang - based simulator to have a very fine - grained fault tolerance support .",
    "the master implements the ` gen_server ` behavior .",
    "it keeps track of a set of _ computing tasks _ , which are dynamically scheduled to the available computing resources , as described below .",
    "each task is related to either a global or a local problem in the mhm methods , and has an associated state``not solved '' , `` being solved '' , `` being reduced '' ( for global problems only ) or `` solved '' . besides , each task maps onto an instance of the simulator primitives described in section  [ sec : primitives ] .",
    "these primitives , which are implemented in c++ ( see section  [ subsec : cpp ] ) , run on computing resources managed by the regis server , which also implements the ` gen_server ` behavior .",
    "the regis server collects status information about the computing resources in the execution platform ( e.g.  computing nodes in an hpc cluster or virtual machines in a cloud ) and , for each such resource , asks the slave supervisor ( another implementation of the ` supervisor ` behavior ) to dynamically create / destroy new _",
    "slaves _ associated with the resource whenever the resource becomes available / unavailable .",
    "this is the second key feature for the fine - grained fault tolerance support in the erlang - based simulator .",
    "the slaves implement the ` gen_fsm ` behavior .",
    "they run in the same erlang node as the other processes of the erlang application .",
    "once started , an slave keeps on asking the master about new computing tasks .",
    "the master replies to a request from a slave by :    selecting an appropriate task ,    changing the state of the task to `` being solved''/``being reduced '' , and    replying to the slave with the simulator primitive to be executed .",
    "the slave then dispatches the execution of the primitive onto its associated computing resource , and keeps track of the primitive execution .",
    "when the slave detects that the primitive is finished , it sends the results of the primitive to the master , which may change the state of the task or create new tasks . if the slave detects that the primitive hasnt completed because of a failure in a computing resource , it notifies the master and the regis server , which dynamically reallocate the task to a different resource .",
    "figure  [ fig : task_states ] illustrates how the state of the tasks evolves in the simulator .",
    "the simulator starts with a single task representing a global problem@xmath7 in algorithm  [ alg : main]being published in the master with the `` not solved '' state .",
    "at this point a single slave will be able to get this task , which triggers the dispatching of the splitproblem primitive to its associated computing resource and the changing of its state to `` being solved '' .",
    "the result of this primitive@xmath96 in algorithm  [ alg : main]is taken by the slave as a set of requests ",
    "one for each @xmath118for the creation of new tasks in the master .",
    "each of these new tasks is related to a different local problem and added to the master with the `` not solved '' state . to simplify the illustration , in figure  [ fig : task_states ] we only present two local problems being derived from the splitproblem primitive .        from this point on the slaves start to get different tasks related with local problems , changing their states to `` being solved '' and dispatching the execution of the solvelocalproblem primitive to their associated computing resources .",
    "we employed the sequential lu factorization implementation of the eigen library within this primitive for solving the linear systems that compute @xmath103 and @xmath104 in equation  , with the intention to make as many local problems be solved in parallel in a single computing resource as the amount of cores the resource provides .",
    "when these primitive invocations are finished , the slaves send their results@xmath119 in algorithm  [ alg : main]to the master , which changes the state of their corresponding tasks to `` solved '' .",
    "the slaves then proceed with getting other tasks and dispatching them to their resources until there are no more tasks in the `` not solved '' state .",
    "once all local problems are in the `` solved '' state , the next request from an slave is replied with the reducelocalproblems primitive , and the state associated with the global problem is changed from `` being solved '' to `` being reduced '' .",
    "again , we employed the lu factorization within this primitive to solve the linear system in equation  . for the reducelocalproblems",
    "primitive , however , we used the shared - memory parallel pardiso solver . the result from the reducelocalproblems primitive triggers the immediate execution of the computesolution primitive and a changing of the task s state to `` solved '' in the master .",
    "after that , the simulator finishes .",
    "figure  [ fig : architecture](b ) illustrates the chief processes of the mpi - based application .",
    "the @xmath120 mpi processes that make part of the application are omnipotent ; in contrast with the erlang version , there s no master or slave process . each mpi process @xmath121 is responsible for running its own _ share _ of the splitproblem primitive .",
    "this share corresponds to the execution of splitproblem over a partition @xmath122 of @xmath7 .",
    "such a partitioning configures a static process scheduling that is kept until the end of the simulation .",
    "the result of the splitproblem primitive at each mpi process ( @xmath123 ) is taken by this process as a set of local problems to be solved ",
    "one for each @xmath124 . from this point on the mpi processes start to execute the solvelocalproblem primitive for all @xmath125 they are responsible for , in the same manner as the erlang version .",
    "an mpi barrier is used for making all mpi processes wait for each other as they finish solving all their corresponding local problems . once all local problems are solved",
    ", each mpi process @xmath121 is responsible for running its own share of the reducelocalproblems primitive .",
    "this is where the mpi version of the mhm simulator mostly differs from the erlang counterpart .",
    "a share of the reducelocalproblems primitive corresponds to the distributed assembly of the global linear system shown in equation  , and its solution through ldl@xmath126 factorization using the distributed interface of the parallel pastix library  @xcite .",
    "the result from the reducelocalproblems primitive triggers at each mpi process the immediate execution of its own share of the computesolution primitive .",
    "after that , the simulator finishes .    as it can be seen from figure  [ fig : architecture](b ) , this implementation is structurally simpler than the one of the erlang - based simulator .",
    "most of such simplicity is due to the lack , in this implementation , of a dynamic process scheduling scheme and of any finer - grained fault - tolerance mechanisms .",
    "this means that a failure in a single computing resource running an mpi process crashes the whole application .",
    "in addition , checkpoints for the fast restart of a crashed application are only available in two points : at the end of the splitproblem primitive , and at the end of all invocations of the solvelocalproblem primitive .",
    "therefore , a failure during the solution of a single local problem results in _ all _ local problems being solved again , but if all local problems have been solved , the application can be restarted from the point of the reducelocalproblems primitive .",
    "c++ is used for implementing the numerical computations that ultimately solve the global and local problems .",
    "the c++ layer is divided into 2 packages .",
    "the first package implements key concepts present in any fem - based simulator as c++ classes :    geometric elements ( ` element ` ) and meshes ( ` mesh ` ) ;    quadrature rules for numerical integration ( ` quadrature ` ) ;    spaces of approximate functions ( ` space ` ) ;    basis function definitions for elements ( ` brick ` ) and meshes ( ` block ` ) ;    coefficient and source term definitions ( ` simpledata ` ) ;    contribution computations ( ` localcontribset ` ) ; and    a wrapper to linear solver libraries ( ` linearsolver ` ) .    figure  [ fig : uml_basic ] shows a uml diagram with the main relationships between these classes .",
    "the ` block ` class is an abstract class ; it must be specialized in classes depending on where the data comes from . the ` galerkinlagrangeblock ` class shown in figure  [ fig : uml_basic ] , for instance , compute its values from the definitions of basis functions in ` finiteelementspace ` .",
    "the second package comprises a set of c++ classes that implement the framework for mhm methods :    global problem definitions ( ` problem ` ) ;    local problem definitions ( ` localproblem ` and its direct subclasses ` stationarylocalproblem ` and ` transientlocalproblem ` ) ;    definitions of spaces @xmath24 and @xmath50 ( ` mhmspace ` and subclasses of ` localspace ` ) ; and    a subclass of block ( ` mhmblock ` ) .",
    "figure  [ fig : uml_mhm ] presents a uml diagram that depicts the main relationships between these classes .",
    "the ` globalproblem ` class implements the simulator for equation  .",
    "its methods ` globalproblem::split ( ) ` , ` globalproblem::reduce ( ) ` , and ` globalproblem::solve ( ) ` correspond to the primitives splitproblem , reducelocalproblems , and computesolution in algorithm  [ alg : main ] .    `",
    "localproblem ` is an abstract class responsible for :    defining a submesh within an element @xmath76 ,    solving a local problem in this submesh , and    computing the local contributions related with this submesh to the global problem .",
    "its main method`localproblem::solvelocalproblem()`corresponds to primitive solvelocalproblem in algorithm  [ alg : main ] .",
    "the subclasses ` stationarylocalproblem ` and ` transientlocalproblem ` specialize the behavior of this method to stationary and transient problems .",
    "specializations of these two subclasses then override ` solvelocalproblem ( ) ` to provide concrete behavior .",
    "for instance ,",
    "the ` diffusionsubproblem ` class specializes ` stationarylocalproblem ` to solve the following local subproblems , which particularize the general equations   and for the darcy equation  @xcite : find @xmath77 such that @xmath127 and for @xmath81 , find @xmath82 such that @xmath128 where @xmath129 is the diffusion coefficient .",
    "we collected strong and weak scaling performance measurements for the two simulator prototypes .",
    "for these measurements , we ran the two prototypes in the santos dumont hpc cluster at lncc in brazil , which has the following per - node configuration : 2x cpu intel xeon e5 - 2695v2 ivy bridge ( 12 cores each cpu ) , 2.4ghz , 64 gb ddr3 ram .",
    "the nodes of the cluster are interconnected through an infiniband fdr network with a fat - tree topology , and they share a distributed file system based on lustre v2.1 . the c++ code was compiled with intel s icpc compiler , version 16.0.2 build 20160204 , with the optimization flags ` @xmath130 ' and ` @xmath131 ' enabled .",
    "we used eigen version 3.3-rc1 , pastix version 5.2.2.22 , and the pardiso implementation provided with the mkl library that comes with intel s parallel studio xe 2016.2.062 .",
    "the erlang code was run with ericsson s open source erlang / otp system , version 17 erts-6.1 , with hipe ( high performance erlang ) native code compilation enabled .",
    "the mpi implementation used was the one that also comes with intel s parallel studio xe 2016.2.062 .",
    "we solve the darcy equation defined in the unit cube as a model problem .",
    "the diffusion coefficient is set as @xmath132 and @xmath133 .",
    "as such , the exact solution reads @xmath134 .",
    "we setup the mhm simulators with 24 , 192 and 1,536 elements in a global tetrahedral mesh and within each such element a submesh of either 59 or 6,046 tetrahedra dynamically generated by the tetgen library  @xcite during runtime , as part of the computation within the solvelocalproblem primitive .",
    "we adopted degree @xmath135 and @xmath136 ( 59 tetrahedra case ) or @xmath137 ( 6,046 tetrahedra case ) for the polynomials in @xmath50 , and degree @xmath138 for the polynomials in the submesh .",
    "we depict the 192 tetrahedra mesh along with its submeshes ( 59 tetrahedra per element ) in figure  [ fig : simulation3d](a ) , and the corresponding numerical solution to our model problem in figure  [ fig : simulation3d](b ) .    for the sake of illustrative comparison , we present in table  [ tab : galerkin_corr ] the number of dofs that the standard finite element ( galerkin ) method should have to reach a similar approximation order ( in terms of accuracy ) compared to the one using the mhm methods . as shown in  @xcite , since we are using @xmath135 and @xmath138 with the mhm methods , the numbers of dofs in the table correspond to the use of the linear space @xmath139 with the galerkin method .",
    "we emphasize that , different from what happens within the mhm methodology , the standard approximation using galerkin needs to compute all such dofs at once .",
    "this is achieved by solving a single linear system which is typically much larger than those generated by the global and local problems of the mhm methods .",
    ".correspondence between mhm parameterization and galerkin dofs for @xmath139 .",
    "[ cols= \" < , < , < , < \" , ]     [ tab : measurements ]    the entries in table  [ tab : measurements ] that are in bold and marked with a ` @xmath140 ' can be also used for a weak scalability analysis .",
    "we first point out that from the @xmath141 to the @xmath142 configuration , using the erlang version , there was an increase of the time of execution by a factor of @xmath143 , so the weak scalability efficiency was of @xmath144 . as for the mpi version , there was an increase of this time by a factor of @xmath145 , with an efficiency of @xmath146 .",
    "we must nevertheless compare these efficiency assessments with 3 different ways to observe the increase in the size of the corresponding problems :    1 .",
    "taking the total number of tetrahedra in each configuration : from the @xmath141 to the @xmath142 configuration there is an increase of @xmath147 times the number of tetrahedra ; 2 .",
    "taking the correspondence with galerkin dofs for @xmath139 as presented in table  [ tab : galerkin_corr ] : from the @xmath141 to the @xmath142 configuration there is an increase of @xmath148 times the number of dofs ; 3 .",
    "taking the approximation error in the @xmath35-norm of the computed solution : from the @xmath141 to the @xmath142 configuration there is a reduction of @xmath149 times the error to the exact solution ( from @xmath150 to @xmath151 ) .    from the numbers above we can verify that the two simulator prototypes scaled well , with a small increase in the time of execution when compared with the large increase in the size of the problems .",
    "in this paper we presented our ongoing work on the implementation of scalable simulators specifically crafted for the mhm methods .",
    "we presented a preliminary evaluation of our two simulator prototypes , whose results are promising with regard to its scalability . these results also show that erlang s overhead was small in comparison with its benefits in terms of productivity and extensive support for fault tolerance .    for future work",
    ", we will be investigating the use of coprocessors ( gpgpus and mics ) to reduce the time taken by the execution of the large amount of solvelocalproblem primitive instances in the simulators .",
    "we also intend to experiment with other parallel sparse linear solvers than pastix to solve equation  [ eq : mat_global ] in the reducelocalproblems primitive for the case of the mpi version .",
    "in particular , the hybrid approaches presented in  @xcite combine direct and iterative methods and seem fit to solve linear systems similar to the one illustrated in equation  ( [ eq : mat_global ] ) .",
    "these research results have received funding from the bull / france , eu h2020 programme and from mcti / rnp - brazil under the hpc4e project , grant agreement n 689772 .",
    "the simulations presented herein used the hpc resources provided by lncc s sdumont supercomputer ( http://sdumont.lncc.br ) .",
    "e.  agullo , l.  giraud , a.  guermouche , a.  haidar , and j.  roman .",
    "parallel algebraic domain decomposition solver for the solution of augmented systems .",
    ", 6061:23  30 , 2013 .",
    "civil - comp : parallel , distributed , grid and cloud computing .",
    "yalchin efendiev , raytcho lazarov , minam moon , and ke  shi .",
    "a spectral multiscale hybridizable discontinuous galerkin method for second order elliptic problems .",
    ", 292:243  256 , 2015 . special issue on advances in simulations of subsurface flow and transport ( honoring professor mary f. wheeler ) ."
  ],
  "abstract_text": [
    "<S> the family of multiscale hybrid - mixed ( mhm ) finite element methods has received considerable attention from the mathematics and engineering community in the last few years . </S>",
    "<S> the mhm methods allow solving highly heterogeneous problems on coarse meshes while providing solutions with high - order precision . </S>",
    "<S> it embeds independent local problems which are responsible for upscaling unresolved scales into the numerical solution . </S>",
    "<S> these local contributions are brought together through a global problem defined on the skeleton of the coarse partition . </S>",
    "<S> since the local problems are completely independent , they can be easily computed in parallel . in this paper , we present two simulator prototypes specifically crafted for the mhm methods , which adopt two different implementation strategies :    a multi - programming language approach , each language tackling different simulation issues ; and    a classical , single - programming language approach .    </S>",
    "<S> specifically , we use c++ for numerical computation of the global and local problems in a modular way ; for process distribution in the simulator , we adopt the erlang concurrent language in the first approach , and the mpi standard in the second approach . </S>",
    "<S> the aim of exploring these different approaches is twofold :    allow for the deployment of the simulator both in high - performance computing ( with mpi ) and in cloud computing environments ( with erlang ) ; and    pave the way for further exploration of quality attributes related to software productivity and fault - tolerance , which are key to exascale systems .    </S>",
    "<S> we present a performance evaluation of the two simulator prototypes taking into account their efficiency . </S>"
  ]
}