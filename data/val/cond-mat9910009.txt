{
  "article_text": [
    "multi - state attractor neural networks in which the units ( neurons ) can be in more than two states are , in general , more flexible and efficient biological or artificial devices than networks of binary units .",
    "much work has been done over some time on the retrieval problem in multi - state networks of various architectures , with either simple or hierarchical patterns in more than two states .",
    "the retrieval problem consists in the recognition of patterns that have been stored in a network by means of a learning ( or training ) rule , when the network is set in an appropriate initial state to start its operating stage @xcite .",
    "thus , the retrieval problem deals with the memorization ability of a network .",
    "the networks that have been considered are the dilute , the layered feed - forward and the fully connected networks @xcite .",
    "more recently , some work has been done on the categorization problem in multi - state attractor networks @xcite , following extensive studies of the problem in binary networks @xcite .",
    "the categorization problem consists in the spontaneous recognition of a level of hierarchical patterns other than those stored in the training process of a network @xcite .",
    "the problem deals with the ability to create a representation for concepts when the network is only exposed to examples in the training stage .",
    "some of the questions that one may ask are the following .",
    "first , one is interested in the minimal structure of the training patterns , and their number , in order to achieve a satisfactory recognition of a macroscopic number of hierarchically related ancestors .",
    "second , one would like to know the recognition rate ( number of patterns per neuron ) of these ancestors and how stable they are as attractors of the network dynamics .",
    "the recognition quality is of primary interest and one may also want to check on the robustness of the recognition process to various kinds of noise .    the simplest , and most studied case of the categorization problem , consists in the recognition of ancestors of a two - level hierarchy of ancestors and descendents trained only with the latter according to a specific learning rule .",
    "the hierarchical patterns that are generated trough a stochastic procedure @xcite lead to correlations between patterns in different levels as well as correlations between patterns in the same level @xcite . as a consequence",
    ", there is a complex structure of attractors in a network with hierarchical patterns in which the attractors may neither coincide with the training patterns nor with the ancestors , and it is of interest to study under what conditions the latter become stable attractors .",
    "patterns in more than two states , which represent a gradual coding , may have a low activity which is biologically appealing .",
    "moreover , `` small '' patterns , in which a number of bits have been turned off , are patterns of low activity that can infer patterns of full size and thereby enhance the performance of a multi - state network , as demonstrated explicitly in works on both the retrieval problem @xcite and the categorization problem .",
    "the dynamics of the latter has been studied in an extremely dilute asymmetric three - state network with a monotonic neuron firing function and a generalized hebbian learning rule @xcite .",
    "the extremely dilute network requires a vanishingly small connectivity between neurons in order to allow for an exact solution of the network dynamics , and one may ask what the behavior would be for a network with full connectivity .",
    "the purpose of the present paper is to answer some of the questions raised above investigating the equilibrium , statistical mechanics behavior for the categorization problem in a fully connected multi - state network with hierarchical patterns of low activity in a two - level hierarchy of ancestors and descendents .",
    "our aim is to obtain the phase diagrams that describe the various regimes of performance of the network in terms of the relevant parameters : the activity of the training patterns , the dynamical activity of the firing units , the correlation between ancestors and descendents , the number of descendents , the multi - state threshold and the synaptic noise level , assuming a fixed activity of the ancestors .",
    "the quality of the performance of the network is described by so - called categorization curves that express the dependence of the categorization error on some of the parameters of the model .",
    "since it is known from the results on the retrieval problem in a @xmath0-state network that the relevant phase diagrams become increasingly complex as one goes from the three to the four - state model @xcite , we consider a @xmath1-state model and a @xmath2-state ( graded response ) model .",
    "we make use of a generalized hebbian learning rule that has been used before @xcite .",
    "the outline of the paper is the following . in section",
    "ii we present the general @xmath0 ising - state model for the categorization problem .",
    "the mean - field theory for that model is summarized in section iii . in section",
    "iv we present and discuss the results for @xmath1 , in the absence or presence of synaptic noise and in section v we present the results for the @xmath2-state model .",
    "we conclude in section vi with a summary of the results .",
    "consider a network of @xmath3 nodes , @xmath4 . at the time step @xmath5",
    ", the state of the node @xmath6 is described by the variable @xmath7 , that can be in any one of the @xmath0 ising states @xmath8 in the interval [ -1,1 ] , for @xmath9 .",
    "the task to be performed by the network is the recognition of a macroscopic set of @xmath10 concepts @xmath11 , with @xmath12 , where @xmath13 is finite . during the learning stage ,",
    "only a set of @xmath14 `` small '' examples @xmath15 of each concept are presented to the network . by `` small '' examples we mean that a macroscopic number of bits in each example are turned off .",
    "the concepts are assumed to be independent identically distributed random variables with zero mean and variance @xmath16 .",
    "the examples @xmath17 of the concept @xmath18 are generated through a stochastic process based on an appropriate probability distribution @xmath19 , given below , such that @xmath20 the properties of the distribution @xmath19 will be chosen in accordance with the states of the neurons , eq .",
    "( [ 1 ] ) . for finite @xmath1 ,",
    "say @xmath21 assumes the values @xmath22 , @xmath23 or @xmath24 depending , respectively , on the example @xmath17 being either in agreement with the concept @xmath18 , being turned off , or opposite to the concept at the site @xmath6 . in the case of continuous neurons , i. e. , @xmath25 , we assume that @xmath21 is a continuous variable in the interval @xmath26 $ ] . in either case , we assume that @xmath21 belongs to a set of independent random microscopic activities with mean @xmath27 and variance @xmath28 \\delta_{ij}\\delta_{\\mu\\nu } \\label{3b}\\ ] ] with @xmath29 .",
    "the symbol @xmath30 represents the kronecker delta . in consequence",
    ", we have the following relations , @xmath31 and @xmath32a\\delta_{ij}\\delta_{\\mu\\nu}\\;. \\label{5}\\ ] ] the mean activity of the examples becomes @xmath33 for every @xmath34 and @xmath35 . according to eqs .",
    "( [ 2 ] ) and ( [ 3a ] ) , @xmath36 is the correlation between an example and the concept to which it belongs .",
    "the pure multi - state model @xcite can be obtained by taking the number of examples @xmath37 , the activity @xmath38 and the correlation @xmath39 . since @xmath40 ,",
    "the activity of the examples is not greater than the activity of the concepts . in this sense",
    ", we refer to `` small '' examples , with the effective `` size '' of the patterns being @xmath41 . in this model , the view point is that the small examples are samples of the full - activity concepts to be inferred .    in this work",
    "we are interested in the capacity of the network to infer only large concepts of full activity from the set of examples and restrict ourselves , therefore , to binary concepts , @xmath42 with equal probability , that is to say , to the case @xmath43 .",
    "this task is considered to be successful if the _ categorization _ overlap @xmath44 between the concept @xmath45 and the network state @xmath46 approaches unity after the network has reached the equilibrium state . to quantify the performance of the network",
    ", we define the categorization error for the concept @xmath34 as @xmath47 thus , @xmath48 should be small in the categorization phase and @xmath49 in the disordered phase .",
    "next we pass to the discussion the dynamics of the model , following the steps of ref .",
    "@xcite and references therein . for a given configuration @xmath46 of the network , the local field @xmath50 on site",
    "@xmath6 is @xmath51 where the synapses @xmath52 are constructed from the examples , according to the modified hebb rule @xmath53 the state of each site is updated asynchronously according to a glauber ( single spin - flip ) dynamics in which the transition probabilities are given by @xmath54 }      { \\sum_{l=1}^q\\exp[\\beta\\epsilon_j(\\sigma_l|h_j(\\{s_i(t)\\}))]}\\ ; , \\label{9}\\ ] ] where @xmath55 is the inverse temperature and the single site energy , @xmath56 , is given by @xmath57 here , @xmath58 is a non - negative constant that favors local states of small dynamical activity . in the absence of stochastic noise , the deterministic evolution of the system",
    "is ruled by @xmath59 where @xmath60 is the non - decreasing step function , for finite @xmath0 , @xmath61 \\label{11}\\ ] ] with @xmath62 and @xmath63 , in which @xmath64 , if @xmath65 and @xmath23 otherwise .",
    "the spin on site @xmath66 assumes the state @xmath67 given by eq .",
    "( [ 1 ] ) if the local field @xmath68 is bound by @xmath69 .",
    "the width of the intermediate states with constant @xmath67 for @xmath70 ( that is , excluding the limiting values of @xmath71 ) , is given by @xmath72 .",
    "thus , the width of the zero state for the three - state network studied below is @xmath73 . in the limit @xmath25 , the input - output function , eq .",
    "( [ 11 ] ) , becomes the piecewise linear function @xmath74 where @xmath75 means the minimum between @xmath76 and @xmath77 .",
    "the slope of the linear part in here is @xmath78 , which is the gain parameter of the continuous network .",
    "the equilibrium thermodynamic properties of the fully connected infinite network that follows from the above dynamics is described by the hamiltonian @xmath79 where the first sum is over all distinct pairs @xmath80 .",
    "the relevant order parameters , when the network is in the ordered sub - space of the phase space , are the _ retrieval _ overlaps @xmath81 between the actual state of the network and each one of the examples @xmath35 of each concept @xmath34 .",
    "the underlying idea in studying the categorization performance of the network is that when the number of correlated examples is higher than a critical value , for a given correlation strength , single examples are no longer local minima of the free - energy , but a mixed state having macroscopic symmetric overlap @xmath82 , for @xmath83 , with all the examples of a given concept @xmath34 , becomes a minimum .",
    "this state characterizes the categorization phase , and it yields a finite , macroscopic , overlap @xmath84 with concept @xmath34 . since we are interested mainly in the categorization ability of the network , we restrict ourselves , in what follows , to the study of configurations that have a macroscopic overlap of order @xmath85 with a mixture of a finite number @xmath14 of examples of a given concept . noting that the concepts are uncorrelated , one may concentrate on the overlap with anyone of them , say @xmath86 for @xmath87 .",
    "the free - energy per site follows as @xmath88 with the averages over examples and concepts in that order , as indicated , where @xmath89 is the canonical partition function @xmath90 in order to average over the quenched disorder , we employ the replica method , in which @xmath91 using the generalized hebb learning rule , eq .",
    "( [ 8 ] ) , and introducing a field @xmath92 , in order to generate an equation for the overlap @xmath86 , the hamiltonian eq .",
    "( [ 12 ] ) , for the replica @xmath93 , becomes @xmath94 introducing this expression in eq .",
    "( [ 132 ] ) , separating the first concept , we linearize the quadratic terms and obtain the replicated partition function @xmath95\\nonumber\\\\      & & \\quad\\times      \\sum_{\\{s_i^a\\}}\\left\\langle\\left\\langle\\exp(\\beta pgn )      \\right\\rangle_{\\{\\lambda_i^{\\mu\\rho}\\}}\\right\\rangle_{\\{\\xi_i^{\\mu}\\ } }      \\left\\langle\\left\\langle\\exp\\left\\{\\beta\\sum_{ia}\\left[\\sum_{\\rho }      m_{1\\rho}^a\\lambda_i^{1\\rho}\\xi_i^1s_i^a      \\right.\\right.\\right.\\right.\\nonumber\\\\      & & \\quad-\\left.\\left.\\left.\\left .",
    "\\frac{1}{2n}\\sum_{\\rho}(\\lambda_i^{1\\rho}\\xi_i^1      s_i^a)^2-\\theta(s_i^a)^2+h_1\\xi_i^1s_i^a\\right]\\right\\ }          \\right\\rangle_{\\{\\lambda_i^{1\\rho}\\}}\\right\\rangle_{\\{\\xi_i^1\\}}\\ , , \\label{135}\\end{aligned}\\ ] ] where @xmath96 involves the uncondensed examples .    in the thermodynamic limit ,",
    "@xmath97 we obtain following ref .",
    "@xcite , @xmath98 where @xmath99 here , @xmath100 is a matrix in the space of replicas with elements given by @xmath101 and @xmath102 thus , @xmath103 is the spin - glass order parameter and @xmath104 is the dynamical activity of the network .",
    "whereas for the binary network in the replica - symmetric theory @xmath105 , in the case of multi - state networks one has , in general , that @xmath106 .",
    "introducing , as usual , the overlap parameter @xmath107 associated to the correlation between the overlaps of the examples and concepts that do not condense , and restricting our study to the replica - symmetric solution , in which @xmath108 we obtain that the replica - symmetric free - energy per site can be re - written as @xmath109          \\nonumber\\\\      & + & \\frac{\\alpha}{2}\\left[\\frac{\\gamma_1 ^ 2qc}{(1-\\gamma_1c)^2 }      + ( s-1)\\frac{\\gamma_2 ^ 2qc}{(1-\\gamma_2c)^2}\\right ]      -\\frac{1}{\\beta}\\left\\langle\\left\\langle\\int{\\cal d}z\\ ,      \\ln\\sum_{\\{s\\}}{\\rm e}^{\\beta{\\cal h}_{eff}}\\right\\rangle      _ { \\{\\lambda^{1\\rho}\\}}\\right\\rangle_{\\{\\xi^1\\}}\\ ; , \\label{14}\\end{aligned}\\ ] ] where @xmath110 is a gaussian measure . the effective hamiltonian , @xmath111 , is given by @xmath112 where @xmath113 is an effective width of the intermediate states , as wil be seen below ( see eq .",
    "( [ 32 ] ) ) .",
    "eventually , depending on the state of the network specified by the dynamical activity @xmath114 and the spin - glass order parameter @xmath115 , @xmath116 may become negative , favoring an order with large absolute values for @xmath117 .",
    "although eqs .",
    "( [ 14])-([16 ] ) follow from the assumption of replica symmetry , we believe that such an order will exist , in general , albeit in a small region of the phase space . here , @xmath118 represents the susceptibility of the network .",
    "the parameter @xmath119 is given by the algebraic saddle - point equation @xmath120 the remaining saddle - point equations determining the order parameters are @xmath121 @xmath122 and @xmath123 in the above equations , @xmath124 the susceptibility @xmath125 remains finite , so that at zero temperature , that is @xmath126 , @xmath127 , while at finite temperature we have in general @xmath128 .",
    "the overlap with the first concept , which measures the categorization ability , is given by @xmath129 performing the configurational average in the saddle - point equations , we obtain @xmath130 for the symmetric overlap , @xmath131 and @xmath132 as well as the overlap with the concept , @xmath133 the effective transfer function @xmath134 is given by @xmath135 in the case of the three - state network , and @xmath136      -\\exp\\left[-\\phi^2_-(h_s,\\theta')\\right ] }      { { \\rm erf}\\left[-\\phi_+(h_s,\\theta')\\right ]      -{\\rm erf}\\left[-\\phi_-(h_s,\\theta')\\right]}\\ , , \\label{29a}\\ ] ] where @xmath137 for @xmath25 .",
    "thus , @xmath138 is the effective gain parameter for the continuous network .",
    "the effective field for the symmetric solution , @xmath139 , is given by @xmath140 where @xmath141 the first term in eq .",
    "( [ 30 ] ) is a signal term , while the second term is the gaussian noise due to the macroscopic number of uncondensed examples and the presence of the symmetric mixture states .",
    "the latter , in which @xmath142 ( cf .",
    "( [ 15 ] ) ) , is reduced in the case of examples of low activity , @xmath143 .",
    "one should expect , thus , an enhancement of the categorization ability of the network in that case .",
    "the above equations are obtained under the assumption that the number of examples @xmath14 is large , so that the average over examples is given by a gaussian distribution @xcite . in the following sections we discuss the results based on the solutions of the saddle - point equations for both , the three - state and the continuous network .",
    "the limit of stability of the replica - symmetric solution comes from the study of quadratic fluctuations of the free - energy in the vicinity of the symmetric saddle - point . following the almeida and thouless ( at ) analysis  @xcite",
    ", we obtain @xmath144 ^ 2      \\right\\rangle_{\\{\\lambda^{1\\rho}\\}}\\right\\rangle_{\\{\\xi^1\\ } }      \\leq 1\\ , .",
    "\\label{22}\\ ] ] as the stability condition for the replica - symmetric solution .",
    "we begin by discussing the results for the categorization performance in three - state networks in the absence of retrieval noise .",
    "the probability distribution in this case is given by @xmath145 satisfying the conditions ( [ 3a ] ) and ( [ 3b ] ) .",
    "thus , the example @xmath17 has a probability @xmath146 to be aligned with the concept , while it has a probability @xmath147 to be turned off and a probability @xmath148 to be opposed to the concept .    the effective transfer function , eq .",
    "( [ 29 ] ) , at zero temperature becomes @xmath149 from eq .",
    "( [ 16 ] ) , we see that @xmath116 may become negative .",
    "since @xmath150 is algebraically the same as @xmath151 the network acts , in this case , as a binary network at @xmath152 .",
    "accordingly , eq .  (",
    "[ 25 ] ) remains unchanged , while eqs .",
    "( [ 26 ] ) and ( [ 27 ] ) become @xmath153 and @xmath154+\\frac{1}{\\sqrt{2\\pi v } }      \\exp\\left[-\\frac{(sm_sb-\\theta'\\theta(\\theta'))^2}{2v}\\right]\\ , .",
    "\\label{34}\\ ] ] the overlap with the concept , eq .",
    "( [ 28 ] ) , is given by @xmath155 we show in fig .  [ fig1 ] the categorization phase - diagram for the case where @xmath156 , @xmath157 . with the choice that @xmath158 , we are looking in a way for an optimal phase diagram in the sense that the training examples either coincide with the corresponding concepts , that is @xmath159 , for @xmath83 , or are zero , but they are never opposed to the concept . for other values of the parameters , similar diagrams are obtained , although with lower capacity @xmath13 .",
    "the categorization phase ( c ) , characterized by @xmath160 and @xmath161 is globally stable below the heavy solid line .",
    "it becomes only locally stable , while the spin - glass ( sg ) phase is globally stable , between the heavy solid and the light solid line , where the system always jumps discontinuously to the spin - glass phase .",
    "this is in distinction with known results for the categorization phase diagram in the dilute network  @xcite , where the transition to the spin glass phase is partly continuous and partly discontinuous . above the light solid line , and at the left of the dash - dotted line , where it disappears continuously , the spin - glass phase , with @xmath162 and @xmath161 , is stable . at the right of the dash - dotted line the paramagnetic ( p ) , or _",
    "zero_-phase , with @xmath162 and @xmath163 , is stable .",
    "note that , for large threshold @xmath58 , there is a direct transition from the categorization phase to the fully disordered p phase , at low @xmath13 .",
    "there exists also a retrieval phase of examples , without categorization , not shown in the figure . since we are dealing with a large number of examples ( thus favoring the categorization ) ,",
    "that phase is present in the phase diagram only at very small values of @xmath13 and @xmath58 . to the left of the dotted line ,",
    "the effective width @xmath116 is negative . here",
    ", every non - zero value of the local field is sufficient to access the neural states @xmath164 and , in consequence , the network behaves in this region as a binary network .",
    "the dashed line signals the optimal @xmath58 , i.e. , the value of the width parameter for which the categorization overlap @xmath86 reaches its maximum value .",
    "it is interesting to note that the present phase diagram is similar to that of ref .",
    "@xcite , for the retrieval problem , with the categorization phase taking the role of the retrieval phase in that problem .",
    "an important question addressed in this paper refers to the role played by the activity of the examples , @xmath93 , on the categorization ability of the network . in fig .",
    "[ fig2 ] the categorization error @xmath165 is shown as a function of the activity , for @xmath166 , @xmath156 , @xmath167 and for several values of @xmath58 .",
    "the results reveal that @xmath165 is a monotonically increasing function of @xmath93 .",
    "since this is the general behavior for other values of the parameters , the results confirm that for the connected , as well as for the dilute  @xcite networks , it is better to train the network with low - activity examples .",
    "this can be understood noting that the activity @xmath93 of the examples is decreased when a macroscopic number of bits of every example is turned off .",
    "but in keeping the overlap @xmath36 between examples and concepts fixed , the bits that are turned off in the examples must be those that are inverted with respect to the concepts .",
    "when the activity @xmath93 reaches its minimal , optimal value , @xmath158 , the only bits that are turned on in the examples are those that are aligned with the concepts , leading to the smallest categorization error . in this case",
    "the categorization task of the network becomes similar to the reconstruction of a puzzle from loose pieces .",
    "finally , the figure also shows the discontinuous jump to the spin - glass ( sg ) phase , at the upper phase boundary of fig .",
    "[ fig1 ] .    the categorization error as a function of the number of examples @xmath14 , for @xmath168 , @xmath169 and @xmath170 and",
    "two different activities , namely @xmath158 ( all wrong bits in the examples are turned off ) and @xmath171 ( all wrong bits in the examples are included ) is shown in fig .",
    "[ fig3 ] . starting from the spin glass phase , with categorization error equal to @xmath49 , the network undergoes a discontinuous transition to the categorization phase as the number of examples increases above a critical value .",
    "the number of examples required for the jump to the categorization phase is considerable smaller for @xmath172 , than for @xmath171 .",
    "nevertheless , the final categorization error is similar for the two activities .",
    "this means that the network is able to overcome a higher amount of errors in the examples by a larger number of these examples . a higher value of @xmath14 is also required for a higher threshold @xmath58 , in order to reach a higher local field to attain the states with non - zero activity .",
    "we consider next the categorization performance obtained from @xmath134 , eq .",
    "( [ 29 ] ) , for finite @xmath173 .",
    "[ fig4 ] illustrates the influence of the temperature on the categorization error , for @xmath174 , @xmath156 , @xmath175 , and activity @xmath93 equal to @xmath176 and @xmath177 . to the left of the arrow in the curve corresponding to @xmath178 ,",
    "the categorization phase is the global minimum , while it is a local minimum to the right . in what concerns the present set of parameters ,",
    "the categorization phase for @xmath179 is a local minimum for all temperatures , whereas the spin - glass phase is the global minimum .",
    "thus , it is also advantageous for an enhancement of the performance of the network , in the presence of synaptic noise , to train the network with examples of low activity . fig .",
    "[ fig4 ] also shows the discontinuous transition to the spin - glass phase at an activity - dependent transition temperature .    the phase diagram for @xmath13 vs. @xmath180",
    "is presented in fig .",
    "[ fig5 ] for @xmath181 , @xmath156 and @xmath157 .",
    "the categorization phase is stable below the upper phase boundary , where it disappears discontinuously , becoming a global minimum below the lower phase boundary .",
    "at very small @xmath13 and @xmath180 there is a retrieval phase without categorization , not shown in the figure . the dashed line on the left is the locus of the at - line .",
    "the replica - symmetric solution for the categorization phase becomes unstable to replica - symmetry - breaking fluctuations at the left of this line .",
    "the re - entrant behavior of the upper phase boundary at low @xmath180 is associated to the instability of the replica - symmetric solution in this region .",
    "the spin - glass phase becomes a global minimum to the right of the heavy solid , and to the left of the dotted line , where it disappears continuously . at the right of the dotted line ,",
    "the paramagnetic phase is the global minimum .",
    "in this section we discuss the categorization properties of a network with continuous , monotonic neurons trained with continuous or discrete examples of binary concepts .",
    "the continuous limit is obtained by taking @xmath25 in eqs .",
    "( [ 1 ] ) and ( [ 11 ] ) .",
    "the following results are independent of the specific form of @xmath19 , provided that its mean and variance are given by eqs .",
    "( [ 3a ] ) and ( [ 3b ] ) , respectively .",
    "the general eqs .",
    "( [ 25])-([28 ] ) , for the saddle - points , apply also to this case . in the absence of noise , the effective transfer function , eq .",
    "( [ 29a ] ) , becomes the stepwise linear function @xmath182 in which @xmath138 is the effective gain parameter .",
    "consequently we obtain @xmath183\\ , , \\label{42}\\end{aligned}\\ ] ] @xmath184      \\nonumber\\\\ & & + \\frac{1}{2}\\left[1+\\frac{v}{2\\theta'^2}\\left(\\frac{1}{2 }      + \\frac{s^2m_s^2b^2}{2v}\\right)\\right]\\left[{\\rm",
    "erf}\\left(m_-\\right )      -{\\rm erf}\\left(m_+\\right)\\right ] \\label{43}\\end{aligned}\\ ] ] and @xmath185\\ , , \\label{44}\\ ] ] where @xmath186 the zero - temperature phase diagram for @xmath13 vs. @xmath58 with @xmath156 and @xmath157 is shown in fig .",
    "the categorization phase exists below the light solid line , and it is the global minimum below the heavy solid line . at the left of the dotted line ,",
    "@xmath116 is zero and the effective gain is infinite . in this region , the states @xmath164 are the only accessible states for non - zero local field and the network behaves as a binary network .",
    "in there , the critical @xmath13 for categorization assumes its value in the binary network for this set of parameters , i. e. , @xmath187 .",
    "when the network enters the multi - state , continuous regime , the categorization capacity starts to increase abruptly , and reaches its maximum value @xmath188 for @xmath189 .",
    "the dashed line signals the optimum @xmath58 for each @xmath13 .",
    "it is worth noting that for @xmath190 the optimal @xmath58 line coincides with the transition to the binary regime .",
    "this means that whenever there is a binary network capable to perform the categorization task , it will give the best categorization properties for low @xmath58 . only when @xmath191 the network with continuous neurons is expected to have a better performance .",
    "contrary to the case of finite @xmath0 , where at zero temperature the replica - symmetric solution is always unstable , there is here a region where it is stable , and this is the part of the phase diagram below the light dash - dotted line .",
    "the phase diagram illustrates that also the network of continuous neurons is robust to low gain in the states .",
    "the existence of a replica - symmetric stable phase at zero temperature was noticed in ref .",
    "@xcite , for the retrieval problem in a network of continuous neurons .",
    "finally , the heavy dash - dotted line represents the onset of the continuous spin glass transition .    in fig .",
    "[ fig7 ] we present the categorization error as a function of the activity of examples , for @xmath166 , @xmath156 , and @xmath181 to @xmath192 .",
    "since we deal with a non specified @xmath19 , the only restriction imposed is @xmath193 .",
    "we note from the figure that the categorization error is no longer a monotonic increasing function of the activity for all values of @xmath58 . for @xmath194",
    ", @xmath165 is a decreasing function of @xmath93 , for small @xmath93 .",
    "the reason is that in the case of large threshold @xmath58 , the local field @xmath195 must be sufficiently high to overcome the threshold , and this is obtained through a moderate increase in the activity of the examples .    finally , we discuss the influence of the number of examples @xmath14 on the categorization ability of networks with continuous neurons .",
    "[ fig8 ] shows the categorization error as a function of @xmath14 for @xmath157 , threshold ranging from @xmath176 to @xmath192 and @xmath166 . as a result of the continuous nature of the units , for low threshold",
    "the categorization error decreases smoothly with the increasing number of examples .",
    "this is distinct to the previous case of discrete units , where an abrupt decrease in @xmath165 was observed even at @xmath169 ( see fig .",
    "[ fig1 ] ) .",
    "furthermore , the decreasing in @xmath165 is no longer monotonic for all values of the threshold .",
    "for example , for @xmath181 there is a local maximum in @xmath165 for @xmath196 .",
    "the categorization problem , that consists of the recognition of ancestors , when a network is trained only with their descendents , is studied in this work for multi - state fully connected neural network models , keeping in mind an application to either artificial or biological networks in which the training is with sparsely coded patterns .",
    "indeed , multi - state networks offer the possibility of recognizing full - sized patterns in networks trained with `` small '' patterns , in which a macroscopic number of bits have been reduced or , eventually , set to zero reducing thereby the activity of the encoded patterns .",
    "we found that a low activity can enhance the categorization ability of a fully connected network in a significant way , by changing the threshold for firing of the units .",
    "this confirms and extends earlier results on an extremely dilute network of @xmath1-state neurons @xcite .",
    "the way the network works for the categorization task is the following .",
    "after training with correlated examples , the network searches for stable symmetric mixtures states , in place of pure examples .",
    "if these patterns have low activity , it will be less likely that they have bits with opposite sign to the corresponding concepts .",
    "the recognition of the latter from the common features of the examples will thereby be enhanced .",
    "we derived formal expressions , within replica - symmetric mean - field theory , for the free energy and the relevant order parameters for the categorization problem in a fully connected neural network model , with units in general @xmath0 ising - states and multi - state patterns belonging to a two - level hierarchy .",
    "training of the network was assumed to take place through a generalized hebbian learning rule involving only the descendents .",
    "these may be considered as corrupted examples of the ancestors ( concepts ) with a number of turned off or inverted bits .",
    "explicit results for the relevant phase diagrams and the categorization curves were then obtained for a @xmath1-state model with a monotonic activation function and for a monotonic @xmath2-state model . in the first case we also checked the robustness of the network performance to synaptic noise .",
    "our results are restricted to binary ancestors and multi - state descendents , although the case of multi - state ancestors has been considered in an extremely dilute network @xcite .",
    "the limit of validity of the replica - symmetric solution was established in this work looking for the almeida - thouless lines . for @xmath1 ,",
    "the replica - symmetric solution is unstable in the absence of synaptic noise ( @xmath152 ) and there is a re - entrant behavior for the ratio @xmath13 of recognized concepts , at small synaptic noise , in accordance with earlier results on the retrieval problem @xcite and on the categorization problem in connected networks of binary neurons @xcite .",
    "nevertheless , since the replica - symmetric solution stabilizes at very small @xmath180 , we argue that replica - symmetry breaking effects should be negligible , even at @xmath152 . on the other hand",
    ", there is a finite region of interest for the categorization performance domain where the replica - symmetric solution is stable , even at @xmath152 , in the case of the @xmath2-state network , as demonstrated explicitly in this work .",
    "to summarize , we succeeded in studying a fully connected multi - state neural network model for the categorization problem of recognizing binary concepts when the network is trained with @xmath0-state examples of low activity , in place of the full activity patterns of a binary network of states @xmath197 .",
    "the work presented here can be extended in various directions .",
    "first , to infer multi - state concepts in a network with full connectivity and to study the categorization performance for sparsely coded sequential examples . in order to come closer to biological networks , it would be interesting to consider the partial dilution of synapses .",
    "we are indebted to alba theumann for showing us how to find the almeida - thouless line for a network with hierarchically correlated patterns .",
    "we thank d. boll for comments and discussions , as well as for the kind hospitality of the institute for theoretical physics of the catholic university of leuven , where part of the work of drcd and wkt was done .",
    "this work was supported , in part , by cnpq ( conselho nacional de desenvolvimento cientfico e tecnolgico , brazil ) , and finep ( financiadora de estudos e projetos , brazil ) ."
  ],
  "abstract_text": [
    "<S> the categorization ability of fully connected neural network models , with either discrete or continuous @xmath0-state units , is studied in this work in replica symmetric mean - field theory . </S>",
    "<S> hierarchically correlated multi - state patterns in a two level structure of ancestors and descendents ( examples ) are embedded in the network and the categorization task consists in recognizing the ancestors when the network is trained exclusively with their descendents . </S>",
    "<S> explicit results for the dependence of the equilibrium properties of a @xmath1-state model and a @xmath2-state model are obtained in the form of phase diagrams and categorization curves . </S>",
    "<S> a strong improvement of the categorization ability is found when the network is trained with examples of low activity . </S>",
    "<S> the categorization ability is found to be robust to finite threshold and synaptic noise . </S>",
    "<S> the almeida - thouless lines that limit the validity of the replica - symmetric results , are also obtained .    </S>",
    "<S> epsf    [ sec : level1 ] </S>"
  ]
}