{
  "article_text": [
    "the problem of efficient communication has been extensively studied for a variety of channels and constraints .",
    "shannon has found the explicit channel capacity of the additive white gaussian noise ( awgn ) channel subject to average power constraint @xcite .",
    "this capacity is achieved by a continuous gaussian input . in practice , however , discrete constellations are often used for transmission . in this case , the probability mass function which maximizes the achievable rate subject to a given discrete constellation and average power constraint can be obtained numerically using the modified _ blahut - arimoto _ algorithm ( baa ) @xcite . * * the binary symmetric and non - symmetric channels ( bsc and bnsc , respectively ) subject to hamming power ( bounded number of 1 ) constraint are also important examples , with various applications . for bsc capacity and applications , see @xcite and @xcite . for the capacity of the noise free aperture channel ( free space optical communication ) , modeled by a special case of bnsc ( z - channel ) ,",
    "see @xcite .",
    "all these examples share single letter constraints and memoryless channels .",
    "such constraints and channels are treated in this paper .",
    "practical shaping methods have been devised over the years to approach channel capacity under constraints .",
    "convey and slone @xcite used nested lattice codes for constellation shaping .",
    "their construction was generalized by forney @xcite , subsequently leading to the trellis shaping technique for average power reduction @xcite . in this method , a structured large code is formed by nesting the shaping code with the channel code .",
    "then , given the coded bits , the free shaping bits creates a set of sequences , i.e. , a coset of the large code , from which one sequence is chosen such that the transmitted codeword has minimal power in the coset .",
    "this operation is very similar to choosing a codeword which satisfies a constraint as will be shown in the sequel .",
    "the chosen codewords from each coset form a shaped subcode , which achieves shaping gain .",
    "another example of such shaped code construction is the ldpc - ldgm scheme @xcite , originally designed for dirty paper coding .",
    "this scheme creates a shaping code for each transmitted message using a nested ldpc - ldgm structure , which is designed to be both good source code and good channel code , thus shaping is possible .",
    "the shaping codes serve as cosets . once again",
    ", the minimum energy codeword is chosen from each coset , forming the shaped subcode .",
    "the large code , as before , in the union of all the cosets representing each message .",
    "all these schemes share a mismatched decoding process .",
    "their decoder is only aware of the large codebook , unaware of the constraints and performs optimal decoding as if all codewords might have been transmitted .",
    "this decoder is suboptimal , but occasionally more practical since the adaptation to the shaped subcode might be extremely complex , and the decoder for the large code is readily available .",
    "we refer to it as the _ mismatched decoder_. the optimal decoder , which is adapted to the shaped subcode , i.e. , familiar with the constraint and is able to repeat the selection process at the receiver , is referred as the",
    "_ matched decoder_.    although these works are practical and demonstrate significant shaping gains , the analysis of their performance limit is partial . in @xcite ,",
    "forney uses `` random shaping '' arguments to suggest why the trellis shaping approaches the ultimate shaping gain so rapidly .",
    "these arguments , however , are based upon geometrical volumes analysis , which is more suitable for large constellations and high snr . the achievable rate using the matched decoder and the shaped code distribution are not addressed . for the mismatched decoding , forney and",
    "others rely on the large codebook performance . on first look",
    ", mismatch performance analysis seems as a trivial exercise .",
    "the transmitted codeword is taken from the large codebook , and therefore the large codebook performance can be used .",
    "this analysis is referred to as a _",
    "naive approach_. the large code performance analysis , however , assumes the transmission of the typical codewords , while in the shaping system we transmit ( almost ) only non - typical codewords .",
    "these codewords may have worse distance spectrum towards their neighbors and error free transmission is not guaranteed at the same noise level .",
    "thus , a more precise analysis is necessary .    in this paper , we construct random code ensembles , which satisfy constraints , via codeword selection process .",
    "the large codebook and the cosets within are generated at random from an i.i.d .",
    "distribution . in this case",
    "the `` cosets '' are random , referred from this point on as random sets .",
    "our code construction provides a framework for performance analysis of practical shaping schemes , since both good channels codes and good shaping codes behave approximately as random codes . in this paper",
    ", we find the minimal size of a random set needed to ensure that given a set of constraints , at least one codeword satisfies the constraints in each random set with probability tending towards 1 .",
    "furthermore , we obtain bounds on the achievable rates of the matched and the mismatched decoders subject to single letter constraints .    more formally , let @xmath0 be the transmission block length and @xmath1 be the transmission rate .",
    "let @xmath2 be a random codebook with rate @xmath3 , referred to as the _ large codebook_. let @xmath4 be a design parameter , called _ shaping rate _ , such that @xmath5 .",
    "the large codebook is the union of @xmath6 random sets , with @xmath7 codewords in each set , where each codeword is generated according to an i.i.d .",
    "probability mass function @xmath8 .",
    "a message @xmath9 to be transmitted chooses the random set . out of this random",
    "set a single codeword @xmath10 is selected such that it satisfies a set of single letter constraints . in this case",
    "the constraints are codeword related and can be formulated as @xmath11 where @xmath12 , @xmath13 is the number of constraints , @xmath14 are constraints and @xmath15 are bounded constraints functions .",
    "the union of the selected codewords is the shaped code @xmath16 .",
    "since each codeword satisfies the constraints , the codebook satisfies the constraints in average .",
    "the shaping rate @xmath4 , which determines the random set size , should be chosen such that a random set includes at least one suitable codeword with probability approaching 1 .",
    "a simple illustration of the selection process can be seen in fig .",
    "[ fig : random - subcode - selection ] , where the large codebook is the union of the three random sets , designated by the different colors and shapes .",
    "the constraint is plotted as the circle and one codeword is chosen from each random set such that it is inside the circle , i.e. , satisfies the constraint .",
    "these three codewords construct the shaped code .",
    "another simple example of a constraint and a selection process is as follows .",
    "let the `` constellation '' be binary and the constraint be on the average number of 1 within the codebook , say limited to @xmath17 . to construct the shaped codebook ,",
    "let the large codebook be binary and i.i.d . according to @xmath18 .",
    "we choose codewords that satisfy this constraint .",
    "thus , each codeword in the random set undergoes the following test @xmath19 using @xmath20 .",
    "if the average number of 1 in the codeword is limited to @xmath17 , then it is chosen for the shaped codebook .",
    "for example , the codeword @xmath21 is typical in the large code , since the number of zeros and ones is equal .",
    "however , it does not satisfy the constraint . on the other hand ,",
    "the non - typical codeword @xmath22 satisfies the constraint .",
    "thus , the questions in hand are how to choose the shaping rate @xmath4 to find such non - typical codewords , and what are the achievable rates using this code construction under the matched and the mismatched decoders",
    ".    random subcode construction process.,width=302,height=302 ]    the large deviation theory , which includes the sanov theorem and the conditional limit theorem @xcite , addresses the probability of very non - likely events , therefore provides some of the answers .",
    "for example , the probability to generate a codeword which satisfies the hamming constraint in ( [ eq : hamming const 1/3 ] ) from a uniform distribution .",
    "this probability dictates the minimum size of each random set , and obviously the minimal shaping rate @xmath4 required to ensure that each random set includes at least one such codeword .",
    "furthermore , the conditional limit theorem shows that the marginals of the @xmath0-dimensional distribution of the selected codewords tend toward the _ conditional limit distribution _ , which is an i.i.d .",
    "distribution @xmath23 .",
    "it is the closest distribution to @xmath8 ( in the sense of divergence ) among all the distributions which satisfy the constraints .",
    "further , we derive achievable bounds on the transmission rate under matched and mismatched decoder .",
    "starting with the matched decoder , one of the methods to prove achievable rate for a random ensemble of codes , which was generated according to an i.i.d .",
    "probability mass function , is the asymptotic equipartition property ( aep ) theorem @xcite .",
    "since the shaping method involves codeword selection and not random generation from a target distribution , we derive a modified aep theorem .",
    "we bound the probability to generate a codeword via this codebook construction using the conditional limit distribution .",
    "the requirement that each codeword satisfies the constraints is stronger than the requirement that the whole codebook satisfies the constraints in average ; since a small amount of codewords which do not satisfy the constraints do not effect the average .",
    "thus , the codebooks that are shaped using this methods are a subset of the codebooks which satisfy the constraints in average .",
    "nevertheless under matched decoding , this code construction may achieve the same rate as a code which was generated i.i.d . according to @xmath23 , i.e. , the mutual information @xmath24 .",
    "this property is shown using the modified aep .",
    "note that the rate achieved by the matched decoder is usually not the channel capacity since @xmath23 is generally not the distribution that maximizes the mutual information .",
    "for example , it can be easily shown that using an i.i.d .",
    "uniform distribution for the large code , the conditional limit distribution @xmath23 maximizes the entropy of the input and not the mutual information .",
    "hence , @xmath23 achieves only the lower bound on the capacity subject to the constraints .",
    "although it is only a lower bound , usually it is quite tight .",
    "this conjecture is based of the tightness of this bound for the single letter average power constraint over the awgn channel .",
    "this can be verified using the modified baa @xcite , since simulations show that the loss in rate due to the suboptimal distribution is insignificant in all snr regions .",
    "next , we derive two novel lower bounds on the capacity under mismatched decoding .",
    "the first is based upon gallager s random exponent . * * the key here is to perform maximum likelihood decoding using the entire codebook , while taking into account that only a subcode was transmitted and the true distribution of the transmitted codewords . based on this analysis , we redefine the average error probability and present a new error exponent . using the new error exponent , we show achievable rate for the mismatched decoder .",
    "our second bound is based upon a modified version of the joint - typicality decoder .",
    "this original decoder is not suitable for non - typical codewords , since it declares an error when the input or the output is not typical .",
    "thus , we define a modified - joint - typicality ( mjt ) decoder , which decodes solely based upon joint typicality between the input and the output with respect to the large codebook distribution",
    ". it might seem surprising that decoding according to a wrong distribution works and indeed this bound is correct in special cases only .",
    "one of them is uniform distribution of the large codebook and additive channels .",
    "therefore , the awgn channel can be treated using this approach and mjt bound can be obtained .",
    "we show result for several examples : pam with power constraints over awgn , bsc and bnsc with hamming constraints .",
    "our bounds show that for pam over the awgn channel there is no significant loss due to mismatch decoding .",
    "thus , shaping gain can be attained even under mismatched decoding .",
    "this conclusion actually justifies the works of forney and others ( unknowingly ) , who used mismatched decoding .",
    "the bsc channel is an example where the naive approach works and coincides with our bounds .",
    "it is known that the bsc capacity is attained for uniform input probability mass function , and that linear codes achieve the bsc capacity .",
    "such codes have the same distance spectrum for all the codewords and therefore the same error probability . in particular",
    ", the error probability of the non - typical codewords equals to the error probability of the typical . in this special case ,",
    "the large codebook performance is the same as the shaped code .",
    "this example provides some insight to why the naive approach is correct for bsc .",
    "the paper is organized as follows .",
    "section [ sec : preliminaries ] presents some of the preliminary background in large deviations and method of types .",
    "section [ sec : subcode - construction ] presents our subcode construction method and the summary of the main results of this paper . including the minimal shaping rate required for this code construction is presented in theorem [ thm : number of nt codewords ] .",
    "it also addresses decoder types and examples of related practical shaping schemes .",
    "section [ sec : matched - decoder ] analyzes the matched decoder .",
    "its significant contribution is the modified aep theorem , which is necessary to establish the achievable rate under matched decoding .",
    "section [ sec : mismatched - decoding ] analyzes the mismatched decoder and presents two achievable bounds , the mjt bound and the gallager bound .",
    "section [ sec : special - cases ] is dedicated to examples of channels and constraints , such as bsc and awgn subject to average power constraints .",
    "let capital letters denote random variables and vectors .",
    "let small and calligraphic letters denote their realizations and support , respectively .",
    "bold letters represent vectors .",
    "let @xmath25 be a random variable with probability mass function @xmath8 and support @xmath26 .",
    "let @xmath27 be a discrete memoryless channel and @xmath28 be a random variable with probability mass function @xmath29 and support @xmath30 , such that @xmath25 and @xmath28 are the input and output to the channel , respectively .",
    "the joint probability mass function of @xmath31 is @xmath32 . for the sake of completeness",
    ", we repeat several basic quantities , which can be found in @xcite .",
    "thus , the entropy of the variable @xmath25 is @xmath33 the conditional entropy of @xmath28 given @xmath25 is @xmath34 and the mutual information of @xmath31 is @xmath35 the mutual information has also other formulations .",
    "furthermore , let @xmath36 be another probability mass function on the same support @xmath26 .",
    "the kullback - leibler information divergence ( also relative entropy ) between @xmath8 and @xmath36 ( not symmetric ) is @xmath37 using the conventions that @xmath38 , @xmath39 and @xmath40 .",
    "next we review several definitions and theorems from the aep ( * ? ? ?",
    "* chapters 3,7 ) .",
    "[ a_px]the typical set @xmath41 with respect to the probability mass function @xmath8 is the set of sequences @xmath42 with length @xmath0 and empirical entropies @xmath43-close to the true entropy @xmath44 : @xmath45    the aep theorem ( * ? ? ?",
    "* theorem 3.1.1 ) , shows that if @xmath46 are sequences of length @xmath0 drawn i.i.d . according to @xmath8 , then : @xmath47 for sufficiently large @xmath0 .",
    "[ a_pxy]the jointly typical set @xmath48 with respect to the probability mass function @xmath32 is the set of sequences @xmath49 with length @xmath0 and empirical entropies @xmath43-close to the true entropies : @xmath50    the joint aep theorem ( * ? ? ?",
    "* theorem 7.6.1 ) , shows that if @xmath51 are sequences of length @xmath0 drawn i.i.d . according to @xmath32 , then : @xmath52 for sufficiently large @xmath0 .",
    "we also repeat some definitions and theorems from the method of types given in ( * ? ? ?",
    "* chapter 11 ) .",
    "the type @xmath53 of a sequence @xmath54 is the relative proportion of occurrences of each symbol of @xmath26 , i.e. , @xmath55 for all @xmath56 , where @xmath57 is the number of times the symbol @xmath58 occurs in the sequence @xmath42 .",
    "let @xmath59 be the set of types with denominator @xmath0 ( all possible types when the sequences have length @xmath0 ) .",
    "hence , if @xmath60 the type class @xmath61 is the set of sequences of length @xmath0 and type @xmath62 , i.e. , @xmath63    the following properties are established in ( * ? ? ?",
    "* theorems 11.1.2 - 11.1.4 ) :    1 .   if @xmath64 are drawn i.i.d .",
    "according to @xmath8 , the probability of @xmath65 depends only on its type and is given by @xmath66 2 .   for any type @xmath60 ,",
    "@xmath67 3 .   for any type @xmath60 and probability mass function @xmath8 , the probability of type class @xmath61 , where the sequences are drawn according to i.i.d .",
    "@xmath8 is given by @xmath68    the sanov theorem ( * ? ? ?",
    "* theorem 11.4.1 ) is part of the large deviation theory , which addresses the probability of very non - likely events .",
    "the theorem is formulated as follows ; let @xmath64 be drawn i.i.d . according to @xmath8 .",
    "let @xmath69 be a set of probability mass functions .",
    "then , the probability that @xmath70 belongs to the set @xmath71 is @xmath72 where @xmath73 is the probability mass function in @xmath74 that is closest to @xmath8 in relative entropy .",
    "this is the conditional limit distribution with respect to @xmath8 and @xmath74 .",
    "if , in addition , the set @xmath74 is the closure of its interior , then @xmath75    the conditional limit theorem ( * ? ? ?",
    "* theorem 11.6.2 ) strengthens the sanov theorem by showing that with high probability the types of the sequences which belong to @xmath74 are very close to @xmath23 in divergence , and the marginals tend towards @xmath23 when @xmath76 .",
    "the theorem is formulated as follows .",
    "let @xmath74 be a closed convex subset of @xmath77 and let @xmath8 be a probability mass function not in @xmath74 .",
    "let @xmath78 be discrete random variables drawn i.i.d . according to @xmath8 .",
    "let @xmath23 achieve @xmath79 . then , @xmath80 where @xmath56 and @xmath81 .",
    "as stated in the introduction , the goal is approaching the channel capacity subject to a set of single letter constraints . maximizing the average mutual information subject to these constraints",
    "obtains the maximal achievable transmission rate .",
    "let @xmath25 and @xmath28 be the input and output of a memoryless channel with probability mass function @xmath27 .",
    "let @xmath74 be a set of probability mass functions subjected to a set of single letter constraints , given by @xmath82 for @xmath83 , where @xmath74 is the closure of its interior , @xmath84 are bounded constraint functions and @xmath14 are constraints .",
    "it is well - known that i.i.d .",
    "input maximizes the average mutual information subject to single letter constraints .",
    "hence , the channel capacity is @xmath85 and the probability mass function which achieves this capacity is @xmath86    let @xmath87 designate the desirable transmission rate . the first method to create a random codebook",
    "is directly generate @xmath6 codewords according to an i.i.d .",
    "probability mass function @xmath36 , such that @xmath88 .",
    "we refer to this method as _ code construction i _ with respect to @xmath36 .",
    "the maximal reliable transmission rate can obviously be achieved using code construction _",
    "i _ and @xmath89 .    generating a practical good codebook with a specific probability mass function is not trivial .",
    "thus , the shaping method can be used . in this method ,",
    "a large codebook is generated according to an i.i.d .",
    "uniform probability mass function and then codewords which satisfy a set of constraints are chosen , creating the shaped subcode .",
    "we generalize this method , by allowing the large codebook to be generated according to any i.i.d .",
    "probability mass function @xmath8 .",
    "we refer to this method as _ code construction ii _ with respect to @xmath8 and @xmath74 .",
    "we are motivated to analyze code construction _",
    ", since good practical codes for uniform probability mass function are readily available , and codeword selection might be a simple procedure for a properly structured code .",
    "more formally , let @xmath2 be a random codebook with @xmath90 codewords , referred to as the large codebook .",
    "let @xmath4 be the shaping rate .",
    "let @xmath91 be the shaped code rate .",
    "the large codebook is the union of @xmath6 random sets , with @xmath7 codewords in each random set .",
    "each codeword with length @xmath0 is generated according to i.i.d .",
    "probability mass function @xmath8 on @xmath26 .",
    "a message @xmath9 to be transmitted chooses the random set . out of this random set a single codeword @xmath10",
    "is selected such that it satisfies the constraints @xmath92 where @xmath70 is type of @xmath46 .",
    "a simple illustration of the selection process can be seen in fig .",
    "[ fig : random - subcode - selection ] .",
    "the following are examples of constraints and selection metrics .",
    "the first is the average power constraint with power limitation of @xmath93 @xmath94 using @xmath95 .",
    "the selected codewords have to satisfy @xmath96 to ensure that the codebook satisfies the constraints in average .",
    "another example is the hamming power constraint , where the number of ones ( 1 ) is limited to @xmath93 , i.e. , @xmath97 using @xmath98 . in this case",
    "the selected codewords have to satisfy @xmath99 .    for every constraint and constraint function @xmath100 ,",
    "the shaping rate @xmath4 is chosen such that a random set includes at least one suitable codeword with probability approaching 1 .",
    "the union of the selected codewords is the shaped code @xmath16 . since each selected codeword @xmath101 satisfies the constraints , the shaped codebook satisfies the constraints in average as required by ( [ eq : e set ] ) .",
    "recall that this code construction provides us with a framework for approximating the performance of practical shaping schemes using information theoretic tools .",
    "thus , the questions in hand are how to choose the shaping rate @xmath4 to find such non - typical codewords , and what are the achievable rates of this codebook construction . in particular , when using the matched decoder which is aware of the selection process and the constraints , and the mismatched decoders which is aware only of the large codebook .",
    "the next section is dedicated to the summary of the main theorems , which provides a full overview of the main results of this paper . after which the reader can safely proceed to section [ sec : special - cases ] for special case and results .",
    "let the shaping code construction be code construction _",
    "ii_. let @xmath4 be the shaping rate .",
    "given the set of constraints @xmath74 , let @xmath102 be the conditional limit distribution .",
    "thus , for @xmath103 the probability that a random set of size @xmath7 has at least one codeword which satisfies the constraints approaches 1 .",
    "furthermore , the probability that each random set includes at least one such codeword , which is the probability that the shaped codebook exists , also approaches 1 .",
    "let @xmath104 be the maximal achievable rate under the constraints in @xmath74 , code construction _",
    "ii _ using @xmath74 and @xmath8 , and the matched decoder . then",
    ", @xmath105 is the achievable rate .",
    "this results from the modified aep theorem ( theorem [ thm : joint - aep - ccii ] ) , developed in this paper with respect to code construction _",
    "ii_.    for the mismatched decoder ,",
    "let @xmath106 be the achievable rate resulting from the modified development of the gallager error exponent , which suites code construction _",
    "ii_. thus , for the rate @xmath107 the average error probability approaches 0 , i.e. , this is an achievable transmission rate .",
    "furthermore , let @xmath108 be the achievable rate resulting from the modified joint typicality analysis given in section [ sec : joint - typicality - decoder ] .",
    "this rate is achievable in special cases only , in particular when the conditions of lemma [ lem : first error event ] are satisfied .",
    "thus in those special cases , @xmath109 is an achievable rate , where the distribution @xmath110 is given in ( [ eq : pstar - mjt ] ) . under the conditions of lemma [ lem : first error event ] , the maximum between the two bounds is the achievable rate .",
    "otherwise , only the gallager bound can be used .",
    "the remaining of the paper is dedicated to elaborated presentation of these results , formulating theorems and proofs , and analyzing interesting and practical special cases .",
    "thus , this section is dedicated to choosing @xmath4 .",
    "this choice has to ensure that the large codebook has the right amount of codewords to support the transmission rate @xmath1 , i.e. , that each random set of size @xmath7 has at least one codeword which satisfies the constraints with very high probability . in theorem [ thm : number of nt codewords ] , we show that in the limit , this probability approaches 1 if @xmath111 where @xmath112 .",
    "the proof highly relies on the method of types and the sanov theorem .",
    "we also prove that ( [ eq : rsplusdelta ] ) guarantees a minimum on @xmath4 , such that the shaped codebook ( using code construction _ ii _ ) exists with very high probability .",
    "this is one of the main results of this paper .",
    "[ thm : number of nt codewords ] let @xmath4 be the shaping rate in code construction ii . let _",
    "@xmath74 _ be a set of probability mass functions , which is a closure of its interior .",
    "let @xmath113 designate the probability of finding at least one codeword which satisfies the constraints ( type belongs to @xmath74 ) in a random set of size @xmath7 , generated according to i.i.d .",
    "probability mass function @xmath8 .",
    "let be the conditional limit probability mass function with respect to @xmath8 and @xmath74 , which achieves @xmath79 .",
    "then :    1 .   _ for any @xmath114 there exists @xmath115 , such that for @xmath116 @xmath117 _ 2 .",
    "_ for any @xmath118 and @xmath76 , such that @xmath119 @xmath120_. 3 .",
    "_ for any @xmath118 and any finite @xmath121 , such that ( [ eq : rsplusdelta ] ) is satisfied , @xmath122 for @xmath76 .",
    "_    the probability that a codeword satisfies the constraints @xmath123 , is given by the sanov theorem ( [ eq : p(n)e ] ) .",
    "let @xmath59 be the set of types with denominator @xmath0 .",
    "since @xmath74 is the closure of its interior , it follows that @xmath71 is nonempty for @xmath124 .",
    "hence , we can find a sequence of probability mass functions @xmath125 such that @xmath126 and @xmath127 since by definition @xmath128 .",
    "this means that for any @xmath114 there exists @xmath129 , such that @xmath130 thus , for @xmath116 @xmath131 where the inequalities follow from ( [ eq : type probability ] ) and ( [ eq : seq divergence ] ) .",
    "it yields that for @xmath116 , the probability that none of the codewords within a random set satisfies the constraints is @xmath132 since @xmath133 , this concludes the first part .    for any @xmath118 and the choice @xmath134 , @xmath135 furthermore , according to ( [ eq : sanov ] ) for every @xmath118 there exists @xmath136 and @xmath137 , such that for @xmath138 @xmath139 hence for @xmath138 , @xmath140 and for @xmath76 , @xmath141 .",
    "thus @xmath142 and @xmath120 .",
    "this concludes the second part .",
    "furthermore ,    @xmath143^{2^{\\rho\\cdot n}p_{f}}.\\end{aligned}\\ ] ]    for @xmath76 , since @xmath142 , it yields that @xmath144 . for @xmath138 , @xmath145 this yields that for @xmath76 , @xmath146 and therefore @xmath147 .",
    "hence , @xmath148 .",
    "thus , we found the shaping rate which ensures that a random set has at least one suitable codeword .",
    "since the random sets are drawn independently , the probability that each one of the random sets has at least one suitable codeword is @xmath149 . using the last theorem with @xmath150",
    ", we conclude that @xmath151 , i.e. , the shaped codebook ( code construction _ ii _ ) exists with high probability .",
    "in the previous section we have shown that using a large code with code rate @xmath3 and redundancy shaping rate @xmath4 a shaped subcode can be obtained .",
    "this subcode consists of @xmath152 codewords which satisfy the constraints with probability tending towards 1 .",
    "hence , the maximal number of codewords in the subcode is @xmath153 the last inequality follows since the number of chosen codewords can not exceed the size of the typical set of @xmath23 .",
    "this property holds as long as @xmath154 .",
    "whereas for @xmath155 , the codewords of the large code satisfy the constraints and @xmath156 .",
    "we conclude that the achievable rate would be the minimum between the number of codewords and the rate which achieves average error probability tending towards zero for @xmath76 . to establish this rate",
    "we analyze the performance of several decoders , which are presented next .",
    "following our codebook construction method , we consider two types of decoders .",
    "the first is the matched decoder , which is aware of the codebook set @xmath16 and performs optimal decoding .",
    "the second decoder is the mismatched decoder , which is only aware of the large code @xmath2 and performs optimal decoding as if all codewords of @xmath2 might have been transmitted .",
    "this decoder is suboptimal , but occasionally more practical than the matched decoder . in the practical special case where the large codebook is uniform code",
    ", the mismatched decoder is readily available while the adaptation to the set @xmath16 might be extremely complex .    the achievable rate of code construction _",
    "i _ is known .",
    "it is the mutual information related to the probability mass function which generated the codebook and the probability mass function of the channel",
    ". the achievable rate of code construction _",
    "decoded with the matched decoder , however , is an open question . it is the objective of section [ sec : matched - decoder ] to show that the achievable rate , in this case , is bounded by @xmath24 .    on first",
    "look , mismatch performance analysis seems as a trivial exercise .",
    "the transmitted codeword is part of the large codebook and therefore the large code performance can be used .",
    "this is the naive approach .",
    "this type of assumption was taken ( without being mentioned ) by forney @xcite , using a decoder which is unaware of the selection process and stating that the performance and complexity of their decoder are not affected by shaping .",
    "a more careful inspection , however , reveals that the large code performance assumes the transmission of the typical codewords , while we transmit only non - typical codewords .",
    "these codewords may have worse distance spectrum towards their neighbors .",
    "the average error probability of the large codebook tends towards zero when @xmath157 .",
    "thus , the bound on @xmath1 using the naive approach is @xmath158 this bound is achievable in special cases only , for example the bsc channel , as will be shown .",
    "this is also correct for codes ( bad codes ) , which operate at working point that justifies the approximation of minimum distance , and this justifies the analysis of forney .",
    "another interesting special case is a large code construction in which all words have the same distance spectrum .",
    "obviously in such case the selection of specific codewords will not make a difference .",
    "for the awgn , geometrically uniform ( gu ) codes ( e.g. linear codes over rings ) are known only to mpsk constellation .",
    "this raises an interesting open question : is the mismatched approximation accurate for mpsk ?",
    "we know that gu codes generate good codes , and we can use them for the large uniform codebook . if the conditional limit theorem holds for gu codes over mpsk , then we can also know the choice of @xmath4 that will generate enough codewords that satisfy the constraints .",
    "the described random shaping code construction method highly resembles existing practical schemes . since well - known good codes",
    "( e.g. convolutional codes , ldpc , block codes ) typically generate uniform probability mass function , combined with a selection process the outcome is a shaped subcode .",
    "our results are bounds on the performance of random constructions which approximate practical systems , which are approached using uniform i.i.d .",
    "probability mass function , the right shaping rate @xmath4 and sufficiently large @xmath0 .",
    "next we present two examples of such practical schemes .",
    "the trellis shaping @xcite , is a sequence oriented approach .",
    "bit sign shaping , width=302,height=302 ]    the main idea is to use a trellis shaping code to create equivalence classes of coded sequences and then perform the shaping operation by choosing the minimum power sequence to be transmitted within each equivalence class .",
    "sign bit shaping ( sbs ) , is a special case of ts . in sbs ,",
    "two bits per symbol @xmath159 are modified by rate 1/2 trellis shaping code , having generating matrix @xmath160 and parity check matrix @xmath161 .",
    "let @xmath0 be the codeword length ( symbols ) , @xmath162 be the number of bits represented by each symbol and @xmath163 be a mapping function to the euclidean space .",
    "each symbol has @xmath164 unshaped bits , possibly coded by a channel code @xmath165 with rate @xmath166 , and two shaped sign bits , as shown in fig .",
    "[ fig : bit - sign - shaping ] .",
    "the sign bit sequence is @xmath167 where @xmath168 are @xmath0 information bits .",
    "the trellis decoder creates an equivalence class @xmath169 where @xmath170 and @xmath171 represents all binary sequences of length @xmath0 .",
    "the bits are mapped into equivalence class of symbols @xmath172 . in this scheme",
    "the goal is power minimization ; therefore the trellis decoder transmits the codeword with the minimal energy within the equivalence class , i.e. , @xmath173 or the minimal average energy @xmath174 therefore , the total information rate of this scheme is @xmath175 bits per symbol and the shaping rate is @xmath176 .",
    "the decoding process is within the large codebook , which includes all the equivalence classes .",
    "once the codeword @xmath177 has been decoded , the demapper extracts the shaped bits and the only additional operation required is reconstruction the shaped bits by @xmath178 this operation is error free in case the entire codeword was decoded correctly ( @xmath179 ) and since @xmath180 .",
    "the selection process in this scheme is to choose the codeword with the minimal average power in each equivalence class , as given in ( [ eq : avg eng ] ) .",
    "for very large @xmath0 , we claim that this selection is equivalent to selecting a sequence which satisfies the power constraint @xmath181 since the shaping rate @xmath4 is set by the shaping code @xmath160 and the constraint metric @xmath182 is given , there exists minimal @xmath93 such that each equivalence class has at least one such codeword .",
    "this is dual to choosing a constraint and finding the minimal @xmath4 for this constraint .",
    "the constraint in turn , defines the probability mass function of the selected codewords . recall that these sequences are very close to @xmath23 in distribution .",
    "the minimum average energy codeword also satisfies the constraint , thus it is one of the codewords in the previous method .",
    "although , the chosen codewords might differ , using the different methods , the outcome is the same .",
    "a code which satisfies the constraint and is very close to @xmath23 .",
    "thus , choosing the minimum and choosing the codeword satisfying the minimal @xmath93 is asymptotically the same . assuming that the large codebook is uniformly distributed , @xmath23 is the entropy maximizing maxwell - boltzmann probability mass function @xcite .",
    "note that in case of several constraints the methods are not longer equivalent since several constraints can not be represented by single metric , which can be minimized using the ts decoder at the transmitter .",
    "another example to the selection process is the scheme in @xcite , which was originally designed for dirty paper coding .",
    "this scheme creates a shaping code @xmath183 for each transmitted message @xmath9 of length @xmath184 using @xmath185 where @xmath186 is a sparse generation matrix , @xmath187 and @xmath188 are sparse parity check matrices , and @xmath189 is a fixed binary sequence of length @xmath190 . in this nested ldgm - ldpc scheme",
    "the matrix @xmath186 is designed to make the shaping code @xmath191 a good source code and thus suitable for average power shaping .",
    "the matrices @xmath187 and @xmath188 embed information and obtain good channel code properties . the encoder , given the information massage @xmath9 , performs a mean square error ( mse ) quantization of a scaled interference ( according to the dirty paper scheme ) using a lattice @xmath192 and transmits the quantization error .",
    "a special use of this scheme is the quantization of a zero interference , which results in transmitting a minimum energy codeword in the shaping code for each message .",
    "the union of the shaping codes is the large codebook and the subcode @xmath193 is the union of all minimum energy codewords selected from each shaping code @xmath183 .",
    "the information rate using this scheme is @xmath3 and the shaping rate can be obtained from the size of the shaping code @xmath194 , i.e. , @xmath195 the selection process is the same as in the sbs scheme , therefore the same conclusions apply to this scheme .",
    "in previous sections , we discussed code construction _ ii _ as a practical method to obtain a codebook which satisfies constraints .",
    "the objective of this section is showing the achievable rate of this code construction method and the matched decoder , with respect to the large codebook probability mass function @xmath8 and the set of constraints @xmath74 .",
    "furthermore , we bound the @xmath0-dimensional distribution of the codewords , using the conditional limit distribution .",
    "these are new results , since the conditional limit theorem treated the distribution of the marginals of the selected codewords , but not the entropy rate nor the average mutual information when transmitted over a channel .",
    "let @xmath8 be i.i.d .",
    "probability mass function on @xmath26 and let @xmath74 be a closed convex set such that @xmath154 .",
    "let @xmath23 be the conditional limit probability mass function , which achieves @xmath79 .",
    "let us have a standard random codebook ( code construction _",
    "i _ ) which uses this distribution to generate the codewords .",
    "thus , @xmath196 is the probability to generate the codeword @xmath65 and @xmath197 is the probability to generate the channel output @xmath198 .",
    "let @xmath199 designate the jointly typical set in definition [ a_pxy ] , and let the decoder be the joint typicality decoder with respect to _ _ the i.i.d . probability mass function _",
    "_ @xmath200 . let @xmath201 , @xmath202 designate the probability to generate the codeword @xmath65 and the channel output @xmath198 , respectively , using code construction _",
    "ii_. let @xmath203 .",
    "the next two lemmas upper bound @xmath201 , @xmath202 using @xmath204 and @xmath205 .",
    "these bounds are later used to derive the modified aep theorem for code construction _",
    "ii _ ( theorem [ thm : joint - aep - ccii ] ) .",
    "the aep , which defines the average probability of the decoding error , allows to deduct the achievable rate .",
    "[ lem : bounding ccii with cci]for every @xmath118 there exists @xmath206 such that for @xmath207 @xmath208    since @xmath8 is i.i.d .",
    "probability mass function on @xmath26 , the probability to generate a codeword @xmath65 from probability mass function @xmath8 is @xmath209 where @xmath210 is the type of @xmath65 .",
    "the probability to generate the codeword @xmath65 , using code construction _",
    ", is @xmath211 where @xmath123 is the probability of the event @xmath212 , when generating from i.i.d .",
    "@xmath8 . since @xmath74 is a closed convex set , ( [ eq : bound on pn(e ) ] ) applies .",
    "furthermore , by pythagorean theorem for divergence ( * ? ? ?",
    "* theorem 11.6.1 ) , every @xmath213 satisfies @xmath214 if @xmath74 is a closed convex set and @xmath154 .",
    "thus , @xmath215 using ( [ eq : bound on pn(e ) ] ) and ( [ eq : pythagorean ] ) .    the probability to generate the codeword @xmath65 with respect to i.i.d .",
    "probability mass function @xmath23 using code construction",
    "( according to ( [ eq : seq .",
    "probability ] ) ) is @xmath216 hence , we conclude that @xmath217 there exists @xmath206 such that for @xmath207 @xmath218    [ lem : bounding ccii with cci y]for every @xmath118 there exists @xmath206 such that for @xmath207 @xmath219    the probability of @xmath198 is the sum of the joint probabilities @xmath220 over all the codewords @xmath65 .",
    "thus , @xmath217 there exists @xmath206 such that for @xmath207 @xmath221 where the first inequality follows from lemma [ lem : bounding ccii with cci ] .",
    "one of the methods to obtain the achievable rate of a random codebook ensemble which was drawn according to an i.i.d .",
    "probability mass function is the channel coding theorem ( * ? ? ?",
    "* theorem 7.7.1 ) .",
    "it relies on the joint aep theorem ( * ? ? ?",
    "* theorem 7.6.1 ) .",
    "the aep establishes the probability that a codeword and a channel output are jointly typical with respect to their joint probability mass function in two cases . in case",
    "the output is a noisy version of the input and in case of two independent variables .",
    "these two probabilities are essential to analyze the average probability of error and deduct the achievable rate such that the probability of error is tending towards zero when the codewords length tends towards infinity .",
    "the decoder is the joint typicality decoder which operates as follows .",
    "an error occurs if the transmitted codeword is not jointly typical with the received sequence or when more than one codeword is jointly typical .",
    "the aep theorem is not suitable for code construction _",
    "ii _ , since the codewords are generated by a selection process and not randomly drawn according to an i.i.d .",
    "distribution . nevertheless , we know that their probability mass function converges in a sense to the i.i.d .",
    "conditional limit probability mass function .",
    "thus , we can use this conditional limit probability mass function to redefine the decoder and derive a modified aep theorem ( theorem [ thm : joint - aep - ccii ] ) .",
    "this theorem is used to show one of the main results of this paper , that ( [ eq : matched achievable rate ] ) is the achievable rate under matched decoder .",
    "we conclude that the same rate can be achieved either by code construction",
    "_ ii _ or by random i.i.d .",
    "generation according to @xmath23 , i.e. @xmath24 if the large codebook is uniformly distributed .",
    "it follows that code construction _ ii _ does not lose rate under matched decoding in this case .",
    "[ thm : joint - aep - ccii](joint aep for code construction ii )",
    "let let let @xmath199 designate the jointly typical set in definition [ a_pxy ] , _",
    "_ @xmath200 .    1 .",
    "_ @xmath222 as @xmath76 .",
    "_ 2 .   _ for every @xmath223 , the exist @xmath224 such that for @xmath225 @xmath226 _",
    "\\1 ) since @xmath227 , it yields that @xmath228 where @xmath229 is the type of @xmath46 .",
    "thus , @xmath230 where the last inequality follows from ( * ? ? ?",
    "* theorem 17.3.3 ) and @xmath231 is the @xmath232 distance between probability mass functions .",
    "the conditional limit theorem ( * ? ? ?",
    "* theorem 11.6.2 ) implies that @xmath233 there exists @xmath234 such that for @xmath124 @xmath235 furthermore , ( * ? ? ?",
    "* lemma 11.6.1 ) yields that @xmath236 thus if @xmath237 , the @xmath232 distance @xmath238 since @xmath239 there exists @xmath240 such that @xmath241 from ( [ eq : first aep cond ] ) and ( [ eq : div qx q star ] ) it yields that there exists @xmath242 such that for @xmath243 @xmath244    in a similar manner , @xmath245 where @xmath246 is the type of @xmath247 .",
    "next we bound the divergence @xmath248 where @xmath249 is the joint type of @xmath51 and the inequality follows from the log sum inequality .",
    "the typicality of the channel implies that @xmath250 there exists @xmath251 such that for @xmath252 @xmath253 put @xmath254 and @xmath255 .",
    "since @xmath239 there exists @xmath256 such that @xmath257 from ( [ eq : div qx q star]),([eq : sec aep cond ] ) and ( [ eq : div py_x py_x typicality ] ) it yields that there exists @xmath258 such that for @xmath259 @xmath260 using similar arguments , @xmath239 there exists @xmath261 such that for @xmath262 @xmath263 thus , @xmath239 there exists @xmath264 , such that @xmath265 this concludes the proof of first property .",
    "\\2 ) if @xmath266 , @xmath267 are independent , then for _ @xmath225 _ @xmath268 where the first inequality follows from lemmas [ lem : bounding ccii with cci ] and [ lem : bounding ccii with cci y ] , and the second follows from the joint aep theorem ( * ? ? ? * theorem 7.6.1 ) , with respect to the i.i.d .",
    "@xmath269 .    hence , theorem [ thm : joint - aep - ccii ] established the joint aep properties of code construction _",
    "ii _ with respect to the set @xmath199 .",
    "the joint aep properties of code construction _ ii _ are enough to show that for @xmath270 , the average error probability is tending towards zero with the block length .",
    "this can be shown by applying the channel coding theorem ( * ? ? ?",
    "* theorem 7.7.1 ) and the joint typicality decoder ( @xmath199 ) to code construction _",
    "ii_.    according to theorem [ thm : number of nt codewords ] , @xmath271 to ensure that each random set of size @xmath7 includes a codeword which satisfies the constraints with very high probability .",
    "therefore , @xmath272 is an achievable rate , taking the maximal rate @xmath273 .",
    "since good codes for uniform probability mass function are readily available , the special case where the large codebook is uniformly distributed , i.e. @xmath274 , is of practical importance .",
    "it is easy to show that in this case , the conditional limit probability mass function @xmath23 achieves @xmath275 , i.e. , maximize the entropy within @xmath74 .",
    "it yields that , @xmath276 since @xmath277 and @xmath278 .",
    "this is the achievable rate under matched decoding in this special case .",
    "since the selected codewords maximize the entropy of the input and not the mutual information , this is only a lower bound on the capacity subject to the constraints .",
    "nevertheless , for the average power constraint on awgn with discrete constellation , the distribution which maximizes the capacity is approximately the distribution which maximizes the entropy .",
    "this observation can be verified numerically , using the modified baa @xcite .",
    "hence , the achievable rate is very close to the maximal rate in this special case .",
    "the previous sections treated code construction and the achievable rate under matched decoding . recall that practical schemes , however , use a mismatched decoder from complexity considerations .",
    "this decoder is only aware of the large codebook and performs optimal decoding as if all the codewords might have been transmitted .",
    "it is suboptimal , but occasionally more practical since the adaptation to the shaped subcode might be extremely complex , and the decoder for the large code is readily available .",
    "this section analyzes the mismatched decoder beyond the naive approach , which showed that the non - typical codewords within the large codebook require special treatment .",
    "we present two novel lower bounds for the achievable rate using the mismatched decoder .",
    "our first lower bound is based upon gallager s error exponent bounding technique @xcite .",
    "the key here is to perform maximum likelihood decoding using the entire codebook , while taking into account that only a subcode was transmitted and the true distribution of the transmitted codewords .",
    "based on this analysis , we redefine the average error probability and present a new error exponent ( [ eq : subcode error prob ] ) .",
    "it is important to note that this bound is suitable for any large codebook ensemble , as long as it is generated according to i.i.d .",
    "distribution and not necessarily a uniform distribution . using the new error exponent , we show that ( [ eq : achievable transmission rate . ] ) is an achievable rate for the mismatched decoder .",
    "our second lower bound is based upon the joint typicality decoder .",
    "this decoder is not suitable for non - typical codewords since it declares an error when the input or the output is not typical .",
    "thus , we define a modified joint typicality ( mjt ) decoder , which decodes solely based on joint typicality between the input and the output with respect to the large codebook distribution . the mjt achievable rate bound is presented in ( [ eq : achievable rate mtp ] ) .",
    "it might seems surprising that decoding according to a wrong distribution might work and indeed this bound is correct in special cases only ( lemma [ lem : first error event ] ) .",
    "one of them is uniform distribution of the large codebook and additive channels .",
    "thus , the awgn channel can be treated using this approach .",
    "more examples are shown in the sequel .      as already claimed , while the matched decoder achieves higher transmission rates , the mismatched decoder is more practical . in this section",
    "we use the gallager bounding technique @xcite to obtain a lower bound on the maximal achievable rate for the shaped subcode and the mismatched decoder .",
    "the mismatched decoder in this section is the maximum likelihood decoder , which uses the entire codebook in the decoding process .",
    "the average error probability , however , is redefined ( theorem [ thm : subcode error ] ) to take into account the true distribution of the transmitted codewords .",
    "thus , a new error exponent is presented . using this error exponent , theorem [ thm : achievable rate ] shows an achievable rate for the mismatched decoder .",
    "it is important to note that this bound is suitable for any large codebook ensemble , as long as it is generated according to i.i.d .",
    "distribution @xmath8 and not necessarily the uniform distribution .",
    "[ thm : subcode error]let @xmath27 be a memoryless channel probability mass function . for a given number of codewords @xmath279 of block length @xmath0 .",
    "ensembleclosed convex setsuch that @xmath154 .",
    "let be the probability mass function which achieves @xmath79 and @xmath203 .",
    "define a corresponding ensemble of subcodes , generated by code construction ii with respect to @xmath74 .",
    "suppose that an arbitrary subcode codeword @xmath280 , @xmath281 , enters the encoder and that maximum - likelihood decoding using the entire code is employed .",
    "then @xmath217 there exists @xmath206 such that for @xmath282 , the average probability of decoding error over this ensemble of subcodes , for any choice of @xmath121 , @xmath283 , is bounded by @xmath284    let @xmath285 designate the average error probability of codeword @xmath280 , when using code construction _",
    "ii _ to create the subcode ensemble and maximum - likelihood decoding using the entire code .",
    "let @xmath286 denote the probability of codeword @xmath280 within the subcode ensemble .",
    "let @xmath287 be the probability of error conditioned on codeword @xmath280 entering the encoder , the particular codeword @xmath288 , the received sequence @xmath198 and @xmath289 .",
    "then , the average decoding error probability for the codeword @xmath280 , when maximum - likelihood decoding using the entire code is employed , is @xmath290 following the derivation of the gallager error exponent ( proof of ( * ? ? ?",
    "* theorem 5.6.1 ) ) , the probability of error given @xmath280 , @xmath288 and @xmath198 is @xmath291 by substituting ( [ eq : perror givem m xm y ] ) into ( [ eq : avg pem ] ) and recognizing that @xmath288 is a dummy variable we obtain @xmath292 using lemma [ lem : bounding ccii with cci ] , @xmath217 there exists @xmath206 such that for @xmath282 , @xmath293 this yields that , @xmath294 where the equality follows since all the probability mass functions are independent and identically distributed .",
    "each subcode codeword @xmath280 is bounded by ( [ eq : avg pem bound ] ) , which does not depend on @xmath280 .",
    "thus , the average probability of decoding error is @xmath295 and we obtain the bound .",
    "thus , the last theorem showed the modified error exponent for the non - typical codewords .",
    "note that the main difference in the derivation lays in ( [ eq : avg pem ] ) , since the averaging is over the actual distribution of the codeword @xmath288 ( @xmath296 ) and not the typical codewords in the large codebook .",
    "since the number of codewords @xmath297 exponentially depends on the large code rate @xmath3 , @xmath217 with @xmath282 , the bound can be expressed as @xmath298 where @xmath299 once the error exponent is obtained , the achievable rate can be deducted .",
    "the following theorem shows the large code rate @xmath3 , for which the average error probability is tending towards zero .",
    "this rate is later translated into an achievable transmission rate @xmath1 , using @xmath5 ( by code construction _",
    "ii _ ) .",
    "[ thm : achievable rate]suppose that for every @xmath118 there exists @xmath282 such that for every @xmath283 and @xmath300 , @xmath301 let @xmath302 then for every @xmath303 and @xmath76 , the average error probability @xmath304 .",
    "according to the limit definition , for every @xmath305 there exists @xmath306 such that @xmath307 taking @xmath308 , this yields @xmath309 thus , for every @xmath43 the exists @xmath118 and the corresponding @xmath206 such that @xmath310 and for @xmath282 @xmath311 and therefore @xmath312    theorem [ thm : achievable rate ] showed that when @xmath303 , the average error probability @xmath313 tends towards zero . furthermore , according to theorem [ thm : number of nt codewords ] , @xmath314 .",
    "hence , the average error probability @xmath304 , for the shaped code if @xmath315 this is the achievable rate under the mismatched decoding using the gallager exponent bounding technique .",
    "the closed form of @xmath316 is given at appendix ( [ eq : iqp ] ) .",
    "this yield that , @xmath317 is an achievable transmission rate using the gallager bounding technique .",
    "this relies on the average error probability and the maximal number of codewords using code construction _ ii , _ which was discussed in section [ sub : number - of - codewords ] .",
    "we use the following lemma to verify that the mismatched decoder performs is worse than the matched .",
    "let @xmath27 be memoryless channel and @xmath318 , @xmath319 be input distributions .",
    "let @xmath320 , @xmath321 be the corresponding channel outputs , such that @xmath322 .",
    "then , @xmath323    according to the definition of divergence @xmath324 where the inequality follows from the log - sum inequality .    using @xmath325 and @xmath326 , we conclude that @xmath327 therefore @xmath328 as expected .",
    "our second lower bound for mismatched decoding is based on the joint typicality decoder analysis . in the original configuration , however , this decoder is not suitable for non - typical codewords in @xmath2 , since it declares an error when the input or the output is not typical according to @xmath32 .",
    "thus , we define a modified joint typicality decoder . the mjt decoder is aware only of the large codebook @xmath2 , and decodes solely based on joint typicality between the input and the output with respect to the large codebook probability mass function @xmath32 .",
    "it might seems surprising that decoding according to a wrong probability mass function might work and indeed this bound is correct in special cases only .",
    "lemma [ lem : first error event ] , provides a sufficient condition on @xmath8 and the channel , such that the mjt decoder is applicable .",
    "we restrict our analysis to this particular case .",
    "we bound the average error probability of the shaped subcode @xmath329 , created by code construction _",
    ", using the mjt decoder to obtain an achievable rate .",
    "suppose that a codeword @xmath330 with index @xmath331 was sent through the channel and a noisy sequence @xmath247 was received .",
    "the original joint typicality decoder operates as follows ; it examines all the codewords in the large codebook and establishes which codewords are jointly typical with the received sequence @xmath247 . since , @xmath332 and @xmath247 are typical with respect to @xmath23 and @xmath333 , respectively ; they are not typical with respect to @xmath8 and @xmath29 . therefore , the joint typicality decoder declares an error for each transmitted codeword .",
    "for this reason , the typicality of @xmath332 and @xmath247 with respect to @xmath8 , @xmath29 is disregarded .",
    "let us define the modified joint typicality set and the modified joint typicality decoder .",
    "[ m_pxy]the modified jointly typical set @xmath334 with respect to the probability mass function @xmath32 is the set of sequences @xmath49 with empirical entropies",
    "@xmath43-close to the true entropy @xmath335 :    @xmath336    the modified joint typicality decoder operates as follows ; first it examines all the codewords in the large codebook and establishes which codewords are mjt with the received sequence @xmath247 , i.e. , belong to the set @xmath334 .",
    "an error occurs , either when the sent codeword @xmath332 is not mjt with the received sequence @xmath247 or when more that one codeword is mjt with @xmath247 .",
    "let us define the event that a codeword @xmath337 is mjt with the output as @xmath338 , @xmath339 .",
    "the event @xmath340 ( the complete to @xmath341 ) is the first error event , i.e. , the sent codeword is not mjt with the received sequence .",
    "thus , the probability of error is therefore the union of these events .",
    "let us now define an event @xmath342 , where the received sequence @xmath247 is typical with respect to the probability mass function @xmath333 , i.e. , @xmath343 and @xmath344 as the complete to @xmath342 . using the aep property for code construction _",
    "@xmath345 for sufficiently large @xmath0 .",
    "hence , the probability of error is bounded    @xmath346    where the last inequality follows from the union bound and since @xmath347 .",
    "first , we analyze @xmath348 , which is equal to @xmath349 . the following lemma introduces a sufficient condition such that this probability is sufficiently small . in the general case",
    ", however , the decoder will not choose the right codeword , since the probability of joint typicality according to a wrong probability mass function ( @xmath32 ) is extremely low .",
    "[ lem : first error event]for sufficiently large @xmath0 @xmath350 if @xmath351 where @xmath352 is a constant .    since @xmath353",
    ", it yields that @xmath354,\\end{aligned}\\ ] ] where @xmath355 is the type of @xmath356 and @xmath357 is the type of the channel , i.e. , of @xmath247 given @xmath332 .",
    "hence , @xmath358\\right.+\\nonumber \\\\   & \\sum_{x\\in\\mathcal{x}}q_{\\boldsymbol{x}\\left(1\\right)}\\left(x\\right)\\left[h_{p_{\\boldsymbol{y}\\mid\\boldsymbol{x}\\left(1\\right)}}\\left(y\\mid x = x\\right)-h_{p}\\left(y\\mid x = x\\right)\\right.+\\nonumber \\\\   & \\left.\\left.d\\left(p_{\\boldsymbol{y}\\mid\\boldsymbol{x}\\left(1\\right)}\\left(y\\mid x = x\\right)\\parallel p\\left(y\\mid x = x\\right)\\right)\\right]\\right|\\leq\\nonumber \\\\   & \\sum_{x\\in\\mathcal{x}}q_{\\boldsymbol{x}\\left(1\\right)}\\left(x\\right)\\left[\\left|h_{p_{\\boldsymbol{y}\\mid\\boldsymbol{x}\\left(1\\right)}}\\left(y\\mid x = x\\right)-h_{p}\\left(y\\mid x = x\\right)\\right|\\right.+\\nonumber \\\\   & \\left.d\\left(p_{\\boldsymbol{y}\\mid\\boldsymbol{x}\\left(1\\right)}\\left(y\\mid x = x\\right)\\parallel p\\left(y\\mid x = x\\right)\\right)\\right],\\end{aligned}\\ ] ] where the last derivation is correct since @xmath359 for every @xmath360 . using the typicality of the channel and the same arguments as in theorem [ thm : joint - aep - ccii ] , for every @xmath43 there exists @xmath361 such that for @xmath362 @xmath363 for every @xmath360 with probability larger than @xmath364 .",
    "we conclude that for @xmath362 @xmath365 i.e. , @xmath366 .    a special case where condition ( [ eq : entropy equality ] ) is satisfied is uniform probability mass function @xmath367 and a channel which satisfies @xmath368 . binary symmetric channel ( bsc ) and additive noise channels @xmath369 , are examples for which this property is satisfied .",
    "hence , for these channels and uniformly distributed large codebook @xmath370 for sufficiently large @xmath0 .",
    "next we bound the probability @xmath371 , @xmath372 .",
    "the event @xmath373 is equivalent to @xmath374 . since by the code generation process ,",
    "@xmath332 and @xmath375 are independent for @xmath376 , so are @xmath247 and @xmath375 for @xmath376 . since @xmath375 was generated according to i.i.d . @xmath8 and",
    "@xmath247 was generated from code construction _",
    "@xmath377 thus , using lemma [ lem : bounding ccii with cci y ] @xmath378 the event @xmath379 is easier to analyze .",
    "it is equivalent to the event @xmath380 , where @xmath381 are drawn according to i.i.d .",
    "@xmath382 , @xmath383 is the type of @xmath381 and @xmath384 according to sanov s theorem ( * ? ? ?",
    "* theorem 11.4.1 ) , the probability that @xmath381 has type @xmath380 is bounded by @xmath385 where @xmath386 to obtain the minimal divergence in @xmath387 , we construct the following lagrangian and its derivative @xmath388 the solution is given in a parametric form @xmath389 where the lagrange multipliers @xmath390 should satisfy the constraints in ( [ eq : pi constraints ] ) .    finally , by substituting ( [ eq : first error event ] ) and ( [ eq : second error event ] ) into ( [ eq : error prob bound ] ) , the error probability can be further bounded @xmath391 hence , for @xmath392 the error probability tend towards zero for @xmath76 . according to theorem [ thm : number of nt codewords ] ,",
    "@xmath271 to ensure that each random set of size @xmath7 includes a codeword which satisfies the constraints with very high probability .",
    "it yields that for @xmath76 , the minimum between @xmath393 and the maximal amount of codewords ( [ eq : maximal number of code words ] ) is an achievable rate using this bounding technique .",
    "in the previous sections we discussed code construction _ ii _ , as a method to obtain shaped codebooks . we have shown the maximal amount of the available codewords and achievable rates for the matched and the mismatched decoders . in this section",
    "we analyze several special cases .",
    "the first is the case where the large codebook is generated according to an i.i.d .",
    "uniform distribution and the channel satisfies the condition in lemma [ lem : first error event ] . in this case , all the bounds are applicable .",
    "the more general case , where the large codebook is non - uniform , is later discusses in context of several special cases : the bnsc and awgn with gaussian large codebook .",
    "the details are given next .",
    "the special case where the large codebook is uniformly distributed on the support is of practical importance .",
    "furthermore , although the gallager bound can be obtained for any i.i.d .",
    "@xmath8 , the mjt bound can be obtained only when condition ( [ eq : entropy equality ] ) is satisfied . in order to compare all the bounds ,",
    "the following examples focus on a special case of ( [ eq : entropy equality ] ) , where @xmath394 and the channel satisfies @xmath395 for any @xmath352 .",
    "in particular , we analyze the awgn and bsc channels .",
    "our first observation is regarding the achievable rate for the matched decoder .",
    "it is easy to show that for uniform @xmath8 , the conditional limit probability mass function @xmath23 does not maximize the mutual information , but the entropy @xmath396 within the set @xmath74 , i.e. , @xmath397 thus , code construction _",
    "ii _ can not achieve the maximal rate even for the matched decoder , since generally different probability mass functions maximize the mutual information and the entropy subject to the constraints .",
    "nevertheless , for the average power constraint over awgn with discrete input constellation , the probability mass function which maximizes the mutual information is approximately the probability mass function which maximizes the entropy .",
    "hence , the achievable rate is very close to the maximal rate .",
    "this can be verified using the modified baa @xcite .    for the mismatched decoder ,",
    "the gallager bound takes the form of @xmath398 since @xmath399 the parametric form of the probability mass function @xmath110 of the mjt bound in ( [ eq : opt p ] ) is also simplified into @xmath400 where @xmath401 , @xmath402 and @xmath403 ( represents all the constants ) satisfy the constraints in ( [ eq : pi constraints ] ) .",
    "there is no closed form solution in the general case and the lagrange multipliers should be obtained numerically .",
    "once these parameters are found , the mjt bound takes the form of @xmath404    the maximal amount of codewords is not a limitation in this case , since the entropy of the large codebook is @xmath405 . thus taking the maximal @xmath406",
    ", we can obtain the typical set of @xmath23 ( up to the @xmath118 ) , since @xmath407 .",
    "for more details recall section [ sub : number - of - codewords ] .",
    "the first example that we analyze is the memoryless awgn channel with uniformly distributed large codebook , which is a special case of ( [ eq : symmetry cond ] ) .",
    "let @xmath26 be a discrete constellation with cardinality @xmath408 .",
    "* * let @xmath409 be a random vector of length @xmath0 , representing the transmitted symbols , where @xmath410 .",
    "we allow the amplitude levels of the constellation points to change to model an amplifier at the transmitter .",
    "this is done by multiplying the constellation by a real and positive scaling parameter @xmath411 .",
    "let @xmath412 be an i.i.d .",
    "gaussian noise vector , where @xmath413 are i.i.d .",
    "gaussian noise samples , normalized to unit variance .",
    "let @xmath414 be the random vector at the receiver , such that @xmath415 the transmission is subject to an average power constraint , where the maximum average power allowed is @xmath93 .",
    "since the scaling of the constellation is a degree of freedom , for each scaling @xmath416 the power constraint can be formulated as @xmath417 using @xmath418 .",
    "the corresponding decision rule for choosing codewords is therefore @xmath419 which is equivalent to @xmath420 for example , let the constellation be @xmath421 ( 4-pam ) , and the power constraint be @xmath422 . for each scaling @xmath416",
    ", @xmath423 is a set of probability mass functions which satisfy the constraint . in particular , lets consider @xmath424 and the corresponding sets @xmath425 , @xmath426 and @xmath427 .",
    "probability mass functions corresponding to @xmath424 and @xmath428.,width=302,height=302 ]    the probability mass functions @xmath429 @xmath430 @xmath431 in fig .",
    "[ fig : probability - mass - functions ] satisfy the constraints in @xmath425 , @xmath426 and @xmath427 , respectively .",
    "it can be seen that larger scaling is required as the distribution becomes more `` gaussian '' .",
    "thus , there is a tradeoff between the scaling parameter and how much the distribution is `` gaussian '' or non - uniform .",
    "thus , we have proved that ( [ eq : rm_uniform ] ) , ( [ eq : gallager rate uniform ] ) and ( [ eq : mjt rate uniform ] ) are achievable rates for the matched and mismatched decoders , corresponding to large code distribution @xmath8 and the constraints sets @xmath74 . since @xmath416 is a degree of freedom , we still need to establish the optimal scaling parameter @xmath432 , which attains the maximum of each bound .",
    "this also dictates the optimal decision rule to choose the codewords in the shaped code construction process .",
    "hence , the achievable rate using the matched decoder is @xmath433 it is also known that the maxwell boltzmann distribution achieves the maximal entropy subject an average power constraint @xcite , therefore the conditional limit probability mass function is @xmath434 the parameter @xmath435 is chosen such that the constraint is satisfied with equality , i.e. , @xmath436 which follows from the concavity of the entropy .",
    "the maximal rates for the mismatched gallager and mjt bounds are also calculated with respect to the optimal @xmath416 for each bound , such that @xmath437 and @xmath438 using ( [ eq : mb dist ] ) .",
    "achievable rates 16-pam , awgn , width=302,height=302 ]    achievable rates 16-pam , awgn , low snr.,width=302,height=302 ]    achievable rates 16-pam , awgn , high snr.,width=302,height=302 ]    figs .",
    "[ fig : awgn16pam]-[fig : awgn16pam - high snr ] present the maximal achievable rates using the matched decoder and our two lower bounds for the mismatched decoder versus snr . the scaling parameter @xmath416 is optimized for each bound at each snr point",
    "we keep the noise variance constant ( @xmath413 ) and change the transmission power @xmath93 to control the snr .",
    "the large codebook is uniformly distributed over 16-pam .",
    "these rates are compared to the capacity of the power constrained continuous awgn channel , which is denoted by `` capacity '' .",
    "they are further compared to the uniformly distributed 16-pam constellation , which is denoted by `` uniform '' .",
    "it can be seen that the matched decoder shaping curve is very close to the capacity curve up to the rate of 3 bps .",
    "the gap to capacity in high snr ( around 3 bps ) is approximately 0.1 db , 0.2 db for the matched and the mismatched decoders , respectively . in low snr",
    "the gap is approximately 0 db , 0.3 db for the matched and the mismatched decoders , respectively .",
    "thus , the main conclusion from the graphs is that for pam over awgn there is no significant loss due to mismatched decoding , and that shaping gain can be attained .",
    "this conclusion actually justifies the works of forney and others ( unknowingly ) , who used mismatched decoding .",
    "our second example is the binary symmetric channel , in which the input @xmath439 is inverted with probability @xmath114 .",
    "the mutual information of this channel is @xmath440 where @xmath62 is the input probability mass function and @xmath441 .",
    "bsc : matched and the mismatched decoder versus @xmath93 , for @xmath442 and uniform large codebook.,width=302,height=302 ]    this channel is another special case of ( [ eq : symmetry cond ] ) , for @xmath443 ( uniform large codebook ) . the hamming power constraint @xmath444 is very common for this channel .",
    "thus , let @xmath74 be the constrained probability mass functions set @xmath445 where @xmath446 is the maximal number of 1 s allowed , and let @xmath23 be the conditional limit probability mass function which attains minimum @xmath447 in @xmath74 . using the convexity of the divergence in @xmath448 , it implies that the constraint is satisfied with an equality .",
    "thus , @xmath449 .",
    "the achievable rate for the matched decoder is therefore @xmath450 .",
    "next we obtain the achievable rates @xmath106 and @xmath108 for this channel , using the gallager and the mjt bounding techniques .",
    "the gallager bound is simplified to @xmath451 the rate @xmath108 may be obtained numerically , using the parametric form in ( [ eq : opt p uniform ] ) .",
    "alternatively , let us guess that the lagrange multipliers are @xmath452 , with @xmath453 .",
    "subsequently , @xmath454 satisfies the constraints in @xmath387 .",
    "this can be easily verified by @xmath455 and @xmath456 which assures that the second constraint is satisfied .",
    "hence , the rate @xmath457 is achievable and it coincides with the gallager bound .",
    "the large code has error probability tending towards zero if @xmath458 and therefore @xmath459 using ( [ eq : uniform rs ] ) , @xmath460 and @xmath461 .",
    "it follows that the three bounds ( gallager , mjt and the naive approach ) coincide in this particular case .",
    "it is known that the bsc capacity is attained for uniform input probability mass function , and that linear codes achieve the bsc capacity .",
    "such codes have the same distance spectrum for all the codewords and therefore the same error probability . in particular",
    ", the error probability of the non - typical codewords equals to the error probability of the typical .",
    "this example provides some insight to why the naive approach is achievable in this case ( as shown using gallager and mjt ) .",
    "the rate loss is @xmath462 comparing to the matched decoder .",
    "results are illustrated in fig .",
    "[ fig : bsc ] , using @xmath442 .",
    "the third example is the binary non - symmetric channel .",
    "this channel is an example where condition ( [ eq : symmetry cond ] ) is not satisfied .",
    "thus , for the practical case of uniform large codebook , only the gallager bound can be obtained .",
    "let the input be @xmath439 .",
    "the input is complemented with probability @xmath240 if @xmath463 and @xmath464 if @xmath465",
    ". the achievable rate of this channel is @xmath466 where @xmath467 is the input probability mass .",
    "bnsc : matched and the mismatched decoder versus @xmath468 , for @xmath469 , @xmath470 and @xmath471.,width=302,height=302 ]    let the hamming power constraint be the same as for the bsc channel ( [ eq : hamming power const ] ) . fig .",
    "[ fig : bnsc_uniform-1 ] shows the achievable rates for the matched and mismatched decoders for @xmath469 and @xmath470 .",
    "the matched decoder outperforms the mismatched as expected . at @xmath472 ,",
    "the typical codewords are transmitted since the large codebook is uniform , thus the two decoders are the same . the naive approach describes the performance of the typical codewords . in this case , the mismatched performs better than the naive approach , thus the non - typical codewords have lower average probability of error than the typical codewords .",
    "the non - uniform large codebook is the more general case .",
    "although it is not common to use non - uniform distributions in practical system , we present several toy examples for the sake of completeness .",
    "the first example is the bnsc which satisfies the general condition ( [ eq : entropy equality ] ) ( required for the mjt bound to be valid ) .",
    "otherwise , we can not compare all the bounds .",
    "the second example is generating a gaussian codebook with power @xmath93 from a larger gaussian codebook with average power @xmath473 , where @xmath474 .      in this section",
    "we consider the binary non - symmetric channel , such that the general condition ( [ eq : entropy equality ] ) is satisfied . in order to be able to demonstrate all our bounds ( i.e. , satisfy ( [ eq : entropy equality ] ) ) we have to choose a large codebook which is not uniformly distributed .",
    "in particular , the large codebook is according to @xmath8 which solves @xmath475 i.e. , @xmath476 the hamming power constraint is used once again , limiting the average number of ones in a codeword to be lower than in the large codebook , i.e. @xmath477 .    bnsc : matched and the mismatched decoder versus @xmath93 , for @xmath469 , @xmath470 and @xmath478.,width=302,height=302 ]    fig .",
    "[ fig : bnsc ] shows the achievable rates for @xmath469 and @xmath470 , using the matched and the mismatched decoders .",
    "the probability mass function of the large codebook ( according to ( [ eq : pbnsc ] ) ) is @xmath478 . in this example , the gallager and the mjt are also above the naive approach .",
    "furthermore , the gallager and the mjt bounds are very close to each other .",
    "all bounds coincide at @xmath479 , since the typical codewords are transmitted .      in the following example we consider the situation where the large codebook has @xmath90 codewords with average power @xmath473 , according to gaussian distribution . whereas , the transmitted codewords have much smaller power @xmath474 .",
    "it is easy to show that the conditional distribution in this case is also gaussian with average power @xmath93 .",
    "thus , the capacity of the gaussian channel is achieved for the matched decoder .",
    "the mismatched decoder derivation is less trivial .",
    "let us first derive the gallager bound using @xmath480 where @xmath481 is gaussian with variance @xmath93 and zero mean , and @xmath482 is gaussian with variance @xmath473 and zero mean . in a similar manner ,",
    "@xmath483    assuming that the channel is awgn with variance @xmath484 and zero mean .",
    "in the scenario where @xmath474 and @xmath485 , the gallager bound is deduced to @xmath486 this example is related to nested lattice shaping .",
    "the nested lattice scheme consists of fine lattice and coarse lattice .",
    "the fine lattice is the `` large codebook '' ( having infinite number of codewords ) .",
    "this lattice represents all the cosets .",
    "the coarse lattice provides the shaping region , such that it bounds the coset representatives ( the minimum power codewords in each coset ) , which construct the shaped codebook .",
    "a practical decoding scheme for nested lattices is lattice decoding , which is in fact very similar to the mismatched decoder described in this paper .",
    "the large gaussian codebook can be viewed as a region in the infinite lattice , which is large enough such that decoding in that codebook is the same as decoding in infinite lattice for codewords transmitted from the small codebook .",
    "it is interesting though to compare our bound ( [ eq : gallagergaussian ] ) to the performance of lattice decoding @xcite , and indeed the result is identical .",
    "in this paper , we proposed a method to analyze practical shaping schemes , in which a shaped subcode is chosen from a larger code ( in most cases uniformly distributed large codebook ) .",
    "the codewords in the shaped codebook satisfy as set of constraints .",
    "we provide a theoretical framework for such analysis , using random coding and a selection process subject to constraints .",
    "in particular , we have found the achievable rates of the matched and the mismatched decoders , where the last is unaware of the selection process at the transmitter .",
    "this decoder is suboptimal , but occasionally more practical since it does not have to repeat the selection process at the receiver . in general , we show that the large code performance does not dictate the subcode performance , and that the non - typical codewords require different analysis .",
    "hence , we obtained two novel achievable bounds for the mismatched decoding using a modification on the gallager and the joint typicality bounding techniques .",
    "we examine several special cases in detail . in the special case of bsc , however , the large code analysis is actually valid and can be justified using capacity achieving linear codes . for m - pam over awgn the loss due to mismatched",
    "decoding is not very significant .",
    "we show other examples where there is a larger difference between the matched and mismatched decoding .    to obtain @xmath316 in closed form",
    ", we calculate the derivative of @xmath487 with respect to @xmath121 .",
    "let us define @xmath488 substituting ( [ eq : a])-([eq : d ] ) into ( [ eq : e0qp ] ) @xmath489 differentiating with respect to @xmath121 , yields @xmath490 such that @xmath491 @xmath492 @xmath493 finally , using these terms at @xmath494 , yields @xmath495 subsequently , using the definitions of entropy ( [ eq : entropy ] ) , conditional entropy ( [ eq : conditional entropy ] ) , mutual information ( [ eq : mutual information ] ) and the kullback - leibler information divergence ( [ eq : divergence ] ) we conclude that @xmath496 this gives @xmath316 in closed form .",
    "j. barron , b. chen , and g. w. wornell , `` the duality between information embedding and source coding with side information and some applications , '' _ ieee trans .",
    "inform . theory _",
    "49 , pp . 1159 - 1180 , may 2003 ."
  ],
  "abstract_text": [
    "<S> shaping gain is attained in schemes where a shaped subcode is chosen from a larger codebook by a codeword selection process . </S>",
    "<S> this includes the popular method of trellis shaping ( ts ) , originally proposed by forney for average power reduction . </S>",
    "<S> the decoding process of such schemes is mismatched , since it is aware of only the large codebook . </S>",
    "<S> this study models such schemes by a random code construction and derives achievable bounds on the transmission rate under matched and mismatched decoding . for matched decoding the bound </S>",
    "<S> is obtained using a modified asymptotic equipartition property ( aep ) theorem derived to suit this particular code construction . for mismatched decoding , </S>",
    "<S> relying on the large codebook performance is generally wrong , since the performance of the non - typical codewords within the large codebook may differ substantially from the typical ones . </S>",
    "<S> hence , we present two novel lower bounds on the capacity under mismatched decoding . </S>",
    "<S> the first is based upon gallager s random exponent , whereas the second on a modified version of the joint - typicality decoder .    </S>",
    "<S> constellation shaping , aep , mismatched decoding , gallager random exponent . </S>"
  ]
}