{
  "article_text": [
    "magnetoencephalography ( meg ) is a noninvasive neurophysiological technique that measures the magnetic field generated by neural activity of the brain using a collection of sensors outside the scalp [ @xcite ] .",
    "when information is being processed at some regions of the brain , small currents will flow in the neural system , producing a small electric field , which in turn produces an orthogonally oriented small magnetic field according to maxwell s equations .",
    "the meg inverse problem refers to recovering neural activity by means of measurements of the magnetic field .",
    "the neural activities are usually represented by magnetic dipoles , which are closed circulations of electric currents , that is , loops with some constant current flowing through .",
    "each dipole has a position , an orientation , and a magnitude .",
    "the inverse problem then becomes determining the position , orientation , and magnitude ( or amplitude ) of the dipoles .",
    "one challenge of the meg inverse problem is that it does not have a  unique solution and so it is ill - posed [ @xcite ; @xcite ; @xcite ] . as early as in the 19th century",
    ", von helmholtz demonstrated theoretically that general inverse problems , such as those aiming at identifying the sources of electromagnetic fields outside a volume conductor , have an infinite number of solutions [ @xcite ] .",
    "hence , to derive a practically meaningful solution from the infinitely many mathematically correct solutions , one has to introduce constraints to the solution and/or use prior knowledge about the brain activity .",
    "existing approaches for the meg inverse problem can be grouped into two major classes that differ in how they impose constraints on the source signals . within the first class ,",
    "the dipole fitting [ @xcite ; @xcite ; @xcite ; @xcite ] and scanning methods [ @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ] assume that there exist a limited number of dipoles as point sources of the magnetic field in the brain . by constraining the number of sources ,",
    "the locations of these dipoles are estimated by least squares fitting [ @xcite ] or iterative computing [ @xcite ] .",
    "dipole orientations and amplitudes can be effectively estimated within these locations . however , estimating the source locations involves solving a difficult nonlinear optimization problem which has multiple local optima [ @xcite ] .",
    "our proposed method belongs to the second class , which contains various imaging methods .",
    "different from the first class , imaging methods assume that there are a large number of potential dipole locations evenly distributed all over the cortex . by dividing the cortical region into a fine grid and attaching a dipole at each grid , imaging methods model the orientations and magnitudes for all the potential dipoles simultaneously .",
    "dipoles with nonzero magnitudes are identified as the source dipoles .",
    "imaging methods are based on the theory that the primary sources can be represented as linear combinations of neuron activities [ @xcite ] .",
    "one can express the inverse problem using a linear model @xmath0 where @xmath1 is an @xmath2 matrix containing meg time courses measured by @xmath3 sensors at @xmath4 time points , recording the amplitudes of the magnetic field . without loss of generality",
    ", it is assumed that the @xmath4 measurements for each time course are sampled at the same evenly - spaced time points .",
    "the known @xmath5 design matrix @xmath6 links the source signals to the sensor measurements , and is computed using a boundary element model prior to application of model ( [ eqdata ] ) [ @xcite ] .",
    "the @xmath7 matrix @xmath8 represents the unknown dipole activities in the form of @xmath9 unobservable source time courses .",
    "the @xmath2 matrix @xmath10 contains some additive noise .",
    "the amplitudes and orientations of the signal for each dipole at a time point can be decomposed into three components in the @xmath11 coordinate system .",
    "therefore , @xmath9 represents the total number of the dipole components , and it is three times that of the number of grid cells . in a typical meg study , @xmath4 is usually from a few hundred to a few thousand , @xmath3 is a few hundred , but @xmath9 is as large as over 10,000 , and so @xmath12 .    defining the matrix frobenius norm as @xmath13 . to recover  @xmath8 , one can solve a penalized least squares problem @xmath14 where @xmath15 is a penalty function that promotes certain desirable properties on @xmath8 .    in the literature of meg source reconstruction ,",
    "spatial focality and temporal smoothness are two valid assumptions .",
    "that is , the source signals are smooth in time , and only a small number of compact areas are responsible for the recordings [ @xcite ] .",
    "many of the imaging methods focus on either the first assumption or the second .",
    "earlier methods using the smoothness assumption usually adopt the @xmath16-norm penalty , @xmath17 for certain weighting matrix @xmath18 .",
    "the simplest such method is the minimum norm estimate ( mne ) [ @xcite ] which uses @xmath19 .",
    "the loreta methods [ @xcite ; @xcite ] set @xmath18 to be the discrete spatial laplacian operator .",
    "two advantages of the @xmath16-penalty based methods are the computational efficiency and the less - spiky property in the time domain . nevertheless , the @xmath16-penalty lowers the spatial resolution and causes the well - known `` blurring effect '' in the spatial domain .",
    "utilizing the @xmath16-penalty , the focuss method [ @xcite ] reduces the blurring effect by reinforcing the strong signals while weakening the weak ones using an iterative algorithm to update @xmath18 .",
    "however , it is noticed that focuss is very sensitive to noise [ @xcite ] .",
    "many hierarchical bayesian approaches induce the temporal smoothness by employing smoothing priors which penalize discontinuities [ see , e.g. , @xcite ; @xcite ; @xcite ] .",
    "an alternative penalty is the @xmath20-norm , @xmath21 , which promotes the focality of the recovered signals .",
    "related work includes the minimum current estimate ( mce ) [ @xcite ; @xcite ; @xcite ] and the sparse source imaging method [ @xcite ] .",
    "in contrast to the @xmath16-penalty , the @xmath20-penalty causes `` spiky '' discontinuities of the recovered signals in both temporal and spatial domains .",
    "bayesian methods developed by @xcite , @xcite , @xcite take into account the spatial focality by employing anatomic sparse priors .",
    "however , these methods have similar problems as methods based on the @xmath20-penalty .    to prevent the spiky property from the @xmath20-penalty and the blurry property from the @xmath16-penalty , some @xmath22-norm methods with @xmath23 and @xmath24 have been introduced [ @xcite ; @xcite ] . however , the optimization problems associated with @xmath22-penalties are more difficult to solve than with @xmath20 and @xmath16 penalties .",
    "more recently , some spatio - temporal regularization methods have been proposed , which take into account both the smoothness and focality properties by combining basis representation with penalization .",
    "the @xmath25-regularization discussed by @xcite first projects @xmath8 onto a temporal basis and then imposes the @xmath20-penalty on the spatial domain and the @xmath16-penalty on the temporal domain .",
    "the event sparse penalty procedure [ @xcite ] divides the brain surface into several patches based on its anatomic features and uses temporal basis functions to represent source time courses within each patch .",
    "one drawback of both methods is that it is not straightforward to choose the basis .",
    "both methods have some shortcomings .",
    "the former makes the assumption that the source temporal basis can be extracted perfectly from the meg recordings .",
    "the latter utilizes comprehensive prior information of the experiment task and the brain geometry .",
    "in addition , the use of basis representation can potentially cause information loss , since information orthogonal to the basis set can not be recovered after the projection to the basis set is done .",
    "the goal of this paper is to develop an innovative two - way regularization method ( twr ) for solving the meg inverse problem that promotes both spatial focality and temporal smoothness of the reconstructed signals .",
    "the proposed method is a two - stage procedure .",
    "the first stage produces a raw estimate of @xmath8 using a fast minimum norm algorithm .",
    "the second stage refines the raw estimate in a penalized least squares matrix decomposition framework . a sparsity - inducing penalty and a roughness penalty are employed to encourage spatial focality and temporal smoothness , respectively .",
    "the proposed twr has three major advantages over the existing methods .",
    "first , twr regularizes in both spatial and temporal domains , and simultaneously takes into account both focality and smoothness properties .",
    "hence , it should be superior to one - way regularization methods ( e.g. , mne and mce ) .",
    "second , unlike some aforementioned spatio - temporal methods , twr does not rely on the choice of basis functions . hence",
    ", it avoids the information loss due to basis approximation .",
    "third , the two - stage procedure is computationally efficient .",
    "the advantages of our method are well illustrated in the empirical studies , which show clearly that twr outperforms one - way regularization methods that focus either on the focality or the smoothness alone , and some existing two - way spatio - temporal methods as well .",
    "two - way regularization techniques for matrix reconstruction have been studied in other contexts . @xcite",
    "present a two - way regularized singular value decomposition for analyzing two - way functional data that imposes separate roughness penalties on the two domains . @xcite and",
    "@xcite develop sparse singular value decomposition methods that impose separate sparsity - inducing penalties on the two domains .",
    "however , to the best of our knowledge , a two - way regularization with the sparsity penalty on one domain and the roughness penalty on the other domain of the data matrix has not appeared in the literature .",
    "this paper provides a novel application of the two - way regularization method in solving the highly ill - posed meg inverse problem , where different types of penalties are naturally used to meet the dual requirements of spatial focality and temporal smoothness on the unknown source signals .",
    "the rest of the paper is organized as follows .",
    "section  [ secmethod ] presents the details of the twr methodology including the computational algorithm . through a synthetic example",
    ", section  [ secsim ] shows advantages of the twr over some existing methods for solving the meg inverse problem .",
    "section  [ secmeg ] applies the twr to a real - world meg source reconstruction problem .",
    "section  [ seccon ] concludes the paper with some discussion about an alternative one - step approach and related complications .",
    "we propose a two - way regularization ( twr ) method to regularize the recovered signals in both spatial and temporal domains .",
    "we adopt a penalized least squares formulation that uses suitable penalty functions to ensure the spatial focality and the temporal smoothness of the recovered signals .",
    "twr is implemented in a two - stage procedure where the first stage produces a rough estimate of the source signals and the second stage refines the initial rough estimate using regularization .      the goal of stage 1 is to obtain a rough estimate of the location and the shape of the source signals . at this stage ,",
    "source information in the data is retained as much as possible .",
    "it is natural to obtain such a  rough estimate by solving the following minimization problem : @xmath26 note that the forward operator @xmath6 contains the information of positions and orientations of the dipoles , and how they are represented at the sensor level .",
    "this information can be decomposed by applying a singular value decomposition ( svd ) to @xmath6 , that is , @xmath27 , where @xmath28 is an orthogonal matrix and @xmath29 is a thin ( since @xmath12 ) orthonomal matrix , such that @xmath30 and @xmath31 .",
    "then the objective function in the optimization problem ( [ eqmin ] ) becomes @xmath32    let @xmath33 and @xmath34 .",
    "the minimization problem ( [ eqmin ] ) is equivalent to @xmath35 let @xmath36 and @xmath37 be the @xmath38th row of @xmath39 and the @xmath38th row of @xmath40 , respectively . since  @xmath41 is a diagonal matrix , the minimization problem ( [ eqminraw ] ) can be obtained by separately solving for each @xmath38 , @xmath42 where @xmath43 is the @xmath38th diagonal element in @xmath41 .",
    "this problem has a unique solution @xmath44 .",
    "then the estimated matrix @xmath45 with @xmath46 in the @xmath38th row can be obtained .",
    "thus , a rough estimate of @xmath8 can be obtained by solving @xmath47 note that @xmath45 is @xmath48 , @xmath49 is @xmath50 , and @xmath8 is @xmath51 . since @xmath52 and @xmath53 , equation ( [ eqchat ] ) does not have a unique solution for @xmath8 .",
    "any solution of ( [ eqchat ] ) can be written as @xmath54 , where @xmath55 is a @xmath56 orthonormal matrix whose columns are orthogonal to the columns of @xmath49 and @xmath57 is a @xmath58 matrix .",
    "we pick the minimum norm solution , which is @xmath59 .",
    "in fact , @xmath60 solves the following optimization problem : @xmath61 we can see this by noticing that @xmath62 and the equality holds when @xmath57 is a matrix of zeros .",
    "we call this @xmath60 the raw estimate . note that the raw estimate can only recover information that lies in the column space of @xmath49 , and thus any information orthogonal to the columns of @xmath49",
    "since the columns of @xmath49 are the right singular vectors of @xmath6 , the column space of @xmath49 is equivalent to the row space of @xmath6 .",
    "this information loss can also be understood by viewing the forward operator @xmath6 as a filter that maps the source @xmath8 to the space of the observations , @xmath1 , and so the information in the columns of @xmath8 that is orthogonal to the rows of @xmath6 can not be recovered .",
    "since all imaging methods are based on model ( [ eqdata ] ) , information loss is a common problem to these methods .",
    "this is the limitation of the meg technology .",
    "fortunately , according to our experience , most important information still remains in many real - world applications , as we will see in our real data example .",
    "note that the methods that require basis representation may cause additional information loss , since any information in the columns of @xmath8 that is orthogonal to the basis chosen will also be lost .",
    "it is obvious that the raw estimate , @xmath60 , can be noisy .",
    "the purpose of stage 2 is to polish the raw estimate by incorporating the smoothness and focality assumptions .",
    "the polished solution from this stage is denoted as @xmath63 .",
    "as we will see in the simulation study in section  [ secsim ] , the shapes of the time courses in the rows of @xmath60 are noisy but usually follow the shapes of the true curves , and @xmath60 may suggest a broader range of active regions . in a  penalized least squares framework",
    ", we apply a roughness penalty to smooth the recovered time courses and apply a sparsity - inducing @xmath20 penalty to refine the active regions .    in order to apply two penalty functions to @xmath8",
    ", we first use the two - way structure of the raw estimate and decompose it into spatial - only and temporal - only components .",
    "specifically , we write @xmath60 as @xmath64 where the matrix @xmath65 contains only the temporal features of  @xmath66 , and @xmath67 can be treated as the spatial coefficients .",
    "when @xmath68 , the decomposition ( [ eqbapp ] ) suggests a reduced - rank representation of @xmath60 . our empirical studies",
    ", however , suggest that any reduced - rank representation would lead to information loss and thus the full rank model is needed in practice .",
    "we shall focus on the full rank model ( @xmath69 ) for the rest of the paper . for identification purposes ,",
    "we require that @xmath70 is an orthogonal matrix , that is , @xmath71 .",
    "note that when the full rank model is used , the reconstruction error of using @xmath72 to represent @xmath66 , @xmath73 , is exactly zero .",
    "we propose to introduce focality and smoothness requirements on @xmath74 and @xmath70 respectively at the cost of allowing some errors in reconstructing @xmath66 .",
    "in particular , we consider the following penalized least squares problem : @xmath75 where @xmath76 and @xmath77 are appropriate penalty functions , and @xmath78 and  @xmath79 are the corresponding penalty parameters .",
    "to ensure the spatial focality of the recovered source signals , we employ a  sparsity - inducing penalty on @xmath74 so that the estimated @xmath74 is a sparse matrix , that is , a large proportion of its entries are zero",
    ". note that if a row of @xmath74 has all zero entries , then the corresponding row of @xmath66 has all zero entries , indicating no signal or an inactive location .",
    "although other choices are possible , we use the @xmath20 penalty @xmath80 to serve our purpose . on the other hand , to induce smoothness to the time course of the recovered source signals , we apply a roughness penalty to the columns of @xmath70 so that each column of @xmath70 is a smooth function of time .",
    "let @xmath81 represent a generic vector representing a column of @xmath70 .",
    "one choice of the roughness penalty is the squared second order difference penalty , defined as @xmath82 .",
    "this penalty is a quadratic form and can be written as @xmath83 for a nonnegative definite roughness penalty matrix ,  @xmath84 .",
    "the overall penalty on @xmath70 is the summation of the penalty on each column , @xmath85 .",
    "using the penalties defined above , the penalized least squares problem ( [ eqpen - ls ] ) becomes @xmath86      we propose an iterative algorithm to solve ( [ eqpen ] ) that alternates the optimization with respect to @xmath74 and @xmath70 .",
    "the algorithm starts with setting the initial @xmath70 to be the orthonormal matrix of the right singular vectors from the svd of @xmath60 .",
    "that is , let @xmath87 , where @xmath88 and @xmath89 are orthonormal matrices , and we set the initial @xmath90 .    _",
    "fix @xmath70 , update @xmath74 .",
    "_ when @xmath70 is fixed as @xmath91 , the roughness penalty term in the objective function ( [ eqpen ] ) is irrelevant to the optimization of @xmath74 .",
    "thus , updating @xmath74 reduces to solving the problem @xmath92 this is similar to one step of the iterative algorithm for the sparse principal component analysis as formulated by @xcite .",
    "express @xmath93 as a summation of a serial of rank - one terms @xmath94 where @xmath95 and @xmath96 are the @xmath97th column of @xmath74 and @xmath91 , respectively .",
    "since simultaneous extracting of all the rank - one terms is computationally expensive , we propose to obtain them sequentially .",
    "for the first rank - one term @xmath98 , we solve for fixed @xmath99 @xmath100 this problem has a closed - from solution which is given below . for the sake of notational simplicity",
    ", we drop the subscripts for now and express the objective function of ( [ eqvecg ] ) as @xmath101 \\\\[-8pt ] & & \\qquad = \\sum^p_{i=1 }   \\biggl\\ { a_i^2\\sum^s_{l=1}\\hat{g}_l^2 - 2a_i\\sum ^s_{l=1}\\hat{b}_{il}\\hat{g}_l+\\sum^s_{l=1}b^2_{il}+\\mu_1|a_i| \\biggr\\ } , \\nonumber\\end{aligned}\\ ] ] where @xmath102 is the @xmath103th element in @xmath66 , and @xmath104 , @xmath105 , are the elements of the vector @xmath106 .",
    "the minimization of ( [ eqseq ] ) is equivalent to independently solving  @xmath9 optimization problems @xmath107 according to lemma  2 of @xcite , the minimizer of each objective function in ( [ eqsoft ] ) is the soft thresholding rule @xmath108 where @xmath109 , and @xmath110 .",
    "the @xmath9-vector @xmath106 that minimizes ( [ eqseq ] ) is @xmath111 .    after the first rank - one term @xmath112 is obtained , we find the second rank - one term by solving the following minimization problem , while fixing @xmath113 : @xmath114 this is the same problem as ( [ eqvecg ] ) except that the @xmath66 in ( [ eqvecg ] ) is replaced by the residual @xmath115 from the rank - one approximation .",
    "the rest of the rank - one terms , @xmath116 , @xmath117 , can be obtained sequentially in a  similar manner by using the residuals from the lower - rank approximations .    _ fix @xmath74 , update @xmath70 . _ when @xmath74 is fixed as @xmath118 , the @xmath20 penalty term in ( [ eqpen ] ) becomes constant and thus is irrelevant to the optimization with respect to  @xmath70 .",
    "the update of  @xmath70 then solves the following problem : @xmath119 since directly solving this problem is complicated , we solve for the columns of @xmath70 sequentially . to obtain the first column of @xmath70",
    ", we solve the problem @xmath120 which has the solution @xmath121 .",
    "let @xmath122 be the eigen - decomposition .",
    "then @xmath123 to obtain an update of @xmath124 , we solve the problem @xmath125 which has the solution @xmath126 \\\\[-8pt ] & = & { { \\mathbf{p}}}(\\hat{\\mathbf{a}}^t_2\\hat{\\mathbf{a}}_2{{\\mathbf{i}}}+ \\mu_2 { \\boldsymbol{\\lambda}})^{-1 } { { \\mathbf{p}}}^t\\hat{{\\mathbf{b}}}_{\\mathrm{res},1}^t \\hat{\\mathbf{a}}_2 , \\nonumber\\end{aligned}\\ ] ] where again @xmath127 is the residual from the rank - one approximation .",
    "the rest of @xmath128 , @xmath117 , can be obtained similarly using the residuals from the corresponding lower rank approximations .",
    "when all columns of @xmath129 are obtained , we orthonormalize the columns of @xmath91 by taking the qr decomposition of @xmath129 and assigning the @xmath130 matrix to @xmath91 .    the iterative twr procedure , including stage 1 and stage 2 , is summarized in algorithm  [ algtwr ] .",
    "we consider the algorithm has converged if the frobenius norm of the relative difference between the current solution and the previous solution is smaller than a prespecified threshold value . in our implementation , we declare convergence when @xmath131 . based on our empirical studies ,",
    "only a few iterations are needed to reach convergence ; 15 iterations are usually sufficient for our numerical examples in sections  [ secsim ] and  [ secmeg ] .",
    "there are two tuning parameters in the twr algorithm : the focality parameter , @xmath78 , and the roughness penalty parameter ,  @xmath79 .",
    "the choice of @xmath78 and @xmath79 can be done using the cross - validation ( cv ) techniques and the generalized cross - validation , respectively .    to select @xmath78",
    ", we can utilize the _ leave - one - out _ cv that minimizes the leave - one - out cv score defined as @xmath132 where @xmath133 is the @xmath38th row of @xmath1 corresponding to the @xmath38th time course , @xmath134 is the @xmath38th row of @xmath6 , and @xmath135 and @xmath136 are the estimates of @xmath74 and @xmath70 using all observations except the @xmath38th time course . however , practical application of the cv has some difficulties .",
    "the gradient - based optimization is not feasible for minimizing the cv score since it is not a smooth function of @xmath78 , a  consequence of using the @xmath20 penalty .",
    "in addition , direct computation of the cv score is costly because of the usual large scale of the real problem . in a typical meg study",
    ", @xmath3 is over 200 , @xmath4 is a few hundred , and @xmath9 can be over 15,000 . in order to reduce the computational cost , we propose to use the @xmath137-fold cross - validation .",
    "specifically , we divide the rows of @xmath1 and @xmath6 into @xmath137 about equally sized parts and leave out one part each time for validation , and use the rest of the parts for estimating @xmath74 and @xmath70 .",
    "the @xmath137-fold cv score is defined as @xmath138 where @xmath139 contains the @xmath140th part of the rows of @xmath1 , @xmath141 contains the corresponding rows of @xmath6 , and @xmath142 and @xmath143 are the estimates of @xmath74 and @xmath70 using all observations except the @xmath140th part of time courses that are left out for validation .",
    "we used @xmath144 in our implementation .",
    "to further speed up the algorithm , we restrict our search only in a moderate - sized set of discrete candidate values for @xmath78 .",
    "such restrictive search is satisfactory , since we find that the results are usually not very sensitive to mild changes of @xmath78 ( see sections  [ secsim ] and [ secmeg ] ) and thus fine tuning of @xmath78 is not necessary .",
    "we used 10 different values evenly - spaced between 0 and 1 for @xmath78 in our simulations and the real - world meg example ; the search range may need to be changed for different problems .    to select @xmath79 , note that given @xmath118 , the update of @xmath145 columns of @xmath91 can be obtained by solving @xmath145 separate penalized regression problems . for the @xmath97th column ,",
    "the regression has @xmath146 as the input , where @xmath147 , @xmath148 as the output , and the hat matrix of the regression is @xmath149 , according to equation ( [ eqg2 ] ) .",
    "theoretically , @xmath79 can take different values for different @xmath148 s , but we decide to use a common @xmath79 for all the @xmath148 s based on computational efficiency consideration .",
    "the advantage of this strategy is that there is only one optimization problem to solve for choosing the tuning parameter when updating  @xmath129 .",
    "then , the overall gcv criterion is the average of all individual gcv criteria : @xmath150 where @xmath151 , and @xmath152 .",
    "the gcv optimization is nested in the iterations because it is defined conditioning on the current value of  @xmath153 .",
    "since the gcv criterion is a smooth function of @xmath79 , the optimization can be done using a combination of golden section search and successive parabolic interpolation [ @xcite ] .      by separately setting one of the penalty parameters in ( [ eqpen ] ) to be zero ,",
    "one can obtain two different one - way regularization methods : towr and sowr , as explained below .",
    "these two one - way regularization methods will be used as a comparison to twr to demonstrate the need for two - way regularization .    letting @xmath154 leads to a method that emphasizes temporal smoothness of the recovered signals , which is referred to as towr ( temporal one - way regularization ) , and is related to the functional pca [ @xcite ] .",
    "the corresponding optimization problem becomes @xmath155 a modified version of algorithm  [ algtwr ] can be applied for computation , with the `` update @xmath74 '' step in the algorithm simplified to @xmath156 .",
    "letting @xmath157 leads to a method that encourages spatial sparsity of the recovered signals , which is referred to as sowr ( spatial one - way regularization ) and is related to the sparse principal component analysis of @xcite . in this case , the optimization problem ( [ eqpen ] ) reduces to @xmath158 again , a modified version of algorithm  [ algtwr ] is applicable , but with the `` update  @xmath70 '' step simplified to @xmath159 .",
    "in this section we illustrate the proposed twr method using a synthetic example that mimics human brain activities .",
    "both the source and the forward operator are created based on real - world meg studies .",
    "@xmath160we generated the forward operator , @xmath6 , from a   subject head boundary element model using the mne software ( available at : http://www.nmr.mgh.harvard.edu/martinos/userinfo/data/sofmne.php ) .",
    "the @xmath6 matrix is a @xmath161 matrix , corresponding to a meg device with 248 valid channels . to mimic real - world scenarios and ensure enough difficulty of the problem",
    ", we located two source areas on the left and the right hemispheres , respectively .",
    "the sources were generated from two sine - exponential functions [ @xcite ] and are shown in figure  [ figbsim](a ) .",
    "the black solid and the red dashed curves are source signals located at the left motor and the right visual cortical areas , respectively .",
    "as we can see , the sources reach their energy peaks at 25 ms and 58  ms , respectively .",
    "the synthetic meg time courses were generated using equation  ( [ eqdata ] ) and were obtained using a sampling frequency 355 hz with a duration of 200 seconds [ see figure  [ figbsim](b ) ] . by mimicking the real meg data after preprocessing ,",
    "that is , denoising and smoothing , the signal - to - noise ratio , @xmath162 , is set to be 5db .",
    "we compare twr with eight different methods that can be put into two categories as given below .",
    "* one - way regularization : * * the @xmath16-based mne method [ @xcite ] * * the @xmath20-based mce method [ @xcite ] * * sowr ( i.e. , spatial sparsity only ) * * towr ( i.e. , temporal smoothness only ) * two - way regularization : * * the @xmath25 method proposed by @xcite * * mne@xmath163sowr ( i.e. , obtaining the mne solution as stage 1 and then applying sowr ) * * mce@xmath163towr ( i.e. , obtaining the mce solution as stage 1 and then applying towr ) * * mne@xmath163twr ( i.e. , obtaining the mne solution as stage 1 and then applying stage 2 of twr )    we put mne+sowr in the two - way regularization category because the  @xmath16 penalty in mne puts constraints on both domains , and sowr puts the  @xmath20 penalty only on the spatial domain . as a result",
    ", the temporal domain is regularized by the @xmath16 penalty , while the spatial domain is regularized first by the @xmath16 penalty and then by the @xmath20 penalty .",
    "similarly , mce+towr is also categorized as a two - way regularization method .",
    "mne+sowr and mce+towr can be considered as two alternative ways for two - way regularization and are suggested by a reviewer .",
    "mne+twr , also suggested by a reviewer , is a slight modification of twr , replacing the first stage of twr by mne .",
    "its inclusion in comparison helps us study the effect of using a different stage 1 estimator on the performance of twr .",
    "we implemented all the methods in r , and the tuning parameters are selected using either cv or gcv .",
    "three comparison criteria are utilized : the overall mean squared error ( mse ) , the standardized distance between the energy peak of the estimated source and the energy peak of the true source , and the computation time .",
    "the overall mse is defined as @xmath164 where @xmath8 and @xmath63 are the true and recovered source matrices , respectively .    @ld3.7d3.6d3.7d9.4@ * method * & & & & + mne & 544.0  ( 9.0 ) & 50.2  ( 7.3 ) & 42.9  ( 5.9 ) & 4371  ( 4.3 ) + mce & 903.7  ( 8.9 ) & 337.1  ( 6.4 ) & 156.1  ( 11.4 ) & 1545  ( 3.0 ) + towr & 407.9  ( 8.9 ) & 40.2  ( 5.8 ) & 39.6  ( 4.3 ) & 1841  ( 3.4 ) +",
    "sowr & 153.2  ( 7.7 ) & 19.3  ( 4.6 ) & 13.9  ( 3.9 ) & 1798  ( 3.6 ) + twr & & & & 1872  ( 3.5 ) + @xmath25 & 44.3  ( 7.1 ) & 31.0  ( 6.1 ) & 17.8  ( 2.3 ) & 40872  ( 8.8 ) + mne+sowr & 187.3  ( 8.8 ) & 27.9  ( 6.8 ) & 14.5  ( 3.1 ) & 5998  ( 3.9 ) + mce+towr & 912.7  ( 10.9 ) & 343.8  ( 6.2 ) & 145.2  ( 12.7 ) & 3321  ( 3.8 ) + mne+twr & 28.6  ( 7.2 ) & 16.9  ( 4.3)&10.7  ( 3.9 ) & 6201  ( 3.1 ) +    the energy of the dipole @xmath97 at time point @xmath140 is defined as @xmath165 , where @xmath166 , are the amplitude components for the @xmath97th dipole at the time point @xmath140 in the cartesian coordinate system .",
    "the energy of the reconstructed source can be defined similarly . the standardized distance between the estimated and the true energy peak at time point @xmath140",
    "is defined as @xmath167 where @xmath168 is the total number of dipoles , @xmath169 are the coordinates of the location for the maximum source energy at time point @xmath140 , and @xmath170 are the coordinates for the maximum estimated source energy at the corresponding time point . in this simulation example , there are two peak times , 25 ms and 58 ms , so we are interested in @xmath171 and @xmath172 .",
    "the simulation was conducted 100 times with the noise term in model ( [ eqdata ] ) newly generated for each run .",
    "the criteria described in the previous subsection ( i.e. , mse , @xmath171 , @xmath172 , computation time ) were evaluated for each simulation run , and the mean and standard error of the criterion values across the 100 runs were calculated .",
    "the numerical results are shown in table  [ tblcompare ] .",
    "several interesting observations can be made from the table .",
    "twr is the best method in the sense of having the smallest mse and the shortest distances between the true and the estimated peaks . among the four one - way regularization methods , sowr and towr",
    "outperform the classical mne and mce methods , and towr outperforms sowr .",
    "the fact that twr outperforms the four one - way regularization methods justifies our proposal of using two - way regularization .",
    "the @xmath25 method is the third most accurate method , but its computation time is more than 21 times as large as that of twr .",
    "mne+sowr and mce+towr are less satisfactory , demonstrating the importance of the first stage .",
    "mne+sowr is not better than sowr because the @xmath16 penalty of mne does not smooth the temporal domain .",
    "the performance of mce+towr is similar to mce and is not better than towr because mce does not recover well important information at the first stage , and hence towr based on mce is inaccurate .",
    "note that the reported computation time for twr , sowr and towr are based on fixed 15 iterations in order to make the calculation of the average computation time meaningful .",
    "such report is conservative because these algorithms usually converge rapidly and fewer iterations ( usually less than 10 ) are enough to obtain considerably good accuracy .",
    "figures  [ fig25 ] and  [ fig58 ] show the 3-d brain mapping by different methods at 25 ms and 58 ms for a randomly selected simulation run .",
    "twr performs the best among the nine methods in detecting the true source locations even though it misses some small regions .",
    "it is able to identify the majority parts of both source locations , and its solutions are focal .",
    "solutions from sowr and mne+sowr are more scattered than twr .",
    "mne and towr produce even more diffuse solutions .",
    "mce misses the main parts of both active areas and so does mce+towr , and they are the least satisfactory methods .",
    "the @xmath25 method recovers some of the activity , but the solution is overly focal .",
    "the plot of mne+twr is very similar to that of twr , so it is not presented here to save space .",
    "direct comparison of results of twr and towr clearly demonstrate the positive effect of using regularization in the spatial domain .",
    "method also identifies the active area but the solution is too focal .",
    "sowr and mne+sowr produce more scattering solutions than twr .",
    "mne and towr detected active areas are diffuse .",
    "mce and mce+towr misidentify the active region . ]     identify the major active area and the solution is focal .",
    "sowr and mne+sowr produce more scattering solutions than twr .",
    "mne and towr detected active areas are diffuse .",
    "mce and mce+towr misidentify the active region . ]",
    "figures  [ figsource1 ] and  [ figsource2 ] show the true and the recovered time courses by the nine methods for an arbitrarily chosen single dipole component in the two active areas , respectively , for a randomly selected simulation run .",
    "each subfigure shows the true time course and the estimated time course by one method .",
    "as one can see , the methods considering the temporal smoothness reconstruct the shape of the source time course well .",
    "twr , towr , @xmath25 , mce+towr and mne+twr all produce smooth time courses .",
    "twr recovers the most energy of the source , while mce+towr recovers the least .",
    "mne+twr tends to overshrink the amplitude of the time course because mne overshrinks the amplitude .",
    "the methods without considering the roughness regularization in the temporal domain result in noisy time courses even though some methods can recover the general trend . in figure  [ figsource2](b )",
    ", mce does not capture the major peaks of the signal , and , consequently , mce+towr [ figure  [ figsource2](h ) ] , which relies on the solution of mce , does not recover any signal activity either .",
    "direct comparison of results of twr and sowr clearly demonstrate the positive effect of using regularization in the time domain .     and mne+twr recover the shape of the time course reasonably well and the solutions are smooth .",
    "but mcr+towr , mne+twr and @xmath25 overshrink the amplitude .",
    "mne , mce , sowr and mne+sowr estimate the general trend reasonably well , but the estimated time courses are too noisy .",
    "twr gives the best result .",
    "]    and mne+twr recover the shape of the time course reasonably well and the solutions are smooth .",
    "but mne+twr overshrinks the amplitude .",
    "mne , sowr and mne+sowr estimate the general trend reasonably well , but the estimated time courses are too noisy .",
    "mce and mce+towr do not recover the shape of the time course .",
    "twr gives the best result . ]",
    "the selection of the focality parameter and the roughness penalty parameter was conducted using the method presented in section [ sectuning ] .",
    "figure  [ figtune](a ) and ( b ) shows the cv and gcv scores for twr as functions of @xmath78 and @xmath79 , respectively .",
    "the optimal values of the tuning parameters are @xmath173 and @xmath174 .",
    "figure  [ figtune](c ) shows the sparsity level of the reconstructed source matrix , @xmath175 , for twr as a function of the number of iterations when the tuning parameters are set at the selected values .",
    "the sparsity level for a  matrix is defined as the number of zero entries over the total number of entries . here",
    "the total number of entries for @xmath175 is @xmath176 . from this figure",
    ", we observe that the sparsity of @xmath175 levels off rather rapidly and stays steadily at about 0.996 , a fairly high sparsity level .",
    "in fact , this sparsity level matches closely the true level in the simulation setup : the number of true source dipoles is 20 , and so the total number of active source components is 60 after considering orientations .",
    "thus , the true sparsity level is @xmath177 .     and @xmath79 and the sparsity level as a function of the number of iterations .",
    "the optimal @xmath78 and @xmath79 are around 0.33 and 5.9 , respectively .",
    "the sparsity measure levels off at around 0.996 . ]",
    "in this section we demonstrate the proposed method using a human meg data set obtained from the center for clinical neurosciences at the university of texas health science center at houston .",
    "the study subject is a 44-year - old female patient with grade three left frontal astrocytoma who underwent the meg test as part of the presurgical evaluation .",
    "the patient underwent a somatosensory task which is designed to noninvasively identify the somatosensory areas of the patient .",
    "we choose this study because of the clinical usefulness of the somatosensory task in presurgical mapping .",
    "data collection was done with a whole - head neuromagnetometer containing 248 first - order axial gradiometers . during the meg somatosensory session ,",
    "558 repeated stimulations were delivered to the patient s right lower lip through a pneumatically driven soft plastic diaphragm .",
    "each stimulation lasted 40 ms with 450 ms epoch duration ( including a prestimulus baseline of 100 ms ) and an interstimulus interval randomized between 0.5  s and 0.6  s. we removed the offset and averaged the 558 epochs to obtain the final event - related magnetic field response .",
    "then a bad channel was removed .",
    "the meg device recorded 228 time points in each epoch .",
    "the measurement matrix @xmath1 is @xmath178 , where @xmath179 is the number of valid meg channels and @xmath180 is the number of recorded data points per epoch .",
    "the @xmath5 forward operator @xmath6 was obtained using the mne software with @xmath181 .",
    "the measured meg recordings from the 247 valid channels are plotted in figure  [ figmeg](a ) . among the 228 time points ,",
    "there are two peaks at time points 85 and 99 , corresponding to the activation of the primary somatosensory area contralateral to the stimuli , as expected by clinical experiences and brain anatomic theories .    nine methods , mne , mce , twr , towr , sowr , mne+twr , mne+sowr , mce+towr and @xmath25 ,",
    "were applied to solve the meg inverse problem .",
    "figure [ figmeg](b ) shows the reconstructed time courses for an arbitrary source location by different methods .",
    "as we can see , twr , sowr and towr , are satisfactory in terms of estimating the shape of the source time course and capturing the peak features at time points 85 and 99 .",
    "but sowr produces a noisy time course .",
    "mne and mne+sowr overshrink the magnitudes in addition to producing a noisy time course .",
    "mne+twr recovers the shape of the time course but underestimates the amplitude .",
    "the @xmath25 method does not distinguish the two peaks .",
    "mce only identifies the first peak but misses the second one .",
    "mce+towr does not capture any activity because it smoothes the spikes caused by mce and hence is the least satisfactory method .",
    "figure  [ figmegmaps ] shows the side views of the brain mapping at time point 85 by different methods .",
    "as we can see , the somatosensory area was correctly identified by twr , which matches the clinical expectation . as with the synthetic example ,",
    "towr and mne produce diffuse solutions , leading to false positives around the somatosensory area .",
    "sowr produces a scattering solution and so does mne+sowr .",
    "mne+twr and @xmath25 also identify some activity in the frontal lobe . solutions from mce and mce+towr",
    "are too focal and do not cover the somatosensory area .",
    "identify some activity in the frontal lobe in addition to the somatosensory area .",
    "solutions from mne , sowr , towr and mne+sowr are too diffuse to be satisfactory .",
    "both mce and mce+towr miss the activity in the somatosensory area . ]",
    "figure  [ figtunmeg](a ) shows the cv error as a function of @xmath182 .",
    "the cv error was minimized when the sparsity parameter , @xmath78 , is about 0.44 .",
    "figure  [ figtunmeg](b ) displays the gcv error as a function of @xmath79 .",
    "it shows that the optimal @xmath79 is about 59.5 . the sparsity level as a function of the number of iterations",
    "is shown in figure  [ figtunmeg](c ) . as we can see , the sparsity level increases at first and then levels off rapidly , indicating the algorithm converges fast .",
    "the optimal sparsity level was about 0.999 .     and @xmath79 and the sparsity level as a function of the number of iterations .",
    "the optimal @xmath78 and @xmath79 are around 0.44 and 59.5 , respectively .",
    "the sparsity measure levels off at around 0.999 . ]",
    "twr solves the meg inverse problem by using two - way penalties that promote both the temporal smoothness and the spatial focality of the solution .",
    "we developed a computational efficient two - stage procedure for implementing twr .",
    "we also considered a one - stage approach that tries to recover the source signal matrix @xmath183 by solving @xmath184 the optimal matrices @xmath74 and @xmath70 can be obtained by alternating optimization . when fixing @xmath74 as @xmath118 , the optimal @xmath70 can be obtained as in algorithm  [ algtwr ] , as described in section  [ secalg ] . when fixing @xmath70 as @xmath91 , the problem ( [ eqpen-1stage])becomes @xmath185+\\mu_1|{{\\mathbf{a}}}|\\ } \\\\ & & \\qquad = \\min_{{{\\mathbf{a}}}}\\{\\|{{\\mathbf{y}}}\\hat{{\\mathbf{g}}}-{{\\mathbf{x}}}{{\\mathbf{a}}}\\|_f^2+\\mu_1|{{\\mathbf{a}}}|\\},\\nonumber \\ ] ] which is equivalent to @xmath4 different problems , one for each column of @xmath74 , namely , @xmath186 where @xmath148 is the @xmath97th column of the matrix @xmath91 .",
    "each of these problems is a  standard lasso regression problem [ @xcite ] with over 10,000 variables . although efficient computational algorithms exist for the lasso regression , the fact that the lasso problem needs to be solved a few hundred times during each iteration of updating @xmath74 makes this approach computationally unattractive .",
    "developing a scalable algorithm for the one - stage approach is an important issue for its practical application and remains an interesting research topic .",
    "the authors thank the editor , the associate editor and two referees for their comments , which helped improve the scope and presentation of the manuscript .",
    "the authors thank the meg lab at the university of texas health science center houston for providing the data . in particular , thanks are due to professors andrew papanicolaou and eduardo castillo for their suggestions and comments ."
  ],
  "abstract_text": [
    "<S> the meg inverse problem refers to the reconstruction of the neural activity of the brain from magnetoencephalography ( meg ) measurements . </S>",
    "<S> we propose a two - way regularization ( twr ) method to solve the meg inverse problem under the assumptions that only a small number of locations in space are responsible for the measured signals ( focality ) , and each source time course is smooth in time ( smoothness ) . </S>",
    "<S> the focality and smoothness of the reconstructed signals are ensured respectively by imposing a sparsity - inducing penalty and a roughness penalty in the data fitting criterion . </S>",
    "<S> a two - stage algorithm is developed for fast computation , where a raw estimate of the source time course is obtained in the first stage and then refined in the second stage by the two - way regularization . </S>",
    "<S> the proposed method is shown to be effective on both synthetic and real - world examples .    ,    ,     +    . </S>"
  ]
}