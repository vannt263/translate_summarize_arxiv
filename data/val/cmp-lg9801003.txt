{
  "article_text": [
    "memory - based learning of classification tasks is a branch of supervised machine learning in which the learning phase consists simply of storing all encountered instances from a training set in memory @xcite .",
    "memory - based learning algorithms do not invest effort during learning in abstracting from the training data , such as eager - learning ( e.g. , decision - tree algorithms , rule - induction , or connectionist - learning algorithms , @xcite ) do .",
    "rather , they defer investing effort until new instances are presented .",
    "on being presented with an instance , a memory - based learning algorithm searches for a best - matching instance , or , more generically , a set of the @xmath0 best - matching instances in memory .",
    "having found such a set of @xmath0 best - matching instances , the algorithm takes the ( majority ) class with which the instances in the set are labeled to be the class of the new instance .",
    "pure memory - based learning algorithms implement the classic @xmath0-nearest neighbour algorithm @xcite ; in different contexts , memory - based learning algorithms have also been named lazy , instance - based , exemplar - based , memory - based , case - based learning or reasoning @xcite )    memory - based learning has been demonstrated to yield accurate models of various natural language tasks such as grapheme - phoneme conversion , word stress assignment , part - of - speech tagging , and pp - attachment @xcite .",
    "for example , the memory - based learning algorithm ib1-ig @xcite , which extends the well - known ib1 algorithm @xcite with an information - gain weighted similarity metric , has been demonstrated to perform adequately and , moreover , consistently and significantly better than _ eager - learning _ algorithms which do invest effort in abstraction during learning ( e.g. , decision - tree learning @xcite , and connectionist learning @xcite ) when trained and tested on a range of morpho - phonological tasks ( e.g. , morphological segmentation , grapheme - phoneme conversion , syllabification , and word stress assignment ) @xcite .",
    "thus , when learning nlp tasks , the abstraction occurring in decision trees ( i.e. , the explicit _ forgetting _ of information considered to be redundant ) and in connectionist networks ( i.e. , a non - symbolic encoding and decoding in relatively small numbers of connection weights ) both hamper accurate generalisation of the learned knowledge to new material .",
    "these findings appear to contrast with the general assumption behind eager learning , that data representing real - world classification tasks tends to contains ( i ) redundancy and ( ii ) exceptions : redundant data can be removed , yielding smaller descriptions of the original data ; some exceptions ( e.g. , low - frequency exceptions ) can ( or should ) be discarded since they are expected to be bad predictors for classifying new ( test ) material .",
    "however , both redundancy and exceptionality can not be computed trivially ; heuristic functions are generally used to estimate them ( e.g. , functions from information theory @xcite ) .",
    "the lower generalisation accuracies of both decision - tree and connectionist learning , compared to memory - based learning , on the above - mentioned nlp tasks , suggest that these heuristic estimates may not hold for data representing nlp tasks .",
    "it appears that in order to learn such tasks successfully , a learning algorithm should not forget ( i.e. , explicitly remove from memory ) any information contained in the learning material : it should not abstract from the individual instances",
    ".    an obvious type of abstraction that is not harmful for generalisation accuracy ( but that is not always acknowledged in implementations of memory - based learning ) is be the straightforward abstraction from tokens to types with frequency information .",
    "in general , data sets representing natural language tasks , when large enough , tend to contain considerable numbers of duplicate sequences mapping to the same output or class .",
    "for example , in data representing word pronunciations , some sequences of letters , such as ing at the end of english words , occur hundreds of times , while each of the sequences is pronounced identically , viz .",
    "/i/. instead of storing all individual sequence tokens in memory , each set of identical tokens can be safely stored in memory as a single sequence type with frequency information , without loss of generalisation accuracy @xcite .",
    "thus , forgetting instance tokens and replacing them by instance types may lead to considerable computational optimisations of memory - based learning , since the memory that needs to be searched may become considerably smaller .",
    "given the safe , performance - preserving optimisation of replacing sets of instance tokens by instance types with frequency information , a next step of investigation into optimising memory - based learning is to measure the effects of _ forgetting instance types _ on grounds of their exceptionality , the underlying idea being that the more exceptional a task instance type is , the more likely it is that it is a bad predictor for new instances .",
    "thus , exceptionality should in some way express the unsuitability of a task instance type to be a best match ( nearest neighbour ) to new instances : it would be unwise to copy its associated classification to best - matching new instances . in this paper",
    ", we investigate three criteria for estimating an instance type s exceptionality , and removing instance types estimated to be the most exceptional by each of these criteria .",
    "the criteria investigated are    1 .",
    "typicality of instance types ; 2 .   class prediction strength of instance types ; 3 .",
    "friendly - neighbourhood size of instance types ; 4 .",
    "random ( to provide a baseline experiment ) .",
    "we base our experiments on a large data set of english word pronunciation .",
    "we briefly describe this data set , and the way it is converted into an instance base fit for memory - based learning , in section  [ data ] . in section  [ experimental ] we describe the settings of our experiments and the memory - based learning algorithm ib1-ig with which the experiments are performed .",
    "we then turn to describing the notions of typicality , class - prediction strength , and friendly - neighbourhood size , and the functions to estimate them , in section  [ criteria ] .",
    "section  [ results ] provides the experimental results . in section  [ discussion ] , we discuss the obtained results and formulate our conclusions .",
    "converting written words to stressed phonemic transcription , i.e. , word pronunciation , is a well - known benchmark task in machine learning @xcite .",
    "we define the task as the conversion of fixed - sized instances representing parts of words to a class representing the phoneme and the stress marker of the instance s middle letter . to generate the instances ,",
    "windowing is used @xcite .",
    "table  [ overall - windows - ex ] displays example instances and their classifications generated on the basis of the sample word booking .",
    "classifications , i.e. , phonemes with stress markers ( henceforth pss ) , are denoted by composite labels .",
    "for example , the first instance in table  [ overall - windows - ex ] , _ _ _ book , maps to class label /b/1 , denoting a /b/ which is the first phoneme of a syllable receiving primary stress .",
    "in this study , we chose a fixed window width of seven letters , which offers sufficient context information for adequate performance , though extension of the window decreases ambiguity within the data set @xcite .    [ cols=\"^,^,^,^,^,^,^,^,^ \" , ]",
    "figure  [ edited - gs ] displays the generalisation accuracies in terms of incorrectly classified test instances obtained with all performed experiments . the leftmost point in the figure ,",
    "from which all lines originate , indicates the performance of ib1-ig when trained on the full data set of 222,601 types , viz .",
    "6.42% incorrectly classified test instances ( when computed in terms of incorrectly pronounced test words , ib1-ig pronounces 64.61 of all test words flawlessly ) .",
    "the line graph representing the four experiments in which instance types are removed randomly can be seen as the baseline graph .",
    "it can be expected that removing instances randomly leads to a degradation of generalisation performance .",
    "the upward curve of the line graph denoting the experiments with random selection indeed shows degrading performance with increasing numbers of left - out instance types .",
    "the relative decrease in generalisation accuracy is 2.0% when 1% of the training material is removed randomly , 3.8% with 2% random removal , 10.7% with 5% random removal , and 20.7% with 10% random removal .",
    "=    surprisingly , the only experiments showing lower performance degradation than removal by random selection are those with class - prediction strength ; the other criteria for removing exceptional instances lead to worse degradations . it does not matter whether instance types are removed on grounds of their typicality : apparently , a markedly low , neutral , or high typicality value indicates that the instance type is ( on average ) important , rather than removable .",
    "the same applies to friendly - neighbourhood size : instances with small neighbourhood sizes appear to contribute significantly to performance on test material .",
    "it is remarkable that the largest errors with 1% and 2% removal are obtained with the friendly - neighbourhood size criterion : it appears that on average , the instances with few or no nearest neighbours are important in the classification of test material .",
    "when using class - prediction strength as removal criterion , performance does not degrade until about 5% of the instance types with the lowest strength are removed from memory .",
    "the reason is that class - prediction strength is the only criterion that detects minority ambiguities , i.e. , instance types with prediction strength 0.0 , that can not contribute to classification since they are always overshadowed by their counterpart instance types with majority classes , even for their own classification . in the training",
    "set , 9,443 instance types are minority ambiguities , i.e. , 4.2% of the instance types ( accounting for 3.8% of the instance tokens in the original token set ) .",
    "thus , among the tested methods for reducing the memory needed for storing an instance base in memory - based learning , only two are performance - preserving while accounting for a substantial reduction in the amount of memory needed by ib1-ig :    1 .",
    "replacing instance tokens by instance types accounts for a reduction of about 63% of memory needed to store instances , excluding the memory needed to store frequency information .",
    "when frequency information is stored in two bytes per instance type , the memory reduction is about 54% .",
    "removing instance types that are minority ambiguities on top of the type / token - reduction accounts only for an additional memory reduction of 2% , i.e. , for a total memory reduction of 65% ; 56% with two - byte frequency information stored per instance .",
    "as previous research has suggested @xcite , keeping full memory in memory - based learning of word pronunciation strongly appears to yield optimal generalisation accuracy .",
    "the experiments in this paper show that optimisation of memory use in memory - based learning while preserving generalisation accuracy can only be performed by ( i ) replacing instance tokens by instance types with frequency information , and ( ii ) removing minority ambiguities .",
    "both optimisations can be performed straightforwardly ; minority ambiguities can be traced with less effort than by using class - prediction strength .",
    "our implementation of ib1-ig described in @xcite already makes use of this knowledge , albeit partially ( it stores class distributions with letter - window types ) .",
    "we note that with @xmath1 ( in our tested implementation of ib1-ig , @xmath2 ) , removing minority ambiguities may distort performance , since taking more than single nearest neighbours into account may allow for minority ambiguities to play a constructive role in classification .",
    "our results also show that atypicality , non - typicality , and typicality @xcite , and friendly - neighbourhood size are all estimates of exceptionality that indicate the importance of instance types for classification , rather than their removability . as far as these estimates of exceptionality",
    "are viable , our results suggest that exceptions should be kept in memory and not be thrown away .      *",
    "the tested criteria can be employed as instance weights as in each @xcite and pebls @xcite , rather than as criteria for instance removal .",
    "instance weighting may add relevant information to similarity matching , and may improve ib1-ig s performance rather than just preserving it .",
    "* the tested implementation of ib1-ig performs a @xmath0-nn search through instance space with @xmath2 .",
    "when @xmath3 , ib1-ig s performance may change , as well as the effect of applying the instance - removal techniques tested here . removing instances from memory may have a less drastic effect when more instance types at more distances are allowed to match a new instance .",
    "daelemans , w. 1996 .",
    "abstraction considered harmful : lazy learning of language processing . in h.  j. van den herik and a.  weijters , editors , _ proceedings of the sixth belgian  dutch conference on machine learning _",
    ", pages 312 , maastricht , the netherlands . matriks .",
    "daelemans , w. and a.  van den bosch",
    "generalisation performance of backpropagation learning on a syllabification task . in m.",
    "f.  j. drossaers and a.  nijholt , editors , _ twlt3 : connectionism and natural language processing _ , pages 2737 , enschede .",
    "twente university .",
    "daelemans , w. , a.  weijters , and a.  van den bosch .",
    "empirical learning of natural language processing tasks .",
    "lecture notes in artificial intelligence , , number 1224 , pages 337344 .",
    "berlin : springer - verlag .",
    "rumelhart , d.  e. , g.  e. hinton , and r.  j. williams .",
    "learning internal representations by error propagation . in d.",
    "e. rumelhart and j.  l. mcclelland , editors , _ parallel distributed processing : explorations in the microstructure of cognition _",
    ", volume 1 : foundations .",
    "cambridge , ma : the mit press , pages 318362 .            , a. , w.  daelemans , and a.  weijters .",
    "morphological analysis as classification : an inductive - learning approach . in k.",
    "oflazer and h.  somers , editors , _ proceedings of the second international conference on new methods in natural language processing , nemlap-2 , ankara , turkey _ , pages 7989 ."
  ],
  "abstract_text": [
    "<S> memory - based learning , keeping full memory of learning material , appears a viable approach to learning nlp tasks , and is often superior in generalisation accuracy to eager learning approaches that abstract from learning material . </S>",
    "<S> here we investigate three _ partial _ memory - based learning approaches which remove from memory specific task instance types estimated to be exceptional . </S>",
    "<S> the three approaches each implement one heuristic function for estimating exceptionality of instance types : ( i ) typicality , ( ii ) class prediction strength , and ( iii ) friendly - neighbourhood size . </S>",
    "<S> experiments are performed with the memory - based learning algorithm ib1-ig trained on english word pronunciation . </S>",
    "<S> we find that removing instance types with low prediction strength ( ii ) is the only tested method which does not seriously harm generalisation accuracy . </S>",
    "<S> we conclude that keeping full memory of types rather than tokens , and excluding minority ambiguities appear to be the only performance - preserving optimisations of memory - based learning . </S>"
  ]
}