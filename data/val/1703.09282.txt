{
  "article_text": [
    "the aim of the present paper is to present a range of cluster validation indexes that provide a multivariate assessment covering different complementary aspects of cluster validity . here",
    "i focus on `` internal '' validation criteria that measure the quality of a clustering without reference to external information such as a known `` true '' clustering .",
    "furthermore i am mostly interested in comparing different clusterings on the same data , which is often referred to as `` relative '' cluster validation .",
    "this can be used to select one of a set of clusterings from different methods , or from the same method ran with different parameters such as different numbers of clusters .    in the literature ( for an overview",
    "see halkidi _ _ et al.__@xcite ) many cluster validation indexes are proposed .",
    "usually these are advertised as measures of global cluster validation in a univariate way , often under the implicit or explicit assumption that for any given dataset there is only a single best clustering .",
    "mostly these indexes are based on contrasting a measure of within - cluster homogeneity and a measure of between - clusters heterogeneity such as the famous index proposed by calinski and harabasz@xcite , which is a standardised ratio of the traces of the pooled within - cluster covariance matrix and the covariance matrix of between - cluster means .    in hennig@xcite ( see also hennig@xcite )",
    "i have argued that depending on the subject - matter background and the clustering aim different clusterings can be optimal on the same dataset .",
    "for example , clustering can be used for data compression and information reduction , in which case it is important that all data are optimally represented by the cluster centroids ; or clustering can be used for recognition of meaningful patterns , which are often characterised by clear separating gaps between them . in the former situation , large within - cluster distances are not desirable , whereas in the latter situation large within - cluster distances may not be problematic as long as data objects occur with high density and without gap between the objects between which the distance is large .",
    "see figure [ fxyunif ] for two different clusterings on an artificial dataset with 3 clusters that may be preferable for these two different clustering aims .    given a multivariate characterisation of the validity of a clustering , for a given application a user can select weights for the different characteristics depending on the clustering aim and the relevance of the different criteria . a weighted average can then be used to choose a clustering that is suitable for the specific application .",
    "this requires that the criteria measuring different aspects of cluster validity and normalised in such a way that their values are comparable when doing the aggregation . although it is easy in most cases to define criteria in such a way that their value range is @xmath0 $ ] , this is not necessarily enough to make their values comparable , because within this range the criteria may have very different variation .",
    "the idea here is that the expected variation of the criteria can be explored using resampled random clusterings ( `` stupid k - centroids '' , `` stupid nearest neighbour clustering '' ) on the same dataset , and this can be used for normalisation and comparison .",
    "the approach presented here can also be used for benchmarking cluster analysis methods .",
    "particularly , it does not only allow to show that methods are better or worse on certain datasets , it also allows to characterise the specific strength and weaknesses of clustering algorithms in terms of the properties of the found clusters .",
    "section [ snotation ] introduces the general setup and defines notation . in section [ saspects ] ,",
    "all the indexes measuring different relevant aspects of a clustering are presented .",
    "section [ sagg ] defines an aggregated index that can be adapted to practical needs .",
    "the indexes can not be suitably aggregated in their raw form , and section [ sstupid ] introduces a calibration scheme using randmly generated clusterings .",
    "section [ sexamples ] applies the methodology to two datasets , one illustrative artificial one and a real dataset regarding species delimitation .",
    "section [ sconc ] concludes the paper .",
    "generally , cluster analysis is about finding groups in a set of objects @xmath1 .",
    "there is much literature in which the objects @xmath2 are assumed to be from euclidean space @xmath3 , but in principle the could be from any space @xmath4 .",
    "a clustering is a set @xmath5 with @xmath6 .",
    "the number of clusters @xmath7 may be fixed in advance or not . for @xmath8 ,",
    "let @xmath9 be the number of objects in @xmath10 .",
    "obviously not every such @xmath11 qualifies as a `` good '' or `` useful '' clustering , but what is demanded of @xmath11 differs in the different approaches of cluster analysis . here",
    "@xmath11 is required to be a partition , e.g. , @xmath12 and @xmath13 . for partitions ,",
    "let @xmath14 be the assignment function , i.e. , @xmath15 if @xmath16 .",
    "some of the indexes introduced below could also by applied to clusterings that are not partitions ( particularly objects that are not a member of any cluster could just be ignored ) , but this is not treated here to keep things simple .",
    "clusters are here also assumed to be crisp rather than fuzzy , i.e. , an object is either a full member of a cluster or not a member of this cluster at all . in case of probabilistic clusterings , which give as output probabilities @xmath17 for object @xmath18 to be member of cluster @xmath19 , it is assumed that objects are assigned to the cluster @xmath19 maximising @xmath17 ; in case of hierarchical clusterings it is assumed that the hierarchy is cut at a certain number of clusters @xmath7 to obtain a partition .",
    "most of the methods introduced here are based on dissimilarity data .",
    "a dissimilarity is a function @xmath20 so that @xmath21 and @xmath22 for @xmath23 .",
    "many dissimilarities are distances , i.e. , they also fulfil the triangle inequality , but this is not necessarily required here .",
    "dissimilarities are extremely flexible , they can be defined for all kinds of data , such as functions , time series , categorical data , image data , text data etc .",
    "if data are euclidean , obviously the euclidean distance can be used .",
    "see hennig@xcite for a more general overview of dissimilarity measures used in cluster analysis .",
    "in this section i introduce measurements for various aspects of cluster validity .      a major aim in most cluster analysis applications",
    "is to find homogeneous clusters .",
    "this often means that all the objects in a cluster should be very similar to each other , although it can in principle also have different meanings , e.g. , that a homogeneous probability model ( such as the gaussian distribution , potentially with large variance ) can account for all observations in a cluster .",
    "the most straightforward way to formalise that all objects within a cluster should be similar to each other is the average within - cluster distance : @xmath24 smaller values are better . knowing the data but not the clustering , the minimum possible value of @xmath25 is zero and the maximum is @xmath26 , so @xmath27 $ ] is a normalised version .",
    "when different criteria are aggregated ( see section [ sagg ] ) , it is useful to define them in such a way that they point in the same direction ; i will define all normalised indexes so that larger values are better . for this reason @xmath28",
    "is subtracted from 1 .",
    "there are alternative ways of measuring whether within - cluster dissimilarities are overall small .",
    "all of these operationalise cluster homogeneity in slightly different ways .",
    "the objective function of k - means clustering can be written down as a constant times the average of all squared within - cluster euclidean distances ( or more general dissimilarities ) , which is an alternative measure , giving more emphasis to the biggest within - cluster dissimilarities .",
    "most radically , one could use the maximum within - cluster dissimilarity . on the other hand one could use quantiles or trimmed means in order to make the index less sensitive to large within - cluster dissimilarities , although i believe that in most applications in which within - cluster similarity is important , these should be avoided and the index should therefore be sensitive against them .",
    "apart from within - cluster homogeneity , the separation between clusters is most often taken into account in the literature on cluster validation ( most univariate indexes balance separation against homogeneity in various ways ) .",
    "separation as it is usually understood can not be measured by averaging all between - cluster dissimilarities , because it refers to what goes on `` between '' the clusters , i.e. , the smallest between - cluster dissimilarities , whereas the dissimilarities between pairs of farthest objects from different clusters should not contribute to this .",
    "the most naive way to measure separation is to use the minimum between - cluster dissimilarity .",
    "this has the disadvantage that with more than two clusters it only looks at the two closest clusters , and also in many applications there may be an inclination to tolerate the odd very small distance between clusters if by and large the closest points of the clusters are well separated .",
    "i propose here an index that takes into account a portion @xmath29 , say @xmath30 , of objects in each cluster that are closest to another cluster .    for every object @xmath31 let @xmath32 .",
    "let @xmath33 be the values of @xmath34 for @xmath16 ordered from the smallest to the largest , and let @xmath35 be the largest integer @xmath36 .",
    "then the @xmath29-separation index is defined as @xmath37 obviously , @xmath38 $ ] and large values are good , therefore @xmath39 $ ] is a suitable normalisation .      in some applications clusters",
    "are used for information reduction , and one way of doing this is to use the cluster centroids for further analysis rather than the full dataset .",
    "it is then relevant to measure how well the observations in a cluster are represented by the cluster centroid .",
    "the most straightforward method to measure this is to average the dissimilarities of all objects to the centroid of the cluster they re assigned to .",
    "let @xmath40 be the centroids of clusters @xmath41 .",
    "then , @xmath42 some clustering methods such as k - means and partitioning around medoids ( pam , kaufman and rousseeuw@xcite ) are centroid - based , i.e. , they compute the cluster centroids along with the clusters .",
    "centroids can also be defined for the output of non - centroid - based methods , most easily as @xmath43 which corresponds to the definition of pam .",
    "again , there are possible variations .",
    "k - means uses squared euclidean distances , and in case of euclidean data the cluster centroids do not necessarily have to be members of @xmath44 , they could also be mean vectors of the observations in the clusters .    again , by definition , @xmath45 $ ] .",
    "small values are better , and therefore @xmath46 $ ] .",
    "another way in which the clustering can be used for information reduction is that the clustering can be seen as a more simple summary or representation of the dissimilarity structure .",
    "this can be measured by correlating the vector of pairwise dissimilarities",
    "@xmath47_{i < j}\\right)$ ] with the vector of a `` clustering induced dissimilarity '' @xmath48_{i < j}\\right)$ ] , where @xmath49 , and @xmath50 denotes the indicator function . with @xmath51 denoting the sample pearson correlation , @xmath52 this index goes back to hubert and schultz@xcite , see also halkidi _ _ et al.__@xcite for alternative versions .",
    "$ ] , and large values are good , so it can be normalised by @xmath54 $ ] .",
    "the idea that a cluster should be homogeneous can mean that there are no `` gaps '' within a cluster , and that the cluster is well connected .",
    "a gap can be characterised as a split of a cluster into two subclusters so that the minimum dissimilarity between the two subclusters is large .",
    "the corresponding index measures the `` length '' ( dissimilarity ) of the widest within - cluster gap ( an alternative would be to average widest gaps over clusters ) : @xmath55 @xmath56 $ ] and low values are good , so it is normalised as @xmath57 $ ] .    a version of this taking into account density values is defined in section [ sdens ] .",
    "widest gaps can be found computationally by constructing the within - cluster minimum spanning trees ; the widest distance occurring there is the widest gap .",
    "a very popular idea of a cluster is that it corresponds to a density mode , and that the density within a cluster goes down from the cluster mode to the outer regions of the cluster .",
    "correspondingly , there should be density valleys between different clusters .",
    "the definition of indexes that measure such a behaviour is based on a density function @xmath58 that assigns a density value @xmath59 to every observation . for euclidean data ,",
    "standard density estimators such as kernel density estimators can be used . for general dissimilarities ,",
    "i here propose a simple kernel density estimator .",
    "let @xmath60 be the @xmath29-quantile of the vector of dissimilarities @xmath61 , e.g. , for @xmath30 , the 10% smallest dissimilarities are @xmath62 .",
    "define the kernel and density as @xmath63 these can be normalised to take a maximum of 1 : @xmath64 alternatively , @xmath65 with @xmath66 being the dissimilarity to the @xmath67th nearest neighbour would be another simple dissimilarity - based density estimator , although this has no trivial upper bound ( @xmath58 , even before normalising by its within - cluster maximum , is bounded by @xmath68 ) .",
    "one could also standardise @xmath58 by the within - cluster maxima if clusters with generally lower densities should have the same weight as high density clusters , but lower density values rely on fewer observations and are therefore less reliable .",
    "three different aspects of density - based clustering are measured by three different indexes :    1 .",
    "the density should decrease within a cluster from the density mode to the `` outskirts '' of the cluster ( @xmath69 ) .",
    "2 .   cluster boundaries should run through density `` valleys '' , i.e. , high density points should not be close to many points from other clusters ( @xmath70 ) .",
    "3 .   there should not be a big gap between high density regions within a cluster ( @xmath71 ; gaps as measured by @xmath72 may be fine in the low density outskirts of a cluster ) .    the idea for @xmath69",
    "is as follows . for every cluster ,",
    "starting from the cluster mode , i.e. , the observation with the highest density , construct a growing sequence of observations that eventually covers the whole cluster by always adding the closest observation that is not yet included .",
    "optimally , in this process , the within - cluster density of newly included points should always decrease .",
    "whenever actually the density goes up , a penalty of the squared difference of the densities is incurred . the index @xmath69 aggregates these penalties .",
    "the following algorithm computes this , and it also constructs a set @xmath73 that collects information about high dissimilarities between high density observations and is used for the definition of @xmath71 below :    initialisation : :    @xmath74 , @xmath75 . for    @xmath8 :",
    "step 1 : :    @xmath76 , where    @xmath77 .",
    "step 2 : :    let @xmath78 .",
    "if    @xmath79 : @xmath80 , if    @xmath81 go to step 1 , if @xmath82 then go to    step 5 .",
    "otherwise : step 3 : :    find    @xmath83 .",
    "@xmath84 ,    @xmath85 .",
    "step 4 : :    if @xmath86 , back    to step 2 .",
    "step 5 : :    @xmath87    @xmath69 collects the penalties from increases of the within - cluster densities during this process .",
    "the definition of @xmath69 does not take into account whether the neighbouring observations that produce high density values @xmath88 for @xmath89 are in the same cluster as @xmath89 .",
    "but this is important , because it would otherwise be easy to achieve a good value of @xmath69 by cutting through high density areas and distributing a single high density area to several clusters .",
    "a second index can be defined that penalises a high contribution of points from different clusters to the density values in a cluster ( measured by @xmath90 below ) , because this means that the cluster border cuts through a high density region .",
    "@xmath91 normalising : @xmath92 a penalty is incurred if for observations with a large density @xmath88 there is a large contribution @xmath93 to that density from other clusters : @xmath94 both @xmath69 and @xmath70 are by definition @xmath95 . also , the maximum contribution of any observation to any of @xmath69 and @xmath70 is @xmath96 , because the normalised @xmath97-values are @xmath98 .",
    "these are penalties , so low values are good , and normalised versions are defined as @xmath99 an issue with @xmath69 is that it is possible that there is a large gap between two observations with high density , which does not incur penalties if there are no low - density observations in between .",
    "this can be picked up by a version of @xmath72 based on the density - weighted gap information collected in @xmath73 above .",
    "this is suggested instead of @xmath72 if a density - based cluster concept is of interest : @xmath100 @xmath101 $ ] and low values are good , so it is normalised as @xmath102 $ ]",
    ".      sometimes different clusters should not ( only ) be characterised by gaps between them ; overlapping regions in data space may be seen as different clusters if they have different within - cluster density levels , which in some applications could point to different data generating mechanisms behind the different clusters , which the researcher would like to discover .",
    "such a cluster concept would require that densities within clusters are more or less uniform .",
    "this can be characterised by the coefficient of variation cv of either the within - cluster density values or the dissimilarities to the @xmath67th nearest within - cluster neighbour @xmath103 ( say @xmath104 ) .",
    "the latter is preferred here because as opposed to the density values , @xmath103 is clean from the influence of observations from the other clusters .",
    "define for @xmath105 , assuming @xmath106 : @xmath107 using this , @xmath108 low values are good .",
    "the maximum value of the coefficient of variation based on @xmath68 observations is @xmath109 ( katsnelson and kotz@xcite ) , so a normalised version is @xmath110 .      in some clustering applications , particularly where clustering is done for `` organisational '' reasons such as information compression , it is useful to have clusters that are roughly of the same size .",
    "this can be measured by the entropy : @xmath111 large values are good .",
    "the entropy is maximised for fixed @xmath7 by @xmath112 , so it can be normalised by @xmath113 .      in case",
    "that there is a preference for a lower number of clusters , one could simply define @xmath114 ( already normalised ) with @xmath115 the maximum number of clusters of interest .",
    "if in a given application there is a known nonlinear loss connected to the number of clusters , this can obviously be used instead , and the principle can be applied also to other free parameters of a clustering method , if desired .",
    "sometimes the meaning of `` homogeneity '' for a cluster is defined by a homogeneous probability model , e.g. , gaussian mixture model - based clustering models all clusters by gaussian distributions with different parameters , requiring euclidean data .",
    "historically , due to the central limit theorem and quetelet s `` elementary error hypothesis '' , measurement errors were widely believed to be normally / gaussian distributed ( see stigler@xcite ) .",
    "under such a hypothesis it makes sense in some situations to regard gaussian distributed observations as homogeneous , and as pointing to the same underlying mechanism ; this could also motivate to cluster observations together that look like being generated from the same ( approximate ) gaussian distribution .",
    "indexes that measure cluster - wise gaussianity can be defined , see , e.g. , lago - fernandez and corbacho@xcite .",
    "one possible principle is to compare a one - dimensional function of the observations within a cluster to its theoretical distribution under the data distribution of interest ; e.g. , coretto and hennig@xcite compare the mahalanobis distances of observations to their cluster centre with their theoretical @xmath116-distribution using the kolmogorow - distance .",
    "this is also possible for other distributions of interest .",
    "clusterings are often interpreted as meaningful in the sense that they can be generalised as substantive patterns .",
    "this at least implicitly requires that they are stable .",
    "stability in cluster analysis can be explored using resampling techniques such as bootstrap and splitting the dataset , and clustering from different resampled datasets can be compared .",
    "this requires to run the clustering method again on the resampled datasets and i will not treat this here in detail , but useful indexes have been defined using this principle , see , e.g. , tibshirani and walther@xcite and fang and wang@xcite .",
    "hennig@xcite lists further potentially desirable characteristics of a clustering , for which further indexes could be defined :    * areas in data space corresponding to clusters should have certain characteristics such as being linear or convex .",
    "* it should be possible to characterise clusters using a small number of variables .",
    "* clusters should correspond well to an externally given partition or values of an external variable ( this could for example imply that clusters of regions should be spatially connected ) . *",
    "variables should be approximately independent within clusters .",
    "the required cluster concept and therefore the way the validation indexes can be used depends on the specific clustering application .",
    "the users need to specify what characteristics of the clustering are desired in the application .",
    "the corresponding indexes can then be aggregated to form a single criterion that can be used to compare different clustering methods , different numbers of clusters and other possible parameter choices of the clustering .",
    "the most straightforward aggregation is to compute a weighted mean of @xmath117 selected indexes @xmath118 with weights @xmath119 expressing the relative importance of the different methods : @xmath120 assuming that large values are desirable for all of @xmath118 , the best clustering for the application in question can be found by maximising @xmath121 .",
    "this can be done by comparing different clusterings from conventional clustering methods , but in principle it would also be an option to try to optimise @xmath121 directly .",
    "the weights can only be chosen to directly reflect the relative importance of the various aspects of a clustering if the values ( or , more precisely , their variations ) of the indexes @xmath118 are comparable , and give the indexes equal influence on @xmath121 if all weights are equal . in section [ saspects ]",
    "i proposed tentative normalisations of all indexes , which give all indexes the same value range @xmath0 $ ] .",
    "unfortunately this is not good enough to ensure comparability ; on many datasets some of these indexes will cover almost the whole value range whereas other indexes may be larger than 0.9 for all clusterings that any clustering method would come up with .",
    "therefore , section [ sstupid ] will introduce a new computational method to standardise the variation of the different criteria .",
    "another issue is that some indexes by their very nature favour large numbers of clusters @xmath7 ( obviously large within - cluster dissimilarities can be more easily avoided for large @xmath7 ) , whereas others favour small values of @xmath7 ( separation is more difficult to achieve with many small clusters ) .",
    "the method introduced in section [ sstupid ] will allow to assess the extent to which the indexes deliver systematically larger or smaller values for larger @xmath7 .",
    "note that this can also be an issue for univariate `` global '' validation indexes from the literature , see hennig and lin@xcite .    if the indexes should be used to find an optimal value of @xmath7 , the indexes in @xmath121 should be chosen in such a way that indexes that systematically favour larger @xmath7 and indexes that systematically favour smaller @xmath7 are balanced .",
    "the user needs to take into account that the proposed indexes are not independent .",
    "for example , good representation of objects by centroids will normally be correlated with having generally small within - cluster dissimilarities . including both indexes will assign extra weight to the information that the two indexes have in common ( which may sometimes but not always be desired ) .",
    "there are alternative ways to aggregate the information from the different indexes .",
    "for example , one could use some indexes as side conditions rather than involving them in the definition of @xmath121 .",
    "for example , rather than giving entropy a weight for aggregation as part of @xmath121 , one may specify a certain minimum entropy value below which clusterings are not accepted , but not use the entropy value to distinguish between clusterings that fulfil the minimum entropy requirement .",
    "multiplicative aggregation is another option .",
    "as explained above , the normalisation in section [ saspects ] does not provide a proper calibration of the validation indexes .",
    "here is an idea for doing this in a more appropriate way .",
    "the idea is that random clusterings are generated on @xmath44 and index values are computed , in order to explore what range of index values can be expected on @xmath44 , so that the clusterings of interest can be compared to these .",
    "so in this section , as opposed to conventional probability modelling , the dataset is considered as fixed but a distribution of index values is generated from various random partitions .",
    "completely random clusterings ( i.e. , assigning every observation independently to a cluster ) are not suitable for this , because it can be expected that indexes formalising desirable characteristics of a clustering will normally give much worse values for them than for clusters that were generated by a clustering method .",
    "therefore i propose two methods for random clusterings that are meant to generate clusterings that make some sense , at least by being connected in data space .",
    "the methods are called `` stupid k - centroids '' and `` stupid nearest neighbours '' ; `` stupid '' because they are versions of popular clustering methods ( centroid - based clustering like k - means or pam , and single linkage / nearest neighbour ) that replace optimisation by random decisions and are meant to be computable very quickly .",
    "centroid - based clustering normally produces somewhat compact clusters , whereas single linkage is notorious for prioritising cluster separation totally over within - cluster homogeneity , and therefore one should expect these two approaches to explore in a certain sense opposite ways of clustering the data .",
    "stupid k - centroids works as follows .",
    "for fixed number of cluster @xmath7 draw a set of @xmath7 cluster centroids @xmath122 from @xmath44 so that every subset of size @xmath7 has the same probability of being drawn .",
    "@xmath123 is defined by assigning every observation to the closest centroid : @xmath124      again , for fixed number of cluster @xmath7 draw a set of @xmath7 cluster initialisation points @xmath122 from @xmath44 so that every subset of size @xmath7 has the same probability of being drawn .",
    "@xmath125 is defined by successively adding the not yet assigned observation closest to any cluster to that cluster until all observations are clustered :    initialisation : :    let @xmath126 .",
    "let    @xmath127 step 1 : :    let @xmath128 .",
    "if    @xmath129 , find    @xmath130 ,    otherwise stop .",
    "step 2 : :    let @xmath131 . for",
    "the    @xmath132 with @xmath133 , let    @xmath134 , updating    @xmath135 accordingly .",
    "go back to step 1 .    at the end ,",
    "@xmath136 .",
    "the random clusterings can be used in various ways to calibrate the indexes . for any value @xmath7 of interest , @xmath137 clusterings @xmath138 @xmath139 on @xmath44",
    "are generated , say @xmath140 .",
    "as mentioned before , indexes may systematically change over @xmath7 and therefore may show a preference for either large or small @xmath7 . in order to account for this , it is possible to calibrate the indexes using stupid clusterings for the same @xmath7 , i.e. , for a clustering @xmath11 with @xmath141 .",
    "consider an index @xmath142 of interest ( the normalised version is used here because this means that large values are good for all indexes ) .",
    "then , @xmath143 where @xmath144 .",
    "a desired set of calibrated indexes can then be used for aggregation in ( [ eagg ] ) .",
    "an important alternative to ( [ ecali ] ) is calibration by using random clusterings for all values of @xmath7 together .",
    "let @xmath145 be the numbers of clusters of interest ( most indexes will not work for @xmath146 ) , @xmath147 , @xmath148 . with this",
    ", @xmath149 @xmath150 does not correct for potential systematic tendencies of the indexes over @xmath151 , but this is not a problem if the user is happy to use the uncalibrated indexes directly for comparing different values of @xmath7 ; a potential bias toward large or small values of @xmath7 in this case needs to be addressed by choosing the indexes to be aggregated in ( [ eagg ] ) in a balanced way .",
    "this can be checked by computing the aggregated index @xmath121 also for the random clusterings and check how these change over the different values of @xmath7 .",
    "another alternative is to calibrate indexes by using their rank value in the set of clusterings ( random clusterings and clusterings to compare ) rather than a mean / standard deviation - based standardisation .",
    "this is probably more robust but comes with some loss of information .",
    "the first example is the artificial dataset shown in figure [ fxyunif ] .",
    "four clusterings are compared ( actually many more clusterings with @xmath7 between 2 and 5 were compared on these data , but the selected clusterings illustrate the most interesting issues ) .",
    "the clusterings were computed by k - means with @xmath152 and @xmath153 , single linkage cut at @xmath153 and pam with @xmath154 .",
    "the k - means clustering with @xmath153 and the single linkage clustering are shown in figure [ fxyunif ] .",
    "the k - means clustering with @xmath152 puts the uniformly distributed widespread point cloud on top together in a single cluster , and the two smaller populations are the second cluster .",
    "this is the most intuitive clustering for these data for @xmath152 and also delivered by most other clustering methods .",
    "pam does not separate the two smaller ( actually gaussian ) populations for @xmath152 , but it does so for @xmath154 , along with splitting the uniform point cloud into three parts .",
    ".normalised index values for four clusterings on artificial data .",
    "[ cols=\"<,>,>,>,>\",options=\"header \" , ]     table [ ttetrac ] shows the corresponding results with calibration using all random clusterings .",
    "this does not result in a different ranking of the clusterings , so this dataset does not give a clear hint which of the two calibration methods is more suitable , or , in other words , the results do not depend on which one is chosen .",
    "the multivariate array of cluster validation indexes presented here provides the user with a detailed characterisation of various relevant aspects of a clustering .",
    "the user can aggregate the indexes in a suitable way to find a useful clustering for the clustering aim at hand .",
    "the indexes can also be used to provide a more detailed comparison of different clustering methods in benchmark studies , and a better understanding of their characteristics .",
    "the methodology is currently partly implemented in the `` fpc''-package of the statistical software system r and will soon be fully implemented there .",
    "most indexes require @xmath155 and the approach can therefore not directly be used for deciding whether the dataset is homogeneous as a whole ( @xmath146 ) .",
    "the individual indexes as well as the aggregated index could be used in a parametric bootstrap scheme as proposed by hennig and lin@xcite to test the homogeneity null hypothesis against a clustering alternative .",
    "research is still required in order to compare the different calibration methods and some alternative versions of indexes .",
    "a theoretical characterisation of the indexes is of interest as well as a study exploring the strength of the information overlap between some of the indexes , looking at , e.g. , correlations over various clusterings and datasets .",
    "random clustering calibration may also be used together with traditional univariate validation indexes .",
    "further methods for random clustering could be developed and it could be explored what collection of random clusterings is most suitable for calibration ( some work in this direction is currently done by my phd student serhat akhanli ) .",
    "this work was supported by epsrc grant ep / k033972/1 .",
    "a. m. bowcock , a. ruiz - linares , j. tomfohrde , e. minch , j. r. kidd , and l. l. cavalli - sforza .",
    "high resolution of human evolutionary trees with polymorphic microsatellites .",
    "_ nature _ , 368 , 455457 , 1994 . t. calinski and j. harabasz . a dendrite method for cluster analysis .",
    "_ communications in statistics _ 3 , 1-27 , 1974 .",
    "p. coretto and c. hennig .",
    "robust improper maximum likelihood : tuning , computation , and a comparison with other methods for robust gaussian clustering . _ journal of the american statistical association _ 111 , 16481659 , 2016 . y. fang and j. wang .",
    "selection of the number of clusters via the bootstrap method .",
    "_ computational statistics and data analysis _",
    ", 56 , 468 - 477 , 2012 .",
    "p. franck , e. cameron , g. good , j .- y .",
    "rasplus and b. p. oldroyd . nest architecture and genetic differentiation in a species complex of australian stingless bees .",
    "_ molecular ecology _ , 13 , 23172331 , 2004 . m. halkidi , m. vazirgiannis and c. hennig .",
    "method - independent indices for cluster validation and estimating the number of clusters . in _ handbook of cluster analysis _ , c. hennig , m. meila , f. murtagh , r. rocci ( eds . ) , crc / chapman & hall , boca raton , 703730 , 2016 . b. hausdorf and c. hennig . species delimitation using dominant and codominant multilocus markers . _ systematic biology _ , 59 , 491503 , 2010 . c. hennig .",
    "how many bee species ?",
    "a case study in determining the number of clusters . in _ data analysis , machine learning and knowledge discovery _ , m. spiliopoulou , l. schmidt - thieme , r. janning ( eds . ) , springer , berlin , 4149 , 2013 .",
    "c. hennig .",
    "what are the true clusters ?",
    "_ pattern recognition letters _ 64 , 5362 , 2015 . c. hennig .",
    "clustering strategy and method selection . in _ handbook of cluster analysis _ , c. hennig , m. meila , f. murtagh , r. rocci ( eds . ) , crc / chapman & hall , boca raton , 703730 , 2016 . c. hennig and c .- j .",
    "flexible parametric bootstrap for testing homogeneity against clustering and assessing the number of clusters . _ statistics and computing _ 25 , 821833 , 2015 . l. j. hubert and p. arabie .",
    "comparing partitions . _ journal of classification _ , 2 , 193218 , 1985 . l. j. hubert and j. schultz .",
    "quadratic assignment as a general data analysis strategy .",
    "_ british journal of mathematical and statistical psychology _ 29 , 190-241 , 1976 . l. kaufman and p. j. rousseeuw .",
    "_ finding groups in data _ , wiley , new york , 1990 .",
    "j. katsnelson and s. kotz . on the upper limits of some measures of variability .",
    "_ archiv fr meteorologie , geophysik und bioklimatologie , series b _ 8 , 103-107 , 1957 . l. f. lago - fernandez and f. corbacho .",
    "normality - based validation for crisp clustering .",
    "_ pattern recognition _ 43 , 782-795 , 2010 . s. stigler .",
    "_ the history of statistics : the measurement of uncertainty before 1900_. harvard university press , cambridge , 1986 .",
    "r. tibshirani and g. walther .",
    "cluster validation by prediction strength .",
    "_ journal of computational and graphical statistics _ , 14 , 511528 , 2005 ."
  ],
  "abstract_text": [
    "<S> there are many cluster analysis methods that can produce quite different clusterings on the same dataset . </S>",
    "<S> cluster validation is about the evaluation of the quality of a clustering ; `` relative cluster validation '' is about using such criteria to compare clusterings . </S>",
    "<S> this can be used to select one of a set of clusterings from different methods , or from the same method ran with different parameters such as different numbers of clusters .    </S>",
    "<S> there are many cluster validation indexes in the literature . </S>",
    "<S> most of them attempt to measure the overall quality of a clustering by a single number , but this can be inappropriate . there are various different characteristics of a clustering that can be relevant in practice , depending on the aim of clustering , such as low within - cluster distances and high between - cluster separation .    in this paper , a number of validation criteria will be introduced that refer to different desirable characteristics of a clustering , and that characterise a clustering in a multidimensional way . in specific applications </S>",
    "<S> the user may be interested in some of these criteria rather than others . </S>",
    "<S> a focus of the paper is on methodology to standardise the different characteristics so that users can aggregate them in a suitable way specifying weights for the various criteria that are relevant in the clustering application at hand .   </S>",
    "<S> +   + * keywords : * number of clusters , separation , homogeneity , density mode , random clustering </S>"
  ]
}