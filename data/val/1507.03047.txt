{
  "article_text": [
    "several important real - world processes can be conceptualised as a series of events occurring in time and space , possibly spontaneously , where each event has the capacity to trigger subsequent events .",
    "for example , events in epidemiology correspond to the infection of an individual by a transmissible disease ; the infected individual may then go on to cause subsequent infections . in many settings it is possible to obtain large , detailed data that catalogue the occurrence of events but do not specify individual cause - effect relationships .",
    "for example , in epidemiology it is generally not possible to identify the cause of a specific infection .",
    "cascades of poisson processes ( copp ) have been successfully applied to facilitate inference and prediction in this setting .",
    "copp were originally developed by the geological community in the 1980s for statistical modelling of earthquake / aftershock data @xcite .",
    "recent and varied applications of copp models have included modelling the dynamics of retaliatory gang violence @xcite , terrorist activity @xcite , success of commercial book sales @xcite , military conflicts @xcite , disease at the cellular level @xcite , `` retweet cascades '' on the social network twitter and editing patterns on wikipedia @xcite .    in mathematical terms",
    ", copp may be viewed as branching processes with immigration .",
    "inference in general branching processes is challenging since complex , nonlinear , multidimensional models do not generally admit closed form solutions for estimators @xcite .",
    "moreover copp models can produce multimodal or very flat log - likelihood functions , precluding reliable numerical algorithms . to overcome these difficulties",
    ", @xcite viewed inference as an incomplete data problem and adapted the expectation - maximisation ( em ) algorithm @xcite to this setting . in brief ,",
    "the information about which events arose from the background , which events were triggered and what those triggers were ( collectively the `` branching structure '' ) is required to specify the likelihood , but is unobservable . by describing the branching structure probabilistically , the em algorithm seeks instead to maximise the expected likelihood , which can be achieved for certain parametric copp models ( see @xcite for examples ) .    in the absence of a mechanistic understanding of either the background or triggering processes ,",
    "nonparametric estimators play an important role @xcite",
    ". however @xcite showed that these estimators can exhibit large bias at small - to - moderate sample sizes ( @xmath3 ) ; it is therefore important to integrate as many samples as possible into nonparametric inference .",
    "unfortunately existing algorithms scale poorly , with computational and storage complexity of existing approaches being @xmath0 .",
    "for example , on earthquake data @xcite reported using a sample size of @xmath4 due to computational limitations , whereas seismological datasets now regularly exceed @xmath5 @xcite .",
    "the `` big data '' now available in many application areas for copp models delivers a pressing need to develop scalable nonparametric estimation procedures .    in this paper",
    "we develop accelerated nonparametrics for copp .",
    "our approach is based on truncation of kernel density estimators ( kdes ) to include only terms which are temporally and spatially local to an event of interest .",
    "the domains of the kdes are chosen adaptively based on nearest neighbour distances , rendering the approach essentially free from hyperparameters .",
    "the complete algorithm , described in section [ methods ] , enjoys computational complexity of just @xmath1 and requires @xmath2 storage .",
    "using seismological data , we demonstrate in section [ results ] that our estimators facilitate the analysis of larger datasets than was previously possible and provide an empirical assessment of performance in this setting .",
    "section [ discuss ] closes with a discussion of challenges and extensions to our methodology .",
    "we proceed as follows : sections [ notations ] and [ defs ] set up notation and formally define the copp model class .",
    "section [ algorithms ] surveys existing algorithms and nonparametric estimators for copp .",
    "section [ core ] contains the core of our methodology for scaling inference to large datasets .",
    "consider a metric space @xmath6 formed as the finite product @xmath7 where each coordinate @xmath8 is assumed to be a locally compact metric space . here",
    "@xmath9 is a product metric ; in this paper all product metrics are induced by the euclidean norm @xmath10 , so that @xmath11 for @xmath12 .",
    "consider a simple point process @xmath13 on @xmath14 adapted to a filtration @xmath15 .",
    "for clarity we equip the time domain @xmath16 with the euclidean metric , though an arbitrary locally compact metric may be used .",
    "write @xmath17 for the open ball of radius @xmath18 centred on @xmath19 .",
    "a measure @xmath20 on @xmath14 , known as the conditional intensity of the point process @xmath13 , is given by @xmath21 in this paper @xmath15 will be the natural filtration ; the history up to time @xmath22 , i.e. @xmath23 where @xmath24 is the time and @xmath25 are additional coordinates associated with event @xmath26 ( e.g. spatial coordinates ) .",
    "it can be shown that the finite - dimensional distributions of @xmath13 are characterised by the conditional intensity function @xmath20 ; in this paper inference and prediction for @xmath13 is carried out entirely through inference for @xmath20 .",
    "( we assume @xmath13 is such that the associated measure @xmath20 is finite on every compact subset of @xmath14 and contains no atoms , so that eqn .",
    "[ limit ] is well - defined .",
    ")      this paper concerns the class of point processes @xmath13 known variously as branching processes with immigration , copp models @xcite , `` self - exciting point processes '' @xcite , `` epidemic type aftershock sequences '' @xcite and `` hawkes processes '' @xcite .",
    "such models are characterised by a conditional intensity @xmath20 which is expressed as a superposition of point processes : @xmath27 here @xmath28 denotes a ( possibly time - varying ) background intensity and each @xmath29 represents a `` triggering '' intensity , giving rise to offspring from the trigger event @xmath30 . in this formulation ,",
    "all events @xmath30 are able , in principle , to trigger offspring at future times @xmath31 ; the generative model therefore can give rise to `` cascades '' of events , which explains the copp nomenclature ( fig . [ illustrate ] ) . estimation for copp models",
    "is equivalent to estimation for the background ( @xmath28 ) and triggering ( @xmath32 ) intensities . in an application to earthquake data",
    "below , we model a time - independent background intensity @xmath33 , but for completeness we present the general case of time - varying background intensities .      in applications of copp models , data @xmath34 obtained over a time interval",
    "@xmath35 $ ] do not themselves specify which events arose spontaneously , which events were triggered by previous events and what these triggers actually were ( collectively the `` branching structure '' ) .",
    "such information can be though of as a forest @xmath36 on the events in @xmath34 and is illustrated in fig .",
    "[ illustrate ] . together",
    "@xmath37 can be thought of as complete data ; inference for each of @xmath28 and @xmath32 based on @xmath38 is a straight forward density estimation problem . in applications where the branching structure @xmath36 is unobservable , inference based only on @xmath34",
    "can proceed via the em algorithm @xcite . in the case where @xmath39 , @xmath40 are parametrised by @xmath41 @xcite ,",
    "the em algorithm proceeds by alternating between taking an expectation over random forests @xmath36 ( `` e - step '' ) and maximising the expected log - likelihood @xmath42 over parameters @xmath41 ( `` m - step '' ) .",
    "this procedure , which is sometimes referred to as `` stochastic declustering '' , also exists in nonparametric flavours @xcite .",
    "whilst theoretically elegant , stochastic declustering is highly computational ; for each event @xmath26 it is required to exhaustively enumerate all possible triggers of @xmath26 , along with their associated probabilities of being the actual trigger .",
    "consequently the m - step must be performed with @xmath0 weighted samples . to reduce this computational burden",
    ", @xcite developed a monte carlo alternative which approximates the e - step by sampling a single forest from the conditional distribution @xmath43 , then substituting this into the complete data likelihood to facilitate the m - step based on only @xmath2 unweighted samples .",
    "as a consequence the algorithm does not converge to a single pair of conditional intensities @xmath28 , @xmath32 , but instead samples from a set of plausible intensities . this algorithm has demonstrated good performance in practice @xcite but does not currently share the same theoretical guarantees as the em approach .",
    "note that the e - step of both existing approaches requires @xmath0 computational and storage complexities . in this paper",
    "we develop a principled methodology , based on the em algorithm , that requires only @xmath44 computational complexity and @xmath2 storage complexity .",
    "below we outline the main component of our proposed methodology , that targets the computational and storage complexity of both the e - step and the m - step in stochastic declustering .",
    "background events that are distant ( either spatially or temporally ) to @xmath45 are likely to contribute little to the value of any estimate @xmath46 of the background intensity at @xmath45 .",
    "our proposal is therefore to estimate intensity functions using _ only _ local information .",
    "a naive approach would be to restrict attention to events within a ball @xmath47 of fixed radius @xmath48 about the point @xmath45 .",
    "however , in areas where background events are sparse , this ball might not contain enough points to enable an accurate approximation of the background intensity at @xmath45 .",
    "noting that the all - nearest - neighbours problem can be solved with time complexity @xmath44 and storage complexity @xmath2 @xcite , it is computationally appealing to choose @xmath48 adaptively such that the ball includes a constant number @xmath49 of nearest neighbours . here",
    "@xmath49 should be taken sufficiently large that approximation error , at the level of derived quantities of interest , is negligible .",
    "full details are provided below .",
    "it will be convenient to relabel the time variable @xmath22 as the first coordinate , so that our data @xmath34 can be represented by a @xmath50 matrix @xmath51 whose entries in the first column @xmath52 represent the time of the @xmath26th event , and the remaining columns @xmath53 represent additional covariates associated with event @xmath26 ( typically spatial coordinates ) .",
    "write @xmath54 for the row index in @xmath51 of the @xmath55th closest event to event @xmath26 as measured by the standardised euclidean distance @xmath56 where @xmath57 and @xmath58 is a characteristic length scale for the covariates @xmath59 that must be specified .",
    "we adopt the convention that @xmath60 and consider @xmath61 .",
    "our approach is nonparametric and will be presented here using gaussian kernels .",
    "we will write @xmath62 for the density of a multivariate gaussian with mean @xmath63 and covariance @xmath64 , evaluated at the point @xmath65 .",
    "then a simple kde , based on data points @xmath66 , is given by @xmath67 here @xmath68 is a hyperparameter that controls kernel bandwidth ; an optimal choice , in terms of minimising mean square error ( mse ) , is given by @xmath69 , with corresponding mse @xmath70 ( see e.g. * ? ? ?",
    "_ remark 1 _ : the use of a vanishing - tailed kernel implies that influences are considered to act locally .",
    "we note that our methodology below also applies to non - gaussian kernels , provided that they are local in this sense .    _ remark 2 _ : the characteristic length scales @xmath71 that must be specified will not affect inference when @xmath49 is sufficiently large and are therefore not hyperparameters _ per se_. indeed , when @xmath49 is increased we allow more distant events to be considered as possible causes , but these will have negligible contribution to any sensible kde .",
    "_ remark 3 _ : the simple , illustrative estimate in eqn .",
    "[ simple kde ] does not respect the copp model structure in eqn .",
    "[ copp eqn ] .",
    "below we accelerate computation in stochastic declustering , directly exploiting the copp model structure .      for accelerated computation the statistical",
    "is encoded by a @xmath72 matrix @xmath73 with entries @xmath74 \\\\ p_{i , l } & = & \\mathbb{p}[\\text{event } i\\text { was triggered by event } \\alpha(i , l)].\\end{aligned}\\ ] ] in particular we assume that @xmath73 is a stochastic matrix ( unit row sums ) , i.e. the cause of an event always belongs to its @xmath49 nearest neighbours . in practice",
    "it will be necessary to choose @xmath49 sufficiently large that results are approximately invariant to further increase in @xmath49 .",
    "the matrix @xmath73 characterises the distribution @xmath75 over forests @xmath36 given data @xmath34 and intensity functions @xmath76 .",
    "indeed , we have that @xmath77 where @xmath78 by restricting attention to the @xmath49 nearest neighbours , this accelerated e - step requires @xmath2 complexity and storage , compared to the @xmath0 of existing approaches .",
    "conditional on @xmath73 , we define estimates @xmath79 for the mean and standard deviation of the @xmath80th covariate corresponding to background events .",
    "fix an integer @xmath81 such that @xmath82 and write @xmath83 for the index in @xmath51 of the @xmath81th closest background event to event @xmath26 .",
    "@xmath81 will be used to adaptively select an appropriate kernel bandwidth and can be elicited following the optimal @xmath84 rule .",
    "now @xmath83 is an unknown quantity with uncertainty encoded by the entries in @xmath73 ; bearing this in mind we can define @xmath85 \\label{eq dx}\\end{aligned}\\ ] ] where the expectation is taken over all possible assignments of background and trigger events , weighted according to @xmath73 .",
    "then @xmath86 is an estimate for the ( standardised ) distance from event @xmath26 to the @xmath55th nearest background event .",
    "these values may be obtained exactly @xcite or numerically using monte carlo estimation @xcite , in the latter case sampling a branching structure @xmath36 from @xmath43 and then computing @xmath87 directly based on @xmath36 . in experiments below",
    "we took the latter approach .",
    "we estimate the background intensity at event @xmath88 as @xmath89 when @xmath90 , the second approximation becomes exact , but for @xmath91 , the latter expression provides a relaxation of the former , with favourable computational and storage complexity .",
    "the same principle of adaptive truncation based on nearest neighbour distances is used in the differential domain to approximate the triggering function @xmath32 .",
    "we construct a @xmath92 matrix @xmath93 whose entry in the @xmath94th row and @xmath80th column is given by @xmath95 , i.e. the @xmath96-dimensional vector that joins event @xmath97 to event @xmath26 .",
    "we refer to these vectors as `` @xmath98-events '' .",
    "let @xmath99 be a @xmath100 vector with @xmath101th entry @xmath102 for @xmath103 and @xmath104 for @xmath105 .",
    "conditional on @xmath99 , we define @xmath106 to be the sample mean and standard deviation of @xmath98-events .",
    "write @xmath107 for the index in @xmath93 of the @xmath108th closest @xmath98-event to @xmath98-event @xmath26 , as measured by @xmath109 where @xmath58 is the standard deviation of @xmath110 , with the convention that @xmath111 .",
    "also write @xmath112 for the index in @xmath93 of the @xmath113th closest cause - effect event to the @xmath98-event @xmath26 .",
    "as before , the optimal bandwidth can be elicited following the @xmath114 rule .",
    "now @xmath112 is an unknown quantity with uncertainty encoded by the entries in @xmath73 ; bearing this in mind we can define @xmath115 \\label{eq dy}\\end{aligned}\\ ] ] where the expectation is taken over all possible assignments of background and trigger events , weighted according to @xmath73 .",
    "these values may again be obtained exactly or numerically using monte carlo estimation , and we did the latter .",
    "we estimate the trigger intensity at @xmath88 , as contributed by event @xmath54 , as @xmath116 again , when @xmath117 , eqn .",
    "[ eq g ] is exactly the standard kde based on all data points @xmath118 , but for @xmath119 , eqn .",
    "[ eq g ] provides a relaxation of this estimator with favourable computational and storage complexity .",
    "this accelerated m - step has computational complexity @xmath44 , compared to the @xmath0 complexity of existing approaches .",
    "moreover since the @xmath49-nearest neighbour problem can be solved using @xmath2 storage , we also achieve @xmath2 storage requirements , compared to the @xmath0 of existing approaches .    2    compute and cache @xmath93 , @xmath120 ,",
    "@xmath121 initialise @xmath122 and @xmath123    compute @xmath124 , @xmath125 using eqn .",
    "[ eq mx ] compute @xmath126 using eqn .",
    "[ eq dx ] estimate @xmath127 using eqn .",
    "[ mu en long ] compute @xmath128 , @xmath129 using eqn .",
    "[ eq my ] compute @xmath130 using eqn .",
    "[ eq dy ] estimate @xmath131 using eqn .",
    "[ eq g ]    re - estimate @xmath132 and @xmath133 using eqn .",
    "[ pq ] @xmath134 , @xmath135    the accelerated algorithm presented above , which is somewhat tricky to derive , is the main contribution of this paper .",
    "complete pseudocode is provided in alg .",
    "[ alg ] , which proceeds by initialising the distribution over forests , as encoded by @xmath136 . for all experiments in this paper we used a uniform distribution such that @xmath137 and @xmath138 otherwise",
    ". then entries of @xmath73 were set to zero according to whether @xmath139 and @xmath73 was row - normalised to ensure each row defines a probability distribution .",
    "the algorithm is terminated when consecutive iterations change the distribution over branching structures @xmath36 by less than @xmath140 in total variation distance .",
    "_ remark 4 _ : as is common for em - type algorithms , formal convergence analysis is mathematically intractable ; the estimator need not converge to a global optimum and estimator performance must be assessed empirically .",
    "we proceed as follows : section [ eqdata ] describes a typical application of copp models arising in seismology .",
    "section [ at sec ] investigates empirically the computational advantages of the accelerated methodology in this setting .",
    "finally section [ geo sec ] compares data - driven nonparametric estimation with model - based estimation via predictive likelihood scores .",
    "we obtained data on @xmath141 earthquakes occurring in a rectangular area around los angeles between longitudes 122@xmath142w and 144@xmath142w and latitudes 32@xmath142n and 37@xmath142n ( 733 km @xmath143 556 km ) between january 1st , 1932 and december 31th , 2012 @xcite .",
    "data , which are available for download from http://www.data.scec.org/ , include occurrence times and locations based on measurements from @xmath144 sensors positioned throughout southern california .",
    "decades of geophysical research have led to a deep understanding of the statistical properties of earthquake aftershocks @xcite .",
    "a widely used parametrisation for the trigger function is the `` epidemic - type aftershock model '' @xmath145 here the @xmath98 prefix denotes coordinates relative to the @xmath26th event @xmath30 and @xmath146 is its associated magnitude . based on eqn . [ geology ] , @xcite performed inference for @xmath147 using a subset of @xmath148 events from the southern california dataset post-1984 ( where data are considered complete above magnitude @xmath149 ) , based on a piecewise constant partition of @xmath150 according to 8 regions of geological fault activity .",
    "given that the approach of @xcite is heavily constrained by eqn .",
    "[ geology ] , it is compelling to see whether nonparametric , data - driven models can compete with this parametric benchmark .",
    "our nonparametric estimators are constructed as mixtures of gaussians and are able , in principle , to approximate non - radial intensities such as eqn .",
    "[ geology ] with arbitrary precision .",
    "previously it had not been possible to perform this comparison , since nonparametric estimators were computationally restricted to @xmath151 samples , which @xcite argued was insufficient for robust estimation .    in experiments below",
    "we follow the seismology literature by assuming a time - independent background intensity @xmath152 .",
    "we did not include magnitude as a coordinate of @xmath153 , since aftershock magnitude need not be similar to mainshock magnitude .",
    "characteristic length scales , required for defining the @xmath49 nearest neighbours in eqn .",
    "[ lnn ] but not hyperparameters _ per se _ , were taken to be 1 day , 0.1@xmath142 latitude and 0.1@xmath142 longitude .",
    "boundary effects were not modelled .",
    "the proposed approach has computational and storage requirements which are linear in the adaptive truncation parameter @xmath49 , with larger @xmath49 leading to smaller approximation error . in order to inform our choice of @xmath49 we plotted the estimated background intensity @xmath154 for a hypothetical earthquake along the 34th parallel , varying @xmath49 .",
    "[ vary l ] suggests that results , based here on the @xmath155 events recorded between 1960 and 1965 , are approximately independent of @xmath49 when @xmath156 ; we therefore took @xmath157 for all subsequent experiments .",
    "our methodology aims to relax current limitations on the size and scope of nonparametric analyses ; to test this we implemented both the proposed and existing algorithms on the same platform ( matlab r2015a ) and performed calculations using a single 2.53ghz processor and 3 gb of ram .",
    "specifically , we compared against exact @xcite and approximate @xcite stochastic declustering . to ensure fair comparison ,",
    "all algorithms were based on the same gaussian kernel with fixed ( non - adaptive ) bandwidth parameters @xmath158 and identical total variation stopping rules were used",
    ". the sampling - based algorithm of @xcite does not converge to a unique estimate ; we therefore terminated this algorithm after 10 iterations in situations where convergence in total variation was not achieved .",
    "the threshold 10 was chosen since our accelerated estimator typically required fewer than 10 iterations to converge in the above sense .",
    "a dataset was constructed based on the first @xmath159 events from the starting point of january 1st , 1932 .",
    "we examined the computational time required for termination of each of the three algorithms , while increasing @xmath159 .",
    "[ times ] demonstrates that our accelerated approach is significantly quicker than both exact @xcite and approximate @xcite stochastic declustering .",
    "the method of @xcite was heavily constrained by @xmath0 storage and quickly ran out of memory , whereas @xcite was able to go further , with storage @xmath2 , but was limited by cpu time .",
    "in contrast , the proposed methodology is able to quickly scale to much larger sample sizes .",
    "we emphasise that , whilst it is surely possible to improve each implementation for a given @xmath159 , it remains true that the proposed methodology enjoys favourable computational @xmath44 and storage @xmath2 complexities , with negligible loss of accuracy compared to the @xmath0 competing approaches .",
    "our accelerated methodology allows us to investigate , for the first time , whether nonparametric estimators based on large datasets can be competitive with domain - specific parametric models .",
    "we initially took the parametric model of @xcite , based on decades of geological research , as a proxy for the true data - generating intensities in the southern california dataset .",
    "in contrast no geological knowledge entered into the nonparametric estimators .",
    "we then compared our nonparametric estimator @xmath160 against the parametric @xmath40 of eqn .",
    "[ geology ] , where the former was based on the @xmath161 events occurring in 1984 and the latter based on parameters @xmath41 reported in @xcite . fig . [ g ] shows that @xmath160 approximately recovered the correct support of @xmath40 , with the spatial marginal being more accurate than the time marginal .",
    "this rough agreement give confidence that the proposed nonparametric estimators are indeed targeting the correct data - generating intensities . in order to probe robustness of these conclusions ,",
    "we repeated the procedure using data on years 1985 - 1990 ; in each case a similar level of approximation was observed between @xmath160 and @xmath40 ( see fig .",
    "encouraged by accurate recovery of the trigger function , we then assessed the predictive performance of nonparametric methods .",
    "the standard approach to testing earthquake models was established by the working group for the development of regional earthquake likelihood models ( relm ) in 2001 and is reviewed in @xcite . in brief , each competing model is required to estimate the number of earthquakes in each of a number of spatio - temporal bins , where the number of events in each bin is assumed to follow a poisson distribution with intensity parameter equivalent to the forecasted rate . the simplest performance measure in this setting",
    "is known as the l - score , that evaluates the joint probability of held - out data according to the proposed model , computed as a product of independent poisson probabilities . using both our accelerated nonparametric estimator and the parametric model of @xcite",
    ", we attempted to predict earthquakes for each of the 31 days of december in each of 2010 , 2011 and 2012 , given the previous 7 days events .",
    "our nonparametric approach was based on a large dataset containing the @xmath162 events recorded from 2003 to 2009 .",
    "given estimated intensity functions , the 7 days prior to each day in december were used to construct a predictive intensity over the domain of the held - out data . by computing the predicted number of events occuring in each of the 30 regions whose boundaries are defined by integer values of latitude and longitude , we are able to quantify predictive performance under the l - score , such that larger values represent better performance .",
    "results showed that accelerated nonparametric methods were competitive with , but not superior to , model - based prediction ( @xmath163 versus @xmath164 respectively ) .",
    "deconstructing this result , we found that nonparametric methods tended to systematically under - estimate the reproductive ratio @xmath48 ( the expected number of offspring from any given event ) relative to the parametric estimator .",
    "s2 compares estimates for @xmath48 based on data from each of the years 1984 - 1990 . due to under - estimation of @xmath48 , more events were deemed to be background and were not foreseen , explaining the lower l - scores .",
    "in addition to estimation of intensity functions , in principle one can also estimate the branching structure @xmath36 .",
    "[ forest ] displays a typical point estimate for branching structure .",
    "however , identification of @xmath36 from occurence data is fundamentally extremely challenging , as pointed out by @xcite .",
    "indeed , we observe in fig .",
    "[ forest ] that @xmath36 contains several mainshock - aftershock links that correspond to a time delay of several weeks ; this would typically be considered unrealistic on geological grounds and supports the intuition that it is extremely challenging to achieve accurate estimation of branching structure .",
    "point processes that admit copp structure arise in many topical scientific analyses , where typically a precise understanding of the background ( @xmath28 ) or triggering ( @xmath32 ) processes is currently unavailable .",
    "for example it is unclear how to formulate a parametric model for the triggering process underlying crime waves , or for the spread of infection through a human population .",
    "the nonparametric methods described here have potential to provide new insights in such systems . in this contribution",
    "we accelerated computation for these estimators : using adaptive truncation based on nearest neighbour distances , we were able to attain computational complexity @xmath44 and storage complexity @xmath2 , with negligible loss of accuracy compared to existing @xmath0 procedures . using seismology data as a test - bed",
    "we demonstrated a practical increase in algorithmic efficiency that allowed the integration of more data for fixed computational cost .",
    "the efficiency of our approach resulted from adaptive truncation in domains of both the background and trigger intensity functions .",
    "a non - adaptive truncation was previously proposed in @xcite .",
    "there an absolute threshold @xmath165 in time was applied , beyond which the triggering function was not evaluated , assumed to be zero . in that approach , @xmath165 must be chosen by hand , which could be difficult in settings where little is known about the triggering process . our proposal , in comparison , thresholds not only in the time domain but also in the space domain and the domain of the background intensity function .",
    "the resulting computational complexity is @xmath1 with constant @xmath166 proportional to the average number of events occurring in the volume @xmath167 \\times b_{\\delta}(\\bm{x})$ ] for thresholds @xmath168 .",
    "moreover , unlike @xcite , our methodology provides a mechanism to select @xmath168 adaptively , by implicitly solving for @xmath169 , thereby mitigating an important practical issue .",
    "the truncation parameter @xmath49 is not a hyperparameter _ per se _ and should be chosen sufficiently large that any further increase in @xmath49 leads to negligible variation in the estimated intensity functions , or indeed any derived quantities of interest .",
    "our preliminary empirical investigation demonstrated inferential and predictive performance that was competitive with parametric estimators , but also revealed systematic downward bias in estimation of reproductive ratios @xmath48 .",
    "this may be because the introduction of nonparametric uncertainty into the trigger function raises the evidence threshold to conclude that an event was triggered .",
    "further research will be required to address this methodological issue ; indeed , this contribution suggests a number of interesting extensions that are made possible by accelerated computation ; ( i ) reformulating the proposed estimators within the bayesian framework , allowing for ( a ) sequential updating of estimators @xmath154 , @xmath160 as new data arrive , and ( b ) regularising the reproductive ratio @xmath48 via a prior distribution @xmath170 , ( ii ) incorporating observation noise into kde , ( iii ) introducing latent variables to account for missing occurrence data , and ( iv ) migrating nearest neighbour computation to gpus @xcite .    whilst we focused on the popular class of processes with continuous state space @xmath153 , there exist a number of additional techniques to reduce computational complexity in discrete state spaces ( e.g. defined by networks ) ; see @xcite for details .    99 blundell , c. , beck , j. and heller , k.a .",
    "( 2012 ) modelling reciprocating relationships with hawkes processes .",
    "_ in proceedings of the 27th conference on advances in neural information processing systems ( nips ) _ , 2609 - 2617",
    ".                    pan , j. , lauterbach , c. and manocha , d. ( 2010 ) efficient nearest - neighbor computation for gpu - based motion planning .",
    "_ in ieee / rsj international conference on intelligent robots and systems ( iros ) _ , 2243 - 2248 .",
    "sornette , d. and utkin , s. ( 2009 ) limits of declustering methods for disentangling exogeneous from endogeneous events in time series with foreshocks , main shocks and aftershocks .",
    "_ phys rev e _ * 79*:061110 ."
  ],
  "abstract_text": [
    "<S> cascades of poisson processes are probabilistic models for spatio - temporal phenomena in which ( i ) previous events may trigger subsequent events , and ( ii ) both the background and triggering processes are conditionally poisson . </S>",
    "<S> such phenomena are typically `` data rich but knowledge poor '' , in the sense that large datasets are available yet a mechanistic understanding of the background and triggering processes which generate the data are unavailable . in these settings </S>",
    "<S> nonparametric estimation plays a central role . </S>",
    "<S> however existing nonparametric estimators have computational and storage complexity @xmath0 , precluding their application on large datasets . here , by assuming the triggering process acts only locally , we derive nonparametric estimators with computational complexity @xmath1 and storage complexity @xmath2 . </S>",
    "<S> our approach automatically learns the domain of the triggering process from data and is essentially free from hyperparameters . </S>",
    "<S> the methodology is applied to a large seismic dataset where estimation under existing algorithms would be infeasible . </S>"
  ]
}