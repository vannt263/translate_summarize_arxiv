{
  "article_text": [
    "the analysis of multivariate time series data is of increased interest and importance in the modern information age .",
    "although the methods and the associate theory for univariate time series analysis are well developed and understood , the picture for the multivariate cases is less complete . in spite of the fact that the conventional univariate time series models ( such as arma ) and the associated time - domain and frequency - domain methods have been formally extended to multivariate cases ,",
    "their usefulness is often limited .",
    "one may face serious issues such as the lack of model identification or flat likelihood functions .",
    "in fact vector arma models are seldom used directly in practice .",
    "dimension - reduction via , for example , reduced - rank structure , structural indices , scalar component models and canonical correlation analysis is more pertinent in modeling multivariate time series data . see  @xcite .    in this paper",
    "we deal with the factor modeling for multivariate time series from a dimension - reduction viewpoint . differently from the factor analysis for independent observations , we search for the factors which drive the serial dependence of the original time series . early attempts in this direction include @xcite .",
    "more recent efforts focus on the inference when the dimension of time series is as large as or even greater than the sample size ; see , for example ,  @xcite and the references within .",
    "high - dimensional time series data are often encountered nowadays in many fields including finance , economics , environmental and medical studies .",
    "for example , understanding the dynamics of the returns of large numbers of assets is the key for asset pricing , portfolio allocation , and risk management .",
    "panel time series are commonplace in studying economic and business phenomena .",
    "environmental time series are often of a high dimension due to a large number of indices monitored across many different locations .",
    "our approach is from a dimension - reduction point of view .",
    "the model adopted can be traced back at least to that of @xcite .",
    "we decompose a high - dimensional time series into two parts : a dynamic part driven by , hopefully , a lower - dimensional factor time series , and a static part which is a vector white noise .",
    "since the white noise exhibits no serial correlations , the decomposition is unique in the sense that both the number of factors ( i.e. , the dimension of the factor process ) and the factor loading space in our model are identifiable .",
    "such a conceptually simple decomposition also makes the statistical inference easy .",
    "although the setting allows the factor process to be nonstationary ( see  @xcite ; also section  [ sec31 ] below ) , we focus on stationary models only in this paper : under the stationary condition , the estimation for both the number of factors and the factor loadings is carried out in an eigenanalysis for a nonnegative definite matrix , and is therefore applicable when the dimension of time series is on the order of a few thousands .",
    "furthermore , the asymptotic properties of the proposed method are investigated under two settings : ( i ) the sample size goes to infinity while the dimension of time series is fixed ; and ( ii ) both the sample size and the dimension of time series go to infinity together . in particular , our estimators for zero - eigenvalues enjoy the faster convergence ( or slower divergence ) rates , from which the proposed ratio - based estimator for the number of factors benefits .",
    "in fact when all the factors are strong , the performance of our estimation for the number of factors improves when the dimension of time series increases .",
    "this phenomenon is coined as `` blessing of dimensionality . ''",
    "the new contributions of this paper include : ( i ) the ratio - based estimator for the number of factors and the associated asymptotic theory which underpins the `` blessing of dimensionality '' phenomenon observed in numerical experiments , and ( ii ) a two - step estimation procedure when the factors are of different degrees of strength .",
    "we focus on the results related to the estimation for the number of factors in this paper .",
    "the results on the estimation of the factor loading space under the assumption that the number of factors is known are reported in  @xcite .",
    "there exists a large body of literature in econometrics and finance on factor models for high - dimensional time series .",
    "however , most of them are based on a different viewpoint , as those models attempt to identify the _ common factors _ that affect the dynamics of most original component series . in analyzing economic and financial phenomena",
    ", it is often appealing to separate these common factors from the so - called idiosyncratic components : each idiosyncratic component may at most affect the dynamics of a few original time series .",
    "an idiosyncratic series may exhibit serial correlations and , therefore , may be a time series itself .",
    "this poses technical difficulties in both model identification and inference .",
    "in fact the rigorous definition of the common factors and the idiosyncratic components can only be established asymptotically when the dimension of time series tends to infinity ; see  @xcite .",
    "hence those factor models are only asymptotically identifiable . according to the definition adopted in this paper ,",
    "both `` the common factors '' and those serially correlated idiosyncratic components will be identified as factors .",
    "this is not ideal for the applications with the purpose to identify those common factors .",
    "however , this makes the tasks of model identification and inference much simpler .",
    "the rest of the paper is organized as follows .",
    "the model and the estimation methods are introduced in section  [ sec3 ] .",
    "the sampling properties of the estimation methods are investigated in section [ sec5 ] .",
    "simulation results are inserted whenever appropriate to illustrate the various asymptotic properties of the methods . section  [ sec6 ] deals with the cases when different factors are of different strength , for which a two - step estimation procedure is preferred .",
    "the analysis of two real data sets is reported in section [ sec7 ] .",
    "all mathematical proofs are relegated to the .",
    "if we are interested in the linear dynamic structure of @xmath0 only , conceptually we may think that @xmath0 consists of two parts : a static part ( i.e. , a white noise ) , and a dynamic component driven by , hopefully , a low - dimensional process .",
    "this leads to the decomposition : @xmath1 where @xmath2 is an @xmath3 latent process with ( unknown ) @xmath4 , @xmath5 is a @xmath6 unknown constant matrix , and @xmath7 is a vector white - noise process . when @xmath8 is much smaller than @xmath9 , we achieve an effective dimension - reduction , as then the serial dependence of @xmath0 is driven by that of a much lower - dimensional process @xmath10 .",
    "we call @xmath10 a factor process . the setting  ( [ c1 ] )",
    "may be traced back at least to  @xcite ; see also its further development in dealing with cointegrated factors in  @xcite . since none of the elements on the rhs of  ( [ c1 ] )",
    "are observable , we have to characterize them further to make them identifiable .",
    "first we assume that no linear combinations of @xmath10 are white noise , as any such components can be absorbed into  @xmath11 [ see condition ( c1 ) below ] .",
    "we also assume that the rank of @xmath5 is @xmath8 .",
    "otherwise  ( [ c1 ] ) may be expressed equivalently in terms of a  lower - dimensional factor .",
    "furthermore , since ( [ c1 ] ) is unchanged if we replace @xmath12 by @xmath13 for any invertible @xmath14 matrix  @xmath15 , we may assume that the columns of @xmath16 are orthonormal , that is , @xmath17 , where @xmath18 denotes the @xmath14 identity matrix .",
    "note that even with this constraint , @xmath5 and @xmath10 are not uniquely determined in  ( [ c1 ] ) , as the aforementioned replacement is still applicable for any orthogonal @xmath15 .",
    "however , the factor loading space , that is , the @xmath8-dimensional linear space spanned by the columns of @xmath5 , denoted by @xmath19 , is uniquely defined .",
    "we summarize into condition ( c1 ) all the assumptions introduced so far :    in model  ( [ c1 ] ) , @xmath20 .",
    "if @xmath21 is white noise for a constant , then @xmath22 for any nonzero integers @xmath23 . furthermore .",
    "the key for the inference for model  ( [ c1 ] ) is to determine the number of factors @xmath8 and to estimate the @xmath6 factor loading matrix @xmath5 , or more precisely the factor loading space @xmath19 .",
    "once we have obtained an estimator , say ,  @xmath24 , a natural estimator for the factor process is @xmath25 and the resulting residuals are @xmath26 the dynamic modeling for @xmath0 is achieved via such a modeling for @xmath27 and the relationship @xmath28 . a parsimonious fitting for @xmath27",
    "may be obtained by rotating @xmath27",
    "appropriately  @xcite .",
    "such a rotation is equivalent to replacing @xmath24 by @xmath29 for an appropriate @xmath14 orthogonal matrix @xmath15 .",
    "note that @xmath30 , and the residuals  ( [ c3 ] ) are unchanged with such a replacement.=1      an innovation expansion algorithm is proposed in  @xcite for estimating @xmath5 based on solving a sequence of nonlinear optimization problems with at most @xmath9 variables .",
    "although the algorithm is feasible for small or moderate @xmath9 only , it can handle the situations when the factor process @xmath2 is nonstationary .",
    "we outline the key idea below , as our computationally more efficient estimation method for stationary cases is based on the same principle .",
    "our goal is to estimate @xmath19 , or , equivalently , its orthogonal complement @xmath31 , where @xmath32 is a @xmath33 matrix for which @xmath34 forms a @xmath35 orthogonal matrix , that is , @xmath36 and @xmath37 [ see also  ( c1 ) ] .",
    "it follows from  ( [ c1 ] ) that @xmath38 implying that for any @xmath39 , @xmath40 is a white - noise process .",
    "hence , we may search for mutually orthogonal directions @xmath41 one by one such that the projection of @xmath0 on each of those directions is a  white noise .",
    "we stop the search when such a direction is no longer available , and take @xmath42 as the estimated value of @xmath8 , where @xmath23 is the number of directions obtained in the search .",
    "this is essentially how  @xcite accomplish the estimation .",
    "it is irrelevant in the above derivation if @xmath10 is stationary or not .",
    "however , a much simpler method is available when @xmath2 , therefore also @xmath0 , is stationary :    @xmath10 is weakly stationary , and cov@xmath43 for any @xmath44 .    in most factor modeling literature , @xmath10 and",
    "@xmath45 are assumed to be uncorrelated for any @xmath46 and @xmath47 .",
    "condition ( c2 ) requires only that the future white - noise components are uncorrelated with the factors up to the present .",
    "this enlarges the model capacity substantially .",
    "put @xmath48 it follows from  ( [ c1 ] ) and ( c2 ) that @xmath49 for a prescribed integer @xmath50 , define @xmath51 then @xmath52 is a @xmath35 nonnegative matrix .",
    "it follows from  ( [ d2 ] ) that @xmath53 , that is , the columns of @xmath54 are the eigenvectors of @xmath52 corresponding to zero - eigenvalues .",
    "hence conditions ( c1 ) and ( c2 ) imply :    we take the sum in the definition of @xmath52 to accumulate the information from different time lags .",
    "this is useful especially when the sample size @xmath55 is small .",
    "we use the nonnegative definite matrix @xmath56 [ instead of @xmath57 to avoid the cancellation of the information from different lags .",
    "this is guaranteed by the fact that for any matrix @xmath58 , @xmath59 if and only if @xmath60 for all @xmath61 .",
    "we tend to use small @xmath62 , as the autocorrelation is often at its strongest at the small time lags . on the other hand , adding more terms will not alter the value of @xmath8 , although the estimation for @xmath63 with large  @xmath23 is less accurate .",
    "the simulation results reported in  @xcite also confirm that the estimation for @xmath5 and @xmath8 , defined below , is not sensitive to the choice of  @xmath62 .",
    "to estimate @xmath19 , we only need to perform an eigenanalysis on @xmath64 where @xmath65 denotes the sample covariance matrix of @xmath0 at lag @xmath23 .",
    "then the estimator @xmath66 for the number of factors is defined in  ( [ d5 ] ) below .",
    "the columns of the estimated factor loading matrix @xmath24 are the @xmath66 orthonormal eigenvectors of @xmath67 corresponding to its @xmath66 largest eigenvalues .",
    "note that the estimator  @xmath24 is essentially the same as that defined in section 2.4 of @xcite , although a canonical form of the model is used there in order to define the factor loading matrix uniquely .",
    "due to the random fluctuation in a finite sample , the estimates for the zero - eigenvalues of @xmath52 are unlikely to be 0 exactly .",
    "a common practice is to plot all the estimated eigenvalues in a descending order , and look for a cut - off value @xmath66 such that the @xmath68th largest eigenvalue is substantially smaller than the @xmath66 largest eigenvalues .",
    "this is effectively an eyeball - test .",
    "the ratio - based estimator defined below may be viewed as an enhanced eyeball - test , based on the same idea as  @xcite .",
    "in fact this ratio - based estimator benefits from the faster convergence rates of the estimators for the zero - eigenvalues ; see proposition  [ prop1 ] in section  [ sec51 ] below , and also theorems  [ thm1 ] and  [ thm2 ] in section  [ sec52 ] below .",
    "the other available methods for determining @xmath8 include the information criteria approaches of  @xcite and  @xcite , and the bootstrap approach of  @xcite , though the settings considered in those papers are different .    _",
    "a ratio - based estimator for @xmath8_. we define an estimator for the number of factors @xmath8 as follows : @xmath69 where @xmath70 are the eigenvalues of @xmath67 , and @xmath71 is a constant .    in practice",
    "we may use , for example , @xmath72 .",
    "we can not extend the search up to @xmath9 , as the minimum eigenvalue of @xmath67 is likely to be practically  0 , especially when  @xmath55 is small and  @xmath9 is large .",
    "it is worthy noting that when  @xmath9 and  @xmath55 are on the same order , the estimators for eigenvalues are no longer consistent .",
    "however , the ratio - based estimator  ( [ d5 ] ) still works well",
    ". see theorem  [ thm2](iii ) below .",
    "the above estimation methods for @xmath5 and @xmath8 can be extended to those nonstationary time series for which a generalized lag-@xmath23 autocovariance matrix is well defined ( see , e.g. ,  @xcite ) .",
    "in fact , the methods are still applicable when the weak limit of the generalized lag-@xmath23 autocovariance matrix @xmath73 exists for @xmath61 , where @xmath74 is a constant .",
    "further developments on those lines will be reported elsewhere . for the factor modeling for high - dimensional volatility processes based on a similar idea",
    ", we refer to  @xcite .",
    "conventional asymptotic properties are established under the setting that the sample size @xmath55 tends to @xmath75 and everything else remains fixed .",
    "modern time series analysis encounters the situation when the number of time series @xmath9 is as large as , or even larger than , the sample size @xmath55",
    ". then the asymptotic properties established under the setting when both @xmath55 and @xmath9 tend to @xmath75 are more relevant .",
    "we deal with these two settings in section  [ sec51 ] and sections  [ sec52][sec525 ] separately .",
    "we first consider the asymptotic properties under the assumption that @xmath76 and @xmath9 is fixed .",
    "these properties reflect the behavior of our estimation method in the cases when  @xmath55 is large and @xmath9 is small .",
    "we introduce some regularity conditions first .",
    "let @xmath77 be the eigenvalues of the matrix @xmath52 :    @xmath0 is strictly stationary and @xmath78-mixing with the mixing coefficients  @xmath79 satisfying the condition that @xmath80 .",
    "furthermore , @xmath81 element - wisely .",
    "@xmath82 .",
    "section 2.6 of  @xcite gives a compact survey on the mixing properties of time series .",
    "the use of the @xmath78-mixing condition in ( c3 ) is for technical convenience .",
    "note that @xmath52 is a nonnegative definite matrix .",
    "all its eigenvalues are nonnegative .",
    "condition ( c4 ) assumes that its @xmath8 nonzero eigenvalues are distinct from each other .",
    "while this condition is not essential , it substantially simplifies the presentation of the convergence properties in proposition  [ prop1 ] below .",
    "let @xmath83 be a unit eigenvector of @xmath52 corresponding to the eigenvalue @xmath84 .",
    "we denote by @xmath85 the @xmath9 pairs of eigenvalue and eigenvector of matrix @xmath67 : the eigenvalues @xmath86 are arranged in descending order , and the eigenvectors @xmath87 are orthonormal .",
    "furthermore , it may go without explicit statement that @xmath87 may be replaced by @xmath88 in order to match the direction of @xmath83 for @xmath89 .",
    "[ prop1 ] let conditions hold . then as @xmath90 ( but @xmath9 fixed ) , it holds that :    @xmath91 and @xmath92 for @xmath93 , and    @xmath94 for @xmath95 .",
    "the proof of the above proposition is in principle the same as that of theorem  1 in  @xcite , and is therefore omitted .      to highlight the radically different behavior",
    "when @xmath9 diverges together with @xmath55 , we first conduct some simulations : we set in model  ( [ c1 ] ) @xmath96 , @xmath97 , @xmath11 are independent @xmath98 , and @xmath99 is an ar(1 ) process defined by @xmath100 we set the sample size @xmath101 , @xmath102 and 3200 , and the dimension fixed at half the sample size , that is , @xmath103 .",
    "let @xmath52 be defined as in  ( [ d3 ] ) with @xmath104 .",
    "for each setting , we draw 200 samples",
    ". the boxplots of the errors @xmath105 , @xmath106 , are depicted in figure  [ largep ] .",
    "note that @xmath107 for @xmath108 , since @xmath96 .     with @xmath96 and all the factor loading coefficients being 1 .",
    "]    the figure shows that those estimation errors do not converge to 0 .",
    "in fact those errors seem to increase when @xmath55 ( and also @xmath103 ) increases .",
    "therefore the classical asymptotic theory ( i.e. , @xmath90 and @xmath9 fixed ) such as proposition  [ prop1 ] above is irrelevant when @xmath9 increases together with @xmath55 . in spite of the lack of consistency in estimating the eigenvalues , the ratio - based estimator for the number of factors @xmath8 ( @xmath1091 ) defined in  ( [ d5 ] ) works perfectly fine for this example , as shown in figure  [ fig5 ] .",
    "in fact it is always the case that @xmath110 in all our experiments even when the sample size is as small as @xmath111 ; see figure  [ fig5 ] .",
    ", with @xmath96 and all the factor loading coefficients being 1 . ]    to develop the relevant asymptotic theory , we introduce some notation first . for any matrix @xmath112 ,",
    "let @xmath113 be the square root of the maximum eigenvalue of @xmath114 , and @xmath115 be the square root of the smallest nonzero eigenvalue of @xmath114 .",
    "we write @xmath116 if @xmath117 and @xmath118 .",
    "recall @xmath119 and @xmath120 .",
    "some regularity conditions are now in order :    for a constant @xmath121 $ ] , it holds that @xmath122 .    for @xmath123 , @xmath124 .",
    "[ remark1 ] ( i ) condition ( c5 ) looks unnatural .",
    "it is derived from more natural conditions  ( [ e2 ] ) and  ( [ e3 ] ) below coupled with the standardization @xmath125 . since @xmath126 is @xmath6 and @xmath127 now , it is natural to let the norm of each column of @xmath5 , before standardizing to @xmath125 , tend to @xmath75 as well . to this end",
    ", we assume that @xmath128 where @xmath129 $ ] are constants .",
    "we take @xmath130 as a measure of the strength of the factor @xmath131 .",
    "we call @xmath131 a strong factor when @xmath132 , and a weak factor when @xmath133",
    ". since @xmath8 is fixed , it is also reasonable to assume that for @xmath123 , @xmath134 under conditions  ( [ e3 ] ) and  ( [ e2 ] ) with @xmath135 for all @xmath136 .",
    "( ii ) the condition assumed on @xmath137 in ( c6 ) requires that the correlation between @xmath138 ( @xmath139 ) and @xmath11 is not too strong .",
    "in fact under a natural condition that @xmath140 element - wisely , it is implied by ( [ e2 ] ) and the standardization @xmath141 [ hence now @xmath142 as a result of such standardization ] that @xmath143 .",
    "now we deal with the convergence rates of the estimated eigenvalues , and establish the results in the same spirit as proposition  [ prop1 ] .",
    "of course the convergence ( or divergence ) rate for each estimator @xmath144 is slower , as the number of estimated parameters goes to infinity now .",
    "[ thm1 ] let conditions hold and @xmath145 . then as @xmath90 and @xmath127 , it holds that :    @xmath146 for @xmath147 , and    @xmath148 for @xmath149 .",
    "[ cor0 ] under the condition of theorem  [ thm1 ] , it holds that @xmath150    the proofs of theorem  [ thm1 ] and corollary  [ cor0 ] are presented in the .",
    "obviously when @xmath9 is fixed , theorem [ thm1 ] formally reduces to proposition  [ prop1 ] .",
    "some remarks are now in order .",
    "[ remark2 ] ( i ) corollary  [ cor0 ] implies that the plot of ratios @xmath151 , @xmath152 will drop sharply at @xmath153 .",
    "this provides a partial theoretical underpinning for the estimator @xmath66 defined in  ( [ d5 ] ) .",
    "especially when all factors are strong ( i.e. , @xmath154 ) , @xmath155 .",
    "this convergence rate is independent of @xmath9 , suggesting that the estimation for @xmath8 may not suffer as @xmath9 increases .",
    "in fact when all the factors are strong , the estimation for @xmath8 may improve as @xmath9 increases .",
    "see remark  [ remark3](iv ) in section  [ sec525 ] below .",
    "\\(ii ) unfortunately , we are unable to derive an explicit asymptotic expression for the ratios @xmath151 with @xmath156 , although we make the following conjecture : @xmath157 where @xmath62 is the number of lags used in defining matrix @xmath52 in ( [ d3 ] ) , and @xmath158 is any fixed integer .",
    "see also figure  [ fig5 ] .",
    "further simulation results , not reported explicitly , also conform with  ( [ e4 ] ) .",
    "this conjecture arises from the following observation : for @xmath159 , the @xmath136th largest eigenvalue of  @xmath67 is predominately contributed by the term @xmath160 which has a  cluster of largest eigenvalues on the order of @xmath161 , where @xmath162 is the sample lag-@xmath23 autocovariance matrix for @xmath11 .",
    "see also theorem  [ thm2](iii ) in section  [ sec525 ] below .",
    "\\(iii ) the errors in estimating eigenvalues are on the order of @xmath163 or @xmath164 , and both do not necessarily converge to 0 .",
    "however , since @xmath165 the estimation errors for the zero - eigenvalues is asymptotically of an order of magnitude smaller than those for the nonzero - eigenvalues .      to illustrate the asymptotic properties in section  [ sec52 ] above , we report some simulation results .",
    "we set in model  ( [ c1 ] ) @xmath166 , @xmath167 , @xmath168 and 3200 , and @xmath169 , @xmath170 and @xmath171 .",
    "all the @xmath6 elements of @xmath5 are generated independently from the uniform distribution on the interval @xmath172 $ ] first , and we then divide each of them by  @xmath173 to make all three factors of the strength  @xmath174 ; see  ( [ e2 ] ) .",
    "we generate factor  @xmath10 from a @xmath175 vector - ar(1 ) process with independent @xmath176 innovations and the diagonal autoregressive coefficient matrix with 0.6 , @xmath177 and 0.3 as the main diagonal elements .",
    "we let @xmath11 in  ( [ c1 ] ) consist of independent @xmath176 components and they are also independent across @xmath46 .",
    "we set @xmath178 in  ( [ d3 ] ) and  ( [ d4 ] ) .",
    "for each setting , we replicate the simulation 200 times .",
    "@lccccd1.3d1.3cc@ & @xmath179 & * 50 * & * 100 * & * 200 * & & & & * 3200 * + @xmath180 & @xmath181 & 0.165 & 0.680 & 0.940 & 0.995 & 1 & 1 & 1 + & @xmath182 & 0.410 & 0.800 & 0.980 & 1 & 1 & 1 & 1 + & @xmath183 & 0.560 & 0.815 & 0.990 & 1 & 1 & 1 & 1 + & @xmath184 & 0.590 & 0.820 & 0.990 & 1 & 1 & 1 & 1 + [ 4pt ] @xmath185 & @xmath181 & 0.075 & 0.155 & 0.270 & 0.570 & 0.980 & 1 & 1 + & @xmath182 & 0.090 & 0.285 & 0.285 & 0.820 & 0.960 & 1 & 1 + & @xmath183 & 0.060 & 0.180 & 0.490 & 0.745 & 0.970 & 1 & 1 + & @xmath184 & 0.090 & 0.180 & 0.310 & 0.760 & 0.915 & 1 & 1 +    table  [ table1 ] reports the relative frequency estimates for the probability @xmath186 with @xmath187 and 0.5 .",
    "the estimation performs better when the factors are stronger .",
    "even when the factors are weak ( i.e. , @xmath185 ) , the estimation for @xmath8 is very accurate for @xmath188 .",
    "when the factors are strong ( i.e. , @xmath187 ) , we observe a phenomenon coined as `` blessing of dimensionality '' in the sense that the estimation for @xmath8 improves as the dimension @xmath9 increases .",
    "for example , when the sample size @xmath189 , the relative frequencies for @xmath190 are , respectively , 0.68 , 0.8 , 0.815 and 0.82 for @xmath191 80 and 120 .",
    "the improvement is due to the increased information on @xmath8 from the added components of @xmath0 when @xmath9 increases . when @xmath185 , the columns of @xmath5 are @xmath9-vectors with the norm @xmath192 [ see  ( [ e2 ] ) ]",
    ". hence we may think that many elements of @xmath5 are now effectively 0 .",
    "the increase of the information on the factors is coupled with the increase of `` noise '' when @xmath9 increases .",
    "indeed , table  [ table1 ] shows that when factors are weak as @xmath185 , the estimation for @xmath8 does not necessarily improve as @xmath9 increases .     with two strong factors ( @xmath180 ) and one weak factor ( @xmath185 ) and @xmath166 , @xmath103 .",
    "]    we also experiment with a setting with two strong factors ( with ) and one weak factor ( with @xmath185 ) . then the ratio - based estimator @xmath66 tends to take two values , picking up the two strong factors only",
    ". however figure  [ fig11 ] indicates that the information on the third weak factor is not lost .",
    "in fact , @xmath193 tends to take the second smallest value at @xmath194 . in this case",
    "a  two - step estimation procedure should be employed in order to identify the number of factors correctly ; see section  [ sec6 ] below .",
    "the rates in theorem  [ thm1 ] can be further improved , if we are prepared to entertain some additional conditions on @xmath11 in model  ( [ c1 ] ) .",
    "such an improvement is relevant as the condition that @xmath195 , required in theorem  [ thm1 ] , is sometimes unnecessary .",
    "for example , in table  [ table1 ] , the ratio - based estimator @xmath66 works perfectly well when @xmath185 and @xmath55 is sufficiently large ( e.g. , @xmath188 ) , even though @xmath196 .",
    "furthermore , in relation to the phenomenon of `` blessing of dimensionality '' exhibited in table  [ table1 ] , theorem  [ thm1 ] fails to reflect the possible improvement on the estimation for @xmath8 when @xmath9 increases ; see also remark  [ remark2](i ) .",
    "we first introduce some additional conditions on @xmath11 :    let @xmath197 denote the @xmath136th component of @xmath11 .",
    "then @xmath197 are independent for different @xmath46 and @xmath136 , and have mean 0 and common variance @xmath198 .",
    "the distribution of each @xmath197 is symmetric .",
    "furthermore , @xmath199 , and @xmath200 for all @xmath201 and @xmath202 , where @xmath203 is a constant independent of @xmath204 .",
    "all the eigenvalues of @xmath205 are uniformly bounded as @xmath206 .",
    "the moment condition @xmath207 in ( c8 ) implies that @xmath197 are sub - gaussian .",
    "condition ( c9 ) imposes some constraint on the correlations among the components of @xmath11 .",
    "when all components of @xmath208 are independent @xmath209 , ( c7)(c9 ) hold .",
    "see also conditions ( i@xmath210)(iv@xmath210 ) of  @xcite .",
    "[ thm2 ] let conditions hold , @xmath211 and @xmath212 .",
    "then as @xmath213 , the following assertions hold :    @xmath214 for @xmath215 ,    @xmath216 for @xmath217 ,    @xmath218 for @xmath219 .",
    "if in addition holds , the rate in above can be further improved to @xmath220    [ cor1 ] under the conditions of theorem  [ thm2 ] , it holds that @xmath221 if in addition also holds , @xmath222 .",
    "the proofs of theorem  [ thm2 ] and corollary  [ cor0 ] are given in the .",
    "[ remark3 ] ( i ) by comparing with theorem  [ thm1 ] , the error rate for nonzero  @xmath84 in theorem  [ thm2 ] is improved by a factor @xmath223 , the error rate for zero - eigenvalues is by a factor @xmath224 at least .",
    "however , those estimators themselves may still diverge , as illustrated in figure  [ largep ] .",
    "( ii ) theorem  [ thm2](iii ) is an interesting consequence of the random matrix theory .",
    "the key message here is as follows : for the eigenvalues corresponding purely to the matrix @xmath225 , their magnitudes adjusted for @xmath226 converge at a super - fast rate .",
    "the matrix @xmath225 is a part of @xmath67 in  ( [ d4 ] ) , where @xmath162 is the sample lag-@xmath23 autocovariance matrix for @xmath208 .",
    "in particular , when all the factors are strong ( i.e. , @xmath154 ) , the convergence rate is @xmath227 .",
    "such a super convergence rate never occurs when @xmath9 is fixed .",
    "\\(iii ) condition @xmath228 is mild , and is weaker than condition @xmath229 required in theorem  [ thm1 ] . for example , when @xmath230 , this condition is implied by the condition @xmath231 .",
    "\\(iv ) with additional condition ( c9 ) , @xmath232 when all factors are strong .",
    "this shows that the speed at which @xmath233 converges to 0 increases when @xmath9 increases .",
    "this property gives a theoretical explanation why the identification for @xmath8 becomes easier for larger @xmath9 when all factors are strong ( i.e. , @xmath154 ) .",
    "see table  [ table1 ] .",
    "in this section , we outline a two - step estimation procedure",
    ". we will show that it is superior than the one - step procedure presented in section  [ sec4 ] for the determination of the number of factors as well as for the estimation of the factor loading matrices in the presence of the factors with different degrees of strength .",
    "a similar procedure is described in  @xcite to improve the estimation for factor loading matrices in the presence of small eigenvalues , although they gave no theoretical underpinning on why and when such a procedure is advantageous .",
    "consider model  ( [ c1 ] ) with @xmath234 strong factors with strength @xmath235 and @xmath236 weak factors with strength @xmath237 , where @xmath238 . now",
    "( [ c1 ] ) may be written as @xmath239 where @xmath240 , @xmath241 with @xmath125 , @xmath242 consists of @xmath234 strong factors , and @xmath243 consists of @xmath236 weak factors . like model  ( [ c1 ] ) in section  [ sec31 ] , @xmath244 and @xmath245 are not uniquely defined , but only @xmath19 is .",
    "hereafter @xmath244 corresponds to a suitably rotated version of the original  @xmath5 in model  ( [ e6 ] ) , where now  @xmath5 contains all the eigenvectors of  @xmath52 corresponding to its nonzero eigenvalues .",
    "refer to  ( [ d3 ] ) for the definition of  @xmath52 .    to present the two - step estimation procedure clearly ,",
    "let us assume that we know @xmath234 and @xmath236 first . using the method in section  [ sec4 ]",
    ", we first obtain the estimator @xmath246 for the factor loading matrix @xmath247 , where the columns of @xmath248 are the @xmath234 orthonormal eigenvectors of @xmath67 corresponding to its @xmath234 largest eigenvalues . in practice",
    "we may identify @xmath234 using , for example , the ratio - based estimation method  ( [ d5 ] ) ; see figure  [ fig11 ] .",
    "we carry out the second - step estimation as follows .",
    "let @xmath249 for all @xmath46 .",
    "we perform the same estimation for data @xmath250 now , and obtain the @xmath251 estimated factor loading matrix @xmath252 for the @xmath236 weak factors . combining the two estimators together , we obtain the final estimator for @xmath5 as @xmath253    theorem  [ thm3 ] below presents the convergence rates for both the one - step estimator @xmath254 and the two - step estimator @xmath255 .",
    "it shows that @xmath256 converges to @xmath5 at a faster rate than @xmath24 .",
    "the results are established with known @xmath234 and @xmath236 . in practice",
    "we estimate @xmath234 and @xmath236 using the ratio - based estimators .",
    "see also theorem  [ thm4 ] below .",
    "we introduce some regularity conditions first .",
    "let @xmath257 , @xmath258 , @xmath259 and @xmath260 for @xmath261 :    for @xmath261 , @xmath262 , @xmath263 , @xmath264 and @xmath265 .",
    "@xmath266 for any @xmath267 .",
    "the condition on @xmath268 in ( c5)@xmath210 is an analogue to condition ( c5 ) .",
    "see remark  [ remark1](i ) in section  [ sec52 ] for the background of those conditions .",
    "the order of @xmath269 will be specified in the theorems below .",
    "the order of @xmath270 is not restrictive , since @xmath271 is the largest possible order when @xmath272 .",
    "see also the discussion in remark  [ remark1](ii ) .",
    "condition ( c6)@xmath210 replaces condition ( c6 ) . here",
    "we impose a strong condition @xmath273 to highlight the benefits of the two - step estimation procedure .",
    "see remark  [ remark4](iii ) below .",
    "put @xmath274    [ thm3 ] let conditions , , , and hold .",
    "let @xmath212 and @xmath275 , as @xmath76 .",
    "then it holds that @xmath276 furthermore , @xmath277 if , in addition , @xmath278 and @xmath279 , where @xmath280 and @xmath281 are defined as follows : @xmath282    note that @xmath283 .",
    "theorem  [ thm3 ] indicates that between @xmath284 and @xmath285 , the latter is more difficult to estimate , and the convergence rate of an estimator for @xmath5 is determined by the rate for @xmath285 .",
    "this is intuitively understandable as the coefficient vectors for weak factors effectively contain many zero - components ; see  ( [ e2 ] ) .",
    "therefore a nontrivial proportion of the components of @xmath0 may contain little information on weak factors . when @xmath286 , @xmath287 is dominated by @xmath288 . the condition @xmath289 for @xmath290 is imposed to control the behavior of the @xmath291th to the @xmath8th largest eigenvalues of @xmath52 under this situation .",
    "if this is not valid , those eigenvalues can become very small and give a bad estimator for @xmath285 , and thus @xmath5 . under this condition , the structure of the autocovariance for the strong factors , and the structure of the cross - autocovariance between the strong and weak factors , are not similar .",
    "recall that @xmath84 and @xmath292 are the @xmath136th largest eigenvalue of , respectively , @xmath52 defined in  ( [ d3 ] ) and @xmath67 defined in ( [ d4 ] ) .",
    "we define matrices  @xmath293 and @xmath294 in the same manner as  @xmath52 and @xmath67 but with @xmath295 replaced by @xmath296 defined in  ( [ e7 ] ) , and denote by  @xmath297 and @xmath298 the @xmath136th largest eigenvalue of , respectively ,  @xmath293 and @xmath294 .",
    "the following theorem shows the different behavior of the ratio of eigenvalues under the one - step and two - step estimation .",
    "readers who are interested in the explicit rates for the eigenvalues are referred to lemma  [ lemma ] in the .",
    "[ thm4 ] under the same conditions of theorem  [ thm3 ] , the following assertions hold :    for @xmath299 or @xmath300 , @xmath301 . for @xmath302 , .",
    "@xmath303 and @xmath304 provided @xmath305    @xmath303 and @xmath306 provided @xmath307 .",
    "[ remark4 ] ( i ) theorem  [ thm4](i ) and ( ii ) imply that the one - step estimation is likely to lead to @xmath308 .",
    "for instance , when @xmath230 , then theorem  [ thm4](ii ) says that @xmath309 has a faster rate of convergence than @xmath310 as long as @xmath311 .",
    "figure  [ fig11 ] shows exactly this situation .",
    "\\(ii ) theorem  [ thm4](iii ) implies that the two - step estimation is more capable to identify the additional @xmath236 factors than the one - step estimation . in particular , if @xmath230 , @xmath312 always has a faster rate of convergence than @xmath313 .",
    "unfortunately we are unable to establish the asymptotic properties for @xmath314 for @xmath156 , and @xmath315 for @xmath316 , though we believe that conjectures similar to  ( [ e4 ] ) continue to hold .",
    "\\(iii ) when @xmath317 and/or the cross - autocovariances between different factors and the noise are stronger , the similar and more complex results can be established via more involved algebra in the proofs .",
    "we illustrate our method using two real data sets .",
    "[ example1 ] we first analyze the daily returns of 123 stocks in the period 2 january 200211 july 2008 .",
    "those stocks were selected among those included in the s&p500 and were traded every day during the period .",
    "the returns were calculated in percentages based on the daily close prices .",
    "we have in total @xmath318 observations with @xmath319 .",
    "we apply the eigenanalysis to the matrix @xmath67 defined in  ( [ d4 ] ) with @xmath320 .",
    "the obtained eigenvalues ( in descending order ) and their ratios are plotted in figure  [ figspeig ] .",
    "it is clear that the ratio - based estimator  ( [ d5 ] ) leads to @xmath321 , indicating two factors .",
    "@c@     for example [ example1].,title=\"fig : \" ] + ( a ) +   for example [ example1].,title=\"fig : \" ] + ( b )    varying the value of @xmath62 between 1 and 100 in the definition of @xmath67 leads to little change in the ratios @xmath193 , and the estimate @xmath321 remains unchanged .",
    "figure  [ figspeig](a ) shows that @xmath322 is close to 0 for all @xmath323 .",
    "figure  [ figspeig](b ) indicates that the ratio @xmath193 is close to 1 for all large @xmath324 , which is in line with conjecture  ( [ e4 ] ) .",
    "the first two panels of figure  [ figspts ] display the time series plots of the two component series of the estimated factors @xmath27 defined as in  ( [ c2 ] ) .",
    "their cross - autocorrelations are presented in figure  [ figspfacacf ] .",
    "although each of the two estimated factors shows little significant autocorrelation , there are some significant cross - correlations between the two series .",
    "the cross - autocorrelations of the three residual series @xmath325 for @xmath326 are not significantly different from 0 , where @xmath87 is the unit eigenvector of @xmath67 corresponding to its @xmath136th largest eigenvalue .",
    "if there were any serial correlations left in the data after extracting the two estimated factors , those correlations are most likely to show up in those three residual series .",
    "figure  [ figspeig ] may suggest the existence of a third and weaker factor , though there are hardly any significant autocorrelations in the series @xmath327 .",
    "in fact @xmath328 and @xmath329 .",
    "note that now @xmath86 is not necessarily a consistent estimator for @xmath84 although @xmath330 ; see theorem [ thm1](ii ) and corollary  [ cor0 ] . to investigate this further",
    ", we apply the two - step estimation procedure presented in section  [ sec6 ] . by subtracting the two estimated factors from the above , we obtain the new data",
    "@xmath331 [ see  ( [ e8 ] ) ] .",
    "we then calculate the eigenvalues and their ratios of the matrix @xmath294 .",
    "the minimum value of the ratios is @xmath332 , which is closely followed by @xmath333 and @xmath334 .",
    "there is no evidence to suggest that ; see theorem  [ thm4 ] .",
    "this reinforces our choice @xmath321 .    .",
    "]    with @xmath9 as large as 123 , it is difficult to gain insightful interpretation on the estimated factors by looking through the coefficients in @xmath24 [ see  ( [ c2 ] ) ] . to link our fitted factor model with some classical asset pricing theory in finance , we wonder if the market index ( i.e. , the s&p500 index ) is a factor in our fitted model , or more precisely , if it can be written as a linear combination of the two estimated factors .",
    "when this is true , @xmath335 , where @xmath336 is the @xmath337 vector consisting of the returns of the s&p500 index over the same time period , and @xmath338 denotes the projection matrix onto the orthogonal complement of the linear space spanned by the two component series @xmath27 , which is a 1640-dimensional subspace in @xmath339 .",
    "this s&p500 return series is plotted together with the two component series @xmath27 in figure  [ figspts ] .",
    "it turns out that @xmath340 is not exactly 0 but @xmath341 , that is , the 97.7% of the s&p500 returns can be expressed as a linear combination of the two estimated factors .",
    "thus our analysis suggests the following model for @xmath0the daily returns of the 123 stocks : @xmath342 where @xmath343 denotes the return of the s&p500 on the day @xmath46 , @xmath344 is another factor , and @xmath11 is a @xmath345 vector white - noise process .",
    "figure  [ figspts ] shows that there is an early period with big sparks in the two estimated factor processes .",
    "those sparks occurred around 24 september 2002 when the markets were highly volatile and the dow jones industrial average had lost 27% of the value it held on 1 january 2001 .",
    "however , those sparks are significantly less extreme in the returns of the s&p500 index ; see the third panel in figure  [ figspts ] .",
    "in fact the projected s&p500 return @xmath346 is the linear combination of those two estimated factors with the coefficients ( @xmath347 ) .",
    "two observations may be drawn from the opposite signs of those two coefficients : ( i ) there is an indication that those two factors draw the energy from the markets with opposite directions , and ( ii ) the portfolio s&p500 index hedges the risks across different markets .",
    "[ example2 ] we analyze a set of monthly average sea surface air pressure records ( in pascal ) from january 1958 to december 2001 ( i.e. , 528 months in total ) over a @xmath348 grid in a range of @xmath349@xmath350 longitude in the north atlantic ocean .",
    "let @xmath351 denote the air pressure in the @xmath46th month at the location @xmath352 , where @xmath353 and @xmath354 .",
    "we first subtract each data point by the monthly mean over the 44 years at its location : @xmath355 , where @xmath356 , representing the 12 different months over a year .",
    "we then line up the new data over @xmath357 grid points as a vector @xmath0 , so that @xmath0 is a @xmath9-variate time series with @xmath358 .",
    "we have @xmath359 observations . to fit the factor model  ( [ c1 ] ) to @xmath0",
    ", we calculate the eigenvalues and the eigenvectors of the matrix @xmath67 defined in  ( [ d4 ] ) with @xmath320 .",
    "let @xmath360 denote the eigenvalues of @xmath67 .",
    "the ratios @xmath361 are plotted against @xmath324 in the top panel of figure  [ fig13 ] which indicates the ratio - based estimate for the number of factor is @xmath362 ; see  ( [ d5 ] ) . however , the second smallest ratio is @xmath363 .",
    "this suggests that there may exist two weaker factors in addition ; see theorem  [ thm4](ii ) and also figure  [ fig11 ] .",
    "we adopt the two - step estimation procedure presented in section  [ sec6 ] to identify the factors of different strength . by removing the factor corresponding to the largest eigenvalue of @xmath67 ,    the ratio of the eigenvalues of @xmath67 ( the top panel ) and @xmath294 ( the bottom panel ) , against @xmath324 , for example [ example2 ] . ]",
    "the resulting `` residuals '' are denoted as @xmath331 ; see  ( [ e7 ] ) .",
    "now we repeat the factor modeling for data @xmath331 , and plot the ratios of eigenvalues of matrix @xmath294 in the second panel of figure  [ fig13 ] .",
    "it shows clearly the minimum value at 2 , indicating further two ( weaker ) factors . combining the above two steps together , we set @xmath364 in the fitted model .",
    "we repeated the above calculation with @xmath365 in  ( [ d4 ] ) .",
    "we still find three factors with the two - step procedure , and the estimated factors series are very similar to the case when @xmath366 .",
    "this is consistent with the simulation results in  @xcite , where they showed empirically that the estimated factor models are not sensitive to the choice of @xmath62 .    .",
    "]    [ fig14 ]    we present the time series plots for the three estimated factors @xmath367 in figure  [ fig14 ] , where @xmath256 is a @xmath368 matrix with the first column being the unit eigenvector of @xmath67 corresponding to its largest eigenvalue , and the other two columns being the orthonormal eigenvectors of @xmath294 corresponding    . ]    to its two largest eigenvalues ; see  ( [ e8 ] ) and also ( [ c2 ] ) . they collectively account for 85.3% of the total variation in @xmath0 which has @xmath369 components .",
    "in fact each of the three factors accounts for , respectively , 57.5% , 18.2% and 9.7% of the total variation of @xmath0 .",
    "figure  [ fig15 ] depicts the factor loading surfaces of the three factors .",
    "some interesting regional patterns are observed from those plots .",
    "for example , the first factor is the main driving force for the dynamics in the north and especially the northeast .",
    "the second factor influences the dynamics in the east and the west in the opposite directions , and has little impact in the narrow void between them .",
    "the third factor impacts mainly the dynamics of the southeast region .",
    "we also notice that none of those factors can be seen as idiosyncratic components as each of them affects quite a large number of locations .    :",
    "sample cross - correlation functions for the three estimated factors . ]    : sample cross - correlation functions for three residual series .",
    "50 represents grid position ( 10 , 5 ) , 100 for ( 10 , 10 ) and 400 for ( 10 , 40 ) . ]",
    "figure  [ fig16 ] presents the sample cross - correlations for the three estimated factors .",
    "it shows significant , though small , autocorrelations or cross - correlations at some nonzero lags .",
    "figure  [ fig17 ] is the sample cross - correlations for three residuals series selected from three locations for which one is far apart from the other two spatially , showing little autocorrelations at nonzero lags .",
    "this indicates that our approach is capable to identify the factors based on serial correlations .",
    "finally we note that the bic method of  @xcite yields the estimate @xmath370 for this particular data set .",
    "we suspect that this may be due to the fact that  @xcite requires all the eigenvalues of @xmath371 be uniformly bounded when @xmath372",
    ". this may not be the case for this particular data set , as the nearby locations are strongly spatially correlated , which may lead to very large and also very small eigenvalues for @xmath371 .",
    "indeed , for this data set , the three largest eigenvalues of @xmath373 are on the order of @xmath374 , and the three smallest eigenvalues are practically 0 . since the typical magnitude of @xmath375 is @xmath376 from our analysis , we have done simulations ( not shown here ) showing that the typical largest eigenvalues for @xmath377 , if @xmath208 is weakly correlated white noise , should be around @xmath378 to @xmath379 , and the smallest around @xmath376 to @xmath380 when @xmath358 and @xmath359 .",
    "such a huge difference in the magnitude of the eigenvalues suggests strongly that the components of the white - noise vector @xmath11 are strongly correlated . our method does not require the uniform boundedness of the eigenvalues of  @xmath381 .",
    "proof of theorem  [ thm1 ] we present some notational definitions first .",
    "we denote by @xmath382 the @xmath136th largest eigenvalue of @xmath67 and the corresponding orthonormal eigenvector , respectively .",
    "the corresponding population values are denoted by @xmath383 and @xmath384 for the matrix @xmath52 .",
    "hence @xmath385 and @xmath386 .",
    "we also have @xmath387    we show some intermediate results now . with conditions ( c3 ) and ( c5 ) and the fact that @xmath208 is white noise , we have @xmath388\\\\[-8pt ] { \\|{\\widehat}{\\bolds{\\sigma}}_{x\\epsilon}(k ) - { \\bolds{\\sigma}}_{x\\epsilon}(k)\\|},\\qquad { \\|{\\widehat}{\\bolds{\\sigma}}_{\\epsilon x}(k)\\| } & = & o_p(p^{1-\\delta/2}n^{-1/2 } ) , \\nonumber\\end{aligned}\\ ] ] where @xmath389 . then following the proof of theorem 1 of @xcite , we have the following for @xmath390 : @xmath391\\\\[-8pt ] & & \\qquad\\hphantom{\\mbox{where } } { \\|{\\widehat}{\\bolds{\\sigma}}_y(k ) - { \\bolds{\\sigma}}_y(k)\\| } = o_p\\bigl(p^{1-\\delta}n^{-1/2 } + p^{1-\\delta/2}n^{-1/2 } + { \\|{\\widehat}{\\bolds{\\sigma}}_\\epsilon(k)\\|}\\bigr)\\nonumber\\\\ & & \\qquad\\hphantom{\\mbox{where } { \\|{\\widehat}{\\bolds{\\sigma}}_y(k ) - { \\bolds{\\sigma}}_y(k)\\|}}= o_p\\bigl(p^{1-\\delta/2}n^{-1/2 } + { \\|{\\widehat}{\\bolds{\\sigma}}_\\epsilon(k)\\|}\\bigr).\\nonumber\\end{aligned}\\ ] ] now @xmath392 , where @xmath393 denotes the frobenius norm of @xmath52 . hence from ( [ app1 ] ) , @xmath394\\\\[-8pt ] { \\|{\\widehat}{{\\mathbf m}}- { { \\mathbf m}}\\| } & = & o_p(p^{1-\\delta } \\cdot pn^{-1/2 } ) = o_p(p^{2-\\delta}n^{-1/2 } ) .",
    "\\nonumber\\end{aligned}\\ ] ]    for the main proof , consider for @xmath215 , the decomposition @xmath395\\\\[-8pt ] & & \\qquad\\hphantom{\\mbox{where } } i_3 = ( { \\widehat}{\\bolds{\\gamma}}_j - { \\mathbf a}_j)'{{\\mathbf m}}{\\mathbf a}_j,\\qquad i_4 = { \\mathbf a}_j'({\\widehat}{{\\mathbf m}}- { { \\mathbf m}}){\\widehat}{\\bolds{\\gamma}}_j , \\nonumber\\\\ & & \\qquad\\hphantom{\\mbox{where } } i_5 = { \\mathbf a}_j'{{\\mathbf m}}({\\widehat}{\\bolds{\\gamma}}_j - { \\mathbf a}_j).\\nonumber\\end{aligned}\\ ] ] for @xmath396 , since @xmath397 where @xmath398 , and @xmath399 by  ( [ app1 ] ) , together with  ( [ app2 ] ) we have that @xmath400 so that @xmath401 , which proves theorem  [ thm1](i ) .",
    "now consider @xmath402 .",
    "define @xmath403 following the same proof of theorem 1 of  @xcite , we can actually show that @xmath404 , so that @xmath405 .    noting @xmath406 for @xmath402 ,",
    "consider the decomposition @xmath407\\\\[-8pt ] & & \\hphantom{\\qquad\\mbox{where } } k_2 = 2{\\widehat}{\\bolds{\\gamma}}_j'({\\widetilde}{{\\mathbf m}}- { { \\mathbf m}})({\\widehat}{\\bolds{\\gamma}}_j - { \\mathbf a}_j),\\nonumber\\\\ & & \\hphantom{\\qquad\\mbox{where } } k_3 = ( { \\widehat}{\\bolds{\\gamma}}_j - { \\mathbf a}_j)'{{\\mathbf m}}({\\widehat}{\\bolds{\\gamma}}_j - { \\mathbf a}_j ) . \\nonumber\\end{aligned}\\ ] ] using  ( [ app2 ] ) , @xmath408 similarly , using  ( [ app1 ] ) and  ( [ app2 ] ) , and @xmath409 , we can show that @xmath410 hence @xmath148 , and the proof of the theorem is completed .",
    "proof of corollary  [ cor0 ] the proof of theorem 1 of  @xcite has shown that ( in the notation of this paper ) @xmath411 but we also have @xmath412 where the last equality sign follows from @xmath413 in  ( [ app1 ] ) .",
    "hence we have @xmath414 for @xmath147 .",
    "letting @xmath415 for @xmath147 , we then have @xmath416 from theorem  [ thm1](i ) .",
    "but since @xmath417 implying that @xmath418 , we have @xmath419 .",
    "hence we must have @xmath420 for @xmath147 .",
    "this implies that @xmath421 for @xmath422 , and together with theorem  [ thm1](ii ) , @xmath423 this completes the proof of the corollary .    in the following , we use @xmath424 to denote the @xmath136th largest singular value of a matrix @xmath52 , so that @xmath425 .",
    "we use @xmath426 to denote the @xmath136th largest eigenvalue of @xmath52 .",
    "proof of theorem  [ thm2 ] the first part of the theorem is actually theorem 2 of  @xcite .",
    "we prove the other parts of the theorem . from equation",
    "( 22 ) of  @xcite , the sample lag-@xmath23 autocovariance matrix for @xmath11 satisfies @xmath427    note that  ( [ app1 ] ) together with  ( [ app3 ] ) implies @xmath428 since @xmath429 .",
    "we also have @xmath430 , similar to the proof of theorem  [ thm1 ] .    with these , for @xmath215 , using decomposition  ( [ appd1 ] ) , we have @xmath431 which is theorem  [ thm2](i ) . for @xmath432 ,",
    "using decomposition  ( [ appd2 ] ) , we have @xmath433 hence @xmath434 , which is theorem  [ thm2](ii ) .    for part ( iii )",
    ", we define @xmath435 so that @xmath436 and @xmath437 .",
    "we define similarly @xmath438 , @xmath439 and @xmath440",
    ". then we can write @xmath441 where @xmath442 , @xmath443 .",
    "it is easy to see that @xmath444 so that rank@xmath445 .",
    "this implies that @xmath446 then by theorem 3.3.16(a ) of  @xcite , for @xmath447 , @xmath448 where the last equality sign follows from  ( [ app3 ] ) .",
    "this proves theorem  [ thm2](iii ) .",
    "we prove theorem  [ thm2](ii)@xmath210 now . using lemma 3 of  @xcite , with the same technique as in the proof of theorem 1 in their paper , we can write @xmath449    with the definition of @xmath450 as in the proof of theorem  [ thm1 ] , we can write @xmath451 , the @xmath452th largest eigenvalue of @xmath67 , as the @xmath453 element of the diagonal matrix @xmath454 , where @xmath455 .",
    "but from  ( [ app15 ] ) , we also have @xmath456 , hence @xmath457 further , by using neumann series expansions of @xmath458 and @xmath459 , we see that the largest order term of @xmath460 is contributed from @xmath461 , since from  ( [ app15 ] ) we have @xmath462 .",
    "hence the rate of @xmath463 can be analyzed using the @xmath453 element of @xmath461 .",
    "some notation first .",
    "define @xmath464 the column vector of @xmath23 ones , and @xmath465 since @xmath23 is finite and @xmath208 and @xmath466 are stationary , for convenience in this proof we take the sample lag-@xmath23 autocovariance matrix for @xmath208 , @xmath466 and the cross lag-@xmath23 autocovariance matrix between @xmath208 and @xmath467 to be respectively , for @xmath468 , @xmath469 and @xmath470 where @xmath471 .",
    "then @xmath472 where @xmath473 some tedious algebra ( omitted here ) shows that the dominating term of the above product is @xmath474 . defining @xmath475 and @xmath476 the first column of @xmath477 , the @xmath453 element of the said term is then @xmath478 in the last line we used @xmath479 , by noting that @xmath480 where @xmath481 is from  ( [ app16 ] ) and @xmath482 is assumed in condition ( c5 ) . with condition ( c9 ) , we can show that @xmath483 , since @xmath484 where we used the markov inequality with @xmath485 the maximum eigenvalue of  @xmath205 , and the fact that @xmath486 .",
    "hence the @xmath453 element of @xmath461 has rate @xmath487 , which is also the rate of  @xmath488 for @xmath489 .",
    "this completes the proof of the theorem .",
    "we outline the proofs of theorems  [ thm3 ] and  [ thm4 ] below",
    ". detailed proofs can be found in the supplemental article ( lam and yao  @xcite ) .",
    "outline proof of theorem  [ thm3 ] first , under model ( [ e6 ] ) and @xmath52 defined in  ( [ d3 ] ) , with conditions ( c1)(c4 ) , ( c5)@xmath210 , ( c6)@xmath210 , we can show that the rates of the eigenvalues of @xmath52 are given by @xmath490 for model  ( [ e6 ] ) , and @xmath293 defined in section  [ sec6 ] by @xmath331 in  ( [ e7 ] ) , we have @xmath491    we can not use lemma 3 of  @xcite to prove this theorem for the one - step estimation , since the condition @xmath492 gives a restrictive condition on the growth rate of @xmath9 , and also restricts the range of @xmath493 allowed .",
    "instead , we use theorem 4.1 of  @xcite . write @xmath494 for @xmath495 , where @xmath496 , @xmath54 is the orthogonal complement of @xmath497 , and @xmath498 is diagonal with @xmath499 where @xmath500 contains @xmath383 for @xmath501 and @xmath502 contains @xmath383 for @xmath503 . with @xmath504 , define @xmath505 where @xmath506 if we denote @xmath507 .",
    "define @xmath508 . if we can show that @xmath509\\\\[-8pt ] & & \\eqntext{\\mbox{with } \\gamma_{ij } = \\operatorname{sep}\\left ( { { \\mathbf d}}_i + { { \\mathbf e}}_{ii } , \\pmatrix { { { \\mathbf d}}_j + { { \\mathbf e}}_{jj } & { { \\mathbf e}}_{j3 } \\cr { { \\mathbf e}}_{3j } & { { \\mathbf e}}_{33 } } \\right),}\\end{aligned}\\ ] ] then condition ( 4.2 ) in  @xcite is satisfied asymptotically , so that we can use their theorem 4.1 to conclude that for @xmath510 , @xmath511 since we can show that @xmath512 , we have @xmath513 .",
    "we can also show that @xmath514 using  ( [ appo1 ] ) . hence ( [ appo3 ] ) is satisfied , and  ( [ appo4 ] ) implies that @xmath515 also , we can show that @xmath516 , implying that @xmath517 .",
    "we can also show that @xmath518 using  ( [ appo1 ] ) , provided @xmath279 . hence  ( [ appo3 ] ) is satisfied since we assumed @xmath519 , and so  ( [ appo4 ] ) implies that @xmath520 which completes the proof for the one - step estimation .",
    "for the two - step estimation , write @xmath521 , where @xmath522 is the orthogonal complement of @xmath285 , and @xmath523 is diagonal with @xmath524 .",
    "the matrix @xmath525 contains @xmath526 for @xmath527 , so that  ( [ appo2 ] ) implies @xmath528 .",
    "we can show that @xmath529 , hence @xmath530 , as @xmath531 .",
    "hence we can use lemma 3 of  @xcite to conclude that @xmath532 since we can show that @xmath533 , we then have @xmath534 which completes the proof of the theorem .",
    "to prove theorem  [ thm4 ] , we need two lemmas first .",
    "[ lemma ] under the same conditions and notations of theorem  [ thm3 ] , the following assertions hold :    for @xmath501 , @xmath535 .    for @xmath536 ,",
    "@xmath537 provided @xmath519 , @xmath279 .    for @xmath402 , @xmath538 ,",
    "provided @xmath519 , @xmath279 .    for @xmath527 , @xmath539 .    for @xmath540 , @xmath541",
    "for @xmath447 , @xmath542 .",
    "if in addition condition holds , then for @xmath402 , @xmath543 , provided @xmath519 , @xmath279 .",
    "the proof of this lemma is left in the supplementary materials for this paper . together with  ( [ appo1 ] ) and  ( [ appo2 ] ) , we have the following lemma .",
    "[ lemma2 ] let conditions , , , and hold .",
    "then as @xmath544 with @xmath212 , and with @xmath545 , @xmath546 the same as in theorem  [ thm3 ] and @xmath279 , we have @xmath547 furthermore , if @xmath548 and @xmath549 , we have @xmath550    if @xmath286 for @xmath551 , @xmath552 for @xmath553 , and @xmath554 , we have @xmath555    for the two - step procedure , let conditions , , , and hold and @xmath212 .",
    "then we have @xmath556    we only need to find the asymptotic rate for each @xmath557 and @xmath558 .",
    "the rate of each ratio can then be obtained from the results of lemma  [ lemma ] .    for @xmath501 , from lemma  [ lemma ] , @xmath559 , and hence @xmath560 from  ( [ appo1 ] ) .",
    "consider the case @xmath286 .",
    "for @xmath503 , since @xmath561 , we have @xmath562 , and hence @xmath563 the other case is proved similarly .",
    "for @xmath503 , to make sure @xmath557 will not be zero or close to zero , we need @xmath564 where @xmath565 as in  ( [ appo1 ] ) . hence we need @xmath566 , which is equivalent to the condition @xmath554 . with this condition satisfied , then @xmath567 for @xmath503 .",
    "since @xmath568 for @xmath402 , we then have @xmath569 all other rates can be proved similarly , and thus are omitted .",
    "proof of theorem  [ thm4 ] with lemma  [ lemma2 ] , theorem  [ thm4](i ) is obvious . for theorem  [ thm4](ii ) ,",
    "note that the range of @xmath493 and the rates given in the theorem ensure that @xmath570 .",
    "hence lemma  [ lemma2 ] implies a better rate of convergence for @xmath571 no matter whether condition ( c9 ) holds or not .",
    "we can use a similar argument to prove part ( iii ) , and details are thus omitted .",
    "we are grateful to the joint editor professor peter bhlmann , the associate editor and the two referees for their helpful comments and suggestions ."
  ],
  "abstract_text": [
    "<S> this paper deals with the factor modeling for high - dimensional time series based on a dimension - reduction viewpoint . under stationary settings , </S>",
    "<S> the inference is simple in the sense that both the number of factors and the factor loadings are estimated in terms of an eigenanalysis for a  nonnegative definite matrix , and is therefore applicable when the dimension of time series is on the order of a few thousands . </S>",
    "<S> asymptotic properties of the proposed method are investigated under two settings : ( i ) the sample size goes to infinity while the dimension of time series is fixed ; and ( ii ) both the sample size and the dimension of time series go to infinity together . in particular , our estimators for zero - eigenvalues enjoy faster convergence ( or slower divergence ) rates , hence making the estimation for the number of factors easier . in particular , when the sample size and the dimension of time series go to infinity together , the estimators for the eigenvalues are no longer consistent . however , our estimator for the number of the factors , which is based on the ratios of the estimated eigenvalues , still works fine . </S>",
    "<S> furthermore , this estimation shows the so - called `` blessing of dimensionality '' property in the sense that the performance of the estimation may improve when the dimension of time series increases . </S>",
    "<S> a two - step procedure is investigated when the factors are of different degrees of strength . numerical illustration with both simulated and real data </S>",
    "<S> is also reported .    .    . </S>"
  ]
}