{
  "article_text": [
    "discrete choice models form the basis for understanding the behavioural process that results in a choice made by a decision maker ( or agent ) among a finite set of alternatives .",
    "they are highly flexible and can be applied in a wide variety of choice situations .",
    "for example , the agents can be consumers choosing between different brands in a product category ; or households selecting among different types of heating systems .",
    "discrete choice models are widely used to predict demand for new systems in transportation planning @xcite , develop pricing policies in marketing research @xcite , elicit preferences for healthcare products and programmes in health economics @xcite and in many other applications .",
    "the mixed multinomial logit ( mmnl ) model is a popular discrete choice model that captures heterogeneity in the preferences of decision makers through random coefficients ( see , e.g. * ? ? ?",
    "choice probabilities of the mmnl model are expressed in the form @xmath0 where @xmath1 denotes the number of alternatives , @xmath2 is a @xmath3 vector of observed variables relating to alternative @xmath4 and agent @xmath5 , and @xmath6 is a @xmath3 vector of random coefficients that vary over agents in the population with mixing distribution @xmath7 . the distribution @xmath8 may be discrete or continuous ( e.g. normal , lognormal , triangular and uniform ) . the parameters of @xmath8 , and in some cases , the values of @xmath6 ( which represent the preferences of individual decision makers ) are of interest .",
    "the importance of the mmnl model and its ability to accommodate heterogeneity is well established ( e.g. * ? ? ? * ; * ? ? ?",
    "@xcite showed that any random utility model can be approximated to an arbitrary degree of accuracy by a mmnl model with appropriate choice of variables and mixing distribution .",
    "the classical approach to estimating mmnl models is via maximization of the simulated likelihood function ( see , e.g. * ? ? ? * ) .",
    "this procedure can be difficult numerically ; the algorithm may not converge due to various reasons and there is a risk of getting trapped in local maxima .",
    "bayesian procedures avoid some of these issues , and consistency and efficiency in estimation can be attained under fewer restrictions ( see * ? ? ? * chap .",
    "markov chain monte carlo ( mcmc ) methods provide the bayesian analogue to classical procedures for estimating mmnl models . in the hierarchical bayesian approach , draws from the posterior distribution can be obtained using gibbs sampling and the metropolis - hastings algorithm ( see , e.g. * ? ? ?",
    "however , computations can be prohibitively expensive for large datasets , which are increasingly common .",
    "an alternative is to obtain approximate bayesian inference via variational approximation methods @xcite .",
    "@xcite showed that predictive inference for mmnl models can be obtained using variational methods at a lower computational cost but with accuracy close to that of mcmc .    in this paper",
    ", we develop variational methods for estimating mmnl models that allow the coefficients in @xmath6 to be correlated in the posterior .",
    "previously , @xcite considered mmnl models where @xmath6 is normally distributed and derived a variational bayes @xcite procedure for posterior approximation .",
    "the approximating density was assumed to be of a factorized parametric form and the reverse kullback - leibler divergence between the true posterior and the variational approximation was minimized .",
    "this is equivalent to maximizing a lower bound on the log marginal likelihood .",
    "the intractable lower bound was approximated using the multivariate delta method for moments @xcite .",
    "maximization of the lower bound over individual - level variational parameters was performed using standard unconstrained convex optimization techniques , and the covariance matrix of @xmath6 was assumed to be diagonal in the variational posterior .",
    "this is due likely to the high computational cost of optimizing a full covariance matrix .",
    "we explore three alternatives that allow the posterior independence assumption among random coefficients to be relaxed at low computational cost : ( 1 ) laplace variational inference , ( 2 ) nonconjugate variational message passing and ( 3 ) stochastic linear regression . the performances of these approaches are compared using real and simulated datasets .",
    "laplace variational inference was first considered by @xcite .",
    "@xcite formalized the approach for a class of nonconjugate models .",
    "this method uses laplace approximations within the optimal density update in variational bayes , and results in a gaussian approximation of the posterior .",
    "nonconjugate variational message passing is an algorithm proposed by @xcite to extend variational bayes to nonconjugate models .",
    "the variational posterior is assumed to be some member of the exponential family and variational parameters can be obtained using fast fixed point updates .",
    "we continue to use the delta method to approximate the intractable lower bound when using nonconjugate variational message passing .",
    "it is important to note that convergence is not guaranteed when the delta method is used , as the objective function being optimized is no longer a proper bound on the log marginal likelihood .",
    "we have experienced divergence in a small number of experiments .",
    "stochastic linear regression @xcite is useful in such cases as it makes the same assumptions as in nonconjugate variational message passing , but does not require expectations to be evaluated analytically . instead , updates are obtained stochastically using weighted monte carlo by simulating variates from the variational posterior . @xcite",
    "considered an alternate stochastic optimization method for optimizing the intractable lower bound , which uses control variates ( functions with tractable expectations that are highly correlated with the intractable function in the lower bound ) to reduce the variance of the stochastic search gradient .    to accelerate convergence for large datasets",
    ", we develop stochastic variational inference @xcite for mmnl models using each of the above three alternatives . in stochastic variational inference",
    ", a random subset of agents is selected at each iteration and local variational parameters specific to these agents are optimized .",
    "global variational parameters are then updated using stochastic gradient ascent @xcite , where the gradients are computed based only on the minibatch of optimized local variational parameters .",
    "stochastic variational inference was developed for conjugate - exponential models and has been applied to latent dirichlet allocation @xcite and the hierarchical dirichlet process @xcite in topic modeling .",
    "@xcite extended stochastic variational inference to logistic and poisson mixed models using nonconjugate variational message passing .",
    "here , we further extend stochastic variational inference to nonconjugate models via laplace variational inference and stochastic linear regression , with applications to mmnl models . as large choice sets ( e.g. scanner panel data in marketing )",
    "become more readily available , stochastic variational inference can play an important role in deriving inference efficiently from large - scale discrete choice models .",
    "another contribution of this article is the proposal of a novel strategy to increase minibatch sizes adaptively within stochastic variational inference . at the beginning of the procedure , estimates of the global variational parameters are far from the optimum and only a small minibatch is required to compute the appropriate direction to move in . as the estimates move closer towards the optimum , a more accurate definition of the direction in which to move is required and this can be supplied through using larger minibatches .",
    "the idea of adapting batch sizes has been studied by @xcite , @xcite and @xcite in machine learning problems .",
    "the results of @xcite are of theoretical interest and they suggest that the best adaptive batch schedule is exponential . @xcite and @xcite construct frequentist hypothesis tests using the central limit theorem for large sums of random variables , and propose increasing batch sizes if the probability of updating parameters in the wrong direction is large .",
    "we develop a new criterion based on  ratio of progress and path \" @xcite while using constant step sizes within the stochastic approximation .",
    "minibatch sizes are increased when the ratio falls beneath a critical value .",
    "this paper is organized as follows .",
    "section [ sec_mmnl_model ] defines the mmnl model .",
    "section [ sec_vi ] develops variational inference for the mmnl model using three different approaches .",
    "section [ sec_svi ] develops stochastic variational inference for the mmnl model and describes the proposal for increasing minibatch sizes adaptively .",
    "section [ sec_assessment ] outlines measures for assessing the accuracy of proposed variational methods .",
    "sec_examples ] considers examples including real and simulated datasets and section [ conclusion ] concludes .",
    "the mixed multinomial logit ( mmnl ) model considered in this paper is defined as follows .",
    "suppose @xmath9 choice events are observed for each agent @xmath5 , @xmath10 , and the agent selects from among @xmath1 alternatives at each choice event .",
    "let the utility that agent @xmath5 obtains from alternative @xmath4 at the @xmath11th choice event be @xmath12 here , @xmath13 is a @xmath3 vector of observed variables that relate to alternative @xmath4 and agent @xmath5 at the @xmath11th choice event , @xmath6 is a @xmath3 random vector of coefficients for agent @xmath5 representing the agent s preferences and @xmath14 is a random error term representing unobserved utility .",
    "coefficients in @xmath6 are assumed to be distributed as @xmath15 let @xmath16^t$ ] be a @xmath17 indicator vector denoting the outcome of agent @xmath5 at the @xmath11th choice event and @xmath18^t$ ] be a @xmath19 matrix of covariates .",
    "assuming that the random error terms @xmath14 are iid extreme value ( see * ? ? ? * ) , the choice probabilities become @xmath20 for @xmath21 , and @xmath22    we adopt a full bayesian approach to inference and assume the priors : @xmath23^t ,   \\label{iwprior}\\\\ a_k| a_k \\stackrel{\\text{iid}}{\\sim } ig(1/2,\\ ; 1/a_k^2 ) , \\ ; a_k > 0 \\ ; \\text{for } \\ ; k=1,\\dots , k .",
    "\\label{igprior}\\end{gathered}\\ ] ] the hyperparameters @xmath24 , @xmath25 , @xmath26 and @xmath27 are considered known .",
    "the prior distributions for @xmath28 are marginally noninformative .",
    "@xcite showed that and induce half-@xmath11 distributions on the standard deviation terms in @xmath28 , and a large @xmath29 leads to weakly informative priors on these terms .",
    "moreover , setting @xmath30 leads to marginal uniform distributions for all correlation terms in @xmath28 .",
    "the set of unknown parameters in the mmnl model is @xmath31 , where @xmath32^t$ ] .",
    "the variables @xmath33 , @xmath28 and @xmath34 are considered as  global \" variables as they are common across all agents . the coefficients in @xmath6 are , however , specific to a particular agent @xmath5 and are considered as  local \" variables .",
    "the joint density is given by @xmath35",
    "in this section , we develop variational inference for the mmnl model .",
    "three different approaches are presented .",
    "the first approach is laplace variational inference @xcite .",
    "the second approach approximates the variational objective function using the multivariate delta method for moments @xcite and optimization is performed using nonconjugate variational message passing @xcite .",
    "the last approach considers stochastic linear regression @xcite .",
    "we first give a brief introduction to variational methods .",
    "in variational approximation , the true posterior @xmath36 is approximated by a more tractable density function @xmath37 , which is optimized to be close to @xmath36 in terms of the kullback - leibler divergence .",
    "minimizing the kullback - leibler divergence is equivalent to maximizing a lower bound @xmath38 on the log marginal likelihood since @xmath39    variational bayes @xcite assumes @xmath40 for some partition @xmath41 , and the optimal @xmath42 maximizing the lower bound @xmath38 satisfies @xmath43 where @xmath44 denotes expectation with respect to @xmath45 ( see , e.g. * ? ? ?",
    "when conjugate priors are used , the optimal densities @xmath42 belong to recognizable density families and it suffices to optimize the parameters of each @xmath42 .    applying variational bayes to the mmnl model , we assume @xmath46 the factors @xmath47 , @xmath48 and @xmath49 have conjugate priors . using , the optimal densities can be shown to be @xmath50 , @xmath51 and @xmath52 where @xmath53 ( see [ opt_den_deriv ] ) . let @xmath54 $ ] and @xmath55 $ ] .",
    "variational parameter updates for these factors are given in algorithm 1 .",
    "the optimal @xmath56 does not belong to any recognizable density family , however , as the likelihood @xmath57 is nonconjugate with respect to the prior over @xmath6 .",
    "next , we present three approaches on optimizating @xmath56 .",
    "suppose @xmath36 is some intractable posterior density .",
    "laplace approximation is based on a second order taylor approximation to @xmath58 , centered at the maximum a posterior ( map ) estimate @xmath59 such that @xmath60 where @xmath61 .",
    "note that @xmath62 since @xmath63 is maximized at @xmath59 .",
    "this gives rise to a gaussian approximation of the posterior density , @xmath64    @xcite develop laplace variational inference for a different class of nonconjugate models by applying laplace approximation to , the optimal density update in variational bayes .",
    "for the mmnl model , implies that the optimal @xmath56 should satisfy @xmath65 where @xmath66   \\\\",
    "-\\frac{\\omega}{2}(\\beta_h-\\mu_\\zeta)^t   \\upsilon^{-1 } ( \\beta_h -\\mu_\\zeta).\\end{gathered}\\ ] ] suppose @xmath67 is maximized at @xmath68 . we consider a second order taylor approximation of @xmath67 at @xmath68 , in a similar fashion as , such that @xmath69 where @xmath70 .",
    "this approximation , combined with @xmath71 , results in @xmath72 let @xmath73 and @xmath74 $ ] .",
    "the gradient and hessian of @xmath75 are given by @xmath76 general numerical optimization methods can be used to find @xmath68 .",
    "we use the bfgs algorithm via the optim function in r for optimizing @xmath68 .",
    "an approximation @xmath77 of the variational lower bound @xmath38 can be obtained using and is given in [ appendix_laplace ] .",
    "the second approach optimizes @xmath56 using nonconjugate variational message passing @xcite . besides assuming @xmath40 for some partition @xmath41 , each @xmath78",
    "is further assumed to belong to some exponential family such that @xmath79 where @xmath80 is the vector of natural parameters and @xmath81 are the sufficient statistics .",
    "the condition that @xmath82 when @xmath38 is maximized leads to the fixed point update : @xmath83 ^{-1}\\ ; \\nabla_{\\lambda_i } e_q\\{\\log p(y,\\theta)\\}\\ ] ] for @xmath84 , where @xmath85 $ ] denotes the covariance matrix of @xmath86 .",
    "details are given in [ appendix_lb_grad ] .",
    "nonconjugate variational message passing thus enables updates of variational parameters to be made in the same spirit as when variational bayes is applied to conjugate models .",
    "there is also flexibility in the evaluation of expectations , such as using bounds or quadrature .",
    "however , as a fixed point iterations algorithm , nonconjugate variational message passing is not guaranteed to converge and each update does not necessarily lead to an increase in @xmath38 .    applying nonconjugate variational message passing to the mmnl model , we assume @xmath87 .",
    "@xcite showed that for a multivariate gaussian , the update in can be simplified to @xmath88^{-1 } \\;\\ ; \\text{and } \\\\",
    "\\mu_h & \\leftarrow \\mu_h + \\sigma_h \\ ;   \\nabla\\negthinspace_{\\mu_h } e_q\\{\\log p(y,\\theta)\\}. \\end{aligned}\\ ] ] for a @xmath89 matrix a , @xmath90 is the @xmath91 vector obtained by stacking the columns of a under each other , from left to right in order .",
    "we let @xmath92 denote the reverse operation of @xmath93 so that @xmath94 can be recovered from @xmath34 as @xmath95 .",
    "the availability of such explicit updates reduces computational cost significantly as @xmath96 is a full @xmath89 covariance matrix and numerical optimization of @xmath96 can be expensive for large @xmath97 .    for the mmnl model ,",
    "@xmath98 can not be computed in closed form as @xmath99\\ ] ] is intractable .",
    "integration using quadrature is computationally intensive , and @xcite approximate using either jensen s inequality or the delta method for moments @xcite .",
    "they found that the delta method yielded better performance . here , we approximate using the delta method .",
    "while @xcite restricted @xmath96 to be diagonal , a full covariance matrix for @xmath96 is considered here .",
    "this is feasible as optimization using nonconjugate variational message passing is fast .",
    "let @xmath100 .",
    "approximating @xmath101 with a second order taylor expansion at @xmath102 and taking expectations : @xmath103 let @xmath104 and @xmath105 $ ] .",
    "it can be shown that @xmath106 with , updates for @xmath102 and @xmath96 in can be evaluated in closed form and these are given in algorithm 1 .",
    "an approximation @xmath77 of the variational lower bound @xmath38 can also be obtained using and details are given in [ appendix_delta ] .",
    "we observe that the delta method leads to good posterior estimation generally .",
    "however , this algorithm is not guaranteed to converge as @xmath107 is not a proper lower bound to the marginal log likelihood @xmath108 .",
    "an example of such divergence is given in section [ eg_electricity ] .",
    "in such cases , the third approach described next will be helpful .",
    "@xcite present a stochastic linear regression algorithm that allows fixed - form vb to be applied to any posterior ( available in closed form up to the proportionality constant ) without having to evaluate integrals analytically .",
    "suppose we make the same assumptions as in nonconjugate variational message passing .",
    "the fixed point update in can be expressed as ( see [ appendix_lb_grad ] ) @xmath109 ^{-1 } \\;\\text{cov}_{q_i }   \\left [ t_i(\\theta_i ) ,   e_{-q_i } \\ { \\log p(y,\\theta)\\ }   \\right].\\ ] ] instead of evaluating @xmath110 $ ] and @xmath85 $ ] directly , @xcite approximate these terms iteratively using weighted monte carlo by generating random samples from @xmath78 .",
    "when @xmath111 , they showed that implies that @xmath112 where @xmath113 , \\\\ m_i = e_{q_i }   \\ { \\theta_i   \\ } , \\\\",
    "g_i = e_{q_i } [ \\nabla_{\\theta_i } e_{-q_i } \\ { \\log p(y,\\theta)\\ } ] , \\end{gathered}\\ ] ] and",
    "@xmath114 denotes the hessian matrix of @xmath115 in @xmath116 .",
    "@xcite present a proof for the univariate case by considering a linear transformation and the identities derived by @xcite and @xcite .",
    "these identities can be restated as @xmath117,\\end{gathered}\\ ] ] where @xmath118 is any function in @xmath116 .",
    "we provide a proof for the multivariate case by substituting into .",
    "details are given in [ slrmg ] .",
    "the quantities @xmath119 , @xmath120 and @xmath121 are approximated stochastically using weighted monte carlo .",
    "the procedure is described in figure [ slrproc ] .    in figure [ slrproc ] , @xmath122 is updated continually and the weights @xmath123 help to diminish effects from earlier iterations when @xmath42 was less accurate .",
    "following @xcite , we adopt fixed weights @xmath123 and average iterates over the second half of the iterations to reduce variability ( see algorithm 2 ) . in setting @xmath124 ( the total number of iterations ) , it is important to balance accuracy and efficiency .",
    "the accuracy of stochastic linear regression deteriorates if @xmath124 is too small and @xmath125 are not sufficiently close to convergence .",
    "however , setting @xmath124 to a very large value can be inefficient as well .",
    "when generating draws from @xmath126 , it is computationally more efficient to consider transformation of standard normal random variables using a cholesky decomposition of @xmath119 instead of evaluating @xmath127 and @xmath128 explicitly at each iteration .",
    "for the mmnl model , the expectation of the log - sum - exp term in can not be evaluated in closed form and the delta method was used to approximate this term in the previous section .",
    "stochastic linear regression , on the other hand , does not require expectations to be evaluated analytically and is well suited to the mmnl model .",
    "it can also overcome convergence issues in nonconjugate variational message passing , as choosing @xmath123 to be sufficiently small ensures convergence .    instead of updating all variational parameters using stochastic linear regression , a combined approach",
    "is considered in this paper .",
    "we update @xmath56 for @xmath129 using stochastic linear regression while @xmath47 , @xmath48 and @xmath49 are updated using explicit variational parameter updates .",
    "this approach allows for a straightforward extension to stochastic variational inference , which is discussed in section [ sec_svi ] .",
    "note that @xmath130 defined in .",
    "the gradient @xmath131 and hessian @xmath132 of @xmath133 are given by @xmath134 and @xmath135 in .",
    "this result highlights a close connection between stochastic linear regression and laplace variational inference .",
    "while both approximates @xmath56 by a gaussian distribution , an important distinction is that stochastic linear regression optimizes both @xmath102 and @xmath96 , while laplace variational inference optimizes only @xmath102 , the location of the gaussian variational posterior ( @xmath96 is set as the negative inverse hessian at this point ) . in the examples ,",
    "we observe that this procedure in laplace variational inference often results in underestimation of the standard deviation terms in @xmath28 .",
    "we present the algorithm for computing the variational approximation @xmath37 . in algorithm [ alg1 ] ,",
    "@xmath56 may be updated using ( 1 ) laplace variational inference , ( 2 ) nonconjugate variational message passing or ( 3 ) stochastic linear regression",
    ". the computational complexity of these algorithms are @xmath136 , @xmath137 and @xmath138 respectively , where @xmath139 denotes the number of iterations in bfgs .    in variational algorithms ,",
    "the lower bound @xmath38 is commonly used to check for convergence . however , for stochastic linear regression , it is not easy to compute @xmath38 at each iteration . for laplace approximation and nonconjugate variational message passing , we can only compute approximations of @xmath38 which are not guaranteed to increase after each cycle of updates .",
    "we consider the following stopping criterion instead .",
    "let @xmath140^t$ ] and @xmath141 denote the @xmath142th element of @xmath143 at the @xmath11th iteration .",
    "we terminate algorithm 1 when @xmath144 is negligible ( less than 0.005 ) . for small datasets",
    ", there may be some fluctuations in @xmath145 for the option stochastic linear regression . in these cases ,",
    "we replace @xmath143 by its average over the past five iterations .",
    "in algorithm 1 , the local variational parameters @xmath102 and @xmath96 have to be updated for each agent @xmath129 , before the global variational parameters @xmath146 , @xmath147 , @xmath148 and @xmath149 can be re - estimated at each iteration .",
    "this procedure becomes increasingly inefficient as the number of agents @xmath150 increases .",
    "stochastic variational inference @xcite overcomes this issue by optimizing the global variational parameters using stochastic natural gradient ascent @xcite .",
    "this approach uses only a small random subset of data to compute unbiased estimates of the natural gradients at each iteration , and computation time is reduced significantly when @xmath150 is large .",
    "the procedure is described in figure [ svi ] . as large datasets in discrete choice modeling",
    "become increasingly common , stochastic variational inference can play an important role in estimating mmnl models .    ' '' ''    at each iteration ,    * draw a minibatch @xmath151 of agents randomly from the entire pool of agents . *",
    "optimize local variational parameters @xmath102 and @xmath96 for agents @xmath152 ( as a function of the global variational parameters at their current setting ) .",
    "* update global variational parameters using stochastic natural gradient ascent .",
    "noisy gradient estimates are computed using optimized local variational parameters @xmath102 and @xmath96 for agents @xmath152 .    ' '' ''    we develop stochastic variational inference for the mmnl model by building upon the methods discussed in section [ sec_vi ] . the use of laplace variational inference , nonconjugate variational message passing and stochastic linear regression within stochastic variational inference are explored .",
    "in addition , a novel approach to increase minibatch sizes adaptively is proposed .",
    "first , we explain how global variational parameters are updated .      in stochastic variational inference , global variational parameters are updated using stochastic natural gradient ascent . at the @xmath153th iteration ,",
    "an update of the form @xmath154 is applied where @xmath155 represents a small step taken in the direction of @xmath156 , the natural gradient of the lower bound with respect to @xmath80 . in stochastic natural gradient ascent ,",
    "noisy estimates are used in place of the true natural gradients .",
    "@xcite provides a motivation for the use of natural gradients in coordinate ascent by considering the geometry of the parameter space .",
    "the natural gradient @xmath156 can be obtained by premultiplying the ordinary gradient @xmath157 with the inverse of the fisher information matrix of @xmath78 @xcite .",
    "when @xmath78 belongs to an exponential family , the natural gradient is given by ( see [ appendix_lb_grad ] ) @xmath158 ^{-1}\\ ; \\nabla_{\\lambda_i } e_q\\{\\log p(y,\\theta)\\ } - \\lambda_i.\\ ] ]    let @xmath159 , @xmath160 and @xmath161 denote the natural parameter vectors of @xmath47 , @xmath48 and @xmath56 respectively . let @xmath162 denote @xmath161 optimized as a function of the current global variational parameters . from , @xmath163 ^{-1 }",
    "\\nabla_{\\lambda_\\zeta } \\bigg [ e_q\\{\\log p(\\zeta| \\mu_0 , \\sigma_0 ) \\ }",
    "\\\\   + \\sum_{h=1}^h   e_q \\{\\log p(\\beta_h|\\zeta , \\omega)\\}\\vert_{\\lambda_{\\beta_h } = \\lambda_{\\beta_h}^{\\text{opt } } } \\bigg ] - \\lambda_\\zeta.\\end{gathered}\\ ] ] suppose that a minibatch @xmath151 of agents is drawn randomly from the entire pool of agents .",
    "an unbiased estimate of @xmath164 is @xmath165 , where @xmath166 ^{-1 } \\nabla_{\\lambda_\\zeta } \\bigg [ e_q\\{\\log p(\\zeta| \\mu_0 , \\sigma_0 ) \\ } +   \\\\",
    "\\frac{h}{|b|}\\sum_{h \\in b }   e_q \\{\\log p(\\beta_h|\\zeta , \\omega)\\}\\vert_{\\lambda_{\\beta_h } = \\lambda_{\\beta_h}^{\\text{opt } } } \\bigg].\\end{gathered}\\ ] ] similarly , an unbiased estimate of @xmath167 is @xmath168 , where @xmath169 ^{-1 } \\nabla_{\\lambda_\\omega } \\bigg[e_q\\{\\log p(\\omega| \\nu , a ) \\ } + \\\\",
    "\\frac{h}{|b|}\\sum_{h \\in b }   e_q \\{\\log p(\\beta_h|\\zeta , \\omega)\\}\\vert_{\\lambda_{\\beta_h } = \\lambda_{\\beta_h}^{\\text{opt } } } \\bigg].\\end{gathered}\\ ] ] from , the stochastic gradient updates for @xmath159 and @xmath160 thus take the form of @xmath170 the present estimate @xmath171 is a weighted average of the previous estimate @xmath172 and the estimate of @xmath159 computed using minibatch @xmath151 , @xmath173 .",
    "simplified updates are given in algorithm 2 .",
    "note that updates in algorithm 1 are recovered when @xmath174 and @xmath175 .",
    "the updates for @xmath149 remain the same as in algorithm 1 as they do not depend on the local variational parameters .",
    "the iterates can be shown to converge under certain regularity conditions ( see * ? ? ?",
    "in particular , the stepsizes @xmath155 should satisfy @xmath176 a commonly used gain sequence that satisfies these rules is @xmath177 , where @xmath178 .",
    "smaller values of @xmath179 slow down the rate at which stepsizes decline , @xmath180 helps to maintain larger stepsizes in later iterations and @xmath181 is a stability constant that helps avoid unstable behavior in early iterations .",
    "the performance of stochastic approximation algorithms tends to be very sensitive to the rate of decrease of stepsizes and some tuning is usually required to achieve optimal performance .",
    "a review of rules for choosing stepsizes in deterministic or stochastic manners can be found in @xcite .",
    "@xcite developed an adaptive stepsize for stochastic variational inference , which is designed to minimize the expected distance between stochastic and batch updates .",
    "we propose a new approach towards constructing an automatic algorithm for implementing stochastic variational inference .",
    "in contrast to existing approaches of keeping the minibatch size fixed and using a stepsize with a decreasing trend ( deterministic or adaptive ) to reduce noise , we propose increasing the minibatch size adaptively as optimization proceeds , until the minibatch size is equal to the size of the whole dataset . in cases where the dataset is too large to be processed in batch mode",
    ", our adaptive strategy may still be useful with the upper bound set at a feasible minibatch size .",
    "the idea of increasing batch size adaptively has been investigated by @xcite , @xcite and @xcite in machine learning tasks .",
    "intuitively , estimates of the global variational parameters are far from the optimum at the beginning and hence only a small minibatch is required to compute the appropriate direction to move in . as the estimates move closer towards the optimum , a more accurate definition of the direction in which to move is required and this can be supplied through using larger minibatches . eventually , the entire dataset is used .",
    "this ensures convergence and the same level of accuracy can also be attained as in batch mode . with this approach",
    ", we avoid having to specify a stopping criterion for a stochastic approximation algorithm .",
    "developing a good stopping criterion can be very challenging .",
    "most commonly used criteria do not guarantee that the terminal iterate is close to the optimum and may be satisfied by chance @xcite . very often ,",
    "stochastic approximation algorithms are terminated based on some predetermined computational budget @xcite .",
    "the risk of `` apparent convergence '' associated with a declining stepsize is also avoided .",
    "`` apparent convergence '' refers to the case where iterates appear to have converge due to diminishing stepsizes even though they are actually far from the optimum ( see * ? ? ?",
    "* ) .    to obtain maximal computational savings ,",
    "the minibatch size should be increased only when the current minibatch size can no longer provide adequate information about the appropriate direction in which to move .",
    "@xcite investigates the convergence behavior of least mean squares and derives a formula for the optimal minibatch size at each iteration ( by maximizing the reduction in weight error per input presented ) .",
    "@xcite notes that their results are of interest theoretically but difficult to apply in practice due to the presence of complex quantities such as the hessian , which are hard to compute .",
    "@xcite and @xcite construct frequentist hypothesis tests to determine if parameter updates are likely to be in the correct direction , and suggest increasing the minibatch size by a certain factor , when all parameters are failing their hypothesis tests .",
    "they observe that stochastic gradients in gradient ascent algorithms often involve averaging over a large number of random variables and make use of the central limit theorem as a basis for their tests .",
    "we have attempted to apply their approach in stochastic variational inference .",
    "however , we find that in our context , the hypothesis tests tend to fail at a stage which is still too early for minibatch sizes to be increased , resulting in suboptimal performance .",
    "we propose the following strategy for increasing minibatch sizes adaptively . starting with a minibatch @xmath151",
    ", we implement the procedure in figure [ svi ] repeatedly , updating the global variational parameters with a constant stepsize . in general stochastic gradient optimization algorithms ,",
    "constant stepsizes are popular even though they do not lead to formal convergence as the algorithm tends to be more robust . with constant stepsizes ,",
    "iterates tend to move monotonically towards the optimum at first .",
    "however , near the optimum , they will bounce around instead of converge towards it as stepsizes remain large .",
    "this oscillating phenomenon is an indication that the current minibatch size is no longer adequate in defining the direction to move .",
    "more resolution is required and we increase the minibatch size by a factor @xmath182 .",
    "this process is repeated until the whole dataset is used .    to detect",
    "if iterates have reached the stage where they are merely bouncing around the optimum , we consider the  ratio of progress and path \" defined in @xcite as @xmath183 for a univariate variable @xmath80 at iteration @xmath153 .",
    "@xcite used this ratio to define an adaptive stepsize which decreases by a factor if @xmath184 is less than a certain value and remains the same otherwise .",
    "the ratio @xmath184 lies between zero and one .",
    "it is zero when @xmath185 ( i.e. there is no progress after @xmath186 iterations ) and it is one when the path from @xmath187 to @xmath188 is monotonic .",
    "small values of @xmath184 indicate that the path of the algorithm is erratic and there is a lot of back and forth movement .",
    "this ratio is thus a good indicator of whether iterates are close to the optimum and are simply bouncing around .",
    "note that @xmath189 will have to be stored in memory for the computation of @xmath184 .",
    "we monitor the  ratio of progress and path \" for elements in @xmath146 and the diagonal of @xmath148 . in the examples , we set @xmath190 and store the past @xmath186 values of @xmath146 and @xmath191 .",
    "however , as it is unlikely that the algorithm will have to stay at every minibatch size for more than 20 iterations , we start computing the ratios as soon as @xmath192 using the available history . thus , we compute @xmath193   \\frac{| \\upsilon_{kk}^{(l - m ) } - \\upsilon_{kk}^{(l)}|}{\\sum_{r = l - m}^{l-1 } | \\upsilon_{kk}^{(r ) } - \\upsilon_{kk}^{(r+1)}| } \\;\\;\\ ;",
    "\\text{if $ l \\geq m$ }    \\end{array } \\right .",
    "\\text{and}\\;\\ ; \\\\",
    "\\phi^{(l)}_{2k } & =   \\left\\ {     \\begin{array}{l l }   \\frac{| { \\mu_\\zeta}_k^{(0 ) } - { \\mu_\\zeta}_k^{(l)}|}{\\sum_{r=0}^{l-1 } |{\\mu_\\zeta}_k^{(r ) } - { \\mu_\\zeta}_k^{(r+1)}| }   \\;\\;\\ ; \\text{if $ 5   < l < m$}\\\\ [ 4 mm ]    \\frac{|{\\mu_\\zeta}_k^{(l - m ) } -{\\mu_\\zeta}_k^{(l)}|}{\\sum_{r = l - m}^{l-1 } |{\\mu_\\zeta}_k^{(r ) } - { \\mu_\\zeta}_k^{(r+1)}|   } \\;\\;\\ ; \\text{if $ l \\geq m$}.     \\end{array } \\right .",
    "\\end{aligned}\\ ] ] the minibatch size is increased by a factor @xmath182 when the minimum value of the @xmath194 ratios falls beneath a critical value @xmath195 .",
    "we allow @xmath195 to vary according to the minibatch size @xmath196 . for a small @xmath196",
    ", a smaller @xmath195 is required as the path of the algorithm can be quite erratic even though progress is being made due to the greater randomness present between iterations .",
    "the proposed algorithm for implementing stochastic variational inference using adaptive batch sizes is outlined in algorithm 2 .    in stochastic variational inference",
    ", the local variational parameters should be optimized as a function of the global variational parameters at their current setting ( step 3 of algorithm 2 ) .",
    "laplace variational inference optimizes only @xmath102 , the location of the gaussian variational posterior and sets @xmath96 as the inverse of the negative hessian at this point . using our adaptive batch size approach , convergence is ensured , however , as the entire dataset is used eventually . for nonconjugate variational message passing , the updates for @xmath102 and @xmath96 are recursive and they have to be performed repeatedly until convergence is reached in order for @xmath102 and @xmath96 to be optimized .",
    "let @xmath197 be a concatenation of the vectors @xmath102 for @xmath152 at the @xmath153th iteration . in the examples ,",
    "we terminate the number of iterations in nonconjugate variational message passing when @xmath198 where @xmath199 denotes the euclidean norm , or when the number of iterations hit a maximum of three .",
    "we have used a loose stopping criterion here for greater computational efficiency . in the case of stochastic linear regression",
    ", we fix the number of iterations at @xmath124 for simplicity and assume that this number of iterations is sufficient for @xmath200 to be sufficiently close to convergence .",
    "as described in section [ sec_proposed strategy ] , constant stepsizes are used within each minibatch size , and we allow the stepsize to increase with the minibatch size @xmath196 . intuitively , smaller stepsizes are required at the beginning as we are less confident in the direction of gradient ascent computed based on the small minibatch of optimized variational parameters . as the minibatch size increase , our confidence level increases .",
    "the stepsize is 1 when the algorithm transits to batch mode ( when @xmath174 ) . in the examples , we start with a minibatch size of @xmath201 and an initial stepsize @xmath202 .",
    "we let the stepsize increase linearly with the minibatch size , and @xmath203 is computed similarly . ] until it reaches 1 when @xmath174 . we find that the performance of minibatches with sizes smaller than 25",
    "tend to be more erratic .",
    "the critical value @xmath204 for the `` ratio of progress and path '' is also initialized at 0.4 for the initial minibatch size of 25 , and allowed to increased linearly with the minibatch size @xmath196 until it reaches 1 when @xmath174 .",
    "we have used linear increments as this is a straightforward option .",
    "it is possible to experiment with other settings .",
    "our experiments indicate that minor variations from these settings do not result in much changes in the performance of algorithm 2 .",
    "the standard bayesian procedure for obtaining inference from mmnl models is via mcmc methods . @xcite",
    "describes how posterior samples for a mmnl model can be obtained using a metropolis - hastings within gibbs algorithm .",
    "@xcite proposed an improved random walk metropolis algorithm for drawing @xmath6 using a fractional likelihood approach .",
    "they demonstrate that the improved random walk metropolis exhibits better mixing and dissipates initial conditions in a shorter time than a random walk metropolis and an independence metropolis sampler .",
    "this algorithm in implemented in the r package bayesm via the function rhiermnlrwmixture .",
    "we modify this function slightly to accommodate the marginally noninformative priors for @xmath28 defined in and , and use it to compare mcmc with proposed variational methods .      for assessing the accuracy of proposed variational methods",
    ", we use the measures discussed in @xcite , which are based on the predictive choice distribution .",
    "the true predictive choice distribution of a @xmath205 vector of outcomes @xmath206 given the @xmath19 matrix of observed variables @xmath207 is defined as @xmath208 for data simulated artificially from the mmnl model , the true predictive choice distribution can be computed using monte carlo integration as @xmath33 and @xmath28 are known . in section [ eg_sim ] ,",
    "we use 1000,000 draws of @xmath209 from @xmath210 to compute the true predictive choice distribution for simulated data .",
    "@xcite showed that variability arising from monte carlo integration is not noticeable if this many draws of @xmath209 are used .",
    "a point estimate of the predictive choice distribution can be obtained by taking the mean of under the posterior of @xmath33 and @xmath28 : @xmath211 the estimated predictive choice distribution can be computed using monte carlo integration for both variational and mcmc methods . for variational methods , we approximate the posterior density @xmath212 with the fitted variational posterior density @xmath213 .",
    "we use 500 draws of @xmath214 from @xmath213 for variational methods and 10,000 draws for mcmc .",
    "more samples are used in the case of mcmc as there is some autocorrelation among the draws . for the estimated predictive choice distributions , we use 10,000 iid draws of @xmath209 .",
    "following @xcite , we use the total variation ( tv ) metric to compute the distance between two predictive choice distributions ( see , e.g. * ? ? ?",
    "* ) . for the simulated datasets ,",
    "the tv distance between the estimated and true predictive choice distributions at the attribute matrix @xmath207 can be computed using @xmath215 \\\\ = \\frac{1}{2}\\sum_{j=1}^j |p_{true } ( y_{\\text{new}}^j=1| x_{\\text{new } } ) - \\hat{p } ( y_{\\text{new}}^j=1| x_{\\text{new}})|.\\end{gathered}\\ ] ] for real datasets , the true predictive choice distribution is unknown and we compute the tv distances between the predictive choice distribution estimated using mcmc and the variational methods instead .",
    "this provides a means of assessing the degree of agreement between mcmc and variational methods .",
    "in the following examples , the performances of laplace approximation ( laplace ) , nonconjugate variational message passing ( ncvmp ) and stochastic linear regression ( slr ) are compared with that of mcmc in terms of the predictive choice distribution .",
    "we set @xmath216 and @xmath217 for slr in both algorithms 1 and 2 .",
    "the runtime of slr in algorithm 1 varies slightly between runs and we report the mean runtime and standard deviation over 5 runs .",
    "estimates of the variational parameters are almost identical in each run of slr .",
    "we use the run with runtime closest to the mean runtime to compute the predictive choice distribution . for algorithm 2",
    ", there is greater variation and we repeat runs for each alternative ten times .",
    "the mean runtime and standard deviation over the ten runs are reported .    for mcmc",
    ", 4 independent chains were run in each example and the first half of each chain was discarded as burn - in . in each example , 10,000 draws remained after thinning and the gelman - rubin diagnostics were used to check that these draws are a good approximation of the posterior distribution .",
    "these draws were then used to compute the estimated predictive choice distribution for mcmc .",
    "we note that there are some inherent difficulties in comparing runtimes of variational methods with mcmc .",
    "mcmc is a simulation - based method and runtimes depend on the thinning factor , length of burn - in and number of sampling iterations .",
    "these are problem dependent ; a higher thinning factor or longer burn - in may be required when mixing is poor . on the other hand ,",
    "variational bayes is deterministic and the time to convergence ( which is also problem dependent ) depends on the initialization and stopping rule . due to these concerns , we only present the results of a predictive log - likelihood experiment in section 6.1 , where the variational algorithms and mcmc are given an equal computational budget .",
    "we use a vague @xmath218 for @xmath33 and set @xmath219 , @xmath220 for @xmath221 . all code was written in r and computations",
    "were carried out on a 64-bit 3.20 ghz intel core i5 processor with 8 gb of memory .",
    "our r code is available as supplementary materials for this article .",
    "the first example considers data representing consumers choices among vehicles in stated preference experiments .",
    "this dataset consists of @xmath222 respondents and comes from a study for toyota and general motors on the marketability of electric and hybrid vehicles .",
    "it is available at http://elsa.berkeley.edu/users/train/ec244.html and is a subset of the full dataset of 500 respondents ( see * ? ? ?",
    "each respondent faced up to 15 experiments and chose among @xmath223 different vehicles in each experiment .",
    "the vehicles are described in terms of the following attributes : negative of price ( in ten thousand dollars ) , negative of operating cost per month ( in ten dollars ) , engine type ( gas , electric , or hybrid ) , range in hundreds of miles between recharging ( if engine is electric ) and performance level ( high , medium , or low ) .",
    "the range of a vehicle is set to zero for all non - electric vehicles .",
    "an indicator variable for hybrid vehicle is included as a covariate .",
    "performance level is represented using two dummy variables with low performance as the base .",
    "we have @xmath224 covariates and @xmath225 as some respondents did not complete all 15 experiments .",
    "we run algorithm 1 using laplace , ncvmp and slr and runtimes are given in table [ vehicle_runtime ] .",
    "mcmc was run for 4 chains , each with 25000 iterations and a thinning factor of 5 was applied .",
    "mixing is poor for parameters corresponding to the indicator variable for medium performance and so a larger number of iterations and higher thinning factor were used .",
    ".[vehicle_runtime ] vehicles example : cpu times ( seconds ) for algorithm 1 ( laplace , ncvmp and slr ) .",
    "standard deviation over repeated runs given in brackets .",
    "[ cols=\"<,^,^,^,^\",options=\"header \" , ]      barplot shows the average number of iterations spent by algorithm 2 ( laplace , ncvmp , slr ) at each minibatch size @xmath196 for the electricity data.,scaledwidth=22.0% ]",
    "in this paper , we have developed three different approaches for fitting mmnl models using variational bayes : ( 1 ) laplace approximation , ( 2 ) nonconjugate variational message passing with a delta method approximation and ( 3 ) stochastic linear regression .",
    "we also proposed a novel adaptive batch size strategy for implementing stochastic variational inference using these approaches .",
    "the performances of these variational methods were investigated for a wide range of datasets , both real and simulated . across all examples ,",
    "predictive choice distributions computed using stochastic linear regression were closest to that of mcmc , with nonconjugate variational message passing close behind .",
    "the discrepancy between laplace approximation and mcmc is much more pronounced . in terms of stability ,",
    "stochastic linear regression and laplace approximation are very stable and we did not encounter any convergence issues in all our experiments . while nonconjugate variational message passing failed to converge in one of the examples , the failure is due to the delta method approximation rather than nonconjugate variational message passing itself . in the rest of the examples ,",
    "nonconjugate variational message passing performed very well and converged in the shortest time .",
    "stochastic variational inference further accelerates convergence for large scale data sets . with our adaptive batch size strategy ,",
    "algorithm 2 is nearly automatic and we recommend increasing @xmath182 proportionately with the number of agents @xmath150 .",
    "significant speedups can be obtained using algorithm 2 for datasets as small as a few hundreds .",
    "variational methods provide an important alternative as well as complement to mcmc methods for fitting mmnl models , yielding high computational efficiency with competitive accuracy .",
    "investigating how variational methods can be used for performing model selection with large - scale discrete choice datasets is another important area for future research . with",
    "the availability of large scale data sets in marketing and other applications , variational methods enable predictive inference to be obtained in a timely manner .",
    "the author is grateful to cathy trower , jordan louviere and the institute for choice at the university of south australia for sharing data from the project on faculty appointments .",
    "the author also wish to thank kenneth train for sharing data from the vehicle choice stated preference experiments , david nott for his comments on the manuscript , and the editor , associate editor and referees for their comments and suggestions for improving the manuscript .",
    "references    amari , s. ( 1998 ) .",
    "natural gradient works efficiently in learning . _ neural computation _ ,",
    "10 , 251276 .",
    "attias , h. ( 1999 ) .",
    "inferring parameters and structure of latent variable models by variational bayes . in _ proceedings of the 15th conference on uncertainty in artificial intelligence _ ( eds .",
    "laskey , k. and prade , h. ) , 2130 .",
    "san francisco , ca : morgan kaufmann .",
    "ben - akiva , m. e. and lerman , s. r. ( 1985 ) .",
    "_ discrete choice analysis : theory and application to travel demand _",
    "( vol . 9 ) .",
    "usa : mit press .",
    "bhat , c. ( 1998 ) .",
    "accommodating variations in responsiveness to level - of - service variables in travel mode choice models .",
    "_ transportation research a _ , 32 , 455507 .",
    "boyles , l. , korattikara , a. , ramanan , d. and welling , m. ( 2011 ) .",
    "statistical tests for optimization efficiency . in _ advances in neural information processing systems 24 _ ( eds .",
    "shawe - taylor , j. , zemel , r. s. , bartlett , p. l. , pereira , f. and weinberger , k.q . ) , 21962204 .",
    "ny usa : curran associates , inc .",
    "braun , m. and mcauliffe , j. ( 2010 ) .",
    "variational inference for large - scale models of discrete choice .",
    "_ journal of the american statistical association _",
    ", 105 , 324335 .",
    "brownstone , d. and train , k. ( 1999 ) .",
    "forecasting new product penetration with flexible substitution patterns .",
    "_ journal of econometrics _ , 89 , 109129 .",
    "bickel , p. j. and doksum , k. a. ( 2007 ) .",
    "_ mathematical statistics : basic ideas and selected topics _",
    "( 2nd ed . ) , vol 1 .",
    ", upper saddle river , nj : pearson prentice hall .",
    "gaivoronski , a. ( 1988 ) . implementation of stochastic quasigradient methods . in _",
    "numerical techniques for stochastic optimization _ ( eds .",
    "ermoliev , yu . and wets , r. j - b ) , 313352 .",
    "ny : springer - verlag .",
    "hess , s. , train , k. e. amd polak , j. w. ( 2006 ) . on the use of a modified latin hypercube sampling ( mlhs )",
    "method in the estimation of a mixed logit model for vehicle choice .",
    "_ transportation research part b _ , 40 , 147163 .",
    "hoffman , m. d. , blei , d. m. and bach , f. ( 2010 ) .",
    "online learning for latent dirichlet allocation . in _ advances in neural information processing systems",
    "23 _ ( eds .",
    "lafferty , j. , williams , c. , shawe - taylor , j. , zemel , r. and culotta , a. ) , 856864 .",
    "ny usa : curran associates , inc .",
    "hoffman , m. d. , blei , d. m. , wang , c. and paisley , j. ( 2013 ) .",
    "stochastic variational inference .",
    "_ journal of machine learning research _ , 14 , 13031347 .",
    "huang , a. and wand , m. p. ( 2013 ) .",
    "simple marginally noninformative prior distributions for covariance matrices .",
    "_ bayesian analysis _ , 8 , 439452 .",
    "hubert , j. and train , k. ( 2001 ) . on the similarity of classical and bayesian estimates of individual mean pathworths .",
    "_ marketing letters _",
    ", 12 , 259269 .",
    "jank , w. ( 2006 ) . implementing and diagnosing the stochastic approximation em algorithm .",
    "_ journal of computational and graphical statistics _",
    ", 15 , 803829 .",
    "jordan , m. i. , ghahramani , z. , jaakkola , t. s. and saul , l. k. ( 1999 ) .",
    "an introduction to variational methods for graphical models .",
    "_ machine learning _",
    ", 37 , 183233 .",
    "kim , b - d . ,",
    "blattberg , r. c. and rossi , p. e. ( 1995 ) .",
    "modeling the distribution of price sensitivity and implications for optimal retail pricing .",
    "_ journal of business and economics statistics _",
    ", 13 , 291303 .",
    "knowles , d. a. and minka , t. p. ( 2011 ) .",
    "non - conjugate variational message passing for multinomial and binary regression . in _ advances in neural information processing systems 24 _ ( eds .",
    "shawe - taylor , j. , zemel , r. s. , bartlett , p. l. , pereira , f. and weinberger , k.q . ) , 17011709 .",
    "ny usa : curran associates , inc .",
    "korattikara , a. , boyles , l. , welling , m. , kim , j , and park , h. ( 2011 ) .",
    "statistical optimization of non - negative matrix factorization . in _ jmlr : workshop and conference proceedings _ , 15 , 128136 .",
    "lancsar , e. and louviere , j. ( 2008 ) .",
    "conducting discrete choice experiments to inform healthcare decision making . _",
    "pharmacoeconomics _ , 26 , 661677 .",
    "levin , d. a. , peres , y. and wilmer , e. l. ( 2009 ) .",
    "_ markov chains and mixing times _ providence , rhode island : american mathematical society .",
    "mcfadden , d. ( 1980 ) .",
    "econometric models for probabilistic choice among products .",
    "_ journal of business _ , 53 , 1329 .",
    "mcfadden , d. and train , k. ( 2000 ) .",
    "mixed mnl models for discrete response .",
    "_ journal of applied econometrics _",
    ", 15 , 447470 .",
    "minka , t. p. ( 2001 ) . _ a family of algorithms for approximate bayesian inference_. ph.d .",
    "thesis , mit .",
    "opper , m. and archambeau , c. ( 2009 ) . the variational gaussian approximation revisited .",
    "_ neural computation _ , 21 , 786792 .",
    "ormerod , j. t. and wand , m. p. ( 2010 ) .",
    "explaining variational approximations . _ the american statistician _ , 64 , 140153 .",
    "orr , g. b. ( 1996 ) .",
    "removing noise in on - line search using adaptive batch sizes . in ( eds .",
    "mozer , m. c. , jordan , m. i. and petsche , t. ) _ advances in neural information processing systems 9 _ , 232238 .",
    "cambridge ma : mit press .",
    "paisley , j. , blei , d. m. and jordan , m. i. ( 2012 ) .",
    "variational bayesian inference with stochastic search . in ( eds .",
    "langford , j. and pineau , j. ) _ proceedings of the 29th international conference on machine learning _",
    ", 13671374 .",
    "omnipress , ny usa .",
    "powell , w. b. ( 2011 ) .",
    "_ approximate dynamic programming : solving the curses of dimensionality _ , 2nd ed .",
    "hoboken , nj : john wiley & sons , inc .",
    "ranganath , r. , wang , c. , blei , d. m. and xing , e. p. ( 2013 ) .",
    "an adaptive learning rate for stochastic variational inference . in _ jmlr : workshop and conference proceedings _ , 28 , 298306 .",
    "robbins , h. and monro , s. ( 1951 ) . a stochastic approximation method .",
    "_ the annals of mathematical statistics _ , 22 , 400407 .",
    "rossi , p. e. , allenby , g. m. and mcculloch , r. ( 2005 ) .",
    "_ bayesian statistics and marketing_. nj usa : john wiley & sons .",
    "salimans , t. and knowles , d. a. ( 2013 ) fixed - form variational posterior approximation through stochastic linear regression . _ bayesian analysis _ , 4 , 837882 .",
    "spall , j. c. ( 2003 ) introduction to stochastic search and optimization : estimation , simulation and control .",
    "new jersey ; wiley .",
    "tan , l. s. l. and nott , d. j. ( 2013 ) variational inference for generalized linear mixed models using partially non - centered parametrizations . _ statistical science _",
    ", 28 , 168188 .",
    "tan , l. s. l. and nott , d. j. ( 2014 ) a stochastic variational framework for fitting and diagnosing generalized linear mixed models .",
    "_ bayesian analysis _ , to appear .",
    "train , k. e. ( 2009 ) _ discrete choice methods with simulation _",
    "ny : cambridge university press .",
    "trower , c. a. ( 2002 ) can colleges competitively recruit faculty without the prospect of tenure ? in _ the questions of tenure _",
    "r. p. chait ) , 182220 .",
    "cambridge , ma : harvard university press .    wand , m. p. ( 2013 ) fully simplified multivariate normal updates in non - conjugate variational message passing .",
    "available at http://www.uow.edu.au/~mwand/fsupap.pdf .",
    "wang , c. , paisley , j. and blei , d. m. ( 2011 ) online variational inference for the hierarchical dirichlet process . in _ jmlr : workshop and conference proceedings _ , 15 , 752760 .",
    "wang , c. and blei , d. m. ( 2013 ) variational inference in nonconjugate models . _",
    "journal of machine learning research _ , 14 , 10051031 .",
    "waterhouse , s. , mackay , d. and robinson , t. ( 1996 ) .",
    "bayesian methods for mixtures of experts . in _ advances in neural information procesing systems 8 _ ( eds .",
    "touretzky , d. s. , mozer , m. c. and hasselmo , m. e. ) , 351357 .",
    "usa : mit press .",
    "optimal densities for @xmath49 , @xmath47 and @xmath48 can be derived using .",
    "@xmath226 where @xmath227    @xmath228 \\\\              & = n(\\mu_\\zeta , \\sigma_\\zeta ) , \\end{aligned}\\ ] ]    where @xmath229    @xmath230 \\\\ & = iw ( \\omega , \\upsilon )     \\end{aligned}\\ ] ]    where @xmath231",
    "@xmath232    @xmath233   \\\\ & \\quad + \\sum_{h=1}^h    \\left\\{- \\frac{1}{2 } \\log |\\omega| - \\frac{1}{2 } ( \\beta_h -\\zeta)^t \\omega^{-1 } ( \\beta_h -\\zeta )   \\right\\ }   \\\\ & \\quad   -\\frac{(h+1)k}{2 } \\log ( 2\\pi ) - \\frac{1}{2 } ( \\zeta -\\mu_0)^t \\sigma_0^{-1 } ( \\zeta -\\mu_0 ) \\\\ & \\quad + \\frac{(\\nu+k-1)k}{2 } \\log \\nu - \\sum_{k=1}^k \\log \\gamma \\left ( \\frac{\\nu+k - k}{2 } \\right ) \\\\ & \\quad - \\frac{\\nu+k-1}{2}\\sum_{k=1}^k \\log a_k   - \\frac{1}{2 } \\log |\\sigma_0|- \\nu \\sum_{k=1}^k \\frac{\\omega_{kk}^{-1}}{a_k}\\\\ & \\quad + \\sum_{k=1}^k\\left\\ { \\log a_k - \\log \\gamma\\left(\\frac{1}{2}\\right ) -\\frac{3}{2 } \\log a_k -\\frac{1}{a_k^2 a_k } \\right\\ } \\\\ & \\quad - \\frac{\\nu+2k}{2 } \\log |\\omega|   - \\frac{k(k-1)}{4 } \\log \\pi .",
    "\\end{aligned}\\ ] ]    @xmath234.\\end{aligned}\\ ] ]    @xmath235    where @xmath67 is as defined in .",
    "noting that the updates @xmath236 for @xmath237 and @xmath238 are deterministic , the variational lower bound @xmath38 can be simplified as @xmath239      from , @xmath240 since @xmath241 with @xmath242 and @xmath243 , @xmath244 using this approximation in the variational lower bound gives us an estimate of @xmath38 .      from , @xmath245 is approximately given by @xmath246 \\\\",
    "- \\frac { \\omega}{2 } ( \\mu_h -\\mu_\\zeta)^t \\upsilon^{-1 } ( \\mu_h -\\mu_\\zeta ) - \\frac { \\omega}{2 } \\text{tr } ( \\sigma_h \\upsilon^{-1}).\\end{gathered}\\ ] ] using vector differential calculus , it can be shown that @xmath247 @xmath248- \\omega\\upsilon^{-1 } ( \\mu_h-\\mu_\\zeta).\\end{gathered}\\ ] ]",
    "the gradient of the variational lower bound with respect to @xmath80 is @xmath249 the first term @xmath250 can be written as @xmath251 .",
    "alternatively , @xmath252 since @xmath253 .",
    "the second term is @xmath254 \\lambda_i + 0 \\\\ & = \\text{cov}_{q_i } [ t_i(\\theta_i ) ] \\lambda_i . \\end{aligned}\\ ] ] when @xmath42 is a member of the exponential family , the fisher information matrix is given by @xmath255 \\\\ & = e_{q_i } \\left [ \\left\\ {   t_i(\\theta_i)-\\nabla_{\\lambda_i } h_i(\\lambda_i ) \\right\\ } \\left\\ { t_i(\\theta_i)-\\nabla_{\\lambda_i}h_i(\\lambda_i ) \\right\\ } ^t\\right ]   \\\\ & = \\text{cov}_{q_i } [ t_i(\\theta_i)].\\end{aligned}\\ ] ]",
    "when @xmath122 , the updates of @xmath127 and @xmath128 from nonconjugate variational message passing are @xmath256^{-1 } \\;\\ ; \\text{and } \\\\",
    "\\mu_i \\leftarrow \\mu_i + \\sigma_i \\ ;   \\nabla\\negthinspace_{\\mu_i } e_q\\{\\log p(y,\\theta)\\}.\\end{gathered}\\ ] ] using the identities in , we have @xmath257 \\right)\\\\ & =   -2 \\text{vec}^{-1}\\left ( \\frac{1}{2 } \\text{vec } \\left\\ { e_{q_i } [ \\nabla_{\\theta_i}^2 e_{-q_i}\\{\\log p(y,\\theta)\\ } ] \\right\\ } \\right)\\\\ & =   - e_{q_i } [ \\nabla_{\\theta_i}^2 e_{-q_i}\\{\\log p(y,\\theta)\\ } ] \\\\",
    "\\end{aligned}\\ ] ] and @xmath258\\\\   & = e_{q_i}(\\theta_i ) + \\sigma_i \\ ; e_{q_i } [ \\nabla_{\\theta_i } e_{-q_i}\\{\\log p(y,\\theta)\\ } ] .",
    "\\end{aligned}\\ ] ] hence , we have derived the updates in from nonconjugate variational message passing updates for multivariate gaussian .",
    "we provide a proof below for the identities in , which were first derived by @xcite and @xcite . for simplicity ,",
    "we omit the subscript @xmath142 . for @xmath259 and any real function @xmath260 ,",
    "we show that @xmath261 .",
    "\\label{vr}\\end{gathered}\\ ] ]          the @xmath267th element of @xmath268 is @xmath269 applying integration by parts twice .",
    "therefore @xmath270 \\\\ & = \\frac{1}{2}\\text{vec}\\left [ \\int \\nabla_\\theta^2 q(\\theta ) \\ ; v(\\theta )   \\;d\\theta \\right ] \\\\   & = \\frac{1}{2}\\text{vec}\\bigg [ \\int   q(\\theta ) \\left\\ { \\sigma^{-1 } ( \\theta - \\mu)(\\theta - \\mu)^t \\sigma^{-1 } - \\sigma^{-1 }   \\right\\ }   \\\\ & \\quad v(\\theta )   \\;d\\theta \\bigg ]   \\\\ & = \\frac{1}{2 } \\int q(\\theta ) [ ( \\sigma^{-1 } \\otimes \\sigma^{-1 } ) \\text{vec } \\{(\\theta-\\mu)(\\theta-\\mu)^t\\ } \\\\ & \\quad - \\text{vec}(\\sigma^{-1 } ) ] v(\\theta ) \\;d\\theta . \\end{aligned}\\ ] ]"
  ],
  "abstract_text": [
    "<S> discrete choice models describe the choices made by decision makers among alternatives and play an important role in transportation planning , marketing research and other applications . </S>",
    "<S> the mixed multinomial logit ( mmnl ) model is a popular discrete choice model that captures heterogeneity in the preferences of decision makers through random coefficients . </S>",
    "<S> while markov chain monte carlo methods provide the bayesian analogue to classical procedures for estimating mmnl models , computations can be prohibitively expensive for large datasets . </S>",
    "<S> approximate inference can be obtained using variational methods at a lower computational cost with competitive accuracy . in this paper </S>",
    "<S> , we develop variational methods for estimating mmnl models that allow random coefficients to be correlated in the posterior and can be extended easily to large - scale datasets . </S>",
    "<S> we explore three alternatives : ( 1 ) laplace variational inference , ( 2 ) nonconjugate variational message passing and ( 3 ) stochastic linear regression . </S>",
    "<S> their performances are compared using real and simulated data . to accelerate convergence for large datasets , we develop stochastic variational inference for mmnl models using each of the above alternatives . </S>",
    "<S> stochastic variational inference allows data to be processed in minibatches by optimizing global variational parameters using stochastic gradient approximation . </S>",
    "<S> a novel strategy for increasing minibatch sizes adaptively within stochastic variational inference is proposed .    </S>",
    "<S> example.eps gsave newpath 20 20 moveto 20 220 lineto 220 220 lineto 220 20 lineto closepath 2 setlinewidth gsave .4 setgray fill grestore stroke grestore </S>"
  ]
}