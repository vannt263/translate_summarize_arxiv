{
  "article_text": [
    "many statistical procedures have been developed to classify data into two or more given classes .",
    "generally , if the data arise from a known class of distributions , properties of the classifiers are established through either theoretical considerations or simulation studies . by this , alternative classifiers are compared and procedures identified that are optimal under properly chosen assumptions . however , real - world data do often not fit into standard parametric distribution models . classifying them",
    "requires nonparametric procedures , while , due to the lack of established general properties , selecting a good classifier has to be mainly based on empirical evidence .",
    "usually such evidence is sought from simulation studies mimicking certain features of the data that arise in practical applications . at best , in a given field of application",
    "so called ` stylized facts ' are identified and translated into a simulation setting .",
    "but often such ` stylized facts ' do not exist .",
    "then , we can not learn much from simulation studies how a statistical procedure really works in practice .",
    "the adequacy and fitness of the procedure can only be demonstrated when it is applied to real - world data , and its _ general _ fitness can be only established by successful application to a _ large variety _ of such data .    in the sequel",
    "this is done for a newly developed nonparametric classifier , the @xmath0-procedure @xcite .",
    "it is applied to fifty binary classification problems regarding real - world benchmark data .",
    "the @xmath0-procedure first transforms the data from their original property space into a _ depth space _ , which is a low - dimensional unit cube , and then separates them by a projective invariant procedure , called _",
    "@xmath1-procedure_. to each data point the transformation assigns its depth values with respect to the @xmath2 given classes .",
    "the depth coordinates of the data reflect their degree of centrality w.r.t .",
    "each of the classes .",
    "this central ordering is carried out using a properly chosen depth function .",
    "the subsequent separation in the depth space accounts only for differences in the depth values : if @xmath3 a binary separator is determined by the @xmath1-procedure .",
    "the @xmath1-procedure stepwise selects pairs of _ extended depth properties _ ( that is , depth coordinates and powers and products of them ) and separates them by a linear rule .",
    "the separator is a hyperplane in the extended depth space , which corresponds to a polynomial line in the basic depth plane , both containing the origin as an element . with @xmath4 classes , @xmath5",
    "such @xmath1-separations can be performed and a majority rule applied ; alternatively @xmath2 one - against - all separators can be used .",
    "we restrict the present study to the case @xmath3 , see  @xcite for @xmath4",
    ".    in @xcite the zonoid depth @xcite is applied , which is efficiently computed also in higher dimensions . here",
    "we employ four alternative depths : the mahalanobis depth , the spatial depth , the projection depth and the tukey depth .",
    "the first three depths are positive everywhere , while the tukey depth ( like the zonoid depth ) vanishes outside the convex hull of the data .",
    "however , the tukey depth reflects the shape of the data much better than the previous three do and it is more robust against outliers than these ( and than the zonoid depth as well ) . for computational reasons we use the _ random tukey depth _",
    "@xcite , which approximates the tukey (= location ) depth by minimizing univariate tukey depths over a finite number of directions .",
    "when using the random tukey depth ( or another depth that vanishes outside the convex hull of the data ) a first practical question is how outsiders , that is data points having zero depth in all classes , should be treated .",
    "these points are , by construction , represented by the origin of the depth space and , hence , arbitrarily assigned . with real data , often a large portion of the data turn out to be such outsiders . as our task is to classify all points , we need either a depth that does not produce outsiders , or a supplementary treatment of outsiders . by definition ,",
    "the @xmath0-procedure includes a treatment of outsiders if necessary . for the @xmath0-classifier with the random tukey depth",
    ", several possible treatments are introduced in the sequel .",
    "the paper considers the respective variants of the @xmath0-classifier and compares them with the @xmath0-classifiers based on mahalanobis , spatial and projection depths .",
    "recall that the latter depths , as they are positive on the entire @xmath6 , do not yield outsiders .",
    "a second question is how many directions should be chosen to approximate the tukey depth and how they should be generated ; it is addressed in @xcite by means of a simulation study .",
    "( for the random projection depth this question is less important as it has no outsiders .",
    ") broad numerical experience is provided about the relative usefulness of the classifiers .",
    "further we investigate how many features in the extended depth space are needed on an average to satisfactorily separate the data .",
    "finally we demonstrate the robustness of our procedure when applied to real data containing substantial amounts of outliers .    for comparison",
    "several indicators are introduced , two of which refer to a combination of classical procedures as benchmark . to evaluate the performance of the @xmath0-procedure under different depths and outsider treatments ( when using the random tukey depth ) we have set up experiments with a large number of binary classification tasks with real data .",
    "these data sets have been selected from open internet sources and different fields of application . some of them have already served as benchmark sets in other classification studies . by this",
    "they are well suited for evaluating and comparing our new approach .",
    "the data can be downloaded in standardized form from www.wisostat.uni-koeln.de/28969.html .",
    "the complete @xmath0-procedure is available as an r - package named ddalpha .",
    "the @xmath0-classifiers are also compared with three traditional procedures : _ linear ( lda ) _ and _ quadratic ( qda ) discriminant analysis _ and the _ @xmath7-nearest - neighbors ( knn ) _ classifier , as in many cases ( including the data sets considered here ) at least one of them performs satisfactorily .",
    "we exclude neural network methods because they offer too many possible architectures , among which it is difficult to select in an automatic and computationally feasible way . while we expect that , given the specific data set , a properly adapted neural network performs rather well , such an approach affords a by hand tuning for each data set .",
    "therefore we do not regard neural networks as fair competitors to the @xmath0-classifiers .",
    "we exclude as well the usual support vector machine ( svm ) as a classifier because for each data set it has to be specially tuned .    overview : sect .",
    "[ sec : ddaclassifier ] describes the training phase of the @xmath0-classifier , which consists of the depth transformation and the @xmath1-separation in the extended depth space .",
    "the problem of generating directions for the random tukey depth is discussed .",
    "[ sec : treatments ] regards the classification phase , where the problem of outsiders arises .",
    "several classical approaches to classify the outsiders ( lda , maximum mahalanobis depth , knn ) are introduced , as well as a simplified svm approach , which liaises with the @xmath0-separation in two ways .",
    "[ sec : realdata ] first describes the 50 classification tasks , which vary by absolute and relative sizes of training classes and include different portions of outliers and ties .",
    "then the settings and results of the empirical study are presented . in sect .",
    "[ sec : evidence ] further evidence on outsider treatments and the number of ( extended ) depth properties needed is discussed .",
    "[ sec : conclusions ] concludes .",
    "consider a @xmath2-class classification problem , @xmath8 .",
    "the @xmath0-classifier has been recently proposed by @xcite .",
    "its classification phase consists of two parts : a transformation of the data from the original space into the _ depth space _ ( depth transformation ) and their subsequent separation using a modified version of the @xmath1-procedure ( @xmath1-separation ) , see @xcite , @xcite , @xcite , @xcite .",
    "this procedure is a projective invariant method to separate the depth transformed data .      the _ depth transformation _ maps @xmath9 into @xmath10^q$ ] , the depth space , where the coordinates of the transformed data reflect their degree of centrality w.r.t .",
    "each of the @xmath2 classes , so that the subsequent class separation accounts only for the depth ordering .",
    "this central ordering is carried out using a properly chosen depth function @xmath11 . for more information on depth functions the reader",
    "is referred to the literature : e.g.  @xcite for properties and @xcite for a recent survey .",
    "here we only briefly recall definitions of those used in the current work .",
    "the depth representation of the training sets in @xmath10^q$ ] is called the _",
    "depth plot_.    first we briefly regard three depths whose empirical versions take positive values beyond the convex hull of the data . for a point @xmath12 and a random vector @xmath13 in @xmath6 ( especially one having an empirical distribution on a set of @xmath14-variate observations @xmath15 ) the _ mahalanobis depth _",
    "@xcite of @xmath16 w.r.t .",
    "@xmath13 is defined as @xmath17 where @xmath18 measures the location ( e.g. the mean ) of @xmath13 , and @xmath19 the scatter ( e.g. the covariance matrix ) of @xmath13 .    the _ affine invariant spatial depth _",
    "@xcite of @xmath16 regarding @xmath13 is defined as @xmath20\\|\\,.\\ ] ] here @xmath21 for @xmath22 and @xmath23 , and @xmath19 is the covariance matrix of @xmath13 . as the mahalanobis and spatial depths lack robustness when using standard moment estimates for @xmath24 and @xmath25 , we consider also robustified versions of them , where @xmath24 and @xmath25 are estimated by mcd ( minimum covariance determinant ) .    the _ projection depth _",
    "@xcite of @xmath16 regarding @xmath13 is given by @xmath26 with @xmath27 where @xmath28 denotes the ( univariate ) median of @xmath28 and @xmath29 the ( univariate ) median of the absolute deviation of @xmath30 from its median .",
    "the _ tukey depth _ or _ location depth _",
    "@xcite of @xmath16 w.r.t .",
    "@xmath13 is defined as the minimal probability of @xmath13 lying in a halfspace bounded by a hyperplane through @xmath16 , @xmath31 where @xmath32 is the probability distribution of @xmath13 .",
    "( a closed halfspace is the set of all points that lie on one side of a hyperplane including that hyperplane . )",
    "if @xmath32 is an empirical distribution , this means that @xmath33 is the minimal portion of the data that can be cut off by a hyperplane through @xmath16 . obviously , in general , the tukey depth vanishes outside the convex hull of the distributions support .",
    "the mahalanobis , spatial , projection depths are everywhere positive ; thus outsiders can not occur .",
    "however they are not very sensitive to the shape of the underlying distribution , which is illustrated in figure  [ fig : depths ] .",
    "it exhibits the data of the `` non - donating '' class in the `` blood - transfusion '' task .",
    "the figure shows the level sets of mahalanobis , spatial , projection and tukey depths .",
    "observe that the mahalanobis depth yield ellipses , while with the spatial and projection depth rather symmetric , ellipsis - like level sets are obtained .",
    "the asymmetric shape of the data is much more reflected by the tukey depth ( bottom right ) ; therefore it appears to be better suited to extracting the relevant information from the training classes .",
    "note that , by construction , the mahalanobis depth has exact elliptical regions and the projection depth contains a symmetric factor , namely mad , which accounts for the quasi - symmetric shape of the level sets .",
    "moreover , the projection depth comes with an enormous computational cost . for these reasons",
    "we include the tukey depth in our study , in spite of its need for extra outsider treatments .    ; spatial depth using the same mcd estimates ; tukey depth .",
    "the level sets are pictured for the depth values @xmath34 for the tukey depth , and for @xmath35 for the rest of the depths.,title=\"fig : \" ] ; spatial depth using the same mcd estimates ; tukey depth .",
    "the level sets are pictured for the depth values @xmath34 for the tukey depth , and for @xmath35 for the rest of the depths.,title=\"fig : \" ] ; spatial depth using the same mcd estimates ; tukey depth .",
    "the level sets are pictured for the depth values @xmath34 for the tukey depth , and for @xmath35 for the rest of the depths.,title=\"fig : \" ] + ; spatial depth using the same mcd estimates ; tukey depth .",
    "the level sets are pictured for the depth values @xmath34 for the tukey depth , and for @xmath35 for the rest of the depths.,title=\"fig : \" ] ; spatial depth using the same mcd estimates ; tukey depth .",
    "the level sets are pictured for the depth values @xmath34 for the tukey depth , and for @xmath35 for the rest of the depths.,title=\"fig : \" ] ; spatial depth using the same mcd estimates ; tukey depth .",
    "the level sets are pictured for the depth values @xmath34 for the tukey depth , and for @xmath35 for the rest of the depths.,title=\"fig : \" ]    @xmath36 and @xmath37 are easily computed . to estimate @xmath18 ( for @xmath36 ) and @xmath19 we use empirical moments and minimum covariance determinant ( mcd ) estimates that have outlyingness parameter @xmath38 , see @xcite , @xcite .",
    "the tukey depth as well as the projection depth satisfy the weak projection property @xcite , i.e. the depth of a point can be represented as the minimum of the depths on all unidimensional projections .",
    "based on this we approximate the tukey depth by the random tukey depth @xcite , which is the minimum univariate tukey depth over a set of unidimensional projections in randomly selected directions . as the exact calculation of @xmath39 is rather elaborate @xcite , we approximate it in the same way .      for each binary separation",
    "@xmath1-procedure _ constructs a decision hyperplane in the extended property space @xmath40^r$ ] , @xmath41 . the extended property space includes the powers and products of objects attributes up to some degree @xmath42 as additional coordinates .",
    "we mention the original depth coordinates as the _ basic _ , and the other coordinates as the _ extended properties_. from all these properties , the relevant ones are stepwise chosen by the @xmath1-procedure .",
    "the final separation then is performed by an ` optimal ' hyperplane in @xmath10^r$ ] .",
    "let @xmath43 and @xmath44 be two training classes in @xmath6 having @xmath45 and @xmath46 elements respectively .",
    "each data point @xmath47 is transformed to @xmath48 ^ 2 $ ] , with @xmath49 . in the _ first step",
    "_ all pairs of ( basic or extended ) properties are selected that involve depths in both classes and the respective two - dimensional coordinate subspaces are considered . in each of these planes",
    "we separate the two classes by a straight line passing through the origin .",
    "the separating line is determined by the angle @xmath1 ( formed with one of the axes ) that yields the minimal _ empirical misclassification rate ( emr)_. among all considered two - dimensional property spaces we choose the one delivering the smallest _ emr _ ; then we project its points onto a straight line @xmath50 that is orthogonal to the separating one .",
    "now , separation in @xmath50 is performed by the origin , and @xmath50 becomes the first _ projection axis _ of a synthesized space .",
    "we illustrate the procedure with data from the pima indians diabetes study ; see www.stats.ox.ac.uk/pub/prnn/pima.tr2 .",
    "a subsample consisting of @xmath3 classes ( 68 diseased and 132 not diseased females ) has been selected , having seven attributes ( number of pregnancies , 2 hours glucose concentration , blood pressure , triceps skin thickness , body mass index , diabetes pedigree function , age ) .",
    "the data points are represented in the unit square by their random tukey depths regarding the two classes ( using 10  000 random directions ) , and powers and products of depth values are considered up to degree @xmath51 ; thus the extended depth space has dimension @xmath52 .",
    "figure [ figurealpha ] ( left ) shows the first step applied to the pima data . here ,",
    "after mapping @xmath53 into @xmath10 ^ 2 $ ] , the depth space was extended up to degree @xmath51 , which yields the extended depth space @xmath10 ^ 5 $ ] . with the pima data the smallest _ emr _",
    "is achieved at the two basic depth properties @xmath54 and @xmath55 .",
    "after the first step the extended space is reduced by removing the two ( possibly extended ) properties that have obtained minimal _ emr _ , and a similar second step is performed . in the _ second step",
    "_ we consider all planar subspaces based on @xmath50 and one of the properties of @xmath10^{r-2}$ ] and find a separating straight line minimizing _ emr _ in each of them .",
    "again , among these planes we choose the one that yields the smallest _ emr _ and obtain the second projection axis @xmath56 .",
    "it separates the data at its zero point , as the first axis did in the initial step ; see figure [ figurealpha ] ( right ) for the pima example . similarly , after step 2 the extended space is reduced to an @xmath57-dimensional unit hypercube .",
    "the steps are iterated , and properties are selected from the extended space , as long as the minimum _ emr _ decreases and the remaining extended space is non - void .",
    "note that with the pima data , the procedure stops after step 2 .",
    "-separation ; step 1 ( left ) and step 2 ( right).,title=\"fig:\"]-separation ; step 1 ( left ) and step 2 ( right).,title=\"fig : \" ]      as mentioned above , we approximate the @xmath14-variate tukey depth by the minimum univariate depth of the ( on lines in several directions ) projected data .",
    "the random tukey depth inherits the robustness from the tukey depth . when implementing the random tukey depth we have to answer the following questions in a way that makes our classifier reasonable and computationally feasible .    1 .",
    "how should the random directions be generated ? 2 .   how many directions should we consider ? 3 .",
    "shall we generate a new set of directions for each point @xmath16 ?",
    "ad ( 1 ) : as in @xcite we generate directions that are uniformly and independently distributed on the unit sphere and independent of the data .",
    "alternatively we could proceed as in @xcite , i.e.  search through the normals of randomly formed @xmath58-simplices .",
    "however we find it more efficient to spend computational time on generating random directions and evaluating univariate depths rather than computing exact directions from the data ; moreover , a moderate number of random directions proves to be enough .",
    "ad ( 2 ) : we mention two qualitative arguments concerning this number .",
    "firstly , a larger dimension of the space needs a larger number of directions , which is obvious from the geometry of @xmath6 .",
    "however , we are not able to indicate the precise dependency of this number on the dimension @xmath14 .",
    "secondly , we point out that there exists a trade - off between the number of directions used for the random tukey depth and the number of outsiders .",
    "a simulation study that illuminates this trade - off has been conducted in @xcite . if the data stems from a centrally symmetric distribution ( e.g. gaussian or another elliptical distribution ) the depth can be rather precisely approximated with a small number of directions .",
    "but if the data exhibit asymmetries and possible outliers , as real - world data mostly do , we need a larger number of directions to adequately represent them in the depth space . the degree of asymmetry and fat - tailedness found in the data may guide us in choosing this number ; see @xcite . to be able to compare the procedure on different data sets we have chosen a fixed number of directions",
    "this number has been set to 10  000 as a practical compromise between accuracy of the depth calculation and computational load .",
    "it is obvious ( see e.g. @xcite ) that the random tukey depth of a point can widely deviate from the tukey depth . on the other hand ,",
    "the @xmath1-separation is rather robust , since the only invariant it uses is whether a point belongs to a class or not .",
    "therefore an upward bias at a few points does not much influence the final separation .",
    "further , a trade - off exists between the number of directions used for the random tukey depth and the number of effective outsiders , which favors a moderate number of directions .",
    "ad ( 3 ) : we use the same set of directions for each data point .",
    "though the generation of a new set of directions for each point produces different depth values , there is no reason to expect them to be more precise .",
    "by keeping the set of directions constant we increase the speed of calculations in the training phase .",
    "the computations are boosted by avoiding the generation of the directions and projection of the points onto them .",
    "let @xmath7 be the number of random directions .",
    "when instantly generating the direction set for each point the complexity of the depth calculation amounts to @xmath59 .",
    "if the direction set is not constantly changed , first , the time for their generation is saved .",
    "second , recall that in the training phase the depth of all points of the training sample w.r.t .",
    "each class has to be computed .",
    "hence , univariate projections are ordered , and all the univariate depths on each projection can be determined in a single pass .",
    "this yields complexity @xmath60 .",
    "note that @xmath61 holds .",
    "therefore , for all @xmath14 , @xmath45 and @xmath46 that are large enough to suppress eventual constants , the constant direction approach is substantially faster , see also @xcite .",
    "it also enhances the classifier s stability in the classification phase , as the same directions are used to approximate the depth of a new point to be classified .",
    "exact calculation of the projection depth appears to be a heavy computational task @xcite .",
    "therefore we approximate the projection depth in the same way as the tukey depth by minimizing the univariate depth of projections in randomly drawn directions .",
    "compared with the tukey depth , many more directions are needed to produce a reasonable approximation of the projection depth : when traversing the unit sphere the projection depth ( which is a piecewise linear function ) changes direction much more frequently because of the median and mad estimates . to be able to compare the procedure on different data sets we have chosen two fixed numbers of directions .",
    "these have been set to 10  000 and 100  000 as a practical compromise between accuracy of the depth calculation and computational load .",
    "the _ classification phase _ proceeds as follows .",
    "consider a point @xmath16 to be classified .",
    "first the depth transform of @xmath16 is calculated as @xmath62 . if @xmath54 is zero but not @xmath55 , the point @xmath16 is assigned to class @xmath44 , and viceversa .",
    "if both @xmath54 and @xmath55 are non - zero the point is classified according to the separation rule determined by the @xmath1-procedure .",
    "this is always the case when using the mahalanobis , spatial or projection depths .",
    "when employing the random tukey depth some @xmath16 may have @xmath63 , then @xmath16 is regarded as an _",
    "outsider_. an outsider , being represented by the origin of the depth plot , can not be readily classified but needs some special treatment .",
    "specifically , a point @xmath64 that , in the original data space , lies outside the convex hulls of the two training sets has zero tukey depth in both classes and , thus , is an outsider . if a point @xmath64 is no outsider it is mentioned as an _",
    "insider_. insiders are instantaneously classified by the @xmath1-procedure .    as in @xcite",
    ", outsiders may be classified by determining their nearest neighbors . in doing so ,",
    "euclidean and mahalanobis distances can be employed , the latter to account for scatter within the classes . alternatively outsiders can be classified according to their maximum mahalanobis depth , which is always positive . @xcite",
    "introduce a depth function , which is the maximum of the zonoid depth and a properly scaled mahalanobis depth , and thus circumvent the outsider problem .",
    "@xcite propose an approach that avoids the outsider problem as well . in the classification phase , for each point @xmath16 to be classified , ( 1 ) the sample is extended by reflecting the training classes symmetrically at center @xmath16 , ( 2 ) the depth of points in the extended sample is considered , and ( 3 ) a @xmath7-nearest - neighbor rule that uses depth in place of distance is applied for classifying @xmath16 . here not only the classification phase is computationally hard ( by instantaneous calculation of the depths of all data points ) , but also the training phase , where the classifier has to be validated in order to determine @xmath7 .",
    "this requires onerous computations .    in the sequel",
    "we compare several alternative outsider treatments , which are classifiers applied to data in the original space .",
    "the treatments include three well known classifiers : linear discriminant analysis , maximum mahalanobis depth , and @xmath7-nearest neighbors as well as a new one , which we call _ svm - simplified_. the performance of the @xmath0-classifier with mahalanobis , spatial and projection depth is contrasted as well .",
    "note that all depths ( tukey , mahalanobis , spatial and projection ) are affine invariant as well as all treatments used ( lda , knn with an affine invariant distance , mahalanobis depth and the support vector machine ) .",
    "therefore all considered @xmath0-classifiers are affine invariant ( under appropriate moment assumptions ) , if the exact versions of the depths are calculated .",
    "since the random tukey depth and the random projection depth converge to the exact versions , using them makes the @xmath0-classifiers approximately affine invariant .    the random tukey depth ( rtd )",
    "used for the depth transform is very efficiently calculated , but yields outsiders . as it approximates the tukey depth ( td ) from above , some td - outsiders will have non - zero rtd and , by this , be assigned to one of the classes",
    ". a smaller number of directions yields a worse approximation of the td , but reduces the number of outsiders .",
    "the remaining rtd - outsiders still need a special treatment , though .",
    "thus , when using the rtd , we face a trade - off between the quality of depth approximation and the extent of outsider treatment needed . as we will see below with real data , choosing a moderate number of random directions gives best results",
    ".      _ linear discriminant analysis _ ( lda ) , introduced in @xcite , separates the classes by a hyperplane in the original data space ; see also @xcite .",
    "the lda classifier is particularly simple .",
    "it is optimal if the data follow a gaussian or , more general , a unimodal elliptical distribution and the classes differ by location shifts only .",
    "however , in classifying real data it is often outperformed by other approaches . in many applications , the real data can not be assumed to be gaussian and ask for procedures different from lda .",
    "however , after having classified the rtd - insiders , the remaining task of classifying the outsiders appears to be a much less exigent task and may be successfully done by a simple procedure like lda .    the _ maximum - mahalanobis - depth _",
    "classifier is given by @xmath65 where @xmath66 is the prior probability for class @xmath67 .",
    "the priors are estimated by the training class portions .",
    "again , this classifier has optimality properties under ellipticity .",
    "applied to outsiders it is expected to perform satisfactorily .",
    "the _ @xmath7-nearest - neighbors _ classifier is still another option for treating outsiders .",
    "its parameter @xmath7 , the number of the nearest neighbors , has to be chosen by cross - validation .",
    "often a relatively small @xmath7 is enough ; see , e.g. , @xcite , where already @xmath68 produces satisfying results . to make the procedure affine invariant we use mahalanobis distances ( based on the pooled data set ) for finding nearest neighbors .      as another way to handle the outsider problem we propose to supplement the @xmath0-classifier by an additional svm - rule , which is restricted to classifying the outsiders .",
    "it has a particularly simple structure .",
    "recall that the @xmath0-procedure delivers a separator which is a hyperplane in the extended depth space .",
    "this hyperplane induces a decision rule in the original data space .",
    "next , we remove all training points which are not correctly classified by this rule ( so that _ _",
    "emr__@xmath69 ) and subject the remaining points to an additional svm classification step that involves determining a single kernel parameter but no box - constraint .",
    "this new approach is named _ svm - simplified _ ( _ svm - s _ ) . as the _",
    "svm - s _ rule is defined on the whole @xmath6 , it is able to assign points which are outsiders in the @xmath0-classification .",
    "figure [ figsvm13 ] , left panel , shows two classes , each containing 250 points , which are simulated from n@xmath70,\\bigl[\\begin{smallmatrix } 1 & 1\\\\ 1 & 4 \\end{smallmatrix}\\bigr])$ ] ( red ) and n@xmath71,\\bigl[\\begin{smallmatrix } 4 & 4\\\\ 4 & 16 \\end{smallmatrix}\\bigr])$ ] ( blue ) , together with the separating lines of the optimal bayes ( dashed ) and @xmath0 ( solid ) classifiers .",
    "the left panel regards the original data , while the right panel exhibits the data after the removal step .",
    "the _ svm - s _ step consists in solving the following quadratic programming problem @xcite : @xmath72 subject to the constraints @xmath73 @xmath74 here we notate @xmath75 , @xmath76 , @xmath77 and @xmath78 .",
    "@xmath79 stands for the @xmath80-dimensional vector of responses @xmath81 , and @xmath82 is a symmetric @xmath83-matrix with elements @xmath84 where @xmath85 is a gaussian kernel . note that no box - constraint condition is needed here as the points are separable without error .",
    "but still the kernel parameter @xmath86 has to be chosen . for given @xmath86 a solution @xmath87 of ( [ eqn : svmproblem ] ) is obtained , provided the two classes are linearly separable in the reproducing kernel hilbert space that corresponds to @xmath88 .",
    "every such solution @xmath89 determines a margin between the classes @xmath90 and a number of support vectors , @xmath91 .    in figure [ figsvm2 ] , depending on @xmath86 , the values of @xmath92 ( dashed line ) and the corresponding numbers of support vectors ( solid line ) are plotted for the above example .",
    "a zero value of the margin @xmath92 or of the number of support vectors indicates that with the given @xmath86 no errorless discrimination is possible .",
    "( logarithm to the base 10 ) . ]    loosely speaking , @xmath86 controls the complexity of the _ svm - s _ rule ; for small values of @xmath86 the decision rule is not able to separate the classes at all .",
    "therefore it suggests itself to use the simplest separating rule , i.e.  selecting the smallest @xmath86 for which the classes are separated without error ( as indicated by the small circle in figure [ figsvm2 ] ) .",
    "this also comes out as a most stable decision rule .",
    "figure [ figsvm13 ] exhibits the corresponding decision rule in its right panel as a solid line , while the dashed line indicates the same optimal bayes decision rule as on the left panel of the figure ; the rules appear to be very similar .",
    "most important , calculating the _ svm - s _ rule needs no parameter tuning besides selecting the parameter @xmath86 , which is a straightforward task .",
    "needless to say , with the usual support vector machine ( svm ) a solution can be obtained that is at least as good as the one achieved here . however , obtaining this solution needs a tuning of parameters that is computationally much more intensive .    to summarize the above procedure : the training phase consists of two steps , first determining the @xmath0-classifier and then determining an _ svm - s _ rule based on the correctly @xmath0-classified points .",
    "note that the classification performance of this procedure is determined by the @xmath0-classifier , and",
    "the _ svm - s _ step just extrapolates this classifier to treat the outsiders . in our experiments the whole training phase took between a few seconds and several minutes of computation time ( 64-bit , 1 kernel of the icore 7 - 2600 having enough operative memory ) .",
    "the time reached a maximum of 10 minutes with four very large data sets only .    in the classification phase",
    "we have two choices : either using the obtained _ svm - s _",
    "rule for all points @xmath93 to be classified , or first check for each @xmath93 whether it is an insider or an outsider and then classify it with the @xmath0-rule if it is an insider and with the _ svm - s _ rule otherwise .",
    "the first choice yields a particularly fast procedure , as the _ svm - s _ rule does not involve any depth calculations in the classification phase .",
    "we choose the second one as it is in line with the application of the outsider treatments mentioned above .",
    "its results are presented in column ` svm - s ' of tables [ tab : performance1 ] and [ tab : performance2 ] .",
    "to evaluate the @xmath0-procedures with different outsider treatments and to judge their usefulness in practical applications we have set up experiments based on a large variety of real data sets .",
    "the methodology is applied to 50 binary classification tasks , which have been obtained from partitioning 33 freely accessible data sets , see tables [ tab : datasets1 ] and [ tab : datasets2 ] .",
    "the subset of the `` pima indian diabetes '' described above is included in the table as `` pima '' ( no .",
    "the authors and introducers of the accessed data sets are @xcite ( `` biomed '' ) , @xcite ( `` cloud '' ) , @xcite ( `` irish - ed '' ) , @xcite ( `` kidney '' ) , @xcite ( `` plasma - retinol '' ) , @xcite ( `` socmob '' ) and @xcite ( `` veteran - lung - cancer '' ) ; these data sets have been downloaded from + lib.stat.cmu.edu/datasets . + data sets `` chemdiab '' @xcite and `` hemophilia '' @xcite have been taken from the r - packages ` locfit ' and ` rrcov ' respectively .",
    "the `` pima '' data set constitutes a training subsample of the `` diabetes '' ( see below ) and can be downloaded from + www.stats.ox.ac.uk/pub/prnn @xcite . +",
    "datasets `` baby '' , `` banknoten '' @xcite , `` crab '' @xcite , `` gemsen '' , `` groessen '' @xcite , `` tennis '' , `` tips '' and `` uscrime '' @xcite have been downloaded from the teaching data base + stat.ethz.ch/teaching/datasets .",
    "+ the rest of the data sets is taken from + archive.ics.uci.edu/ml @xcite ; + it in particular originates from @xcite ( `` blood - transfusion '' ) , @xcite ( `` breast - cancer - wisconsin '' ) and @xcite ( `` vowel '' ) .",
    "multiclass problems were reasonably split into binary classification problems , and some of the data sets were slightly processed by removing correlated attributes , by dropping objects with missing values , and by selecting prevailing classes . for detailed descriptions of the data considered we refer to the corresponding literature and public repositories ; the fifty tasks together with short descriptions of the data can be found on the web page + www.wisostat.uni-koeln.de/28969.html .",
    "as we see from tables [ tab : datasets1 ] and [ tab : datasets2 ] , the classification tasks are much different .",
    "the tables show their basic parameters : dimension @xmath14 of the original space , log ratio of the cardinalities @xmath45 and @xmath46 of the training classes ( so that the sign reflects which is larger ) , total sample length @xmath94 , percentages of outliers and outsiders .",
    "as we see , up to 13 attribute dimensions are considered .",
    "the total sample sizes range from 47 to 1349 , while the relative size of the two classes varies between 1 and 6.5 .    [ cols=\">,<,>,>,>,>,>,>,>\",options=\"header \" , ]",
    "a fast classification procedure , the @xmath0-procedure , has been introduced that is essentially nonparametric , robust , and computationally feasible for any dimension @xmath14 of attributes .",
    "the @xmath0-procedure is available in the r - package ddalpha .",
    "the @xmath0-classifier is particularly robust for two reasons : first , as the classification is done by the @xmath1-procedure , which is _ per se _ robust ; second , as the data are transformed into a low - dimensional compact space . generally ,",
    "two cases are to be distinguished with the depth transform : non - vanishing depth or depth vanishing beyond the convex hulls of the training classes .",
    "non - vanishing depths , e.g.  mahalanobis or spatial , often induce a spurious symmetry and are intrinsically non - robust ( though , can be robustified ) .",
    "the projection depth , which is non - vanishing , also produces ellipsis - like regions .",
    "it is robust , but computationally inefficient when @xmath95 .",
    "the last problem is faced by the tukey depth , which best reflects the shape of the data . in place of the exact versions of projection and tukey depth we employ their random versions by minimizing univariate depths in directions that are uniformly distributed on the sphere .",
    "a very large number of these directions is needed for the calculation of the projection depth , while for the random tukey depth the number of directions can be kept low , as there is a tradeoff between this number and the number of points being classified by their depth values .    on the other hand the random tukey depth yields outsiders when classifying , i.e. points lying outside the convex hulls of all classes , which can not be readily classified and need additional treatment . in real data applications",
    "the percentage of outsiders can be large ( see the introduced measure of outsider proneness ) and thus substantially influence the classification performance .",
    "therefore , the choice of the treatment is important when applying the random tukey depth .",
    "the treatments considered subject the outsiders either to linear discriminant analysis ( lda ) , classification according to @xmath7-nearest neighbors ( knn ) , maximum mahalanobis depth classification based on moment or mcd estimates , or the newly introduced svm - simplified procedure ( _ svm - s _ ) . the latter is very fast as it needs no tuning of a box - constraint ; only the smallest separating @xmath86 has to be computed . additional calculations ( not included here , see also @xcite ) show , that regarding the other possible outsider treatments , the choices of number @xmath7 in knn as well as of the covering parameter in mcd do not much influence their performance .",
    "thus the @xmath0-procedure needs practically no parameter tuning .",
    "the degree of the separating polynomial is chosen by cross - validation within the depth representation only , where the modified @xmath1-procedure , on each of the planar subspaces considered , has a quick sort complexity , o@xmath96 , and by that is very fast .",
    "the above introduced variants of the @xmath0-procedure are challenged by 50 binary classification problems that arise from a broad range of real data .",
    "the tasks are complicated by the presence of outliers and ties . as competitors of the @xmath0-procedure three traditional classification methods (",
    "lda , qda , and knn ) are evaluated with the same data .",
    "naturally , none of the classifiers is best at all tasks in terms of the average error rate , but each classifier is best at some of them .",
    "our results also show that no single depth or outsider treatment dominates the others . just for almost all data",
    "sets the classification of outsiders according to their maximum moment - mahalanobis depth is outperformed by the same with the mcd - mahalanobis depth .",
    "this can be explained by the outliers present .",
    "five goodness measures are introduced aggregating performance of the classifiers over the 50 classification tasks w.r.t .",
    "different aspects and allowing for direct comparison of the classifiers .",
    "clearly , classification performance greatly depends on the choice of the depth and , if needed , the outsider treatment .",
    "the measures point out two @xmath0-classifiers ( starting with the best ) : based on spatial and mahalanobis depth ( both using moment estimates ) .",
    "the rest of the classifiers show varying performance for different goodness measures , which is demonstrated by the goodness visualization .",
    "the experience with real - data problems tells us further that , in most practical cases , the separation procedure in the depth space stops after a few steps . in most cases",
    "the subspace spanned by the depth features needed has dimension two , which points out the high stability of the separating rule .",
    "the problems and solutions investigated in this study are also of interest in more general settings : the problem of coping with outsiders is common to any statistical procedure that is based on depth plots and involves a notion of depth vanishing outside the convex hull . using the random tukey depth as an efficient approximation of the tukey depth and selecting the random directions",
    "has many applications .",
    "available algorithms for exactly calculating the tukey depth @xcite are computationally expensive , but can serve as a benchmark .",
    "finally , the svm - simplified method is introduced and appears as a simple and efficient way to avoid the computational burden of tuning the svm .",
    "we thank the maintainers and contributors of the repositories + archive.ics.uci.edu/ml , + lib.stat.cmu.edu/datasets , + stat.ethz.ch/teaching/datasets .",
    "we are also grateful to oleksii pokotylo , master student at the national technical university of ukraine , for his help in preparation and maintaining the r - package ddalpha .",
    "valuable comments of two anonymous referees and an editor are greatly acknowledged ."
  ],
  "abstract_text": [
    "<S> the @xmath0-classifier , a nonparametric fast and very robust procedure , is described and applied to fifty classification problems regarding a broad spectrum of real - world data . </S>",
    "<S> the procedure first transforms the data from their original property space into a depth space , which is a low - dimensional unit cube , and then separates them by a projective invariant procedure , called @xmath1-procedure . to each data </S>",
    "<S> point the transformation assigns its depth values with respect to the given classes . </S>",
    "<S> several alternative depth notions ( spatial depth , mahalanobis depth , projection depth , and tukey depth , the latter two being approximated by univariate projections ) are used in the procedure , and compared regarding their average error rates . with the tukey depth , which fits the distributions shape best and is most robust , ` outsiders ' , that is data points having zero depth in all classes , appear . </S>",
    "<S> they need an additional treatment for classification . </S>",
    "<S> evidence is also given about the dimension of the extended feature space needed for linear separation . </S>",
    "<S> the @xmath0-procedure is available as an r - package .    </S>",
    "<S> * keywords : * classification , supervised learning , alpha - procedure , data depth , spatial depth , projection depth , random tukey depth , outsiders , features </S>"
  ]
}