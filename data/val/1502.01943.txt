{
  "article_text": [
    "clustering plays a basic role in many parts of data engineering , pattern recognition and image analysis @xcite .",
    "one of the most important is gaussian mixture models @xcite .",
    "it is hard to overestimate the role of gmm in computer science @xcite , including object detection @xcite , object tracking @xcite , learning and modelling @xcite , feature selection @xcite , classification @xcite or statistical background subtraction @xcite .",
    "gmm accommodates data of varied structure , e.g. the component distributions can concentrate around surfaces of lower dimension obtained by principal components ( pca ) @xcite .",
    "however , it often happens that clusters are concentrated around lower dimensional manifolds which are not linear .",
    "since one non - gaussian component can often be approximated by several gaussian ones @xcite , these clusters are in practice represented by introducing more gaussian components which can be seen as a form of piecewise linear approximation , see fig .",
    "[ fig : acacec ] . due to the intrinsic linearity of the gaussian model , when there are nonlinear manifolds in the data cloud , it is natural that many components are required and the fitting error is large .",
    "consequently , the constructed model does not reflect optimally the internal structure of the data .",
    "a similar result gives cross entropy clustering approach , compare fig [ fig : letterb1 ] and [ fig : letterb2 ] .",
    "there are several methods attempting to solve the problem of fitting nonlinear manifolds , e.g. principal curves and principal surfaces @xcite .",
    "principal curves / surfaces algorithms are typically capable of expressing a single complex manifold . in @xcite",
    "the authors present an adaptation of the gaussian mixture model called active curve axis gaussian mixture models ( acagmm ) , which uses a nonlinear curved gaussian probability model in clustering . in its basic version it works with data on the plane and",
    "adapts to the quadratic curves . in other words",
    "acagmm uses a wider class then typical gaussians  namely gaussians which are curved over parabolas .",
    "since our paper aims at solving the same task as acagmm , let us first explain the method and present the typical steps behind it .",
    "first , using an additional tool , the authors find the `` right '' number of clusters ( one of the possible methods is given in @xcite , however , one can also use @xcite ) .",
    "then for each cluster the pca algorithm is applied to determine the reasonable basis , and a gaussian curved along the optimal parabola is used .",
    "the coordinate system is nonlinear , see fig .",
    "[ fig : letterb3 ] ( the @xmath0 coordinate is chosen as a distance from the parabola , and @xmath1 is the length on the parabola from the projected point to the parabola s vertex ) .",
    "acagmm has found applications in particularly in human hand motion recognition @xcite .",
    "it can also be fuzzified @xcite .",
    "[ fig : letterb1 ]   [ fig : letterb2 ]   [ fig : letterb3 ]   [ fig : letterb4 ]     acagmm works well in practice , however , it has some limitations .",
    "the model is naturally restricted to quadratic functions as the nonlinear coordinate system requires the projection onto the graph and length of the curve .",
    "the use of the method in higher dimensional case , although possible , is practically rather limited .",
    "moreover , acagmm is not a theoretically based density model ( see appendix for the detailed explanation ) , and therefore it is not in fact formally em based , but only uses its optimization algorithm .",
    "consequently , contrary to the classical em @xcite , the mle cost function does not necessarily decrease with iterations .",
    "let us recall that in general em aims at finding @xmath2 , @xmath3 and @xmath4 gaussian densities ( where @xmath5 is given beforehand and denotes the number of densities which convex combination builds the desired density model ) such that the convex combination @xmath6 optimally approximates the scatter of our data @xmath7 with respect to mle cost function @xmath8 the em procedure consists of the expectation and maximization steps .",
    "while the expectation step is relatively simple , the maximization usually needs complicated numerical optimization even for relatively simple gaussian models @xcite .          in this paper",
    "we propose the method which is based on the cec model , instead of the expectation maximization ( em ) and gaussian density model in a curvilinear coordinate system . a goal of cec is to minimize the cost function , which is a minor modification of that given in by substituting sum with maximum : @xmath9 instead of focusing on the density estimation as its main task , cec aims itself directly to the clustering problem .",
    "it occurs that at the small cost of minimally worse density approximation @xcite we gain speed in implementation and the ease of using more complicated density models .",
    "roughly speaking , the advantage is obtained because models do not mix with each other , since we take the maximum instead of sum .",
    "consequently , we are able to construct an algorithm which is easy to adapt to the higher dimensional case . the results of and acagmm are similar on the plane , compare fig . [",
    "fig : letterb3 ] and fig .",
    "[ fig : letterb4 ] .",
    "the effect of our algorithm in @xmath10 on a shark - type set @xcite is shown in fig .",
    "[ fig:3d_1 ] .",
    "the method is able to reduce unnecessary clusters . in fig .",
    "[ fig : dog ] we present a convergence process of with initial number of clusters @xmath11 , which is reduced to @xmath12 .    , which is reduced to @xmath12.,title=\"fig:\",width=81 ] , which is reduced to @xmath12.,title=\"fig:\",width=81 ] , which is reduced to @xmath12.,title=\"fig:\",width=81 ] , which is reduced to @xmath12.,title=\"fig:\",width=81 ] , which is reduced to @xmath12.,title=\"fig:\",width=81 ] , which is reduced to @xmath12.,title=\"fig:\",width=81 ] , which is reduced to @xmath12.,title=\"fig:\",width=81 ] , which is reduced to @xmath12.,title=\"fig:\",width=81 ] , which is reduced to @xmath12.,title=\"fig:\",width=81 ] , which is reduced to @xmath12.,title=\"fig:\",width=81 ] , which is reduced to @xmath12.,title=\"fig:\",width=81 ] , which is reduced to @xmath12.,title=\"fig:\",width=81 ]    this paper is arranged as follows . in the next section the theoretical background of the density model will be presented .",
    "since acagmm works in @xmath13 only for parabolas we start with a similar situation .",
    "then we describe a general model for data in @xmath14 . in the third chapter we present the theoretical background of the method . in particular , we prove that the cost function decreases in every iteration , see theorem [ the : ener ] .",
    "the last chapter presents numerical experiments . in appendix",
    "we include details of the description of the acagmm model .",
    "in this section , the @xmath15-adapted gaussian distribution , where @xmath16 is a continuous function , will be presented .",
    "the goal of this approach is to transform a normal distribution ( which assumes the intrinsic linearity of the model ) to the case of curves ( or more generally to manifolds ) , which are given by the graph of the function  @xmath15 .",
    "the above model will be used in the method .",
    "since acagmm works in the two - dimensional case ( in higher dimensional ones the authors use pca to reduce problems to 2d ) with parabolas ( @xmath17 for @xmath18 ) , we start from comparison acagmm and our model in such a case .",
    "let @xmath17 for @xmath18 be given .",
    "the two dimensional gaussian density for @xmath19 $ ] and covariance matrix @xmath20 is given by the following formula @xmath21 where in the one dimensional case we have @xmath22    let @xmath23^t \\in \\r^2 $ ] be given .",
    "the acagmm approach uses the orthogonal projection of the point @xmath24 onto the parabola @xmath15 which is denoted by @xmath25 and the arc length between @xmath25 and @xmath26 which is denoted by @xmath27 . consequently the acagmm function is given by @xmath28    this approach is very intuitive but it causes two basic problems .",
    "it is very hard ( or even impossible ) to give explicit formulas for orthogonal projection and arc length for more complicated curves in higher dimensional spaces .",
    "calculations are complicated ( from the numerical point of view ) , consequently the field of possible generalizations of acagmm is limited . moreover , the function which was used in acagmm , see formula ( [ acagmm_fun ] ) , is not a density .",
    "the jacobian of the respective transformation was not included ( see appendix ) .",
    "in our paper we use a simpler approach , which is based on the euclidean norm and the following formula for the density function @xmath15 : @xmath29 ) = n(m_1,\\sigma_1 ^ 2)(x_1)\\cdot n(m_2,\\sigma_2 ^ 2)(x_2-f(x_1)).\\ ] ] since we do not use orthogonal projection and arc length , it is easy to calculate the parameters of our generalized gaussian distribution , see fig .",
    "[ fig : el ] .",
    "the practical difference in @xmath13 between acagmm and our approach is quite small instead @xmath30 since our method does not apply the change of coordinates given by pca .",
    "] , see fig .",
    "[ fig : one_claster ] . nevertheless , our model is more flexible , as we can use an arbitrary class of functions for which least squares methods work .",
    "[ fig : fu1 ]     + [ fig : fu2 ]        in this subsection , the general notion of @xmath15-adapted gaussian will be presented .",
    "let us recall that the standard gaussian density in @xmath14 is defined by @xmath31 where @xmath26 denotes the mean , @xmath32 is the covariance matrix and @xmath33 is the square of the mahalanobis norm .    in our work",
    "we use a multidimensional gaussian density in a curvilinear coordinate system which is spread along the function @xmath34 ( @xmath15-adapted gaussian density ) .",
    "we treat one of the variables ( for simplicity , the last one ) separately . in such a case",
    "we consider only those @xmath35 ( where @xmath36 denotes the set of @xmath37-dimensional square matrices ) which have the diagonal block matrix form @xmath38 where @xmath39 and @xmath40 . for @xmath41 and @xmath42 we will use the notation @xmath43 for @xmath44",
    ", we denote @xmath45 the set containing vectors from @xmath46 with removed @xmath5 coordinate , and @xmath47 for a function @xmath48 , we denote @xmath49    let @xmath50 , @xmath39 , @xmath40 , @xmath51 be given . the @xmath15-adapted gaussian density for @xmath52 , @xmath53 and @xmath26 is defined as follows @xmath54    level sets for @xmath15-adapted gaussian distributions with different types of functions are presented in fig .  [",
    "fig : e ] .",
    "the @xmath15-adapted gaussian function @xmath55 , where @xmath56 ) , @xmath39 , @xmath40 , @xmath51 is a density .",
    "let @xmath57 be a @xmath37-dimensional gaussian density such , that @xmath58 where @xmath39 , @xmath40 , @xmath51 .",
    "let us consider a substitution @xmath59 in such a case , the jacobian is equal to @xmath60 consequently , @xmath55 is a density .",
    "we will use the family of all @xmath37-dimensional gaussian densities @xmath61 .",
    "moreover , for @xmath34 , we will consider family of @xmath15-adapted gaussian functions @xmath62 for the family @xmath63 , we define @xmath64 we show that if @xmath65 contains all linear transformations , then @xmath66 .",
    "let us start with simple lemma .",
    "[ lem : fam ] let @xmath67 , @xmath39 , @xmath68 and @xmath69 be given .",
    "then for @xmath70 we have @xmath71 where @xmath72 such , that @xmath73 .",
    "let us denote @xmath74 and @xmath75 $ ] , then we have @xmath76 @xmath77 @xmath78 it is easy to show that @xmath79 @xmath80.\\ ] ] therefore we have @xmath81 ( a\\sigma a^t)^{-1 } \\begin{bmatrix } \\x_{\\hat d } \\\\ x_{d } \\end{bmatrix}=[\\x_{\\hat d}^t , x_d ] \\left ( \\begin{bmatrix } \\sigma_{\\hat d}^{-1 } & 0 \\\\ 0 & 0 \\end{bmatrix}+ \\sigma_{d}^{-1 } \\begin{bmatrix } -\\v \\\\ 1 \\end{bmatrix } [ -\\v^t , 1 ] \\right)^{-1 } \\begin{bmatrix } \\x_{\\hat d } \\\\",
    "x_{d } \\end{bmatrix}=\\ ] ] @xmath82 \\begin{bmatrix } \\sigma_{\\hat d } & 0 \\\\ 0 & \\sigma_{d } \\end{bmatrix}^{-1 } \\begin{bmatrix } \\x_{\\hat d } \\\\",
    "x_{d}-\\v^t \\x_{\\hat d }   \\end{bmatrix}.\\ ] ] as a simple consequence we obtain the assertion of the lemma .",
    "now we show that @xmath15-adapted gaussian densities are an extension of the classical gaussian model .",
    "[ the : gr ] let @xmath83 be the family of all linear transformations from @xmath84 into @xmath85 .",
    "then @xmath86    to prove the assertion , we first show the following inclusion : @xmath87 let @xmath67 , @xmath88 ( where @xmath89 , @xmath40 ) , @xmath69 and @xmath90 be given and let @xmath91 .",
    "thanks to lemma  [ lem : fam ] for @xmath92 , we have @xmath93 we now show the opposite inclusion @xmath94 let @xmath95 and @xmath67 be given and let @xmath96 .",
    "we put @xmath97 , @xmath98 , @xmath99 and @xmath100 .",
    "thanks to lemma [ lem : fam ] , we have @xmath101 =   n\\left ( \\m , \\begin{bmatrix } i & 0 \\\\ \\v^{t}\\sigma_{11}^{-1 } & -1 \\end{bmatrix } \\begin{bmatrix } \\sigma_{11 } & 0 \\\\ 0 & -\\v^{t } \\sigma_{11}^{-1 } \\v + \\ssigma_{22 } \\end{bmatrix } \\begin{bmatrix } i & \\sigma_{11}^{-1}\\v \\\\ 0 & -1 \\end{bmatrix } \\right)=\\ ] ] @xmath102 consequently @xmath103 what finished the proof .",
    "the following observation is a corollary of theorem [ the : gr ] .",
    "let @xmath63 contains the family of all linear transformations from @xmath84 into @xmath85 .",
    "then @xmath94    consequently is a natural extension of the classical cec algorithm .",
    "if we consider @xmath65 containing only linear transformations , we obtain exactly the cec algorithm .",
    "on the other hand , for wider classes of functions we detect more general clusters , which describe groups concentrated around manifolds which are not necessarily linear .",
    "in this section the theoretical background of will be presented .",
    "first , we introduce the cost function which will be minimized by the algorithm .",
    "then we prove that the optimal function which describes each cluster can be obtained by least square regression @xcite .",
    "we will end by describing the full algorithm of afcec .",
    "our method is based on the cec approach . therefore , we start with a short introduction to the method ( for a more detailed explanation we refer the reader to @xcite ) . to explain cec we need to introduce the cost function which we want to minimize . in the case of splitting of @xmath104 into @xmath105 such that elements of @xmath106 we  code  by function from family of all gaussian densities @xmath61 , the mean code - length of a randomly chosen element @xmath1 equals @xmath107 where @xmath108 .",
    "the formula uses cross - entropy of a data set with respect to the family @xmath61 .",
    "the aim of cec is to find splitting of @xmath14 into sets @xmath106 which minimize the function given in ( [ en : cec ] ) .",
    "our goal is to calculate an explicit formula for the cost function in the case of @xmath15-adapted gaussian densities .      in this section",
    "we will focus on the situation of one cluster @xmath46 .",
    "in such a case we usually understand the data as a realization of a random variable . consequently , as an estimator for the mean and covariance",
    ", we use @xmath109 @xmath110    as it was said , cec uses cross - entropy of data set @xmath46 with respect to the gaussian family @xmath61 .    [",
    "the : gaus_scross ] let @xmath44 be given .",
    "then @xmath111 where @xmath112    the cec algorithm will be used for a family of @xmath15-adapted gaussian densities .",
    "in such a case the cost function is described by the following theorem .",
    "[ the : ener ] let @xmath104 and a function @xmath16 be given .",
    "then @xmath113 where @xmath114 and @xmath115    let @xmath116 , where @xmath39 , @xmath40 , @xmath51 and @xmath117 .",
    "the assertion of the proposition is a simple consequence of @xmath118 = -\\frac{1}{|x| } \\sum \\limits_{\\x \\in x } \\ln \\left ( n(\\m_{\\hat d},\\sigma_{\\hat d})(\\x_{\\hat d } ) \\cdot n(m_{d},\\ssigma_{d})(x_{d } - f(\\x_{\\hat d } ) ) \\right)=\\\\[6pt ] = -\\frac{1}{|x| } \\sum \\limits_{\\x \\in x } \\left ( \\ln \\left ( n(\\m_{\\hat d},\\sigma_{\\hat d})(\\x_{\\hat d } ) \\right ) + \\ln\\left ( n(m_{d},\\ssigma_{d})(x_{d } - f(\\x_{\\hat d } ) ) \\right ) \\right)=\\\\[6pt ] = - \\frac{1}{|x| } \\sum \\limits_{x \\in \\y } \\ln \\left ( n(\\m_{\\hat d},\\sigma_{\\hat d})(\\x_{\\hat d } ) \\right ) - \\frac{1}{|x| } \\sum \\limits_{\\x \\in x } \\ln\\left ( n(m_{d},\\ssigma_{d})(x_{d } - f(\\x_{\\hat d } ) ) \\right)= \\\\[6pt ] = h^{\\times}(x_{\\hat d}\\|n(\\m_{\\hat d},\\sigma_{\\hat d } ) ) + h^{\\times } ( { x_{d}^{f } } \\| n(m_{d } , \\ssigma_{d } ) ) .",
    "\\end{array}\\ ] ] we can use theorem  [ the : gaus_scross ] for both summands separately : @xmath119 = \\frac{d-1}{2}\\ln(2 \\pi e ) + \\frac{1}{2 } \\ln \\left ( \\det \\left(\\cov(x_{\\hat d } ) \\right ) \\right ) + \\frac{1}{2}\\ln(2 \\pi e ) + \\frac{1}{2 } \\ln \\left ( \\frac{1}{n } \\sum \\limits_{\\x \\in x } ( x_{d } - f(\\x_{\\hat d } ) - m_{d } ) ^2 \\right ) .",
    "\\end{array}\\ ] ]    as a corollary from the above theorem , we obtain that the optimal from the cross - entropy point of view function which describes a cluster can be obtained by a least squares method @xcite .",
    "let @xmath104 be a data set and a family of functions @xmath120 be given .",
    "then @xmath121 where @xmath122 .",
    "consequently , we minimize cross - entropy by finding a least squares estimation . moreover , if @xmath65 is a set of function which are invariant under the operations @xmath123 for any @xmath124 , it is enough to find @xmath125    [ col : en ] let @xmath104 be a data set , and let a family of functions @xmath126 be invariant under the operations @xmath123 for @xmath127 .",
    "let @xmath128 be such that @xmath129 .",
    "then @xmath130 where @xmath131 .",
    "the above theorem guarantees that the cost function is decreasing during iterations .",
    "the analogue of this result does not hold for acagmm ( pca is used for finding a local coordinate system ) .",
    "consequently , in ( contrary to acagmm ) we are able to construct a simple stop condition .      in the previous subsection",
    "there was shown how to determine optimal parameters for one cluster in arbitrarily given coordinate system .",
    "now we describe how to fit the optimal one for the method .    in acagmm the pca ( principal component analysis )",
    "was used for finding a locally adapted coordinate system .",
    "unfortunately , this operation causes problems with convergence ( it is hard to construct a reasonable stop condition ) .",
    "more precisely , by using pca we do not minimize a cost function which is connected with least squares estimation . by applying two methods ( pca and regression )",
    "separately we do not minimize any of them .",
    "in the case of all computation use the canonical basis .",
    "we need only to decide which coordinate is chosen as dependent ( then the rest becomes automatically explanatory ) .",
    "[ fig : ex_cor1 ]   [ fig : ex_cor2 ]     our intuition to verify all possible coordinates in the canonical basis came from the implicit function theorem @xcite . more precisely , under reasonable assumptions , for an implicit function @xmath132 where @xmath133 and an arbitrary zero @xmath134 of @xmath135 , we can find @xmath136 and @xmath137 such that locally in the neighborhood of @xmath138 @xmath139 = \\{(x_1,\\ldots , x_{k-1},f(\\x_{\\hat k}),x_{k+1},\\ldots , x_d ) : \\x_{\\hat k } \\in b(\\x_{\\hat k},r ) \\subset \\r^{d-1}\\ } \\end{array}\\ ] ] where @xmath140 is a ball with center @xmath138 and radius @xmath141 .    consequently for data @xmath44 we search for @xmath142 and @xmath15 such that @xmath46 can be optimally approximated by the set @xmath143    let us consider a c - type set , see fig . [",
    "fig : ex_cor ] .",
    "when using the canonical basis of @xmath13 , we have to consider two possible estimated curves ( in our case parabolas ) .",
    "we can treat @xmath1 as a dependent variable , see fig .",
    "[ fig : ex_cor1 ] , or we choose @xmath0 as a dependent one , see fig .",
    "[ fig : ex_cor2 ] .",
    "if we assume that dependent variable is @xmath1 , we obtain the parabola @xmath144 , and the sum of squared errors is equal to @xmath145 .",
    "on the other hand , if @xmath0 is the dependent coordinate , we have @xmath146 with squared errors @xmath147 . consequently ,",
    "the optimal coordinate system is describe by using @xmath1 as the dependent variable .    in the above example",
    ", we consider only @xmath13 but in higher dimensional spaces we have to consider @xmath37 different possible choices of dependent variable : ( @xmath148 for @xmath149 .    in conclusion , for one cluster",
    "@xmath104 we can estimate parameters of the model in two steps .",
    "first , we consider all possible choices of dependent variable : functions @xmath150 ( corresponding with relations @xmath151 ) , means @xmath152 , @xmath153 and covariances @xmath154 , @xmath155 for @xmath156",
    ". then we determine the optimal dependent variable @xmath157^t,\\sigma_{\\hat k},\\ssigma_{k},f_{k } ) \\right ) \\right\\}.\\ ] ] consequently , our data set is represented by the function , mean and covariance matrix @xmath158 , \\qquad \\sigma= \\begin{bmatrix } \\sigma_{\\hat j } & 0 \\\\ 0 & \\ssigma_{j } \\end{bmatrix}\\ ] ] where subscript @xmath159 denotes the dependent variable in cluster .    the full algorithm can now be described .",
    "we use an adapted lloyd s method which is based on the simultaneous application of two steps .",
    "first , we construct a new division of @xmath46 by matching each element @xmath160 to a group such that the cost function is minimal .",
    "then , we estimate new parameters in each cluster by applying the method presented in previous subsection , see algorithm [ alg1 ] .",
    "number of clusters @xmath161 curve family @xmath162 stop condition @xmath163 dataset @xmath46 ( @xmath37 - dimension of data ) _ obtain _ initial clustering @xmath164 _ obtain _ probabilities @xmath165 for @xmath166 _ obtain _ parameters of each cluster @xmath167 , mean @xmath168 and covariances @xmath169 , @xmath170 in each cluster ( choosing the best orientation ) _ obtain _ cost function @xmath171^t , \\sigma_{\\hat i } , \\sigma_{i } ) ) )   $ ] @xmath172 _ obtain new _ clustering @xmath173 by matching elements to the cluster such that @xmath174^t , \\sigma_{\\hat i } , \\sigma_{i } ) ) \\right ) $ ] is minimal _ delete _ unnecessary clusters ( @xmath175 ) by adding elements to the closest existing one _ update _",
    "parameter @xmath5 @xmath176 _ obtain new _",
    "probabilities @xmath177 for @xmath166 _ obtain new _ parameters of each cluster @xmath167 , mean @xmath168 and covariances @xmath169 , @xmath170 in each cluster ( choosing the best orientation ) _ obtain new _ cost function @xmath178^t , \\sigma_{\\hat i } , \\sigma_{i } ) ) )   $ ]",
    "in this section we present a comparison of the method with acagmm , gmm and cec . since acagmm is not a density model , the log - likelihood function is not well - defined . nevertheless , by the input the jacobian of acagmm transformation , we obtain a valid probability distribution , see appendix .    [",
    "fig : cir1 ]   [ fig : cir2 ]   [ fig : cir3 ]   [ fig : cir4 ]   + [ fig : cir5 ]   [ fig : cir6 ]   [ fig : cir7 ]     to compare the results we use the standard bayesian information criterion ( bic ) @xmath179 and akaike information criterion ( aic ) @xmath180 where @xmath5 is a number of parameters in the model , @xmath181 is a number of points , and @xmath182 is a maximized value of the log - likelihood function .",
    "consequently , we need a number of parameters which are used in each model . in a case of @xmath13 , acagmm uses two scalars for mean , three scalars for covariance matrix , two scalars for parabola and one for local coordinate system ( obtained by pca ) . on the other hand , in we",
    "do not need scalar for the local coordinate system .",
    "consequently , uses two scalars for mean , three scalars for covariance matrix and two scalars for parabola .",
    "let us start from a synthetic data set .",
    "first , we report the results of afcec , acagmm , cec and gmm in the case of a circle - type set , see fig .  [",
    "fig : cir ] . fig .",
    "[ fig : cir5 ] shows how the log - likelihood function changes when the number of clusters increases from @xmath183 to @xmath184 .",
    "similar relation , in respect to number of parameters , is presented in fig .",
    "[ fig : cir6 ] . for a similar values of log - likelihood function",
    ", we need 2 clusters in and acagmm and 4 in gmm and cec , see fig .",
    "[ fig : cir ] .",
    "in such a case , the bic criterion shows that algorithms which use curved densities model better fit data with using smaller number of parameters .",
    "[ fig : sp1 ]   [ fig : sp2 ]   [ fig : sp3 ]   [ fig : sp3 ]     [ fig : sp5 ]   [ fig : sp6 ]   [ fig : sp7 ]     similar situation can be observed in a more complex case of spiral - type set , see fig .",
    "[ fig : sp ] . in table",
    "[ tab : real_datasets_ll ] , the mean and maximum value of log likelihood for 100 initializations of algorithms are shown .",
    "as we see for a similar values of log - likelihood function , we have to use 9 clusters for and acagmm and 14 for gmm and cec . the comparison of algorithms by using bic and aic with similar values of log - likelihood function we present in tab .",
    "[ tab : spirala_datasets ] .",
    "algorithms which are able to adapt to curve type structures ( acagmm , ) better fit data .",
    "more precisely , the log - likelihood function takes a larger value with the same number of parameters , see fig .",
    "[ fig : cir6 ] and fig .",
    "[ fig : sp6 ] . since log - likelihood increases with growing of the number of classes , we use bic criterion which takes into account the number of parameters . in the case of acagmm and afcec",
    ", we obtain optimal value of bic after about 4 - 6 iterations . in conclusion , acagmm and better fit data ( yield a higher value of log - likelihood function ) while require lower number of parameters .",
    "algorithms acagmm and give a comparable value of log - likelihood , see fig .",
    "[ fig : cir5 ] and fig .",
    "[ fig : sp5 ]",
    ". nevertheless , uses less parameters , see fig .",
    "[ fig : cir6 ] and fig .",
    "[ fig : sp6 ] .",
    "moreover , strong theoretical background of the method guarantees that the cost function decreases in each iteration .",
    "consequently , we obtain a simple stop condition for our method .",
    ".comparison of afcec , acagmm , cec and gmm in the case of spiral - type set , see fig .",
    "[ fig : sp ] . [ cols=\"<,^,^,^,^,^ \" , ]     chinese characters mainly consist of straight - line strokes ( horizontal , vertical ) and curve strokes ( slash , backslash and many types of hooks ) .",
    "gmm has already been employed for structure analysis of chinese characters , and achieves commendable performance @xcite",
    ". however , some lines extracted by gmm may be too short and is quite difficult to join these short lines to form semantic strokes due to the ambiguity of joining .",
    "this problem becomes more serious when analyzing handwritten characters by gmm , and this was the motivation to use acagmm to represent chinese characters . in tab .",
    "[ tab : real_datasets ] , we present a comparison of afcec , acagmm , gmm and cec on chinese and latin characters :    utf8gbsn  ( dog ) ,  ( beg ) ,  ( father ) ,  ( mother ) ,  ( fire ) ,  ( master ) , b , r , s. the number of clusters has been determined so as to obtain a similar value of log - likelihood function .",
    "as it was previously mentioned , acagmm does not use densities .",
    "more precisely , the jacobian of the transformation was not taken into consideration .",
    "however , the em procedure , which was used in acagmm , works with probability distributions .",
    "therefore , from the theoretical point of view the above procedure is incorrect .",
    "moreover , if we want to compare our method by using of the log - likelihood function we need densities .",
    "let us start from numerical integration of the original acagmm function and of the model rescaled by jacobian correction .",
    "the simpson method @xcite , on the square @xmath185 \\times [ -5,5]$ ] with @xmath186 segments was used .",
    "the integral in the case of acagmm is equal to @xmath187 .",
    "after correction we obtain @xmath183 ( with a precision of @xmath188 ) .",
    "let us consider situation of the acagmm model .",
    "suppose @xmath46 and @xmath189 are zero mean independent gaussian distributions with variances @xmath190 : @xmath191 moreover , let @xmath192 where @xmath193 .",
    "let @xmath194 represent the jacobian of the original transformation @xmath195 in such a case , we have @xmath196    let us consider the function @xmath15 expressed as parametric equation @xmath197 ( in the case of acagmm it is a parabola ) . using the formula from (",
    "* table 1 . )",
    "we obtain the orthogonal projection @xmath198 of point @xmath199 on curve @xmath15 : @xmath200{r + \\sqrt{d } } + \\sqrt[3 ] { r - \\sqrt{d } } & d>0   \\\\ 0 , \\ 0 , \\ 0 & d = 0 , q = r=0 \\\\ 2\\sqrt{-q } , \\",
    "-\\sqrt{-q } , \\ -\\sqrt{-q } & d = 0 , q \\neq 0 , r \\neq 0 \\\\ 2\\sqrt{-q } \\",
    "cos(\\frac{\\phi+2i\\pi}{3 } ) , i=0,1,2 , & d<0 \\\\",
    "\\mbox { where } \\phi = acos \\left(\\frac{r}{\\sqrt{-q^3}}\\right ) &   \\end{array } \\right.\\ ] ] where @xmath201 @xmath202 and @xmath203    on the other hand , the arc length of @xmath15 between zero and @xmath198 ( * ? ? ?",
    "* formula ( 10 ) ) is given by @xmath204 consequently , we have @xmath205 @xmath206 where @xmath207 .",
    "( 1.5,1 ) ",
    "( 1.5,0 ) ",
    "( 2.5,0 ) ; plot ( , ( 1*(-1.5)^2 ) ;    ( 1.5 + 0.4,0.6 ) circle [ radius=0.02 ] ; at ( 1.5 + 0.4,0.6+.1 ) @xmath199 ; ( 1.5 + 0.6416396472194656,0.6416396472194656 ^ 2 ) circle [ radius=0.02 ] ; at ( 1.8 + 0.6416396472194656,0.6416396472194656 ^ 2 ) @xmath208 ; ( 1.5 + 0.6416396472194656,0.6416396472194656 ^ 2 )  ( 1.5 + 0.4,0.6 ) ; at ( 1.98,0.45 ) @xmath209 ; at ( 1.8,0.2 ) @xmath210 ;    ( 0,1 ) ",
    "( 1,0 ) ; plot ( , ( 0 ) ;    ( 0.7889957113463548,0.3063430560334737 ) circle [ radius=0.02 ] ; at ( ( 0.7889957113463548,0.3063430560334737+.1 ) @xmath211 ; ( 0.7889957113463548,0 ) circle [ radius=0.02 ] ; ( 0.7889957113463548,0.3063430560334737 )  ( 0.7889957113463548,0 ) ; at ( .74,0.15 ) @xmath209 ; at ( .4,0.1 ) @xmath210 ; ( 0.7,1.1)(2.1,1.1 ) ; at ( ( 1.4,1.2 ) @xmath212 ;    our goal is to determine the jacobian of our transformation , see fig .",
    "[ fig : acagmmfix ] .",
    "let us consider an arbitrary small neighborhood of @xmath198 .",
    "in such a case , the local curvature of @xmath15 at @xmath198 is the same as the curvature of the osculating circle at @xmath198 .",
    "the radius of curvature in the case of parametric form of curve is given by @xmath213 consequently , our goal is to determinate how a set is changing under the influence of the transformation , see fig . [",
    "fig : acagmmfix1 ] .",
    "a small square neighborhood of the point @xmath199 is mapped to a trapezoid ( asymptotically when a size of square converges to zero ) .",
    "this operation is showed in fig .",
    "[ fig : acagmmfix1 ] .",
    "it is easy to see that the square area changes linearly depending on the distance @xmath209 .",
    "if we consider the situation where @xmath214 , we obtain that our square is collapsed to a point . consequently , for points above the curve jacobian",
    "is asymptotically proportional to @xmath215 in a natural way , if a point @xmath199 is under the curve , the square area is increasing under the influence of the transformation .",
    "therefore , the jacobian is asymptotically equal to @xmath216    plot ( , ( 1*(-1.5)^2 ) ; ( 2,0.5 ^ 2 )  ( 1.5 - 1.05666,1.7351 ) ; ( 2.28,0.78 ^ 2 ) ",
    "( 1.5 - 1.05666,1.7351 ) ;    ( 1.5 + 0.4,0.6 ) circle [ radius=0.02 ] ; at ( 1.7,0.8 ) @xmath199 ; at ( 1.5,1 ) @xmath141 ; at ( 2,0.4 ) @xmath209 ; ( 1.5 + 0.6416396472194656,0.6416396472194656 ^ 2 ) circle [ radius=0.02 ] ; ( 1.5 + 0.6416396472194656,0.6416396472194656 ^ 2 )  ( 1.5 + 0.4,0.6 ) ; ( 1.5 + 0.6416396472194656,0.6416396472194656 ^ 2 ) ",
    "( 1.5 - 1.05666,1.7351 ) ; plot ( , ( -sqrt(2.15305 ^ 2-(-(1.5 - 1.05666))*(-(1.5 - 1.05666 ) ) ) + 1.7351 ) ; plot ( , ( -sqrt(1.945 ^ 2-(-(1.5 - 1.05666))*(-(1.5 - 1.05666 ) ) ) + 1.7351 ) ; plot ( , ( -sqrt(1.945 ^ 2-(-(1.5 - 1.05666))*(-(1.5 - 1.05666 ) ) ) + 1.7351 ) ;    plot ( , ( -sqrt(1.745 ^ 2-(-(1.5 - 1.05666))*(-(1.5 - 1.05666 ) ) ) + 1.7351 ) ; plot ( , ( -sqrt(1.745 ^ 2-(-(1.5 - 1.05666))*(-(1.5 - 1.05666 ) ) ) + 1.7351 ) ;    ( 1.7,0.524385 )  ( 1.85,0.391846 ) ; ( 1.935,0.829575 )  ( 2.1,0.716031 ) ; plot ( , ( 0 ) ; ( -0.8 - 0.5,0 ) ",
    "( 0.4 - 0.5,0 ) ; ( -0.4 - 0.5,0 ) ",
    "( -0.4 - 0.5,2.15305 ) ; ( 0 - 0.5,0 )  ( 0 - 0.5,2.15305 ) ;    ( -0.4 - 0.5,0.3063430560334737 + 0.11 )  ( 0 - 0.5,0.3063430560334737 + 0.11 ) ;",
    "( -0.4 - 0.5,0.3063430560334737 - 0.11 )  ( 0 - 0.5,0.3063430560334737 - 0.11 ) ; ( -0.4 - 0.5,0.3063430560334737 + 0.11 )  ( -0.4 - 0.5,0.3063430560334737 - 0.11 ) ; ( 0 - 0.5,0.3063430560334737 - 0.11 )  ( 0 - 0.5,0.3063430560334737 + 0.11 ) ;    ( -0.8 - 0.5,0.3063430560334737 + 0.11 )  ( 0.4 - 0.5,0.3063430560334737 + 0.11 ) ; ( -0.8 - 0.5,0.3063430560334737 - 0.11 ) ",
    "( 0.4 - 0.5,0.3063430560334737 - 0.11 ) ; at ( -0.8,0.1 ) @xmath209 ; at ( -0.8,1.2 ) @xmath141 ; at ( -0.7,0.5 ) @xmath199 ; ( 0.7889957113463548 - 1.5,0.3063430560334737 ) circle [ radius=0.02 ] ; ( 0.7889957113463548 - 1.5,0 ) circle [ radius=0.02 ] ; ( 0.7889957113463548 - 1.5,0 )  ( 0.7889957113463548 - 1.5,0.3063430560334737 ) ; ( 0.7889957113463548 - 1.5,2.15305 )  ( 0.7889957113463548 - 1.5,0.3063430560334737 ) ;    ( 0,9 )  ( 0,0 )  ( 5,0 ) ; ( 0,-1 )  ( 0,0 )  ( -5,0 ) ; plot ( , ( ) ^2 ) ;    ( 4,4 ) circle [ radius=0.1 ] ; at ( 4,4.5 ) @xmath217 ;    ( -1.459261,2.129444 ) circle [ radius=0.1 ] ; ( -1.459261,2.129444 )  ( 4,4 ) ;    ( -0.6498321,0.4222817 ) circle [ radius=0.1 ] ; ( -0.6498321,0.4222817) ( 4,4 ) ;    ( 2.109093,4.448275 ) circle [ radius=0.1 ] ; ( 2.109093,4.448275) ( 4,4 ) ;    ( 0,6 ) circle [ radius=0.1 ] ; at ( 0,6.5 ) @xmath218 ;    ( 2.345208,5.5 ) circle [ radius=0.1 ] ; ( 2.345208,5.5 )  ( 0,6 ) ;    ( -2.345208,5.5 ) circle [ radius=0.1 ] ; ( -2.345208,5.5 )  ( 0,6 ) ;    ( 0,0 ) circle [ radius=0.1 ] ; ( 0,0 ) ",
    "( 0,6 ) ;    ( -4,1 ) circle [ radius=0.1 ] ; at ( -5,1.5 ) @xmath219 ;    ( -1.391769,1.93702 ) circle [ radius=0.1 ] ; ( -1.391769,1.93702 ) ",
    "( -4,1 ) ;    now we have the formula for the jacobian of acagmm transformation , but it depends on the relation between a point and its orthogonal projection . more precisely , we have to verify which formula should be used ( or equivalently on which side of parabola a point is found ) , see fig . [",
    "fig : acagmmfix2 ] .",
    "we can easily verify where the point @xmath199 is in relation to the orthogonal projection @xmath198 by checking the orientation of a basis containing the normal vector @xmath220 and the tangent vector @xmath221 at a point @xmath198 .",
    "consequently , we have to verify the sign of the determinant @xmath222",
    "the study is cofounded by the european union from resources of the european social fund",
    ". project po kl `` information technologies : research and their interdisciplinary applications '' , agreement uda - pokl.04.01.01 - 00 - 051/10 - 00 .    45 natexlab#1#1[2]#2 , , , .",
    ", , , , . , , , , ( ) .",
    ", , , volume , , .",
    ", , , , ( ) . , , , in : , volume  , , pp . .",
    ", , , , , ( ) . , , , ( ) .",
    ", , in : , volume  , , pp . .",
    ", , , ( ) .",
    ", , , , ( ) . , , , , , in : , , p. .",
    ", , , ( ) . , , , , ( ) .",
    ", , , in : , volume  , , pp . .",
    ", , , , , ( ) . , , , , , , , ( ) .",
    ", , , in : , volume  , .",
    ", , , in : , , pp . . , , , , in : , volume  , , pp . .",
    ", , , ( ) . , , ph.d .",
    "thesis , citeseer , .",
    ", , , ( ) .",
    ", , , ( ) . , , , , ( ) .",
    ", , , , ( ) . , , , ( ) . , , , ( ) .",
    ", , , ( ) . , , , volume , , . , , , , ( ) .",
    ", , , , ( ) . , , , ( ) .",
    ", , , ( ) . , , , , , .",
    ", , , , ( ) . , , , .",
    ", , , , volume  , , ."
  ],
  "abstract_text": [
    "<S> gaussian mixture models ( gmm ) have found many applications in density estimation and data clustering . </S>",
    "<S> however , the model does not adapt well to curved and strongly nonlinear data . </S>",
    "<S> recently there appeared an improvement called acagmm ( active curve axis gaussian mixture model ) , which fits gaussians along curves using an em - like ( expectation maximization ) approach .    using the ideas standing behind acagmm </S>",
    "<S> , we build an alternative active function model of clustering , which has some advantages over acagmm . </S>",
    "<S> in particular it is naturally defined in arbitrary dimensions and enables an easy adaptation to clustering of complicated datasets along the predefined family of functions . </S>",
    "<S> moreover , it does not need external methods to determine the number of clusters as it automatically reduces the number of groups on - line .    </S>",
    "<S> clustering , gaussian mixture models , expectation maximization , cross - entropy clustering , active curve axis gaussian mixture model . </S>"
  ]
}