{
  "article_text": [
    "advances and accessibility to technology for capturing and storing large amounts of data has transformed many scientific fields . yet , for many important scientific questions very large data sets are not available . in these cases , when learning algorithms are used , resampling techniques allow the estimation of the generalization error , an important aspect of algorithmic performance .    the generalization error is defined as the error an algorithm makes on cases that the algorithm has never seen before . the generalization error is important because it relates to the algorithm s prediction capabilities on independent data .",
    "the literature includes both , theoretical investigations of risk performance of machine learning algorithms as well as numerical comparisons .",
    "estimation of the generalization error can be achieved via the use of resampling techniques .",
    "the process consists of splitting the available data into a learning or training set and a test set a large number of times and averaging over these repetitions .",
    "a very popular resampling technique is cross validation .",
    "a detailed overview of the vast literature on cross - validation is outside the scope of this paper .",
    "the interested reader is referred to @xcite for foundational aspects of cross validation , ( * ? ? ?",
    "* ch.s  3,8 ) , @xcite , and to @xcite for a comprehensive survey .",
    "here , it is sufficient to notice that although cross validation procedures are extensively used in practice , their performance is studied by very few theoretical papers .",
    "an important question in constructing cross validation estimators of the generalization error is the selection of the size of the training ( and hence test ) set for `` optimal '' estimation of the generalization error . in this paper",
    ", we address the question of training sample size selection and refer to work by @xcite in identifying the rules for `` optimal '' selection , where `` optimality '' is defined via minimization of the variance of the cross validation estimators .    estimating the generalization error of a learning method is important not only for understanding its future prediction accuracy , but also for choosing a classifier from a given set , or for combining classifiers @xcite .",
    "furthermore , one may be interested in testing hypotheses about equality of generalization errors between learning algorithms . in @xcite , in the field of biostatistics , an inference framework is developed for the difference in errors between two prediction procedures . in that paper , for each splitting in repeated sub - sampling the predictions of the two classifiers are compared by a wilcoxon test , and the resulting @xmath2-values are combined .",
    "@xcite state that `` @xmath3 little attention is paid to inference for comparing prediction accuracies of two models or , more generally , prediction procedures '' . carrying out inference on equality of generalization errors requires insight into the variance of the estimator of the generalization error . but providing a variance of the generalization error estimator is a more difficult problem because of the complexity of existing estimators and the many dependencies they exhibit on , for example the loss function , the resampling method used , the ratio of the training to test sample size and other factors . @xcite were the first to provide estimators for the variance of the random cross validation estimator of the generalization error .",
    "furthermore , @xcite proposed a moment approximation - based estimator of the same cross validation estimator of the generalization error , and compared this estimator with those provided by @xcite . other relevant work on variance estimation includes @xcite .    selecting the size of the training set and understanding the effect of this selection on the generalization error , its bias and its variance , is of interest to many areas of scientific investigation .",
    "examples include pattern recognition and machine learning @xcite , statistics @xcite , remote sensing @xcite , biostatistics and bioinformatics @xcite among others . in this paper , we offer an analysis of the problem of training sample size selection when the total sample size is fixed and interest centers on carrying out inference on the generalization error .",
    "the analysis is complex , and for this reason we consider three kinds of problems to cover a good range of possible applications .",
    "these include the case where the decision rule is the sample mean , the regression case and classification via logistic regression .",
    "the strategy we follow allows us to draw general conclusions about `` optimal '' selection of the training ( and hance test ) set sample size , and to recommend simple rules for practical use despite the complexity of the problem .",
    "we analyze training sample size selection for cross validation estimators of the generalization error .",
    "specifically , we study random cross validation estimators and @xmath1-fold estimators based on general loss functions .",
    "we first discuss two novel approaches for obtaining the resampling size @xmath4 of the random cross validation estimator of the generalization error @xcite .",
    "* , sect .",
    "5 ) state `` we also want to make recommendations on the value of @xmath4 to use for those methods that involve @xmath5 '' .",
    "we offer methods for obtaining @xmath4 that are based on the @xmath6 .",
    "furthermore , we establish rules for the `` optimal '' selection of the training set sample size for cross validation estimators of the generalization error .",
    "we show that when the decision rule is the sample average and the loss function is sufficiently smooth and belongs in @xcite s @xmath7-class ( @xcite ) , the optimal value of the training sample size is @xmath8 , independent of the data distribution , where @xmath9 indicates the upper integer part of a real number .",
    "when the decision rule is regression and the loss function is squared error loss , or the task is classification via logistic regression , the optimal value of the training sample size is again @xmath8 , independent of the data distribution . in general",
    ", the optimal value of the training sample size depends primarily on the loss function and secondly on the data distribution .",
    "we offer a construction of a new loss function that exemplifies this dependence .",
    "additionally , we provide a rule for selecting the `` optimal '' number of folds @xmath1 , in a @xmath1-fold cross validation experiment .",
    "we follow with a discussion of the merits and limitations of different forms of cross validation , and offer recommendations for use in practice .",
    "this paper is organized as follows .",
    "section [ sec : relevant ] presents relevant previous work and section [ sec : j ] discusses two approaches to obtain the resampling size @xmath4 .",
    "section [ sec : training / test ] presents the analysis for optimally choosing the training sample size , while section [ sec : simu ] presents simulation results .",
    "section [ sec : real ] applies the methods to real data sets , while section [ sec : conclusions ] offers discussions and recommendations .",
    "finally , proofs of the obtained results are presented in the appendix , and also in the online supplement .",
    "in this section we review briefly related work and set up the notation we will use in this paper .",
    "interest in the problem of `` optimal '' partitioning of a fixed sample @xmath0 in two sets , training and testing , dates back to 1962 .",
    "@xcite studied this problem in the context of designing a pattern recognition machine and defined the `` optimum '' partitioning of the total sample as that partitioning which minimizes the variance of an unbiased estimator of the error probability of the pattern recognition machine .",
    "@xcite states that the same rule , that is the minimization of variance of the error of a classification rule , can be used with estimators of the error that can be corrected for bias .",
    "similar work has been carried out by @xcite .",
    "these authors address the problem of splitting a fixed sample @xmath0 into a training , a testing and a validation set using as a criterion the mean squared error .",
    "only the squared error loss is studied in the simple case of location parameter estimation .",
    "the cross validation procedures they study are hold - out cross validation , @xmath1-fold and randomized permutation cross validation .",
    "the last cross validation procedure is defined by resampling the test set @xmath4 times , @xmath10 , @xmath0 is the total sample size and @xmath11 is the test set sample size .",
    "the same problem appears also in the biostatistics literature and in particular in the area of bioinformatics .",
    "@xcite consider the problem of developing genomic classifiers , empirically address the question of what proportion of the samples should be devoted to the training set , and study the impact that this proportion has on the mean squared error of the prediction accuracy estimate .",
    "the context within which @xcite address this question is the one where the number of predictors @xmath2 is considerably larger than the number of samples @xmath0 , i.e.  it is the high - dimensional , small sample size case .",
    "@xcite evaluated two approaches to data split .",
    "the first consists of using results of simulations that were designed to understand qualitatively the relationships among data set characteristics and optimal split proportions to evaluate commonly used rules of thumb , such as allocating @xmath12 or @xmath13 of the data set to training and the remaining to the test sets .",
    "the second approach consists of developing a nonparametric procedure that is based on decomposing the mean squared error into three components , two corresponding to appropriately defined variance terms and one corresponding to a squared bias term .",
    "there are fundamental differences between the work of these authors and our work .",
    "first , their setting of small @xmath0 , large @xmath2 is different than ours and secondly , even in this setting , they do not address the case of cross validation estimators .    in this paper",
    ", we do not address the high - dimensional , small sample size case .",
    "we address the allocation of cases to training / test set when the total sample size @xmath0 is fixed and known , reflecting a limited or medium size data with @xmath2 considerably smaller than @xmath0 . in this setting , we study the rules for optimal split of the data for the random cross validation and the @xmath1-fold cv estimators of the generalization error , which we describe in detail below . but before we do this , we briefly describe two very relevant to our work papers that we use in setting the rules for selecting `` optimally '' the size of training , and hence test , set .    in a series of impressive papers",
    "@xcite studied the performance of @xmath1-fold cross validation and of the repeated learning - testing method , which is another name to describe random cross validation . in what follows , we will succinctly describe burman s results .",
    "reference to the @xmath1-fold cross validation ( which burman calls @xmath14-fold cv ) the following , relevant to our paper , points are made .",
    "1 ) in the case of @xmath1-fold cv with a small number of folds @xmath1 , the uncorrected cv estimate may have large bias ; thus , a corrected estimate is proposed , and it is denoted by @xmath15 ; 2 ) the bias of the corrected @xmath1-fold cv estimator of the generalization error is _ always smaller _ than the bias of the uncorrected estimator ; 3 ) however , @xmath16 when @xmath17 and @xmath18 ; here , @xmath19 is defined as @xmath20 , @xmath21 is a loss function , @xmath22 is the empirical cumulative distribution function of the sample @xmath23 , and @xmath24 represents the underlying true and unknown distribution ; 4 ) the bias and variance of the @xmath1-fold cv estimator of the generalization error decrease as the number of folds increases .",
    "the last two results indicate that for the case of @xmath1-fold cv estimator of the generalization error it is sufficient to study only the variance of the uncorrected estimator as opposed to the mean squared error of the uncorrected estimator .",
    "this is because the variance of the corrected , almost unbiased estimator , is approximately the same with the variance of the uncorrected estimator .",
    "this observation greatly simplifies the analysis of obtaining `` optimal '' splits of the sample .    for the case of repeated learning - testing method , or in our terminology random cross validation , the following points are relevant to the work presented here .",
    "1 ) a bias correction term is needed to obtain an almost unbiased estimate of the generalization error .",
    "this term is provided in @xcite ; 2 ) the variances of the corrected and uncorrected estimators decrease as the number of repeats increases .",
    "@xcite presents a simulation study where the two variances are approximately equal when the number of repetitions @xmath25 and the total sample size is @xmath18 for all values of the proportion of samples allocated to the test set ; 3 ) the bias does not depend on the number @xmath4 of repeats ; 4 ) the only way to reduce variance is by increasing the number of repeats @xmath4 .",
    "we provide rules that allow `` optimal '' selection of the number of repeats in the random cross validation case .",
    "the relevance of these results for our work is that again we can use the variance of the uncorrected random cv estimate of the generalization error , instead of the more complicated mean squared error , to set `` optimal '' rules for data splitting into training and test sets .",
    "this is because the variance of the bias - corrected cross validation estimator is approximately the same with the variance of the uncorrected cross validation estimator .",
    "next we set the notation we use in this paper and explicitly discuss the two estimators of the generalization error we work with .",
    "fix a positive integer @xmath0 and consider the set @xmath26 .",
    "let @xmath27 , @xmath28 be data collected such that the data universe , @xmath29 , is a set of independent , identically distributed observations that follow an unknown distribution @xmath24 .",
    "let @xmath30 be a subset of size @xmath31 , @xmath32 , taken from @xmath33 , @xmath34 , the complement of @xmath30 with respect to @xmath33 .",
    "the subset of observations @xmath35 is called a training set , used for constructing a learning rule .",
    "the test set contains all data that do not belong in @xmath36 ; it is defined as @xmath37 , the complement of @xmath36 with respect to the data universe @xmath38 .",
    "let @xmath11 denote the number of elements in the test set , @xmath39 , @xmath40 .",
    "now let @xmath41 be a function , assume that @xmath42 is a target variable , and @xmath43 is a decision rule .",
    "the function @xmath44 that measures the error between the target variable and the decision rule is called a loss function .",
    "examples of loss functions widely used in the literature , and in the present paper , include the squared error loss function , absolute error loss , and 0/1 loss function used in classification .",
    "the _ generalization error _ of an algorithm is defined as @xmath45,\\ ] ] where @xmath46 is an independent copy of the data @xmath27 and the expectation is taken over everything that is random .",
    "that is , we take into account the variability in both , training and test set .",
    "furthermore , let @xmath47 , @xmath48 , be random sets such that @xmath49 where @xmath50 ; @xmath47 , @xmath48 , are _ uniformly distributed _ on @xmath51 , such that each @xmath47 is independently sampled , with corresponding complement set ( with respect @xmath33 ) @xmath52 .",
    "the number @xmath4 is called the _ resampling size_. if @xmath53 is defined similarly as above for all @xmath54 , the usual _ average test set error _ is @xmath55 and is a function of both the training set @xmath47 and the test set @xmath52 .",
    "@xcite defined the _",
    "random cross validation estimator of the generalization error _ as @xmath56 by a straightforward calculation , it follows that @xmath57    in the cases where the prediction rule takes real values , @xcite presented a wide class of loss functions , called @xmath7-class . specifically , if @xmath7 is an absolutely continuous and concave real function , the corresponding @xmath7-class loss function is @xmath58 where @xmath59 is the almost sure derivative of the _ generator _ @xmath7 .",
    "commonly used loss functions , such as squared error loss and 0/1 loss belong to this class . in section [ sec",
    ": training / test ] we use the @xmath7-class of loss functions to elucidate the training sample size selection rules .",
    "we now provide guidance for the selection of the resampling size @xmath4 entering the construction of the random cross validation estimator of the generalization error . in subsection [ ssec : res - eff ]",
    "we discuss two novel methods for selecting the resampling size @xmath4 , the @xmath61-effectiveness and the @xmath62-reduction methods . here , we note that the @xmath61-effectiveness and @xmath62-reduction , and hence the selection of @xmath4 , are affected by the training set sample size .",
    "we illustrate these relationships in section [ sec : simu ] .",
    "we first improve upon the following result given in @xcite . under the assumption that `` the distribution of @xmath63 does not depend on the particular realization of @xmath47 , @xmath64 '' @xcite",
    "prove that the quantities @xmath65 and @xmath66 in do not depend on the indices @xmath54 and @xmath67 ( see , also , * ? ? ?",
    "* ) . in what follows",
    "we first prove that the aforementioned statement is generally true , that is this assumption is not needed for the result to hold .",
    "this is shown in proposition [ prop.v , c ] of this section .",
    "[ prop.v , c ] let @xmath31 and @xmath11 be fixed , and let @xmath21 be a loss function such that @xmath68 $ ] is finite for each realization of @xmath47 and @xmath64 , where @xmath47 follow a uniform distribution ( described in detail in the proof of the proposition ) .",
    "then , the quantities @xmath65 and @xmath66 are finite and do not depend on the indices @xmath54 and @xmath67 , for both random and @xmath1-fold cross validation .",
    "first , @xmath68<\\infty$ ] gives @xmath69<\\infty$ ] , and an application of cauchy  schwarz inequality gives that @xmath70 exists for each realization of @xmath47 , @xmath64 and @xmath71 .",
    "thus , by definition of @xmath72 , @xmath73 . again by cauchy ",
    "schwarz inequality we obtain that @xmath74 exists .    in the case of random cv",
    ", the random vector @xmath75 is uniformly distributed on @xmath76 with @xmath77 . using the total probability theorem",
    ", the cumulative distribution function of @xmath78 is @xmath79 from the form of @xmath24 we obtain @xmath80 ; and their distribution does not depend on the indices @xmath54 and @xmath67 .    in the @xmath1-fold cv case set @xmath81 with @xmath82 ^ 2(n-2n / k)!\\big\\}$ ] , and observe that the random vector @xmath75 is uniformly distributed on @xmath83 .",
    "using the same arguments as above the proof is completed .",
    "@xmath84    proposition [ prop.v , c ] allows one to write @xmath85 , @xmath86 , effectively obtaining @xmath87 that is , the variance of the random cross validation estimator of the generalization error is a function of the variance of the average test set error and the covariance between two different average test set errors , which are constants with respect to @xmath54 , @xmath67 .",
    "figure [ fig.j , var(mu_j ) ] shows the behavior of @xmath6 as a function of @xmath4 . by cauchy - schwarz inequality",
    "we obtain that @xmath88 , where the equality characterizes the trivial cases in which @xmath21 is a constant with probability 1 , e.g. , if @xmath24 is degenerate .",
    "so ( see figure [ fig.j , var(mu_j ) ] ) , @xmath89 and again the equality characterizes the same trivial cases .    from and",
    "we obtain the following important result .",
    "[ theo.var_bound ] the variance of @xmath5 given by satisfies the following double inequality : @xmath90 where @xmath91 and @xmath92 .    using cauchy ",
    "schwarz inequality and we get @xmath93 , where @xmath94 . also , gives @xmath95 .",
    "finally , for each @xmath4 fixed , the function @xmath96 , @xmath93 , is increasing , and in view of the proof is completed .",
    "@xmath84    the importance of the theorem will become obvious when we define appropriate rules for selecting the sample size of the training set . since the variance of the cv estimator of the generalization error is bounded above by the variance of the test error @xmath97 , we can use @xmath97 to create an `` optimal '' rule for selecting the training ( and hence the test ) sample size .",
    "we discuss two novel methods for selecting the resampling size @xmath4 .",
    "the quantities @xmath97 and @xmath98 in depend on the sizes of the training and test set , @xmath31 and @xmath11 , as well as on @xmath24 and @xmath21 . recall that an experimentalist selects that value of @xmath4 for which there is no appreciable reduction in the variance of cv estimator of the generalization error .",
    "taking into account the behavior of @xmath6 we give the following definition .",
    "[ defi.re,rr ] we define :    \\(a ) the _ resampling effectiveness _ of @xmath5 by @xmath99 .",
    "\\(b ) the _ reduction ratio _ of @xmath5 by @xmath100 , where @xmath101 denote the backward difference with respect to @xmath4 .    notice that while @xmath102 provides a comparison of the variance of @xmath5 for a given @xmath4 with the limiting variance @xmath98 , the @xmath103 provides a comparison between the two variances for given @xmath4 and @xmath104 . in this sense @xmath103 is a local measure of change in variance that is used to obtain @xmath4 .",
    "this is what experimentalists are observing in order to set the value of @xmath4 .",
    ".48   for @xmath105 and @xmath106 ; ( b ) the reduction ratio of @xmath5 for @xmath105 and @xmath107.,title=\"fig : \" ]    .48   for @xmath105 and @xmath106 ; ( b ) the reduction ratio of @xmath5 for @xmath105 and @xmath107.,title=\"fig : \" ]    now we are in a position to give a minimum acceptable resampling size , either via the resampling effectiveness of @xmath5 or via its reduction ratio .",
    "observe that in non - trivial cases the resampling effectiveness of @xmath5 takes a value in the interval @xmath108 ; for the trivial cases this is assumed to be 1 . from",
    "it follows immediately that @xmath109 where @xmath94 .",
    "figure [ sfig.reseff ] shows the behavior of the resampling effectiveness of @xmath5 for various @xmath110-values .",
    "notice that when the correlation is low a larger resampling size is required in order to achieve high resampling effectiveness . as the correlation value increases the resampling size decreases ; for example , when @xmath111 , @xmath112 is sufficient to obtain a resampling effectiveness of 90% , while when @xmath113 , @xmath114 obtains the same resampling effectiveness .",
    "if @xmath61 is the desired resampling effectiveness rate , then @xmath115 , and the minimum value of the resampling number @xmath4 is @xmath116 where @xmath117 denotes the upper integer part of @xmath118 .",
    "we call the number @xmath119 as the _ @xmath61-effectiveness minimum resampling size_.    note that , we define the resampling effectiveness ( and thus select that value of @xmath4 ) as a number indicating percentage of the ratio @xmath120 , which is between 0 and 1 .",
    "the bigger the ratio @xmath121 , the closer the @xmath6 is to the asymptotic value @xmath98 of the variance of the cv estimator of the generalization error .",
    "that indicates that the value of @xmath61 should be close to 1 , i.e.  .8 , .9 , .95 etc .",
    "an alternative way to obtain @xmath4 is as follows . from",
    "we obtain @xmath122 we then fix the desired reduction ratio to a value @xmath123 and require that the reduction ratio of @xmath5 should not exceed this value , that is , @xmath124 .",
    "the minimum positive integer satisfying the preceding inequality is given by @xmath125 which we call the _",
    "@xmath62-reduction ratio minimum resampling size_. we define the reduction ratio @xmath126 ( and thus select that value of @xmath4 ) as a number indicating the relative reduction of the variance of the cv estimator , which is a positive number .",
    "the smaller this ratio , the closer @xmath6 is to the asymptotic value @xmath98 of the variance of the cv estimator of the generalization error .",
    "that indicates that the value of @xmath62 should be close to 0 , i.e.  .1 , .05 , .025 , .01 etc .",
    "figure [ sfig.redratio ] shows the reduction ratio curves as a function of the correlation @xmath127 .",
    "again , the smaller the value of the correlation between @xmath72 , @xmath128 , the larger the value of @xmath4 to obtain a desired variance reduction ratio .",
    "table [ table.jre,jrr ] contains the specific values of @xmath119 and @xmath129 for various values of @xmath61 , @xmath62 and @xmath127 .",
    "notice that large values of @xmath4 are required when the correlation is small and the desired resampling effectiveness and reduction ratio are respectively large or small .",
    "furthermore , the smaller the reduction ratio the larger the value of @xmath4 , for any given correlation @xmath127 . that is , when @xmath113 and the desired @xmath130 , one obtains @xmath131 .",
    "this selection corresponds to resampling effectiveness of approximately @xmath132 .",
    "[ rem.est-v,c ] equations , depend on @xmath127 that is generally unknown . this parameter needs to be estimated from the data .",
    "@xcite propose an approximation of the correlation coefficient given by @xmath133 , where @xmath11 is the cardinality of the test set .",
    "this estimator , however , may overestimate or underestimate the true correlation .",
    "@xcite suggested moment approximation estimators of @xmath97 and @xmath98 in certain cases that may be used to estimate the correlation @xmath127 , and hence obtain the values @xmath119 and @xmath129 .",
    "additionally , if the loss function belongs in efron s @xmath7-class of loss functions , then the parameters can be easily estimated .",
    "see sub - subsection [ sssec : q - class ] for details .",
    "a point that relates to the design of the random cross validation estimator of the generalization error is the selection of the training set .",
    "practitioners select the training sample size @xmath31 to be 5 or even 10 times larger than the test size @xmath11 .",
    "our results indicate that when `` optimality '' is quantified via minimization of the variance of the average test set error , the training sample size , for fixed value of @xmath0 , equals the test set sample size .",
    "in what follows , we present and justify our choice of the optimality rule and apply it in subsections [ ssec : mean ] and [ ssec : regression ] .",
    "relation leads to the following two potential rules .",
    "the first rule , suggesting minimization of the limiting variance @xmath98 , is not useful because , as we will see in section [ sec : simu ] , @xmath98 is a constant .",
    "the second rule suggests that minimization of the @xmath6 is equivalent to minimizing the variance @xmath97 of @xmath72 ( this is equivalent to minimizing @xmath134 ) . in light of theorem [ theo.var_bound ]",
    ", the variance of @xmath5 is bounded above by @xmath97 and places a small upper bound on the limiting variance @xmath98 .",
    "these observations lead us to select quantifying `` optimality '' via this rule . therefore , for fixed @xmath0",
    ", we define the _ optimal value of the training set size _ by @xmath135    the situation is different for the @xmath1-fold cv . because in @xmath1-fold cv there is dependence between @xmath1 , @xmath31 and @xmath11 , for fixed @xmath0 , the aforementioned optimization rules are not useful .",
    "we propose selecting the optimal value of @xmath1 as @xmath136    to be able to work with the optimization rules given in , we need either the exact or an approximate form , of the variances involved . in what follows",
    "we consider three kinds of problems to cover a good range of possible applications .",
    "these correspond to using the sample mean or linear regression ( with and without normality of the errors ) as decision rules and classification via logistic regression .",
    "we begin by studying the simple case where the decision rule is the sample mean .",
    "the variance formulas we use in our rules , and the conditions under which they are obtained , are given in @xcite . here , for reasons of completeness",
    ", we briefly summarize the results we use .",
    "let @xmath137 and @xmath138 .",
    "3.1 ) gave the following moment approximations of @xmath97 , @xmath98 @xmath139+o(\\sfrac{1}{n_1 ^ 2}),\\ ] ] where @xmath140 , \\qquad\\qquad ~\\beta\\doteq{\\operatorname{\\mathsf{var}}}[l_\\mu(x)],\\\\ \\gamma&\\doteq\\sigma^2{\\operatorname{\\mathsf{var}}}[l'_\\mu(x ) ] , \\qquad\\qquad \\delta\\doteq\\sigma^2{\\operatorname{\\mathsf{cov}}}[l_\\mu(x),l''_\\mu(x ) ] , \\end{split}\\ ] ] with @xmath141 , @xmath142 .    note that , since @xmath143 , the notation @xmath144 has the same meaning with the notation @xmath145 , which will be used in the sequel .",
    "in random cv each of @xmath42 and @xmath146 follows a hypergeometric distribution , see ( * ? ? ?",
    "* lemmas 3.1 , 3.2 ) .",
    "specifically , @xmath147 and @xmath148 ; thus , the approximated covariance , @xmath98 , in this case is @xmath149 in @xmath1-fold case @xmath42 and @xmath146 are constants ; specifically , @xmath150      to solve the optimization problem we consider the expression for the variance of @xmath72 given by .",
    "when @xmath0 is sufficiently large , the term @xmath145 is a negligible quantity , and using the fact that @xmath39 , and the identity @xmath151=1/n_1 + 1/(n - n_1)$ ] , becomes @xmath152 furthermore , by definition @xmath153 ; hence , as @xmath0 becomes large the parameter @xmath154 takes positive values . under this observation , using the approximation formula the solution to the optimization problem in is given by theorem [ theo.opt-n_1 ] .",
    "[ theo.opt-n_1 ] the solution of the optimization problem when @xmath97 is given by is @xmath155 where @xmath156 , @xmath154 are defined by and , and @xmath157 stands for the nearest integer of @xmath118 .    obviously , @xmath158 , where @xmath159 , @xmath160 .",
    "if @xmath161 the desired result becomes trivial .",
    "we will study the case @xmath162 .",
    "the derivative of @xmath163 is @xmath164 , and it has the same sign with the numerator @xmath165[(b^{1/2}-a^{1/2})t+a^{1/2}n].\\ ] ] if @xmath166 , @xmath167 is a linear polynomial with root @xmath168 , and takes negative values before @xmath169 and positive values after this . if @xmath170 , @xmath167 is a quadratic polynomial with two distinct roots @xmath171 and @xmath172 . when @xmath173 then @xmath174 , and when @xmath175 then @xmath176 ; for both cases we see that @xmath177 for all @xmath178 and @xmath179 for all @xmath180 . by definition of @xmath181 and the monotonicity of @xmath163 the proof is completed .",
    "@xmath84    in practice , the numbers @xmath156 and @xmath154 ( namely , the parameters @xmath182 , @xmath183 , @xmath184 and @xmath185 ) are unknown and must be estimated .",
    "but there are cases where the estimation of these parameters is unnecessary ; i.e. , when for the theoretical values @xmath156 and @xmath154 we know that @xmath186 or @xmath175 , independently of the data distribution ( e.g. , see the following example ) .    [ exm.sq-n_1opt ] assume that the data are from a population with finite eighth moment and the loss function is squared error , that is @xmath187 , @xmath188",
    ". then , @xmath189 with derivatives @xmath190 and @xmath191 .",
    "we compute @xmath192 , @xmath193 , @xmath194 and @xmath195 , where @xmath196 and @xmath197 . therefore , @xmath198 ; and so , @xmath199 , independently of the distribution of the data .",
    "using , gives @xmath200 and @xmath201 .",
    "also , from and with @xmath202 , we obtain @xmath203 . from the preceding relations , after some algebra , we obtain @xmath204    [ prop.opt-k ] the solution to optimization problem where the approximation variance of @xmath205 is given by is @xmath206 where @xmath184 and @xmath185 are defined by , and @xmath207 is the minimum divisor of @xmath0 that is greater than @xmath208 .",
    "since the function @xmath209 decreases in @xmath1 , the result is obtained in view of relations and .",
    "@xmath84    [ rem.opt-k ] proposition [ prop.opt-k ] states that if @xmath210 then , the optimal value of @xmath1 in terms of minimization of variance of @xmath205 is given by the leave - one - out cv ( loocv ) .",
    "however , one can replace loocv by @xmath1-fold cv , for the large values of @xmath0 because the quantity @xmath211 and the ratio @xmath209 satisfies the inequality @xmath212 therefore , for relatively small values of @xmath1 the ratio @xmath213 takes values close to @xmath208 .",
    "for example , if @xmath214 the ratio @xmath213 is between @xmath215 and @xmath216 , and selecting @xmath1 between @xmath217 and @xmath218 , i.e.  @xmath219 , guarantees that the variance of the generalization error of @xmath205 is close to the minimum variance .    in practice ,",
    "the parameters @xmath184 and @xmath185 are unknown and , usually , must be estimated .",
    "but , sometimes this estimation is unnecessary ; for example , consider the squared error loss , then @xmath220 ( see example [ exm.sq-n_1opt ] ) , and thus the loocv is proposed , independently of the distribution of the data .",
    "example [ exm.sq.k-fold ] illustrates the comments in remark [ rem.opt-k ] for the cases in which the loocv is proposed by proposition [ prop.opt-k ] .",
    "[ exm.sq.k-fold ] let the data be from @xmath221 distribution and the squared error loss is used . then , @xmath192 , @xmath222 , @xmath223 and @xmath195 ; and thus , @xmath224 . defining the relative efficiency of @xmath205",
    "as the ratio of the variance of the estimator for @xmath1 folds over its variance at @xmath225 folds , we have that the relative efficiency is given by @xmath226}{2/n+4/[(n-1)n]}$ ] .",
    "table [ table.re.sq.norm.k-fold ] shows the specific values of this relative efficiency for various values of @xmath0 and @xmath1 , indicating that the relative efficiency is a function of both , sample size and number of folds .",
    "it approaches 1 when @xmath227 and @xmath228 .",
    "the results of sections [ sssec : mean.usual ] and [ sssec : mean.k - fold ] apply to the @xmath7-class of loss functions .",
    "we have @xmath229 where @xmath7 is differentiable having at least up to five derivatives , and such that the fourth derivative of @xmath230 is bounded .",
    "the main gain is that the quantities @xmath182 , @xmath183 , @xmath184 and @xmath185 ( therefore @xmath156 and @xmath154 too ) can be computed easily .",
    "observe that @xmath231 hence , @xmath232",
    "^ 2\\sigma^2+{\\operatorname{\\mathsf{var}}}[q(x)]-2q'(\\mu){\\operatorname{\\mathsf{cov}}}[x , q(x)],\\\\ \\gamma=[q''(\\mu)]^2\\sigma^4 , & \\delta = q'(\\mu)q'''(\\mu)\\sigma^4-q'''(\\mu){\\operatorname{\\mathsf{cov}}}[x , q(x)]\\sigma^2 . \\end{array}\\ ] ]    [ rem.est-a,b,c,d-q ] the function @xmath7 is a known function , but the parameters are unknown ( unless the distribution @xmath24 is known ) .",
    "we estimate these parameters from the sample values @xmath233 by replacing the parameters @xmath234 , @xmath235 , @xmath236 $ ] and @xmath237 $ ] by @xmath238 , @xmath239 , @xmath240 and @xmath241 $ ] respectively , where @xmath242 .",
    "[ rem.copt_n1opt=n/2 ] because @xmath192 and @xmath153 for each loss function in the @xmath7-class , theorem [ theo.opt-n_1 ] guarantees that @xmath199 , for all data distributions and all loss functions that belong in the @xmath7-class .",
    "further , @xmath243 and @xmath244 .",
    "hence , the optimal value of the correlation coefficient is @xmath245 .",
    "since @xmath98 is unaffected by the splitting , for training sample size @xmath31 we have @xmath246 . therefore , the choice @xmath247 leads to the optimized values of resampling effectiveness and reduction ratio of @xmath5 , see figure [ fig.j - re , j - rr ] .",
    "in the regression case we solve the problem of optimal training sample size identification under squared error loss .",
    "furthermore , we offer a solution under both , normality of the errors and under relaxation of the normality assumption . in the second case",
    "we require @xmath248 for some @xmath249 .",
    "the results presented below are different from the results presented in @xcite in following aspects : 1 ) the analysis presented in @xcite is conditional on the training sample @xmath47 ; here , we use the distribution of @xmath47 in light of proposition [ prop.v , c ] ; 2 ) we study the case of @xmath1-fold cv ; and 3 ) we present closed form expressions of the expectation , variance and covariance between two test set errors .",
    "we first derive expressions for the quantities @xmath97 , @xmath98 entering the computation of the variance of @xmath5 by exploiting relation .",
    "let @xmath250 , @xmath28 , be random variables with @xmath251 , @xmath252 is the design matrix , @xmath253 is the parameter vector and @xmath254 is such that @xmath255 , @xmath256 is a vector of errors , @xmath257 and @xmath258 , @xmath259 is the @xmath260 identity matrix .",
    "let a training set @xmath53 be of size @xmath31 ; then , @xmath261 is the @xmath262 matrix according to @xmath47 and @xmath263 indicates the estimator of @xmath264 computed by using the data in the training set @xmath53 .",
    "* assumption : * if @xmath47 is the index set of a training set with @xmath31 indices , then @xmath265 where @xmath266 is finite and positive definite .",
    "[ rem.noether ] in the aforementioned formulation @xmath267 with mean vector @xmath268 and finite variance - covariance matrix @xmath269 .",
    "thus , if @xmath270 is a training index set , the vectors @xmath271 are an independent , identically distributed collection from @xmath272 . therefore , @xmath273 is the usual sample estimator of @xmath274 , which is a @xmath275 positive definite matrix , say @xmath266 .",
    "the strong law of large numbers implies that @xmath276 converges almost surely to @xmath266 , as @xmath31 tends to infinity .",
    "therefore , is a natural assumption , which is generally true under the simple condition that the covariates have finite second order moments .",
    "also , wu s lemma ( see * ? ? ?",
    "* ) proves that if is satisfied then the following generalized noether condition holds @xmath277 , as @xmath278 , where @xmath279 denotes the @xmath1-th row of the matrix @xmath261 .",
    "notice that the fixed design case is obtained as a special case of the aforementioned framework .    under the above conditions , @xmath280    for each @xmath281 we define the @xmath282 matrix @xmath283 , where @xmath284 is the @xmath64-th element of the usual basis of @xmath285 , and the @xmath260 diagonal matrix @xmath286",
    "observe that @xmath287 and @xmath288 ; also , @xmath289 and @xmath290 .",
    "thus , is reformulated as : @xmath291^{-1}x^{t}i_{s_j } x\\to v^{-1},\\quad\\textrm{as } \\",
    "{ \\operatorname{card}}(s_j)\\to\\infty.\\ ] ] generally , the matrix @xmath292 is unknown ; but it is estimated easily from the data by @xmath293 .",
    "hereafter , we use this estimation as the true matrix @xmath292 . in the appendix ,",
    "table [ table.v ] presents an illustration of the convergence of @xmath294 to @xmath266 .    in the analysis that follows ,",
    "we restrict ourselves to the case @xmath295 , i.e.  the explanatory variables are treated as fixed .",
    "this formulation is known as _ the fixed design case_.      the following propositions establish the expressions for @xmath296 , @xmath65 and @xmath297 need to obtain the variance of the cv estimator of generalization error .",
    "[ [ random - cv - estimation ] ] random cv estimation : + + + + + + + + + + + + + + + + + + + + +    using the above results , the following proposition offers closed form expressions for the expectation , variance and covariance of average test set errors .",
    "[ prop.e , v , c - randomcv ] under normality of errors , and squared error loss @xmath298p}{(n-1)n_1 ^ 2n_2}\\right\\},\\\\ & { \\operatorname{\\mathsf{cov}}}({\\widehat{\\mu}}_{j},{\\widehat{\\mu}}_{j'})=\\sigma^4\\left\\{\\frac{2}{n}+\\frac{n+2n_1}{n(n-1)n_1}p+\\frac{2(n+n_1(n_1 - 2)-1)}{(n-1)(n-2)n_1 ^ 2}\\theta\\right.\\\\                                & \\qquad\\qquad\\qquad\\qquad\\,\\left.+\\frac{(n-2)(n+n_1 ^ 2 + 2n_1n_2 - 1)-(n_1 - 1)^2}{(n-1)^2(n-2)n_1 ^ 4}(p-\\theta)\\right\\ } , \\end{split}\\ ] ] where @xmath299 , @xmath300 $ ] , and @xmath301 is the @xmath64-th diagonal element of the hat matrix @xmath302 .",
    "since @xmath303p}{(n-1)n_1 ^ 2n_2}=o(\\sfrac{1}{n^2})$ ] and @xmath304 then @xmath305    see online appendix .",
    "we now relax the assumption of normality of errors by requiring a much weaker condition .",
    "that is , we require that there exists an @xmath249 such that @xmath248 , effectively requiring the existence of moments of order @xmath306 . to relax the assumption of normality of the errors we need to guarantee the convergence of the moments of the estimators of @xmath183 to the moments of the corresponding asymptotic distribution .",
    "@xcite address this issue , as well as the associate rate of convergence .",
    "[ lem.w ] define the random vector @xmath307 , where @xmath308 , @xmath309 , @xmath47 is the index set of a training set and @xmath310 .",
    "then @xmath311    it follows easily by and _ delta_-method .",
    "@xmath84    the following proposition provides the needed expressions for computing the variance of the cross validation generalization error estimate and the optimal training sample size .",
    "[ prop.e , v , c - randomcv - nonnormality ] under the assumption that @xmath248 , @xmath249 , and in the case of squared error loss , @xmath312 where @xmath313 .    see online appendix .",
    "@xmath84    [ [ k - fold - case ] ] @xmath1-fold case : + + + + + + + + + + + + + + + + + + + + + + + + + +    in the @xmath1-fold cv case @xmath314 and @xmath315 .",
    "then ,    [ prop.e , v , c - kfoldcv ] under normality of errors , and squared error loss @xmath316    see online appendix .",
    "@xmath84    [ prop.optim-nonnormality ]    1 .   under normality of errors , 1 .   in the random cv case @xmath199",
    "; 2 .   in the @xmath1-fold case",
    "@xmath225 , that is , the loocv is proposed .",
    "2 .   for general error distribution with @xmath317 for some @xmath249 , and in the case of random cv @xmath199 .",
    "\\(a ) we omit the term @xmath145 of @xmath65 in proposition [ prop.e , v , c - randomcv ] and consider the function @xmath318 , @xmath160 . as in proof of theorem [ theo.opt-n_1 ] , we find that @xmath163 decreases up to @xmath319 and increases after this , thus , @xmath199 , which does not depends on the parameters or the observations .",
    "\\(b ) from proposition [ prop.e , v , c - kfoldcv ] , since @xmath66 is a @xmath145 quantity , omitting the terms @xmath320 we get @xmath321}{(k-1)n^2(n-1 ) }   + \\frac{k^2p}{(k-1)^2n^2 }   + \\frac{2k^3(p-\\theta)}{(k-1)^3n(n-1 ) } , $ ] which implies that @xmath225 .",
    "\\(c ) using the same arguments as in proof of theorem [ theo.opt-n_1 ] the optimal value of @xmath31 in follows .",
    "@xmath84    [ rem.opt ] similarly to the case where the desision rule was @xmath322 , for the optimal value of @xmath323 , @xmath324 and @xmath325 , hence , @xmath326 .",
    "therefore , the resampling size @xmath4 can be chosen either via specification of the resampling effectiveness or specification of the reduction ratio of @xmath5 , for a given @xmath61 or @xmath62 , by the following relations [ see , ] , cf .",
    "table [ table.jre,jrr ] , @xmath327    [ rem.regr_general ] the above results can be extended to a subclass of the @xmath7-loss functions class ; this subclass contains differentiable functions that can be expressed as functions of the errors .      to illustrate the difficulty of obtaining a close form solution for the training set sample size in general , when other than the squared error loss functions are used , we discuss the case of classification via logistic regression .",
    "@xcite formalize logistic regression as a linear classification method , where @xmath328 is the label of the data and @xmath329 is a feature vector .",
    "the loss function here is the logistic loss given as @xmath330 , where @xmath328 is a label and @xmath331 is the algorithm prediction , such that @xmath332    in this case the minimization problem does not have a closed - form solution for obtaining the optimal value of @xmath31 , and the problem is reduced to numerical optimization .",
    "we offer the following algorithm in order to obtain , numerically , the optimal value of the training set sample size .    in view of ,",
    "first we show the following general result .",
    "[ theo.var(mu_j ) ] let @xmath21 be a loss function , @xmath47 an index set of size @xmath32 ( @xmath39 ) and @xmath310 .",
    "if the asymptotic values of @xmath333 $ ] , and @xmath334 $ ] do not depend on the particular realization of @xmath47 , say @xmath335 and @xmath336 respectively , then , the asymptotic value of @xmath296 and @xmath65 are @xmath337\\right\\}. \\end{split}\\ ] ]    see online appendix .",
    "@xmath84    theorem [ theo.var(mu_j ) ] provides expressions for the expected value and variance of @xmath72 that enter the specification of the optimality rule .",
    "compute the matrix @xmath338 and via logistic regression estimate the parameters @xmath339 and @xmath235 , say @xmath340 and @xmath341 respectively , using the entire sample .",
    "compute the following probabilities and quantities @xmath342    for @xmath343    1 .",
    "compute the quantities @xmath335 , @xmath344 and @xmath336 of theorem [ theo.var(mu_j ) ] @xmath345 2 .   using theorem [ theo.var(mu_j ) ] ,",
    "compute @xmath346 .",
    "set @xmath347 .",
    "given @xmath47 and @xmath310 standardize the components of @xmath348 in lemma [ lem.w ] as @xmath349 , @xmath350 and consider the random vector @xmath351 .",
    "then , @xmath352 also , define @xmath353    the decision rule for classification is then given as follows . for a training set @xmath53 , from the model we estimate the probability @xmath331 for each @xmath188 ,",
    "say @xmath354 its estimator .",
    "after , we estimate the value @xmath355 as @xmath356 the loss function is @xmath357 ( see * ? ? ? * ) .",
    "we compute the values @xmath358={\\operatorname{\\mathds{e}}}[l^2_{0/1}({\\widehat{y}}_{s_j , i},y_i)|s_j , i]$ ] and @xmath359 $ ] , see in online appendix , which are given as @xmath360 where @xmath361 is the cumulative distribution function of standard normal distribution and @xmath362 is the cumulative distribution function of @xmath363 .",
    "theorem [ theo.var(mu_j ) ] gives a formula to calculate the variance of @xmath72 .",
    "but , the minimization process of does not give a close form for the optimal value of @xmath31 , it is reduced to a numerical minimization process .",
    "computing the quantities @xmath331 , @xmath364 and @xmath365 , via theorem [ theo.var(mu_j ) ] and , we calculate the variance of @xmath72 for @xmath343 . the value of @xmath31 which gives the minimum value of the variance is the optimal choice of @xmath31 .    the parameters @xmath235 , @xmath366 are unknown and we estimate those before the minimization process begins using the entire data set .",
    "we also compute @xmath292 as @xmath367 .",
    "in this section we present simulation results using a variety of distributions and loss functions with the goal of illustrating empirically our theoretical results .",
    "we discuss the empirical performance of our rules organizing the presentation according to the cases studied above , and we note that general simple recommendations about optimal selection of training sample size are possible .      using the version of , _",
    "ri386 3.1.2 _ on a _ dell latitude e7240 _ pc we simulated @xmath368 samples of size @xmath369 from distributions that can be categorized into symmetric with a variety of tail behaviors ( normal , @xmath370 , @xmath371 , @xmath372 ) and asymmetric ( @xmath373 , log - normal , pareto@xmath374 , pareto@xmath375 ) .",
    "the normal distribution is central in statistics , while the two @xmath376-distributions exhibit heavier than the normal , tail behavior .",
    "the log - normal distribution is used in biostatistics in biomarker studies , while the pareto distribution is a power law distribution used in the description of social , geophysical , scientific , actuarial and many other observable phenomena .",
    "the selection of @xmath372 and pareto(6 ) distributions is not arbitrary .",
    "both the @xmath372 and pareto(6 ) distributions possess less than six moments , and for our theory to apply we require the existence of up to six moments .",
    "thus , these selections reflect the performance of the methods in limit cases . to illustrate the effect of the choice of loss function has on the size of training set we use the loss functions presented in table [ table.loss ] .",
    "our results indicate that for the @xmath7-class of loss functions the optimal training sample size is @xmath8 , independent of the data distribution .",
    "however , one can construct loss functions , such as the modified squared error loss given in table [ table.loss ] , for which the optimal training sample size is not @xmath8 .",
    "notice that , the modified squared error loss functions does not belong in the @xmath7-class .",
    "tables [ table.sq][table.dousq ] ( and table [ table.abs ] included in the online appendix ) present the estimated and theoretical values of @xmath377 , @xmath378 and @xmath379 when the decision rule is the sample mean for aforementioned distributions using the loss functions presented in table [ table.loss ] .",
    "the estimated values of @xmath181 , and hence the estimated proportion @xmath377 , are obtained by estimating the values of the parameters @xmath156 , @xmath154 of theorem [ theo.opt-n_1 ] , using the data .",
    "similarly , to estimate the value of @xmath380 , and thus @xmath379 , we estimate , using the data , the values of @xmath184 , @xmath185 of proposition [ prop.opt-k ] ( see also ) . because @xmath181 and @xmath380 are functions of the total sample size @xmath0 , tables [ table.sq][table.dousq ] present these estimated proportions along with their mean squared error . therefore , if @xmath381 then @xmath199 , and if @xmath382 then @xmath225 , a value that corresponds to loocv .",
    "table [ table.sq ] presents the estimated and , in parenthesis , the theoretical value of @xmath377 , the estimated value of @xmath378 , and the estimated from the data value of @xmath379 .",
    "the second line , for each sample size , reports the mean squared error ( mse ) .",
    "the loss function here is squared error loss , which belongs in the @xmath7-class of loss functions .",
    "the results of table [ table.sq ] indicate that @xmath199 , and @xmath225 corresponds to loocv for all distributions .",
    "furthermore , the estimator of the correlation coefficient is highly accurate .",
    "similarly , tables [ table.efron ] and [ table.abs ] ( see online appendix ) present simulation results when the loss functions are a @xmath7-loss with generator @xmath383 and the approximated absolute error loss . the results of these tables indicate that @xmath199 and @xmath380 corresponds again to loocv .",
    "the approximated absolute error loss does not belong in the @xmath7-class of loss functions ; we take @xmath384 .",
    "we now contrast the above results with those presented in tables [ table.modsq ] and [ table.dousq ] . tables [ table.modsq ] , [ table.dousq ] present simulation results using the sample mean as the decision rule , for a variety of distributions and sample sizes indicated in the tables .",
    "note that the minimum divisor of sample sizes 60 , 100 , 750 and 5000 is 2 , while the minimum divisor for sample size 301 is 7 and of sample size 1501 is 19 .",
    "tables [ table.modsq ] , [ table.dousq ] exemplify clearly the interaction between the loss function and the data distributions , with the selection of the optimal training sample size and the selection of the optimal number of folds in @xmath1-fold cv .",
    "table [ table.modsq ] presents the proportion of the sample size allocated to the training set . while distributions such as @xmath373 or log - normal select @xmath199 , other distributions such as @xmath385 or @xmath386 require @xmath181 to be approximately 80% of the total sample size . on the other hand , in all cases the modified squared error loss function ,",
    "offers @xmath225 , the loocv .",
    "table [ table.dousq ] presents analogous results for the double squared error loss .",
    "the results indicate the impact of the loss function and data distribution on the selection of @xmath380 .",
    "note that , our limit cases here are represented by @xmath387 and pareto(9 ) distributions ( recall that we require the existence of at least eight moments ) .",
    "note that all distributions , with the exception of the pareto(15 ) and pareto(9 ) select as @xmath388 , for all sample sizes used .",
    "the pareto distributions select @xmath225 , indicating optimality for the loocv .",
    "the double squared error loss does not belong in the @xmath7-class of loss functions .",
    "the comparison with the results presented , for example in tables [ table.sq ] and [ table.efron ] , where the loss functions were members of the @xmath7-class , clearly indicates the impact of the loss function on the optimal sample size selection for the training and hence the test set .",
    "when cross validation estimators of the generalization error are used the usual recommendation made is to use 70% - 80% of the data for training and the remaining for testing .",
    "figures [ fig.sq ] and [ fig.modsq ] plot the variance of the test set error as a function of the sample splitting for squared error and modified squared error losses .",
    "the graphs show that the test set variance is minimized for the optimal value of @xmath31 .",
    "on the other hand , figure [ fig.n/2 ] plots the relative efficiency of @xmath5 against the ratio @xmath389 , for @xmath390 ( it corresponds to @xmath391 ) , @xmath218 and @xmath392 .",
    "the relative efficiency of @xmath5 is defined as the ratio of the variance of @xmath5 for any given data splitting over the variance of @xmath5 at the optimal data splitting .",
    "figure [ fig.n/2 ] clearly shows that , for all sample sizes , all loss functions with @xmath199 , and all distributions , the popular recommendations in terms of splitting exhibit substantial increase in the variance of @xmath5 .",
    "for example , when 75% of the total sample size is used for training , leaving 25% for testing , the increase in the variance of @xmath393 when squared error loss is used and @xmath394 is 17.3% , while when 80% ( or 90% ) of the total sample is used for training this increase is 24.4% ( or 88.6% ) . and",
    "if @xmath131 , then the increase in variance is 11.86% when the training set is 75% of the sample ; it is 16.8% ( or 61.2% ) when 80% ( or 90% ) of the sample is used for training .",
    "notice that @xmath395 is almost unaffected by the splitting mechanism across all loss functions , independently of data distribution .",
    "this provides further justification of our selection of the optimality rule .",
    "this indicates that is equivalent to minimizing @xmath134 and to minimizing @xmath6 for all @xmath4 ( see also relation ) .",
    "furthermore , @xmath396 when @xmath199 ( both theoretically and empirically ) , so that @xmath397 , and the limited variance @xmath98 is almost unaffected by the splitting . in view of figure [ fig.n/2 ] , @xmath398 , while @xmath399 , @xmath400 and @xmath401 , where @xmath402 denotes the variance of the test set error @xmath72 for training sample size @xmath403 .",
    "the notations @xmath404 and @xmath405 have similar interpretation .",
    "thus , @xmath406 , @xmath407 , @xmath408,@xmath409 , and @xmath410 .",
    "the relationship of these results to the resampling effectiveness and reduction ratio of @xmath5 are shown in table [ table.re,rr.for_n/2 ] for @xmath411 , @xmath392 and when the training sample size is @xmath412 ( @xmath413 ) , @xmath414 , @xmath415 , @xmath416 and @xmath417 .",
    "these results show that when the training sample size in not optimal then we need to increase the resampling size to obtain acceptable levels of reduction ratio and/or resampling effectiveness , thereby increasing the computational burden of the procedure .    1    1    1    1    1    1    1    1    1      we empirically now study the variance of generalization error in the cases of linear regression and classification via logistic regression .",
    "date were generated as @xmath418 , @xmath419 , where the sample size @xmath420 , @xmath421 , @xmath422 and @xmath423 and the error vector @xmath424 follows @xmath425 distribution .",
    "the four covariates were generated only once as follows .",
    "the first covariate @xmath426 is a binary variable generated from @xmath427 distribution , the second variable is generated from @xmath428 distribution , the third variable is generated from the @xmath429 distribution and variable @xmath430 is generated from the @xmath431 distribution .",
    "the number of monte carlo repetitions equals 5000 .",
    "table [ table.sqregr ] and figure [ fig.sqreg ] present the results ; notice that , both the table and figure , indicate an increase in the variance of the test set error , @xmath65 , as well as in @xmath6 with @xmath432 , when the training sample size moves away from the optimal value of @xmath323 .",
    "this is true for all sample sizes .",
    "table [ table.zeroone ] presents relevant results for classification via logistic regression .",
    "the model contains three covariates , @xmath426 is generated once as @xmath427 , @xmath433 is @xmath428 and @xmath434 is generated from discrete uniform on the set @xmath435 .",
    "the vector of errors is again @xmath425 , and we generate 50 monte carlo samples . table [",
    "table.zeroone ] presents estimates of the @xmath65 for two values of the sample size @xmath0 , 60 and 100 , and for different values of the training sample size @xmath31 .",
    "again , @xmath65 increases if the popular choices of @xmath31 , @xmath414 or @xmath415 , are used , and @xmath65 is minimized when @xmath323 , and this holds for all sample sizes and error distributions .    1    1    1",
    "in what follows , we briefly describe the two data sets we use to illustrate our results .",
    "one is a data set on birth weight and the second is a part of the data set from the _ national health and nutrition examination survey _ ( nhanes iii ) conducted by the _ national center for health statistics _ ( nchs ) between 1988 and 1994 ( see * ? ? ?",
    "low birth weight , defined as a birth weight less than 2500 grams , is an outcome that has been of concern to physicians for years , because of its association between high infant mortality and birth defect rates .",
    "data were collected as part of a large study at baystate medical center in springfield , ma .",
    "the outcome variable was a binary variable taking values 0 if the weight of a baby at birth is greater than or equal to 2500 gr and 1 if it is less than 2500 gr , i.e.  it is actual low birth weight .",
    "measurements on 10 , additional to the outcome , variables were collected on 189 subjects .",
    "out of 189 subjects 59 were observed as low birth weight and 130 as normal birth weight , so the class proportions were 31.22% low birth weight and 68.78% normal birth weight . because the race variable has three categories , white , black and other , it was coded using two design variables defined as follows .",
    "the first design variable takes the value 1 when the race is `` black '' and 0 otherwise , while the second design variable takes the value 1 when the race is `` other '' and 0 when it is `` white '' or `` black '' .",
    "the other variable we use was mother s weight at the last menstrual period , and the logistic regression model we use contains a constant term ( see * ? ? ? * ) .",
    "the second data set has a binary outcome variable and five covariates .",
    "the binary outcome variable @xmath42 takes values 0 if the average systolic blood pressure is less than or equal 140 and is 1 if it is greater than 140 .",
    "data were collected via physical examinations and clinical and laboratory tests , and only adults defined as 20 years of age and older were included in the survey .",
    "we selected a sample of size 1,000 with complete observations on the covariates representing age ( in years ) , sex ( male or female ) , race ( white , black , other ) , body weight ( in pounds ) and standing height ( in inches ) .",
    "because the race variable has three categories , it was coded using the same design variables , as in the previous example . applying our proposed algorithm [ algorithm ] ( for classification via logistic regression ) , we obtain that as the size of the training sample increases so does the variance of the test set error .",
    "we obtain minimum variance when @xmath323 .",
    "in this paper we address the problem of `` optimal '' selection of the size of training sets when interest centers in making inferences about the generalization error of prediction algorithms .",
    "we study two types of cross validation estimators of the generalization error .",
    "these are random cross validation ( or repeated learning - testing method ) and @xmath1-fold cross validation estimators . the statistical rule that defines `` optimality '' , in light in proposition [ prop.v , c ] and theorem [ theo.var_bound ] ,",
    "calls for the minimization of the variance of the test set error .",
    "our results indicate that the optimal training sample size selection is a complex problem and it depends primarily on the loss function that is used , as well as on the data distribution .",
    "describe the complexity of the analysis , simple general rules for practical use can be drawn .",
    "when the loss function belongs to the @xmath7-class then , for the case of random cross validation , @xmath199 ( for both , sample mean and regression decision rules ) .",
    "when the loss function does not belong in the @xmath7-class then the value of @xmath181 may or may not equal @xmath8 , indicating that @xmath7-class membership is a sufficient condition for @xmath199 , but not a necessary condition .",
    "we present cases where the loss function does not belong in the @xmath7-class , yet @xmath199 ( see , for example , table [ table.abs ] ) .",
    "furthermore , we illustrate the effects of complex interactions between the loss function a researcher chooses and the data distribution on the optimal training sample size selection in tables [ table.modsq ] .    furthermore , we studied the effect that popular training set sample size selection , such as @xmath414 or @xmath415 , has on the @xmath436 and @xmath6 , @xmath432 .",
    "we found out that as the training sample size increases away from its optimal value the aforementioned variances increase substantially . to decrease those we need to increase the resampling size @xmath4 from 15 to a value that achieves acceptable reduction ratio and/or resampling effectiveness , thereby increasing the computational cost .",
    "the selection of the resampling size @xmath4 of a random cross validation estimator of the generalization error is important , as it contributes to the variance reduction of this estimator .",
    "we propose two methods of selecting @xmath4 and exemplify their use .",
    "our analysis indicates that , when the correlation between two different test sets is moderate , i.e.  in range of @xmath437 $ ] , the resampling size @xmath438 if we desire resampling effectiveness greater than or equal to @xmath439 .",
    "the higher the desired resampling effectiveness , the higher the value of @xmath4 .",
    "similar results hold when the reduction ratio forces the variance of @xmath5 closer to its asymptotic value , and provides for larger values of @xmath4 .",
    "our work shows that the correlation coefficient @xmath127 is affected by the training sample size and since @xmath4 is a function of @xmath127 it is also affected by the splitting of the total sample .",
    "the effect of training sample size @xmath31 is to decrease the correlation as @xmath31 moves away from its optimal value , requiring larger and larger values of @xmath4 to achieve small values of @xmath6 .    a potential limitation to the complete generality of the results presented here",
    "is the fact that we require the operating data distributions to have finite moments of at least order six ( see subsections [ ssec : mean ] and [ ssec : regression ] ) .",
    "most commonly used distributions satisfy this condition .",
    "but if the data do not follow a distribution satisfying the aforementioned condition , a transformation of the data may be amenable to the analysis presented here .    for the @xmath1-fold cv estimator of the generalization error we provide rules for obtaining @xmath380 , and study the factors that affect the value of it . as in the case of random cv ,",
    "the value of @xmath380 is affected by the loss function and the data distribution .",
    "our results indicate that loocv can be replaced by @xmath1-fold with @xmath440 , in most cases .",
    "table [ table.v ] illustrates the convergence of the matrix sequence @xmath441 to the matrix @xmath442 .",
    "we simulate @xmath368 random samples of size @xmath443 , @xmath422 and @xmath444 from the 3-dimensional normal distribution with mean vector @xmath445 and variance - covariance matrix @xmath446 and from the trinomial distribution with number of the possible outcomes @xmath219 and probability vector @xmath447 . in the table",
    "are shown the average of the estimator @xmath448 from the samples , and the corresponding average of the maximum norm , @xmath449 , between the estimators and the true value of @xmath450 , that is @xmath451 .",
    "also , for the trinomial distribution case , in which its components do not standardized , is shown in a parenthesis the average of the maximum norm between the estimators and the true value of @xmath266 of the corresponding standardized random vectors .",
    "let @xmath47 , @xmath452 be given training sets , @xmath188 , @xmath453 and squared error loss such that @xmath454 and @xmath455 .",
    "define the quantities @xmath456 write the @xmath260 matrix @xmath457 where @xmath458 is the hat matrix of regression , and observe that @xmath459 , @xmath460 and @xmath461 .",
    "it is well known that @xmath462 , @xmath463 and @xmath464 ( see , for example , * ? ? ?",
    "* ) . setting the parameter @xmath465 $ ] , we have that @xmath466    [ [ normal - error - distribution ] ] normal error distribution + + + + + + + + + + + + + + + + + + + + + + + + +    assume the errors in the linear regression model are normally distributed , that is @xmath467 .",
    "let @xmath263 , @xmath468 be the ordinary least square ( ols ) estimators computed on the training sets @xmath47 , @xmath469 .",
    "it is well known that @xmath470 ( and similarly for the index @xmath67 ) , with @xmath471 and @xmath290 ( and similarly for the index @xmath67 ) are defined above as the product of @xmath472 and @xmath473 .",
    "this allows one to write @xmath474 and @xmath475 .",
    "let the bivariate random vector @xmath476 then , as @xmath31 becomes large @xmath477 and the moments of @xmath478 are approximated by the associate moments of the approximated distribution .",
    "we offer a lemma that is fundamental in proving the results presented in this paper .",
    "[ lem.indices ] let @xmath47 and @xmath469 be two index sets of size @xmath32 ( and @xmath479 as in random cv ; and let us consider the fixed indices @xmath480 . then ,    \\(a ) @xmath481 .",
    "\\(b ) @xmath482 and @xmath483 .",
    "\\(c ) @xmath484 .",
    "\\(d ) @xmath485 and @xmath486 .",
    "\\(e ) @xmath487 .",
    "\\(f ) @xmath488 .",
    "\\(g ) @xmath489 .",
    "\\(h ) @xmath490 .",
    "\\(i ) @xmath491 , @xmath492 .",
    "\\(j ) @xmath493}{n^2(n-1)^2(n-2)}$ ] .",
    "\\(k ) @xmath494 .",
    "99  and markatou , m.  ( 2015 ) .",
    "uniform integrability of ols estimators , and the convergence of its moments .",
    "_ arxiv _    ( 2010 ) . a survey of cross - validation procedures for model selection .",
    "_ statistics surveys _ , * 4",
    "* : 4079 .    and pericchi , l.r .",
    "training samples in objective bayesian model selection . _",
    "the annals of statistics _ , *",
    "32*(3 ) : 841869 .    ( 1984 ) . _",
    "classification and regression trees_. wadsworth , belmont , california .",
    "( 1989 ) . a comparative study of ordinary cross - validation , v - fold cross - validation and the repeated learning - testing methods . _ biometrika _ , * 76*(3 ) : 503514 .    ( 1990 ) .",
    "estimation of optimal transformations using v - fold cross validation and repeated learning - testing methods . _ sankhy : the indian journal of statistics , series a _ , * 52*(3 ) : 314345 .",
    "( 2015 ) . a new minimal training sample scheme for intrinsic bayes factors in censored data . _ computational statistics and data analysis _ ,",
    "* 81 * : 5263 .    and",
    "hadi , a.s .",
    "_ sensitivity analysis in linear regression . _ john wiley & sons , new york .",
    ". sample size determination in microarray experiments for class comparison and prognostic classification .",
    "_ biostatistics _ , * 6*(1 ) : 2738 .    ( 2007 ) .",
    "sample size planning for developing classifiers using high dimensional dna microarray data .",
    "_ biostatistics _ , * 8 * : 101117 .    ( 2011 ) . optimally splitting cases for training and testing high dimensional classifiers .",
    "_ bmc : medical genomics _ , *",
    "4*(1 ) : 31 .",
    "http://www.biomedcentral.com/1755-8794/4/31[`www.biomedcentral.com/1755-8794/4/31 ` ]    ( 2013 ) .",
    "sample size requirements for training high - dimensional risk predictors .",
    "_ biostatistics _ , * 14 * : 639652 .",
    "( 2008 ) . how large a training set is needed to develop a classifier for microarray data ? _ clinical cancer research _ , *",
    "14*(1 ) : 108114 .    ( 1983 ) . estimating the error rate of a prediction rule : improvement on cross - validation .",
    "_ journal of the american statistical association _",
    ", * 78 * : 316331 .",
    "how biased is the apparent error rate of a prediction rule ?",
    "_ journal of the american statistical association _ , * 81 * : 461470 .    ( 2004 ) .",
    "the estimation of prediction error : covariance penalties and cross - validation .",
    "_ journal of the american statistical association _",
    ", * 99 * : 619642 .",
    "( 2013 ) . a @xmath495-statistic estimator for the variance of resampling - based error estimators , _ arxiv preprint _ , http://arxiv.org/pdf/1310.8203.pdf[`arxiv:1310.8203",
    "estimation of classifier performance . _ ieee transactions on pattern analysis and machine intelligence _ , * 11*(10 ) : 10871101 .",
    "the predictive sample reuse method with applications .",
    "_ journal of the american statistical association _ , * 70*(350 ) : 320328 .",
    "( 1998 ) . what size test set gives good error rate estimates ? _ ieee transactions on patterns analysis and machine intelligence _ , * 20*(1 ) : 5264 .",
    "( 2009 ) . _",
    "the elements of statistical learning : data mining , inference , and prediction . _ springer , new york ( 2nd ed . ) .",
    "the design and analysis of pattern recognition experiments .",
    "_ bell sys .  tech .",
    "j. _ , * 41 * : 723744 .    ( 2000 ) .",
    "_ applied logistic regression_. second edition .",
    "wiley series in probability and statistics , john wiley & sons , inc .",
    ", new york .",
    "( 1918 ) . on a formula for the product - moment coefficient of any order of a normal frequency distribution in any number of variables . _ biometrika _ , * 12 * : 134139 .    ( 2014 ) . assessing the impact of training sample selection on accuracy of an urban classification : a case study in denver , colorado .",
    "_ international journal of remote sensing _ , * 35*(6 ) : 20672081 .",
    "a bound on the error of cross validation using the approximation and estimation rates , with consequences for the training - test split . _ neural computation _ , * 9 * : 11431161 .    ( 1999 ) . on optimal data split for generalization estimation and model selection .",
    "_ in proceedings of the ieee workshop on neural networks for signal processing ix .",
    "_ , 225234 .",
    "piscataway : ieee .",
    "10.1109/nnsp.1999.788141    and sinha , a.  ( 2011 ) . a comparison of estimators for the variance of cross - validation estimators of the generalization error of computer algorithms . in _",
    "nonparametric statistics and mixture models : a festschrift in honor of thomas p.  hettmansperger _ , hunter , d.r . ; richards , d.p .  and rosenberger , j.l .",
    "( eds ) , world scientific , 226251 .    and",
    "hripcsak , g.  ( 2005 ) . analysis of variance of cross - validation estimators of the generalization error .",
    "_ journal of machine learning research _ , * 6 * : 11271168 .",
    "statistical methods for artificial intelligence .",
    "_ lecture notes for ttic103 , toyota technological institute at chicago_.    http://ttic.uchicago.edu/~dmcallester/ttic101-06/lectures/genreg/genreg.pdf[`ttic.uchicago.edu/  dmcallester / ttic101 - 06/lectures / genreg / genreg.pdf ` ]    and bengio , y.  ( 2003 ) .",
    "inference for the generalization error . _ machine learning _ , * 52 * : 239281 .    ( 2010 ) .",
    "effect of training - sample size and classification difficulty on the accuracy of genomic predictors .",
    "_ breast cancer research _ , * 12*(1 ) : r5 .    ( 1991 ) .",
    "small sample size effects in statistical pattern recognition : recommendations for practitioners . _",
    "ieee transactions on pattern analysis and machine intelligence _ , *",
    "13*(3 ) : 252264 .    ( 2013 ) .",
    "determination of minimum training sample size for microarray - based cancer outcome prediction  an empirical assessment .",
    "_ plos one _ , * 8*(7 ) : e68579 .",
    "doi : 10.1371/journal.pone.0068579    ( 1974 ) .",
    "cross - validatory choice and assessment of statistical predictions .",
    "_ journal of the royal statistical society .",
    "series b _ , * 36*(2 ) : 111147 .",
    "asymptotics for and against cross - validation .",
    "_ biometrika _ , * 64*(1 ) : 2935 .",
    "( 2009 ) . testing the prediction error difference between 2 predictors . _",
    "biostatistics _ , * 10*(3 ) : 550560 .    ( 2014 ) .",
    "variance estimation of a general @xmath495-statistic with application to cross - validation .",
    "_ statistica sinica _ , * 24 * : 11171141 .",
    "( 1992 ) . stacked generalization .",
    "_ neural networks _ , * 5*(2 ) : 241259 .    ( 1981 ) .",
    "asymptotic theory of nonlinear least squares estimation .",
    "_ the annals of statistics _ , * 9*(3 ) : 501513 .",
    "impact of training and validation sample selection on classification accuracy and accuracy assessment when using reference polygons in object - based classification .",
    "_ international journal of remote sensing _ , * 34*(19 ) : 69146930 .    *",
    "supplementary material *",
    "proof of lemma [ lem.indices ] ( a ) @xmath496 .",
    "\\(b ) @xmath497 and @xmath498 .",
    "\\(c ) as above , @xmath499 .",
    "\\(d ) it follows immediately from ( a ) .",
    "\\(e ) @xmath500 , see ( b ) .",
    "\\(f ) due to independence of @xmath47 and @xmath469 and using ( a ) , @xmath501 and similarly for @xmath502 .",
    "\\(g ) as in ( f ) , using ( a ) and ( b ) , @xmath503 ( h ) similarly to ( g ) , using ( b ) , @xmath504 ( i ) for a simple notation , in the sequel of the proof we use the exchangeability of the indices .",
    "set @xmath505 and @xmath506 .",
    "also , observe that @xmath507 .",
    "so , using ( b ) , @xmath508 write @xmath509 as @xmath510 and , since @xmath511 , observe that @xmath512 is @xmath513 hence , the desired expected value is @xmath514 ( j ) write @xmath515 . as in ( i ) @xmath516 the expected value is @xmath517 ( k ) similarly to ( j ) , @xmath518 ; and the expected value is @xmath519 completing the proof .",
    "proof of theorem [ theo.var(mu_j ) ] write @xmath520 .",
    "so , @xmath521 and @xmath522 , since @xmath523 and because the quantities @xmath524 , @xmath525 are constants .",
    "therefore , @xmath526=\\frac{1}{n_2}\\sum\\limits_{i=1}^n { \\mathds{1}_{\\{i\\in s_j^c\\}}}{\\mathsf{e}}_i,\\ ] ] @xmath527\\right\\}\\\\ & = \\frac{1}{n_2 ^ 2}\\left\\{\\sum\\limits_{i=1}^n { \\mathds{1}_{\\{i\\in s_j^c\\}}}{\\mathsf{e}}_{i , i } + 2\\mathop{\\sum\\sum}\\limits_{1\\le i < i'\\le n}{\\mathds{1}_{\\{i , i'\\in s_j^c\\}}}{\\mathsf{e}}_{i , i'}\\right\\}. \\end{split}\\ ] ] using lemma [ lem.indices ] , we get @xmath528=\\frac{1}{n}\\sum_{i=1}^n{\\mathsf{e}}_i$ ] , @xmath529=\\frac{1}{nn_2}\\big\\{\\sum_{i=1}^n{\\mathsf{e}}_{i , i}+\\frac{2(n_2 - 1)}{n-1}\\mathop{\\sum\\sum}_{1\\le i < i'\\le n}{\\mathsf{e}}_{i , i'}\\big\\}$ ] , completing the proof .    proof of @xmath478 can be written as @xmath530 . using standard properties of multivariate normal distribution , after some algebra",
    ", we get @xmath531 , where @xmath532 has elements : @xmath533,\\\\ \\sigma_{22}=\\sigma^2[&\\bm{x}_{i'}^{{t}}(x^{{t}}i_{s_{j'}}x)^{-1}\\bm{x}_{i'}+1],\\\\ \\sigma_{12}=\\sigma^2[&\\bm{x}_i^{{t}}(x^{{t}}i_{s_j}x)^{-1}x^{{t}}i_{s_j{\\cap}s_j}x ( x^{{t}}i_{s_{j'}}x)^{-1}\\bm{x}_{i ' }                        -\\bm{x}_i^{{t}}(x^{{t}}i_{s_j}x)^{-1}x^{{t}}\\bm{x}_{i'}{\\mathds{1}_{\\{i'\\in s_j\\}}}\\\\                       & -\\bm{x}_i^{{t}}(x^{{t}}i_{s_{j'}}x)^{-1 } x^{{t}}\\bm{x}_{i'}{\\mathds{1}_{\\{i\\in s_{j'}\\}}}+{\\mathds{1}_{\\{i'=i\\}}}].\\end{aligned}\\ ] ] as @xmath31 becomes large , in view of , @xmath534 , @xmath535 approximated by @xmath536 .",
    "also , when @xmath537 we write the matrix @xmath538 as @xmath539 if @xmath540 takes large values as @xmath31 becomes large , @xmath541 ; and if @xmath542 , the matrix @xmath543 tends to @xmath275 zero matrix [ @xmath544 and @xmath468 tend to be independent ] thus this matrix is approximated by @xmath545 .",
    "so , for large values of @xmath31 the variance - covariance matrix @xmath546 is approximated by the matrix @xmath547 .",
    "finally , the moments of @xmath478 are continues functions of its variance - covariance matrix , which completes the proof .",
    "[ theo.isserlis ] if @xmath548 , then @xmath549 and @xmath550 .",
    "proof of proposition [ prop.e , v , c - randomcv ] let @xmath551 $ ] , @xmath552 $ ] . from and theorem [ theo.isserlis ]",
    "we obtain @xmath553 , @xmath554 .",
    "now , in assume that @xmath555 and @xmath556 .",
    "observe that @xmath557 and the indicators are zero .",
    "thus , using theorem [ theo.isserlis ] , @xmath558 $ ] . applying theorem [ theo.var(mu_j ) ] , @xmath559 .",
    "for the variance of @xmath72 we write @xmath560 ; and so , @xmath561 also , the quantity @xmath562 $ ] is @xmath563    -(n-1)n_2\\left(1+\\frac{\\bm{x}_i^{t}v\\bm{x}_i}{n_1}\\right)\\left(1+\\frac{\\bm{x}_{i'}^{t}v\\bm{x}_{i'}}{n_1}\\right)\\\\ = & -n_1\\left(1+\\frac{\\bm{x}_i^{t}v\\bm{x}_i}{n_1}\\right)\\left(1+\\frac{\\bm{x}_{i'}^{t}v\\bm{x}_{i'}}{n_1}\\right )    + \\frac{2n(n_2 - 1)}{n_1^{2}}(\\bm{x}_{i}^{t}v\\bm{x}_{i'})^2\\\\ = & -n_1 -(\\bm{x}_i^{t}v\\bm{x}_i+\\bm{x}_{i'}^{t}v\\bm{x}_{i'})-\\frac{1}{n_1}\\bm{x}_i^{t}v\\bm{x}_i\\bm{x}_{i'}^{t}v\\bm{x}_{i ' }    + \\frac{2n(n_2 - 1)}{n_1^{2}}(\\bm{x}_{i}^{t}v\\bm{x}_{i'})^2 .",
    "\\end{split}\\ ] ] observing that @xmath564 , we get @xmath565\\\\ = & ~\\sigma^4\\left\\{-\\frac{n(n-1)n_1}{2}-(n-1)c_1-\\frac{1}{2n_1}c_4+\\frac{n(n_2 - 1)}{n_1^{2}}c_3\\right\\}. \\end{split}\\ ] ] thus , the quantity @xmath566 $ ] is @xmath567 ; and using , @xmath568    now observe that @xmath569 and @xmath570 .",
    "thus , @xmath571 noting that , since @xmath572 and @xmath469 are given , the indicator functions are constants . therefore , @xmath573 in view of and using theorem [ theo.isserlis ] , we obtain the following . if @xmath574 , since @xmath575 and @xmath576 , @xmath577 and if @xmath502 , @xmath578 , and @xmath579 write @xmath580 as @xmath581 , where @xmath582 , @xmath583 and @xmath584 . using lemma [ lem.indices ] , @xmath585 & = \\sigma^4\\sum_{r=0}^{2}a_{r;i}{\\operatorname{\\mathds{e}}}[{\\operatorname{card}}^{r}(s_j\\cap s_{j'}){\\mathds{1}_{\\{i\\in s_j^c\\cap s_{j'}^c\\}}}]\\\\ & = \\sigma^4\\left\\{\\frac{n_2 ^ 2}{n^2}a_{0;i}+\\frac{n_1 ^ 2n_2 ^ 2}{n^2(n-1)}a_{1;i}+\\frac{n_1 ^ 2n_2 ^ 2(n_1(n_1 - 1)+n_2 - 1)}{n^2(n-1)(n-2)}a_{2;i}\\right\\}. \\end{split}\\ ] ] since @xmath586 , @xmath587 and @xmath588 , after some algebra , @xmath589\\\\ = & ~\\sigma^4\\left\\{\\frac{3n_2 ^ 2}{n}+\\frac{2n_2 ^ 2(n+2n_1 - 1)}{n^2(n-1)n_1}c_1+\\frac{n_2 ^ 2[n(n-1)+2n_1(n_1 - 2)]}{n^2(n-1)(n-2)n_1 ^ 2}c_2\\right\\}. \\end{split}\\ ] ] now write @xmath590 , where @xmath591 and @xmath592 .",
    "denote @xmath593 and observe that @xmath594\\\\   &    + \\frac{2}{n_1 ^ 2}{\\mathds{1}_{\\{i\\in s_j^c , i'\\in s_{j'}^c , i'\\in s_{j},i\\in s_{j'}\\ } } }    + \\frac{1}{n_1 ^ 2}[{\\mathds{1}_{\\{i\\in s_j^c , i'\\in s_{j'}^c , i'\\in s_{j}\\}}}+{\\mathds{1}_{\\{i\\in s_j^c , i'\\in s_{j'}^c , i\\in s_{j'}\\ } } } ] .",
    "\\end{split}\\ ] ] from lemma [ lem.indices ] , after some algebra , we get @xmath595}{n^2(n-1)^2(n-2)n_1 ^ 2}.\\ ] ] hence , @xmath596   = \\sigma^4\\left\\{b_{0;i , i'}{\\operatorname{\\mathds{e}}}({\\mathds{1}_{\\{i\\in s_j^c , i'\\in s_{j'}^c\\}}})+b_{1;i , i'}{\\operatorname{\\mathds{e}}}(\\vargamma_{s_j , s_{j'};i , i'})\\right\\}\\\\ & = \\sigma^4\\left\\{\\frac{n_2 ^ 2}{n^2 } b_{0;i , i ' }                   + \\frac{n_2 ^ 2[(n-2)(n+n_1 ^ 2 + 2n_1n_2 - 1)-(n_1 - 1)^2]}{n^2(n-1)^2(n-2)n_1 ^ 2}b_{1;i , i'}\\right\\}. \\end{split}\\ ] ] since @xmath597 and @xmath598 , @xmath599 = \\sigma^4 \\left\\ { \\frac{(n-1)n_2 ^ 2}{n } + \\frac{2(n-1)n_2 ^ 2}{n^2n_1}c_1\\right.\\qquad\\qquad\\\\ \\left.+\\frac{n_2 ^ 2[(n-2)(n+n_1 ^ 2 + 2n_1n_2 - 1)-(n_1 - 1)^2]}{n^2(n-1)^2(n-2)n_1 ^ 4}c_3 + \\frac{n_2 ^ 2}{n^2n_1 ^ 2}c_4 \\right\\}. \\end{split}\\ ] ] thus , by @xmath600 $ ] , @xmath601 + 2\\mathop{\\sum\\sum}\\limits_{1\\le i < i'\\le n}{\\operatorname{\\mathds{e}}}[{\\mathds{1}_{\\{i\\in s_j^c , i'\\in s_{j'}^c\\}}}{\\operatorname{\\mathds{e}}}(\\xi_{j , i}^2\\xi_{j',i'}^2)]\\right\\}\\\\ & = \\sigma^4 \\left\\ { \\frac{n+2}{n } + \\frac{2(n^2+n_1-n_2)}{n^2(n-1)n_1}c_1 + \\frac{n(n-1)+2n_1(n_1 - 2)}{n^2(n-1)(n-2)n_1 ^ 2}c_2 \\right.\\\\ & \\quad\\qquad\\left .",
    "+ \\frac{(n-2)(n+n_1 ^ 2 + 2n_1n_2 - 1)-(n_1 - 1)^2}{n^2(n-1)^2(n-2)n_1 ^ 4}c_3 + \\frac{1}{n^2n_1 ^ 2}c_4 \\right\\}. \\end{split}\\ ] ] since @xmath602 , the relation @xmath603 gives @xmath604 finally , completes the proof .",
    "proof of proposition [ prop.e , v , c - kfoldcv ] the behavior of @xmath72 is the same as in random cv case .",
    "thus , @xmath296 follows by proposition [ prop.e , v , c - randomcv ] , replacing @xmath31 by @xmath605 ; also , ( see above ) gives @xmath606    to compute @xmath66 we have that @xmath607 ( @xmath608 ) and @xmath609 , thus @xmath610 , where @xmath611 now observe that @xmath612 and @xmath613 .",
    "thus , @xmath614=\\frac{n\\sigma^4}{k^2(n-1 ) } \\bigg[\\left(1+\\frac{k\\bm{x}_i^{{t}}v\\bm{x}_i}{(k-1)n}\\right)\\left(1+\\frac{k\\bm{x}_{i'}^{{t}}v\\bm{x}_{i'}}{(k-1)n}\\right ) + \\frac{2k^4(\\bm{x}_i^{{t}}v\\bm{x}_{i'})^2}{(k-1)^4n^2}\\bigg]$ ] ; and so , @xmath615 $ ] . as in proof of proposition",
    "[ prop.e , v , c - randomcv ] , @xmath616 finally , the relation @xmath603 completes the proof .",
    "proof of proposition [ prop.e , v , c - randomcv - nonnormality ] set @xmath617 , @xmath618 , @xmath619 and @xmath620 . from lemma [ lem.w ] , using theorem [ theo.isserlis ] , @xmath621 @xmath622 due the independence of @xmath53 , @xmath355 and @xmath623 , @xmath624={\\operatorname{\\mathds{e}}}(x^2y^2)-2{\\operatorname{\\mathds{e}}}(x^2y){\\operatorname{\\mathds{e}}}(w)+{\\operatorname{\\mathds{e}}}(x^2){\\operatorname{\\mathds{e}}}(w^2)-2{\\operatorname{\\mathds{e}}}(xy^2){\\operatorname{\\mathds{e}}}(z)+4{\\operatorname{\\mathds{e}}}(xy){\\operatorname{\\mathds{e}}}(z){\\operatorname{\\mathds{e}}}(w)-2{\\operatorname{\\mathds{e}}}(x){\\operatorname{\\mathds{e}}}(z)\\times{\\operatorname{\\mathds{e}}}(w^2 ) + { \\operatorname{\\mathds{e}}}(y^2){\\operatorname{\\mathds{e}}}(z^2)-2{\\operatorname{\\mathds{e}}}(y){\\operatorname{\\mathds{e}}}(z^2){\\operatorname{\\mathds{e}}}(w)+{\\operatorname{\\mathds{e}}}(z^2){\\operatorname{\\mathds{e}}}(w^2)$ ] . by the fact that the asymptotic expected values are @xmath625 , @xmath626 , @xmath627 , @xmath628 and @xmath629",
    ", we get @xmath630.\\ ] ] applying theorem [ theo.var(mu_j ) ] and using , the form of the variance of @xmath72 follows .",
    "note that @xmath631 .",
    "proof of using the asymptotic distribution of @xmath632 , the expected values are : @xmath633\\\\ = & { \\operatorname{\\mathds{p}}}(l_{0/1}({\\widehat{y}}_{s_j , i},y_i)=1|s_j , i )   = { \\operatorname{\\mathds{p}}}({\\widehat{y}}_{s_j , i}=0,y_i=1|s_j , i)+{\\operatorname{\\mathds{p}}}({\\widehat{y}}_{s_j , i}=1,y_i=0|s_j , i)\\\\ = & { \\operatorname{\\mathds{p}}}(\\varpsi_i<-\\sqrt{n_1}\\zeta_i){\\operatorname{\\mathds{p}}}(y_i=1 )    + { \\operatorname{\\mathds{p}}}(\\varpsi_i\\ge-\\sqrt{n_1}\\zeta_i){\\operatorname{\\mathds{p}}}(y_i=0)\\\\ = & \\varphi(-\\sqrt{n_1}\\zeta_i)p_i+\\varphi(\\sqrt{n_1}\\zeta_i)(1-p_i ) , \\end{split}\\ ] ] @xmath634\\\\ = & { \\operatorname{\\mathds{p}}}(l_{0/1}({\\widehat{y}}_{s_j , i},y_i)l_{0/1}({\\widehat{y}}_{s_j,{i'}},y_{i'})=1|s_j , i , i')\\\\ = & { \\operatorname{\\mathds{p}}}({\\widehat{y}}_{s_j , i}=0,y_i=1,{\\widehat{y}}_{s_j , i'}=0,y_{i'}=1|s_j , i , i ' ) \\\\   & + { \\operatorname{\\mathds{p}}}({\\widehat{y}}_{s_j , i}=0,y_i=1,{\\widehat{y}}_{s_j , i'}=1,y_{i'}=0|s_j , i , i')\\\\   & + { \\operatorname{\\mathds{p}}}({\\widehat{y}}_{s_j , i}=1,y_i=0,{\\widehat{y}}_{s_j , i'}=0,y_{i'}=1|s_j , i , i')\\\\   & + { \\operatorname{\\mathds{p}}}({\\widehat{y}}_{s_j , i}=1,y_i=0,{\\widehat{y}}_{s_j , i'}=1,y_{i'}=0|s_j , i , i')\\\\ = & { \\operatorname{\\mathds{p}}}(\\varpsi_i<-\\sqrt{n_1}\\zeta_i,\\varpsi_{i'}<-\\sqrt{n_1}\\zeta_{i'}){\\operatorname{\\mathds{p}}}(y_i=1){\\operatorname{\\mathds{p}}}(y_{i'}=1)\\\\   & + { \\operatorname{\\mathds{p}}}(\\varpsi_i<-\\sqrt{n_1}\\zeta_i,\\varpsi_{i'}\\ge-\\sqrt{n_1}\\zeta_{i'}){\\operatorname{\\mathds{p}}}(y_i=1){\\operatorname{\\mathds{p}}}(y_{i'}=0)\\\\   & + { \\operatorname{\\mathds{p}}}(\\varpsi_i\\ge-\\sqrt{n_1}\\zeta_i,\\varpsi_{i'}<-\\sqrt{n_1}\\zeta_{i'}){\\operatorname{\\mathds{p}}}(y_i=0){\\operatorname{\\mathds{p}}}(y_{i'}=1)\\\\   & + { \\operatorname{\\mathds{p}}}(\\varpsi_i\\ge-\\sqrt{n_1}\\zeta_i,\\varpsi_{i'}\\ge-\\sqrt{n_1}\\zeta_{i'}){\\operatorname{\\mathds{p}}}(y_i=0){\\operatorname{\\mathds{p}}}(y_{i'}=0)\\\\ = & { \\operatorname{\\mathds{p}}}(\\varpsi_i<-\\sqrt{n_1}\\zeta_i,\\varpsi_{i'}<-\\sqrt{n_1}\\zeta_{i'}){\\operatorname{\\mathds{p}}}(y_i=1){\\operatorname{\\mathds{p}}}(y_{i'}=1)\\\\   & + { \\operatorname{\\mathds{p}}}(\\varpsi_i<-\\sqrt{n_1}\\zeta_i,-\\varpsi_{i'}\\le\\sqrt{n_1}\\zeta_{i'}){\\operatorname{\\mathds{p}}}(y_i=1){\\operatorname{\\mathds{p}}}(y_{i'}=0)\\\\   & + { \\operatorname{\\mathds{p}}}(-\\varpsi_i\\le\\sqrt{n_1}\\zeta_i,\\varpsi_{i'}<-\\sqrt{n_1}\\zeta_{i'}){\\operatorname{\\mathds{p}}}(y_i=0){\\operatorname{\\mathds{p}}}(y_{i'}=1)\\\\   & + { \\operatorname{\\mathds{p}}}(-\\varpsi_i\\le\\sqrt{n_1}\\zeta_i,-\\varpsi_{i'}\\le\\sqrt{n_1}\\zeta_{i'}){\\operatorname{\\mathds{p}}}(y_i=0){\\operatorname{\\mathds{p}}}(y_{i'}=0)\\\\ = & ~f_{\\varpsi_i,\\varpsi_{i'}}(-\\sqrt{n_1}\\zeta_i,-\\sqrt{n_1}\\zeta_{i'})p_ip_{i ' }    + f_{\\varpsi_i,-\\varpsi_{i'}}(-\\sqrt{n_1}\\zeta_i,\\sqrt{n_1}\\zeta_{i'})p_i(1-p_{i'})\\\\ & + f_{-\\varpsi_i,\\varpsi_{i'}}(\\sqrt{n_1}\\zeta_i,-\\sqrt{n_1}\\zeta_{i'})(1-p_i)p_{i ' }   + f_{-\\varpsi_i,-\\varpsi_{i'}}(\\sqrt{n_1}\\zeta_i,\\sqrt{n_1}\\zeta_{i'})(1-p_i)(1-p_{i ' } ) , \\end{split}\\ ] ] completing the proof ."
  ],
  "abstract_text": [
    "<S> an important question in constructing cross validation ( cv ) estimators of the generalization error is whether rules can be established that allow `` optimal '' selection of the size of the training set , for fixed sample size @xmath0 . </S>",
    "<S> we define the _ resampling effectiveness _ of random cv estimators of the generalization error as the ratio of the limiting value of the variance of the cv estimator over the estimated from the data variance . the variance and the covariance of different average test set errors </S>",
    "<S> are independent of their indices , thus , the resampling effectiveness depends on the correlation and the number of repetitions used in the random cv estimator . </S>",
    "<S> we discuss statistical rules to define optimality and obtain the `` optimal '' training sample size as the solution of an appropriately formulated optimization problem . </S>",
    "<S> we show that in a broad class of loss functions the optimal training size equals half of the total sample size , independently of the data distribution . </S>",
    "<S> we optimally select the number of folds in @xmath1-fold cross validation and offer a computational procedure for obtaining the optimal splitting in the case of classification ( via logistic regression ) . </S>",
    "<S> we substantiate our claims both , theoretically and empirically .    </S>",
    "<S> example.eps gsave newpath 20 20 moveto 20 220 lineto 220 220 lineto 220 20 lineto closepath 2 setlinewidth gsave .4 setgray fill grestore stroke grestore </S>"
  ]
}