{
  "article_text": [
    "in geosciences , principal component analysis ( pca ) has been an essential and powerful tool at detecting spatial structures amongst time series recorded at different locations .",
    "pca aims at building a decorrelated representation of the data and it finds the spatial patterns responsible for the largest proportion of variability @xcite .",
    "pca can also be viewed as a dimensionality reduction technique that extracts the most relevant components of data , i.e. the ones that maximize the variances .",
    "hence , second order moments are the foundation of pca . but relying exclusively on second moments implies that pca is only optimal when applied to multivariate gaussian vectors .",
    "although rarely stated and even more rarely checked , this underlined normality assumption is not always satisfied in practice .",
    "recently , different approaches have been tested to extend the applicability of pca in geosciences .",
    "in particular , nonlinear pca ( nlpca ) has been applied to several geophysical datasets ( e.g. , * ? ? ? * ; * ? ? ? * ) . in the nlpca algorithm ,",
    "data are considered as the input of an auto - associative neural network with five layers , with a bottleneck in the third layer @xcite . through the minimization of a cost function ,",
    "the output is forced to be as close as possible to the input , and the bottleneck layer is a low dimensional representation of the input .",
    "since this neural network is nonlinear , nlpca goes automatically beyond correlations . however , nlpca suffers the intrinsic limitations of multilayered networks ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ) : it is computationally expensive and does not always converge to a global solution . to overcome these difficulties ,",
    "we pursue a less ambitious aim : instead of trying to find decompositions that can explain the entire body of data with respect to a criterion , we focus on the part of data responsible for large anomalous behaviors .",
    "in contrast to pca , our approach tends to give maximal weight to data points which largely deviate from the mean , and to find the corresponding representative spatial patterns , i.e. the directions along which such points are prominently distributed .",
    "the key element in our procedure is the expansion of the cumulant function that can provide information beyond the first two moments . by maximizing the cumulant function @xcite over growing hyperspheres in the data space , a set of components",
    "can be derived .",
    "we first apply our procedure to three types of multivariate random vectors ( normal , skew - normal , gamma ) .",
    "then we demonstrate in the general case that if a direction exists for which the marginal probability density of projected data display a larger tail than in all other directions , then our procedure is able to select that direction .    when finding the first component , we show that pca is a special case of our approach whenever the gaussian assumption is satisfied . besides the gaussian case we show that , for any probability density , the solution derived from the centered cumulant function can be transformed to the first principal component by decreasing the radius of the hypersphere .",
    "other principal components could be found as well , by a generalization of the proposed method .",
    "our method is computationally cheap , and the solutions are found in the form of unit ( normalized ) vectors , as in the case of pca , allowing a unidimensional projection with an easy geometrical interpretation .    in summary",
    ", this paper focuses on the problem of characterizing spatial patterns associated to large anomalies , i.e. large deviations from the sample mean , when the data set under study can not be assumed to be normally distributed .",
    "in the univariate case , the cumulant function of the random variable @xmath0 with finite moments is defined as the following scalar function @xmath1\\big\\}= \\sum_{n=1}^{\\infty}\\kappa_n    { s^n \\over n!},\\ ] ] where @xmath2 , @xmath3 represents the mean function and the scalar @xmath4 corresponds to the @xmath5 cumulant of @xmath0 .",
    "the first two cumulants @xmath6 and @xmath7 are simply the mean and the variance of @xmath0 , respectively .",
    "the third and fourth cumulants are classically called the skewness and the kurtosis parameters . concerning the existence of cumulants , we assume in this paper that all the cumulant coefficients are finite and that the cumulant function is always well defined . the cumulant function and its coefficients",
    "have many interesting properties .",
    "for example , if @xmath0 and @xmath8 are two independent random variables then the @xmath5 cumulant of the sum @xmath9 is equal to the sum of the @xmath5 cumulant of @xmath0 and the @xmath5 cumulant of @xmath8 for any integers @xmath10 .",
    "if @xmath0 follows a gaussian distribution , then all but the first two cumulants are equal to zero . in a multivariate framework , the cumulant function of the random vector @xmath11 is simply defined as @xmath12\\big\\ } \\mbox { , for all   $ { \\mathbf s}^t = ( s_1 , \\dots , s_m ) \\in { \\mathbb r}^m$.}\\ ] ] as in the univariate case , the linear and gaussian properties associated to the cumulant function defined by ( [ eq : g def ] ) still hold , but the cumulant coefficients formulas are more cumbersome to write down in a multivariate framework . for more information about cumulants , we refer the reader to @xcite .    to identify possible favorite projection directions with respect to the multivariate cumulant function , we first rewrite the vector @xmath13 in ( [ eq : g def ] ) as the product @xmath14 , where @xmath15 , the scalar @xmath16 represents the norm (  radius \" ) of @xmath17 and @xmath18 is the unit ",
    "angular / direction \" vector defined as @xmath19 ( note that @xmath20 ) .",
    "secondly , the cumulant function for our vector @xmath11 projected along the direction vector @xmath18 is introduced as @xmath21 \\big\\ } , \\nonumber\\\\ & = & \\sum_{n=1}^\\infty k_n(\\mbox{\\boldmath \\(\\theta\\)})\\frac { |{\\mathbf s}|^n}{n!}.\\end{aligned}\\ ] ] our algorithmic strategy is to maximize the cumulant function at fixed non - small @xmath22 with respect to the angular component @xmath18 that varies over an unit hypersphere .",
    "practically , we have to find the optimal @xmath23 directions defined by @xmath24 , \\mbox { for any $ |{\\mathbf s}|$.}\\ ] ] if the radius @xmath22 is small enough , and the mean @xmath25 is zero , the variance @xmath26 dominates the centered cumulant function and the contributions of other cumulants can be neglected .",
    "in this situation , finding the first pca component can be viewed as a special case of this optimization procedure , because maximizing the cumulant function for small @xmath22 is equivalent to maximize the variance .",
    "as the value of @xmath22 grows , higher and higher order cumulants become more dominant .",
    "our main goal is to find @xmath23 in ( [ eq : max pb ] ) for the largest admissible @xmath22 and study their properties .",
    "we will call the solutions of such an optimization scheme the _ maxima of the cumulant function _ ( mcf ) directions .",
    "we anticipate that , since the scalar product @xmath27 is invariant under orthogonal transformations , the cumulant function is invariant as well .",
    "given that the unit hypersphere is also invariant , our algorithm is symmetric respect to orthogonal transformations .",
    "for instance , if data vectors are rotated by a given angle , the solutions of the algorithm , in terms of @xmath18 , are rotated by the same amount .",
    "this symmetry implies that if the probability density is isotropic , then the cumulant function is isotropic as well , and no relative maxima of @xmath28 exist on the unit hypersphere . in that case",
    "no directions are selected : for a rotationally symmetric distribution there is indeed no preferred direction along which anomalies are prominent , they are distributed uniformly over all angles .    to illustrate our optimization procedure we derive explicit cumulant maximization schemes for three special cases of multivariate family distributions ,",
    "see section [ sec : theory ] . for assessing the outputs of our algorithm",
    "when applied to real data , we refer the reader to the second part of this paper @xcite ) .",
    "to study the properties of the maximization method developed in section [ sec : cumul fct ] , three examples of distribution functions are considered in this paper .",
    "we choose these three families because explicit results can be derived and they have been classically used in the statistical modeling of temperatures and precipitation data .    without loss of generality ,",
    "some relevant matrices are diagonal thereafter . however , since the solutions are invariant respect to orthogonal transformations , they may be rotated along with the corresponding coordinate change .",
    "analytical calculations are performed for the general multivariate case , while figures are given for the bivariate case .",
    "suppose that the data at hand can be appropriately fitted by a multivariate gaussian vector .",
    "we assume that the observations have been centered ( zero ) mean and we denote the covariance matrix as @xmath29 .",
    "the cumulant function of the centered gaussian vector ( e.g. , * ? ? ?",
    "* ) is equal to @xmath30\\big\\ } =    { 1 \\over 2 } { \\bf s}^t \\sigma { \\bf s }   .\\ ] ] hence , it is easy to show that all cumulants but the second are equal to zero .",
    "the decomposition in equation ( [ eq : projg def ] ) , @xmath31 , implies that the cumulant function becomes @xmath32 our optimization problem is to maximize @xmath33 under the constraint @xmath34 . to find the optimal @xmath23 defined by ( [ eq : max pb ] )",
    ", we introduce a function to be maximized , constrained by the lagrange multiplier @xmath35 , as @xmath36 setting the gradient with respect to @xmath18 to zero gives @xmath37 let @xmath38 be the largest eigenvalue of the covariance matrix @xmath39 and @xmath40 its associated eigenvector . introducing @xmath41 , we can write @xmath42 . consequently , @xmath40 is the solution of ( [ eq : eigen ] ) .",
    "note that @xmath40 depends on @xmath29 but not on @xmath43 .",
    "the optimal direction , for the gaussian case , is @xmath44 , and it corresponds to the classical first principal component .    to illustrate this result , a bivariate vector of the normal distribution is presented in figure [ nfig ] ( in which a contour plot is drawn in logarithmic scale ) . the matrix @xmath45 is assumed to be diagonal , with entries 1.2 and 0.5143 .     and @xmath46 ( entries of the diagonal covariance matrix )",
    ". the first principal component is shown ( pc1 ) , together with the two ( opposite ) maxima of the cumulant function ( mcf1 and mcf2 ) .",
    "all vectors are in arbitrary scale .",
    "the maxima of cumulant function are parallel to the first principal component , all pointing towards the large anomalies , in terms of high probability ( at fixed vector norm ) .",
    "probability contours are @xmath47,@xmath48,@xmath49    the first principal component ( pc1 ) , corresponding to the eigenvalue 1.2 is horizontal , while the second principal component , corresponding to the eigenvalue 0.5143 is vertical , and is not displayed .",
    "the two maxima of the cumulant function ( mcf1 and mcf2 ) are just the positive and negative part of pc1 . indeed , both @xmath50 and @xmath51 are solutions of ( [ eq : eigen ] ) .",
    "while this is always true for pca , the maxima of the cumulant function may in general neither be parallel nor orthogonal .",
    "pc1 is indeed the direction along which large anomalies are ditributed in the gaussian case . in order to derive pc2 and other higher principal components from the cumulant function",
    ", one would have to determine not only its maxima , but also its minima and saddle points .      to introduce skewness to the gaussian density , while keeping some of valuable properties of the normal distribution , azzalini and his co - authors ( e.g * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) have extended the normal density to a larger class called the skew - normal ( sn ) density , that is defined as @xmath52 where @xmath53 is a multivariate normal probability density function with zero mean and covariance matrix @xmath29 , and @xmath54 is the cumulative density function of an univariate gaussian random variable with zero mean and unit variance .",
    "the vector @xmath55 corresponds to the degree of skewness .",
    "when @xmath56 , there is no skewness , and the sn distribution reduces to the gaussian case . from ( [ sndist ] )",
    ", it is possible to derive the cumulant function ( e.g * ? ? ?",
    "* ) of a sn vector @xmath30\\big\\ } =    { 1 \\over 2 } { \\bf s}^t \\sigma { \\bf s } + \\log \\left ( 2 \\phi({\\scriptstyle \\sqrt{\\frac{\\pi}{2 } } }   \\mbox{\\boldmath $ \\mu$}^t { \\bf s } ) \\right).\\ ] ] where @xmath57 represents the mean vector , and is equal to @xmath58 note that the covariance matrix of the sn distribution is not @xmath29 but @xmath59 @xcite .",
    "a bivariate example of the sn distribution is presented in figure [ snfig ] , in which a contour plot is drawn in logarithmic scale .",
    "the matrix @xmath29 is chosen to be diagonal with entries @xmath60 and @xmath46 .",
    "the skewness vector @xmath55 is taken to be equal to @xmath61 .",
    "the distribution in figure [ snfig ] has been centered such that the mean vector is zero . in the bivariate example",
    ", @xmath62 is @xmath63 .     and",
    "the matrix @xmath29 is chosen to be diagonal with entries @xmath60 and @xmath46 .",
    "the first principal component is shown ( pc1 ) , together with the two maxima of the cumulant function ( mcf1 and mcf2 ) .",
    "all vectors are in arbitrary scale . in this case , the two maxima of the cumulant function are not related to the first principal component , and point towards the ( local ) large anomalies , in terms of high probability ( at fixed , and large , vector norm ) .",
    "probability contours are @xmath47,@xmath48,@xmath64    from the sn cumulant function @xcite , we can write the cumulant function of the centered vector @xmath65 ) as @xmath66\\ ] ] while it is not possible to find explicit solutions of the maximization problem defined by ( [ eq : max pb ] ) for ( [ genfunsn ] ) , one can provide valuable approximated solutions for both small and large @xmath22 . in the former case , the following two taylor expansions are the key elements to derive our results @xmath67 @xmath68 where erf corresponds to the error function defined by + @xmath69 then we can write the following approximation @xmath70    & = & \\mbox{log}\\big[1+\\mbox{erf}({\\scriptstyle \\frac{\\sqrt{\\pi}}{2}}|{\\mathbf    s}|\\mbox{\\boldmath $ \\mu$}^t\\mbox{\\boldmath \\(\\theta\\)})\\big ]   \\\\    &   =   & \\mbox{log}\\big[1+|{\\mathbf    s}|\\mbox{\\boldmath $ \\mu$}^t\\mbox{\\boldmath \\(\\theta\\)}+o(|{\\mathbf    s}|^3)\\big ] \\mbox { , by ( \\ref{log(1+s ) } ) } , \\\\    & = & |{\\mathbf    s}|\\mbox{\\boldmath $ \\mu$}^t\\mbox{\\boldmath \\(\\theta\\)}-{|{\\mathbf s}|^2",
    "\\over 2}\\mbox{\\boldmath \\(\\theta\\)}^t\\mbox{\\boldmath $ \\mu$}\\mbox{\\boldmath    $ \\mu$}^t\\mbox{\\boldmath \\(\\theta\\)}+o(|{\\mathbf s}|^3 ) \\mbox { , by ( \\ref{erf})}.\\end{aligned}\\ ] ] from equation ( [ genfunsn ] ) , it follows that the cumulant function @xmath33 is approximately equal to @xmath71 as previously noticed , the matrix @xmath72@xmath73@xmath74 represents the covariance of the sn distribution @xcite .",
    "hence , the maximization of the right hand side of ( [ genfunsnsmalls ] ) is equivalent to solving the system defined by ( [ eq : eigen ] ) , but instead of working with @xmath29 , we just need to replace @xmath29 by @xmath72@xmath73@xmath74 in ( [ eq : eigen ] ) .",
    "consequently , the solution to maximize the sn cumulant function in the neighborhood of zero is the largest eigenvector of the matrix @xmath72@xmath73@xmath74 , i.e. the pc1 of the sn covariance matrix .    for large @xmath22 , this result does not hold and different directions are obtained .",
    "we need to recall the asymptotic expansion of the error function @xmath75 & \\mbox { , as $ s\\downarrow-\\infty$ , } \\\\                  2   & \\mbox { ,   as $ s\\uparrow+\\infty$. }          \\end{array }           \\right.\\ ] ] if @xmath76 , the logarithm in equation ( [ genfunsn ] ) can be expanded as @xmath70    & = & \\mbox{log}\\big[1+\\mbox{erf}({\\scriptstyle \\frac{\\sqrt{\\pi}}{2}}|{\\mathbf    s}|\\mbox{\\boldmath $ \\mu$}^t\\mbox{\\boldmath \\(\\theta\\)})\\big]\\\\ & \\simeq & -\\frac{\\pi}{2}\\frac{|{\\mathbf    s}|^2}{2}\\mbox{\\boldmath \\(\\theta\\)}^t \\mbox{\\boldmath $ \\mu$}\\mbox{\\boldmath $ \\mu$}^t\\mbox{\\boldmath \\(\\theta\\)}+o\\big(\\mbox{log}(|{\\mathbf s}|^{-1})\\big).\\end{aligned}\\ ] ] in this case , we have @xmath77 the direction that maximizes @xmath78 is again a eigenvector , but this time is the eigevector of the matrix @xmath79 , pointing towards @xmath62@xmath80 .    if @xmath81 , we have @xmath82\\simeq     \\left\\ { \\begin{array}{ll }     \\log 2 & \\mbox { , $ \\mbox{\\boldmath $ \\mu$}^t\\mbox{\\boldmath \\(\\theta\\ ) } > 0$},\\\\     0 & \\mbox { , if $ \\mbox{\\boldmath $ \\mu$}^t\\mbox{\\boldmath \\(\\theta\\)}=0$}.\\\\    \\end{array } \\right.\\ ] ] then , the cumulant function @xmath33 can be approximated by @xmath83 and maximizes by the largest eigenvector of @xmath29 , pointing towards @xmath62@xmath84 .    in summary ,",
    "depending on the size of @xmath22 ( small or large ) and the sign @xmath62@xmath85 ( positive or negative ) , the solutions of equation ( [ eq : max pb ] ) for the sn distribution can be viewed as the largest eigenvectors of three different matrices , @xmath86@xmath62@xmath62@xmath74 , @xmath29 and @xmath87@xmath62@xmath62@xmath74 .    for the bivariate example of figure [ snfig ] , the pc1 is shown ( in arbitrary scale ) , explaining 60% of the variance .",
    "the second pc ( 40% of the variance ) is orthogonal to pc1 and is not displayed .",
    "both maxima of the cumulant function for large @xmath22 , denoted as mcf1 and mcf2 ( respectively for @xmath62@xmath88 and @xmath62@xmath89 ) , are presented in figure [ snfig ] for the bivariate example ( same scale as pc1 ) .",
    "the two local maxima point towards the large anomalies of the distribution : this can be seen by noting that a point at the upper - right end of pc1 corresponds to a small probability , and hence is less likely to be found , than a point at the right end of mcf1 ( the two points being of equal norm ) . similarly , a point at the down - left end of pc1 is less likely , in probability , than a point at the down end of mcf2 .      this section investigates the multivariate gamma distribution defined by cheriyan and ramabhadran ( see * ? ? ?",
    "each component of the data vector @xmath90 is distributed following a gamma distribution , and the components depend each other by means of an auxiliary variable @xmath91 .",
    "the joint distribution is @xmath92 where @xmath93 is a gamma distribution , i.e. @xmath94 for @xmath95 , equal to zero otherwise .",
    "a bivariate example is presented in figure [ gfig ] , with @xmath96 , @xmath97 and @xmath98 in ( [ gampdf ] ) .    , @xmath99 , @xmath98 .",
    "the first principal component is shown ( pc1 ) , together with the maximum of the cumulant function ( mcf1 ) .",
    "all vectors are in arbitrary scale . again",
    ", the maximum of the cumulant function is different from the first principal component , and points towards the large anomalies , in terms of high probability ( at fixed , and large , vector norm ) .",
    "probability contours are @xmath100,@xmath47,@xmath101    the cumulant function for this multivariate gamma distribution can be written @xcite as @xmath102\\big\\}=-\\alpha_0\\mbox{log}\\big(1-\\sum_{i=1}^n s_i\\big )     - \\sum_{i=1}^n\\alpha_i\\mbox{log}(1-s_i)\\ ] ] and the mean of the @xmath103 component is @xmath104 . by replacing @xmath105 by @xmath106 , the cumulant function of the centered vector @xmath107 can be written as @xmath108 for small @xmath22 , the logarithms are approximated by the truncated taylor expansions , i.e. @xmath109 and @xmath110 it follows that the cumulant function can be approximated by @xmath111 where the covariance matrix @xmath112 is defined by @xcite @xmath113 hence , for small @xmath22 , the solution of ( [ eq : max pb ] ) is the classical pc1 eigenvector of the covariance matrix @xmath112 associated with the largest eigenvalue .",
    "it is plotted in figure [ gfig ] for the bivariate example .",
    "the negative part of pc1 is not displayed , as well as the second principal component which is just orthogonal to the first .    for large @xmath22 , the cumulant function",
    "is not defined , since the logarithms in equation ( [ genfungam ] ) must have positive arguments , for all unit vectors @xmath18 . in particular",
    ", the following two inequalities have to be satisfied @xmath114 however , we take the largest allowed value of @xmath22 , which is considered as a valid limit .",
    "since @xmath18 is a unit vector , the maximum of each component @xmath115 is @xmath116 , which holds when all the other components are zero , while the maximum for the sum @xmath117 is @xmath118 , which holds when @xmath119 .",
    "the largest allowed value of @xmath22 is then @xmath120 : in that case @xmath28 remains finite for all @xmath18 s , except for @xmath119 , where it diverges due to the first logarithm of equation ( [ genfungam ] ) ( all the others remain finite ) . for larger values of @xmath22 , @xmath28 diverges over subspaces larger than a single point .",
    "hence the boundary case @xmath121 is taken as representative for a `` large '' @xmath22 limit , and the point @xmath119 is taken as the maximum of the cumulant function .",
    "for the bivariate example , the maximum is plotted in figure [ gfig ] , denoted as mcf1 , and scaled to pc1 .",
    "note that for high values of the probability distribution ( e.g. the smallest contour ) , pc1 seems a representative direction of the egg - like shape of the distribution , but for low probabilities it becomes clear that the line @xmath122 is responsible for the large deviations .",
    "a point at the end of pc1 is indeed less probable than a point at the end of mcf1 .",
    "this result is understood by noting that the joint density ( [ gampdf ] ) is the probability distribution of variables @xmath123 defined as @xmath124 , where the variables @xmath125 are independent and gamma distributed with parameters @xmath126 .",
    "hence , a large deviation of @xmath127 , which occurs independently on others @xmath91 s , corresponds to a large deviation of @xmath128 which is placed on average along the line @xmath129 ( that corresponds to @xmath130 for the cumulant function ) .",
    "we have seen that the multivariate cumulant function reduces to the variance if the radius @xmath22 tends to zero . in that case",
    ", its maxima corresponds to the first principal component of data set .",
    "if @xmath22 grows , higher order cumulants come into play , but is not clear what the corresponding maxima represent . in order to clarify this point , we rewrite the cumulant function defined by ( [ eq : projg def ] ) in terms of the explicit integral over the probability density @xmath131 this expression can be reduced to an unidimensional integral , by defining the projected data as @xmath132 , and the marginal probability density of the projected data , @xmath133 .",
    "the cumulant function is then    @xmath134    which corresponds to the cumulant function of the univariate vector @xmath132 with distribution density @xmath133 . in the light of this representation ,",
    "our maximization procedure is better understood : we are looking for directions @xmath135 that correspond to a marginal probability density @xmath133 displaying maximal cumulant function , at fixed @xmath22 .",
    "we want to demonstrate that if @xmath22 grows , a larger cumulant function corresponds to a marginal density with a fatter tail .",
    "hence our procedure selects the directions corresponding to the marginal densities with fatter tails , where the anomalous behaviour is expected .",
    "specifically , consider two different directions @xmath135 and @xmath135@xmath136 : we want to demonstrate that if the marginal distribution along @xmath135 has a fatter tail than the distribution along @xmath135@xmath136 , then the cumulant function has also a fatter tail along @xmath135 respect to @xmath135@xmath136 .",
    "more formally , we have the following theorem .",
    "let @xmath137 and @xmath138 be two directions .",
    "if there exists a real @xmath139 such that the density distribution @xmath133 of the random variable @xmath140 is strictly larger than the density @xmath141 for all @xmath142 , i.e. @xmath143 then there exists a radius @xmath144 such the cumulant function of @xmath140 and @xmath145 satisfies @xmath146    in order to prove the result , we start by noting that the following inequality holds @xmath147 >   \\exp(|{\\mathbf s}|z^*)\\big[f_{\\mbox{\\boldmath \\(\\theta\\)}}(z)-f_{\\mbox{\\boldmath \\(\\theta\\)}'}(z)\\big]\\ ] ] for all @xmath148 , where we have replaced the exponential function with its minimum value in the interval @xmath149",
    ". the inequality holds because the density difference @xmath150 is positive in this interval , by assumption .",
    "since the above inequality holds in the whole interval @xmath149 , it can be integrated over , i.e. @xmath151 dz > \\\\ \\;\\;\\;\\;\\;\\ ; \\exp(|{\\mathbf s}|z^*)\\int_{z^*}^{+\\infty } \\big[f_{\\mbox{\\boldmath \\(\\theta\\)}}(z)-f_{\\mbox{\\boldmath \\(\\theta\\)}'}(z)\\big ] dz.\\end{aligned}\\ ] ] given that the two densities are normalized , i.e.    @xmath152 we can rewrite the right hand side ( r.h.s . ) of the last inequality by inverting the integration order and the marginal densities as @xmath153 dz=\\nonumber \\\\",
    "\\exp(|{\\mathbf s}|z^*)\\int_{-\\infty}^{z^*}\\big[f_{\\mbox{\\boldmath \\(\\theta\\)}'}(z)-f_{\\mbox{\\boldmath \\(\\theta\\)}}(z)\\big ] dz\\nonumber\\end{aligned}\\ ] ] rearranging the four terms gives indeed the normalization condition times the exponential .",
    "note that since the integral in the left hand side ( l.h.s . )",
    "is , by assumption , positive , the integral in the r.h.s . must be positive as well , because they are multiplied by the same positive constant @xmath154 .    given the above inequalities , we only need to demonstrate that it exists an @xmath144 such that , for all @xmath155 , @xmath156 dz > \\\\",
    "\\int_{-\\infty}^{z^*}\\exp(|{\\mathbf s}|z)\\big[f_{\\mbox{\\boldmath \\(\\theta\\)}'}(z)-f_{\\mbox{\\boldmath \\(\\theta\\)}}(z)\\big ] dz\\nonumber \\end{aligned}\\ ] ] where the value of @xmath144 must be determined .",
    "note that even if the integral in the l.h.s .",
    "is positive , the density difference @xmath157 is not guaranteed to be positive for all @xmath158 .",
    "if the integrand was positive as well , equation ( [ in3 ] ) would hold trivially for all values of @xmath22 , because the exponential in the l.h.s .",
    "is always larger than that of r.h.s .",
    "this corresponds to the case in which , beside @xmath159 , we assume also @xmath160 .",
    "however , we do not need this additional request , and we just note that it would help our procedure by leaving the problem of using a large @xmath22 .    since the exponential is positive , we can rewrite equation ( [ in3 ] ) as @xmath161 dz > \\\\",
    "\\int_{-\\infty}^{z^*}\\exp\\big(|{\\mathbf s}|(z - z^*)\\big)\\big[f_{\\mbox{\\boldmath \\(\\theta\\)}'}(z)-f_{\\mbox{\\boldmath \\(\\theta\\)}}(z)\\big ] dz\\end{aligned}\\ ] ] for all @xmath162 .",
    "the integral in the l.h.s .",
    "is positive , as stated above , and is independent on @xmath22 , while the integral in the r.h.s .",
    "converges to zero for @xmath163 , as long as the density difference remains finite , because the exponential tends to zero in the whole interval @xmath158 .",
    "if we define @xmath144 as the largest possible value of @xmath22 for which the two integrals are equal , then for all @xmath164 the integral in the l.h.s . is larger than that of r.h.s .",
    "finally , we have for all @xmath162 @xmath165 > \\\\ \\int_{-\\infty}^{z^*}dz\\;\\exp(|{\\mathbf s}|z)\\big[f_{\\mbox{\\boldmath \\(\\theta\\)}'}(z)-f_{\\mbox{\\boldmath \\(\\theta\\)}}(z)\\big].\\end{aligned}\\ ] ] by rearranging terms , this is equivalent to @xmath166 , and the theorem is demonstrated .",
    "in this paper , we have introduced a novel method selecting the spatial patterns representative for the large deviations in the dataset .",
    "the method consists in finding the vectors in the space of data for which the cumulant function is maximal . as in the case of pca ,",
    "the spatial patterns are found as normalized directions in the space of data , and a linear projection can be performed , with an easy geometrical interpretation .",
    "if one is interested on the large deviations , the projection allows to safely perform extreme value analysis ( @xcite ) . in both cases the subspaces",
    "are ordered : in pca the order follows the fraction of variance of each subspace ; the maxima of the cumulant function are ordered by the value of @xmath28 . however , while pca accounts for the mass of the distribution , the cumulant function can cover the full range of the distribution .",
    "principal components are always symmetric , while large anomalous patterns , if generated by nonlinear processes , are expected to be neither specular nor orthogonal .",
    "accordingly , the maxima of the cumulant function are not necessarily symmetric , since they account for the whole structure of dependencies , and not only covariances .",
    "other nonsymmetric techniques , such as oblique varimax rotations , generally depend on the frame of reference : vector solutions do not covary with the space of data under orthogonal transformations .",
    "hence , solutions depend not only on the shape of the underlying probability distribution , but also on its orientation : an undesirable property for our purposes . the maxima of the cumulant function , instead , vary with the probability density whose shape is the only feature determining the maxima .    in the case of normally distributed data ,",
    "the maximization of cumulant function yields the first principal component for all values of @xmath167 : the elliptically symmetric distribution is characterized by the two tails along the major axis of the ellipse , i.e. the first principal component .",
    "when the method is applied to skew - normal and gamma distributions , for non - small @xmath167 , the maxima of the cumulant function determine large anomalies : high probability directions far from the center of mass .",
    "note that the limit radius @xmath168 is the innovative key from a technical point of view , allowing for analytical solutions . using the limit , we were also able to demonstrate that the solutions of our algorithm correspond , in general , to the directions along which the marginal probability density display the fattest tails .    using the cumulant function is computationally cheap",
    ", there is no free parameter , and has the advantage of searching for local solutions , all of which are of interest .",
    "when a solution is found , is always a good solution , in contrast with neural networks applications , for which it must be questioned if it is the global or just a local solution . in real applications",
    "( see * ? ? ?",
    "* ) , the radius @xmath167 must be taken as large as possible , until the expected error in the estimate of the cumulant function , due to the finite sample , reach a tolerance value ( see * ? ? ?",
    "this corresponds to maximize a combination of cumulants which is of the highest reliable order with the given amount of data , accounting for the available set of anomalies .",
    "the solutions of our algorithm are expected to transform continuously as the radius @xmath167 varies .",
    "hence , even if the limit @xmath168 can not be taken in practice , the solutions for a finite value of @xmath167 are expected to represent a substantial departure from the pca solution , towards the formal solution at @xmath168 . from the theoretical point of view , future work could be devoted to studying more in detail the nature of solutions at varying @xmath167 .",
    "for instance , one could attempt to find under which conditions and to what extent the solutions are in between the pca solution and the formal solution at infinite @xmath167 . from the applicative point of view",
    ", we expect several datasets ( e.g. * ? ? ?",
    "* ) to be fruitfully analyzed with our new method .",
    "note that the logarithm is taken for illustrative purposes : the moment generating function could be used instead of the cumulant function , since the maximization is invariant under application of a monotonous function .",
    "the present definition is however confortable in avoiding extremely large numbers .",
    "centering of data about the mean is also a practical step , related with the constraint of dealing with finite samples : if the limit @xmath168 could be really taken , the mean would be irrelevant .",
    "results of our procedure are corrupted if variables are standardized by a rescaling , since the relative scale of different directions is the key in detecting anomalies and comparing the size of tails .",
    "if variables are standardized , our procedure reduce to a special case of independent component analyisis ( ica , see @xcite ) , detecting independent components rather than large anomalies .",
    "results are also corrupted if we try to get an empirical estimate of the cumulant function when the underlying probability density decays less than exponentially fast . in that case , the cumulant function diverge , and the variance of the empirical estimate increases with the size of the sample ( @xcite ) , implying that the estimate is always unreliable .",
    "this work was supported by the european e2-c2 grant , the national science foundation ( grant : nsf - gmc ( atm-0327936 ) ) , by the weather and climate impact assessment science initiative at the national center for atmospheric research ( ncar ) and the anr - assimilex project .",
    "the authors would also like to credit the contributors of the r project .",
    "finally , we would like to thank isabella bordi , marcello petitta and alfonso sutera for valuable discussions .",
    "azzalini a. , capitanio a.(1999 ) .",
    "statistical applications of the multivariate skew - normal distribution , _",
    "j.roy.stat.soc._ , b61:579 - 602 .",
    "azzalini a. , dalla valle a.(1996 ) .",
    "the multivariate skew - normal distribution , _ biometrika _ , 83:715 - 726 .",
    "bernacchia a , naveau p , yiou p. and vrac m. ( 2007 ) . detecting spatial patterns with the cumulant function ii : an application to el nino , _ nonlinear process .",
    "_ , submitted .",
    "christiansen b. ( 2005 ) . the shortcomings of nonlinear principal component analysis in identifying circulation regimes , _",
    "j.climate_ , 18:4814 - 4823 .",
    "coles s. ( 2001 ) .",
    "an introduction to statistical modeling of extreme values , springer , berlin gonzalez - farias g. , dominguez - molina a. , gupta a.k .",
    "additive properties of skew - normal random vectors , _",
    "j.stat.plan.infer._ , 126:521 - 534 .",
    "hsieh w.w .",
    "nonlinear multivariate and time series analysis by neural network methods , _",
    "rev.geophys._ , 42 , rg1003 , doi : 1010.1029 / 2002rg000112233 - 243 .",
    "hyvarinen a , oja e ( 2000 ) .",
    "independent component analysis : algorithms and applications , _ neural networks _ , 13:411 - 430 kenney j.f . , keeping e.s .",
    "cumulants and the cumulant - generating function , in mathematics of statistics , princeton , nj .",
    "kotz s. , balakrishnan n. , johnson n.h .",
    "continuous multivariate distributions , john wiley and sons , new york kramer m.a .",
    "nonlinear principal component analysis using autoassociative neural networks , _",
    "j.amer.inst.chem.eng._ , 37:233 - 243 .",
    "malthouse e.c .",
    ". limitations of nonlinear pca as performed with generic neural networks , _ ieee trans.nn_ , 9 : 165 - 173 .",
    "monahan a.h . , pandolfo l. , fyfe j.c .",
    "the preferred structure of variability of the northern hemisphere atmospheric circulation , _ geophys.res.lett._ , 28:1019 - 1022 .",
    "rencher a.c .",
    "multivariate statistical inference and applications , john wiley and sons , new york sornette d. ( 2000 ) .",
    "critical phenomena in natural sciences , springer - verlag , berlin"
  ],
  "abstract_text": [
    "<S> in climate studies , detecting spatial patterns that largely deviate from the sample mean still remains a statistical challenge . </S>",
    "<S> although a principal component analysis ( pca ) , or equivalently a empirical orthogonal functions ( eof ) decomposition , is often applied on this purpose , it can only provide meaningful results if the underlying multivariate distribution is gaussian . </S>",
    "<S> indeed , pca is based on optimizing second order moments quantities and the covariance matrix can only capture the full dependence structure for multivariate gaussian vectors . </S>",
    "<S> whenever the application at hand can not satisfy this normality hypothesis ( e.g. precipitation data ) , alternatives and/or improvements to pca have to be developed and studied .    </S>",
    "<S> to go beyond this second order statistics constraint that limits the applicability of the pca , we take advantage of the cumulant function that can produce higher order moments information . this cumulant function , well - known in the statistical literature , allows us to propose a new , simple and fast procedure to identify spatial patterns for non - gaussian data . </S>",
    "<S> our algorithm consists in maximizing the cumulant function . to illustrate our approach , </S>",
    "<S> its implementation for which explicit computations are obtained is performed on three family of of multivariate random vectors . </S>",
    "<S> in addition , we show that our algorithm corresponds to selecting the directions along which projected data display the largest spread over the marginal probability density tails . </S>"
  ]
}