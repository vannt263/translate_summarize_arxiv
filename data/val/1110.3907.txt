{
  "article_text": [
    "boosting is successful for both binary and multi - class classification  @xcite . among those popular variants",
    ", we are particularly focusing on logitboost  @xcite in this paper .",
    "originally , logitboost is motivated by statistical view  @xcite , where boosting algorithms consists of three key components : the loss , the function model , and the optimization algorithm . in the case of logitboost , these are the logit loss , the use of additive tree models , and a stage - wise optimization , respectively",
    ". there are two important factors in the logitboost setting .",
    "firstly , the posterior class probability estimate must be normalised so as to sum to one in order to use the logit loss .",
    "this leads to a coupled classifier output , _",
    "i.e. _ , the sum - to - zero classifier output .",
    "secondly , a dense hessian matrix arises when deriving the tree node split gain and node value fitting .",
    "it is challenging to design a tractable optimization algorithm that fully handles both these factors .",
    "consequently , some simplification and/or approximation is needed .",
    "@xcite proposes a `` one scalar regression tree for one class '' strategy .",
    "this breaks the coupling in the classifier output so that at each boosting iteration the model updating collapses to @xmath0 independent regression tree fittings , where @xmath0 denotes the number of classes . in this way",
    ", the sum - to - zero constraint is dropped and the hessian is approximated diagonally .",
    "unfortunately , friedman s prescription turns out to have some drawbacks .",
    "a later improvement , abc - logitboost , is shown to outperform logitboost in terms of both classification accuracy and convergence rate  @xcite .",
    "this is due to abc - logitboost s careful handling of the above key problems of the logitboost setting . at each iteration ,",
    "the sum - to - zero constraint is enforced so that only @xmath1 scalar trees are fitted for @xmath1 classes .",
    "the remaining class  called the base class  is selected adaptively per iteration ( or every several iterations ) , hence the acronym abc ( adaptive base class ) .",
    "also , the hessian matrix is approximated in a more refined manner than the original logitboost when computing the tree split gain and fitting node value .    in this paper , we propose two novel techniques to address the challenging aspects of the logitboost setting . in our approach , one vector tree is added per iteration .",
    "we allow a @xmath0 dimensional sum - to - zero vector to be fitted for each tree node .",
    "this permits us to explicitly formulate the computation for both node split gain and node value fitting as a @xmath0 dimensional constrained quadratic optimization , which arises as a subproblem in the inner loop for split seeking when fitting a new tree . to avoid the difficulty of a dense hessian",
    ", we propose that for * each * of these subproblems , only two coordinates ( _ i.e. _ , two classes or a class pair ) are adaptively selected for updating , hence the name aoso ( adaptive one vs one ) logitboost .",
    "figure  [ fig - tree - model ] gives an overview of our approach . in section  [ sec - subset sel ]",
    "we show that first order and second order approximation of loss reduction can be a good measure for the quality of selected class pair .    following the above formulation , abc - logitboost , although derived from a somewhat different framework in  @xcite , can thus be shown to be a special case of aoso - logitboost , but with a less flexible tree model . in section  [ sec - compabc ]",
    "we compare the differences between the two approaches in detail and provide some intuition for aoso s improvement over abc .",
    "the rest of this paper is organised as follows : in section  [ sec - algo ] we first formulate the problem setting for logitboost and then give the details of our approach . in section  [ sec - comp ] we compare our approach to ( abc-)logitboost . in section  [ sec - exp ] , experimental results in terms of classification errors and convergence rates are reported on a range of public datasets .",
    "we begin with the basic setting for the logitboost algorithm . for @xmath0-class classification ( @xmath2 ) , consider an @xmath3 example training set @xmath4 where @xmath5 denotes a feature value and @xmath6 denotes a class label .",
    "class probabilities conditioned on @xmath7 , denoted by @xmath8 , are learned from the training set . for a test example with known @xmath7 and unknown @xmath9",
    ", we predict a class label by using the bayes rule : @xmath10 .    instead of learning the class probability directly",
    ", one learns its `` proxy '' @xmath11 given by the so - called logit link function : @xmath12 with the constraint @xmath13  @xcite . for simplicity and without confusion ,",
    "we hereafter omit the dependence on @xmath14 for @xmath15 and for other related variables .",
    "the @xmath15 is obtained by minimizing a target function on training data : @xmath16 where @xmath17 is shorthand for @xmath18 and @xmath19 is the logit loss for a single training example : @xmath20 where @xmath21 if @xmath22 and @xmath23 otherwise .",
    "the probability @xmath24 is connected to @xmath25 via ( [ eq - link ] ) .    to make the optimization of ( [ eq - total loss ] ) feasible",
    ", a model is needed to describe how @xmath15 depends on @xmath7 .",
    "for example , linear model @xmath26 is used in traditional logit regression , while generalized additive model is adopted in logitboost : @xmath27 where each @xmath28 , a @xmath0 dimensional sum - to - zero vector , is learned by greedy stage - wise optimization .",
    "that is , at each iteration @xmath28 is added only based on @xmath29 . formally , @xmath30 this procedure repeats @xmath31 times with initial condition @xmath32 . owing to its iterative nature , we only need to know how to solve ( [ eq - fm ] ) in order to implement the optimization .      the @xmath33 in ( [ eq - fm ] ) is typically represented by @xmath0 scalar regression trees ( _ e.g. _ , in logitboost  @xcite or the real adaboost.mh implementation in @xcite ) or a single vector tree ( _ e.g. _ , the real adaboost.mh implementation in @xcite ) . in this paper , we adopt a single vector tree .",
    "we further restrict that it is a binary tree ( _ i.e. _ , only binary splits on internal node are allowed ) and the split must be vertical to coordinate axis , as in @xcite or @xcite .",
    "formally , @xmath34 where @xmath35 describes how the feature space is partitioned , while @xmath36 with a sum - to - zero constraint is the node values / vector associated with @xmath37 .",
    "see figure  [ fig - tree - model ] for an example .      solving ( [ eq - fm ] ) with the tree model ( [ eq - vector tree ] ) is equivalent to determining the parameters @xmath38 at the @xmath39-th iteration . in this subsection",
    "we will show how this problem reduces to solving a collection of convex optimization subproblems for which we can use any numerical method . following friedman s logitboost settings , here",
    "we use newton descent .",
    "also , we will show how the gradient and hessian can be computed incrementally .",
    "we begin with some shorthand notation for the _ node loss _ : @xmath40 where @xmath41 denotes the index set of the training examples on some either internal or terminal node ( _ i.e. _ , those falling into the corresponding region ) . minimizing ( [ eq - node loss ] )",
    "is the bridge to @xmath42 in that :    1 .   to obtain @xmath43 with given @xmath44",
    ", we simply take the minimizer of ( [ eq - node loss ] ) : @xmath45 where @xmath46 denotes the index set for @xmath47 .",
    "2 .   to obtain @xmath44 , we recursively perform binary split until there are @xmath48-terminal nodes .",
    "the key to the second point is to explicitly give the node split gain .",
    "suppose an internal node with @xmath49 training examples ( @xmath50 for the root node ) , we fix on some feature and re - index all the @xmath49 examples according to their sorted feature values .",
    "now we need to find the index @xmath51 with @xmath52 that maximizes the node gain defined as loss reduction after a division between the @xmath51-th and @xmath53-th examples : @xmath54 where @xmath55 , @xmath56 and @xmath57 ; @xmath58 , @xmath59 and @xmath60 are the minimizers of ( [ eq - node loss ] ) with index sets @xmath41 , @xmath61 and @xmath62 , respectively .",
    "generally , this searching applies to all features .",
    "the best division resulting to largest ( [ eq - node gain ] ) is then recorded to perform the actual node split .",
    "note that ( [ eq - node gain ] ) arises in the context of an @xmath63 outer loop , where @xmath64 is number of features .",
    "however , a nave summing of the losses for ( [ eq - node loss ] ) incurs an additional @xmath65 factor in complexity , which finally results in an unacceptable @xmath66 complexity for a single boosting iteration .",
    "a workaround is to use a newton descent method for which both the gradient and hessian can be incrementally computed .",
    "let @xmath67 , @xmath68 respectively be the @xmath69 gradient vector and @xmath70 hessian matrix at @xmath71 . by dropping the constant @xmath72 that is irrelevant to @xmath73 , the taylor expansion of ( [ eq - node loss ] ) w.r.t .",
    "@xmath73 up to second order is : @xmath74 by noting the additive separability of ( [ eq - node loss taylor ] ) and using some matrix derivatives , we have    [ cols=\"^,^ \" , ]     where the diagonal matrix @xmath75 .",
    "we then use ( [ eq - node loss taylor ] ) to compute the approximated node loss in ( [ eq - node gain ] ) .",
    "thanks to the additive form , both ( [ eq - taylor d1 ] ) and ( [ eq - taylor d2 ] ) can be incrementally / decrementally computed in constant time when the split searching proceeds from one training example to the next .",
    "therefore , the computation of ( [ eq - node loss taylor ] ) eliminates the @xmath65 complexity in the nave summing of losses . ) by simply setting the derivative to @xmath76 . here also , the computation can be incremental / decremental .",
    "since the loss design and adaboost.mh are not our main interests , we do not discuss this further .",
    "]      to minimise ( [ eq - node loss taylor ] ) , we give some properties for ( [ eq - node loss taylor ] ) that should be taken into account when seeking a solver .",
    "we begin with the sum - to - zero constraint .",
    "the probability estimate @xmath77 in the logit loss ( [ eq5-log loss ] ) must be non - zero and sum - to - one , which is ensured by the link function ( [ eq - link ] ) .",
    "such a link , in turn , means that @xmath77 is unchanged by adding an arbitrary constant to each component in @xmath78 . as a result",
    ", the single example loss ( [ eq5-log loss ] ) is invariant to moving it along an all-1 vector @xmath79 .",
    "that is , @xmath80 where @xmath81 is an arbitrary real constant ( note that @xmath79 is , coincidentally , the orthogonal complement to the space defined by sum - to - zero constraint ) .",
    "this property also carries over to the approximated node loss ( [ eq - node loss taylor ] ) :    [ theo - inv ] @xmath82 .",
    "this is obvious by noting the additive separability in ( [ eq - node loss taylor ] ) , as well as that @xmath83 , @xmath84 holds since @xmath85 is sum - to - one .    for the hessian , we have @xmath86 by noting the additive form in ( [ eq - taylor d1 ] ) . in @xcite",
    "it is shown that @xmath87 by brute - force determinant expansion . here",
    "we give a stronger property :    [ theo - h ] @xmath88 is a positive semi - definite matrix such that 1 ) @xmath89 , where @xmath90 is the number of non - zero elements in @xmath85 ; 2 ) @xmath79 is the eigenvector for eigenvalue 0 .",
    "the proof can be found in this paper s extended version @xcite .",
    "the properties shown above indicate that 1 ) @xmath68 is singular , so that unconstrained newton descent is not applicable here , and 2 ) @xmath91 could be as high as @xmath1 , which prohibits the application of the standard fast quadratic solver designed for low rank hessians . in the following",
    "we propose to address this problem via block coordinate descent , a technique that has been successfully used in training svms  @xcite .",
    "for the variable @xmath92 in ( [ eq - node loss taylor ] ) , we only choose two ( the least possible number due to the sum - to - zero constraint ) coordinates , _ i.e. _ , a class pair , to update while keeping the others fixed .",
    "suppose we have chosen the @xmath93-th and the @xmath94-th coordinate ( how to do so is deferred to next subsection ) .",
    "let @xmath95 and @xmath96 be the free variables ( such that @xmath97 ) and @xmath98 for @xmath99 , @xmath100 .",
    "plugging these into ( [ eq - node loss taylor ] ) yields an unconstrained one dimensional quadratic problem with regards to the scalar variable @xmath101 : @xmath102 where the gradient and hessian collapse to scalars : @xmath103 @xmath104 to this extent , we are able to obtain the analytical expression for the minimizer and minimum of ( [ eq - node loss taylor scalar ] ) : @xmath105 @xmath106 by noting the non - negativity of ( [ eq - taylor d2 scalar ] ) .",
    "based on ( [ eq - newton step 1d ] ) , node vector ( [ eq - node vector ] ) can be approximated as @xmath107 where @xmath108 and @xmath109 are respectively computed by using ( [ eq - taylor d1 scalar ] ) and ( [ eq - taylor d2 scalar ] ) with index set @xmath110 .",
    "based on ( [ eq - newton dec 1d ] ) , the node gain ( [ eq - node gain ] ) can be approximated as @xmath111 where @xmath108 ( or @xmath112 , @xmath113 ) and @xmath109 ( or @xmath114 , @xmath115 ) are computed by using ( [ eq - taylor d1 scalar ] ) and ( [ eq - taylor d2 scalar ] ) with index set @xmath41 ( or @xmath61 , @xmath62 ) .      in  @xcite",
    "two methods for selecting @xmath116 are proposed .",
    "one is based on a first order approximation .",
    "let @xmath117 and @xmath118 be the free variables and the rest be fixed to @xmath23 . for a @xmath92 with sufficiently small fixed length , let @xmath119 and @xmath120 where @xmath121 is some small enough constant .",
    "the first order approximation of ( [ eq - node loss taylor ] ) is : @xmath122 it follows that the indices @xmath93 , @xmath94 resulting in largest decrement to ( [ eq20 - 1 order exp ] ) are : @xmath123 another method that can be derived in a similar way takes into account the second order information : @xmath124 both methods are @xmath125 procedures that are better than the @xmath126 nave enumeration .",
    "however , in our implementation we find that ( [ eq - subsel 2 ] ) achieves better results for aoso - logitboost .",
    "pseudocode for aoso - logitboost is given in algorithm  [ alg - aosoboost ] .",
    "@xmath127 , @xmath128 , @xmath129 @xmath130 , @xmath128 , @xmath129 .",
    "obtain @xmath131 by recursive region partition .",
    "node split gain is computed as ( [ eq - node gain appro ] ) , where the class pair ( @xmath93 , @xmath94 ) is selected using ( [ eq - subsel 2 ] ) .",
    "compute @xmath132 by ( [ eq - node vector appro ] ) , where the class pair ( @xmath93 , @xmath94 ) is selected using ( [ eq - subsel 2 ] ) .",
    "@xmath133 , @xmath129 .",
    "in this section we compare the derivations of logitboost and abc - logitboost and provide some intuition for observed behaviours in the experiments in section  [ sec - exp ] .      to solve ( [ eq - fm ] ) with a sum - to - zero constraint , abc - logitboost uses @xmath1 independent trees : @xmath134 in  @xcite , the so - called base class @xmath135 is selected by exhaustive search per iteration , _",
    "i.e. _ , trying all possible @xmath135 , which involves growing @xmath136 trees . to reduce the time complexity ,",
    "li also proposed other methods . in  @xcite , @xmath135 is selected only every several iterations , while in  @xcite , @xmath135 is , intuitively , set to be the class that leads to largest loss reduction at last iteration .    in abc - logitboost",
    "the sum - to - zero constraint is explicitly considered when deriving the node value and the node split gain for the scalar regression tree .",
    "indeed , they are the same as ( [ eq - node vector appro ] ) and ( [ eq - node gain appro ] ) in this paper , although derived using a slightly different motivation . in this sense ,",
    "abc - logitboost can be seen as a special form of the aoso - logitboost since : 1 ) for each tree , the class pair is fixed for every node in abc , while it is selected adaptively in aoso , and 2 ) @xmath1 trees are added per iteration in abc ( using the same set of probability estimates @xmath137 ) , while only one tree is added per iteration by aoso ( and @xmath137 are updated as soon as each tree is added ) .    since two changes are made to abc - logitboost , an immediate question is what happens if we only make one ? that is ,",
    "what happens if one vector tree is added per iteration for a single class pair selected only for the root node and shared by all other tree nodes , as in abc , but the @xmath137 are updated as soon as a tree is added , as in aoso .",
    "this was tried but unfortunately , * degraded performance * was observed for this combination so the results are not reported here .    from the above analysis , we believe the more flexible model ( as well as the model updating strategy ) in aoso is what contributes to its improvement over abc , as seen section [ sec - exp ] ) .      in the original logitboost",
    "@xcite , the hessian matrix ( [ eq - taylor - d2-sample ] ) is approximated diagonally . in this way",
    ", the @xmath138 in ( [ eq - fm ] ) is expressed by @xmath0 uncoupled scalar tress : @xmath139 with the gradient and hessian for computing node value and node split gain given by : @xmath140 here we use the subscript @xmath141 for @xmath108 and @xmath109 to emphasize the @xmath141-th tree is built independently to the other @xmath1 trees ( _ i.e. _ , the sum - to - zero constraint is dropped ) .",
    "although this simplifies the mathematics , such an aggressive approximation turns out to harm both classification accuracy and convergence rate , as shown in li s experiments  @xcite .",
    "in this section we compare aoso - logitboost with abc - logitboost , which was shown to outperform original logitboost in li s experiments  @xcite .",
    "we test aoso on all the datasets used in @xcite , as listed in table  [ tab - datasets ] . in the top section",
    "are uci datasets and in the bottom are mnist datasets with many variations ( see @xcite for detailed descriptions ) .",
    "to exhaust the learning ability of ( abc-)logitboost , li let the boosting stop when either the training converges ( _ i.e. _ , the loss ( [ eq - total loss ] ) approaches @xmath23 , implemented as @xmath142 ) or a maximum number of iterations , @xmath31 , is reached .",
    "test errors at last iteration are simply reported since no obvious over - fitting is observed . by default , @xmath143 , while for those large datasets ( _ * covertype290k , poker525k , pokder275k , poker150k , poker100k * _ ) @xmath144  @xcite .",
    "we adopt the same criteria , except that our maximum iterations @xmath145 , where @xmath0 is the number of classes .",
    "note that only one tree is added at each iteration in aoso , while @xmath1 are added in abc .",
    "thus , this correction compares the same maximum number of trees for both aoso and abc .",
    "the most important tuning parameters in logitboost are the number of terminal nodes @xmath48 , and the shrinkage factor @xmath146 . in @xcite , li reported results of ( abc-)logitboost for a number of @xmath48-@xmath146 combinations .",
    "we report the corresponding results for aoso - logitboost for the same combinations . in the following ,",
    "we intend to show that * for nearly all @xmath48-@xmath146 combinations , aoso - logitboost has lower classification error and faster convergence rates than abc - logitboost*.      table  [ tab - mnist10k ] shows results of various @xmath48-@xmath146 combinations for a representative datasets .",
    "results on more datasets can be found in this paper s extended version @xcite .    in table",
    "[ tab - summary ] we summarize the results for all datasets . in @xcite",
    ", li reported that abc - logitboost is insensitive to @xmath48 , @xmath146 on all datasets except for _ * poker25kt1 * _ and _ * poker25kt2*_. therefore , li summarized classification errors for abc simply with @xmath147 and @xmath148 , except that on _ * poker25kt1 * _ and _ * poker25kt2 * _ errors are reported by using the other s test set as a validation set . based on the same criteria we summarize aoso in the middle panel of table  [ tab - summary ] where the test errors as well as the improvement relative to abc are given .",
    "in the right panel of table  [ tab - summary ] we provide the comparison for the best results achieved over all @xmath48-@xmath146 combinations when the corresponding results for abc are available in @xcite or @xcite .",
    "we also tested the statistical significance between aoso and abc .",
    "we assume the classification error rate is subject to some binomial distribution .",
    "let @xmath149 denote the number of errors and @xmath49 the number of tests , then the estimate of error rate @xmath150 and its variance is @xmath151 .",
    "subsequently , we approximate the binomial distribution by a gaussian distribution and perform a hypothesis test . the @xmath152-values are reported in table  [ tab - summary ] .",
    "for some problems , we note logitboost ( both abc and aoso ) outperforms other state - of - the - art classifier such as svm or deep learning .",
    "( _ e.g. _ , the test error rate on _ * poker * _ is @xmath153 for svm and @xmath154 for both abc and aoso ( even lower than abc ) ; on _ * m - image * _ it is @xmath155 for dbn-1 , @xmath156 for abc and @xmath157 for aoso ) . see this paper s extended version @xcite for details .",
    "this shows that the aoso s improvement over abc does deserve the efforts .",
    "recall that we stop the boosting procedure if either the maximum number of iterations is reached or it converges ( i.e. the loss ( [ eq - total loss ] ) @xmath142 ) .",
    "the fewer trees added when boosting stops , the faster the convergence and the lower the time cost for either training or testing .",
    "we compare aoso with abc in terms of the number of trees added when boosting stops for the results of abc available in @xcite .",
    "note that simply comparing number of boosting iterations is unfair to aoso , since at each iteration only one tree is added in aoso and @xmath1 in abc .",
    "results are shown in table  [ tab - conv ] and table  [ tab - conv - minist10k ] . except for",
    "when @xmath48-@xmath146 is too small , or particularly difficult datasets where both abc and aoso reach maximum iterations , we found that trees needed in aoso are typically only @xmath158 to @xmath159 of those in abc .",
    "figure  [ fig - other ] shows plots for test classification error vs. iterations in both abc and aoso and show that aoso s test error decreases faster .",
    "more plots for aoso can be found in this paper s extended version @xcite .",
    "we present an improved logitboost , namely aoso - logitboost , for multi - class classification . compared with abc - logitboost",
    ", our experiments suggest that our adaptive class pair selection technique results in lower classification error and faster convergence rates .",
    "we appreciate ping li s inspiring discussion and generous encouragement .",
    "comments from nips2011 and icml2012 anonymous reviewers helped improve the readability of this paper .",
    "this work was supported by national natural science foundation of china ( 61020106004 , 61021063 , 61005023 ) , the national key technology r&d program ( 2009bah40b03 ) .",
    "nicta is funded by the australian government as represented by the department of broadband , communications and the digital economy and the arc through the ict centre of excellence program ."
  ],
  "abstract_text": [
    "<S> this paper presents an improvement to model learning when using multi - class logitboost for classification . motivated by the statistical view </S>",
    "<S> , logitboost can be seen as additive tree regression . </S>",
    "<S> two important factors in this setting are : 1 ) coupled classifier output due to a sum - to - zero constraint , and 2 ) the dense hessian matrices that arise when computing tree node split gain and node value fittings . in general , this setting is too complicated for a tractable model learning algorithm . </S>",
    "<S> however , too aggressive simplification of the setting may lead to degraded performance . </S>",
    "<S> for example , the original logitboost is outperformed by abc - logitboost due to the latter s more careful treatment of the above two factors .    in this paper </S>",
    "<S> we propose techniques to address the two main difficulties of the logitboost setting : 1 ) we adopt a vector tree ( _ i.e. _ , each node value is vector ) that enforces a sum - to - zero constraint , and 2 ) we use an adaptive block coordinate descent that exploits the dense hessian when computing tree split gain and node values . higher classification accuracy and faster convergence rates </S>",
    "<S> are observed for a range of public data sets when compared to both the original and the abc - logitboost implementations . </S>"
  ]
}