{
  "article_text": [
    "neural systems process information .",
    "this processing is of fundamental biological importance for all animals and humans alike as its main ( if not sole ) biological purpose is to ensure the survival of an individual ( in the short run ) and its species ( in the long run ) in a given environment by means of perception , cognition , action and adaption .",
    "information enters a neural system in form of sensory input representing some aspect of the outside world , perceivable by the sensory modalities present in the system . after processing this information or parts of it ,",
    "the system may then adjust its state and act according to a perceived change in the environment .",
    "this general model is applicable to very basic acts of cognition as well as to ones requiring higher degrees of cognitive processing .",
    "yet , the underlying principle is the same . thus measuring , modeling and ( in the long run )",
    "understanding information processing in neural systems is of prime importance for the goal of gaining insight to the functioning of neural systems on a theoretical level .",
    "note that this question is of theoretical and abstract nature so that we take an abstract view on information in what follows .",
    "we use shannon s theory of information @xcite as a tool that provides us with a rigid mathematical theory and quantitative measures of information .",
    "using information theory , we will have a conceptual look at information in neural systems .",
    "in this context , information theory can provide both explorative and normative views on the processing of information in a neural system as we will see in section  [ sec : neuralsystems ] . in some cases , it is even possible to gain insights on the nature of the `` neural code '' , i.e.  the way neurons transmit information via their spiking activity .",
    "information theory was originally used to analyze and optimize man - made communication systems , for which the functioning principles are known . none the less",
    ", it was soon realized that the theory could also be used in a broader setting , namely to gain insight into the functioning of systems for which the underlying principles are far from fully understood , such as neural systems for example .",
    "this was the beginning of the success story of information - theoretic methods in many fields of science such as economics , psychology , biology , chemistry , physics and many more .",
    "the idea of using information theory to quantitatively assess information processing in neural systems has been around since the 1950s , see the works of attneave @xcite , barlow @xcite and eckhorn and ppel @xcite . yet , as information - theoretic analyses are data - intensive , these methods were rather heavily restricted by ( a ) the limited resources of computer memory and computational power available and ( b ) the limited accuracy and amount of measured data that could be obtained from neural systems ( on the single cell as well as at the systems level ) at that time .",
    "however , given the constant rise in available computing power and the evolution and invention of data acquisition techniques that can be used to obtain data from neural systems ( such magnetoencephalography ( meg ) , functional magnetic resonance imaging ( fmri ) or calcium imaging ) , information - theoretic analyses of all kinds of biological and neural systems became more and more feasible and could be carried out with greater accuracy and for larger and larger ( sub-)systems .    over the last decades such analyses became possible using an average workstation computer , a situation that could only be dreamed of in the 1970s . additionally , the emergence of new non - invasive data - collection methods such as fmri and meg that outperform more traditional methods like electroencephalography ( eeg ) in terms of spatial resultion ( fmri , meg ) or noise - levels ( meg ) made it possible to even obtain and analyze system - scale data of the human brain in vivo .    the goal of this chapter is to give a short introduction to the fundamentals of information theory and its application to data analysis problems in the neurosciences .",
    "and although information - theoretic analyses of neural systems were not often used in order to gain insight on or characterize neural dysfunction so far , this could prove to be a helpful tool in the future .",
    "the chapter is organized as follows .",
    "we first talk a bit about the process of modeling in section  [ sec : setting ] that is fundamental for all what follows as it connects reality with theory .",
    "as information theory is fundamentally based on probability theory , following this we give an introduction to the mathematical notions of probabilities , probability distributions and random variables in section  [ sec : probabilities ] .",
    "if you are familiar with probability theory , you may well skim or skip this section .",
    "section  [ sec : informationentropy ] deals with the main ideas of information theory .",
    "we first take a view on what we mean by information and introduce the core concept of information theory , namely _",
    "entropy_. starting from the concept of entropy , we will then continue to look at more complex notions such as _ conditional entropy _ and _ mutual information _ in section  [ sec : mutualinformation ] .",
    "we will then consider a variant of _ conditional mutual information _ called _ transfer entropy _ in section  [ sec : transferentropy ] .",
    "we conclude the theoretical part by discussing methods used for the estimation of information - theoretic quantities from sampled data in section  [ sec : estimation ] .",
    "what follows will deal with the application of the theoretical measures to neural data .",
    "we then give a short overview of applications of the discussed theoretical methods in the neurosciences in section  [ sec : neuralsystems ] , and last ( but not least ) , section  [ sec : software ] constrains a list of software packages that can be used to estimate information theoretic quantities for some given data set .",
    "in order to analyze the dynamics and gain a theoretical understanding of a given complex system , one usually defines a model first , i.e.  a simplified theoretical version of the system to be investigated . the rest of the analysis",
    "is then based on this model and can only capture aspects of the system that are also contained in the model .",
    "thus , care has to be taken when creating the model as the following analysis crucially depends on the quality of the model .",
    "when building a model based on measured data , there is an important thing we have to pay attention to , namely that any data obtained by measurement of physical quantities is only accurate up to a certain degree and corrupted by noise .",
    "this naturally also holds for neural data ( e.g.  electrophysiological single- or multi - cell measurements , eeg , fmri or meg data ) .",
    "therefore , when observing the state of some system by measuring it , one can only deduce the true state of the system up to a certain error determined by the noise in the measurement ( which may depend both on the measurement method and the system itself ) . in order to model this uncertainty in a mathematical way",
    ", one uses probabilistic models for the states of the measured quantities of a system .",
    "this makes probability theory a key ingredient to many mathematical models in the natural sciences .",
    "the roots of the mathematical theory of probability lie in the works of cardano , fermat , pascal , bernoulli and de moivre in the 16th and 17th century , in which the authors attempted to analyze games of chance .",
    "pascal and bernoulli were the first to treat the subject as a branch of mathematics , see @xcite for a historical overview .",
    "mathematically speaking , probability theory is concerned with the analysis of random phenomena . over the last centuries , it has become a well - established mathematical subject . for a more in - depth treatment of the subject",
    "see @xcite .",
    "let us consider an experiment that can produce a certain fixed number of outcomes ( say a coin toss , where the possible outcomes are heads or tails or the throw of a die where the die will show one of the numbers @xmath0 to @xmath1 ) .",
    "the set of all possible outcomes is called the _ sample space _ of the experiment .",
    "one possible result of an experiment is called _ outcome _ and a set of outcomes is called an _ event _ ( for the mathematically adept : an event is a subset of the power set of all outcomes ) .",
    "take for example the throw of a regular , @xmath1-sided die as an experiment .",
    "the set of results in this case would be the set of natural numbers @xmath2 and examples of events are @xmath3 or @xmath4 corresponding to the events `` an odd number was thrown '' and `` an even number was thrown '' , respectively .",
    "the classical definition of the probability of an event is due to laplace : `` the probability of an event to occur is the number of cases favorable for the event divided by the number of total outcomes possible '' @xcite .    we thus assign each possible outcome a _ probability _ , a real number between @xmath5 and @xmath0 that is thought of as to describe how `` likely '' it is that the given event will occur , where @xmath5 means `` the event does nt ever occur '' and @xmath0 means `` the event always occurs '' .",
    "the sum of all the assigned numbers is restricted to be @xmath0 as we assume that one of our considered events always occurs . for the coin toss , the possible outcomes heads and tails",
    "thus each have probability @xmath6 ( considering that the number of favourable outcomes is @xmath0 and the number of possible outcomes is @xmath7 ) and for the throw of a die this number is @xmath8 for each digit .",
    "this assumes that we have a so - called _ fair _ coin or die , i.e.  one that does not favor any particular outcomes over the others .",
    "the probability of a given event to occur is then just the sum of the probabilities of the outcomes the event is composed of , e.g.  when considering the throw of a die , the probability of the event `` an odd number is thrown '' is @xmath9 .",
    "such types of experiments in which all possible outcomes have the same probability ( they are called equiprobable ) are called _",
    "laplacian experiments_. the simplest case of an experiment not having equiprobable outcomes is the so called _",
    "bernoulli experiment_. here , two possible outcomes `` success '' and `` failure '' , with probabilities @xmath10 $ ] and @xmath11 are considered .",
    "let us now consider probabilities in the general setting .",
    "the foundations of modern probability theory were laid by kolmogorov  @xcite in the 1930s .",
    "he was the first to give an axiomatic description of probability theory based on measure theory , putting the field on a mathematically sound basis .",
    "we will state his axiomatic description of probabilities in the following .",
    "this rather technical approach might seem a little complicated and cumbersome first and we will try to give well - understandable explanations of the concepts and notions used as they are of general importance .",
    "kolmogorov s definition is based on what is known as measure theory , a field of mathematics that is concerned with measuring the ( geometric ) size of subsets of a given space .",
    "measure theory gives an axiomatic description of a _ measure _ ( as a function @xmath12 assigning a non - negative number to each subset ) that fulfills the usual properties of a geometric measure of length ( in @xmath0-dimensional space ) , area ( in @xmath7-dimensional space ) , volume ( in @xmath13-dimensional space ) , and so on .",
    "for example , if we take the measure of two disjoint ( i.e.  non - overlapping ) sets , we expect the measure of their union to be the sum of the measures of the two sets and so on .    one prior remark on the definition : when looking at sample spaces ( remember , these are the sets of possible outcomes of a random experiment ) we have to make a fundamental distinction between _ discrete sample spaces _ ( i.e.  ones in which the outcomes can be separated and counted , like in a pile of sand , where we think of each little sand particle representing one possible outcome ) and _ continuous sample spaces _ ( where the outcomes form a continuum and can not be separated and counted , think of this sample space as some kind of dough in which the outcomes can not be separated ) .",
    "although in most cases the continuous setting can be treated as a straightforward generalization of the discrete case and we just have to replace sums by integrals in the formulas , some technical subtleties exist , that makes a distinction between the two cases necessary .",
    "this is why we separate the two cases in all of what follows .",
    "[ def : probspace ] a _ measure space _ is a triple @xmath14 . here    * the _ base space _",
    "@xmath15 denotes an arbitrary nonempty set , * @xmath16 denotes the set of _ measurable sets _ in @xmath15 which has to be a so called _",
    "@xmath17-algebra _ over @xmath15 , i.e.  it has to fulfill a.   @xmath18 b.   @xmath16 is closed under complements : if @xmath19 , then @xmath20 , c.   @xmath16 is closed under countable unions : if @xmath21 for @xmath22 , then @xmath23 , * @xmath12 is the so called _ measure _ : it is a function @xmath24 with the following properties a.   @xmath25 and @xmath26 ( non - negativity ) , b.   @xmath12 is countably additive : if @xmath21 , @xmath22 is a collection of pairwise disjoint ( i.e.  non - overlapping ) sets , then @xmath27 .",
    "why this complicated definition of measurable sets , measures , etc . ?",
    "well , this is mathematically the probably ( no pun intended ) most simple way to formalize the notion of a `` measure '' ( in terms of geometric volume ) as we know it over the real numbers .",
    "when defining a measure , we first have to fix the whole space in which we want to measure .",
    "this is the base space @xmath15 .",
    "@xmath15 can be any arbitrary set : the sample space of a random experiment , e.g.  @xmath28heads , tails@xmath29 when we look at a coin toss or @xmath30 when we look at the throw of a die ( these are two examples of discrete sets ) , the set of real numbers @xmath31 , the real plane @xmath32 ( these are two examples of continuous sets ) or whatever you choose it to be . when modeling the spiking activity of a neuron the two states could be `` neuron spiked '' or `` neuron did nt spike '' .    in a second step",
    "we choose a collection of subsets of @xmath15 that we name @xmath16 , the collection of subsets of @xmath15 that we want to be measurable .",
    "note that the measurable subsets of @xmath15 are not given a priori , but that we determining those by choosing @xmath16 .",
    "so , you may ask , why this complicated setup with @xmath16 , why not make every possible subset of @xmath15 measurable , i.e.  make @xmath16 the power set of @xmath15 ( the power set is the set of all subsets of @xmath15 ) ? this is totally reasonable and can easily been done when the number of elements of @xmath15 is finite . but as with many things in mathematics , things get complicated when we deal with the continuum : in many natural settings , e.g.  when @xmath15 is a continuous set , this is just not possible or desirable for technical reasons .",
    "that is why we choose only a subset of the power set ( you might refer to its elements as the `` privileged '' subsets ) and make only the contained subsets measurable .",
    "we want to choose this subset in a way that the usual constructions that we know from geometric measures still work in the usual way , though .",
    "this motivates the properties that we impose on @xmath16 : we expect to be able to measure the complements of measurable sets , as well as the union and intersection of a finite number of measurable sets to again be measurable .",
    "these properties are motivated by the corresponding properties of geometric measures ( i.e.  the union , intersection and complement of intervals of certain lengths has a length and so on ) .",
    "so to sum up , the set @xmath16 is a subset of the power set of @xmath15 , and sets that are not in @xmath16 are not measurable .    in a last step ,",
    "we choose a function @xmath12 that assigns a measure ( think of it as a generalized geometric volume ) to each measurable set ( i.e.  each element of @xmath16 ) , where the measure has to fulfill some basic properties that we know from geometric measures : the measure is non - negative , the empty set ( that is contained in every set ) should have measure @xmath5 and the measure is additive .    all together , this makes the triple @xmath14 a space in which we can measure events and use constructions that we know from basic geometry .",
    "our definition makes sure that the measure @xmath12 behaves in the way we expect it to ( mathematicians call this a natural construction ) .",
    "take some time to think about it : definition  [ def : probspace ] above generalizes the notion of the geometric measure in terms of the length @xmath33 of intervals @xmath34 $ ] over the real numbers .",
    "in fact , when choosing the set @xmath35 we can construct the so called _",
    "borel @xmath17-algebra _ @xmath36 that contains all closed intervals @xmath34 $ ] , @xmath37 and a measure @xmath38 that assigns each interval @xmath34\\in \\mathcal{b}$ ] its length @xmath39 .",
    "the measure @xmath38 is called _",
    "borel measure_. it is the standard measure of length that we know from geometry and makes @xmath40 a measure space .",
    "this construction can easily be extended to arbitrary dimensions ( using closed sets ) resulting in the measure space @xmath41 that fulfills the properties of a @xmath42-dimensional geometric measure of volume .",
    "let us look at some examples of measure spaces now :    1 .",
    "let @xmath43 , @xmath44 and @xmath45 with @xmath46 .",
    "this makes @xmath47 a measure space for our coin toss experiment .",
    "note that in this simple case , @xmath16 equals the full power set of @xmath15 .",
    "2 .   let @xmath48 and let @xmath49 with @xmath50 and @xmath51 , where @xmath52 denotes an arbitrary number between @xmath5 and @xmath0 .",
    "this makes @xmath47 a measure space .",
    "having understood the general case of a measure space , defining a probability space and a probability distribution is easy .",
    "a _ probability space _ is a measure space @xmath14 for which the measure @xmath12 is normed , i.e.  @xmath53 $ ] with @xmath54 .",
    "the measure @xmath12 is called _ probability distribution _ and is often also denoted by @xmath45 ( for probability ) .",
    "@xmath15 is called the _ sample space _ , elements of @xmath15 are called _ outcomes _ and @xmath16 is the set of _",
    "events_.    note that again",
    ", we make the distinction between discrete and continuous sample spaces here .",
    "in the course of history , a probability distribution on a discrete sample space came to be called _ probability mass function _ ( or _ pmf _ ) and a probability distribution defined on a continuous sample space came to be called _ probability density function _ ( or _ pdf _ ) .",
    "let us look at a few examples , where the probability spaces in the following are given by the triple @xmath47 .    1 .",
    "let @xmath55heads , tails@xmath29 and let @xmath56heads@xmath57tails@xmath58 .",
    "this is a probability space for our coin toss experiment , where @xmath59 relates to the event `` neither heads nor tails '' and @xmath15 to the event `` either heads or tails '' .",
    "note that in this simple case , @xmath16 equals the full power set of @xmath15 .",
    "2 .   let @xmath60 and let @xmath16 be the full power set of @xmath15 ( i.e.  the set of all subsets of @xmath15 , there are @xmath61 , can you enumerate them all ? ) .",
    "this is a probability for our experiment of dice throws , where we can distinguish all possible events .",
    "it is important to stress that probabilities themselves are a mathematical and purely theoretical construct to help in understanding and analyzing random experiments , and per se they do not have to do anything with reality .",
    "they can be understood as an `` underlying law '' that generates the outcomes of a random experiment and _ can never _ be directly observed , see figure  [ fig : theoryworld ] .",
    "but with some restrictions they can be estimated for a certain given experiment by looking at the outcomes of many repetitions of that experiment .",
    "let us consider the following example .",
    "assume that our experiment is the roll of a six - sided die .",
    "when repeating the experiment for @xmath62 times ( also called _ trials _ )",
    "we will obtain frequencies for each of the numbers as given in figure  [ fig : diefreq1 ] .",
    "repeating the experiment for @xmath63 times we will get frequencies that look similar to the ones given in figure  [ fig : diefreq2 ] . if we look at the relative frequencies ( i.e.  the frequency divided by the total number of trials ) , we see that these converge to the theoretically predicted value of @xmath8 as our number of trials grows larger .",
    "this fundamental finding is also called the `` borel s law of large numbers '' .",
    "let @xmath15 be a sample space of some experiment and let @xmath45 be a probability mass function on @xmath15 .",
    "furthermore let @xmath64 be the number of occurrences of the event @xmath65 when the experiment is repeated @xmath42 times .",
    "then the following holds : @xmath66    borel s law of large numbers states that if an experiment is repeated many times ( where the trials have to be independent and done under identical conditions ) , then the relative frequency of the outcomes converge to their probability as assigned by the probability mass function .",
    "the theorem thus establishes the notion of probability as the long - run relative frequency of an events occurrence and thereby connects the theoretical side to the experimental side .",
    "keep in mind though that we can never directly measure probabilities and although relative frequencies will converge to the probability values , they will usually not be exactly equal .",
    "a fundamental notion in probability theory is the idea of independence of events . intuitively , we call two events independent if the occurrence of one does not affect the probability of occurrence of the other . consider for example the events that it rains and the event that the current day of the week is monday .",
    "these two are clearly independent , unless we lived in a world where there would be a correlation between the two , i.e.  where the probability of rain would be different on mondays compared to the other days of the week which is clearly not the case .",
    "similarly , we establish the notion of independence of two events in the sense of probability theory as follows .",
    "let @xmath67 and @xmath68 be two events of some probability space @xmath69 . then @xmath67 and @xmath68 are called _ independent _ if and only if @xmath70    the term @xmath71 is referred to _ joint probability _ of @xmath67 and @xmath68 , see figure  [ fig : vennab ] .     and @xmath68 , their union @xmath72 , their intersection @xmath73 ( i.e.  common occurrence in terms of probability ) and their exclusive occurrences @xmath74 ( @xmath67 and not @xmath68 occurs ) , @xmath75 ( @xmath68 occurs and not @xmath67 ) , where @xmath76 denotes the complement in @xmath72 . ]    another important concept is the notion of conditional probability , i.e.  the probability of one event @xmath67 occurring , given the fact that another event @xmath68 occurred",
    ".    given two events @xmath67 and @xmath68 of some probability space @xmath77 with @xmath78 we call    @xmath79    the _ conditional probability of @xmath67 given @xmath68_.    note that for independent events @xmath67 and @xmath68 , we have @xmath80 and thus @xmath81 and @xmath82",
    ". we can thus write    @xmath83    and this means that the occurrence of @xmath67 does not affect the conditional probability of @xmath68 given @xmath67 ( and vice versa ) .",
    "this exactly reflects the intuitive definition of independence that we gave in the first paragraph of this section .",
    "note that we could have also used the conditional probabilities to define independence in the first place .",
    "none the less the definition of equation  [ eqn : indevents ] is preferred , as it is shorter , symmetrical in @xmath67 and @xmath68 and more general as the conditional probabilities above are not defined in the case where @xmath84 or @xmath85 .      in many cases",
    "the sample spaces of random experiments are a lot more complicated than the ones of the toy examples we looked at so far .",
    "think for example of measurements of membrane potentials of certain neurons , that we want to model mathematically , or the state of some complicated system , e.g.  a network of neurons receiving some stimulus .",
    "thus mathematicians came up with a way to tame the sample spaces by looking at the events indirectly , namely by first mapping the events to some better understood space , like the set of real numbers ( or some higher dimensional real vector space ) and then look at outcomes of the random experiment in the simplified space rather than in the complicated original space .",
    "looking at spaces of numbers has many advantages : order relations exist ( smaller , equal , larger ) , we can form averages and much more .",
    "this leads to the concept of random variables .",
    "a ( real ) _ random variable _ is a function that maps each outcome of a random experiment to some ( real ) number .",
    "thus , a random variable can be thought of as a variable whose value is subject to variations due to chance . but keep in mind that a random variable is a mapping and not a variable in the usual sense .",
    "mathematically , a random variable is defined using what is called a _",
    "measurable function_. a measurable function is nothing more than a map from one measurable space to another for which the pre - image of each measurable set is again measurable ( with respect to the two different measures in the two measure spaces involved ) .",
    "so a measurable map is nothing more than a `` nice '' map respecting the structures of the spaces involved ( take as an example for such maps the continuous functions over @xmath31 ) .",
    "let @xmath69 be a probability space and @xmath86 a measure space .",
    "a @xmath87-measurable function @xmath88 is called _ @xmath89-valued random variable _",
    "( or just _",
    "@xmath89-random variable _ ) on @xmath15 .    commonly , a distinction between _ continuous random variables _ and _ discrete random variables _ is made , the former taking values on some continuum ( in most cases @xmath31 ) and the latter on a discrete set ( in most cases @xmath90 ) .",
    "a type of random variable that plays an important role in modeling is the the so called _ bernoulli random variable _ that only takes two distinct values @xmath5 with probability @xmath52 and @xmath0 with probability @xmath11 ( i.e.  it has a bernoulli distribution as its underlying probability distribution ) . spiking behavior of a neuron",
    "is often modeled that way , where @xmath0 stands for `` neuron spiked '' and @xmath5 for `` neuron did nt spike '' ( in some interval of time ) .",
    "a real- or integer - valued random variable @xmath91 thus assigns a number @xmath92 to every event @xmath93 .",
    "a value @xmath92 corresponds to the occurrence of the event @xmath94 and is called a _ realization of x_. thus , random variables allow for the change of space in which outcomes of probabilistic processes are considered . instead of considering an outcome directly in some complicated space , we first project it to a simpler space using our mapping ( the random variable @xmath91 ) and interpret its outcome in that simpler space",
    ".    in terms of measure theory , a random variable @xmath95 ( again , considered as a measurable mapping here ) induces a probability measure @xmath96 on the measure space @xmath86 via    @xmath97    where again @xmath98 denotes the pre - image of @xmath99 .",
    "this also justifies the restriction of @xmath91 to be measurable : if it were not , such a construction would not be possible , but this is a technical detail . as a result , this makes @xmath100 a probability space and we can think of the measure @xmath96 as the `` projection '' of the measure @xmath45 from @xmath15 onto @xmath89 ( via the measurable mapping @xmath91 ) .",
    "the measures @xmath45 and @xmath96 are probability densities for the probability distributions over @xmath15 and @xmath89 : they measure the likelihood of occurrence for each event ( @xmath45 ) or value ( @xmath96 ) .    as a simple example of a random variable consider again the example of the coin toss . here",
    ", we have @xmath55heads , tails@xmath29 , @xmath101heads@xmath57tails@xmath58 and @xmath45 that assigns to both heads and tails the probability @xmath6 forming the probability space .",
    "consider as a random variable @xmath102 with @xmath103 that maps @xmath15 to @xmath104 such that @xmath105heads@xmath106 and @xmath105tails@xmath107 .",
    "if we choose @xmath108 as a @xmath17-algebra for @xmath89 this makes @xmath109 a measurable space and @xmath91 induces a measure @xmath110 on @xmath111 with @xmath112 .",
    "that makes @xmath113 a measure space and since @xmath114 is normed it is a probability space .",
    "using random variables that take on values of whole or the real numbers , the natural total ordering of elements in these spaces enables us to define the so called _ cumulative distribution function _ ( or _ cdf _ ) for a random variable .",
    "let @xmath91 be a @xmath31-valued or @xmath90-valued random variable on some probability space @xmath69 .",
    "then the function    @xmath115    is called the _ cumulative distribution function _ of @xmath91 .    the expression expression",
    "@xmath116 evaluates to    @xmath117    in the continuous case and to    @xmath118    in the discrete case .    in that sense",
    ", the measure @xmath96 can be understood as the derivative of the cumulative distribution function @xmath119    @xmath120    and we also write @xmath121 in the continuous case .",
    "the definition of independent events directly transfers to random variables : two random variables @xmath122 are called independent if the conditional probability distribution of @xmath91 ( @xmath123 ) given an observed value of @xmath123 ( @xmath91 ) does not differ from the probability distribution of @xmath91 ( @xmath123 ) alone .",
    "let @xmath124 be two random variables .",
    "then @xmath91 and @xmath123 are called _ independent _ , if the following holds for any observed values @xmath125 of @xmath91 and @xmath126 of @xmath123 :    @xmath127    this notion can be generalized to the case of three or more random variables naturally .",
    "two very important concepts of random variables are the so called _ expectation value _ ( or just _ expectation _ ) and the _ variance_. the expectation of a random variable @xmath91 is the mean value of the random variable , where the weighting of the values corresponds to the probability density distribution .",
    "it thus tells us what value of @xmath91 we should expect `` on average '' :    let @xmath91 be a @xmath31- or @xmath90-valued random variable .",
    "then its _ expectation value _ ( sometimes also denoted by @xmath12 ) is given by    @xmath128:=\\int_{\\mathbb{r } } x p_x(x)\\ ; \\mathrm{d}x=\\int_{\\mathbb{r } } x \\ ; \\mathrm{d}p_x,\\ ] ]    for a real - valued random variable @xmath91 and by    @xmath128:=\\sum_{x\\in\\mathbb{z } } x p_x(x)\\ ] ]    if @xmath91 is @xmath90-valued .",
    "note that if confusion can be made as to which probability distribution the expectation value is taken , we will include the probability distribution to which the expectation value is taken in the index .",
    "consider for example two random variables @xmath91 and @xmath123 defined on the same base space but with different underlying probability distributions . in this case , we denote by @xmath129 $ ] the expectation value of @xmath123 taken with respect to the probability distribution of @xmath91 .",
    "let us now look an example .",
    "if we consider the throw of a fair die with @xmath130 for each digit @xmath131 and take @xmath91 as the random variable that just assigns each digit its integer value @xmath132 , we get @xmath133=\\frac{1}{6 } ( 1+\\dots+6)=3.5 $ ] .",
    "another important concept is the so - called _ variance _ of a random variable .",
    "the variance is a measure for how far the values of the random variable are spread around its expected value .",
    "it is defined as follows .",
    "let @xmath91 be a @xmath31- or @xmath90-valued random variable .",
    "then its _ variance _ is given as    @xmath134:=e[(e[x]-x)^2]=e[x^2]-(e[x])^2,\\ ] ]    sometimes also denoted as @xmath135 .",
    "the variance is thus the expected squared distance of the values of the random variable to its expected value .",
    "another commonly used measure is the so called _ standard deviation _",
    "@xmath136 , a measure for the average deviation of realizations of @xmath91 from the mean value .",
    "often one also talks about the expectation value as `` first order moment '' of the random variable , the variance as a `` second order moment '' .",
    "higher order moments can be constructed by iteration , but will not be of interest to us in the following .",
    "note again that the concepts of expectation and variance live on the theoretical side of the world , i.e.  we can not measure these quantities directly .",
    "the only thing that we can do is try to estimate them from a set of measurements ( i.e.  realizations of the involved random variables ) , see figure  [ fig : theoryworld ] .",
    "the statistical discipline of estimation theory deals with question regarding the estimation of theoretical quantities from real data .",
    "we will talk about estimation in more detail in section  [ sec : estimation ] and just give two examples here .",
    "for estimating the expected value we can use what is called the _",
    "sample mean_.    let @xmath91 be a @xmath31- or @xmath90-valued random variable with @xmath42 realizations @xmath137 .",
    "then the _ sample mean @xmath138 _ of the realizations is given as @xmath139    as we will see below , this sample mean provides a good estimation of the expected value if the number @xmath42 of samples is large enough .",
    "similarly , we can estimate the variance as follows .",
    "let @xmath91 be a @xmath31- or @xmath90-valued random variable with @xmath42 realizations @xmath137 .",
    "then the _ population variance @xmath140 _ of the realizations is given as @xmath141 where @xmath138 denotes the sample mean .    before going on",
    "let us calculate some examples of expectations and variances of random variables .",
    "take the coin toss example from above . here , the expected value of @xmath91 is @xmath133=\\frac{1}{2}\\cdot 0 + \\frac{1}{2}\\cdot 1 = \\frac{1}{2}$ ] , the variance @xmath142-x)^2]=\\frac{1}{2}\\cdot ( 0-\\frac{1}{2})^2 + \\frac{1}{2}\\cdot ( 1-\\frac{1}{2})^2 = \\frac{1}{4}$ ] .",
    "for the example of the dice roll ( where the random variable @xmath91 takes the value of the number thrown ) we get @xmath133 = \\frac{1 + 2 + 3 + 4 + 5 + 6}{6 } = \\frac{7}{2 } = 3.5 $ ] and @xmath143-(e[x])^2=\\frac{91}{6}- \\frac{49}{4}=\\frac{35}{12}\\approx 2.92 $ ] .",
    "studying relations between the expected value and the variance of a random variable can be helpful in order to get a clue on the shape of the underlying probability distribution .",
    "the quantities we will discuss below are usually only used in conjunction with positive statistics , such as count data or the time between events , but can be extended to the general case without greater problems ( if this is needed ) .    a first fundamental quantity that can be derived as a relation between the expected value and the variance is the so called signal to noise ratio ( remember that we can interpret a sequence of realizations of a random variable as a `` signal '' ) .",
    "let @xmath91 be a non - negative real valued random variable with expected value @xmath26 , variance @xmath144 and standard deviation @xmath17 .",
    "then its _ signal to noise ratio _ ( snr ) is given by    @xmath145    this dimensionless quantity has wide applications in physics and signal processing .",
    "note that the signal to noise ratio approaches infinity as @xmath17 approaches @xmath5 and that is defined to be infinity for @xmath146 .",
    "being defined via the the expectation and variance , the signal to noise ratio again is a theoretical quantity that can only be computed if the full probability distribution of @xmath91 is known . as this usually is not the case , we have to resort on estimating @xmath147 via the so called _ population snr _ which we calculate for a given population ( our samples ) using the ratio of the sample standard mean @xmath148 , to the sample standard deviation @xmath149 to obtain    @xmath150    keep in mind though that this estimation is _",
    "biased_. this means that it tends to yield a value shifted with respect to the real value , in this case a value higher than @xmath147 .",
    "this especially applies to cases with fewer samples .",
    "this means that @xmath151 can be used for an estimation of an upper bound for @xmath147 .",
    "another fundamental property defined via expectation and variance of a random variable are the so called _ index of dispersion _ and the _ coefficient of variation_.    let @xmath91 be a a non - negative real valued random variable with expected value @xmath26 , variance @xmath144 and standard deviation @xmath17 . then",
    "its _ index of dispersion _ is given by    @xmath152    the _ coefficient of variation _",
    "( cv ) is given by    @xmath153    we will only discuss the coefficient of variation in more detail in the following , but the same also holds for the index of dispersion .    again , the coefficient of variation is a theoretical quantity that can be estimated via the _ population cv _",
    "analogously to the population snr .",
    "note that this estimator also is positively biased , i.e.  it usually overestimates the value of @xmath154 .",
    "there are some other noteworthy properties of the coefficient of variation .",
    "first of all , it is a dimensionless number and independent of the unit of the ( sample ) data , which is a desirable property , especially when comparing different data sets .",
    "this makes the cv a popular and often used quantity in statistics which is better suited than e.g.  the standard deviation when comparing different data sets .",
    "but there are certain drawbacks to the quotient construction , too : the cv becomes very sensitive to fluctuations in the mean close to @xmath155 and it is undefined ( or infinity ) for a mean of @xmath155 .",
    "why do these quantities matter ?",
    "they will allow us to distinguish certain families of probability distributions ( as discussed in section  [ sec : paramprobdistr ] )  for example a poisson - distributed random variable has a coefficient of variation of @xmath0 and this is a necessary condition for the random variable to be poisson - distributed .",
    "the laws of large numbers ( there exist two versions as we will see below ) state that the sample average of a set of realizations of a random variable `` almost certainly '' converges the the random variable s expected value when the number of realizations grows to infinity .",
    "let @xmath156 be an infinite sequence of independent , identically distributed random variables with expected values @xmath157 .",
    "let @xmath158 be the sample average .",
    "a.   * weak law of large numbers*. the sample average converges in probability towards the expected value , i.e.  for any @xmath159 @xmath160 this is sometimes also expressed as @xmath161 b.   * strong law of large numbers*. the sample average converges almost surely towards the expected value , i.e. @xmath162 this is sometimes also expressed as @xmath163    the weak version of the law states that the sample average @xmath164 is likely to be close to @xmath12 for some large value of @xmath42 .",
    "but this does not exclude the possibility of @xmath165 occurring an infinite number of times .",
    "the strong law says that this `` almost surely '' will not be the case : with probability @xmath0 , the inequality @xmath166 holds for all @xmath159 and all large enough @xmath42 .",
    "certain probability distributions often occur naturally when looking at typical random experiments . in the course of history , these were thus put ( mathematicians like doing such things ) into families or classes and the members of one class are distinguished by a set of parameters ( a parameter is just a number than can be chosen freely in some specified range ) . to specify a certain probability distribution we simply have to specify in which class it lies and which parameter values it exhibits , which is more convenient than specifying the probability distribution explicitly every time . this",
    "also allows proving ( and reusing ) results for whole classes of probability distributions and , facilitates communication with other scientists .",
    "note that we will only give a concise version of the most important distributions relevant in neuroscientific applications here and point the reader to @xcite for a more in - depth treatment of the subject .",
    "the _ normal distribution _",
    "@xmath167 is a family of continuous probability distributions parametrized by two real - valued parameters @xmath168 and @xmath169 , called _",
    "mean _ and _ variance_. its probability density function is given as    @xmath170    the family is closed under linear combinations , i.e.  linear combinations of normally distributed random variables are again normally distributed .",
    "it is the most important and often used probability distribution in probability theory and statistics as many other probability distributions can be approximated by a normal distribution when the sample size is large enough ( this fact is called the _ central limit theorem _ ) . see figure  [ fig : distnormal ] for examples of the pdf and cdf for normally - distributed random variables .",
    "the _ bernoulli probability distribution _",
    "@xmath171 describes the two possible outcomes of a bernoulli experiment with the probability of success and failure being @xmath52 and @xmath11 , respectively .",
    "it is thus a discrete probability distribution on two elements and it is parametrized by one parameter @xmath10\\subset \\mathbb{r}$ ] .",
    "its probability mass function is given by the two values @xmath172 and @xmath173 .",
    "the _ binomial probability distribution _",
    "@xmath174 is a discrete probability distribution parametrized by two parameters @xmath175 and @xmath10\\subset \\mathbb{r}$ ] .",
    "its probability mass function is    @xmath176    and it can be thought of as a model for the probability of @xmath177 successful outcomes in a trial with @xmath42 independent bernoulli experiments , each having success probability @xmath52 .",
    "the _ poisson distribution _",
    "@xmath178 is a family of discrete probability distributions parametrized by one real parameter @xmath179 .",
    "its probability mass function is given by    @xmath180    the poisson distribution plays an important role in the modeling of neuroscience data .",
    "this is the case because the firing statistics of cortical neurons ( and also other kinds of neurons ) can often be well fit by a poisson process , where @xmath181 is considered the mean firing rate of a given neuron , see @xcite .",
    "this fact comes at no surprise if we invest some thought .",
    "the poisson distribution can be seen as a special case of the binomial distribution .",
    "a theorem known as poisson limit theorem ( sometimes also called `` law of rare events '' ) now tells us that in the limit @xmath182 and @xmath183 the binomial distribution converges to the poisson distribution with @xmath184 .",
    "consider for example the spiking activity of our neuron that we could model via a binomial distribution .",
    "we discretize time and consider time bins of say @xmath7 ms and assume a mean firing rate of the neuron denoted by @xmath181 ( measured in hertz ) . clearly , in most time bins the neuron does not spike ( corresponding to a small value of @xmath52 ) and the number of bins is large ( corresponding to a large @xmath42 ) .",
    "the poisson limit theorem tells us that in this case the probability distribution concerning spike emission is well matched by a poisson distribution .",
    "see figure  [ fig : distpoiss ] for examples of the pmf and cdf for poisson - distributed random variables for a selection of parameters @xmath181 .    the so called _ exponential distribution _",
    "@xmath185 is a continuous probability distribution parametrized by one real parameter @xmath179 .",
    "its probability density function is given by    @xmath186    the exponential distribution with parameter @xmath181 can be interpreted as the probability distribution describing the time between two events in a poisson process with parameter @xmath181 , see the next section .",
    "see figure  [ fig : distexp ] for examples of the pdf and cdf for exponentially - distributed random variables for a selection of parameters @xmath181 .",
    "we want to conclude our view on families on probability distributions at this point and point the interested reader to @xcite regarding further examples and details of families of probability distributions .",
    "a _ stochastic process _",
    "( sometimes also called _ random process _ ) is a collection of random variables indexed by a totally ordered set , which is usually taken as time .",
    "stochastic processes are commonly used to model the evolution of some random variable over time .",
    "we will only look at discrete - time processes in the following , i.e.  stochastic processes that are indexed by a discrete set .",
    "the extension to the continuous case is straightforward , see @xcite for an introduction to the subject .",
    "mathematically , a stochastic process is defined as follows .",
    "let @xmath47 be a probability space and let @xmath187 be a measure space .",
    "let furthermore @xmath188 be a set of random variables , where @xmath189 .",
    "then an _ @xmath104-valued stochastic process @xmath190 _ is given by    @xmath191    where @xmath192 is some totally ordered set , commonly interpreted as time .",
    "the space @xmath104 is referred to as the _ sample space of the process @xmath190_.    if the distribution underlying the random variables @xmath193 does not vary over time , the process is called _ homogeneous _ , in the case where the probability distributions @xmath194 depend on the time @xmath195 , it is called _",
    "inhomogeneous_.    a special kind and well - studied type of stochastic process is the so called _",
    "markov process_. a discrete markov process of order @xmath196 is a inhomogeneous stochastic process subject to the restriction that for any time @xmath197 , the probability distribution underlying @xmath193 only depends on the preceding @xmath177 probability distributions of @xmath198 , i.e. that for any @xmath195 and any set of realizations @xmath199 of @xmath200 ( @xmath201 ) we have    @xmath202    another process often considered in neuroscientific applications if the _ poisson process_. it is a discrete - time stochastic process @xmath190 for which the random variables are poisson - distributed with some parameter @xmath203 ( in the inhomogeneous case , for the homogeneous case we have @xmath204constant ) . as can be shown , the time delay between each pair of consecutive events of a poisson process is exponentially distributed . see figure  [ fig : procpoiss ] for examples of the number of instantaneous ( occurring during one time slice ) and the number of cumulated events ( over all preceding time slices ) of poisson processes for a selection of parameters @xmath181 .",
    "poisson processes have proven to be a good model for many natural as well as man - made processes such as radioactive decay , telephone calls and queues , and also for modeling neural data .",
    "an influential paper in the neurosciences was @xcite , showing the random nature of the closing and opening of single ion channels in certain neurons .",
    "using as model a poisson process with the right parameter provides a good fit to the measured data here .",
    "another prominent example of neuroscientific models employing a poisson process is the commonly used model for the sparse and highly irregular firing patterns of cortical neurons in vivo @xcite .",
    "the firing patterns of such cells are usually modeled using inhomogeneous poisson processes ( with @xmath203 modeling the average firing rate of a cell ) .",
    "information theory was introduced by shannon @xcite as a mathematically rigid theory to describe the process of transmission of information over some channel of communication .",
    "his goal was quantitatively measure the `` information content '' of a `` message '' sent over some `` channel '' , see figure  [ fig : shannonchannel ] . in what follows",
    "we will not go into detail regarding all aspects of shannon s theory , but we will mainly focus on his idea of measuring `` information content '' of a message . for a more in - depth treatment of the subject ,",
    "the interested reader is pointed to the excellent book @xcite .",
    "the central elements of shannon s theory are depicted in figure  [ fig : shannonchannel ] . in the standard setting considered in information theory , an _ information source _ produces _ messages _ that are subsequently encoded using _",
    "symbols _ from an _ alphabet _ and sent over a noisy _ channel _ to be received by a _ receiver _ that decodes the message and attempts to reconstruct the original message .    a communication channel ( or just channel ) in shannon s model",
    "transmits the encoded message from the sender to the receiver .",
    "due to noise present in the channel the receiver does not receive the original message dispatched by the sender but rather some noisy version of it .",
    "the whole theory is set in the field of probability theory ( hence our introduction to the concepts in the last section ) and in this context , the messages emitted by the source are modeled as a random variable @xmath91 with some underlying probability distribution @xmath96 . for each message @xmath125 ( a realization of @xmath91 )",
    ", the receiver sees a corrupted version @xmath126 of @xmath125 and this fact is modeled by interpreting the received messages as realizations of a random variable @xmath123 with some probability distribution @xmath205 ( that depends both on @xmath96 and the channel properties ) .",
    "the transmission characteristics of the channel itself are characterized by the stochastic correspondence of the signals transmitted by the sender to the ones received by the receiver , i.e.  by modeling the channel as a conditional probability distribution @xmath206 .",
    "being based upon probability theory , keep in mind that the all the information - theoretic quantities that we will look at in the following such as `` entropy '' or `` mutual information '' are just properties of the random variables involved , i.e.  properties of the probability distributions underlying these random variables .",
    "information - theoretic analyses have proven to be a valuable tool in many areas of science such as physics , biology , chemistry , finance and linguistics and generally in the study of complex systems @xcite .",
    "we will have a look at applications in the neurosciences in section  [ sec : neuralsystems ] .",
    "note that a vast number of works was published in the field of information theory and its applications since its first presentation in the 1950s .",
    "we will focus on the core concepts in the following and point the reader to @xcite for a more in - depth treatment of the subject .    in the following",
    "we will start by looking at a notion of information and using this proceed to define _ entropy _ ( sometimes also called _ shannon entropy _ ) , a core concept in information theory . as all further information - theoretic concepts are based on the idea of entropy ,",
    "it is of vital importance to understand this concept well .",
    "we will then look at mutual information , the information shared by two or more random variables .",
    "furthermore , we will look at a measure of distance for probability distributions called kullback - leibler divergence and give an interpretation of mutual information in terms of kullback - leibler divergence . after a quick look at the multivariate case of mutual information between more than two variables and the relation between mutual information and channel capacity we",
    "will then proceed to a information - theoretic measure called transfer entropy .",
    "transfer entropy is based on mutual information but in contrast to mutual information is of directed nature .      before defining entropy ,",
    "let us try to give an axiomatic definition of the concept of `` information '' @xcite .",
    "the entropy of a random variable will then be nothing more that the expected ( i.e.  average ) amount of information contained in a realization of that random variable .",
    "we want to consider a probabilistic model in what follows , i.e. we have a set of events , each occurring with a given probability .",
    "the goal is to assess how informative the occurrence of a given event is . what would we intuitively expect from a measure of information @xmath207 that maps the set of the events to the set of non - negative real number , i.e.  when we restrict @xmath207 to be a non - negative real number ?",
    "first of all , it should certainly be additive for independent events and sub - additive for non - independent events .",
    "this is easily justified : if you read two newspaper articles about totally unrelated subjects , the total amount of information you obtain consists of both the information in the first and the second article . when you read articles about related subjects , they often have some common information .",
    "furthermore , events that occur regularly and unsurprisingly are not considered informative and the more seldom or surprising an event occurs , the more informative it is .",
    "think about an article about your favorite sports team winning a match that usually wins all matches .",
    "you will consider this not very informative .",
    "but when the local newspaper reports about an earthquake with its epicenter in the part of town where you live , this will certainly be informative to you ( unless you were at home during the time the earthquake happened ) , assuming that earthquakes do not occur on a regular basis where you live .",
    "we thus have the following axioms for the information content @xmath207 of an event , where we look at the information content of events contained in some probability space @xmath69 .",
    "a.   @xmath207 is non - negative : @xmath208 .",
    "b.   @xmath207 is sub - additive : for any two messages @xmath209 we have @xmath210 , where equality holds if and only if @xmath211 and @xmath212 are independent .",
    "c.   @xmath207 is continuous and monotonic with respect to the probability measure @xmath45 .",
    "d.   events with probability @xmath0 are not informative : @xmath213 for @xmath214 with @xmath215 .",
    "now calculus tells us ( this is not hard to show  you paid attention in the mathematics class at school , did nt you ? ) that these four requirements leave only one possible function that fulfills all these requirements : the logarithm .",
    "this leads us to the following natural definition .    .",
    "]    let @xmath69 be a probability space .",
    "then the information @xmath207 of an event @xmath216 is defined as    @xmath217    where @xmath218 denotes the basis of the logarithm .    for the basis of the logarithm , usually @xmath219 or @xmath220",
    "is chosen , fixing the unit of @xmath207 as `` bit '' or `` nat '' , respectively .",
    "we resort to using @xmath219 for the rest of this chapter and write @xmath221 for the logarithm to the basis of two .",
    "the natural logarithm will be denoted by @xmath222 .",
    "note that the information content in our definition only depends on the probability of the occurrence of the event and not the event itself .",
    "it is thus a property of the probability distribution @xmath45 .",
    "let us give some examples in order to illustrate this idea of information content .",
    "consider a toss of a fair coin , where the possible outcomes are heads ( h ) or tails ( t ) , each occurring with probability @xmath6 .",
    "what is the information contained in a coin toss ? as the information solely depends on the probability , we have @xmath223 , which comes at no surprise .",
    "furthermore we have @xmath224 bit , when we apply the fundamental logarithmic identity @xmath225 .",
    "thus one toss of a fair coin gives us one bit of information .",
    "this fact also lets us explain the unit attached to @xmath207 . if measured in bit ( i.e.  with @xmath219 ) , this is the amount of bits needed to store that information . for the toss of a coin",
    "we need one bit , assigning each outcome to either @xmath5 or @xmath0 .    repeating the same game for the roll of a fair die where each digit has probability @xmath8",
    ", we again have the same amount of information for each digit @xmath226 , namely @xmath227 bit .",
    "this means that in this case we need @xmath13 bits to store the information associated to each outcome , namely the number shown .",
    "looking at the two examples above , we can give another ( hopefully intuitive ) characterization of the term information content : it is the minimal number of yes - no - questions that we have to ask until we know which event occurred , assuming that we have a knowledge of the underlying probability distribution .",
    "consider the example of the coin toss above .",
    "we have to ask exactly one question and we know the outcome ( `` was it heads ? '' , `` was it tails ? '' ) .",
    "things get more interesting when we look at the case of the die throw . here several question asking strategies are possible and you can freely choose your favorite  we will give one example below .",
    "say a digit @xmath228 was thrown .",
    "the first question could be `` was the digit less or equal to 3 ? ''",
    "( other strategies `` was the digit greater or equal to 3 ? '' , `` was the digit even ? '' , `` was the digit odd ? '' ) .",
    "we then go on depending on the answer and cut off at least half of the remaining probability mass in each step , leaving us with a single possibility after at most @xmath13 steps . from the information",
    "content we know that on average we have to ask @xmath229 times on average .    the two examples above were both cases with uniform probability distributions but in principle the same applies to arbitrary probability distributions .",
    "the term entropy is at the heart of shannon s information theory @xcite .",
    "using the notion of the information as discussed in section  [ ssec : information ] , we can readily define the entropy of a discrete random variable as its expected information .",
    "let @xmath91 be a random variable on some probability space @xmath69 with values in the integer or the real numbers .",
    "then its _ _ entropy _ _ for denoting entropy after boltzmann s @xmath230-theorem in classical statistical mechanics . ]",
    "( sometimes also called _ shannon entropy _ or _ self - information _ ) @xmath231 is defined as the expected amount of information of @xmath91 ,    @xmath232 .",
    "\\label{eq : defentropy}\\ ] ]    if @xmath91 is a random variable that takes integer values ( i.e.  a discrete random variable ) , equation  [ eq : defentropy ] evaluates to    @xmath233    in the case of a real - valued , continuous random variable we get    @xmath234    and the resulting quantities is called _ differential entropy _ @xcite .    as the information content is a function solely dependent on the probability of the events one also speaks of the entropy of a probability distribution .",
    "looking at the definition in equation  [ eq : defentropy ] , we see that entropy is a measure for the average amount of information that we expect to obtain when looking at realizations of a given random variable @xmath91",
    ". an equivalent characterization would be to interpret it as the average information one is missing when one would not know the value of the random variable ( i.e.  its realization ) and a third one would be to interpret it as the average reduction of uncertainty about the possible values of a random variable having observed one or more realizations .",
    "akin to the information content @xmath207 , entropy @xmath230 is a dimensionless number and usually measured in bits ( i.e.  the expected number of binary digits needed to store the information ) by taking a logarithm to the base of @xmath7 .",
    "shannon entropy has many applications as we will see in the following and constitutes the core of all things labeled `` information theory '' .",
    "let us thus look a bit closer at this quantity .",
    "let @xmath91 be some discrete random variable .",
    "then its entropy @xmath231 satisfies the two inequalities    @xmath235    note that the first inequality is a direct consequence of the properties of the information content and the second follows from gibbs inequality @xcite .    with regard to entropy ,",
    "probability distributions having maximal entropy are often of interest in applications as they can be seen as the least restricted ones ( i.e.  having the least a priori assumptions ) , given the model parameters .",
    "the _ principle of maximum entropy _ states that when choosing among a set of probability distributions with certain fixed properties , the preference should be given to distributions that have the maximal entropy among all considered distributions . this choice is justified as the one making the fewest assumptions on the shape of the distribution apart from the prescribed properties .    for discrete probability distributions ,",
    "the uniform distribution is the one with the highest entropy among all other distributions on the same base set .",
    "this can be well seen in the example in figure  [ fig : binent ] : the entropy of a bernoulli distribution takes its maximum at @xmath236 , the parameter value for which it corresponds to the uniform probability distribution on the two elements @xmath5 and @xmath0 , each occurring with probability @xmath237 .    for continuous , real - valued random variables with a given finite mean @xmath12 and variance @xmath135 , the normal distribution with mean @xmath12 and variance @xmath135 has highest entropy .",
    "demanding non - negativity and a non - vanishing probability on the positive real numbers ( i.e. , an infinite support ) with positive given mean @xmath12 yields the exponential distribution with parameter @xmath238 as a maximum - entropy distribution .      before continuing ,",
    "let us now compute some more entropies in order to get a feeling for this quantity .    for a uniform probability distribution @xmath45 on @xmath42 events",
    "@xmath239 each event has probability @xmath240 and we obtain    @xmath241    as the maximal entropy for all discrete probability distributions on the set @xmath15 .",
    "let us now compute the entropy of a bernoulli random variable @xmath91 , i.e.  a binary random variable @xmath91 taking values @xmath5 and @xmath0 with probability @xmath52 and @xmath11 , respectively . for",
    "the entropy of @xmath91 we get    @xmath242    see figure  [ fig : binent ] for a plot of the entropy seen as a function of the success probability @xmath52 .",
    "as expected , the maximum is attained at @xmath236 , corresponding to the case of the uniform distribution .",
    "of a bernoulli random variable @xmath91 as a function of success probability @xmath243 .",
    "the maximum is attained at @xmath236 . ]    computing the differential entropy of a normal distribution @xmath244 with mean @xmath12 and variance @xmath135 yields    @xmath245    and we see that the entropy does not depend on the mean value of the distribution but just its variance .",
    "this is not surprising , as the shape of the probability distribution is only changed by @xmath135 and not @xmath12 .",
    "for an example of how to compute the entropy of spike trains see section  [ sec : neuralsystems ] .",
    "generalizing the notion of entropy to two or more variables we can define the so called joint entropy to quantify the expected uncertainty ( or expected information ) in a joint distribution of random variables .",
    "let @xmath91 and @xmath123 be discrete random variables on some probability spaces . then the _ joint entropy _ of @xmath91 and @xmath123 is given by    @xmath246=-\\sum_{x , y}^ { } p(x , y)\\log p(x , y ) ,      \\label{eq : defjointent}\\ ] ]    where @xmath247 denotes the joint probability distribution of @xmath91 and @xmath123 and the sum runs over all possible values @xmath125 and @xmath126 of @xmath91 and @xmath123 , respectively .",
    "this definition allows a straightforward extension to the case of more than two random variables .",
    "the conditional entropy @xmath248 of two random variables @xmath91 and @xmath123 quantifies the expected uncertainty ( respectively expected information ) remaining in a random variable @xmath91 under the condition that a second variable @xmath123 was observed or equivalently as the reduction of the expected uncertainty in @xmath91 upon the knowledge of @xmath123 .",
    "let @xmath91 and @xmath123 be discrete random variables on some probability spaces",
    ". then the _ conditional entropy _ of @xmath91 given @xmath123 is given by    @xmath249=-\\sum_{x , y}^ { } p(x , y)\\log p(x|y),\\ ] ]    where @xmath247 denotes the joint probability distribution of @xmath91 and @xmath123 .",
    "in this section we will introduce the notion of mutual information , an entropy - based measure for the information shared between two ( or more ) random variables .",
    "mutual information can be thought of as a measure for the mutual dependence of random variables , i.e.  as a measure for how far they are from being independent .",
    "we will give two different approaches to this concept in the following : a direct one based on the point - wise mutual information @xmath250 and one using the idea of conditional entropy .",
    "note that in essence , these are just different approaches to defining the same object .",
    "we give the two approaches in the following , hoping that they help in understanding the concept better . in section  [ sec : kullbackleibler ] we will see yet another characterization in terms of the kullback - leibler divergence .      in terms of information content , the case of considering two events that are independent is straightforward : one of the axioms tells us that the information content of the two events occurring together is the sum of the information contents of the single events .",
    "but what about the case where the events non - independent ?",
    "in this case we certainly have to consider the conditional probabilities of the two events occurring : if one event often occurs given that the other one occurs ( think of the two events `` it is snowing . '' and `` it is winter '' ) , the information overlap is higher than when the occurrence of one given the other is rare ( think of `` it is snowing '' and `` it is summer . '' ) .",
    "using the notion of information from section  [ ssec : information ] , let us express this in a mathematical way by defining the _ mutual information _",
    "( i.e.  shared information content ) of two events .",
    "we call this the _ point - wise mutual information _ or _ pmi_.    let @xmath125 and @xmath126 be two events of a probability space @xmath251",
    ". then their _ point - wise mutual information _",
    "( pmi ) is given as @xmath252    note that we used joint probability distribution of @xmath125 and @xmath126 is for the definition of @xmath253 to avoid the ambiguities introduced by the conditional distributions . yet , the latter are probably the easier way to gain a first understanding of this quantity .",
    "let us note that this measure of shared information is symmetric ( @xmath254 ) and that it can take any real value , particularly also negative values .",
    "such negative values of point - wise mutual inforamtion are commonly referred to as _ misinformation _",
    "point - wise mutual information is zero if the two events @xmath125 and @xmath126 are independent and it is bounded above by the information content of @xmath125 and @xmath126 . more generally , the following inequality holds :    @xmath255    defining the information content of the co - occurrence of @xmath125 and @xmath126 as @xmath256 another way of writing the point - wise mutual information is    @xmath257    where the first identity above is readily obtained from equation  [ eq : pmi ] by just expanding the logarithmic term and in the second and third line the formula for the conditional probability was used .    before considering mutual information of random variables as a straightforward generalization of the above ,",
    "let us look at an example .",
    "say we have two probability spaces @xmath258 and @xmath259 , with @xmath260 and @xmath261 .",
    "we want to compute the point - wise mutual information of two events @xmath262 and @xmath263 subject to the joint probability distributions of @xmath264 and @xmath265 as given in table  [ tab : pmitabomega ] .",
    "note that the joint probability distribution can also be written as matrix    @xmath266    if we label rows by possible outcomes of @xmath264 and columns by possible outcomes of @xmath265 .",
    "the marginal distributions @xmath267 and @xmath268 are now obtained as row , respectively column sums as @xmath269 , @xmath270 , @xmath271 , @xmath272 .",
    "we can now calculate the point - wise mutual information of for example    @xmath273    and    @xmath274    note again that in contrast to mutual information ( that we will discuss in the next section ) , point - wise mutual information can take negative values called , see@xcite .",
    ".table of joint probabilities @xmath275 of two events @xmath264 and @xmath265 . [ cols=\">,>,>\",options=\"header \" , ]      using point - wise mutual information , the definition of mutual information of two random variables is straightforward : mutual information of two random variables is the expected value of the point - wise mutual information of all realizations .",
    "let @xmath91 and @xmath123 be two discrete random variables",
    ". then the mutual information @xmath276 is given as the expected point - wise mutual information , @xmath277\\\\       & = \\sum_{y } \\sum_{x } p(x , y ) \\operatorname{i}(x;y)\\\\      & = -\\sum_{y } \\sum_{x } p(x , y ) \\log { \\left(\\frac{p(x , y)}{p(x)\\,p(y ) } \\right ) } , \\end{aligned}\\ ] ] where",
    "the sums are taken over all possible values @xmath125 of @xmath91 and @xmath126 of @xmath123 .",
    "remember again that the joint probability @xmath278 is just a two - dimensional matrix where the rows are indexed by @xmath91-values and the columns by @xmath123-values and that each row ( column ) tells us how likely each possible value of @xmath123 ( @xmath91 ) is , given the value @xmath125 of @xmath91 ( @xmath126 of @xmath123 ) determined by the row ( column ) index .",
    "the rows ( columns ) sum to the marginal probability distributions @xmath279 ( @xmath280 ) , that can be written as vectors .",
    "if @xmath91 and @xmath123 are continuous random variables we just replace the sums by integrals and obtain what is known as _ differential mutual information _ : @xmath281    here @xmath278 denotes the joint probability distribution function of @xmath91 and @xmath123 , and @xmath279 and @xmath280 the marginal probability distribution functions of @xmath91 and @xmath123 , respectively .    as we can see , mutual information can be interpreted as the information ( i.e.  entropy ) shared by the two variables , hence its name . like point - wise mutual information it is a symmetric quantity @xmath282 and it is non - negative , @xmath283 .",
    "note though that it is not a metric , as in the general case it does not satisfy the triangle inequality .",
    "furthermore we have @xmath284 and this identity is the reason why entropy is sometimes is also called _ self - information_.    taking the expected value of equation  [ eq : pmi ] and using the notion of conditional entropy we can define mutual information between two random variables as follows .    @xmath285    where in the last two steps the identity @xmath286 was used .",
    "note that equation  [ eq : defmicondent ] is the generalization of equation  [ eq : pmiconditional ] to the case of random variables .",
    "see figure  [ fig : venndiagmi ] for an illustration of how the relation between the different entropies and mutual information .    a possible interpretation of mutual information of two random variables @xmath91 and @xmath123 is to consider it as a measure for the shared entropy between the two variables .     and @xmath287 , the joint entropy @xmath288 , the conditional entropies @xmath248 and @xmath289 , and mutual information @xmath276 .",
    "]      we will look at channels in shannon s sense of communication in the following and relate mutual information to channel capacity .",
    "but rather than looking at the subject in its full generality , we restrict ourselves to discrete , memoryless channels .",
    "the interested reader is pointed to @xcite for a more thorough treatment of the subject .",
    "let us take as usual @xmath91 and @xmath123 for the signal transmitted by some sender and received by some receiver , respectively . in terms of information transmission we can interpret mutual information @xmath276 as the average amount of information the received signal constrains about the transmitted signal , where the averaging is done over the probability distribution of the source signal @xmath96 .",
    "this makes mutual information a function of @xmath96 and @xmath206 and as we know , it is a symmetric quantity .",
    "shannon defines the _ capacity _ @xmath290 of some channel as the maximum amount of information that a signal @xmath123 received by the receiver can contain about the signal @xmath91 transmitted through the channel by the source .    in terms of mutual information @xmath276",
    "we can define the channel capacity as the maximum mutual information @xmath276 among all realizations of the signal @xmath91 .",
    "channel capacity is thus not dependent on the distribution of @xmath96 of @xmath91 but rather a property of the channel itself , i.e.  a property of the conditional distribution @xmath206 and as such asymmetric and causal @xcite .",
    "note that channel capacity is bound from below by @xmath5 and from above by the entropy @xmath231 of @xmath91 , with the maximal capacity being attained by a noise - free channel . in the presence of noise",
    "the capacity is lower .",
    "we will have a look at channels again when dealing with applications of the theory in section  [ sec : neuralsystems ] .",
    "in many applications one is often interested in making values of mutual information comparable by employing a suitable normalization .",
    "consequently , there exists a variety of proposed normalized measures of mutual information , most based on the simple idea of normalizing by one of the entropies that appear in the upper bounds of the mutual information . using the entropy of one variable as a normalization factor ,",
    "there a two possible choices and both were proposed : the so called _ coefficient of constraint _ @xmath291 @xcite    @xmath292    and the _ uncertainty coefficient _",
    "@xmath293 @xcite    @xmath294    these two quantities are obviously non - symmetric but can easily be symmetrized for example by setting    @xmath295    another symmetric normalized measure for mutual information , usually referred to as _ redundancy measure _ , is obtained when normalizing using the sum of the entropy of the variables    @xmath296    note that @xmath297 takes its minimum of @xmath5 when the two variables are independent and its maximum when one variable is completely redundant knowing the other .",
    "note that the list of normalized variants of mutual information given here is far from complete . but as said earlier , the principle behind most normalizations is to use one or a combination of the entropies of the involved random variables as a normalizing factor .",
    "what if we want to calculate the mutual information between not only between two random variables but rather three or more ?",
    "a natural generalization of mutual information to this so called _ multivariate _ case is given by the following definition using conditional entropies and is also called _ multi - information _ or _ integration _ @xcite .",
    "the mutual information of three random variables @xmath298 is given by    @xmath299    where the last term is defined as    @xmath300,\\ ] ]    thee latter being called the _ conditional mutual information _ of @xmath301 and @xmath302 given @xmath303 .",
    "the conditional mutual information @xmath304 can also be interpreted as the average common information shared by @xmath301 and @xmath302 that is not already contained in @xmath303 .",
    "inductively , the generalization to the case of @xmath42 random variables @xmath305 is straightforward :    @xmath306    where the last term is again the conditional mutual information    @xmath307.\\ ] ]    beware that while the interpretations of mutual information directly generalize from the bi - variate case @xmath276 to the multivariate case @xmath308 there is an important difference between the bivariate and the multivariate measure .",
    "whereas mutual information @xmath276 is a non - negative quantity , multivariate mutual information ( mmi for short ) behaves a bit differently than the usual mutual information in the aspect that it can also take negative values which makes this information - theoretic quantity sometimes difficult to interpret .",
    "let us first look at an example of three variables with positive mmi . to make things a bit more hands on , let us look at three binary random variables , one telling us whether it is cloudy , the other whether it is raining and the third one whether it is sunny .",
    "we want to compute @xmath309 . in our model , clouds can cause rain and can block the sun and so we have    @xmath310    as it is more likely that it is raining and there is no sun visible when it is cloudy than when there are no clouds visible .",
    "this results in positive mmi for @xmath309 , a typical situation for a common - cause structure in the variables : here , the fact that the sun is not shining can partly be due to the fact that it is raining and partly due to the fact that there are clouds visible .    in a sense",
    "the inverse is the situation where we have two causes with a common effect : this situation can lead to negative values for the mmi , see @xcite .",
    "in this situation , observing a common effect induces a dependency between the causes that did not exist before .",
    "this fact is called `` explaining away '' in the context of bayesian networks , see @xcite .",
    "pearl @xcite also gives a car - related example where the three ( binary ) variables are `` engine fails to start '' ( @xmath91 ) , `` battery dead '' ( @xmath123 ) and `` fuel pump broken '' ( @xmath311 ) .",
    "clearly , both @xmath123 and @xmath311 can cause @xmath91 and are uncorrelated if we have no knowledge of the value of @xmath91 .",
    "but fixing the common effect @xmath91 , namely observing that the engine did not start , induces a dependency between @xmath123 and @xmath311 that can lead to negative values of the mmi .",
    "another problem with the @xmath42-variate case to keep in mind is the combinatorial explosion of the degrees of freedom regarding their interactions . as a priori every non - empty subset of the variables",
    "could interact in an information - theoretic sense , this yields @xmath312 degrees of freedom .",
    "the _ kullback - leibler divergence _",
    "@xcite ( or _ kl - divergence _ for short ) is a kind of `` distance measure '' on the space of probability distributions : given two probability distributions on the same base space @xmath15 interpreted as two points in the space of all probability distributions over the base set @xmath15 , it tells us how far they are `` apart '' .",
    "we again use the usual expectation - value construction as used for the entropy before .",
    "let @xmath45 and @xmath313 be two discrete probability distributions over the same base space @xmath15 . then the _ kullback - leibler divergence _ of @xmath45 and @xmath313 is given by    @xmath314    the kullback - leibler divergence is non - negative @xmath315 ( and it is zero if @xmath45 equals @xmath313 almost everywhere ) , but it is not a metric in the mathematical sense as in general it is non - symmetric @xmath316 and it does not fulfill the triangle inequality . note that in their original work , kullback and leibler @xcite defined the divergence via the sum    @xmath317    making it a symmetric measure .",
    "@xmath318 is additive for independent distributions , namely    @xmath319    where the two pairs @xmath320 and @xmath321 are independent probability distributions with the joint distributions @xmath322 and @xmath323 , respectively .",
    "note that the expression in equation  [ eq : kldefinition ] is nothing else than the expected value @xmath324 $ ] with the expectation value taken with respect to @xmath45 , which in term can be interpreted as `` expected distance of @xmath45 and @xmath313 '' , measured in terms of the information content .",
    "another interpretation can be given in the language of codes : @xmath318 is the average number of extra bits needed to code samples from @xmath45 using a code book based on @xmath313 .",
    "analogous to previous examples , the kl - divergence can also be defined for continuous random variables in a straightforward way via    @xmath325    where @xmath52 and @xmath326 denote the pdf of two continuous probability distributions @xmath45 and @xmath313 .",
    "expanding the logarithm in equation  [ eq : kldefinition ] we can write the kullback - leibler divergence between two probability distributions @xmath45 and @xmath313 in terms of entropies as    @xmath327    where @xmath52 and @xmath326 denote the pdf or pmf of the distributions @xmath45 and @xmath313 and @xmath328 is the so - called _ cross - entropy _ of @xmath45 and @xmath313 given by    @xmath329    this relation lets us easily compute a closed form of the kl - divergence for many common families of probability distributions .",
    "let us for example look at the value of the kl - divergence between two normal distributions @xmath330 and @xmath331 , see figure  [ fig : klnorm ] .",
    "this can be calculated as    @xmath332    another example : the kl - divergence between two exponential distributions @xmath333 and @xmath334 is    @xmath335    using the kullback - leibler divergence we can give yet another characterization of mutual information : it is a measure of how far two measured variables are from being independent , this time in terms of the kullback - leibler divergence .",
    "@xmath336    thus , mutual information of two random variables can be seen as the kl - divergence of their underlying joint probability distribution from the products of their marginal probability distributions , i.e.  as a measure for how far the two variables are from being independent .      in the past",
    ", mutual information was often used as a measure of information transfer between units ( modeled as random variables ) in some system .",
    "this approach faces the problem that mutual information is a symmetric measure and does not have an inherent directionality . in some applications this symmetry is not desired though , namely whenever we want to explicitly obtain information about the `` direction of flow '' of information , for example to measure causality in an information - theoretic setting , see section  [ sec : causality ] .    in order to make mutual information a directed measure ,",
    "a variant called _ time - lagged mutual information _ was proposed , calculating mutual information for two variables including a previous state of the source variable and a next state of the destination variable ( where discrete time is assumed ) .    yet , as schreiber @xcite points out , while time - lagged mutual information provides a directed measure of information transfer , it does not allow for a time - dynamic aspect as it measures the statically shared information between the two elements . with a suitable conditioning on the past of the variables ,",
    "the introduction of a time - dynamic aspect is possible though . the resulting quantity is commonly referred to as _ transfer entropy _ @xcite .",
    "its common definition is the following .",
    "let @xmath91 and @xmath123 be discrete random variables given on a discrete time scale and let @xmath337 be two natural numbers . then the _ transfer entropy from @xmath123 to @xmath91 with @xmath177 memory steps in @xmath91 and @xmath338 memory steps in @xmath123",
    "_ is defined as    @xmath339    where we denoted by @xmath340 the value of @xmath91 and @xmath123 at time @xmath42 and by @xmath341 the past @xmath177 values of @xmath91 , counted from time @xmath42 on : @xmath342 , and analogously @xmath343 .",
    "although this definition might look complicated at first , the idea behind it is quite simple .",
    "it is merely the kullback - leibler divergence between the two conditional probability distributions @xmath344 and @xmath345 ,    @xmath346    i.e.  a measure of how far the two distributions are from fulfilling the generalized markov property ( see section  [ sec : stochasticproc ] )    @xmath347    note that for small values of transfer entropy , we can say that @xmath123 has little influence on @xmath91 at time @xmath195 , whereas we can say that information is transferred from @xmath123 to @xmath91 at time @xmath195 when the value is large . yet , keep in mind that transfer entropy is just a measure of statistical correlation , see section  [ sec : causality ] .",
    "another interpretation of transfer entropy is seeing it as a conditional mutual information @xmath348 , measuring the average information the source @xmath123 constrains about the next state @xmath349 of the destination @xmath91 that was not contained in the destination s past @xmath350 ( see @xcite ) or alternatively as the average information provided by the source about the state transition in the destination , see @xcite .",
    "as so often before , the concept can be generalized to the continuous case @xcite , although the continuous setting introduces some subtleties that have to be addressed .",
    "from source @xmath123 to target @xmath91 at time @xmath195 as a measure of the average information present in @xmath351 about the future state @xmath352 .",
    "the memory vectors @xmath341 and @xmath353 are shown in gray . ]    concerning the memory - parameters @xmath177 and @xmath338 of the source and the destination , although arbitrary choices are possible , the values chosen fundamentally influence the nature of the questions asked . in order to get correct measures for systems being far from markovian",
    "( i.e.  systems which states are not influenced by more than a certain fixed number of preceding system states ) , high values of @xmath177 have to be used , and for non - markovian systems the case @xmath354 has to be considered .",
    "on the other hand , commonly just one previous state of the source variable is considered , setting @xmath355 @xcite , this being also due to the growing data intensity in @xmath177 and @xmath338 and the usually high computational cost of the method .",
    "note that akin to the case of mutual information , there exist point - wise versions of transfer entropy ( also called _ local transfer entropy _ ) , as well as extensions to the multivariate case , see @xcite .",
    "as we have seen in the preceding sections , one needs to know the full sample spaces and probability distributions of the random variables involved in order to precisely calculate information - theoretic quantities such as the entropy , mutual information or transfer entropy . but",
    "obtaining this data is in most cases impossible in reality , as the spaces are usually high - dimensional and sparsely sampled , rendering the direct methods for the calculation of such quantities impossible to carry out . a way around this problem is to come up with estimation techniques that estimate entropies and derived quantities such as mutual information from the data .",
    "over the last decades a large body of research was published concerning the estimation of entropies and related quantities , leading to a whole zoo of estimation techniques , each class having its own advantages and drawbacks .",
    "so rather than a full overview , we will give a sketch of some central ideas here and give references to further literature .",
    "the reader is also pointed to the review articles @xcite .    before looking at estimation techniques for neural ( and",
    "other ) data let us first give a swift and painless review some important theoretical concepts regarding statistical estimation .      from a statistical point of view",
    ", the process of estimation in its most general form can be regarded in the following setting : we have some data ( say measurements or data obtained via simulations ) that is believed to be generated by some stochastic process with an underlying non - autonomous , i.e. time - dependent , or autonomous probability distribution .",
    "we then want to estimate either the value of some function defined on that probability distribution ( for example the entropy ) or the shape of this probability distribution as a whole ( from which we can then obtain an estimate of a derived quantity ) .",
    "this process is called _ estimation _ and a function mapping the data to an estimated quantities estimator . in this section",
    "will will first look at estimators and their desired properties and then look at what is called maximum likelihood estimation , the most commonly used method for the estimation of parameters in the field of statistics .",
    "let @xmath356 be a set of realizations of the random variable @xmath91 that is believed to have a probability distribution that comes from a family of probability distributions @xmath357 parametrized by a parameter @xmath358 and assume that the underlying probability distribution of @xmath91 is @xmath359 .",
    "let @xmath360 be an estimator for the parameter @xmath358 with the true value @xmath361 .",
    "for the value of the estimated parameter we usually write @xmath362 .",
    "the _ bias _ of @xmath363 is the expected difference between @xmath364 and @xmath361 : @xmath365,\\ ] ] and an estimator with vanishing bias is called _",
    "unbiased_.    we usually also want the estimator to be _ consistent _ , i.e.  we want the estimated value @xmath364 to converge to the value of the true parameter @xmath361 in probability as the sample @xmath125 increases in size , i.e.  as @xmath183 : @xmath366    another important property of an estimator is its variance @xmath367 and an unbiased estimator having the minimal variance among all unbiased estimators of the same parameter is called _",
    "yet another measure often used when assessing the quality of an estimator @xmath192 is its mean squared error    @xmath368    and as we can see , any unbiased estimator with minimal variance minimizes the mean squared error .    without further going into detail here ,",
    "it is noted that there exists a theoretical lower bound to the minimal variance obtainable by an unbiased estimator , the _ cramr - rao bound_. the cramr - rao bound sets the variance of the estimator in relation with the so called _ fisher information _ ( that can be set into relation with mutual information , see @xcite ) .",
    "the interested reader is pointed to @xcite .",
    "_ maximum likelihood estimation _ is the most - widely used estimation technique in statistics and , as we will see in the next few paragraphs , a straightforward procedure that in essence tells us what the most likely parameter value in an assumed family of probability distributions is , given a set of realizations of a random variable that is believed to have a underlying probability distribution from the family considered .    in statistical applications",
    "one often faces the following situation : we have a finite set of realizations @xmath369 of a random variable @xmath91 .",
    "we assume @xmath91 to have a probability distribution @xmath370 in a certain parametrized class of probability distributions @xmath371 , where the true parameter @xmath361 is unknown .",
    "the goal is to get an estimate @xmath364 of @xmath361 using the realizations @xmath369 , i.e.  to do statistical inference of the parameter @xmath358 .",
    "let us consider the so called _ likelihood function _",
    "@xmath372    as a function of @xmath358 .",
    "it is a measure of how likely it is that the parameter of the probability distribution has the value @xmath358 , given the observed realization @xmath125 of @xmath91 . in maximum likelihood estimation , we look for the parameter that maximizes the likelihood function .",
    "this is @xmath364 :    @xmath373    choosing a value of @xmath374 minimizes the kl - divergence between @xmath357 and @xmath359 for all possible values of @xmath358 . the value @xmath364 , often written as @xmath375 is called the _ maximum likelihood estimate _",
    "( mle for short ) of @xmath361 .    in this",
    "setting , one often not uses the likelihood function directly , but works with the @xmath221 of the likelihood function ( this is referred to as log - likelihood ) .",
    "the likelihood functions are often very complicated and situated in high dimensions , making it impossible to find a maximum of the function analytically .",
    "thus , numerical methods ( such as newton s method and variants or the simplex method ) have to be employed in order to find a solution .",
    "these numerical methods work best ( and can be shown to converge to a unique solution ) if the function they operate on is concave ( bowl - shaped , where the closed end is on the top ) . the log - function has the property to make the likelihood function concave in many cases , that being the reason why one considers the log - likelihood function , rather than the likelihood function directly , see also @xcite .",
    "having looked at some core theoretical concepts regarding the estimation of quantities depending on probability distributions let us now come back to dealing with real data .    as in real - world data , the involved probability distributions are often continuous and infinite - dimensional , the resulting estimation problem is very difficult ( if not impossible ) to solve in its original setting . as a remedy ,",
    "the problem is often _",
    "regularized _ ,",
    "i.e.  mapped to a discrete , more easily solvable problem .",
    "this of course introduces errors and often makes a direct estimation of the information - theoretic quantities impossible , but even in that simplified model we can estimate lower bounds of the quantities that by using shannon s _ information processing inequality _",
    "@xcite    @xmath376    where @xmath91 and @xmath123 are ( discrete ) random variables and @xmath104 and @xmath192 are measurable maps .    by choosing the mappings @xmath104 and",
    "@xmath192 as our regularization mappings ( you might also regard them as parameters ) we can change the coarseness of the regularization .",
    "the regularization can be chosen arbitrarily coarse , i.e.  choosing @xmath104 and @xmath192 as constant functions , but this of course comes with a price .",
    "for example in the latter case of constant @xmath104 and @xmath192 the mutual information @xmath377 would be equal to @xmath5 , clearly not a very useful estimate .",
    "this means that a trade - off between complexity reduction and the quality of the estimation has to be made . in general",
    ", there exists no all - purpose recipe for this , each problem requiring an appropriate regularization .",
    "as this discretization technique has become the standard method in many fields , we will solely consider the regularized , discrete case in the following and point the reader to the review article @xcite concerning the continuous case .    in the neurosciences ,",
    "such a regularization technique was also proposed and is known as the `` direct method '' @xcite . here , spike trains of recorded neurons are discretized into time bins of a given fixed width and the neuronal spiking activity is interpreted as a series of symbols from an alphabet defined via the observed spiking pattern in the time bins .",
    "commonly , two different classes of estimation techniques regarding the shape of probability distributions are distinguished .",
    "parametric estimation techniques assume that the probability distribution is contained in some family of probability distributions having some prescribed shape ( see section  [ sec : paramprobdistr ] ) . here , one estimates the value of the parameter from the data observed , whereas non - parametric estimation techniques make no assumptions about the shape of the underlying distribution .",
    "we will solely look at non - parametric estimation techniques in the following as in many cases one tries to not assume prior information about the shape of the distribution .",
    "histogram - based estimation is the most popular and most - widely used estimation technique .",
    "as the name implies , this method uses a histogram obtained from the data to estimate the probability distribution of the underlying random generation mechanism .    for",
    "the following , assume that we obtained a finite set of @xmath378 samples @xmath379 of some real random variable @xmath91 defined over some probability space @xmath69 .",
    "we then divide the domain of @xmath91 into @xmath380 equally sized bins @xmath381 and subsequently count the number of realizations @xmath199 in our data set contained in each each bin . here , the number @xmath382 of bins can be freely chosen .",
    "it controls the coarseness of our discretization , where the limit @xmath383 is the continuous case .",
    "this allows us to define relative frequencies of occurrences for @xmath91 with respect to each bin that we interpret as estimations @xmath384 ( note that we make the dependence on the number of bins @xmath382 explicit in the notation ) of the probability of @xmath91 taking a value in bin @xmath385 which we denote by @xmath386 .",
    "the law of large numbers then tells us that our estimated probability values converge to the real probabilities as @xmath387 .",
    "note that although histogram - based estimations are usually called non - parametric as they do not assume a certain shape of the underlying probability distribution , they do have parameters , namely one parameter for each bin , the estimated probability value @xmath384 .",
    "these estimates @xmath384 can also be interpreted as maximum - likelihood estimates of @xmath388 .",
    "the following defines an estimator of the entropy based on the histogram .",
    "it is often called `` plug in '' estimator :    @xmath389    the are some problems with this estimator @xmath390 , though .",
    "its convergence to the true value @xmath231 can be slow and it is negatively biased , i.e.  its value is almost always below the true value @xmath231 , see @xcite .",
    "this shift can be quite significant even for large @xmath378 , see figure  [ fig : hest ] and @xcite .",
    "more specifically , one can show that the expected value of the estimated entropy is always smaller than the true value of the entropy ,    @xmath391\\leq h(x),\\ ] ]    where the expectation value is taken with respect to the true probability distribution @xmath45 .",
    "bias generally is a problem for history - based estimation techniques @xcite and although we can correct for the bias , this may not always be a feasible solution @xcite .",
    "none the less we will have a look at a bias - corrected version of the estimator given in equation  [ eq : hxmle ] below .     of the entropy of a given distribution with true entropy @xmath392 bits .",
    "estimated values are shown for three different sample sizes @xmath378 . adapted from @xcite , figure  1 . ]    as a remedy to the bias problem , miller and madow @xcite calculated the bias of the estimator of equation  [ eq : hxmle ] and came up with a bias - corrected version of the maximum likelihood estimator for the entropy , referred to as _ miller - madow _ estimator :    @xmath393    where @xmath394 is an estimate of the number of bins with non - zero probability .",
    "we will not go into the detail of the method here , the interested reader is referred to @xcite .",
    "another way of bias - correction @xmath390 is the so called `` jack - knifed '' version of the maximum - likelihood estimator by efron and stein @xcite :    @xmath395    yet another bias - corrected variant of the mle estimator based on polynomial approximation is presented in @xcite , for which also bounds on the maximal estimation error were derived .    in an effort to overcome the problems faced by histogram - based estimation",
    ", many new and more powerful estimation techniques have emerged over the last years , both for entropy and other information - theoretic quantities . as our focus here",
    "is to give an introduction to the field , we will not review all of those methods here but rather point the interested reader to the literature , where a variety of approaches are discussed .",
    "there exist methods based on the idea of adaptive partitioning of sample space @xcite , ones using entropy production rates and allowing for confidence intervals @xcite , ones using bayesian methods @xcite and ones based on density estimation using nearest - neighbors @xcite , along with many more .",
    "see @xcite for an overview concerning several estimation techniques for entropy and mutual information .",
    "we note here that in contrast to estimations of entropy , estimators of mutual information are usually positively biased , i.e.  tend to overestimate mutual information .",
    "some time after its discovery by shannon , neuroscientists started to recognize information theory as a valuable mathematical tool to assess information processing in neural systems . using information theory , several questions regarding information processing and the neural code",
    "can be addressed in a quantitative way , among those    * how much information single cells or populations carry about a stimulus and how this information is coded , * what aspects of a stimulus are encoded in the neural system and * how `` effective connectivity '' @xcite in neural systems can be defined via causal relationships between units in the system .",
    "see figure  [ fig : shannonchannel - neuron ] for an illustration of how shannon s theory can be used in a neural setting .",
    "attneave @xcite and barlow @xcite were the first to consider information processing in neural systems from an information - theoretic point of view .",
    "subsequently , eckhorn and ppel  @xcite applied information - theoretic methods to electrophysiologically recorded data of neurons in a cat . but being data - intensive in nature these methods faced some quite strong restrictions during that time , namely the limited amount of computing power ( and computer memory ) and the limited amount ( and often low quality ) of data obtainable via measurements at that time .",
    "but over the last decades , available computing became more and more available and classical measurement techniques were improved , along with new ones emerging such as fmri , meg and calcium imaging .",
    "this made information theoretic analyses of neural systems more and more feasible and through the invention of recording techniques such as meg and fmri it is nowadays even possible to perform such analyses on a system - scale for the human brain in vivo . yet , even with the newly available recording techniques today there are some conceptual difficulties with information - theoretic analyses as it is often a challenge to obtain enough data in order to get good estimates of information - theoretic quantities .",
    "special attention has to be paid to using the data efficiently and the validity of such analyses has to be assessed to their statistical significance .    in the following we will discuss some conceptual questions relevant when regarding information theoretic analyses of neural systems .",
    "more detailed reviews can be found in @xcite .",
    "marr described `` three levels at which any machine carrying out an information - processing task must be understood '' @xcite[chapter  1.2 ] .",
    "they are :    1 .   computational theory : what is the goal of the computation , why is it appropriate , and what is the logic of the strategy by which it can be carried out ? 2 .   representation and algorithm",
    ": how can this computational theory be implemented ?",
    "in particular , what is the representation for the input and output , and what is the algorithm for the transformation ?",
    "hardware implementation : how can the representation and algorithm be realized physically ?",
    "when performing an information - theoretic analysis of a system one naturally faces the fundamental problem related to the coding of the information : in order to calculate ( i.e.  estimate ) information theoretic quantities , one has to define a family of probability distributions over the state space of the system , each member of that family describing one system state that is to be considered .",
    "as we know , all information - theoretic quantities such as entropy and mutual information ( between the system state and the state of some external quantity ) are determined by the probability distributions involved .",
    "the big question now is how to define the system state in the first point , a question which is especially difficult to answer in the case of neural systems on all scales .",
    "one possible way to construct such a probabilistic model for a sensory neurophysiological experiment involving just one neuron is the following . typically , the experiment consists of many trials , where per trial @xmath396 in some defined time window a stimulus @xmath397 is presented eliciting a neural response @xmath398 consisting of a sequence of action potentials . presenting the same stimulus @xmath104 many times",
    "allows for the definition of a probability distribution of responses @xmath399 of the neuron to a stimulus @xmath104 .",
    "this is modeled as a conditional probability distribution @xmath400 .",
    "as noted earlier , we usually have no direct access to @xmath400 but rather have to find an estimate @xmath401 from the available data .",
    "note that in practice , usually the joint probability distribution @xmath402 is estimated and estimates of conditional probability distributions are subsequently obtained from the estimate of the joint distribution .",
    "let us now assume that the stimuli are drawn from the set of stimuli @xmath403 according to some probability distribution @xmath404 ( that can be freely chosen by the experimenter ) .",
    "we can then compute the mutual information between the stimulus ensemble @xmath104 and its elicited response @xmath399    @xmath405    using the probability distributions @xmath404 and @xmath401 , see section  [ sec : mutualinformation ] .    as usual , by mutual information",
    "we assess the expected shared information between the stimulus and its elicited response averaged over all stimuli and responses . in order to break this down to the level of single stimuli we can either consider the point - wise mutual information or employ one of the proposed decompositions of mutual information such as _ stimulus - specific information _ or _ stimulus - specific surprise _ , see @xcite for a review .    having sketched the general setting let us come back to the question of coding of information by the neurons involved .",
    "this is important as we have to adjust our model of the neural responses accordingly , the goal being to capture all relevant features of the neural response in the model .    regarding neural coding ,",
    "there are two main hypotheses of how single neurons might code information : neurons could use a _ rate code _ , i.e.  encode the information via their mean firing rates neglecting the timing patterns of spikes or",
    "they could employ a _ temporal code _",
    ", i.e.  a code where the precise timing of single spikes plays an important role .",
    "yet another hypothesis would be that neurons code information in bursts of spikes , i.e.  groups of spikes emitted in a small time window , which is a variant of the time code . for questions regarding coding in populations",
    "see the review @xcite .",
    "note that the question of neural coding is a highly debated one in the neurosciences as of today ( see @xcite ) and we do not want to favor one view point over the other in the following . as with many things in nature",
    "there does not seem to be a clear black an white picture regarding neuronal coding .",
    "rather it seems that a gradient of different coding schemes is employed depending on which sensory system is considered and at which stage of neuronal processing , see @xcite .",
    "let us now compute the entropy of spike trains and subsequently single spikes , assuming that the neurons we model either employ a rate or a time code .",
    "we are especially interested in the maximal entropy attainable by our model spike trains as these can give us upper bounds for the amount of information such trains and even single spikes can carry in theory .",
    "the following examples here are adapted from @xcite . concerning the topics of spike trains and their analysis , the interested reader is also pointed to @xcite .",
    "first , we define a model for the spike train emitted by a neuron measured for some fixed time interval of length @xmath192 .",
    "we can consider two different models for the spike train , a continuous and a discrete one . in the continuous case , we model each spike by a dirac delta function and the whole spike train as a combination of such functions .",
    "the discrete model is obtained from the continuous one by introducing small time bins of size @xmath406 in a way that one bin can at most contain one spike , say @xmath407 ms .",
    "we then assign to each bin in which no spike occurred a value of @xmath5 and ones in which a spike occurred a value of @xmath0 , see figure  [ fig : spikebins ] .",
    "let us use this discrete model for the spike train of a neuron , representing a spike train as a binary string @xmath104 in the following .",
    "fixing the time span to be @xmath192 and the bin width to be @xmath406 , each spike train @xmath104 has length @xmath408 .",
    "we want to calculate the maximal entropy among all such spike trains @xmath104 , subject to the condition that the number of spikes in @xmath104 is a fixed number @xmath409 which we call the spike rate of @xmath104 .",
    "let us now calculate the entropy in the firing pattern of a neuron of which we assume that spike timing carries important information , i.e.  a neuron employing a time code . in order to keep the model simple , let us further assume that the spiking behavior is not restricted in any way , i.e.  that all possible binary strings @xmath104 are equiprobable .",
    "then we can calculate the entropy of this uniform probability distribution @xmath45 as    @xmath410    where @xmath411 denotes the binomial coefficient @xmath412 , the number of all distinct binary strings of length @xmath378 having exactly @xmath413 non - zero entries .",
    "the entropy in equation  [ eq : entropytimecode ] can be approximated by    @xmath414    where @xmath222 denotes the natural logarithm to the base @xmath415 .",
    "the expression in equation  [ eq : entropytimeln ] is obtained by using the approximation formula    @xmath416    which is valid for large @xmath42 and @xmath177 and in turn based on stirling s approximation formula for @xmath417 .",
    "see figure  [ fig : entropyrate]a for the maximum entropy attainable by the time code as a function of bin size @xmath406 for different firing rates @xmath413 .     as a function of the size @xmath406 of the time bins .",
    "( b ) rate code using poisson and exponential spiking statistics .",
    "figure adapted from @xcite fig . d.4 . ]    on the other hand , modeling a neuron that reacts to different stimuli with a graded response it its firing rate is usually done using a rate code . assuming a rate code where the timing of spikes does not play any role yields different results , as we will see in the following , see figure  [ fig : entropyrate]b . in the rate code only the number of spikes @xmath378 occurring in a given time interval of length @xmath192 matters , i.e.  we consider probability distributions @xmath418 parametrized by @xmath378 and @xmath192 describing how likely the occurrence of @xmath378 spikes in a time window of length @xmath192 is .",
    "being well - backed with experimental data @xcite , a popular choice of @xmath418 is taking a poisson distribution with some fixed mean @xmath419 , where @xmath413 is thought of as the mean firing rate of the neuron .",
    "the probability @xmath420 of observing @xmath42 spikes in an interval of length @xmath192 now is given by the pmf of the poisson distribution    @xmath421    and the entropy of @xmath418 computes as    @xmath422    again using stirling s formula this can be written as    @xmath423    dividing the entropy @xmath424 by the number of spikes that occurred yields the entropy per spike .",
    "see figure  [ fig : entropyrate]b for a plot of the entropy per spike as a function of the number of observed spikes .",
    "an interesting question is to ask for the maximal information ( i.e.  entropy ) that spike trains can carry , assuming a rate code . assuming continuous time and prescribing mean and variance of the firing rate , this leaves the exponential distribution @xmath425 as the one with the maximal entropy .",
    "the entropy of an exponentially distributed spike train with mean rate @xmath426 is    @xmath427    see also figure  [ fig : entropyrate]b .",
    "note that while it was possible to compute the exact entropies in the preceding as we assumed full knowledge of the underlying probability distributions .",
    "this is of course not the case for data obtained by recordings . here",
    "the estimation of entropies faces the bias - related problems of sparsely sampled probability distributions as discussed earlier . concerning entropy estimation in spike trains the reader",
    "is also pointed to @xcite .",
    "the principle of efficient coding @xcite ( also called _ infomax principle _ ) was first proposed by attneave and barlow .",
    "it views the early sensory pathway as a channel in shannon s sense and postulates that early sensory systems try to maximize information transmission under the constraint of an efficient code , i.e.  that neurons maximize mutual information between a stimulus and their output spike train , using as few spikes as possible .",
    "this minimization of spikes for a given stimulus results in a maximal compression of the stimulus data , minimizing redundancies between different neurons on a population level . one key prediction of this optimality principle is that neurons involved in the processing of stimulus data ( and ultimately the whole brain ) is adapted to natural stimuli , i.e. some form of natural ( and structured ) sensory input such as sounds or images rather than noise .",
    "for some sensory systems it could be shown that there is strong evidence that early stages of processing indeed perform an optimal coding , see e.g.  @xcite . while first mainly the visual system was studied and it was shown that the infomax principle holds here @xcite , other sensory modalities were also considered in the following years @xcite .",
    "but whereas the infomax principle could explain certain experimental findings in the early sensory processing stages , the picture becomes less clear the more upstream the information processing in neural networks is considered . here ,",
    "other principles were also argued for , see for example @xcite .",
    "on the system - level , friston et al .",
    "@xcite proposed an information theoretic measure of free energy in the brain , that can be understood as generalization of the concept of efficient coding . also arguing for optimal information transfer",
    ", norwich @xcite gave a theory of perception based on information - theoretic principles .",
    "he argues that the information present in some stimulus is relayed to the brain by the sensory system with negligible loss .",
    "many empirical equations of psychophysics can be derived from this model .",
    "there are many scales at which information - theoretic analyses of neural systems can be performed . from the level of a single synapse @xcite over the level of single neurons @xcite over the population level @xcite up to the system level @xcite . in the former cases the analyses are usually carried out on electrophysiologically recorded data of single cells , whereas on the system level data",
    "is usually obtained by eeg , fmri or meg measurements .",
    "notice that most of the information - theoretic analyses of neural systems were done for early stages of sensory systems , focusing on the assessment of the amount of mutual information between some stimulus and its neural response . here",
    "different questions can be answered , about the nature and efficiency of the neural code and the information conveyed by neural representations of stimuli , see @xcite .",
    "this stimulus - response - based approach has already provided a lot of insight into the processing of information in early sensory systems , but things get more and more complicated the more downstream an analysis is performed @xcite .    on the systems level , the abilities of neural systems to process and store information are due to interactions of neurons , populations of neurons and sub - networks .",
    "as these interactions are highly - nonlinear and in contrast to the early sensory systems neural activity is mainly driven by the internal network dynamics ( see @xcite ) , stimulus - response - type models often are not very useful here . here , transfer entropy has proven to be a valuable tool here , making analyses of information transfer in the human brain in vivo possible @xcite .",
    "transfer entropy can also be used as a measure for causality , as we will discuss in the next section .      the idea of _ causality _ , namely the question of what are the causes resulting in the observable state and dynamics of complex systems of physical , biological or social nature is a deep , philosophical question that has been driving scientists in all fields ever since . in a sense",
    "this question lies at the heart of science itself and as such is often notoriously difficult to answer .    in the neurosciences ,",
    "this principle is related to one of the core questions of neural coding and subsequently neural information processing : what stimuli make neurons spike ( or change their membrane potential , for non - spiking neurons ) ? for many years now , neuroscientists have investigated neurophysiological correlates of information presented to a sensory system in form of stimuli .    while considerable progress has been made regarding the answer to this question in the early stages of sensory processing ( see the preceding sections ) , where often a clear correlation between a stimulus and the resulting neuronal activity could be found , things get less and less clear the further downstream this question is addressed . in the latter case , neuronal activity is subject to higher and higher degrees of internal dynamics and a clear stimulus - response relation is often lacking .    considering early sensory systems , even though merely a correlation between a stimulus and neural activity can be measured ,",
    "it is justified to speak of causality here , as it is possible to actively influence the stimulus and observe the change in neural activity .",
    "note that the idea of intervention is crucial here , see @xcite .",
    "looking at more downstream systems or at the cognitive level , an active intervention albeit possible ( but often not as directly as for sensory systems ) may not have the same easy to detect effects on system dynamics . here , often just statistical correlations can be observed and in most cases it is very hard if not impossible to show that the principle of causality in its purest form holds .",
    "yet , one can still make some statements regarding what one might call `` statistical causality '' in this case , as we will see .    in an attempt to give a statistical characterization of the notion of causality",
    ", the mathematician wiener @xcite came up with the following probabilistic framing of this concept that came to be known as _ wiener causality _ : consider two stochastic processes @xmath428 and @xmath429 .",
    "then @xmath123 is said to wiener - cause @xmath91 if the knowledge of past values of @xmath123 diminishes uncertainty about the future values of @xmath91 .",
    "note that wiener causality is a measure of predictive information transfer and not one of causality and thus the naming is a bit unfortunate , see @xcite .",
    "the economist granger employed wiener s principle of causality and developed the notion of what is nowadays called _ wiener - granger causality _",
    "subsequently , the linear wiener - granger causality and its generalizations were often employed as measure of statistical causality in the neurosciences , see @xcite . another model for causality in the neurosciences",
    "is _ dynamic causal modeling _ @xcite .",
    "in contrast to dynamic causal modeling , causality measures based on information - theoretic concepts are usually purely data - driven and thus inherently model - free @xcite .",
    "this fact can be of advantage in some cases but we do not want to make a judgment here , calling one method better per se , as each has its advantages and drawbacks @xcite .",
    "the directional and time - dynamic nature of transfer entropy allows using it as a measure of wiener - causality , as was proposed in the field of neurosciences recently @xcite . as such",
    ", transfer entropy can be seen as a non - linear extension of the concept of wiener - granger causality , see @xcite for an comparison of transfer entropy to other measures .",
    "note again that transfer entropy still essentially is a measure of conditional correlation rather than one of direct effect ( i.e.  causality ) and that correlation is not causation .",
    "thus it is a philosophical question to which extent transfer entropy can be used to infer some form of causality , a question that we will not further pursue here , rather pointing the reader to @xcite .    in any case",
    "the statistical significance of the inferred causality ( remember that transfer entropy just measures conditional correlation ) has to be verified . for trial based data - sets as often found in the neurosciences ,",
    "this testing is usually done against the null - hypothesis @xmath430 of average transfer entropy obtained by random shuffling of the data .",
    "given the fact that information - theoretic analyses can provide insights about the functioning of neural systems , the next logical step is to ask how these this might help in better understanding neural dysfunction and neural diseases , maybe even giving hints to new treatments .    the field one might call `` computational neuroscience of disease '' is an emerging field of research within the neurosciences , see the special issue of _ neural networks _ @xcite .",
    "the discipline faces some hard questions as in many cases dysfunction is observed on the cognitive ( i.e.  systems- ) level but has causes on many scales of neural function ( sub - cellular , cellular , population , system ) .    over the last years",
    ", different theoretical models regarding neural dysfunction and disease were proposed , among them computational models applicable to the field of psychiatry @xcite , models for brain lesions @xcite , models of epilepsy @xcite , models for deep brain stimulation @xcite , models for aspects of parkinson s @xcite and alzheimer s @xcite disease , of abnormal auditory processing @xcite and for congenital prosopagnosia ( a deficit in face identification ) @xcite .",
    "some of these models employ information - theoretic ideas in order to assess differences between the healthy and dysfunctional states @xcite .",
    "for example , information - theoretic analyses of cognitive and systems - level processes in the prefrontal cortex were carried out recently @xcite and differences in information - processing could be assessed between the healthy and dysfunctional system by means of information - theory @xcite .    yet",
    ", computational neuroscience of disease is a very young field of research and it remains to be elucidated if and in what way analyses of neural systems employing information - theoretic principles could be of help in medicine on a broader scale .",
    "there exist several open source software packages that can be used to estimate information - theoretic quantities of neural data .",
    "the list below is by no means complete , but should give a good overview of things , see also @xcite .    * ` entropy : entropy and mutual information estimation ` + url : http://cran.r-project.org/web/packages/entropy + authors : jean hausser and korbinian strimmer .",
    "+ type : r package + from the website : this package implements various estimators of entropy , such as the shrinkage estimator by hausser and strimmer , the maximum likelihood and the millow - madow estimator , various bayesian estimators , and the chao - shen estimator .",
    "it also offers an r interface to the nsb estimator .",
    "furthermore , it provides functions for estimating mutual information . * ` information - dynamics - toolkit ` + url : http://code.google.com/p/information-dynamics-toolkit + author : joseph lizier + type : standalone java software + from the website : provides a java implementation of information - theoretic measures of distributed computation in complex systems : i.e. information storage , transfer and modification .",
    "includes implementations for both discrete and continuous - valued variables for entropy , entropy rate , mutual information , conditional mutual information , transfer entropy , conditional / complete transfer entropy , active information storage , excess entropy / predictive information , separable information . * ` ite ( information theoretical estimators ) ` + url : https://bitbucket.org/szzoli/ite/ + author : zoltn szab + type : matlab / octave plugin + from the website : ite is capable of estimating many different variants of entropy , mutual information and divergence measures . thanks to its highly modular design",
    ", ite supports additionally the combinations of the estimation techniques , the easy construction and embedding of novel information theoretical estimators , and their immediate application in information theoretical optimization problems .",
    "ite can estimate shannon- , rnyi entropy ; generalized variance , kernel canonical correlation analysis , kernel generalized variance , hilbert - schmidt independence criterion , shannon- , l2- , rnyi- , tsallis mutual information , copula - based kernel dependency , multivariate version of hoeffding s phi ; complex variants of entropy and mutual information ; l2- , rnyi- , tsallis divergence , maximum mean discrepancy , and j - distance .",
    "ite offers solution methods for independent subspace analysis ( isa ) and its extensions to different linear- , controlled- , post nonlinear- , complex valued- , partially observed systems , as well as to systems with nonparametric source dynamics . * ` pyentropy ` + url : http://code.google.com/p/pyentropy + authors : robin ince , rasmus petersen , daniel swan , stefano panzeri + type : python module + from the website : pyentropy is a python module for estimating entropy and information theoretic quantities using a range of bias correction methods . * ` spike train analysis toolkit ` + url : http://neuroanalysis.org/toolkit + authors : michael repucci , david goldberg , jonathan victor , daniel gardner + type : matlab / octave plugin + from the website : information theoretic methods are now widely used for the analysis of spike train data .",
    "however , developing robust implementations of these methods can be tedious and time - consuming . in order to facilitate further adoption of these methods ,",
    "we have developed the spike train analysis toolkit , a software package which implements several information - theoretic spike train analysis techniques .",
    "* ` trentool ` + url : http://trentool.de + authors : michael lindner , raul vicente , michael wibral , nicu pampu and patricia wollstadt + type : matlab plugin + from the website : trentool uses the data format of the open source matlab toolbox fieldtrip , that is popular for electrophysiology data ( eeg / meg / lfp ) .",
    "parameters for delay embedding are automatically obtained from the data . te values",
    "are estimated by the kraskov - stgbauer - grassberger estimator and subjected to a statistical test against suitable surrogate data .",
    "experimental effects can then be tested on a second level .",
    "results can be plotted using fieldtrip layout formats .",
    "the author would like to thank _ nihat ay _ , _ yuri campbell _ , _ jrg lehnert _ , _ timm lochmann _ , _ wiktor mynarski _ and _ carolin stier _ for their useful comments on the manuscript ."
  ],
  "abstract_text": [
    "<S> given the constant rise in quantity and quality of data obtained from neural systems on many scales ranging from molecular to systems , information - theoretic analyses became increasingly necessary during the past few decades in the neurosciences . </S>",
    "<S> such analyses can provide deep insights into the functionality of such systems , as well as a rigid mathematical theory and quantitative measures of information processing in both healthy and diseased states of neural systems . </S>",
    "<S> this chapter will present a short introduction to the fundamentals of information theory , especially suited for people having a less firm background in mathematics and probability theory . to begin , the fundamentals of probability theory such as the notion of probability , probability distributions , and random variables will be reviewed . </S>",
    "<S> then , the concepts of information and entropy ( in the sense of shannon ) , mutual information , and transfer entropy ( sometimes also referred to as conditional mutual information ) will be outlined . as these quantities can not be computed exactly from measured data in practice , estimation techniques for information - theoretic quantities </S>",
    "<S> will be presented . </S>",
    "<S> the chapter will conclude with the applications of information theory in the field of neuroscience , including questions of possible medical applications and a short review of software packages that can be used for information - theoretic analyses of neural data . </S>"
  ]
}