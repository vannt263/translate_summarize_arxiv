{
  "article_text": [
    "we derive a network of neurons with standard spike - generating currents and synapses with realistic timescales that computes based upon the principle that the precise timing of each spike is important for the computation .",
    "we then show that our network reproduces a number of key features of cortical networks including irregular , poisson - like spike times and a tight balance between excitation and inhibition .",
    "these results significantly increase the biological plausibility of the spike - based approach to network computation , and uncover how several components of biological networks may work together to efficiently carry out computation .",
    "neural networks transform their inputs through a variety of computations from the integration of stimulus information for decision - making @xcite to the persistent activity observed in working memory tasks  @xcite .",
    "how such transformations occur in biological networks has not yet been understood .",
    "such operations have been proposed to be carried out by the averaged firing rates of neurons in a network ( a  rate model \" ) @xcite .",
    "for example , persistent activity may be realized in a rate model by including recurrent connections that balance the intrinsic leak of the system @xcite .",
    "however , most real neural circuits consist of spiking neurons . spiking network implementations of rate model operations",
    "can be constructed by assuming that the computation is distributed among a large population of functionally similar neurons , so that the averaged firing rate matches that of the desired rate model @xcite .",
    "rate - based approaches have been used to model a variety of behaviors including persistent activity in the oculomotor integrator @xcite , decision - making @xcite , and working memory @xcite .",
    "while rate models capture features of both psychophysical and electrophysiological data , such approaches have a few potential limitations .",
    "first , any rate - based approach disregards the timing of individual spikes , and hence any capacity to compute that precise timing may confer .",
    "second , the performance of rate models is typically quite sensitive to the choice of connection weights between neural populations @xcite .",
    "if the recurrent connections are either too strong or too weak , the activity of the network can either quickly increase to saturation or decrease to a baseline level .",
    "further , spiking network implementations of rate - based networks typically ( though not always , e.g. @xcite ) require strong added noise to match the irregular firing observed in cortical networks @xcite .",
    "this injected noise often dominates the feed - forward or intrinsic currents generated in the network , diminishing the accuracy with which inputs can be integrated or maintained over time .",
    "recently , boerlin et al .",
    "( 2013 ) have proposed a distinct alternative by assuming that a computation is carried out directly by the spiking times of individual neurons . based upon the premise that the membrane potentials of neurons in the network track a prediction error between a desired output and the network estimate , and that neurons spike only if that error exceeds a certain value , boerlin et al .",
    "( 2013 ) derived a spiking neural network that can perform any linear computation . in this",
    "_ predictive coding _",
    "approach , the computation error is mapped to the voltage of integrate - and - fire ( if ) neurons , while a bound on this error is mapped to the neuron s threshold .",
    "this leads to a recurrent network of if neurons with a mixture of instantaneous and exponential synapses that is able to reproduce many features of cortical circuits while performing a variety of linear computations including pure and leaky integration , differentiation , and transforming inputs into damped oscillations .",
    "furthermore , as the computation is efficiently distributed among the participating neurons , the network is robust to perturbations such as lesions and synaptic failure .",
    "nevertheless , two components of this work potentially limit its implementation in biological networks : neurons communicate instantaneously , while true synaptic dynamics occur with a finite timescale ; and the threshold of if neurons is set arbitrarily , rather than being established by intrinsic nonlinear spike - generating kinetics .    here ,",
    "starting from the same spike - based framework @xcite , we derive a computing network of neurons with standard spike - generating currents  @xcite and synapses with realistic timescales . like in many cortical networks ,",
    "the spike times of the model network are irregular and there is a tight balance between excitation and inhibition @xcite .",
    "moreover , the precise timing of spikes is important for accurate decoding : the network actively produces correlations in the spike times of different neurons which act to reduce the decoding variance .",
    "taken together , the results uncover how several components of biological networks may work together to efficiently carry out computation .",
    "here , we follow boerlin et al . ( 2013 ) to construct a spiking network that implements the computation of a @xmath0-dimensional linear dynamical system .",
    "we define the target system as    @xmath1    where @xmath2 is a @xmath0-dimensional vector of functions of time , @xmath3 is a @xmath0-dimensional vector of stimulus inputs , and @xmath4 is a @xmath5 matrix ( with units of @xmath6 ) that determines the linear computation .",
    "for example , if @xmath4 is the zero matrix , then the computation would be pure integration with @xmath2 being the integral of the stimulus inputs @xmath3 .",
    "the dynamic variables @xmath7 are unitless while time has units of seconds . we want to build a network of @xmath8 neurons such that an estimate of the dynamic variable @xmath9 can be read out from the network s spike trains @xmath10 , where @xmath11 indexes the @xmath8 neurons .",
    "we assume that the dynamics of the network estimate @xmath12 are given by :    @xmath13    where @xmath14 , @xmath15 is the heaviside function , @xmath16 , @xmath17 , and @xmath18 are constants that are defined below , and @xmath19 is a @xmath20 dimensional decoding matrix . in the original lif network @xcite , @xmath21 .",
    "the solution of the above equation , assuming @xmath22 , is given by the convolution of the network s spike trains with a double - exponential function    @xmath23    where    @xmath24    and @xmath18 is a constant so that the maximum of the double - exponential function is @xmath25 , @xmath16 ( @xmath17 ) is the rate of rise ( decay ) of the double - exponential function . note that the normalization term @xmath26 in the definition of @xmath27 comes from the fact that we wanted @xmath28 to have the form given above .",
    "in what follows , we will show that this alteration to the decoder dynamics will result in a neuronal network with finite time - scale synapses .",
    "we now derive network dynamics such that neurons spike in order to reduce the error between the signal @xmath2 and the estimate @xmath29 . defining the error function @xmath30 as    @xmath31",
    "our goal is to derive conditions under which cell @xmath11 spikes only if the error is reduced by doing so : @xmath32 .",
    "when cell @xmath11 spikes at time @xmath33 , this changes @xmath34 .",
    "thus , we need to find conditions such that    @xmath35    up to this point , our derivation is nearly identical to that of boerlin et al .",
    "( 2013 ) , except for the use of the double - exponential function synapse .",
    "however , we must now alter the above condition in order to account for the fact that the double - exponential function synapse has a finite rise time . more specifically , since @xmath28 is equal to zero at the time of the spike , the terms on either side of the above inequality are equal ( since @xmath36 for @xmath37 ) .",
    "in contrast , boerlin et al . used exponential synapses which have an infinitely fast rise time and thus yield a non - zero contribution at the time of a spike .",
    "thus , in order to account for the effects of the spike at time @xmath33 on the error , we need to extend the integration a short time @xmath38 into the future :    @xmath39    after some algebra , and using the fact that @xmath36 for @xmath37 , this leads to    @xmath40    since @xmath38 is assumed to be small , we approximate the above integral using the trapezoidal rule    @xmath41    other integral approximations lead to similar results .",
    "using the fact that @xmath38 is small , we can taylor expand @xmath42 and @xmath43 to first order    @xmath44\\right)t^ * > \\frac{1 } { 2}\\left(\\sum_{j=1}^j\\gamma_{jk}^2\\alpha^2(t^*)\\right)t^*,\\ ] ]    where we used the fact that @xmath45 . dividing both sides of the above equation by @xmath46 we arrive at    @xmath47    lastly , we drop terms of order @xmath38 and define    @xmath48    with the condition that neuron @xmath11 spikes when it reaches threshold @xmath49 .",
    "the network dynamics are given by differentiating the above equation    @xmath50    to close the problem using only information available to the network , we replace the desired signal with the spike - based _ estimate _ of the signal , @xmath51 :    @xmath52    the above form highlights the fact that there are now two different kinds of synapses in our network : double - exponential `` slow '' synapses and exponential `` fast '' synapses .",
    "the reason why these two types of synapses arise is because both @xmath29 and its temporal derivative appear in the equation for the voltage dynamics .",
    "if we had chosen to decode the spike trains using an exponential kernel as in @xcite , we would end up with exponential slow synapses and @xmath53-function fast synapses .    in previous approaches , the neurons voltage ",
    "reset \" following spikes arose from autaptic ( i.e. , from a neuron to itself ) input currents via the delta - function synapses just discussed .",
    "such fast synapses do not occur in our derivation . to obtain an analogous reset condition , we would need to include an additional , explicit reset current in our voltage equation .",
    "this would result in :    @xmath54    where the term @xmath55 resets neuron @xmath11 to @xmath56 once it reaches threshold @xmath57 .",
    "we illustrate this particular reset rule because it matches that of @xcite .",
    "however , in the next section we will remove this reset term and replace it with more biologically realistic ionic currents .",
    "next , we rescale the voltage to be in terms of @xmath58 ( recall that time is in units of seconds ) . to do so , we introduce the scaling @xmath59 ( where @xmath60 has units of @xmath58 ) which leads to    @xmath61,\\ ] ]    where the threshold voltage is @xmath60 and the reset voltage is @xmath62 .",
    "the parameter @xmath60 also modifies the gain of the synaptic input .",
    "however , it is also linked to the value of the voltage threshold and reset potential . finally , to frame the network equations in terms of current , we multiply both sides by the membrane capacitance @xmath63 ( in units of @xmath64 )    @xmath65.\\ ] ]      we began by deriving a network of neurons that do not contain any intrinsic biophysical currents and solely integrate their synaptic input prior to spiking . to incorporate the nonlinear dynamics of spike - generating ion channels , we now replace the reset currents @xmath66 with generic hodgkin - huxley - type ( hh - type ) ionic currents @xmath67 ( see section for a specific example )    @xmath68,\\ ] ]    where the @xmath69 in @xmath67 represent the gating variables for standard hh currents . for example , @xmath70 $ ] for the hh - type model we consider here ( see section ) . for simplicity , we assume that every neuron in the network has the same type of spike - generating currents @xmath67 . note that if we wanted to use a leaky - integrate - and - fire neuron , we would set @xmath71 , where @xmath72 is the conductance of the leak channel ( in @xmath73 ) , @xmath74 is the leak channel reversal potential , and we used the same reset current we previously described . as stated above , for standard hh - type model currents there is no longer a need for a reset current as the spiking process is carried out by the intrinsic currents .",
    "next , we add a white noise current to our voltage equations .",
    "this is meant to roughly model a combination of background synaptic input , randomness in vesicle release , and stochastic fluctuations in ion channel states ( channel noise ) , but also contributes to computation in our networks by helping to prevent synchrony ( see results section ) .",
    "the result is :    @xmath75+\\sigma_v\\xi_k(t),\\ ] ]    where @xmath76 is white noise ( @xmath77 and @xmath78 ) and @xmath79 has units of @xmath80 .",
    "lastly , to emphasize the fact that the input to the system @xmath3 has the physical interpretation of current , we introduce the scaling @xmath81 where @xmath82 has units of @xmath83 and @xmath84 has units of @xmath58 and scales the stimulus input into neurons in our network .",
    "thus , we rewrite the above equation as    @xmath85+\\frac{\\gamma_{k}^t } { t_k}\\frac{g } { c_0}\\mathbf{c}(t)+\\sigma_v\\xi_k(t).\\ ] ]    switching to vector notation , the population dynamics are given by    @xmath86+\\tilde{t}^{-1 } \\gamma^t \\frac{g } { c_0}\\mathbf{c}(t)+\\sigma_v{\\boldsymbol \\xi(t)},\\ ] ]    where @xmath87 is an @xmath88 diagonal matrix with @xmath57 on the diagonal .    in the integrate - and - fire network , spiking occurs due to an explicit threshold crossing and reset condition . with the addition of ionic currents ,",
    "action potentials are now intrinsically generated , but it is still necessary to identify a voltage to identify spike times .",
    "we treat this detection threshold as a separate parameter . in the simulation presented , we chose to use @xmath89 @xmath58 which is sufficiently high on the upswing of the action potential to allow reliable spike detection .",
    "however , different choices for @xmath90 can lead to different behaviors for the network . in particular , our simulations show that in order to use a larger value for @xmath90 , one must also increase the voltage noise in order to prevent the network from synchronizing .      in the previous section",
    ", we incorporated spike - generating currents into the voltage dynamics of each cell in our network .",
    "the point of this is to add biological realism , but the immediate consequence is that the voltages no longer evolve to precisely track error signals for the intended computation .",
    "this degrades the accuracy with which the network can perform .",
    "however , in this section we show that it is possible to effectively `` compensate '' the network for the effects of the spike - generating currents .    to begin",
    ", we note that , assuming no noise , a network optimized for the underlying computation maintains the relationship    @xmath91    i.e. , the voltage of each cell represents a projection of the error signal .",
    "however , the addition of the spike - generating currents disrupts the relationship ( [ vrel ] ) .",
    "thus , we seek to derive alterations to both the network and decoder dynamics in order to make ( [ vrel ] ) valid .",
    "that is , we take the dynamics of @xmath92 and @xmath12 to be given by    @xmath93    where @xmath94 and @xmath95 are functions to be determined in order to restore the relationship between voltage and error , eq .",
    "( [ vrel ] ) . taking the derivative of equation ( [ vrel ] ) and using ( [ newxh ] ) ,",
    "we find    @xmath96-g \\tilde{t}^{-1}\\gamma^t\\mathbf{g}(\\mathbf{v}),\\end{aligned}\\ ] ]    where above we again used the fact that @xmath97 . equating this definition of the derivative of @xmath92 to ( [ newv ] )",
    ", we find    @xmath98\\\\ & & \\mathbf{g}(\\mathbf{v})=-\\phi\\frac{\\mathbf{i_{ion}}(\\mathbf{v},\\vec{\\mathbf{w } } ) } { c_m g},\\end{aligned}\\ ] ]    where @xmath99 and @xmath100 is the moore - penrose pseudoinverse of the rectangular matrix @xmath101 .",
    "thus , the new dynamics would be    @xmath102 + \\tilde{t}^{-1}\\gamma^t\\frac{g } { c_0}\\frac{\\mathbf{c}(t ) } { c_m}\\\\ & & \\mathbf{\\dot{\\hat{x}}}=-a_d\\mathbf{\\hat{x}}+\\gamma\\boldsymbol{\\rho}\\ast h_r(t)-\\phi\\frac{\\mathbf{i_{ion}}(\\mathbf{v},\\vec{\\mathbf{w } } ) } { c_m g},\\end{aligned}\\ ] ]    which implies that @xmath92 and @xmath12 are coupled , as the solution of @xmath12 is ( ignoring initial conditions ) :    @xmath103    this coupling implies that the decoder @xmath12 requires instantaneous knowledge of the voltages of each cell . clearly , a more realistic",
    " and simpler  implementation would be if the decoder had access only to the spike times of the cells .",
    "we next show how this can be achieved .",
    "we begin with the assumption that the primary cause of the disruption of ( [ vrel ] ) occurs only during an action potential .",
    "we then find an approximation of the intrinsic current @xmath104 that follows a spike .",
    "that is , we seek a kernel @xmath105 such that    @xmath106    where @xmath107 is the time of the @xmath108 spike of cell @xmath11 and @xmath109 is the width of the kernel @xmath105 .",
    "more details on obtaining the kernel @xmath105 are provided in the next section . plugging the above approximation into the last term in eq .",
    "( [ xhat1 ] ) , we obtain    @xmath110 \\nonumber\\\\ & = & \\gamma\\boldsymbol{\\rho}\\ast\\alpha(t ) - \\frac{1 } { g } \\phi\\boldsymbol{\\rho}\\ast\\tilde{\\eta}(t ) , \\label{xhatapp}\\end{aligned}\\ ] ]    where @xmath111 .",
    "note that @xmath105 has units of @xmath112 while @xmath113 has units of @xmath58 .",
    "we can then rewrite the network dynamics as    @xmath114\\\\ & & \\hspace{2cm}-\\tilde{t}^{-1}\\gamma^t(a+a_d \\mathbb{i}_{j\\times j})\\phi\\boldsymbol{\\rho}\\ast\\tilde{\\eta}(t)+\\tilde{t}^{-1}\\gamma^t\\frac{g } { c_0}\\frac{\\mathbf{c}(t ) } { c_m}+\\frac{\\sigma_v } { c_m } { \\boldsymbol \\xi}(t)\\\\ & & \\mathbf{\\dot{\\hat{x}}}=-a_d\\mathbf{\\hat{x}}+\\gamma\\boldsymbol{\\rho}\\ast h_r(t)-\\frac{1 } { g}\\phi\\boldsymbol{\\rho}\\ast \\eta(t),\\end{aligned}\\ ] ]    where the voltage noise term has again been included .",
    "finally , we introduce the following more compact notation :    @xmath115    where    @xmath116    we reiterate that the compensation affects both the network dynamics and the read - out .",
    "note also that the parameter @xmath60 scales the strength of the slow and fast synaptic input .",
    "the compensation kernel @xmath105 was obtained by stimulating a single model neuron with a gaussian noise current ( specifically , an ornstein - uhlenbeck process  @xcite ) , and keeping track of the times @xmath117 that the voltage crossed a threshold from below .",
    "this threshold was the same as that used for detecting spikes in the network simulations . for each spike , we then obtain an action potential waveform @xmath118 for @xmath119 , where @xmath109 sets the width of the @xmath105 kernel .",
    "we then sum these traces to obtain the average waveform of the action potential @xmath120 .",
    "that is , if @xmath121 spikes were recorded , then    @xmath122    thus , an approximation to the change in voltage during the spike is given by    @xmath123    the kernel @xmath105 is then defined as    @xmath124    see figure [ compex ] for an illustration of this procedure . for our simulations",
    ", we set @xmath125 @xmath126 . using a larger value of @xmath109",
    "did not significantly affect the results , but too small a value does , as the voltage trace during the entire time course of the action potential will not be accounted for .",
    "in this section , we assume that the network tracks a one - dimensional signal ; that is , @xmath127 .",
    "the decoder is given by    @xmath128    where @xmath129 , @xmath130 .",
    "the variance of the decoder is then given by    @xmath131    where @xmath132 , @xmath133 , and @xmath134 .",
    "similarly , the variance of a decoder that assumes that all neurons are independent is given by @xmath135    where @xmath136 shares the same diagonal elements with @xmath137 but is zero on the off - diagonals and @xmath138 .",
    "in the main text we quantify the relative decoding variance of the independent vs.  full \" ( i.e. , correlated ) network via the fraction @xmath139 .",
    "values of this fraction greater than one indicate that the network produces correlated spike times that reduce decoding variance vs. the  shuffled , \" independent case ; we refer to it as the  reduction in decoding variance . \" to compute this quantity , we performed @xmath140 two - second runs of the network , with a new noise realization on each trial , calculated the covariance matrices for each trial , averaged the covariance matrices across all trials and used the averaged matrices in eqs .",
    "( [ decvar ] ) and ( [ decvarind ] ) .    for the homogeneous network considered below , we can obtain a simple estimate for the reduction in decoding variance .",
    "suppose that @xmath141 for @xmath142 ( stimulus activated population ; see main text ) and @xmath143 for @xmath144 ( stimulus depressed population ; see main text ) for some constant @xmath145 .",
    "then @xmath146 for @xmath142 and @xmath147 for @xmath144 for some constant @xmath148 related to @xmath145 .",
    "assume that the variance of each neuron is very close to the average variance over the population , i.e. , that the diagonals of each of the above covariance matrices are constant .",
    "dividing each of the above covariance matrices by this average variance yields a matrix with ones on the diagonal and the various pairwise correlation coefficients on the off - diagonals . assuming that the pairwise correlation coefficients are close to their average values , the above matrices have a very simple form :    @xmath149    where @xmath150 ( @xmath151 ) is the mean correlation coefficient for the stimulus activated ( stimulus depressed ) population computed using kernel @xmath152 , and @xmath153 is the mean correlation coefficient between the two different populations using kernel @xmath152 . with this approximation ,",
    "the elements of the above variance calculations take a simple form    @xmath154\\\\ & & \\phi c^{\\tilde{\\eta } } \\phi^t = \\sigma_{\\tilde{\\eta } } b^2\\left[n + \\frac{n^2 - 2n } { 4 } ( a_{\\tilde{\\eta } } + d_{\\tilde{\\eta } } ) -\\frac{n^2 } { 2 } c_{\\tilde{\\eta}}\\right]\\\\ & & \\gamma c^{\\alpha\\tilde{\\eta } } \\phi^t = \\sigma_{\\alpha\\tilde{\\eta } } a^2\\left[n + \\frac{n^2 - 2n } { 4 } ( a_{\\alpha\\tilde{\\eta } } + d_{\\alpha\\tilde{\\eta } } ) -\\frac{n^2 } { 2 } c_{\\alpha\\tilde{\\eta}}\\right],\\end{aligned}\\ ] ]    and    @xmath155    thus , an approximation to the reduction in decoding variance obtained by recording from only a subset of the full network is given by using the above formulae in @xmath139 , since the correlation coefficients do not vary with @xmath8 . however , if we assume that the dominant contribution to the variance calculation is given by those terms involving the @xmath156 matrix ( which is what we find numerically , cf .",
    "[ recsubsetfig ] ( a ) ) , then an even simpler formula can be obtained    @xmath157      the reported correlation coefficients between cells @xmath158 and @xmath159 are computed by convolving spike trains with a double - exponential function , so that @xmath160 :    @xmath161    where @xmath162 is the total number of time points taken for a given simulation , @xmath163 is the @xmath164-th time point , and @xmath165 is the sample mean . to remove the covariance in firing rates of the cells , the correlation coefficients were corrected by subtracting off the correlation coefficient obtained from shift - predictor data ( shifted by one trial ) .",
    "since our networks consist of two populations of neurons , i.e. , those with a positive value for @xmath19 and those with a negative value for @xmath19 , the correlation coefficients reported in the histograms are the population - averaged correlation coefficients for each trial simulation of the network . to generate the histograms , we ran @xmath140 two - second simulations of the network with the same box function input .",
    "the only thing that varied between the simulations was the realization of the white background noise .",
    "the fano factors for each neuron were computed by binning the spike times into @xmath166 @xmath126 windows and computing the mean @xmath167 and variance @xmath168 of the spike count in a particular window over @xmath140 repeated trials of the box function input stimulus . the fano factor in a particular window",
    "is then given by @xmath169 .",
    "for each neuron , the time averaged fano factor was computed by taking the mean over all windows .",
    "we then averaged these values over all neurons in a given population and report them in figure [ isifig ] .",
    "two measures of error quantify the network performance .",
    "the first is the relative error between the signal and the estimate ,    @xmath170    where @xmath171 ^ 2ds}$ ] and @xmath162 is the simulation time .",
    "relative error is useful for comparing errors across signals that vary in magnitude .",
    "the second error measure is the integrated squared error ,    @xmath172 ^ 2 ds.\\ ] ]      to analyze the subthreshold voltages of cells in our network , we first truncated the membrane potentials at @xmath173 @xmath58 to remove the spikes and subtracted out the temporal mean , i.e. , @xmath174 where @xmath8 is the total number of data points .",
    "voltage power spectra for individual neurons were then computed using matlab s _ fft _ function .",
    "cross - correlations between two cells @xmath175 and @xmath176 were also calculated using matlab s _ xcorr _ function :    @xmath177    where @xmath178 is the time lag @xcite .",
    "we then subtracted off the cross - correlation for shift - predictor data ( shifted by one trial ) .",
    "both the power spectra and cross - correlograms were then averaged over @xmath179 eight - hundred millisecond simulations of the homogeneous integrator network with the box function input .",
    "the spike - triggered error of figure [ isifig ] was computed from @xmath140 two - second simulations of the network with a box function input ( see below ) . for each simulation",
    ", we computed    @xmath180    where @xmath181 is the nondimensional error each neuron is supposed to be representing in its voltage traces .",
    "the error @xmath182 was aligned to the spike times for cell @xmath11 and these traces averaged over all neurons in the network .",
    "the shuffled spike - triggered error , computed by aligning @xmath182 to the spike times of cell @xmath11 on a different trial , was then subtracted .",
    "this removed the slow bias present in the original spike - triggered error .",
    "lastly , the shuffle - corrected spike - triggered errors were averaged over all trials .",
    "the level of synchrony in the simulated network was evaluated using a measure introduced by golomb @xcite . with @xmath183 as the instantaneous firing rate of neuron @xmath11 , synchrony is given by    @xmath184 ^ 2\\right\\rangle_t - \\left [ \\left\\langle\\frac{1 } { n}\\sum_{k=1}^n f_k(t)\\right\\rangle_t\\right]^2 } { \\frac{1 } { n } \\sum_{k=1}^n\\left\\langle \\left[f_k(t)\\right]^2\\right\\rangle_t - \\left [ \\left\\langle f_k(t)\\right\\rangle_t\\right]^2},\\ ] ]    where @xmath185 denotes time - averaging over the length of the simulation . to estimate instantaneous firing rates ,",
    "the spike trains were convolved with a gaussian kernel with standard deviation @xmath186 @xmath126 .      when varying the simulated network size as in figure [ varynfig ] , we scaled the connection strengths of the network so that the total input to any cell in the network remains constant as the network size is increased . in particular , for the homogeneous integrator ( @xmath187 ) network tracking a one - dimensional dynamical system where @xmath188 for @xmath189 and @xmath190 for @xmath144 , we employed the scaling :    @xmath191    thus , both the connection weights and the synaptic gain parameter @xmath60 scale with @xmath192 .",
    "the factors of @xmath193 and @xmath194 above were chosen so that at @xmath195 , @xmath196 and @xmath197 , which matches our earlier simulations of our network when we fixed @xmath8 at @xmath194 . with this scaling ,",
    "the connection strengths all scale the same way with @xmath8 and the input @xmath198 remains constant . to see this , recall that when @xmath187 our network equations are given by    @xmath199-a_d\\frac{1 } { t_k}\\gamma_k^t\\phi \\boldsymbol{\\rho}\\ast \\tilde{\\eta}(t ) + \\frac{\\sigma_v } { c_m } \\xi_k(t),\\end{aligned}\\ ] ]    where @xmath200 , @xmath99 , and @xmath87 is a diagonal matrix with @xmath57 on the diagonal .",
    "thus , we need to determine the scaling of the following :    @xmath201    first , we explore the term @xmath202 which involves the pseudoinverse of the @xmath101 matrix . in the case of the homogeneous integrator network ,",
    "the pseudoinverse is simply given by @xmath203 , because @xmath204 .",
    "thus , if we let @xmath205 as listed above , @xmath206 .",
    "@xmath207_k$ ] then scales like @xmath208 . using this fact , and recalling that @xmath209 , we can now compute the scalings for all the connections in the network :    @xmath210    where we used the fact that since @xmath17 and @xmath79 are constants , they scale like @xmath25 .",
    "thus , the connection weights scale like @xmath192 .",
    "however , since each cell in the network receives input from all @xmath8 other cells , this scaling means that the total input each cell receives remains constant as the network size is varied .",
    "we use a neuron model due to traub et al .",
    "@xcite :    @xmath211    where    @xmath212 = = @xmath213 + @xmath214 @xmath215 + @xmath216 @xmath217    and    @xmath218 @xmath64 = = @xmath219 @xmath73= = @xmath220 @xmath73 + @xmath221 @xmath73 @xmath222 @xmath58 @xmath223 @xmath58 + @xmath224 @xmath58    the factor of @xmath225 in the gating variable equations comes from conversion of time units from milliseconds to seconds .",
    "other neuron models , including an exponential integrate - and - fire model , were used with similar results .",
    "other parameters held constant in our simulation are :    @xmath226 @xmath227 = = @xmath228 @xmath227 + @xmath229 @xmath58 .",
    "the decay rate of @xmath228 @xmath227 yields a decay time constant of @xmath166 @xmath126 for the slow , double - exponential function synapses in our network .",
    "this decay time constant is in the range of those observed in inhibitory and excitatory post synaptic currents @xcite .",
    "the rise rate @xmath226 @xmath227 sets the decay time scale for the fast , exponential synapses .",
    "these synapses have a decay time constant of @xmath230 @xmath126 , as has been observed in inhibitory cells in rat somatosensory cortex @xcite .",
    "simulations were written in matlab .",
    "the euler - maruyama method was used to integrate the stochastic differential equations using a time step of @xmath231 @xmath126 .",
    "simulations with time steps of @xmath232 and @xmath233 @xmath126 yielded similar results .",
    "spikes were counted as voltage crossings of a threshold of @xmath234 @xmath58 from below .",
    "the initial voltages for the network were chosen randomly , while the channel variables were set to their steady - state values given the fixed initial voltage .",
    "in particular , the initial voltages were chosen from a gaussian distribution with a mean of @xmath74 and a standard deviation of @xmath235 .",
    "the initial state for the signal and the decoded estimate were both set to zero , i.e. , @xmath236 .    though we have provided the most general form for the network tracking any linear dynamical system , throughout the majority of the paper , we focus on the case of a homogeneous network integrating a one - dimensional signal .",
    "that is , we set @xmath127 , @xmath187 , and @xmath237 for @xmath238 and @xmath239 for @xmath240 , where @xmath145 is a constant .",
    "the only exception to this is in the examples in figure 1 where we set @xmath241 in order to remove the slow synapses in the network dynamics .",
    "we also set @xmath197 for all figures except figure [ varynfig ] .",
    "we focus on the network integrating one of two different signals .",
    "the first varies between two constant values (  box \" input ) :",
    "@xmath242    where @xmath243 @xmath126 for figure [ netex ] and @xmath244 @xmath126 for all subsequent figures .",
    "the second is a frozen ornstein - uhlenbeck @xcite signal given by    @xmath245    where @xmath76 is a frozen white noise realization with zero mean and unit variance , @xmath246 @xmath126 , and @xmath247 @xmath248 .",
    "our goal in this work is to design a network to carry out an arbitrary linear computation on an input over time  and to do so with neurons that generate spikes via realistic ionic currents and synaptic timescales .",
    "writing the computation as a linear dynamical system , @xmath249 , where @xmath4 is a constant matrix and @xmath7 is the signal we desire to compute , boerlin et al .",
    "( 2013 ) were able to construct a recurrent spiking network to accomplish this goal .",
    "the strategy was to arrange connections so that the voltage of each neuron would be proportional to a difference between the currently decoded network output and the ideal computation , trigger spikes when this error exceeds a threshold , and communicate these spikes ( and hence the error ) to other neurons in the network .",
    "thus , every action potential occurs at a precise time that serves to reduce the ",
    "global \" computational error across the network .",
    "we refer to this framework as spike - based computation .    in this previous work",
    ", the authors successfully mapped the requirement of each spike reducing output error onto a network of recurrently connected linear integrate - and - fire neurons with instantaneous synaptic dynamics . however , biological networks have slower synaptic kinetics , and have ionic currents with nonlinear dynamics that determine spike generation . here",
    ", we will show how these two aspects of neurophysiology in fact can fit naturally with spike - based computation .",
    "in particular , we want to design a network of neurons such that an estimate @xmath29 of a @xmath250 vector of signal variables @xmath2 can be linearly read out from the spike times of the network . as above",
    ", we assume the signal variables obey a general linear differential equation @xmath249 .",
    "thus , @xmath4 is a @xmath5 dimensional matrix and the input is @xmath0-dimensional .",
    "the entries of the matrix @xmath4 determine the type of computation the network is asked to perform on the @xmath0-dimensional inputs , which we will denote as @xmath82 .",
    "for example , if @xmath4 is the zero matrix , then the network integrates each component of the input over time .",
    "our network will consist of @xmath8 neurons with output given by the @xmath8 spike trains , written as @xmath10 @xmath251 .",
    "our first goal is to incorporate synapses that have finite temporal dynamics .",
    "the synaptic dynamics enter through the definition of a _ decoder _ that provides an estimate for the variable @xmath7 .",
    "this decoder includes a linear transformation of the network spike trains @xmath252 via a @xmath20 linear decoding matrix @xmath19 .",
    "the spike trains @xmath253 are first convolved with the synaptic filter @xmath28 ( @xmath254 ) , which we take to be a standard double - exponential function . with these synaptic dynamics in this decoding ,",
    "an estimate of the computed variable is given by @xmath255 .",
    "the @xmath19 matrix will determine the connectivity structure of the network ( see materials and methods section ) .",
    "given this decoder , we now follow @xcite to derive the network dynamics and connectivity . the key step is to requiring that neurons in the network only spike in order to reduce the integrated squared error between the signal and its decoded estimate . as shown in materials and methods ( see eq .",
    "[ errorfun ] ) , this has the consequence that each neuron in the network has a voltage that is equivalent to a weighted error signal , i.e. , the voltage of the @xmath256 neuron is given by @xmath257 ( @xmath258 is the @xmath11-th column of the @xmath259 matrix @xmath101 ) .",
    "each neuron then fires when its own internal copy of the error signal exceeds a set threshold value .",
    "the optimal network that carries out this spike - based computation is given by a network of  pure integrate - and - fire \" neuron models that directly integrate synaptic inputs without any leak or intrinsic membrane currents ; however , a linear leakage current can be added to the voltage dynamics for each neuron with minimal disruption of the network dynamics  @xcite . in this case , the voltage dynamics are given by    @xmath260    where @xmath261 represents the leakage current .",
    "each neuron receives synaptic input from other cells in the computing network as well as external input .",
    "the external input is given by @xmath262 where @xmath263 is a @xmath259 matrix of input weights , and @xmath82 is the @xmath264 vector of inputs introduced above .",
    "the synaptic input is given by @xmath265 where @xmath266 is the network connectivity matrix , @xmath267 scales the strength of the synaptic input , and @xmath268 is a single exponential synapse ( see materials and methods section for details ) .",
    "figure [ netschem1 ] ( a ) illustrates the resulting network structure in the simplest possible case .",
    "this is a network consisting of a single neuron that receives stimulus input as well as input from recurrent ( here , autaptic ) connections , and a decoder @xmath12 that reads out the computation from the single neuron s spike train .",
    "figure [ netschem1 ] ( b ) shows the resulting network behavior . for the examples in this figure , the network performs leaky integration on a single - variable , square wave input ( i.e.",
    ", the matrix @xmath4 is simply @xmath269 ) .",
    "the upper plots show the decoded signal ( @xmath270 ) from the spiking output of a single neuron ( red traces ) plotted against the actual desired signal @xmath271 ( dashed black lines ) along with the neurons voltage trace ( lower panels ) . in the first column",
    ", we illustrate the output of a single neuron from the leaky - integrate - and - fire ( lif ) network of @xcite . comparing the red decoded signal and the actual desired signal",
    "@xmath271 demonstrates the principle of spike - based computation in action : when the decoded signal deviates too far from the desired signal , an additional spike is triggered , and the process repeats .    in the next column",
    ", we replace the exponential kernel used for decoding the network spike trains with a double - exponential function ( first arrow ) , as described above , which results in an lif network without instantaneous ( @xmath53-function ) synaptic dynamics .",
    "next , as real neurons contain a variety of intrinsic currents , we replace the linear leakage current with generic hodgkin - huxley - type ( hh ) ionic currents :    @xmath272    where @xmath273 represents the sum of all ionic currents and also depends on the corresponding dynamical gating variables .",
    "the third column in figure [ netschem1 ] ( b ) illustrates how the network behaves with this change to the intrinsic voltage dynamics ( labeled as adding `` spike currents '' ) .    in general",
    ", the addition of such ionic currents to voltage dynamics will disrupt the ability of the network to accurately perform a given computation .",
    "this is because the large excursions of the membrane potential during the action potential will cause the voltage of the individual neurons to deviate from their derived optimal relationship with the error .",
    "however , in materials and methods section , we show that incorporating a new synaptic kernel in both the voltage and decoder dynamics allows the network to effectively compensate for the inclusion of ionic currents , so that it can perform the required computation with improved accuracy compared to the network where these compensation currents are not included .",
    "this new synaptic kernel , which we denote by @xmath113 , is constructed to counteract the total change in voltage that occurs during a spike .",
    "we provide details on how this kernel is derived as well as how it is obtained for our simulations in materials and methods sections and and in figure [ compex ] .",
    "the resulting voltage dynamics and decoder are :    @xmath274    where @xmath275 is the connectivity matrix for the compensating synaptic connections and @xmath276 is the new decoding kernel ( given in materials and methods section ) .",
    "the final column of figure [ netschem1 ] ( b ) shows how the addition of this compensation current affects the output of a single neuron .",
    "for the single neuron case , this adds large fluctuations in the decoder output .",
    "thus , compared with the original effects of adding the spike - generating currents , it appears that the compensation current can decrease accuracy .",
    "however , our simulations show that this effect only occurs for very small ( fewer than 4 neurons ) networks . for larger networks",
    ", compensation allows the network to perform the computation with a high degree of accuracy , as we will show .    to show how the framework generalizes to larger networks",
    ", we plot the output of an example network of @xmath277 neurons . for this network ,",
    "we take @xmath278 while @xmath279 , where @xmath145 is a constant .",
    "the output weights @xmath19 also determine the connectivity structure of the network .",
    "this particular choice of @xmath19 will lead to a network with all - to - all connectivity .",
    "the matrix @xmath263 that scales the stimulus input also depends on @xmath19 : the network structure that allows the system to perform accurate spike - based computations requires that @xmath280 ( see materials and methods sections and ) .",
    "this implies that neurons @xmath25 and @xmath281 ( @xmath282 and @xmath283 ) will be depolarized ( hyperpolarized ) when @xmath198 is positive .",
    "the cartoon in figure [ netschem2 ] ( a ) shows the structure of this network .",
    "we next explore the output of our example 4-cell network . here",
    ", the input to the network is a simple square - wave function of time , taking a fixed positive value from @xmath284 to @xmath285 @xmath126 and a fixed negative value from @xmath285 to @xmath286 @xmath126 .",
    "figure [ netschem2 ] ( b ) shows the resulting spike rasters .",
    "the individual spike times are highly irregular , and the upper ( lower ) two cells appear to be more active when the input is positive ( negative ) . in figure [ netschem2 ] ( c ) , we again plot the network estimate @xmath270 ( red ) against the actual signal @xmath271 ( black dashed ) .",
    "in addition , we also plot what the network estimate would be had the compensating synapses not been included ( grey trace ) .",
    "this shows that compensation indeed corrects for systematic biases .",
    "lastly , figure [ netschem2 ] ( d ) plots the voltage trace for an example neuron .",
    "there are two key points to take away from this final panel .",
    "the first is that the synaptic input is not overwhelming the intrinsic spike - generating currents .",
    "indeed , one way to force the network to behave like an if network would be to increase the synaptic gain so that the synaptic input is much larger than the intrinsic currents ; this is clearly not the case here .",
    "the second point to take away from the plot is that the membrane potentials and spike times of individual neurons appear highly irregular .",
    "the above examples , in implementing equations  - , used a special choice for the matrix @xmath4 that defines the linear computation implemented by the network ; here , we set @xmath241 so that the connectivity matrix for the double - exponential function synapses is zero ( see below and materials and methods section ) . for an arbitrary choice of @xmath4 ,",
    "the network dynamics are given by    @xmath287-c_m\\omega_c\\boldsymbol{\\rho}\\ast\\tilde{\\eta}(t)+d\\mathbf{c}(t ) + \\sigma_v { \\boldsymbol \\xi}(t ) , \\label{e.fullnet}\\end{aligned}\\ ] ]    where @xmath288 represents the `` slow '' ( compared to the exponential `` fast '' synapses ) synaptic connectivity matrix .",
    "this effectively corresponds to the decoded estimate @xmath270 being fed back into the network , which allows the network to perform more general computations on inputs .",
    "the parameter @xmath60 scales the strength of both the slow and fast synapses in the network .",
    "lastly , in equation   we also added a white noise current ( @xmath289 ) , drawn independently for each cell , to our voltage evolution equations .",
    "this represents random synaptic and channel fluctuations as well as noisy background inputs , but , as we will see below , also serves a functional role in decreasing network synchrony .      for the remainder of the paper , we focus on the case of a network of neurons with spike - generating currents based on the miles - traub model @xcite ( materials and methods section ) which contains hh - type sodium , potassium , and leakage ionic currents .",
    "although we use a specific model , similar results were obtained with different neuron models , e.g. a fast - spiking interneuron model @xcite and different sodium , potassium , and leakage current kinetic and biophysical parameters taken from @xcite .",
    "we will initially show how such a spiking network can integrate a one - dimensional stimulus input . in terms of the notation previously introduced",
    ", this corresponds to the case where the number of inputs , or dimensionality , @xmath127 and the matrix @xmath187 .",
    "we choose the input connections such that @xmath188 for half of the cells in the network , @xmath290 , and @xmath190 for the remaining half , @xmath144 . thus , the network has all - to - all connectivity ( recall that the network connectivity matrices depend on @xmath19 , for example , @xmath291 ) ; the input to individual neurons within the  first \" or ",
    "second \" half of the network differs only via their ( independent ) background noise terms . with this configuration",
    ", half of the network will be depolarized when the stimulus input @xmath198 is positive , while the other half will be hyperpolarized .",
    "we will refer to the depolarized half as the `` stimulus - activated '' population and the hyperpolarized half as the `` stimulus - depressed '' population .",
    "note that this distinction does not refer in any way to excitatory vs. inhibitory neurons , as in our formulation neurons can both excite and inhibit one another , a point that we will return to later .",
    "the addition of voltage noise in this case is critical as the network is very homogenous and will synchronize in the absence of noise .",
    "we systematically explore the dependence of network performance on the noise level ( as well as other parameters ) in a later section .    for purposes of illustration ,",
    "the network was driven with two different types of inputs @xmath198 , a box function and a frozen random trace generated from an ornstein - uhlenbeck ( ou ) process @xcite ( figure [ netex](a ) ; see methods for details ) .",
    "the remainder of figure [ netex ] shows the resulting output for a @xmath194-neuron network , integrating a box input in panels ( a)-(e ) and integrating the frozen random trace in panels ( f)-(j ) .",
    "figures ( a ) and ( f ) plot the different inputs , while ( b ) and ( g ) show the raster plots for all @xmath194 neurons .",
    "the neurons spike fairly sparsely and highly irregularly .",
    "the network estimates , @xmath270 ( red trace ) , along with the true signal @xmath271 ( blue trace ) are shown in ( c ) and ( h ) .",
    "the network is able to track both the box and ou inputs with a high degree of accuracy : the relative error ( @xmath292 ) between the estimate and the actual signal is @xmath293 for ( c ) and @xmath293 for ( h ) . to illustrate the improvement in accuracy due to the synaptic inputs that compensate for spike - generating currents ( see materials and methods section ) , we also plot signal estimates from a network where this compensation was not included ( grey traces ) . for these estimates ,",
    "the relative error is @xmath294 in ( c ) and @xmath295 in ( h ) ; thus , our compensating synapses yield an almost tenfold increase in accuracy .",
    "next , we show the population - averaged firing rates for the stimulus - activated ( magenta ) and stimulus - depressed populations ( green ) in ( d ) and ( i ) .",
    "figure ( d ) shows that in the absence of input , the populations maintain persistent activity for roughly @xmath296 @xmath126 .",
    "this is consistent with observations of neural activity during working memory tasks @xcite .",
    "however in panel ( i ) , the firing rates of the populations fluctuate depending upon the input .",
    "lastly , ( e ) and ( j ) plot the average autocorrelation functions for the spiking activity of neurons in the different populations .",
    "these display a clear refractory effect , and small tendency to fire in the window that follows .",
    "differences between the stimulus - activated and stimulus - depressed populations , especially for the box function input , are likely due to the different firing rates and inputs that the two populations receive .",
    "we explore these spiking statistics further in the section that follows .",
    "we next show that our network displays two key features of cortical networks : the spike times of the network are irregular and poisson - like , and there is a tight balance between excitation and inhibition for each neuron in the network .",
    "figure [ isifig ] shows responses from the homogeneous integrator network introduced in the previous section with a box function input stimulus .",
    "the irregularity of spike times is illustrated by the voltage trace of an example neuron in the network , in figure [ isifig](a ) . to quantify this irregularity",
    ", we generated a histogram of the inter - spike intervals ( isi ) during the period of zero input where the firing rates are nearly constant [ isifig](b ) .",
    "to generate the histogram , we simulated the response of the network during @xmath140 repetitions of the box function input .",
    "the only thing that varied between trials was the realization of the additive background noise current .",
    "the isis follow an almost exponential distribution , see inset , and the coefficient of variation ( cv ) is @xmath297 .",
    "thus , the spiking in our network is , by this measure , less variable but not far from what we would expect for poisson spiking ( which would yield a cv=1 ) or levels of variability that have been observed in cortical networks @xcite .",
    "we also explore the trial - to - trial variability of individual neurons in the network .",
    "figure [ isifig](c ) shows a raster plot with the spike times of two example neurons over @xmath166 different trials .",
    "the upper ( lower ) dots correspond to the spike times of a neuron from the stimulus - activated ( stimulus - depressed ) population .",
    "one can see that the spike times of individual neurons vary considerably between trials . to quantify this , we computed the time - averaged fano factors for each neuron in the network ( materials and methods section ) .",
    "the fano factor gives a measure of the trial - to - trial variability of individual neurons .",
    "for the stimulus - activated population , the time averaged fano factor , averaged across the population , is @xmath298 , while for the stimulus - depressed population , it is @xmath299 . for a time homogeneous poisson process",
    ", one would expect a fano factor of 1 .",
    "thus , by this measure , neurons in both populations display variable spiking from trial - to - trial , but less variable than what would be expected from a poisson process .    by examining the total excitatory and inhibitory current that each neuron receives",
    ", we can check whether the network is in the balanced state @xcite . to do this",
    ", we compute the total positive ( negative ) input a cell receives .",
    "a complication here is that the @xmath113 kernel changes sign ; to deal with this , we rewrote the kernel as a difference of two separate , positive kernels , i.e. , @xmath300 , and computed the resulting current from each kernel .",
    "we also ignore the noisy background current for visualization purposes as similar results were obtained when the noise is included .",
    "figure [ balancefig](a ) shows the total excitatory ( red ) and inhibitory ( blue ) current for an example neuron in the network . note that while the balance is imperfect ( as shown by the inset ) , the two currents do appear to track each other fairly well .",
    "panel ( b ) shows the total excitatory ( red ) and inhibitory ( blue ) current averaged over all neurons in the network .",
    "this shows that the currents are tightly balanced at the level of the entire network , which is typically what one finds when deriving so - called balanced networks @xcite    next , we demonstrate that , even after altering the synaptic time scales and including spike - generating currents , neurons in the network still perform predictive coding by firing when their projected error signal is large .",
    "we computed the spike - triggered error ( ste ) for the network by aligning the projected error signal for each neuron @xmath11 ( @xmath301 ) to that neuron s spike times , averaging across all spike times and then averaging over all neurons ( materials and methods section ) , fig .",
    "[ balancefig](c ) .",
    "the ste is indeed largest at the time of the spike and rapidly decreases right after the spike , indicating that spikes do in fact decrease the error .",
    "the oscillatory behavior of the ste is indicative of the fact that there is some amount of synchrony in the spike times of the network .",
    "signatures of spike - based computation are also present in the subthreshold membrane potentials of cells in our network .",
    "first , figure [ vcorrfig ] ( a ) shows the trial - averaged cross - correlogram ( see materials and methods section ) between the subthreshold voltages of two example cells in the stimulus - depressed population ( blue solid trace ) and two example cells in different populations ( red dashed trace ) .",
    "the voltage traces of cells within the same population appear to be correlated over short time lags , as we expect from the fact that neurons in the same population receive highly similar synaptic input .",
    "meanwhile , voltages of cells in different populations are anti - correlated .",
    "thus , cells in different populations can be differentiated via correlations in their subthreshold voltages .",
    "next , we explore the voltage statistics of single cells .",
    "figure [ vcorrfig ] ( b ) shows the voltage power spectrum of an example cell in the stimulus - depressed population ( solid trace ) . for comparison ,",
    "the power spectrum of an isolated neuron that only receives background noise input is shown in the dashed trace .",
    "it appears that noise input drives the peak in the power spectrum around @xmath193 @xmath227 , while the fast predictive coding implemented by the feed - forward input and lateral connections is responsible for the remaining peak around @xmath302 @xmath227 ( figure [ vcorrfig ] ( c ) gives a closer view of this second peak . )",
    "the presence of this second peak is therefore another prediction of the spike - based predictive coding framework .",
    "we now explore the structure of correlations that emerge among the spikes of different cells in the network , and whether these correlations are beneficial or harmful to the network s encoding of an input that has been integrated over time .",
    "specifically , we ask whether these coordinated spike times increase or decrease the variance of the decoded signal around its mean value .",
    "as shown in materials and methods section , the variance of the decoded signal is given by    @xmath303    where @xmath132 , @xmath133 , and @xmath134 are the average covariance matrices of the spike trains convolved with the two synaptic kernels , i.e. , @xmath129 , @xmath130 .",
    "this quantity measures the variability of the network estimate around its average value ; lower values of this variance correspond to highly repeatable network estimates from one trial to the next . if the neurons in our network were independent , then the off - diagonal terms in these covariance matrices would all be zero .",
    "thus , the variance of an independent decoder @xmath304 would have the same form as the above equation , except that the off - diagonal terms of the covariance matrices would be set to zero .",
    "the ratio @xmath305 measures the reduction in decoding variance caused by the structure of pairwise interactions between neurons in the network .",
    "the larger this ratio is , the greater the benefit of pairwise correlations between cells . if the neurons in our network were indeed independent , then this ratio would be @xmath25 .",
    "how do correlations affect decoding variance in the homogeneous integrator network ?",
    "for both of the different inputs , the structure of pairwise interactions between neurons causes a roughly fivefold decrease in the variability of the network estimate : for the box function input , the reduction in decoding variance is @xmath306 , while for the ou input , it is @xmath307 . to gain insight into how the correlation structure of the network causes this , figure [ corrfig ] plots the population - averaged correlation coefficients and cross - correlograms for the homogeneous integrator network .",
    "we first focus on the case of the box input function . in figure [ corrfig](a )",
    "we show a histogram of the population - averaged pairwise correlation coefficients for both the stimulus - activated ( magenta ) and stimulus - depressed ( green ) populations .",
    "neurons in both populations appear to have weak ( and slightly negative ) pairwise interactions with one another on average : the mean correlation coefficient for the stimulus - activated ( stimulus - depressed ) population is @xmath308 ( @xmath309 ) .",
    "on the other hand , figure [ corrfig](b ) shows that the pairwise correlation coefficients between cells in the two different populations are small but positive , with a mean of @xmath310 .",
    "thus , the network reduces decoding variance by creating negative correlations between neurons that represent the same aspect of the stimulus , and positive correlations between neurons that represent different aspects of the stimulus . from a coding perspective , these represent `` good '' correlations as the negative correlations between cells in the same population act to reduce redundancy , while the positive correlations across populations allow for some of the background noise to be cancelled out when the estimates from two populations are subtracted @xcite .",
    "this can also be seen in the cross - correlograms of the different populations , fig .",
    "[ corrfig](c ) and ( d ) .",
    "the situation is very similar for the ou stimulus input as shown in figures [ corrfig](e)-(h ) .",
    "there are slight differences in that the correlation coefficients are more broadly distributed , [ corrfig](e ) , and the correlation structure of the stimulus - activated and stimulus - depressed populations are more similar than for the box function stimulus .",
    "this is likely due to the fact that , with the ou stimulus , the two populations receive a more similar range of inputs over time .",
    "we have shown that the structure of pairwise interactions between neurons in the network acts to greatly reduce the variability of the network estimate of the underlying computation on a stimulus input .",
    "this already reveals a difference between this framework and the underlying assumptions of a rate model , in which neurons in the network are assumed to be statistically independent .",
    "as such , one could shuffle the spiking output of individual neurons from different trials and the rate - based computation would suffer no loss in accuracy .",
    "however , for the predictive coding network , it was shown that the structure of interactions between spike trains for individual neurons from trial to trial is important to the accuracy of the desired computation @xcite . to give a more direct illustration of this effect with our current network",
    ", we explored how the relative error between the decoded network estimate and the actual signal varied as we replaced an increasing number of spike trains with variations recorded from separate trials ( `` shuffled '' trains ) .",
    "figure [ reconerr ] ( a ) plots the average relative error between desired ( @xmath311 ) and network - decoded ( @xmath312 ) signals ( see materials and methods section ) as a function of the number of shuffled spike trains , for the box function input .",
    "as expected , the error increases with the number of shuffled trains and reaches its maximum when all spike trains are taken from separate trials . to see how the shuffling affects the network estimate",
    ", we show an example decoded estimate ( red ) plotted against the true signal ( blue ) in ( b ) when all spike trains are taken from the same trial . in figure [ reconerr](c ) , we plot the estimate decoded from entirely shuffled spike trains , where all are taken from different trials .",
    "as also expected from the previous section , the effect of shuffling spike trains appears to increase the magnitude of the fluctuations of the decoded estimate around its mean value .",
    "figures [ reconerr](d)-(f ) show that the situation is similar with the ou stimulus , although it is more difficult to see the effects on the decoded signal due to the fluctuations in the ou signal itself .",
    "our previous examples of the behavior of the homogeneous integrator system made use of a particular choice of network parameters .",
    "we now explore the sensitivity of its performance to changes in these parameters .",
    "in particular , we vary the strength of the fast and slow synaptic input , @xmath60 , and the strength of the added voltage noise , @xmath79 . for the homogeneous integrator network ,",
    "these two parameters have the largest effect on performance as @xmath60 effectively scales the strength of synaptic connectivity between neurons in the network and @xmath79 creates a level of heterogeneity in the individual voltage dynamics that prevents cells from synchronizing .",
    "we will show that the performance of our network is fairly robust to changes in these parameters .",
    "we quantify network behavior using several measures .",
    "as before , the accuracy of the computation is evaluated using the relative error between the network estimate and the true signal . to assess the firing properties of the network , we compute a population synchrony index introduced by @xcite ( materials and methods section ) , and the coefficient of variation of the interspike intervals during periods of zero stimulus input ( for the box function input ) .",
    "we also track the maximum population - averaged firing rate , to ensure that the populations are not firing at unrealistically high levels .",
    "because similar results were obtained with the ou stimulus , we only report these metrics for the box function stimulus .",
    "we first investigate how the level of population synchrony interacts with the accuracy of the network and neuronal firing rates .",
    "figure [ perfmetrics](a ) plots the population synchrony index as a function of the synaptic gain @xmath60 for three different values of the noise strength .",
    "the population synchrony has a @xmath313-shaped dependence on @xmath60 ; this is easiest to see at the smallest noise level .",
    "when the population synchrony is high , the relative error is large ( figure [ perfmetrics ] ( b ) ) and firing rates approach unrealistic levels ( figure [ perfmetrics ] ( c ) ) .",
    "thus , desynchronizing the firing dynamics of individual neurons in the network by increasing the noise to moderate levels improves network accuracy .",
    "our interpretation is that moderate noise distributes the computation more efficiently among individual neurons .",
    "if the noise is too small , then individual neurons behave too similarly and eventually synchronize , effectively reducing the dimensionality of the network and also the computational power .",
    "when the noise is too large , the computation is overpowered by the noise .",
    "figure [ perfmetrics ] ( b ) plots the relative error between the network estimate and the true signal as a function of @xmath60 for three different noise levels . as in fig .",
    "[ perfmetrics ] ( a ) , for the first two noise levels ( blue and magenta traces ) , the error appears to display an almost @xmath313-shaped dependence on @xmath60 , indicating that there is an optimal choice for @xmath60 that minimizes the error for each noise level .",
    "this value of @xmath60 also corresponds to the lowest value of the population synchrony index .",
    "however , for the largest noise level ( red trace ) , the error monotonically decreases as @xmath60 is increased .",
    "this could be indicative of the fact that , for this noise level , the population remains fairly desynchronized for a wide range of @xmath60 values .",
    "the effects of increasing the noise also depend on the value of @xmath60 . for small @xmath60 ,",
    "increasing the noise level first acts to decrease the error ( compare blue to magenta ) , but then drives it to its highest level ( red trace ) .",
    "however , when @xmath60 is larger , noise appears to always cause the error to decrease . for reference ,",
    "the black circle on the magenta trace shows the values of @xmath60 and @xmath79 that were used in the previous sections .    how do these parameter choices affect the networks firing rates ? like the relative error traces in ( b ) , the maximum population - averaged firing rates , figure [ perfmetrics ] ( c ) , also display a @xmath313-shaped dependence on @xmath60 , and the shallowness of the @xmath313 increases as the noise level is increased .",
    "this indicates that with increasing noise , there is a larger range of @xmath60 values that lead to low firing rates .",
    "lastly , figure [ perfmetrics ] ( d ) plots the cv of the isis of the network during the period of zero stimulus input . for moderate noise and moderate @xmath60 ,",
    "the network maintains cv on the order of @xmath314 .    in conclusion , network performance is not highly sensitive to changes in synaptic strength @xmath60 or to the level of added voltage noise , as there exist many combinations of choices that lead to similar network performance .",
    "until now , we have assumed that the decoder has access to all neurons in the network that is performing the computation on the input ; that is , we have fixed our network size at @xmath195 cells and have examined its performance using the spiking output of all @xmath194 cells . however , when recording from real neural circuits , it is more likely that one would be measuring from a subset of cells involved in a given computation .",
    "the same is possible for different circuits `` downstream '' of a computing network .",
    "we explore how the reduction in decoding variance and the decoding error scales with the number of simultaneously recorded neurons .",
    "figure [ recsubsetfig ] ( a ) plots the reduction in decoding variance @xmath305 as a function of the number of simultaneously recorded neurons @xmath315 for the homogeneous integrator network with the box function input stimulus .",
    "the simulated network size was fixed at @xmath195 . to compute the reduction in decoding variance for a smaller network of size @xmath315 , a random subset of @xmath315 spike trains",
    "was chosen from a single simulated trial of the full network .",
    "we then computed the necessary covariance matrices using these spike trains , and averaged these matrices over all @xmath140 trials .",
    "these averaged covariance matrices were used to compute the ratio @xmath305 according to the formulae given in materials and methods section . the solid trace in ( a ) plots the result of these numerical simulations whereas the dashed trace plots the approximation ,    @xmath316    where @xmath317 ( @xmath318 ) is the mean correlation coefficient between cells in the stimulus - activated ( stimulus - depressed ) population and @xmath319 is the mean correlation coefficient between cells in the two different populations .",
    "note that these correlation coefficients were computed using all @xmath8 cells in the simulated network .",
    "figure [ recsubsetfig ] ( b ) plots the square root of the decoding error ,    @xmath172 ^ 2ds,\\ ] ]    as a function of the number of simultaneously recorded neurons . as the number of recorded neurons increases , the decoding error initially decreases as @xmath320 ( black dashed line ) , similar to what one would expect for independent poisson spiking , as implicitly assumed in many rate models .",
    "however , as the number of recorded neurons is increased further , the error from the spiking network decreases faster than @xmath320 .",
    "the predictions of our network about how the reduction in decoding variance and the decoding error both scale with the number of simultaneously recorded neurons could in principle be tested with dense multi - electrode arrays or optical imaging .",
    "however , these predictions would have to be modified to incorporate the effects of shared sensory noise or noise in the output of the decoder .",
    "we now explore how the total number of neurons in the network , @xmath8 , affects the fidelity of the computation .",
    "we limit our analysis to integration of the box function stimulus .",
    "as derived in materials and methods section , we scale both the entries of the matrix @xmath19 and the synaptic gain parameter @xmath60 with @xmath192 . using this scaling",
    "allows the total input to each neuron in the network to remain constant as the network size is varied .",
    "figure [ varynfig ] shows the results of these simulations .",
    "in particular , we explore how the population synchrony index , the relative error , the time- and population- averaged firing rate , and the integrated error vary as the network size is increased . in all plots ,",
    "the cyan trace at @xmath195 corresponds to the parameters used in our previous network simulations .",
    "panel ( a ) plots the inverse of the synchrony index as a function of @xmath8 for @xmath283 different values of the parameter @xmath84 , which scales the synaptic gain ( that is , @xmath321 ) .",
    "this highlights the differences between the curves corresponding to the different values of @xmath84 .",
    "it is clear that synchrony tends to always decrease as the network size is increased , though the maximum level of synchrony reached as well as the rate at which it decreases with n are both affected by @xmath84 .",
    "thus , as we have seen previously in figure [ perfmetrics ] , increasing the synaptic gain can lead to increased population synchrony ( compare the cyan and magenta traces ) .",
    "figure ( b ) plots the relative error as a function of @xmath8 . for small values of @xmath84 ,",
    "the error initially increases with @xmath8 , but quickly reaches an asymptote and remains constant with further increases in network size ( blue trace ) . as @xmath84 is increased , we quickly see a transition in the curves as the relative error now begins to decrease with @xmath8 .",
    "increasing @xmath84 initially causes the error to drop off faster with @xmath8 ( compare the red and cyan traces ) , but too large of a value for @xmath84 cause the error to drop off more slowly with @xmath8 ( compare the cyan and magenta traces ) .",
    "figure ( c ) plots the inverse time- and population - averaged firing rate during the period of zero stimulus input as a function of @xmath8 . as with the population synchrony index , the firing rates tend to decrease as @xmath8 is increased .    in sum",
    ", figures  [ varynfig ] ( a)-(c ) illustrate that the computational error produced by the network , as well as its firing rates and synchrony , all tend to decrease for larger networks .",
    "we next compare the trend in error against what would be naively expected in a simple  rate network \"  that is , one in which each neuron fires according to a prescribed firing rate in a population , and does so with independent poisson statistics . in this case",
    ", we expect that the square root of the mean integrated squared error will scale like @xmath322 . to compare the error in our spiking network , we plot the square root of the mean integrated squared error as a function of @xmath8 in figure  [ varynfig ] ( d ) . for @xmath323 ( cyan trace ) ,",
    "the error decreases as @xmath322 ( black dashed line ) just as for the poisson rate network .",
    "however , for such rate networks , the firing rates of individual units are fixed and do not vary with the network size . in our network , we clearly see a dramatic decrease in firing rates as network size grows , up to around @xmath195 cells ( fig .",
    "[ varynfig ] ( c ) ) .",
    "further increases in network size past this point lead to minimal decreases in the firing rates .",
    "the fact that the firing rates for our network change with network size is a strong difference from a poisson rate network .",
    "thus , even though our network produces a similar error scaling of error with @xmath8 as predicted under basic assumptions for firing rate networks , for network sizes between @xmath284-@xmath194 neurons , it manages to do so in a more efficient manner ",
    "it produces the same error with a lower average firing rate ( i.e. , fewer spikes ) .      in this section ,",
    "we highlight the generality of our approach by showing the output of the spike - based predictive coding networks that are performing computations other than  pure \" integration of its inputs over time .",
    "first , we study leaky integration , obtained in equation ( [ xeqn ] ) by choosing @xmath4 to be negative ( and continuing to take @xmath127 dimension for the signal @xmath271 ) .",
    "figure [ leakyintfig ] shows an example of the network performing leaky integration ( @xmath324 ) on the same box function input from figure [ netex ] ( a ) .",
    "all other network parameters are the same as in figure [ netex ] .",
    "the raster plot in figure [ leakyintfig ] ( a ) shows that the network still displays sparse irregular spiking when performing leaky integration . figure [ leakyintfig ] ( b ) shows the network estimate ( red ) plotted along with the actual signal ( blue ) , demonstrating that the leaky integration computation is performed with a high degree of accuracy ( the relative error is @xmath325 ) .",
    "lastly , figure [ leakyintfig ] ( c ) shows the firing rates of both the stimulus - activated and stimulus - depressed populations ; note that these eventually return to their baseline ( pre - input ) levels because the computation is leaky .",
    "next , we consider the computation of processing inputs through a two dimensional dynamical system that displays damped harmonic oscillations . here , the matrix @xmath4 is chosen as    @xmath326    in this case , the eigenvalues of the matrix @xmath4 are @xmath327 , and the solutions @xmath2 of the linear system ( [ xeqn ] ) will display damped oscillations as long as @xmath328 ( we use @xmath329 and @xmath330 ) .",
    "we take @xmath19 to be a @xmath331 matrix whose elements are chosen randomly .",
    "lastly , we use as our input @xmath82 a vector with @xmath332 being the box function input from figure [ netex ] ( a ) and @xmath333 .",
    "figure [ dampedoscfig ] plots the resulting network behavior .",
    "we again see sparse irregular spiking with firing rates that eventually return to their baseline level ( figure [ dampedoscfig ] ( a ) and ( b ) ) . as",
    "the signal @xmath2 is two dimensional , the network estimate @xmath29 is also two dimensional , and we plot both of the network estimates ( red ) along with the actual signals ( blue ) in figure [ dampedoscfig ] ( c ) and ( d ) . once again , the network is able to perform the required computation with a high degree of accuracy ( the relative error in ( c ) is @xmath334 and in ( d ) is @xmath335 ) .",
    "we have shown that networks of neurons with voltage - dependent spike - generating currents and realistic synaptic timescales can perform accurate spike - based computations .",
    "these networks are derived based upon the premise that the voltage traces of individual neurons represent an error signal between the network estimate and the actual signal , and that spikes occur whenever the error becomes too large .",
    "the key innovation we present that allows the network to accurately perform these computations is the inclusion of synapses with appropriate kinetics .",
    "two factors determine these kinetics .",
    "we begin by assuming that signals are  decoded \" from the network with synapses that have finite timescales of rise and decay (  double - exponential \" synapses ) .",
    "next , we account for the nonlinear dynamics of spike generating currents with `` compensating '' synapses , which allow the system to represent the projected error signal in the voltage traces of individual neurons .",
    "it is important to note two limitations of these additional factors .",
    "the first is that if the rate of rise of the double - exponential synapse is slower than the rate of change of the signal , then the accuracy of the computation will be affected .",
    "this is the case because the network simply can not respond quickly enough to accurately track the signal .",
    "secondly , the `` compensating '' synapses we introduce were designed to compensate for currents acting on the timescale of a single action potential .",
    "thus , slow adaptation currents are not accounted for by our approach .",
    "this will make it difficult for the network to maintain the persistent activity that is required when the desired computation is pure integration .",
    "interestingly , however , we find that when the desired computation is leaky , simulations suggest that adaptation may have a minimal effect on the performance of the network .",
    "thus , a network with strong adaptive currents is perhaps better suited to implement leaky integration or fast dynamics rather than perfect integration .",
    "our results prove the principle that mechanisms of spike - based computation previously derived for networks of idealized neurons and synapses @xcite can be extended to settings closer to the underlying biophysics . however , there is still distance to travel before we arrive at a  realistic \" biologically based system .",
    "the compensating synapses are somewhat complicated functions of time . moreover , these and other synaptic connections provide both positive and negative currents following a spike , a clear violation of dale s rule ( although recently neurons that release both gaba and glutamate have been found in rodents @xcite ) .",
    "more complex synaptic waveforms , and ones that change sign , could be implemented via intermediate synapses with different kinetics ( for example , a pathway with delayed feedforward inhibition will produce first positive , then negative , synaptic current ) .",
    "furthermore , recent advances in learning temporal connections between neurons @xcite , together with learning algorithms for the present spike - based computation framework @xcite , provide a basis to potentially derive a learning rule for the compensation filters . however , a question for future work is whether there are other network configurations that perform spike - based computation without the need for intermediate connections ( as for simpler settings in @xcite ) , and additionally with simpler synaptic waveforms than the compensating ones derived here .      as in @xcite ,",
    "our network approaches the notion of computations in neural circuits from the standpoint that a computation is distributed among the spike times of individual neurons .",
    "this stands in contrast to many studies in which the computation is assumed to be carried at the level of averaged firing rates @xcite , and to other related studies that derive network dynamics that minimize squared error in signal representation  @xcite . demonstrating the importance of spike timing in our network , we compared the accuracy of the underlying computation before and after shuffling these times but preserving trial - averaged firing rates ( fig .",
    "[ reconerr ] ) .",
    "shuffling indeed reduced the accuracy , a fact we related to the structure of spike - time correlations produced by the network .",
    "what are the advantages of using such a precise temporal representation ?",
    "it could be that distributing a computation among the spike times of individual cells endows the network with robustness to perturbations such as synaptic failure and lesions ( as was demonstrated by @xcite ) .",
    "moreover , a computation performed on the level of spike times opens the possibility that the underlying network structure could be learned via spike - based plasticity rules , as suggested by recent work @xcite .",
    "finally , a precise temporal code may leverage the computational power of individual spiking neurons in a more efficient manner than rate - based approaches .",
    "traditional spiking network implementations of rate - based networks employ large amounts of added voltage noise to avoid synchrony , and large cell populations , so that the resulting population output is well described by  mean field \" rate equations @xcite .",
    "thus large populations of cells with noise - driven spiking represent signals in traditional rate - based approaches .",
    "the possibility that spike - based computation might give rise to a significantly lower total error for a given population size .",
    "this seems likely , given the results in @xcite and in results section .",
    "that is , the error in our networks can decrease faster than expected for a population of cells with independent spike times . making a more direct comparison to rate - based networks",
    "is an interesting area for future work .",
    "this said , this spike - based approach to computation is not immune to problems with synchrony , and the need for additive noise to combat it .",
    "we showed that there is an optimal level of this noise at which the network retains characteristics of spike - based computation .",
    "moreover , in the current work , we have used a very homogeneous population in which all neurons have the same spike - generating currents and magnitude of synaptic connectivity .",
    "preliminary simulations suggest that more heterogeneous networks better avoid synchrony , and hence may be able to perform computations with higher accuracy .",
    "cortical neurons are known to display irregular poisson - like firing @xcite .",
    "what might be the basis of the observed irregularity ?",
    "various authors have proposed that the variability is a result of a tight balance between the total excitatory and inhibitory current each neuron in the network receives @xcite . indeed cortical networks have been shown to display such a balance between excitation and inhibition @xcite .",
    "while successful in reproducing the poisson - like firing of cortical neurons , it is only very recently been shown that balanced networks can be used to perform particular computations , including integration of inputs over time @xcite .",
    "our work contributes further results in this direction .",
    "in contrast to much previous modeling work on balanced networks , the specific condition for balancing excitation and inhibition is not built into the derivation of spike - based computation .",
    "rather , the balanced state arises naturally as a consequence of optimizing the computation at the level of single cells  that is , assuming that neurons represent a projected error signal in their voltage traces and spike when this error becomes too large @xcite .",
    "furthermore , our network maintains this balanced state with a relatively small number of cells ( e.g. , fig .",
    "[ netex ] ) .",
    "thus , our work suggests that a computational unit in the brain may require dramatically fewer neurons than predicted by rate - based approaches @xcite .",
    "finally , nearly all modeling work on balanced networks makes use of simplified integrate - and - fire neuronal spiking dynamics .",
    "thus , it has remained unclear whether or not the balanced state can be maintained by a network of neurons with more complex spike - generating dynamics .",
    "we have shown here that it is indeed possible for a network of such neurons to display a tight balance between excitation and inhibition , and thus display irregular spiking .    even during a single trial , the neurons in our network display variable spiking behavior .",
    "this can be seen from the population firing rates in figure [ netex ] ( d ) . even though the network maintains a constant decoded signal ,",
    "the average firing rates fluctuate , indicating that not all neurons are displaying the same firing rate .",
    "this phenomenon of variable neuronal activity underlying stable network stimulus representation is known to occur @xcite and has recently been shown in a rate model network with a specific architecture @xcite . here",
    ", we show that stable stimulus representation with variable neuronal responses arises as a natural feature of networks that perform spike - based computation .",
    "the performance of network models that integrate inputs over time is typically quite sensitive to the choice of connection weights between neural populations @xcite .",
    "if the recurrent connections are either too strong or too weak , the activity of the network can either quickly increase to saturation or decrease to a baseline level .",
    "recent work by lim and goldman @xcite has shown that this sensitivity issue can be resolved in a rate - based network where inhibition and excitation are balanced . in particular , lim and goldman show that a balanced rate - based network of leaky - integrate - and - fire ( lif ) neurons can robustly maintain information for working memory with irregular spiking .",
    "further work will be needed to assess whether the spike - based networks derived here have similar robustness to changes in network connection strengths ; our preliminary studies suggest that they may not , as perturbing the network structure away from the optimally derived connectivity will lead to decreased network accuracy",
    ". however , there is evidence to suggest that the optimal connectivity structure could potentially be learned , and maintained , by plasticity rules @xcite .",
    "averbeck b , latham p , pouget a ( 2006 ) neural correlations , population coding and computation .  7 : .",
    "boerlin m , denve s ( 2011 ) spike - based population coding and working memory .",
    "7(2 ) : .",
    "boerlin m , machens c , denve s ( 2013 ) predictive coding of dynamical variables in balanced spiking networks .  9 : .",
    "bogacz r , brown e , moehlis j , hu p , holmes p , cohen j ( 2006 ) the physics of optimal decision making : a formal analysis of models of performance in two alternative forced choice tasks .",
    "bourdoukan r , barrett d , machens c , denve s ( 2012 ) learning optimal spike - based representations . in _ nips _ , .",
    "brody c , romo r , kepecs a ( 2003 ) basic mechanisms for graded persistent activity : discrete attractors , continuous attractors , and dynamic representations .  13 : .",
    "brunel n ( 2000 ) dynamics of sparsely connected networks of excitatory and inhibitory spiking neurons .  8 : .",
    "buzski g , draguhn a ( 2004 ) neuronal oscillations in cortical networks .",
    "cain n , shea - brown e ( 2012 ) computational models of decision making : integration , stability , and noise .  22 : .",
    "compte a , brunel n , goldman - rakic p , wang xj ( 2000 ) synaptic mechanisms and network dynamics underlying spatial working memory in a cortical network model .",
    "10(9 ) : .",
    "druckmann s , chklovskii db ( 2012 ) neuronal circuits underlying persistent representations despite time varying activity .  22 : .",
    "eckhoff p , wong - lin kf , holmes p ( 2011 ) dimension reduction and dynamics of a spiking neural network model for decision making under neuromodulation .  10 : .    erisir a , lau d , rudy b , leonard c ( 1999 ) function of specific k+ channels in sustained high - frequency firing of fast - spiking neocortical interneurons .  82 : .",
    "faisal a , selen l , wolpert d ( 2008 ) noise in the nervous system .  9 : .    gold j , shadlen m ( 2007 ) the neural basis of decision making .  30 : .",
    "goldman m ( 2009 ) memory without feedback in a neural network .",
    "goldman m , levine j , major g , tank d , seung h ( 2003 ) robust persistent neural activity in a model integrator with multiple hysteretic dendrites per neuron .  13 : .    golomb d ( 2007 ) neural synchrony measures .",
    "2(1 ) : .",
    "haider b , duque a , hasenstaub a , mccormick d ( 2006 ) neocortical network activity in vivo is generated through a dynamic balance of excitation and inhibition .",
    "26(17 ) : .",
    "haider b , mccormick da ( 2009 ) rapid neocortical dynamics : cellular and network mechanisms .  62 : .",
    "hodgkin a , huxley a ( 1952 ) a quantitative description of membrane current and its application to conduction and excitation in nerve .  117 : .",
    "hoppensteadt fc , peskin c ( 2001 ) springer - verlag , new york .",
    "hu y , zylberberg j , shea - brown e ( 2014 ) the sign rule and beyond : boundary effects , flexibility , and noise correlations in neural population codes .  10 : .",
    "jonides j , lewis r , nee d , lustig c , berman m , moore k ( 2008 ) the mind and brain of short - term memory .  59 : .",
    "kennedy a , wayne g , kaifosh p , alvia k , abbott l , sawtell n ( 2014 ) a temporal basis for predicting the sensory consequences of motor commands in an electric fish .  17 : .",
    "lampl i , reichova i , ferster d ( 1999 ) synchronous membrane potential fluctuations in neurons of the cat visual cortex .  22 : .    lim s , goldman m ( 2013 ) balanced cortical microcircuitry for maintaining information in working memory .",
    "16(9 ) : .",
    "lim s , goldman m ( 2014 ) balanced cortical microcircuitry for spatial working memory based on corrective feedback control .  34 : .    machens c , romo r , brody c ( 2005 ) flexible control of mutual inhibition : a neural model of two - interval discrimination .  307 : .    mainen zf , joerges j , huguenard jr , sejnowski tj ( 1995 ) a model of spike initiation in neocortical pyramidal neurons .  15 : .",
    "okun m , lampl i ( 2008 ) instantaneous correlation of excitation and inhibition during ongoing and sensory - evoked activities .  11 : .    ostojic s ( 2014 ) two types of asynchronous activity in networks of excitatory and inhibitory spiking neurons .  17 : .",
    "renart a , brunel n , wang xj ( 2004 ) mean - field theory of irregularly spiking neuronal populations and working memory in recurrent cortical networks . in _",
    "computational neuroscience : a comprehensive approach _ , . chapman and hall .",
    "renart a , de  la rocha j , bartho p , hollender l , parga n , reyes a , harris k ( 2010 ) the asynchronous state in cortical circuits .  327 :",
    ".    root d , mejias - aponte c , zhang s , wang hl , hoffman a , lupica c , morales m ( 2014 ) single rodent mesohabenular axons release glutamate and gaba .",
    "rotaru d , yoshino h , lewis d , ermentrout g , gonzalez - burgos g ( 2011 ) glutamate receptor subtypes mediating synaptic activation of prefrontal cortex neurons : relevance for schizophrenia .  31 : .",
    "rozell c , johnson d , baraniuk r , olshausen b ( 2008 ) sparse coding via thresholding and local competition in neural circuits .  20 : .    salin p , prince d ( 1996 ) spontaneous @xmath336 receptor - mediated inhibitory currents in adult rat somatosensory cortex .  75 : .    seung h ( 1996 ) how the brain keeps the eyes still .  93 : .",
    "seung h , lee d , reis b , tank d ( 2000 ) stability of the memory of eye position in a recurrent network of conductance - based model neurons .  26 : .",
    "shadlen mn , newsome wt ( 1998 ) the variable discharge of cortical neurons : implications for connectivity , computation , and information coding .  18 :",
    ".    softky w , koch c ( 1993 ) the highly irregular firing of cortical cells is incosistent with temporal integration of random epsp s .  13 : .",
    "tchumatchenko t , malyshev a , wolf f , volgushev m ( 2011 ) ultrafast population encoding by cortical neurons .  31 : .",
    "traub r , miles r ( 1995 ) pyramidal cell - to - inhibitory cell spike transduction explicable by active dendritic conductances in inhibitory cell .",
    "2(4 ) : .",
    "uhlenbeck g , ornstein l ( 1930 ) on the theory of the brownian motion .  36 : .",
    "usher m , mcclelland j ( 2001 ) on the time course of perceptual choice : the leaky competing accumulator model .  108 : .",
    "van vreeswijk c , sompolinsky h ( 1996 ) chaos in neuronal networks with balanced excitatory and inhibitory activity .  274 : .",
    "wang xj ( 2002 ) probabilistic decision making by slow reverberation in cortical circuits .  36 : .",
    "wehr m , zador a ( 2003 ) balanced inhibition underlies tuning and sharpens spike timing in auditory cortex .  426 : .",
    "wong kf , wang xj ( 2006 ) a recurrent network mechanism of time integration in perceptual decisions .  26 : .",
    "xiang z , huguenard j , prince d ( 1998 ) @xmath336 receptor - mediated currents in interneurons and pyramidal cells of rat visual cortex .  506 : .    yu j , ferster d ( 2010 ) membrane potential synchrony in primary visual cortex during sensory stimulation .  68 :",
    "+ * figure 1 *"
  ],
  "abstract_text": [
    "<S> while spike timing has been shown to carry detailed stimulus information at the sensory periphery , its possible role in network computation is less clear . </S>",
    "<S> most models of computation by neural networks are based on population firing rates . in equivalent spiking implementations , </S>",
    "<S> firing is assumed to be random such that averaging across populations of neurons recovers the rate - based approach . </S>",
    "<S> recently , however , denve and colleagues have suggested that the spiking behavior of neurons may be fundamental to how neuronal networks compute , with precise spike timing determined by each neuron s contribution to producing the desired output . by postulating that each neuron fires in order to reduce the error in the network s output , it was demonstrated that linear computations can be carried out by networks of integrate - and - fire neurons that communicate through instantaneous synapses . </S>",
    "<S> this left open , however , the possibility that realistic networks , with conductance - based neurons with subthreshold nonlinearity and the slower timescales of biophysical synapses , may not fit into this framework . here , we show how the spike - based approach can be extended to biophysically plausible networks . </S>",
    "<S> we then show that our network reproduces a number of key features of cortical networks including irregular and poisson - like spike times and a tight balance between excitation and inhibition . </S>",
    "<S> lastly , we discuss how the behavior of our model scales with network size , or with the number of neurons  recorded \" from a larger computing network . </S>",
    "<S> these results significantly increase the biological plausibility of the spike - based approach to network computation . </S>"
  ]
}