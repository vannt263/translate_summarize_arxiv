{
  "article_text": [
    "we consider the minimization problem of an unknown risk function @xmath0 , where @xmath1 is the dimension of the statistical model .",
    "we assume the existence of a risk minimizer @xmath2 where the risk function corresponds to the expectation of an appropriate loss function w.r.t . an unknown distribution . in empirical risk minimization , this quantity is usually estimated by its empirical version from an i.i.d",
    "however , in many problems such as local @xmath3-estimation or errors - in - variables models , a nuisance parameter can be involved in the empirical version .",
    "this parameter most often coincides with some bandwidth related to a kernel that gives rise to `` kernel empirical risk minimization . ''",
    "one typically deals with this issue in pointwise estimation , as , for example , in polzehl and spokoiny @xcite with localized likelihoods or in chichignoud and lederer @xcite with local @xmath3-estimators . in learning theory , many authors have recently investigated supervised and unsupervised learning with errors in variables . as a rule ,",
    "such matters require one to plug deconvolution kernels into the empirical risk , as loustau and marteau @xcite in noisy discriminant analysis or hall and lahiri @xcite in quantile and moment estimation ; see also dattner , reiss and trabs @xcite .    in the above papers",
    ", the authors studied the theoretical properties of kernel empirical risk minimizers and proposed deterministic choices of bandwidths to deduce optimal minimax results .",
    "as usual , these optimal bandwidths are related to the smoothness of the target function or the underlying density and are not achievable in practice .",
    "adaptivity is therefore one of the biggest challenges . in this respect ,",
    "data - driven bandwidth selections have been already proposed in @xcite , which are all based on lepski - type procedures .",
    "lepski - type procedures are rather appropriate to construct data - driven bandwidths involved in kernels ; for further details , see , for example , @xcite .",
    "it is well known that they suffer from the restriction to isotropic bandwidths with multidimensional data , which is the consideration of nested neighborhoods ( hyper - cube ) .",
    "many improvements were made by kerkyacharian , lepski and picard @xcite and more recently by goldenshluger and lepski @xcite to select anisotropic bandwidths ( hyper - rectangle ) .",
    "nevertheless , their approach still does not provide anisotropic bandwidth selection for nonlinear estimators , which is the scope of this paper .",
    "the only work we can mention is @xcite in a restrictive case , which is pointwise estimation in nonparametric regression .",
    "therefore , the study of data - driven selection of anisotropic bandwidths is still an open issue .",
    "moreover , this field is of great interest in practice , especially in image denoising ; see , for example , @xcite .",
    "the main contribution of our paper is to bring new insights to the problem of bandwidth selection in kernel empirical risk minimization in a possible anisotropic framework .",
    "to this end , we first introduce a new criterion called _ gradient excess risk _ , which makes the anisotropic bandwidth selection possible .",
    "we then provide a novel data - driven selection based on the comparison of `` gradient empirical risks . ''",
    "that can be viewed as an extension of the so - called goldenshluger ",
    "lepski method ( gl method ; see @xcite ) and of the empirical risk comparison method ( erc method ; see @xcite ) . eventually , we derive an upper bound for the gradient excess risk ( called gradient inequality ) and optimal results in many settings , such as pointwise and global estimation in nonparametric regression and clustering with errors in variables .",
    "note that we consider the risk minimization over the finite dimensional set @xmath4 . in statistical learning or nonparametric",
    "estimation , one usually aims at estimating a functional object belonging to some hilbert space .",
    "however , in many examples , the target function can be approximated by a finite object , thanks , for instance , to a suitable decomposition in a basis of the hilbert space .",
    "this is typically the case in local @xmath3-estimation , where the target function is assumed to be locally polynomial ( and even constant in many cases ) .",
    "moreover , in statistical learning , one is often interested in the estimation of a finite number of parameters , as in clustering .",
    "the extension to the infinite - dimensional case is discussed in section  [ sdiscussion ] .",
    "the structure of this paper is as follows : the main ideas behind the gradient excess risk are introduced in the remainder of this section .",
    "an upper bound for the gradient excess risk of the data - driven procedure is presented in section  [ gradientinequality ] .",
    "this procedure is applied to clustering in section  [ sectionkmeans ] and to robust nonparametric regression in section  [ sectionlocalglobal ] .",
    "additionally , a discussion of our assumptions and an outlook are given in section  [ sdiscussion ] , and section  [ ssimu ] illustrates the behavior of the method with numerical results .",
    "the proofs are finally conducted in the .      in the literature , such as in statistical learning ,",
    "the excess risk @xmath5 is the main criterion to measure the performance of some estimator @xmath6 .",
    "originally , vapnik and chervonenkis @xcite proposed to control this quantity via the empirical process theory , which gives rise to slow rates @xmath7 for the excess risk ; see also @xcite . in the last decade",
    ", many authors have improved such a bound by giving fast rates @xmath8 using the so - called localization technique ; see @xcite and boucheron , bousquet and lugosi @xcite for an overview in classification .",
    "this technique consists of studying the increments of an empirical process in the neighborhood of the target @xmath9 .",
    "in particular , it requires a variance - risk correspondence , equivalent to the eminent margin assumption .",
    "as far as we know , this complicated modus operandi is the major obstacle to the anisotropic bandwidth selection issue . in",
    "what follows , we introduce an alternative criterion to solve this issue , namely the gradient excess risk ( @xmath10-excess risk , for short , in the sequel ) .",
    "this quantity is defined as @xmath11 whereas denotes the euclidean norm on @xmath12 and @xmath13 denotes the gradient of the risk @xmath14 . with a slight abuse of notation",
    ", @xmath10 denotes the gradient , whereas @xmath15 denotes the @xmath16-excess risk . under regularity assumptions on @xmath17",
    ", the @xmath10-excess risk is linked with the excess risk , thanks to the following lemma .",
    "[ lemmadmargin ] let @xmath9 , defined as in ( [ oracle ] ) , and @xmath18 be the euclidean ball of @xmath19 centered at @xmath9 , with radius @xmath20 .",
    "assume @xmath21 is @xmath22 , each second partial derivative of @xmath14 is bounded on @xmath18 by a constant @xmath23 and the hessian matrix @xmath24 is positive definite at @xmath9 .",
    "then , for @xmath20 small enough , we have @xmath25 where @xmath26 is the smallest eigenvalue of @xmath27 .    the proof is based on the inverse function theorem and a taylor expansion of the function @xmath17 .",
    "let us explain how this lemma , together with standard probabilistic tools , leads to fast rates for the excess risk . in this section",
    ", @xmath28 denotes the usual empirical risk with associated gradient @xmath29 and associated erm @xmath30 for ease of exposition . under the assumptions of lemma [ lemmadmargin ] , @xmath31 , and we have the following heuristic : @xmath32\\\\[-8pt]\\nonumber & \\leq & \\sup_{{\\theta}\\in{{\\mathbb r}}^m}\\bigl { \\vert}{g}({\\theta})-{\\widehat{g}}({\\theta})\\bigr{\\vert}_2\\lesssim n^{-1/2},\\end{aligned}\\ ] ] where @xmath33 denotes the inequality up to some positive constant",
    "the last bound only requires a concentration inequality applied to the empirical process @xmath34 .",
    "therefore , this heuristic provides fast rates for the excess risk without any localization technique .",
    "furthermore , similar bounds can be obtained for the @xmath35-norm @xmath36 using the same path .",
    "indeed , under the same assumptions , the assertion of lemma [ lemmadmargin ] holds , replacing the square root of the excess risk by @xmath36 ( see the proof of lemma [ lemmadmargin ] ) , and then optimal rates are deduced .    from the model selection point of view ,",
    "standard penalization techniques  based on localization ",
    "suffer from the dependency on parameters involved in the margin assumption .",
    "more precisely , in the strong margin assumption framework , the construction of the penalty requires the knowledge of @xmath26 , related to the hessian matrix of the risk .",
    "although many authors have recently investigated the adaptivity w.r.t .",
    "these parameters , by proposing `` margin - adaptive '' procedures ( see @xcite for the propagation method , @xcite for aggregation and @xcite for the slope heuristic ) , the theory is not completed and remains a hard issue ; see the related discussion in section  [ sdiscussion ] .",
    "as an alternative , our data - driven procedure does not suffer from the dependency on @xmath26 since we focus on a gradient inequality in section  [ gradientinequality ] .      in this section ,",
    "the kernel empirical risk minimization is properly defined and illustrated with two examples : local estimators and deconvolution @xmath37-means .",
    "for some @xmath38 , consider a random variable @xmath39 distributed according to @xmath40 , absolutely continuous w.r.t . the lebesgue measure . in",
    "what follows , we observe a sample @xmath41 of independent and identically distributed ( i.i.d . )",
    "random variables according to @xmath40 .",
    "moreover , we call a kernel of order @xmath42 a symmetric function @xmath43 , @xmath44 , which satisfies the following properties :    * @xmath45 , * @xmath46 @xmath47 , * @xmath48 , @xmath49 .    for any @xmath50 , the dilation @xmath51 is defined as @xmath52 where @xmath53 . for a given kernel @xmath54",
    ", we define the kernel empirical risk indexed by an anisotropic bandwidth @xmath55^d$ ] as @xmath56 and an associated kernel empirical risk minimizer ( kernel erm ) as @xmath57 the function @xmath58 is a loss function associated to a kernel @xmath59 such that @xmath60 is twice differentiable @xmath40-almost surely and such that the limit of its expectation coincides with the risk , that is , @xmath61 where @xmath62 denotes the expectation w.r.t . the distribution of the sample @xmath63 .",
    "the agenda is the data - driven selection of the `` best '' estimator in the family @xmath64 .",
    "this issue arises in many examples , such as local fitted likelihood ( polzehl and spokoiny @xcite ) , image denoising ( astola et  al .",
    "@xcite ) and robust nonparametric regression ; see chichignoud and lederer @xcite .",
    "in such a framework , we observe a sample of i.i.d .",
    "pairs @xmath65 , and the kernel empirical risk has the following general form : @xmath66 where @xmath67 is some likelihood and @xmath68 .",
    "another example arises when we observe a contaminated sample @xmath69 , @xmath70 in the problem of clustering . in this case",
    ", the kernel empirical risk is defined according to @xmath71 where @xmath72 is a deconvolution kernel and @xmath73 is a codebook .    in the next section ,",
    "we present the bandwidth selection rule in the general context of kernel empirical risk minimization .",
    "we especially deal with clustering with errors in variables and robust nonparametric regression in sections  [ sectionkmeans ] and [ sectionlocalglobal ] , respectively .",
    "the anisotropic bandwidth selection issue has been recently investigated in goldenshluger and lepski @xcite ( gl  method ) in density estimation ; see also @xcite for deconvolution estimation and @xcite for the white noise model .",
    "this method , based on the comparison of estimators , requires some `` linearity '' property , which is trivially satisfied by kernel estimators .",
    "however , kernel erms are usually nonlinear ( except for the least square estimator ) , and the gl method can not be directly applied to such estimators . to tackle this issue , we introduce a new selection rule based on the comparison of gradient empirical risks instead of estimators ( i.e. , kernel erm ) . to that end , we first introduce some notations . for any @xmath74 and any @xmath75 , the gradient empirical risk ( @xmath10-empirical risk ) is defined as @xmath76 note that we have coarsely @xmath77 since @xmath78 is twice differentiable almost surely . according to ( [ eqlimitrisk ] )",
    ", we also notice that the limit of the expectation of the @xmath16-empirical risk coincides with the gradient of the risk .",
    "following goldenshluger and lepski @xcite , we introduce an auxiliaryempirical risk in the comparison .",
    "for any couple of bandwidths @xmath79 and any @xmath80 , the auxiliary @xmath16-empirical risk is defined as @xmath81 where @xmath82 stands for the convolution between @xmath51 and @xmath83 . the gradient inequality stated in theorem [ thmainresult ]",
    "is based on the control of some random processes as follows .",
    "[ defmajorant ] for any integer @xmath84 , we call _ majorant _ a function @xmath85 such that @xmath86 where @xmath87 for all @xmath88 with @xmath89 the euclidean norm on @xmath19 , and @xmath90 is understood coordinatewise .    the main issue for applications is to compute right order majorants .",
    "it follows from classical tools such as talagrand s inequalities ( talagrand @xcite , boucheron , lugosi and massart @xcite , bousquet @xcite ; see also @xcite ) . in sections",
    "[ sectionkmeans ] and  [ sectionlocalglobal ] such majorant functions are computed in clustering and in robust nonparametric regression .",
    "we are now ready to define the selection rule as @xmath91 where @xmath92 is an estimate of the bias  variance decomposition at a given bandwidth @xmath74 .",
    "it is explicitly defined as @xmath93 the kernel erm @xmath94 , defined in ( [ defkerm ] ) , with bandwidth @xmath95 , selected in ( [ defrule ] ) , satisfies the following bound .    [ thmainresult ] for any @xmath96 and for any @xmath97 , we have with probability @xmath98 , @xmath99 where @xmath100 is a bias function defined as @xmath101    theorem [ thmainresult ] is the main result of this paper .",
    "the @xmath16-excess risk of the data - driven estimator @xmath94 is bounded with high probability .",
    "the rhs in the gradient inequality can be viewed as the minimization of a usual bias  variance trade - off . indeed , the bias term @xmath102 is deterministic and tends to @xmath103 as @xmath104 .",
    "the majorant @xmath105 upper bounds the stochastic part of the @xmath106-empirical risk and can be viewed as a variance term .",
    "the gradient inequality of theorem [ thmainresult ] is sufficient to establish adaptive fast rates in noisy clustering and adaptive minimax rates in nonparametric estimation ; see sections  [ sectionkmeans ]  and  [ sectionlocalglobal ] .",
    "moreover , the construction of the selection rule ( [ defrule ] ) , as well as the upper bound in theorem [ thmainresult ] , does not suffer from the dependency on @xmath26 related to the smallest eigenvalue of the hessian matrix of the risk ; see lemma [ lemmadmargin ] .",
    "in other words , the method is robust w.r.t .",
    "this parameter , which is a major improvement in comparison with other adaptive or model selection methods of the literature cited in the .",
    "proof of theorem [ thmainresult ] for some @xmath74 , we start with the following decomposition : @xmath107\\\\[-8pt]\\nonumber & \\leq&{\\vert}{\\widehat{g}}_{\\widehat h}-{\\widehat{g}}_{\\widehat h , h}{\\vert}_{2,\\infty}+ { \\vert}{\\widehat{g}}_{\\widehat h , h}-{\\widehat{g}}_{h}{\\vert}_{2,\\infty}+{\\vert}{\\widehat{g}}_{h}-{g}{\\vert}_{2,\\infty}.\\end{aligned}\\ ] ] by definition of @xmath95 in ( [ defrule ] ) , the first two terms in the rhs of ( [ eqdecompositionexcessrisk ] ) are bounded as follows : @xmath108\\\\[-8pt]\\nonumber & & \\qquad \\leq \\sup_{\\eta\\in{{\\mathcal h } } } \\bigl\\{{\\vert}{\\widehat{g}}_{h,\\eta}- { \\widehat{g}}_{\\eta } { \\vert}_{2,\\infty}-{\\mathcal{m}}_{\\ell}(h , \\eta ) \\bigr\\ } + { \\mathcal{m } } _ { \\ell}^\\infty ( h ) \\nonumber \\\\ & & \\quad\\qquad{}+ \\sup_{\\eta\\in{{\\mathcal h } } } \\bigl\\{{\\vert}{\\widehat{g}}_{\\widehat h,\\eta}- { \\widehat{g}}_{\\eta } { \\vert}_{2,\\infty}-{\\mathcal{m}}_{\\ell}(\\widehat h , \\eta ) \\bigr\\}+{\\mathcal{m}}_{\\ell}^\\infty(\\widehat h ) \\nonumber \\\\ & & \\qquad = \\widehat{\\mathrm{bv}}(h)+\\widehat{\\mathrm{bv}}(\\widehat h)\\leq2\\widehat { \\mathrm{bv}}(h).\\nonumber\\end{aligned}\\ ] ] besides , the last term in ( [ eqdecompositionexcessrisk ] ) is controlled as follows : @xmath109 using ( [ eqdecompositionexcessrisk ] ) and ( [ eqcontrolvar ] ) , together with the last inequality , we have for all @xmath110 , @xmath111 it then remains to control the term @xmath92 .",
    "we have @xmath112 the gradient inequality follows directly from ( [ eqbounddexcessrisk ] ) , definition [ defmajorant ] and the definition of @xmath113 .",
    "let us consider an integer @xmath114 and a @xmath115-random variable @xmath116 with law @xmath40 with density @xmath117 w.r.t .",
    "the lebesgue measure on @xmath115 satisfying @xmath118 , where stands for the euclidean norm in @xmath115 .",
    "moreover , we restrict the study to the compact set @xmath119^d$ ] , assuming that @xmath120^d$ ] almost surely .",
    "we want to construct @xmath37 centroids minimizing some distortion , @xmath121 where @xmath122 is a candidate codebook of @xmath37 centroids .",
    "for ease of exposition , we study this quantization problem with the euclidean distance , by choosing the standard @xmath37-means loss function , namely , @xmath123 in this section , we are interested in the inverse statistical learning context ( see  @xcite ) , which corresponds to the minimization of ( [ distortion ] ) , thanks to a noisy set of observations , @xmath124 where @xmath125 are i.i.d . with density @xmath126 w.r.t . the lebesgue measure on @xmath115 and mutually independent of the original sample @xmath127 .",
    "this topic was first considered in @xcite , where general oracle inequalities are proposed .",
    "let us fix a kernel @xmath51 of order @xmath128 with @xmath74 and consider @xmath129 a deconvolution kernel defined such that @xmath130={{\\mathcal f}}[k_h]/{{\\mathcal f}}[g]$ ] , where @xmath131 stands for the usual fourier transform .",
    "as introduced in section  [ sectionkerm ] , we have at our disposal the family of kernel erm defined as @xmath132 where @xmath133^d}f(x)g(\\cdot - x)\\,dx$ ] stands for the convolution product ( restricted to the compact @xmath119^d$ ] for simplicity ) . from an adaptive point of view , chichignoud and loustau @xcite",
    "have recently investigated the problem of choosing the bandwidth in ( [ noisykmeans ] ) .",
    "they established fast rates of convergence  up to a logarithmic term  for a data - driven selection of @xmath134 , based on a comparison of kernel empirical risks .",
    "however , their approach is restricted to isotropic bandwidth selection and depends on the parameters involved in the margin assumption ( in particular on @xmath26 in lemma [ lemmadmargin ] ) .    in the following , adaptive fast rates of convergence for the excess risk",
    "are obtained via the gradient approach . for this purpose",
    ", we assume that the hessian matrix @xmath135 is positive definite . this assumption was considered for the first time in pollard @xcite and is often referred as pollard s regularity assumptions ; see also  @xcite . under these assumptions , we can state the same kind of result as lemma [ lemmadmargin ] in the framework of clustering with @xmath37-means .    [ dclustering ]",
    "let @xmath136 be a minimizer of ( [ distortion ] ) , and assume @xmath117 is continuous and @xmath137 is positive definite .",
    "let @xmath18 be the euclidean ball center at @xmath138 with radius @xmath20 .",
    "then , for @xmath139 sufficiently small , @xmath140 where @xmath141 is a constant which depends on @xmath137 , @xmath37 and @xmath142 .",
    "we have at our disposal a family of kernel erm @xmath143 with associated kernel empirical risk @xmath144 defined in ( [ noisykmeans ] ) .",
    "we propose to apply the selection rule ( [ defrule ] ) to choose the bandwidth @xmath74 . in this problem",
    "as well , we first consider the @xmath16-excess risk approach to establish adaptive fast rates of convergence for the excess risk . for any @xmath74 ,",
    "the @xmath106-empirical risk vector of @xmath145 is given by @xmath146^d}w ( { \\mathbf{c}},x)\\widetilde k_h(z_i - x)\\,dx \\biggr)_{(u , j)\\in\\{1 , \\ldots , d\\}\\times\\{1,\\ldots , k\\ } } \\\\ & = & \\biggl(-\\frac{1}{n}\\sum_{i=1}^n2 \\int_{v_j({\\mathbf{c}})}\\bigl(x^u - c_j^u \\bigr)\\widetilde k_h(z_i - x)\\,dx \\biggr)_{(u , j)\\in\\{1 , \\ldots , d\\}\\times\\{1,\\ldots , k\\}},\\end{aligned}\\ ] ] where @xmath147 denotes the @xmath148th coordinate of @xmath149 and @xmath150 , @xmath151 are open vorono cells associated to @xmath152 , defined as @xmath153^d\\dvtx \\forall u\\neq j , { \\vert}x - c_j{\\vert}_2<{\\vert}x - c_u{\\vert}_2\\}$ ] .",
    "note that @xmath154 a.s . by smoothness .",
    "the construction of the rule follows the general case of section  [ gradientinequality ] , which requires the introduction of an auxiliary @xmath16-empirical risk .",
    "for any couple of bandwidths @xmath79 , the auxiliary @xmath16-empirical risk is defined as @xmath155 where @xmath156 is the auxiliary deconvolution kernel as in comte and lacour @xcite .",
    "the statement of the oracle inequality is based on the computation of a majorant function . for this purpose , we need the following additional assumptions on the kernel @xmath157 .",
    "( * k1 * ) there exists @xmath158 such that the kernel @xmath54 satisfies @xmath159\\subset[-s , s]\\quad\\mbox{and}\\quad\\sup_{t\\in{{\\mathbb r}}^d } \\bigl{\\vert}\\mathcal{f}[k](t)\\bigr{\\vert } < \\infty , \\ ] ] where @xmath160 and @xmath161=\\bigotimes_{v=1}^d [ -s_v , s_v]$ ] .",
    "this assumption is standard in deconvolution estimation and is satisfied by many standard kernels , such as the _ sinc _ kernel .",
    "we also consider a kernel @xmath54 of order @xmath162 , according to the definition of section  [ sectionkerm ] .",
    "kernels of order @xmath163 satisfying ( * k1 * ) could be constructed by using the so - called meyer wavelet ; see @xcite .",
    "additionally , we need an assumption on the noise distribution @xmath126 :    there exist a vector @xmath164 and a positive constant @xmath165 such that for all @xmath166 , @xmath167(t)\\bigr{\\vert}\\geq\\rho\\prod_{j=1}^d \\biggl(\\frac { t_j^2 + 1}{2 } \\biggr)^{-\\beta_j/2}. \\ ] ]    * na(@xmath168 ) * deals with a polynomial behavior of the fourier transform of the noise density @xmath126 .",
    "an exponential decreasing of the characteristic function of @xmath126 is not considered in this paper for simplicity ; see @xcite in multivariate deconvolution for such a study .",
    "we are now ready to compute some majorant functions in our context . for",
    "some @xmath169 , let @xmath170^d $ ] be the bandwidth set such that @xmath171 , @xmath172    [ lemmamajkmeans ] assume @xmath173 and @xmath174 hold for some @xmath175 and some @xmath176 .",
    "let @xmath177 , and consider @xmath178 an exponential net of @xmath179^d $ ] , such that @xmath180 .",
    "for any integer @xmath84 , let us introduce the function @xmath181 defined as @xmath182 where @xmath183 is linear in @xmath184 and independent of @xmath185 ; see the for details .    then , for @xmath185 sufficiently large , the function @xmath186 is a majorant , that is , @xmath187 where @xmath62 denotes the expectation w.r.t . to the sample and @xmath188^{dk}}{\\vert}t({\\mathbf{c}}){\\vert}_2 $ ] for all @xmath189 with @xmath89 the euclidean norm on @xmath145 .",
    "the proof is based on a talagrand inequality ; see the .",
    "this lemma is the cornerstone and gives the order of the variance term in such a problem .",
    "we are now ready to define the selection rule in this setting as @xmath190 where @xmath191 and @xmath192 is defined in lemma [ lemmamajkmeans ] .",
    "eventually , we need an additional assumption on the regularity of the density @xmath117 to control the bias term in theorem [ thmkmeans ] .",
    "the regularity is expressed in terms of anisotropic nikolskii class .",
    "[ defnikolskiianisot ] let @xmath193 , @xmath194 and @xmath195 be fixed .",
    "we say that @xmath196^d\\rightarrow[-l , l ] $ ] belongs to the anisotropic nikolskii class @xmath197 if for all @xmath198 , @xmath199 and for all @xmath200^d $ ] , @xmath201 and @xmath202 , for any @xmath203 , where @xmath204 is the largest integer strictly less than @xmath205 .",
    "nikolskii classes were introduced in approximation theory by nikolskii ; see @xcite , for example .",
    "we also refer to @xcite where the problem of adaptive estimation has been treated for the gaussian white noise model and for density estimation , respectively .    in the sequel",
    ", we assume that the multivariate density @xmath117 belongs to the anisotropic nikolskii class @xmath206 , for some @xmath207 and some @xmath208 .",
    "in other words , the density has possible different regularities in all directions .",
    "the statement of a nonadaptive upper bound for the excess risk in the anisotropic case has been already investigated in @xcite . in the following theorem , we propose the adaptive version of the previous cited result , where the bandwidth @xmath95 is chosen via the selection rule ( [ defrulekmeans ] ) .",
    "[ thmkmeans ] assume @xmath209 and @xmath174 hold for some @xmath175 and some @xmath176 .",
    "assume the hessian matrix of @xmath210 is positive definite for any @xmath211 .",
    "then , for any @xmath212^d $ ] , any @xmath208 , we have @xmath213 < \\infty,\\end{aligned}\\ ] ] where @xmath95 is driven in ( [ defrulekmeans ] ) .",
    "this theorem is a direct application of theorem [ thmainresult ] , lemma [ dclustering ] and the majorant construction .",
    "it gives adaptive fast rates of convergence for the excess risk of  @xmath214 and significantly improves the result stated in @xcite for two reasons : first , the selection rule allows the extension to the anisotropic case ; besides , there is no logarithmic term in the adaptive rate . in our opinion ,",
    "the localization technique used in @xcite seems to be the major obstacle to avoid the extra @xmath215 term .",
    "in this section , we apply the gradient inequality to the framework of local @xmath3-estimation in nonparametric robust regression",
    ". it will give adaptive minimax results for nonlinear estimators for both pointwise and global estimation .",
    "let us specify the model beforehand .",
    "for some @xmath96 , we observe a training set @xmath216 of i.i.d .",
    "pairs , distributed according to the probability measure @xmath217 on @xmath119^d\\times{{\\mathbb r}}$ ] satisfying the set of equations @xmath218 we aim at estimating the target function @xmath219^d\\rightarrow[-b , b]$ ] , @xmath220 .",
    "the noise variables @xmath221 are assumed to be i.i.d . with symmetric density @xmath222 w.r.t .",
    "the lebesgue measure .",
    "we also assume @xmath222 is continuous at @xmath223 and @xmath224 . for simplicity ,",
    "the design points @xmath225 are assumed to be i.i.d . according to the uniform law on @xmath119^d$ ]",
    "( extension to a more general design is straightforward ) , and we assume that @xmath225 and @xmath226 are mutually independent for ease of exposition . eventually , we restrict the estimation of @xmath227 to the closed set @xmath228^d $ ] to avoid discussion on boundary effects .",
    "we will consider a point @xmath229 for pointwise estimation and the @xmath230-risk for global estimation , for @xmath231 .",
    "next , we introduce an estimate of @xmath232 at any @xmath233 with the local constant approach ( lca ) . the key idea of lca , as described , for example , in @xcite , chapter  1 , is to approximate the target function by a constant in a neighborhood of size @xmath234 of a given point @xmath235 , which corresponds to a model of dimension @xmath236 . to deal with heavy - tailed noises ,",
    "we especially employ the huber loss ( see @xcite ) defined as follows . for any scale @xmath237 and @xmath199 ,",
    "@xmath238 the parameter @xmath239 selects the level of robustness of the huber loss between the square loss ( large value of @xmath239 ) and the absolute loss ( small value of @xmath240 ) .",
    "let @xmath170^d $ ] be the bandwidth set such that @xmath171 , @xmath241 for any @xmath233 , the local estimator @xmath242 of @xmath243 is defined as @xmath244 } \\widehat{r}^{\\mathrm{loc}}_{h}(t),\\qquad   h\\in{{\\mathcal h}},\\ ] ] where @xmath245 is the local empirical risk , and @xmath59 is a @xmath246-lipschitz kernel of order @xmath246 .",
    "we notice that the local empirical risk estimates the local risk @xmath247 whose @xmath243 is its unique minimizer .    in nonparametric estimation ,",
    "one is usually interested in pointwise or global risk instead of excess risk .",
    "since theorem [ thmainresult ] controls the @xmath106-excess risk of the adaptive estimator , we present the following lemma that links the pointwise risk with the @xmath16-excess risk .    [ lemlocalmargincondition ] assume that @xmath248 holds .",
    "then , for all @xmath249 , @xmath250 where @xmath251 ( and , resp . , @xmath252 ) denotes the derivative of @xmath253 ( resp . , the second derivative of @xmath254 ) .",
    "the proof is given in the .",
    "we can also deduce the same inequality with the @xmath255-norm .",
    "the assumption @xmath256 is necessary to use the theory of differential calculus and can be satisfied by using the consistency of @xmath257 . in this respect ,",
    "the definitions of @xmath258 and @xmath259 above imply the consistency of all estimators @xmath260 ; for further details , see below as well as  @xcite , theorem 1 .",
    "we now present the application of the selection rule for pointwise estimation . to compute the procedure",
    ", we define the @xmath106-empirical risk as @xmath261 for two bandwidths @xmath262 , we introduce the auxiliary @xmath106-empirical risk as @xmath263 where @xmath264 , as before .",
    "to apply the results of section  [ gradientinequality ] , we need to compute optimal majorants of the associated empirical processes .",
    "the construction of such bounds for the pointwise case has already received attention in the literature ; see @xcite , proposition  2 . for any integer @xmath97 ,",
    "let us introduce the function @xmath265 defined as @xmath266 ^ 2 } \\biggl(\\sqrt { \\frac{l \\log(n)}{n\\prod_{j=1}^dh_j\\vee\\eta_j}}+\\sqrt{\\frac{l \\log(n)}{n\\prod_{j=1}^d\\eta_j } } \\biggr),\\end{aligned}\\ ] ] where @xmath267 is an absolute constant which does not depend on the model .",
    "then if we set @xmath178 , @xmath177 , an exponential net of @xmath179^d $ ] , such that @xmath180 , for any @xmath268 , the function @xmath269 is a majorant according to definition [ defmajorant ] .",
    "eventually , we introduce the data - driven bandwidth following the schema of the selection rule in section  [ gradientinequality ] , @xmath270 where @xmath271 .",
    "to derive minimax adaptive rates for local estimation , we start with the definition of the anisotropic hlder class .    [ defholderanisot ] let @xmath272 and @xmath273 be fixed .",
    "we say that @xmath196^d\\rightarrow[-l , l ] $ ] belongs to the anisotropic hlder class @xmath274 of functions if for all @xmath198 and for all @xmath200^d $ ] , @xmath275 and @xmath276^d}\\biggl{\\vert}\\frac{\\partial^{l}}{\\partial x_j^{l}}f(x)\\biggr{\\vert}\\leq l\\qquad \\forall l=0 , \\ldots , \\lfloor s_j\\rfloor , \\ ] ] where @xmath204 is the largest integer strictly less than @xmath205 .",
    "[ thholderadapation ] for any @xmath277^d $ ] , any @xmath273 and any @xmath278 , it holds for all @xmath279 , @xmath280 where @xmath281 denotes the harmonic average .",
    "the proposed estimator @xmath282 is then adaptive minimax over anisotropic hlder classes in pointwise estimation .",
    "the minimax optimality of this rate [ with the @xmath283 factor ] has been stated by @xcite in the white noise model for pointwise estimation ; see also @xcite . for simplicity",
    ", we did not study the case of locally polynomial functions [ i.e. , @xmath284 .",
    "chichignoud and lederer @xcite , theorem  2 , have shown that the variance of local @xmath3-estimators is of order @xmath285 ^ 2/n({\\mathbb e}\\rho_\\gamma''(\\xi_1))^2 $ ] , and therefore their lepski - type procedure depends on this quantity . thanks to the gradient approach , we obtain the same result without the dependency on the parameter @xmath286 , which corresponds to @xmath287 in the general setting .",
    "the selection rule is therefore robust w.r.t . to the fluctuations of this parameter , in particular when @xmath239 is small ( median estimator ) .",
    "the aim of this section is to derive adaptive minimax results for @xmath288 for the @xmath289-risk . to this end",
    ", we need to modify the selection rule ( [ defruleloc ] ) including a global ( @xmath289-norm ) comparison of @xmath106-empirical risks . for this purpose , for all @xmath290",
    ", we denote the @xmath16-empirical risks at a given point @xmath279 as @xmath291 and @xmath292 where the dependence in @xmath235 is explicitly written",
    ". then we define , for @xmath293 and for any function @xmath294 , the @xmath295-norm and @xmath296-semi - norm @xmath297}\\bigl{\\vert}\\omega(t,\\cdot)\\bigr { \\vert}_q . \\ ] ]    the construction of majorants is based on uniform bounds for @xmath289-norms of empirical processes .",
    "recently , goldenshluger and lepski investigated this topic @xcite , theorem 2 . for any integer @xmath97 ,",
    "let us introduce the function @xmath298 defined as @xmath299 where @xmath300 is an absolute constant which does not depend on @xmath185 .",
    "then , for any @xmath268 , the function @xmath301 is a majorant according to definition [ defmajorant ] .",
    "we finally select the bandwidth according to @xmath302    the above choice of the bandwidth leads to the estimator @xmath303 with the following adaptive minimax properties for the @xmath295-risk over anisotropic nikolskii classes ; see definition [ defnikolskiianisot ] .",
    "[ thnikolskiiadapation ] for any @xmath304^d $ ] , any @xmath273 and any @xmath278 , it holds that @xmath305 where @xmath281 denotes the harmonic average and @xmath306    we refer to @xcite for the minimax optimality of these rates over nikolskii classes .",
    "the proposed estimate @xmath307 is then adaptive minimax . to the best of our knowledge ,",
    "the minimax adaptivity over anisotropic nikolskii classes has never been studied in regression with possible heavy - tailed noises .",
    "we finally refer to the remarks after theorem [ thholderadapation ] .",
    "our paper solves the general bandwidth selection issue in kernel erm by using a novel selection rule , based on the minimization of an estimate of the bias  variance decomposition of the gradient excess risk .",
    "this new criterion simultaneously upper bounds the estimation error ( @xmath308-norm ) and the prediction error ( excess risk ) with optimal rates .",
    "one of the key messages we would like to highlight is the following : if we consider smooth loss functions and a family of consistent erm , fast rates of convergence are automatically reached , provided that the hessian matrix of the risk function is positive definite .",
    "this statement is based on the key lemma [ lemmadmargin ] in section  [ sgradient ] , where the square root of the excess risk is controlled by the @xmath16-excess risk .    from an adaptive point of view",
    ", one can take another look at lemma [ lemmadmargin ] . on the rhs of lemma [ lemmadmargin ]",
    ", the @xmath16-excess risk is multiplied by the constant @xmath309 , that is , the smallest eigenvalue of the hessian matrix at @xmath9 .",
    "this parameter is also involved in the margin assumption . as a result ,",
    "our selection rule does not depend on this parameter since the margin assumption is not required to obtain slow rates for the @xmath16-excess risk .",
    "this fact partially solves an issue highlighted by massart @xcite , section  8.5.2 , in the model selection framework :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ it is indeed a really hard work in this context to design margin adaptive penalties .",
    "of course recent works on the topic , involving local rademacher penalties , for instance , provide at least some theoretical solution to the problem but still if one carefully looks at the penalties which are proposed in these works , they systematically involve constants which are typically unknown . in some cases , these constants are absolute constants which should nevertheless considered as unknown just because the numerical values coming from the theory are obviously over pessimistic . in some other cases , it is even worse since they also depend on nuisance parameters related to the unknown distribution .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    in section  [ ssimu ] below , we also illustrate the robustness of the method with numerical results",
    ". an interesting and challenging open problem would be to employ the gradient approach in the model selection framework in order to propose a more robust penalization technique ( i.e. , which does not depend on the parameter @xmath310 ) .",
    "the gradient approach requires two main ingredients : the first one concerns the smoothness of the loss function in terms of differentiability ; the second one affects the dimension of the statistical model that we have at hand , which has to be parametric , that is , of finite dimension @xmath311 . from our point of view , the smoothness of the loss function is not a restriction , since modern algorithms are usually based  in order to reduce computational complexity  on some kind of gradient descent methods in practice . on the other hand , the second ingredient might be more restrictive from the model selection point of view .",
    "an interesting open problem would be to employ the same path when the dimension @xmath1 is possibly larger than @xmath185 , that is , in a high - dimensional setting .",
    "for completeness , we illustrate the performance of our selection rule in the context of clustering with errors in variables , and compare it to the most recent bandwidth selection procedure in that framework : erc method , recently evolved in @xcite .",
    "this method has both theoretical and computational advantages ( see also @xcite ) ; however , it only provides isotropic bandwidth selection . for this reason ,",
    "our anisotropic selection rule may outperform erc method .",
    "the computation of the selection rule ( [ defrulekmeans ] ) requires many optimization steps .",
    "we first compute a family of codebooks @xmath312 according to ( [ noisykmeans ] ) , by using a noisy version of the vanilla @xmath37-means algorithm .",
    "this technique gives an approximation of the optimal solution ( [ noisykmeans ] ) thanks to an iterative procedure based on newton optimization .",
    "more theoretical foundations are detailed in @xcite .",
    "second , we use parallel execution in order to compute the comparison of gradient empirical risks .",
    "we generate an i.i.d .",
    "noisy sample @xmath313 such that for any @xmath314 , @xmath315 where @xmath316 [ resp .",
    ", @xmath317 are i.i.d .",
    "gaussian with density @xmath318 ( resp . , @xmath319 ) and",
    "@xmath320 are i.i.d .",
    "such that @xmath321 . here ,",
    "@xmath322 are i.i.d . with gaussian noise with zero mean @xmath323 and covariance matrix @xmath324 for @xmath325 .",
    "in this setting , we compare both adaptive procedures [ our selection rule ( [ noisykmeans ] ) and erc method ] to the standard @xmath37-means with lloyd s algorithm by computing the empirical clustering error according to @xmath326 where @xmath327 and @xmath328 , @xmath70 correspond to the latent class labels defined in ( [ modexp ] ) .    similar to many adaptive methods ,",
    "lepki - type procedures suffer from a dependency on a tuning parameter .",
    "in particular , in erc method , a constant governs the variance threshold ( see @xcite or @xcite ) , and in our selection rule as well , a constant @xmath329 appears in the majorant function of lemma [ lemmamajkmeans ] . as discussed earlier",
    ", the choice of this constant remains an hard issue for application . in the sequel",
    ", we illustrate the behavior of both adaptive methods w.r.t .",
    "3 constants : @xmath330 , @xmath246 and @xmath331 .",
    "figure  [ fig1](a)(b ) illustrates the evolution of the clustering risk ( [ cerror ] ) when @xmath332 in model ( [ modexp ] ) for @xmath37-means ( red curve ) versus both adaptive procedures .    [ cols=\"^,^ \" , ]     [ fig1 ]    in figure  [ fig1](a ) , we compare the clustering risk ( [ cerror ] ) of @xmath37-means ( red curve ) with erc with 3 different constants ( erc1 , erc2 and erc3 ) .",
    "the methods are comparable , and we observe that erc performance is sensitive to the choice of the constant .",
    "nevertheless , a good calibration of this constant gives slightly better results than @xmath37-means . in figure  [ fig1](b ) , the gradient approach with three different constants ( g1 , g2 and g3 ) gives a clustering risk less than 5@xmath333 for any @xmath332 . in comparison ,",
    "standard @xmath37-means completely fails when @xmath148 is increasing . as a conclusion",
    ", our selection rule significantly outperforms @xmath37-means and erc for any constant .",
    "this highlights the importance in practice to choose two different bandwidths in each direction in this model , that is , an anisotropic bandwidth .",
    "our selection rule is also robust to the choice of the constant , which confirms the theoretical study .",
    "the proof is based on standard tools from differential calculus applied to the multivariate risk function @xmath334 , where @xmath18 is an open ball centered at @xmath9 . the first step is to apply a taylor expansion of first order which gives , for all @xmath335 , @xmath336 where @xmath337 , @xmath338 and @xmath339 .",
    "now , by the property @xmath340 and the boundedness of the second partial derivatives , we can write @xmath341 it then remains to show the inequality @xmath342 where @xmath26 is defined in the lemma .",
    "this can be done by using standard inverse function theorem and the mean value theorem for multi - dimensional functions .",
    "indeed , since the hessian matrix of @xmath343also viewed as the jacobian matrix of @xmath106is positive definite at @xmath9 , and since @xmath344 , the inverse function theorem shows the existence of a bijective function @xmath345 such that @xmath346 we can then apply a vector - valued version of the mean value theorem to obtain @xmath347}{\\big|\\!\\big|\\!\\big|}j_{{g}^{-1}}(u){\\big|\\!\\big|\\!\\big|}_2 \\bigl{\\vert}{g}\\bigl({\\theta}^\\star\\bigr)-{g}({\\theta})\\bigr{\\vert}_2 \\nonumber\\\\[-8pt]\\\\[-8pt ] \\eqntext{\\mbox{for any } { \\theta}\\in u,}\\end{aligned}\\ ] ] where @xmath348 $ ] denotes the multi - dimensional bracket between @xmath349 and @xmath350 , and @xmath351 denotes the operator norm associated to the euclidean norm @xmath89 .",
    "since @xmath352 and @xmath10 is continuous , we now have @xmath353}{\\big|\\!\\big|\\!\\big|}j_{{g}^{-1}}(u){\\big|\\!\\big|\\!\\big|}_2= { \\big|\\!\\big|\\!\\big|}j_{{g}^{-1 } } \\bigl(g \\bigl({\\theta}^\\star\\bigr)\\bigr){\\big|\\!\\big|\\!\\big|}_2 . \\ ] ] then , for @xmath20 small enough , we have with ( [ mvt2 ] ) @xmath354 where @xmath355 is the hessian matrix of @xmath356 .",
    "( [ secondstep ] ) follows easily , and the proof is complete .",
    "proof of lemma [ dclustering ] the hessian matrix of @xmath357 involves integrals over faces of the vorono diagram @xmath358 . for @xmath359 ,",
    "let us denote the face ( possibly empty ) common to @xmath360 and @xmath150 as @xmath361 .",
    "moreover , denote @xmath362 the @xmath363-dimensional lebesgue measure .",
    "then , since @xmath117 is continuous and @xmath364^d$ ] , uniform continuity arguments ensure that the integral @xmath365 exists and depends continuously on the location of the center @xmath366 , for any @xmath367 and for any @xmath368 .",
    "then we can use the following lemma due to @xcite .",
    "[ pollard ] suppose @xmath369 and @xmath40 has a continuous density @xmath117 w.r.t .",
    "lebesgue measure .",
    "assume integral @xmath365 exists and depends continuously on the location of the centers , for any @xmath367 and for any @xmath368 .",
    "then if centers @xmath370 , @xmath371 are all distinct , @xmath372 has a hessian matrix @xmath373 made up of @xmath374 blocks , @xmath375 where @xmath376 and @xmath377 .",
    "proof of lemma [ lemmamajkmeans ] we start with the study of @xmath380 . for ease of exposition , we denote by @xmath381 the empirical measure with respect to @xmath382 , @xmath70 and by @xmath383 the expectation w.r.t . the law of @xmath39",
    "then we have @xmath384^{dk}}\\bigl{\\vert}{\\nabla\\widehat{\\mathcal{w}}}_h({\\mathbf{c}})-{\\mathbb e}{\\nabla\\widehat{\\mathcal{w}}}_h({\\mathbf{c}})\\bigr{\\vert}_2 \\\\ & & \\qquad \\leq \\sqrt{kd}\\sup_{{\\mathbf{c}},i , j}\\biggl{\\vert}\\bigl(p_n^z - p^z \\bigr ) \\biggl(\\int_{v_j}2\\bigl(x^i - c_j^i \\bigr)\\widetilde k_h(z - x)\\,dx \\biggr)\\biggr{\\vert}.\\nonumber\\end{aligned}\\ ] ] the cornerstone of the proof is to apply a concentration inequality to this supremum of empirical process .",
    "we use in the sequel the following talagrand - type inequality ; see , for example , @xcite .",
    "[ lemmatal ] let @xmath385 be i.i.d .",
    "random variables , and let @xmath386 be a countable subset of @xmath387 . consider the random variable @xmath388 where @xmath389 is such that @xmath390 , @xmath391 and @xmath392\\leq v$ ] .",
    "then , for any @xmath20 , we have @xmath393    the proof of lemma [ lemmatal ] is omitted ; see @xcite .",
    "we hence have to compile the quantities @xmath394 and @xmath3 associated with the random variable @xmath395 the compilation of @xmath396 uses the same path as @xcite , lemma  3 .",
    "more precisely , we can apply a chaining argument to the function @xmath397 , for any @xmath398 .",
    "then we have , together with a maximum inequality due to @xcite , chapter  6 , @xmath399 where @xmath400 for @xmath401 provided that @xmath402 ( thanks to the definition of @xmath403 and @xmath185 sufficiently large ) .",
    "the constant @xmath404 can be explicitly computed .",
    "this calculation is omitted for simplicity . besides , using @xcite , lemma 1 , with @xmath405 , we have @xmath406\\leq\\frac { b_6}{\\pi_h(2 \\beta)}:=v(h),\\ ] ] whereas @xcite , lemma 2 , allows us to write @xmath407 where @xmath408 are absolute constants .",
    "hence , lemma [ lemmatal ] , together with ( [ eqlemma1])([mbound ] ) , gives us , for all @xmath409 , @xmath410 moreover , note that from the previous calculations , we have @xmath411 and @xmath412 , where @xmath413 depend on @xmath414 and @xmath415 , respectively . provided that @xmath416 ( thanks to the definition of @xmath417 and @xmath185 sufficiently large ) , we come up with @xmath418 this gives us the first part of the majorant of lemma [ lemmamajkmeans ] .",
    "the last step is to show a similar bound for the auxiliary empirical process @xmath419 .",
    "this can be easily done by using lemma [ lemmatal ] together with the previous results .",
    "then we have for any @xmath420 , @xmath421 where with a slight abuse of notation , the maximum @xmath422 is understood coordinatewise . using the union bound ,",
    "the definition of @xmath423 allows us to write @xmath424 where we choose @xmath425 with @xmath426 .",
    "proof of theorem [ thmkmeans ] the proof of theorem [ thmkmeans ] is a direct application of theorem [ thmainresult ] and lemma [ lemmamajkmeans ] .",
    "indeed , for any @xmath97 , for @xmath185 large enough , we have with probability @xmath98 , @xmath427 where @xmath428 is defined as @xmath429 the control of the bias function is as follows : @xmath430^{dk}}\\sum_{i , j } \\biggl\\ { \\int _ { v_j}2\\bigl(x^i - c_j^i \\bigr ) \\bigl({\\mathbb e}_{p^z } \\widetilde k_{h,\\eta}(z - x)- { \\mathbb e}_{p^z}\\widetilde k_\\eta(z - x ) \\bigr)\\,dx \\biggr \\}^2 \\\\ & & \\qquad = \\sup_{{\\mathbf{c}}\\in[0,1]^{dk}}\\sum _ { i , j } \\biggl\\{\\int_{v_j}2 \\bigl(x^i - c_j^i\\bigr ) \\bigl ( { \\mathbb e}_{p^x } k_{h,\\eta}(x - x)-{\\mathbb e}_{p^x } k_\\eta(x - x ) \\bigr)\\,dx \\biggr\\}^2 \\\\ & & \\qquad \\leq 4\\sup_{{\\mathbf{c}}\\in[0,1]^{dk}}\\sum_{i , j}\\int _ { v_j}\\bigl(x^i - c_j^i \\bigr)^2\\,dx\\bigl{\\vert}k_{\\eta}*(k_h*f - f)\\bigr { \\vert}_2 ^ 2 \\\\ & & \\qquad \\leq 4k\\bigl{\\vert}\\mathcal{f}[k]\\bigr{\\vert}_{\\infty}{\\vert}f_h - f{\\vert}_2 ^ 2,\\end{aligned}\\ ] ] where @xmath431 is the usual nonparametric bias term in deconvolution estimation . besides",
    ", note that @xmath432^{dk}}\\sum_{i , j } \\biggl\\{\\int _ { v_j}2\\bigl(x^i - c_j^i \\bigr ) \\bigl({\\mathbb e}_{p^x } k_{h}(x - x)-f(x ) \\bigr)\\,dx \\biggr \\}^2 \\\\ & & \\qquad \\leq 4\\sup_{{\\mathbf{c}}\\in[0,1]^{dk}}\\sum_{i , j}\\int _ { v_j}\\bigl(x^i - c_j^i \\bigr)^2\\,dx{\\vert}k_{h}*f - f{\\vert}_2 ^ 2.\\end{aligned}\\ ] ] then we need a control of the bias function , @xmath433\\bigr{\\vert}_{\\infty } \\bigr){\\vert}k_h*f - f{\\vert}_2\\qquad \\forall h\\in{{\\mathcal h}}.\\ ] ] by using comte and lacour @xcite , proposition  3 , we directly have for all @xmath434 , @xmath435 \\bigr{\\vert}_{\\infty } \\bigr)l\\sum_{j=1}^dh_j^{s_j}\\qquad \\forall h\\in{{\\mathcal h}}.\\ ] ] now , we have to use a result such as lemma [ dclustering ] , for our family of estimators @xmath436 .",
    "in other words , we need to check that this family of estimators is consistent with respect to the euclidean norm in @xmath437 .",
    "[ consistencykmeans ] assume @xmath117 is continuous , @xmath364^d$ ] a.s .",
    "and the hessian matrix of @xmath210 is positive definite on @xmath438 . consider the family @xmath439 with @xmath192 defined in lemma [ lemmamajkmeans ]",
    "then , for any @xmath20 , for any @xmath440 , for any @xmath441 , there exists @xmath442 such that for @xmath185 great enough , with probability @xmath98 , @xmath443    using @xcite , the positive definiteness of the hessian matrix on @xmath444 and the continuity of @xmath117 , we have , for any @xmath441 , for some constant @xmath445 , @xmath446 , where @xmath447 .",
    "it remains to show that by definition of @xmath192 in lemma [ lemmamajkmeans ] , with high probability , @xmath448 as @xmath185 tends to infinity .",
    "this can be seen easily from chichignoud and loustau @xcite , which gives the order of the bias term and the variance term for such a problem . at this stage",
    ", we can notice that localization is used in @xcite , and appears to be necessary here .",
    "however , using a global approach ( i.e. , a simple hoeffding inequality to the family of kernel erm ) , we can have , for any @xmath449 , with probability @xmath98 , @xmath450 by definition of @xmath192 , the rhs tends to zero as @xmath451 , and then for @xmath185 great enough , this term is controlled by @xmath139 .",
    "then , for any @xmath452 and @xmath185 great enough , lemma [ dclustering ] allows us to write with probability @xmath98 , @xmath453 using theorem [ thmainresult ] with @xmath454 , bias control ( [ bcontrolkmeans ] ) and the last inequality , there exists an absolute constant @xmath455 such that @xmath456\\leq b_8 \\inf_{h\\in{{\\mathcal h}}_a } \\biggl\\{\\sum_{j=1}^dh_j^ { s_j}+ \\frac{\\pi_h(-\\beta ) } { n } \\biggr\\ } ^2+b_8n^{-q}.\\ ] ] let @xmath457 denote the oracle bandwidth as @xmath458 , and define the oracle bandwidth @xmath459 on the net @xmath192 such that @xmath460 , for all @xmath461 .",
    "eventually , we have @xmath456\\leq b_8 a^{-qd/2}\\inf _ { h\\in{{\\mathcal h } } } \\biggl\\{\\sum_{j=1}^dh_j^ { s_j}+ \\frac{\\pi _ h(-\\beta)}{n } \\biggr\\}^2+b_8n^{-q}. \\ ] ] by a standard bias variance trade - off , we obtain the assertion of the theorem , provided that @xmath462",
    ".      proof of lemma [ lemlocalmargincondition ] by definition , we first note that @xmath463 using the mean value theorem and the assumption @xmath464 , there exists @xmath465 $ ] such that @xmath466 since @xmath467 is a 2-lipschitz function , it yields @xmath468 the proof is complete .",
    "proof of theorem [ thholderadapation ] from @xcite , theorem 1 , we notice that all estimators @xmath469 are consistent , and thus , for @xmath185 sufficiently large , the assumption of lemma [ lemlocalmargincondition ] holds for all @xmath279 . using theorem [ thmainresult ] with @xmath268 and lemma [ lemlocalmargincondition ] , we get @xmath470 with @xmath471 .",
    "the control of @xmath472 over hlder classes is based on the same schema as in @xcite , applied to the function @xmath473 . for any @xmath474 and any @xmath249",
    ", we then want to show @xmath475}\\sup _ { y\\in{{\\mathcal t}}}\\biggl{\\vert}\\int k_{h}(x - y ) \\bigl[f_t(x)-f_t(y ) \\bigr]\\,dx\\biggr{\\vert}\\nonumber\\\\[-8pt]\\\\[-8pt]\\nonumber & \\leq & l { \\vert}k{\\vert}_\\infty\\sum_{j=1}^dh_j^ { s_j}.\\end{aligned}\\ ] ] by definition , we see that @xmath476}{\\vert}{\\mathbb e}k_{h}(w - x_0 ) [ f_t(w)-f_t(x_0 ) ] { \\vert}$ ] and by definition of @xmath477 and @xmath478 , we have @xmath479 using fubini s theorem and the equation @xmath480 for all @xmath481 , we get @xmath482\\,dx \\biggr)\\,dy \\\\ & = & \\int k_{\\eta}(y - x_0 ) f_t(y)\\,dy \\\\ & & { } + \\int k_{\\eta}(y - x_0 ) \\int k_{h}(x - y ) \\bigl[f_t(x)-f_t(y ) \\bigr]\\,dx\\,dy.\\end{aligned}\\ ] ] then it holds for any @xmath279 , @xmath483\\,dx\\,dy\\biggr{\\vert}\\\\ & & \\qquad \\leq \\bigl{\\vert}k_{\\eta}(\\cdot - x_0)\\bigr{\\vert}_1\\sup_{y\\in{{\\mathcal t}}}\\biggl{\\vert}\\int k_{h}(x - y ) \\bigl[f_t(x)-f_t(y )",
    "\\bigr]\\,dx \\biggr{\\vert}\\\\ & & \\qquad = \\sup_{y\\in{{\\mathcal t}}}\\biggl{\\vert}\\int k_{h}(x - y ) \\bigl[f_t(x)-f_t(y ) \\bigr]\\,dx\\biggr{\\vert}.\\end{aligned}\\ ] ] we have then shown the first inequality in ( [ eqlocalbiascontrol ] ) .",
    "using the smoothness of @xmath484 , we have for all @xmath485 , @xmath486\\,dx\\biggr{\\vert}\\\\ & & \\qquad = \\biggl{\\vert}\\int k_{h}(x - y){\\mathbb e}\\bigl [ \\rho_\\gamma ' \\bigl(f(x)-t+\\xi_1 \\bigr)- \\rho_\\gamma ' \\bigl(f(y)-t+\\xi _ 1 \\bigr ) \\bigr ] \\,dx\\biggr{\\vert}\\\\ & & \\qquad \\leq\\biggl{\\vert}\\int k_h(x - y ) \\bigl(f(x)-f(y)\\bigr)\\,dx\\biggr { \\vert}\\\\ & & \\qquad \\leq l{\\vert}k{\\vert}_\\infty\\sum_{j=1}^dh_j^ { s_j}.\\end{aligned}\\ ] ] therefore , ( [ eqlocalbiascontrol ] ) holds .",
    "then , using theorem [ thmainresult ] with @xmath454 , lemma [ lemlocalmargincondition ] and ( [ eqlocalbiascontrol ] ) , there exists an absolute constant @xmath487 such that @xmath488 let @xmath457 denote the oracle bandwidth as @xmath489 , and define the oracle bandwidth @xmath459 such that @xmath460 , for all @xmath461 .",
    "then we get @xmath490 by a standard bias variance trade - off , we obtain the assertion of the theorem .",
    "proof of theorem [ thnikolskiiadapation ] here again , the assumption of lemma [ lemlocalmargincondition ] holds for @xmath185 sufficiently large for all @xmath279 . using theorem [ thmainresult ] with @xmath268 and adding the @xmath295-norm",
    ", we have @xmath491 where @xmath492 .",
    "the control of the bias term is based on the schema of @xcite for linear estimates . for any @xmath249 ,",
    "we want to show that @xmath493}\\biggl{\\vert}\\int k_{h}(x-\\cdot ) \\bigl[f_t(x)-f_t(\\cdot ) \\bigr]\\,dx\\biggr{\\vert}_q \\leq l\\sum_{j=1}^dh_j^ { s_j},\\ ] ] where we recall @xmath494 . by definition",
    ", one has @xmath495}\\bigl{\\vert}{\\mathbb e}k_{h}(w-\\cdot ) \\bigl[f_t(w)-f_t(\\cdot ) \\bigr]\\bigr{\\vert}_q . \\",
    "] ] moreover , in the proof of theorem [ thholderadapation ] , we have shown that for any @xmath279 , @xmath496\\,dx\\,dy .",
    "\\ ] ] by young s inequality and the definition of the kernel in section  [ sectionkerm ] , it yields @xmath497}\\biggl{\\vert}\\int k_{\\eta}(y-\\cdot ) \\int k_{h}(x - y ) \\bigl[f_t(x)-f_t(y ) \\bigr]\\,dx\\,dy \\biggr{\\vert}_{q,\\infty } \\\\ & & \\qquad \\leq\\sup_{t\\in[-b , b]}\\biggl{\\vert}\\int k_{h}(x-\\cdot ) \\bigl{\\vert}f_t(x)-f_t(\\cdot)\\bigr{\\vert}\\ , dx\\biggr { \\vert}_{q,\\infty}.\\end{aligned}\\ ] ] using the smoothness of @xmath484 , we have for any @xmath498 and any @xmath499 } $ ] , @xmath500 \\leq\\bigl{\\vert}f(x)-f(y)\\bigr { \\vert}.\\end{aligned}\\ ] ] therefore , ( [ eqglobalbiascontrol ] ) holds for all @xmath501 .",
    "then , using theorem [ thmainresult ] with @xmath454 , lemma [ lemlocalmargincondition ] and ( [ eqglobalbiascontrol ] ) , there exists an absolute constant @xmath502 such that @xmath503 computing these infimums , we obtain the assertion of the theorem ."
  ],
  "abstract_text": [
    "<S> in this paper , we deal with the data - driven selection of multidimensional and possibly anisotropic bandwidths in the general framework of kernel empirical risk minimization . we propose a universal selection rule , which leads to optimal adaptive results in a large variety of statistical models such as nonparametric robust regression and statistical learning with errors in variables . </S>",
    "<S> these results are stated in the context of smooth loss functions , where the gradient of the risk appears as a good criterion to measure the performance of our estimators . </S>",
    "<S> the selection rule consists of a comparison of gradient empirical risks . </S>",
    "<S> it can be viewed as a nontrivial improvement of the goldenshluger  </S>",
    "<S> lepski method to nonlinear estimators . </S>",
    "<S> furthermore , one main advantage of our selection rule is the nondependency on the hessian matrix of the risk , usually involved in standard adaptive procedures .    ./style / arxiv - general.cfg </S>"
  ]
}