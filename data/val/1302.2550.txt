{
  "article_text": [
    "real world problems usually demand continuous state or action spaces , and one of the challenges for reinforcement learning is to deal with such continuous domains . in many problems",
    "there is a natural metric on the state space such that close states exhibit similar behavior .",
    "often such similarities can be formalized as lipschitz or more generally hlder continuity of reward and transition functions .",
    "the simplest continuous reinforcement learning problem is the 1-dimensional continuum - armed bandit , where the learner has to choose arms from a bounded interval .",
    "bounds on the regret with respect to an optimal policy under the assumption that the reward function is hlder continuous have been given in @xcite .",
    "the proposed algorithms apply the ucb algorithm  @xcite to a discretization of the problem .",
    "that way , the regret suffered by the algorithm consists of the loss by aggregation ( which can be bounded using hlder continuity ) plus the regret the algorithm incurs in the discretized setting .",
    "more recently , algorithms that adapt the used discretization ( making it finer in more promising regions ) have been proposed and analyzed  @xcite .    while the continuous bandit case has been investigated in detail , in the general case of continuous state markov decision processes ( mdps ) a lot of work",
    "is confined to rather particular settings , primarily with respect to the considered transition model . in the simplest case ,",
    "the transition function is considered to be deterministic as in  @xcite , and mistake bounds for the respective discounted setting have been derived in  @xcite .",
    "another common assumption is that transition functions are linear functions of state and action plus some noise . for such settings sample complexity bounds",
    "have been given in  @xcite , while @xmath0 bounds for the regret after @xmath1 steps are shown in  @xcite .",
    "however , there is also some research considering more general transition dynamics under the assumption that close states behave similarly , as will be considered here . while most of this work is purely experimental @xcite , there are also some contributions with theoretical guarantees .",
    "thus , @xcite considers pac - learning for continuous reinforcement learning in metric state spaces , when generative sampling is possible .",
    "the proposed algorithm is a generalization of the e@xmath2 algorithm  @xcite to continuous domains .",
    "a respective adaptive discretization approach is suggested in  @xcite .",
    "the pac - like bounds derived there however depend on the ( random ) behavior of the proposed algorithm .    here",
    "we suggest a learning algorithm for undiscounted reinforcement learning in continuous state space .",
    "the proposed algorithm is in the tradition of algorithms like ucrl2  @xcite in that it implements the `` optimism in the face of uncertainty '' maxim , here combined with state aggregation .",
    "thus , the algorithm does not need a generative model or access to `` resets : '' learning is done online , that is , in a single continual session of interactions between the environment and the learning policy .",
    "for our algorithm we derive regret bounds of @xmath3 for mdps with @xmath4-dimensional state space and hlder - continuous rewards and transition probabilities with parameter @xmath5 .",
    "these bounds also straightforwardly generalize to dimension @xmath6 where the regret is bounded by @xmath7 .",
    "thus , in particular , if rewards and transition probabilities are lipschitz , the regret is bounded by @xmath8 in dimension @xmath6 and @xmath9 in dimension 1 .",
    "we also present an accompanying lower bound of @xmath10 . as far as we know , these are the first regret bounds for a general undiscounted continuous reinforcement learning setting .",
    "we consider the following setting . given is a markov decision process ( mdp ) @xmath11 with state space @xmath12^d$ ] and finite action space @xmath13 . for the sake of simplicity , in the following we assume @xmath14 .",
    "however , proofs and results generalize straightforwardly to arbitrary dimension , cf .",
    "remark [ rem : d ] below .",
    "the random rewards in state @xmath15 under action @xmath16 are assumed to be bounded in @xmath17 $ ] with mean @xmath18 .",
    "the transition probability distribution in state @xmath15 under action @xmath16 is denoted by @xmath19 .",
    "we will make the natural assumption that rewards and transition probabilities are similar in close states .",
    "more precisely , we assume that rewards and transition probabilities are _ hlder continuous_.    [ ass : rew ] there are @xmath20 such that for any two states @xmath21 and all actions @xmath16 , @xmath22    [ ass : prob ] there are @xmath20 such that for any two states @xmath21 and all actions @xmath16 , @xmath23    for the sake of simplicity we will assume that @xmath5 and @xmath24 in assumptions [ ass : rew ] and [ ass : prob ] are the same .",
    "we also assume existence of an optimal policy @xmath25 which gives optimal average reward @xmath26 on @xmath11 independent of the initial state .",
    "a sufficient condition for state - independent optimal reward is geometric convergence of @xmath27 to an invariant probability measure .",
    "this is a natural condition which e.g.  holds for any communicating finite state mdp",
    ". it also ensures ( cf .",
    "chapter 10 of @xcite ) that the poisson equation holds for the optimal policy .",
    "in general , under suitable technical conditions ( like geometric convergence to an invariant probability measure @xmath28 ) the _ poisson equation _ @xmath29 relates the rewards and transition probabilities under any measurable policy @xmath30 to its average reward  @xmath31 and the _ bias _ function @xmath32 of @xmath30 . intuitively , the bias is the difference in accumulated rewards when starting in a different state .",
    "formally , the bias is defined by the poisson equation and the normalizing equation @xmath33 ( cf .",
    "e.g.  @xcite ) .",
    "the following result follows from the bias definition and assumptions  [ ass : rew ] and  [ ass : prob ] ( together with results from chapter  10 of  @xcite ) .",
    "[ ass : mix ] under assumptions [ ass : rew ] and [ ass : prob ] , the bias of the optimal policy is bounded .",
    "consequently , it makes sense to define the _ bias span _ @xmath34 of a continuous state mdp  @xmath11 satisfying assumptions  [ ass : rew ] and  [ ass : prob ] to be @xmath35 . note that since @xmath36 by definition of the bias , the bias function  @xmath37 is upper bounded by  @xmath34 .",
    "we are interested in algorithms which can compete with the optimal policy @xmath27 and measure their performance by the _ regret _",
    "( after @xmath1 steps ) defined as @xmath38 , where @xmath39 is the random reward obtained by the algorithm at step @xmath40 . indeed , within @xmath1 steps no _ canonical _ or even _ bias optimal _ optimal policy ( cf",
    ". chapter  10 of  @xcite ) can obtain higher accumulated reward than @xmath41 .",
    "our algorithm uccrl , shown in detail in figure [ alg : col ] , implements the `` optimism in the face of uncertainty maxim '' just like ucrl2  @xcite or regal  @xcite .    state space @xmath12 $ ] , action space @xmath13 , confidence parameter @xmath42 , aggregation parameter  @xmath43 , upper bound @xmath44 on the bias span , lipschitz parameters @xmath45 .",
    "+ @xmath46let @xmath47 $ ] , @xmath48 $ ] for @xmath49 .",
    "+ @xmath46set @xmath50 , and observe the initial state @xmath51 and interval @xmath52 .",
    "@xmath46let @xmath53 be the number of times action  @xmath16 has been chosen in a state @xmath54 _ prior _ to episode  @xmath55 , and @xmath56 the respective counts _ in _ episode @xmath55 . * initialize episode * @xmath55 : + @xmath46set the start time of episode @xmath55 , @xmath57 .",
    "+ @xmath46compute estimates @xmath58 and @xmath59 for rewards and transition probabilities , using all samples from states in the same interval @xmath60 , respectively . *",
    "compute policy * @xmath61 : + @xmath46let @xmath62 be the set of plausible mdps @xmath63 with @xmath64 and rewards @xmath65 and transition probabilities @xmath66 satisfying @xmath67 @xmath46choose policy @xmath61 and @xmath68 such that @xmath69    * execute policy @xmath61 * : * while * @xmath70 * do *    * choose action @xmath71 , obtain reward @xmath39 , and observe next state  @xmath72 .",
    "* set @xmath73 .",
    "* end while *    it maintains a set of plausible mdps  @xmath74 and chooses optimistically an mdp @xmath75 and a policy @xmath76 such that the average reward @xmath77 is maximized , cf .  .",
    "whereas for ucrl2 and regal the set of plausible mdps is defined by confidence intervals for rewards and transition probabilities for each individual state - action pair , for uccrl  we assume an mdp to be plausible if its _ aggregated _ rewards and transition probabilities are within a certain range .",
    "this range is defined by the aggregation error ( determined by the assumed hlder continuity ) and respective confidence intervals , cf .  , .",
    "correspondingly , the estimates for rewards and transition probabilities for some state action - pair @xmath78 are calculated from all sampled values of action @xmath16 in states close to @xmath15 .",
    "more precisely , for the aggregation uccrl  partitions the state space into intervals @xmath47 $ ] , @xmath79 $ ] for @xmath80 .",
    "the corresponding aggregated transition probabilities are defined by @xmath81 generally , for a ( transition ) probability distribution @xmath82 over @xmath83 we write @xmath84 for the aggregated probability distribution with respect to @xmath85 .",
    "now , given the aggregated state space @xmath85 , estimates @xmath86 and @xmath87 are calculated from all samples of action @xmath16 in states in @xmath60 , the interval @xmath88 containing @xmath15 .",
    "( consequently , the estimates are the same for states in the same interval . )    as ucrl2 and regal , uccrl  proceeds in episodes in which the chosen policy remains fixed .",
    "episodes are terminated when the number of times an action has been sampled from some interval  @xmath88 has been doubled . only then estimates are updated and a new policy is calculated .    since all states in the same interval @xmath88 have the same confidence intervals ,",
    "finding the optimal pair  @xmath89 in   is equivalent to finding the respective optimistic discretized mdp @xmath90 and an optimal policy @xmath91 on @xmath90 .",
    "then @xmath61 can be set to be the extension of @xmath91 to @xmath92 , that is , @xmath93 for all @xmath15 .",
    "however , due to the constraint on the bias even in this finite case efficient computation of @xmath90 and @xmath91 is still an open problem .",
    "we note that the algorithm  @xcite selects optimistic mdp and optimal policy in the same way as uccrl .    while the algorithm presented here",
    "is the first modification of ucrl2 to continuous reinforcement learning problems , there are similar adaptations to online aggregation  @xcite and learning in finite state mdps with some additional similarity structure known to the learner @xcite .",
    "for uccrl  we can derive the following bounds on the regret .",
    "[ thm ] let @xmath11 be an mdp with continuous state space @xmath17 $ ] , @xmath94 actions , rewards and transition probabilities satisfying assumptions  [ ass : rew ] and  [ ass : prob ] , and bias span upper bounded by @xmath44 .",
    "then with probability @xmath95 , the regret of uccrl  ( run with input parameters @xmath96 and @xmath44 ) after @xmath1 steps is upper bounded by @xmath97 therefore , setting @xmath98 gives regret upper bounded by @xmath99 with no known upper bound on the bias span , guessing @xmath44 by @xmath100 one still obtains an upper bound on the regret of @xmath3 .    intuitively , the second term in the regret bound of is the discretization error , while the first term corresponds to the regret on the discretized mdp . a detailed proof of theorem  [ thm ] can be found in section  [ sec : proof ] below .",
    "[ rem : d ] the general @xmath6-dimensional case can be handled as described for dimension 1 , with the only difference being that the discretization now has @xmath101 states , so that one has @xmath101 instead of @xmath96 in the first term of  .",
    "then choosing @xmath102 bounds the regret by @xmath7 .",
    "if the horizon @xmath1 is unknown then the doubling trick ( executing the algorithm in rounds @xmath103 guessing @xmath104 and setting the confidence parameter to @xmath105 ) gives the same bounds .",
    "[ r : uh ] the uccrl  algorithm receives ( bounds on ) the hlder parameters @xmath24 as @xmath5 as inputs . if these parameters are not known , then one can still obtain sublinear regret bounds albeit with worse dependence on @xmath1 .",
    "specifically , we can use the model - selection technique introduced in @xcite . to do this , fix a certain number @xmath106 of values for the constants @xmath24 and  @xmath5 ; each of these values will be considered as a model .",
    "the model selection consists in running uccrl  with each of these parameter values for a certain period of @xmath107 time steps ( _ exploration _ ) .",
    "then one selects the model with the highest reward and uses it for a period of @xmath108 time steps ( _ exploitation _ ) , while checking that its average reward stays within  ( [ eq : thm ] ) of what was obtained in the exploitation phase .",
    "if the average reward does not pass this test , then the model with the second - best average reward is selected , and so on .",
    "then one switches to exploration with longer periods @xmath109 , etc .",
    "since there are no guarantees on the behavior of uccrl  when the hlder parameters are wrong , none of the models can be discarded at any stage .",
    "optimizing over the parameters @xmath110 and @xmath111 as done in  @xcite , and increasing the number @xmath106 of considered parameter values , one can obtain regret bounds of @xmath112 , or @xmath113 in the lipschitz case .",
    "for details see @xcite .",
    "since in this model - selection process is used in a `` black - box '' fashion , the exploration is rather wasteful , and thus we think that this bound is suboptimal .",
    "recently , the results of  @xcite have been improved  @xcite , and it seems that similar analysis gives improved regret bounds for the case of unknown hlder parameters as well .",
    "the following is a complementing lower bound on the regret for continuous state reinforcement learning .",
    "[ thm : lower ] for any @xmath114 and any reinforcement learning algorithm there is a continuous state reinforcement learning problem with @xmath94 actions and bias span @xmath44 satisfying assumption  [ ass : rew ] such that the algorithm suffers regret of @xmath115 .",
    "consider the following reinforcement learning problem with state space @xmath17 $ ] .",
    "the state space is partitioned into @xmath96 intervals @xmath88 of equal size .",
    "the transition probabilities for each action  @xmath16 are on each of the intervals  @xmath88 concentrated and equally distributed on the same interval  @xmath88 .",
    "the rewards on each interval  @xmath88 are also constant for each @xmath16 and are chosen as in the lower bounds for a multi - armed bandit problem @xcite with @xmath116 arms . that is , giving only one arm slightly higher reward , it is known  @xcite that regret of @xmath117",
    "can be forced upon any algorithm on the respective bandit problem . adding another action giving no reward and equally distributing over the whole state space , the bias span of the problem is @xmath96 and the regret @xmath118 .",
    "[ rem : lower ] note that assumption  [ ass : prob ] does not hold in the example used in the proof of theorem  [ thm : lower ]",
    ". however , the transition probabilities are piecewise constant ( and hence lipschitz ) and known to the learner . actually , it is straightforward to deal with piecewise hlder continuous rewards and transition probabilities where the finitely many points of discontinuity are known to the learner .",
    "if one makes sure that the intervals of the discretized state space do not contain any discontinuities , it is easy to adapt uccrl   and theorem  [ thm ] accordingly .",
    "the bounds of theorems  [ thm ] and [ thm : lower ] can not be directly compared to bounds for the continuous - armed bandit problem  @xcite , because the latter is no special case of learning mdps with continuous state space ( and rather corresponds to a continuous action space ) .",
    "thus , in particular one can not freely sample an arbitrary state of the state space as assumed in continuous - armed bandits .",
    "for the proof of the main theorem we adapt the proof of the regret bounds for finite mdps in  @xcite and  @xcite .",
    "although the state space is now continuous , due to the finite horizon @xmath1 , we can reuse some arguments , so that we keep the structure of the original proof of theorem  2 in  @xcite .",
    "some of the necessary adaptations made are similar to techniques used for showing regret bounds for other modifications of the original ucrl2 algorithm @xcite , which however only considered finite - state mdps .",
    "let @xmath119 be the number of times action @xmath16 has been chosen in episode  @xmath55 when being in state @xmath15 , and denote the total number of episodes by @xmath120 . then setting @xmath121 , with probability at least @xmath122 the regret of uccrl  after @xmath1 steps is upper bounded by ( cf .",
    "section 4.1 of @xcite ) , @xmath123      next , we consider the regret incurred when the true mdp  @xmath11 is not contained in the set of plausible mdps  @xmath124 .",
    "thus , fix a state - action pair @xmath78 , and recall that @xmath125 and @xmath126 are the estimates for rewards and transition probabilities calculated from all samples of state - action pairs contained in the same interval  @xmath60 .",
    "now assume that at step @xmath40 there have been @xmath127 samples of action @xmath16 in states in  @xmath60 and that in the @xmath128-th sample a transition from state @xmath129 to state @xmath130 has been observed @xmath131 .",
    "first , concerning the rewards one obtains as in the proof of lemma 17 in appendix c.1 of @xcite  but now using hoeffding for independent and not necessarily identically distributed random variables  that @xmath132        \\right\\vert        \\geq \\sqrt{\\frac{7}{2n}\\log\\big ( \\tfrac{2 na t}{\\delta}\\big ) }      \\right\\ }    & \\leq & \\frac{\\delta}{60 n a t^7}.    \\end{aligned}\\ ] ]    concerning the transition probabilities , we have for a suitable @xmath133 @xmath134 \\big\\|_1        =      \\sum_{j=1}^n \\big|   \\hat{p}{^{\\rm agg}}(i_j|s , a ) - \\mathbb{e}[\\hat{p}{^{\\rm agg}}(i_j|s , a ) ]   \\big| }   \\nonumber      \\\\      & = &         \\sum_{j=1}^n \\big (   \\hat{p}{^{\\rm agg}}(i_j|s , a ) - \\mathbb{e}[\\hat{p}{^{\\rm agg}}(i_j|s , a ) ]   \\big)\\ , x(i_j )   \\nonumber\\\\      & = &       \\tfrac{1}{n}\\sum_{i=1}^n               \\big ( x(i(s'_i ) )   - \\int_{{{\\mathcal{s } } } } p(ds'|s_i , a)\\cdot x(i(s ' ) )   \\big)\\ , .",
    "\\quad \\label{eq : ah}\\end{aligned}\\ ] ] for any @xmath133 , @xmath135 is a martingale difference sequence with @xmath136 , so that by azuma - hoeffding inequality ( e.g. , lemma 10 in @xcite ) , @xmath137 and in particular @xmath138 a union bound over all sequences @xmath139 then yields from that @xmath140        \\big\\|_1        \\geq \\sqrt{\\frac{56 n}{n}\\log\\big ( \\tfrac{2 a t}{\\delta}\\big ) }      \\right\\ }    &   \\leq & \\frac{\\delta}{20 na t^7}.    \\end{aligned}\\ ] ]    another union bound over all @xmath40 possible values for @xmath141 , all @xmath96 intervals and all actions shows that the confidence intervals in and hold at time @xmath40 with probability at least @xmath142 for the actual counts @xmath143 and all state - action pairs @xmath78 .",
    "( note that the equations and are the same for state - action pairs with states in the same interval . )    now , by linearity of expectation @xmath144 $ ] can be written as @xmath145 . since",
    "the @xmath146 are assumed to be in the same interval @xmath60 , it follows that @xmath147 - r(s , a)| < l n^{-\\alpha}$ ] .",
    "similarly , @xmath148-p{^{\\rm agg}}(\\cdot|s , a)\\big\\|_1 < l n^{-\\alpha}$ ] .",
    "together with and this shows that with probability at least @xmath142 for all state - action pairs @xmath78 @xmath149 this shows that the true mdp is contained in the set of plausible mdps @xmath150 at step @xmath40 with probability at least @xmath142 , just as in lemma 17 of  @xcite . the argument that",
    "@xmath151 with probability at least @xmath152 then can be taken without any changes from section 4.2 of @xcite .",
    "now for episodes with @xmath153 , by the optimistic choice of @xmath154 and @xmath61 in   we can bound @xmath155    any term @xmath156 is bounded according to and , as we assume that @xmath157 , so that summarizing states in the same interval @xmath88 @xmath158 since @xmath159 , setting @xmath160 to be the length of episode @xmath55 we have @xmath161          & & + \\ ;",
    "2l n^{-\\alpha } \\tau_k + \\sqrt { 14\\log\\left ( \\tfrac{2 n a t}{\\delta }   \\right ) }            \\sum_{j=1}^n \\sum_{a\\in { { \\mathcal{a } } } } \\frac { v_k(i_j , a)}{\\sqrt{\\max\\{1,n_k(i_j , a)\\ } } } \\,.\\quad   \\label{eq : poisson3}\\end{aligned}\\ ] ]    we continue analyzing the first term on the right hand side of . by the poisson equation   for @xmath61 on @xmath154 , denoting the respective bias by @xmath162 we can write @xmath163           & = &   \\sum_{s } v_k(s,{\\tilde\\pi_{k}}(s ) )   \\big ( \\int_{{\\mathcal{s}}}{\\tilde{p}}_k(ds'|s,{\\tilde\\pi_{k}}(s ) ) \\cdot { \\tilde{\\lambda}}_k(s ' ) - { \\tilde{\\lambda}}_k(s ) \\big ) \\nonumber \\\\",
    "\\nonumber \\\\            & = &   \\sum_{s } v_k(s,{\\tilde\\pi_{k}}(s ) )   \\big ( \\int_{{{\\mathcal{s } } } } p(ds'|s,{\\tilde\\pi_{k}}(s ) ) \\cdot { \\tilde{\\lambda}}_k(s ' ) - { \\tilde{\\lambda}}_k(s ) \\big ) \\label{eq : neu1 } \\\\            & & + \\sum_{s } v_k(s,{\\tilde\\pi_{k}}(s ) ) \\sum_{j=1}^n\\int_{i_j } \\big ( { \\tilde{p}}_k(ds'|s,{\\tilde\\pi_{k}}(s ) ) -p(ds'|s,{\\tilde\\pi_{k}}(s ) ) \\big ) \\cdot { \\tilde{\\lambda}}_k(s ' ) .",
    "\\label{eq : neu2}\\end{aligned}\\ ] ]      now @xmath164 can be bounded by and , because we assume @xmath165 . hence , since by definition of the algorithm @xmath44 bounds the bias function @xmath166 , the term in   is bounded by @xmath167 while for the term in   @xmath168    let @xmath169 be the index of the episode time step @xmath40 belongs to .",
    "then the sequence @xmath170 is a sequence of martingale differences so that azuma - hoeffding inequality shows ( cf .",
    "section 4.3.2 and in particular eq .",
    "( 18 ) in  @xcite ) that after summing over all episodes we have @xmath171 where the second term comes from an upper bound on the number of episodes , which can be derived analogously to appendix  c.2 of @xcite .",
    "to conclude , we sum over all the episodes with @xmath172 , using , , and .",
    "this yields that with probability at least @xmath173 @xmath174 analogously to section 4.3.3 and appendix c.3 of @xcite , one can show that @xmath175 and we get from after some simplifications that with probability @xmath176 @xmath177 finally , evaluating by summing @xmath178 over all episodes , by and we have with probability @xmath179 an upper bound on the regret of @xmath180 a union bound over all possible values of @xmath1 and further simplifications as in appendix c.4 of @xcite finish the proof .",
    "we think that a generalization of our results to continuous action space should not pose any major problems . in order to improve over the given bounds",
    ", it may be promising to investigate more sophisticated discretization patterns .",
    "the assumption of hlder continuity is an obvious , yet not the only possible assumption one can make about the transition probabilities and reward functions .",
    "a more general problem is to assume a set @xmath181 of functions , find a way to measure the `` size '' of @xmath181 , and derive regret bounds depending on this size of @xmath181 .",
    "the authors would like to thank the three anonymous reviewers for their helpful suggestions and rmi munos for useful discussion which helped to improve the bounds .",
    "this research was funded by the ministry of higher education and research , nord - pas - de - calais regional council and feder ( contrat de projets etat region cper 2007 - 2013 ) , anr projects explo - ra ( anr-08-cosi-004 ) , lampada ( anr-09-emer-007 ) and coadapt , and by the european community s fp7 program under grant agreements n@xmath182216886 ( pascal2 ) and n@xmath182270327 ( complacs ) . the first author is currently funded by the austrian science fund ( fwf ) : j  3259-n13 .",
    "sbastien bubeck , rmi munos , gilles stoltz , and csaba szepesv ' ari .",
    "online optimization of @xmath183-armed bandits . in _ advances in neural information processing systems 22 , nips 2009 _ , pages 201208 , 2010 ."
  ],
  "abstract_text": [
    "<S> we derive sublinear regret bounds for undiscounted reinforcement learning in continuous state space . </S>",
    "<S> the proposed algorithm combines state aggregation with the use of upper confidence bounds for implementing optimism in the face of uncertainty . beside the existence of an optimal policy which satisfies the poisson equation , the only assumptions made are hlder continuity of rewards and transition probabilities </S>"
  ]
}