{
  "article_text": [
    "let us consider the constrained optimization @xmath0 subject to the unilateral constraint @xmath1 the bilateral constraint @xmath2 can be transformed into the unilateral constraint , and we only consider without loss of generality .",
    "we assume @xmath3 is a smooth functional on @xmath4 and @xmath5 is onto .",
    "inequality constrained optimization problems appear in a vast range of applications such as contact problems @xcite , obstacle problems @xcite , topology optimization @xcite , robotics and gait analysis @xcite , contact mechanics@xcite , and there are several numerical methods which can be used as practical tools for solving the problems ; @xcite . for references to the literature on the numerical methods for optimization problems",
    ", one may also refer to the monographs @xcite .",
    "interior or exterior penalty methods require solving a sequence of unconstrained problems in which the penalty parameter ( the controlling parameter ) approaches 0 or infinity .",
    "this yields the ill - conditioning of the unconstrained problem , which is the main drawback of the penalty method .",
    "in contrast , exact penalty methods transform the constrained problem - into a single unconstrained problem .",
    "surprisingly , the penalized unconstrained problems are exact under certain sufficient conditions for a local optimality in the problem - , i.e. , all solutions of the penalized unconstrained problem are also solutions of the original problem for all values of the penalty parameter grater than some positive value .",
    "for this reason , considerable attention has been devoted to the use of exact penalty approaches in solving constrained optimization problems .",
    "a survey of the chronological development of the penalty methods ( including multiplier methods ) since 1968 to 1993 can be found in @xcite .",
    "the exact penalty methods require , however , minimization of a nondifferentiable cost functional .",
    "one may not employ a standard optimization solver that are customized for optimization problems with smooth functions .",
    "therefore numerical techniques should be developed by utilizing the particular structures of the penalty functions that compensate for the absence of differentiability .",
    "numerical methods to approximate the solution of exact penalty methods have been considered by several authors .",
    "we only mention the articles of @xcite . in this article",
    "we consider the exact penalty formulation where max is used for the penalty ; @xmath6 \\displaystyle \\mbox{with } \\ ; \\psi(y ) = \\sum_i\\max(0 , y_i),\\quad y\\in { \\mathbb{r}}^m , \\end{array}\\ ] ] for @xmath7 , and we develop fast iterative methods for finding the minimizer based on the nonsmooth optimization theory .",
    "the optimality condition of is given by @xmath8 where @xmath9 is the convex sub - differential of @xmath10 , i.e. , @xmath11 , &   s=0 , \\\\       1 , &   s>0 .",
    "\\end{array}\\right . \\end{array}\\ ] ] on the other hand , the necessary optimality of - is given by @xmath12 where @xmath13 is the lagrange multiplier of the unilateral constraint @xcite .",
    "let the pair @xmath14 be a solution to .",
    "then @xmath15 is also a solution to provided that @xmath16 , ( e.g.,@xcite ) .    due to the singularity and the non - uniquness of the subgradient of @xmath17 , the direct treatment of the condition",
    "many not be feasible for numerical computation .",
    "the common strategy to alleviate the technical difficulty resulting from the non - differentiability of the penalty functional is to introduce a regularized penalty functional : let us consider the regularized problem to ; @xmath18 \\displaystyle \\mbox{with } \\ ;",
    "\\psi_\\epsilon(y ) = \\sum_i\\phi_\\epsilon(y_i),\\quad y\\in { \\mathbb{r}}^m , \\end{array}\\ ] ] where @xmath19 for @xmath20 is a regularization of the function @xmath21 defined by @xmath22 \\\\ \\\\ s &    s \\ge \\epsilon . \\end{array } \\right.\\ ] ] an arbitrary @xmath23 in is used to avoid the singularity and to determine a single value in the subdifferential @xmath24 . since @xmath25",
    ", the necessary optimality condition of is given by the equation @xmath26 although the non - uniqueness for concerning subdifferential in the optimality condition is now bypassed through regularization , the optimality condition is still nonlinear .",
    "one of the strategies for solving is to use the asymptotic solution at infinity of the nonlinear ode @xmath27 with some positive definite matrix @xmath28 which serves as a precondition of @xmath29 .",
    "( see @xcite ) .",
    "the method is simple and easy to implement , however , the convergence speed is quite slow and the numerical solution obtained by the algorithm is not accurate .",
    "this is due to the fact that the nonlinearity in @xmath30 is not fully taken into account and not incorporated in algorithms .",
    "one of the objective of this paper is to design the fast , accurate numerical algorithm for by taking the nonlinearity into consideration .",
    "the outline of the paper is as follows . in section 2",
    "an implicit iterative algorithm for is proposed .",
    "the property and convergence of the proposed algorithm are analyzed . in section 3 the primal - dual active method",
    "is introduced and the relation to the proposed method is discussed . in section 4 several numerical tests are reported to assess the performance of the method .",
    "in this section we introduce the algorithm for and analyze its convergence .",
    "first , we have the consistency result as @xmath31 .",
    "[ thm : converge ] let @xmath32 be a solution of the regularized problem",
    ". for an arbitrary @xmath33 , any cluster point of @xmath34 is also a solution of .",
    "let @xmath35 be a solution to .",
    "then we have @xmath36 let @xmath15 be a cluster point of @xmath34 . from",
    ", we have @xmath37 thus , from continuity of @xmath3 , @xmath10 and the fact that @xmath38 for all @xmath39 , we obtain @xmath40    as a consequence of theorem [ thm : converge ] , we have that    suppose that each of the problem and admits the unique solution .",
    "if @xmath7 is sufficiently large , @xmath32 converges to the solution @xmath15 of .",
    "we propose the fast algorithm that provides an accurate numerical solution of .",
    "for this objective , we first observe that the necessary optimality condition is written as @xmath41 where @xmath42 denotes a diagonal matrix with the entries @xmath43_{j , j}=\\left\\{\\begin{array}{ll } { \\displaystyle}\\frac{1}{\\max(\\epsilon,(gx - g)_j)}~ , & ( gx - g)_j\\ge0\\\\ 0~ , &   ( gx - g)_j    < 0 \\end{array}\\right . \\end{array}\\ ] ] and @xmath44 is a column vector depending on @xmath39 defined by @xmath45_j=\\left\\{\\begin{array}{ll } { \\displaystyle}\\frac { g_j}{\\max(\\epsilon,(gx -g)_j)}~ , & ( gx - g)_j\\ge0 \\\\ 0~ , &   ( gx - g)_j < 0 \\end{array}\\right . \\end{array}\\ ] ] the optimality condition in the form suggests the following fixed point iteration ; @xmath46 where @xmath47 is positive , symmetric and serves a pre - conditioner for @xmath48 .",
    "the parameter @xmath49 serves a stabilizing and acceleration stepsize ( see , theorem 3 ) .",
    "let @xmath50 . equation for @xmath51 is equivalent to the equation for @xmath52 @xmath53 which gives us @xmath54    * lemma 1 * the direction @xmath52 is a descent direction for @xmath55 at @xmath56 .    from @xmath57 where we used the fact that @xmath47 is strictly positive definite .",
    "so the iteration can be seen as a descent method and is written as ;    * algorithm 1 : fixed point iteration . *",
    "set parameters : @xmath58 .",
    "compute the direction by @xmath59 .",
    "+ [ 10pt ] step 2 .",
    "update @xmath60 + if @xmath61 , then stop",
    ". otherwise repeat step 1 - step 2 .",
    "+ let us make some remarks on the algorithm :    in many applications , the structure of @xmath62 , @xmath47 and @xmath63 are sparse block diagonals , and the resulting system for the direction @xmath52 then becomes a linear system with a sparse symmetric positive - definite matrix , and can be efficiently solved by , for example the cholesky decomposition method .    if @xmath64 , then we have @xmath65 . for this case we may use the alternative update @xmath66 assuming that it does nt cost much to perform @xmath67    algorithm 1 is globally and rapidly convergent and the following results justify the fact",
    "let us introduce some notations ; @xmath68 and @xmath69 .",
    "[ lem : r ] let @xmath70 and @xmath71 .",
    "the following identity holds for all @xmath72 ; @xmath73_{j , j},|(gx^{k+1}-g)_{j}|^2-     multiplying by @xmath50 @xmath74 where @xmath75 from the identity @xmath76 with @xmath77 and @xmath78 , we obtain @xmath79_{j , j},|(gx^{k+1}-g)_{j}|^2-     assume there exists @xmath80 such that @xmath81 if there exists @xmath82 such that @xmath83 for all @xmath84 , then @xmath85 and @xmath86 is globally convergent .    since @xmath87 on @xmath88 is concave",
    "( see for the definition of @xmath89 ) , we have @xmath90 thus @xmath91_{j , j},|(gx^{k+1}-g)_{j}|^2-|(gx^{k}-g)_{j}|^2 ) \\ge \\phi_\\epsilon((gx^{k+1}-g)_{j})-\\phi_\\epsilon((gx^k - g)_{j}),\\quad \\forall j\\in { \\cal a}^k.\\ ] ] hence @xmath92_{j , j},|(gx^{k+1}-g)_{j}|^2-|(gx^{k}-g)_{j}|^2 ) \\ge \\sum_{j\\in   { \\cal a}^k}\\left [   \\phi_{\\epsilon}((gx^{k+1}-g)_{j})-\\phi_{\\epsilon}((gx^{k}-g)_{j})\\right]\\\\   & = \\psi_\\epsilon(gx^{k+1}-g)-\\psi_\\epsilon(gx^k - g ) -\\sum_{i\\in{\\cal i}^k   } \\left[\\phi_{\\epsilon}((gx^{k+1}-g)_{i})-\\phi_{\\epsilon}((gx^{k}-g)_{i } ) \\right]\\\\ & \\ge \\psi_\\epsilon(gx^{k+1}-g)-\\psi_\\epsilon(gx^k - g).\\end{aligned}\\ ] ] thus , we obtain @xmath93 if we assume @xmath94 for some @xmath80 , then @xmath95 is monotonically decreasing and @xmath96    suppose @xmath97 but @xmath98 .",
    "assume @xmath99 with @xmath100 , then the algorithm is globally convergent .",
    "algorithm 1 closely resembles to the semismooth newton s method @xcite applied to the equation : the gradient @xmath101 at @xmath102 has a newton derivative @xmath103 where the diagonal matrix @xmath104 is defined by @xmath105_{i , i } = \\left\\{\\begin{array}{cc } { \\displaystyle}\\frac{1}{\\epsilon } , &   ( gx^k - g)_i   \\in ( 0,\\epsilon ) \\\\[10pt ]    0 ,   & \\mbox{otherwise}. \\end{array } \\right.\\ ] ] replacing step 1 with the system for the semi - smooth newton step @xmath106 one arrives at a semismooth newton s method . in general , the sequence @xmath86 generated by the newton s method is guaranteed to converge when the initial guess @xmath107 is sufficiently close to the true solution @xmath108 .",
    "when @xmath107 is not close enough to the minimum , taking the full newton step @xmath109 need not decrease the objective function @xmath55 , moreover it may generate a non - convergence sequence . on the other hand , through several numerical experiment",
    "the sequence generated by algorithm 1 converges to the true solution within a few iterations even when an initial guess is far from the true solution .",
    "if the @xmath110 iterate @xmath56 is close to the solution @xmath111 and satisfies @xmath112 , then @xmath113 and algorithm 1 enjoys the superlinear convergence of semi - smooth newton s method",
    ".    we shall investigate algorithm 1 and the semi - smooth newton through a simple problem : we consider the optimization problem @xmath114 in this case and are explicitly given as @xmath115 and @xmath116 it is easy to prove that the sequence @xmath86 generated by the iteration and converges to @xmath117 for any initial @xmath107 provided that @xmath118 and @xmath119 .",
    "on the other hand , if we assume that @xmath120 , then for any initial @xmath107 we have @xmath121 for all @xmath122 .    we depict @xmath123 , @xmath124 in fig .",
    "[ fig : ours ] and [ fig : newton ] respectively .",
    "the outcomes after three iterations starting from @xmath125 are also plotted to visualize the iteration process .",
    "the parameters @xmath126 , @xmath127 and @xmath128 were used to draw these graphs .",
    "( we select a large @xmath129 just for the purpose of the visualization . in practice",
    ", we will take much smaller number , say , @xmath130 . )",
    "we observe from the figure that the performance of algorithm 1 is much better than the one of the semismooth newton s method .    .",
    "]    . ]",
    "since @xmath52 determined by is a descent direction of @xmath131 , one can use the line search method ;    * algorithm 2 : successive iteration with line search * + step 0 . set parameters : @xmath132 .",
    "compute the direction by @xmath133 step 2 .",
    "determine the steplength @xmath134 by minimizing @xmath135 , i.e. , @xmath136 + step 3 .",
    "update @xmath137 + if @xmath138 , then stop",
    ". otherwise repeat step 1 - step 3 .",
    "+ step 2 can be replaced by the line search algorithms such as armijo s rule @xcite .",
    "the proof of the convergence of algorithm 2 is quite standard and we omit the proof .",
    "an alternative to our gradient - based algorithm is the newton update .",
    "the semi - smooth newton method in @xcite is based on complementarity condition for @xmath139 ; @xmath140 the semi - smooth newton method reduces to the primal - dual active set method @xcite ;    * primal - dual active set method *    * choose @xmath141 and set @xmath142 . * set @xmath143 and @xmath144 . *",
    "solve for @xmath145 @xmath146 * convergent or set @xmath147 and return to step 2 .",
    "\\(1 ) if @xmath3 is quadratic , i.e. , @xmath148 , then step 3 is written as @xmath149 if @xmath150 , then step 3 is solvable .",
    "otherwise , we assume that @xmath151 is positive on @xmath152 .",
    "\\(2 ) if @xmath3 is @xmath153 , the newton step for step 3 is given by @xmath154    \\(3 ) in general one may use the regularized update @xmath155 to avoid the possible singularity of linear system .",
    "it reduces to @xmath156 which is very similar to with @xmath157 .",
    "consequently , algorithm 1 is much stabler than prima - dual active method .",
    "\\(4 ) but , it is shown in @xcite if the primal - dual active method converges , it converges q - super linearly and in a finite step .",
    "\\(5 ) one can hybrid algorithm 1 and prima - dual active method so that one may accelerate the convergence .",
    "in this section we show some numerical experiments using algorithm 1 proposed in section 2 for unilateral constrained quadratic optimization problems @xmath158 with several @xmath159 and @xmath160 .",
    "all tests confirm the fact convergence and effectiveness of the proposed algorithm .",
    "let @xmath161\\times[0,1]$ ] .",
    "we solve an obstacle problem @xmath162 { \\displaystyle}k = \\ { u\\in h^1_0(\\omega ) \\mid    u(x )    \\le \\delta ( x , \\partial \\omega)\\ } , \\end{array}\\ ] ] where @xmath163 is used and @xmath164 is distance from @xmath39 to @xmath165 ; @xmath166 we use the standard bilinear finite element method to discretize the problem : for cartesian grid @xmath167 , @xmath168 , we define a finite element by @xmath169 ; the element domain @xmath170 is a rectangle , @xmath171\\times [ y_j , y_{j+1}]$ ] , and the space of shape functions @xmath172 is given by @xmath173\\otimes [ 1,\\frac{y - y_{i , j}}{\\delta x}][u_{i , j},u_{i+1,j},u_{i+1,j+1},u_{i , j+1}]^\\top.\\ ] ] and @xmath174 is nodal variables at the grid points .",
    "the subscript @xmath175 indicates the mesh size @xmath176.the finite element discretization yields the discrete energy functional @xmath177 for @xmath178 , where @xmath179 and @xmath180 and @xmath181 denote the stiffness matrix and the load vector associated with the discretization .",
    "the inequality constrained is approximated by @xmath182 , which is equivalent to @xmath183 , where @xmath184 .",
    "let @xmath185 denote the generated sequence by algorithm 1 .",
    "we report @xmath186 and the sup norm of the gradient of @xmath187 : @xmath188    we run algorithm 1 with the following parameters and preconditioner :    1 .",
    "mesh size @xmath189 , @xmath190 , @xmath191 , @xmath192 , @xmath193 .",
    "mesh size @xmath194 , @xmath190 , @xmath191 , @xmath192 , @xmath193",
    "[ fig : j_r_implicit2 ] shows the monotone convergence of the objective function @xmath131 : the convergence achieves after 11 iteration for @xmath189 , and 20 iteration for @xmath194 .     and",
    "@xmath195 of the finite element solutions @xmath196 generated by algorithm 1 with @xmath190 , @xmath191 , @xmath192 and @xmath193.,title=\"fig:\",height=170 ]   and @xmath195 of the finite element solutions @xmath196 generated by algorithm 1 with @xmath190 , @xmath191 , @xmath192 and @xmath193.,title=\"fig:\",height=170 ]      let @xmath161 ^ 2 $ ] .",
    "the problem consists in recovering the source term @xmath197 in the equation @xmath198 from the noisy data @xmath199 such that @xmath200 where @xmath201 is an additive ( unknown ) noise .",
    "we assume that the source term @xmath202 is constrained ; @xmath203 .",
    "let @xmath204 denote the ( weak ) solution of .",
    "the problem is well - known to be ill - posed and an approximation to the solution @xmath202 can be obtained by tikhonov regularization method : @xmath205 @xmath206 where @xmath207 , @xmath208 is a regularization parameter .",
    "algorithm 1 requires the computation of the gradient @xmath209 .",
    "one can calculate the gradient by @xmath210 where the adjoint variable @xmath211 is obtained by solving the adjoint equation @xmath212 in our computation , the noisy data @xmath213 is generated by adding a random noise to the observation data @xmath214 : @xmath215 where rand(@xmath39 ) is a uniformly distributed random function in @xmath216 $ ] , and @xmath217 is the noise level . the unknown source ( exact solution ) @xmath202 and the noise free data @xmath214 are depicted in the first row of fig .",
    "[ fig : is ] .",
    "the domain @xmath161 ^ 2 $ ] is divided into @xmath218 subsquares of the mesh size @xmath219 .",
    "the central finite difference method is used to approximate @xmath220 at @xmath221 ; @xmath222 and we approximate @xmath223 as follows : @xmath224_{i , j } - [ y_\\delta ] _ { i , j})^2 + \\frac{h^2\\eta}{2 } \\sum_{i , j } u_{i , j}^2.\\ ] ] hence the discretized exact penalty problem is equivalent to @xmath225 here @xmath170 is the matrix for the second order central difference associated to .",
    "we employed algorithm 1 to the problem with parameters @xmath226 , @xmath227 , @xmath228 .",
    "the preconditioner @xmath229 is used : step 1 in algorithm 1 is written as @xmath230^t k^{-1 }    + \\eta",
    "i   + \\beta\\chi_\\epsilon(u^k ) ) d^k = - (    p   +    \\beta u^k + \\beta \\psi_\\epsilon^\\prime(u^k))\\ ] ] which is equivalently written as @xmath231 where we use @xmath232 .",
    "the reconstructed source obtained by the nonsmooth tikhonov regularization with the regularization parameter @xmath233 , and the noisy data with noise level @xmath217 are shown in fig . [",
    "fig : is ] .",
    "we observed that algorithm 1 converged ( i.e. , @xmath234 ) within 20 iterations in all cases .",
    "the regularization parameter @xmath233 was selected manually according to the noise level @xmath217 .",
    "the study of automated selection of @xmath233 can be found in vast literature on tikhonov regularization .    .",
    "parameters used in algorithm 1 are ; @xmath226 , @xmath235 , @xmath227 and @xmath228 .",
    "the regularization parameter @xmath233 was selected manually according to the noise level @xmath217 .",
    ", title=\"fig:\",height=170 ] .",
    "parameters used in algorithm 1 are ; @xmath226 , @xmath235 , @xmath227 and @xmath228 .",
    "the regularization parameter @xmath233 was selected manually according to the noise level @xmath217 .",
    ", title=\"fig:\",height=170 ] .",
    "parameters used in algorithm 1 are ; @xmath226 , @xmath235 , @xmath227 and @xmath228 .",
    "the regularization parameter @xmath233 was selected manually according to the noise level @xmath217 .",
    ", title=\"fig:\",height=170 ] .",
    "parameters used in algorithm 1 are ; @xmath226 , @xmath235 , @xmath227 and @xmath228 .",
    "the regularization parameter @xmath233 was selected manually according to the noise level @xmath217 .",
    ", title=\"fig:\",height=170 ] .",
    "parameters used in algorithm 1 are ; @xmath226 , @xmath235 , @xmath227 and @xmath228 .",
    "the regularization parameter @xmath233 was selected manually according to the noise level @xmath217 .",
    ", title=\"fig:\",height=170 ] .",
    "parameters used in algorithm 1 are ; @xmath226 , @xmath235 , @xmath227 and @xmath228 .",
    "the regularization parameter @xmath233 was selected manually according to the noise level @xmath217 .",
    ", title=\"fig:\",height=170 ] .",
    "parameters used in algorithm 1 are ; @xmath226 , @xmath235 , @xmath227 and @xmath228 .",
    "the regularization parameter @xmath233 was selected manually according to the noise level @xmath217 .",
    ", title=\"fig:\",height=170 ] .",
    "parameters used in algorithm 1 are ; @xmath226 , @xmath235 , @xmath227 and @xmath228 .",
    "the regularization parameter @xmath233 was selected manually according to the noise level @xmath217 .",
    ", title=\"fig:\",height=170 ]      consider the inverse medium problem ; determine the potential function @xmath236 in @xmath237 from measurement @xmath238 of the potential @xmath214 .",
    "the problem can be casted as a constrained least square problem ; find @xmath202 @xmath239 subject to @xmath240 with a priori upper bound @xmath241 , where @xmath204 is the solution to .",
    "one can calculate @xmath209 using the adjoint equation @xmath242 i.e. , @xmath243 in the computation , we use the function in fig.[fig : ip ] ( top left ) as the unknown potential to be recovered .",
    "all the noisy data @xmath213 was generated by adding a random noise to the exact data @xmath214 : @xmath244 where @xmath214 was computed by solving the equation @xmath245 with @xmath246 .",
    "the noise free data @xmath214 is depicted in fig.[fig : ip ] ( top right ) .",
    "+ as the preconditioner in algorithm 1 , we used @xmath247 . since @xmath248 ,",
    "the matrix @xmath249 in step 1 is diagonal .",
    "hence , the computation of the decent direction @xmath52 is cheap but one faces the slow convergence of the algorithm due to the poorly chosen preconditioner .",
    "more than 1000 time iteration was required to meet the stopping criterion @xmath250 in each test .    .",
    "parameters used in algorithm 1 are ; @xmath251 , @xmath247 , @xmath227 and @xmath228 .",
    "the regularization parameter @xmath233 was selected manually according to the noise level @xmath217 .",
    ", title=\"fig:\",height=170 ] .",
    "parameters used in algorithm 1 are ; @xmath251 , @xmath247 , @xmath227 and @xmath228 .",
    "the regularization parameter @xmath233 was selected manually according to the noise level @xmath217 .",
    ", title=\"fig:\",height=170 ] .",
    "parameters used in algorithm 1 are ; @xmath251 , @xmath247 , @xmath227 and @xmath228 .",
    "the regularization parameter @xmath233 was selected manually according to the noise level @xmath217 .",
    ", title=\"fig:\",height=170 ] .",
    "parameters used in algorithm 1 are ; @xmath251 , @xmath247 , @xmath227 and @xmath228 .",
    "the regularization parameter @xmath233 was selected manually according to the noise level @xmath217 .",
    ", title=\"fig:\",height=170 ] .",
    "parameters used in algorithm 1 are ; @xmath251 , @xmath247 , @xmath227 and @xmath228 .",
    "the regularization parameter @xmath233 was selected manually according to the noise level @xmath217 .",
    ", title=\"fig:\",height=170 ] .",
    "parameters used in algorithm 1 are ; @xmath251 , @xmath247 , @xmath227 and @xmath228 .",
    "the regularization parameter @xmath233 was selected manually according to the noise level @xmath217 .",
    ", title=\"fig:\",height=170 ]",
    "the implicit fixed point iteration proposed in section 2 is also applicable to the optimization problem involving @xmath252 ( or the sum @xmath253 ) in the objective function , for instance , @xmath254_{i } ) + \\phi([d_y u]_{i } ) \\right ) + \\eta_2 \\sum_{i=1}^{n^2 } \\phi ( [ h u]_{i}),\\ ] ] where @xmath255 is a given noisy image , @xmath256\\in { \\mathbb{r}}^{n^2}$ ] , @xmath257 , @xmath258 represent finite differences in @xmath39 and @xmath214 direction and @xmath259 denotes a discrete laplacian . the problem is obtained by discretizing the multi - parameter nonsmooth tikhonov regularization for a denoising problem @xmath260 here @xmath261 are regularization parameters which must be appropriately selected in order to obtain a desired reconstructed image @xcite .",
    "we define @xmath262 , the regularization of @xmath263 , by @xmath264 \\\\ \\\\",
    "{ \\displaystyle}\\frac{s^2}{2\\epsilon}+\\frac{\\epsilon}{2 } & s \\in [ -\\epsilon,0 ] \\\\ \\\\ -s & s \\le-\\epsilon . \\end{array } \\right.\\ ] ] the derivative is written as @xmath265 , and one follows the similar argument in section 2 to arrive at the successive iteration algorithm : @xmath266 here the diagonal matrix @xmath267 for @xmath268 is defined by @xmath269_{j , j}= \\frac{1}{\\max(\\epsilon,\\vert v_j\\vert ) } ~,\\ ] ] and @xmath131 is a regularization of @xmath270 ; @xmath271_{i } ) + \\phi_\\epsilon([d_y u]_{i } ) \\right ) + \\eta_2 \\sum_{i=1}^{n^2 } \\phi_\\epsilon ( [ h u]_{i}).\\ ] ] another example that the proposed method can handle includes the denosing problem by total variation : @xmath272 let @xmath89 be a function defined for @xmath273 by @xmath274    s &   \\epsilon \\le s    \\end{array}\\right.\\quad    \\phi^\\prime_\\epsilon(s ) = \\left\\{\\begin{array}{cc }   { \\displaystyle}\\frac{s } { \\epsilon } &   0\\le s\\le \\epsilon \\\\[10pt ]    1 &   \\epsilon \\le s    \\end{array}\\right.\\ ] ] the regularized objective functional takes the form @xmath275 let @xmath276 be a discretization of the second term @xmath277 ^ 2_i + [ d_yu]_i^2 } ) \\delta x",
    "\\delta y=   \\sum_i   \\phi_\\epsilon ( r_i ) \\delta x \\delta y\\ ] ] where @xmath278 ^ 2_i + [ d_yu]_i^2}$ ] .",
    "the derivative of @xmath276 @xmath279_i d_x(i , k ) + [ d_yu]_i d_y(i , k)}{r_i } \\\\ & = \\sum_i \\frac{r_i}{\\max(\\epsilon , r_i)}\\frac{1}{r_i}\\left([d_xu]_i d_x(i , k ) + [ d_yu]_i d_y(i , k )   \\right)\\end{aligned}\\ ] ] thus we have @xmath280 here the diagonal matrix @xmath281 is defined by @xmath282_{i , i}= \\frac{1}{\\max(\\epsilon , r_i)}.\\ ] ] from the observation we arrive at the successive iteration algorithm for solving the nonlinear equation @xmath283 : @xmath284 the details of the method and the numerical tests will be reported elsewhere .                                                  , _ contact problems in elasticity : a study of variational inequalities and finite element methods _ , vol .",
    "8 of siam studies in applied mathematics , society for industrial and applied mathematics ( siam ) , philadelphia , 1988 ."
  ],
  "abstract_text": [
    "<S> we propose an implicit iterative algorithm for an exact penalty method arising from inequality constrained optimization problems . </S>",
    "<S> a rapidly convergent fixed point method is developed for a regularized penalty functional . </S>",
    "<S> the applicability and feasibility of the proposed method is demonstrated using large scale inequality constrained problems .    * . * + * . * </S>"
  ]
}