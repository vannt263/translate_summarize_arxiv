{
  "article_text": [
    "let @xmath0 be a finite - dimensional linear vector space with the dual space @xmath1 as the space of all linear function on @xmath0 .",
    "we assume @xmath2 is a proper , @xmath3-strongly convex ( @xmath4 for strongly convex case and @xmath5 for convex case ) , and lower semicontinuous function satisfying f(x)-f(y ) _ *",
    "l _ x - y^   x , y v , where @xmath6 denotes the gradient of @xmath7 at @xmath8 for @xmath9 $ ] or any subgradient of @xmath7 at @xmath8 ( @xmath10 ) for @xmath11 .",
    "let the function @xmath12 be simple , proper , @xmath13-strongly convex ( @xmath14 ) , and lower semicontinuous function .",
    "we consider the structured convex minimization problem    ll &   h(x):=f(x)+(x ) + &   x c ,    where @xmath15 is a simple , nonempty , closed , and convex set . by ( [ e.holder ] ) , we have @xmath16 , i.e. , @xmath7 can be smooth with lipschitz continuous gradients ( @xmath17 ) , weakly smooth problems with hlder continuous gradients ( @xmath180,1[}$ ] ) , or nonsmooth with bounded variation of subgradients ( @xmath19 ) .",
    "hence the objective @xmath20 is @xmath21-strongly convex with @xmath22 .",
    "we assume that the first - order black - box oracle of the objective @xmath20 is available .      over the past few decades , due to the dramatic increase in the size of data for many applications , first - order methods",
    "have been received much attention thanks to their simple structures and low memory requirements .",
    "the efficiency of first - order methods can be poor ( a large number of function values and subgradients is needed ) for solving the general convex problems if the structure of the problem is not available . as a result , to develop practically appealing schemes ,",
    "it is necessary to make an additional restriction on problem classes . in particular",
    ", developing efficient methods for solving large - scale convex optimization problems is possible if the underlying objective has a suitable structure and the domain is simple enough .",
    "convexity and level of smoothness are two important factors playing key roles in construction of efficient schemes for such structured optimization problems .",
    "let @xmath23 be an optimizer of ( [ e.gfun ] ) and @xmath24 be an approximate solution given by a first - order method .",
    "we call @xmath24 an @xmath25-solution of ( [ e.gfun ] ) if @xmath26 , for a prescribed accuracy parameter @xmath27 . in 1983 , nemirovski & yudin in @xcite derived optimal worst - case complexities for first - order methods to achieve an @xmath25-solution for several classes of convex problems ( see table [ t.complexity ] ) .",
    "if a first - order scheme attains the worst - case complexity of a class of problems , it is called optimal .",
    "a special feature of these methods is that the corresponding complexity does not depend explicitly on the problem dimension . from practical point of view , studying the effect of an uniform boundedness of the complexity is very attractive and such methods are highly recommended when the prescribed accuracy @xmath25 is not too small , whereas the dimension of problem is considerably large .",
    "[ t.complexity ]    .list of the best known complexities of first - order methods for several classes of problems with respect to levels of smoothness and convexity ( cf .",
    "@xcite ) [ cols=\"<,^,^ \" , ]",
    "in this paper , we propose several novel ( sub)gradient methods for solving large - scale convex composite minimization .",
    "more precisely , we give two estimation sequences approximating the objective function with some local and global information of the objective . for each of the estimation sequences , we give two iterative schemes attaining the optimal complexities for smooth , nonsmooth , weakly smooth , and smooth strongly convex problems .",
    "these schemes are optimal up to a logarithmic factors for nonsmooth strongly convex problems , and for weakly smooth strongly convex problems they attain a much better complexity than the complexity for weakly smooth convex problems . for each estimation sequence ,",
    "the first scheme needs to know about the level of smoothness and the hlder constant , while the second one is parameter - free ( except for the strong convexity parameter which we set zero if it is not available ) at the price of applying a backtracking line search .",
    "we then consider solutions of the auxiliary problems appearing in these four schemes and study the important cases appearing in applications that can be solved efficiently either in a closed form or by a simple iterative scheme . considering some applicationsin the fields of sparse optimization and machine learning , we report numerical results showing the encouraging behavior of the proposed schemes . + * acknowledgement .",
    "* i would like to thank arnold neumaier for his useful comments on this paper .",
    "golub , t. , slonim , d. , tamayo , p. , huard , c. , gaasenbeek , m. , mesirov , j. , coller , h. , loh , m. , downing , j. , caligiuri , m. : molecular classification of cancer : class discovery and class prediction by gene expression monitoring , science 286 , 531536 ( 1999 )                            nesterov , y. : a method of solving a convex programming problem with convergence rate @xmath29 , doklady an sssr ( in russian ) , 269 ( 1983 ) , 543547 .",
    "english translation : soviet math . dokl .",
    ", * 27 * , 372376 ( 1983 )"
  ],
  "abstract_text": [
    "<S> this paper discusses several ( sub)gradient methods attaining the optimal complexity for smooth problems with lipschitz continuous gradients , nonsmooth problems with bounded variation of subgradients , weakly smooth problems with hlder continuous gradients . </S>",
    "<S> the proposed schemes are optimal for smooth strongly convex problems with lipschitz continuous gradients and optimal up to a logarithmic factor for nonsmooth problems with bounded variation of subgradients . more specifically , we propose two estimation sequences of the objective and give two iterative schemes for each of them . in both cases , </S>",
    "<S> the first scheme requires the smoothness parameter and the hlder constant , while the second scheme is parameter - free ( except for the strong convexity parameter which we set zero if it is not available ) at the price of applying a nonmonotone backtracking line search . a complexity analysis for all the proposed schemes </S>",
    "<S> is given . </S>",
    "<S> numerical results for some applications in sparse optimization and machine learning are reported , which confirm the theoretical foundations .    </S>",
    "<S> example.eps gsave newpath 20 20 moveto 20 220 lineto 220 220 lineto 220 20 lineto closepath 2 setlinewidth gsave .4 setgray fill grestore stroke grestore </S>"
  ]
}