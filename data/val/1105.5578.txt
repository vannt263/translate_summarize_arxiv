{
  "article_text": [
    "the problem of variable selection has received a huge amount of attention over the last 15 years , motivated by the desire to understand structure in massive data sets that are now routinely encountered across many scientific disciplines .",
    "it is now very common , e.g. in biological applications , image analysis and portfolio allocation problems as well as many others , for the number of variables ( or predictors ) @xmath1 that are measured to exceed the number of observations @xmath2 . in such circumstances , variable selection is essential for model interpretation .    in a notable recent contribution to the now vast literature on this topic , @xcite proposed stability selection as a very general technique designed to improve the performance of a variable selection algorithm .",
    "the basic idea is that instead of applying one s favourite algorithm to the whole data set to determine the selected set of variables , one instead applies it several times to random subsamples of the data of size @xmath3 , and chooses those variables that are selected most frequently on the subsamples .",
    "stability selection is therefore intimately connected with bagging @xcite and subagging @xcite .",
    "a particularly attractive feature of stability selection is the error control provided by an upper bound on the expected number of falsely selected variables ( * ? ? ?",
    "* theorem  1 ) .",
    "such control is typically unavailable when applying the original selection procedure to the whole data set , and allows the practitioner to select the threshold @xmath4 for the proportion of subsamples for which a variable must be selected in order for it to be declared significant .    however , the bound does have a couple of drawbacks .",
    "firstly , it applies to the ` population version ' of the subsampling process , i.e. to the version of the procedure that aggregates results over the non - random choice of all @xmath5 subsamples . even for @xmath2 as small as 15 , it is unrealistic to expect this version to be used in practice , and in fact choosing around @xmath6 random subsamples is probably typical .",
    "more seriously , the bound is derived under a very strong exchangeability assumption on the selection of noise variables ( as well as a weak one on the quality of the original selection procedure , namely that it is not worse than random guessing ) .    in this paper",
    ", we develop the methodology and conceptual understanding of stability selection in several respects .",
    "we introduce a variant of stability selection , where the subsamples are drawn as complementary pairs from @xmath7 .",
    "thus the subsampling procedure outputs index sets @xmath8 , where each @xmath9 is a subset of @xmath7 of size @xmath3 , and @xmath10 .",
    "we call this variant complementary pairs stability selection ( cpss ) .    at first glance",
    "it would seem that cpss would be expected to yield very similar results to the original version of stability selection .",
    "however , we show that cpss in fact has the following properties :    a.   the meinshausen  bhlmann bound holds for cpss regardless of the number of complementary pairs @xmath11 chosen  even with @xmath12 . b.   there is a corresponding bound for the number of important variables excluded by cpss . c.   our results have no conditions on the original selection procedure , and in particular",
    "do not require the strong exchangeability assumption on the selection of noise variables .",
    "indeed , we argue that even a precise definition of ` signal ' and ` noise ' variables is not helpful in trying to understand the properties of cpss , and we instead state the bounds in terms of the expected number of variables chosen by cpss that have low selection probability under the base selection procedure , and the expected number of high selection probability variables that are excluded by cpss .",
    "see section  [ sec : cpss ] for further discussion .",
    "d.   the bound on the number of low selection probability variables chosen by cpss can be significantly sharpened under mild shape restrictions ( e.g. unimodality or @xmath0-concavity ) on the distribution of the proportion of times a variable is selected in both @xmath13 and @xmath14 .",
    "we discuss these conditions in detail in sections  [ sec : improvedtail ] and  [ sec : r_concave ] respectively , and compare both the original and new bounds to demonstrate the marked improvement .",
    "our improved bounds are based on new versions of markov s inequality that hold for random variables whose distributions are unimodal or @xmath0-concave .",
    "however , it is important to note at this point that the results are not just a theoretical contribution ; they allow the practitioner to reduce @xmath4 ( and therefore select more variables ) for the same control of the number of low selection probability variables chosen by cpss . in section  [ sec :",
    "practice ] , we give recommendations on how a practitioner can make use of the bounds in applying cpss .    in section  [ sec :",
    "simstudy ] , we present the results of an extensive simulation study designed to illustrate the appropriateness of our shape restrictions , and to compare stability selection and cpss with their base selection procedures",
    ".    a review of some of the extensive literature on variable selection can be found in  @xcite .",
    "work related more specifically to stability selection includes @xcite , who studied the bolasso ( short for bootstrapped enhanced lasso ) .",
    "this involves applying the lasso to bootstrap ( with replacement ) samples from the original data , rather than subsampling without replacement .",
    "a final estimate is obtained by applying the lasso to the intersection of the set of variables selected across the bootstrap samples .",
    "various authors , particularly in the machine learning literature , have considered the _ stability _ of a feature selection algorithm , i.e. the insensitivity of the output of the algorithm to variations in the training set ; such studies include @xcite , @xcite , @xcite , @xcite and @xcite .",
    "@xcite consider obtaining a final feature ranking by aggregating the rankings across bootstrap samples .",
    "in order to keep our discussion rather general , we only assume that we have vector - valued data @xmath15 which we take to be a realisation of independent and identically distributed random elements @xmath16 .",
    "informally , we think of some of the components of @xmath17 as being ` signal variables ' , and others as being ` noise variables ' , though for our purposes it is not necessary to define these notions precisely . formally , we let @xmath18 and @xmath19 , thought of as the index sets of the signal and noise variables respectively .",
    "a _ variable selection procedure _ is a statistic @xmath20 taking values in the set of all subsets of @xmath21 , and we think of @xmath22 as an estimator of @xmath23 . as a typical example , we may often write @xmath24 with the covariate @xmath25 and the response @xmath26 , and our ( pseudo ) log - likelihood might be of the form @xmath27 for some @xmath28 . in this context , we regard @xmath29 as the signal indices , @xmath30 as noise indices .",
    "examples from graphical modelling can also be cast within our framework .",
    "note however that we do not require a ( pseudo ) log - likelihood of the form  ( [ eq : loglike ] ) .",
    "we define the selection probability of a variable index @xmath31 under @xmath22 as @xmath32 we take the view that for understanding the properties of stability selection , the selection probabilities @xmath33 are the fundamental quantities of interest .",
    "since an application of stability selection is contingent on a choice of base selection procedure @xmath22 , all we can hope is that it selects variables having high selection probability under the base procedure , and avoids selecting those variables with low selection probability .",
    "indeed this turns out to be the case ; see theorem  [ thm : hi_low ] below .    of course",
    ", @xmath34 has a bernoulli distribution with parameter @xmath33 , so we may view @xmath34 as an unbiased estimator of @xmath33 ( though @xmath33 is not a model parameter in the conventional sense ) .",
    "the key idea of stability selection is to improve on this simple estimator of @xmath33 through subsampling .    for a subset @xmath35 with @xmath36 ,",
    "we shall write @xmath37    let @xmath8 be randomly chosen independent pairs of subsets of @xmath7 of size @xmath3 such that @xmath10 . for @xmath38 $ ] , the complementary pairs stability selection version of a variable selection procedure @xmath22 is @xmath39 , where the function @xmath40 is given by @xmath41    note that @xmath42 is an unbiased estimator of @xmath43 , but , in general , a biased estimator of @xmath33 .",
    "however , by means of the averaging involved in , we hope that @xmath42 will have reduced variance compared with @xmath34 , and that this increased stability will more than compensate for the bias incurred . indeed , this is the case in other situations where bagging and subagging have been successfully applied , such as classification trees @xcite or nearest neighbour classifiers @xcite .",
    "an alternative to subsampling complementary pairs would be to use bootstrap sampling .",
    "we have found that this gives very similar estimates of @xmath33 , though most of our theoretical arguments do not apply when the bootstrap is used ( the approach in section  [ sec : lowertau ] is an exception in this regard ) .",
    "in fact , taking subsamples of size @xmath3 can be thought of as the subsampling scheme that most closely mimics the bootstrap ( e.g. * ? ? ?",
    "it is convenient at this stage to define another related selection procedure based on sample splitting .",
    "let @xmath8 be randomly chosen independent pairs of subsets of @xmath7 of size @xmath3 such that @xmath10 . for @xmath38 $ ] ,",
    "the simultaneous selection version of @xmath22 is @xmath44 , where @xmath45    for our purposes , simultaneous selection is a tool for understanding the properties of cpss .",
    "however , the special case of @xmath46 of simultaneous selection was studied by @xcite , and a variant involving all possible disjoint pairs of subsets was considered in @xcite .",
    "in theorem  [ thm : hi_low ] below , we show that the expected number of low selection probability variables chosen by cpss is controlled in terms of the expected number chosen by the original selection procedure , with a corresponding result for the expected number of high selection probability variables not chosen by cpss .",
    "the appealing feature of these results is their generality : they require no assumptions on the underlying model or on the quality of the original selection procedure , and they apply regardless of the number @xmath11 of complementary pairs of subsets chosen .    for @xmath47 $ ] ,",
    "let @xmath48 denote the set of variable indices that have low selection probability under @xmath49 , and let @xmath50 denote the set of those that have high selection probability .",
    "[ thm : hi_low ]    a.   if @xmath51 $ ] , then @xmath52 b.   let @xmath53 and @xmath54 .",
    "if @xmath55 , then @xmath56    in many applications , and for a good base selection procedure , we imagine that the set of selection probabilities @xmath57 is positively skewed in @xmath58 $ ] , with many selection probabilities being very low ( predominantly noise variables ) , and with just a few being large ( including at least some of the signal variables ) . to illustrate theorem  [ thm : hi_low](i ) , consider a situation with @xmath59 variables and where the base selection procedure chooses 50 of them",
    ". then theorem  [ thm : hi_low](i ) shows that on average cpss with @xmath60 selects no more than a quarter of the below average selection probability variables chosen by @xmath49 .",
    "our theorem  [ thm : hi_low](i ) is analogous to theorem  1 of @xcite .",
    "the differences are that we do not require the condition that @xmath61 is exchangeable , nor that the original procedure is no worse than random guessing , and our result holds for all @xmath11 .",
    "the price we pay is that the bound is stated in terms of the expected number of low selection probability variables chosen by cpss , rather than the expected number of noise variables , which we do for the reasons described in section  [ sec : cpss ] .",
    "if the exchangeability and random guessing conditions mentioned above do hold , then , writing @xmath62 , we recover @xmath63 the final bound here was obtained in theorem  1 of @xcite for the population version of stability selection .      despite the attractions of theorem  [ thm : hi_low ] , the following observations suggest there may be scope for improvement .",
    "firstly , we expect we should be able to obtain tighter bounds as @xmath11 increases .",
    "secondly , and more importantly , examination of the proof of theorem  [ thm : hi_low](i ) shows that our bound relies on first noting that @xmath64 and then applying markov s inequality to @xmath65 .",
    "for equality in markov s inequality , @xmath65 must be a mixture of point masses at @xmath66 and @xmath67 , but figure  [ fig : compare_dist ] suggests that the distribution of @xmath65 , which is supported on @xmath68 , can be very different from this .",
    "indeed , our experience , based on extensive simulation studies , is that when @xmath69 is close to @xmath70 ( which is where the bound in theorem  [ thm : hi_low](i ) is probably of most interest ) , the distribution of @xmath65 over @xmath71 is remarkably consistent over different data generating processes , and figure  [ fig : compare_dist ] is typical .    [ ] [ ] [ _ plots_9_4]0.0 [ ] [ ] [ _ plots_9_4]0.2 [ ] [ ] [ _ plots_9_4]0.4 [ ] [ ] [ _ plots_9_4]0.6 [ ] [ ] [ _ plots_9_4]0.8 [ ] [ ] [ _ plots_9_4]1.0 [ ] [ ] [ _ plots_9_4]0 [ ] [ ] [ _ plots_9_4]2 [ ] [ ] [ _ plots_9_4]4 [ ] [ ] [ _ plots_9_4]6 [ ] [ ] [ _ plots_9_4]8 [ ] [ ] [ _ plots_9_4]10 [ ] [ ] [ _ plots_9_4]1.2 [ ] [ ] [ _ plots_9_4]0 [ ] [ ] [ _ plots_9_4]50 [ ] [ ] [ _ plots_9_4]100 [ ] [ ] [ _ plots_9_4]150 [ ] [ ] [ _ plots_9_4]200 [ ] [ ] [ _ plots_9_4]250 [ ] [ ] [ 0.8]probability [ ] [ ] [ 0.8]@xmath72 [ ] [ ] [ 0.8]@xmath73 [ ] [ ] [ 0.7]worst case [ ] [ ] [ 0.7]unimodal [ ] [ ] [ 0.7]@xmath74-concave [ ] [ ] [ 0.7]empirical   for @xmath75 ( black ) , alongside the unrestricted , unimodal and @xmath76-concave distributions respectively ( grey ) , which have maximum tail probability beyond 0.2 .",
    "this situation corresponds to selecting @xmath60 .",
    "bottom left : the observed mass function ( circles ) and the extremal @xmath76-concave mass function ( crosses ) on the @xmath77 scale .",
    "bottom right : tail probabilities from 0.2 onwards for each of the distributions.,title=\"fig : \" ]    it is therefore natural to consider placing shape restrictions on the distribution of @xmath65 which encompass what we see in practice , and which yield stronger versions of markov s inequality .",
    "as a first step in this direction , we consider the assumption of unimodality .",
    "[ thm : unimodal ] suppose that the distribution of @xmath65 is unimodal for each @xmath71 . if @xmath78 , then @xmath79 where , when @xmath80 , @xmath81 $ } \\\\   \\dfrac{4(1- \\tau + 1/2b)}{1 + 1/b } & \\mbox{if $ \\tau \\in ( \\frac{3}{4},1]$}. \\end{array } \\right.\\ ] ]    the proof of theorem  [ thm : unimodal ] is based on a new version of markov s inequality ( theorem  [ thm : unimarkov ] in the appendix ) for random variables with unimodal distributions supported on a finite lattice .",
    "there is also an explicit expression for @xmath82 when @xmath83 , which follows from theorem  [ thm : unimarkov ] in the same way , but we do not present it here because it is a little more complicated , and because we anticipate the bound when @xmath69 is ( much ) smaller than @xmath84 being of most use in practice . see section  [ sec : practice ] for further discussion .",
    "figure  [ fig : bounds_compare ] compares the bounds provided by theorems  [ thm : hi_low ] and theorem  [ thm : unimodal ] as a function of @xmath4 , for the illustration discussed after the statement of theorem  [ thm : hi_low ] .",
    "[ ] [ ] [ _ bounds_compare.ps]0.4 [ ] [ ] [ _ bounds_compare.ps]0.5 [ ] [ ] [ _ bounds_compare.ps]0.6 [ ] [ ] [ _ bounds_compare.ps]0.7 [ ] [ ] [ _ bounds_compare.ps]0.8 [ ] [ ] [ _ bounds_compare.ps]0.9 [ ] [ ] [ _ bounds_compare.ps]1.0 [ ] [ ] [ _ bounds_compare.ps]0 [ ] [ ] [ _ bounds_compare.ps]5 [ ] [ ] [ _ bounds_compare.ps]10 [ ] [ ] [ _ bounds_compare.ps]15 [ ] [ ] [ 0.9]bound on @xmath85 [ ] [ ] [ 0.9]@xmath4      the unimodal assumption allows for a significant improvement in the bounds attainable from a naive application of markov s inequality . however , figure  [ fig : compare_dist ] suggests that further gains may be realised by placing tighter constraints on the family of distributions for @xmath65 that we consider , in order to match better the empirical distributions that we see in practice .",
    "a very natural constraint to impose on the distribution of @xmath65 is log - concavity . by this , we mean that , if @xmath86 denotes the probability mass function of @xmath65 , then the linear interpolant to @xmath87 is a log - concave function on @xmath58 $ ] .",
    "log - concavity is a shape constraint that has received a great deal of attention recently ( e.g. @xcite ) , and at first sight it seems reasonable in our context , because if the summands in were independent , then we would have @xmath88 , which is log - concave .",
    "it is indeed possible to obtain a version of markov s inequality under log - concavity that leads to another improvement in the bound on @xmath89 .",
    "however , we found that in practice , the dependence structure of the summands in meant that the log - concavity constraint was a little too strong .",
    "we therefore consider instead the class of _ @xmath0-concave _ distributions , which we claim defines a continuum of constraints that interpolate between log - concavity and unimodality ( see propositions  [ prop : log_iff._r ] and  [ prop : uni_implies_r ] below ) .",
    "this constraint has also been studied recently in the context of density estimation by @xcite and @xcite ; see also @xcite .    to define the class , we recall that the @xmath90 generalised mean @xmath91 of @xmath92 is given by @xmath93 for @xmath94 .",
    "this is also well - defined for @xmath95 if we take @xmath96 when @xmath97 , and define @xmath98 .",
    "in addition , we may define @xmath99 we are now in a position to define @xmath0-concavity .",
    "a non - negative function @xmath86 on an interval @xmath100 is _ @xmath0-concave _",
    "if for every @xmath101 and @xmath102 , we have @xmath103    a probability mass function @xmath86 supported on @xmath68 is _ @xmath0-concave _ if the linear interpolant to @xmath87 is @xmath0-concave .    when @xmath104 , it is easy to see that @xmath86 is @xmath0-concave if and only if @xmath105 is convex .",
    "let @xmath106 denote the class of @xmath0-concave probability mass functions on @xmath68 .",
    "then each @xmath107 is unimodal , and as @xmath91 is non - decreasing in @xmath0 for fixed @xmath108 and @xmath109 , we have @xmath110 for @xmath111 .",
    "furthermore , @xmath86 is unimodal if it is @xmath112-concave , and @xmath86 is log - concave if it is @xmath66-concave .",
    "the following two results further support the interpretation of @xmath0-concavity for @xmath113 $ ] as an interpolation between log - concavity and unimodality .",
    "[ prop : log_iff._r ] a function @xmath86 is log - concave if and only if it is @xmath0-concave for every @xmath95 .",
    "[ prop : uni_implies_r ] let @xmath86 be a unimodal probability mass function supported on @xmath68 and suppose both that @xmath114 and that @xmath115 , for some @xmath116",
    ". then @xmath86 is @xmath0-concave for some @xmath104 .    in proposition  [ prop : rbound ] in the appendix",
    ", we present a result that characterises those @xmath0-concave distributions that attain equality in a version of markov s inequality for random variables with @xmath0-concave distributions on @xmath68 .",
    "if we assume that @xmath65 is @xmath0-concave for all @xmath71 , using , for these variables we can obtain a bound of the form @xmath117 where @xmath118 denotes the maximum of @xmath119 over all @xmath0-concave random variables supported on @xmath68 with @xmath120 .",
    "although @xmath121 does not appear to have a closed form , it is straightforward to compute numerically , as we describe in section  [ sec : alg ] .",
    "the lack of a simple form means a direct analogue theorem  [ thm : unimodal ] is not available .",
    "we can nevertheless obtain the following bound on the expected number of low selection probability variables chosen by cpss : @xmath122 our simulation studies suggest that @xmath123 is a sensible choice to use for the bound . in other words , if @xmath86 denotes the probability mass function of @xmath65 , then the linear interpolant to @xmath124 is typically well approximated by a convex function .",
    "this is illustrated in the bottom left panel of figure  [ fig : compare_dist ] ( note that the right - hand tail in this plot corresponds to tiny probabilities ) .",
    "the bounds obtained thus far have used the relationship to convert a markov bound for @xmath65 into a corresponding one for the statistic of interest , @xmath42 .",
    "the advantage of this approach is that @xmath125 is much smaller than @xmath126 for variables with low selection probability , so the markov bound is quite tight .",
    "however , for @xmath4 close to @xmath127 , the inequality starts to become weak , and bounds can only be obtained for @xmath128 in any case .    to solve this problem , we can apply our versions of markov s inequality directly to @xmath42 .",
    "we have found , through our simulations , that for variables with low selection probability , the distribution of @xmath42 can be modelled very well as a @xmath129-concave distribution ( see figure  [ fig : minus_quarter_concave ] ) . that the distribution of @xmath42 is closer to log - concavity than that of @xmath65 is intuitive because",
    "although the summands in are not independent , terms involving subsamples which have little overlap will be close to independent .",
    "if we assume that @xmath65 is @xmath76-concave and that @xmath42 is @xmath129-concave for all @xmath71 , we can obtain our best bound @xmath130 which is valid for all @xmath131 $ ] , provided we adopt the convention that @xmath132 for @xmath133 .",
    "the resulting improvements in the bounds can been seen in figure  [ fig : bounds_compare ] .",
    "note the kink in figure  [ fig : bounds_compare ] for the @xmath0-concave bound just before @xmath60 .",
    "this corresponds to the transition from where @xmath134 is smaller to where @xmath135 is smaller .",
    "we applied the algorithm described in section  [ sec : alg ] to produce tables of values of @xmath136 over a grid of @xmath69 and @xmath4 values ; see table  [ tab : r_conc1 ] and table  [ tab : r_conc2 ] .",
    "[ ] [ ] [ _ comp_plots_9_2_2]0.0 [ ] [ ] [ _ comp_plots_9_2_2]0.2 [ ] [ ] [ _ comp_plots_9_2_2]0.4 [ ] [ ] [ _ comp_plots_9_2_2]0.6 [ ] [ ] [ _ comp_plots_9_2_2]0.8 [ ] [ ] [ _ comp_plots_9_2_2]1.0 [ ] [ ] [ _ comp_plots_9_2_2]0.1 [ ] [ ] [ _ comp_plots_9_2_2]0.3 [ ] [ ] [ _ comp_plots_9_2_2]0.5 [ ] [ ] [ _ comp_plots_9_2_2]5 [ ] [ ] [ _ comp_plots_9_2_2]10 [ ] [ ] [ _ comp_plots_9_2_2]15 [ ] [ ] [ _ comp_plots_9_2_2]20 [ ] [ ] [ _ comp_plots_9_2_2]25 [ ] [ ] [ 0.8]probability [ ] [ ] [ 0.8]@xmath137      the quantities @xmath138 and @xmath139 , which appear on the right hand sides of the bounds , will in general be unknown to the statistician .",
    "thus when using the bounds , they will typically need to be replaced by @xmath1 and @xmath140 respectively .",
    "in addition , several parameters must be selected , and in this section we go through each of these in turn and give guidance on how to choose them .    [ [ choice - of - b . ] ] choice of @xmath11 .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + +    we recommend @xmath141 as a default value .",
    "choosing @xmath11 larger than this increases the computational burden , and may lead to the @xmath0-concavity assumptions being violated .",
    "[ [ choice - of - theta . ] ] choice of @xmath69 .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    as mentioned at the beginning of section  [ sec : improvedtail ] , @xmath142 is a natural choice . in other words ,",
    "we regard the below average selection probability variables as the irrelevant variables",
    ". other choices of @xmath69 are possible , but the use of and to construct the bound suggests that the inequality will be tightest when most of the variables have a selection probability close to @xmath69 .    [ [ choice - of - q - and - threshold - tau . ] ] choice of @xmath140 and threshold @xmath4 .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    one can regard the choice of @xmath143 ( which is usually fixed through a tuning parameter @xmath144 ) as part of the choice of the base selection procedure .",
    "one option is to fix @xmath140 by varying @xmath144 at each evaluation of the selection procedure until it selects @xmath140 variables .",
    "however , if the number of variables selected at each iteration is unknown in advance ( e.g. if @xmath144 is fixed , or if cross - validation is used to choose @xmath144 at each iteration ) , then @xmath140 can be estimated by @xmath145 .    an important point to note",
    "is that although choosing @xmath144 or @xmath140 is usually crucial when carrying out variable selection , this is not the case when using cpss .",
    "our experience is that the performance of cpss is surprisingly insensitive to the choice of @xmath140 ( see also @xcite ) .",
    "that is to say , @xmath146 does not vary much as @xmath140 varies , and also the final selected sets for different values of @xmath140 tend to be similar ( where different thresholds are chosen to control the selection of variables in @xmath146 at a pre - specified level ) .",
    "thus , when using cpss , it is the threshold @xmath4 that plays a role similar to that of a tuning parameter for the base procedure .",
    "the great advantage of cpss is that our bounds allow one to choose @xmath4 to control the expected number of low selection probability variables selected .    to summarise : we recommend as a sensible default cpss procedure taking @xmath141 and @xmath142 .",
    "we then choose @xmath4 using the bound  ( [ eq : finalbound ] ) with @xmath138 replaced by @xmath1 to control the expected number of low selection probability variables chosen .",
    "in this section we investigate the performance and validity of the bounds derived in the previous section by applying cpss to simulated data .",
    "we consider both linear and logistic regression and different values of @xmath1 and @xmath2 . in each of these settings ,",
    "we first generate independent explanatory vectors @xmath147 with each @xmath148 .",
    "we use a toeplitz covariance matrix @xmath149 with entries @xmath150 and we look at various values of @xmath151 in @xmath152 .",
    "so the correlation between the components decays exponentially with the distance between them in @xmath153 .    for linear regression",
    ", we generate a vector of errors @xmath154 and set @xmath155 where the design matrix @xmath156 has @xmath157 row @xmath158 .",
    "the error variance @xmath159 is chosen to achieve different values of the signal - to - noise ratio ( snr ) , which we define here by @xmath160    for logistic regression , we generate independent responses @xmath161 where @xmath162 here @xmath163 is a scaling factor which is chosen to achieve a particular bayes error rate .    in both cases ,",
    "we fix the @xmath1-dimensional vector of coefficients @xmath164 to have @xmath165 non - zero components , @xmath166 of which we choose as equally spaced points within @xmath167 $ ] with the remaining @xmath166 equally spaced in @xmath168 $ ] . the indices of the non - zero components , @xmath23 ,",
    "are chosen to follow a geometric progression up to rounding , with first term 1 and @xmath169 term @xmath170 .",
    "the values are then randomly assigned to each index in @xmath23 , but this choice is then fixed for each particular simulation setting .    with @xmath171 , this setup will have several signal variables correlated amongst themselves , and also some signal correlated with noise . in this way , the framework above includes a very wide variety of different data generating processes on which we can test the theory of the previous section .    by varying the base selection procedure , its tuning parameters , the values of @xmath151 , @xmath2 , @xmath1 , @xmath172 and also the snr and bayes error rates , we have applied cpss in several hundred different simulation settings . for reasons of space , we present only a subset of these numerical experiments below , but the results from those omitted are not qualitatively different .    in the graphs which follow ,",
    "we look at cpss applied to the lasso @xcite , which we implemented using the package ` glmnet ` @xcite in ` r ` @xcite .",
    "we follow the original stability selection procedure put forward in @xcite and compare this to the method suggested by our @xmath0-concave bound . thus we first choose the level @xmath173 at which we wish to control the expected number of low selection probability variables ( so we aim to have @xmath174 ) .",
    "then we fix @xmath175 and set the threshold @xmath4 at 0.9 .",
    "this ensures that , according to the original worst case bound , we control the expected number of low selection probability variables selected at the required level . in the @xmath0-concave case , we take our threshold as @xmath176 we also give the results one would obtain using the lasso alone , but with the benefit of an oracle which knows the optimal value of the tuning parameter @xmath144 .",
    "that is , we take @xmath177 as our selected set , where @xmath178 and @xmath179 is the selected set when using the lasso with tuning parameter @xmath144 applied to the whole data set .",
    "we present all of our results relative to the performance of cpss using an oracle - driven threshold @xmath180 , where @xmath180 is defined by @xmath181 referring to figures  [ fig : gaussian1]-[fig : binomial2 ] , the heights of the black bars , grey bars and crosses are given by @xmath182 respectively .",
    "thus the heights of the black and grey bars relate to the loss of power in using the threshold suggested by the corresponding bounds . in all of our simulations , we used @xmath141 .",
    "each scenario was run 500 times , and in order to determine the set @xmath146 , in each scenario , we applied the particular selection procedure @xmath49 to 50,000 independent data sets .",
    "it is immediately obvious from the results that using the @xmath0-concave bound , we are able to recover significantly more variables in @xmath23 than when using the the worst case bound . furthermore , though it is not shown in the graphs explicitly , we also achieve the required level of error control in all but one case ( where the @xmath0-concavity assumption fails ) .",
    "in fact the one particular example is hardly exceptional in that we have @xmath183 .",
    "thus in close accordance with our theory , there are no significant violations of the @xmath0-concave bound .",
    "we also see that the loss in power due to using @xmath184 rather than @xmath180 , is very low . in almost all of the scenarios , we are able to select more than 75% of the signal we could select with the benefit of an oracle , and usually much more than this .",
    "it is interesting that the performance of the oracle cpss and oracle lasso procedures are fairly similar .",
    "the key advantage of cpss is that it allows for error control whereas there is in general no way of determining ( or even approximating ) the optimal @xmath185 that achieves the required error control .",
    "in fact , the performance of cpss with our bound is only slightly worse then that of the oracle lasso procedure , and in a few cases , particularly when @xmath151 is small , it is even slightly better . in the cases",
    "where @xmath186 , we see that cpss is not quite as powerful .",
    "this is because having such large correlations between variables causes @xmath187 to be relatively spread out in @xmath58 $ ] . as explained in section  [ sec : practice ] , we expect our bound to weaken in this situation .",
    "however , even when the correlation is as high as 0.9 , we recover a sizeable proportion of the signal we would select had we used the optimal @xmath180 .      here",
    "we illustrate our cpss methodology on the widely studied colon data set of @xcite , freely available at ` http://microarray.princeton.edu/oncology/affydata/index.html ` .",
    "the data consist of 2000 gene expression levels from 40 colon tumour samples and 22 normal colon tissue samples , measured using affymetrix oligonucleotide arrays .",
    "our goal is to identify a small subset of genes which we are confident are linked with the development of colon cancer .",
    "such a task is important for improving scientific understanding of the disease and for selecting genes as potential drug targets .",
    "the data were first preprocessed by averaging over the expression levels for repeated genes ( which had been tiled more than once on each array ) , log - transforming each gene expression level , standardising each row to have mean zero and unit variance , and finally removing the columns corresponding to control genes , so that @xmath188 genes remained .",
    "the transformation and standardisation are very common preprocessing steps to reduce skewness in the data and help eliminate the effects of systematic variations between different microarrays ( see for example @xcite and @xcite ) .",
    "we applied cpss with @xmath189 ( lasso ) penalised logistic regression as the base procedure , with @xmath141 , and choosing @xmath4 both using the @xmath0-concave bound of section  [ sec : practice ] , and the original bound of @xcite .",
    "we estimated the expected classification error in the two cases by averaging over 128 repetitions of stratified random subsampling validation , taking 8 cancerous and 4 normal observations in each test set .",
    "thus when applying cpss , we had @xmath190 .",
    "we looked at @xmath191 and @xmath192 , and set @xmath4 to control @xmath174 with @xmath193 and @xmath194 .    rather than subsampling completely at random when using cpss , we also stratified these subsamples to include the same proportion of cancerous to normal samples as in the training data supplied to the procedure . without this step , some of the subsamples may not include any samples from one of the classes , and applying @xmath49 to such a subsample would give misleading results .",
    "using stratified random subsampling is still compatible with our theory , provided that @xmath195 is interpreted as an expectation over random data which contain the same class proportions as observed in the original data . in general , this approach of stratified random subsampling is useful when the response is categorical .",
    "the results in table  [ tab01 ] show that , as expected , the new error bounds allow one to select more variables than the conservative bounds of @xcite for the same level of error control , and as a consequence , the expected prediction error is reduced .",
    "figure  [ fig : robustness ] demonstrates the robustness of the selected set to the different values of @xmath140 .",
    "finally , we also applied cpss on the entire dataset with @xmath196 and @xmath141 and using the @xmath0-concave bound of section  [ sec : practice ] to choose @xmath4 to control @xmath197 ( cf .",
    "figure  [ fig : genemap ] ) .",
    "we see that with just 5 genes out of 1908 , we manage to separate the two classes quite well .     for @xmath193 ( left ) and @xmath198 ( right ) ,",
    "we have plotted the proportion of times a gene was selected by our @xmath0-concave cpss procedure for all genes which were selected at least 5% of the time among the 128 repetitions .",
    "solid black means the gene was selected in every repetition , and white means it was never selected .",
    "thus dark vertical lines indicate that the choice of @xmath140 has little effect on the end result of cpss . ]",
    "\\(i ) let @xmath201 be randomly chosen independent pairs of subsets of @xmath7 of size @xmath3 such that @xmath10 .",
    "then @xmath202 now @xmath203 because @xmath204 and @xmath205 are independent conditional on @xmath206 .",
    "it follows using that @xmath207 where we have used markov s inequality in the final step .",
    "\\(ii ) define @xmath208 and @xmath209 by replacing @xmath22 with @xmath210 in the definitions of @xmath211 and @xmath212 respectively . then , using the bound corresponding to and markov s inequality again , @xmath213        the proof of theorem  [ thm : unimodal ] requires several preliminary results , and we use the following notation .",
    "let @xmath217 denote the finite lattice @xmath218 $ ] .",
    "if @xmath86 is a probability mass function on @xmath217 , we write @xmath219 for @xmath220 , thereby associating @xmath86 with @xmath221 .    for @xmath222",
    ", we denote the probability that a random variable distributed according to @xmath86 takes values greater than or equal to @xmath223 by @xmath224 .",
    "we also write @xmath225 for the expectation of this random variable and @xmath226 for the support of @xmath86 .",
    "let @xmath227 be the set of all unimodal probability mass functions @xmath86 on @xmath217 , and let @xmath228 .",
    "we consider the problem of maximising @xmath229 over @xmath230 .",
    "since the cases @xmath231 and @xmath232 are trivial , there is no loss of generality in assuming throughout that @xmath233 and @xmath222 , so in particular @xmath234 .",
    "since @xmath236 is linear and therefore continuous , it suffices to show that @xmath237 is closed and bounded .",
    "now @xmath235 is bounded as @xmath238^{b+1}$ ] . moreover",
    ", the hyperplane @xmath239 is closed . also , @xmath240 is a continuous function on @xmath241 , so @xmath242)$ ] is closed .",
    "now let @xmath243 .",
    "if @xmath244 then there must exist @xmath245 such that @xmath246 .",
    "clearly this inequality must hold for all @xmath247 in a sufficiently small open ball about @xmath86 , so @xmath248 is open .",
    "we see that @xmath249 ) \\cap o^c .\\ ] ] thus @xmath235 is an intersection of closed sets and hence is closed .",
    "[ prop : ineq ] suppose that @xmath250 and @xmath251 satisfy @xmath252 and that there exists some @xmath253 with @xmath254 for all @xmath255 and @xmath256 for all @xmath257 .",
    "then @xmath258 with equality if and only if @xmath259 for @xmath260 .",
    "the following result characterises the extremal elements of @xmath235 in the sense of maximising the tail probability @xmath229 .",
    "in particular , it shows that such extremal elements can take only one of two simple forms .",
    "\\(i ) suppose @xmath262 maximises @xmath229 , but that @xmath269 .",
    "define @xmath270 . as @xmath271",
    ", we must have @xmath272 .",
    "define @xmath247 by @xmath273 where @xmath274 are chosen such that @xmath275 , but are small enough that @xmath276 . then @xmath277 but @xmath278 , a contradiction .",
    "\\(ii ) suppose first that there exists a mode of @xmath279 which is at least @xmath223 .",
    "let @xmath277 be such that @xmath280 for @xmath281 and @xmath282 for @xmath283 .",
    "as @xmath284 , we can apply proposition  [ prop : ineq ] to see that @xmath285 but @xmath286 , so by optimality of @xmath279 we must have equality in .",
    "thus proposition  [ prop : ineq ] gives us that @xmath287 .",
    "next , define @xmath288 by @xmath289 for @xmath283 , @xmath290 , and @xmath291 for @xmath292 . then @xmath293 .",
    "again proposition  [ prop : ineq ] and the optimality of @xmath279 give that @xmath294 .",
    "thus @xmath279 satisfies property ( ii)(b ) of the theorem .",
    "now suppose that there is no mode of @xmath279 which is at least @xmath223 , so @xmath295 .",
    "let @xmath277 satisfy @xmath280 for @xmath281 and @xmath296 .",
    "we must have @xmath297 , otherwise @xmath279 would have a mode at @xmath223 . as @xmath286 , optimality of @xmath279 and proposition  [ prop : ineq ]",
    "imply @xmath298 .",
    "finally , let @xmath288 satisfy @xmath289 for @xmath299 and @xmath300 , where @xmath301 and @xmath302 are chosen such that @xmath303 .",
    "as before , proposition  [ prop : ineq ] allows us to deduce that @xmath294 .",
    "thus @xmath279 satisfies property ( ii)(a ) of the theorem .",
    "[ thm : unimarkov ] let @xmath156 be a random variable with a unimodal distribution on @xmath304 , and let @xmath222 .",
    "if @xmath305 , then @xmath306 \\\\                                   \\dfrac{\\eta}{2t-\\frac{1}{b } } & \\text { if } \\;\\ ; t \\in \\left(\\min\\left(\\tfrac{3}{2}\\eta + \\tfrac{1}{2b } , 2\\eta\\right ) , \\ , \\tfrac{1}{2}\\right ] \\\\                                                         \\dfrac{2\\eta(1- t + \\frac{1}{b})}{1 + \\frac{1}{b } } & \\text { if } \\;\\ ; t \\in \\left(\\tfrac{1}{2 } , \\ , 1\\right ] .",
    "\\end{array } \\right.\\ ] ] let @xmath307 be defined by @xmath308 if @xmath309 and @xmath310 , then @xmath311 \\\\                                                                \\dfrac{2\\eta(1- t + \\frac{1}{b})}{1 + \\frac{1}{b } } & \\text { if } \\;\\ ; t \\in \\left(\\tfrac{1}{2 } + \\tfrac{1}{4\\eta } ( 1 + \\tfrac{1}{b } - d^{1/2 } ) , \\ ,",
    "1\\right ] .",
    "\\end{cases}\\ ] ] finally , if @xmath309 and @xmath312 , then @xmath313    proposition  [ prop : ubound ] tells us that @xmath119 must be at most the maximum of the optimal solutions to the following two optimisation problems : @xmath314 problem @xmath315 corresponds to case ( ii)(a ) of proposition  [ prop : ubound ] , and problem @xmath316 to case ( ii)(b ) .",
    "the solution to @xmath316 is determined entirely by the constraints , and we see that the optimal value is @xmath317 to solve @xmath315 , we break it into @xmath318 subproblems : for @xmath319 , we define subproblem @xmath320 as follows : @xmath321 notice that we have not included the @xmath322 constraint .",
    "this is because proposition  [ prop : ubound ] ensures that this constraint is always satisfied at an optimal solution of @xmath315 , so there exists @xmath323 such that every optimal solution of @xmath324 corresponds to an optimal solution of @xmath315 .",
    "now each subproblem is a standard linear programming problem , so we know that one of the basic feasible solutions must be optimal . since @xmath325 , all basic feasible solutions must have either @xmath326 or @xmath327 .",
    "thus we may replace the subproblems @xmath320 by @xmath328 the second constraint is enough to determine that the optimal value of @xmath329 is @xmath330 now we can proceed to find an @xmath323 which maximises @xmath163 over @xmath331 .",
    "the sign of @xmath332 is the sign of @xmath333 this quadratic in @xmath172 has roots @xmath334",
    "so @xmath335 is increasing for all @xmath336 with @xmath337 when @xmath338 , we must have @xmath339 .",
    "in fact , by examining , we see that @xmath340 .",
    "also , from , we see that when @xmath341 , we have that @xmath342 , so @xmath343 .",
    "so far , we have shown that @xmath344 where bounds @xmath345 and @xmath346 are given by @xmath347    all that remains now is to determine which of @xmath345 and @xmath346 have the largest value .",
    "we first consider the case when @xmath348 .",
    "when @xmath349 , @xmath350 now for @xmath351 , @xmath352 furthermore , @xmath353 putting this together gives the required bound for @xmath354 .",
    "of theorem  [ thm : unimodal ] recalling that @xmath357 , we follow the proof of lemma  [ lemma : selprob ] , but apply theorem  [ thm : unimarkov ] at the last step of with @xmath358 to deduce that if the distribution of @xmath359 is unimodal , then @xmath360 where @xmath82 is given in the statement of theorem  [ thm : unimodal ] .",
    "the bound for @xmath89 then follows in the same way that theorem  [ thm : hi_low ] follows from lemma  [ lemma : selprob ] .",
    "@xmath361      of proposition  [ prop : log_iff._r ] suppose that @xmath86 is log - concave , so we may write @xmath362 where @xmath363 is a convex function . if @xmath104 , then @xmath364 is convex , and as the exponential function is increasing and convex , @xmath365 is convex .",
    "conversely , suppose that @xmath86 is not log - concave , so there exist @xmath366 and @xmath102 with @xmath367 .",
    "then as @xmath368 as @xmath369 , we must have @xmath370 for some @xmath95 , and so @xmath86 can not be @xmath0-concave . @xmath361    of proposition  [ prop : uni_implies_r ] let @xmath371 .",
    "the conditions on @xmath86 imply that @xmath372 then as @xmath373 as @xmath374 , for each @xmath375 , may choose an @xmath376 with @xmath377 set @xmath378 .",
    "observe that as @xmath379 is increasing in @xmath0 for all fixed @xmath108 and @xmath109 , the inequalities are all satisfied when @xmath380 .",
    "thus @xmath381 for all @xmath382 , so @xmath86 is @xmath0-concave .",
    "@xmath361        this proof is almost identical to that of lemma  [ lemma : u_exists ] , except here we let @xmath385 .",
    "if @xmath244 , then there must exist @xmath245 such that @xmath386 and it is clear that the above inequality must hold for all @xmath247 in a sufficiently small open ball about @xmath86 .",
    "thus @xmath248 is open , and the rest of the proof is clear .",
    "\\(i ) suppose that @xmath269 .",
    "define @xmath392 .",
    "let @xmath393 and define a new sequence @xmath394 by @xmath395 where @xmath274 are chosen such that @xmath396 , but are small enough that @xmath397 .",
    "then @xmath398 is convex , so @xmath399 . since @xmath400",
    ", we must have @xmath401 so @xmath402 .",
    "also , as we are assuming @xmath403 , we must have @xmath404 .",
    "therefore @xmath405 , which is a contradiction .",
    "\\(ii ) set @xmath406 , so @xmath363 is convex and @xmath407 .",
    "define @xmath408 as follows .",
    "take @xmath409 for @xmath281 , but make @xmath410 linear between @xmath411 and @xmath412 such that @xmath413 has @xmath414 and @xmath415 .",
    "this is possible since @xmath416 , so @xmath417 .",
    "note that @xmath410 is still convex since we must have @xmath418 .",
    "also @xmath419 .",
    "applying proposition  [ prop : ineq ] , we see that @xmath420 .",
    "optimality of @xmath279 means that equality must hold , so @xmath287 and also @xmath421 .",
    "now if @xmath363 is in fact linear between @xmath422 and @xmath423 , condition ( ii ) of the theorem is satisfied and we are done .",
    "otherwise we may assume @xmath363 is not a linear function between @xmath424 and @xmath423 and we can define @xmath398 such that @xmath425 for @xmath299 , that @xmath398 is linear between @xmath426 and @xmath427 and @xmath428 for @xmath429 . here",
    ", @xmath301 is chosen such that @xmath430 has @xmath275 , and the convexity of @xmath363 ensures that such a @xmath431 exists .",
    "applying proposition  [ prop : ineq ] , we see that @xmath420 . since @xmath286 , as before , optimality of @xmath279 allows us to conclude that @xmath287 .      here",
    "we describe a numerical algorithm that computes the function @xmath121 defined in section  [ sec : r_concave ] .",
    "note that this is the maximum of @xmath432 over @xmath433 .",
    "we shall only discuss the case where @xmath279 is decreasing , as is always the case when @xmath434 .",
    "the increasing case is very similar and less important for our application .",
    "we first note that we may parametrise the @xmath0-concave probability mass functions whose @xmath435 powers are linear as follows : @xmath436 where @xmath431 . as @xmath437 is strictly increasing in @xmath108 , for each @xmath301 , there is a unique @xmath438 for which @xmath439 . we also note here that @xmath438 decreases with @xmath301 .",
    "this is easily seen by observing that , regardless of the value of @xmath301 , the parameter @xmath108 in determines the ratio of @xmath440 to @xmath441 , each @xmath442 .    according to proposition  [ prop : rbound ] , if @xmath387 maximises @xmath229 , then @xmath388 is linear up to its penultimate support point .",
    "we can parametrise these in the following way .",
    "write @xmath443 and then solve for @xmath444 : @xmath445 we see that as @xmath108 ranges through @xmath446 $ ] , we obtain all the relevant probability mass functions supported on @xmath447 via @xmath448 the tail probability of @xmath449 , when the threshold is @xmath223 , is @xmath450 and we may maximise this over @xmath451 $ ] to obtain an optimal @xmath452 for each @xmath301 .",
    "this is easily accomplished using a general purpose optimiser such as ` optimize ` in ` r ` . to summarise , we have the following simple procedure for computing @xmath453 .      then @xmath458 .",
    "when we wish to evaluate @xmath453 for a range of values of @xmath223 , the process is simplified by the observation that @xmath459 is increasing in @xmath223 , and thus in step 2 we need only consider those @xmath301 which are at least @xmath460 .",
    "using the algorithm described above , we have computed @xmath461 over a grid of @xmath69 and @xmath4 values ( cf .",
    "tables  [ tab : r_conc1 ] and  [ tab : r_conc2 ] ) .",
    "an ` r ` implementation of the algorithm is available from both authors websites .    75 alon , u. , barkai , n. , notterman , d.a . , gish , k. , ybarra , s. , mack , d. and levine , a.j .",
    "broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide arrays .",
    "usa , * 96 * , 6745-6750 .",
    "lange , t. , braun , m. , roth , v. and buhmann , j. ( 2003 ) stability - based model selection . in s. becker , s. thrun , and k. obermayer ( eds . ) , _ advances in neural information processing systems _ ,",
    "* 15 * ( pp .  617624 ) .",
    "cambridge , ma : mit press .",
    "loscalzo , s. , yu , l. and ding , c. ( 2009 ) consensus group based stable feature selection . in _ proc .",
    "15th acm sigkdd international conference on knowledge discovery and data mining ( kdd-09 ) _ , pp ."
  ],
  "abstract_text": [
    "<S> stability selection was recently introduced by @xcite as a very general technique designed to improve the performance of a variable selection algorithm . </S>",
    "<S> it is based on aggregating the results of applying a selection procedure to subsamples of the data . </S>",
    "<S> we introduce a variant , called complementary pairs stability selection ( cpss ) , and derive bounds both on the expected number of variables included by cpss that have low selection probability under the original procedure , and on the expected number of high selection probability variables that are excluded . </S>",
    "<S> these results require no ( e.g. exchangeability ) assumptions on the underlying model or on the quality of the original selection procedure . under reasonable shape restrictions , </S>",
    "<S> the bounds can be further tightened , yielding improved error control , and therefore increasing the applicability of the methodology .    </S>",
    "<S> key words : complementary pairs stability selection , @xmath0-concavity , subagging , subsampling , variable selection    _ plots_9_40.7 _ bounds_compare.ps0.8 _ comp_plots_9_2_20.8 </S>"
  ]
}