{
  "article_text": [
    "a boolean network ( bn ) is a discrete dynamical system , which is , for example , used to study and model a variety of biochemical networks such as genetic regulatory networks .",
    "bns have been introduced in the late 1960s by kauffman @xcite who proposed to study random bns as models of gene regulatory networks .",
    "kauffman investigated their dynamical behavior and a phenomena called self - organization . aside from its original purpose , bns were also used to model ( small - scale ) genetic regulatory networks ; for example , in @xcite , it was demonstrated that bns are capable of reproducing the underlying biological processes ( i.e. , the cell cycle ) well .",
    "bns are also used to model large - scale networks , such as the _ escherichia coli _ regulatory network @xcite which is analyzed in section  [ sec : ecoli ] .",
    "this network is , in contrast to kauffman s automata and the regulatory networks considered in @xcite , not an autonomous system , since the gene s states are determined by external factors .    in the literature addressing the analysis of bns , it is common to consider measures that quantify the effect of perturbations .",
    "whether a random bn operates in the so called ordered or disordered regime is determined by whether a single perturbation , i.e. , flipping the state of a node , is expected to spread or die out eventually .",
    "kauffman @xcite argues that biological networks must operate at the border of the ordered and disordered regime ; hence , they must be tolerant to perturbations to some extent .",
    "in contrast to measures of perturbations , determinative power in bns has not received much attention , even though there are several settings where such a notion is of interest .",
    "for example , given a feed forward network where the states of the nodes are controlled by the states of nodes in the input layer , we might ask whether a possibly small set of inputs suffices to determine most states , i.e. , reduces the uncertainty about the network s states significantly .",
    "this can be addressed by quantifying the determinative power of the input nodes .",
    "for example , in the _",
    "e. coli _ regulatory network , it turns out that a small set of metabolites and other inputs determine most genes that account for _ e. coli _ s metabolism ( see section  [ sec : ecoli ] ) .    in this paper",
    ", we view the state of each node in the network as an independent random variable .",
    "this modeling assumption applies for networks with a tree - like topology , e.g. , a feed forward network , and is often applied when studying the effect of perturbations . for this setting , determinative power of nodes and perturbation - related measures are properties of single functions ; hence , the analysis of the bn reduces to the analysis of single functions .",
    "our main tool for the theoretical results is fourier analysis of boolean functions .",
    "fourier analytic techniques were first applied to bns by kesseli et al .",
    "@xcite . in @xcite , results related to derrida plots and convergence of trajectories in random bns were derived .",
    "ribeiro et al .",
    "@xcite considered the pairwise mutual information in time series of random bns , under a different setup that we use .",
    "specifically , in @xcite , the functions are random ; whereas here , the functions are deterministic , but the argument is random . finally , note that part of this paper was presented at the 2012 international workshop on computational systems biology @xcite .      mutual information between a set of inputs to a node and",
    "the state of this node is a measure of the determinative power of this set of inputs , as mutual information quantifies mutual dependence of random variables . in order to understand the determinative power and mutual dependencies in boolean networks",
    ", we systematically study the mutual information of sets of inputs and the state of a node .",
    "we relate mutual information to a measure of perturbations and prove that ( maybe surprisingly ) a set of inputs that is highly sensitive to perturbations might not necessarily have determinative power .",
    "conversely , a set of inputs which has determinative power must be sensitive to perturbations . to prove those results ,",
    "we show that the concentration of weight in the fourier domain on certain sets of inputs characterizes a function in terms of tolerance to perturbations and determinative power of input nodes .",
    "furthermore , we generalize a result by xiao and massey @xcite , which gives a necessary and sufficient condition of statistical independence of a set of inputs and a function s output in terms of the fourier coefficients .",
    "this result can for instance be applied to decide for which classes of functions the algorithm presented in @xcite , which detects functional dependencies based on estimating mutual information , can succeed or fails . for _ unate _ functions , we show that any input and the function s output are statistically dependent and provide a direct relation between the mutual information and the influence of a variable .",
    "the class of unate functions is especially relevant for biological networks , as it includes all linear threshold functions and all nested canalizing functions , and describes functional dependencies in gene regulatory networks well @xcite .",
    "as an application of the theoretical results in this paper , we show that mutual information can be used to identify the determinative nodes in the large - scale model of the control network of _",
    "e. coli _ s metabolism @xcite .",
    "the paper is organized as follows .",
    "boolean networks and fourier analysis of boolean functions are reviewed in section  [ sec : preliminaries ] . in section  [ sec : inf ] , the influence and average sensitivity as measures of perturbations are reviewed , and their relation to the fourier spectrum is discussed . in section  [ sec : mi ] , we study the mutual information of sets of inputs and the function s output .",
    "section  [ sec : unate ] is devoted to unate functions .",
    "section  [ sec : ecoli ] contains an analysis of the large - scale _ e. coli _ regulatory network , using the tools and ideas developed in previous sections .",
    "we start with a short introduction to boolean networks and fourier analysis of boolean functions , and introduce notation .",
    "a ( synchronous ) bn can be viewed as a collection of @xmath3 nodes with memory .",
    "the state of a node @xmath1 is described by a binary state @xmath4 at discrete time @xmath5 . choosing the alphabet to be @xmath6 rather than @xmath7 as more common in the literature on bns will turn out to be advantageous later . however , both choices are equivalent",
    ". the state of the network at time @xmath8 can be described by the vector @xmath9 \\in \\{-1,+1\\}^{n}$ ] .",
    "the network dynamic is defined by @xmath10 where @xmath11 is the boolean function associated with node @xmath1 . at time @xmath12 , an initial state @xmath13 is chosen .",
    "in general , not all arguments @xmath14 of a function @xmath15 need to be _",
    "relevant_. the variable @xmath16 is said to be relevant for @xmath17 if there exists at least one @xmath18 , such that changing @xmath19 to @xmath20 changes the function s value . in most of the bn models in biology ,",
    "the functions depend on a small subset of their arguments only .",
    "furthermore , not every state must have a function associated with it ; states can also be external inputs to the network .    to study the determinative power and tolerance to perturbations , a probabilistic setup is needed . in our analysis , we assume that each state is an independent random variable @xmath21 with distribution @xmath22}}$ ] , @xmath23 .",
    "the assumption of independence holds for networks with tree - like topology , but is not feasible for networks with strong local dependencies and feedback loops .",
    "however , in many relevant settings , a bn has a tree - like topology , for instance the _ e. coli _ network analyzed in section  [ sec : ecoli ] . for a network with few local dependencies",
    ", assuming independence will lead to a small modeling error .",
    "major results concerning the analysis of bns have been obtained under the assumptions as stated above , e.g. , the annealed approximation @xcite , an important result on the spread of perturbations in random bns .",
    "several important results on random bns , e.g. , @xcite , let the network size @xmath24 tend to infinity ; hence , there are no local dependencies .",
    "we use @xmath25 $ ] for the set @xmath26 , and all sets are subsets of @xmath25 $ ] . with @xmath27 , we mean the sum over all sets @xmath28 that are subsets of @xmath29 . throughout this paper , we use capital letters for random variables , e.g. , @xmath30 , and lower case letters for their realizations , e.g. , @xmath31 .",
    "boldface letters denote vectors , e.g. , @xmath32 is a random vector , and @xmath33 its realization . for a vector @xmath33 and a set @xmath34",
    "$ ] , @xmath35 denotes the subvector of @xmath33 corresponding to the entries indexed by @xmath29 .      in the following ,",
    "we give a short introduction to fourier analysis of boolean functions .",
    "let @xmath36 be a binary , product distributed random vector , i.e. , the entries of @xmath32 are independent random variables @xmath37 $ ] with distribution @xmath38 } } , x_i \\in \\{-1,+1\\}$ ] . throughout this paper , probabilities @xmath39}}$ ] and expectations @xmath40}{\\ensuremath{\\underset{{}}{\\mathbb e}\\left [ \\cdot\\right]}}}$ ]",
    "are with respect to the distribution of @xmath32 .",
    "we denote @xmath41}}$ ] , the variance of @xmath42 by @xmath43 , its standard deviation by @xmath44 and finally @xmath45}{\\ensuremath{\\underset{{}}{\\mathbb e}\\left [ x_i\\right]}}}$ ] . the inner product of @xmath46 with respect to the distribution of @xmath32 is defined as @xmath47}{\\ensuremath{\\underset{{}}{\\mathbb e}\\left [ f({\\ensuremath{\\mathbf { x } } } ) g({\\ensuremath{\\mathbf { x}}})\\right ] } } } = \\!\\!\\!\\ !",
    "\\sum_{{\\ensuremath{\\mathbf { x } } } \\in \\{-1,1\\}^n } \\!\\!\\ ! { \\ensuremath { { \\mathrm{p}\\!}\\left[{\\ensuremath{\\mathbf { x}}}={\\ensuremath{\\mathbf { x}}}\\right ] } }   f({\\ensuremath{\\mathbf { x } } } ) g({\\ensuremath{\\mathbf { x } } } ) \\label{eq : inner_prod}\\ ] ] which induces the norm @xmath48 .",
    "an orthonormal basis with respect to the distribution of @xmath32 is @xmath49 \\setminus \\emptyset \\label{eq : basisfunc}\\ ] ] and @xmath50 this basis was first proposed by bahadur @xcite .",
    "thus , each boolean function @xmath51 can be uniquely expressed as @xmath52 } { \\hat f } ( s ) { \\phi}_s({\\ensuremath{\\mathbf { x } } } ) , \\label{eq : fourier_expansion}\\ ] ] where @xmath53 are the fourier coefficients of @xmath54 .",
    "note that is a representation of @xmath54 as a multilinear polynomial . as an example , consider the and2 function defined as @xmath55 if and only if @xmath56 , and let @xmath57 .",
    "according to @xmath58 as a second example consider parity2 , i.e. , the xor function , defined as @xmath59 if @xmath56 or if @xmath60 , and @xmath61 for all other choices of @xmath33 .",
    "written as a polynomial , @xmath62 .",
    "we conclude this section by listing properties of the basis functions which are used frequently throughout this paper .",
    "* decomposition : * let @xmath34 $ ] and @xmath63 , and denote @xmath64 .",
    "then , @xmath65    * orthonormality : * for @xmath66 $ ] , @xmath67}{\\ensuremath{\\underset{{}}{\\mathbb e}\\left [ { \\phi}_a({\\ensuremath{\\mathbf { x } } } )   { \\phi}_b({\\ensuremath{\\mathbf { x}}})\\right ] } } } =   \\begin{cases } 1 , \\text{if } a = b \\\\                 0 , \\text{otherwise}. \\end{cases}\\ ] ]    * parseval s identity : * for @xmath68 , @xmath69}{\\ensuremath{\\underset{{}}{\\mathbb e}\\left [ f({\\ensuremath{\\mathbf { x}}})^2\\right ] } } } = { \\ensuremath{{\\|f\\|}_{{}}}}^2 = \\sum_{s \\subseteq [ n ] } { \\hat f } ( s)^2 = 1.\\ ] ]",
    "next , we discuss measures of perturbations and their relation to the fourier spectrum .",
    "we start with a measure of the perturbation of a single input .",
    "define the influence of variable @xmath1 on the function @xmath54 as @xmath70}},\\ ] ] where @xmath71 is the vector obtained from @xmath33 by flipping its @xmath1th entry .    by definition ,",
    "the influence of variable @xmath1 is the probability that perturbing , i.e. , flipping , input @xmath1 changes the function s output .",
    "influence can be viewed as the capability of input @xmath1 to change the output of @xmath54 . in bns , usually ,",
    "the sum of all influences , i.e. , the _ average sensitivity _ is studied .",
    "the average sensitivity of @xmath54 to the variables in the set @xmath29 is defined as @xmath72 the average sensitivity of @xmath54 is defined as @xmath73 .",
    "@xmath74 captures whether flipping an input chosen uniformly at random from @xmath29 affects the function s output .",
    "most commonly , all inputs are taken into account , i.e. , the average sensitivity @xmath75 is studied . as an example , @xmath76 and @xmath77 ; hence , parity2 is more sensitive to single perturbations than and2 . influence and average sensitivity",
    "have the following convenient expressions in terms of fourier coefficients .    for any boolean function @xmath54 ,",
    "@xmath78\\colon i\\in s } { \\hat f } ( s)^2 .",
    "\\label{eq : infinteroffcoef}\\end{aligned}\\ ] ] [ prop : influenc_spectrum ]    for any boolean function @xmath54 , @xmath79 } { \\hat f } ( s)^2 \\sum _ { i\\in s \\cap a } \\frac{1}{\\sigma_i^2}. \\label{eq : av_fourier}\\ ] ] [ pr : av_fourier ]    proposition  [ pr : av_fourier ] follows directly from proposition  [ prop : influenc_spectrum ] and the definition of @xmath74 . from",
    ", we see that @xmath75 is large if the fourier weight is concentrated on the coefficients of high degree @xmath80 , i.e. , if @xmath81 is large ( i.e. , close to one ) . for this case",
    ", parseval s identity implies that the @xmath82 with @xmath83 must be small .",
    "let s see an example : suppose @xmath84 and consider the and3 function , i.e. , @xmath85 if and only if @xmath86 . @xmath87 is tolerant to perturbations since @xmath88 , and as figure [ fig1 ] shows , its spectrum is concentrated on the coefficients of low degree .",
    "in contrast for @xmath89 , @xmath90 . hence , parity3 is maximally sensitive to perturbations .",
    "figure [ fig1 ] shows that its spectrum is maximally concentrated on the coefficient of highest degree .",
    "according to @xmath75 is small only if the fourier weight is concentrated on the coefficients of low degree .",
    "this is the case either if @xmath54 is strongly biased ( i.e. , if @xmath91 , for most inputs @xmath33 , where @xmath92 is a constant ) or if @xmath54 depends on few variables only .",
    "this is in accordance with the results of kauffman @xcite ; he found that a random bn operates in the ordered regime if the functions in the network depend on average on few variables .",
    "we will state our result for measures of single perturbations .",
    "however , these results also apply to other noise models , specifically to the _ noise sensitivity _ of @xmath54 .",
    "that is , because the noise sensitivity of @xmath54 is small if @xmath54 is tolerant to single perturbations .",
    "the noise sensitivity of a boolean function is defined as the probability that the function s output changes if each input is flipped independently with probability @xmath93 .",
    "for uniformly distributed @xmath32 , @xmath94 is an upper bound for the noise sensitivity ; for small values of @xmath93 , @xmath95 approximates the noise sensitivity well . for the @xmath42 being equally but possibly nonuniformly distributed and a slightly different noise model , it was found in @xcite that @xmath96 still upper bounds the noise sensitivity .",
    "this result was generalized to product distributed @xmath32 in @xcite .",
    "in this section , we study the determinative power of a subset of variables @xmath97 , where @xmath97 consists of the entries of @xmath32 corresponding to the indices in the set @xmath34 $ ] , over the function s output @xmath98 . as a measure of determinative power , we take the mutual information @xmath99 between @xmath98 and @xmath97 , since @xmath99 quantifies the statistical dependence between the random variable @xmath97 and @xmath98 .",
    "hence , this section is devoted to the study of @xmath99 .    before giving a formal definition of mutual information ,",
    "let us start with an example .",
    "consider the parity2 function and let its inputs @xmath100 be uniformly distributed . intuitively ,",
    "if @xmath101 has determinative power , knowledge about @xmath101 should provide us with information about @xmath102 .",
    "suppose we know the value of @xmath101 , say @xmath103 .",
    "since @xmath104 , we have with @xmath105}}=1/2 $ ] that @xmath106 } } = { \\ensuremath { { \\mathrm{p}\\!}\\left[f_{\\mathrm{parity2 } } = 1 |x_1=1\\right]}}$ ] .",
    "hence , knowledge of @xmath101 does not help to predict the value of @xmath102 .",
    "therefore , @xmath101 has no determinative power over @xmath102 .",
    "we indeed have @xmath107 .",
    "we next define mutual information .",
    "mutual information is the reduction of uncertainty of a random variable @xmath108 due to the knowledge of @xmath30 ; therefore , we need to define a measure of uncertainty first , which is entropy . as a reference for the following definitions ,",
    "see @xcite .",
    "the entropy @xmath109 of a discrete random variable @xmath30 with alphabet @xmath110 is defined as @xmath111 } } \\log_2   { \\ensuremath { { \\mathrm{p}\\!}\\left[x = x\\right]}}.\\ ] ]    the conditional entropy @xmath112 of a pair of discrete and jointly distributed random variables @xmath113 is defined as @xmath114 } } h ( y | x = x ) .\\ ] ]    the mutual information @xmath115 is the reduction of uncertainty of the random variable @xmath108 due to the knowledge of @xmath30 @xmath116    for a binary random variable @xmath30 with alphabet @xmath117 and @xmath118}}$ ] , we have @xmath119 , where @xmath120 is the binary entropy function , defined as @xmath121 the properties of mutual information are what we intuitively expect from a measure of determinative power : if knowledge of @xmath42 reduces the uncertainty of @xmath98 , then @xmath42 determines the state of @xmath98 to some extent , because then , knowledge about the state of @xmath42 helps in predicting @xmath98 .",
    "furthermore , we require from a measure of determinative power that not all variables can have large determinative power simultaneously .",
    "this is guaranteed for mutual information as @xmath122 which follows from the chain rule of mutual information ( as a reference , see @xcite ) and independence of the @xmath123 $ ] .",
    "hence , if @xmath124 is large , i.e. , close to 1 , we can be sure that @xmath42 has determinative power over @xmath98 since implies that @xmath125 for @xmath126 must be small then .      in order to study determinative power , its relation to measures of perturbations , and statistical dependencies , we start by characterizing the mutual information in terms of fourier coefficients .",
    "our results are based on the following novel characterization of entropy in terms of fourier coefficients .",
    "let @xmath54 be a boolean function , let @xmath32 be product distributed , and let @xmath127 be a fixed set of arguments , where @xmath128 $ ] .",
    "then , @xmath129,\\ ] ] where @xmath130 is the binary entropy function as defined in .",
    "[ th : relation_fourier_h ]    see appendix [ proof : relation_fourier_h ] . for the special case of",
    "uniformly distributed @xmath32 , a proof appears in @xcite , in the context of designing s - boxes .",
    "using the definition of mutual information , an immediate corollary of theorem  [ th : relation_fourier_h ] is the following :    let @xmath54 be a boolean function , @xmath32 be product distributed , and @xmath127 .",
    "then , @xmath131.\\ ] ] [ co : michar ]    theorem  [ th : relation_fourier_h ] ( and corollary  [ co : michar ] ) shows that the conditional entropy @xmath132 and the mutual information @xmath99 are functions of the coefficients @xmath133 only .",
    "this already hints at a fundamental difference to the average sensitivity , since the average sensitivity depends on the coefficients @xmath134 , according to .",
    "we next discuss @xmath124 based on .",
    "first , note that @xmath124 has previously been studied under the notion _ information gain _ as a measure of ` goodness ' for split variables in greedy tree learners @xcite and also under the notion of _ informativeness _ to quantify voting power @xcite . according to",
    ", the mutual information @xmath124 just depends on @xmath135 , @xmath136 , and @xmath137 .",
    "in contrast , the influence @xmath138 is a function of the coefficients @xmath139 , i\\in s \\}$ ] , according to . in figure",
    "[ fig2 ] , we depict @xmath124 for @xmath140 as a function of @xmath135 and @xmath136 .     as a function of @xmath141 and @xmath142 for @xmath143.,scaledwidth=50.0% ]    it can be seen that @xmath144 , i.e. , @xmath98 and @xmath42 are statistically independent if and only if @xmath145 .",
    "that can be formalized as follows : @xmath124 is convex in @xmath135 .",
    "this can be proven by taking the second derivative of and observing that it is larger than zero for all pairs of values ( @xmath136,@xmath135 ) for which @xmath124 is defined .",
    "next , from , we see that @xmath144 if @xmath145 ; hence , it follows that @xmath144 if and only if @xmath145 , which proves the following result :    let @xmath54 be a boolean function , and @xmath32 be product distributed . @xmath42 and @xmath98 are statistically independent if and only if @xmath145 .",
    "[ co : stat_indep_onev ]    corollary  [ co : stat_indep_onev ] also follows immediately from a more general result , namely theorem  [ le : statistical_independence_fourier_coeffs ] , which is presented later .",
    "recall that for parity2 , @xmath146 and @xmath147 ; hence , corollary  [ co : stat_indep_onev ] comes at no surprise .    from figure [ fig2 ] , it can be seen that the larger @xmath148 , the larger @xmath124 becomes .",
    "formally , it follows from the convexity of @xmath124 and corollary  [ co : stat_indep_onev ] that @xmath124 is increasing in @xmath148 .",
    "hence , @xmath42 has large determinative power , i.e. , @xmath124 is large , if and only if @xmath148 is large ( i.e. , close to one ) .",
    "@xmath148 is trivially maximized for the _ dictatorship function _ ,",
    "i.e. , for @xmath149 , or its negation , i.e. , @xmath150 .",
    "the output @xmath98 of the dictatorship function is fully determined by @xmath151 .",
    "next , let us consider the ( trivial ) case where @xmath152 $ ] and hence @xmath153 .",
    "then , @xmath154 .",
    "it follows that @xmath155 is maximized for @xmath156 , i.e , @xmath157 } } = 1/2 $ ] , i.e. , if the variance of @xmath98 is @xmath158 .",
    "in general , the closer to zero @xmath136 is , the larger the mutual information between a function s output and all its inputs becomes .",
    "let us finally relate the conditional entropy @xmath132 to the concentration of the fourier weight on the coefficients @xmath159 , @xmath128 $ ] .",
    "let @xmath54 be a boolean function , let @xmath32 be product distributed , and let @xmath127 be a fixed set of arguments , where @xmath128 $ ] .",
    "then , @xmath160 [ th : hlowerbound ]    see appendix [ proof_pr7 ] .",
    "theorem  [ th : hlowerbound ] shows that @xmath132 can be approximated with @xmath161 .",
    "it further shows that @xmath132 is small if the fourier weight is concentrated on the variables in the set @xmath29 , i.e. , if @xmath162 is close to one .",
    "in contrast , as mentioned previously , for @xmath74 , it is relevant whether the fourier weight is concentrated on the coefficients with high degree .",
    "mutual information and average sensitivity are related as follows .    for any boolean function @xmath54 , for any product distributed @xmath32 , @xmath163 with @xmath164 [ pr : inf_bound_mi ]    see appendix [ proof_pr5 ] .",
    "note that the term @xmath165 is close to zero .",
    "specifically , for any @xmath98 we have @xmath166 , and for settings of interest , @xmath165 is very close to zero , as explained in more detail in the following .",
    "theorem  [ pr : inf_bound_mi ] shows that if @xmath99 if large ( i.e. , close to one ) , @xmath54 must be sensitive to perturbations of the entries of @xmath97 .",
    "moreover , if @xmath74 is small ( i.e. , if @xmath54 is tolerant to perturbations of the entries of @xmath97 ) , then @xmath99 must be small ( i.e. , the entries of @xmath97 do not have determinative power ) .",
    "for the case that @xmath167 $ ] , theorem  [ pr : inf_bound_mi ] states that the average sensitivity @xmath75 is lower - bounded by @xmath155 minus some small term .",
    "we next discuss the special case that @xmath168 .",
    "theorem  [ pr : inf_bound_mi ] evaluated for @xmath168 yields a lower bound on the influence of a variable in terms of the mutual information of that variable , namely @xmath169 again , @xmath165 is close to zero for settings of interest , as the following argument explains .",
    "equation   will not be evaluated for small @xmath170 ; since then , @xmath98 is close to a constant function ( i.e. , close to @xmath171 or @xmath172 ) , and @xmath173 and @xmath124 must both be small ( i.e. , close to zero ) anyway . hence , is of interest when @xmath170 is large , i.e. , close to 1 ; for this case , the term @xmath174 is small ( e.g. , for @xmath175 , @xmath176 ) .",
    "observe that , according to , if @xmath124 is large , then @xmath138 is also large .",
    "that proves the intuitive idea that if an input determines @xmath98 to some extent , this input must be sensitive to perturbations .",
    "conversely , as mentioned previously , an input @xmath1 can have large influence and still @xmath177 .",
    "e.g. , for the parity2 function , we have @xmath178 and @xmath177 .",
    "interestingly , the influence also has an information theoretic interpretation .",
    "the following theorem generalizes theorem 1 in @xcite .    for any boolean function @xmath54 , for any product distributed @xmath32 , @xmath179\\setminus\\{i\\}}\\right )   } { h(x_i ) } .\\ ] ] [ pr : inf_infortheor ]    see appendix [ proof_pr4 ] . for uniformly distributed @xmath32 , a proof appears in @xcite .",
    "theorem  [ pr : inf_infortheor ] shows that the influence of a variable is a measure for the uncertainty of the function s output that remains if all variables except variable @xmath1 are set .",
    "next , we characterize statistical independence of @xmath98 and a set of its arguments @xmath97 in terms of fourier coefficients .",
    "this result generalizes a theorem derived by xiao and massey @xcite from uniform to product distributed @xmath32 .",
    "let @xmath128 $ ] be fixed , @xmath54 be a boolean function , and @xmath32 be product distributed .",
    "then , @xmath98 and the inputs @xmath180 are statistically independent if and only if @xmath181 [ le : statistical_independence_fourier_coeffs ]    see appendix [ proof_pr6 ] .",
    "for uniformly distributed @xmath32 , i.e. , @xmath182 } } = 1/2 $ ] for all @xmath183 $ ] , theorem  [ le : statistical_independence_fourier_coeffs ] has been derived by xiao and massey @xcite .",
    "note that the proof provided here is also conceptually different from the proof for the uniform case in @xcite , as it does not rely on the xiao - massey lemma .",
    "theorem  [ le : statistical_independence_fourier_coeffs ] shows that a function and small sets of its inputs are statistically independent if the spectrum is concentrated on the coefficients of high degree @xmath184 .",
    "the most prominent example is the parity function of @xmath24 variables , i.e. , @xmath185 : for uniformly distributed @xmath32 , each subset of @xmath186 or fewer arguments and @xmath187 are statistically independent .",
    "conversely , if a function is concentrated on the coefficients of low degree @xmath184 , which is the case for functions that are tolerant to perturbations , then small sets of inputs and the function s output are statistically dependent .    theorem  [ le : statistical_independence_fourier_coeffs ] also has an important implication for algorithms that detect functional dependencies in a bn based on estimating the mutual information from observations of the network s states , such as the algorithm presented in @xcite .",
    "theorem  [ le : statistical_independence_fourier_coeffs ] characterizes the classes of functions for which such an algorithm may succeed and for which it will fail .",
    "moreover , theorem  [ le : statistical_independence_fourier_coeffs ] shows that in a boolean model of a genetic regulatory network , a functional dependency between a gene and a regulator can not be detected based on statistical dependence of a regulator @xmath42 and a gene s state @xmath188 , unless the regulatory functions are restricted to those for which @xmath189 holds for each relevant input @xmath1 .",
    "in this section , we discuss unate , i.e. , locally monotone functions .",
    "a boolean function @xmath54 is said to be unate in @xmath151 if for each @xmath190 and for some fixed @xmath191 , @xmath192 holds .",
    "@xmath54 is said to be unate if @xmath54 is unate in each variable @xmath151 , @xmath193 $ ] .",
    "[ def : unate ]    each linear threshold function and nested canalizing function is unate .",
    "moreover , most , if not all , regulatory interactions in a biological network are considered to be unate .",
    "that can be deduced from @xcite , and the basic argument is the following : if an element acts either as a repressor or an activator for some gene , but never as both ( which is a reasonable assumption for regulatory interactions@xcite ) , then the function determining the gene s state is unate by definition . for unate functions ,",
    "the following property holds :    let @xmath194 be unate .",
    "then , @xmath195 , \\label{eq : condition_unate_statdep}\\ ] ] where @xmath196 is the parameter in definition  [ def : unate ] .",
    "[ pr : rel_inf_fou ]    goes along the same lines as the proof for monotone functions in lemma 4.5 of @xcite .",
    "note that conversely , if holds for each @xmath197 $ ] , @xmath54 is not necessarily unate . inserting into yields @xmath198,\\ ] ] where the expectation in is over @xmath42 .",
    "based on , the discussion from section  [ sec : reltov ] on @xmath199 applies by using @xmath135 and @xmath200 synonymously .",
    "hence , for unate functions , the mutual information @xmath199 is increasing in the influence @xmath201 .",
    "moreover , if @xmath54 is unate , and @xmath151 is a relevant variable , i.e. , a variable on which the functions actually depend on , then @xmath189 . from this fact and the same arguments as given in section  [ sec : reltov ] follows :    let @xmath202 be unate . if and only if @xmath151 is a relevant variable , then @xmath203 .",
    "[ co : stat_dep_unate ]    in a boolean model of a biological regulatory network , this implies that if the functions in the network are unate , then a regulator and the target gene must be statistically dependent .",
    "in @xcite , the authors presented a complex computational model of the _ e. coli _ transcriptional regulatory network that controls central parts of the _ e.  coli _ metabolism .",
    "the network consists of 798 nodes and 1160 edges . of the nodes ,",
    "636 represent genes and of the remaining 162 nodes , most ( 103 ) are external metabolites .",
    "the rest are stimuli , and others are state variables such as internal metabolites .",
    "the network has a layered feed - forward structure , i.e. , no feedback loops exist .",
    "the elements in the first layer can be viewed as the inputs of the system , and the elements in the following seven layers are interacting genes that represent the internal state of the system .",
    "our experiments revealed that all functions are unate ; therefore , the properties derived in section  [ sec : unate ] apply .",
    "note that all functions being unate is a special property of the network , since if functions are chosen uniformly at random , it is unlikely to sample a unate function , in particular if the number of inputs @xmath24 is large .",
    "we first identify the input nodes that have large determinative power ( we will define what that means in a network setting shortly ) and then show that a small number thereof reduces the uncertainty of the network s state significantly .",
    "specifically , we show that on average , the entropy of the node s states conditioned on a small set of determinative input nodes , is small .    to put this result into perspective , we perform the same experiment for random networks with the same and different topology as the _ e. coli _ network .",
    "we denote by @xmath204 the set of inputs of the feed forward network and assume that the @xmath42 are independent and uniformly distributed .",
    "the remaining variables are denoted by @xmath205 and are a function of the inputs and the network s states , i.e. , @xmath206 . for our analysis , the distributions of the random variables @xmath207 need to be computed , since some of those variables are arguments to other functions .",
    "this can be circumvented by defining a collapsed network , i.e. , a network where each state of a node is given as a function of the input nodes only , i.e. , @xmath208 .",
    "the collapsed network is obtained by consecutively inserting functions into each other , until each function only depends on states of nodes in the input layer , i.e. , on @xmath32 .",
    "the collapsed network reveals the dependencies of each node on the input variables .",
    "interestingly , in the collapsed network , it is seen that the variables chol_xt@xmath2090 , salicylate , 2ddglcn_xt@xmath2090 , mnnh@xmath2090 , altrh@xmath2090 , and his - l_xt@xmath2090 ( here , and in the following , we adopt the names from the original dataset ) , which appear to be inputs when considering the original _ e. coli _ network , turn out to be not .",
    "consider , for example , the node salicylate .",
    "the only node dependent on salicylate is mara = ( ( not arca or not fnr ) or oxyr or salicylate ) .",
    "however , arca = ( fnr and not oxyr ) , and it is easily seen that mara simplifies to mara = 1 .",
    "next , we identify the determinative nodes . as argued in section  [ sec : mi ] , @xmath210 is a measure of the determinative power of @xmath211 over @xmath208 .",
    "this motivates the definition of the determinative power of input @xmath211 over the states in the network as @xmath212 note that a small value of @xmath213 implies that @xmath211 alone does not have large determinative power over the network s states , but @xmath211 may have large determinative power over the network states in conjunction with other variables . in principle @xmath214",
    "can be large for some @xmath215 $ ] , even though @xmath213 and @xmath216 are equal to zero .",
    "this is , however , not possible in the _ e. coli _ network since the functions are unate .",
    "specifically , @xmath217 implies that @xmath218 or @xmath219 are relevant variables , and according to theorem  [ co : stat_dep_unate ] , @xmath220 or @xmath221 .",
    "we computed @xmath213 for each input variable and found that @xmath213 is large just for some inputs , such as o2_xt ( 37 bit ) , leu - l_xt ( 20.9 bit ) , glc - d_xt ( 19.3 bit ) , and glcn_xt@xmath2090 ( 17 bit ) , but is small for most other variables .",
    "partly , this can be explained by the out - degree ( i.e. , the number of outgoing edges of a node ) distribution of the input nodes .",
    "however , having a large out - degree does not necessarily result in large values of @xmath213 .",
    "in fact , in the _ e. coli _ network , glc - d_xt , glcn_xt@xmath2090 , and o2_xt have 99 , 93 , and 73 outgoing edges , respectively . on the other hand , d(glc - d_xt ) = 19.3 bit and d(glcn_xt@xmath2090 )",
    "= 17 bit , whereas d(o2_xt ) = 37 bit .",
    "denote @xmath222 as a permutation on @xmath25 $ ] , such that @xmath223 , i.e. , @xmath222 orders the input nodes in descending order in their determinative power .",
    "we next consider @xmath224 as a function of @xmath225 to see whether knowledge of a small set of input nodes reduces the entropy of the overall network state significantly .",
    "@xmath224 has an interesting interpretation which arises as a consequence of the so called asymptotic equipartition property @xcite ( as discussed in greater detail in @xcite ) : consider a sequence @xmath226 of @xmath227 samples of the random variable @xmath228 .",
    "for @xmath229 and @xmath227 sufficiently large , there exists a set @xmath230 of typical sequences @xmath226 , such that @xmath231 and @xmath232 } } > 1-\\epsilon,\\ ] ] where @xmath233 denotes the cardinality of the set @xmath230 .",
    "this shows that the sequences obtained as samples of @xmath228 are likely to fall in a set of size determined by the uncertainty of @xmath228 .",
    "since the output layer consists of 653 nodes , the network s state space has maximal size @xmath234 . since @xmath228 is a function of @xmath32 , @xmath235 , where for the last equality , we assume uniformly distributed inputs .",
    "thus , without knowing the state of any input variable , the network s state is likely to be in a set of size roughly @xmath236 . given the knowledge about the states @xmath237 , the state of the network is likely to be in a set of size roughly @xmath238 .",
    "for a large network , however , @xmath239 is expensive to compute as by definition : @xmath240 } } \\label{eq : adf } \\cdot \\sum_{{\\ensuremath{\\mathbf { y } } } }   { \\ensuremath { { \\mathrm{p}\\!}\\left[{\\ensuremath{\\mathbf { y } } } = { \\ensuremath{\\mathbf { y } } } | { \\mathbf{x}_a } = { \\mathbf{x}_a } \\right ] } } \\log_2   { \\ensuremath { { \\mathrm{p}\\!}\\left[{\\ensuremath{\\mathbf { y } } } = { \\ensuremath{\\mathbf { y } } } | { \\mathbf{x}_a } = { \\mathbf{x}_a } \\right]}}.\\ ] ] hence , the number of terms in the sum is exponential in @xmath24 and @xmath241 .",
    "an estimate of can be obtained by sampling uniformly at random over @xmath242 and @xmath243 .",
    "instead , we will consider the following upper bound which is computationally inexpensive to compute : @xmath244 with @xmath245 the bound above follows from the chain rule for entropy @xcite .",
    "@xmath246 is computationally inexpensive to compute , since @xmath247 depends on few variables only ( in the _",
    "e. coli _ network , on @xmath248 ) . for the _ e. coli _ network ,",
    "@xmath249 is depicted in figure [ fig3 ] as a function of @xmath225 .",
    "figure [ fig3 ] shows that knowledge of the states of the most determinative nodes reduces the uncertainty about the network s states significantly .",
    "in fact , the upper bound @xmath249 is loose ; hence , we even expect @xmath239 to lie significantly below @xmath249",
    ". also , note that when @xmath249 is small , @xmath250 must be small on average ; hence , @xmath251}}$ ] is close to one or zero on average .     on @xmath252 as a function of @xmath225 for the _ e.  coli _ network and random networks.,scaledwidth=50.0% ]    to put @xmath249 for the _ e. coli _ network in figure [ fig3 ] into perspective , we compute @xmath249 for random networks .",
    "first , we took the _",
    "e. coli _ network and exchanged each function with one chosen uniformly at random from the set of all boolean functions of corresponding degree .",
    "we also exchanged each function with one chosen uniformly at random from all unate functions .",
    "we performed the same experiment for the original _ e. coli _ network for 25 choices of random and random unate functions , respectively .",
    "the mean of @xmath249 , along with one standard deviation from the mean ( dashed lines ) , is plotted in figure [ fig3 ] for random and random unate functions .",
    "it is seen that fewer inputs determine the output of the original _ e. coli _ network , compared to its random counterparts .",
    "for example , to obtain @xmath253 , about twice as many inputs need to be known if the functions in the _ e. coli _ network are exchanged for functions chosen uniformly at random .",
    "next , we generated at random feed forward networks with @xmath254 outputs and @xmath255 inputs , each with out - degree 8 , i.e. , the average out - degree of the inputs in the collapsed _",
    "e. coli _ network .",
    "again , we computed @xmath249 for 25 choices of random and random unate functions , respectively .",
    "the mean and one standard deviation from the mean are depicted in figure [ fig3 ] .",
    "the results show that , as expected , for a random feed forward network , there seems to be no small set of inputs that determines the outputs .",
    "finally , we discuss the average sensitivity of individual functions in the _ e. coli _ network . in section  [ sec : inf ] , we found that the average sensitivity is small if the fourier spectrum is concentrated on the coefficients of low degree .",
    "this appears to be the case for functions that are highly biased and for functions that depend on few variables only .",
    "figure [ fig4 ] shows pairs of values @xmath256)$ ] for each function in the _",
    "e. coli _ network , again assuming that the @xmath42 are independent and uniformly distributed .",
    "we can see from figure [ fig4 ] that the average sensitivity of all functions is close to the lower bound on the average sensitivity .",
    "note that the functions with high in - degree @xmath257 ( i.e. , number of relevant input variables ) , which could have average sensitivity up to @xmath257 , also have small average sensitivity , because those functions are highly biased .",
    "we , therefore , can conclude that the functions have small average sensitivity either because they depend on few variables only or because they are highly biased .",
    "for other input distributions , i.e. , other values of @xmath258 } } , \\forall i \\in [ n]$ ] , we obtained the same results .    )",
    "$ ] of each function in the _ e. coli _ network for different in - degrees @xmath257 and uniformly distributed @xmath32 . moreover , a lower bound on the average sensitivity @xmath75 , i.e. , poincare s inequality , is plotted.,scaledwidth=50.0% ]",
    "in a boolean network , tolerance to perturbations , determinative power , and statistical dependencies between nodes are properties of single functions in a probabilistic setting .",
    "hence , we analyzed single functions with product distributed argument .",
    "we used fourier analysis of boolean functions to study the mutual information between a function @xmath98 and a set of its inputs @xmath97 , as a measure of determinative power of @xmath97 over @xmath98 .",
    "we related the mutual information to the fourier spectrum and proved that the mutual information lower bounds the influence , a measure of perturbation .",
    "we also gave necessary and sufficient conditions for statistical independence of @xmath98 and @xmath97 .",
    "for the class of unate functions , which are particularly interesting for biological networks , we found that mutual information and influence are directly related ( not just via an inequality ) .",
    "we also found that @xmath259 for each relevant input @xmath1 , which , as an application , implies that in a unate regulatory network , a gene and its regulator must be statistically dependent . as an application of our results",
    ", we analyzed the large - scale regulatory network of _",
    "e. coli_. we identified the most determinative input nodes in the network and found that it is sufficient to know only a small subset of those in order to reduce the uncertainty of the overall network state significantly .",
    "this , in turn , reduces the size of the state space in which the network is likely to be found significantly .    a possible direction for future work is to provide an analysis similar to that of the _ e. coli _ regulatory network for other boolean models of biological networks , and see if similar conclusions as in section  [ sec : ecoli ] can be reached .",
    "one of the main assumptions in our work is the independence among the input variables of the network .",
    "it would be interesting to provide methods that can be used beyond this setup .",
    "however , deriving such results is challenging because for dependent inputs , the basis functions @xmath260 do not factorize as in , and many results cited and derived in this paper make use of this particular form of the basis functions . in this paper ,",
    "we focused on generic properties of information - processing networks that may help identify possible principles that underly biological networks .",
    "assessing our findings from a biological perspective would be an interesting next step .",
    "for the proof of theorems  [ th : relation_fourier_h ] and  [ le : statistical_independence_fourier_coeffs ] , we will need the following lemma :    let @xmath54 be a boolean function , let @xmath32 be product distributed , and let @xmath34 $ ] and some fixed @xmath261 be given . then , @xmath262}{\\ensuremath{\\underset{{}}{\\mathbb e}\\left [   f ( { \\ensuremath{\\mathbf { x } } } ) |   { \\ensuremath{\\mathbf { x}}}_a = { { \\mathbf{x}_a } } \\right ] } } } = \\sum_{s \\subseteq a } { \\hat f } ( s )   { \\phi}_{s}({{\\mathbf{x}_a } } ) .",
    "\\label{lemma1:eq1}\\ ] ] [ le : relation_fourier_evalue_general ]    inserting the fourier expansion of @xmath98 given by in the left - hand side of and utilizing the linearity of conditional expectation yields @xmath262}{\\ensuremath{\\underset{{}}{\\mathbb e}\\left [   f ( { \\ensuremath{\\mathbf { x } } } ) |   { \\ensuremath{\\mathbf { x}}}_a = { { \\mathbf{x}_a } } \\right ] } } } = \\sum_{s\\subseteq[n ] } \\hat f(s ) ~ { \\ifthenelse{\\equal{{}}{}}{{\\mathbb e } [ { \\phi}_{s } ( { \\ensuremath{\\mathbf { x } } } ) |   { \\ensuremath{\\mathbf { x}}}_a = { { \\mathbf{x}_a } } ] } { \\ensuremath{\\underset{{}}{\\mathbb e}\\left [   { \\phi}_{s } ( { \\ensuremath{\\mathbf { x } } } ) |   { \\ensuremath{\\mathbf { x}}}_a = { { \\mathbf{x}_a } } \\right]}}}.\\ ] ]    for @xmath263 , @xmath264}{\\ensuremath{\\underset{{}}{\\mathbb e}\\left [   { \\phi}_{s } ( { \\ensuremath{\\mathbf { x } } } ) | { \\ensuremath{\\mathbf { x}}}_a = { { \\mathbf{x}_a } } \\right ] } } } = { \\phi}_{s } ( { { \\mathbf{x}_a } } ) .\\ ] ] conversely , for @xmath265 , @xmath264}{\\ensuremath{\\underset{{}}{\\mathbb e}\\left [   { \\phi}_{s } ( { \\ensuremath{\\mathbf { x } } } ) | { \\ensuremath{\\mathbf { x}}}_a = { { \\mathbf{x}_a } } \\right ] } } } = 0.\\ ] ] to see this , assume without loss of generality that @xmath266 and @xmath267 . using the decomposition property of the basis function as given in section  [ sec : fanalysisintro ] , @xmath268}{\\ensuremath{\\underset{{}}{\\mathbb e}\\left [   { \\phi}_{s } ( { \\ensuremath{\\mathbf { x } } } ) | { \\ensuremath{\\mathbf { x}}}_a = { { \\mathbf{x}_a } } \\right ] } } } & = \\mathbb e \\left [   \\prod_{i\\in s } \\phi_{\\{i\\}}({\\ensuremath{\\mathbf { x } } } ) | { \\ensuremath{\\mathbf { x}}}_a = { { \\mathbf{x}_a } } \\right ]   \\\\ & = \\prod_{i\\in s } { \\ifthenelse{\\equal{{}}{}}{{\\mathbb e } [ \\phi_{\\{i\\}}({\\ensuremath{\\mathbf { x } } } ) | { \\ensuremath{\\mathbf { x}}}_a = { { \\mathbf{x}_a } } ] } { \\ensuremath{\\underset{{}}{\\mathbb e}\\left [   \\phi_{\\{i\\}}({\\ensuremath{\\mathbf { x } } } ) | { \\ensuremath{\\mathbf { x}}}_a = { { \\mathbf{x}_a } } \\right]}}}\\end{aligned}\\ ] ] which is equal to zero as @xmath269}{\\ensuremath{\\underset{{}}{\\mathbb e}\\left [   \\phi_{\\{j\\}}({\\ensuremath{\\mathbf { x } } } ) | { \\ensuremath{\\mathbf { x}}}_a = { { \\mathbf{x}_a } } \\right ] } } } =   { \\ifthenelse{\\equal{{}}{}}{{\\mathbb e } [ \\phi_{\\{j\\}}({\\ensuremath{\\mathbf { x}}})]}{\\ensuremath{\\underset{{}}{\\mathbb e}\\left [   \\phi_{\\{j\\}}({\\ensuremath{\\mathbf { x}}})\\right ] } } } = 0.\\ ] ]",
    "we would like to thank sara al - sayed and dejan lazich for their helpful discussions and careful reading of the manuscript .",
    "j. saez - rodriguez , l. simeoni , j.a .",
    "lindquist , r. hemenway , u. bommhardt , b. arndt , u haus , r weismantel , e.d .",
    "gilles , s. klamt , b. schraven , a logical model provides insights into t cell receptor signaling .",
    ". biol . * 3*(8 ) , e163 ( 2007 )              r. heckel , s. schober , m. bossert , determinative power and tolerance to perturbations in boolean networks .",
    "paper presented at the 9th international workshop on computational systems biology , ulm , germany , 4 - 6 june 2012            r.r .",
    "bahadur , a representation of the joint distribution of responses to n dichotomous items . in _ studies in item analysis and prediction _ , ed . by h. solomon ( stanford university press , stanford , 1961 ) , pp.158168    m. ben - or , n. linial , collective coin flipping , robust voting schemes and minima of banzhaf values .",
    "paper presented at the 26th annual symposium on foundations of computer science , portland , oregon , usa , 2123 october 1985              b. rosell , l. hellerstein , s. ray , why skewing works : learning difficult boolean functions with greedy tree learners . paper presented at the 22nd international conference on machine learning , bonn , germany , 711 august 2005"
  ],
  "abstract_text": [
    "<S> consider a large boolean network with a feed forward structure . given a probability distribution on the inputs , </S>",
    "<S> can one find , possibly small , collections of input nodes that determine the states of most other nodes in the network ? to answer this question , a notion that quantifies the _ determinative power _ of an input over the states of the nodes in the network is needed . </S>",
    "<S> we argue that the mutual information ( mi ) between a given subset of the inputs @xmath0 of some node @xmath1 and its associated function @xmath2 quantifies the determinative power of this set of inputs over node @xmath1 . </S>",
    "<S> we compare the determinative power of a set of inputs to the sensitivity to perturbations to these inputs , and find that , maybe surprisingly , an input that has large sensitivity to perturbations does not necessarily have large determinative power . </S>",
    "<S> however , for _ unate _ functions , which play an important role in genetic regulatory networks , we find a direct relation between mi and sensitivity to perturbations . as an application of our results , we analyze the large - scale regulatory network of _ escherichia coli_. we identify the most determinative nodes and show that a small subset of those reduces the overall uncertainty of the network state significantly . </S>",
    "<S> furthermore , the network is found to be tolerant to perturbations of its inputs . </S>"
  ]
}