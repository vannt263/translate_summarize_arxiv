{
  "article_text": [
    "with the proliferation of text data available online , text classification has attracted a lot of interest .",
    "traditionally , @xmath0-grams are considered as document features and are subsequently fed to a classifier such as support vector machines ( svms ) @xcite .",
    "one - hot - encoding representations , although prominent in the literature , have two significant drawbacks : ( i ) they result in a very high dimensional and sparse feature space , and ( ii ) they do not encode similarity between words . lately , a lot of research has been devoted to the direction of distributed representations @xcite . distributed representations of words , are continuous , low dimensional , dense vectors that characterize the meaning and the semantic content of words .",
    "each dimension of the embedding represents a latent feature of the word , hopefully capturing useful syntactic and semantic properties . as a result ,",
    "semantically similar words , such as `` strong '' and `` powerful '' , will be close in the output vectorial space .    in this work , we present a focused contribution as part of an interesting classification application .",
    "we investigate the performance of word embeddings learned using the skip - gram model @xcite in the context of large scale , multi - label document classification .",
    "we report results on real - world classification problems with up to 10,000 classes and we demonstrate a straightforward way to combine document - level embeddings with one - hot - encoding representations .",
    "the contributions of our work are twofold : we , first , propose an efficient way to achieve satisfactory classification performance using only embedding representations and , we further improve it by applying a naturally parallelizable fusion mechanism between the distributed representations and one - hot - encoding representations .",
    "several methods have been proposed for obtaining distributed word - level representations .",
    "we cite for instance : @xcite .",
    "extensions of those methods have been proposed to cope with larger portions of text such as sentences or paragraphs : @xcite . however , generalizing from words to larger text spans can be inefficient , since for every unseen span new passes over a neural network are required . hence , we focus on methods that given a dictionary of word representations apply composition functions @xcite to produce document representations .    although distributed embeddings have been applied in a plethora of tasks from analogies evaluation @xcite to extractive summarization @xcite , their application on large scale text classification has not been investigated . what is more , most of the work in this direction deals with short text spans , such as sentences or tweets , and the size of the investigated problems with regard to the number of classes is small .",
    "recently , for instance , @xcite investigated the problem of short text similarity with applications to classification with a limited number of classes , like binary sentiment analysis @xcite and ternary sentiment analysis @xcite .",
    "more frequently , word embeddings for text classification are used for initializing architectures such as convolutional and recurrent networks .",
    "the works of @xcite and @xcite for instance , are in this line but , again , they limit their study at sentence - length spans .",
    "the latter is , also , due to hardware limitations of the gpus used : their limited memory capacity in conjunction with the vocabulary size of large scale classification problems require many data transfer operations which results in significant overhead . in this work we place ourselves at the large scale",
    "setting ( number of classes in the order of @xmath1 ) where compositional methods based on neural networks are difficult to be applied .",
    "the remainder of this paper is organised as follows : in section 2 we propose and evaluate composition functions for obtaining document from word representations , and in section 3 we discuss their integration with one - hot - encoding schemes .",
    "finally , section 4 concludes with remarks to our future work .",
    "lccc2cmc1.8 cm & instances & vocabulary & avg .",
    "labels per instance & avg . doc .",
    "size ( words ) + train data & 12.5 m & 868,219 & - & 215 + pubmed@xmath2 & 225,000&289,789&1.42&239 + pubmed@xmath3 & 225,000&302,802&3.23&233 + pubmed@xmath4 & 225,000&303,637&6.64&230 +    we explore three functions to compose document representations : _ min _ , _ average _ and _ max _",
    "@xcite which have been used as simple yet effective methods for compositionality learning in vector - based semantics @xcite , to obtain a document s representation .",
    "we use the output of each composition function as document features and we evaluate both their classification performance as well as the performance of their concatenation : @xmath5\\ ] ] where @xmath6 is the representation of a document and @xmath7 is the result of applying the composition @xmath8 element - wise , in the distributed vectors of the words of the documents . for instance , assuming we want the representation of `` harsh winter '' , the composition function requires the vector representations of the words `` harsh '' and `` winter '' . then , applying the _ max _ function will produce a vector of the same dimensionality with the original representations where each element will have the maximum value of the corresponding elements of the original word representations . in this process , we assume access to a vocabulary of distributed representations , where each word is associated with a @xmath9-dimensional vector .",
    "this vocabulary , which we assume readily available , may have been generated beforehand .",
    "table [ table : data ] shows the data we use throughout the paper .",
    "they are abstracts of biomedical texts from pubmed released by the bioasq challenge organisers @xcite as well as texts from wikipedia @xcite . to obtain the dictionary of word embeddings we used the skipgram model of ` word2vec ` tool @xcite .",
    "we have kept the tool s default parameters for skip - gram apart from the number of iterations that we have set to 15 . for training the representations , we used 10 m pubmed abstracts and we added 2.5 m wikipedia documents for better generalization ( `` train data '' of table [ table : data ] ) . in the preprocessing steps we applied lower - casing , we space - padded the punctuation symbols and we filtered the words with less than 5 occurrences . for classification purposes we used the remaining pubmed abstracts from the released data .",
    "we created three classification datasets : `` pubmed@xmath10 '' with @xmath11 being the @xmath12 most common classes in the data .",
    "we performed a stratified split in train - test ( 200k-25k instances ) parts . in the experiments hereafter , we use svms with linear kernel which have been widely used for text classification . the @xmath13 value that controls the importance of the regularization term in the optimization problem",
    "was selected using 5-fold cross validation on the training data of each experiment and for each representation .",
    "the classification problem is multi - label : each instance is associated with several classes as shown in table [ table : data ] .",
    "we cope with the multi - label problem using a binary relevance approach @xcite .",
    "for svms , tackling the multi - label problem with binary relevance results in predicting for each instance every label with positive distance from the separating hyperplanes of the one - vs - rest binary problems .",
    "if the classifier does not return any label for an instance , the most common label of the training data is assigned . for our implementations , we have used python s scikit - learn @xcite .",
    "we now present results for classification experiments on the pubmed@xmath14 and pubmed@xmath15 datasets when the described composition functions are used .",
    "figure [ fig : class1 ] shows the scores for the @xmath16 measure obtained on the test data , when varying the size of the training data . from the figure , first note that the _ avg _ function performs better compared to both _ min _ and",
    "_ max_. the latter two , perform equally but they are not competitive .",
    "however , the best results are consistently obtained with the concatenation ( _ conc _ ) of the outputs of the three composition functions .",
    "interestingly , adding the _ min _ and _ max _ representations creates a richer representation that benefits the performance . to this direction , note the steep increase in the performance of _ conc _ representations with the availability of training data : being richer , those representations have bigger discriminative power . depending on the dataset and the dimension of the representations , the achieved improvements using _ conc _ vary from @xmath173 - 5 @xmath16 points which is important for such problems .",
    "we believe that the _ avg _ function does not retain enough information for large documents , given that they consist on average of more than 200 words ( table [ table : data ] ) . to this end ,",
    "table [ table : docdimensionimpact ] reports the micro @xmath16 measure for the pubmed@xmath15 dataset , with respect to the document length in words .",
    "note that the best performance is achieved for smaller documents independently of the embedding dimension .",
    ".the impact of the document length on the classification performance ( micro @xmath16 measure ) when using _ avg _ representations and the embeddings dimension is @xmath18 .",
    "the representations perform better on smaller documents , which is in line with the outcome that the _ avg _ representations do not capture enough information for large documents .",
    "[ cols=\"<,^,^,^,^,^ \" , ]     another observation from figure [ fig : class1 ] and table [ table : docdimensionimpact ] concerns the effect of the dimension of word representations in the classification performance .",
    "representations of bigger dimensions benefit performance .",
    "in fact , increasing the dimension from 50 to 400 , improves the @xmath16 measure for _ conc _ for pubmed@xmath14 ( resp .",
    "pubmed@xmath15 ) by @xmath177 ( resp .",
    "@xmath1713 ) points .",
    "summarizing , we highlight here two of the advantages of the proposed approach : ( i ) although simple , our composition functions have yielded significant performance improvements and , ( ii ) their application on large datasets is naturally parallelizable , hence , they can be easily applied on real , large scale problems .",
    "lccccccccc & & & + ( lr)2 - 10 _ hash _ & & & + _ tf - idf _ & & & + ( lr)1 - 10    & d=100&d=200&d=400&d=100&d=200&d=400&d=100&d=200&d=400 + ( lr)2 - 4 ( lr)5 - 7(lr)8 - 10    _ x_conc_&0.592 & 0.614&0.626&0.362&0.41&0.436&0.386&0.434&0.454 + _ hash+x_conc_&0.651 & 0.654&0.646&0.464&0.476&0.473&0.488&0.495&0.491 + _ tfidf+x_conc_&**0.66**@xmath19 & * * 0.66**@xmath19 & 0.656&0.484@xmath19 & 0.486@xmath19 & * * 0.487**@xmath19 & 0.507@xmath19 & 0.507@xmath19 & * * 0.512**@xmath19 +",
    "in the previous section we have shown that the concatenated representations obtained by the output of the composition functions achieved the best classification performance .",
    "we , now , compare them with the traditional one - hot - encoding representations .",
    "we focus on two ways of representing text : ( i ) by using _ tf - idf _ representations of unigrams , and ( ii ) by employing a _ hash _",
    "function @xcite . for",
    "the former , to generate the tf - idf representations we used sub - linear term frequency counts multiplied by their respective smoothed inverse document frequency , i.e. , for a term @xmath20 we have @xmath21 .",
    "for the latter , the hash function , given a text string , transforms it on a numerical value in a pre - specified space , that is used as the index to generate a vector representation .",
    "increasing the output dimension of the hash functions reduces the probability of collisions , i.e. different words mapped on the same vector indices , but also increases the output vector size .",
    "we investigate this trade - off in figure [ fig : hashdim ] : we present the effect of the size of the hash representations with respect to classification performance for pubmed@xmath22 and pubmed@xmath15 which have the biggest vocabulary size .",
    "we also report exact timings of each scenario , executed on 10 cores of an intel(r ) xeon(r ) cpu e5 - 2640 v3 @ 2.60ghz . from the figure , note that after 70k features the classification performance does not improve when increasing the size of the feature space .",
    "on the other hand , increasing the representation s dimension , the training time also increases . for reference , _ tf - idf _ representations in the same computational setting need 602 sec . and 1203 sec .",
    "for pubmed@xmath22 and pubmed@xmath15 respectively . as a result , we set the hash dimension to 70,000 features .",
    "note the significant dimensionality reduction achieved , given the vocabulary sizes of our problems reported in table [ table : data ] .",
    "table [ table : classification ] details the scores achieved with regard to the representations used .",
    "we first discuss the performance when single representations are used : _ x_conc _",
    ", _ tf - idf _ and _ hash_. notice that _ tf - idf _ performs better than both _",
    "x_conc _ and _ hash _ , with the latter achieving the lowest performance .",
    "the performance of the three representations on pubmed@xmath14 is comparable , but in the bigger classification problems _ tf - idf _ and performs considerably better .",
    "we , thus , consider the _ tf - idf _ representations as our baseline model , and we examine how the models with concatenated representations behave compared to it .",
    "we investigate now whether the fusion of distributed document representations with the one - hot - encoding representations benefits the classification performance .",
    "the last two lines of table [ table : classification ] present the performance of the fusion , by concatenation , of the embedding representations with @xmath23 with _ hash _ and _ tf - idf_. in the experiments , both _ hash _ and",
    "_ tf - idf _ consistently achieve better performance when combined with the distributed representations .",
    "for instance , for pubmed@xmath15 and @xmath24 the _ tf - idf _",
    "_ hash _ ) representations improve in absolute numbers by 2 ( resp .",
    "3.5 ) @xmath16 points .",
    "we have performed two - sided student s t - tests ( @xmath25 ) to compare whether the improvements obtained for each classification problem are statistically significant compared to using _ tf - idf _ representations .",
    "those results , indicated by @xmath26 in table [ table : classification ] , reveal that the concatenation of _ tf - idf _ with distributed representations improves the classification performance in a statistically significant way .",
    "in addition to the important improvements , note than in the fused representations , the effect of @xmath9 diminishes .",
    "for instance in pubmed@xmath14 , one can obtain the optimal performance using embedding dimensions with @xmath27 and similar observations can be made in the rest of the datasets .     and",
    "in this work we have restricted ourselves in representations learned in the word level .",
    "this is advantageous in terms of speed since the dictionary of representations can be generated offline .",
    "then , applying composition functions is naturally parallelizable and fast for prediction",
    ". however , this poses the challenge of having robust composition functions , which if carefully selected , can result in performance gains such as those reported above .",
    "also , similarly to the bag - of - words paradigm , it does not take into account the word order and the words grouping in coherent segments like sentences or phrases .    in this line",
    ", it would be interesting to further investigate how more complex embeddings such as paragraph vectors @xcite perform .",
    "even if such approaches are computationally expensive in the document level , previous research has shown their effectiveness on the sentence level .",
    "hence , a direct extension of this work is to test the investigated composition functions with sentence level representations . in terms of applications ,",
    "those sentence representations can be directly used to evaluate the effectiveness of the embeddings and the composition functions .",
    "importantly , their can be evaluated simultaneously in different levels of text granularity from sentences to documents in the framework of passage retrieval and document classification from instance .",
    "another interesting line of research concerns the memory efficiency of such dense representations .",
    "recent research efforts @xcite have investigated ways of compressing the learned ( or composed ) representations using either linear ( e.g. pca ) or non - linear ( e.g. auto - encoders ) approaches to decrease the memory requirements or the dimension of the representations .",
    "such approaches , apart from having a positive effect on the memory footprint , also have a positive effect on the required computational requirements for training .",
    "in this work we have evaluated different composition functions for obtaining document - level representations using distributed embeddings of words . summarizing our findings , adding the concatenated vector of word - level skip - gram derived features to tf - idf unigrams performs better than tf - idf features alone .",
    "also , the result seems to be more pronounced as the representations cardinality increases and as the output label space increases . given the obtained improvements ,",
    "we have also outlined promising future research directions .",
    "w.  blacoe and m.  lapata . a comparison of vector - based representations for semantic composition . in _ proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning",
    "_ , pages 546556 .",
    "association for computational linguistics , 2012 .",
    "g.  forman and e.  kirshenbaum .",
    "extremely fast text feature extraction for classification and indexing . in _ proceedings of the 17th acm conference on information and knowledge management _ , pages 12211230 .",
    "acm , 2008 .",
    "m.  kgebck , o.  mogren , n.  tahmasebi , and d.  dubhashi .",
    "extractive summarization using continuous vector space models . in _ proceedings of the 2nd workshop on continuous vector space models and their compositionality ( cvsc)@ eacl _ ,",
    "pages 3139 , 2014 .",
    "j.  kim , f.  rousseau , and m.  vazirgiannis .",
    "convolutional sentence kernel from word embeddings for short text categorization . in _ proceedings of the 2015 conference on empirical methods in natural language processing . in emnlp _ , volume  15 , 2015 .",
    "t.  mikolov , i.  sutskever , k.  chen , g.  s. corrado , and j.  dean . distributed representations of words and phrases and their compositionality . in _ advances in neural information processing systems _ ,",
    "pages 31113119 , 2013 .",
    "b.  pang and l.  lee .",
    "seeing stars : exploiting class relationships for sentiment categorization with respect to rating scales . in _ proceedings of the 43rd annual meeting on association for computational linguistics _ , pages 115124 .",
    "association for computational linguistics , 2005 .",
    "f.  pedregosa , g.  varoquaux , a.  gramfort , v.  michel , b.  thirion , o.  grisel , m.  blondel , p.  prettenhofer , r.  weiss , v.  dubourg , j.  vanderplas , a.  passos , d.  cournapeau , m.  brucher , m.  perrot , and e.  duchesnay .",
    "scikit - learn : machine learning in python .",
    ", 12:28252830 , 2011 .",
    "r.  socher , e.  h. huang , j.  pennin , c.  d. manning , and a.  y. ng .",
    "dynamic pooling and unfolding recursive autoencoders for paraphrase detection . in _ advances in neural information processing systems _ , pages 801809 , 2011 .",
    "g.  tsatsaronis , g.  balikas , p.  malakasiotis , i.  partalas , m.  zschunke , m.  r. alvers , d.  weissenborn , a.  krithara , s.  petridis , d.  polychronopoulos , et  al .",
    "an overview of the bioasq large - scale biomedical semantic indexing and question answering competition .",
    ", 16(1):1 , 2015 ."
  ],
  "abstract_text": [
    "<S> we investigate the integration of word embeddings as classification features in the setting of large scale text classification . </S>",
    "<S> such representations have been used in a plethora of tasks , however their application in classification scenarios with thousands of classes has not been extensively researched , partially due to hardware limitations . in this work , </S>",
    "<S> we examine efficient composition functions to obtain document - level from word - level embeddings and we subsequently investigate their combination with the traditional one - hot - encoding representations . by presenting empirical evidence on large , multi - class , multi - label classification problems </S>",
    "<S> , we demonstrate the efficiency and the performance benefits of this combination . </S>"
  ]
}