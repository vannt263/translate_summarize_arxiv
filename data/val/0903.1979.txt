{
  "article_text": [
    "it is well known that the brain has a highly developed and complex self - generated dynamical neural activity .",
    "we are therefore confronted with a dichotomy when attempting to understand the overall functioning of the brain or when designing an artificial cognitive system : a highly developed cognitive system , such as the brain @xcite , is influenced by sensory input but it is not driven directly by the input signals .",
    "the cognitive system needs however this sensory information vitally for adapting to a changing environment and survival .    in this context",
    "we then want to discuss two mutually interrelated questions :    * can we formulate a meaningful paradigm for the self - sustained internal dynamics of an autonomous cognitive system ? * how is the internal activity influenced by sensory signals , _",
    "viz _ which are the principles for the respective learning processes ?",
    "we believe that these topics represent important challenges for research in the field of recurrent neural networks and the modeling of neural processes . from an experimental point of view",
    "we note that an increasing flux of results from neurobiology supports the notion of quasi - stationary spontaneous neural activity in the cortex @xcite .",
    "it is therefore reasonable to investigate the two questions formulated above with the help of neural architectures centrally based on the notion of spontaneously generated transient states , as we will do in the present investigation using appropriate recurrent neural networks .",
    "standard classification schemes of dynamical systems are based on their long - time behavior , which may be characterized , e.g. , by periodic or chaotic trajectories @xcite .",
    "the term ` transient - state dynamics ' refers , on the other hand , to the type of activity occurring on intermediate time scales , as illustrated in fig .",
    "[ figure_trans_activity ] .",
    "a time series of semi - stable activity patterns , also denoted transient attractors , is characterized by two time scales .",
    "the typical duration @xmath0 of the activity plateaus and the typical time @xmath1 needed to perform the transition from one semi - stable state to the subsequent one .",
    "the transient attractors turn into stable attractors in the limit @xmath2 .",
    "transient state dynamics is intrinsically competitive in nature .",
    "when the current transient attractor turns unstable the subsequent transient state is selected by a competitive process .",
    "transient - state dynamics is a form of ` multi - winners - take - all ' process , with the winning coalition of dynamical variables suppressing all other competing activities .",
    "humans can discern about 10 - 12 objects per second @xcite and it is therefore tempting to identify the cognitive time scale of about 80 - 100ms with the duration @xmath0 of the transient - state dynamics illustrated in fig .",
    "[ figure_trans_activity ] .",
    "interestingly , this time scale also coincides with the typical duration @xcite of the transiently active neural activity patterns observed in the cortex @xcite .",
    "several high - level functionalities have been proposed for the spontaneous neural brain dynamics .",
    "edelman and tononi @xcite argue that ` critical reentrant events ' constitute transient conscious states in the human brain .",
    "these ` states - of - mind ' are in their view semi - stable global activity states of a continuously changing ensemble of neurons , the ` dynamic core ' .",
    "this activity takes place in what dehaene and naccache @xcite denote the ` global workspace ' .",
    "the global workspace serves , in the view of baars and franklin @xcite , as an exchange platform for conscious experience and working memory .",
    "crick and koch @xcite and koch @xcite have suggested that the global workspace is made - up of ` essential nodes ' , i.e. ensembles of neurons responsible for the explicit representation of particular aspects of visual scenes or other sensory information .     and @xmath1 for the length of activity - plateau and of the transient period respectively.,scaledwidth=85.0% ]      traditional neural network architectures are not continuously active on their own .",
    "feedforward setups are explicitly driven by external input @xcite and hopfield - type recurrent nets settle into a given attractor after an initial period of transient activities @xcite .",
    "the possibilities of performing cognitive computation with autonomously active neural networks , the route chosen by nature , are however investigated increasingly @xcite . in this context",
    "the time encoding of neural information , one of the possible neural codes @xcite , has been studied in various contexts .",
    "two network architectures , the echo state network suitable for rate - encoding neurons @xcite , and the liquid state machine suitable for spiking neurons @xcite , have been proposed to transiently encode in time a given input for further linear analysis by a subsequent perceptron .",
    "both architectures , the echo - state network and the liquid - state machine , are examples of reservoir architectures with fading memories , which however remain inactive in the absence of sensory input .",
    "an example of a continuously active recurrent network architecture is the winnerless competition based on stable heteroclinic cycles @xcite . in this case",
    "the trajectory moves along heteroclines from one saddle point to the next approaching a complex limiting cycle .",
    "close to the saddle points the dynamics slows down leading to well defined transiently active neural activity patterns .",
    "in order to study the issues raised in the introduction , the notion of autonomous neural activity and its relation to the sensory input , we will consider a specific model based on clique - encoding recurrent nets .",
    "the emphasis will be on the discussion of the general properties and of the underlying challenges .",
    "we will therefore present here an overview of the algorithmic implementation , referring in part to the literature for further details .",
    "experimental evidence indicates that sparse neural coding is an important operating principle in the brain , as it minimizes energy consumption , maximizes storage capacity and contributes to make information encoding spatially explicit @xcite .",
    "a powerful form of sparse coding is multi - winners - take - all encoding in the form of cliques .",
    "the term cliques stems from network theory and denotes subgraphs which are fully interconnected @xcite , a few examples are given in fig .",
    "[ figure_networks_cliques ] .",
    "cliques are fully interconnected subgraphs of maximal size , in the sense that they are not part of another fully interconnected subgraph containing a larger number of vertices .",
    "clique encoding is an instance of sparse coding with spatially overlapping memory states .",
    "the use of clique encoding is in fact motivated by experimental findings indicating a hierarchical organization of overlapping neural clique assemblies for the real - time memory representation in the hippocampus @xcite . in the framework of a straightforward auto - associative neural network",
    "the cliques are defined by the network of the excitatory connections , which are shown as lines in fig .",
    "[ figure_networks_cliques ] , in the presence of an inhibitory background @xcite . in this setting all cliques correspond to attractors of the network , _ viz _ to spatially explicit and overlapping memory representations .",
    "one can transform the attractor network with clique encoding into a continuously active transient - state network , by introducing a reservoir variable for every neuron . in this setting the reservoir of a neuron is depleted whenever the neuron is active and refilled whenever the neuron is inactive . via a suitable local coupling between the individual neural activity and reservoir variables a well defined and stable transient state dynamics is obtained @xcite . when a given clique becomes a winning coalition ,",
    "the reservoirs of its constituting sites are depleted over time . when fully depleted the winning coalition becomes unstable and the subsequent winning coalition is activated through a competitive associative process , leading to an ever ongoing associative thought process .",
    "the resulting network architecture is a dense and homogeneous associative network ( dhan ) @xcite .",
    "an illustrative result of a numerical simulation is given in fig .  [ figure_ai7 ] .    for the isolated system , not coupled to any sensory input , this associative thought process has no semantic content , as the transient attractors , the cliques , have none .",
    "the semantic content can be acquired only by coupling to a sensory input and by the generation of correlation between the transient attractors and patterns extracted from the input data stream .    .",
    "shown are , vertically displaced , the time developments of the respective neural activities ( solid lines ) and of the neural reservoirs ( dashed lines ) .",
    "the time series of spontaneously generated transient states are the cliques @xmath3 ( from the left to the right ) .",
    ", scaledwidth=55.0% ]      to be definite , we utilize a continuous - time formulation for the dhan architecture , with rate - encoding neurons , characterized by normalized activity levels @xmath4 $ ] .",
    "one can then define , via @xmath5 the respective growth rates @xmath6 .",
    "representative time series of growth rates @xmath6 are illustrated in fig .",
    "[ figure_sensitive_periods ] . when @xmath7 , the respective neural activity @xmath8 increases , approaching rapidly the upper bound ; when @xmath9 , it decays to zero .",
    "the model is specified @xcite , by providing the functional dependence of the growth rates with respect to the set of activity - levels @xmath10 of all sites and on the synaptic weights , as usual for recurrent or auto - associative networks .    during the transition",
    "periods many , if not all , neurons will enter the competition to become a member of the new winning coalition .",
    "the competition is especially pronounced whenever most of the growth rates @xmath6 are small in magnitude , with no subset of growth rates dominating over all the others . whether this does or does not happen depends on the specifics of the model setup . in fig .",
    "[ figure_sensitive_periods ] , two cases are illustrated . in the first case ( lower graph ) the competition for the next winning coalition",
    "is restricted to a subset of neurons , in the second case ( upper graph ) the competition is network - wide . when most neurons participate in the competition process for a new winning coalition the model will have ` sensitive periods ' during the transition times and it will be able to react to eventual external signals .     generating an internal transient state - dynamics via eq .",
    "( [ eq_x_dot ] ) .",
    "the two examples differ in the functional dependence of the @xmath11 on the @xmath12 .",
    "the top graph corresponds to a system having sensitive periods , the bottom graph to a system without distinctive sensitive periods.,scaledwidth=85.0% ]      so far we have discussed in general terms the properties of isolated models exhibiting a self - sustained dynamical behavior in terms of a never - ending time series of semi - stable transient states , as illustrated in figs .",
    "[ figure_ai7 ] and [ figure_sensitive_periods ] , using the dhan architecture with continuous - time and rate - encoding neurons .",
    "the importance of sensitive periods comes in when the network exhibiting transient - state dynamics is coupled to a stream of sensory input signals .",
    "it is reasonable to assume , that external input signals will contribute to the growth rates @xmath6 via @xmath13 here the @xmath14 encode the influence of the input signals and we have denoted now with @xmath15 the contribution to the growth rate a neuron in the dhan layer receives from the other dhan neurons .",
    "the factor @xmath16 in eq .",
    "( [ eq_delta_r ] ) ensures that the input signal does not deactivate the current winning coalition as we will discuss further below .",
    "let us here assume for a moment , as an illustration , that the input signals are suitably normalized , such that @xmath17 in order of magnitude .",
    "for the simulations presented further below a qualitatively similar optimization will occur homeostatically .",
    "for the transient states the @xmath18 for all sites not forming part of the winning coalition and the input signal @xmath14 will therefore not destroy the transient state , compare figs .",
    "[ figure_sensitive_periods ] and [ figure_arb_7 ] . with the normalization given by eq .",
    "( [ eq_delta_r_magnitude ] ) the total growth rate @xmath19 will remain negative for all inactive sites and the sensory input will not be able to destroy the current winning coalition . the input signal will however enter the competition for the next winning coalition during a sensitive period , providing an additional boost for the respective neurons .    this situation is exemplified in fig .",
    "[ figure_arb_7 ] , where we present simulation results for the 7-site system shown in fig .",
    "[ figure_networks_cliques ] , subject to two sensory inputs @xmath20 .",
    "the self - generated time series of winning coalitions is not redirected for the first sensory input .",
    "the second stimulus overlaps with a sensory period and its strongest components determine the new winning coalitions .",
    "the simulation results presented in fig .  [ figure_arb_7 ] therefore demonstrate the existence of well defined time - windows suitable for the learning of correlations between the input signal and the intrinsic dynamical activity .",
    "the time windows , or sensitive periods , are present during and shortly after a transition from one winning coalition to the subsequent .",
    "a possible concrete implementation for this type of learning algorithm will be given further below .",
    ", the total growth rates @xmath11 and the input signals @xmath20 from a simulation of the 7-site system shown in fig .  [ figure_networks_cliques ] ( color coding ) .",
    "the time series of winning coalitions is given .",
    "the first input signal does not change the composition of the winning coalition , whereas the second does.,scaledwidth=75.0% ]    let us now come to the factor @xmath16 in eq .",
    "( [ eq_delta_r ] ) , containing the heaviside - step functions @xmath21 . for vertices @xmath22 of the current winning coalition",
    "the intra dhan - layer growth rates are positive , @xmath23 .",
    "therefore , the above factor ensures , that a suppressive @xmath24 has no effect on the members of the current winning coalition .",
    "the contribution @xmath14 from the input may therefore alter the balance in the competition for the next winning coalition during the sensitive periods , but not suppress the current active clique .",
    "let us note , that the setup discussed here allows the system also to react to an occasional strong excitatory input signal having @xmath25 .",
    "such a strong signal would suppress the current transient state altogether and impose itself .",
    "this possibility of rare strong input signals is evidently important for animals and would be , presumably , also helpful for an artificial cognitive system .",
    "let us return to the central problem inherent to all systems reacting to input signals and having at the same time a non - trivial intrinsic dynamical activity .",
    "namely , when should learning occur , i.e.when should a distinct neuron become more sensitive to a specific input pattern and when should it suppress its sensibility to a sensory signal .",
    "the framework of competitive dynamics developed above allows for a straightforward solution of this central issue : learning should occur exclusively when the input signal makes a qualitative difference , _ viz _ when the input signal deviates the transient - state process . for illustration",
    "let us assume that the series of winning coalitions is @xmath26\\atop\\longrightarrow}\\ ( 0,1 )        \\ { [ a]\\atop\\longrightarrow}\\ ( 1,2,4,5)~,\\ ] ] where the index @xmath27 $ ] indicates that the transition is driven by the autonomous internal dynamics and that the series of winning coalitions take the form @xmath26\\atop\\longrightarrow}\\ ( 0,1 )        \\ { [ s]\\atop\\longrightarrow}\\ ( 3,6)~,\\ ] ] in the presence of a sensory signal @xmath28 $ ] , as it is the case for the data presented in fig .  [ figure_arb_7 ] .",
    "note , that a background of weak or noisy sensory input could be present in the first case , but learning should nevertheless occur only in the second case .",
    "a reliable distinction between these two cases can be achieved via a suitable diffusive learning signal @xmath29 .",
    "it is activated whenever any of the input contributions @xmath14 changes the sign of the respective growth rates during the sensitive periods , @xmath30 _ viz _ when it makes a qualitative difference .",
    "let us remember that the @xmath15 are the internal contributions to the growth rate , i.e.  the input a dhan neuron receives via recurrent connections from the other dhan neurons .",
    "the diffusive learning signal @xmath31 is therefore increasing in strength only when a neuron is activated externally , but not when activated internally , with the @xmath32 denoting the respective growth and decay rates .",
    "the diffusive learning signal @xmath29 is a global signal and a sum @xmath33 over all dynamical variables is therefore implicit on the right - hand side of eq .",
    "( [ eq_dot_s ] ) .      the general procedure for the learning of correlation between external signals and intrinsic dynamical states for a cognitive system presented here does not rule out other mechanisms .",
    "here we concentrate on the learning algorithm which occurs automatically , one could say sub - consciously .",
    "active attention focusing , which is well known in the brain to potentially shut off a sensory input pathway , or to enhance sensibility to it , may very well work in parallel to the continuously ongoing mechanism investigated here .",
    "we note , however , that the associative thought process within the dhan carries with it a dynamical attention field @xcite .",
    "neurons receiving both positive and negative contributions from the winning coalition will need smaller sensory input signals in order to be activated than neurons receiving only negative contributions . to put it colloquially : when thinking of the color blue it is easier to spot a blue car in the traffic than a white one .",
    "connecting the input with the dhan layer are adapted during the learning process ( illustrated selectively by respective thick / thin blue lines in the graph ) .",
    "the dhan layer consists of active and inactive neurons ( red / yellow circles ) connected by intra - layer synaptic weights .",
    "the topology shows five cliques ( denoted c - i to c - v in the graph ) of which c - ii is active , as emphasized by the red - color neurons .",
    ", scaledwidth=85.0% ]",
    "so far we have described , in general terms , the system we are investigating . it has sensitive periods during the transition periods of the continuously ongoing transient - state process , with learning of input signals regulated by a diffusive learning signal .",
    "the main two components are therefore the dhan layer and the input layer , as illustrated in fig .",
    "[ fig_twolayers ] .",
    "the input signal acts via eq .",
    "( [ eq_delta_r ] ) on the dhan layer , with the contribution @xmath14 to the growth rate of the dhan neuron @xmath22 given by @xmath34 where we have denoted with @xmath35 $ ] the activity - levels of the neurons in the input layer . for subsequent use",
    "we have defined in eq .",
    "( [ eq_delta_r_v_pq ] ) an auxiliary variable @xmath36 , which quantifies the influence of inactive input - neurons . the task is now to find a suitable learning algorithm which extracts relevant information from the input - data stream by mapping distinct input - patterns onto selected winning coalitions of the dhan layer .",
    "this setup is typical for an independent component analysis @xcite .",
    "the multi - winners - take - all dynamics in the dhan module implies individual neural activities to be close to 0/1 during the transient states and we can therefore define three types of inter - layer links @xmath37 ( see fig .  [ fig_twolayers ] ) :    * ( _ ` act ' _ ) + links connecting active input neurons with the winning coalition of the dhan module . *",
    "( _ ` orth ' _ ) + links connecting inactive input neurons with the winning coalition of the dhan module .",
    "* ( _ ` ina ' _ ) + links connecting active input neurons with inactive neurons of the dhan module .",
    "the orthogonal links take their name from the circumstance that the receptive fields of the winning coalition of the target layer need to orthogonalize to all input - patters differing from the present one .",
    "note that it is not the receptive field of individual dhan neurons which is relevant , but rather the cumulative receptive field of a given winning coalition .",
    "we can then formulate three simple rules for the respective link - plasticity .",
    "whenever the new winning coalition in the dhan layer is activated by the input layer , _ viz _ whenever there is a substantial diffusive learning signal , i.e.  when @xmath38 exceeds a certain threshold @xmath39 , the following optimization procedures should take place :    *   + the sum over active afferent links should take a large but finite value @xmath40 , @xmath41 *   + the sum over orthogonal afferent links should take a small value @xmath42 , @xmath43 *   + the sum over inactive links should take a small but non - vanishing value @xmath44 , @xmath45    the @xmath40 , @xmath44 and @xmath42 are the target values for the respective optimization processes . in order to implement these three rules we define three corresponding contributions to the link plasticities : @xmath46 where the inputs @xmath14 and @xmath36 to the dhan layer are defined by eq .",
    "( [ eq_delta_r_v_pq ] ) . for the sign - function @xmath47 is valid , for @xmath48 and @xmath49 respectively , @xmath21 denotes the heaviside - step function . in eq .",
    "( [ eq_back_contributions ] ) the @xmath50 , @xmath51 and @xmath52 are suitable optimization rates and the @xmath53 and @xmath54 the activity levels defining active and inactive dhan neurons respectively . a suitable set of parameters , which has been used for the numerical simulations ,",
    "is given in table  [ tab_par_back ] .",
    ".the set of parameters entering the time - evolution equations of the links connecting the input to the dhan layer , with @xmath55 and @xmath56 , used in the actual simulations . [ cols=\"^,^,^,^\",options=\"header \" , ]     [ tab_par_back ]    using these definitions , the link plasticity may be written as @xmath57~ ,   \\nonumber\\ ] ] where @xmath39 is an appropriate threshold for the diffusive learning signal .",
    "the inter - layer links @xmath37 cease to be modified whenever the total input is optimal , _ viz _ when no more  mistakes  are made @xcite .",
    "we note , that a given interlayer - link @xmath37 is in general subject to competitive optimization from the three processes ( act / orth / ina ) .",
    "averaging would occur if the respective learning rates @xmath50/@xmath51/@xmath52 would be of the same order of magnitude .",
    "it is therefore necessary , that @xmath58 and @xmath59 .",
    "bars problem with a probability @xmath60 for the occurrence of the individual horizontal or vertical bars .",
    "the problem is non - linear since the pattern intensity is not enhanced when an elementary horizontal and vertical bar overlap each other.,scaledwidth=85.0% ]      it is desirable that the interlayer connections @xmath37 neither grow unbounded with time ( runaway - effect ) nor disappear into irrelevance .",
    "suitable normalization procedures are therefore normally included explicitly into the respective neural learning rules and are present implicitly in eqs .",
    "( [ eq_back_contributions ] ) and eq .",
    "( [ eq_vdot ] ) .",
    "the strength of the input - signal is optimized by eq .",
    "( [ eq_vdot ] ) both for active as well as for inactive dhan neurons , a property referred to as fan - in normalization .",
    "( [ eq_back_contributions ] ) and ( [ eq_vdot ] ) also regulate the overall strength of inter - layer links emanating from a given input layer neuron , a property called fan - out normalization .",
    "next we note , that the time scales for the intrinsic autonomous dynamics in the dhan layer and for the input signal could in principle differ substantially .",
    "potential interference problems can be avoided when learning is switched - on very fast . in this case",
    "the activation and decay rates @xmath61 for the diffusive learning signal are large and the corresponding characteristic time scales @xmath62 are smaller than both the typical time scales of the input and of the self - sustained dhan dynamics .",
    "a cognitive system needs to extract autonomously meaningful information about its environment from its sensory input data stream via signal separation and feature extraction .",
    "the identification of recurrently appearing patterns , i.e.  of objects , in the background of fluctuation and of combinations of distinct and noisy patterns , constitutes a core demand in this context .",
    "this is the domain of the independent component analysis @xcite and blind source separation @xcite , which seeks to find distinct representations of statistically independent input patterns .    in order to test our system made - up by an input layer coupled to a dhan layer , as illustrated in fig .",
    "[ fig_twolayers ] , we have selected the bars problem @xcite .",
    "the bars problem constitutes a standard non - linear reference task for the feature extraction via a non - linear independent component analysis for an @xmath63 input layer .",
    "basic patterns are the @xmath64 vertical and @xmath64 horizontal bars . the individual input patterns are made - up of a non - linear superposition of the @xmath65 basic bars , containing with probability @xmath60 any one of them , as illustrated in fig .",
    "[ figure_bars_pattern ] .",
    "bars problem the response , as defined by eq .",
    "( [ eq_clique_rec_patt ] ) , for the 10 winning coalitions in the dhan layer ( compare fig .  [ figure_networks_cliques ] and fig .  [ fig_twolayers ] ) to the ten reference patterns , _ viz _ the 5 horizontal bars and the 5 vertical bars of the @xmath66 input field . in the top row",
    "the numbering of the cliques c - i, .. ,c - x having the maximal response to the respective reference patterns is given .",
    "in the bottom row , below each of the 10 black / white reference patterns , the receptive fields , eq .",
    "( [ eq_clique_recp_fields ] ) , for the winning coalitions c - i, .. ,c - x given in the top row are given color - coded , with black / blue / red / yellow coding synaptic strengths of increasing intensities .",
    ", scaledwidth=95.0% ]      for the simulations we presented to the system about @xmath67 randomly generated @xmath68 input patterns of the type shown in fig .",
    "[ figure_bars_pattern ] .",
    "the bars pattern are black / white with the @xmath69 for active / inactive sites , irrespectively of possible overlaps of vertical and horizontal bars .",
    "the individual patterns lasted @xmath70 with about @xmath71 for the time between two successive input signals .",
    "these time scales are to be compared with the time scale of the autonomous dhan dynamics illustrated in the figs .",
    "[ figure_sensitive_periods ] and [ figure_arb_7 ] , for which the typical stability - period for a transient state is about @xmath72 .",
    "we also note that there is no active training for the system .",
    "the associative thought process continuous in the dhan layer , at no time are the neural activities reset and the system restarted .",
    "all that happens is that the ongoing associative thought process is influenced from time to time by the input layer and that then the synaptic strengths @xmath37 connecting the input layer to the dhan layer are modified via eq .",
    "( [ eq_vdot ] ) .",
    "the results for the simulation are presented in fig .",
    "[ figure_crp_graph ] . for the geometry of the dhan network we used a regular 20-site star containing 10 cliques , with every clique being composed of four neurons ,",
    "see fig .",
    "[ figure_networks_cliques ] . in fig .",
    "[ figure_crp_graph ] we present the response @xmath73 of the 10 cliques @xmath74 in the dhan layer to the 10 basic input patterns @xmath75 , the isolated bars . here",
    "the @xmath76 denotes the set of sites of the winning - coalition @xmath77 and @xmath78 its size , here @xmath79 .",
    "the response @xmath80 is equivalent to the clique averaged afferent synaptic signals @xmath14 , compare eq .",
    "( [ eq_delta_r_v_pq ] ) , in the presence of an elementary bar in the sensory input field .",
    "the individual potential winning coalitions , _ viz _ the cliques , have acquired in the course of the simulation , via the learning rule eq .",
    "( [ eq_vdot ] ) , distinct susceptibilities to the 10 bars , compare fig .  [ figure_crp_graph ] . at the start of the simulation",
    "the winning coalitions were just given by properties of the network typology , _ viz _ by the cliques , having no explicit semantic significance .",
    "the susceptibilities to the individual bars , which the cliques have acquired via the competition of the internal dhan dynamics with the sensory data input stream , can then be interpreted as a semantic assignment .",
    "the internal associative thought process of the dhan layer therefore becomes semantically meaningful via the coupling to the environment , corresponding to a sequence of horizontal and vertical bars .",
    "this learning paradigm is compatible with multi - electrode array studies of the visual cortex of developing ferrets @xcite , which indicate that the ongoing cortical dynamics is void of semantic content immediately after birth , acquiring semantic content however during the adolescence .",
    "the winning coalitions of the dhan layer are overlapping and every link @xmath37 targets in general more than one potential winning coalition in the dhan layer .",
    "this feature contrasts with the ` single - winner - takes - all ' setup , normally used for standard neural algorithms performing an independent component analysis @xcite , for which the target neurons are physically separated . for the regular 20-site network used in the simulation",
    "every dhan neuron appertains to exactly two cliques , compare fig .",
    "[ figure_networks_cliques ] . the unsupervised learning procedure , eq .",
    "( [ eq_vdot ] ) , involves therefore a competition between the contribution @xmath81 , @xmath82 and @xmath83 , as given by eq .",
    "( [ eq_back_contributions ] ) . for the simulations we used a set of parameters ,",
    "see table  [ tab_par_back ] , for which the contribution to @xmath81 is adapted at a much higher rate than the contributions to @xmath82 and @xmath83 .",
    "the responses @xmath80 of the winning coalitions are therefore close to , but somewhat below , the optimal value @xmath84 used for the simulations , compare fig .",
    "[ figure_crp_graph ] .",
    "the target value @xmath40 will not be reached even for extended simulations , due to the competition with the other optimization procedures , namely @xmath82 and @xmath83 , compare eq .",
    "( [ eq_back_contributions ] ) .",
    "bars problem the response , as defined by eq .",
    "( [ eq_clique_rec_patt ] ) , for the 20 winning coalitions c - i, .. ,c - xx in the dhan layer ( compare fig .",
    "[ figure_networks_cliques ] and fig .  [ fig_twolayers ] ) to the twenty reference patterns , _ viz _ the 10 horizontal bars and the 10 vertical bars of the @xmath85 input field .",
    ", scaledwidth=85.0% ]      the averaged receptive fields @xmath86 of the @xmath87 cliques in the dhan layer with respect to the @xmath88 input neurons are also presented in fig .",
    "[ figure_crp_graph ] .",
    "the inter - layer synaptic weights @xmath37 can be both positive and negative and the orthogonalization procedure , eq .",
    "( [ eq_back_contributions ] ) , results in complex receptive fields .",
    "the time evolution equations for the inter - layer synaptic strengths ( [ eq_vdot ] ) are optimizing , but not maximizing , the response of the winning coalition to a given input signal .",
    "the receptive fields retain consequently a certain scatter , since the optimization via eq .",
    "( [ eq_vdot ] ) ceases whenever a satisfactory signal separation has been obtained . this behavior is consistent with the ` learning by mistakes ' paradigm @xcite , which states that a cognitive system needs to learn in general only when committing a mistake .",
    "bars problem the color - coded receptive fields , eq .",
    "( [ eq_clique_recp_fields ] ) , for the 20 cliques c - i, .. ,c - xx of the dhan layer , compare fig .",
    "[ figure_crp_100 ] , with black / blue / red / yellow coding synaptic strengths of increasing intensities .",
    ", scaledwidth=85.0% ]      the simulation results for the @xmath66 bars problem presented in fig .",
    "[ figure_crp_graph ] may be generalized to larger systems . for comparison",
    "we discuss now the results for the @xmath85 bars problem , for which there are ten horizontal and ten vertical elementary bars . for the dhan network we used a regular 40-site star with 20 cliques , a straightforward generalization of the regular star illustrated in fig .",
    "[ figure_networks_cliques ] .",
    "we used otherwise exactly the same set of parameters as previously for the @xmath66 bars problem , in particular also the same number of input training patterns .",
    "no optimization of parameters has been performed .",
    "the respective responses @xmath80 and receptive fields @xmath89 ( compare eqs .",
    "( [ eq_clique_rec_patt ] ) and ( [ eq_clique_recp_fields ] ) ) are presented in fig .",
    "[ figure_crp_100 ] and [ figure_crf_100 ] .",
    "the probability for any of the 20 bars to occur in a given input pattern , like the ones for the @xmath66 bars problem illustrated in fig .",
    "[ figure_bars_pattern ] , is @xmath60 and any individual @xmath85 input patterns contains on the average @xmath90 bars superposed non - linearly .",
    "the separation of the 20 statistically independent components in the input data stream is therefore a non - trivial task .",
    "the results presented in fig .  [ figure_crp_100 ] indicate that the system performs the source separation surprisingly well , but not perfectly .",
    "the respective receptive fields , shown in fig .  [ figure_crf_100 ] , are only in part self - evident .",
    "this is , again , due to the competitive nature of the unsupervised and local learning process , which has the task to optimize the input rates to the dhan layer and not to maximize the signal - to - noise ratio .",
    "we note in this context that the system contains no prior knowledge about the nature and statistics of the input signals .",
    "in fact , the system has not been constructed in the first place to tackle the non - linear independent component task .",
    "the setup used here has been motivated by two simple guiding principles , the occurrence of self - sustained internal neural activity and the principle of competitive neural dynamics .",
    "these principles have been used in our study to examine the interplay of the self - sustained internal neural dynamics with the inflow of external information via a sensory data stream .",
    "one can therefore interpret , to a certain extend , the capability of the system to perform a non - linear independent component analysis as an example of an ` emergent cognitive capability ' .",
    "this information processing capability emerges from general construction principles and does not result from the implementation of a specific neural algorithm .",
    "a standard approach in the field of neural networks is to optimize the design of a network such that a given cognitive or computational task can be tackled efficiently .",
    "this strategy has been very successful in the past with respect to technical applications like handwriting recognition @xcite and regarding the modeling of initial feed - forward sensory information processing in cortical areas like the primary optical cortex @xcite .",
    "task - driven network design standardly results in input - driven neural networks , with cognitive computation coming to a standstill in the absence of sensory inputs .",
    "real - world cognitive systems like the human brain are however driven by their own internal dynamics and it constitutes a challenge to present and to future research in the field of neural networks to combine models of this self - sustained brain activity with the processing of sensory data .",
    "this challenge regards especially recurrent neural networks , since recurrency is an essential ingredient for the occurrence of spontaneous internal neural activities .    in this work we studied the interplay of self - generated neural states , the time - series of winning coalitions , with the sensory input for the purpose of unsupervised feature extraction .",
    "we proposed learning to be autonomously activated during the transition from one winning coalition to the subsequent one .",
    "this general principle may be implemented algorithmically in various fashions .",
    "here we used a generalized neural net ( dhan - dense homogeneous associative net ) for the autonomous generation of a time series of associatively connected winning coalitions and controlled the unsupervised extraction of input - features by an autonomously generated diffusive learning signal .",
    "we tested the algorithm for the bars problem and found good and fast learning and that the initially semantically void transient states acquired , through interaction with the data input stream , a semantic significance .",
    "further preliminary results indicate that the learning algorithm retains functionality under a wide range of conditions and for various sets of parameters .",
    "we plan to extend the simulations to various forms of temporal inputs , especially to quasi - continuous input and to natural scene analysis , and to study the embedding of the here proposed concept within the framework of a full - fledged and autonomously active cognitive system .",
    "there is a growing research effort trying to develop universal operating principles for biologically inspired cognitive systems , the rational being , that the number of genes in the human genome is by far too small for the detailed encoding of the fast array of neural algorithms the brain is capable off .",
    "there is therefore a growing consensus , that universal operating principles may be potentially of key importance also for synthetic cognitive and complex systems @xcite .",
    "the present work is motivated by this line of approach .",
    "universal operating principles for a cognitive system remain functionally operative for a wide range of environmental conditions .",
    "examples are , universal time prediction tasks for the unsupervised extraction of abstract concepts and intrinsic generalized grammars from the sensory data input stream @xcite and the optimization of complexity and information theoretical measures for closed - loop sensorimotor behavioral studies of simulated robots @xcite .",
    "the present study is motivated by a similar line of thinking , investigating the consequences of a self - sustained internal neural activity in recurrent networks , being based on the notion of transient - state and competitive neural dynamics .",
    "the long - term goal of an autonomous cognitive system is pursued in this approach via a modular approach , with each module being based on one of the above mentioned general architectural and operational principles .",
    "m. rabinovich , a. volkovskii , p. lecanda , r. huerta , h.d.i .",
    "abarbanel and g. laurent , _ dynamical encoding by networks of competing neuron groups : winnerless competition _ , physical review letters 87 ( 2001 ) 68102 .        c. gros ,",
    "_ self - sustained thought processes in a dense associative network _",
    ", in ki 2005 , u. furbach ( ed . )",
    ", springer lecture notes in artificial intelligence 3698 ( 2005 ) 366 - 379 ; also available as http://arxiv.org/abs/q-bio.nc/0508032 .                          j.l .",
    "elman , _ an alternative view of the mental lexicon _",
    ", trends in cognitive sciences 8 ( 2004 ) 301 - 306 .",
    "seth and g.m .",
    "edelman , _ environment and behavior influence the complexity of evolved neural networks _ , adaptive behavior 12 ( 2004 ) 5 - 20 ."
  ],
  "abstract_text": [
    "<S> the human brain is autonomously active , being characterized by a self - sustained neural activity which would be present even in the absence of external sensory stimuli . here </S>",
    "<S> we study the interrelation between the self - sustained activity in autonomously active recurrent neural nets and external sensory stimuli .    </S>",
    "<S> there is no a priori semantical relation between the influx of external stimuli and the patterns generated internally by the autonomous and ongoing brain dynamics . </S>",
    "<S> the question then arises when and how are semantic correlations between internal and external dynamical processes learned and built up ?    </S>",
    "<S> we study this problem within the paradigm of transient state dynamics for the neural activity in recurrent neural nets , i.e. for an autonomous neural activity characterized by an infinite time - series of transiently stable attractor states . </S>",
    "<S> we propose that external stimuli will be relevant during the sensitive periods , _ viz _ the transition period between one transient state and the subsequent semi - stable attractor . </S>",
    "<S> a diffusive learning signal is generated unsupervised whenever the stimulus influences the internal dynamics qualitatively .    for testing </S>",
    "<S> we have presented to the model system stimuli corresponding to the bars and stripes problem . </S>",
    "<S> we found that the system performs a non - linear independent component analysis on its own , being continuously and autonomously active . </S>",
    "<S> this emergent cognitive capability results here from a general principle for the neural dynamics , the competition between neural ensembles .    </S>",
    "<S> recurrent neural networks , autonomous neural dynamics , transient state dynamics , emergent cognitive capabilities </S>"
  ]
}