{
  "article_text": [
    "adaptive filtering algorithms deal with real - time learning scenarios , in which the environment is often nonstationary . in general , these algorithms need to fulfill three basic requirements : 1 ) to sequentially learn from each observation ; 2 ) to be adaptive to changing environments ; and 3 ) to be computationally efficient . among many existing algorithms that fulfill these requirements , one that has stood the test of time is the celebrated least mean squares ( lms ) algorithm .",
    "this algorithm has several interesting properties , in particular its inherent computational simplicity , and its implicit tracking ability despite its assumption of stationarity .    inspired by the success of the lms algorithm ,",
    "a `` kernelization '' has been recently proposed under the name kernel least mean squares ( klms ) algorithm  @xcite .",
    "the klms inherits many desirable properties of lms and extends it to a large class of nonlinear filtering algorithms .",
    "nevertheless , it has certain limitations that arise from its formulation as an adaptive filter in a possibly infinite dimensional feature space .",
    "specifically , if implemented naively , the representation of the filter grows linearly with the number of data samples processed .",
    "moreover , both the lms and the klms explicitly minimize the squared error between the desired and the estimated observation values , hence , they can not be naturally applied to problems with discrete observations such as class labels .",
    "to summarize , klms , in its current format ,    1 .   can not be extended to tackle discrete observations , 2",
    ".   grows indefinitely with new observations , and 3 .",
    "does not provide an explicit understanding of its tracking ability ( see below ) .",
    "the available adaptive filters in the statistical signal processing literature can be broadly categorized as 1 ) filters derived from a ( stationary ) regression framework , such as lms and recursive least squares ( rls ) , and 2 ) filters derived from a ( nonstationary ) state - space framework such as the kalman filter ( fig .",
    "[ fig : graphical : model ] ) .",
    "these two classes of algorithms are both successfully applied to nonstationary systems , and occasionally show similar computational characteristics , e.g. , extended rls can be estabilished as a special case of the kalman filter  @xcite .",
    "we seek a principled explanation  based on the bayesian filtering framework  of how lms achieves tracking despite its formulation under the assumption of stationarity , since it would allow us to systematically address the limitations of the klms discussed earlier .",
    "the connection we seek is related but distinct from that of bayesian approaches recently explored by  @xcite for kernel rls ( krls ) ; specifically , it was shown that a recursive filtering implementation of the bayesian regression framework ( gaussian processes ) naturally leads to a formulation that is equivalent to kernelization of rls .",
    "they have explicitly introduced two types of forgetting methods to enable tracking  from which we were inspired for extending the klms in this paper  but the forgetting is not incorportated within the bayesian framework .",
    "is assumed to have the same relation @xmath0 as in the classic regression setting .",
    "original derivation of lms is given in this context .",
    "( b ) non - stationary model .",
    "the weight @xmath0 evolves over time  hence the relation between @xmath1through diffusion with parameter @xmath2 .",
    "the kalman filter is derived under this model .",
    "we show the klms can also be derived from the nonstationary model .",
    ", width=307 ]    in this paper , we show that lms ( and klms ) can indeed be derived as an approximation of a state - space based bayesian filtering ( section [ sec : bayes ] ) . in order to achieve a low computational complexity , though",
    ", only the mode of the posterior distribution can be estimated and retained for each sample .",
    "this new interpretation allows us to derive extensions of the klms algorithm by tweaking the underlying state - space and observation models ( section [ sec : forgetting ] and [ sec : observation ] ) . here",
    ", we extend the klms algorithm to integer and binary - valued observations , and also introduce a forgetting factor in order to improve its tracking ability .",
    "we furthermore illustrate each extension with a simple example to demonstrate its performance in tracking nonstationary signals , and compare with existing methods such as the quantized klms ( qklms )  @xcite and naive online regularized risk minimization algorithm ( norma )  @xcite . some interesting properties of these extensions are that each algorithm learns by assigning a coefficient for each new observation , and that the time complexity of the @xmath3-th iteration has linear cost , @xmath4 , in both space and time , as is the case for the klms .",
    "both widrow & hoff s lms , and klms are derived from mean squared error cost function  @xcite which is prevalent in traditional signal processing .",
    "the filtering setting assumes a linear model @xmath5 in the _ feature space _ where @xmath6 is the feature vector associated with the input vector @xmath7 , and @xmath8 is the vector representation of the filter in a hilbert space @xmath9 .",
    "the following derivation holds for both lms and klms , taking into account that for lms the feature space is the ( euclidean ) input space itself , i.e. , @xmath10 , while klms uses a ( potentially ) infinite dimensional ( reproducing kernel ) hilbert space induced by a positive definite kernel @xmath11 where @xmath12  @xcite .",
    "the mean squared error is defined as , @xmath13,\\end{aligned}\\ ] ] where @xmath14 and @xmath15 are the random vector and variable for the input signal and the desired output , respectively .",
    "note that this cost function corresponds to a regression problem corrupted by gaussian noise assuming independence , illustrated in fig .",
    "[ fig : graphical : model]a , where given a set of @xmath3 input - output data pairs @xmath16 , the negative log - likelihood is given by @xmath17 where @xmath18 is the noise variance .",
    "in the limit of large number of _ iid _ samples , due to the law of large numbers , it converges to its expectation which is the mean squared error ( up to a factor and a constant ) .",
    "therefore , the ( asymptotic ) maximum likelihood solution under this model coincides with the minimum mse ( mmse ) solution .",
    "the basic steepest descent learning rule has the form @xmath19.\\end{aligned}\\ ] ] since the cost function is convex , it will converge to the mmse solution for a sufficiently small learning rate @xmath20 . to make an online learning rule ,",
    "a stochastic gradient descent is used in practice . in particular , the learning rule of the lms algorithm is obtained by dropping the expectation from  , which yields @xmath21 hence , after processing @xmath22 samples , the prediction for the next sample @xmath23 is given by , @xmath24 where @xmath25 is the error for each sample . for klms ,",
    "the prediction can be directly computed from the samples despite @xmath26 , since @xmath27 .",
    "the stochastic gradient descent algorithm is guaranteed to convergence ( almost surely ) to the global optimal solution under stationary and ergodic observations if a proper step size scheduling is used ( e.g. , @xmath28 and @xmath29 ) .",
    "however , the tracking capability of lms / klms is dependent on the step size ; if the step - size were annealed , it would be tracking less efficiently as more samples are seen .",
    "therefore , to have constant tracking , step size is not annealed in practice , that is , @xmath30 .",
    "then , for the price of non - zero misadjustment , the algorithms can surprisingly learn continuously from new samples , and overwrite what was learned before .",
    "note that this `` hack '' disconnects the algorithm from the graphical model fig .",
    "[ fig : graphical : model]a which inherently assumes a stationary data generation process . in the following section ,",
    "we show how this tracking ability can be derived from first principles .",
    "a slowly changing system can be explicitly described by a probabilistic model with latent dynamics on the parameter . in such a model ,",
    "each parameter associated with each sample or time is considered as an interdependent random ( latent ) variable , as illustrated in fig .",
    "[ fig : graphical : model]b .",
    "our goal is to show that the klms is an approximate sequential inference for @xmath31 .",
    "we start with a diffusion process as a reasonable model for nonstationarity : @xmath32 where @xmath33 denotes a gaussian distribution in the feature space , and @xmath34 is the variance of diffusion on each direction . the likelihood model is assumed to be a linear  gaussian model , similar to the stationary case , @xmath35 where @xmath18 is the observation noise variance .",
    "we remark that the conditional distributions and for the finite dimensional feature space is a special case of the kalman filter model with linear dynamics .      if we wish to ( recursively )",
    "infer the posterior weight distribution @xmath36 , we only need the mean and covariance , since we assume gaussianity in this model  @xcite . assuming @xmath37 , a single linear gaussian observation results in a one step evolution of the posterior as another gaussian @xmath38 , with @xmath39.\\end{aligned}\\ ] ] this recursion can be solved efficiently , and the solution is known as the extended recursive least squares algorithm  @xcite",
    ". however , it requires a quadratic number of operations in terms of the dimension of the feature vector for updating the ( inverse ) covariance matrix . in case of an infinite - dimensional feature space , the feature vector dimension grows linearly with the number of observations , rendering this approach prohibitive .",
    "therefore , in order to obtain a linear time complexity algorithm , we assume the posterior to be concentrated around the maximum .",
    "in other words , we approximate the posterior as a delta function at the maximum a posteriori ( map ) estimate @xmath40 before inferring @xmath41 .",
    "below , we show the steps for online inference rules using this approximation .",
    "first , the approximation is equivalent to assuming an isotropic gaussian around the map estimate for the previous sample . @xmath42",
    "using bayes rule , the posterior weight distribution is , @xmath43 where the parameters for the posterior are , @xmath44.\\end{aligned}\\ ] ] this can be simplified using the matrix inversion lemma , @xmath45 where the learning rate is determined by the diffusion - to - noise ratio @xmath46 .",
    "this is very similar to the normalized lms ( nlms ) update rule , although not identical . if the kernel is normalized , such that @xmath47 , then it can be simply rewritten as , @xmath48 where @xmath49 .",
    "note that the stochastic gradient derivation is identical to the approximate bayesian learning rule ; we have rederived klms with a state - space model .",
    "also , note that @xmath50 , thus we have a frequentist convergence guarantee of the weight vector to the optimal weight vector @xmath51 in mean in a stationary environment , i.e. , @xmath52 \\rightarrow { \\mathbf{w}}^\\ast$ ]  @xcite .",
    "the resulting algorithm   has linear time complexity @xmath53 where @xmath54 is the number of @xmath55 s used to represent the weight , while the extended krls requires quadratic time complexity @xmath56 at each iteration  @xcite .",
    "however , this does not come without a sacrifice .",
    "this derivation forgets the covariance of the weights after each iteration .",
    "the covariance contains the information on the uncertainty over the weight vector ; the weights with less uncertainty are updated less .",
    "however , with the covariance being approximated by a constant diagonal  , roughly speaking , the weight vector is updated equally , and independently .",
    "this brings a couple of disadvantages .",
    "first , the posterior broadness information is lost , so we can not report how much confidence we have about the current estimate .",
    "second , the update is not optimal , and we can not guarantee its asymptotic convergence to the bayesian posterior .",
    "however , as we will see in the following section , we have some practical frequentist convergence results .",
    "despite the possible slower convergence , we provide a frequentist guarantee that klms tracks the true weight under mild conditions .",
    "specifically , we assume that the true system @xmath57 slowly changes over time , i.e. , @xmath58 where @xmath59 is a small independent stochastic perturbation with @xmath60 = 0 $ ] and @xmath61 = \\sigma^2_q$ ] . from",
    ", we can write the difference in the estimate as , @xmath62 our goal is to bound the asymptotic expected norm of  .",
    "the norm can be expanded as , @xmath63 by taking the expectation on both sides , we get , @xmath64      - 2\\eta \\operatorname*{\\rm e}[e_k \\zeta_k]\\end{aligned}\\ ] ] where @xmath65 is the model mismatch error , and we have assumed @xmath66 for simplicity .",
    "notice that the error @xmath67 can be decomposed as @xmath68 where @xmath69 and @xmath70 are independent of each other , and also @xmath71 .",
    "therefore , by taking the limit @xmath72 , we obtain the steady state condition : @xmath73",
    "+ \\eta^2 \\sigma_n^2      - 2\\eta\\operatorname*{\\rm e}[\\zeta_\\infty^2 ] = 0 .",
    "\\nonumber\\\\ \\rightarrow & \\operatorname*{\\rm e}[\\zeta_\\infty^2 ]      = \\frac{\\sigma^2_q   + \\eta^2 \\sigma_n^2}{\\eta(2-\\eta)}.\\end{aligned}\\ ] ] in a stationary environment we have @xmath74 , and therefore , the steady state error disappears as @xmath20 tends to zero .",
    "however , in a nonstationary environment such is not the case , since in the limiting learning rate value the filter fails to track .",
    "thus , for tracking , one needs a finite learning rate value . a suitable learning rate interval that guarantees convergence remains to be explored .",
    "we made a theoretical connection between the klms and the kalman filter in section [ sec : bayes ] , providing a state - space interpretation to klms which explains why klms is appropriate for nonstationary environments . in this section",
    ", we extended them in a principled manner to obtain forgetting dynamics in the weights .",
    "this is achieved by a simple modification of the latent dynamics ; we will generalize the noise distribution ( likelihood function ) in section  [ sec : observation ] .    instead of a pure random walk dynamics  ,",
    "we add a leakage towards the origin , effectively forgetting the past exponentially .",
    "the resulting diffusion is a discrete - time analogue of the ornstein - uhlenbeck process ( equivalently , a first order auto - regressive process ) .",
    "@xmath75 the asymptotic marginal distribution of the prior dynamics is @xmath76 , hence the weights are isotropically distributed around the origin in the absence of observation . if @xmath77 is constant , the rkhs norm is proportional to the function norm , and the norm of the corresponding functions follows a gaussian distribution centered around the origin . as a result",
    "the learning rule becomes @xmath78 this learning rule   is very similar to that of norma , which aims to regularize the solution  @xcite .    , where @xmath79 and @xmath80 indicate temporal indices , with @xmath81 db independent additive gaussian noise .",
    "the budgets of the algorithms are fixed as follows : norma and fklms use a budget of @xmath82 centers , qklms has a budget parameter of @xmath83 , yielding a maximum of @xmath82 centers in @xmath84 $ ] , and klms is not constrained at all .",
    "pruning is accomplished in fklms and norma by dropping the oldest coefficient .",
    "a squared exponential kernel with kernel size of @xmath85 is used for all the algorithms .",
    "the mse is computed for a one - step prediction , and averaged out over @xmath86 simulations .",
    "we scan several values of the parameters , and report the parameter with least average normalized mse over the iterations .",
    "asymptotic normalized mse is estimated from 800 samples after time step 200 ( error bars indicate 2 standard error ) .",
    "interestingly , we observe that klms and qklms , which lack a forgetting mechanism , obtain weaker tracking performance . ]",
    "note that the learning rule   can be expanded , and rewritten as , @xmath87 where @xmath88 is a scalar corresponding to the coefficient at the learning step .",
    "we can see that each effective coefficient @xmath89 for each @xmath90 shrinks geometrically over time .",
    "thus , the effect of older observation to the current weight estimate is small in general .",
    "note , however , that the algorithm forgets not by making the covariance larger as in the kalman filter , but by changing the the mean .    like norma , the forgetting dynamics klms can be interpreted as introducing a regularization for the weight vector .",
    "however , as shown in @xcite , klms is self - regularizing and hence such extra regularization is usually not necessary .",
    "however , it provides a significant practical benefit for maintaining a compact representation  we can prune the @xmath90 component that are too small to have any effect .",
    "a simple strategy of pruning is to drop effective coefficients below a small pre - specified threshold .",
    "then , assuming a stationary distribution over the new coefficients , the forgetting factor @xmath91 and the threshold control the expected representation length .",
    "one can also maintain a fixed budget by removing the oldest coefficient after a predefined number of centers is reached .",
    "we employ the latter strategy in the experiment shown in fig .",
    "[ fig : tracking : fklms ] .",
    "gaussian observation model   is widely used for continuous observations , however , it is inappropriate where the observations are natural numbers , or binary labels . in this section ,",
    "we extend klms by replacing the gaussian observation model with other distributions in the exponential family .",
    "poisson likelihood is widely used when the observations are natural numbers : @xmath92 .",
    "for example , in neuroscience , neural response is often quantified by the number of spikes , and tracking how the neural code changes during experiment is of great importance  @xcite .",
    "we use the canonical inverse link function ( exponential ) for the poisson distribution to map the linear ( or nonlinear ) function from the input to the non - negative rate parameter , i.e. , @xmath93 to derive the adaptive filtering algorithm , once again , we approximate the current state given the previous observations as , for which the log prior is , @xmath94 therefore , using bayes rule , the posterior at time @xmath95 is , @xmath96 where irrelevant constants are omitted .",
    "we need to maximize this log - posterior over @xmath97 to estimate @xmath98 .",
    "the map estimate must satisfy , the stationary condition @xmath99 , which implies , @xmath100 we observe that the solution of can be expressed as , @xmath101 where @xmath102 is a scalar , and therefore , we can rewrite the log - posterior as , @xmath103 where @xmath104 , and we have assumed a normalized kernel for simplicity .",
    "thus , we reduce the problem of finding an infinite dimensional weight vector to a one dimensional optimization . although , there is no analytical solution , the cost function @xmath105 is strictly concave and therefore , its maximum can be easily found by existing optimization tools .",
    "the complexity of this algorithm is still @xmath4 , with a constant overhead for solving a concave maximization problem at each step .",
    "we demonstrate its performance on a neurally inspired example in fig .",
    "[ fig : tracking : poisson ] .",
    "a typical nonlinear response function ( tuning curve ) is set to shift its center slowly , creating a non - stationary tracking problem .",
    "the poisson - klms extension correctly tracks , and maintains a small mse throughout the experiment .     where @xmath106 constantly drifted @xmath107 degrees during the @xmath108 iterations .",
    "we measure the normalized estimation error between the true tuning curve and the estimated curve .",
    "insets show the actual function estimate at 25 , 50 , 100 , 500 time steps .",
    "gray lines show 11 repeats of the experiment , and the dark curves correspond to their average .",
    "the kernel was @xmath109 and @xmath110 . ]      similarly , bernoulli likelihood is widely used when the observations are binary labels , such as in a classification problem .",
    "here we only address the binary classification problem , while the generalization to multi - class classification is certainly feasible , and straightforward .",
    "we use the inverse canonical link function ( logistic ) for the bernoulli distribution to map the linear ( or nonlinear ) function from the input to the non - negative probability score between @xmath84 $ ] , i.e. , @xmath111 where @xmath112 , and @xmath113 is the bernoulli distribution with probability of success @xmath114 .",
    "then the posterior log - liklihood can be written as , @xmath115 once again , we need to maximize this likelihood to get @xmath116 . taking derivative of this function , we get , @xmath117 as in the poisson case , it can be observed that the stationary point for which the gradient is zero , can be expressed as @xmath118 .",
    "so , we need to only solve for @xmath102 by maximizing @xmath119 where @xmath120 .",
    "the cost function @xmath105 is again strictly concave , and thus , the optimization problem can be solved efficiently . in fig .",
    "[ fig : tracking : logistic ] , we demonstrate the effectiveness of this approach in tracking a shifting binary classification boundary .     to @xmath121 ) .",
    "three contours represent 0.25 , 0.5 , and 0.75 probability of the posterior .",
    "the kernel was @xmath122 and @xmath123 . ]",
    "it should be noted that both poisson - klms and bernoulli - klms do not have an explicit learning rate parameter .",
    "this is because the observation model is not gaussian , where the learning rate parameter is simply the ratio between the diffusion variance and the noise variance .",
    "however , in these extensions the diffusion variance plays a similar role ; the lower the @xmath34 , the slower the adaption process .",
    "we acquired data from a wireless communication test bed that is used to evaluate the performance of digital communication systems in realistic indoor environments .",
    "the platform is composed of several transmit and receive nodes , each one including a radio - frequency front - end and baseband hardware for signal generation and acquisition .",
    "the front - end also incorporates a programmable variable attenuator to control the transmit power value and therefore the signal saturation .",
    "a more detailed description of the test bed can be found in @xcite . using the hardware platform",
    ", we transmitted clipped orthogonal frequency - division multiplexing ( ofdm ) signals centered at @xmath124 ghz over real frequency - selective and time - varying channels , with normalized doppler frequency around @xmath125 .",
    "the transmit amplifier was operated close to saturation . in this experiment",
    "the transmitted and received signals are used to track the variations of the nonlinear channel .",
    "we compare 4 algorithms with hyperparameters tuned using the first 500 samples .",
    "[ fig : tracking : testbed ] displays the one - step ahead prediction normalized mean squared error ( nmse ) of the tracking experiment .",
    "average nmse are : nlms @xmath126 , norma @xmath127 , qklms @xmath128 , fklms @xmath129  db . fklms and norma used 500 total basis functions .",
    "qklms and fklms show similar performances better than norma and ( linear ) nlms .",
    "in this paper , we derived a family of linear time and space complexity kaf algorithms from bayesian filtering by maintaining only the map solution at each iteration and discarding the posterior distribution ( summarized by the covariance ) .",
    "one of the basic resulting algorithms is identical to klms , which is simple and practical .",
    "the tracking ability of lms / klms is usually understood by its stochastic nature that allows it to continually adjust itself to the non - stationary environment .",
    "we provide an alternate explanation of this mechanism by showing the klms can also be seen as an approximation to state - space modeling which possesses explicit tracking abilities .",
    "our framework allows flexibility in the state - space models which can be used to induce forgetting behavior , as well as novel observation noise models , such as poisson and bernoulli .    the optimal nonlinear bayesian filtering for gaussian diffusion dynamics , given by either eq .   or eq . , and a gaussian observation model can be iteratively solved by extended kernel recursive least squares algorithm  @xcite . for a general dynamics and a general likelihood such as , or , we often do not have a closed form iterative solution , and one must resort to slower sampling based inference such as sequential monte carlo algorithm or approximate inferences .",
    "exact optimal solutions are impractical due to its high computational cost .",
    "hence , certain approximations must be made ; for example , @xcite uses a low - dimensional subspace approximation .",
    "we have derived kafs for natural number and binary observation , and it can be also extended to multi - class by using multinomial - logistic model .",
    "however , we do not have convergence results for poisson and logistic variants as in the gaussian version where it can be shown that for @xmath130 the solution converges in mean .",
    "our algorithms have a few hyperparameters that need to be set ahead ; the kernel parameters , the diffusion variance , and the likelihood parameters .",
    "we have not presented a formal way for choosing those hyperparameters , which plays a crucial role in the speed of convergence and the tracking ability of the algorithm . if training time series is available , one can use expectation - maximization or sampling methods to find appropriate parameters  @xcite .",
    "we leave these as future work .",
    "j.  gutirrez ,  .",
    "gonzlez , j.  prez , d.  ramrez , l.  vielva , j.  ibez , and i.  santamara .",
    "frequency - domain methodology for measuring mimo channels using a generic test bed . ,",
    "60(3):827838 , march 2011 ."
  ],
  "abstract_text": [
    "<S> the kernel least mean squares ( klms ) algorithm is a computationally efficient nonlinear adaptive filtering method that `` kernelizes '' the celebrated ( linear ) least mean squares algorithm . </S>",
    "<S> we demonstrate that the least mean squares algorithm is closely related to the kalman filtering , and thus , the klms can be interpreted as an approximate bayesian filtering method . </S>",
    "<S> this allows us to systematically develop extensions of the klms by modifying the underlying state - space and observation models . </S>",
    "<S> the resulting extensions introduce many desirable properties such as `` forgetting '' , and the ability to learn from discrete data , while retaining the computational simplicity and time complexity of the original algorithm . </S>"
  ]
}