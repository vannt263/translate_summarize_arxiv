{
  "article_text": [
    "many systems of current interest to the scientific community can usefully be represented as networks  @xcite .",
    "examples include the internet  @xcite and the world - wide web  @xcite , social networks  @xcite , citation networks  @xcite , food webs  @xcite , and biochemical networks  @xcite .",
    "each of these networks consists of a set of nodes or vertices representing , for instance , computers or routers on the internet or people in a social network , connected together by links or edges , representing data connections between computers , friendships between people , and so forth .",
    "one network feature that has been emphasized in recent work is community structure , the gathering of vertices into groups such that there is a higher density of edges within groups than between them  @xcite .",
    "the problem of detecting such communities within networks has been well studied .",
    "early approaches such as the kernighan ",
    "lin algorithm  @xcite , spectral partitioning  @xcite , or hierarchical clustering  @xcite work well for specific types of problems ( particularly graph bisection or problems with well defined vertex similarity measures ) , but perform poorly in more general cases  @xcite .    to combat this problem a number of new algorithms",
    "have been proposed in recent years .",
    "girvan and newman  @xcite proposed a divisive algorithm that uses edge betweenness as a metric to identify the boundaries of communities . this algorithm has been applied successfully to a variety of networks , including networks of email messages , human and animal social networks , networks of collaborations between scientists and musicians , metabolic networks and gene networks  @xcite .",
    "however , as noted in  @xcite , the algorithm makes heavy demands on computational resources , running in @xmath8 time on an arbitrary network with @xmath1 edges and @xmath0 vertices , or @xmath9 time on a sparse graph ( one in which @xmath10 , which covers most real - world networks of interest ) .",
    "this restricts the algorithm s use to networks of at most a few thousand vertices with current hardware .",
    "more recently a number of faster algorithms have been proposed  @xcite . in  @xcite ,",
    "one of us proposed an algorithm based on the greedy optimization of the quantity known as modularity  @xcite .",
    "this method appears to work well both in contrived test cases and in real - world situations , and is substantially faster than the algorithm of girvan and newman .",
    "a naive implementation runs in time @xmath11 , or @xmath12 on a sparse graph .",
    "here we propose a new algorithm that performs the same greedy optimization as the algorithm of  @xcite and therefore gives identical results for the communities found .",
    "however , by exploiting some shortcuts in the optimization problem and using more sophisticated data structures , it runs far more quickly , in time @xmath2 where @xmath3 is the depth of the `` dendrogram '' describing the network s community structure .",
    "many real - world networks are sparse , so that @xmath4 ; and moreover , for networks that have a hierarchical structure with communities at many scales , @xmath5 . for such networks",
    "our algorithm has essentially linear running time , @xmath6 .",
    "this is not merely a technical advance but has substantial practical implications , bringing within reach the analysis of extremely large networks .",
    "networks of ten million vertices or more should be possible in reasonable run times .",
    "as an example , we give results from the application of the algorithm to a recommender network of books from the online bookseller amazon.com , which has more than @xmath7 vertices and two million edges .",
    "modularity  @xcite is a property of a network and a specific proposed division of that network into communities .",
    "it measures when the division is a good one , in the sense that there are many edges within communities and only a few between them .",
    "let @xmath13 be an element of the adjacency matrix of the network thus : @xmath14 and suppose the vertices are divided into communities such that vertex  @xmath15 belongs to community  @xmath16 .",
    "then the fraction of edges that fall within communities , i.e. ,  that connect vertices that both lie in the same community , is @xmath17 where the @xmath18-function @xmath19 is 1 if @xmath20 and 0 otherwise , and @xmath21 is the number of edges in the graph .",
    "this quantity will be large for good divisions of the network , in the sense of having many within - community edges , but it is not , on its own , a good measure of community structure since it takes its largest value of  1 in the trivial case where all vertices belong to a single community .",
    "however , if we subtract from it the expected value of the same quantity in the case of a randomized network , we do get a useful measure .",
    "the degree  @xmath22 of a vertex  @xmath15 is defined to be the number of edges incident upon it : @xmath23 the probability of an edge existing between vertices @xmath15 and @xmath24 if connections are made at random but respecting vertex degrees is @xmath25 .",
    "we define the modularity  @xmath26 to be @xmath27      \\delta(c_v , c_w ) .",
    "\\label{modularity}\\ ] ] if the fraction of within - community edges is no different from what we would expect for the randomized network , then this quantity will be zero .",
    "nonzero values represent deviations from randomness , and in practice it is found that a value above about 0.3 is a good indicator of significant community structure in a network .",
    "if high values of the modularity correspond to good divisions of a network into communities , then one should be able to find such good divisions by searching through the possible candidates for ones with high modularity . while finding the global maximum modularity over all possible divisions seems hard in general , reasonably good solutions can be found with approximate optimization techniques .",
    "the algorithm proposed in  @xcite uses a greedy optimization in which , starting with each vertex being the sole member of a community of one , we repeatedly join together the two communities whose amalgamation produces the largest increase in  @xmath26 . for a network of @xmath0 vertices ,",
    "after @xmath28 such joins we are left with a single community and the algorithm stops .",
    "the entire process can be represented as a tree whose leaves are the vertices of the original network and whose internal nodes correspond to the joins .",
    "this dendrogram represents a hierarchical decomposition of the network into communities at all levels .",
    "the most straightforward implementation of this idea ( and the only one considered in  @xcite ) involves storing the adjacency matrix of the graph as an array of integers and repeatedly merging pairs of rows and columns as the corresponding communities are merged . for the case of the sparse graphs that are of primary interest in the field",
    ", however , this approach wastes a good deal of time and memory space on the storage and merging of matrix elements with value  0 , which is the vast majority of the adjacency matrix .",
    "the algorithm proposed in this paper achieves speed ( and memory efficiency ) by eliminating these needless operations .    to simplify the description of our algorithm",
    "let us define the following two quantities : @xmath29 which is the fraction of edges that join vertices in community  @xmath30 to vertices in community  @xmath31 , and @xmath32 which is the fraction of ends of edges that are attached to vertices in community  @xmath30 . then , writing @xmath33 , we have , from eq .",
    "( [ modularity ] ) @xmath34        \\sum_i\\delta(c_v , i)\\delta(c_w , i)\\nonumber\\\\    & = & \\sum_i \\biggl [ { 1\\over2 m } \\sum_{vw } a_{vw } \\,\\delta(c_v , i)\\delta(c_w , i )          \\nonumber\\\\    & & \\qquad { } - { 1\\over2m}\\sum_v k_v \\,\\delta(c_v , i )             { 1\\over2m}\\sum_w k_w\\delta(c_w , i ) \\biggr]\\nonumber\\\\    & = & \\sum_i ( e_{ii } - a_i^2).\\end{aligned}\\ ] ] the operation of the algorithm involves finding the changes in @xmath26 that would result from the amalgamation of each pair of communities , choosing the largest of them , and performing the corresponding amalgamation .",
    "one way to envisage ( and implement ) this process is to think of network as a multigraph , in which a whole community is represented by a vertex , bundles of edges connect one vertex to another , and edges internal to communities are represented by self - edges .",
    "the adjacency matrix of this multigraph has elements@xmath35 , and the joining of two communities @xmath30 and @xmath31 corresponds to replacing the @xmath30th and @xmath31th rows and columns by their sum . in the algorithm of  @xcite this operation is done explicitly on the entire matrix , but if the adjacency matrix is sparse ( which we expect in the early stages of the process ) the operation can be carried out more efficiently using data structures for sparse matrices .",
    "unfortunately , calculating @xmath36 and finding the pair @xmath37 with the largest @xmath36 then becomes time - consuming .    in our new algorithm , rather than maintaining the adjacency matrix and calculating  @xmath36 , we instead maintain and update a matrix of value of @xmath36 .",
    "since joining two communities with no edge between them can never produce an increase in  @xmath26 , we need only store @xmath36 for those pairs @xmath37 that are joined by one or more edges . since this matrix has the same support as the adjacency matrix , it will be similarly sparse , so we can again represent it with efficient data structures .",
    "in addition , we make use of an efficient data structure to keep track of the largest @xmath36 .",
    "these improvements result in a considerable saving of both memory and time .    in total",
    ", we maintain three data structures :    1 .   a sparse matrix containing @xmath36 for each pair @xmath37 of communities with at least one edge between them .",
    "we store each row of the matrix both as a balanced binary tree ( so that elements can be found or inserted in @xmath38 time ) and as a max - heap ( so that the largest element can be found in constant time ) .",
    "2 .   a max - heap @xmath39 containing the largest element of each row of the matrix @xmath36 along with the labels @xmath37 of the corresponding pair of communities .",
    "3 .   an ordinary vector array with elements  @xmath40 .    as described above we start off with each vertex being the sole member of a community of one , in which case @xmath41 if @xmath30 and @xmath31 are connected and zero otherwise , and @xmath42 .",
    "thus we initially set @xmath43 and @xmath44 for each  @xmath30 .",
    "( this assumes the graph is unweighted ; weighted graphs are a simple generalization  @xcite . )",
    "our algorithm can now be defined as follows .    1 .",
    "calculate the initial values of @xmath36 and @xmath40 according to  ( [ eq : qinit ] ) and  ( [ eq : ainit ] ) , and populate the max - heap with the largest element of each row of the matrix @xmath45 .",
    "2 .   select the largest @xmath36 from @xmath39 , join the corresponding communities , update the matrix @xmath45 , the heap @xmath39 and @xmath40 ( as described below ) and increment @xmath26 by @xmath36 .",
    "repeat step 2 until only one community remains .",
    "our data structures allow us to carry out the updates in step 2 quickly .",
    "first , note that we need only adjust a few of the elements of @xmath45 .",
    "if we join communities @xmath30 and  @xmath31 , labeling the combined community  @xmath31 , say , we need only update the @xmath31th row and column , and remove the @xmath30th row and column altogether .",
    "the update rules are as follows .    if community @xmath46 is connected to both @xmath30 and @xmath31 , then @xmath47 if @xmath46 is connected to @xmath30 but not to  @xmath31 , then @xmath48 if @xmath46 is connected to @xmath31 but not to @xmath30 , then @xmath49    note that these equations imply that @xmath26 has a single peak over the course of the algorithm , since after the largest @xmath45 becomes negative all the @xmath45 can only decrease .    to analyze how long the algorithm takes using our data structures , let us denote the degrees of @xmath30 and @xmath31 in the reduced graph  i.e .",
    ",  the numbers of neighboring communities  as @xmath50 and @xmath51 respectively .",
    "the first operation in a step of the algorithm is to update the @xmath31th row .",
    "to implement eq .",
    "( [ eq : both ] ) , we insert the elements of the @xmath30th row into the @xmath31th row , summing them wherever an element exists in both columns .",
    "since we store the rows as balanced binary trees , each of these @xmath50 insertions takes @xmath52 time .",
    "we then update the other elements of the @xmath31th row , of which there are at most @xmath53 , according to eqs .",
    "( [ eq : justi ] ) and  ( [ eq : justj ] ) . in the @xmath46th row",
    ", we update a single element , taking @xmath54 time , and there are at most @xmath53 values of @xmath46 for which we have to do this .",
    "all of this thus takes @xmath55 time .",
    "we also have to update the max - heaps for each row and the overall max - heap @xmath39 . reforming the max - heap corresponding to the @xmath31th row",
    "can be done in @xmath56 time  @xcite .",
    "updating the max - heap for the @xmath46th row by inserting , raising , or lowering @xmath57 takes @xmath58 time . since we have changed the maximum element on at most @xmath53 rows , we need to do at most @xmath53 updates of @xmath39 , each of which takes @xmath38 time , for a total of @xmath55 .    finally ,",
    "the update @xmath59 ( and @xmath60 ) is trivial and can be done in constant time .",
    "since each join takes @xmath55 time , the total running time is at most @xmath38 times the sum over all nodes of the dendrogram of the degrees of the corresponding communities .",
    "let us make the worst - case assumption that the degree of a community is the sum of the degrees of all the vertices in the original network comprising it . in that case",
    ", each vertex of the original network contributes its degree to all of the communities it is a part of , along the path in the dendrogram from it to the root .",
    "if the dendrogram has depth  @xmath3 , there are at most @xmath3 nodes in this path , and since the total degree of all the vertices is  @xmath61 , we have a running time of @xmath62 as stated .",
    "we note that , if the dendrogram is unbalanced , some time savings can be gained by inserting the sparser row into the less sparse one .",
    "in addition , we have found that in practical situations it is usually unnecessary to maintain the separate max - heaps for each row .",
    "these heaps are used to find the largest element in a row quickly , but their maintenance takes a moderate amount of effort and this effort is wasted if the largest element in a row does not change when two rows are amalgamated , which turns out often to be the case .",
    "thus we find that the following simpler implementation works quite well in realistic situations : if the largest element of the @xmath46th row was @xmath63 or @xmath57 and is now reduced by eq .",
    "( [ eq : justi ] ) or  ( [ eq : justj ] ) , we simply scan the @xmath46th row to find the new largest element .",
    "although the worst - case running time of this approach has an additional factor of  @xmath0 , the average - case running time is often better than that of the more sophisticated algorithm .",
    "it should be noted that the hierarchies generated by these two versions of our algorithm will differ slightly as a result of the differences in how ties are broken for the maximum element in a row .",
    "however , we find that in practice these differences do not cause significant deviations in the modularity , the community size distribution , or the composition of the largest communities .",
    "over the course of the algorithm ( the @xmath64 axis shows the number of joins ) . its maximum value is @xmath65 , where the partition consists of @xmath66 communities . ]",
    "the output of the algorithm described above is precisely the same as that of the slower hierarchical algorithm of  @xcite .",
    "the much improved speed of our algorithm however makes possible studies of very large networks for which previous methods were too slow to produce useful results . here",
    "we give one example , the analysis of a co - purchasing or `` recommender '' network from the online vendor amazon.com .",
    "amazon sells a variety of products , particularly books and music , and as part of their web sales operation they list for each item  a the ten other items most frequently purchased by buyers of  a. this information can be represented as a directed network in which vertices represent items and there is a edge from item  a to another item  b if b was frequently purchased by buyers of  a. in our study we have ignored the directed nature of the network ( as is common in community structure calculations ) , assuming any link between two items , regardless of direction , to be an indication of their similarity .",
    "the network we study consists of items listed on the amazon web site in august 2003 .",
    "we concentrate on the largest component of the network , which has @xmath67 items and @xmath68 edges .",
    "the dendrogram for this calculation is of course too big to draw , but fig .",
    "[ fig : q ] illustrates the modularity over the course of the algorithm as vertices are joined into larger and larger groups .",
    "the maximum value is @xmath65 , which is high as calculations of this type go  @xcite and indicates strong community structure in the network .",
    "the maximum occurs when there are @xmath66 communities with a mean size of @xmath69 items each .",
    "[ fig : visual ] gives a visualization of the community structure , including the major communities , smaller `` satellite '' communities connected to them , and `` bridge '' communities that connect two major communities with each other .    [ cols=\"^,>,<\",options=\"header \" , ]     looking at the largest communities in the network , we find that they tend to consist of items ( books , music ) in similar genres or on similar topics . in table  [ table : labels ] , we give informal descriptions of the ten largest communities , which account for about 87% of the entire network .",
    "the remainder is generally divided into small , densely connected communities that represent highly specific co - purchasing habits , e.g. ,  major works of science fiction ( @xmath70 items ) , music by john cougar mellencamp ( @xmath71 items ) , and books about ( mostly female ) spies in the american civil war ( @xmath72 items ) .",
    "it is worth noting that because few real - world networks have community metadata associated with them to which we may compare the inferred communities , this type of manual check of the veracity and coherence of the algorithm s output is often necessary .    , which corresponds to an exponent of @xmath73 for the raw probability distribution . ]",
    "one interesting property recently noted in some networks  @xcite is that when partitioned at the point of maximum modularity , the distribution of community sizes  @xmath74 appears to have a power - law form @xmath75 for some constant  @xmath76 , at least over some significant range .",
    "the amazon co - purchasing network also seems to exhibit this property , as we show in fig .",
    "[ fig : distribution ] , with an exponent @xmath77 .",
    "it is unclear why such a distribution should arise , but we speculate that it could be a result either of the sociology of the network ( a power - law distribution in the number of people interested in various topics ) or of the dynamics of the community structure algorithm .",
    "we propose this as a direction for further research .",
    "we have described a new algorithm for inferring community structure from network topology which works by greedily optimizing the modularity .",
    "our algorithm runs in time @xmath2 for a network with @xmath0 vertices and @xmath1 edges where @xmath3 is the depth of the dendrogram . for networks that are hierarchical , in the sense that there are communities at many scales and the dendrogram is roughly balanced , we have @xmath5 . if the network is also sparse , @xmath4 , then the running time is essentially linear , @xmath6",
    "this is considerably faster than most previous general algorithms , and allows us to extend community structure analysis to networks that had been considered too large to be tractable .",
    "we have demonstrated our algorithm with an application to a large network of co - purchasing data from the online retailer amazon.com .",
    "our algorithm discovers clear communities within this network that correspond to specific topics or genres of books or music , indicating that the co - purchasing tendencies of amazon customers are strongly correlated with subject matter",
    ". our algorithm should allow researchers to analyze even larger networks with millions of vertices and tens of millions of edges using current computing resources , and we look forward to seeing such applications .",
    "the authors are grateful to amazon.com and eric promislow for providing the purchasing network data .",
    "this work was funded in part by the national science foundation under grant phy-0200909 ( ac , cm ) and by a grant from the james s. mcdonell foundation ( mejn ) .",
    "j.  m. kleinberg , s.  r. kumar , p.  raghavan , s.  rajagopalan , and a.  tomkins , the web as a graph : measurements , models and methods . in _ proceedings of the international conference on combinatorics and computing _ , number 1627 in lecture notes in computer science , pp .",
    "118 , springer , berlin ( 1999 ) .",
    "community structure is sometimes referred to as `` clustering '' in sociology or computer science , but this term is commonly used to mean something else in the physics literature  @xcite , so to prevent confusion we avoid it here .",
    "we note also that the problem of finding communities in a network is somewhat ill - posed , since we havent defined precisely what a community is .",
    "a number of definitions have been proposed  @xcite , but none is standard .",
    "p.  holme and m.  huss , discovery and analysis of biochemical subnetwork hierarchies . in r.  gauges , u.  kummer , j.  pahle , and u.  rost ( eds . ) , _ proceedings of the 3rd workshop on computation of biochemical pathways and genetic networks _ , pp .",
    "39 , logos , berlin ( 2003 ) .",
    "j.  r. tyler , d.  m. wilkinson , and b.  a. huberman , email as spectroscopy : automated discovery of community structure within organizations . in m.",
    "huysman , e.  wenger , and v.  wulf ( eds . ) , _ proceedings of the first international conference on communities and technologies _ ,",
    "kluwer , dordrecht ( 2003 ) ."
  ],
  "abstract_text": [
    "<S> the discovery and analysis of community structure in networks is a topic of considerable recent interest within the physics community , but most methods proposed so far are unsuitable for very large networks because of their computational cost . here </S>",
    "<S> we present a hierarchical agglomeration algorithm for detecting community structure which is faster than many competing algorithms : its running time on a network with @xmath0 vertices and @xmath1 edges is @xmath2 where @xmath3 is the depth of the dendrogram describing the community structure . </S>",
    "<S> many real - world networks are sparse and hierarchical , with @xmath4 and @xmath5 , in which case our algorithm runs in essentially linear time , @xmath6 . as an example of the application of this algorithm we use it to analyze a network of items for sale on the web - site of a large online retailer , items in the network being linked if they are frequently purchased by the same buyer . </S>",
    "<S> the network has more than @xmath7 vertices and 2 million edges . </S>",
    "<S> we show that our algorithm can extract meaningful communities from this network , revealing large - scale patterns present in the purchasing habits of customers . </S>"
  ]
}