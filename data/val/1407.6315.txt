{
  "article_text": [
    "a class of algorithms originated for minimizing or maximizing a function @xmath0 , while satisfying some constraints @xmath1 . in the history of optimization , the function @xmath0 and the constraints @xmath1",
    "were linear and the problem was known as linear programming ( lp ) .",
    "one of the early published algorithms for solving the linear programming was given by dantzig , popularly known as simplex method @xcite . as the number of dimensions and constraints increased , solving the lp using simplex method became hard .",
    "the inability of simplex method was that it could not solve lp in polynomial time .",
    "khachiyan proposed ellipsoid algorithm as an alternative to simplex method and proved that it could reach the solution iteratively in polynomial time @xcite .",
    "the practical infeasible condition of ellipsoid algorithm led to the evolution of several interior or barrier point methods .",
    "one of the well known interior point methods is karmarkar s method proposed by narendra karmarkar @xcite .",
    "binary classification is one of the active research areas in machine learning @xcite .",
    "there are several ways to train a binary classifier .",
    "the class labels of a data set can be stored and retrieved during classification using the approach of nearest neighbor @xcite .",
    "a hyperplane is learnt for classification by training a neural network , which may not always be optimal @xcite .",
    "vapnik and others formulated the problem of classification as optimization .",
    "this method is known as support vector machines ( svms ) @xcite .",
    "sequential minimal optimization ( smo ) is a technique which solves the optimization problem in svms @xcite .",
    "decision trees , bagging , and boosting techniques are also used in binary classification @xcite .",
    "nearest neighbor method does not involve any modeling to reduce the storage of the training data set @xcite . on the other hand , neural network and svm model the data with an objective function to estimate a hyperplane , which is used in classification . in neural network approach ,",
    "the objective function is a least squares , which is quadratic in nature and is minimized for the given data set .",
    "the hyperplane obtained from a neural network may or may not be optimal , since it depends on the number of layers and weights used to train the network .",
    "svm uses quadratic programming formulation with linear constraints for minimizing the objective function @xcite .",
    "several variants of even though the objective function used in neural network or svm is a quadratic programming problem , the constraints are linear .    if there is a way to model linear constraints as quadratic constraints , then the objective function becomes quadratically constrained quadratic programming ( qcqp ) . in this paper , binary classification",
    "is posed as a qcqp problem and a novel solution is proposed using particle swarm optimization ( pso ) .",
    "one of the advantages of this approach is that it solves the qcqp problem without the need for gradient estimation .",
    "the paper is organized as follows : qcqp and pso are described in the background section and the solution for quadratically constrained quadratic programming using particle swarms is described in sec .",
    "the proposed method is compared with khachiyan s and karmarkar s algorithms for linear programming and with neural networks and svm for quadratic programming in sec . 4 on experiments and results .",
    "section 5 concludes the paper with suggestions for future work .",
    "the formulation of qcqp and pso are described in the subsections below .",
    "a general quadratically constrained quadratic programming problem is expressed as follows @xcite : @xmath2 where @xmath3 .",
    "if @xmath4 are positive semi - definite @xmath5 , then the problem becomes convex qcqp .",
    "particle swarm optimization was proposed for optimizing in the weights space of a neural network @xcite .",
    "pso has been applied to numerous applications for optimizing non - linear functions @xcite .",
    "pso evolved by simulating bird flocking and fish schooling .",
    "the advantages of pso are that it is simple in conception and easy to implement .",
    "particles are deployed in search space and each particle is evaluated against an optimization function .",
    "the best particle is chosen as a directing agent for the rest .",
    "the velocity of each particle is controlled by both the particle s personal best and the global best . during the movement of the particles ,",
    "a few of them may reach the global best .",
    "the movement of particles is adapted from the genetic algorithms or evolutionary programming .",
    "let * x * = @xmath6 be the particles deployed in the search space of the optimization function , where @xmath7 is the number of particles and * v * = @xmath8 are the velocities of the respective particles .",
    "@xmath9 for all the @xmath7 particles .",
    "a simple pso update is as follows ,    * velocity update equation + @xmath10 where @xmath11 is the weight for the previous velocity ; @xmath12 , @xmath13 are constants and @xmath14 , @xmath15 are random values varied in each iteration .",
    "@xmath16 is the personal best value for particle @xmath17 and @xmath18 is the global best value among all the particles .",
    "@xmath19 is the updated velocity of the @xmath20 particle in the @xmath21 iteration and @xmath22 is the velocity value in the @xmath23 iteration .",
    "@xmath24 is the position of the @xmath20 particle after the @xmath23 iteration . * for position update",
    ", the updated velocity is added to the existing position of the particle .",
    "the position update equation is + @xmath25",
    "our interest lies in determining the shortest path between the two non - intersecting ellipsoids .",
    "the shortest path between the two ellipsoids in @xmath26 can be formulated as a convex qcqp problem .",
    "arriving at the two end points of the shortest path , one on each ellipsoid , can be posed as minimization of @xmath0 in a quadratic form and formulated as follows .",
    "@xmath27 where @xmath28 and @xmath29,@xmath30 are non - intersecting regions in @xmath26 , @xmath31 .",
    "@xmath32 and @xmath33 are the matrices depicting the ellipsoids used in the optimization .    in case",
    "the euclidean distance metric is used for minimization of the path length , then @xmath34 is the identity matrix .",
    "the modified equation is , @xmath35    suppose one end point in the shortest path is known and fixed as @xmath36 .",
    "then , eq ( 5 ) can be reformulated as , @xmath37    figure [ theoryexample ] shows the ellipsoid with the covariance matrix @xmath32 with points @xmath38 ( inside ) , @xmath39 ( on the boundary ) and @xmath36 ( outside ) .",
    "the boundary point @xmath39 is nearest to the point @xmath36 outside the ellipsoid .",
    "we need to determine the unknown @xmath39 by the minimization of @xmath0 .",
    "the novelty of this paper is the application of particle swarms for solving the qcqp problem .",
    "particle swarms are deployed within the ellipsoid to determine @xmath39 .",
    "the function @xmath0 is evaluated for each particle in the search space .",
    "the particle with the minimum @xmath0 is considered as the closest to the point @xmath36 .",
    "only one particle of the swarm is shown in figure [ theoryexample ] for ease of representation .     inside and",
    "a point @xmath40 on its boundary nearest to the point @xmath36 outside it ( boundary point on the other ellipsoid ) .",
    "the dotted lines connecting the point @xmath36 to the points @xmath38 and @xmath40 are shown .",
    "the desired direction of movement from @xmath38 is also shown by an arrow , which is required to reach point @xmath40.,width=453 ]    pso is a stochastic evolutionary algorithm , which takes several generations to reach the optimal value and its performance depends on the initialization .",
    "the velocity update equation of the pso algorithm is modified by including the function @xmath0 .",
    "the addition of evaluation function restricts the particle from moving away from the actual course towards the global best position .",
    "this addition provides an advantage in computation .",
    "however , it also constrains the particle to move in a particular direction . to counter this effect",
    ", we add a craziness term in the velocity update equation .",
    "the modified velocity update equation is :    @xmath41    where @xmath42 and @xmath43 are constants , and @xmath44 is a random value that is varied during each iteration . the value of @xmath42 is chosen such that the term @xmath45 does not take the particle outside the ellipsoid .",
    "@xmath46 is a random point on the surface of a sphere of dimension @xmath47 with randomly varying radius .",
    "@xmath48 forms the craziness term .    the velocity vector @xmath49 of particle @xmath38 should be directed towards the minimization of @xmath0 .",
    "the function @xmath50 needs to be evaluated .",
    "we use @xmath51 instead of using @xmath52 as the objective function .",
    "the value of @xmath39 is unknown and needs to be arrived at by minimizing the function @xmath0 .",
    "the value of the function for the @xmath20 particle at the @xmath21 iteration is evaluated as @xmath53    let the present position of the @xmath20 particle be split into the previous position and the velocity vector of the particle .",
    "@xmath54    here , @xmath36 is fixed and the particle position @xmath55 in the objective function is dependent on the velocity vector @xmath56 .",
    "so , one possible option for minimizing the function @xmath0 is by changing the direction of the velocity vector of the particle as follows :    @xmath57    thus , the function @xmath0 is minimized if the direction of the velocity vector is as given by eq ( 10 ) .",
    "the velocity vector update equation ( 2 ) does not have any term relating to minimization , but the modified eq ( 7 ) includes the direction for minimization .",
    "it improves the convergence rate and thus reduces the computation time .",
    "the arrow ( for representation purpose ) shown in figure [ theoryexample ] is in the direction of minimizing the function @xmath0 given by eq ( 6 ) .    on the assumption that one end point is known in the shortest path ,",
    "particle swarms are placed in the search space of the other region and the other end point of the shortest path is determined . in order to evaluate the objective function in eq ( 5 ) , we need to determine one end point from region @xmath29 and the other from region @xmath30 .",
    "two sets of particle swarms , one for each region , are placed within the search spaces of the respective regions .",
    "the objective function is evaluated based on the particles present in both the regions . in every iteration ,",
    "the best position of a particle in one region is used as the known end point in the shortest path in the velocity update equation of the particles of the other region .",
    "the objective function reaches its optimal value after several generations .    in the process of optimization",
    ", some particles may often go out of the search space . to limit the particles within the search space , we inspect after every tentative position update as to whether the particle is lying within the search space by carrying out a check on the qcqp constraints .",
    "if the intended new position of a particle is going to violate the constraint , then its position is not updated . in other words , the particles likely to go out of the search space are redeployed back to their previous positions .",
    "the proposed solution may be used in control system problems such as optimization of sensor networks or collaborative data mining , which are based on multiple agents or gradient projection @xcite .",
    "general consensus problem may be solved using the proposed method , where multiple agents need to reach a common goal @xcite .",
    "consensus or distributed optimization is discussed in @xcite as ` consensus and sharing ' using alternating direction method of multipliers ( admom ) .",
    "admom uses one agent for each constraint .",
    "such solvers are used for solving svm in distributed format @xcite .",
    "table [ patab1 ] presents a pseudo code for the algorithm .",
    "the parameters @xmath11,@xmath58,@xmath59,@xmath60 , and @xmath61 are set to fixed values and the randomly varying parameters @xmath14,@xmath62,@xmath63 , and @xmath64 are updated in each iteration . the position , velocity , personal best , and global best of each particle are stored .",
    "the maximum number of iterations for the algorithm is specified as @xmath65 in the experiments and the size of swarm used in the algorithm is ` 10 ' . the proposed algorithm is implemented in matlab .",
    ".proposed algorithm for qcqp using pso [ cols= \" < \" , ]     the performance of our method is close to that of the variants of svm and is superior to that of the neural network .",
    "we notice that in iris and wine datasets , the cross - validation error obtained by our technique is better than those achieved by svm with linear and rbf kernels and also the neural network . in pima and thyroid datasets ,",
    "the cross - validation error is high , due to the high degree of correlation .",
    "we have developed a classification method and optimization algorithm for solving qcqp problems .",
    "the novelty in this method is the application of particle swarms , an evolutionary technique , for optimization .",
    "the results indicate that our approach is a possible method in solving general qcqp problems without gradient estimation .",
    "we have shown the results of our algorithm under quadratic constraints by evaluating different optimization functions .",
    "the issue with pso based methods is their computational complexity and the need for parameter tuning .",
    "the number of function evaluations linearly increases with the number of particles employed and the number of iterations carried out .    in future , we intend to learn multiple hyperplanes by placing multiple kernels in each class and evaluating the performance against multiple - kernel learning algorithms .",
    "the hyperplanes estimated for different kernels may reduce the cross - validation error for the pima and thyroid datasets .",
    "boyd , s. , parikh , n. , chu , e. , peleato , b. , eckstein , j. : distributed optimization and statistical learning via the alternating direction method of multipliers .",
    "foundations and trends in machine learning , volume 3 , issue 1 , p.1 - 122 ( 2010 )          derrac , j. , garca , s. , herrera , f. : fuzzy nearest neighbor algorithms : taxonomy , experimental analysis and prospects .",
    "information sciences , volume 260 , p.98 - 119 ( 2014 ) doi : http://dx.doi.org/10.1016/j.ins.2013.10.038      fernndez , a. , lpez , v. , galar , m. , jesus , m.j . ,",
    "herrera , f. : analysing the classification of imbalanced data - sets with multiple classes : binarization techniques and ad - hoc approaches .",
    "knowledge - based systems , p.97 - 110 ( 2013 )      galar , m. , fernndez , a. , barrenechea , e. , herrera , f. : eusboost : enhancing ensembles for highly imbalanced data - sets by evolutionary undersampling .",
    "pattern recognition ( 2013 ) doi : http://dx.doi.org/10.1016/j.patcog.2013.05.006      gonzalez - abril , l. , nuez , h. , angulo , c. , velasco , f. : gsvm : an svm for handling imbalanced accuracy between classes in bi - classification problems . applied soft computing , volume 17 , p.23 - 31 ( 2014 ) doi : http://dx.doi.org/10.1016/j.asoc.2013.12.013              lpez , v. , fernndez , a. , garca , s. , palade , v. , herrera , f. : an insight into classification with imbalanced data : empirical results and current trends on using data intrinsic characteristics , information sciences ( 2013 ) doi : http://dx.doi.org/10.1016/j.ins.2013.07.007    matei , i. , baras , j.s . : a performance comparison between two consensus - based distributed optimization algorithms .",
    "3rd ifac workshop on distributed estimation and control in networked systems , p.168 - 173 , santa barbara , ca , usa ( 2012 )        platt , j. : fast training of support vector machines using sequential minimal optimization . in b. schoelkopf and",
    "c. burges and a. smola , editors , advances in kernel methods - support vector learning ( 1998 )    spadoni , m. , stefanini , l. : a differential evolution algorithm to deal with box , linear and quadratic - convex constraints for boundary optimization journal of global optimization 52(1 ) , p.171 - 192 ( 2012 )      zanella , f. , varagnolo , d. , cenedese , a. , pillonetto , g. , schenato , l. : multidimensional newton - raphson consensus for distributed convex optimization .",
    "the 2012 american control conference ( acc ) , p.1079 - 1084 , montreal , qc ( 2012 )"
  ],
  "abstract_text": [
    "<S> particle swarm optimization is used in several combinatorial optimization problems . in this work , </S>",
    "<S> particle swarms are used to solve quadratic programming problems with quadratic constraints . </S>",
    "<S> the approach of particle swarms is an example for interior point methods in optimization as an iterative technique . </S>",
    "<S> this approach is novel and deals with classification problems without the use of a traditional classifier . </S>",
    "<S> our method determines the optimal hyperplane or classification boundary for a data set . in a binary classification problem </S>",
    "<S> , we constrain each class as a cluster , which is enclosed by an ellipsoid . </S>",
    "<S> the estimation of the optimal hyperplane between the two clusters is posed as a quadratically constrained quadratic problem . </S>",
    "<S> the optimization problem is solved in distributed format using modified particle swarms . </S>",
    "<S> our method has the advantage of using the direction towards optimal solution rather than searching the entire feasible region . </S>",
    "<S> our results on the iris , pima , wine , and thyroid datasets show that the proposed method works better than a neural network and the performance is close to that of svm .    </S>",
    "<S> * keywords * quadratic programming ; particle swarms ; hyperplane ; quadratic constraints ; binary classification . </S>"
  ]
}