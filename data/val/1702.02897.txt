{
  "article_text": [
    "many real - world brain - computer interface ( bci ) applications rely on single - trial classification of event - related potentials ( erps ) in eeg signals @xcite .",
    "for example , in a rapid serial visual presentation ( rsvp ) based bci system , a sequence of images are shown to the subject rapidly ( e.g. 2 - 10 hz ) @xcite , and the subject needs to detect some target images in them .",
    "the target images are much less frequent than the non - target ones , so that they can elicit p300 erps in the oddball paradigm .",
    "the p300 erps can be detected by a bci system @xcite , and the corresponding images are then triaged for further inspection .",
    "research @xcite has shown that these bci systems enable the subject to detect targets in large aerial photographs faster and more accurately than traditional standard searches .",
    "unfortunately , because different subjects have different neural responses to even the same visual stimulus @xcite , it is very difficult , if not impossible , to build a generic erp classifier whose parameters fit all subjects .",
    "so , we need to calibrate the classifier for each individual subject , using some labeled subject - specific data . reducing this calibration effort ,",
    "i.e. , minimizing the amount of labeled subject - specific data required in the calibration , would greatly increase the utility of the bci system .",
    "this is the research problem tackled in this paper .",
    "more specifically , we distinguish between two types of calibration in bci :    1 .   _ offline calibration _ , in which a pool of unlabeled eeg epochs have been obtained",
    "_ a priori _ , and a subject is queried to label some of these epochs , which are then used to train a classifier to label the remaining epochs in that pool .",
    "a potential application of offline calibration is personalized automatic game highlight detection , e.g. , a subject s eeg signals are recorded continuously while watching a football game ; after the game , the subject manually labels a few highlights , which are then used to train an erp classifier to detect more highlights . 2 .",
    "_ online calibration _ ,",
    "in which some labeled eeg epochs are obtained on - the - fly , and then a classifier is trained from them to classify future eeg epochs .",
    "a potential application of online calibration is the afore - mentioned rsvp image tagging problem : at the beginning of the task , the subject is asked to explicitly indicate it ( e.g. , press a button ) every time he / she detects a target image , and that information is used to train a p300 erp classifier . after a certain number of calibration epochs ,",
    "the performance of the classifier can become reliable enough so that it can label further images using eeg epochs only .",
    "one major difference between offline calibration and online calibration is that , in offline calibration , the unlabeled eeg epochs can be used to help design the erp classifier , whereas in online calibration there are no unlabeled eeg epochs .",
    "additionally , in offline calibration we can query any epoch in the pool for the label ( an optimal query strategy can hence be designed by using machine learning methods such as active learning @xcite ) , but in online calibration usually the sequence of the epochs is pre - determined and the subject has no control over which epoch he / she will see next .",
    "many signal processing and machine learning approaches have been proposed to reduce the bci calibration effort @xcite .",
    "they may be grouped into five categories",
    "@xcite :    1 .   _",
    "regularization _ , which is a very effective machine learning approach for constructing robust models @xcite , especially when the training data size is small . a popular regularization approach in bci calibration is shrinkage @xcite , which gives a regularized estimate of the covariance matrices .",
    "_ transfer / multi - task learning _ , which uses relevant data from other subjects to help the current subject @xcite .",
    "the transfer learning ( tl ) @xcite based approaches are particularly popular @xcite , because in many bci applications we can easily find legacy data from the same subject in the same task or similar tasks , or legacy data from different subjects in the same task or similar tasks . these data , which will be called auxiliary data in this paper , can be used to improve the learning performance of a new subject , or for a new task .",
    "adaptive learning _ , which refines the machine learning model as new ( labeled or unlabeled ) subject - specific data are available @xcite .",
    "the main approach in this category is semi - supervised learning @xcite , which is often used for offline bci calibration where unlabeled data are available .",
    "semi - supervised learning first constructs an initial model from the labeled training data and then applies it to the unlabeled test data .",
    "the newly labeled test data are then integrated with the groundtruth training data to retrain the model , and hence to improve it iteratively .",
    "active learning _ , which optimally selects the most informative unlabeled samples to label @xcite .",
    "there are many criteria to determine which unlabeled samples are the most informative @xcite .",
    "the most popular , and probably also the simplest , approach for classification is to select the samples that are closest to the current decision boundary , because the classifier is most uncertain about them .",
    "active learning has been mainly used for offline bci calibration , where unlabeled samples are available . however",
    ", a closely related technique , active class selection @xcite , can be used for online bci calibration @xcite .",
    "its idea is to optimize the classes from which the new training samples are generated .",
    "_ _ a priori _ physiological information _ , which can be used to construct the most useful eeg features . for example , prior information on which eeg channels are the most likely to be useful was used in @xcite as a regularizer to optimize spatial filters , and beamforming has been used in @xcite to find relevant features from prior regions of interest to reduce the calibration data requirement .",
    "interestingly , these five categories of approaches are not mutually exclusive : in fact they can be freely combined to further reduce the amount of subject - specific calibration data .",
    "an optimal spatial filters was designed in @xcite for efficient subject - to - subject transfer by combining regularization and _ a prior _ information on which channels are the most likely to be useful .",
    "a collaborative filtering approach was developed in @xcite , which combined tl and active class selection to minimize the online calibration effort .",
    "an active tl approach was proposed in @xcite for offline calibration , which combined tl and active learning to minimize the offline calibration effort .",
    "an active weighted adaptation regularization approach , which combines active learning , tl , regularization and semi - supervised learning , was proposed in @xcite to facilitate the switching between different eeg headsets .",
    "all these approaches are for bci classification problems .",
    "however , recently researchers have also started to apply these techniques for regression problems in bci calibration .",
    "for example , a domain adaptation with model fusion approach , which combines regularization and tl , was developed in @xcite to estimate driver s drowsiness online continuously .",
    "this paper presents a comprehensive overview and comparison of the offline and online weighted adaptation regularization with source domain selection ( warsds ) algorithms , which we proposed recently @xcite .",
    "the offline warsds algorithm , which combined tl , regularization , and semi - supervised learning , was first developed in @xcite for offline single - trial classification of erps in a visually - evoked potential ( vep ) oddball task .",
    "it was later extended to online calibration in a rsvp task @xcite , which still includes tl and regularization but not semi - supervised learning because unlabeled samples are not available in online calibration . in this paper",
    "we use a vep oddball task and three different eeg headsets to show that they have consistently good performance across subjects and headsets .",
    "we also compare the performances of the offline and online warsds algorithms in identical experimental settings to investigate the effect of semi - supervised learning , and show that it can indeed help improve the calibration performance .",
    "the remainder of the paper is organized as follows : section  [ sect : warsds ] introduces the details of the offline warsds algorithm .",
    "section  [ sect : owarsds ] introduces the online warsds ( owarsds ) algorithm .",
    "section  [ sect : experiments ] describes the experiment setup that is used to evaluate the performances of different algorithms .",
    "section  [ sect : offline ] presents performance comparison of different offline calibration algorithms .",
    "section  [ sect : online ] presents performance comparison of different online calibration algorithms .",
    "section  [ sect : onoff ] compares the performances of offline and online algorithms .",
    "finally , section  [ sect : conclusions ] draws conclusions .",
    "this section describes the offline warsds algorithm @xcite , which originates from the adaptation regularization ",
    "regularized least squares ( arrls ) algorithm in @xcite .",
    "we made several major enhancements to arrls to better handle class - imbalance and multiple source domains , and also to make use of labeled samples in the target domain .",
    "warsds first uses source domain selection ( sds ) to select the closest source domains for a given target domain , then uses weighted adaptation regularization ( war ) for each selected source domain to build individual classifiers , and finally performs model fusion . for simplicity , we only consider 2-class classification .",
    "a _ domain _ @xcite @xmath1 in tl consists of a multi - dimensional feature space @xmath2 and a marginal probability distribution @xmath3 , i.e. , @xmath4 , where @xmath5 .",
    "two domains @xmath6 and @xmath7 are different if @xmath8 , and/or @xmath9 .    a _ task _ @xcite @xmath10 in tl consists of a label space @xmath11 and a conditional probability distribution @xmath12 . two tasks @xmath13 and",
    "@xmath14 are different if @xmath15 , or @xmath16 .    given a source domain @xmath6 with @xmath17 labeled samples , @xmath18 , and a target domain @xmath7 with @xmath19 labeled samples @xmath20 and @xmath21 unlabeled samples @xmath22 , _ domain adaptation _ ( da ) tl learns a target prediction function @xmath23 with low expected error on @xmath7 , under the assumptions @xmath24 , @xmath25 , @xmath9 , and @xmath16 .    for example , in single - trial classification of veps , eeg epochs from the new subject are in the target domain , while eeg epochs from an existing subject ( usually different from the new subject ) are in the source domain . when there are multiple source domains , we perform da for each of them separately and then aggregate the classifiers .",
    "a sample consists of the feature vector for an eeg epoch from a subject in either domain , collected as a response to a specific visual stimulus .",
    "though usually the source and target domains employ the same feature extraction method , generally their marginal and conditional probability distributions are different , i.e. , @xmath9 and @xmath16 , because the two subjects usually have different neural responses to the same visual stimulus @xcite . as a result",
    ", the auxiliary data from a source domain can not represent the primary data in the target domain accurately , and must be integrated with some labeled target domain data to induce an accurate target domain classifier .",
    "because @xmath26 to use the source domain data in the target domain , we need to make sure @xmath27 is close to @xmath28 , @xmath29 is close to @xmath30 , and @xmath31 is also close to @xmath32 .",
    "however , in this paper we focus only on the first two requirements by assuming all subjects conduct similar vep tasks [ so @xmath31 and @xmath32 are intrinsically close ]",
    ". our future research will consider the more general case that @xmath31 and @xmath32 are different .",
    "let the classifier be @xmath33 , where @xmath34 is the classifier parameters , and @xmath35 is the feature mapping function that projects the original feature vector to a hilbert space @xmath36 . as in@xcite , the learning framework of war is formulated as : @xmath37 \\label{eq : f}\\end{aligned}\\ ] ] where @xmath38 is the loss function , @xmath39 is the kernel matrix with @xmath40 , and @xmath41 and @xmath42 are non - negative regularization parameters .",
    "@xmath43 is the overall weight for target domain samples , which should be larger than 1 so that more emphasis is given to target domain samples than source domain samples .",
    "however , eventually the learned classifier will be applied to the target domain .",
    "so , the target domain should be emphasized .",
    "we choose @xmath44 so that the @xmath19 labeled target domain samples are less overwhelmed by the @xmath17 source domain samples . ] .",
    "@xmath45 and @xmath46 are the weight for the @xmath47 sample in the source domain and target domain , respectively , i.e. , @xmath48 in which @xmath49 is the set of samples in class @xmath50 of the source domain , @xmath51 is the set of samples in class @xmath50 of the target domain , @xmath52 is the number of elements in @xmath53 , and @xmath54 is the number of elements in @xmath55 .",
    "the goal of @xmath45 ( @xmath46 ) is to balance the number of samples from difference classes in the source ( target ) domain .",
    "this is very important , because class imbalance is intrinsic to many applications @xcite , particularly bci applications . in many cases",
    "the minority class is the one of particular interest ( e.g. , the vep experiment presented in this paper ) , but it can be easily overwhelmed by the majority class if not properly weighted .",
    "of course , there are many other approaches for handling class imbalance @xcite .",
    "we used the weighting approach for its simplicity .",
    "briefly speaking , the 1st term in ( [ eq : f ] ) minimizes the loss on fitting the labeled samples in the source domain , the 2nd term minimizes the loss on fitting the labeled samples in the target domain , the 3rd term minimizes the structural risk of the classifier , and the 4th term minimizes the distance between the marginal probability distributions @xmath27 and @xmath28 , and also the distance between the conditional probability distributions @xmath29 and @xmath30 .",
    "according to the representer theorem @xcite , the solution of ( [ eq : f ] ) can be expressed as : @xmath56 where @xmath57^t \\label{eq : x}\\end{aligned}\\ ] ] and @xmath58^t$ ] are coefficients to be computed .",
    "let @xmath59^t \\label{eq : y}\\end{aligned}\\ ] ] where @xmath60 are known labels in the source domain , @xmath61 are known labels in the target domain , and @xmath62 are pseudo labels for the unlabeled target domain samples , i.e. , labels estimated using available sample information in both source and target domains .",
    "define @xmath63 as a diagonal matrix with @xmath64    we use the squared loss in this paper : @xmath65    substituting ( [ eq : f2 ] ) and ( [ eq : l2 ] ) into the first two terms in ( [ eq : f ] ) , we have @xmath66      as in @xcite , we define the structural risk as the squared norm of @xmath67 in @xmath68 , i.e. , @xmath69      as in @xcite , we compute @xmath70 using the projected maximum mean discrepancy ( mmd ) between the source and target domains : @xmath71 ^ 2 \\nonumber\\\\ & = \\boldsymbol{\\alpha}^tkm_0k\\boldsymbol{\\alpha } \\label{eq : dfkp}\\end{aligned}\\ ] ] where @xmath72 is the mmd matrix : @xmath73      as in @xcite , we first compute pseudo labels for the unlabeled target domain samples and construct the label vector @xmath74 in ( [ eq : y ] ) .",
    "these pseudo labels can be computed using the classifier built in the previous iteration if war is used iteratively , or estimated using another classifier , e.g. , a support vector machine ( svm ) @xcite .",
    "we then compute the projected mmd with respect to each class .",
    "let @xmath49 be the set of samples in class @xmath50 of the source domain , @xmath75 be the set of samples in class @xmath50 of the target domain , @xmath52 be the number of elements in @xmath53 , and @xmath54 be the number of elements in @xmath55 .",
    "then , the distance between the conditional probability distributions in the two domains is computed as : @xmath76 ^ 2 \\label{eq : dfkq}\\end{aligned}\\ ] ]    substituting ( [ eq : f2 ] ) into ( [ eq : dfkq ] ) , it follows that @xmath77 ^ 2 \\nonumber\\\\ = & \\sum_{c=1}^2\\boldsymbol{\\alpha}^tkm_ck\\boldsymbol{\\alpha } = \\boldsymbol{\\alpha}^tkmk\\boldsymbol{\\alpha } \\label{eq : dfkq2}\\end{aligned}\\ ] ] where @xmath78 in which @xmath79 and @xmath80 are mmd matrices computed as : @xmath81      substituting ( [ eq : l3 ] ) , ( [ eq : fk ] ) , ( [ eq : dfkp ] ) and ( [ eq : dfkq2 ] ) into ( [ eq : f ] ) , we have @xmath82 setting the derivative of the objective function above to 0 leads to the following closed - form solution for @xmath83 : @xmath84^{-1}e\\mathbf{y } \\label{eq : alpha}\\end{aligned}\\ ] ]      when there are multiple source domains , it is very time - consuming to perform war for each source domain and then aggregate the results .",
    "additionally , aggregating results from source domains that are outliers or very different from the target domain may also hurt the classification performance .",
    "so , we introduce a source domain selection approach @xcite , which selects the closest source domains to reduce the computational cost , and also to ( potentially ) improve the classification performance .",
    "assume there are @xmath85 different source domains . for the @xmath86 source domain , we first compute @xmath87 ( @xmath88 ) , the mean feature vector of each class . then , we also compute @xmath89 , the mean feature vector of each target domain class , by making use of the @xmath19 known labels and the @xmath21 pseudo - labels . the distance between the two domains",
    "is then computed as : @xmath90 we next cluster these @xmath85 distances , @xmath91 , by @xmath92-means clustering , and finally choose the cluster that has the smallest centroid , i.e. , the source domains that are closest to the target domain .",
    "in this way , on average we only need to perform war for @xmath93 ( @xmath92 is the number of clusters in @xmath92-means clustering ) source domains , corresponding to a 50% computational cost saving if @xmath94 .",
    "a larger @xmath92 will result in a larger saving ; however , when @xmath92 is too large , there may not be enough source domains selected for war , and hence the classification performance may be unstable .",
    "so , there is a trade - off between computational cost saving and classification performance .",
    "@xmath94 was used in this paper , and it demonstrated satisfactory performance .",
    "the pseudo code for the complete warsds algorithm is shown in algorithm  1 .",
    "it first uses sds to select the closest source domains , then performs war for each of them separately to build individual classifiers , and finally aggregates them using a weighted average , where the weights are the corresponding training accuracies .      as mentioned at the beginning of this section ,",
    "the formulation and derivation of war closely resemble the arrls algorithm in @xcite ; however , there are several major differences :    1 .",
    "war assumes a subject or an oracle is available to label a small number of samples in the target domain , whereas arrls assumes all target domain samples are unlabeled . as a result",
    ", war can be iterative , and the classifier can be updated every time new labeled target domain samples are available .",
    "2 .   war explicitly considers the class imbalance problem in both source and target domains by introducing the class - dependent weights on samples .",
    "as it will be shown in section  [ sect : experiments ] , this makes a huge difference in the balanced classification accuracy for the class imbalance problem , which is intrinsic in erp - based bci systems .",
    "arrls also includes manifold regularization @xcite .",
    "we investigated it but was not able to observe improved performance in our application , so it is not included in this paper .",
    "finally , when combined with sds , warsds can effectively handle multiple source domains , whereas arrls only considers one source domain .",
    "the warsds classifier @xmath95 .",
    "retain all @xmath85 source domains ; compute pseudo - labels @xmath96 using another classifier , e.g. , an svm ; go to war .",
    "compute pseudo - labels @xmath96 using the warsds classifier built from the previous iteration ; compute @xmath97 , the distance between the target domain and the @xmath86 source domain , by ( [ eq : dst ] ) .",
    "cluster @xmath91 by @xmath92-means clustering ; retain the @xmath98 source domains that belong to the cluster with the smallest centroid .",
    "choose a kernel function @xmath99 ; construct the feature matrix @xmath100 in ( [ eq : x ] ) ; compute the kernel matrix @xmath101 from @xmath100 ; construct @xmath74 in ( [ eq : y ] ) , @xmath102 in ( [ eq : e ] ) , @xmath103 in ( [ eq : m0 ] ) , and @xmath104 in ( [ eq : m ] ) ; compute @xmath83 by ( [ eq : alpha ] ) and record it as @xmath105 ; use @xmath83 to classify the @xmath106 labeled samples and record the accuracy , @xmath107 ; + @xmath108 .",
    "+    retain all @xmath85 source domains ; go to owar .",
    "compute @xmath97 , the distance between the target domain and the @xmath86 source domain , by ( [ eq : dst ] ) .",
    "cluster @xmath91 by @xmath92-means clustering ; retain the @xmath98 source domains that belong to the cluster with the smallest centroid . + choose a kernel function @xmath109 ; construct the feature matrix @xmath110 in ( [ eq : xo ] ) ; compute the kernel matrix @xmath111 from @xmath110 ; construct @xmath112 in ( [ eq : yonline ] ) , @xmath113 in ( [ eq : eonline ] ) , @xmath114 in ( [ eq : m0online ] ) , @xmath115 in ( [ eq : dfkq2online ] ) ; compute @xmath116 by ( [ eq : alpha ] ) and record it as @xmath117 ; use @xmath117 to classify the @xmath106 labeled samples and record the accuracy , @xmath118 ; + @xmath119 . +",
    "this section introduces the owarsds algorithm @xcite , which extends the offline warsds algorithm to online bci calibration .",
    "owarsds first uses sds to select the closest source domains , then performs online weighted adaptation regularization ( owar ) for each selected source domain to build individual classifiers , and finally aggregates them .      using the notations introduced in the previous section , the learning framework of owar",
    "can still be formulated as ( [ eq : f ] ) . however , because in online calibration there are no unlabeled target domain samples , the kernel matrix @xmath120 has dimensionality @xmath121 , instead of @xmath122 in offline calibration . as a result ,",
    "the solution of ( [ eq : f ] ) admits a different expression : @xmath123 where @xmath124^t   \\label{eq : xo}\\end{aligned}\\ ] ] and @xmath125^t$ ] are coefficients to be computed .",
    "it has been shown @xcite that the closed - form solution for @xmath116 is : @xmath126^{-1}e^o\\mathbf{y}^o \\label{eq : alphaonline}\\end{aligned}\\ ] ] next we briefly introduce how the various terms in ( [ eq : alphaonline ] ) are derived .",
    "define @xmath127^t \\label{eq : yonline}\\end{aligned}\\ ] ] where @xmath60 are known labels in the source domain , and @xmath61 are known labels in the target domain . define also @xmath128 as a diagonal matrix with @xmath129 then , following the derivation in ( [ eq : l3 ] ) , we now have @xmath130e^o(\\mathbf{y}^o - k^o\\boldsymbol{\\alpha}^o ) \\label{eq : l3online}\\end{aligned}\\ ] ]      again , we define the structural risk as the squared norm of @xmath131 in @xmath68 , i.e. , @xmath132      we compute @xmath133 using the projected mmd between the source and target domains : @xmath134 ^ 2 \\nonumber \\\\ & = ( \\boldsymbol{\\alpha}^o)^tk^om_0^ok^o\\boldsymbol{\\alpha}^o \\label{eq : dfkponline}\\end{aligned}\\ ] ] where @xmath135 is the mmd matrix : @xmath136      in offline calibration , to minimize the discrepancy between the conditional probability distributions in the source and target domains , we need to first compute the pseudo - labels for the @xmath21 unlabeled target domain samples . in online calibration , because there are no unlabeled target domain samples , this step is skipped . following the derivation of ( [ eq : dfkq2 ] ) , we still have : @xmath137 where @xmath115 is still computed by ( [ eq : m ] ) , but using only the @xmath17 source domain samples and @xmath19 target domain samples .",
    "the sds procedure in owarsds is almost identical to that in warsds .",
    "the only difference is that the latter also makes use of the @xmath21 unlabeled target domain samples in computing @xmath89 in ( [ eq : dst ] ) , whereas the former only uses the @xmath19 labeled target domain samples , because there are no unlabeled target domain samples in online calibration .",
    "the pseudo code for the complete owarsds algorithm is described in algorithm  2 .",
    "it first uses sds to select the closest source domains , then performs owar for each of them separately to build individual classifiers , and finally aggregates them using a weighted average , where the weights are the corresponding training accuracies .",
    "observe that owarsds is very similar to warsds , the major difference being that no unlabeled target domain samples are available for use in owarsds .",
    "this section describes the setup of the vep oddball experiment , which is used in the following three sections to evaluate the performances of different algorithms .",
    "a two - stimulus vep oddball task was used @xcite . in this task , participants were seated in a sound - attenuated recording chamber , and image stimuli were presented to them at a rate of 0.5 hz ( one image every two seconds ) . the images ( 152@xmath138375 pixels ) , presented for 150 ms at the center of a 24 inch dell p2410 monitor at a distance of approximately 70 cm , were either an enemy combatant [ _ target _ ; an example is shown in fig .",
    "[ fig : t ] ] or a u.s . soldier [ _ non - target _ ; an example is shown in fig .  [",
    "fig : nt ] ] .",
    "the subjects were instructed to maintain fixation on the center of the screen and identify each image as being target or non - target with a unique button press as quickly and accurately as possible .",
    "a total of 270 images were presented to each subject , among which 34 were targets .",
    "the experiments were approved by u.s .",
    "army research laboratory ( arl ) institutional review board .",
    "the voluntary , fully informed consent of the persons used in this research was obtained as required by federal and army regulations @xcite .",
    "the investigator has adhered to army policies for the protection of human subjects .",
    "18 subjects participated the experiments , which lasted on average 15 minutes .",
    "signals for each subject were recorded with three different eeg headsets , including a 64-channel 512hz biosemi activetwo system , a 9-channel 256hz advanced brain monitoring ( abm ) x10 system , and a 14-channel 128hz emotiv epoc headset . however , due to some exceptions at the experiment , data were correctly recorded for only 16 subjects for abm , 15 subjects for biosemi , and 15 subjects for emotiv . there were 14 subjects whose data were correctly recorded for all three headsets , so we used only these 14 subjects in this study .",
    "the preprocessing and feature extraction method for all three headsets was the same , except that for abm and emotiv headsets we used all the channels , but for the biosemi headset we only used 21 channels ( cz , fz , p1 , p3 , p5 , p7 , p9 , po7 , po3 , o1 , oz , poz , pz , p2 , p4 , p6 , p8 , p10 , po8 , po4 , o2 ) mainly in the parietal and occipital areas , as in @xcite .",
    "eeglab @xcite was used for eeg signal preprocessing and feature extraction . for each headset ,",
    "we first band - passed the eeg signals to [ 1 , 50 ] hz , then downsampled them to 64 hz , performed average reference , and next epoched them to the @xmath139 $ ] second interval timelocked to stimulus onset .",
    "we removed mean baseline from each channel in each epoch and removed epochs with incorrect button press responses .",
    "the final numbers of epochs from the 14 subjects are shown in table  [ tab : epoch ] .",
    "observe that there is significant class imbalance for every subject ; that s why we need to use @xmath45 and @xmath46 in ( [ eq : f ] ) to balance the two classes in both domains .    [",
    "cols=\"<,^,^,^,^,^,^,^,^,^,^,^,^,^,^\",options=\"header \" , ]     each [ 0 , 0.7 ] second epoch contains hundreds of raw eeg magnitude samples ( e.g. , @xmath140 for biosemi ) . to reduce the dimensionality",
    ", we performed a simple principal component analysis ( pca ) to take the scores on the first 20 principal components as features .",
    "we then normalized each feature dimension separately to @xmath141 $ ] .",
    "let @xmath142 and @xmath143 be the true number of epochs from the target and non - target class , respectively .",
    "let @xmath144 and @xmath145 be the number of epochs that are correctly classified by an algorithm as target and non - target , respectively .",
    "then , we compute @xmath146 where @xmath147 is the classification accuracy on the target class , and @xmath148 is the classification accuracy on the non - target class .    the following balanced classification accuracy ( bca ) was then used as the performance measure in this paper : @xmath149",
    "this selection presents performance comparison of warsds with several other offline calibration algorithms .      although we knew the labels of all eeg epochs for all 14 subjects in the vep experiment , we simulated a realistic offline calibration scenario : we had labeled eeg epochs from 13 subjects , and also all epochs from the 14th subject , but initially none of them was labeled .",
    "our goal was to iteratively label epochs from the 14th subject and build a classifier so that his / her remaining unlabeled epochs can be reliably classified .",
    "the flowchart for the simulated offline calibration scenario is shown in fig .",
    "[ fig : offlinec ] .",
    "assume the 14th subject has @xmath150 sequential epochs in the vep experiment , and we want to label @xmath151 epochs in each iteration , starting from zero .",
    "we first generate a random number @xmath152 $ ] , representing the starting position in the vep sequence .",
    "then , in the first iteration , we use the @xmath150 unlabeled epochs from the 14th subject and all labeled epochs from the other 13 subjects to build different classifiers and compute their bcas on the @xmath150 unlabeled epochs . in the second iteration",
    ", we obtain labels for epochs epochs . however , the labeled epochs are always sequential in online calibration .",
    "to facilitate the comparison between offline and online algorithms , we used sequential sampling in both offline and online calibrations . but note that there is no statistic difference between random sampling and sequential sampling in offline calibration . ]",
    "@xmath153 , @xmath154 , ... ,",
    "@xmath155 from the 14th subject , build different classifiers , and compute their bcas on the remaining @xmath156 unlabeled epochs .",
    "we iterate until the maximum number of iterations is reached . when the end of the vep sequence is reached during the iteration , we rewind to the beginning of the sequence , e.g. , if @xmath157 , then epoch @xmath154 is the 1st epoch in the vep sequence , epoch @xmath158 is the 2nd , and so on .    to obtain statistically meaningful results ,",
    "the above process was repeated 30 times for each subject , each time with a random starting point @xmath153 .",
    "we repeated this procedure 14 times so that each subject had a chance to be the  14th \" subject .",
    "we compared the performance of warsds with six other algorithms",
    "@xcite :    1 .   _",
    "bl1 _ , a baseline approach in which we assume we know labels of all samples from the new subject , and use 5-fold cross - validation and svm to find the highest bca .",
    "this represents an upper bound of the bca we can get , by using the data from the new subject only .",
    "bl2 _ , which is a simple iterative procedure : in each iteration we randomly select five unlabeled samples from the new subject to label , and then train an svm classifier by 5-fold cross - validation .",
    "we iterate until the maximum number of iterations is reached .",
    "tl _ , which is the tl algorithm introduced in @xcite .",
    "it simply combines the labeled samples from the new subject with samples from each existing subject and train an svm classifier .",
    "the final classifier is a weighted average of all individual classifiers , and the weights are the corresponding cross - validation bcas . note that this algorithm can be used both online and offline , because it does not use any information from the unlabeled epochs .",
    "tlsds _ , which also performs sds before the above tl algorithm .",
    "_ arrls _ , which was proposed in @xcite ( manifold regularization was removed ) , and is also the war algorithm introduced in algorithm  1 , by setting @xmath159 .",
    "_ war _ , which excludes the sds part in algorithm  1 .",
    "weighted libsvm @xcite with rbf kernel was used as the classifier in bl1 , bl2 , tl and tlsds .",
    "the optimal rbf parameter was found by cross - validation .",
    "we chose @xmath160 , @xmath161 , and @xmath162 , following the practice in @xcite .",
    "the bcas of the seven algorithms , averaged over the 30 runs and across the 14 subjects , are shown in fig .",
    "[ fig : offline ] for the three headsets .",
    "observe that :    1 .",
    "generally the performances of all algorithms ( except bl1 , which is not iterative ) increased as more labeled subject - specific samples were available , which is intuitive .",
    "bl2 can not build a classifier when there were no labeled subject - specific samples at all ( observe that the bca for @xmath163 on the bl2 curve in fig .",
    "[ fig : offline ] was always @xmath164 , representing random guess ) , but all tl / da based algorithms can , because they can make use of information from other subjects .",
    "moreover , without any labeled subject - specific samples , war and warsds can build a classifier with a bca of @xmath165 for biosemi , @xmath166 for abm , and @xmath167 for emotiv , much better than random guess .",
    "3 .   generally",
    "the performance of tl was worse than bl2 , suggesting that it can not cope well with large individual differences among the subjects increases .",
    "this is also the case in fig .",
    "[ fig : biosemi_offline_avgs ] , when @xmath19 is small . ] .",
    "4 .   tlsds always outperformed tl .",
    "this is because tl used a very simple way to combine the labeled samples from the new and existing subjects , and hence an existing subject whose erps are significantly different from the new subject s would have a negative impact on the final bca .",
    "sds can identify and remove ( some of ) such subjects , and hence benefited the bca .",
    "arrls demonstrated the worst bca , because all other algorithms explicitly handled class - imbalance using weights , whereas arrls did not . for our dataset ,",
    "the non - target class had seven times more samples than the target class , so many times arrls simply classified all samples as non - target , resulting in a bca of @xmath164 .",
    "war and warsds significantly outperformed bl2 , tl , tlsds and arrls .",
    "this is because a sophisticated da approach was used in war and warsds , which explicitly considered class imbalance , and was optimized not only for high classification accuracy , but also for small structural risk and close feature similarity between the two domains .",
    "warsds and war had very similar performance , but instead of using 13 auxiliary subjects , warsds only used on average 6.84 subjects for biosemi , 6.03 subjects for abm , and 6.85 subjects for emotiv , corresponding to @xmath168 , @xmath169 and @xmath170 computational cost saving , respectively .",
    "as in @xcite , we also performed comprehensive statistical tests to check if the performance differences among the six algorithms ( bl1 was not included because it is not iterative ) were statistically significant .",
    "we used the area - under - performance - curve ( aupc ) @xcite to assess overall performance differences among these algorithms .",
    "the aupc is the area under the curve of the bcas obtained at each of the 30 runs , and is normalized to @xmath141 $ ] .",
    "a larger aupc value indicates a better overall classification performance .",
    "first , we checked the normality of our data to see if parametric anova tests can be used .",
    "the histograms of the @xmath171 aupcs for each of the six algorithms on the three headsets are shown in fig .",
    "[ fig : hist ] . observe that most of them are not even close to normal .",
    "so , parametric anova tests can not be applied .        as a result",
    ", we used friedman s test @xcite , a two - way non - parametric anova where column effects are tested for significant differences after adjusting for possible row effects .",
    "we treated the algorithm type ( bl2 , tl , tlsds , arrls , war , warsds ) as the column effects , with subjects as the row effects .",
    "each combination of algorithm and subject had 30 values corresponding to 30 runs performed .",
    "friedman s test showed statistically significant differences among the six algorithms for each headset ( @xmath172 , @xmath173 ) .",
    "then , non - parametric multiple comparison tests using dunn s procedure @xcite was used to determine if the difference between any pair of algorithms was statistically significant , with a @xmath151-value correction using the false discovery rate method @xcite .",
    "the results showed that the performances of war and warsds were statistically significantly different from bl2 , tl , tlsds and arrls for each headset ( @xmath174 for all cases , except @xmath175 for abm war vs bl2 , and @xmath176 for abm warsds vs bl2 ) .",
    "there was no statistically significant performance difference between war and warsds ( @xmath177 for biosemi , @xmath178 for abm , and @xmath179 for emotiv ) .    in summary",
    ", we have demonstrated that given the same number of labeled subject - specific training samples , war and warsds can significantly improve the offline calibration performance . in other words ,",
    "given a desired classification accuracy , war and warsds can significantly reduce the number of labeled subject - specific training samples .",
    "for example , in fig .",
    "[ fig : biosemi_offline_avgs ] , the average bca of bl2 is @xmath180 , given 100 labeled subject - specific training samples .",
    "however , to achieve that bca , on average war and warsds only need 5 samples , corresponding to @xmath181 saving of the labeling effort . moreover , fig .",
    "[ fig : biosemi_offline_avgs ] also shows that , without using any labeled subject - specific samples , war and warsds can achieve similar performance as bl2 which uses 65 samples .",
    "similar observations can also be made for the abm and emotiv headsets .      in this subsection",
    "we study the sensitivity of war and warsds to parameters @xmath41 and @xmath182 ( @xmath183 ) . to save space",
    ", we only show the bca results for the biosemi headset .",
    "similar results were obtained from the other two headsets .",
    "the average bcas of war and warsds for different @xmath41 ( @xmath182 and @xmath183 were fixed at 10 ) are shown in fig .",
    "[ fig : sigma ] , and for different @xmath182 and and @xmath183 identical value because they are conceptually close . ] @xmath183 ( @xmath41 was fixed at 0.1 ) are shown in fig .",
    "[ fig : lambda ] . observe from fig .",
    "[ fig : sigma ] that both war and warsds achieved good bcas for @xmath184 $ ] , and from fig .",
    "[ fig : lambda ] that both war and warsds achieved good bcas for @xmath185 $ ] and @xmath186 $ ] .",
    "moreover , @xmath41 , @xmath182 and @xmath183 have more impact to the bca when @xmath19 is small .",
    "as @xmath19 increases , the impact diminishes .",
    "this is intuitive , as the need for transfer diminishes as the amount of labeled target domain data increases .",
    "this selection compares the performance of owarsds with several other online calibration algorithms .",
    "although we knew the labels of all eeg epochs for all 14 subjects in the experiment , we simulated a realistic online calibration scenario : we had labeled eeg epochs from 13 subjects , but initially no epoch from the 14th subject ; we generated labeled epochs from the 14th subject iteratively and sequentially on - the - fly , which were used to train a classifier to label the remaining epochs from that subject .",
    "the flowchart for the simulated online calibration scenario is shown in fig .",
    "[ fig : onlinec ] . compared with the offline calibration scenario in fig .",
    "[ fig : offlinec ] , the main difference is that offline calibration has access to all @xmath150 unlabeled samples from the 14th subject , but online calibration does not .    more specifically , assume the 14th subject has @xmath150 sequential epochs in the vep experiment , and we want to label @xmath151 epochs in each iteration , starting from zero .",
    "we first generate a random number @xmath152 $ ] , representing the starting position in the vep sequence .",
    "then , in the first iteration , we use all labeled epochs from the other 13 subjects to build different classifiers , and compute their bcas on the @xmath150 unlabeled epochs . in the second iteration ,",
    "we generated labeled epochs @xmath153 , @xmath154 , ... ,",
    "@xmath155 from the 14th subject , build different classifiers , and compute their bcas on the remaining @xmath156 unlabeled epochs .",
    "we iterate until the maximum number of iterations is reached . to obtain statistically meaningful results",
    ", the above process was repeated 30 times for each subject , each time with a random starting point @xmath153 .",
    "the whole process was repeated 14 times so that each subject had a chance to be the  14th \" subject .",
    "we compared the performances of owarsds with five other algorithms :    1 .   _",
    "bl1 _ in section  [ sect : aoffline ] .",
    "bl2 _ in section  [ sect : aoffline ] , using different pca features .",
    "tl _ in section  [ sect : aoffline ] , using different pca features .",
    "tlsds _ , which is the above tl algorithm with sds .",
    "_ owar _ , which uses all existing subjects , instead of performing sds .",
    "again , weighted libsvm @xcite with rbf kernel was used as the classifier in bl1 , bl2 , tl and tlsds .",
    "we chose @xmath160 , @xmath161 , and @xmath162 .",
    "note that the online algorithms still used pca features , but they were computed differently from those in offline calibration . in offline calibration we had access to the @xmath19 labeled samples plus the @xmath21 unlabeled samples ,",
    "so the pca bases can be pre - computed from all @xmath187 samples and kept fixed in each iteration .",
    "however , in online calibration , we only had access to the @xmath19 labeled samples , so the pca bases were computed from the @xmath19 samples only , and we updated them in each iteration as @xmath19 changed .",
    "the bcas of the six algorithms , averaged over the 30 runs and across the 14 subjects , are shown in fig .",
    "[ fig : online ] for different eeg headsets .",
    "the observations made in section  [ sect : results ] for offline calibration still hold here , except that arrls was not included in online calibration .",
    "particularly , both owar and owarsds achieved much better performance than bl2 , tl and tlsds .",
    "however , instead of using 13 auxiliary subjects in owar , owarsds only used on average 6.51 subjects for biosemi , 6.01 subjects for abm , and 7.09 subjects for emotiv , corresponding to @xmath188 , @xmath189 and @xmath190 computational cost saving , respectively .",
    "friedman s test showed statistically significant performance differences among the five algorithms ( excluding bl1 , which is not iterative ) for each headset ( @xmath191 , @xmath173 ) .",
    "dunn s procedure showed that the bcas of owar and owarsds are statistically significantly different from bl2 , tl , and tlsds for each headset ( @xmath192 for abm owarsds vs bl2 , @xmath193 for abm owarsds vs tlsds , and @xmath174 in all other cases ) .",
    "there was no statistically significant performance difference between owar and owarsds ( @xmath194 for biosemi , @xmath195 for abm , and @xmath196 for emotiv ) .    in summary",
    ", we have demonstrated that given the same number of labeled subject - specific training samples , owar and owarsds can significantly improve the online calibration performance .",
    "in other words , given a desired classification accuracy , owar and owarsds can significantly reduce the number of labeled subject - specific samples . for example , in fig .",
    "[ fig : biosemi_online_avgs ] , the average bca of bl2 was @xmath197 , given 100 labeled subject - specific training samples .",
    "however , to achieve that bca , on average owar only needed 15 samples , and owarsds only needed 20 samples , corresponding to 85% and 80% saving of labeling effort , respectively . moreover , fig .",
    "[ fig : biosemi_online_avgs ] also shows that , without using any labeled subject - specific samples , owar and owarsds can achieve similar bca as bl2 which used 60 labeled subject - specific samples .",
    "similar observations can also be made for the abm and emotiv headsets .",
    "this section compares the performances of warsds and owarsds ( war and owar ) .",
    "intuitively , we expect the performances of the offline calibration algorithms to be better than their online counterparts , because : 1 ) offline calibration uses all @xmath187 eeg epochs to compute the pca bases , whereas online calibration only uses @xmath19 epochs , so the pca bases in offline calibration should be more representative ; and , 2 ) offline calibration also uses the @xmath21 unlabeled epochs in the optimization , whereas online calibration does not , so offline calibration makes use of more information . in other words ,",
    "offline calibration makes use of semi - supervised learning whereas online calibration does not .",
    "the average performances of war , warsds , owar and owarsds across the 14 subjects are shown in fig .",
    "[ fig : onoff ] . observe that the results were consistent with our expectation : for all three headsets , the offline algorithms ( war and warsds ) achieved better bcas than their online counterparts ( owar and owarsds ) .",
    "additionally , fig .",
    "[ fig : onoff ] shows that the algorithms had best performance using the biosemi headset , and worst performance using the abm headset .",
    "this is not surprising , as biosemi used the most number of channels , and it was wired , which means better signal quality .",
    "the abm headset had the least number of channels , and was wireless",
    ". moreover , epochs with incorrect button presses were filtered out for biosemi and emotiv headsets , but not for most subjects for the abm headset .",
    "so , the epochs in abm were noisier .",
    "we also performed statistical tests to check if the performance differences among the four algorithms were statistically significant .",
    "friedman s test showed statistically significant differences among the four algorithms for biosemi ( @xmath198 , @xmath173 ) and emotiv ( @xmath198 , @xmath199 ) , but not abm ( @xmath198 , @xmath200 ) .",
    "dunn s procedure showed that for biosemi the bcas of war and owar were statistically significantly different ( @xmath201 ) , so were bcas of warsds and owarsds ( @xmath174 ) . for abm and",
    "emotiv the performance differences between online and offline algorithms were not statistically significant ( @xmath202 for abm war vs owar , @xmath203 for abm warsds vs owarsds , @xmath204 for emotiv war vs owar , and @xmath205 for emotiv warsds vs owarsds ) .    in conclusion ,",
    "we have shown that generally the offline war and warsds algorithms , which include a semi - supervised learning component , can achieve better calibration performance than the corresponding online owar and owarsds algorithms , i.e. , semi - supervised learning is effective .",
    "single - trial classification of erps in eeg signals is used in many bci applications . however , because different subjects have different neural responses to even the same stimulus , it is very difficult to build a generic erp classifier whose parameters fit all subjects .",
    "so , the classifier needs to be calibrated for each individual subject , using some labeled subject - specific data . reducing this calibration effort ,",
    "i.e. , minimizing the number of labeled subject - specific data required in the calibration , would greatly increase the utility of a bci system .",
    "this paper introduced both online and offline war algorithms for this purpose .",
    "we have demonstrated using a vep oddball task and three different eeg headsets that both algorithms can cope well with the class - imbalance problem , which is intrinsic in many real - world bci applications , and they also significantly outperformed several other algorithms .",
    "we also compared the performances of the online and offline war algorithms in identical experimental settings and showed that the offline war algorithm , which includes an extra semi - supervised component than the online war algorithm , can achieve better calibration performance , i.e. , semi - supervised learning is effective in bci calibration .",
    "moreover , we further proposed a source domain selection approach , which can reduce the computational cost of both online and offline war algorithms by about @xmath0 .",
    "we expect our algorithms to find broad applications in various bci calibration scenarios , and beyond .",
    "the most intuitive bci calibration scenario , as described in this paper , is to reduce the subject - specific calibration data requirement by making use of relevant data from other subjects .",
    "another scenario is to make use of the same subject s data from previous usages to facilitate a new calibration .",
    "for example , the subject may need to work on the same bci task at different locations using different eeg headsets ( office , home , etc . ) , or may upgrade a bci game with a new eeg headset . in such applications , war can be used to make use of the data obtained from a previous eeg headset to facilitate the calibration for the new headset , as introduced in @xcite .",
    "of course , the above two scenarios can also be combined : auxiliary data from other subjects and from the subject himself / herself can be integrated to expedite the calibration .",
    "furthermore , because of human - machine mutual adaptation and non - stationarity , a well - calibrated bci system may degrade gradually .",
    "the proposed war algorithms can be used to re - calibrate it from time to time .",
    "additionally , eegs , together with many other body signals ( facial expressions , speech , gesture , galvanic skin response , etc . ) , are also frequently used in affective computing @xcite ,  _ computing that relates to , arises from , or deliberately influences emotion or other affective phenomena . _",
    "\" the war algorithms can also be used to handle individual differences and non - stationarity in such applications .    finally , we need to point out that the current war algorithms still have some limitations , which will be improved in our future research .",
    "first , we will develop incremental updating rules to reduce their computational cost .",
    "second , we will develop criteria to determine when a negative transfer may occur , and hence use subject - only calibration data in such cases .",
    "third , although war can map the features to a new kernel space to make them more consistent across the source and target domains , it still relies on good initial features .",
    "simple pca features were used in this paper . in our future research",
    "we will consider more sophisticated and robust features , e.g. , the information geometry @xcite .",
    "the author would like to thank scott kerick , jean vettel , anthony ries , and david w. hairston at the u.s .",
    "army research laboratory ( arl ) for designing the experiment and collecting the data , and brent j. lance and vernon j. lawhern from the arl for helpful discussions .",
    "_ research was sponsored by the u.s .",
    "army research laboratory and was accomplished under cooperative agreement numbers w911nf-10 - 2 - 0022 and w911nf-10-d-0002/to 0023 .",
    "the views and the conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies , either expressed or implied , of the u.s .",
    "army research laboratory or the u.s government .",
    "_      m.  alamgir , m.  grosse - wentrup , and y.  altun , `` multitask learning for brain - computer interfaces , '' in _ proc .",
    "13th intl conf . on artificial intelligence and statistics ( aistats )",
    "_ , sardinia , italy , may 2010 , pp . 1724 .",
    "a.  barachant , s.  bonnet , m.  congedo , and c.  jutten , `` multiclass brain - computer interface classification by riemannian geometry , '' _ ieee trans . on biomedical engineering _ , vol .",
    "59 , no .  4 , pp . 920928 , 2012 .",
    "m.  belkin , p.  niyogi , and v.  sindhwani , `` manifold regularization : a geometric framework for learning from labeled and unlabeled examples , '' _ journal of machine learning research _ , vol .  7 , pp . 23992434 , 2006 .",
    "y.  benjamini and y.  hochberg , `` controlling the false discovery rate : a practical and powerful approach to multiple testing , '' _ journal of the royal statistical society , series b ( methodological ) _ , vol .",
    "57 , pp . 289300 , 1995 .",
    "n.  bigdely - shamlo , a.  vankov , r.  ramirez , and s.  makeig , `` brain activity - based image classification from rapid serial visual presentation , '' _ ieee trans . on neural systems and rehabilitation engineering _ ,",
    "16 , no .  5 , pp .",
    "432441 , 2008 .",
    "d.  bottger , c.  s. herrmann , and d.  y. von cramon , `` amplitude differences of evoked alpha and gamma oscillations in two different age groups , '' _ international journal of psychophysiology _ , vol .",
    "45 , pp . 245251 , 2002 .",
    "chang and c .- j .",
    "lin , `` libsvm : a library for support vector machines , '' _ acm trans .",
    "on intelligent systems and technology _ , vol .  2 , no .  3 , pp .",
    "27:127:27 , 2011 , software available at http://www.csie.ntu.edu.tw/$\\sim$cjlin/libsvm .",
    "m.  m. chun and m.  c. potter , `` a two - stage model for multiple target detection in rapid serial visual presentation , '' _ journal of experimental psychology : human perception and performance _",
    "21 , no .  1 ,",
    "pp . 109127 , 1995 .",
    "s.  crottaz - herbette and v.  menon , `` where and when the anterior cingulate cortex modulates",
    "attentional response : combined fmri and erp evidence , '' _ journal of cognitive neuroscience _",
    "18 , no .  5 , pp . 766780 , 2006 .",
    "a.  delorme and s.  makeig , `` eeglab : an open source toolbox for analysis of single - trial eeg dynamics including independent component analysis , '' _ journal of neuroscience methods _ , vol .",
    "134 , pp . 921 , 2004 .",
    "kindermans , h.  verschore , d.  verstraeten , and b.  schrauwen , `` a p300 bci for the masses : prior information enables instant unsupervised spelling , '' in _ proc .",
    "neural information processing systems ( nips ) _ , lake tahoe , nv , december 2012 .",
    "v.  j. lawhern , d.  j. slayback , d.  wu , and b.  j. lance , `` efficient labeling of eeg signal artifacts using active learning , '' in _ proc .",
    "ieee intl conf . on systems ,",
    "man and cybernetics _ , hong kong , october 2015 .",
    "m.  long , j.  wang , g.  ding , s.  j. pan , and p.  s. yu , `` adaptation regularization : a general framework for transfer learning , '' _ ieee trans . on knowledge and data engineering _ , vol .",
    "26 , no .  5 , pp . 10761089 , 2014 .",
    "f.  lotte and c.  guan , `` learning from other subjects helps reducing brain - computer interface calibration time , '' in _ proc .",
    "ieee intl .",
    "conf . on acoustics speech and signal processing ( icassp )",
    "_ , dallas , tx , march 2010 .",
    "s.  makeig , c.  kothe , t.  mullen , n.  bigdely - shamlo , z.  zhang , and k.  kreutz - delgado , `` evolving signal processing for brain - computer interfaces , '' _ proc . of the ieee _",
    "100 , no .  3 , pp .",
    "15671584 , 2012 .",
    "a.  marathe , v.  lawhern , d.  wu , d.  slayback , and b.  lance , `` improved neural signal classification in a rapid serial visual presentation task using active learning , '' _ ieee trans . on neural systems and rehabilitation engineering _ ,",
    "24 , no .  3 , pp .",
    "333343 , 2016 .",
    "l.  parra , c.  christoforou , a.  gerson , m.  dyrholm , a.  luo , m.  wagner , m.  philiastides , and p.  sajda , `` spatiotemporal linear decoding of brain state , '' _ ieee signal processing magazine _ , vol .",
    "25 , no .  1 ,",
    "pp . 107115 , 2008 .",
    "e.  a. pohlmeyer , j.  wang , d.  c. jangraw , b.  lou , s .- f .",
    "chang , and p.  sajda , `` closing the loop in cortically - coupled computer vision : a brain - computer interface for searching image databases , '' _ journal of neural engineering _",
    ", vol .  8 , no .  3 , 2011 .",
    "a.  j. ries , j.  touryan , j.  vettel , k.  mcdowell , and w.  d. hairston , `` a comparison of electroencephalography signals acquired from conventional and mobile systems , '' _ journal of neuroscience and neuroengineering _ , vol .  3 , no .  1 ,",
    "pp . 1020 , 2014 .",
    "p.  sajda , e.  pohlmeyer , j.  wang , l.  parra , c.  christoforou , j.  dmochowski , b.  hanna , c.  bahlmann , m.  singh , and s .- f .",
    "chang , `` in a blink of an eye and a switch of a transistor : cortically coupled computer vision , '' _ proc . of the ieee _",
    "98 , no .  3 , pp .",
    "462478 , 2010 .                p.",
    "wang , j.  lu , b.  zhang , and z.  tang , `` a review on transfer learning for brain - computer interface classification , '' in _ prof .",
    "5th intl conf . on information science and technology ( ic1st ) _ , changsha , china , april 2015 .",
    "d.  wu , c .- h .",
    "chuang , and c .- t .",
    "lin , `` online driver s drowsiness estimation using domain adaptation with model fusion , '' in _ proc .",
    "intl conf . on affective computing and intelligent interaction _ , xian , china , september 2015 .",
    "d.  wu , b.  j. lance , and v.  j. lawhern , `` active transfer learning for reducing calibration data in single - trial classification of visually - evoked potentials , '' in _ proc .",
    "ieee intl conf . on systems ,",
    "man , and cybernetics _",
    ", san diego , ca , october 2014 .",
    "d.  wu , v.  j. lawhern , s.  gordon , b.  j. lance , and c .- t .",
    "lin , `` offline eeg - based driver drowsiness estimation using enhanced batch - mode active learning ( ebmal ) for regression , '' in _ proc .",
    "ieee intl conf . on systems ,",
    "man and cybernetics _ ,",
    "budapest , hungary , october 2016 .",
    "d.  wu , v.  j. lawhern , w.  d. hairston , and b.  j. lance , `` switching eeg headsets made easy : reducing offline calibration effort using active weighted adaptation regularization , '' _ ieee trans . on neural systems and rehabilitation engineering _",
    ", 2016 , in press .",
    "d.  wu , v.  j. lawhern , and b.  j. lance , `` reducing bci calibration effort in rsvp tasks using online weighted adaptation regularization with source domain selection , '' in _ proc .",
    "intl conf . on affective computing and intelligent interaction _ , xian , china , september 2015 .",
    "d.  wu , v.  j. lawhern , and b.  j. lance , `` reducing offline bci calibration effort using weighted adaptation regularization with source domain selection , '' in _ proc .",
    "ieee intl conf . on systems ,",
    "man and cybernetics _",
    ", hong kong , october 2015 .",
    "t.  o. zander and c.  kothe , `` towards passive brain - computer interfaces : applying brain - computer interface technology to human - machine systems in general , '' _ journal of neural engineering _",
    ", vol .  8 , no .  2 , 2011 ."
  ],
  "abstract_text": [
    "<S> many real - world brain - computer interface ( bci ) applications rely on single - trial classification of event - related potentials ( erps ) in eeg signals . </S>",
    "<S> however , because different subjects have different neural responses to even the same stimulus , it is very difficult to build a generic erp classifier whose parameters fit all subjects . </S>",
    "<S> the classifier needs to be calibrated for each individual subject , using some labeled subject - specific data . </S>",
    "<S> this paper proposes both online and offline weighted adaptation regularization ( war ) algorithms to reduce this calibration effort , i.e. , to minimize the amount of labeled subject - specific eeg data required in bci calibration , and hence to increase the utility of the bci system . </S>",
    "<S> we demonstrate using a visually - evoked potential oddball task and three different eeg headsets that both online and offline war algorithms significantly outperform several other algorithms . </S>",
    "<S> moreover , through source domain selection , we can reduce their computational cost by about @xmath0 , making them more suitable for real - time applications .    </S>",
    "<S> brain - computer interface , event - related potential , eeg , domain adaptation , transfer learning </S>"
  ]
}