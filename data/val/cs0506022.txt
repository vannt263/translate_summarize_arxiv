{
  "article_text": [
    " always prefer the _ simplest _ explanation for your observation , \" says occam s razor . in learning and information theory , simplicity",
    "is often quantified in terms of description length , giving immediate rise to the minimum description length ( mdl ) principle @xcite .",
    "thus mdl can be seen as a strategy against overfitting .",
    "an alternative way to think of mdl is bayesian .",
    "the explanations for the observations ( the _ models _ ) are endowed with a prior .",
    "then the model having maximum a posteriori ( map ) probability is also a two - part mdl estimate , where the correspondence between probabilities and description lengths is simply by a negative logarithm .    how does two - part mdl perform for prediction ? some very accurate answers to this question have been already given .",
    "if the data is generated by an independently identically distributed ( i.i.d . )",
    "process , then the mdl estimates are consistent @xcite . in this case , an important quantity to consider is the _ index of resolvability _ , which depends on the complexity of the data generating process .",
    "this quantity is a tight bound on the regret in terms of coding ( i.e.  the excess code length ) .",
    "moreover , the index of resolvability also bounds the predictive regret , namely the rate of convergence of the predictive distribution to the true one .",
    "these results apply to both discrete and continuously parameterized model classes , where in the latter case the mdl estimator must be discretized with an appropriate precision .    under the relaxed assumption that the data generating process obeys a central limit theorem and some additional conditions",
    ", rissanen @xcite proves an asymptotic bound on the regret of mdl codes . here , he also removes the coding redundancy arising if two - part codes are defined in the straightforward way . the resulting bound is very similar to that in @xcite for bayes mixture codes and i.i.d.processes , where the i.i.d .  assumption may also be relaxed @xcite .",
    "other similar and related results can be found in @xcite .    in this work ,",
    "we develop new methods in order to arrive at very general consistency theorems for mdl on _ countable model classes_. our setup is _ online sequence prediction _ ,",
    "that is , the symbols @xmath0 of an infinite sequence are revealed successively by the environment , where our task is to predict the next symbol in each time step .",
    "consistency is established by proving _",
    "finite cumulative bounds _ on the differences of the predictive to the true distribution .",
    "differences will be measured in terms of the relative entropy , the quadratic distance , and the hellinger distance .",
    "most of our results are based on the only assumption that the data generating process is _ contained in the models class_. ( the discussion of how strong this assumption is , will be postponed to the last section . )",
    "our results imply regret bounds with _ arbitrary _ loss functions .",
    "moreover , they can be directly applied to important general setups such as pattern classification , regression , and universal induction .",
    "as many scientific models ( e.g.  in physics or biology ) are smooth , much statistical work is focussed on continuous model classes . on the other hand ,",
    "the largest relevant classes from a computational point of view are at most countable .",
    "in particular , the field of algorithmic information theory ( also known as kolmogorov complexity , e.g.  @xcite ) studies the class of _ all lower - semicomputable semimeasures_. then there is a one - to - one correspondence of models and programs on a fixed universal turing machine .",
    "( since programs need not halt on each input , models are semimeasures instead of measures , see e.g.  @xcite for details )",
    ". this model class can be considered the largest one which can be in the limit processed under standard computational restrictions .",
    "we will develop all our results for semimeasures , so that they can be applied in this context , which we refer to as _",
    "universal _ sequence prediction .    in the universal setup ,",
    "the bayes mixture is also termed solomonoff - levin prior and has been intensely studied first by solomonoff @xcite .",
    "its predictive properties are excellent @xcite .",
    "precisely one can bound the cumulative loss by the complexity of the data generating process .",
    "this is the reference performance we compare mdl to .",
    "it turns out that the predictive properties of mdl can be exponentially worse , even in the case that the model class contains only bernoulli distributions .",
    "another related quantity in the universal setup is _ one - part mdl _ , which has been studied in @xcite",
    ". we will briefly encounter it in section [ subsecuniversal ] .",
    "the paper is structured as follows .",
    "section [ secpn ] establishes basic definitions . in section [ secmdl ]",
    ", we introduce the mdl estimator and show how it can be used for sequence prediction in at least three ways .",
    "sections [ secdynamic ] and [ secstatic ] are devoted to convergence theorems . in sections",
    "[ sechybrid ] and [ secstabilization ] , we study the stabilization properties of the mdl estimator .",
    "section [ secapp ] presents more general loss bounds as well as three important applications : pattern classification , regression , and universal induction .",
    "finally , section [ secdc ] contains the conclusions .",
    "we build on the notation of @xcite and @xcite .",
    "let the alphabet @xmath1 be a finite set of symbols .",
    "we consider the spaces @xmath2 and @xmath3 of finite strings and infinite sequences over @xmath1 .",
    "the initial part of a sequence up to a time @xmath4 or @xmath5 is denoted by @xmath6 or @xmath7 , respectively .",
    "the empty string is denoted by @xmath8 .",
    "a _ semimeasure _ is a function @xmath9 $ ] such that [ eq : semimeasure ] ( ) 1 ( x)_a(xa ) _ all x^ * holds .",
    "if equality holds in both inequalities of ( [ eq : semimeasure ] ) , then we have a _",
    "measure_. intuitively , the quantity @xmath10 can be understood as the probability that a data generating process yields a string starting with @xmath11 .",
    "then , for a measure , the probabilities of all joint continuations of @xmath11 add up to @xmath10 , while for a semimeasure , there may be a  probability leak \" ( [ eq : semimeasure ] ) . recall that we are interested in semimeasures ( and not only in measures ) because of their correspondence to programs on a fixed universal turing machine in the universal setup and our inability to decide the halting problem .",
    "let @xmath12 be a countable class of ( semi)measures , i.e.@xmath13 with finite or infinite index set @xmath14 .",
    "a ( semi)measure @xmath15 _ dominates _ the class @xmath12 iff for every @xmath16 there is a constant @xmath17 such that @xmath18 holds for all @xmath19 .",
    "a dominant semimeasure @xmath15 need not be contained in @xmath12 .    each ( semi)measure @xmath20 is associated with a weight @xmath21 , and we require @xmath22 .",
    "we may interpret the weights as a _",
    "prior _ on @xmath12 .",
    "then it is obvious that the bayes mixture [ eq : xi ] ( x)_[](x):=_w_(x ) ( x^ * ) dominates @xmath12 .",
    "assume that there is some measure @xmath23 , the _ true distribution _",
    ", generating sequences @xmath24 .",
    "typically @xmath25 is unknown .",
    "( note that we require @xmath25 to be a measure : the data stream always continues , there are no  probability leaks \" . ) if some initial part @xmath7 of a sequence is given , the probability of observing @xmath26 as a next symbol is [ eq : basicprediction ] ( x_t|x)= ( x)>0   ( x_t|x)=0 ( x)=0 . and , for well - definedness , @xmath27 if @xmath28 ( this case has probability zero ) . note that @xmath29 can depend on the complete history @xmath7 .",
    "we may generally define the quantity ( [ eq : basicprediction ] ) for _ any _ function @xmath30 $ ] ; we call @xmath31 the _",
    "@xmath32-prediction_. clearly , this is not necessarily a probability on @xmath1 for general @xmath32 . for a semimeasure @xmath33 in particular ,",
    "the @xmath33-prediction @xmath34 is a semimeasure on @xmath1 .",
    "we define the _ expectation _ with respect to the true probability @xmath25 : let @xmath35 and @xmath36 be a function , then [ eq : expectation ]  f=  f(x)=_x^n(x)f(x ) .",
    "more general , the expectation may be defined as an integral over infinite sequences .",
    "but since we wo nt need it , we can keep things simple .",
    "the following is a central result about prediction with bayes mixtures in a form independent of algorithmic information theory .",
    "[ theorem : solomonoff ] for any class of ( semi)measures @xmath12 containing the true distribution @xmath25 , which is a measure , we have [ eq : solomonoff ] _ t=1^_a ( ( a|x)-(a|x ) ) ^2  w_^-1 .",
    "this was found by solomonoff ( @xcite ) for universal sequence prediction . a proof is also given in @xcite ( only for binary alphabet ) or @xcite ( arbitrary alphabet ) .",
    "it is surprisingly simple once lemma [ lemma : entropyineq ] is known .",
    "a few lines analogous to ( [ eq : theoremrhoconvergesims0 ] ) and ( [ eq : theoremrhoconvergesims00 ] ) exploiting the dominance of @xmath37 are sufficient .",
    "one should be aware that the condition @xmath23 is essential in general , for both bayes and mdl predictions @xcite .",
    "on the other hand , one can show that if there is an element in @xmath12 which is sufficiently close to @xmath25 in an appropriate sense , then still good predictive properties hold @xcite .",
    "note that although @xmath38 can be interpreted as a prior on the model class , we do not assume any probabilistic structure for @xmath12 ( e.g.  a sampling mechanism ) .",
    "the theorem rather states that the cumulative loss is bounded by a quantity depending on the complexity @xmath39 of the true distribution .",
    "the same kind of assertion will be proven for mdl predictions later .",
    "the bound ( [ eq : solomonoff ] ) implies that the @xmath37-predictions converge to the @xmath25-predictions almost surely ( i.e.  with @xmath25-probability one ) .",
    "this is not hard to see , since with the abbreviation @xmath40 and for each @xmath41 , we have ( tn : s_t ) & = & ( _ tn\\{s_t } ) + [ eqihswp1 ] & & _",
    "tn(s_t )  _ t = n^s_t",
    "actually , ( [ eq : solomonoff ] ) yields an even stronger assertion , since it characterizes the _ speed of convergence _ by the quantity on the right hand side .",
    "precisely , the expected number of times @xmath42 in which @xmath43 deviates by more than @xmath44 from @xmath45 is finite and bounded by @xmath46 , and the probability that the number of @xmath44-deviations exceeds @xmath47 is smaller than @xmath48 .",
    "( however , we _ can not _",
    "conclude a convergence rate of @xmath49 from ( [ eq : solomonoff ] ) , since the quadratic differences generally do not decrease monotonically . )    since we will encounter this type of convergence ( [ eq : solomonoff ] ) frequently in the following , we call it _ convergence in mean sum ( i.m.s ) _ : [ eq : ims ]  c>0 :  _ t=1^_a ( ( a|x)-(a|x ) ) ^2 < .",
    "then theorem [ theorem : solomonoff ] states that the @xmath37 predictions converge to the @xmath25 predictions i.m.s . , or ",
    "@xmath37 converges to @xmath25 i.m.s . \" for short . by ( [ eqihswp1 ] ) , convergence i.m.s",
    ".  implies almost sure convergence ( with respect to the true distribution @xmath25 ) . note that in contrast , convergence in the mean , i.e.@xmath50\\toinfty{t } 0 $ ] , only implies convergence in probability .",
    "probabilities vs.  description lengths [ par : prob ] by the kraft inequality , each ( semi)measure can be associated with a code length or _",
    "complexity _ by means of the negative logarithm , where all ( binary ) codewords form a prefix - free set .",
    "the converse holds as well .",
    "we introduce the abbreviation [ eq : k ]   = -  , ( x)=-(x ) for a semimeasure @xmath33 and @xmath19 and @xmath51 for the bayes mixture @xmath37 .",
    "it is common to ignore the somewhat irrelevant restriction that code lengths must be integer . in particular , given a class of semimeasures @xmath12 together with weights , each weight @xmath38 corresponds to a description length or complexity [ eq : kw ] w()=-w_. it is often only a matter of notational convenience if description lengths or probabilities are used , but description lengths are generally preferred in algorithmic information theory .",
    "keeping the equivalence in mind , we will develop the general theory in terms of probabilities , but formulate parts of the results in universal sequence prediction rather in terms of complexities .",
    "assume that @xmath12 is a countable class of semimeasures together with weights @xmath52 , and @xmath19 is some string .",
    "then the _ maximizing element _",
    "@xmath53 , often called map ( maximum a posteriori ) estimator , is defined as [ eqmaxelem ] ^x = ^x_[]=_\\{w_(x)}. in case of a tie , we need not specify the further choice at this point , just pick any of the maximizing elements . but for concreteness , you may think that ties are broken in favor of larger prior weights .",
    "the maximum is always attained in ( [ eqmaxelem ] ) since for each @xmath41 at most a finite number of elements fulfil @xmath54 .",
    "observe immediately the correspondence in terms of _ description lengths _ rather than _ probabilities _ : ^x = _ \\{w()+(x)}.",
    "then the _ minimum description length principle _ is obvious : @xmath53 minimizes the joint description length of the model plus the data given the modelthe term map estimator is more precise . for two reasons ,",
    "our definition might not be considered as mdl in the strict sense .",
    "first , mdl is often associated with a specific prior , while we admit arbitrary priors ( compare the discussion section at the end of this paper ) .",
    "second , when coding some data @xmath11 , one can exploit the fact that once the distribution @xmath53 is specified , only data which leads to this @xmath53 needs to be considered .",
    "this allows for a description shorter than @xmath55 .",
    "nevertheless , the _ construction principle _ is commonly termed mdl , compare e.g.  the  ideal mdl \" in @xcite . ]",
    "( see ( [ eq : k ] ) and ( [ eq : kw ] ) ) . as explained before , we stick to the product notation .    for notational simplicity ,",
    "let @xmath56 .",
    "the _ two - part mdl estimator _ is defined by ( x ) = _ [ ] ( x ) = w_^x^x(x ) = _ \\{w_(x)}. so @xmath57 chooses the maximizing element with respect to its argument . we may also use the version @xmath58 for which the choice depends on the superscript instead of the argument",
    "note that the use of the term  estimator \" is non - standard , since @xmath57 is a product of the estimator @xmath59 ( this use is standard ) and its prior weight .",
    "there will be no confusion between these two meanings of  estimator \" in the following .    for each @xmath60 , [ eq :",
    "xirho ] ( x)(x)^y(x ) is immediate .",
    "if @xmath12 contains only measures , we have @xmath61 for all @xmath19 , so @xmath57 has some  anti - semimeasure \" property .",
    "if @xmath12 contains semimeasures , no semimeasure or anti - semimeasure property can be established for @xmath57 .",
    "we can define mdl predictors according to ( [ eq : basicprediction ] ) .",
    "there are basically _ two _ possible ways to use mdl for prediction .",
    "[ def : dynamicmdl ] the _ dynamic _ mdl predictor is defined as ( a|x ) = = .",
    "that is , we look for a short description of @xmath62 and relate it to a short description of @xmath63 .",
    "we call this dynamic since for each possible @xmath64 we have to find a new mdl estimator .",
    "this is the closest correspondence to the bayes mixture @xmath37-predictor .",
    "[ def : staticmdl ] the _ static _ mdl predictor is given by ^(a|x ) = ^x(a|x ) = = = . here obviously only _ one _ mdl estimator @xmath65 has to be identified .",
    "this is usually more efficient in practice .",
    "we will define another mdl predictor , the _ hybrid _ one , in section [ sechybrid ] .",
    "it can be paraphrased as  do dynamic mdl but drop weights \" .",
    "we will see that its predictive performance is weaker .    the range of the static mdl predictor is obviously contained in @xmath66 $ ] . for the dynamic mdl predictor , this holds by [ eq : rhoin01 ] ^x(x)^xa(x)^xa(xa ) .",
    "static mdl is omnipresent in machine learning and applications , see also section [ secapp ] .",
    "in fact , many common prediction algorithms can be abstractly understood as static mdl , or rather as approximations .",
    "namely , if a prediction task is accomplished by building a _ model _ such as a neural network with a suitable regularization to prevent  overfitting \" , this is just searching an mdl estimator within a certain class of distributions .",
    "after that , only this model is used for prediction .",
    "dynamic mdl is applied more rarely due to its larger computational effort .",
    "for example , the similarity metric proposed in @xcite can be interpreted as ( a deterministic variant of ) dynamic mdl .",
    "we will need to convert our mdl predictors to _ measures _ by means of _",
    "normalization_. if @xmath30 $ ] is any function , then _",
    "norm(a|x )  : =   =   is a measure ( assume that the denominator is different from zero , which is always true with probability 1 ( w.p.1 ) if @xmath32 is an mdl predictor ) .",
    "this procedure is known as _",
    "solomonoff normalization _",
    "@xcite and results in _",
    "norm(x ) = _ t=1^n = ( x)()n_(x_<n ) , where [ eq : normalizer ] n_(x ) = _ t=1^(x)+1 is the normalizer .",
    "we conclude this section with a simple example .",
    "bernoulli and i.i.d .",
    "classes [ ex : iid ] let @xmath67 , @xmath68 , and = \\{_(x_1:t)=_x_1 ... _x_t : } = \\{([0,1])^n:_i=1^n _ i=1 } be the set of all rational probability vectors with any prior @xmath69 .",
    "each @xmath70 generates sequences @xmath71 of _ independently identically distributed ( i.i.d . )",
    "_ random variables such that @xmath72 for all @xmath73 and @xmath74 .",
    "if @xmath6 is the initial part of a sequence and @xmath75 is defined by @xmath76 , then it is easy to see that ^x=_\\ { w()2 + td ( ) } , where @xmath77 is the _ kullback - leibler divergence_. if @xmath78 , then @xmath79 is also called a _ bernoulli class _ , and one usually takes the binary alphabet @xmath80 in this case .",
    "we may now develop convergence results , beginning with the dynamic mdl predictor from definition [ def : dynamicmdl ] .",
    "the following simple lemma is crucial for all subsequent proofs .",
    "[ lemma : diffissemimeasure ] for an arbitrary class of ( semi)measures @xmath12 , we have & ( i ) &   ( x)-_a(xa )   ( x)-_a(xa )  and + & ( ii ) &   ^x(x)-_a^x(xa )   ( x)-_a(xa ) for all @xmath19 . in particular , @xmath81 is a semimeasure .",
    "for all @xmath19 , with @xmath82 we have _ a f(xa ) & = & _ a ( ( xa)-(xa ) ) _ a ( ( xa)-^x(xa ) ) + & = & _ \\{^x } _ a w_(xa ) _ \\{^x } w_(x ) = ( x)-(x ) = f(x ) .",
    "the first inequality follows from @xmath83 , and the second one holds since all @xmath33 are semimeasures .",
    "finally , @xmath84 and @xmath85 .",
    "hence @xmath86 is a semimeasure .",
    "the following proposition demonstrates how simple it can be to obtain a convergence result , however a weak one .",
    "various similar results have been already obtained in the past , e.g.in @xcite .",
    "[ prop : rhoconvergesmart ] for any class of ( semi)measures @xmath12 containing the true distribution @xmath25 , we have 1w .. p.1    since @xmath81 is a positive semimeasure by lemma [ lemma : diffissemimeasure ] , @xmath87 is a positive super - martingale . by doob s martingale convergence theorem",
    "( see e.g.  @xcite or @xcite or any textbook on advanced probability theory ) , it therefore converges on a set of @xmath25-measure one .",
    "moreover , @xmath88 converges on a set of measure one , being a positive super - martingale as well ( * ? ? ?",
    "* thm.5.2.2 ) .",
    "thus @xmath89 must converge on a set of measure one .",
    "we denote this limit by @xmath86 and observe that @xmath90 since @xmath91 everywhere . on this set of measure one",
    ", the denominator @xmath92 of = converges to @xmath93 , and so does the numerator .",
    "the whole fraction thus converges to one , which was to be shown .",
    "proposition [ prop : rhoconvergesmart ] gives only a statement about  on - sequence \" ( @xmath94 ) convergence of the @xmath57-predictions .",
    "indeed , no conclusion about  off - sequence \" convergence , i.e.  @xmath95 for arbitrary @xmath96 , can be drawn from the proposition , not even in the deterministic case . there , the true measure @xmath25 is concentrated on the particular sequence @xmath97 .",
    "so for @xmath98 , we have @xmath99 , and thus no assertion for @xmath95 can be made .",
    "on the other hand , an off - sequence result is essential for prediction : even if on the _ correct _ next symbol the predictive probability is very close to the true value , we must be sure that this is so also for all _",
    "alternatives_. this is particularly important if we base some decision on the prediction ; compare section [ subsec : lossbounds ] .",
    "the following theorem closes this gap .",
    "in addition , it provides a statement about the speed of convergence . in order to prove it",
    ", we need a lemma establishing a relation between the square distance and the kullback - leibler distance , which is proven for instance in ( * ? ? ?",
    "* sec.3.9.2 ) .",
    "[ lemma : entropyineq ] let @xmath25 and @xmath100 be measures on @xmath1 , then _",
    "a((a)-(a))^2 _ a ( a ) .    [ theorem : rhoconvergesims ] for any class of ( semi)measures @xmath12 containing the true distribution @xmath25 ( which is a measure ) , we have _ t=1^  _ a ( ( a|x)-_norm(a|x))^2 w_^-1 + w_^-1 .",
    "that is , @xmath101 ( see ( [ eq : ims ] ) ) , which implies @xmath102 with @xmath25-probability one .",
    "let @xmath67 . from lemma",
    "[ lemma : entropyineq ] , we know + & = & _ t=1^n   = _ t=1^n  .",
    "[ eq : theoremrhoconvergesims0 ] then we can estimate [ eq : theoremrhoconvergesims00 ] _",
    "t=1^n   =  _ t=1^n  =   w_^-1 , since always @xmath103 . moreover , by setting @xmath63 , using @xmath104 , adding an always positive max - term , and finally using @xmath103 again , we obtain + & & _ + [ eq : theoremrhoconvergesims1 ] & & w_^-1 _ (x)=t-1 . if @xmath12 contains only measures , the max - term is not necessary , since @xmath57 is an `` anti - semimeasure '' in this case .",
    "we proceed by observing [ eq : theoremrhoconvergesims2 ] _",
    "(x)=t-1 = _",
    "t=1^n = - ( ) which is true since for successive @xmath42 the positive and negative terms cancel . from lemma [ lemma : diffissemimeasure ]",
    "we know @xmath105 and therefore _",
    "t=1^n _ \\{0,(x)-_a(xa ) } & & _ t=1^n _ \\{0,(x)-_a(xa ) } + = _ t=1^n _ & = & ( ) -_(x)=n(x ) .",
    "[ eq : theoremrhoconvergesims3 ] here we have again used the fact that positive and negative terms cancel for successive @xmath42 , and moreover the fact that @xmath37 is a semimeasure . combining ( [ eq : theoremrhoconvergesims1 ] ) , ( [ eq : theoremrhoconvergesims2 ] ) and ( [ eq : theoremrhoconvergesims3 ] ) , and observing @xmath106 , we obtain [ eq : theoremrhoconvergesimsnormalizer ] _",
    "t=1^n   w_^-1 w_^-1 ( ) w_^-1 . therefore , ( [ eq : theoremrhoconvergesims0 ] ) , ( [ eq : theoremrhoconvergesims00 ] ) and ( [ eq : theoremrhoconvergesimsnormalizer ] ) finally prove the assertion .",
    "we point out again that the proof gets a bit simpler if @xmath12 contains only measures , since then ( [ eq : theoremrhoconvergesims3 ] ) becomes irrelevant .",
    "however , this case does nt give a tighter bound .",
    "this is the first convergence result  in mean sum \" , see ( [ eq : ims ] ) .",
    "it implies both on - sequence and off - sequence convergence .",
    "moreover , it asserts the convergence is  fast \" in the sense that the sum of the total expected deviations is bounded by @xmath107 .",
    "of course , @xmath108 can be very large , namely @xmath109 .",
    "the following example will show that this bound is sharp ( save for a constant factor ) .",
    "observe that in the corresponding result for mixtures , theorem [ theorem : solomonoff ] , the bound is much smaller , namely @xmath110 .",
    "[ ex : lowerbound ] let @xmath111 , @xmath112 and @xmath113 .",
    "each @xmath114 is a deterministic measure concentrated on the sequence @xmath115 , while the true distribution @xmath25 is deterministic and concentrated on @xmath116 .",
    "let @xmath117 for all @xmath118 .",
    "then @xmath25 generates @xmath71 , and for each @xmath119 we have @xmath120 .",
    "hence , @xmath121 . in example",
    "[ ex : bernoulli ] we will even see a case where the model class contains only bernoulli distributions and nevertheless the exponential bound is sharp .    the next result implies that convergence holds also for the un - normalized dynamic mdl predictor .",
    "[ thnormalizer ] [ thrhonorm ] for any class of ( semi)measures @xmath12 containing the true distribution @xmath25 , we have & ( i ) & _ t=1^| _ a(a|x ) |  2 w_^-1 and + & ( ii ) & _ t=1^_a | _ norm(a|x)-(a|x ) |  = _",
    "t=1^ | 1- _",
    "a(a|x ) |  2 w_^-1 .",
    "@xmath122 define @xmath123 for @xmath124 , then for @xmath125 we have + & &  +   + & = & _ ( x)(_a(xa)-(x))^+(x )  + _ ( x)((x)-_a(xa))^+_a(xa ) + & & w_^-1 _ ( _ a(xa)-(x))^+  +  w_^-1 _ ( ( x)-_a(xa))^+ + & = & w_^-1 _ |(x)-| = w_^-1 _ here , @xmath126 , @xmath104 , and @xmath127 have been used , the latter implies also @xmath128 . the last expression in this ( in)equality chain , when summed over @xmath129 is bounded by @xmath130 by essentially the same arguments ( [ eq : theoremrhoconvergesims1 ] ) - ( [ eq : theoremrhoconvergesimsnormalizer ] ) as in the proof of theorem [ theorem : rhoconvergesims ] .",
    "@xmath131 let again @xmath132 and use @xmath133 to obtain [ pr : rnr ] _ a | _ norm(a|x)-(a|x ) | & = & _ a ( a|x)_b(b|x)| 1-_b(b|x ) | = | 1-_b(b|x ) | + & = & ( _ a(xa)-(x))^+(x ) + ( ( x)-_a(xa))^+(x ) .",
    "then take the expectation @xmath134 and the sum @xmath135 and proceed as in @xmath122 .",
    "[ cor : murho ] for any class of ( semi)measures @xmath12 containing the true distribution @xmath25 , we have _ t=1^_a ( ( a|x)-(a|x ) ) ^2  8 w_^-1 .",
    "that is , @xmath136 ( see ( [ eq : ims ] ) ) .    for two functions",
    "@xmath137 $ ] , let [ eq : delta ] ( _ 1,_2)=(_t=1^_a ( _ 1(a|x)-_2(a|x))^2)^. then the triangle inequality holds for @xmath138 , since @xmath139 is ( proportional to ) an euclidian distance ( 2-norm )",
    ". moreover , @xmath140 by theorem [ theorem : rhoconvergesims ] and @xmath141 .",
    "we also have @xmath142 by multiplying @xmath143 in theorem [ thrhonorm]@xmath131 with another @xmath143 .",
    "note @xmath144 , since both @xmath145 $ ] , for @xmath57 this holds by ( [ eq : rhoin01 ] ) .",
    "this implies @xmath146 .    for almost all @xmath147 ,",
    "the normalizer @xmath148 defined in ( [ eq : normalizer ] ) converges to a number which is finite and greater than zero , i.e.@xmath149 .",
    "moreover , the sum of the mdl posterior estimates converges to one almost surely , [ eq : theoremrhonormalizerconverges ] _ a ( a|x ) = 1 as tw .. p.1 .",
    "theorem [ thnormalizer ] implies that with probability one , the sum @xmath150 is bounded in @xmath151 , hence converges absolutely , hence also the limit n_(x _ < )  = _",
    "t=1^ exists and is finite .",
    "for these sequences , @xmath149 and ( [ eq : theoremrhonormalizerconverges ] ) follows .",
    "static mdl as introduced in definition [ def : staticmdl ] is usually more efficient and thus preferred in practice , since only one mdl estimator has to be computed .",
    "the following technical result will allow to conclude that the static mdl predictions converge in mean sum like the dynamic ones .",
    "[ thsmdlbound ] for any class of ( semi)measures @xmath12 containing the true distribution @xmath25 , we have _ t=1^ _ a | _ norm^x(a|x ) - ^x(a|x ) |  =  _ t=1^ | 1- _",
    "a^x(a|x ) |  w_^-1 .",
    "we proceed in a similar way as in the proof of theorem [ theorem : rhoconvergesims ] , ( [ eq : theoremrhoconvergesims1 ] ) - ( [ eq : theoremrhoconvergesims3 ] ) . from lemma [ lemma :",
    "diffissemimeasure ] , we know @xmath152 . then _",
    "t=1^n | 1- _",
    "a^x(a|x ) | & = & _ t=1^n   + & = & _ t=1^n _ (x)=t-1 ( x ) + & & w_^-1  _ t=1^n _ (x)=t-1 + & & w_^-1  _ t=1^n _ (x)=t-1 + & & w_^-1  w_^-1 for all @xmath67 . this implies the assertion .",
    "again we have used @xmath103 and the fact that positive and negative terms cancel for successive @xmath42 .",
    "[ cor : murhostatic ] for any class of ( semi)measures @xmath12 containing the true distribution @xmath25 , we have _ t=1^_a ( ( a|x)-^x(a|x ) ) ^2 & & 21 w_^-1 + _ t=1^_a ( ( a|x)-^x_norm(a|x ) ) ^2 & & 32 w_^-1 .",
    "that is , @xmath153 and @xmath154 .",
    "using @xmath155 and the triangle inequality , we see _ a| ( a|x)-^x(a|x ) | = | _ a(a|x)- _ a^x(a|x ) | | _ a(a|x)- 1 | + | 1 - _ a^x(a|x ) | with @xmath138 as in ( [ eq : delta ] ) , using @xmath156 we therefore have ^2(,^)_t=1^_a| ( a|x)-^x(a|x)| 3w_^-1 according to theorem [ thrhonorm ] @xmath131 and theorem [ thsmdlbound ] .",
    "since @xmath157 holds by corollary [ cor : murho ] , we obtain @xmath158 .",
    "theorem [ thsmdlbound ] also asserts @xmath159 , hence @xmath160 follows .",
    "distance measures the total expected square error is not the only possible choice for measuring distance of distributions and speed of convergence . in fact , looking at the proof of theorem [ theorem : rhoconvergesims ] , the expected kullback - leibler distance may seem more natural at a first glance .",
    "however this quantity behaves well only under dynamic mdl , not static mdl . to see this ,",
    "let @xmath161 contain two bernoulli distributions , both with prior weight @xmath162 , and let @xmath163 be the uniform measure .",
    "if the first symbol happens to be 0 , which occurs with probability @xmath162 , then the static mdl estimate is @xmath164 .",
    "then @xmath165 , hence the expectation is @xmath166 , too .",
    "the quadratic distance behaves locally like the kullback - leibler distance ( lemma [ lemma : entropyineq ] ) , but otherwise is bounded and thus more convenient .",
    "another possible choice is the _",
    "hellinger distance _",
    "[ eq : hellinger ] h_t(,)|_x&=&_a(-)^2 + [ eq : cumhellinger ] h(,)&=&_t=1^nh_t ( , ) . like the square distance",
    ", the hellinger distance is bounded by both the relative entropy and the absolute distance : [ eqhellingerkl ] h_t ( , ) & & _ a ( a|x ) + [ eqhellingerabs ] h_t ( , ) & & _ a | ( a|x)- ( a|x ) | .",
    "the former is e.g.  shown in ( * ? ? ?",
    "* lem.3.11 , p.114 ) , the latter follows from @xmath167 for any @xmath168 .",
    "therefore , the same bounds we have proven for the square distance also hold for the hellinger distance ; they are subsumed in corollary [ cor : murhoall ] below .",
    "although for simplicity of notation we have preferred the square distance over the hellinger distance in the presentation so far , in sections [ subsec : lossbounds ] and [ subsec : regression ] we will meet situations where the quadratic distance is not sufficient .",
    "then the hellinger distance will be useful .",
    "the following corollary recapitulates our results and states convergence i.m.s ( and therefore also w.@xmath25-p.1 ) for all combinations of un - normalized / normalized and dynamic / static mdl predictions .",
    "[ cor : murhoall ] let @xmath12 contain the true distribution @xmath25 , thens(,_norm)2 w_^-1 , & & h(,_norm)2 w_^-1 , + s(,)8 w_^-1 , & & h(,)8 w_^-1 , + s(,^)21w_^-1 , & & h(,^)21w_^-1 , + s(,^_norm)32w_^-1 , & & h(,^_norm)32w_^-1 , where @xmath169 and @xmath170 is as in ( [ eq : cumhellinger ] ) .",
    "the following example shows that the exponential bound is sharp ( except for a multiplicative constant ) , even if the model class contains only bernoulli distributions .",
    "it is stated in terms of static mdl , however it equally holds for dynamic mdl .",
    "[ ex : bernoulli ] let @xmath112 and @xmath171 be a bernoulli class as discussed at the end of section [ secmdl ] .",
    "let @xmath25 be bernoulli with parameter @xmath162 , i.e.  the distribution generating fair coin flips .",
    "assume that all weights are equally @xmath172 .",
    "then it is shown in ( * ? ? ?",
    "5 ) that _ t=1^(-^x(1|x))^2(n-4 ) .",
    "so the bound equals @xmath108 within a multiplicative constant .",
    "this shows that in general there is no hope to improve the bounds , even for very simple model classes .",
    "but the situation is not as bad as it might seem .",
    "first , the bounds may be exponentially smaller under certain regularity conditions on the class and the weights , as @xcite and the positive assertions in @xcite show .",
    "it is open to define such conditions for more general model classes .",
    "second , the example just given behaves differently than example [ ex : lowerbound ] .",
    "there , the error remains at a significant level for @xmath173 time steps , which must be regarded critical . here in contrast",
    ", the error drops to zero as @xmath174 for a very long time , namely @xmath175 steps , and decreases more rapidly only afterwards .",
    "this behavior is tolerable in practice .",
    "recently , @xcite have proven that this favorable case always occurs for i.i.d .",
    ", if the weights satisfy the _ light tails _ condition @xmath176 for some @xmath177 @xcite .",
    "precisely , they give a rapidly decaying bound on the instantaneous error .",
    "it is open if similar results also hold in more general setups than i.i.d .",
    "example [ ex : lowerbound ] shows that at least some additional assumption is necessary .",
    "so far , we have not cared about what happens if two or more ( semi)measures obtain the same value @xmath178 for some string @xmath11 .",
    "in fact , for the previous results , the _ tie - breaking strategy _ can be completely arbitrary . this need not be so for all thinkable prediction methods , as we will see with the hybrid mdl predictor in the subsequent example .",
    "[ def : hybridmdl ] the _ hybrid _ mdl predictor is given by @xmath179 ( compare ( [ eqmaxelem ] ) ) .",
    "this can be paraphrased as  do dynamic mdl and drop the weights \" .",
    "it is somewhat in - between static and dynamic mdl .",
    "[ ex : first ] let @xmath80 and @xmath12 contain only two measures , the uniform measure @xmath180 which is defined by @xmath181 , and another measure @xmath33 having @xmath182 and @xmath183 .",
    "the respective weights are @xmath184 and @xmath185 .",
    "then , for each @xmath11 starting with @xmath186 , we have @xmath187 .",
    "therefore , for all @xmath71 starting with @xmath186 ( a set which has uniform measure @xmath162 ) , we have a tie . if the maximizing element @xmath59 is chosen to be @xmath180 for even @xmath42 and @xmath33 for odd @xmath42 , then both static and dynamic mdl predict probabilities of constantly = ( a|x)=(a|x)= = for all @xmath188 .",
    "however , the hybrid mdl predictor values @xmath189 oscillate between @xmath190 and @xmath186 .",
    "if the ambiguity in the tie - breaking process is removed , e.g.in favor of larger weights , then the hybrid mdl predictor does converge for this example .",
    "we replace ( [ eqmaxelem ] ) by this rule : ^x = \\{w_:\\{=_w_(x)}}. then , do the hybrid mdl predictions always converge ? this is equivalent to asking if the process of selecting a maximizing element eventually _",
    "stabilizes_. if stabilization does not occur , then hybrid mdl will necessarily fail as soon as the weights are not equal .",
    "a possible counterexample could consist of two measures the fraction of which oscillates perpetually around a certain value .",
    "we show that this can indeed happen , even for different reasons .",
    "[ example : notstabilizeapprox ] let @xmath1 be binary , @xmath191 and @xmath192 with _ i(1)=1 - 2 ^ -2  and   _ i(1)=1 - 2 ^ -2 + 1",
    ". then one can easily see that @xmath193 , @xmath194 , and @xmath195 converges and oscillates .",
    "in fact , each sequence having positive measure under @xmath25 and @xmath33 contains eventually only ones , and the quotient oscillates .",
    "[ example : notstabilizedependent ] this example is a little more complex .",
    "we assume the uniform distribution @xmath180 to be the true distribution .",
    "we now construct a positive martingale @xmath196 that converges to @xmath197 with high probability and thereby oscillates infinitely often .",
    "the martingale is defined on strings @xmath11 of successively increasing length .",
    "of course , @xmath198 . if @xmath199 is already defined for strings of length @xmath200 , we extend the definition on strings of length @xmath151 in the following way : if @xmath201 , we set f(x0 ) & : = & -2 ^ -n-2 and + f(x1 ) & : = & 2f(x)-(-2 ^ -n-2 ) .",
    "this guarantees the martingale property @xmath202 .",
    "if @xmath203 and @xmath204 , then we can similarly define f(x0 ) & : = & 2f(x)-(+2 ^ -n-2 ) and + f(x1 ) & : = & + 2 ^ -n-2 .",
    "however , if @xmath205 , we can not proceed in this way , since @xmath86 must be positive .",
    "therefore , we set @xmath206 in this case and call those @xmath11  dead \" strings .",
    "strings that are not dead will be called  alive \" .",
    "a few steps of the construction are shown in figure [ fig : martingaleex ] .",
    "for example , it can be observed that the string 000 is dead , all other strings in the figure are alive .",
    "it is obvious from the construction that @xmath207 is a martingale , it oscillates and converges to @xmath197 as @xmath208 for all sequences @xmath71 that always stay alive .",
    "the only thing we must show is that many sequences in fact stay alive .",
    "we have @xmath209 .",
    "after the @xmath151th step , i.e.  when @xmath86 has been defined for strings of length @xmath151 , @xmath199 assumes the value a_0^n=-2",
    "^ -n-2 on a set of measure at most @xmath162 . in the next step @xmath210",
    ", @xmath86 is defined to a_1^n=-2 ^ -n-1(1 + ) on half of the extended strings . generally , in the @xmath211th",
    "next step , @xmath86 is defined to a_k^n=-2 ^ -n+k-2(_j=0^k2 ^ -2j ) on a @xmath212 fraction of the extended strings .",
    "the extended strings stay alive as long as @xmath213 holds .",
    "some elementary calculations show that this is equivalent to @xmath214 .",
    "so precisely after @xmath210 additional steps , a fraction of @xmath215 of the extended strings die .",
    "we already noted that for @xmath216 , we have @xmath217 .",
    "thus , @xmath218 hence , one can conclude @xmath219 which proves the claim .",
    "we now define the measure @xmath33 by @xmath220 and set the weights to @xmath221 and @xmath222 .",
    "then this provides an example where the maximizing element never stops oscillating with probability at least @xmath197 .",
    "both examples point out different possible reasons for failure of stabilizing .",
    "example [ example : notstabilizeapprox ] works since the measure @xmath25 and @xmath33 are asymptotically very similar and close to deterministic .",
    "in contrast , in example [ example : notstabilizedependent ] stabilizing fails because of lack of independence : the quantity @xmath223 strongly depends on @xmath11 .",
    "in particular , one can note that even markovian dependence may spoil the stabilization , since @xmath223 only depends on the last symbol of @xmath11 .",
    "in the light of the previous section , it is therefore natural to ask when the maximizing element stabilizes ( almost surely ) .",
    "barron @xcite has shown that this happens if all distributions in @xmath12 are _ asymptotically mutually singular_. under this condition , the true distribution is even eventually identified almost surely . and has prior weight @xmath224 , the other one assigns probability @xmath225 independently of the past @xmath7 .",
    "then the maximizing element will remain the incorrect distribution @xmath33 , however with predictions rapidly converging to the truth . ]",
    "the condition of asymptotic mutual singularity holds in many important cases , e.g.  if the distributions are i.i.d .",
    "however , one can not always build on it . ) and another measure be a product of bernoullis with parameter rapidly converging to @xmath162 .",
    "these distributions are not asymptotically mutually singular , nevertheless a.s .",
    "stabilization holds , as we will see .",
    "] therefore , in this section we give a different approach : in order to prevent stabilization , it is necessary that the ratio of two predictive distributions oscillates around the inverse ratio of the respective weights .",
    "therefore , stabilization must occur almost surely if the ratio of two predictive distributions converges almost surely but is not _",
    "concentrated _ in the limit .",
    "this is satisfied under appropriate conditions , as we will prove .",
    "we start with a general theorem which allows to conclude almost sure stabilization in a countable model class , if for any _ pair _ of models we have almost sure stabilization .    [",
    "theorem : mdlstabilizesf2c ] let @xmath12 be a countable class of ( semi)measures containing the true measure @xmath25 . assume that for each two @xmath226 the maximizing element chosen from @xmath227 eventually stabilizes almost surely .",
    "then also the maximizing element chosen from all of @xmath12 stabilizes almost surely .",
    "it is immediate that the maximizing element chosen from any finite subset of @xmath12 stabilizes almost surely .",
    "now , for all @xmath20 and @xmath228 , define the set @xmath229 by a_c^= \\{x :  t1_thatc}. then we have ( a_c^ ) & = & ( \\{_x : c < c  s < (x ) } ) + & = & \\{(x):c < c  s < (x ) } + & & \\{:c < c  s < (x ) } + & = & \\{(x ) :  }  , since @xmath33 is a ( semi)measure and the set @xmath230 is prefix - free .",
    "let b^= \\{x :  t1_that1 } = a_(w_/w_)^ , then @xmath231 holds .",
    "we arrange the ( semi)measures @xmath20 in an order @xmath232 such that the weights @xmath233 are descending . for each @xmath234",
    ", we can now find an index @xmath211 and a set _",
    "c = \\{_i : ik } _ that _ _ c w_. defining @xmath235 , we get ( b_c)_^c . for all @xmath236",
    ", @xmath33 can never be the maximizing element .",
    "therefore , for all @xmath237 , there are only finitely many @xmath238 having the chance of becoming the maximizing element at any time . by assumption , the maximizing element chosen from the finite set @xmath239 stabilizes a.s .",
    "thus , we conclude almost sure stabilization on the sequences in @xmath240 .",
    "since this holds for all @xmath241 and @xmath242 as @xmath243 , the maximizing element stabilizes with @xmath25-probability one .    for the rest of this section , we assume that the model class @xmath12 contains only proper measures .",
    "a measure @xmath25 is called _ factorizable _ if there are measures @xmath244 on @xmath1 such that @xmath245 for all @xmath19 .",
    "that is , the symbols of sequences @xmath71 generated by @xmath25 are independent . a factorizable measure @xmath246",
    "is called _ uniformly stochastic _ ,",
    "if there is some @xmath247 such that at each time @xmath118 the probability of all symbols @xmath96 is either 0 or at least @xmath48 .",
    "that is [ eq : ustochastic ] _",
    "i(a)>0 _ i(a)_all ai1 .",
    "in particular , all deterministic measures and all i.i.d.distributions are uniformly stochastic .",
    "another simple example of a uniformly stochastic measure is a probability distribution which generates alternately random bits by fair coin flips and the digits of the binary representation of @xmath248    [ lemma : mdlstabilizesfinite ] let @xmath25 , @xmath33 , and @xmath15 be factorizable and uniformly stochastic measures , where @xmath25 is the true distribution . + @xmath122 the maximizing element chosen from @xmath25 and @xmath33 stabilizes almost surely .",
    "+ @xmath131 if @xmath25 is not eventually always preferred over @xmath33 or @xmath15 in which case we the maximizing element stabilizes a.s . by @xmath122 ,",
    "then the maximizing element chosen from @xmath33 and @xmath15 stabilizes almost surely .",
    "we will show only @xmath131 , as the proof of @xmath122 is similar but simpler .",
    "so we assume that both @xmath33 and @xmath15 remain competitive in the process of choosing the maximizing element , and show that then maximizing element chosen from @xmath33 and @xmath15 stabilizes almost surely .",
    "let @xmath249 , @xmath250 , and @xmath251 .",
    "the @xmath252 are independent random variables depending on the event @xmath71 .",
    "moreover , both fractions @xmath253 and @xmath254 are martingales ( with respect to @xmath25 ) and thus converge almost surely for @xmath208 .",
    "we are interested only in the events in a_=\\{x^:>0 } , since otherwise @xmath33 eventually is no longer competitive .",
    "so we assume that @xmath255 , which implies @xmath256 by the kolmogorov zero - one - law ( see e.g.  @xcite ) .",
    "similarly , @xmath257 for the analogously defined set @xmath258 .",
    "that is , _",
    "i=1^t x_i= =/ converges to a value @xmath259 almost surely , and in particular @xmath260 a.s .",
    "now we will use the _ concentration function _ of a real valued random variable @xmath261 , [ eq : concentrationfunction ] q(u,)=_u(uuu+ ) ,  0 .",
    "this quantity was introduced by lvy , see e.g.@xcite .",
    "the concentration function is non - decreasing in @xmath262 .",
    "moreover , when two independent random variables @xmath261 and @xmath263 are added , we have ( * ? ? ?",
    "* lemma 1.11 ) [ eq : concentrationlemma ] q(u+v,)\\{q(u,),q(v , ) }  0 .",
    "we first assume that the following set is unbounded : [ eq : concentrationconditionset ] b=\\{_i=1^n(1-q(x_i , ) ) : n,>0}^+ , & & + [ eq : concentrationcondition ] ( b)=+ , & & we show that then @xmath264 ( which converges a.s . )",
    "is not concentrated in the limit .",
    "that is , it converges to some given @xmath228 , in particular to @xmath265 , with @xmath25-probability zero .",
    "this shows that almost surely it does not oscillate around @xmath266 .",
    "define independent random variables @xmath267 .",
    "let @xmath268 and denote its almost everywhere existing limit by @xmath269 .",
    "the assertion is verified under condition ( [ eq : concentrationcondition ] ) , if we can show that the distribution of @xmath270 is not concentrated to any point since then also @xmath271 is not concentrated to any point . in terms of the concentration function",
    "defined in ( [ eq : concentrationfunction ] ) , this reads @xmath272 . according to ( [ eq : concentrationcondition ] ) , for each @xmath273",
    ", we find @xmath274 and @xmath67 such that @xmath275 .",
    "then , because of @xmath276 ( ignoring the measure - zero set where this may fail ) , w=_1 in x_i= \\{:1 in  and  ( x_i)>0 } is finite . the mapping ( 0,w]w  ( w)(- ,  w ] is bijective and has derivative at least @xmath277 .",
    "let @xmath278 .",
    "then by definition of @xmath279 , we have @xmath280 for @xmath74 and consequently _ i=1^n(1-q(y_i,))>r .    by the kolmogorov - rogozin inequality ( see ( * ? ? ?",
    "* theorem 2.15 ) ) , there is a constant @xmath281 such that q(s_n,)c(_i=1^n(1-q(y_i,)))^- .",
    "thus , for each @xmath41 , we can choose @xmath282 sufficiently large to guarantee @xmath283 . then @xmath284 for @xmath151 and @xmath285 as before . by ( [ eq : concentrationlemma ] )",
    "we conclude q(s,)=q(s_n+(_i = n+1^y_i ) , ) q(s_n,)<and consequently @xmath272 since @xmath286 is non - decreasing .",
    "this proves the assertion under assumption ( [ eq : concentrationcondition ] ) .",
    "now assume that @xmath287 is bounded , i.e.([eq : concentrationcondition ] ) does not hold .",
    "then there is @xmath273 such that @xmath288 for all @xmath274 and @xmath67 .",
    "since the distribution of @xmath252 is a finite convex combination of point measures , for each @xmath118 there is an @xmath274 such that @xmath289 and thus @xmath290 for all @xmath67 .",
    "therefore , also @xmath291 holds .",
    "since @xmath292 is equivalent to @xmath293 , this implies that there are constants @xmath294 such that [ eq : sum1 ] _ i=1^_i\\{a:_i(a)c_i_i(a)}r .",
    "next we argue that if @xmath295 for infinitely many @xmath118 , then either @xmath33 or @xmath15 is eventually not competitive .",
    "to verify this claim , let @xmath296 and @xmath297 and observe that @xmath298 holds for sufficiently large @xmath118 , since the sum ( [ eq : sum1 ] ) is bounded",
    ". on the other hand @xmath25 is uniformly stochastic , so there are no events of probability @xmath299 , hence @xmath300 and @xmath301 for sufficiently large @xmath118 .",
    "now for these @xmath118 , @xmath302 together with @xmath303 implies the contradiction @xmath304 .",
    "so @xmath302 necessarily requires @xmath305 , hence @xmath306 , since @xmath33 is uniformly stochastic .",
    "if this happens infinitely often , then @xmath33 is eventually not competitive .",
    "a symmetric argument with @xmath15 holds for @xmath307 .",
    "the last paragraph shows that , if both @xmath33 and @xmath15 stay competitive , eventually @xmath308 holds a.s . in this case , @xmath264 is eventually constant , which completes the proof .",
    "[ cor : mdlstabilizes ]",
    "let @xmath12 be a countable class of factorizable and uniformly stochastic measures , then the maximizing element stabilizes almost surely .",
    "this follows from theorem [ theorem : mdlstabilizesf2c ] and lemma [ lemma : mdlstabilizesfinite ] .",
    "lemma [ lemma : mdlstabilizesfinite ] and corollary [ cor : mdlstabilizes ] are certainly not the only or the strongest assertions obtainable for stabilization .",
    "they rather give a flavor how a proof can look like , even if the distributions are not asymptotically mutually singular .",
    "on the other hand , the given result is optimal at least in some sense , as shown by the previous examples [ example : notstabilizeapprox ] and [ example : notstabilizedependent ] . in the former example , @xmath25 is not uniformly stochastic but both @xmath25 and @xmath33 are factorizable , while in the latter one , @xmath25 is uniformly stochastic but @xmath33 is not factorizable .",
    "the proof of lemma [ lemma : mdlstabilizesfinite ] crucially relies on the independence assumption , which is necessary in order to use the kolmogorov - rogozin inequality .",
    "it is possible to relax this and require independent sampling only  every so often \" .",
    "it is however not clear how to remove this condition completely .",
    "in the following , we present some applications of the theory developed so far .",
    "we begin by stating general loss bounds .",
    "after that , three very general applications are discussed .",
    "so far we have only considered special loss functions , like the square loss , the hellinger loss , or the relative entropy .",
    "we now show how these results , in particular the bounds for the hellinger loss , imply regret bounds for _ arbitrary loss functions_. ( as we will see , square distance is not sufficient . )",
    "this parallels the bounds in @xcite .",
    "the proofs are simplified , in particular lemma [ lemma : instant2cum ] facilitates the analysis considerably .",
    "the reader should compare the results to the bounds for  prediction with expert advice \" , e.g.  @xcite .    in order to keep things simple , we restrict to binary alphabet @xmath80 in this section .",
    "our results extend to general alphabet by the techniques used in @xcite . consider a binary predictor having access to a belief probability @xmath32 depending on the current history , e.g.@xmath309 . which actual prediction",
    "should he output , 0 or 1 ?",
    "we can answer this question if we know the _ loss function _ , according to which losses are assigned to the ( wrong ) predictions .",
    "consider for example the 0/1 loss ( also known as classification error loss ) , i.e.  a wrong prediction gives loss of 1 and a right prediction gives no loss .",
    "then we should predict 1 if our belief is @xmath310 .",
    "this may be different under other loss functions .",
    "in general , we should predict in a _",
    "bayes optimal _ way : we should output the symbol with the least expected loss , x^:= _ x\\{0,1 } \\{(1-)(0,x)+(1,x ) } , where @xmath311 is the loss incurred by prediction @xmath312 if the true symbol is @xmath11 . in the following",
    ", we will restrict to _ bounded _ loss functions @xmath313 $ ] . breaking ties in the above expression in an arbitrary deterministic way ,",
    "the resulting prediction is _ deterministic _ for given @xmath32 and loss function @xmath314 .",
    "if @xmath25 is the true distribution as usual , then let @xmath315 be the @xmath25-expected loss of the @xmath32-predictor .",
    "then , by l^= = _ t=1^n ( x)l_t^(x ) we denote the cumulative @xmath25-expected loss of the @xmath32-predictor . with @xmath32 being the variants of the mdl predictor , we will bound the quantity @xmath316 , i.e.  the cumulative _ regret _ , by an expression depending on @xmath317 and @xmath108 .",
    "we admit arbitrary non - stationary loss functions @xmath318 which may depend on the history .",
    "our analysis considers the worst possible choice of loss functions and consists of three steps .",
    "first the cumulative regret bound is reduced to an instantaneous regret bound ( lemma [ lemma : instant2cum ] ) .",
    "then the instantaneous bound is reduced to a bound in terms of special functions of @xmath25 and @xmath32 ( lemma [ lemma : instant2special ] ) .",
    "finally , the bound for the special functions is given ( lemma [ lemma : special ] ) .",
    "[ lemma : instant2cum ] assume that some @xmath32-predictor satisfies the instantaneous regret bound @xmath319 , where @xmath320 is the hellinger distance of the instantaneous predictive probabilities ( [ eq : hellinger ] ) .",
    "then the cumulative @xmath32-regret is bounded in the same way : = l^-l^2h(,)+2 .",
    "this and the following lemma hold with arbitrary constants , the choices @xmath321 and @xmath322 are the smallest ones for which lemma [ lemma : special ] is true . note that if the hellinger distance is replaced by the relative entropy , then @xmath322 may be replaced by @xmath321 .",
    "thus , normalized dynamic mdl and bayes mixture admit smaller bounds , compare @xcite . however , this is not true for the other mdl variants , as we have no relative entropy bound there .",
    "the key property is the _ super - additivity _ of the bound .",
    "a function @xmath323 is said to be super - additive if f(x_1+x_2,y_1+y_2)f(x_1,y_1)+f(x_2,y_2 ) .",
    "the function @xmath324 satisfies this condition .",
    "we now use an inductive argument .",
    "assume @xmath325 , where the summation starts at @xmath326 and the superscript @xmath327 indicates that the first symbol of the sequence was @xmath327 .",
    "let the same hold for the first symbol @xmath186 .",
    "writing @xmath328 and using @xmath329 , we obtain + & & 2 + & & 2 + & & 2h+2 . here , the first inequality is the induction hypothesis together with the instantaneous bound , the second bound is cauchy - schwarz s inequality , and the last estimate is the super - additivity .",
    "[ lemma : instant2special ] assume that some @xmath32-predictor satisfies @xmath330 for all @xmath331 $ ] , with the hellinger distance @xmath332 and the special functions @xmath333 and @xmath334 defined in the following way , where we slightly abuse notation and abbreviate @xmath335 and @xmath336 : = = \\ {    l @   l & , + ( 1-)/ & , + 1- & , + ( 1-)/(1- ) & .    .",
    "then for arbitrary bounded loss function @xmath337 $ ] , we have [ eq : instantbound ] 2h+2 .",
    "first we show that we may assume @xmath338 , i.e.  we do not incur loss for correct predictions . to this end ,",
    "consider the modified loss function @xmath339 and assume w.l.o.g @xmath340 $ ] .",
    "then it is not hard to see that the regrets under the original and the modified loss functions coincide , while the expected loss of the @xmath25-predictor clearly decreases with the modified loss function .",
    "thus , ( [ eq : instantbound ] ) holds for @xmath314 if it holds for @xmath341 .",
    "hence we may assume @xmath338 .",
    "for each possible outcome @xmath342 , we abbreviate @xmath343",
    ".    now assume w.l.o.g .",
    "@xmath344 . in order to show the assertion , we need to consider the cases in the definition of @xmath345 separately .",
    "we show this only for the first case , i.e.  @xmath346 . then @xmath347 , @xmath348 .",
    "we assume that the @xmath25-predictor outputs 0 and the @xmath32-predictor 1 , otherwise they give the same prediction and the @xmath32-predictor has no regret at all .",
    "this condition is equivalent to @xmath349 for some @xmath350 $ ] .",
    "we consider the worst case by maximizing @xmath351 , i.e.  choosing @xmath352 as large as possible .",
    "for this @xmath353 , we obtain @xmath354 and = ^1[-]=^1 ^ 1[2h+2]2h+22h+2 , showing ( [ eq : instantbound ] ) provided that @xmath346 .",
    "the other cases are shown similarly .",
    "[ lemma : special ] the bound @xmath330 holds for all @xmath331 $ ] , with the functions @xmath355 ^ 1\\to[0,1]$ ] as defined in lemma [ lemma : instant2special ] .    the technical and not very interesting proof of this lemma is omitted .",
    "the careful reader may check the assertion numerically or graphically , as it is just the boundedness of some function on the unit square .",
    "we remark that the bound does _ not _ hold if the hellinger distance is replaced by the quadratic distance , not even with larger constants .    for arbitrary non - stationary loss function which is bounded in",
    "@xmath66 $ ] and known to the mdl predictors , their respective losses are bounded by l^_norm , l^ , l^^ , l^^_norml^_norm+ 2 + 2c w_^-1 , where the constant @xmath356 , according to which mdl predictor is used ( compare corollary [ cor : murhoall ] ) .",
    "this follows from the above three lemmas and from @xmath357 ( corollary  [ cor : murhoall ] ) .",
    "this shows in particular that , regardless of the loss function , the average expected per - round regret tends to zero .",
    "again , the direct practical relevance of the bounds is limited because of the potentially huge @xmath108 .",
    "transferring our results to pattern classification is very easy .",
    "all we have to do is to add _ inputs _ to our models .",
    "that is , we consider an arbitrary input space @xmath358 and ( as before ) a finite observation or output space @xmath1 .",
    "a model is now a _ measure _ ( x|u ) ,  x ,  u , _ x ( x|u)=1 _ all u. that is",
    ", we have a distribution which is conditionalized to the input .",
    "we restrict our discussion to measures , since there is no motivation to consider semimeasures for classification .",
    "the definition of a model does not include history dependence .",
    "there is no loss of generality : we may include the history in the arbitrary input space .",
    "transferring the proofs in the previous sections to the present setup is straightforward .",
    "we therefore obtain immediately the following corollaries .",
    "let @xmath12 be a countable set of classification models containing the true distribution @xmath25 .",
    "then for any sequence of inputs @xmath359 , we have    l @ c @ l _ t _ a ( ( a|u_t)-_norm(a|u_t , u , x ) ) ^2 & & 2 w_^-1 , + _",
    "t _ a ( ( a|u_t)-(a|u_t , u , x ) ) ^2 & & 8 w_^-1 , + _",
    "t _ a ( ( a|u_t)-^(a|u_t , u , x ) ) ^2 & & 21 w_^-1 .",
    "( note that although each single model formally does not depend on the history , the mdl estimators necessarily do . )",
    "we need not consider the normalized static variant here , since all models are measures anyway .",
    "if there is a distribution over @xmath358 , the result therefore also holds in expectation over the inputs .",
    "an analogue of corollary [ cor : mdlstabilizes ] is obtained as easily . if the inputs are i.i.d . , which is usually assumed for classification , then the two conditions of factorizability and uniform stochasticity are trivially satisfied . therefore , the true distribution @xmath25 is eventually discovered by mdl almost surely . note that in this case , the distributions are also asymptotically mutually singular , so that the assertion also follows from barron s @xcite earlier result .",
    "note that again , the assumption @xmath23 is essential . in practical applications , if this is not clear , it may be therefore favorable to choose a different method having guarantees without this condition , compare @xcite",
    ".      we may also apply our results in the regression setup , that is for predicting continuous densities .",
    "our use of the term regression is a bit non - standard here , since it normally refers to just estimating the mean of some prediction , where the distribution is often assumed to be gaussian . again",
    "the assumption @xmath23 is essential , so that in practice some other method not relying on it might be preferred .",
    "continuous densities cause some additional difficulties .",
    "the observation space is now @xmath360 .",
    "this implies in particular that , like for the loss bounds , the square distance is no longer appropriate for our purpose by its density @xmath361}+\\frac{2n}{3}\\chi_{(0,\\frac{1}{n}]}$ ] , where @xmath362 is the characteristic function of an interval .",
    "let @xmath363 , then the quadratic distance is @xmath364 , whereas the relative entropy @xmath365 is constant . ]",
    "( note that our use of the squared error is completely different from the standard use in regression ) .",
    "so we will use the hellinger distance instead , defined similarly to ( [ eq : hellinger ] ) by [ eqhellingercont ] h(f , f ) = ( - ) ^2dx f , f:[0 , ) .",
    "accordingly , @xmath366 is the cumulative hellinger distance of two predictive distributions @xmath25 and @xmath32 .",
    "similarly as in ( [ eqhellingerkl ] ) and ( [ eqhellingerabs ] ) , the hellinger distance is bounded by the ( continuous ) relative entropy and absolute distance .",
    "this shows in particular that the integral ( [ eqhellingercont ] ) exists .",
    "we now consider a countable class @xmath12 of models that are functions @xmath33 from @xmath358 to _ uniformly bounded probability densities _ on @xmath367 .",
    "that is , there is some @xmath368 such that [ eqc ] 0_i(x|u)c _ -^_i(x|u)dx=1 _ all i1 ,  u , x. for all @xmath369 , @xmath370 , and @xmath371 .",
    "the mdl estimator is then defined as the element which maximizes the _ density _ , @xmath372 .",
    "the uniform boundedness condition asserts that the mdl estimator exists .",
    "it may be relaxed , provided that the mdl estimator remains well - defined , such as for a family of gaussian densities which tend to the point measure .    with these definitions ,",
    "the proofs of the theorems for static and dynamic mdl can be adapted .",
    "since the triangle inequality holds for the hellinger distance @xmath373 , we obtain the following .",
    "let @xmath12 be a countable model class according to ( [ eqc ] ) , containing the true distribution @xmath25 .",
    "then for any sequence of inputs @xmath359 , we have @xmath374 , @xmath375 , and @xmath376",
    ".    we may apply this for example to model classes with gaussian noise , concluding that the mean and the variances converge to the true values , see @xcite for an example .",
    "it is not immediately clear how to obtain an analogue of corollary [ cor : mdlstabilizes ] for continuous densities .",
    "since the assertions on static and dynamic mdl have been proven generally for _ semimeasures _ , we may apply them to the universal setup . here",
    "@xmath377 is the countable set of all lower semicomputable (= enumerable ) semimeasures on @xmath2 .",
    "so @xmath378 contains stochastic models in general , and in particular all models for computable deterministic sequences .",
    "there is a one - to - one correspondence of @xmath378 to the class of all programs on some fixed universal monotone turing machine @xmath261 , see e.g. @xcite",
    ". we will assume programs to be _ binary _ , in contrast to outputs , which are strings @xmath19 .",
    "this relation defines in particular the complexities and weights of each @xmath33 by [ eq : canweights ] w ( ) = , w_=2^w ( ) .",
    "we call these weights the _ canonical weights_. they satisfy @xmath21 for all @xmath33 and @xmath379 .",
    "an enumerable semimeasure which dominates all other enumerable semimeasures is called universal .",
    "the bayes mixture @xmath37 defined in ( [ eq : xi ] ) has this property .",
    "one can show that @xmath37 is equal within a multiplicative constant to solomonoff s prior ( * ? ? ?",
    "* eq . ( 7 ) ) , which is the a priori probability that ( some extension of ) a string @xmath11 is generated provided that the input of @xmath261 consists of fair coin flips .",
    "that is ( x)(x)= _ 2 ^ -(p ) _ all x^*. here , we use the notations fg : fg+o(1 ) , & & fg : fggf , + fg : fgo(1 ) , & & fg : fggf .",
    "the mdl definitions in section [ secmdl ] directly transfer to this setup .",
    "all bounds on the cumulative square loss ( subsumed in corollary [ cor : murhoall ] ) therefore apply to @xmath380}$ ] .",
    "the necessary assumption now reads that @xmath25 must be a recursive (= computable ) measure .",
    "also , theorem [ theorem : solomonoff ] implies solomonoff s important universal induction theorem .",
    "in addition to @xmath378 , we also consider the set of all recursive measures @xmath381 together with the same canonical weights ( [ eq : canweights ] ) . we define @xmath382}$ ] and @xmath383}$ ]",
    ". then @xmath384 and @xmath385 for all @xmath19 is immediate .",
    "it is straightforward that @xmath386 since @xmath387 .",
    "moreover , for any string @xmath19 , define the _ monotone complexity _ @xmath388 as the length of the shortest program such that @xmath261 s output starts with @xmath11 .",
    "the following assertion holds .",
    "we have @xmath389 .",
    "we must show that given a string @xmath19 and a recursive measure @xmath33 ( which in particular may be the mdl descriptor @xmath390 ) it is possible to specify a program @xmath391 of length at most @xmath392 that outputs a string starting with @xmath11 , where constant @xmath393 is independent of @xmath11 and @xmath33 .",
    "consider all strings @xmath394 ( @xmath395 ) of length @xmath396 arranged in lexicographical order .",
    "each @xmath397 has measure @xmath398 .",
    "let @xmath399 be the cumulated measures : @xmath400 and @xmath401 .",
    "let @xmath402 be the index of @xmath11 , i.e.  @xmath403 .",
    "then , the interval @xmath404 has measure @xmath405 and therefore contains exactly one @xmath406-bit number @xmath407 .",
    "we describe @xmath11 with the number @xmath408 , this is known as _ arithmetic encoding _ ( see e.g.  @xcite ) .",
    "the coding is injective since @xmath409 and @xmath410 are disjoint for @xmath411 .    in order to decode @xmath408",
    ", we may descend the @xmath412-ary tree of all possible strings @xmath413 , first considering strings of length one , then of length two , etc .",
    "for each possible string @xmath413 , we can determine its binary code by approximating @xmath10 sufficiently accurately .",
    "eventually we will find @xmath408 , then we print the current @xmath413 . at this stage , @xmath413 might be only a prefix of @xmath11 , since an extension of @xmath413 might have a measure very close to @xmath413 and thus map to the same code @xmath408 .",
    "therefore we continue the procedure until all codes starting with @xmath408 are proper extensions of @xmath408 ( which may never be the case , then the algorithm runs forever ) . in each step , the appropriate additional symbol is written on the output tape .",
    "the resulting output will be @xmath11 or some extension of @xmath11 .",
    "this algorithm can be specified in a constant @xmath414 number of bits .",
    "the description of @xmath33 needs another @xmath415 bits .",
    "finally , @xmath408 has length @xmath416 .",
    "thus , the overall description has length @xmath392 as required .",
    "it is also possible to prove the proposition indirectly using ( * ? ? ?",
    "* thm.4.5.4 ) .",
    "this implies that @xmath417 for all @xmath19 and all recursive measures @xmath418 .",
    "then , also @xmath419 holds .",
    "so together with the above observations , we have [ eq : complexityrelations ] ( x )  ( x )  ( x )  ( x )   ( x ) . on the other hand , there is a deep result in algorithmic information theory which states that an exact coding theorem does _ not _ hold on continuous sample space , @xmath420 @xcite .",
    "therefore , at least one of the above @xmath421 must be proper .",
    "[ problemgacs ] which of the two inequalities @xmath422 and @xmath423 is proper ( or are both ) ?",
    "the proof in @xcite is very subtle , and the phenomenon is still not completely understood .",
    "there is some hope that by answering problem [ problemgacs ] , one arrives at a better understanding of the continuous coding theorem and even at a simpler proof for its failure .",
    "in this last section , we recapitulate the main achievements of this work and discuss their philosophical and practical consequences . in the first place , we have shown that if two - part mdl is used for predicting a stochastic sequence , then the predictive probabilities converge to the true ones in mean sum , provided that the distribution generating the sequence is contained in the model class .",
    "the two most important implications are almost sure convergence and loss bounds for arbitrary loss functions .",
    "the guaranteed convergence is slow in general : all bounds depend linearly on @xmath108 , the inverse of the prior weight of the true distribution . for large model classes ,",
    "this number must be regarded too huge to be relevant for practical applications .",
    "examples show that this bound is sharp .",
    "this is in contrast to the exponentially smaller corresponding bound for the bayes mixture .",
    "the latter predictor however is often computationally more expensive to approximate in practice .",
    "we believe that this principally indicates that with mdl , some care has to be taken when choosing the model class and the prior .",
    "conditions which are sufficient for fast convergence have been given for instance in @xcite .",
    "it remains a major challenge to generalize these results in order to obtain fast convergence under assumptions that are as weak as possible .",
    "in particular for universal induction , this question is interesting and possibly difficult .",
    "even when considering only computable bernoulli distributions endowed with a universal prior , fast convergence possibly holds for many environments , but maybe not for all @xcite .",
    "we also need to distinguish how the large error cumulates .",
    "either the instantaneous error remains significant for a long time , which is critical , or the instantaneous error drops just too slowly to be summable , e.g.  as @xmath424 , which is tolerable .",
    "we have seen instances for both cases ; compare the discussion after example [ ex : bernoulli ] . in this light , the cumulative error might not be the right quantity to assess convergence speed .",
    "the main results have been shown under the only assumption that the data generating process is contained in the model class .",
    "this condition is essential in general , as @xcite shows that in its absence mdl can fail dramatically . in the universal setup",
    ", the assumption merely requires that the data is generated in some ( probabilistically ) computable way .",
    "this is a very weak condition .",
    "laplace , zuse @xcite and successors argue that nature operates in a computable way , and consequently _ all _ thinkable data satisfies the assumption . on the other hand ,",
    "predicting with a universal model is computationally very expensive .",
    "in particular it is provably infeasible if the thesis of computable nature holds . despite these practical problems ,",
    "the theory of universal prediction is valuable since it explores the limits of computational induction .",
    "m.  hutter .",
    "sequence prediction based on monotone complexity . in _ proc .",
    "16th annual conference on learning theory ( colt-2003 ) _ , lecture notes in artificial intelligence , pages 506521 , berlin , 2003 . springer ."
  ],
  "abstract_text": [
    "<S> minimum description length ( mdl ) is an important principle for induction and prediction , with strong relations to optimal bayesian learning . </S>",
    "<S> this paper deals with learning non-i.i.d.processes by means of two - part mdl , where the underlying model class is countable . </S>",
    "<S> we consider the online learning framework , i.e.  observations come in one by one , and the predictor is allowed to update his state of mind after each time step . </S>",
    "<S> we identify two ways of predicting by mdl for this setup , namely a _ </S>",
    "<S> static _ and a _ dynamic _ one . </S>",
    "<S> ( a third variant , hybrid mdl , will turn out inferior . ) </S>",
    "<S> we will prove that under the only assumption that the data is generated by a distribution contained in the model class , the mdl predictions converge to the true values almost surely . </S>",
    "<S> this is accomplished by proving finite bounds on the quadratic , the hellinger , and the kullback - leibler loss of the mdl learner , which are however exponentially worse than for bayesian prediction . </S>",
    "<S> we demonstrate that these bounds are sharp , even for model classes containing only bernoulli distributions . </S>",
    "<S> we show how these bounds imply regret bounds for arbitrary loss functions . </S>",
    "<S> our results apply to a wide range of setups , namely sequence prediction , pattern classification , regression , and universal induction in the sense of algorithmic information theory among others .    minimum description length , sequence prediction , consistency , discrete model class , universal induction , stabilization , algorithmic information theory , loss bounds , classification , regression . </S>"
  ]
}