{
  "article_text": [
    "( 0,0 ) .,title=\"fig : \" ]    # 1#2#3#4#5 @font    ( 3676,2368)(0,-1483 )    in this work we are concerned with two topics whose name contains the word `` smooth '' , but in totally different meaning .",
    "the first is _ balancing ( smoothing ) networks _ , the second is _ smoothed analysis_. let us start by introducing these two topics , and then introduce our contribution  interrelating the two .      in the standard abstraction of _ smoothing _ ( balancing ) networks  @xcite , processors are modeled as the vertices of a graph and connection between them as edges .",
    "each process has an initial collection of unit - size jobs ( which we call tokens ) .",
    "tokens are routed through the network by transmitting tokens along the edges according to some local rule .",
    "we measure the quality of such a balancing procedure by the maximum difference between the number of tokens at any two vertices at the end .",
    "the local scheme of communication we study is a _ balancer _",
    "gate : the number of tokens is split as evenly as possible between the communicating vertices with the excess token ( if such remains ) routed to the vertex towards which the balancer points .",
    "more formally , the balancing network consists of @xmath5 vertices @xmath6 , and @xmath7 matchings ( either perfect or not ) @xmath8 .",
    "we associate with every matching edge a balancer gate ( that is , we think of the edges as directed edges ) . at the beginning of the first iteration ,",
    "@xmath9 tokens are placed in vertex @xmath10 , and at every iteration @xmath11 , the vertices of the network perform a balancing operation according to the matching @xmath12 ( that is , vertices @xmath13 and @xmath10 interact if @xmath14 ) .",
    "one motivation for considering smoothing networks comes from the server - client world .",
    "each token represents a client request for some service ; the service is provided by the servers residing at the vertices .",
    "routing tokens through the network must ensure that all servers receive approximately the same number of tokens , no matter how unbalanced the initial number of tokens is ( cf .",
    "@xcite ) . more generally ,",
    "smoothing networks are attractive for multiprocessor coordination and load balancing applications where low - contention is a requirement ; these include _ producers - consumers _ @xcite and distributed numerical computations @xcite . together with _ counting networks _ , smoothing networks have been studied quite extensively since introduced in the seminal paper of @xcite .",
    "@xcite initiated the study of the _ @xmath15  network _",
    "( cube - connected - cycles , see figure  [ fig : ccc16 ] ) as a smoothing network - network and therefore we will stick to the latter in the following . ] .",
    "for the special case of the @xmath16 , sticking to previous conventions , we adopt a `` topographical '' view of the network , thus calling the vertices _ wires _ , and looking at the left - most side of the network as the `` input '' and the right - most as the `` output '' . in the @xmath16 , two wires at layer @xmath17 are connected by a balancer if the respective bit strings of the wires differ exactly in bit @xmath17 .",
    "the @xmath15  is a canonical network in the sense that it has the smallest possible depth ( number of rounds ) of @xmath18 as a smaller depth can not ensure any discrepancy independent of the initial one .",
    "moreover , it has been used in more advanced constructions such as the _ periodic ( counting ) network _  @xcite .    as it turns out , the initial setting of the balancers directions is crucial .",
    "two popular options are an arbitrary orientation or a uniformly random one . a maximal discrepancy of @xmath18 was established for the @xmath19 for an arbitrary initial orientation  @xcite . for a random initial orientation of the @xmath19 , @xcite",
    "show a discrepancy of at most @xmath20 for the @xmath19 ( this holds _ whp _ we mean with probability tending to @xmath21 as @xmath5 goes to infinity . ] over the random initialization ) , which was improved by @xcite to @xmath22 ( and a matching lower bound ) .",
    "results for more general networks have been derived in @xcite for arbitrary orientations . for expander graphs ,",
    "they show an @xmath1-discrepancy after @xmath23 rounds when @xmath24 is the discrepancy of the initial load vector .",
    "this was recently strengthened assuming the orientations are set randomly and in addition the matchings themselves are chosen randomly @xcite .",
    "specifically , for expander graphs one can achieve within the same number of rounds a discrepancy of only @xmath25 .",
    "let us now turn to the second meaning of `` smoothed '' . smoothed analysis comes to bridge between the random instance , which typically has a very specific `` unrealistic '' structure , and the completely arbitrary instance , which in many cases reflects just the worst case scenario , and is thus over - pessimistic in general . in the smoothed analysis paradigm ,",
    "first an adversary generates an input instance , then this instance is randomly perturbed .",
    "the smoothed analysis paradigm was introduced by @xcite in 2001 @xcite to help explain why the simplex algorithm for linear programming works well in practice but not in ( worst - case ) theory .",
    "they considered instances formed by taking an arbitrary constraint matrix and perturbing it by adding independent gaussian noise with variance @xmath26 to each entry .",
    "they showed that , in this case , the shadow - vertex pivot rule succeeds in expected polynomial time .",
    "independently , @xcite studied the issue of hamiltonicity in a dense graph when random edges are added .",
    "in the context of graph optimization problems we can also mention @xcite , in the context of @xmath27-sat @xcite , and in various other problems @xcite .",
    "our work joins this long series of papers studying perturbed instances in a variety of problems . specifically in our setting we study the following question : what if the balancers were not set completely adversarially but also not in a completely random fashion .",
    "besides the mathematical and analytical challenge that such a problem poses , in real network applications one may not always assume that the random source is unbiased , or in some cases one will not be able to quantitatively measure the amount of randomness involved in the network generation .",
    "still it is desirable to have an estimate of the typical behavior of the network .",
    "although we do not claim that our smoothed - analysis model captures all possible behaviors , it does give a rigorous and tight characterization of the tradeoff between the quality of load balancing and the randomness involved in setting the balancers directions , under rather natural probabilistic assumptions .",
    "as far as we know , no smoothed analysis framework was suggested to a networking related problem .",
    "formally , we suggest the following framework .",
    "we define both the smoothed - analysis aspect of the model , and the load - balancing one . for the load balancing part ,",
    "our model is similar to ( and , as we will shortly explain , a generalization of ) the periodic balancing circuits studied in @xcite .",
    "we think of the balancing network in terms of an @xmath5-vertex graph . the processors in the network are the vertices of the graph , and balancers connecting processors are the ( directed ) edges of the graph .    before we proceed , since what follows is somewhat heavy on notation and indices , it will be helpful for the reader to bear in mind the following legend : we use superscripts ( in round brackets ) to denote a time stamp , and subscripts to denote an index .",
    "in subscripts , we use the vertices of the graph as indices ( thus assuming some ordering of the vertex set ) .",
    "for example , @xmath28 stands for the @xmath29-entry in matrix @xmath30 , which corresponds to time / round @xmath31 .",
    "let @xmath32 be an arbitrary sequence of @xmath33 ( not necessarily perfect ) matchings .",
    "with each matching @xmath34 we associate a matrix @xmath35 with @xmath36 if @xmath37 and @xmath38 are matched in @xmath34 , @xmath39 if @xmath37 is matched in @xmath34 , @xmath40 if @xmath37 is not matched in @xmath34 , and @xmath41 otherwise .    in round @xmath31 ,",
    "every two vertices matched in @xmath34 perform a balancing operation .",
    "that is , the sum of the number of tokens in both vertices is split evenly between the two , with the remaining token ( if exists ) directed to the vertex pointed by the matching edge .    in * periodic balancing networks * ( see @xcite for example )",
    "an ordered set of @xmath42 ( usually perfect ) matchings is fixed .",
    "every round of balancing is a successive application of the @xmath42 matchings .",
    "our model is a ( slight ) generalization of the latter .",
    "let us now turn to the smoothed - analysis part . given a balancing network consisting of a set @xmath33 of directed matchings ,",
    "an * @xmath0-perturbation * of the network is a flip of direction for every edge with probability @xmath0 independently of all other edges .",
    "setting @xmath3 gives the completely `` adversarial model '' , and @xmath4 is the uniform random case .",
    "for our results , it suffices to consider @xmath43 $ ] .",
    "the case @xmath44 can be reduced to the case @xmath45 by flipping the initial orientation of all balancers and taking @xmath46 instead of @xmath0 .",
    "it is easy to see that both distributions are identical .      for a load vector @xmath47",
    ", its discrepancy is defined to be @xmath48 .",
    "we use @xmath49 to denote the unit vector whose all entries are 0 except the @xmath50 . for a matrix @xmath51 , @xmath52 stands for the second largest eigenvalue of @xmath51 ( in absolute value ) . unless stated otherwise , @xmath53 stands for the @xmath54-norm of the vector @xmath55 . in the following",
    ", we will assume an ordering of the vertices from @xmath21 to @xmath5 .",
    "when we write @xmath56 , we refer to the case where @xmath37 and @xmath38 are connected by an undirected edge and @xmath57 .",
    "[ thm : upper ] let @xmath58 be a balancing network with matchings @xmath32 .",
    "for any two time stamps @xmath59 satisfying @xmath60 , and any input vector with initial discrepancy @xmath24 , the discrepancy at time step @xmath61 in @xmath0-perturbed  @xmath58 is _",
    "whp_at most @xmath62 where @xmath63\\in { \\ensuremath{m^{(i ) } } } }             \\left((e_u - e_v)^{t } \\left({\\textstyle\\prod}_{j = i+1}^{t_2 }             { \\ensuremath{\\mathbf{p}}}^{(i)}\\right ) e_w \\right)^2 } ,     \\\\",
    "\\lambda_2 & =   \\lambda \\left({\\textstyle\\prod}_{i=1}^{t_2 } { \\ensuremath{\\mathbf{p}}}^{(i)}\\right ) { \\hksqrt}{n } k.     \\end{aligned}\\ ] ]    before we proceed let us motivate the result stated in theorem  [ thm : upper ] .",
    "there are two factors that affect the discrepancy : the fact that tokens are indivisible ( and therefore the balancing operation may not be `` perfect '' ) , and how many balancing rounds are there . on the one hand , the more rounds there are the more balancing operations are carried , and the smoother the output is . on the other hand , the longer the process runs , its susceptibility to rounding errors and arbitrary placement of excess tokens increases .",
    "this is however only a seemingly tension , as indeed the more rounds there are , the smoother the output is .",
    "nevertheless , in the analysis ( at least as we carry it ) , this tension plays part .",
    "specifically , optimizing over these two contesting tendencies is reflected in the choice of @xmath64 and @xmath61 .",
    "@xmath65 is the contribution resulting from the number of balancing rounds being bounded , and @xmath66 , along with the first two terms , account for the indivisibly of the tokens . in the cases that will interest us ,",
    "@xmath59 will be chosen so that @xmath67 will be low - order terms compared to the first two terms .",
    "our theorem  [ thm : upper ] also implies the following results :    * for the aforementioned periodic setting theorem [ thm : upper ] implies the following : after @xmath68 rounds ( @xmath69 , @xmath70 is the so - called round matrix which corresponds to one period , @xmath24 the initial discrepancy ) the discrepancy is @xmath71 at most @xmath72 setting @xmath3 ( and assuming @xmath24 is polynomial in @xmath5 ) we get the result of @xcite smaller . ] , and for @xmath4 we get the result of @xcite .",
    "( the restriction on @xmath24 being polynomial can be lifted but at the price of more cumbersome expressions in theorem [ thm : upper ] . arguably , the interesting cases are anyway when the total number of tokens , and in particular  @xmath24 , is polynomial ) .",
    "complete details in section  [ sec : derivationofperiodiccase ] .",
    "* for the @xmath19 , after @xmath18 rounds the discrepancy is _",
    "whp_at most @xmath73 full details in section  [ sec : derivationofccc ] .",
    "let us now turn to the lower bound .",
    "here we consider the all - up - orientation of the balancers of a @xmath19 meaning that before the @xmath0-perturbation , all balancers are directed to the wire with a smaller number .",
    "[ thm : lower ] consider a @xmath19 with the all - up orientation of the balancers and assume that the number of tokens at each wire is uniformly distributed over @xmath74 ( independently at each wire ) .",
    "the discrepancy of the @xmath0-perturbed network is @xmath71 at least @xmath75    two more points to note regarding the lower bound :    * for @xmath3 our lower bound matches the experimental findings of @xcite .",
    "the authors examined @xmath15 s of size up to @xmath76 where all balancers are set in the same direction and the number of tokens at each input is a random number between @xmath21 and @xmath77 .",
    "their observation was that the average discrepancy is close to @xmath78 ( which matches our lower bound with @xmath3 ) . *",
    "the input distribution that we use for the lower bound is arguably more natural than the tailored and somewhat artificial ones used in previous lower bound proofs @xcite .    finally , we state a somewhat more technical result that we obtain , which lies in the heart of the proof of the lower bound and sheds light on the mechanics of the @xmath16 in the average case input . in",
    "what follows , for a balancer @xmath79 , we let @xmath80 be an indicator function which is 1 if @xmath79 had an excess token . by @xmath81",
    "we denote the set of balancers that affect output wire @xmath31 ( that is , there is a path through consecutive layers from the balancers to the output wire @xmath31 ) .",
    "@xmath82 is the restriction of @xmath81 to balancers in layer @xmath17 .",
    "[ lem : balancersoddwithprobhalf ] consider a @xmath19 network with any fixed orientation of the balancers .",
    "assume that the number of tokens at each wire is uniformly distributed over @xmath74 ( independently at each wire ) .",
    "then every balancer @xmath79 in layer @xmath17 , @xmath83 , satisfies the following properties :    * @xmath84}}=1/2 $ ] , * moreover , for every wire @xmath31 , @xmath85 is a set of independent random variables .",
    "the proof of the lemma is given in section  [ sec : proofofbalancersprop ] .",
    "let us remark at this point that the lemma holding under such weak conditions is rather surprising .",
    "first , it is valid regardless of the given orientation .",
    "secondly , the @xmath86 s of the balancers that affect the _ same _ output wire are independent . while this is obvious for balancers that are in the same layer , it seems somewhat surprising for balancers in subsequent layers that are connected .",
    "the remainder of the paper is organized as follows .",
    "we set out with the proof of theorem  [ thm : lower ]  in section [ sec : lower ] , preceding the proof of theorem [ thm : upper ] in section [ sec : upper ] .",
    "the reason is that the lower bound is concerned with the @xmath19 ( a special case of our general model ) .",
    "the techniques used in the proof of the lower bound serve as a good introduction to the more complicated proof of the upper bound . in sections [ sec : derivationofperiodiccase ] and [ sec : derivationofccc ] we show how to derive the special cases of the periodic balancing network and the @xmath19 from theorem [ thm : upper ] . finally we present experimental results that we obtain for the @xmath19 in section [ sec : conclusions ] .",
    "as we mentioned before , for the special case of the @xmath19 we adopt a `` topographical '' view of the network : calling the vertices _ wires _ , the time steps _ layers _ , the left - most side of the network the `` input '' and the right - most the `` output '' .",
    "the proof outline is the following . given an input vector @xmath47 ( uniformly distributed over the range @xmath87 )",
    ", we shall calculate the expected divergence from the average load @xmath88 .",
    "the expectation is taken over both the smoothing operation and the input .",
    "after establishing the `` right '' order of divergence ( in expectation ) we shall prove a concentration result .",
    "one of the main keys to estimating the expectation is lemma  [ lem : balancersoddwithprobhalf ] saying that if the input is uniformly distributed as above , then for every balancer @xmath79 , @xmath84}}=1/2 $ ] ( the probability is taken only over the input ) .    before proceeding with the proof ,",
    "let us introduce some further notation .",
    "let @xmath89 be the number of tokens exiting on the top output wire of the network .",
    "for any balancer @xmath79 , @xmath90 is an indicator random variable which takes the value @xmath91 if the balancer @xmath79 was perturbed , and @xmath92 otherwise ( recall that all balancers are pointing up before the perturbation takes place ) .",
    "using the `` standard '' backward ( recursive ) unfolding ( see also @xcite for a concrete derivation for the @xmath19 ) we obtain that , @xmath93 the latter already implies that the discrepancy of the entire network is at least @xmath94 because there is at least one wire whose output has at most @xmath95 tokens ( a further improvement of a factor of @xmath96 will be obtained by considering additionally the bottom output wire and proving that on this wire only a small number of tokens exit ) .",
    "write @xmath97 , defining for each layer @xmath83 , @xmath98      we now turn to bounding the expected value of @xmath100 . using the following facts : ( a ) the @xmath80 and @xmath90 are independent ( b ) lemma  [ lem : balancersoddwithprobhalf ] which gives @xmath101}}=1/2 $ ] , ( c ) the simple fact that @xmath102}}=\\tfrac{1}{2}-\\alpha$ ] and ( d ) the fact that in layer @xmath17 there are @xmath103 balancers which affect output wire 1 ( this is simply by the structure of the @xmath19 ) , we get @xmath104 } }      & = 2^{-\\log n + \\ell}\\sum_{{\\mathsf{b}}\\in { \\ensuremath{\\mathcal{b}}}_1(\\ell ) } \\tfrac{1}{2 } \\cdot ( \\tfrac{1}{2 } - \\alpha)\\\\      & = 2^{-\\log n + \\ell}\\cdot 2^{\\log n - \\ell } \\cdot \\tfrac{1}{2 } \\cdot \\left(\\tfrac{1}{2}-\\alpha\\right)\\\\      & =   \\tfrac{1}{2 } \\ , \\left(\\tfrac{1}{2}-\\alpha\\right).\\end{aligned}\\ ] ] this in turn",
    "gives that @xmath105 } }      = { \\ensuremath{\\operatorname{\\mathbf{e}}\\left[\\sum_{\\ell=1}^{\\log n } s_{\\ell}\\right ] } }      = \\tfrac{1}{2 } \\left(\\tfrac{1}{2}-\\alpha\\right)\\log n.\\ ] ] our next goal is to claim that typically the discrepancy behaves like the expectation ; in other words , a concentration result .",
    "specifically , we apply hoeffdings bound to each layer @xmath100 separately . it is applicable as the random variables @xmath106 are independent for balancers within the same layer ( such balancers concern disjoint sets of input wires , and the input to the network was chosen independently for each wire ) . for the bound to be useful",
    "we need the range of values for the random variables to be small .",
    "thus , in the probabilistic argument , we shall be concerned only with the first @xmath107 layers ( the last @xmath108 layers we shall bound deterministically ) .",
    "we use the following hoeffding bound :    [ lem : hoeffding ] let @xmath109 be a sequence of independent random variables with @xmath110 $ ] for each @xmath31 . then for any number @xmath111 , @xmath112 } } \\right| { \\geqslant}\\varepsilon\\right ] } }          { \\leqslant}2 \\cdot \\exp \\left(-\\frac{2 \\varepsilon^2}{\\sum_{i=1}^n ( b_i - a_i)^2 } \\right).\\ ] ]    for any random variable @xmath113 , let @xmath114}}$ ] be the difference between the maximum and minimum value that @xmath113 can attain . for a balancer @xmath115 we plug in , @xmath116}}^2&=\\left(2^{\\ell-\\log n}\\right)^2,\\end{aligned}\\ ] ] and",
    "the sum is over @xmath117 balancers in layer @xmath17 .",
    "therefore , @xmath118 } } \\right|      { \\geqslant}2^{(\\ell-\\log n+\\log \\log n)/2 } \\right]}}\\\\      & \\qquad{\\leqslant}2 \\exp \\left ( -\\frac { 2 \\cdot 2^{\\ell - \\log n + \\log \\log n } } { 2^{\\ell- \\log n }   } \\right)\\\\      & \\qquad{\\leqslant}n^{-1}.\\end{aligned}\\ ] ] in turn , with probability at least @xmath119 ( take the union bound over at most @xmath18 @xmath100 terms ) : @xmath120 the second term is just a geometric series with quotient @xmath121 , and therefore can be bounded by @xmath122 .    for the last @xmath123 layers",
    ", we have that for every @xmath17 , @xmath124 can not exceed @xmath125 , and therefore their contribution , in absolute value is at most @xmath126 .",
    "wrapping it up , _ whp_@xmath127 the same calculation implies that the number of tokens at the _",
    "bottom_-most output wire deviates from @xmath95 in the same way ( just in the opposite direction ) , so @xmath128 hence , the discrepancy is _",
    "whp_lower bounded by ( using the union bound over the top and bottom wire , and not claiming independence ) @xmath129      the proof here goes along similar lines to section  [ sec : proofofpro : logn_lowerbound ] , only that now we choose the set of balancers we apply it to more carefully . by the structure of the @xmath19 , the last @xmath131 layers form the parallel cascade of @xmath132 independent @xmath15",
    "subnetworks each of which has @xmath133 wires ( by independent we mean that the sets of balancers are disjoint ) .",
    "we call a subnetwork _ good _ if after an @xmath0-perturbation of the all - up initial orientation , all the balancers affecting the top ( or bottom ) output wire were not flipped ( that is , still point up ) .",
    "the first observation that we make is that @xmath71 ( for a suitable choice of @xmath131 , to be determined shortly ) at least one subnetwork is good . let us prove this fact .",
    "the number of balancers affecting the top ( or bottom ) wire in one of the subnetworks is @xmath134 in total , there are no more than @xmath135 affecting both wires .",
    "the probability that none of these balancers was flipped is ( using our assumption @xmath45 ) @xmath136 choosing @xmath137 , this probability is at least @xmath138 ; there are at least @xmath139 such subnetworks , thus the probability that none is good is at most @xmath140    fix one good subnetwork and let @xmath141 be the average load at the input to that subnetwork . repeating the arguments from section  [ sec : proofofpro : logn_lowerbound ] ( with @xmath3 , @xmath18 re - scaled to @xmath142 , and now using the second item in lemma  [ lem : balancersoddwithprobhalf ] which guarantees that the probability of @xmath143 is still @xmath92 , for any orientation of the balancers ) gives that in the top output wire of the subnetwork there are @xmath71 at least @xmath144 tokens , while on the bottom output wire there are @xmath71 at most @xmath145 tokens . using the union bound , the discrepancy is @xmath71 at least their difference , that is , at least @xmath146 .",
    "the following observation is the key idea in proving lemma  [ lem : balancersoddwithprobhalf ] .",
    "recall that @xmath147 stands for the set of balancers that affect wire @xmath148 . for a balancer @xmath79 in layer @xmath17 let @xmath149 describe an assignment of @xmath150 values for all balancers @xmath151 in preceding layers that affect @xmath79 ( that is , there is a path from @xmath151 to @xmath79 through consecutive layers ) .",
    "[ lem : odd ] consider a @xmath19 network with any orientation of the balancers .",
    "assume that the number of tokens at each wire is uniformly distributed over @xmath74 .",
    "consider a balancer @xmath79 in layer @xmath17 with @xmath83 and let @xmath152 denote the two input wires that go into @xmath79 .",
    "then for any assignment @xmath149 , @xmath153 is uniformly distributed over @xmath154 , for @xmath155 .",
    "the lemma easily implies lemma  [ lem : balancersoddwithprobhalf ] : consider a balancer @xmath79 in layer @xmath17 , with @xmath83 with two inputs @xmath152 .",
    "lemma  [ lem : odd ] implies that @xmath156 is uniformly distributed over @xmath157 , and in particular is uniform over that range mod  2 .",
    "hence both @xmath152 are odd / even with probability 1/2 .",
    "further observe that by the structure of the @xmath15  network , the input wires @xmath152 depend on disjoint sets of wires and balancers .",
    "by the two latter facts it follows that the sum @xmath158 is odd ( or even ) with probability @xmath92 , or in other words @xmath84}}=1/2 $ ] .",
    "the independence part follows from the fact that this is true for every conditioning @xmath149 on balancers from previous layers that affect @xmath79 ( by lemma  [ lem : odd ] ) and the fact that balancers in the same layer that affect the same output wire are independent by construction , as those balancers depend on disjoint sets of balancers and disjoint sets of input wires .",
    "we prove the lemma by induction on @xmath17 , the layer of the balancer .",
    "the base case is immediate : the input to a balancer in layer @xmath21 is just the original input , which is by definition distributed uniformly over @xmath159 which is simply @xmath74 .",
    "assume the lemma is true for all balancers in layer @xmath17 and consider a balancer in layer @xmath160 .",
    "let @xmath152 be its two input wires . by the structure of the @xmath15 , the value on each wire",
    "is determined by a disjoint set of balancers and preceding wires , therefore we can treat , w.l.o.g . ,",
    "only @xmath161 .",
    "let @xmath162 be the part of @xmath149 that affects @xmath161 .",
    "( @xmath162 would be the set of balancers so that there is a path from them to @xmath161 .",
    "since the initial load on the input wires is chosen independently at every wire , and the sets of balancers affecting @xmath161 and @xmath163 are disjoint , indeed only @xmath164 affects @xmath161 , and the same applies for @xmath165 and @xmath163 ) .",
    "thus our goal is to calculate @xmath166}}.\\ ] ] let @xmath151 be the balancer in layer @xmath17 whose one outlet is @xmath161 , and let @xmath167 be its two inputs . recall that @xmath168 , and a possible + 1 addition in case this sum is odd and the balancer points in the direction of @xmath161 .",
    "furthermore it is easy to verify that for every @xmath169 ( assume @xmath169 is even , if odd write @xmath170 instead ) @xmath171 therefore for the event @xmath172 to occur , either    * @xmath173 , or * @xmath174 ( assuming w.l.o.g .  that the balancer points towards @xmath161 , otherwise the sum equals @xmath175 ) .",
    "let us consider the first case .",
    "@xmath176 } }",
    "\\\\      & = { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[\\bigvee_{i=0}^{n/2^{\\ell-1}-1 } x_1 ' \\equiv i \\mod \\left(n/2^{\\ell-1}\\right )        \\wedge   x_2 ' \\equiv 2k - i \\mod \\left(n/2^{\\ell-1}\\right ) \\mid a_1(b)\\right]}}.\\end{aligned}\\ ] ] now observe that the values of @xmath177 and @xmath178 are determined independently , as again , by the structure of the @xmath15 , they involve disjoint sets of balancers and input wires . similarly to @xmath164 we can define @xmath179 and @xmath180 which correspond to the parts of @xmath51 affecting @xmath181 and @xmath182 . by the structure of the @xmath16 , @xmath179 and @xmath180",
    "depend on a disjoint set of input wires ( and balancers ) . as the input is chosen independently for every wire",
    ", @xmath180 does not affect @xmath181 and similarly @xmath179 does not affect @xmath182 .",
    "thus the latter reduces to @xmath183}}\\\\      & \\phantom{\\sum_{i=0}^{n/2^{\\ell-1}-1 } } \\cdot { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[x_2'\\equiv 2k - i \\mod \\left(n/2^{\\ell-1}\\right ) \\mid a_2'(b)\\right]}}.\\end{aligned}\\ ] ] by the induction hypothesis ( applied to the @xmath184 at layer @xmath185 ) , for every @xmath31 , each of @xmath186}}$ ] and @xmath187}}$ ] is uniformly distributed over the range @xmath188 , and therefore in particular the entire expression does not depend on @xmath27 , or , put differently is the same for every choice of @xmath27 .",
    "the same argument holds for the case @xmath189 .",
    "this completes the proof .",
    "we shall derive our bound by measuring the difference between the number of tokens at any vertex and the average load ( as we did in the proof of the lower bound for the @xmath19 ) .",
    "specifically we shall bound @xmath190 , @xmath191 being the number of tokens at vertex @xmath31 at time @xmath192 ( we use @xmath193 for the vector of loads at time @xmath192 ) .",
    "there are two contributions to the divergence from @xmath95 ( which we analyze separately ) :    * the divergence of the idealized process from @xmath95 due to its finiteness . * the divergence of the actual process from the idealized process due to indivisibility .",
    "the idea to compare the actual process to an idealized one was suggested in @xcite and was combined with convergence results of markov chains .",
    "though we were inspired by the basic setup from @xcite and the probabilistic analysis from @xcite , our setting differs in a crucial point : when dealing with the case @xmath194 , we get a delicate mixture of the deterministic and the random model .",
    "for example , the random variables in our analysis are not symmetric anymore which leads to additional technicalities ( cf .",
    "lemma  [ lem : independent ] ) .",
    "formally , let @xmath195 be the load vector of the idealized process at time @xmath192 , then by the triangle inequality ( @xmath196 is the all - one vector ) @xmath197    [ pro : triangleineq ] let @xmath58 be a balancing network with matchings @xmath32 .",
    "then ,    * @xmath198 , * @xmath71 over the @xmath0-perturbation operation , + @xmath199    theorem  [ thm : upper ] then follows .",
    "the proof of the first part of the proposition consists of standard spectral arguments and is given in section  [ sec : spectral ] for completeness .",
    "the proof the second part is more involved and is given in section  [ sec : triangleineqtwo ] .",
    "letting @xmath201 be the initial load vector , it is easily seen that @xmath202 where @xmath35 is the matrix corresponding to matching @xmath203 ( as defined in section  [ sec : themodel ] ) . for simplicity",
    "let us abbreviate @xmath204}:= { \\ensuremath{\\mathbf{p}}}^{(i)}\\,{\\ensuremath{\\mathbf{p}}}^{(i+1)}\\,{\\ensuremath{\\mathbf{p}}}^{(i+2)}\\cdots { \\ensuremath{\\mathbf{p}}}^{(j)}.\\ ] ] since @xmath205}$ ] is real valued and symmetric ( as each of the @xmath35 s is ) , it has @xmath5 real - valued eigenvalues @xmath206 whose corresponding eigenvectors @xmath207 form an orthogonal basis of @xmath208 .",
    "next we observe that @xmath209}-\\mu\\,{\\textbf{1}}\\ , { \\ensuremath{\\mathbf{p}}}^{[1,t_2 ] }      =      ( \\xi^{(0)}-\\mu{\\textbf{1}})\\,{\\ensuremath{\\mathbf{p}}}^{[1,t_2]},\\ ] ] since @xmath196 is an eigenvector of @xmath205}$ ] corresponding to @xmath210 .",
    "furthermore @xmath211 is just the total ( initial ) number of tokens , and therefore by definition we get @xmath212 .",
    "finally , let us project @xmath213 onto @xmath207 , that is , write @xmath214 ( @xmath215 as we said ) . for our goal to bound @xmath200 , it suffices to bound @xmath216 ( recall that @xmath217 refers to the @xmath54-norm ) as for every vector @xmath55 , @xmath218 by the above , @xmath219 } \\big\\|\\\\      & = \\bigg\\| \\sum_{i=2}^n c_i v_i \\cdot { \\ensuremath{\\mathbf{p}}}^{[1,t_2 ] } \\bigg\\|\\\\      & = \\bigg\\| \\sum_{i=2}^n c_i \\lambda_i v_i \\bigg\\|.\\end{aligned}\\ ] ] recall that @xmath220})$ ] denotes the second largest eigenvalue of @xmath205}$ ] in absolute value . by the definition of the @xmath54-norm , and using the fact that the @xmath221 form an orthogonal basis , the latter equals @xmath222 } ) \\cdot \\left(\\sum_{i=2}^n c_i^2 \\|v_i\\|^2 \\right)^{1/2}\\\\      = & \\lambda({\\ensuremath{\\mathbf{p}}}^{[1,t_2 ] } ) \\cdot \\|\\xi^{(0)}-\\mu{\\textbf{1}}\\|\\\\      { \\leqslant } &   \\lambda({\\ensuremath{\\mathbf{p}}}^{[1,t_2]})\\ , k { \\hksqrt}{n}.\\end{aligned}\\ ] ]      the proof of this part resembles in nature the proof of theorem  [ thm : lower ] . assuming an ordering of @xmath58 s vertices , for a balancer @xmath79 in round @xmath192 , @xmath224 , @xmath57 , we set @xmath225 if the initial direction ( before the perturbation ) is @xmath226 and @xmath227 otherwise ( in the lower bound we considered the all - up orientation thus we had no use of these variables ) . as in section",
    "[ sec : lower ] , for a balancer @xmath224 in round @xmath192 , the random variable @xmath228 is @xmath91 if the balancer is perturbed and @xmath92 otherwise . using these notations",
    "we define a _ rounding vector _ @xmath229 , which accounts for the rounding errors in step @xmath192 .",
    "formally , @xmath230    now we can write the actual process as follows :    @xmath231    let @xmath232 be the set of balancers at time @xmath192 with no excess token , and @xmath233 the ones with",
    ". we can rewrite @xmath229 as follows : @xmath234    unfolding equation  , yields then @xmath235 } + \\sum_{i=1}^t \\rho^{(i ) } { \\ensuremath{\\mathbf{p}}}^{[i+1,t]},\\ ] ] where @xmath236 } = \\mathbf{i}$ ] .",
    "observe that @xmath237}$ ] is just @xmath195 ( as @xmath238 ) , and therefore @xmath239 } \\\\      & = { \\textstyle\\sum}_{i=1}^t             { \\textstyle\\sum}_{(u , v ) \\in { \\ensuremath{m^{(i)}_{{\\mathsf{odd } } } } } }             \\psi^{(i)}_{u , v } \\cdot \\phi^{(i)}_{u , v}\\cdot(e_u - e_v)\\cdot { \\ensuremath{\\mathbf{p}}}^{[i+1,t]}.\\end{aligned}\\ ] ] in turn , @xmath240}_{u , w } - { \\ensuremath{\\mathbf{p}}}^{[i+1,t]}_{v , w}\\right).\\ ] ]    our next task is to bound equation   to receive the desired term from proposition  [ pro : triangleineq ] .",
    "we do that similar in spirit to the way we went around in section  [ sec : proofofpro : logn_lowerbound ] .",
    "we break this sum into its first @xmath64 summands ( whose expected sum we calculate and to which we apply a large - deviation - bound ) .",
    "the remaining @xmath241 terms are bounded deterministically .",
    "one major difficulty in the general setting is that lemma  [ lem : balancersoddwithprobhalf ] ( which was crucial in the proof of theorem  [ thm : lower ] ) does not hold in general as its proof makes substantial use of the special structure of the @xmath16 .",
    "equation [ eq : xminusxi ] with @xmath242 yields @xmath243}_{u , w } -             { \\ensuremath{\\mathbf{p}}}^{[i+1,t_2]}_{v , w }             \\big ) .       \\end{aligned}\\ ] ] with @xmath244 denoting the row - vector with @xmath245 at @xmath37-th column and @xmath227 at @xmath38-th column and zeros elsewhere , we can rewrite and split this equation as follows : @xmath246 } e_{w } \\big ) \\\\             & & +             { \\textstyle\\sum}_{i = t_1 + 1}^{t_2 }             { \\textstyle\\sum}_{(u , v)\\in { \\ensuremath{m^{(i)}_{{\\mathsf{odd } } } } } }             \\psi_{u , v}^{(i ) } \\ , \\phi_{u , v}^{(i ) }             \\ ,             \\big ( e_{u , v } { \\ensuremath{\\mathbf{p}}}^{[i+1,t_2 ] } e_{w } \\big ) .",
    "\\end{aligned}\\ ] ] clearly , @xmath247 } e_{w } \\big ) \\\\           & = &   { \\textstyle\\sum}_{i = t_1 + 1}^{t_2 } \\left (             { \\textstyle\\sum}_{(u , v)\\in { \\ensuremath{m^{(i)}_{{\\mathsf{odd } } } } } }              \\psi_{u , v}^{(i ) } \\ , \\phi_{u , v}^{(i ) }             \\,e_{u , v }   \\right ) \\cdot \\left (   { \\ensuremath{\\mathbf{p}}}^{[i+1,t_2 ] }    \\ ,   e_{w }   \\right ) .",
    "\\end{aligned}\\ ] ] observe that @xmath248 is a vector all of whose entries are bounded by @xmath21 in absolute value . since @xmath249}$ ] is a stochastic matrix , the sum of all entries of the @xmath250-th column of @xmath249}$ ] , which is @xmath251 }    \\ ,   e_{w}|_1 $ ] , is exactly one , hence @xmath252 }    \\ ,   e_{w }   \\right )   \\right|      & { \\leqslant } &    ( { t_2}-t_1 ) .",
    "\\end{aligned}\\ ] ] it remains to bound @xmath253 } e_{w } \\big).\\ ] ] because @xmath254 is not necessarily a sum of independent random variables , it will be more convenient to work with the following quantity ( which is a sum of independent random variables , as it assumes that every balancer gets an excess token ) , @xmath255 } e_{w } \\big ) .      \\end{aligned}\\",
    "] ] so our strategy is first to bound the deviation of @xmath256 from its mean by hoeffdings bound and then apply the following lemma ( whose proof is in section  [ sec : independent ] ) , which justifies using @xmath256 instead of @xmath254 .",
    "[ lem : independent ] fix @xmath257 . for all @xmath59 with @xmath60 and arbitrary weights @xmath258 , let @xmath259 then for any @xmath260 , @xmath261}}| , |{\\ensuremath{\\operatorname{\\mathbf{e}}\\left[w^-\\right]}}|   \\ } \\right ] } }          { \\leqslant}16 \\ , { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[|w| { \\geqslant}\\delta\\right ] } } ,      \\end{aligned}\\ ] ] where @xmath262    in order to apply lemma [ lem : independent ] we first need to bound the expectation : @xmath263 } } \\right|&= \\left|          { \\textstyle\\sum}_{i=1}^{t_1 }          { \\textstyle\\sum}_{(u , v)\\in { \\ensuremath{m^{(i ) } } } }           { \\ensuremath{\\operatorname{\\mathbf{e}}\\left [ \\psi_{u , v}^{(i ) } \\ , \\phi_{u , v}^{(i ) } \\right ] } }          \\ ,          e_{u , v } { \\ensuremath{\\mathbf{p}}}^{[i+1,t_2 ] } e_{w}\\right| \\\\       & { \\leqslant}\\max_{i=1}^{t_1 }          \\max_{(u , v)\\in { \\ensuremath{m^{(i ) } } } } \\left| { \\ensuremath{\\operatorname{\\mathbf{e}}\\left [ \\psi_{u , v}^{(i ) } \\ , \\phi_{u , v}^{(i)}\\right ] } }   \\right| \\\\          & \\phantom{{\\leqslant}\\max_{i=1}^{t_1 } \\max_{(u , v)\\in { \\ensuremath{m^{(i ) } } } } }          \\cdot          \\left|{\\textstyle\\sum}_{i=1}^{t_1 } { \\textstyle\\sum}_{(u , v)\\in { \\ensuremath{m^{(i ) } } } } e_{u , v } { \\ensuremath{\\mathbf{p}}}^{[i+1,t_2 ] } e_{w } \\right| .",
    "\\end{aligned}\\ ] ] as we explained before , the last term is at most @xmath64 , and for any @xmath264 $ ] and @xmath265 , @xmath266 } } { \\leqslant}(1/2-   \\alpha).$ ] thus , @xmath267 } } \\right| { \\leqslant}t_1 \\ , ( 1/2 -\\ , \\alpha ) .",
    "\\notag          \\label{eq : ew }      \\end{aligned}\\ ] ] similarly , @xmath268 } } \\right| { \\leqslant}t_1 \\ , ( 1/2 -\\ , \\alpha ) $ ] and @xmath269 } } \\right| { \\leqslant}t_1 \\ , ( 1/2 -\\ , \\alpha ) $ ] .    to apply hoeffdings bound on @xmath256 , we bound the sum of the squared ranges of the involved random variables as follows : @xmath270}e_w             \\right]}}^2 \\\\      & { \\leqslant}{\\textstyle\\sum}_{i=1}^{t_1 }             { \\textstyle\\sum}_{(u , v)\\in { \\ensuremath{m^{(i ) } } } }             \\left(e_{u , v}{\\ensuremath{\\mathbf{p}}}^{[i+1,t_2]}e_w \\right)^2       = :   \\gamma .",
    "\\end{aligned}\\ ] ] now by hoeffdings bound , @xmath271}}| + \\varepsilon\\right ] } }          { \\leqslant}{\\ensuremath{\\operatorname{\\mathbf{pr}}\\left [ |w - { \\ensuremath{\\operatorname{\\mathbf{e}}\\left[w\\right]}}| { \\geqslant}\\varepsilon\\right]}}\\\\          & { \\leqslant}2 \\ , \\exp \\left(- 2 \\varepsilon^2 \\big/ { \\textstyle\\sum}_{i=1}^{t_1 }             { \\textstyle\\sum}_{(u , v)\\in { \\ensuremath{m^{(i ) } } } }             { \\ensuremath{\\operatorname{\\mathbf{range}}\\left[\\psi_{u , v}^{(\\ell ) } \\ , \\phi_{u , v}^{(i ) }             \\ ,             \\big (             e_{u , v}{\\ensuremath{\\mathbf{p}}}^{[i+1,t_2]}e_w             \\big)\\right]}}^2   \\right)\\\\          & { \\leqslant}2 \\ , \\exp \\left(- 2 \\varepsilon^2/\\gamma \\right ) .",
    "\\end{aligned}\\ ] ] choosing @xmath272 we get @xmath273}}| + { \\hksqrt}{\\gamma   \\log n}\\right ] } } { \\leqslant}2 n^{-2}.      $ ] hence by lemma  [ lem : independent ] we obtain with @xmath274}}| + { \\hksqrt}{\\gamma \\ , \\log n}$ ] that @xmath275}}| + 2 \\max \\ { | { \\ensuremath{\\operatorname{\\mathbf{e}}\\left[w^+\\right]}}| , |{\\ensuremath{\\operatorname{\\mathbf{e}}\\left[w^-\\right]}}| \\ }    + { \\hksqrt}{\\gamma \\log n}\\right]}}\\\\          & { \\leqslant}16   { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left [ |w| { \\geqslant}|{\\ensuremath{\\operatorname{\\mathbf{e}}\\left[w\\right]}}| + { \\hksqrt}{\\gamma \\log n}\\right ] } } \\\\          & { \\leqslant}32 \\ , n^{-2}.      \\end{aligned}\\ ] ] as @xmath276 is an upper bound on each of @xmath277}}| ,   | { \\ensuremath{\\operatorname{\\mathbf{e}}\\left[w^+\\right]}}|$ ] and @xmath278}}|$ ] , this readily implies @xmath279 } } & { \\leqslant}32 n^{-2}.      \\end{aligned}\\ ] ] finally , taking the union bound , we conclude by equation   that for all vertices @xmath280 @xmath281 } }              & { \\geqslant}1 - 32 n ^{-1}.           \\qedhere      \\end{aligned}\\ ] ]      before we begin the proof of lemma  [ lem : independent ] , we require the following two technical lemmas .    [ lem : simplelemma ] for @xmath282 , arbitrary @xmath283 , and independent random variables @xmath284 that are @xmath227 with probability @xmath0 and @xmath21 otherwise , let @xmath285 . then ,    * if all @xmath286 , then @xmath287 } } { \\leqslant}1/2 $ ] , + and for any number @xmath288 , @xmath289 } } { \\geqslant}{\\ensuremath{\\operatorname{\\mathbf{pr}}\\left [ x < - \\delta\\right]}}$ ] .",
    "* if all @xmath290 , then @xmath291 } } { \\leqslant}1/2 $ ] , + and for any number @xmath288 , @xmath292 } } { \\geqslant}{\\ensuremath{\\operatorname{\\mathbf{pr}}\\left [ x > \\delta\\right]}}$ ] .",
    "note that it suffices to prove the statement with all @xmath286 , as the case with @xmath290 follows from the first by considering @xmath293 with @xmath294 .",
    "let @xmath295 where each @xmath296 is an independent and uniform random variable taking values in @xmath297 ( corresponding to @xmath113 with @xmath4 ) . as @xmath298 is a sum of symmetrical distributed random variables , we have @xmath299 } } { \\leqslant}1/2 $ ] . since for any @xmath45 , @xmath113 stochastically dominates @xmath298 , we obtain @xmath300 } } { \\leqslant}{\\ensuremath{\\operatorname{\\mathbf{pr}}\\left[y < 0\\right ] } } { \\leqslant}1/2 ,      $ ] and the first claim of the lemma follows .",
    "the second claim is proven similarly by observing that @xmath301 } } { \\geqslant}{\\ensuremath{\\operatorname{\\mathbf{pr}}\\left [ y >   \\delta\\right ] } }          = { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left [ y < -\\delta\\right ] } }          { \\geqslant}{\\ensuremath{\\operatorname{\\mathbf{pr}}\\left [ x < - \\delta\\right]}}.          \\qedhere\\ ] ]    the following lemma bounds the probability that a sum @xmath285 deviates by more than a factor two from its expectation .",
    "[ lem : posrattail][lem : negrattail ] for @xmath282 , arbitrary @xmath302 , and @xmath284 independent random variables that are @xmath227 with probability @xmath0 and @xmath21 otherwise , let @xmath285 .",
    "then ,    1 .   if all @xmath286 , then @xmath303}}\\right ] } } { \\leqslant}7/8 .",
    "$ ] 2 .   if all @xmath290 , then @xmath304}}\\right ] } } { \\leqslant}7/8 .",
    "$ ]    as before , it suffices to prove the first statement .",
    "moreover , we may focus on the case @xmath305 } } \\neq 0 $ ] , as otherwise @xmath113 is symmetrically distributed around @xmath306 .",
    "finally , we also assume the weights to be integral , that is , @xmath307 .",
    "rational weights @xmath308 can be easily reduced to integral weights by multiplying all weights with their least common multiple of the denominators and applying the bound for the integral case .    for the sake of contradiction ,",
    "suppose that @xmath309}}\\right ] } } > 7/8 $ ] . as @xmath282",
    ", we have @xmath305}}= \\sum_{i=1}^n w_i ( 1 - 2 \\alpha ) { \\geqslant}0 $ ] and hence @xmath310 } } { \\leqslant}{\\ensuremath{\\operatorname{\\mathbf{pr}}\\left [ x { \\leqslant}2 { \\ensuremath{\\operatorname{\\mathbf{e}}\\left[x\\right]}}\\right ] } } < 1/8 $ ] . let @xmath311}}$ ] be such that @xmath312 } } { \\geqslant}1/8,$ ] and @xmath313 } }     < 1/8.$ ]    as we assumed the weights @xmath314 to be integral , we can use the following two well - known counting tricks : @xmath315 } }              & = \\sum_{x{\\geqslant}k } \\sum_{y=1}^x { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[x = x\\right]}}\\\\              & = \\sum_{y{\\geqslant}1 } \\sum_{x{\\geqslant}\\max(k , y ) } { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[x = x\\right]}}\\\\              & = \\sum_{x{\\geqslant}1 } { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[x{\\geqslant}\\max(k , x)\\right ] } } ,          \\end{aligned}\\ ] ] and similarly , @xmath316 } } = \\sum_{x{\\geqslant}1 } { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[x{\\leqslant}-\\max(k , x)\\right]}}$ ] .",
    "we use both to obtain @xmath315 } }              & = \\sum_{x{\\geqslant}1 } { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[x{\\geqslant}\\max(k , x)\\right ] } } \\\\              & = \\sum_{1{\\leqslant}x < k } { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[x{\\geqslant}k\\right ] } }                 + \\sum_{x{\\geqslant}k } { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[x{\\geqslant}x\\right ] } } \\\\              & > \\sum_{1{\\leqslant}x < k } \\left ( { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[x{\\leqslant}0\\right ] } } + { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[x{\\geqslant}k\\right]}}-1/8 \\right)\\\\              & \\phantom { > } + \\sum_{x{\\geqslant}k } { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[x{\\geqslant}x\\right]}}. \\\\",
    "\\end{aligned}\\ ] ] by lemma  [ lem : simplelemma ] we now get @xmath315 } }              & { \\geqslant}k\\,({\\ensuremath{\\operatorname{\\mathbf{pr}}\\left[x{\\geqslant}k\\right]}}-1/8 )                 + \\sum_{1{\\leqslant}x < k } { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[x{\\leqslant}-x\\right]}}\\\\              & \\phantom{{\\geqslant } } + \\sum_{x{\\geqslant}k } { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[x{\\leqslant}-x\\right ] } } \\\\              & = k\\ , ( { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[x{\\geqslant}k\\right]}}-1/8 )                 + \\sum_{x{\\geqslant}1 } { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[x{\\leqslant}-x\\right ] } } \\\\              & = k\\,({\\ensuremath{\\operatorname{\\mathbf{pr}}\\left[x{\\geqslant}k\\right]}}-1/8 )                 + \\sum_{x{\\leqslant}-1 } -x { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[x = x\\right ] } }      \\end{aligned}\\ ] ]    plugging this in the definition of @xmath305}}$ ] , we get @xmath317 } } & = \\sum_{x } x { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[x = x\\right]}}\\\\              & = \\sum_{x{\\leqslant}-1 } x { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[x = x\\right ] } } +                \\sum_{0{\\leqslant}x < k } x { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[x = x\\right ] } } +                \\sum_{x{\\geqslant}k } x { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[x = x\\right ] } } \\\\              & { \\geqslant}k\\,({\\ensuremath{\\operatorname{\\mathbf{pr}}\\left[x{\\geqslant}k\\right]}}-1/8 ) + \\sum_{0{\\leqslant}x < k } x { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[x = x\\right]}}.      \\end{aligned}\\ ] ] using above assumptions on @xmath27 and @xmath318}}\\right ] } } > 7/8 $ ] we now arrive at the desired contradiction , @xmath317 } }              & >",
    "2{\\ensuremath{\\operatorname{\\mathbf{e}}\\left[x\\right]}}\\,({\\ensuremath{\\operatorname{\\mathbf{pr}}\\left[x{\\geqslant}k\\right]}}-1/8 ) + \\sum_{2{\\ensuremath{\\operatorname{\\mathbf{e}}\\left[x\\right]}}{\\leqslant}x <",
    "k } x { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[x = x\\right ] } } \\\\              & { \\geqslant}2{\\ensuremath{\\operatorname{\\mathbf{e}}\\left[x\\right]}}\\,\\bigl ( { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[x{\\geqslant}k\\right]}}-1/8 + { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[2{\\ensuremath{\\operatorname{\\mathbf{e}}\\left[x\\right]}}{\\leqslant}x < k\\right]}}\\bigr ) \\\\              & { \\geqslant}2{\\ensuremath{\\operatorname{\\mathbf{e}}\\left[x\\right]}}\\,\\left ( { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[x{\\geqslant}2{\\ensuremath{\\operatorname{\\mathbf{e}}\\left[x\\right]}}\\right]}}-1/8   \\right ) \\\\              & { \\geqslant}3/2 { \\ensuremath{\\operatorname{\\mathbf{e}}\\left[x\\right]}}.              \\qedhere      \\end{aligned}\\ ] ]    we are now ready to prove lemma  [ lem : independent ] . in the following , we will use subsums of @xmath256 denoted as @xmath319 which are defined by combining the previous definitions in a natural way ,",
    "e.  g. , @xmath320    let @xmath321}}}=\\bigcup_{i = t_1}^{t_2 } { \\ensuremath{m^{(i)}}}$ ] be the set of all matching edges in the given time span .",
    "let @xmath321}_{{\\mathsf{odd}}}}}$ ] be the set of all odd ones",
    ". moreover , let us simply write @xmath322 for a particular assignment of @xmath323 for all balancers in @xmath321}}}$ ] .",
    "we now begin by bounding @xmath254 in terms of @xmath256 . by the definition of conditional probability and the law of total probabilities",
    ", it follows for arbitrary @xmath324 , @xmath325 } }      =      \\frac          { { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[w<\\delta_2 \\wedge w_{{\\mathsf{odd}}}{\\geqslant}\\delta_1\\right ] } } }          { { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[w_{{\\mathsf{odd}}}{\\geqslant}\\delta_1\\right]}}}\\notag\\\\      & { \\leqslant}\\frac          { { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[w - w_{{\\mathsf{odd}}}<\\delta_2-\\delta_1 \\wedge w_{{\\mathsf{odd}}}{\\geqslant}\\delta_1\\right ] } } }          { { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[w_{{\\mathsf{odd}}}{\\geqslant}\\delta_1\\right]}}}\\notag\\\\      & =      \\sum_{{\\ensuremath{\\mathcal{m } } } }          \\frac {              { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[{\\ensuremath{m^{[t_1\\!,t_2]}_{{\\mathsf{odd}}}}}={\\ensuremath{\\mathcal{m}}}\\right ] } }              { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[w_{{\\mathsf{even}}}<\\delta_2-\\delta_1 \\wedge              w_{{\\mathsf{odd}}}{\\geqslant}\\delta_1 \\mid { \\ensuremath{m^{[t_1\\!,t_2]}_{{\\mathsf{odd}}}}}={\\ensuremath{\\mathcal{m}}}\\right ] } }          } { { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[w_{{\\mathsf{odd}}}{\\geqslant}\\delta_1\\right ] } } } \\notag.\\end{aligned}\\ ] ]    note that for fixed @xmath322 , the ranges of @xmath254 and @xmath326 are determined and therefore the probability spaces are disjoint .",
    "this implies that @xmath254 and @xmath326 are independent conditioned on @xmath321}_{{\\mathsf{odd}}}}}={\\ensuremath{\\mathcal{m}}}$ ] . using this observation",
    ", we get @xmath325 } } \\notag\\\\[-.3 cm ]      & \\qquad{\\leqslant}\\sum_{\\ensuremath{\\mathcal{m}}}{\\ensuremath{\\operatorname{\\mathbf{pr}}\\left[{\\ensuremath{m^{[t_1\\!,t_2]}_{{\\mathsf{odd}}}}}={\\ensuremath{\\mathcal{m}}}\\right ] } }          \\overbrace{{\\ensuremath{\\operatorname{\\mathbf{pr}}\\left[w_{{\\mathsf{even}}}<\\delta_2-\\delta_1 \\mid { \\ensuremath{m^{[t_1\\!,t_2]}_{{\\mathsf{odd}}}}}={\\ensuremath{\\mathcal{m}}}\\right]}}}^ { ( * ) }          \\label{eq : thomasfavorite4}\\\\[-.1 cm ]          & \\qquad \\qquad \\qquad \\cdot{\\ensuremath{\\operatorname{\\mathbf{pr}}\\left[w_{{\\mathsf{odd}}}{\\geqslant}\\delta_1 \\mid { \\ensuremath{m^{[t_1\\!,t_2]}_{{\\mathsf{odd}}}}}={\\ensuremath{\\mathcal{m}}}\\right ] } }           \\big/ { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[w_{{\\mathsf{odd}}}{\\geqslant}\\delta_1\\right]}}.\\notag\\end{aligned}\\ ] ]    by plugging @xmath327}}$ ] and @xmath328 in ( * ) we obtain @xmath329 } } \\mid { \\ensuremath{m^{[t_1\\!,t_2]}_{{\\mathsf{odd}}}}}={\\ensuremath{\\mathcal{m}}}\\right ] } } \\notag\\\\      & \\qquad= 1 - { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[w_{{\\mathsf{even } } } { \\geqslant}2 \\ , { \\ensuremath{\\operatorname{\\mathbf{e}}\\left[w^{-}\\right ] } } \\mid { \\ensuremath{m^{[t_1\\!,t_2]}_{{\\mathsf{odd}}}}}={\\ensuremath{\\mathcal{m}}}\\right ] } } \\notag \\\\      & \\qquad{\\leqslant}1 - { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left [ w^+_{{\\mathsf{even } } } { \\geqslant}0 \\mid { \\ensuremath{m^{[t_1\\!,t_2]}_{{\\mathsf{odd}}}}}={\\ensuremath{\\mathcal{m}}}\\right ] } } \\notag\\\\      & \\qquad\\phantom{{\\leqslant}}\\cdot { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left [ w^-_{{\\mathsf{even } } } { \\geqslant}2 \\ , { \\ensuremath{\\operatorname{\\mathbf{e}}\\left[w^{-}\\right ] } } \\mid { \\ensuremath{m^{[t_1\\!,t_2]}_{{\\mathsf{odd}}}}}={\\ensuremath{\\mathcal{m}}}\\right]}}. \\notag\\end{aligned}\\ ] ]    observing that @xmath330 } } = { \\ensuremath{\\operatorname{\\mathbf{e}}\\left[\\psi_{u , v } \\mid { \\ensuremath{m^{[t_1\\!,t_2]}_{{\\mathsf{odd}}}}}={\\ensuremath{\\mathcal{m}}}\\right]}}$ ] for @xmath331 , we get @xmath332 } } { \\leqslant}{\\ensuremath{\\operatorname{\\mathbf{e}}\\left[w^{-}_{{\\mathsf{even } } } \\mid   { \\ensuremath{m^{[t_1\\!,t_2]}_{{\\mathsf{odd}}}}}={\\ensuremath{\\mathcal{m}}}\\right]}}$ ] and hence by lemmas  [ lem : simplelemma ] and  [ lem : negrattail ] , @xmath333 } } \\mid { \\ensuremath{m^{[t_1\\!,t_2]}_{{\\mathsf{odd}}}}}={\\ensuremath{\\mathcal{m}}}\\right ] } } \\notag \\\\      & \\qquad{\\leqslant}1 - { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left [ w^+_{{\\mathsf{even } } } { \\geqslant}0 \\mid { \\ensuremath{m^{[t_1\\!,t_2]}_{{\\mathsf{odd}}}}}={\\ensuremath{\\mathcal{m}}}\\right ] } } \\cdot \\notag\\\\      & \\qquad\\qquad{\\ensuremath{\\operatorname{\\mathbf{pr}}\\left [ w^-_{{\\mathsf{even } } } { \\geqslant}2 \\ , { \\ensuremath{\\operatorname{\\mathbf{e}}\\left[w^{-}_{{\\mathsf{even } } } \\ , \\mid \\ , { \\ensuremath{m^{[t_1\\!,t_2]}_{{\\mathsf{odd}}}}}={\\ensuremath{\\mathcal{m}}}\\right ] } } \\mid { \\ensuremath{m^{[t_1\\!,t_2]}_{{\\mathsf{odd}}}}}={\\ensuremath{\\mathcal{m}}}\\right ] } } \\notag \\\\      & \\qquad{\\leqslant}1 - \\tfrac{1}{2 } \\cdot \\tfrac{1}{8 }      = \\tfrac{15}{16}.      \\label{eq : applytail}\\end{aligned}\\ ] ] plugging this into equation   yields @xmath334 } }   \\mid w_{{\\mathsf{odd}}}{\\geqslant}\\delta\\right ] } } \\\\      & \\qquad{\\leqslant}\\frac{15}{16 } \\ , \\sum_{\\ensuremath{\\mathcal{m}}}\\frac {              { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[{\\ensuremath{m^{[t_1\\!,t_2]}_{{\\mathsf{odd}}}}}={\\ensuremath{\\mathcal{m}}}\\right ] } }                  { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[w_{{\\mathsf{odd}}}{\\geqslant}\\delta_1 \\mid { \\ensuremath{m^{[t_1\\!,t_2]}_{{\\mathsf{odd}}}}}={\\ensuremath{\\mathcal{m}}}\\right ] } }          } { { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[w_{{\\mathsf{odd}}}{\\geqslant}\\delta_1\\right ] } } } = \\frac{15}{16},\\end{aligned}\\ ] ] and hence @xmath335}}\\right]}}\\notag\\\\     & \\qquad{\\geqslant}{\\ensuremath{\\operatorname{\\mathbf{pr}}\\left [ w { \\geqslant}\\delta + 2 { \\ensuremath{\\operatorname{\\mathbf{e}}\\left[w^-\\right ] } } \\wedge w_{{\\mathsf{odd } } } { \\geqslant}\\delta\\right ] } } \\notag\\\\     & \\qquad= { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[w_{{\\mathsf{odd } } } { \\geqslant}\\delta\\right ] } } \\cdot { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[w { \\geqslant}\\delta + 2 { \\ensuremath{\\operatorname{\\mathbf{e}}\\left[w^-\\right ] } }        \\ , \\mid \\ ,",
    "w_{{\\mathsf{odd } } } { \\geqslant}\\delta\\right ] } } \\notag\\\\     & \\qquad{\\geqslant}\\tfrac{1}{16 } \\cdot { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left [ w_{{\\mathsf{odd } } } { \\geqslant}\\delta\\right]}}. \\label{eq : firststep}\\end{aligned}\\ ] ]    it remains to lower bound the deviation of @xmath254 in terms of @xmath256 in a similar fashion . as before , we derive @xmath336 } } \\notag\\\\[-.3 cm ]      & \\qquad{\\leqslant}\\sum_{\\ensuremath{\\mathcal{m}}}{\\ensuremath{\\operatorname{\\mathbf{pr}}\\left[{\\ensuremath{m^{[t_1\\!,t_2]}_{{\\mathsf{odd}}}}}={\\ensuremath{\\mathcal{m}}}\\right ] } }          \\overbrace{{\\ensuremath{\\operatorname{\\mathbf{pr}}\\left[w_{{\\mathsf{even}}}>\\delta_2-\\delta_1 \\mid { \\ensuremath{m^{[t_1\\!,t_2]}_{{\\mathsf{odd}}}}}={\\ensuremath{\\mathcal{m}}}\\right]}}}^ { ( * ) }          \\label{eq : thomasfavorite2}\\\\[-.1 cm ]          & \\qquad \\qquad \\qquad \\cdot{\\ensuremath{\\operatorname{\\mathbf{pr}}\\left[w_{{\\mathsf{odd}}}{\\leqslant}\\delta_1 \\mid { \\ensuremath{m^{[t_1\\!,t_2]}_{{\\mathsf{odd}}}}}={\\ensuremath{\\mathcal{m}}}\\right ] } }           \\big/ { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[w_{{\\mathsf{odd}}}{\\leqslant}\\delta_1\\right]}}.\\notag\\end{aligned}\\ ] ] we now choose @xmath337}},$ ] and @xmath338 . as before in equation  , we can now use lemmas  [ lem : simplelemma ] and  [ lem : posrattail ] to get for ( * ) that @xmath339 } } \\mid { \\ensuremath{m^{[t_1\\!,t_2]}_{{\\mathsf{odd}}}}}={\\ensuremath{\\mathcal{m}}}\\right ] } }      & { \\leqslant}\\tfrac{15}{16}.\\end{aligned}\\ ] ] plugging this into equation   yields @xmath340 } } \\mid w_{{\\mathsf{odd}}}{\\leqslant}-\\delta\\right ] } }      { \\leqslant}15/16\\ ] ] and hence @xmath341}}\\right]}}\\notag\\\\      & \\qquad{\\geqslant}{\\ensuremath{\\operatorname{\\mathbf{pr}}\\left [ w { \\leqslant}-\\delta + 2 { \\ensuremath{\\operatorname{\\mathbf{e}}\\left[w^+\\right ] } } \\wedge w_{{\\mathsf{odd } } } { \\leqslant}-\\delta\\right ] } } \\notag \\\\     & \\qquad= { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[w_{{\\mathsf{odd } } } { \\leqslant}-\\delta\\right ] } } \\cdot { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left[w { \\leqslant}-\\delta + 2 { \\ensuremath{\\operatorname{\\mathbf{e}}\\left[w^+\\right ] } }    \\ , \\mid \\ ,",
    "w_{{\\mathsf{odd } } } { \\leqslant}-\\delta\\right ] } } \\notag \\\\     & \\qquad{\\geqslant}\\tfrac{1}{16 } \\cdot { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left [ w_{{\\mathsf{odd } } } { \\leqslant}-\\delta\\right ] } }   \\label{eq : secondstep}.\\end{aligned}\\ ] ] combining equation   and equation  , we have shown for any @xmath288 , @xmath342 } } \\\\   & \\qquad= { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left [ w_{{\\mathsf{odd } } } { \\geqslant}\\delta\\right ] } } + { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left [ w_{{\\mathsf{odd } } } { \\leqslant}-\\delta\\right ] } } \\\\   & \\qquad{\\leqslant}16 \\ , { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left [ w { \\geqslant}\\delta + 2 { \\ensuremath{\\operatorname{\\mathbf{e}}\\left[w^{-}\\right]}}\\right ] } } + 16 \\ , { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left [ w { \\leqslant}- \\delta + 2 { \\ensuremath{\\operatorname{\\mathbf{e}}\\left[w^{+}\\right]}}\\right ] } } \\\\   & \\qquad{\\leqslant}16 \\ , { \\ensuremath{\\operatorname{\\mathbf{pr}}\\left [ |w| { \\geqslant}\\delta - 2 \\max \\ { |{\\ensuremath{\\operatorname{\\mathbf{e}}\\left[w^+\\right]}}| , |{\\ensuremath{\\operatorname{\\mathbf{e}}\\left[w^-\\right]}}| \\ } \\right]}}.\\end{aligned}\\ ] ] adding @xmath343}}| , |{\\ensuremath{\\operatorname{\\mathbf{e}}\\left[w^-\\right]}}| \\}$ ] to @xmath344 gives the assertion of the lemma .",
    "consider the network after @xmath345 ( periodic ) repetitions of the balancing network ( each repetition consists of @xmath42 rounds , so the network consists of a total of @xmath346 matchings ) .",
    "@xmath70 is the so - called round matrix corresponding to the application of @xmath42 consecutive matchings ( recall that we use the following abbreviations @xmath347}={\\textstyle\\prod}_{i = t_1}^{t_2 } { \\ensuremath{\\mathbf{p}}}^{(i)}$ ] ) .",
    "this is indeed a special case of our general scheme where the matrices @xmath348 are applied , but now @xmath349}={\\ensuremath{\\mathbf{p}}}^{[d+1,2d]}$ ] and so on .",
    "recall the notation that @xmath244 denotes the row - vector with @xmath245 at @xmath37-th column and @xmath227 at @xmath38-th column .",
    "in theorem  [ thm : upper ] we plug @xmath350 and @xmath351 .",
    "hence , @xmath352}e_w",
    "\\right)^2}.     \\end{aligned}\\ ] ] let @xmath353 be the eigenvectors of @xmath249}$ ] forming an orthogonal basis of @xmath208 and let @xmath354 be the corresponding eigenvalues . since @xmath249}$ ] is a stochastic matrix , @xmath355 .",
    "write @xmath356 . using this ,",
    "we rewrite @xmath66 as follows ( for short we drop the `` @xmath357 '' part as our final result will not depend on @xmath250 ) , @xmath358 } \\sum_{j=1}^n c_j^{(i ) } v_j^{(i ) } \\right)^2 } \\\\         & = { \\hksqrt}{\\log n \\ , { \\textstyle\\sum}_{i=1}^{t_1 }             { \\textstyle\\sum}_{(u , v)\\in { \\ensuremath{m^{(i ) } } } }             \\left(e_{u , v } \\sum_{j=1}^n c_j^{(i ) } \\lambda_j^{(i ) } v_j^{(i ) }   \\right)^2}.    \\end{aligned}\\ ] ] observe that @xmath244 is orthogonal to @xmath359 .",
    "therefore @xmath360 for every @xmath31 which gives @xmath361 define a vector @xmath362 .",
    "the latter is then @xmath363 since @xmath34 is a matching , for each @xmath31 each vertex is counted only at most once , thus @xmath364    by standard calculation ( cf .",
    "section  [ sec : spectral ] ) , we have @xmath365})^2 \\cdot \\| e_w \\|^2 = \\lambda({\\ensuremath{\\mathbf{p}}}^{[i+1,t_2]})^2 .     \\end{aligned}\\ ] ] plugging this into ( [ eq : letzter ] ) and using the facts that for any two stochastic matrices @xmath366 and @xmath367 , @xmath368 and for any integer @xmath27 , @xmath369 , @xmath370})^2 } \\\\      & { \\leqslant}2 { \\hksqrt } { \\log n \\ , { \\textstyle\\sum}_{i=1}^{t_1 } \\lambda ( { \\ensuremath{\\mathbf{q}}}^{\\lfloor ( t_2-(i+1))/d \\rfloor } ) ^2}.     \\end{aligned}\\ ] ] regrouping the matrices according to the periods the latter reduces to @xmath371 with @xmath372 , we can upper bound @xmath66 by @xmath373 plugging in our choices for @xmath64 and @xmath61 we end up with @xmath374    with the same arguments , we get @xmath375 } ) \\ , { \\hksqrt}{n } k { \\leqslant}\\ , { \\hksqrt}{n } k/ ( kn)^2 .",
    "\\end{aligned}\\ ] ] plugging all this into theorem  [ thm : upper ] shows that at step @xmath376 the discrepancy is at most @xmath377",
    "in theorem  [ thm : upper ] we choose @xmath378 and @xmath379 . for matrix multiplication",
    "we use the following abbreviated form : @xmath380}:= { \\ensuremath{\\mathbf{p}}}^{(i)}{\\ensuremath{\\mathbf{p}}}^{(i+1)}\\cdots { \\ensuremath{\\mathbf{p}}}^{(j)}$ ] .",
    "first we observe that    * @xmath381}_{u , v}=2^k / n$ ] if wires @xmath37 and @xmath38 are at distance @xmath382 and differ only in the last @xmath383 bits ( in their binary representation ) , * otherwise , @xmath381}_{u , v}= 0 $ ] .",
    "in particular this shows that @xmath205}$ ] is equal to the all-@xmath384 matrix and thus @xmath385 } \\right)=0 $ ] ( that is , @xmath386 ) .",
    "moreover , for any fixed wire @xmath250 , @xmath387 } \\right ) e_w          \\right)^2 }          \\\\          & =           { \\hksqrt } { \\log n\\,{\\textstyle\\sum}_{i=1}^{\\log n - \\log \\log n }   { \\textstyle\\sum}_{(u , v ) \\in { \\ensuremath{m^{(i ) } } } }          \\left ( { \\ensuremath{\\mathbf{p}}}^{[i+1,t_2]}_{u , w } - { \\ensuremath{\\mathbf{p}}}^{[i+1,t_2]}_{v , w }          \\right)^2 }           \\\\          & { \\leqslant}{\\hksqrt } { \\log n \\ , { \\textstyle\\sum}_{i=1}^{\\log n - \\log \\log n } \\frac{n}{2^{i } } \\left ( \\frac{2^{i+1}}{n } \\right)^2 } \\\\          & = 4 \\ , { \\hksqrt } { \\log n \\ , { \\textstyle\\sum}_{i=1}^{\\log n - \\log \\log n } \\frac{2^i}{n } } \\\\          & { \\leqslant}4 \\ , { \\hksqrt } { \\log n\\cdot \\frac{2}{\\log n } } \\\\          & = { \\mathcal{o}}(1).\\end{aligned}\\ ] ]    -values of @xmath388 with random uniformly distributed input from @xmath389 . @xmath3 corresponds to the adversarial model while @xmath4 is the completely random model .",
    "the dotted line describes the experimental results , the broken lines are our theoretical lower and upper bounds.,scaledwidth=66.0% ]",
    "we examined experimentally how well a @xmath15  balances a random input .",
    "we implemented a @xmath388 consisting of roughly one billion wires and thirty billion balancers .",
    "the input was chosen independently uniformly at random from @xmath389 . for initial directions of the balancers all up and different values @xmath0 between @xmath306 and @xmath92 we measured the resulted discrepancy .",
    "figure  [ fig : experiments ] presents the average over 100 runs , together with theoretical lower and upper bounds . as the input is random ,",
    "the so - far presented bounds can be tightened .",
    "following the same lines of proof , one can easily show the following slightly better bounds on the _ expected _ discrepancy @xmath390 in the random - input case :          d.  arthur and s.  vassilvitskii .",
    "worst - case and smoothed analysis of the icp algorithm , with an application to the k - means method . in _",
    "47th ieee symp . on found . of comp .",
    "science ( focs06 ) _ , pages 153164 , 2006 .",
    "a.  coja - oghlan , u.  feige , a.  m. frieze , m.  krivelevich , and d.  vilenchik . on smoothed @xmath27-cnf formulas and the walksat algorithm . in _",
    "20th acm - siam symp . on discrete algorithms ( soda09 )",
    "_ , pages 451460 , 2009 .",
    "t.  friedrich , t.  sauerwald , and d.  vilenchik . smoothed analysis of balancing networks . in _",
    "36th international colloquium on automata , languages , and programming ( icalp09 ) _ , volume 5556 of _ lecture notes in computer science _ , pages 472483 .",
    "springer , 2009 .",
    "y.  rabani , a.  sinclair , and r.  wanka . local divergence of markov chains and the analysis of iterative load balancing schemes . in _",
    "39th annual ieee symposium on foundations of computer science ( focs98 ) _ , pages 694705 , 1998 ."
  ],
  "abstract_text": [
    "<S> in a balancing network each processor has an initial collection of unit - size jobs ( tokens ) and in each round , pairs of processors connected by balancers split their load as evenly as possible . an excess token ( if any ) is placed according to some predefined rule . </S>",
    "<S> as it turns out , this rule crucially affects the performance of the network . in this work </S>",
    "<S> we propose a model that studies this effect . </S>",
    "<S> we suggest a model bridging the uniformly - random assignment rule , and the arbitrary one ( in the spirit of smoothed - analysis ) . </S>",
    "<S> we start with an arbitrary assignment of balancer directions and then flip each assignment with probability @xmath0 independently . for a large class of balancing networks </S>",
    "<S> our result implies that after @xmath1 rounds the discrepancy is @xmath2 with high probability . </S>",
    "<S> this matches and generalizes known upper bounds for @xmath3 and @xmath4 . </S>",
    "<S> we also show that a natural network matches the upper bound for any @xmath0 . </S>"
  ]
}