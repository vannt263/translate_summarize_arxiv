{
  "article_text": [
    "high - dimensional time series include video streams , electroencephalography ( eeg ) and sensor network data .",
    "dynamical models describing such data are desired for forecasting ( prediction ) and controller design , both of which play an important role , e.g. , in autonomous systems , machine translation , robotics and surveillance applications . a key challenge is system identification , i.e. , finding a mathematical model of the dynamical system based on the information provided by measurements from the underlying system . in the context of state - space models",
    "this includes finding two functional relationships between ( a ) the states at different time steps ( prediction / transition model ) and ( b ) states and corresponding measurements ( observation / measurement model ) . in the linear case ,",
    "this problem is very well studied , and many standard techniques exist , e.g. , subspace methods  @xcite , expectation maximization  @xcite and prediction - error methods  @xcite . however , in realistic and practical scenarios we require non - linear system identification techniques .    learning non - linear dynamical models is an inherently difficult problem , and it has been one of the most active areas in system identification for the last decades  @xcite . in recent years , sequential monte carlo ( smc ) methods have received attention for identifying non - linear state - space models  @xcite , see also the recent surveys  @xcite .",
    "while methods based on smc are powerful , they are also computationally expensive .",
    "learning non - linear dynamical models from very high - dimensional sensor data is even more challenging .",
    "first , finding ( non - linear ) functional relationships in very high dimensions is hard ( un - identifiability , local optima , overfitting , etc . ) ; second , the amount of data required to find a good function approximator is enormous .",
    "fortunately , high - dimensional data often possesses an intrinsic lower dimensionality .",
    "we will exploit this property for system identification by finding a low - dimensional representation of high - dimensional data and learning predictive models in this low - dimensional space . for this purpose ,",
    "we need an automated procedure for finding compact low - dimensional representations / features .",
    "the state of the art in learning parsimonious representations of high - dimensional data is currently defined by deep learning architectures , such as deep neural networks  @xcite , stacked / deep auto - encoders  @xcite and convolutional neural networks  @xcite , all of which have been successfully applied to image , text , speech and audio data in commercial products , e.g. , by google , amazon and facebook .",
    "typically , these feature learning methods are applied to static data sets , e.g. , for image classification .",
    "the auto - encoder gives explicit expressions of two generative mappings : 1 ) an encoder @xmath0 mapping the high - dimensional data to the features , and 2 ) a decoder @xmath1 mapping the features to high - dimensional reconstructions . in the machine learning literature",
    ", there exists a vast number of other well studied nonlinear dimensionality reduction methods such as the gaussian process latent variable model ( gp - lvm ) @xcite , kernel pca  @xcite , laplacian eigenmaps  @xcite and locally linear embedding @xcite .",
    "however , all of them provide at most one of the two mappings @xmath1 and @xmath0 .    in this paper , we combine feature learning and dynamical systems modeling to obtain good predictive models for high - dimensional time series .",
    "in particular , we use deep auto - encoder neural networks for automatically finding a compact low - dimensional representation of an image . in this low - dimensional feature space",
    ", we use a neural network for modeling the nonlinear system dynamics .",
    "the embedding and the predictive model in feature space are learned _",
    "jointly_. an simplified illustration of our approach is shown in [ fig : model illustration ] .",
    "an encoder @xmath0 maps an image @xmath2 at time step @xmath3 to a low - dimensional feature @xmath4 . in this feature space ,",
    "a prediction model @xmath5 maps the feature forward in time to @xmath6 .",
    "subsequently , the decoder @xmath1 can be used to generate a predicted image @xmath7 at the next time step .",
    "this framework needs access to both the encoder @xmath0 and the decoder @xmath1 , which motivates our use of the auto - encoder as dimensionality reduction technique .",
    "consequently , the contributions of this paper are ( a ) a model for learning a low - dimensional dynamical representation of high - dimensional data , which can be used for long - term predictions ; ( b ) experimental evidence demonstrating that jointly learning the parameters of the latent embedding and the predictive model in latent space can increase the performance compared to a separate training .",
    "we consider a dynamical system where control inputs are denoted by @xmath8 and observations are denoted by @xmath9 . in the context of this paper , the observations are pixel information from images .",
    "we assume that a low - dimensional latent variable @xmath10 exists that compactly represents the relevant properties of @xmath9 . since we consider dynamical systems , a low - dimensional representation @xmath10 of a ( static ) image @xmath9 is insufficient to capture important dynamic information , such as velocities .",
    "therefore , we introduce an additional latent variable @xmath11 , the _ state_. in our case , the state @xmath12 contains features from multiple time steps ( e.g. , @xmath3 and @xmath13 ) to capture velocity ( or higher - order ) information .",
    "therefore , our transition model does not map features at time @xmath3 to time @xmath13 ( as illustrated in [ fig : model illustration ] ) , but the transition function @xmath14 maps states @xmath15 ( and controls @xmath16 ) to states @xmath12 at time @xmath13 .",
    "the full dynamical system is given as the state - space model    [ eq : model ] @xmath17    where each measurement @xmath7 can be described by a low - dimensional feature representation @xmath6 .",
    "these features are in turn modeled with a low - dimensional state - space model in and , where the state @xmath12 contains the full information about the state of the system at time instant  @xmath13 , see also [ fig : probabilistic_model ] . here",
    "@xmath18 , @xmath19 and @xmath20 are sequences of independent random variables and @xmath21 are the model parameters .    0.46    ( x1 ) at ( -1,0 ) @xmath15 ; ( x2 ) at ( 1,0 ) @xmath22 ; ( x0 ) at ( -2.5,0 ) ; ( d ) at ( -2.5,-1.5 ) ; ( x3 ) at ( 2.5,0 ) ;",
    "( y1 ) at ( -1,1.5 ) @xmath4 ; ( y2 ) at ( 1,1.5 ) @xmath23 ; ( z1 ) at ( -1,3.2 ) @xmath2 ; ( z2 ) at ( 1,3.2 ) @xmath24 ; ( u0 ) at ( -2.5,-1 ) ; ( u1 ) at ( -1,-1.5 ) @xmath16 ; at ( -2.5,3.2 ) high - dim . + data ; at ( -2.5,1.5 ) features ; at ( -2.5,0 ) hidden + state ; at ( -2.5,-1.5 ) control + inputs ; ( x1 )  ( x2 ) node[midway , below ] @xmath14 ; ( x0 ) ",
    "( x1 ) ; ( x2 )  ( x3 ) ; ( x1 )  ( y1 )",
    "node[midway , left ] @xmath25 ; ( y1 )  ( z1 ) node[midway , left ] @xmath1 ; ( x2 )  ( y2 )",
    "node[midway , left ] @xmath25 ; ( y2 )  ( z2 ) node[midway , left ] @xmath1 ; ( u0 )  ( x1 ) ; ( u1 )  ( x2 ) ;    0.46    ( u0 ) at ( -4,-1.5 ) @xmath26 ; ( u1 ) at ( -1,-1.5 ) @xmath16 ; ( y0 ) at ( -4,1.5 ) @xmath27 ; ( y1 ) at ( -1,1.5 ) @xmath4 ; ( y2 ) at ( 1,1.5 ) @xmath28 ; ( d1 ) at ( -2.5,-1.5 ) ; ( d2 ) at ( -2.5,1.5 ) ; ( d3 ) at ( -2.5,3.2 ) ; ( z0 ) at ( -4,3.2 ) @xmath29 ; ( z1 ) at ( -1,3.2 ) @xmath2 ; ( z2 ) at ( 1,3.2 ) @xmath30 ; at ( -4.5,3.2 ) high - dim .",
    "+ data ; at ( -4.5,1.5 ) features ; at ( -4.5,-1.5 ) control + inputs ; ( y1 )  ( y2 ) ; ( u1 )  ( y2 ) ; ( u0 )  ( y2 ) node[midway , below ] @xmath5 ; ( z1 )  ( y1 )",
    "node[midway , left ] @xmath31 ; ( z0 )  ( y0 )",
    "node[midway , left ] @xmath31 ; ( y2 )  ( z2 ) node[midway , left ] @xmath1 ; ( y0 ) .. controls ( -1,0.5 ) .. ( y2 ) ;      to identify parameters in dynamical systems , the prediction - error method @xcite has been applied extensively within the system identification community during the last five decades .",
    "it is based on minimizing the error between the sequence of measurements @xmath7 and the predictions @xmath32 , usually the one - step ahead prediction . to achieve this",
    ", we need a predictor model that relates the prediction @xmath32 to all previous measurements , control inputs and the system parameters @xmath21 .    in general , it is difficult to derive a predictor model based on the nonlinear state - space model , and a closed form expression for the prediction is only available in a few special cases  @xcite .",
    "however , by approximating the optimal solution , a predictor model for the features @xmath6 can be stated in the form @xmath33 where @xmath34 includes all past features and control inputs , @xmath5 is a nonlinear function and @xmath35 is the corresponding model parameters . note that the predictor model no longer has an explicit notion of the state @xmath12 .",
    "the model introduced in   is indeed very flexible , and in this work we have restricted this flexibility somewhat by working with a nonlinear autoregressive exogenous model ( narx ) @xcite , which relates the predicted current value @xmath36 of the time series to the past @xmath37 values of the time series @xmath38 , as well as the past @xmath37 values of the control inputs @xmath39 .",
    "the resulting narx predictor model is given by @xmath40 where @xmath5 is a nonlinear function , in our case a neural network .",
    "the model parameters in the nonlinear function are normally estimated by minimizing the sum of the prediction errors @xmath41 .",
    "however , since we are interested in a good predictive performance for the high - dimensional data @xmath9 rather than for the features @xmath10 , we transform the predictions back to the high - dimensional space and obtain a prediction @xmath42 , which we use in our error measure .",
    "an additional complication is that we do not have access to the features @xmath6 .",
    "therefore , before training , the past values of the time series have to be replaced with their feature representation @xmath43 , which we compute from the pixel information @xmath9 . here , @xmath31 is an approximate inverse of @xmath1 , which will be described in more detail the next section .",
    "this gives the final predictor model    @xmath44    which is also illustrated in [ fig : predictor_model ] .",
    "the corresponding prediction error will be @xmath45      /[count= ] in 1,2,3,missing,4 ( input- ) at ( 0,2.5- ) ;    in 1,missing,2 ( hiddena- ) at ( 2,2 - 1.25 ) ;    in 1,missing,2 ( hiddenb- ) at ( 4,1.5- ) ;    in 1,missing,2 ( hiddenc- ) at ( 6,2 - 1.25 ) ;    /[count= ] in 1,2,3,missing,4 ( output- ) at ( 8,2.5- ) ;    [count = i ] in 1,2,3,m ( input - i )  + + ( -1.2,0 ) node [ above , midway ] @xmath46 ;    [count = i ] in 1,m at ( hiddenb-i.north ) @xmath47 ;    [count = i ] in 1,2,3,m ( output - i )  + + ( 1.9,0 ) node [ above , midway ] @xmath48 ;    iin 1, ... ,4 in 1, ... ,2 ( input - i ) ",
    "( hiddena- ) ;    iin 1, ... ,2 in 1, ... ,2 ( hiddena - i ) ",
    "( hiddenb- ) ; iin 1, ... ,2 in 1, ... ,2 ( hiddenb - i ) ",
    "( hiddenc- ) ;    iin 1, ... ,2 in 1, ... ,4 ( hiddenc - i ) ",
    "( output- ) ;    [count = from 0 ] in input layer + ( high - dim .",
    "data ) , , hidden layer + ( feature ) , , output layer + ( reconstructed ) at ( 2,3 )  ; [count = from 0 ] in , @xmath49 , , @xmath50 , at ( 2,-2.8 )  ;    we use a deep auto - encoder neural network to parameterize the feature mapping and its inverse .",
    "it consists of a deep encoder network @xmath31 and a deep decoder network @xmath1 .",
    "each layer @xmath51 of the encoder neural network @xmath31 computes @xmath52 , where @xmath53 is a squashing function and @xmath54 and @xmath55 are free parameters .",
    "the control input to the first layer is the image , i.e. , @xmath56 .",
    "the last layer is the low - dimensional feature representation of the image @xmath57 , where @xmath58 $ ] are the parameters of all neural network layers .",
    "the decoder @xmath1 consists of the same number of layers in reverse order , see [ fig : autoencoder ] . it can be considered an approximate inverse of the encoder , such that @xmath59 , where @xmath60 is the reconstructed version of @xmath7 . in the classical setting ,",
    "the encoder and the decoder are trained simultaneously to minimize the reconstruction error @xmath61 where the parameters of @xmath1 and @xmath31 optionally can be coupled to constrain the solution to some degree  @xcite .",
    "we realize that the autoencoder suits our problem at hand very well , since it provides an explicit expression of both the mapping from features to data @xmath1 as well as its approximate inverse @xmath31 , which is convenient to form the predictions in  .",
    "many other nonlinear dimensionality reduction methods such as the gaussian process latent variable model ( gp - lvm ) @xcite , kernel pca  @xcite , laplacian eigenmaps  @xcite and locally linear embedding @xcite do not provide an explicit expression of both mappings @xmath1 and @xmath0 .",
    "this is the main motivation way we use the ( deep ) auto - encoder model for dimensionality reduction .",
    "to summarize , our model contains the following free parameters : the parameters for the encoder @xmath62 , the parameters for the decoder @xmath63 and the parameters for the predictor model @xmath35 . to train the model , we employ two cost functions , the sum of the prediction errors  ,    @xmath64    and the sum of the reconstruction errors  , @xmath65    generally , there are two ways of finding the model parameters : ( 1 ) separate training and ( 2 ) joint training , both of which are explained below .      normally when features are used for inference of dynamical models , they are first extracted from the data in a pre - processing step and as a second step the predictor model is estimated based on these features . in our setting",
    ", this would correspond to sequentially training the model with using two cost functions  where we first learn a compact feature representation by minimizing the reconstruction error    [ eq : separate_training ] @xmath66 and subsequently minimize the prediction error @xmath67    with fixed auto - encoder parameters @xmath68 .",
    "the gradients of these cost functions with respect to the model parameters can be computed efficiently by back - propagation .",
    "the cost functions are then minimized by the bfgs algorithm  @xcite .",
    "an alternative to separate training is to minimize the reconstruction error and the prediction error simultaneously by considering the optimization problem @xmath69 where we jointly optimize the free parameters in both the auto - encoder @xmath62 , @xmath63 and the predictor model @xmath35 .",
    "again , back - propagation is used for computing the gradients of this cost function .",
    "the auto - encoder has strong similarities with principal component analysis ( pca ) .",
    "more precisely , if we use a linear activation function and only consider a single layer , the auto - encoder and pca are identical @xcite .",
    "we exploited this relationship to initialize the parameters of the auto - encoder .",
    "the auto - encoder network has been unfolded , each pair of layers in the encoder and the decoder have been combined , and the corresponding pca solution has been computed for each of these pairs . by starting with the high - dimensional data at the top layer and using the principal components from that pair of layers as input to the next pair of layers , we recursively compute a good initialization for all parameters in the auto - encoder network .",
    "similar pre - training routines are found in @xcite where a restricted boltzmann machine is used instead of pca .",
    "we report results on identification of ( 1 ) the nonlinear dynamics of a pendulum ( 1-link robot arm ) moving in a horizontal plane and the torque as control input , ( 2 ) an object moving in the 2d - plane , where the 2d velocity serves as control input . in both examples ,",
    "we learn the dynamics solely based on pixel information .",
    "each pixel @xmath70 is a component of the measurement @xmath71^\\transp$ ] and assumes a continuous gray - value in @xmath72 $ ] .",
    "0.46    0.46      we simulated 400 frames of a pendulum moving in a plane with @xmath73 pixels in each frame and the torque of the pendulum as control input . to speed up training ,",
    "the image input has been reduced to @xmath74 prior to model learning ( system identification ) using pca . with these 50 dimensional inputs ,",
    "four layers have been used for the encoder @xmath31 as well as the decoder @xmath1 with dimension 50 - 25 - 12 - 6 - 2 .",
    "hence , the features have dimension @xmath75 .",
    "the order of the dynamics was chosen as @xmath76 to capture velocity information .",
    "for the predictor model @xmath5 we used a two - layer neural network with a 6 - 4 - 2 architecture .",
    "we evaluate the performance in terms of long term predictions .",
    "these predictions are constructed by concatenating multiple 1-step ahead predictions .",
    "more precisely , the @xmath77-step ahead prediction @xmath78 is computed iteratively as    [ eq : multi_step_prediction ] @xmath79    where @xmath80 is the features of the data point at time instance @xmath13 .",
    "we assumed that the applied future torques were known .",
    "the predictive performance on two exemplary image sequences of the validation data of our system identification models is illustrated in [ fig : results ] , where control inputs ( torques ) are assumed known .",
    "the top rows show the ground truth images , the center rows show the predictions based on a model using joint training  , the bottom rows show the corresponding predictions of a model where the auto - encoder and the predictive model were trained sequentially according to  . for the model based on jointly training all parameters , we obtain good predictive performance for both one - step ahead prediction and multiple - step ahead prediction .",
    "in contrast , the predictive performance of learning the features and the dynamics separately is worse than the predictive performance of the model trained by jointly optimizing all parameters .",
    "although the auto - encoder does a perfect job ( left - most frame , 0-step ahead prediction ) , already the ( reconstructed ) one - step ahead prediction is not similar to the ground - truth image .",
    "this can also be seen in table  [ tab : result ] where the reconstruction error is equally good for both models , but for the prediction error we manage to get a better value using joint training than using separate training .",
    "let us have a closer look at the model based on separate training : since the auto - encoder performs well , the learned transition model is the reason for bad predictive performance .",
    "we believe that the auto - encoder found a good feature representation for reconstruction , but this representation was not ideal for learning a transition model .",
    "1 : prediction error @xmath81 and reconstruction error @xmath82 for separate and joint training . [ cols=\"<,<,<\",options=\"header \" , ]     0.46    0.46    [ fig : latentboth ] displays the `` decoded '' images corresponding to the latent representations using joint and separate training , respectively . in the joint training the feature values line up in a circular shape enabling a low - dimensional dynamical description , whereas separate training finds feature values , which are not even placed sequentially in the low - dimensional representation .",
    "separate training extracts the low - dimensional representations without context , i.e. , the knowledge that these features constitute a time - series . on the other hand ,",
    "joint training enables the extraction of features that can also model the dynamical behavior in a compact manner .    in this particular data set",
    ", the data points clearly reside on one - dimensional manifold , encoded by the pendulum angle",
    ". however , a one - dimensional feature space would be insufficient since this one - dimensional manifold is cyclic , see [ fig : latentboth ] , compare also with the @xmath83 period of an angle .",
    "therefore , we have used a two - dimensional latent space .",
    "further , only along the manifold in the latent space where the training data reside the decoder produces reasonable outputs .",
    "this can be further inspected by zooming in on a smaller region as displayed in [ fig : latent_example ] .",
    "plot coordinates ( 0 , 96.1978 ) ( 1 , 96.1541 ) ( 2 , 95.9451 ) ( 3 , 95.5584 ) ( 4 , 95.1162 ) ( 5 , 94.6709 ) ( 6 , 94.2409 ) ( 7 , 93.8712 ) ( 8 , 93.5397 ) ;    plot coordinates ( 0 , 96.0393 ) ( 1 , 88.8115 ) ( 2 , 88.9518 ) ( 3 , 89.823 ) ( 4 , 89.5416 ) ( 5 , 88.8983 ) ( 6 , 89.0535 ) ( 7 , 89.572 ) ( 8 , 89.5154 ) ;    plot coordinates ( 0 , 100 ) ( 1 , 92.9682 ) ( 2 , 90.9634 ) ( 3 , 90.0684 ) ( 4 , 89.5501 ) ( 5 , 89.2161 ) ( 6 , 88.9749 ) ( 7 , 88.8094 ) ( 8 , 88.6715 ) ; plot coordinates ( 0 , 92.301 ) ( 1 , 92.123 ) ( 2 , 91.9723 ) ( 3 , 91.8444 ) ( 4 , 91.7392 ) ( 5 , 91.654 ) ( 6 , 91.5849 ) ( 7 , 91.5264 ) ( 8 , 91.4775 ) ;    to analyze the predictive performance of the two training methods , we define the fitting quality as @xmath84 as a reference , the predictive performance is compared with a naive prediction using the previous frame at time step @xmath85 as the prediction at @xmath13 as @xmath86 .",
    "the result for a prediction horizon ranging from @xmath87 to @xmath88 is displayed in [ fig : prediction ] .",
    "clearly , joint learning ( blue ) outperforms separate learning in terms of predictive performance for prediction horizons greater than 0 . even by using the last available image frame for prediction ( const .",
    ", brown ) , we obtain a better fit than the model that learns its parameter sequentially ( red ) .",
    "this is due to the fact that the dynamical model often predicts frames , which do not correspond to any real pendulum , see [ fig : results ] , leading to a poor fit .",
    "furthermore , joint training gives better predictions than the naive prediction .",
    "the predictive performance slightly degrades when the prediction horizon @xmath77 increases , which is to be expected .",
    "finally we also compare with the subspace identification method  @xcite ( black , starred ) , which is restricted to linear models .",
    "such a restriction does not capture the non - linear , embedded features and , hence , the predictive performance is sub - optimal .      in this experiment",
    "we simulated 601 frames of a moving tile in a @xmath89 pixels image .",
    "the control inputs are the increments in position in both of the two cartesian directions . as in the previous experiment ,",
    "the image sequence was reduced to @xmath74 prior to the parameter learning using pca . a four layer 50 - 25 - 12 - 8 - 2 autoencoder",
    "was used for feature learning , and an 8 - 5 - 2 neural network for the dynamics .    as in the previous example",
    ", we evaluate the performance in terms of long - term predictions .",
    "the performance of joint training is illustrated in [ fig : results_2dtile ] on a validation data set .",
    "the model predicts future frames of the tile with high accuracy . in [ fig : latent_2dtile ] , the feature representation of the data is displayed .",
    "the features reside on a two - dimensional manifold encoding the two - dimensional position of the moving tile .",
    "the four corners in this manifold represent the four corners of the tile position within the image frame .",
    "this structure is induced by the dynamical description .",
    "the corresponding feature representation for the case of separate learning does not exhibit such a structure , see the supplementary material .    in [ fig",
    ": prediction_2dtile ] , the prediction performance is displayed , where our model achieves a substantially better fit than naively using the previous frame as prediction .",
    "from a system identification point of view , the prediction error method , where we minimize the one - step ahead prediction error , is fairly standard .",
    "however , in a future control or reinforcement learning setting , we are primarily interested in good prediction performance on a longer horizon in order to do planning .",
    "thus , we have also investigated whether to additionally include a multi - step ahead prediction error in the cost  .",
    "these models achieved similar performance , but no significantly better prediction error could be observed either for one - step ahead predictions or for longer prediction horizons .",
    "+    plot coordinates ( 0 , 94.1555 ) ( 1 , 94.0422 ) ( 2 , 93.868 ) ( 3 , 93.6945 ) ( 4 , 93.5584 ) ( 5 , 93.4499 ) ( 6 , 93.3689 ) ( 7 , 93.298 ) ( 8 , 93.2361 ) ;    plot coordinates ( 0 , 100 ) ( 1 , 87.1909 ) ( 2 , 86.3216 ) ( 3 , 86.0445 ) ( 4 , 85.8246 ) ( 5 , 85.6132 ) ( 6 , 85.5812 ) ( 7 , 85.4631 ) ( 8 , 85.3723 ) ;    instead of computing the prediction errors in image space , see , we can compute errors in feature space to avoid a decoding step back to the high - dimensional space according to  .",
    "however , this will require an extra penalty term in order to avoid trivial solutions that map everything to zero , eventually resulting in a more complicated and less intuitive cost function .",
    "although joint learning aims at finding a feature representation that is suitable for modeling the low - dimensional dynamical behavior , the pre - training initialization as described in section  [ sec : pretraining ] does not . if this pre - training yields feature values far from `` useful '' ones for modeling the dynamics , joint training might not find a good model .",
    "the network structure has to be chosen before the actual training starts .",
    "especially the dimension of the latent state and the order of the dynamics have to be chosen by the user , which requires a some prior knowledge about the system to be identified . in our examples , we chose the latent dimensionality based on insights about the true dynamics of the problem .",
    "in general , a model selection procedure will be preferable to find both a good network structure and a good latent dimensionality .",
    "we have presented an approach to non - linear system identification from high - dimensional time series data .",
    "our model combines techniques from both the system identification and the machine learning community .",
    "in particular , we used a deep auto - encoder for finding low - dimensional features from high - dimensional data , and a nonlinear autoregressive exogenous model was used to describe the low - dimensional dynamics .",
    "the framework has been applied to a pendulum moving in the horizontal plane .",
    "the proposed model exhibits good predictive performance and a major improvement has been identified by training the auto - encoder and the dynamical model jointly instead of training them separately / sequentially .",
    "possible directions for future work include ( a ) robustify learning by using denoising autoencoders @xcite to deal with noisy real - world data ( b ) apply convolutional neural networks , which are often more suitable for images ; ( c ) exploiting the learned model for learning controller purely based on pixel information ; ( c ) sequential monte carlo methods will be investigated for systematic treatments of such nonlinear probabilistic models , which are required in a reinforcement learning setting .    in a setting where we make decisions based on ( one - step or multiple - step ahead ) predictions , such as optimal control or model - based reinforcement learning ,",
    "a probabilistic model is often needed for robust decision making as we need to account for models errors  @xcite .",
    "an extension of our present model to a probabilistic setting is non - trivial since random variables have to be transformed through the neural networks , and their exact probability density functions will be intractable to compute .",
    "sampling - based approaches or deterministic approximate inference are two options that we will investigate in future .",
    "this work was supported by the swedish foundation for strategic research under the project _ cooperative localization _ and by the swedish research council under the project _",
    "probabilistic modeling of dynamical systems _",
    "( contract number : 621 - 2013 - 5524 ) .",
    "mpd was supported by an imperial college junior research fellowship .",
    "in the second experiment in the paper , results the joint learning of prediction and reconstruction error have been reported .",
    "the joint learning brings structure to feature values , which is not present if the autoencoder is learn separately , see fig .  [",
    "fig : latent_2dtile_sep ] ."
  ],
  "abstract_text": [
    "<S> modeling dynamical systems is important in many disciplines , e.g. , control , robotics , or neurotechnology . </S>",
    "<S> commonly the state of these systems is not directly observed , but only available through noisy and potentially high - dimensional observations . in these cases , system identification , </S>",
    "<S> i.e. , finding the measurement mapping and the transition mapping ( system dynamics ) in latent space can be challenging . for linear system dynamics and measurement mappings </S>",
    "<S> efficient solutions for system identification are available . however , </S>",
    "<S> in practical applications , the linearity assumptions does not hold , requiring non - linear system identification techniques . </S>",
    "<S> if additionally the observations are high - dimensional ( e.g. , images ) , non - linear system identification is inherently hard . to address the problem of non - linear system identification from high - dimensional observations , </S>",
    "<S> we combine recent advances in deep learning and system identification . </S>",
    "<S> in particular , we jointly learn a low - dimensional embedding of the observation by means of deep auto - encoders and a predictive transition model in this low - dimensional space . </S>",
    "<S> we demonstrate that our model enables learning good predictive models of dynamical systems from pixel information only . </S>"
  ]
}