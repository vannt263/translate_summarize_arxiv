{
  "article_text": [
    "the brazilian government classifies the goods it buys according to a particular taxonomy : the catmat ( catalog of materials ) .",
    "when the government purchases a new good , the person in charge of classifying it must choose one of the 560 catmat classes .",
    "the cognitive load is heavy , especially if the person is new to catmat .",
    "the result is misclassification .",
    "for instance , of the 30,774 goods classified as wheeled vehicles ( catmat class # 2320 ) between 1999 - 04 - 01 and 2015 - 04 - 02 , 17,469 cost less than r$ 1,000 ( us$ 250 as of 2015 - 12 - 01 ) .",
    "that price seems too low for a wheeled vehicle .",
    "when we inspect the product descriptions we see that the problem is misclassification : spare parts ( catmat classes # 2510 , # 2520 , # 2530 , and # 2590 ) are often misclassified as wheeled vehicles ( catmat class # 2320 ) .    when the misclassification rate is so high it is hard to audit government expenditures .",
    "in particular , it is hard to know whether the price paid for a given item was reasonable ( i.e. , similar to the prices paid for the same item before ) or not . auditing authorities waste resources investigating legitimate purchases , on the one hand , and fail to investigate fraudulent purchases , on the other hand .",
    "to reduce misclassification i trained a support vector machine ( svm ) classifier that takes a product description as input and returns the three most likely catmat classes as output .",
    "the idea is to semi - automate the process : we want to suggest likely classes to the person classifying the good ( the user of our app ) .",
    "he or she may choose among the suggested classes or ignore them and choose a different class .",
    "for instance , say some public hospital is buying insulin syringes .",
    "the hospital will describe the product in detail on the invitation to bid - say , as  disposable insulin syringes with 0.5ml and ultra - thin ( 6 mm ) needle , 50 units \" .",
    "the user feeds that description to the svm classifier , which then suggests three catmat classes : # 6515 ( medical and surgical equipment and supplies ) , # 6550 ( supplies for medical diagnostic and testing ) , and # 6640 ( laboratory equipment and supplies ) .",
    "the user chooses one of these three classes or , alternatively , discards them and chooses a different class",
    ".    we can not fully automate the process ( i.e. , substitute the svm classifier for the human classifier ) , for two reasons . first , in about 16.7% of the cases none of the svm - suggested classes is the correct one ( more on this later ) . hence a human must remain in charge of the final decision .",
    "but in the remaining 83.3% of the cases the svm classifier will be correct , which should reduce classification error . and perhaps this is a foundation upon which a fully automated classifier can be created in the future .",
    "second , after the user has chosen a catmat class he or she must also choose a subclass and an item .",
    "the catmat is a hierarchical taxonomy : it comprises 79 groups , 560 classes , 18,380 subclasses , and 217,907 items ( as of 2015 - 04 - 02 ) .",
    "but the svm classifier can not offer any suggestions when it comes to subclasses or items : with 18,380 or 217,907 categories there is just not enough data to train the classifier ; its accuracy would be too low . but",
    "reducing misclassification at the catmat level already improves the quality of the data , even if the user chooses the wrong subclass or item - research and auditing done at the class level will be more reliable . and perhaps the user is more likely to choose the correct subclass and item when the catmat class is correct .",
    "svm is a well - known classification algorithm first introduced by vapnik and lerner ( 1963 ) and later popularized by boser , guyon and vapnik ( 1992 ) and by cortes and vapnik ( 1995 ) .",
    "it learns from manually classified samples ( the training samples ) and then uses that knowledge to classify new samples .",
    "here we have a total of 28,420,761 samples , which are all the goods the brazilian government purchased between 1999 - 04 - 01 and 2015 - 04 - 02 .",
    "i split the samples into 70% training , 15% validation , and 15% testing .",
    "the classifier  sees \" the description _ and _ the catmat class of each of the training samples and thus learns how to map descriptions onto catmat classes .",
    "for instance , the classifier learns that when  stapler \" appears in the description the good is usually classified under catmat class # 7520 .",
    "it learns that when  tea \" appears in the description the good is usually classified under catmat class # 8955 .",
    "and so on .",
    "once the classifier is trained we use the validation samples to assess its performance . for each validation sample and each catmat class",
    "the classifier estimates the probability that the sample belongs to that class .",
    "hence for each validation sample the classifier estimates 560 probabilities .",
    "the estimation is word - based .",
    "for instance , having the word  stapler \" in the description increases the probability that the sample belongs to catmat class # 7520 and decreases the probability that it belongs to catmat class # 8955 .",
    "when all estimations are done we check how often the catmat class of highest probability is the correct one .",
    "we then go back to the training phase , changing one or more model parameters ( like the learning rate or the regularization term ) .",
    "we assess the classifier s performance once more .",
    "we train the model again , with new changes in the parameters .",
    "and so on and so forth until we are satisfied with the results .",
    "when the classifier performs well enough with the validation samples we check how well it performs with the testing samples .",
    "that is the final result - once the testing samples are used we can not go back and adjust the parameters again , lest we overfit the classifier ( i.e. , lest we make the classifier too responsive to the idiosyncrasies of our data and thus non - generalizable ) .",
    "before we can train the svm classifier we need to collect , clean , and process the data .",
    "all the data are in the comprasnet database .",
    "the descriptions are in the d_itcp_item_compra table .",
    "the catmat classes are in the f_item_compra table .",
    "a common key allows joins between these two tables .",
    "i discarded the cases whose description and/or class was empty or null we are left .",
    "i also discarded the cases that corresponded to services ( about 10% of the total ) , as services are too unique and idiosyncratic for algorithmic classification .",
    "the result was a total of 28,420,761 samples ; as mentioned above , they correspond to all the goods the brazilian government purchased between 1999 - 04 - 01 and 2015 - 04 - 02 .",
    "i lower - cased all the descriptions and removed all special characters , numbers , and one - character words .",
    "i also converted plural words to singular words , using the first step of a portuguese stemmer algorithm ( the rslp algorithm , created by orengo and huyck [ 2001 ] ) .",
    "finally , i removed all words that only appeared once in the entire dataset .",
    "i transformed each document ( description ) into a vector of word counts and merged all vectors .",
    "this resulted in a term - frequency matrix - a matrix whose rows represent words , columns represent documents , and each entry is the frequency of word _",
    "i _ on document _ j _ ( i.e. , the term - frequency , @xmath0 ) .",
    "next i apply the @xmath1 transformation .",
    "the @xmath1 of each entry is given by its term - frequency ( @xmath0 ) multiplied by @xmath2 , where @xmath3 is the total number of documents and @xmath4 is the number of documents in which word @xmath5 appears ( i.e. , the word s document frequency  @xmath6 ; the @xmath2 ratio thus gives us the inverse document frequency ",
    "@xmath7 ) .",
    "what the @xmath1 transformation does is increase the importance of the word the more it appears in the document but the less it appears in the whole corpus .",
    "hence it helps us reduce the weights of inane words like ` the ' ( ` o ' , in portuguese ) , ` of ' ( ` de ' ) , etc and increase the weights of discriminant words ( i.e. , words that appear a lot but only in a few documents ) . for more details on @xmath1",
    "see manning , raghavan , and schtze ( 2008 ) .",
    "the next step is normalization .",
    "here we have documents of widely different sizes , ranging from two or three words to dozens of words .",
    "longer documents contain more unique words and have larger @xmath8 values , which may skew the results ( manning , raghavan , and schtze 2008 ) . to avoid that we normalize the columns of the @xmath1 matrix , transforming them into unit vectors .",
    "the resulting @xmath1 matrix has dimensions 28,420,761 ( descriptions ) by 505,938 ( unique words ) .",
    "it is this matrix that we use to train , validate , and test the svm classifier .",
    "our svm classifier achieved an accuracy of 83.3% .",
    "the errors seem to have two main causes . first , misclassification in the training data",
    ". the svm classifier is learning from imperfect examples .",
    "as we discussed before , half the goods currently classified as wheeled vehicles ( catmat class # 2320 ) are actually spare parts ( catmat classes # 2510 , # 2520 , # 2530 , and # 2590 ) .",
    "second , class frequency .",
    "catmat classes that are bought more often tend to be more accurately classified by the svm classifier .",
    "this is expected : for these classes there are more training samples , so the classifier has a larger set of descriptions from which to learn .",
    "in fact , there is a negative linear correlation between class frequency and misclassification rate ( -0.36 , with p @xmath9 0.0001 ) .",
    "i used the trained classifier to build an open source web app .",
    "its interface is very simple : there is a single input form , wherein the user writes or pastes the description of the good being purchased ( as it appears on the invitation to bid , for example ) .",
    "the user then clicks submit and is shown the three most likely catmat classes .    in the example below we inputed the description of an air conditioning unit and",
    "got back three catmat classes : # 4120 ( air conditioning equipment ) , with probability 58% ; # 4130 ( air conditioning and cooling components ) , with probability 22% ; and # 6550 ( supplies for medical diagnostic and testing ) , with probability 4% . here",
    "the first class - the one with the highest probability - is the correct one .",
    "initially the idea was to put the app up online but the cost was prohibitive .",
    "the trained classifier is heavy ( the coefficients take up over 4 gb on disk ) and requires a computer with large ram - at least 16 gb but ideally more . on cloud providers like amazon web services and",
    "google compute engine that would cost around us$ 200 per month ( r$ 800 as of 2015 - 12 - 01 ) .",
    "the code for the app is available on github though : https://github.com/thiagomarzagao/catmatfinder .",
    "i made the code open source so that anyone can use and modify it .",
    "bird , steven , edward loper , and ewan klein .",
    "2009 .  natural language processing with python . \"",
    "oreilly media inc .",
    "boser , bernhard , isabelle guyon and vladimir vapnik .",
    "1992 .  a training algorithm for optimal margin classifiers . \" in d. haussler , editor , _ proceedings of the annual conference on computational learning theory _ , 144 - 152 .",
    "platt , john . 2000 .",
    "probabilistic outputs for support vector machines and comparison to regularized likelihood methods . in a.j .",
    "smola , p.l .",
    "bartlett , b. schlkopf , and d. schuurmans , editors , _ advances in large margin classifiers_. mit press ."
  ],
  "abstract_text": [
    "<S> the brazilian government often misclassifies the goods it buys . </S>",
    "<S> that makes it hard to audit government expenditures . </S>",
    "<S> we can not know whether the price paid for a ballpoint pen ( code # 7510 ) was reasonable if the pen was misclassified as a technical drawing pen ( code # 6675 ) or as any other good . </S>",
    "<S> this paper shows how we can use machine learning to reduce misclassification . </S>",
    "<S> i trained a support vector machine ( svm ) classifier that takes a product description as input and returns the most likely category codes as output . </S>",
    "<S> i trained the classifier using 20 million goods purchased by the brazilian government between 1999 - 04 - 01 and 2015 - 04 - 02 . in 83.3% of the cases the correct category code was one of the three most likely category codes identified by the classifier . </S>",
    "<S> i used the trained classifier to develop a web app that might help the government reduce misclassification . </S>",
    "<S> i open sourced the code on github ; anyone can use and modify it . </S>"
  ]
}