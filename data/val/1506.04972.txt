{
  "article_text": [
    "in this paper , we propose an iterative algorithm to solve the following general optimization problem : @xmath0 where @xmath1 is a closed and convex set , and @xmath2 is a proper and differentiable function with a continuous gradient .",
    "we assume that problem ( [ eq : original_function ] ) has a solution .",
    "problem ( [ eq : original_function ] ) also includes some class of nondifferentiable optimization problems , if the nondifferentiable function @xmath3 is convex : @xmath4 because problem ( [ eq : original_function_nonsmooth ] ) can be rewritten into a problem with the form of ( [ eq : original_function ] ) by the help of auxiliary variables : @xmath5    we do not assume that @xmath6 is convex , so ( [ eq : original_function ] ) is in general a nonconvex optimization problem .",
    "the focus of this paper is on the development of efficient iterative algorithms for computing the stationary points of problem ( [ eq : original_function ] ) .",
    "the optimization problem ( [ eq : original_function ] ) represents general class of optimization problems with a vast number of diverse applications .",
    "consider for example the sum - rate maximization in the mimo multiple access channel ( mac ) @xcite , the broadcast channel ( bc ) @xcite and the interference channel ( ic ) @xcite , where @xmath6 is the sum - rate function of multiple users ( to be maximized ) while the set @xmath7 characterizes the users power constraints . in the context of the mimo ic , ( [ eq : original_function ] ) is a nonconvex problem and np - hard @xcite . as another example , consider portfolio optimization in which @xmath6 represents the expected return of the portfolio ( to be maximized ) and the set @xmath7 characterizes the trading constraints @xcite .",
    "furthermore , in sparse ( @xmath8-regularized ) linear regression , @xmath6 denotes the least square function and @xmath3 is the sparsity regularization function @xcite .",
    "commonly used iterative algorithms belong to the class of descent direction methods such as the conditional gradient method and the gradient projection method for the differentiable problem ( [ eq : original_function ] ) @xcite and the proximal gradient method for the nondifferentiable problem ( [ eq : original_function_nonsmooth ] ) @xcite , which often suffer from slow convergence . to speed up the convergence , the block coordinate descent ( bcd ) method that uses the notion of the nonlinear best - response",
    "has been widely studied ( * ? ? ?",
    "in particular , this method is applicable if the constraint set of ( [ eq : original_function ] ) has a cartesian product structure @xmath9 such that @xmath10 the bcd method is an iterative algorithm : in each iteration , only one variable is updated by its best - response @xmath11 ( i.e. , the point that minimizes @xmath6 with respect to ( w.r.t . )",
    "the variable @xmath12 only while the remaining variables are fixed to their values of the preceding iteration ) and the variables are updated sequentially .",
    "this method and its variants have been successfully adopted to many practical problems @xcite .",
    "when the number of variables is large , the convergence speed of the bcd method may be slow due to the sequential nature of the update .",
    "a parallel variable update based on the best - response seems attractive as a mean to speed up the updating procedure , however , the convergence of a parallel best - response algorithm is only guaranteed under rather restrictive conditions , c.f . the diagonal dominance condition on the objective function @xmath13 @xcite , which is not only difficult to satisfy but also hard to verify . if @xmath13 is convex , the parallel algorithms converge if the stepsize is inversely proportional to the number of block variables @xmath14 .",
    "this choice of stepsize , however , tends to be overly conservative in systems with a large number of block variables and inevitably slows down the convergence @xcite .",
    "a recent progress in parallel algorithms has been made in @xcite , in which it was shown that the stationary point of ( [ eq : original_function ] ) can be found by solving a sequence of successively refined _ approximate problems _ of the original problem ( [ eq : original_function ] ) , and convergence to a stationary point is established if , among other conditions , the approximate function ( the objective function of the approximate problem ) and stepsizes are properly selected .",
    "the parallel algorithms proposed in @xcite are essentially descent direction methods . a description on how to construct the approximate problem such that the convexity of the original problem is preserved as much as possible is also contained in @xcite to achieve faster convergence than standard descent directions methods such as classical conditional gradient method and gradient projection method .",
    "despite its novelty , the parallel algorithms proposed in @xcite suffer from two limitations .",
    "firstly , the approximate function must be strongly convex , and this is usually guaranteed by artificially adding a quadratic regularization term to the original objective function @xmath6 , which however may destroy the desirable characteristic structure of the original problem that could otherwise be exploited , e.g. , to obtain computationally efficient closed - form solutions of the approximate problems @xcite .",
    "secondly , the algorithms require the use of a decreasing stepsize . on the one hand ,",
    "a slow decay of the stepsize is preferable to make notable progress and to achieve satisfactory convergence speed ; on the other hand , theoretical convergence is guaranteed only when the stepsize decays fast enough . in practice",
    ", it is a difficult task on its own to find a decay rate for the stepsize that provides a good trade - off between convergence speed and convergence guarantee , and current practices mainly rely on heuristics @xcite .",
    "the contribution of this paper consists in the development of a novel iterative convex approximation method to solve problem ( [ eq : original_function ] ) .",
    "in particular , the advantages of the proposed iterative algorithm are the following :    \\1 ) the approximate function of the original problem ( [ eq : original_function ] ) in each iteration only needs to exhibit a weak form of convexity , namely , pseudo - convexity .",
    "the proposed iterative method not only includes as special cases many existing methods , for example , @xcite , but also opens new possibilities for constructing approximate problems that are easier to solve .",
    "for example , in the mimo bc sum - rate maximization problems ( sec . [ sub : mimo - broadcast - channel ] ) , the new approximate problems can be solved in closed - form .",
    "we also show by a counterexample that the assumption on pseudo - convexity is tight in the sense that if it is not satisfied , the algorithm may not converge .",
    "\\2 ) the stepsizes can be determined based on the problem structure , typically resulting in faster convergence than in cases where constant stepsizes @xcite and decreasing stepsizes @xcite are used .",
    "for example , a constant stepsize can be used when @xmath6 is given as the difference of two convex functions as in dc programming @xcite .",
    "when the objective function is nondifferentiable , we propose a new exact / successive line search method that is carried out over a properly constructed differentiable function .",
    "thus it is much easier to implement than state - of - the - art techniques that operate on the original nondifferentiable objective function directly .    in the proposed algorithm ,",
    "the exact / successive line search is used to determine the stepsize and it can be implemented in a centralized controller , whose existence presence is justified for particular applications , e.g. , the base station in the mimo bc , and the portfolio manager in multi - portfolio optimization @xcite .",
    "we remark that also in applications in which centralized controller are not admitted , however , the line search procedure does not necessarily imply an increased signaling burden when it is implemented in a distributed manner among different distributed processors . for example , in the lasso problem studied in sec .",
    "[ sub : lasso ] , the stepsize based on the exact line search can be computed in closed - form and it does not incur any additional signaling as in predetermined stepsizes , e.g. , decreasing stepsizes and constant stepsizes .",
    "besides , even in cases where the line search procedure induces additional signaling , the burden is often fully amortized by the significant increase in the convergence rate .",
    "the rest of the paper is organized as follows . in sec .",
    "[ sec : preliminaries ] we introduce the mathematical background .",
    "the novel iterative method is proposed and its convergence is analyzed in sec .",
    "[ sec : proposed - method ] ; its connection to several existing descent direction algorithms is presented there . in sec .",
    "[ sec : applications ] , several applications are considered : the sum rate maximization problem of mimo bc , the energy efficiency maximization of a massive mimo system to illustrate the advantage of the proposed approximate function , and the lasso problem to illustrate the advantage of the proposed stepsize .",
    "the paper is finally concluded in sec .",
    "[ sec : concluding - remarks ] .    _",
    "notation : _ we use @xmath15 , @xmath16 and @xmath17 to denote a scalar , vector and matrix , respectively .",
    "we use @xmath18 to denote the @xmath19-th element of @xmath17 ; @xmath20 is the @xmath21-th element of @xmath16 where @xmath22 , and @xmath23 denotes all elements of @xmath16 except @xmath20 : @xmath24 .",
    "we denote @xmath25 as the element - wise inverse of @xmath16 , i.e. , @xmath26 .",
    "notation @xmath27 and @xmath28 denotes the hadamard product between @xmath16 and @xmath29 , and the kronecker product between @xmath17 and @xmath30 , respectively .",
    "the operator @xmath31_{\\mathbf{a}}^{\\mathbf{b}}$ ] returns the element - wise projection of @xmath16 onto @xmath32 $ ] : @xmath31_{\\mathbf{a}}^{\\mathbf{b}}\\triangleq\\max(\\min(\\mathbf{x},\\mathbf{b}),\\mathbf{a})$ ] , and @xmath33^{+}\\triangleq\\left[\\mathbf{x}\\right]_{\\mathbf{0}}$ ] .",
    "we denote @xmath34 as the smallest integer that is larger than or equal to @xmath15 .",
    "we denote @xmath35 as the vector that consists of the diagonal elements of @xmath17 and @xmath36 is a diagonal matrix whose diagonal elements are as same as @xmath16 .",
    "we use @xmath37 to denote the vector whose elements are equal to 1 .",
    "in this section , we introduce the basic definitions and concepts that are fundamental in the development of the mathematical formalism used in the rest of the paper .    * stationary point*. _",
    "_ a point @xmath38 is a stationary point of ( [ eq : original_function ] ) if @xmath39 condition ( [ eq : stationary - point ] ) is the necessary condition for local optimality of the variable @xmath29 . for nonconvex problems , where global optimality conditions are difficult to establish , the computation of stationary points of the optimization problem ( [ eq : original_function ] )",
    "is generally desired .",
    "if ( [ eq : original_function ] ) is convex , stationary points coincide with ( globally ) optimal points and condition ( [ eq : stationary - point ] ) is also sufficient for @xmath29 to be ( globally ) optimal .",
    "* descent direction*. _ _ the vector @xmath40 is a descent direction of the function @xmath6 at @xmath41 if @xmath42",
    "if ( [ eq : def - descent - direction ] ) is satisfied , the function @xmath6 can be decreased when @xmath16 is updated from @xmath43 along direction @xmath40 .",
    "this is because in the taylor expansion of @xmath6 around @xmath41 is given by : @xmath44 where the first order term is negative in view of ( [ eq : def - descent - direction ] ) . for sufficiently small @xmath45",
    ", the first order term dominates all higher order terms .",
    "more rigorously , if @xmath40 is a descent direction , there exists a @xmath46 such that @xmath47 note that the converse is not necessarily true , i.e. , @xmath48 for arbitrary functions @xmath6 does not necessarily imply that @xmath49 is a descent direction of @xmath6 at @xmath41 .",
    "* quasi - convex function*. _ _ a function @xmath50 is quasi - convex if for any @xmath51 $ ] : @xmath52 a locally optimal point @xmath29 of a quasi - convex function @xmath50 over a convex set @xmath7 is also globally optimal , i.e. , @xmath53    * pseudo - convex function*. _ _ a function @xmath50 is pseudo - convex if @xcite @xmath54 another equivalent definition of pseudo - convex functions is also useful in our context @xcite : @xmath55 in other words , @xmath56 implies that @xmath57 is a descent direction of @xmath50 .",
    "a pseudo - convex function is also quasi - convex ( * ? ? ?",
    "9.3.5 ) , and thus any locally optimal points of pseudo - convex functions are also globally optimal .",
    "* convex function*. _ _ a function @xmath50 is convex if @xmath58 it is strictly convex if the above inequality is satisfied with strict inequality whenever @xmath59 .",
    "it is easy to see that a convex function is pseudo - convex .    *",
    "strongly convex functions*. _ _",
    "a function @xmath50 is strongly convex with constant @xmath60 if @xmath61 for some positive constant @xmath60 .",
    "the relationship of functions with different degree of convexity is summarized in figure [ fig : relationship - of - functions ] where the arrow denotes implication in the direction of the arrow .",
    "relationship of functions with different degree of convexity ]",
    "in this section , we propose an iterative algorithm that solves ( [ eq : original_function ] ) as a sequence of successively refined approximate problems , each of which is much easier to solve than the original problem ( [ eq : original_function ] ) , e.g. , the approximate problem can be decomposed into independent subproblems that might even exhibit closed - form solutions .    in iteration @xmath62 ,",
    "let @xmath63 be the approximate function of @xmath6 around the point @xmath43 .",
    "then the approximate problem is @xmath64 and its optimal point and solution set is denoted as @xmath65 and @xmath66 , respectively : @xmath67 we assume that the approximate function @xmath68 satisfies the following technical conditions :    ( a1 ) the approximate function @xmath68 is pseudo - convex in @xmath16 for any given @xmath38 ;    ( a2 ) the approximate function @xmath68 is continuously differentiable in @xmath16 for any given @xmath38 and continuous in @xmath29 for any @xmath69 ;    ( a3 ) the gradient of @xmath68 and the gradient of @xmath6 are identical at @xmath70 for any @xmath38 , i.e. , @xmath71 ;    based on ( [ eq : mapping_definition ] ) , we define the mapping @xmath72 that is used to generate the sequence of points in the proposed algorithm : @xmath73 given the mapping @xmath72 , the following properties hold .    [ stationary point and descent direction][prop : descent - property ] provided that assumptions ( a1)-(a3 ) are satisfied : ( i ) a point @xmath29 is a stationary point of ( [ eq : original_function ] ) if and only if @xmath74 defined in ( [ eq : mapping_definition ] ) ; ( ii ) if @xmath29 is not a stationary point of ( [ eq : original_function ] ) , then @xmath75 is a descent direction of @xmath6 : @xmath76    see appendix [ sec : proof - of - proposition - descent ] .",
    "if @xmath43 is not a stationary point , according to proposition [ prop : descent - property ] , we define the vector update @xmath77 in the @xmath78-th iteration as : @xmath79 where @xmath80 $ ] is an appropriate stepsize that can be determined by either the exact line search ( also known as the minimization rule ) or the successive line search ( also known as the armijo rule ) .",
    "since @xmath81 , @xmath82 and @xmath80 $ ] , it follows from the convexity of @xmath7 that @xmath83 for all @xmath62 .",
    "* exact line search*. * * the stepsize is selected such that the function @xmath6 is decreased to the largest extent along the descent direction @xmath84 : @xmath85 with this stepsize rule , it is easy to see that if @xmath43 is not a stationary point , then @xmath48 .    in the special case that @xmath6 in ( [ eq : original_function ] ) is convex and @xmath86 nulls the gradient of @xmath87 , i.e. , @xmath88 , then @xmath89 in ( [ eq : minimization - rule ] ) is simply the projection of @xmath86 onto the interval @xmath90 $ ] : @xmath91_{0}^{1}=\\begin{cases } 1 , & \\textrm { if } \\left.\\nabla_{\\gamma}f(\\mathbf{x}^{t}+\\gamma(\\mathbb{b}\\mathbf{x}^{t}-\\mathbf{x}^{t}))\\right|_{\\gamma=1}\\geq0,\\\\ 0 , & \\textrm { if } \\left.\\nabla_{\\gamma}f(\\mathbf{x}^{t}+\\gamma(\\mathbb{b}\\mathbf{x}^{t}-\\mathbf{x}^{t}))\\right|_{\\gamma=0}\\leq0,\\\\ \\gamma^{\\star } , & \\textrm { otherwise}. \\end{cases}\\ ] ] if @xmath92 , the constrained optimization problem in ( [ eq : minimization - rule ] ) is essentially unconstrained . in some applications it is possible to compute @xmath86 analytically , e.g. , if @xmath6 is quadratic as in the lasso problem ( sec .",
    "[ sub : lasso ] ) . otherwise , for general convex functions , @xmath86 can be found efficiently by the bisection method as follows .",
    "restricting the function @xmath6 to a line @xmath93 , the new function @xmath87 is convex in @xmath45 @xcite .",
    "it thus follows that @xmath94 if @xmath95 and @xmath96 if @xmath97 .",
    "given an interval @xmath98 $ ] containing @xmath86 ( the initial value of @xmath99 and @xmath100 is 0 and 1 , respectively ) , set @xmath101 and refine @xmath99 and @xmath100 according to the following rule : @xmath102 the procedure is repeated for finite times until the gap @xmath103 is smaller than a prescribed precision .    * successive line search*. * * if no structure in @xmath6 ( e.g. , convexity ) can be exploited to efficiently compute @xmath89 according to the exact line search ( [ eq : minimization - rule ] ) , the successive line search can instead be employed : given scalars @xmath104 and @xmath105 , the stepsize @xmath89 is set to be @xmath106 , where @xmath107 is the smallest nonnegative integer @xmath108 satisfying the following inequality : @xmath109 note that the existence of a finite @xmath107 satisfying ( [ eq : armijo - rule ] ) is always guaranteed if @xmath84 is a descent direction at @xmath43 and @xmath110 @xcite , i.e. , from proposition [ prop : descent - property ] inequality ( [ eq : armijo - rule ] ) always admits a solution .    the algorithm is formally summarized in algorithm [ alg : successive - approximation - method ] and its convergence properties are given in the following theorem .",
    "* data : * @xmath111 and @xmath112 .    repeat the following steps until convergence :    1 .",
    "compute @xmath65 using ( [ eq : mapping_definition ] ) .",
    "2 .   compute @xmath89 by the exact line search ( [ eq : minimization - rule ] ) or the successive line search ( [ eq : armijo - rule ] ) .",
    "update @xmath77 according to ( [ eq : variable - update ] ) and set @xmath113 .",
    "[ convergence to a stationary point][thm : convergence]consider the sequence @xmath114 generated by algorithm [ alg : successive - approximation - method ] . provided that assumptions ( a1)-(a3 ) as well as the following assumptions are satisfied :    1 .",
    "the solution set @xmath66 is nonempty for @xmath115 ; 2 .   given any convergent subsequence @xmath116 where @xmath117 , the sequence @xmath118 is bounded .",
    "then any limit point of @xmath114 is a stationary point of ( [ eq : original_function ] ) .",
    "see appendix [ appendix : proof - of - theorem ] .    in the following we discuss some properties of the proposed algorithm [ alg : successive - approximation - method ] .",
    "* on the conditions ( a1)-(a5)*. _ _",
    "the only requirement on the convexity of the approximate function @xmath63 is that it is pseudo - convex , cf .",
    "( a1 ) . to the best of our knowledge ,",
    "these are the weakest conditions for descent direction methods available in the literature . as a result",
    ", it enables the construction of new approximate functions that can often be optimized more easily or even in closed - form , resulting in a significant reduction of the computational cost .",
    "assumptions ( a2)-(a3 ) represent standard conditions for successive convex approximation techniques and are satisfied for many existing approximation functions , cf .",
    "[ sec : special - cases ] .",
    "sufficient conditions for assumptions ( a4)-(a5 ) are that either the feasible set @xmath7 in ( [ eq : approximate - problem ] ) is bounded or the approximate function in ( [ eq : approximate - problem ] ) is strongly convex @xcite . we show that how these assumptions are satisfied in popular applications considered in sec .",
    "[ sec : applications ] .",
    "* on the pseudo - convexity of the approximate function*. _",
    "_ assumption ( a1 ) is tight in the sense that if it is not satisfied , proposition [ prop : descent - property ] may not hold .",
    "consider the following simple example : @xmath119 , where @xmath120 and the point @xmath121 at iteration @xmath62 .",
    "choosing the approximate function @xmath122 which is quasi - convex but not pseudo - convex , all assumptions except ( a1 ) are satisfied .",
    "it is easy to see that @xmath123 , however @xmath124 and thus @xmath125 is not a descent direction , i.e. , inequality ( [ eq : descent - direction ] ) in proposition [ prop : descent - property ] is violated .",
    "* on the stepsize .",
    "* the stepsize can be determined in a more straightforward way if @xmath63 is a global upper bound of @xmath6 that is exact at @xmath41 , i.e. , assume that    1 .",
    "@xmath126 and @xmath127 ,    then algorithm [ alg : successive - approximation - method ] converges under the choice @xmath128 which results in the update @xmath129 . to see this , we first remark that @xmath128 must be an optimal point of the following problem : @xmath130 otherwise the optimality of @xmath65 is contradicted , cf .",
    "( [ eq : mapping_definition ] ) . at the same time",
    ", it follows from proposition [ prop : descent - property ] that @xmath131 .",
    "the successive line search over @xmath132 thus yields a nonnegative and finite integer @xmath107 such that for some @xmath104 and @xmath105 : @xmath133 where the second inequality comes from the definition of successive line search [ cf .",
    "( [ eq : armijo - rule ] ) ] and the last equality follows from assumptions ( a3 ) and ( a6 ) .",
    "invoking assumption ( a6 ) again , we obtain @xmath134 the proof of theorem [ thm : convergence ] can be used verbatim to prove the convergence of algorithm [ alg : successive - approximation - method ] with a constant stepsize @xmath128 .      in the following",
    "we show that the proposed algorithm [ alg : successive - approximation - method ] can also be applied to solve problem ( [ eq : original - function - smooth ] ) , and its equivalent formulation ( [ eq : original_function_nonsmooth ] ) which contains a nondifferentiable objective function .",
    "suppose that @xmath63 is an approximate function of @xmath6 in ( [ eq : original - function - smooth ] ) around @xmath43 and it satisfies assumptions ( a1)-(a3 ) .",
    "then the approximation of problem ( [ eq : original - function - smooth ] ) around @xmath135 is @xmath136 that is , we only need to replace the differentiable function @xmath6 by its approximate function @xmath63 . to see this , it is sufficient to verify assumption ( a3 ) only : @xmath137 based on the exact line search , the stepsize @xmath89 in this case is given as @xmath138 where @xmath139",
    ". then the variables @xmath77 and @xmath140 are defined as follows :    [ eq : nonsmooth - update-0 ] @xmath141    the convergence of algorithm [ alg : successive - approximation - method ] with @xmath142 and @xmath89 given by ( [ eq : nonsmooth - approximate - problem])-([eq : nonsmooth - stepsize - exact ] ) directly follows from theorem [ thm : convergence ] .",
    "the point @xmath140 given in ( [ eq : nonsmooth - update-2 ] ) can be further refined : @xmath143 where the first and the second inequality comes from the fact that @xmath139 as well as @xmath144 and jensen s inequality of convex functions @xmath3 @xcite , respectively . since @xmath145 by definition , the point @xmath146 always yields a lower value of @xmath147 than @xmath148 while @xmath146 is still a feasible point for problem ( [ eq : original - function - smooth ] ) .",
    "the update ( [ eq : nonsmooth - update-2 ] ) is then replaced by the following enhanced rule : @xmath149 algorithm [ alg : successive - approximation - method ] with @xmath65 given in ( [ eq : nonsmooth - update-1 ] ) and @xmath140 given in ( [ eq : nonsmooth - update-3 ] ) still converges to a stationary point of ( [ eq : original - function - smooth ] ) .    the notation in ( [ eq : nonsmooth - approximate - problem])-([eq : nonsmooth - stepsize - exact ] ) can be simplified by removing the auxiliary variable @xmath29 : @xmath65 in ( [ eq : nonsmooth - approximate - problem ] ) can be equivalently written as @xmath150    and combining ( [ eq : nonsmooth - stepsize - exact ] ) and ( [ eq : nonsmooth - update-3 ] ) yields @xmath151    in the context of the successive line search , customizing the general definition ( [ eq : armijo - rule ] ) for problem ( [ eq : original_function_nonsmooth ] ) yields the choice @xmath106 with @xmath107 being the smallest integer that satisfies the inequality :    @xmath152    based on the derivations above , the proposed algorithm for the nondifferentiable problem ( [ eq : original_function_nonsmooth ] ) is formally summarized in algorithm [ alg : successive - approximation - method - nonsmooth ] .    * data : * @xmath111 and @xmath112 .",
    "repeat the following steps until convergence :    1 .",
    "compute @xmath65 using ( [ eq : nonsmooth - approximate - problem-1 ] ) .",
    "2 .   compute @xmath89 by the exact line search ( [ eq : nonsmooth - stepsize-1 ] ) or the successive line search ( [ eq : nonsmooth - stepsize - successive-1 ] ) .",
    "3 .   update @xmath77 according to @xmath153 set @xmath113 .",
    "it is much easier to calculate @xmath89 according to ( [ eq : nonsmooth - stepsize-1 ] ) than in state - of - the - art techniques that directly carry out the exact line search over the original nondifferentiable objective function in ( [ eq : original_function_nonsmooth ] ) ( * ? ? ?",
    "* rule e ) , i.e. , @xmath154 this is because the objective function in ( [ eq : nonsmooth - stepsize-1 ] ) is differentiable in @xmath45 while state - of - the - art techniques involve the minimization of a nondifferentiable function . if @xmath6 exhibits a specific structure such as in quadratic functions , @xmath89 can even be calculated in closed - form .",
    "this property will be exploited to develop fast and easily implementable algorithm for the popular lasso problem in sec .",
    "[ sub : lasso ] .    in the proposed successive line search ,",
    "the left hand side of ( [ eq : nonsmooth - stepsize - successive-1 ] ) depends on @xmath6 while the right hand side is linear in @xmath155 .",
    "the proposed variation of the successive line search thus involves only the evaluation of the differentiable function @xmath6 and its computational complexity and signaling exchange ( when implemented in a distributed manner ) is thus lower than state - of - the - art techniques ( for example ( * ? ? ?",
    "* rule a ) , ( * ? ? ?",
    "* equations ( 9)-(10 ) ) , ( * ? ? ?",
    "* remark 4 ) and ( * ? ? ?",
    "* algorithm 2.1 ) ) , in which the whole nondifferentiable function @xmath156 must be repeatedly evaluated ( for different @xmath108 ) and compared with a certain benchmark before @xmath107 is found .      in this subsection",
    ", we interpret some existing methods in the context of algorithm [ alg : successive - approximation - method ] and show that they can be considered as special cases of the proposed algorithm .    *",
    "conditional gradient method * : _ _ in this iterative algorithm for problem ( [ eq : original_function ] ) , the approximate function is given as the first - order approximation of @xmath6 at @xmath41 ( * ? ? ?",
    "2.2.2 ) , i.e. , @xmath157 then the stepsize is selected by either the exact line search or the successive line search .",
    "* gradient projection method * : _ _ in this iterative algorithm for problem ( [ eq : original_function ] ) , @xmath65 is given by ( * ? ? ?",
    "2.3 ) @xmath158_{\\mathcal{x}},\\ ] ] where @xmath159 and @xmath33_{\\mathcal{x}}$ ] denotes the projection of @xmath16 onto @xmath7 .",
    "this is equivalent to defining @xmath63 in ( [ eq : mapping_definition ] ) as follows : @xmath160 which is the first - order approximation of @xmath6 augmented by a quadratic regularization term that is introduced to improve the numerical stability @xcite .",
    "a generalization of ( [ eq : gradient - projection - approximate - function ] ) is to replace the quadratic term by @xmath161 where @xmath162 @xcite .    * proximal gradient method * : if @xmath6 is convex and has a lipschitz continuous gradient with a constant @xmath163 , the proximal gradient method for problem ( [ eq : original_function_nonsmooth ] ) has the following form ( * ? ? ?",
    "4.2 ) : @xmath164 where @xmath159 . in the context of the proposed framework ( [ eq : nonsmooth - approximate - problem-1 ] )",
    ", the update ( [ eq : proximal - gradient - update ] ) is equivalent to defining @xmath63 as follows : @xmath165 and setting the stepsize @xmath128 for all @xmath62 . according to theorem [ thm : convergence ] and the discussion following assumption ( a6 ) , the proposed algorithm converges under a constant unit stepsize if @xmath63 is a global upper bound of @xmath6 , which is indeed the case when @xmath166 in view of the descent lemma ( * ? ? ?",
    "a.24 ) .",
    "* jacobi algorithm * : in problem ( [ eq : original_function ] ) , if @xmath6 is convex in each @xmath12 where @xmath167 ( but not necessarily jointly convex in @xmath168 ) , the approximate function is defined as @xcite @xmath169 where @xmath170 for @xmath167 . the @xmath21-th component function @xmath171 in ( [ eq : jacobi - approximate - function ] ) is obtained from the original function @xmath6 by fixing all variables except @xmath12 , i.e. , @xmath172 , and further adding a quadratic regularization term . since @xmath63 in ( [ eq : jacobi - approximate - function ] ) is convex , assumption ( a1 ) is satisfied . based on the observations that @xmath173 we conclude that assumption ( a3 ) is satisfied by the choice of the approximate function in ( [ eq : jacobi - approximate - function ] ) .",
    "the resulting approximate problem is given by @xmath174 this is commonly known as the jacobi algorithm .",
    "the structure inside the constraint set @xmath7 , if any , may be exploited to solve ( [ eq : jacobi - approximate - problem ] ) even more efficiently .",
    "for example , the constraint set @xmath7 consists of separable constraints in the form of @xmath175 for some convex functions @xmath176 . since subproblem ( [ eq : jacobi - approximate - problem ] ) is convex , primal and dual decomposition techniques can readily be used to solve ( [ eq : jacobi - approximate - problem ] ) efficiently @xcite ( such an example is studied in sec . [ sub : mimo - broadcast - channel ] ) .    to guarantee the convergence ,",
    "the condition proposed in @xcite is that @xmath177 for all @xmath21 in ( [ eq : jacobi - approximate - function ] ) unless @xmath6 is strongly convex in each @xmath12 .",
    "however , the strong convexity of @xmath6 in each @xmath12 is a strong assumption that can not always be satisfied and the additional quadratic regularization term that is otherwise required may destroy the convenient structure that could otherwise be exploited , as we will show through an example application in the mimo bc in sec .",
    "[ sub : mimo - broadcast - channel ] . in the case",
    "@xmath178 , convergence of the jacobi algorithm ( [ eq : jacobi - approximate - problem ] ) is only proved when @xmath6 is jointly convex in @xmath168 and the stepsize is inversely proportional to the number of variables @xmath14 @xcite , namely , @xmath179 . however , the resulting convergence speed is usually slow when @xmath14 is large , as we will later demonstrate numerically in sec .",
    "[ sub : mimo - broadcast - channel ] .    with the technical assumptions specified in theorem [ thm : convergence ] ,",
    "the convergence of the jacobi algorithm with the approximate problem ( [ eq : jacobi - approximate - problem ] ) and successive line search is guaranteed even when @xmath178 .",
    "this is because @xmath63 in ( [ eq : jacobi - approximate - function ] ) is already convex when @xmath178 for all @xmath21 and it naturally satisfies the pseudo - convexity assumption specified by assumption ( a1 ) .    in the case",
    "that the constraint set @xmath7 has a cartesian product structure ( [ eq : original - function - cartesian ] ) , the subproblem ( [ eq : jacobi - approximate - problem ] ) is naturally decomposed into @xmath14 sub - problems , one for each variable , which are then solved _ in parallel_. in this case , the requirement in the convexity of @xmath6 in each @xmath12 can even be relaxed to pseudo - convexity only ( although the sum function @xmath180 is not necessarily pseudo - convex in @xmath16 as pseudo - convexity is not preserved under nonnegative weighted sum operator ) , and this leads to the following update : @xmath181 and    @xmath182    where @xmath183 can be interpreted as variable @xmath12 s best - response to other variables @xmath184 when @xmath172 .",
    "the proposed jacobi algorithm is formally summarized in algorithm [ alg : jacobi ] and its convergence is proved in theorem [ thm : cartesian ] .    *",
    "data : * @xmath111 and @xmath185 for all @xmath167 .    repeat the following steps until convergence :    1 .   for @xmath167 ,",
    "compute @xmath183 using ( [ eq : jacobi - approximate - problem - cartesian ] ) .",
    "2 .   compute @xmath89 by the exact line search ( [ eq : minimization - rule ] ) or the successive line search ( [ eq : armijo - rule ] ) .",
    "update @xmath77 according to @xmath186 set @xmath113 .",
    "[ thm : cartesian]consider the sequence @xmath187 generated by algorithm [ alg : jacobi ] . provided that @xmath6 is pseudo - convex in @xmath12 for all @xmath167 and assumptions ( a4)-(a5 ) are satisfied .",
    "then any limit point of the sequence generated by algorithm [ alg : jacobi ] is a stationary point of ( [ eq : original - function - cartesian ] ) .",
    "see appendix [ sec : appendix - of - theorem - cartesian ] .",
    "the convergence condition specified in theorem [ thm : cartesian ] relaxes those in @xcite : @xmath6 only needs to be pseudo - convex in each variable @xmath12 and no regularization term is needed ( i.e. , @xmath178 ) . to the best of our knowledge ,",
    "this is the weakest convergence condition on jacobi algorithms available in the literature .",
    "we will show in sec .",
    "[ sub : energy - efficiency - maximization ] by an example application of the energy efficiency maximization problem in massive mimo systems how the weak assumption on the approximate function s convexity proposed in theorem [ thm : convergence ] can be exploited to the largest extent . besides this",
    ", the line search usually yields much faster convergence than the fixed stepsize adopted in @xcite even under the same approximate problem , cf .",
    "[ sub : mimo - broadcast - channel ] .    *",
    "dc algorithm * : _ _ if the objective function in ( [ eq : original_function ] ) is the difference of two convex functions @xmath188 and @xmath189 : @xmath190 the following approximate function can be used : @xmath191 since @xmath189 is convex and @xmath192 , assumption ( a6 ) is satisfied and the constant unit stepsize can be chosen @xcite .",
    "in this subsection , we study the mimo bc capacity computation problem to illustrate the advantage of the proposed approximate function .    consider a mimo bc where the channel matrix characterizing the transmission from the base station to user @xmath21 is denoted by @xmath193 , the transmit covariance matrix of the signal from the base station to user @xmath21 is denoted as @xmath194 , and the noise at each user @xmath21 is an additive independent and identically distributed gaussian vector with unit variance on each of its elements .",
    "then the sum capacity of the mimo bc is @xcite @xmath195 where @xmath196 is the power budget at the base station .",
    "problem ( [ eq : mimo - bc ] ) is a convex problem whose solution can not be expressed in closed - form and can only be found iteratively . to apply algorithm [ alg : successive - approximation - method ] , we invoke ( [ eq : jacobi - approximate - function])-([eq : jacobi - approximate - problem ] ) and the approximate problem at the @xmath62-th iteration is @xmath197 where @xmath198 . the approximate function is concave in @xmath199 and differentiable in both @xmath199 and @xmath200 , and thus assumptions ( a1)-(a3 ) are satisfied .",
    "since the constraint set in ( [ eq : mimo - bc - approx ] ) is compact , the approximate problem ( [ eq : mimo - bc - approx ] ) has a solution and assumptions ( a4)-(a5 ) are satisfied .",
    "problem ( [ eq : mimo - bc - approx ] ) is convex and the sum - power constraint coupling @xmath201 is separable , so dual decomposition techniques can be used @xcite . in particular",
    ", the constraint set has a nonempty interior , so strong duality holds and ( [ eq : mimo - bc - approx ] ) can be solved from the dual domain by relaxing the sum - power constraint into the lagrangian @xcite : @xmath202 where @xmath203 and @xmath204 is the optimal lagrange multiplier that satisfies the following conditions : @xmath205 , @xmath206 , @xmath207 , and can be found efficiently using the bisection method .    the problem in ( [ eq : mimo - bc - dual ] ) is uncoupled among different variables @xmath194 in both the objective function and the constraint set , so it can be decomposed into a set of smaller subproblems which are solved in parallel : @xmath203 and @xmath208 and @xmath209 exhibits a closed - form expression based on the waterfilling solution @xcite . thus problem ( [ eq : mimo - bc - approx ] ) also has a closed - form solution up to a lagrange multiplier that can be found efficiently using the bisection method . with the update direction @xmath210",
    ", the base station can implement the exact line search to determine the stepsize using the bisection method described after ( [ eq : minimization - rule ] ) in sec .",
    "[ sec : proposed - method ] .",
    "we remark that when the channel matrices @xmath193 are rank deficient , problem ( [ eq : mimo - bc - approx ] ) is convex but not strongly convex , but the proposed algorithm with the approximate problem ( [ eq : mimo - bc - approx ] ) still converges .",
    "however , if the approximate function in @xcite is used [ cf .",
    "( [ eq : jacobi - approximate - function ] ) ] , an additional quadratic regularization term must be included into ( [ eq : mimo - bc - approx ] ) ( and thus ( [ eq : mimo - bc - dual - decomp ] ) ) to make the approximate problem strongly convex , but the resulting approximate problem no longer exhibits a closed - form solution and thus are much more difficult to solve .    * simulations . *",
    "the parameters are set as follows .",
    "the number of users is @xmath211 and @xmath212 , the number of transmit and receive antenna is ( 5,4 ) , and @xmath213 db . the simulation results are averaged over 20 instances .",
    "we apply algorithm [ alg : successive - approximation - method ] with approximate problem ( [ eq : mimo - bc - approx ] ) and stepsize based on the exact line search , and compare it with the iterative algorithm proposed in @xcite , which uses the same approximate problem ( [ eq : mimo - bc - approx ] ) but with a fixed stepsize @xmath179 ( @xmath14 is the number of users ) .",
    "it is easy to see from figure [ fig : mimo - bc - sum - capacity ] that the proposed method converges very fast ( in less than 10 iterations ) to the sum capacity , while the method of @xcite requires many more iterations .",
    "this is due to the benefit of the exact line search applied in our algorithm over the fixed stepsize which tends to be overly conservative . employing the exact line search",
    "adds complexity as compared to the simple choice of a fixed stepsize , however , since the objective function of ( [ eq : mimo - bc ] ) is concave , the exact line search consists in maximizing a differentiable concave function with a scalar variable , and it can be solved efficiently by the bisection method with affordable cost . more specifically , it takes 0.0023 seconds to solve problem ( [ eq : mimo - bc - approx ] ) and 0.0018 seconds to perform the exact line search ( the software / hardware environment is further specified in sec . [ sub : lasso ] ) .",
    "therefore , the overall cpu time ( time per iteration@xmath214number of iterations ) is still dramatically decreased due to the notable reduction in the number of iterations . besides",
    ", in contrast to the method of @xcite , increasing the number of users @xmath14 does not slow down the convergence , so the proposed algorithm is scalable in large networks .",
    "mimo bc : sum - rate versus the number of iterations . ]",
    "we also compare the proposed algorithm with the iterative algorithm of @xcite , which uses the approximate problem ( [ eq : mimo - bc - approx ] ) but with an additional quadratic regularization term , cf .",
    "( [ eq : jacobi - approximate - function ] ) , where @xmath215 for all @xmath21 , and decreasing stepsizes @xmath216 where @xmath217 is the so - called decreasing rate that controls the rate of decrease in the stepsize .",
    "we can see from figure [ fig : mimo - bc - variable - error ] that the convergence behavior of @xcite is rather sensitive to the decreasing rate @xmath218 .",
    "the choice @xmath217 performs well when the number of transmit and receive antennas is 5 and 4 , respectively , but it is no longer a good choice when the number of transmit and receive antenna increases to 10 and 8 , respectively .",
    "a good decreasing rate @xmath218 is usually dependent on the problem parameters and no general rule performs equally well for all choices of parameters .",
    "we remark once again that the complexity of each iteration of the proposed algorithm is very low because of the existence of a closed - form solution to the approximate problem ( [ eq : mimo - bc - approx ] ) , while the approximate problem proposed in @xcite does not exhibit a closed - form solution and can only be solved iteratively .",
    "specifically , it takes @xmath219 ( version 2.0 @xcite ) 21.1785 seconds ( based on the dual approach ( [ eq : mimo - bc - dual - decomp ] ) where @xmath204 is found by bisection ) .",
    "therefore , the overall complexity per iteration of the proposed algorithm is much lower than that of @xcite .",
    "mimo bc : error @xmath220 versus the number of iterations . ]      in this subsection , we study the energy efficiency maximization problem in massive mimo systems to illustrate the advantage of the relaxed convexity requirement of the approximate function @xmath63 in the proposed iterative optimization approach : according to assumption ( a1 ) , @xmath221 only needs to exhibit the pseudo - convexity property rather than the convexity or strong convexity property that is conventionally required .",
    "consider the massive mimo network with @xmath14 cells and each cell serves one user .",
    "the achievable transmission rate for user @xmath21 in the uplink can be formulated into the following general form : @xmath222 where @xmath223 is the transmission power for user @xmath21 , @xmath224 is the covariance of the additive noise at the receiver of user @xmath21 , while @xmath225 and @xmath226 are positive constants that depend on the channel conditions only .",
    "in particular , @xmath227 accounts for the hardware impairments , and @xmath228 accounts for the interference from other users @xcite .    in 5 g wireless communication networks ,",
    "the energy efficiency is a key performance indicator . to address this issue",
    ", we look for the optimal power allocation that maximizes the energy efficiency : @xmath229 where @xmath230 is a positive constant representing the total circuit power dissipated in the network , @xmath231 and @xmath232 specifies the lower and upper bound constraint , respectively .",
    "problem ( [ eq : ee - problem - formulation ] ) is nonconvex and it is a np - hard problem to find a globally optimal point @xcite",
    ". therefore we aim at finding a stationary point of ( [ eq : ee - problem - formulation ] ) using the proposed algorithm .",
    "to begin with , we propose the following approximate function at @xmath233 : @xmath234 where @xmath235 and @xmath236 note that the approximate function @xmath237 consists of @xmath14 component functions , one for each variable @xmath223 , and the @xmath21-th component function is constructed as follows : since @xmath238 is concave in @xmath223 ( shown in the right column of this page ) but @xmath239 is not concave in @xmath223 ( as a matter of fact , it is convex in @xmath223 ) , the concave function @xmath240 is preserved in @xmath241 in ( [ eq : ee - approximate - function - individual ] ) with @xmath242 fixed to be @xmath243 while the nonconcave functions @xmath244 are linearized w.r.t .",
    "@xmath223 at @xmath233 . in this way",
    ", the partial concavity in the nonconcave function @xmath245 is preserved in @xmath237 .",
    "similarly , since @xmath246 in the denominator is linear in @xmath223 , we only set @xmath247 .",
    "note that the division operator in the original problem ( [ eq : ee - problem - formulation ] ) is kept in the approximate function @xmath237 ( [ eq : ee - approximate - function ] ) . although it will destroy the concavity ( recall that a concave function divided by a linear function is no longer a concave function ) , the pseudo - concavity of @xmath248 is still preserved , as we show in two steps .",
    "step 1 : the function @xmath240 is concave in @xmath223 . for the simplicity of notation , we define two constants @xmath249 and @xmath250 .",
    "the first - order derivative and second - order derivative of @xmath240 w.r.t .",
    "@xmath223 are @xmath251 since @xmath252 when @xmath253 , @xmath240 is a concave function of @xmath223 in the nonnegative orthant @xmath254 @xcite .    step 2 : given the concavity of @xmath240 , the function@xmath241 is concave in @xmath223 .",
    "since the denominator function of @xmath255 is a linear ( and thus convex ) function of @xmath223 , it follows from ( * ? ? ?",
    "* lemma 3.8 ) that @xmath248 is pseudo - concave .",
    "@xmath256_{\\underline{p}_{k}}^{\\overline{p}_{k}},\\label{eq : ee - closed - form-3}\\ ] ]    then we verify that the gradient of the approximate function and that of the original objective function are identical at @xmath233 .",
    "it follows that @xmath257 therefore assumption ( a3 ) in theorem [ thm : cartesian ] is satisfied .",
    "assumption ( a2 ) is also satisfied because both @xmath241 and @xmath258 are continuously differentiable for any @xmath259 .    given the approximate function ( [ eq : ee - approximate - function ] ) , the approximate problem in iteration @xmath62 is thus @xmath260 assumptions ( a4 ) and ( a5 ) can be proved to hold in a similar procedure shown in the previous example application . since the objective function in ( [ eq : ee - problem - formulation ] ) is nonconcave , it may not be computationally affordable to perform the exact line search . instead , the successive line search can be applied to calculate the stepsize . as a result",
    ", the convergence of the proposed algorithm with approximate problem ( [ eq : ee - approximate - problem - overall ] ) and successive line search follows from the same line of analysis used in the proof of theorem [ thm : cartesian ] .    the optimization problem in ( [ eq : ee - approximate - problem - overall ] )",
    "can be decomposed into independent subproblems ( [ eq : ee - approximate - problem - decomposition ] ) that can be solved in parallel : @xmath261 where @xmath262 . as we have just shown ,",
    "the numerator function and the denominator function in ( [ eq : ee - approximate - problem - decomposition ] ) is concave and linear , respectively , so the optimization problem in ( [ eq : ee - approximate - problem - decomposition ] ) is a fractional programming problem and can be solved by the dinkelbach s algorithm ( * ? ? ?",
    "* algorithm 5 ) : given @xmath263 ( @xmath264 can be set to 0 ) , the following optimization problem in iteration @xmath265 is solved :    [ eq : ee - closed - form ] @xmath266 where @xmath241 is the numerator function in ( [ eq : ee - approximate - problem - decomposition ] ) .",
    "the variable @xmath263 is then updated in iteration @xmath265 as @xmath267    it follows from the convergence properties of the dinkelbach s algorithm that @xmath268 at a superlinear convergence rate .",
    "note that problem ( [ eq : ee - closed - form-1 ] ) can be solved in closed - form , as @xmath269 is simply the projection of the point that sets the gradient of the objective function in ( [ eq : ee - closed - form-1 ] ) to zero onto the interval @xmath270 $ ] .",
    "it can be verified that finding that point is equivalent to finding the root of a polynomial with order 2 and it thus admits a closed - form expression .",
    "we omit the detailed derivations and directly give the expression of @xmath269 in ( [ eq : ee - closed - form-3 ] ) at the top of this page , where @xmath271 and @xmath272 .",
    "we finally remark that the approximate function in ( [ eq : ee - approximate - function ] ) is constructed in the same spirit as @xcite by keeping as much concavity as possible , namely , @xmath238 in the numerator and @xmath273 in the denominator , and linearizing the nonconcave functions only , namely , @xmath274 in the numerator .",
    "besides this , the denominator function is also kept . therefore , the proposed algorithm is of a best - response nature and expected to converge faster than gradient based algorithms which linearizes the objective function @xmath275 in ( [ eq : ee - problem - formulation ] ) completely .",
    "however , the convergence of the proposed algorithm with the approximate problem given in ( [ eq : ee - approximate - problem - overall ] ) can not be derived from existing works , since the approximate function presents only a weak form of convexity , namely , the pseudo - convexity , which is much weaker than those required in state - of - the - art convergence analysis , e.g. , uniform strong convexity in @xcite .",
    "* simulations . *",
    "the number of antennas at the bs in each cell is @xmath276 , and the channel from user @xmath277 to cell @xmath21 is @xmath278 .",
    "we assume a similar setup as @xcite : @xmath279 , @xmath280 for @xmath281 and @xmath282 , where @xmath283 is the error magnitude of hardware impairments at the bs and @xmath284 .",
    "the noise covariance @xmath285 , and the hardware dissipated power @xmath286 is 10dbm , while @xmath287 is -10dbm and @xmath288 is 10dbm for all users .",
    "the benchmark algorithm is ( * ? ? ?",
    "* algorithm 1 ) , which successively maximizes the following lower bound function of the objective function in ( [ eq : ee - problem - formulation ] ) , which is tight at @xmath233 :    @xmath289    where @xmath290 and @xmath291 denote the optimal variable of ( [ eq : ee - jorswieck - approximateproblem ] ) as @xmath292 ( which can be found by the dinkelbach s algorithm ) ; then the variable @xmath293 is updated as @xmath294 for all @xmath167",
    ". we thus coin ( * ? ? ?",
    "* algorithm 1 ) as the successive lower bound maximization ( slbm ) method .    in figure",
    "[ fig : ee - convergence ] , we compare the convergence behavior of the proposed method and the slbm method in terms of both the number of iterations ( the upper subplots ) and the cpu time ( the lower subplots ) , for two different number of users : @xmath295 in figure [ fig : ee - convergence ] ( a ) and @xmath296 in figure [ fig : ee - convergence ] ( b ) .",
    "it is obvious that the convergence speed of the proposed algorithm in terms of the number of iterations is comparable to that of the slbm method .",
    "however , we remark that the approximate problem ( [ eq : ee - approximate - problem - overall ] ) of the proposed algorithm is superior to that of the slbm method in the following aspects :    firstly , the approximate problem of the proposed algorithm consists of independent subproblems that can be solved in parallel , cf .",
    "( [ eq : ee - approximate - problem - decomposition ] ) , while each subproblem has a closed - form solution , cf .",
    "( [ eq : ee - closed - form])-([eq : ee - closed - form-3 ] ) . however , the optimization variable in the approximate problem of the slbm method ( [ eq : ee - jorswieck - approximateproblem ] ) is a vector @xmath297 and the approximate problem can only be solved by a general purpose solver .    in the simulations , we use the matlab optimization toolbox to solve ( [ eq : ee - jorswieck - approximateproblem ] ) and the iterative update specified in ( [ eq : ee - closed - form])-([eq : ee - closed - form-3 ] ) to solve ( [ eq : ee - approximate - problem - overall ] ) , where the stopping criterion for ( [ eq : ee - closed - form ] ) is @xmath298 . the upper subplots in figure [ fig : ee - convergence ]",
    "show that the numbers of iterations required for convergence are approximately the same for the slbm method when @xmath295 in figure [ fig : ee - convergence ] ( a ) and when @xmath296 in figure [ fig : ee - convergence ] ( b ) .",
    "however , we see from the lower subplots in figure [ fig : ee - convergence ] that the cpu time of each iteration of the slbm method is dramatically increased when @xmath14 is increased from 10 to 50 . on the other hand , the cpu time of the proposed algorithm is not notably changed because the operations are parallelizable s into the vector form @xmath299 we can see that only element wise operations between vectors and matrix vector multiplications are involved . the simulations on which figure [ fig : ee - convergence ] are based are not performed in a real parallel computing environment with @xmath14 processors , but only make use of the efficient linear algebraic implementations available in matlab which already implicitly admits a certain level of parallelism . ] and the required cpu time is thus not affected by the problem size .    secondly , since a variable substitution @xmath300 is adopted in the slbm method ( we refer to @xcite for more details ) , the lower bound constraint @xmath301 ( which corresponds to @xmath302 ) can not be handled by the slbm method numerically . this limitation impairs the applicability of the slbm method in many practical scenarios .      in this subsection",
    ", we study the lasso problem to illustrate the advantage of the proposed line search method for nondifferentiable optimization problems .",
    "lasso is an important and widely studied problem in sparse signal recovery @xcite : @xmath303 where @xmath304 ( with @xmath305 ) , @xmath306 and @xmath307 are given parameters .",
    "problem ( [ eq : lasso ] ) is an instance of the general problem structure defined in ( [ eq : original_function_nonsmooth ] ) with the following decomposition : @xmath308    problem ( [ eq : lasso ] ) is convex , but its objective function is nondifferentiable and it does not have a closed - form solution . to apply algorithm [ alg : successive - approximation - method - nonsmooth ] , the scalar decomposition @xmath22 is adopted . recalling ( [ eq : nonsmooth - approximate - problem-1 ] ) and ( [ eq : jacobi - approximate - function ] ) , the approximate problem is @xmath309 note that @xmath3 can be decomposed among different components of @xmath16 , i.e. , @xmath310 , so the vector problem ( [ eq : lasso - approximate ] ) reduces to @xmath14 independent scalar subproblems that can be solved in parallel : @xmath311 where @xmath312 is the @xmath21-th diagonal element of @xmath313 , @xmath314^{+}-\\left[\\mathbf{-b - a}\\right]^{+}$ ] is the so - called soft - thresholding operator @xcite and @xmath315 or more compactly : @xmath316 thus the update direction exhibits a closed - form expression .",
    "the stepsize based on the proposed exact line search ( [ eq : nonsmooth - stepsize-1 ] ) is @xmath317_{0}^{1}.\\label{eq : lasso - approximate - stepsize}\\end{aligned}\\ ] ] the exact line search consists in solving a convex quadratic optimization problem with a scalar variable and a bound constraint , so the problem exhibits a closed - form solution ( [ eq : lasso - approximate - stepsize ] ) .",
    "therefore , both the update direction and stepsize can be calculated in closed - form .",
    "we name the proposed update ( [ eq : lasso - approximate - problem])-([eq : lasso - approximate - stepsize ] ) as soft - thresholding with exact line search algorithm ( @xmath318 ) .    the proposed update ( [ eq : lasso - approximate - problem])-([eq : lasso - approximate - stepsize ] ) has several desirable features that make it appealing in practice .",
    "firstly , in each iteration , all elements are updated in parallel based on the nonlinear best - response ( [ eq : lasso - approximate - problem ] ) .",
    "this is in the same spirit as @xcite and the convergence speed is generally faster than bcd @xcite or the gradient - based update @xcite .",
    "secondly , the proposed exact line search ( [ eq : lasso - approximate - stepsize ] ) not only yields notable progress in each iteration but also enjoys an easy implementation given the closed - form expression .",
    "the convergence speed is thus further enhanced as compared to the procedures proposed in @xcite where either decreasing stepsizes are used @xcite or the line search is over the original nondifferentiable objective function in ( [ eq : lasso ] ) @xcite : @xmath319    * computational complexity*. the computational overhead associated with the proposed exact line search ( [ eq : lasso - approximate - stepsize ] ) can significantly be reduced if ( [ eq : lasso - approximate - stepsize ] ) is carefully implemented as outlined in the following .",
    "the most complex operation in ( [ eq : lasso - approximate - stepsize ] ) is the matrix - vector multiplication , namely , @xmath320 in the numerator and @xmath321 in the denominator . on the one hand ,",
    "the term @xmath320 is already available from @xmath322 , which is computed in order to determine the best - response in ( [ eq : lasso - approximate - problem ] ) . on the other hand ,",
    "the matrix - vector multiplication @xmath321 is also required for the computation of @xmath323 as it can alternatively be computed as : @xmath324 where then only an additional vector addition is involved . as a result",
    ", the stepsize ( [ eq : lasso - approximate - stepsize ] ) does not incur any additional matrix - vector multiplications , but only affordable vector - vector multiplications .",
    "operation flow and signaling exchange between local processor @xmath325 and the central processor .",
    "a solid line indicates the computation that is locally performed by the central / local processor , and a solid line with an arrow indicates signaling exchange between the central and local processor and the direction of the signaling exchange . ]    * signaling exchange*. when @xmath326 can not be stored and processed by a centralized processing unit , a parallel architecture can be employed .",
    "assume there are @xmath327 ( @xmath328 ) processors .",
    "we label the first @xmath196 processors as local processors and the last one as the central processor , and partition @xmath326 as @xmath329,\\ ] ] where @xmath330 and @xmath331 .",
    "matrix @xmath332 is stored and processed in the local processor @xmath325 , and the following computations are decomposed among the local processors :    [ eq : decompose ] @xmath333    where @xmath334 .",
    "the central processor computes the best - response @xmath65 in ( [ eq : lasso - approximate - problem ] ) and the stepsize @xmath89 in ( [ eq : lasso - approximate - stepsize ] ) .",
    "the decomposition in ( [ eq : decompose ] ) enables us to analyze the signaling exchange between local processor @xmath325 and the central processor involved in ( [ eq : lasso - approximate - problem ] ) and ( [ eq : lasso - approximate - stepsize ] ) ) and ( [ eq : lasso - approximate - stepsize ] ) can also be implemented by a parallel architecture without a central processor . in this case , the signaling is exchanged mutually between every two of the local processors , but the analysis is similar and the conclusion to be drawn remains same : the proposed exact line search ( [ eq : lasso - approximate - stepsize ] ) does not incur additional signaling compared with predetermined stepsizes . ] .    the signaling exchange is summarized in figure [ fig : lasso - signaling - exchange ] .",
    "firstly , the central processor sends @xmath320 to the local processors ( * s1.1 * ) is set to @xmath335 , so @xmath336 . ] , and each local processor @xmath325 for @xmath337 first computes @xmath338 and then sends it back to the central processor ( * s1.2 * ) , which forms @xmath339 ( * s1.3 * ) as in ( [ eq : decompose-2 ] ) and calculates @xmath322 as in ( [ eq : r(x ) ] ) ( * s1.4 * ) and then @xmath65 as in ( [ eq : lasso - approximate - problem ] ) ( * s1.5 * ) .",
    "then the central processor sends @xmath340 to the local processor @xmath325 for @xmath337 ( * s2.1 * ) , and each local processor first computes @xmath341 and then sends it back to the central processor ( * s2.2 * ) , which forms @xmath321 ( * s2.3 * ) as in ( [ eq : decompose-1 ] ) , calculates @xmath89 as in ( [ eq : lasso - approximate - stepsize ] ) ( * s2.4 * ) , and updates @xmath77 ( * s3.1 * ) and @xmath323 ( * s3.2 * ) according to ( [ eq : s_(t+1 ) ] ) . from figure",
    "[ fig : lasso - signaling - exchange ] we observe that the exact line search does _ not _ incur any additional signaling compared with that of predetermined stepsizes ( e.g. , constant and decreasing stepsize ) , because the signaling exchange in * s2.1-s2.2 * has also to be carried out in the computation of @xmath323 in * s3.2 * , cf .",
    "( [ eq : s_(t+1 ) ] ) .",
    "we finally remark that the proposed successive line search can also be applied and it exhibits a closed - form expression as well .",
    "however , since the exact line search yields faster convergence , we omit the details at this point .    convergence of @xmath318 ( proposed ) and @xmath342 ( state - of - the - art ) for lasso : error versus the number of iterations . ]    * simulations*. * * we first compare in figure [ fig : lasso_decreasing_fails ] the proposed algorithm @xmath318 with @xmath342 @xcite in terms of the error criterion @xmath343 defined as : @xmath344_{-\\mu\\mathbf{1}}^{\\mu\\mathbf{1}}\\bigr\\vert_{2}.\\label{eq : lasso - error}\\ ] ] note that @xmath345 is a solution of ( [ eq : lasso ] ) if and only if @xmath346 @xcite .",
    "@xmath342 is implemented as outlined in @xcite ; however , the selective update scheme @xcite is not implemented in @xmath342 because it is also applicable for @xmath318 and it can not eliminate the slow convergence and sensitivity of the decreasing stepsize .",
    "we also remark that the stepsize rule for @xmath342 is @xmath347 @xcite , where @xmath218 is the decreasing rate and @xmath348 .",
    "the code and the data generating the figure can be downloaded online @xcite .    note that the error @xmath343 plotted in figure [ fig : lasso_decreasing_fails ] does not necessarily decrease monotonically while the objective function @xmath349 always does .",
    "this is because @xmath318 and @xmath342 are descent direction methods . for @xmath342 ,",
    "when the decreasing rate is low ( @xmath350 ) , no improvement is observed after 100 iterations . as a matter of fact",
    ", the stepsize in those iterations is so large that the function value is actually dramatically increased , and thus the associated iterations are discarded in figure [ fig : lasso_decreasing_fails ] .",
    "a similar behavior is also observed for @xmath351 , until the stepsize becomes sufficiently small . when the stepsize is quickly decreasing ( @xmath352 ) ,",
    "although improvement is made in all iterations , the asymptotic convergence speed is slow because the stepsize is too small to make notable improvement . for this example",
    ", the choice @xmath353 performs well , but the value of a good decreasing rate depends on the parameter setup ( e.g. , @xmath326 , @xmath354 and @xmath355 ) and no general rule performs equally well for all choices of parameters . by comparison , the proposed algorithm @xmath318 is fast to converge and exhibits stable performance without requiring any parameter tuning .",
    "we also compare in figure [ fig : lasso_other_algorithms ] the proposed algorithm @xmath318 with other competitive algorithms in literature : @xmath356 @xcite , @xmath357 @xcite , @xmath358 @xcite and @xmath359 @xcite .",
    "we simulated @xmath358 of @xcite because it exhibits guaranteed convergence .",
    "the dimension of @xmath326 is @xmath360 ( the left column of figure [ fig : lasso_other_algorithms ] ) and @xmath361 ( the right column ) .",
    "it is generated by the matlab command @xmath362 with each row being normalized to unity .",
    "the density ( the proportion of nonzero elements ) of the sparse vector @xmath363 is 0.1 ( the upper row of figure [ fig : lasso_other_algorithms ] ) , 0.2 ( the middle row ) and 0.4 ( the lower row ) . the vector @xmath354 is generated as @xmath364 where @xmath365 is drawn from an i.i.d . gaussian distribution with variance @xmath366 .",
    "the regularization gain @xmath355 is set to @xmath367 , which allows @xmath363 to be recovered to a high accuracy @xcite .",
    "the simulations are carried out under matlab r2012a on a pc equipped with an operating system of windows 7 64-bit home premium edition , an intel i5 - 3210 2.50ghz cpu , and a 8 gb ram .",
    "all of the matlab codes are available online @xcite .",
    "the comparison is made in terms of cpu time that is required until either a given error bound @xmath368 is reached or the maximum number of iterations , namely , 2000 , is reached .",
    "the running time consists of both the initialization stage required for preprocessing ( represented by a flat curve ) and the formal stage in which the iterations are carried out . for example , in the proposed algorithm @xmath318 , @xmath369 is computed , so matrix - matrix multiplication between @xmath370 and @xmath326 is not required .",
    "] in the initialization stage since it is required in the iterative variable update in the formal stage , cf .",
    "( [ eq : lasso - approximate - problem ] ) .",
    "the simulation results are averaged over 20 instances .",
    "we observe from figure [ fig : lasso_other_algorithms ] that the proposed algorithm @xmath318 converges faster than all competing algorithms .",
    "some further observations are in order .",
    "time versus error of different algorithms for lasso . in the left and",
    "right column , the dimension of @xmath326 is @xmath360 and @xmath361 , respectively . in the higher , middle and lower column ,",
    "the density of @xmath363 is 0.1 , 0.2 and 0.4 . ]",
    "@xmath371 the proposed algorithm @xmath318 is not sensitive to the density of the true signal @xmath363 . when the density is increased from 0.1 ( left column ) to 0.2 ( middle column ) and then to 0.4 ( right column ) , the cpu time increases negligibly .",
    "@xmath371 the proposed algorithm @xmath318 scales relatively well with the problem dimension .",
    "when the dimension of @xmath326 is increased from @xmath360 ( the left column ) to @xmath361 ( the right column ) , the cpu time is only marginally increased .",
    "@xmath371 the initialization stage of @xmath357 is time consuming because of some expensive matrix operations as , e.g. , @xmath372 , @xmath373 and @xmath374 ( @xmath375 is a given positive constant ) .",
    "more details can be found in ( * ? ? ?",
    "furthermore , the cpu time of the initialization stage of @xmath357 is increased dramatically when the dimension of @xmath326 is increased from @xmath360 to @xmath361 .",
    "@xmath371 @xmath359 performs better when the density of @xmath363 is smaller , e.g. , 0.1 , than in the case when it is large , e.g. , 0.2 and 0.4 .",
    "@xmath371 the asymptotic convergence speed of @xmath358 is slow , because only one variable is updated in each iteration .    to further evaluate the performance of the proposed algorithm @xmath318",
    ", we test it on the benchmarking platform developed by the optimization group from the department of mathematics at the darmstadt university of technology and compare it with different algorithms in various setups ( data set , problem dimension , etc . ) for the basis pursuit ( bp ) problem @xcite : @xmath376 to adapt @xmath318for the bp problem , we use the augmented lagrangian approach @xcite : @xmath377 where @xmath378 ( @xmath379 ) , @xmath77 is computed by @xmath318 and this process is repeated until @xmath380 converges .",
    "the numerical results summarized in @xcite show that , although @xmath318 must be called multiple times before the lagrange multiplier @xmath381 converges , the proposed algorithm for bp based on @xmath318 is very competitive in terms of running time and robust in the sense that it solved all problem instances in the test platform database .",
    "in this paper , we have proposed a novel iterative algorithm based on convex approximation .",
    "the most critical requirement on the approximate function is that it is pseudo - convex . on the one hand ,",
    "the relaxation of the assumptions on the approximate functions can make the approximate problems much easier to solve .",
    "we show by a counter - example that the assumption on pseudo - convexity is tight in the sense that when it is violated , the algorithm may not converge . on the another hand , the stepsize based on the exact / successive line search yields notable progress in each iteration .",
    "additional structures can be exploited to assist with the selection of the stepsize , so that the algorithm can be further accelerated .",
    "the advantages and benefits of the proposed algorithm have been demonstrated using prominent applications in communication networks and signal processing , and they are also numerically consolidated .",
    "the proposed algorithm can readily be applied to solve other problems as well , such as portfolio optimization @xcite .",
    "\\i ) firstly , suppose @xmath29 is a stationary point of ( [ eq : original_function ] ) ; it satisfies the first - order optimality condition : @xmath382 using assumption ( a3 ) , we get @xmath383 since @xmath384 is pseudo - convex , the above condition implies @xmath385 that is , @xmath386 and @xmath74 .    secondly ,",
    "suppose @xmath74 .",
    "we readily get @xmath387 where the equality and inequality comes from assumption ( a3 ) and the first - order optimality condition , respectively , so @xmath29 is a stationary point of ( [ eq : original_function ] ) .",
    "\\ii ) from the definition of @xmath72 , it is either    @xmath388    or @xmath389",
    "if ( [ eq : possibility-1 ] ) is true , then @xmath74 and , as we have just shown , it is a stationary point of ( [ eq : original_function ] ) .",
    "so only ( [ eq : possibility-2 ] ) can be true . we know from the pseudo - convexity of @xmath68 in @xmath16 ( cf .",
    "assumption ( a1 ) ) and ( [ eq : possibility-2 ] ) that @xmath390 and @xmath391 where the equality comes from assumption ( a3 ) .",
    "since @xmath65 is the optimal point of ( [ eq : approximate - problem ] ) , it satisfies the first - order optimality condition : @xmath392    if ( [ eq : possibility-1 ] ) is true , then @xmath393 and it is a stationary point of ( [ eq : original_function ] ) according to proposition [ prop : descent - property ] ( i ) .",
    "besides , it follows from ( [ eq : minimum - principle-1 ] ) ( with @xmath394 and @xmath395 ) that @xmath396 .",
    "note that equality is actually achieved , i.e. , @xmath397 because otherwise @xmath84 would be an ascent direction of @xmath63 at @xmath41 and the definition of @xmath65 would be contradicted .",
    "then from the definition of the successive line search , we can readily infer that @xmath398 it is easy to see ( [ eq : decreasing-1 ] ) holds for the exact line search as well .",
    "if ( [ eq : possibility-2 ] ) is true , @xmath43 is not a stationary point and @xmath84 is a strict descent direction of @xmath6 at @xmath41 according to proposition [ prop : descent - property ] ( ii ) : @xmath6 is strictly decreased compared with @xmath399 if @xmath16 is updated at @xmath43 along the direction @xmath84 . from the definition of the successive line search , there always exists a @xmath89 such that @xmath400 and @xmath401 this strict decreasing property also holds for the exact line search because it is the stepsize that yields the largest decrease , which is always larger than or equal to that of the successive line search .    we know from ( [ eq :",
    "decreasing-1 ] ) and ( [ eq : decreasing-2 ] ) that @xmath402 is a monotonically decreasing sequence and it thus converges .",
    "besides , for any two ( possibly different ) convergent subsequences @xmath403 and @xmath404 , the following holds : @xmath405 since @xmath6 is a continuous function , we infer from the preceding equation that @xmath406    now consider any convergent subsequence @xmath407 with limit point @xmath29 , i.e. , @xmath408 . to show that @xmath29 is a stationary point , we first assume the contrary : @xmath29 is not a stationary point . since @xmath63 is continuous in both @xmath16 and @xmath43 by assumption ( a2 ) and @xmath409 is bounded by assumption ( a5 ) , it follows from ( * ? ? ?",
    "1 ) that there exists a sequence @xmath410 with @xmath411 such that it converges and @xmath412 . since both @xmath6 and @xmath413 are continuous , applying ( * ? ? ? * th .",
    "1 ) again implies there is a @xmath414 such that @xmath415 and @xmath416 converges to @xmath417 defined as : @xmath418 where @xmath419 is the stepsize when either the exact or successive line search is applied to @xmath420 along the direction @xmath75 . since @xmath29 is not a stationary point , it follows from ( [ eq : decreasing-2 ] ) that @xmath421 , but this would contradict ( [ eq : value - convergence ] ) . therefore @xmath29 is a stationary point , and the proof is completed .",
    "we first need to show that proposition [ prop : descent - property ] still holds .",
    "suppose @xmath29 is a stationary point of ( [ eq : original - function - cartesian ] ) , it satisfies the first - order optimality condition : @xmath423 and it is equivalent to @xmath424 since @xmath6 is pseudo - convex in @xmath12 , the above condition implies @xmath425 for all @xmath167 .",
    "suppose @xmath422 for all @xmath167 .",
    "the first - order optimality conditions yields @xmath424 adding the above inequality for all @xmath167 yields @xmath426 therefore , @xmath29 is a stationary point of ( [ eq : original - function - cartesian ] ) .",
    "firstly , there must exist an index @xmath277 such that @xmath431 otherwise @xmath29 would be a stationary point of ( [ eq : original - function - cartesian ] ) .",
    "since @xmath6 is pseudo - convex in @xmath12 for @xmath167 , it follows from ( [ eq : cartesian - proof-2 ] ) that @xmath432    secondly , for any index @xmath21 such that @xmath433 , @xmath434 minimizes @xmath435 over @xmath436 and @xmath437 for any @xmath438 .",
    "setting @xmath439 yields @xmath440 similarly , setting @xmath441 in ( [ eq : cartesian - proof-1 ] ) yields @xmath442 adding ( [ eq : cartesian - proof-4 ] ) and ( [ eq : cartesian - proof-5 ] ) , we can infer that @xmath443 .",
    "therefore , we can rewrite ( [ eq : cartesian - proof-5 ] ) as follows @xmath444 and thus @xmath445 adding ( [ eq : cartesian - proof-3 ] ) and ( [ eq : cartesian - proof-6 ] ) over all @xmath167 yields @xmath446 that is , @xmath75 is a descent direction of @xmath6 at the point @xmath29 .",
    "the proof of theorem [ thm : convergence ] can then be used verbatim to prove the convergence of the algorithm with the approximate problem ( [ eq : jacobi - approximate - problem - cartesian ] ) and the exact / successive line search .",
    "n.  jindal , w.  rhee , s.  vishwanath , s.  jafar , and a.  goldsmith , `` sum power iterative water - filling for multi - antenna gaussian broadcast channels , '' _ ieee transactions on information theory",
    "_ , vol .",
    "51 , no .  4 , pp . 15701580 , apr .",
    "2005 .",
    "q.  shi , m.  razaviyayn , z .- q .",
    "luo , and c.  he , `` an iteratively weighted mmse approach to distributed sum - utility maximization for a mimo interfering broadcast channel , '' _ ieee transactions on signal processing _ ,",
    "59 , no .  9 , pp .",
    "43314340 , sep .",
    "g.  scutari , f.  facchinei , p.  song , d.  p. palomar , and j .- s .",
    "pang , `` decomposition by partial linearization : parallel optimization of multi - agent systems , '' _ ieee transactions on signal processing _",
    "62 , no .  3 , pp . 641656 ,",
    "y.  yang , g.  scutari , p.  song , and d.  p. palomar , `` robust mimo cognitive radio systems under interference temperature constraints , '' _ ieee journal on selected areas in communications _ ,",
    "31 , no .",
    "24652482 , nov .",
    "2013 .",
    "kim , k.  koh , m.  lustig , s.  boyd , and d.  gorinevsky , `` an interior - point method for large - scale @xmath447-regularized least squares , '' _ ieee journal of selected topics in signal processing _ ,",
    "vol .  1 , no .  4 , pp .",
    "606617 , dec . 2007 .",
    "s.  boyd , n.  parikh , e.  chu , b.  peleato , and j.  eckstein , `` distributed optimization and statistical learning via the alternating direction method of multipliers , '' _ foundations and trends in machine learning _ , vol .  3 , no .  1 , 2010 .        p.  l. combettes and j .- c .",
    "pesquet , `` proximal splitting methods in signal processing , '' in _ fixed - point algorithms for inverse problems in science and engineering _ , ser .",
    "springer optimization and its applications , h.  h. bauschke , r.  s. burachik , p.  l. combettes , v.  elser , d.  r. luke , and h.  wolkowicz , eds.1em plus 0.5em minus 0.4emnew york , ny : springer new york , 2011 , vol .",
    "185212 .",
    "m.  razaviyayn , m.  hong , and z .- q .",
    "luo , `` a unified convergence analysis of block successive minimization methods for nonsmooth optimization , '' _ siam journal on optimization _ , vol .",
    "23 , no .  2 ,",
    "pp . 11261153 , 2013 .",
    "g.  scutari , f.  facchinei , l.  lampariello , , and p.  song , `` distributed methods for constrained nonconvex multi - agent optimization - part i : theory , '' oct .",
    "2014 , submitted to _ ieee transactions on signal processing_. [ online ] .",
    "available : http://arxiv.org/abs/1410.4754            s.  m. robinson and r.  h. day , `` a sufficient condition for continuity of optimal sets in mathematical programming , '' _ journal of mathematical analysis and applications _ , vol .  45 , no .  2 , pp . 506511 , feb .",
    "1974 .",
    "r.  h. byrd , j.  nocedal , and f.  oztoprak , `` an inexact successive quadratic approximation method for convex l-1 regularized optimization , '' sep .",
    "[ online ] .",
    "available : http://arxiv.org/abs/1309.3529            a.  zappone , l.  sanguinetti , g.  bacci , e.  jorswieck , and m.  debbah , `` energy - efficient power control : a look at 5 g wireless technologies , '' _ ieee transactions on signal processing _",
    "64 , no .",
    "7 , pp . 16681683 , april 2016 .",
    "c.  olsson , a.  p. eriksson , and f.  kahl , `` efficient optimization for @xmath448-problems using pseudoconvexity , '' in _ 2007 ieee 11th international conference on computer vision_.1em plus 0.5em minus 0.4emieee , 2007 , pp .",
    "y.  yang , g.  scutari , d.  p. palomar , and m.  pesavento , `` a parallel decomposition method for nonconvex stochastic multi - agent optimization problems , '' dec .",
    "2015 , to appear in _ ieee transactions on signal processing_.    r.  tibshirani , `` regression shrinkage and selection via the lasso : a retrospective , '' _ journal of the royal statistical society : series b ( statistical methodology ) _ , vol .",
    "58 , no .  1 ,",
    "267288 , jun . 1996 .",
    "m.  a.  t. figueiredo , r.  d. nowak , and s.  j. wright , `` gradient projection for sparse reconstruction : application to compressed sensing and other inverse problems , '' _ ieee journal of selected topics in signal processing _ ,",
    "vol .  1 , no .  4 , pp .",
    "586597 , dec . 2007 ."
  ],
  "abstract_text": [
    "<S> in this paper , we propose a successive pseudo - convex approximation algorithm to efficiently compute stationary points for a large class of possibly nonconvex optimization problems . </S>",
    "<S> the stationary points are obtained by solving a sequence of successively refined approximate problems , each of which is much easier to solve than the original problem . to achieve convergence , the approximate problem only needs to exhibit a weak form of convexity , namely , pseudo - convexity . </S>",
    "<S> we show that the proposed framework not only includes as special cases a number of existing methods , for example , the gradient method and the jacobi algorithm , but also leads to new algorithms which enjoy easier implementation and faster convergence speed . </S>",
    "<S> we also propose a novel line search method for nondifferentiable optimization problems , which is carried out over a properly constructed differentiable function with the benefit of a simplified implementation as compared to state - of - the - art line search techniques that directly operate on the original nondifferentiable objective function . </S>",
    "<S> the advantages of the proposed algorithm are shown , both theoretically and numerically , by several example applications , namely , mimo broadcast channel capacity computation , energy efficiency maximization in massive mimo systems and lasso in sparse signal recovery .    energy efficiency , exact line search , lasso , massive mimo , mimo broadcast channel , nonconvex optimization , nondifferentiable optimization , successive convex approximation . </S>"
  ]
}