{
  "article_text": [
    "decoding plays a very important role in modern data communications . the best known decoding algorithm for turbo codes and ldpc codes",
    "is called the sum - product algorithm  @xcite .",
    "it is a message passing algorithm operating in a general graph . to the surprise of many mathematicians",
    ", we have little theoretical understanding of its principles despite of its effectiveness .",
    "although it has demonstrated satisfying performances in decoding and solving many other optimization problems , it may also give poor results or fail to converge .",
    "this paper presents the application of cooperative optimization in decoding ldpc codes .",
    "it is a new optimization principle completely unknown to the mathematics and engineering societies before .",
    "similar to the sum - product algorithm , the cooperative algorithm also employs message passing operated in a general graph . unlike the sum - product algorithm , its computational properties are better understood . while many classic methods struggled with local minima , our method always has a unique equilibrium and converges to it with an exponential rate regardless of initial conditions .",
    "it can determine if the equilibrium is a global optimum or not .",
    "in many important cases , it guarantees to find the global optima for difficult optimization problems when conventional methods often fail .",
    "theories for optimization have been studied for centuries .",
    "they caught special attention after the invention of computers because of their importance in solving many practical problems with the use of computers . yet in the past many effective optimization methods are found not by applying the known optimization theories . instead they are empirical ones discovered with some threads of chances , just like the sum - product algorithm for decoding turbo codes and ldpc codes used in data communications .",
    "this crucial realization demands us to discover new principles for optimization and build new theories for them .",
    "we can always expect better results through deeper theoretical understanding beyond discovering empirical rules .",
    "hopefully , the application of a new optimization principle for decoding ldpc codes presented in this paper could support this point of view .",
    "to solve a hard problem , we follow the divide - and - conquer principle .",
    "we first break up the problem into a number of sub - problems of manageable sizes and complexities . following that",
    ", we assign each sub - problem to an agent , and ask those agents to solve the sub - problems in a cooperative way .",
    "the cooperation is achieved by asking each agent to compromise its solution with the solutions of others instead of solving the sub - problems independently .",
    "we can make an analogy with team playing , where the team members work together to achieve the best for the team , but not necessarily the best for each member . in many cases , cooperation of this kind",
    "can dramatically improve the problem - solving capabilities of the agents as a team , even when each agent may have very limited power .    to be more specific",
    ", the cooperation is achieved in such a multi - agent system via two vital steps executed by each agent in an iterative way , 1 ) solving its sub - problem by soft decision making , and 2 ) passing its soft decisions to its neighboring agents . at the very beginning ,",
    "each agent makes soft decisions by solving its own sub - problem and ranking the solutions in order of preferences measured by some values . for an agent ,",
    "the most preferable one is the best solution to its sub - problem and the less preferable ones are the solutions sub - optimal to its sub - problem . following that , each agent passes its soft decisions as messages to its neighboring agents . after receiving its neighbor agents soft decisions",
    ", each agent goes back to the soft decision making step again . at this time , instead of solving its sub - problem independently , it tries to solve its sub - problem by compromising its solutions with its neighboring agents. the best solution for one agent may not be the best one for another .",
    "if there is any conflict among the agents , it is required for each agent to compromise its solutions with its neighbors to reach a consensus .",
    "if a consensus in picking solutions is reached through compromising , the system reports it as a solution for the original problem . otherwise , the system iterates until a consensus is reached among the agents or the iteration exceeds some cap .",
    "the very core of cooperation is the soft decision making via solution compromising .",
    "paper  @xcite formally describes the cooperative optimization in the language of game theory .",
    "it has been shown in @xcite that there are different cooperation schemes yielding different computational behaviors of the system .",
    "one of them leads the system to find the nash equilibria .",
    "another ensures the system of a unique equilibrium . with this scheme",
    ", the system always converges to the equilibrium with an exponential rate regardless of initial conditions .",
    "theory also tells us that the equilibrium must be the global optimum if it is a consensus solution .",
    "details about these together with the theoretical investigation of the cooperative optimization are provided in @xcite .",
    "let the cost function ( also referred to as energy function or objective function ) to be minimized be @xmath0 , which can be expressed as an aggregation @xmath1 of three binary sub - functions , @xmath2 , @xmath3 , and @xmath4 .    to illustrate the decomposition of this problem into simple sub - problems , we map the cost function ( [ cost_function ] ) into a graph ( shown in the upper portion of fig .  [ fig_decomposition ] ) .",
    "we can view each variable as a node in the graph and each binary sub - function as a connection between two nodes .",
    "this graph has one loop and we can decompose it into three sub - graphs of no loop shown in the lower portion of fig .",
    "[ fig_decomposition ] , one for each variable ( double circled ) .",
    "each sub - graph is associated with one cost function , @xmath5 .",
    "for example , the sub - graph for variable @xmath6 has its cost function @xmath7 as @xmath8 so are the cost functions of the sub - graphs for other two variables : @xmath9 @xmath10 obviously , @xmath11 with such a decomposition , the original problem , @xmath12 , becomes three sub - problems , @xmath13 , @xmath14 .        for the @xmath15th sub - problem ,",
    "the preferences for picking values for variable @xmath16 are used as the soft decisions for solving the sub - problem . those preferences",
    "are measured by some real values and are described as a function of @xmath16 , denoted as @xmath17 .",
    "it is also called the assignment constraint for variable @xmath16 .",
    "the different function values , @xmath17 , stand for the different preferences in picking values for variable @xmath16 .",
    "because we are dealing with minimizing @xmath18 , for the convenience of the mathematical manipulation , we choose to use smaller function values , @xmath17s , for more preferable variable values .",
    "to introduce cooperation in solving the sub - problems , we iteratively update the assignment constraints ( soft decisions in assigning variables ) as @xmath19 where @xmath20 is the iteration step , @xmath21 are non - negative weight values satisfying @xmath22 .",
    "it has been found  @xcite that such a choice of @xmath21 makes sure the iterative update functions converge .",
    "parameter @xmath23 controls the level of the cooperation at step @xmath20 and is called the cooperation strength , satisfying @xmath24 a higher value for @xmath23 will weigh the solutions of the other sub - problems @xmath25 more than the one of the current sub - problem @xmath26 .",
    "in other words , the solution of each sub - problem will compromise more with the solutions of other sub - problems . as a consequence , a higher level of cooperation in the optimization",
    "is reached in this case .",
    "the update functions , ( [ u1]),([u2 ] ) , and ( [ u3 ] ) , are a set of difference equations of the assignment constraints @xmath17 . unlike conventional difference equations used by probabilistic relaxation algorithms  @xcite , and hopfield networks  @xcite",
    ", this set of difference equations always has one and only one equilibrium given @xmath27 and @xmath21 .",
    "some important properties of this cooperative optimization will be shown in the following subsections .",
    "let @xmath28 be a multivariate cost function , or simply denoted as @xmath29 , where each variable @xmath16 has a finite domain @xmath30 of size @xmath31 ( @xmath32 ) .",
    "we break the function into @xmath33 sub - cost functions @xmath26",
    "( @xmath34 ) , one for each variable , such that @xmath26 contains at least variable @xmath16 , the minimization of each cost function @xmath26 ( the sub - problem ) is computational manageable in complexity , and @xmath35    the cooperative optimization is defined by the following set of difference equations : @xmath36    intuitively , we might choose @xmath21 such that it is non - zero if @xmath37 is contained by @xmath26 .",
    "however , theory tells us that this is too restrictive . to make the algorithm work , we need to choose @xmath38 to be a propagation matrix defined as follows :    a propagation matrix @xmath39 is a irreducible , nonnegative , real - valued square matrix and satisfies @xmath40 [ definition_propagation_matrix ]    a matrix @xmath41 is called reducible if there exists a permutation matrix @xmath42 such that @xmath43 has the block form @xmath44    the system is called reaching a consensus solution if , for any @xmath15 and @xmath45 where @xmath46 contains @xmath16 , @xmath47 where @xmath48 is defined as @xmath49 [ consensus ]    a solution to the difference equations  ( [ cooperative_optimization ] ) is called an equilibrium of the system . specifically , it is a set of values for all the assignment constraints ( the soft decisions ) , @xmath50 , such that the difference equations are satisfied .    to simplify the notations in the following discussions ,",
    "let @xmath51 let @xmath52 , the favorable value for assigning variable @xmath16 .",
    "let @xmath53 .",
    "it is the candidate solution obtained by the cooperative algorithm at iteration @xmath20 .",
    "the theoretical understanding of the cooperative optimization has been given in detail in @xcite .",
    "here we list some important properties .",
    "the following theorem shows that @xmath54 for @xmath55 have a direct relationship to the lower bound on the cost function @xmath29 .",
    "given any propagation matrix @xmath41 and the general initial condition @xmath56 or @xmath57 , @xmath58 is a lower bound function on @xmath59 , denoted as @xmath60 .",
    "that is @xmath61 in particular , let @xmath62 , then @xmath63 is a lower bound on the optimal cost @xmath64 , that is @xmath65 .",
    "[ theorem_1 ]    here , subscript `` - '' in @xmath63 indicates that it is a lower bound on @xmath64 .",
    "this theorem tells us that @xmath66 provides a lower bound on the cost function @xmath18 .",
    "we will show in the next theorem that this lower bound is guaranteed to be improved as the iteration proceeds .",
    "given any propagation matrix @xmath41 , a constant cooperation strength @xmath27 , and the general condition @xmath56 , @xmath67 is a non - decreasing sequence with upper bound @xmath64 .",
    "[ theorem_3 ]",
    "if a consensus solution is found at some step or steps , then we can find out the closeness between the consensus solution and the global optimum in cost .",
    "if the algorithm converges to a consensus solution , then it must be the global optimum also .",
    "the following theorem makes these points clearer .",
    "given any propagation matrix @xmath41 , and the general initial condition @xmath56 or @xmath68 .",
    "if a consensus solution @xmath69 is found at iteration step @xmath70 and remains the same from step @xmath70 to step @xmath71 , then the closeness between the cost of @xmath69 , @xmath72 , and the optimal cost , @xmath64 , satisfies the following two inequalities , @xmath73 @xmath74 where @xmath75 is the difference between the optimal cost @xmath64 and the lower bound on the optimal cost , @xmath76 , obtained at step @xmath77 .",
    "when @xmath78 and @xmath79 for @xmath80 , @xmath81 .",
    "[ theorem_2 ]    the performance of the cooperative algorithm further depends on the dynamic behavior of the difference equations  ( [ cooperative_optimization ] ) .",
    "its convergence property is revealed in the following two theorems .",
    "the first one shows that , given any propagation matrix and a constant cooperation strength , there does exist a solution to satisfy the difference equations ( [ cooperative_optimization ] ) .",
    "the second part shows that the cooperative algorithm converges exponentially to that solution .",
    "given any symmetric propagation matrix @xmath41 and a constant cooperation strength @xmath27 , then difference equations  ( [ cooperative_optimization ] ) have one and only one solution , denoted as @xmath82 or simply @xmath83 .",
    "[ theorem_7 ]    given any symmetric propagation matrix @xmath41 and a constant cooperation strength @xmath27 , the cooperative algorithm , with any choice of the initial condition @xmath84 , converges to @xmath85 with an exponential convergence rate @xmath27 .",
    "that is @xmath86 [ theorem_8 ]    this theorem is called the convergence theorem .",
    "it indicates that our cooperative algorithm is stable and has a unique attractor , @xmath85 .",
    "hence , the evolution of our cooperative algorithm is robust , insensitive to perturbations , and the final solution of the algorithm is independent of initial conditions . in contrast ,",
    "conventional algorithms based on iterative improvement ( e.g. gradient descent ) have many local attractors due to the local minima problem .",
    "the evolution of these algorithms are sensitive to perturbations , and the final solutions of these algorithms are dependent on initial conditions .",
    "ldpc codes belong to a special class of linear block codes whose parity check matrix @xmath87 has a low density of ones .",
    "ldpc codes were originally introduced by gallager in his thesis  @xcite . after the discovery of turbo codes in 1993 by berrou et al .",
    "@xcite , ldpc codes were rediscovered by mackay and neal  @xcite in 1995 .",
    "both classes have excellent performances in terms of error correction close to the shannon limit .",
    "the parity check matrix @xmath87 is a binary matrix with elements in @xmath88 .",
    "it is sparse with a few non - zero elements .",
    "let the code word length be @xmath33 and the input data be @xmath89 then @xmath87 is a @xmath90 matrix , where @xmath20 is the number of rows .",
    "each row of @xmath87 , denoted as @xmath91 , introduces one parity check constraint on @xmath92 , @xmath93 since @xmath87 has @xmath20 rows , there are @xmath20 constraints on @xmath92 .",
    "that is , @xmath94      to minimize the probability of decoding error , the optimal decoder for a channel code finds an input @xmath92 that has the maximum posterior probability @xmath95 given an output @xmath96 .",
    "usually , we assume a uniform prior distribution on @xmath92 . in this case , the maximum posterior criteria reduces to the maximum likelihood , i.e. , finding an input @xmath92 which makes the likelihood distribution @xmath97 a maximum .    for a discrete memoryless additive guassian channel and a binary modulation , the output data bit at position @xmath15 , @xmath98",
    ", can be modeled as the following random variable : @xmath99 where @xmath100 and @xmath101 is a additive noise of the gaussian distribution with variance @xmath102 .",
    "let @xmath103 where @xmath104 , is the conditional distribution of output data bit @xmath98 given the input data bit @xmath16 . in this case",
    ", the maximum likelihood decoding becomes @xmath105    this is a constrained maximization problem . without loss of generality , we can transform it into an unconstrained minimization problem in a more general form . to do that",
    ", we introduce unary constraints on variables @xmath16 , @xmath106 and convert each parity check constraint to a @xmath107-ary constraint on the variables of the constraint . let the @xmath45th parity check constraint , @xmath108 , define on a subset of variables , denoted as @xmath109 of size @xmath110 .",
    "then the @xmath107-ary constraint on @xmath109 is defined as @xmath111 using those definitions , ( [ original_problem ] ) becomes @xmath112 in general , the above problem is called the constraint - based optimization , which is np - hard .",
    "it is a core problem in mathematical logic and computing theory . in practice",
    ", it is fundamental in solving many problems in machine vision , image processing , computational chemistry , integrated circuit design , computer network design , artificial intelligence and more .",
    "the tanner graph is used to help us understand the decomposition .",
    "a tanner graph for a ldpc code is a bipartite graph with variable nodes on one side and constraint nodes ( parity check nodes ) on the other side .",
    "edges in the graph connect constraint nodes to variable nodes .",
    "a constraint node connects to those variable nodes that participate in its parity check .",
    "a variable node connects to those constraint nodes that use the variable in the parity checks .",
    "the tanner graph can be decomposed into @xmath33 tree - like sub - graphs , one for each variable .",
    "those sub - graphs can have overlaps .",
    "because their tree - like structures , we can find the exact solutions for the sub - problems associated with those sub - graphs .",
    "there are many ways of decomposing a graph which lead to different performances of the cooperative algorithm . a simple",
    ", straightforward way of decomposition is to have the sub - graph of each variable node consisting of all the constraint nodes linked to the variable node , together with their connections , all the variable nodes linked to those constraint nodes , together with their connections , and the variable node itself .",
    "we developed high performance us patent pending methods and apparatus for decoding turbo codes and ldpc codes using the cooperative algorithm .",
    "usually , short ldpc codes have high commercial values because the decoding time is also short .",
    "it also has been found that irregular ldpc codes have better performances than regular ones  @xcite .",
    "a candidate code for china hdtv ( proposed by the author using a new way of code construction called quantum coding ) is a @xmath113-irregular ldpc code of a data rate @xmath114 .",
    "china decides to use ldpc codes instead of turbo codes for channel coding because of their higher coding gains and lower complexity in decoding .",
    "fig  [ decodingldpc ] shows the performances of the cooperative algorithm and the sum - product algorithm in decoding the ldpc code using @xmath115 code words and awgn ( additive white gaussian noise ) channel .",
    "the maximum number of iterations for the sum - product algorithm is 30 .",
    "it was found that there is not much improvement in the decoding quality after 30 iterations .",
    "an error floor was observed at bers below @xmath116 using the sum - product algorithm .",
    "the acceptable error rate is below @xmath117 for china hdtv .",
    "the sum - product algorithm can not achieve that even after the @xmath118 is higher than @xmath119    the error floor trouble was completely removed by the cooperative algorithm .",
    "the error rate drops to zero after the @xmath118 is higher than @xmath120 . at the `` water fall '' region ,",
    "the cooperative algorithm has reduced the decoding error rates further by more than three fold . for the cooperative algorithm ,",
    "the maximum iteration number is 120 mainly because of much less complexity of its computation .",
    "even with that number , it was still more than six times faster than the sum - product algorithm .",
    "in the second example , we use a regular ldpc code to demonstrate that the cooperative algorithm has much less dependence on the code structure than the sum - product algorithm .",
    "the code is the turbo - like production of a simple parity check code @xmath121 @xmath122 the configuration of the code is a @xmath123 dimensional cubic @xmath124 .",
    "the block size is @xmath125 , the data size is @xmath126 , and the data rate is @xmath127 .",
    "lpdc codes of this kind are simplest in structure and the most easy to encode ( but not necessarily the best code distances ) .",
    "the sum - product algorithm has terrible performance in decoding this kind of codes due to the high regularity of the code structure .",
    "fig  [ decodingldpc ] shows the performances of both algorithms using @xmath128 code words and the awgn channel .",
    "the cooperative algorithm was much better than the sum - product algorithm in this case .",
    "the success of the cooperative algorithm in decoding this type of lpdc codes implies that we can have greater flexibility at constructing high performance codes without worrying too much about the limitations of decoding algorithms .",
    "we have presented the application of a new optimization technique called cooperative optimization for decoding ldpc codes .",
    "like the well - known sum - product algorithm , the cooperative algorithm is also based on iterative message passing operating in the tanner graph .",
    "although similar in operations , they are derived from different principles .",
    "the sum - product algorithm is a generalization of the belief propagation algorithm  @xcite used in ai .",
    "it can find exact solutions when the graph it operates on has no cycles . with cycles ,",
    "we still lack a theoretical understanding of the behavior of the algorithm .",
    "unlike many conventional methods , cooperative optimization has a solid theoretical foundation on many computational properties . in our experiments",
    ", it significantly outperformed the sum - product algorithm both in efficiency and accuracy for decoding different ldpc codes .",
    "the new cooperative decoding algorithm can be extended further from the min - sum semiring to other semirings similar to those done for the sum - product algorithm  @xcite and general distributive law  @xcite .",
    "t.  j. richardson , m.  a. shokrollahi , and r.  l. urbanke , `` design of capacity - approaching irregular low - density parity - check codes , '' _ ieee trans .",
    "inform . theory _",
    "47 , no .  2 ,",
    "619637 , feb 2001 ."
  ],
  "abstract_text": [
    "<S> cooperative optimization is a new way for finding global optima of complicated functions of many variables . </S>",
    "<S> it has some important properties not possessed by any conventional optimization methods . </S>",
    "<S> it has been successfully applied in solving many large scale optimization problems in image processing , computer vision , and computational chemistry . </S>",
    "<S> this paper shows the application of this optimization principle in decoding ldpc codes , which is another hard combinatorial optimization problem . in our experiments , </S>",
    "<S> it significantly out - performed the sum - product algorithm , the best known method for decoding ldpc codes . </S>",
    "<S> compared to the sum - product algorithm , our algorithm reduced the error rate further by three fold , improved the speed by six times , and lowered error floors dramatically in the decoding . </S>"
  ]
}