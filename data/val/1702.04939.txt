{
  "article_text": [
    "arrival processes provide a useful description for events occurring with some probability in a given time or space interval .",
    "applications range from communications and transports to medicine ( e.g. , diagnostic imaging ) and astronomy ( e.g. , particle detection ) , @xcite . from a statistical point of view the prominent model for arrival processes",
    "is notoriously poisson . indeed , even when independence between arrivals can not be assumed , the superposition of a large number of non - poisson processes is approximately distributed as a poisson ( palm - khintchine theorem ) , @xcite .",
    "also , the limit distribution of counting processes described by the binomial distribution is a poisson , according to the law of rare events .    in modern network contexts ( as , e.g. , data , communication and sensor networks or intelligent transportation systems ) estimating the process intensity at different locations , i.e. , the arrival rates at different nodes , is an important preliminary problem to be solved in order to gain context awareness .",
    "clearly , each arrival rate can be estimated by performing a _",
    "decentralized _ maximum likelihood ( ml ) estimation based only on the local arrivals at the given node . in this paper we want to investigate how the estimation can be improved by exploiting the cooperation among the nodes .",
    "recently , a great interest has been devoted to cooperation schemes in which estimation is performed by a network of computing nodes in a distributed way , rather than by collecting all the data in a central unit , @xcite . how to use the information from other nodes in the network and how to design a distributed algorithm merging such information will be the focus of the paper .",
    "distributed estimation has received a widespread attention in the distributed computation literature , especially as a natural application of linear ( average ) consensus algorithms , see , e.g. @xcite , or the recent surveys @xcite .",
    "nodes typically interact iteratively with their neighbors by means of a `` diffusion - like '' process in which the estimation is improved by suitably combining the estimations from neighboring nodes , @xcite . an incremental and a diffusive distributed algorithm with finite time convergence",
    "are proposed in @xcite for ( static ) state estimation .",
    "distributed optimization is also strictly related to distributed estimation . in @xcite",
    "a distributed alternating direction method of multipliers ( admm ) has been introduced as a tool for distributed ml estimation of vector parameters in a wireless sensor network . notice that admm and other distributed optimization algorithms , as , e.g. , @xcite , can be used as building blocks to solve parts of an estimation problem in a distributed set - up .",
    "ml approaches for distributed estimation of a commonly observed parameter have been proposed also in @xcite . in @xcite consensus - based algorithms",
    "have been developed to simultaneously estimate a common parameter measured by noisy sensors and classify sensor types .",
    "in @xcite consensus - based algorithms have been developed to estimate global parameters in a linear bayesian framework . in @xcite a distributed algorithm is proposed for adaptive bayesian estimation of a common parameter with known prior .",
    "dynamic methods have been also proposed in which the nodes keep collecting new measurements while interacting with each other . in @xcite a diffusion - based recursive least squares ( rls ) algorithm is proposed to estimate a constant parameter , but with dynamically acquired measurements .",
    "finally , in @xcite and @xcite distributed admm - based algorithms are proposed for the estimation of random signals and dynamical processes .    differently from the above references , in our work we consider a more general bayesian framework that allows the nodes to improve their local estimate , rather than reaching a consensus on a common parameter .",
    "as detailed later , we will consider a special model for continuous mixtures of poisson variables .",
    "poisson mixtures have been widely used in the arrival - rate estimation literature to model non - homogeneous scenarios , see , e.g. , @xcite as early references .",
    "the survey @xcite provides an extensive review of properties and applications for poisson mixtures . in a bayesian setting the classical mixing ( prior ) distribution is the gamma @xcite , which results in a closed - form posterior @xcite .",
    "the main contribution of the paper is twofold .",
    "first , we propose a distributed estimation scheme for arrival rates in an asynchronous network , based on a hierarchical probabilistic framework . specifically , we develop an empirical bayes approach , in which the arrival rates are treated as random variables , whose prior distribution is parametrized by an unknown hyperparameter to be determined via ml estimation . in particular , we borrow from the centralized statistics literature the classical gamma - poisson model ( for poisson mixtures ) , assuming however the hyperparameter is unknown , and extend it to a scenario with non - homogeneous sample sizes .",
    "we show that the ml estimation of the hyperparameter is a separable optimization problem , that can be solved in a distributed way over the network by using a distributed optimization algorithm .",
    "thanks to this modeling idea , the local estimates are obtained by taking advantage of the whole network data , thus improving the accuracy especially when the amount of local data is scarce . with this approach we are able to capture the fact that arrival rates are not the outcomes of isolated phenomena , but rather the expression of global properties of the process . for this `` optimal '' estimator",
    "we characterize mean and variance at steady - state ( i.e. , once agents have reached consensus on the optimal solution ) for networks with a large number of agents .",
    "second , we propose an alternative ad - hoc distributed estimator that , although suboptimal , performs comparably to the optimal one .",
    "the main advantage of this ad - hoc estimator is that the resulting algorithm has a simple update rule based on linear consensus protocols , thus exhibiting the same appealing exponential convergence . for the ad - hoc estimator",
    "we also characterize transient ( at any time - instant ) mean and variance for a given number of agents .",
    "notably , we show that at steady - state and for large number of agents the ad - hoc estimator attains the performance of the optimal one .    to strengthen these two contributions we perform a monte carlo analysis and compare the theoretical expressions obtained for mean and variance with their sample counterparts .",
    "the numerical computations confirm the theoretical analysis and show that some key assumptions made for a rigorous , but tractable analysis are not restrictive .",
    "moreover , they highlight other interesting features of the proposed distributed estimators .",
    "for example , the ad - hoc estimator achieves performances close to the optimal one even for a limited number of nodes .",
    "the paper is organized as follows . in section  [ sec : prob_form ] we introduce the model of a monitoring network and set up the estimation problem . in section  [ sec : estimators ] we develop the proposed distributed estimators based on the empirical bayes approach .",
    "the statistical performances of the two estimators are analyzed in section  [ sec : theoretical_analysis ] , while in section  [ sec : numerical_analysis ] we perform a monte carlo analysis to confirm the theoretical results .",
    "we consider a _ network of monitors _ having _ sensing _ , _ communication _ and _ computation _ capabilities .",
    "that is , each monitor can measure the number of arrivals in a given measurement time - scale ( e.g. , 1 second or 1 minute ) , share local data with neighboring agents , and perform local computations on its own and its neighbors data .",
    "the objective is to fuse the data in order to improve the estimation of the arrival rates .",
    "formally , each node collects measurements _ asynchronously _ in an _ observation window _ , over which the underlying process can be assumed to be stationary , i.e. , the set of rates can be considered constant over the observation window .",
    "clearly , the arrival rates have stationary increments , so that the number of arrivals in disjoint intervals are statistically independent .",
    "a measurement consists of the number of arrivals detected at a given location in the ( common ) _ time - scale interval_. accordingly , for each monitor @xmath0 we introduce the following variables :    * @xmath1 unknown arrival rate ; * @xmath2 the @xmath3-th collected measurement ( number of arrivals per time - scale interval ) ; * @xmath4 $ ] number of measurements @xmath2 collected in the observation window ( where @xmath5 is the maximum number of measurements that can be collected ) .",
    "the conditional distribution of @xmath2 given @xmath1 is a poisson random variable with parameter @xmath1 , i.e. , @xmath6 .",
    "all measurements are assumed to be independent .",
    "we denote by @xmath7 the total number of measurements .",
    "if all the nodes have the same number of measurements , i.e. , @xmath8 for all @xmath9 we say that the network is _",
    "homogeneous_.    in figure",
    "[ fig : measurement_scenario ] a scheme of the network measurement scenario is depicted with the variables of interest .",
    "we assume that the network evolution is triggered by a _ universal slotted time _ , @xmath10 , not necessarily known by the monitors .",
    "the monitors communicate according to a time - dependent directed communication graph @xmath11 , where @xmath12 are the monitor _ identifiers _ and the edge set @xmath13 describes the communication among monitors : @xmath14 if monitor @xmath15 communicates to @xmath16 at time @xmath17 . for each node @xmath15 , the nodes sending information to @xmath15 at time @xmath18 , i.e. , the set of @xmath19 such that @xmath20 is the set of _ in - neighbors _ of @xmath15 at time @xmath18 , and is denoted by @xmath21 .",
    "we make the following minimal assumption on the graph connectivity .",
    "first , we recall that a fixed directed graph is strongly connected if for any pair of nodes , @xmath15 and @xmath22 , there exists a directed path ( i.e. , a set of consecutive edges ) from @xmath15 to @xmath22 .",
    "[ assum : graph ] there exists an integer @xmath23 such that the graph @xmath24 is strongly connected @xmath25 .",
    "it is worth remarking that this network setup is very general , since it naturally embeds asynchronous scenarios as well as missing measurements due to , e.g. , sensor failures .",
    "in a _ decentralized set - up _ , in which nodes do not communicate , each node could estimate @xmath1 based on the sample @xmath26 by simply computing the empirical mean of the available measurements .",
    "that is , the decentralized estimator is @xmath27 where @xmath28 .",
    "notice that , the decentralized estimator turns out to be the ml estimator of @xmath1 when node @xmath15 can use only its own data .",
    "however , decentralized estimation yields reliable estimates only when the number of samples @xmath29 is large enough .    in our heterogenous set - up , it may happen that some nodes satisfy such a condition , while other ones do not have enough data , thus resulting in a poor estimation . in this paper",
    "we propose a distributed estimation scheme in which every node , especially the ones with fewer measurements , take advantage from cooperating with neighboring nodes .",
    "how the measurements at other nodes can help the local estimation at a given node is a nontrivial issue and needs to be investigated by means of a suitable probabilistic framework .",
    "specifically , we adopt a bayesian model in which all the unknown arrival rates @xmath1s are i.i.d .",
    "random variables ruled by a common probability distribution that captures the spatial variability of the process intensity .",
    "this model belongs to the family of poisson mixtures and is broadly accepted in the ( centralized ) statistical literature for modeling non - homogeneous scenarios , see , e.g. , @xcite , and ( * ? ? ?",
    "* and references therein ) for a through and extensive survey of properties and applications .",
    "as customary , see , e.g. , @xcite , we adopt as common distribution the conjugate prior of the poisson , which is the gamma distribution .",
    "this choice allows one to obtain a close - form expression for the posterior ( predictive ) distribution .",
    "however , notice that , our model extends the classical gamma - poisson mixture since the sample sizes @xmath29 , @xmath9 , can be different and thus exhibits even more flexibility .    a scheme of the proposed bayesian framework for our network scenario is depicted in figure  [ fig : network_scheme ] .          in applying a bayesian estimation approach to a network context ,",
    "the assumption that the prior distribution is fully known to all monitors is rather strong and may be a severe limitation in realistic scenarios . to overcome this limitation",
    "we adopt the empirical bayes approach in which only the class of the prior is known , i.e. , @xmath30 , where the shape parameter @xmath31 is known , but the scale parameter @xmath32 is unknown . the assumption that @xmath31 is known , while only @xmath32 is unknown , says that only the shape of the gamma distribution ( determined by the parameter @xmath31 ) is known , while the scaling is not .",
    "this assumption is reasonable in many applications , since it is a way to embed a rough information on the phenomenon , and is customary for the sake of mathematical tractability , @xcite",
    ".    the hyperparameter @xmath32 can be estimated via a ml procedure . to this aim , we need the joint distribution of all measurements @xmath33 for each agent @xmath15 .",
    "the likelihood function is the product of the marginal distributions of all agents @xmath34 where @xmath35^t$ ] .",
    "the marginal distribution of agent @xmath15 is derived from the joint distribution of @xmath36 and @xmath1 , @xmath37 by using eq . into eq .",
    "the likelihood is rewritten as : @xmath38 thus , the ml estimator @xmath39 of @xmath32 can be found by solving the following optimization problem @xmath40    the problem can be solved in closed - form only for the _ homogeneous _ case where all @xmath29s are equal , i.e. , for @xmath41 , recalling that @xmath42 is the total number of measurements . in this case",
    "the ml estimator of @xmath32 based on the entire set of measurements is given by @xmath43 where @xmath7 , and @xmath44 .",
    "after obtaining an estimate for @xmath32 , the empirical bayes estimator of the arrival rate @xmath1 that minimizes the mean square error ( mmse ) can be obtained by computing the conditional mean of the posterior distribution @xmath45 .",
    "the latter is given by the ratio between the joint pdf @xmath46 and the marginal pdf @xmath47 as from eq .",
    ", i.e. , @xmath48 eq . is a gamma pdf with parameters @xmath49 , hence the _ empirical bayes mmse estimator _ of each @xmath1 is @xmath50 = \\frac{{\\hat{b}^{\\text{ml}}}}{{\\hat{b}^{\\text{ml}}}n_i+1 } ( a + \\sigma_i).\\ ] ]    it is worth highlighting that the bayesian estimate is especially useful when @xmath29 is small .",
    "in fact , nodes improve the quality of their local estimate by combining the frequentist estimation ( based only on local observations ) with a correction term ( based on a prior global knowledge ) , which is estimated in a cooperative way .",
    "indeed , we can rewrite @xmath51 , with @xmath52 .",
    "when the local information is abundant ( @xmath53 and thus @xmath54 ) , the mmse estimator tends towards @xmath55 , meaning that when @xmath29 is large no further information can be inferred from the network .",
    "conversely , when local information is scarce , i.e. , the sample size @xmath29 is small , even one , the mmse estimator approaches the estimate of the global mean @xmath56 = a b$ ] .    in the following",
    "we will also consider an _ ad - hoc estimator _ obtained by using @xmath57 instead of the optimal @xmath39 , i.e. , @xmath58    clearly , the estimator @xmath59 follows the `` rationale '' of the empirical bayes approach and , therefore , has performance guarantees inherited from the ml procedure .",
    "conversely , the ad - hoc estimator is an alternative that at the moment has the only advantage of having a closed - form expression , whose performances need to be understood . in the rest of the paper",
    "we will show that , not only this estimator leads to a simpler and faster distributed algorithm , but also that for large number of agents performs as @xmath59 .      from eq .",
    "it is clear that each agent can compute the empirical bayes mmse estimator provided it knows @xmath39 .",
    "optimization problem , giving the ml estimator of @xmath32 , has a separable cost ( i.e. , the sum of @xmath60 local costs ) , hence it can be solved by using available distributed optimization algorithms for asynchronous networks @xcite .",
    "we propose a distributed estimator in which each node implements the local update rule of the chosen distributed optimization algorithm .",
    "the ml estimation problem   is not guaranteed to be convex in general .",
    "this is quite common in the estimation literature .",
    "however , since the function is coercive , there exists ( at least ) a minimizer and , thus , it is reasonable to apply descent algorithms , as the one in @xcite , which , under suitable conditions , guarantee convergence to a local minimizer .",
    "the * empirical bayes distributed estimator * is as follows . at each @xmath17 , each agent @xmath15 stores a local state @xmath61 , an estimate @xmath62 of @xmath39 and an estimate @xmath63 of @xmath59 .",
    "the node initializes its local state @xmath64 to an initial value @xmath65 chosen according to the distributed optimization algorithm in use , and sets @xmath66 ( which would be the solution of if @xmath15 were the only agent ) .",
    "then it updates its estimate of @xmath39 by using the local update rule of the chosen distributed optimization algorithm , and updates the current estimate @xmath63 by using .",
    "the algorithm is defined formally in the following table . for each @xmath17 , let @xmath67 be the collection of states of the in - neighbors of node @xmath15 , ` opt_local ` the local update of the chosen distributed optimization algorithm , and @xmath68 an algorithm parameter as , e.g. , a time - varying step - size .",
    "@xmath69 , @xmath70 .",
    "+ @xmath71          { \\hat{\\lambda}^{\\text{eb}}}_i&(t+1 ) = \\frac{{\\hat{b}^{\\text{ml}}}_i(t+1)}{{\\hat{b}^{\\text{ml}}}_i(t+1 ) n_i+1 } ( a +          \\sigma_i ) .",
    "\\nonumber        \\end{aligned}\\ ] ]    as an example , we show the ` opt_local ` function for the distributed _ subgradient - push _ method proposed in @xcite . to be consistent with the notation in @xcite we let @xmath72 , which is initialized to @xmath73 , with @xmath74 an arbitrary initial value .",
    "also , @xmath75 denotes the number of out - neighbors of node @xmath16 at time @xmath18 .",
    "@xmath76 @xmath77    y_i(t+1 )   & = \\sum_{k\\in n_i^i(t)\\cup \\{i\\ } } \\frac{y_k(t)}{d_k(t)}\\\\[1.2ex ]    { \\hat{b}^{\\text{ml}}}_i(t+1 ) & = \\frac{v_i(t+1)}{y_i(t+1 ) } \\\\[1.2ex ]    x_i(t+1 ) & = v_i(t+1 ) - \\gamma(t+1 ) \\nabla f({\\hat{b}^{\\text{ml}}}_i(t+1);n_i , \\sigma_i)\\end{aligned}\\ ] ]    with @xmath78 .",
    "if problem has a unique minimizer , the distributed optimization algorithm guarantees that all nodes reach consensus on the global minimizer .",
    "that is , @xmath79 from the convergence properties of the chosen distributed optimization algorithm it follows immediately that the proposed distributed estimator asymptotically computes at each node @xmath15 the empirical bayes mmse estimator of @xmath1",
    ".    however , most of the available distributed optimization algorithms , as the ones in @xcite , need the tuning of a global parameter ( we denoted it @xmath80 ) , and typically exhibit a sub - exponential convergence even in static graphs . to overcome these drawbacks",
    ", we propose an alternative distributed estimator with reduced complexity that , although suboptimal , will be shown to perform comparably to the optimal one .",
    "the * ad - hoc distributed estimator * is defined as follows . for each @xmath17 , each node @xmath9 stores in memory two local states @xmath81 and @xmath82 , an estimate @xmath83 of @xmath57 , and an estimate @xmath84 of @xmath85 .",
    "let @xmath86 be a set of weights such that @xmath87 if @xmath14 or @xmath88 , and @xmath89 otherwise .",
    "the ad - hoc distributed estimator  is given in the following table .",
    "@xmath90 , @xmath91 , @xmath92 , @xmath93 .",
    "+ @xmath94          \\eta_i(t+1 )      & = \\sum_{k\\in n_i^i(t ) \\cup \\{i\\ } } w_{ik}(t ) \\eta_k(t )          \\nonumber\\\\                  { \\hat{b}^{\\text{hom}}}_i(t+1 ) & = \\frac{1}{a}\\,\\frac{s_i(t+1)}{\\eta_i(t+1 ) } \\nonumber\\\\[1.0ex ]          { \\hat{\\lambda}^{\\text{ad - hoc}}}_i(t+1 ) & = \\frac{{\\hat{b}^{\\text{hom}}}_i(t+1)}{{\\hat{b}^{\\text{hom}}}_i(t+1 ) n_i+1 } ( a +          \\sigma_i ) .",
    "\\nonumber        \\end{aligned}\\ ] ]    we can rewrite the update of @xmath82 and @xmath81 by using an aggregate dynamics .",
    "that is , let @xmath95^t$ ] and @xmath96^t$ ] be the aggregate states , their dynamics is given by @xmath97 with @xmath98^t { \\stackrel{\\textrm{\\tiny def}}{=}}{\\bm{\\sigma}}$ ] , @xmath99^t { \\stackrel{\\textrm{\\tiny def}}{=}}{\\bm{n}}$ ] and @xmath100 the matrix with elements @xmath101 .",
    "let us denote @xmath102 the state transition matrix associated to each one of the linear systems , so that @xmath103 for the algorithm to converge we need the following assumption together with the assumption  [ assum : graph ] ( uniform joint connectivity of the communication digraph ) .",
    "[ assum : col_stochastic ] for each @xmath17 , the matrix @xmath100 is column stochastic , i.e. , @xmath104 , and there exists a positive constant @xmath105 such that @xmath106 and @xmath107 $ ] .",
    "it is worth noting that the column stochasticity assumption above is not the usual assumption used in linear consensus algorithms in which row stochasticity is assumed .",
    "[ lem : weak_ergodicity ] let @xmath108 be a uniformly jointly strongly connected graph ( assumption  [ assum : graph ] ) and @xmath109 a sequence of matrices satisfying assumption  [ assum : col_stochastic ] . then denoting @xmath110 with @xmath111 element @xmath112 , the following holds true .    1 .",
    "the matrix sequence @xmath109 is weakly ergodic , i.e. , @xmath113 for all @xmath114 .",
    "2 .   there exists some @xmath115 such that for all @xmath116 , @xmath117 , i.e. , for any @xmath9 , @xmath118    the result is well - known and can be found , e.g. , in @xcite .",
    "further references on this result under the same or different connectivity assumptions are @xcite .",
    "[ prop : ad_hoc_convergence ] let assumption  [ assum : graph ] hold .",
    "then the ad - hoc distributed estimator ( algorithm  [ alg : ad - hoc_estimator ] ) satisfies @xmath119    from lemma  [ lem : weak_ergodicity ] , @xmath109 is weakly ergodic so that there exists a sequence of stochastic vectors @xmath120 $ ] such that for each @xmath9 , @xmath121 for all @xmath122 . from eq .",
    "it follows that @xmath123 and @xmath124 thus , @xmath125 , so that the proof follows .    the result of proposition  [ prop : ad_hoc_convergence ] can be proven also if assumption  [ assum : graph ] is replaced by assumption  1 in @xcite , that is if @xmath126 is a stationary and ergodic sequence of stochastic matrices with positive diagonals , and @xmath127 $ ] is irreducible .",
    "the result can be proven by following the same line of proof developed therein .",
    "thus , the ad - hoc estimator could be implemented also in a monitoring network with stochastic gossip communication .",
    "the proposed algorithms are based on the assumption that the arrival - rates are constant in the observation window .",
    "therefore , one can apply the algorithms iteratively by recomputing the estimates on different windows of data , thus getting time - varying arrival - rates .    to conclude this section ,",
    "we point out that the update of the subgradient push optimization algorithm includes a push - sum consensus step , i.e. , a diffusive update based on a column stochastic matrix ( with coefficients @xmath128 for @xmath129 ) , which is the same used in the ad - hoc distributed estimator ( algorithm  [ alg : ad - hoc_estimator ] ) .",
    "however , in the subgradient push this update is part of a gradient descent step .",
    "in fact , the role and the evolution of the involved variables , i.e. , @xmath130 and @xmath131 respectively , are different as well as the convergence rates . indeed , the ad - hoc estimator exhibits the exponential convergence of linear consensus protocols as opposed to the much slower @xmath132 rate of the subgradient - push @xcite .",
    "these considerations , together with the lower computational burden , make the ad - hoc distributed estimator appealing even though not optimal . in the following section we will show that it actually performs very closely to the mmse estimator .",
    "in this section we analyze the performance of the proposed distributed estimators . in particular , for the empirical bayes distributed estimator , lacking a closed form for the update rule of @xmath39 and , in turn , of @xmath133 , we are able to derive only steady - state ( @xmath134 ) and asymptotic ( @xmath135 ) bounds .",
    "conversely , for the ad - hoc distributed estimator a transient analysis ( at any @xmath17 ) can be derived for any @xmath136 .    to develop the performance analysis",
    ", we consider a `` special '' agent , we label it as @xmath22 , that does not participate to the computation of @xmath39 ( respectively @xmath57 ) . under this assumption , it turns out that @xmath137 is independent of @xmath39 ( respectively @xmath57 ) .",
    "clearly , @xmath137 is also independent of any local estimate @xmath62 ( respectively @xmath83 ) , @xmath9 , at any @xmath17 .",
    "consistently , here we can simply assume that for the computation of @xmath138 ( respectively @xmath139 ) , agent @xmath22 uses the local estimate of @xmath32 computed by one of its in - neighbors , that is , e.g. , @xmath140 for some @xmath141 .",
    "notice that , in practice , the analysis developed for such an agent @xmath22 holds approximately for any node in the network participating to the distributed computation .",
    "indeed , due to the large number of agents @xmath60 , for any agent @xmath15 ( running the distributed algorithm ) , @xmath39 ( respectively @xmath57 ) and @xmath142 are very weakly correlated .",
    "the validity of this statement will be corroborated by the monte carlo analysis in the next section .",
    "we start by deriving the cramer - rao lower bound ( crb ) for any unbiased estimator @xmath143 of @xmath32 , i.e. , @xmath144 \\geq \\mathrm{crb}(b).\\ ] ]    the crb for the estimation of the hyperparameter @xmath32 is given by @xmath145 [ lem : crb ]    the proof is reported in appendix  [ sec : lem_crb ] .      for the empirical bayes distributed estimator we can analyze the performance only once consensus on the optimal value @xmath39 , and thus on @xmath146 , has been reached .",
    "specifically , let us recall that @xmath147    despite no analytical expression is available for the ml estimator @xmath39 ( nor for its moments ) the asymptotic analysis ( @xmath135 ) of the corresponding mmse estimator @xmath146 can be obtained by using the properties of ml estimation . in particular ,",
    "any ml estimator is asymptotically unbiased and efficient , hence @xmath148\\rightarrow b$ ] and @xmath149\\rightarrow \\mathrm{crb}(b)$ ] as @xmath135 . from equation , @xmath150 so that @xmath149\\rightarrow 0 $ ] as @xmath135 .",
    "these results on the asymptotic properties of @xmath39 can be used to prove the following theorem characterizing the asymptotic behavior of the empirical bayes distributed estimator .    due to the nonlinear dependency of @xmath133 from @xmath39",
    ", we will perform an approximate analysis by considering the taylor expansion of @xmath133 around @xmath148 $ ] . for tractability",
    "we will consider respectively the second - order and the first - order approximations for mean and variance .",
    "the numerical analysis in section  [ sec : numerical_analysis ] will confirm the validity of such an approximation .",
    "[ thm : mean_var_eb_estimator ] consider a network of monitors as in section  [ sec : prob_form ] running the empirical bayes distributed estimator .",
    "then , as @xmath151 , it holds true @xmath152 \\longrightarrow \\frac{b}{1 + n_j b}(a+n_j\\lambda_j ) .",
    "\\label{eq : e_lambdaeb_lim_n}\\ ] ] to second order and @xmath153 \\longrightarrow \\left ( \\frac{b}{1+n_jb }    \\right)^2 n_j\\lambda_j , \\label{eq : var_lambdaeb_lim_n}\\ ] ] to first order .      the closed - form update of the ad - hoc distributed estimator allows us to perform a more detailed analysis . in particular",
    ", we are able to characterize mean and variance of the local estimator @xmath139 during the algorithm evolution ( transient analysis ) and for any fixed value of the number of nodes @xmath60 .    as we have done for the empirical bayes distributed estimator , also for the ad - hoc one we first characterize the estimator of the hyperparameter @xmath32 .",
    "before addressing the transient analysis , we compute mean and variance of the consensus value @xmath57 .",
    "the homogeneous estimator of @xmath32 , @xmath57 , is unbiased , i.e. , @xmath154=b,\\ ] ] and has variance @xmath155 = \\frac{b}{a n } + \\frac{b^2}{a n^2 } \\sum_{i=1}^{n } n_i^2 \\ , .",
    "\\label{eq : var_bhom}\\ ] ] moreover , the estimator @xmath57 is consistent , i.e. , it converges in probability to the true value @xmath32 as @xmath151 .",
    "[ prop : var_bhom ]    the proof is given in appendix  [ sec : prop_var_bhom ] .",
    "the estimator @xmath57 is the ml estimator in the homogenous case , i.e. , when @xmath8 for all @xmath9 , and it attains the cramer - rao bound ( crb ) not only asymptotically , but for any @xmath60 .",
    "it follows by substituting @xmath8 in and .",
    "interestingly , we will show in the numerical analysis that even in the non - homogeneous scenario the ml estimator @xmath39 approaches the crb at any fixed @xmath60 .",
    "the following lemma gives a characterization of the transient local estimates .",
    "[ lem : e_var_bhom_t ] let assumption  [ assum : graph ] and assumption  [ assum : col_stochastic ] hold . for all @xmath9 ,",
    "the local estimator @xmath83 used in the ad - hoc distributed estimator ( algorithm  [ alg : ad - hoc_estimator ] ) is unbiased at any @xmath17 , i.e. , @xmath156=b,\\ ] ] and has variance @xmath157 = \\frac{b}{a } \\frac{\\sum_{k=1}^n \\phi_{ik}(t)^2             n_k}{{\\left(\\sum_{k=1}^n \\phi_{ik}(t ) n_k\\right)^2 } } + \\frac{b^2}{a }           \\frac { \\sum_{k=1}^n \\phi_{ik}(t)^2 n_k^2}{\\left(\\sum_{k=1}^n               \\phi_{ik}(t ) n_k\\right)^2 }         \\end{split}\\ ] ] where @xmath112 is the element @xmath111 of the state transition matrix @xmath158 defined in .",
    "moreover , as @xmath159 , the variance of @xmath83 converges exponentially to the variance of @xmath57 , , and satisfies @xmath160-{\\mathrm{var}}[{\\hat{b}^{\\text{hom}}}]| \\leq \\frac{b(1 + 2b{n_{\\text{max}}})}{\\mu a }         \\delta(\\phi(t ) ) ,          \\label{eq : bound_var_bhom_t}\\ ] ] where @xmath115 is given in lemma  [ lem : weak_ergodicity ] and @xmath161 is a ( proper ) coefficient of ergodicity continuous on the set of raw ( respectively column ) stochastic matrices satisfying @xmath162 .",
    "it is proper if @xmath163 if and only if @xmath164 , with @xmath165^t$ ] and @xmath166 a stochastic vector , @xcite . ]",
    "( exponentially decaying with time ) defined as @xmath167 .    the proof is reported in appendix  [ sec : lem_e_var_bhom_t ] .",
    "it is worth noting that @xmath168 and @xmath161 in equation   typically depend on @xmath60 ( and thus also on @xmath42 ) .",
    "the interesting aspect of the given result is that the bound provided in the previous lemma relates the convergence rate of @xmath169 $ ] to parameters of the communication graph and the diffusion protocol as @xmath168 and @xmath161 .",
    "for example , if @xmath170 is balanced , the matrix @xmath158 is doubly stochastic , hence @xmath171 .    with this characterization of the hyperparameter estimator , we are ready to analyze the estimator @xmath139 . recalling the assumption that the measurements of agent @xmath22 do not contribute to the computation of @xmath172",
    ", we clearly have uses as @xmath172 the estimate @xmath83 of some neighbor , @xmath173 participating to the distributed computation . ]",
    "@xmath174 = { \\mathrm{e}}[{\\hat{b}^{\\text{hom}}}_j(t ) ] = b.\\end{aligned}\\ ] ] and @xmath175 =    { \\mathrm{var}}[{\\hat{b}^{\\text{hom}}}_j(t ) ] .",
    "\\ ] ]    it is worth noticing once more that although for a tractable , rigorous analysis we need the assumption that agent @xmath22 does not contribute to the distributed computation , in practice the results hold with good approximation also in the scenario in which agent @xmath22 contributes to the distributed computation .",
    "this is due to the weak impact of single measurements onto the aggregate quantities .",
    "in fact , for example , following analogous calculations as in the proof of lemma  [ lem : e_var_bhom_t ] , the conditional mean in this latter case turns out to be @xmath176 = b - \\frac{\\phi_{jj}(t ) n_j}{\\eta_j(t ) } \\left(b       - \\frac{\\lambda_j}{a}\\right),\\ ] ] where @xmath177 is , again , the element @xmath178 of @xmath158 and we have also used that , conditioned to the @xmath179 of agent @xmath22 , @xmath180 = \\left\\ { \\begin{array}{ll } \\lambda_j & i = j\\\\ ab        & i\\neq j\\end{array } \\right . .\\ ] ] since as @xmath134 , @xmath181 , then the difference between equations and is practically negligible .    in the next theorem",
    "we provide explicit transient expressions for the conditional mean and variance of @xmath139 .",
    "again , we use second - order and first - order approximations respectively .    [",
    "thm : mean_var_ad - hoc_estimator ] consider a network of monitors as in section  [ sec : prob_form ] running the ad - hoc distributed estimator ( algorithm  [ alg : ad - hoc_estimator ] ) . then the transient conditional mean and variance of @xmath139 are given by @xmath182 \\approx \\nonumber\\\\ & ( a+n_j\\lambda_j)\\left [ \\frac{b}{1+n_j b }      - \\frac{n_j}{(1+n_j b)^3 } { \\mathrm{var}}[{\\hat{b}^{\\text{hom}}}_j(t ) ]    \\right ] \\label{eq : e_cond_approx_ah}\\end{aligned}\\ ] ] and @xmath183 \\approx \\nonumber\\\\   & \\left [ \\frac{b}{1+n_jb } -    \\!\\frac{n_j}{(1+n_jb)^3 } { \\mathrm{var}}[{\\hat{b}^{\\text{hom}}}_j(t ) ] \\right]^2 n_j\\lambda_j \\nonumber\\\\   & \\;\\ ; + \\left[\\frac{2n_j}{(1+n_jb)^3}\\right]^2 { \\mathrm{var}}[{\\hat{b}^{\\text{hom}}}_j(t)]\\left [     ( a+n_j\\lambda_j)^2+n_j\\lambda_j\\right]\\label{eq : var_cond_approx_ah}\\end{aligned}\\ ] ] where @xmath184 indicates respectively the second - order and the first - order approximations , and @xmath185 $ ] is given in .",
    "+ moreover , the asymptotic value @xmath186 satisfies , as @xmath151 , @xmath187 & \\longrightarrow    \\frac{b}{1 + n_j b}(a+n_j\\lambda_j ) \\label{eq : e_cond_approx_ah_asympt}\\end{aligned}\\ ] ] to second order and @xmath188 & \\longrightarrow   \\left(\\frac{b}{1+n_j b } \\right)^2 n_j\\lambda_j \\label{eq : var_cond_approx_ah_asympt}\\end{aligned}\\ ] ] to first order .    by comparing equations - with - , one can notice that at steady - state ( @xmath159 ) and for large number of agents ( @xmath135 ) the mean and variance of the ad - hoc distributed estimator approach the ones of the optimal empirical bayes .",
    "moreover , the right - hand side of can be rewritten as @xmath189 , which is clearly smaller than the variance of the decentralized estimator @xmath190 = \\frac{\\lambda_j}{n_j}$ ] .",
    "in this section we analyze the performance of the proposed estimators . starting from the theoretical characterization developed in the previous section ,",
    "we perform a monte carlo analysis confirming the theoretical bounds and adding other insights on the performance of the estimators .    as performance metric",
    "we adopt the root mean square error ( rmse ) , thus taking into account both bias and variance of the estimators .",
    "we recall that for an estimator @xmath191 of a parameter @xmath192 , the rmse is defined as @xmath193 & = \\sqrt{{\\mathrm{e}}[(\\hat\\omega - \\omega)^2 ] } \\nonumber\\\\    & = \\sqrt{{\\mathrm{var}}[\\hat\\omega ] + { \\mathrm{e}}^2[\\hat\\omega ] - 2\\omega{\\mathrm{e}}[\\hat\\omega ] +      \\omega^2 } .\\end{aligned}\\ ] ] clearly , if the estimator is unbiased the rmse coincides with the standard deviation , i.e. , @xmath194 = \\sqrt{{\\mathrm{var}}[\\hat\\omega]}$ ] .",
    "the statistical rmse , @xmath195 $ ] , will be compared with the sample value obtained through the monte carlo trials , @xmath196 , computed as @xmath197 - \\omega)^2},\\ ] ] where @xmath198 is the number of trials and @xmath199 $ ] is the @xmath200th estimate of @xmath192 .    in the following we set @xmath201 and , to generate the random values",
    ", we use a gamma distribution with parameters @xmath202 and @xmath203 , which gives values of @xmath1 in the range @xmath204 $ ] with @xmath205 probability .    in order to challenge the ad - hoc distributed estimator  we focus on a strongly inhomogeneous network scenario .",
    "that is , we consider a network in which half of the nodes have the maximum number of measurements in the observation window , @xmath206 ( we set @xmath207 ) , and the remaining ones only one measurement , @xmath208 .",
    "we start by analyzing the transient performance of the ad - hoc distributed estimator .",
    "consistently with the theoretical analysis , we first focus on the time evolution of the rmse of @xmath83 .",
    "notice that , since @xmath83 is an unbiased estimator it holds @xmath209 = \\sqrt{{\\mathrm{var}}[{\\hat{b}^{\\text{hom}}}_i(t)]}$ ] .",
    "we compare the evolution of the sample rmse with the theoretical expression obtained from . for this analysis we set @xmath210 and consider two possible communication models . in the first one we consider a fixed , directed and very sparse communication graph defined as follows : starting from a directed cycle , we add the edges @xmath211 , @xmath212 , @xmath213 and @xmath214 to unbalance the graph . in figure",
    "[ fig : b_transient_fixed ] we plot the evolution of @xmath209 $ ] for @xmath215 nodes , two of them with @xmath5 measurements and two with one measurement .",
    "the theoretical curve predicts very accurately the sample rmse obtained by the monte carlo trials , as highlighted in the inset .",
    "as expected , the rmse of the different nodes converges to the consensus value @xmath216 $ ] obtained from .    as a second scenario we challenge the algorithm on a time - varying topology .",
    "namely , we consider a graph obtained by extracting at each time - instant an erds - rnyi graph with parameter @xmath217 .",
    "we choose a small value , so that at a given instant the graph is disconnected with high probability , but it turns out to be uniformly jointly connected with @xmath218 .    in figure",
    "[ fig : b_transient_erdos ] we again compare the theoretical evolutions of @xmath209 $ ] with their sample counterparts .",
    "we can highlight two main differences with respect to the previous scenario .",
    "the curves have some constant portions showing that nodes can be isolated for some time - intervals",
    ". however , the convergence is faster compared to the fixed scenario .",
    "this can be explained by the higher density of the union graph in the time - varying scenario as opposed to the sparsity of the fixed graph .",
    "in fact , we noticed that increasing the erds - rnyi graph parameter increases the convergence speed .",
    "$ ] under fixed communication graph .",
    "the solid lines indicate the theoretical expressions , while the dash - dot lines are the ones obtained via monte carlo simulations .",
    "the dashed horizontal line is the theoretical consensus value.,scaledwidth=40.0% ]    $ ] under time - varying communication graph .",
    "the solid lines indicate the theoretical expressions , while the dash - dot lines are the ones obtained via monte carlo simulations .",
    "the dashed horizontal line is the theoretical consensus value.,scaledwidth=40.0% ]    $ ] under fixed communication graph for @xmath210 ( red lines ) and @xmath219 ( black lines ) .",
    "the solid lines indicate the theoretical expressions , while the dash - dot lines are the ones obtained via monte carlo simulations .",
    "the dashed horizontal lines are the theoretical steady - state values .",
    ", scaledwidth=40.0% ]    now we focus on the transient behavior of @xmath220 $ ] for a generic node @xmath15 .",
    "notice that we use the index @xmath15 , rather than @xmath22 , meaning that in the monte carlo trials the node under investigation participates to the computation of @xmath83 .",
    "this allows us to show that the uncorrelation assumption in section  [ sec : theoretical_analysis ] is in fact reasonable .",
    "moreover , the computations will also confirm the validity of the low - order approximation made to derive the theoretical expressions of mean and variance .    in figure",
    "[ fig : lambda_transient_fixed ] we compare the theoretical and sample evolution of @xmath220 $ ] .",
    "the theoretical curve is obtained by plugging equations and into equation .",
    "we compute the curves for two different values of @xmath60 , namely @xmath210 and @xmath219 .",
    "the difference between the theoretical and sample curves is already minimal for @xmath210 ( showing a very weak correlation between @xmath83 and @xmath142 ) and completely disappears for @xmath219 ( showing that the correlation has practically no more influence ) .",
    "we want to stress that running the same computation for a node not participating to the distributed computation ( hence matching the uncorrelation assumption ) the theoretical and sample curves are indistinguishable even for @xmath210 .",
    "this suggests that the low - order approximation does not affect the goodness of the prediction .",
    "it is worth noting that by increasing @xmath60 the steady - state value , @xmath221 $ ] , decreases , since the hyperparameter @xmath32 is estimated by means of a larger sample .",
    "this aspect will be better highlighted in the following asymptotic analysis in which we focus on how @xmath221 $ ] varies with @xmath60 .    in the asymptotic analysis",
    "we consider both the empirical bayes distributed estimator  and the ad - hoc distributed estimator by comparing again the predicted theoretical values with the sample counterparts . as in the transient analysis we first focus on the estimation of @xmath32 .    in figure",
    "[ fig : b_asympt_n ] we plot the sample rmse of the two estimators ( ml and homogeneous ) and compare them with the theoretical value of the homogeneous estimator and with the cramer - rao bound ( crb ) .",
    "as expected , for each fixed @xmath60 the rmse of the ml estimator is lower than the homogeneous one due to the ( strong ) inhomogeneity of the network .",
    "once again , we recall that the homogeneous estimator coincides with the ml estimator only when the network is homogeneous ( i.e. , @xmath8 for any @xmath9 ) .",
    "as already experienced in the transient analysis , the theoretical values of @xmath216 $ ] practically coincide with the sample ones .",
    "although for the ml estimator we have no theoretical expression for fixed @xmath60 , the picture shows a very interesting property .",
    "that is , the ml estimator achieves the crb not only asymptotically ( @xmath135 ) as predicted by the theory , but also for each fixed @xmath60 .",
    "interestingly , in accordance to the theoretical results in the previous section , also the homogeneous estimator achieves the crb as @xmath60 goes to infinity .",
    "finally , we analyze the rmse of the estimators of @xmath1 .",
    "we consider an agent with one measurement ( @xmath222 ) and use the most frequent value for the arrival rate , i.e. , the mode of the gamma distribution , @xmath223 .    in figure",
    "[ fig : lambda_asympt_n ] we plot the sample rmse of the empirical bayes and ad - hoc estimators .",
    "we also plot the ( sample ) values of the decentralized estimator .",
    "we decided to normalize all the curves to the theoretical value of the decentralized estimator in order to highlight the improvements of the proposed distributed estimators .",
    "clearly , the sample values of the decentralized estimator are approximately equal to one , with minor fluctuations only due to the finite number of samples .",
    "we compare the sample curves with the theoretical curve of the homogeneous estimator and with the theoretical asymptotic value as @xmath135 .",
    "the theoretical curve is obtained as follows : @xmath224 $ ] and @xmath225 $ ] can be computed by plugging @xmath169 $ ] from in and ; then , @xmath221 $ ] is obtained by plugging @xmath224 $ ] and @xmath225 $ ] into .    again , although computed under the uncorrelation assumption and neglecting higher order terms , the theoretical expression @xmath221 $ ] predicts very accurately the sample values ( cross markers and solid curve ) .    the plot confirms how the distributed estimators take advantage from the network growth although the local sample remains constant ( even @xmath208 ) .",
    "indeed , the rmse decreases as @xmath60 grows .",
    "the empirical bayes distributed estimator , being the optimal estimator , always outperforms the ad - hoc distributed estimator .",
    "however , as predicted by the theoretical analysis , the two estimators achieve the same asymptotic limit as @xmath135 .",
    "moreover , it is interesting to notice that the rmse of the two estimators practically coincide already for @xmath226 , thus strengthening the already appealing features of the ad - hoc distributed estimatorfound from the theoretical analysis ( i.e. , easier computation and asymptotic optimality ) .     as a function of the number of agents @xmath60 .",
    "the sample values are compared with the cramer - rao bound ( dashed line ) and the theoretical rmse of the homogeneous estimator ( solid line).,scaledwidth=40.0% ]     ( dot markers ) as a function of the number of agents @xmath60 .",
    "the theoretical rmse of the ad - hoc estimator ( solid line ) and the theoretical limit ( dashed line ) are shown for comparison.,scaledwidth=40.0% ]",
    "in this paper we have proposed a novel distributed scheme , based on a hierarchical framework , for the bayesian estimation of arrival rates in asynchronous monitoring networks .",
    "the proposed distributed approach allows each node to gain information from the network and thus outperforms the decentralized estimator , especially when the node local information is scarce .",
    "in particular , the distributed estimator consists of the convex combination of a global information , computed through a distributed optimization algorithm , and a function of the local data .",
    "then we have proposed an ad - hoc distributed estimator that performs closely to the optimal empirical bayes estimator , but is much simpler to implement and exhibits faster ( exponential ) convergence .",
    "we have analyzed the two estimators and provided expressions for mean and variance as the network size goes to infinity , showing that in this asymptotic situation the ad - hoc estimator achieves the same rmse of the optimal one .",
    "moreover , for the ad - hoc estimator we have provided transient expressions for mean and variance . a numerical monte carlo analysis has been performed to corroborate the theoretical results and highlight the interesting features of the two distributed estimators .",
    "the cramer - rao bound is defined as @xmath227 , where @xmath228 is the fisher information .",
    "the fisher information is obtained from the likelihood as : @xmath229,\\ ] ] and , computing the derivatives , @xmath230\\\\     & = -\\frac{an}{b^2 } + \\sum_{i=1}^n \\frac{2n_ib+1}{b^2(n_ib+1)^2 } ( { \\mathrm{e}}\\left[\\sigma_i \\right]+a).\\end{aligned}\\ ] ] now , the mean of @xmath142 turns out to be @xmath231 = { \\mathrm{e}}[{\\mathrm{e}}[\\sigma_i | \\lambda_i]]= { \\mathrm{e}}[n_i \\lambda_i ] = n_i a b$ ] , so that , after some manipulation @xmath232 \\\\    & = \\frac{a}{b^2}\\sum_{i=1}^n \\frac{n_ib}{n_ib+1},\\end{aligned}\\ ] ] so that the proof follows .",
    "the mmse estimator of @xmath179 in can be written as @xmath233 , where @xmath234 and @xmath235 . due to the independence between @xmath39 and @xmath137",
    ", we have @xmath236={\\mathrm{e}}[zy |\\lambda_j]={\\mathrm{e}}[z |\\lambda_j ] { \\mathrm{e}}[y    |\\lambda_j ]    \\label{eq : tot_e_cond_ml}\\ ] ] and @xmath237&={\\mathrm{e}}^2[z |\\lambda_j ] { \\mathrm{var}}[y |\\lambda_j]\\nonumber\\\\    & \\quad+ \\!{\\mathrm{var}}[z |\\lambda_j ] ( { \\mathrm{e}}^2[y |\\lambda_j ] + { \\mathrm{var}}[y    |\\lambda_j ] ) .",
    "\\label{eq : tot_var_cond_ml}\\end{aligned}\\ ] ] the conditional moments of @xmath238 are easily obtained @xmath239=a+n_j \\lambda_j    \\label{eq : e_y}\\ ] ] and @xmath240=n_j\\lambda_j .",
    "\\label{eq : var_y}\\ ] ] for the nonlinear function @xmath241 we resort to an approximate analysis .",
    "that is , we consider the taylor expansion for the moments of the function @xmath242 around the mean value @xmath243 = { \\mathrm{e}}[{\\hat{b}^{\\text{ml}}}]$ ] .",
    "@xmath244 & = { \\mathrm{e}}[z({\\hat{b}^{\\text{ml } } } ) ]   \\nonumber \\\\     & \\approx{\\mathrm{e}}\\big [ z({\\mathrm{e}}[{\\hat{b}^{\\text{ml}}}])+ z'({\\mathrm{e}}[{\\hat{b}^{\\text{ml } } } ] ) ( { \\hat{b}^{\\text{ml}}}-{\\mathrm{e}}[{\\hat{b}^{\\text{ml}}}])\\nonumber\\\\     & \\qquad + \\frac{1}{2 } z''({\\mathrm{e}}[{\\hat{b}^{\\text{ml } } } ] ) ( { \\hat{b}^{\\text{ml}}}-{\\mathrm{e}}[{\\hat{b}^{\\text{ml}}}])^2\\big ] , \\ ] ] where we have neglected terms of order higher than two in the expansion of @xmath245 , hence the @xmath184 symbol .",
    "then @xmath246 \\approx z({\\mathrm{e}}[{\\hat{b}^{\\text{ml}}}])+ \\frac{1}{2 } z''({\\mathrm{e}}[{\\hat{b}^{\\text{ml } } } ] ) { \\mathrm{var}}[{\\hat{b}^{\\text{ml } } } ] .",
    "\\label{eq : e_z}\\ ] ]    observing that @xmath247 and @xmath248 , and plugging and in , we get @xmath249 \\approx\\\\     & ( a+n_j\\lambda_j)\\left [      \\frac{{\\mathrm{e}}[{\\hat{b}^{\\text{ml}}}]}{{\\mathrm{e}}[{\\hat{b}^{\\text{ml}}}]n_j + 1 } - \\frac{n_j}{({\\mathrm{e}}[{\\hat{b}^{\\text{ml}}}]n_j+1)^3 }      { \\mathrm{var}}[{\\hat{b}^{\\text{ml } } } ] \\right]\\!. \\end{aligned}\\ ] ] from the asymptotic properties of the ml estimators , we know that for @xmath135 , @xmath148\\rightarrow b$ ] and @xmath149\\rightarrow 0 $ ] , so that equation follows .",
    "similarly , it turns out that @xmath250 & \\approx \\left(z''({\\mathrm{e}}[{\\hat{b}^{\\text{ml}}}])\\right)^2 { \\mathrm{var}}[{\\hat{b}^{\\text{ml } } } ]   \\label{eq : var_z}.\\end{aligned}\\ ] ] using - into eq .",
    "we obtain @xmath251 \\approx \\left ( \\frac{{\\mathrm{e}}[{\\hat{b}^{\\text{ml}}}]}{{\\mathrm{e}}[{\\hat{b}^{\\text{ml}}}]n_j + 1 } \\right)^2 n_j\\lambda_j\\\\    & \\quad+ \\left[\\frac{2n_j } { ( { \\mathrm{e}}[{\\hat{b}^{\\text{ml}}}]n_j + 1)^3}\\right]^2 { \\mathrm{var}}[{\\hat{b}^{\\text{ml } } } ] \\left[(a+n_j\\lambda_j)^2+n_j\\lambda_j\\right]\\!.\\end{aligned}\\ ] ] using again the asymptotic properties of @xmath39 , equation follows , thus concluding the proof .",
    "first , notice that @xmath252 = n_i \\lambda_i$ ] , thus @xmath253 & = { \\mathrm{e}}[{\\mathrm{e}}[\\sigma | \\bm{\\lambda } ] ] = { \\mathrm{e}}\\left [ \\sum_{i=1}^n { \\mathrm{e}}[\\sigma_i |      \\lambda_i ] \\right]= { \\mathrm{e}}\\left[\\sum_{i=1}^n n_i\\lambda_i\\right]\\\\   & = \\sum_{i=1}^n n_i a b = n a b,\\end{aligned}\\ ] ] so that @xmath254=b$ ] .      variance , we have @xmath257 & = { \\mathrm{e}}[{\\mathrm{var}}[\\sigma |      \\bm{\\lambda } ] ] + { \\mathrm{var}}[{\\mathrm{e}}[\\sigma | \\bm{\\lambda}]]\\\\   & = { \\mathrm{e}}\\left[\\sum_{i=1}^n      { \\mathrm{var}}[\\sigma_i | \\lambda_i ] \\right ] + { \\mathrm{var}}\\left [ \\sum_{i=1}^n { \\mathrm{e}}[\\sigma_i | \\lambda_i ] \\right]\\\\       & = { \\mathrm{e}}\\left [      \\sum_{i=1}^n n_i \\lambda_i \\right ] + { \\mathrm{var}}\\left [ \\sum_{i=1}^n n_i      \\lambda_i\\right]\\\\   & = a b n + a b^2      \\sum_{i=1}^n n_i^2 .     \\end{split}\\ ] ] the variance of the homogeneous estimator is given by @xmath169 =    { \\mathrm{var}}\\left [ \\frac{\\sigma}{a n } \\right]$ ] , so that equation follows . by markov s inequality and lemma  [ prop : var_bhom ] @xmath258,\\ ] ] and the variance can be bounded as follows @xmath259 & = \\frac{b}{a n } +    \\frac{b^2}{a n^2 } \\sum_{i=1}^{n } n_i^2\\\\   & \\leq \\frac{b}{a n } + \\frac{b^2}{a n^2 }    \\sum_{i=1}^{n } { n_{\\text{max}}}^2\\\\ & = \\frac{b}{a n } + \\frac { { { n_{\\text{max}}}}^2 b^2}{a n } \\ ;    \\stackrel{n \\rightarrow \\infty}{\\xrightarrow{\\hspace*{1 cm } } } \\ ; 0 \\end{split}\\ ] ] so that @xmath260 in probability , thus concluding the proof .      to prove that @xmath83 is unbiased at any @xmath17 ,",
    "first let us recall that the aggregate states @xmath95^t$ ] and @xmath261^t$ ] evolve according to the dynamics with @xmath98^t { \\stackrel{\\textrm{\\tiny def}}{=}}{\\bm{\\sigma}}$ ] , @xmath262^t { \\stackrel{\\textrm{\\tiny def}}{=}}{\\bm{n}}$ ] and @xmath100 the column stochastic matrix with elements @xmath101 .",
    "the evolution of @xmath263 and @xmath264 is given by where @xmath158 is the deterministic state transition matrix defined in .",
    "next , observe that the update @xmath82 depends only on the initial value @xmath29 and thus is deterministic .",
    "therefore , using the update in algorithm  [ alg : ad - hoc_estimator ] , we have @xmath156 = \\frac{1}{a \\eta_i(t ) } { \\mathrm{e}}[s_i(t)]\\ ] ] using the evolution of the aggregate state and denoting @xmath265 the @xmath15th canonical vector ( e.g. , @xmath266^t$ ] ) , we have @xmath267 & = \\frac{1}{a \\eta_i(t ) } { \\mathrm{e}}[e_i^t ( \\phi(t){\\bm{\\sigma } } ) ] \\\\                       & = \\frac{1}{a \\eta_i(t ) } e_i^t \\phi(t ) { \\mathrm{e}}[{\\bm{\\sigma } } ] .",
    "\\end{split}\\ ] ] noting that @xmath268 = a b n_i$ ] it follows @xmath267 & = \\frac{1}{a \\eta_i(t ) } e_i^t \\phi(t ) a b \\ , \\bm{n}\\\\                       & = \\frac{b}{\\eta_i(t ) } e_i^t \\eta(t ) = b\\\\ \\end{split}\\ ] ] where the last two steps follow respectively from ( @xmath269 ) and from @xmath270 .      using again the update in algorithm  [ alg : ad - hoc_estimator ] ,",
    "it holds @xmath271 & =   \\frac{1}{a^2 \\eta_i(t)^2 } { \\mathrm{var}}[e_i^t ( \\phi(t){\\bm{\\sigma}})].\\\\ \\end{split}\\ ] ] noting that @xmath272 is the @xmath15th row of @xmath158 , it follows @xmath271 & =   \\frac{1}{a^2 \\eta_i(t)^2 } { \\mathrm{var}}\\left[\\sum_{k=1}^n \\phi_{ik}(t ) \\sigma_k\\right ] , \\end{split}\\ ] ] where @xmath112 is the element @xmath111 of the matrix @xmath158 . by the law of total variance we have that @xmath271 & =   \\frac{1}{a^2 \\eta_i(t)^2 } \\left\\ { { \\mathrm{e}}\\left[{\\mathrm{var}}\\left[\\sum_{k=1}^n \\phi_{ik}(t )        \\sigma_k \\;\\big |\\ ; { \\bm{\\lambda}}\\right]\\right ] \\right.\\\\   & \\qquad\\left .",
    "+ { \\mathrm{var}}\\left [ { \\mathrm{e}}\\left[\\sum_{k=1}^n \\phi_{ik}(t ) \\sigma_k   \\;\\big |\\ ;        { \\bm{\\lambda}}\\right ] \\right ] \\right\\}\\\\    & = \\frac{1}{a^2 \\eta_i(t)^2 } \\left\\ {   { \\mathrm{e}}\\left[\\sum_{k=1}^n \\phi_{ik}(t)^2 n_k        \\lambda_k \\right]\\right.\\\\       & \\left.\\qquad +   { \\mathrm{var}}\\left[\\sum_{k=1}^n \\phi_{ik}(t ) n_k        \\lambda_k \\right ] \\right\\ } , \\end{split}\\ ] ] where we have used @xmath273 = n_k \\lambda_k$ ] and @xmath274 = n_k \\lambda_k$ ] . finally , recalling that @xmath275 = a b$ ] , @xmath276 = a b^2 $ ] and the @xmath277s are independent , it turns out @xmath271 & = \\frac{b}{a \\eta_i(t)^2 } \\left ( \\sum_{k=1}^n \\phi_{ik}(t)^2",
    "n_k + b \\sum_{k=1}^n \\phi_{ik}(t)^2 n_k^2 \\right ) , \\end{split}\\ ] ] so that equation   follows .    to prove the asymptotic result , we work out @xmath278 - { \\mathrm{var}}[{\\hat{b}^{\\text{hom}}}]|$ ] by using and @xmath279|   = \\\\ & = \\left| \\frac{b}{a } \\left(\\frac{\\sum_{k=1}^n \\phi_{ik}(t)^2    n_k}{{\\left(\\sum_{k=1}^n \\phi_{ik}(t ) n_k\\right)^2 } } - \\frac{1}{n}\\right)\\right.\\\\   & \\qquad\\left.+ \\frac{b^2}{a } \\left(\\frac {    \\sum_{k=1}^n \\phi_{ik}(t)^2 n_k^2}{\\left(\\sum_{k=1}^n",
    "\\phi_{ik}(t )      n_k\\right)^2 } -   \\frac{\\sum_{k=1}^{n } n_k^2}{n^2 } \\right ) \\right|\\\\ & =    \\frac{1}{\\eta_i(t)^2 } \\left| \\frac{b}{a n } \\left ( n \\sum_{k=1}^n \\phi_{ik}(t)^2    n_k -   \\left(\\sum_{k=1}^n \\phi_{ik}(t ) n_k\\right)^2\\right ) \\right.\\\\   & \\qquad \\left .",
    "+ \\frac{b^2}{a n^2 } \\left (    n^2 \\sum_{k=1}^n \\phi_{ik}(t)^2 n_k^2 - \\left(\\sum_{k=1}^n \\phi_{ik}(t )      n_k\\right)^2 \\sum_{k=1}^{n } n_k^2 \\right)\\!\\right|\\\\ \\end{split}\\ ] ] by using the definition of @xmath280 and writing @xmath281",
    "@xmath282|   = \\\\   & = \\frac{1}{\\eta_i(t)^2 } \\left| \\frac{b}{a n }      \\left ( \\sum_{h=1}^n n_h \\sum_{k=1}^n \\phi_{ik}(t)^2    n_k \\right.\\right.\\\\   & \\qquad\\left.-   \\sum_{k=1}^n \\phi_{ik}(t ) n_k \\sum_{h=1}^n \\phi_{ih}(t ) n_h\\right ) \\\\   &",
    "\\qquad + \\frac{b^2}{a n^2 } \\left (   \\sum_{h=1}^n n_h \\sum_{\\ell=1}^n n_\\ell \\sum_{k=1}^n \\phi_{ik}(t)^2 n_k^2 \\right.\\\\   & \\qquad\\left .",
    "\\left.- \\sum_{h=1}^n \\phi_{ih}(t )      n_h \\sum_{\\ell=1}^n",
    "\\phi_{i\\ell}(t )      n_\\ell \\sum_{k=1}^{n } n_k^2 \\right)\\right|\\\\ & =    \\frac{1}{\\eta_i(t)^2 } \\bigg| \\frac{b}{a",
    "n }      \\sum_{h=1}^n \\sum_{k=1}^n   n_h n_k \\phi_{ik}(t )    \\left ( \\phi_{ik}(t )    -   \\phi_{ih}(t ) \\right ) \\\\   & \\qquad + \\frac{b^2}{a n^2 }    \\sum_{h=1}^n \\sum_{\\ell=1}^n \\sum_{k=1}^n n_h n_\\ell n_k^2 \\left(\\phi_{ik}(t)^2 - \\phi_{ih}(t )       \\phi_{i\\ell}(t ) \\right.\\\\   & \\qquad + \\left.\\phi_{ik}(t )",
    "\\phi_{i\\ell}(t ) - \\phi_{ik}(t ) \\phi_{i\\ell}(t ) \\right)\\bigg|\\\\ & \\leq    \\frac{1}{\\eta_i(t)^2}\\bigg",
    "n }      \\sum_{h=1}^n \\sum_{k=1}^n   n_h n_k    \\phi_{ik}(t ) \\left| \\phi_{ik}(t )    -   \\phi_{ih}(t ) \\right| \\\\   & \\qquad + \\frac{b^2}{a",
    "n^2 }    \\sum_{h=1}^n \\sum_{\\ell=1}^n \\sum_{k=1}^n n_h n_\\ell n_k^2",
    "\\phi_{ik}(t ) \\big ( \\left|\\phi_{ik}(t ) - \\phi_{ih}(t)\\right|\\\\   & \\qquad + \\left|\\phi_{ik } - \\phi_{i\\ell}(t ) \\right| \\big ) \\bigg ] \\end{split}\\ ] ] where the last inequality follows by using the triangular inequality .",
    "now , weak ergodicity of @xmath100 implies that for any @xmath283 it holds @xmath284 for some @xmath285 and @xmath286 , see , e.g. , ( * ? ? ?",
    "* corollary 8) , so that the exponential convergence of @xmath287 $ ] follows .",
    "then , from the definition of the coefficient of ergodicity , it follows @xmath288|   \\nonumber \\\\&\\leq\\frac{1}{\\sum_{k=1}^n \\phi_{ik}(t ) n_k } \\left ( \\frac{b}{a n }     n   \\delta(\\phi(t ) ) + \\frac{b^2}{a n^2 }     n^2 { n_{\\text{max}}}2 \\delta(\\phi(t ) ) \\right)\\end{aligned}\\ ] ] where we have simplified the common factor @xmath289 . finally , by lemma  [ lem : weak_ergodicity ] , @xmath290 , with @xmath115 so that @xmath288|   & \\leq    \\frac{\\delta(\\phi(t ) ) b(1 + 2b{n_{\\text{max}}})}{\\mu a } \\ ] ] thus concluding the proof .      the local update of the ad - hoc distributed estimator ( algorithm  [ alg : ad - hoc_estimator ] ) can be written as @xmath291 , where @xmath292 and @xmath235 . using again the independence between @xmath172 and @xmath137 ,",
    "we can write @xmath293 $ ] and @xmath294 $ ] as @xmath295 = { \\mathrm{e}}[xy |\\lambda_j ] = { \\mathrm{e}}[x |\\lambda_j ] { \\mathrm{e}}[y |\\lambda_j ]        \\label{eq : tot_e_cond_ah}\\ ] ] and @xmath296={\\mathrm{e}}^2[x |\\lambda_j ] { \\mathrm{var}}[y |\\lambda_j]\\nonumber\\\\                                     & + { \\mathrm{var}}[x |\\lambda_j ] ( { \\mathrm{e}}^2[y |\\lambda_j ] \\!+\\!{\\mathrm{var}}[y |\\lambda_j ] ) .",
    "\\label{eq : tot_var_cond_ah }    \\end{aligned}\\ ] ] considering , as in the previous theorem , the taylor expansion for the moments of the function @xmath297 around the mean @xmath298=b$ ] , we obtain @xmath299 = { \\mathrm{e}}[x({\\hat{b}^{\\text{hom}}}_j(t))]\\nonumber\\\\      & \\approx x({\\mathrm{e}}[{\\hat{b}^{\\text{hom}}}_j(t)])+ \\frac{1}{2 } x''({\\mathrm{e}}[{\\hat{b}^{\\text{hom}}}_j(t ) ] ) { \\mathrm{var}}[{\\hat{b}^{\\text{hom}}}_j(t ) ] \\nonumber\\\\    & = \\frac{b}{1+n_j b }    - \\frac{n_j}{(1+n_j b)^3 } { \\mathrm{var}}[{\\hat{b}^{\\text{hom}}}_j(t ) ]    \\label{eq : e_x}\\end{aligned}\\ ] ] where @xmath185 $ ] is the one given in and , again , the @xmath184 symbol indicates that we have neglected higher - order terms in the expansion of @xmath300 .",
    "hence , plugging and in , equation follows .",
    "similarly , @xmath301 \\approx \\left(x'({\\mathrm{e}}[{\\hat{b}^{\\text{hom}}}_j(t)])\\right)^2    { \\mathrm{var}}[{\\hat{b}^{\\text{hom}}}_j(t ) ] \\nonumber\\\\    & = \\left [ \\frac{1}{(1+n_j b)^2 } \\right]^2   \\left ( \\frac{b}{a n } +    \\frac{b^2}{a n^2 } \\sum_{i=1}^{n } n_i^2",
    "\\right ) .",
    "\\label{eq : var_x}\\end{aligned}\\ ] ] using - into , equation follows , thus concluding the first part of the proof .",
    "a.  coluccia and g.  notarstefano , `` distributed bayesian estimation of arrival rates in asynchronous monitoring networks , '' in _ ieee international conference on acoustics , speech and signal processing ( icassp ) _ , 2014 , pp .",
    "50505054 .",
    "s.  barbarossa , s.  sardellitti , and p.  di  lorenzo , _ distributed detection and estimation in wireless sensor networks _ , ser .",
    "communications and radar signal processing.1em plus 0.5em minus 0.4emacademic press library in signal processing , october 2013 , vol .  2 , ch .  7 , pp . 329408 .",
    "i.  d. schizas , a.  ribeiro , and g.  b. giannakis , `` consensus in ad hoc wsns with noisy links part i : distributed estimation of deterministic signals , '' _ ieee transactions on signal processing _ ,",
    "56 , no .  1 ,",
    "pp . 350364 , 2008 .",
    "p.  frasca , h.  ishii , c.  ravazzi , and r.  tempo , `` distributed randomized algorithms for opinion formation , centrality computation and power systems estimation : a tutorial overview , '' _",
    "european journal of control _",
    ", 2015 .",
    "s.  barbarossa and g.  scutari , `` decentralized maximum - likelihood estimation for sensor networks composed of nonlinearly coupled dynamical systems , '' _ ieee transactions on signal processing _ , vol .",
    "55 , no .  7 , 2007 .",
    "a.  chiuso , f.  fagnani , l.  schenato , and s.  zampieri , `` gossip algorithms for simultaneous distributed estimation and classification in sensor networks , '' _ ieee journal of selected topics in signal processing _ ,",
    "vol .  5 , no .  4 , pp .",
    "691706 , 2011 .",
    "f.  fagnani , s.  m. fosson , and c.  ravazzi , `` input driven consensus algorithm for distributed estimation and classification in sensor networks , '' in _ 50th ieee conference on decision and control and european control conference ( cdc - ecc)_.1em plus 0.5em minus 0.4emieee , 2011 , pp .",
    "66546659 .",
    "d.  varagnolo , g.  pillonetto , and l.  schenato , `` distributed consensus - based bayesian estimation : sufficient conditions for performance characterization , '' in _ american control conference_.1em plus 0.5em minus 0.4emieee , 2010 , pp .",
    "39863991 .    p.  di",
    "lorenzo and s.  barbarossa , `` distributed least mean squares strategies for sparsity - aware estimation over gaussian markov random fields , '' in _ ieee international conference on acoustics , speech and signal processing ( icassp ) _ , 2014 , pp .",
    "54725476 .",
    "g.  mateos , i.  d. schizas , and g.  b. giannakis , `` distributed recursive least - squares for consensus - based in - network adaptive estimation , '' _ ieee transactions on signal processing _ , vol .",
    "57 , no .  11 , pp . 45834588 , 2009 .",
    "i.  d. schizas , g.  b. giannakis , s.  i. roumeliotis , and a.  ribeiro , `` consensus in ad hoc wsns with noisy links part ii : distributed estimation and smoothing of random signals , '' _ ieee transactions on signal processing _ ,",
    "56 , no .  4 , pp . 16501666 , 2008 .",
    "f.  zanella , d.  varagnolo , a.  cenedese , g.  pillonetto , and l.  schenato , `` asynchronous newton - raphson consensus for distributed convex optimization , '' in _",
    "3rd ifac workshop on distributed estimation and control in networked systems ( necsys12 ) _ , 2012 .",
    "j.  n. tsitsiklis , d.  p. bertsekas , m.  athans _",
    "et  al . _ ,",
    "`` distributed asynchronous deterministic and stochastic gradient optimization algorithms , '' _ ieee transactions on automatic control _ , vol .  31 , no .  9 , pp . 803812 , 1986 .",
    "a.  jadbabaie , j.  lin , and a.  s. morse , `` coordination of groups of mobile autonomous agents using nearest neighbor rules , '' _ ieee transactions on automatic control _ ,",
    "48 , no .  6 , pp .",
    "9881001 , 2003 .",
    "f.  bnzit , v.  blondel , p.  thiran , j.  tsitsiklis , and m.  vetterli , `` weighted gossip : distributed averaging using non - doubly stochastic matrices , '' in _ ieee international symposium on information theory proceedings ( isit)_.1em plus 0.5em minus 0.4emieee , 2010 , pp .",
    "17531757 .",
    "n.  h. vaidya , c.  n. hadjicostis , and a.  d. dominguez - garcia , `` distributed algorithms for consensus and coordination in the presence of packet - dropping communication links - part ii : coefficients of ergodicity analysis approach , '' _ arxiv preprint arxiv:1109.6392 _ , 2011 .",
    "[ ] angelo coluccia ( m13 ) received the eng .",
    "degree in telecommunication engineering ( summa cum laude ) in 2007 and the phd degree in information engineering in 2011 , both from the university of salento , lecce , italy .",
    "former researcher at forschungszentrum telekommunikation wien , vienna , since 2008 he has been engaged in research projects on traffic analysis , security and anomaly detection in operational cellular networks .",
    "he is currently assistant professor at the dipartimento di ingegneria dellinnovazione , university of salento , where he teaches the course of telecommunication systems .",
    "his research interests are signal processing , communications and wireless networks , in particular cooperative sensing / estimation approaches for localization and other ( possibly distributed ) applications .",
    "[ ] giuseppe notarstefano has been an assistant professor ( ricercatore ) at the universit del salento ( lecce , italy ) since february 2007 .",
    "he received the laurea degree `` summa cum laude '' in electronics engineering from the universit di pisa in 2003 and the ph.d .",
    "degree in automation and operation research from the universit di padova in april 2007 .",
    "he has been visiting scholar at the universities of stuttgart , california santa barbara and colorado boulder .",
    "his research interests include distributed optimization , cooperative control in multi - agent networks , applied nonlinear optimal control , and trajectory optimization and maneuvering of aerial and car vehicles .",
    "he serves as an associate editor in the conference editorial board of the ieee control systems society and for the european control conference , ifac world congress and ieee multi - conference on systems and control .",
    "he coordinated the vi - rtus team winning the international student competition virtual formula 2012 .",
    "he is recipient of an erc starting grant 2014 ."
  ],
  "abstract_text": [
    "<S> in this paper we consider a network of agents monitoring a spatially distributed arrival process . </S>",
    "<S> each node measures the number of arrivals seen at its monitoring point in a given time - interval with the objective of estimating the unknown local arrival rate . </S>",
    "<S> we propose an asynchronous distributed approach based on a bayesian model with unknown hyperparameter , where each node computes the minimum mean square error ( mmse ) estimator of its local arrival rate in a distributed way . as a result , </S>",
    "<S> the estimation at each node `` optimally '' fuses the information from the whole network through a distributed optimization algorithm . </S>",
    "<S> moreover , we propose an _ ad - hoc _ distributed estimator , based on a consensus algorithm for time - varying and directed graphs , which exhibits reduced complexity and exponential convergence . we analyze the performance of the proposed distributed estimators , showing that they : ( i ) are reliable even in presence of limited local data , and ( ii ) improve the estimation accuracy compared to the purely decentralized setup . </S>",
    "<S> finally , we provide a statistical characterization of the proposed estimators . in particular , for the ad - hoc estimator , we show that as the number of nodes goes to infinity its mean square error converges to the optimal one . </S>",
    "<S> numerical monte carlo simulations confirm the theoretical characterization and highlight the appealing performances of the estimators .    </S>",
    "<S> distributed estimation , empirical bayes , push - sum consensus , cyber - physical systems . </S>"
  ]
}