{
  "article_text": [
    "the fermionic quantum many - body problem is believed to be in the class of problems whose full solution is exponentially hard @xcite .",
    "approximate methods have been developed , but many of these are also extremely computationally demanding .",
    "there is , therefore , an ongoing search for efficient approximate methods , useful , for example , in computational surveys of wide classes of materials , or to provide a first look at a complicated situation .",
    "+   + the predominant approach has been to use simplifying approximations , for example , truncated perturbation - theory series expansions , variants of mean - field theory , quasiclassical approximations , or analytical interpolation functions .",
    "the development of machine learning ( ml ) techniques in computer science motivates us to explore a complementary approach .",
    "ml provides an estimate of the result of a calculation based on interpolation from a statistical analysis of datasets of solved problems @xcite .",
    "ml is widely used in many big - data applications , and has recently been proposed as a method for obtaining approximate solutions of the equations of density functional theory ( dft)@xcite , of the molecular electronic schrdinger equation  @xcite , and of transmission coefficients for electron quantum transport  @xcite .",
    "ml is also used to construct force - fields from molecular dynamics@xcite .",
    "+   + in this paper we investigate ml techniques to infer solutions to the quantum many - body problem arising in applications of the dynamical mean - field theory ( dmft ) method.@xcite dmft has become widely used in condensed - matter physics and materials science for obtaining nonperturbative information about materials with strong local correlations .",
    "while dmft is an approximation to the full many - body problem , it does require the solution of a fully interacting quantum impurity model ( a quantum field theory defined in zero space but one time dimension ) , and accurate solutions require substantial numerical effort which is time consuming even with modern algorithms and hardware@xcite .",
    "a sufficiently accurate ml model of dmft could provide an inexpensive _ solver _ , useful for rapid preliminary screening of wide ranges of materials and as a method for identifying promising starting points for further refinement using more expensive and sophisticated methods or experiments .",
    "+   + in its conventional formulation , dmft maps one function of frequency into another .",
    "the input is the _ bare hybridization function _ , which encapsulates relevant information about the crystal structure and quantum chemistry of a material via a representation of what the local density of electronic excitations would be if many - body correlations were neglected .",
    "a small number of additional parameters , such as the on - site interaction strength , must also be specified .",
    "the output is the electron green s function ( or equivalently self - energy ) , which provides an approximation to the exact density of states ( dos ) obtained by including local effects of many - body correlations .",
    "+   + implementing a ml approach to dmft thus entails constructing a training set of physically reasonable hybridization functions , determining the spectral functions corresponding to the training examples , and constructing a model that provides the needed interpolation formula .",
    "such a ml procedure goes beyond previous applications of ml to electronic structure because we are mapping a function to a function whereas the ml approaches to dft or the schr@xmath0edinger equation provide only a small number of scalar outputs , such as the total energy of atomization , ionization potential , or excitation energy@xcite .",
    "a key issue is thus to devise an efficient representation of the functions of interest in terms of a reasonably sized set of parameters .",
    "the first application we foresee for real systems is a material science computation tool to optimize a desired property .",
    "+   + in this paper we address this key issue for the anderson impurity model ( aim ) , the archetypical quantum impurity model . for this model",
    "the input hybridization function is known _ a priori _ , and can be specified by few parameters .",
    "the focus therefore lies on the prediction of the output , namely , the electron green s function . in future",
    "work we will discuss ml applications to the full dmft problem of determining the self - consistent relation between the green s function and hybridization function , starting from an arbitrary hybridization function . +   + this paper is organized as follows : in sec .",
    "[ ml_supervised ] , we summarize the supervised learning approach . in sec .",
    "[ ml_supervised_repres ] we discuss how to represent a function for ml and in sec .",
    "[ ml_supervised_kernel ] we discuss the kernel ridge regression that we use in this paper , while in sec .",
    "[ quality_ml ] we show how the ml procedure is tested . in sec .",
    "[ qi_aim ] the one site aim and its solution by exact diagonalization are presented . in sec .",
    "[ qi_ml_aim ] we present the machine learning solution .",
    "section  [ qi_metho ] discusses the methodology of the calculation .",
    "section  [ qi_repre_g ] presents in detail the four types of representation for the green s functions we study : in sec .",
    "[ qi_repre_g_cont_frac ] the continued fraction , in sec .",
    "[ qi_sec_gwn ] the matsubara frequency , in sec .",
    "[ qi_sec_gtau ] imaginary time , and in sec .",
    "[ qi_sec_gl ] legendre polynomials .",
    "the results for these different representations are shown in secs .",
    "[ qi_sec_cont_frac_results ] , [ qi_sec_gwn_results ] , [ qi_sec_gtau_results ] and [ qi_sec_gl_results ] . in secs .",
    "[ qi_sec_prediction_size ] and [ qi_sec_mass_enh ] we show how the size of the learning set affects the predictions of the dos and the mass enhancement . finally , in sec .  [ qi_predic_min_set ]",
    "we look at how we can define an absolute minimal learning set for our problem .",
    "section  [ sum_concl ] is the summary and conclusion .",
    "appendix  [ ml_appen ] gives details on the kernel ridge regression , appendix  [ ed_appen ] gives details of the exact diagonalization method , appendix  [ legendre_appen ] gives details of the representation of the green s function using legendre polynomials , and appendix  [ appen : eff_alpha ] presents the derivation of the effective @xmath1 matrix for the legendre polynomials approach .",
    "we wish to learn a function of one complex variable , @xmath2 , with @xmath3 representing the green s function or self - energy .",
    "the model whose solution gives @xmath2 is specified by a descriptor , @xmath4 , the set of input parameters needed to describe the model . where appropriate we denote the dependence of @xmath3 on the input parameters as @xmath5 .",
    "the ml approach is to infer @xmath5 from a given set of @xmath6 results @xmath7 .",
    "+   + the functions of physical interest have a spectral representation @xmath8 with @xmath9 a real number .",
    "the spectral function @xmath10 is non - negative , integrable , and typically nonzero only over a finite range of @xmath9 .",
    "+   + while @xmath3 is fully specified by @xmath11 , for technical reasons one often has data only for , or is only interested in , @xmath3 on the _ matsubara frequencies _",
    "@xmath12 where @xmath13 is the temperature , @xmath14 and @xmath15 is an integer .",
    "@xmath3 is sometimes also studied on the _ imaginary time _",
    "( @xmath16 ) axis in the interval @xmath17 .",
    "the @xmath16-dependent function is related to the matsubara frequency values by @xmath18 it is worth noting that the definition of the matsubara frequencies along with eq",
    ".   implies three important results that will be used later : @xmath19 where @xmath20 is the first moment of the spectral function which depends upon the particular hamiltonian .",
    "+   + equation   implies that the values of @xmath2 at different @xmath21 values are correlated .",
    "this presents a challenge since the machine learning algorithms we will use treat the different components of @xmath3 ( or the coefficients of its expansion ) as independent . therefore , by independently adding numerical errors on each point the machine may not fully respect the needed correlations and constraints . put differently , the @xmath3 predicted by our machine learning algorithm could arise from a spectral function with negative regions .",
    "this issue is not unique to ml and is related to the well - known _ analytical continuation _ problem of inverting eq .   to determine @xmath10 from the full set of values of @xmath22 . while general theorems from complex variable theory imply that the inversion is possible in principle , the kernel @xmath23 of eq",
    ".   has many very small eigenvalues so the inversion problem is ill conditioned .",
    "however , in all the cases we have considered , these issues seem not to pose any difficulties in practice , so we proceed straightforwardly , with the assumptions justified _ a posteriori_. +   + in any numerical approach to problems involving continuous functions , a choice of discretization must be made .",
    "any discretization expresses the continuous function @xmath2 in terms of a set of @xmath24 numbers @xmath25 which we assemble into an @xmath24-dimensional vector @xmath26 : @xmath27 one must note that the @xmath28 s need not be direct points of the function but can also be coefficients that define the function .",
    "+   + we have considered four discretizations of @xmath2 : ( 1 ) a continued fraction representation ; ( 2 ) values on the matsubara axis @xmath22 up to some cutoff frequency @xmath29 ; ( 3 ) discrete values on the imaginary time axis , @xmath30 ; and ( 4 ) an expansion of @xmath31 in terms of orthogonal ( legendre ) polynomials .",
    "both the continued fraction and legendre polynomials representation lead to a correctly normalized and non - negative spectral function , but as will be seen the continued fraction representation is for other reasons not optimal .",
    "the other representations do not necessarily lead to a @xmath2 which has a spectral representation with a non - negative spectral function for small and random sets of @xmath6 results , but , as will be seen , they seem to work well in practice .",
    "in this paper we apply the kernel approach@xcite , which involves two ingredients : a _ distance kernel _ @xmath32 , a symmetric and positive definite function , fixed _ a priori _ , and a coefficient matrix @xmath33 which is to be determined . in terms of these",
    ", one generates an approximation @xmath34 to the desired @xmath3 given by the so - called kernel ridge regression ( krr ) , an expansion in an abstract kernel space . in terms of the expansion defined in eq .  )",
    "we approximate the components @xmath28 of @xmath26 in terms of approximate components @xmath35 given by @xmath36 here we use the subscript @xmath37 to label the entries in @xmath3 ( i.e. , the different components in the discrete representation of the function : direct points or appropriate coefficients ) and @xmath38 to label the training examples . +   + the coefficients @xmath33",
    "are calculated by minimizing a cost function defined in terms of the difference between the approximations @xmath39 and the exactly known @xmath40 , with the addition of a set of lagrange multipliers introduced to regularize the problem . in our approach",
    "we assume that each component @xmath37 of @xmath26 is learned separately and independently , so we define a cost function separately for each @xmath37 as @xmath41 minimization then gives at fixed @xmath37 ( see appendix  [ ml_appen ] for details ) @xmath42 in eq .  , @xmath43 is a column vector of length @xmath6 containing the different @xmath1 for @xmath37 fixed and @xmath44 is also a column vector of length @xmath6 but containing the different values of @xmath28 ( @xmath37 fixed ) on the training set ( @xmath45 ) .",
    "it is important to note that this vector @xmath44 is not the same as the vector @xmath26 in eq .   which is a vector of length @xmath24 containing the values of @xmath46 for one specific @xmath4 .",
    "finally , @xmath47 and @xmath48 are the kernel and identity matrices of size @xmath49 .",
    "+   + in our actual calculations we assume that the kernel @xmath50 and the lagrange multiplier @xmath51 are independent of @xmath37 , i.e. , the same for all components of the function to be learned",
    ". we may then assemble all of the examples @xmath44 into a @xmath52 matrix @xmath53 in which each column is for a different @xmath37 and combine eq .   into a unique matrix equation @xmath54",
    "this makes the process of learning a function very efficient since even if @xmath55 is a very large matrix , the solution needs to be obtained only once . for @xmath50 , we use the weighted exponential kernel @xmath56 where @xmath57 is the manhattan distance between the two parameter sets and @xmath58 gives the radius of effect that a particular point of the data set @xmath59 will have in the prediction process@xcite . +   + for numerical calculation , direct matrix inversion should be avoided and eq .",
    "is solved in the form @xmath60 . since @xmath61 is a square , real , symmetric , and positive definite matrix ,",
    "a standard cholesky solver can be used .",
    "the process is very fast ; for example , on a desktop computer with a intel core i7 - 4770 quad core at 3.40 ghz , using a dataset of 5000 points , it takes about 4 s to learn 2400 parameters defining @xmath62 in its continued fraction representation plus the ground - state energy .",
    "+   + the way we _ train _ our machine is to choose @xmath6 results from a database of solutions .",
    "this subset of solutions is called the training set and we keep the remaining solutions in the database apart and consider a subset of them as the test set .",
    "the training set is then used to construct @xmath63 and eq .   can be solved .",
    "we then use the test set to check if the predictions from the ml give accurate results and thus can be used to predict solutions not contained in the database .",
    "more details on the training process we use are given in sec .",
    "[ qi_metho ] .",
    "+   + as shown in this section , we can see that ml is an interpolation approach , never an extrapolation .",
    "the obvious question of how small the shift in parameters of a desired prediction @xmath4 from the @xmath64 must be ( i ) depends on the training set density ( the @xmath6 examples that will be used to obtain the @xmath33 ) , ( ii ) depends on it s distribution , and ( iii ) still has to be investigated in more rigorous ways than is done either this paper or in the literature in general .      in most ml studies ,",
    "the quality of the machine is assessed in terms of the mean absolute error ( mae ) of the predicted quantities  @xcite . because we are interested in predicting a function we introduce a different metric , the average relative difference ( ard ) , defined as @xmath65 where the factor @xmath66 is introduced so that the ard is given in percentage and @xmath67 denotes the average over a suitably defined set of function arguments @xmath68 . in practice",
    "we take the set of arguments @xmath69 to be the first few matsubara frequencies corresponding to energies up to the typical scales of the problem ; the values at higher @xmath70 are typically controlled by sum rules and are small , with very small errors .",
    "the matsubara frequencies are an appropriate measure because ( i ) they contain the relevant physical information , ( ii ) there is a natural discretization , and ( iii ) the use of a common estimator permits straightforward comparison of different approaches .",
    "note that three of the four representations ( the continued fraction , the matsubara frequency , and legendre polynomials ) give @xmath71 in a direct way .",
    "we only compare @xmath72 .",
    "+   + however , the ard or any measure that give an error representation for the entire function as a single number can be misleading , because it can mask narrow but important regions of the function which might be badly predicted .",
    "therefore , as a further test of the quality of the machine , we examine in detail , for two specific cases the predictions of the ml for ( a ) @xmath72 ( b ) the density of states obtained by using pad analytical continuation , and ( c ) the low - frequency renormalization factor @xmath73 estimated as the extrapolation @xmath74 this renormalization factor is a unique number characterizing an important low - frequency property of the model .",
    "in this paper we apply machine learning techniques to the single impurity anderson impurity model .",
    "the hamiltonian is @xmath75 as shown in fig  [ fig : aim](a ) , this represents one localized level ( with two - fold spin degeneracy ) , embedded in a bath of noninteracting electrons .",
    "the localized electronic states are represented by the creation ( annihilation ) operators @xmath76 ( @xmath58 the spin ) while the continuous bath states are represented by the operators @xmath77 .",
    "the on - site energy of the impurity is @xmath78 and the interaction term which penalizes double occupancy if positive , is @xmath79 .",
    "the bath dispersion is @xmath80 and the hybridization between the bath and the impurity is @xmath81 , where @xmath82 is a wave number .",
    ", the aim can be uniquely described by three parameters @xmath83 $ ] and be used in a ml approach to predict correlation functions and other physical properties . ]",
    "+   + the relevant features of the bath and hybridization are encoded by the hybridization function @xmath84 which for this model is @xmath85 we choose the bath to have a semicircular density of states and to define the energy unit to be the half width @xmath86 , so @xmath87 we further choose the hybridization to be a constant , @xmath88 and set the chemical potential to be equal to the energy at the center of the bath density of states @xmath89 in the notations of eq .  .",
    "the parameters of the model are then @xmath90 , and @xmath78 .",
    "we find it more convenient to replace the on - site energy @xmath78 by the occupation @xmath91 , which is a single - valued function of @xmath78 for nonzero @xmath88 and which we multiply by the half bandwidth @xmath92 so it has the same dimension as the other parameters .",
    "the descriptor will be a vector of three values @xmath93.\\ ] ] the machine learning task is then to predict the green s function of the model , @xmath94 , in terms of @xmath95 .",
    "this ml representation of the aim , as well as what the prediction leads to , is shown in fig  [ fig : aim](b ) .",
    "+   + the anderson impurity model is a full many - body problem , and while an exact bethe - ansatz solution exists for particular choices of parameters@xcite , there is no known exact analytic solution for the general case . while many different methods are available for solving the problem including approximate ones ( such as the renormalized strong - coupling expansion proposed by krivenko _",
    "[ ] and diagrammatic resummation methods such as the non - crossing approximation ( nca ) and one - crossing approximation ( oca ) as in [ ] ) and numerically exact ones ( quantum monte carlo ( qmc ) [ ] and numerical renormalization group ( nrg ) [ ] ) , we will use the exact diagonalization ( ed ) method [ ] , in which the hybridization function is represented as a sum of small number ( @xmath96 ) poles and weights in the form @xmath97 one is then left with a finite - size hamiltonian ( size @xmath98 ) which can be diagonalized exactly , allowing computation of the many - body ground - state wave function and energy . the green s function is obtained as a continued fraction : @xmath99 we define the exact green s function as eq .",
    "truncated at 600 continued fraction coefficients but note that the results for our model are not materially different if only the first 100 coefficients are retained .",
    "we use eq .   to compute @xmath62 on the real axis , on the matsubara axis ( with a fictitious temperature @xmath100 ) , or as a function of imaginary time by fourier transform .",
    "appendix  [ ed_appen ] gives more details about ed .",
    "we created a database of examples as input to the machine learning process by solving the anderson impurity model for @xmath101 combinations of @xmath79,@xmath88 , and @xmath102 .",
    "the interaction @xmath79 was varied from a small value to twice the bandwidth 0.16 to 4 in 25 equal intervals .",
    "the hybridization @xmath88 is varied from a small value to the order of half the bandwidth 0.1 to 0.75 in 20 equal intervals .",
    "finally , the filling of the impurity @xmath102 is varied from 0.6 to 1.4 ( 0 - 40% doping in both sides ) in intervals of 0.1 and we also included @xmath103 . for ed ,",
    "we have used @xmath104 and as already mentioned a fictitious temperature of @xmath100 . for the aim with a bath with finite bandwidth , in the low - temperature limit",
    ", the kondo temperature is given by@xcite @xmath105 where @xmath106 .",
    "the validity of eq .   is limited for @xmath107 .",
    "this leads for our parameters to a @xmath108 in the range from about @xmath109 to a maximum of about 0.48 .",
    "this maximal @xmath108 is obtained for maximum @xmath79 , @xmath88 , and doping , i.e. , @xmath110 , @xmath111 , and @xmath112 or @xmath113 . +   + from the database of 5000 solutions we randomly choose @xmath114 results to serve as the learning and the test sets .",
    "these @xmath114 results are then divided into @xmath115 subsets .",
    "one is the testing set and the remaining @xmath116 form the learning set .",
    "for example , to calculate the ard , we always use test sets of 100 such that we will choose @xmath117 .",
    "we use a null lagrange multiplier ( @xmath118 ) and @xmath58 is most of the time equal to @xmath119 except for one case where we considered @xmath120 . +   +      we have investigated four different representations of the green s function : ( i ) the continued fraction representation , eq .  ;",
    "( ii ) the green s function in matsubara frequency ; ( iii ) the green s function in imaginary time ; and finally , ( iv ) a representation of the imaginary time green s function as a sum of legendre polynomials .      here",
    "we proceed directly from eq .  .",
    "we must learn some number @xmath121 of coefficients @xmath122 and @xmath123 for the particle and hole green s function , along with the ground - state energy , thus @xmath124 coefficients .",
    "hence @xmath26 ( eq .  ) is a vector with 2401 elements ( 401 if we only learn the first 100 ) and @xmath55 is    @xmath125    it is important to note that although the continued fraction representation is a formal way to write any function with a spectral representation , in practice , an accurate numerical process for obtaining the coefficients is available only in the context of ed calculations",
    ".      we may evaluate the calculated @xmath62 on the matsubara points @xmath126 ( many qmc codes also give @xmath62 evaluated on these points )",
    ". then @xmath127      by evaluating eq .",
    "on the matsubara points and then fourier transforming we obtain the green s function in imaginary time ( this is also a standard output of qmc calculations ) .",
    "@xmath128 is real and smooth .",
    "we then approximate the continuous @xmath128 by its values on the @xmath121 discrete points @xmath129 with @xmath130 .",
    "the value at @xmath131 does not have to be learned since it can be obtained from eq .  ) .",
    "it is also useful to learn a @xmath132 point , namely , the first derivative at @xmath133 .",
    "knowledge of this value helps in evaluation of the reverse fourier transform@xcite . in this work",
    "we use @xmath134 and we write    @xmath135      recently , boehnke _",
    "et al_.@xcite proposed to represent the imaginary time green s function as a sum of legendre polynomials and measure the coefficients in a qmc calculation .",
    "legendre polynomials are chosen instead of other sets of orthogonal polynomials because of the simplicity they offer for transformation to the matsubara axis .",
    "such an expansion acts as a physically motivated low - pass filter that eliminates the large statistical noise at high frequency coming from the direct calculation of @xmath62 in matsubara frequency well known in continuous time qmc ( ctqmc ) .",
    "the standard legendre polynomials @xmath136 are defined on an interval @xmath9 @xmath137 @xmath138 $ ] . in the case of the green s function in positive imaginary time ,",
    "the time interval is @xmath139 so that we may define the variable @xmath140 .",
    "thus ( see appendix  [ legendre_appen ] ) @xmath141 where the coefficients are formally given by @xmath142 the fourier transform to @xmath70 is given by [ ] as @xmath143 where @xmath144 and @xmath145 are the spherical bessel functions .",
    "@xmath146 may be directly measured in a ctqmc calculation @xcite and the maximum order @xmath147 is defined as the largest @xmath38 where @xmath146 is greater than the statistical noise . in the present case",
    "we must calculate the @xmath146 and devise an alternative prescription for @xmath147 .",
    "we use a recently introduced algorithm based on a fast chebyshev - legendre transform@xcite that exploits the idea that smooth functions can be represented by polynomial interpolation in chebyshev points , i.e. , by expansions in chebyshev polynomials using fast fourier transform .",
    "this algorithm is implemented in a free matlab toolbox called chebfun@xcite .",
    "we still have to define @xmath147 .",
    "the @xmath147 is chosen by looking at the odd legendre coefficients .",
    "for every example we have investigated , we find that @xmath146 for @xmath38 odd decreases rapidly as @xmath38 increases . at some @xmath38 ,",
    "@xmath146 changes sign and starts oscillating around zero .",
    "we set @xmath147 by finding the @xmath38 at which @xmath146 changes sign .",
    "for each example ( a particular set of parameters @xmath79 , @xmath88 , and @xmath148 ) , the value of @xmath38 for which the sign change happens is different .",
    "however , for the 5000 examples in the database , we found that the first sign change happens for @xmath38 at most around 111 .",
    "for security , we use as our definition of the expansion the first 121 ( @xmath149 ) terms . for each example , for @xmath150 for the odd @xmath38",
    "we use the values we have while we have to decide what values to use for @xmath151 ( for @xmath38 even we do not change anything ) .",
    "two easily implemented options are to replace the odd coefficients by zero for @xmath151 or to replace them by the last value before the first sign change of @xmath146 so that either @xmath152 for @xmath38 odd @xmath153 or @xmath154 .",
    "we verified that both solutions work very well to reconstruct @xmath128 from eq .",
    "however , for machine learning there is a difference . indeed , in ml",
    ", having pure zeros in the learning set makes the learning process much more difficult .",
    "we thus use the second solution where @xmath146 for @xmath38 odd @xmath153 is a very small constant .",
    "+   + the great advantage of the legendre polynomial representation is that either by obtaining @xmath146 from ctqmc or directly from @xmath128 , the number of coefficients is very limited , making the learning process much smaller , and , if after learning all coefficients at once , we see that some fine tuning is needed , the maximum number of new machines that will be necessary is limited and manageable . also , as we do not directly learn the function @xmath155 but reconstruct it from eq .",
    "( or eq .  ) it helps smooth things out .",
    "+   + therefore what is directly learned is the vector of coefficients . @xmath156",
    "in this section we present the results both of computation of the full @xmath62 and , for two particular examples , of the renormalization factor @xmath157 .",
    "for the two examples we will use parameters similar to those used in [ ] and [ ] . in one case [ ] @xmath158 , @xmath159 , and @xmath160 . in the second case [ ] , an infinite @xmath79 aim is considered with a flat conduction - band dos with @xmath161 and @xmath162 . in the case of infinite @xmath79 [ ] ,",
    "this gives an occupation @xmath163 .",
    "we will use the two examples in our database that are the closest to these parameters .",
    "for the first case , we have @xmath164 , @xmath165 , and @xmath166 . for the second case , as infinite @xmath79",
    ", we will use @xmath110 , @xmath167 , and @xmath168 .",
    "we present the results both in @xmath70 and @xmath169 .",
    "for real frequencies , we need to choose a small imaginary part @xmath170 for the frequency .",
    "since our ed fitting procedure relies on choosing a fictitious temperature to fit on the matsubara axis defined by this temperature , we take the minimum - energy unit to be twice the difference between two @xmath70 ( @xmath171 ) . for small dataset length , we generated many random combinations for the learning set , predicted the results for every combination and averaged out .",
    "we first use ml to learn the coefficients of the continued fraction representation of the green s function ( eq .  ) .",
    "we learned the first hundred of each type since , as we mentioned in sec .",
    "[ qi_aim ] , the remaining coefficients do not contribute to @xmath62 within our accuracy . using test sets of length 100 and generating learning and test sets multiple times , from the reconstructed @xmath62",
    "we calculate the ard , and we show the results as a function of training set size in fig .  [",
    "fig : ard_coeffs ] as dots .",
    "we see that the ard decreases from about 6.5% for a random learning set of 500 examples to about 0.016% for a learning set of 4900 .",
    "+   +    , squares ( @xmath172 ) denote the predictions of @xmath128 , and diamonds ( @xmath173 ) denote the legendre polynomial expansion . ]    for the comparison with the two specific examples , the results are presented in fig .",
    "[ fig : predic_wn_w_coeffs ] .",
    "only the first 50 matsubara frequencies are shown for @xmath72 .",
    "+   +    for the half - filled case , fig .",
    "[ fig : predic_wn_w_coeffs]-(a ) and ( b ) , we see that the prediction for randomly chosen learning sets of 500 in @xmath70 is not very good . in terms of real frequency , we see that , however , even with the size 500 random learning set , the high - frequency regions are well predicted . however , around the fermi level , the prediction is wrong .",
    "for example , the prediction is not particle - hole symmetric even though the model is .",
    "however , at the largest learning set size ( 4999 ) the prediction is correct .",
    "the doped case is similar . from fig .",
    "[ fig : predic_wn_w_coeffs](c ) we see that for this particular example , the value at the lowest matsubara frequency is well predicted for a small random learning set but the values at the next three frequencies are not good .",
    "turning now to real frequencies we see that the high - frequency regions and the region around the fermi level are qualitatively fairly predicted . for a large learning set ,",
    "the doped case is also well represented .",
    "+   + thus , if the green s function is learned from its continued fraction coefficients , a small and random learning set is able to capture the high - frequency physics for the half - filled case , but it is not reliable for the low @xmath169 which carry most of the interesting physical information at @xmath174 .",
    "the ards are denoted by x s in fig .",
    "[ fig : ard_coeffs ] .",
    "the values are systematically smaller than the equivalent from the continued fraction coefficients , but still the same order of magnitude .",
    "the real frequency result at half filling of fig .",
    "[ fig : predic_wn_w_gwn](b ) shows that this time , around the fermi level , the ml predicted dos has the correct qualitative behavior of particle - hole symmetry . if we compare the matsubara frequency results of fig .",
    "[ fig : predic_wn_w_gwn](a ) and ( c ) with those of fig .",
    "[ fig : predic_wn_w_coeffs](a ) and ( c ) we see that for these two examples , learning directly @xmath71 is better . however , the analytically continued results for the learning set of length 500 are of poor quality , essentially because analytical continuation is sensitive to small errors in @xmath71 .",
    "this is thus hard to assess if the prediction really contains noncausality ( fig .",
    "[ fig : predic_wn_w_gwn](b ) ) ( @xmath175 ) , gives such shape at high frequency ( fig .",
    "[ fig : predic_wn_w_gwn](b ) and ( d ) ) and give spurious states where the dos should be zero ( fig .",
    "[ fig : predic_wn_w_gwn](d ) at about @xmath176 ) .",
    "once again , for a large learning set , the predicted results are very good .",
    "it is important to note that since we need numerical analytical continuation to obtain the real frequency results , 100% perfect matching is impossible .",
    "+      the ards for imaginary time representation are denoted by squares in fig .",
    "[ fig : ard_coeffs ] .",
    "the values are systematically smaller than the corresponding ards from the continued fraction coefficients but still of the same order of magnitude and similar to those from the representation in matsubara frequency .",
    "most comments made regarding fig .",
    "[ fig : predic_wn_w_gwn ] can also be made for fig .",
    "[ fig : predic_wn_w_gtau ] .",
    "+   + based on the ard and the results in @xmath70 we can conclude that directly learning @xmath128 is a good procedure and of general applicability .",
    "the ards of the legendre orthogonal polynomials representation are denoted by diamonds in fig .",
    "[ fig : ard_coeffs ] .",
    "the values are very similar to those from @xmath128 and @xmath71 .",
    "the results for the two examples in matsubara and real frequency presented in fig .",
    "[ fig : predic_wn_w_gl ] are comparable to what is obtained from learning directly @xmath128 , but this time the pad continuation is less problematic because the coefficients are learned and then the green s function is reconstructed , giving a @xmath71 with less independent error on each @xmath70 . moreover , in sec .",
    "[ qi_sec_gtau ] , we had to learn 2048 slices of @xmath128 plus @xmath177 while here we only needed to learn 121 coefficients .",
    "+   + we may therefore conclude that the representation by an expansion in terms of legendre polynomials is the most efficient way to learn many - body green s functions using machine learning .",
    "it is important to show how our prediction of the dos evolves with increasing learning set length .",
    "[ fig : ard_coeffs ] displays the learning set length dependence of the @xmath178 .",
    "now we look at the dos around the fermi level for the half - filled case .",
    "the conclusions we show for the dos around the fermi level are generally applicable to the high - frequency results also .",
    "one exception is the continued fraction representation where much smaller learning sets can predict the high - frequency behavior ( see fig .",
    "[ fig : predic_wn_w_coeffs ] ) . in fig .",
    "[ fig : predic_dos_nd1_4_representations ] , we show the dos around @xmath179 for different random learning set lengths and for the four representations : in fig .",
    "[ fig : predic_dos_nd1_4_representations](a ) the continued fraction , in fig .",
    "[ fig : predic_dos_nd1_4_representations](b ) the matsubara frequency , in fig .",
    "[ fig : predic_dos_nd1_4_representations](c ) imaginary time , and in fig .",
    "[ fig : predic_dos_nd1_4_representations](d ) legendre polynomials . the dots ( . )",
    "denote the exact result , the red dashed lines ( - - ) denote the result for a learning set of length 500 , the blue dot - dashed lines ( - . )",
    "denote the result for a learning set of length 1000 , the cyan circles ( o ) denote the result for a learning set of length 2000 , and the magenta solid lines ( - ) denote the result for a learning set of length 3000 .",
    "we can see that convergence to the correct prediction is attained before the learning set is maximum ( 4999 ) .",
    "we now consider how well machine learning predicts the renormalization factor @xmath157 , equivalent in this model to the mass enhancement .",
    "in addition to being an important physical property for fermi liquids , @xmath157 also acts here as a unique number that can be used to estimate the quality of the ml prediction of low - frequency properties .",
    "the results are shown in fig .  [",
    "fig : z ] .",
    "overall , the @xmath157 predicted from ml learned continued fraction coefficients never completely converges ( the black line is the exact result ) and is quite inaccurate for learning set of 500 and 1000 when @xmath166 . even if it never perfectly converges , the relative difference at the larger learning set is small , consistent with the reasonable visual appearance of the dos ( fig  [ fig : predic_wn_w_coeffs ] ) . for @xmath168",
    "there is a peculiar concordance where the prediction is better for 500 than 1000 and 2000 .",
    "this is clearly accidental and the overall shape of the dos around @xmath179 is better for 1000 and 2000 .",
    "it is also important to note that for fig .",
    "[ fig : z]-(b ) the exact @xmath157 is small and thus we predict very small numbers . for the other three representations , they all systematically converged to the correct answer , around a learning set of 2000 for fig .  [",
    "fig : z](a ) , which also can be seen from fig .  , and around a learning set of 3000 for fig .",
    "[ fig : z]-(b ) .",
    "again , these minimal converging lengths are for totally random sets of examples as learning sets .      in this section",
    "we study how to introduce a selection bias into the learning set such that a new system is predicted as accurately as possible with the smallest learning set possible . because our database is very homogenous ( dense coverage of @xmath79 , @xmath88 , and @xmath102 ) we can ask how good the learning would be if we find the members of the dataset that are closest to the @xmath180 , @xmath181 , and @xmath182 and form the learning set as combinations thereof .",
    "since the descriptor has three components and , at most , the new parameters can be between two values of the database , this would give us a maximally localized learning set of minimal size 8 or less .",
    "let us once again choose two examples for which we will do the predictions .",
    "the first example is @xmath183 , @xmath184 , and @xmath185 .",
    "the second example is @xmath186 , @xmath159 , and @xmath166 .",
    "+   + for the first case , none of the results in the 5000 database share any of these parameters so that none of the components of the difference between the descriptor of the example and any descriptors of the database is ever zero .",
    "this means that the learning set will be of length 8 . in the second",
    "example , since we look at half - filling and the database contains half - filled results , the learning set size is 4 .",
    "this means that we are really looking at how our ml scheme can predict new results .",
    "we show the results in fig .  [",
    "fig : predic_wn_w_gl_min ] using the legendre polynomial representation and we see that with a very small learning set we can predict quite precisely new results .",
    "+   +    this possibility of prediction with very small learning enables us to look closely at how the machine itself behaves .",
    "indeed , what we call the machine is given by the @xmath1 parameters of eq .  .",
    "only in the cases of the representations in terms of matsubara frequencies and imaginary time does the @xmath1 really represent the green s function we are trying to predict itself .",
    "however , within the legendre polynomial representation , using our approximation that the kernel functions have fixed parameters , we can define an effective @xmath1-like parameter for the reconstructed green s function in matsubara frequency from the predicted legendre polynomials coefficients .",
    "we show the derivation in appendix  [ appen : eff_alpha ] , and we obtain @xmath187 where the effective @xmath1 , called @xmath188 , is given by @xmath189 the results are shown in fig .",
    "[ fig : alpha_gl ] .",
    "the case of @xmath183 , @xmath184 , and @xmath185 is presented in figs .",
    "[ fig : alpha_gl](a ) and ( b ) while the case @xmath186 , @xmath159 , and @xmath166 is presented in fig .",
    "[ fig : alpha_gl](c ) . for the half - filled example",
    ", only the imaginary part of @xmath188 is shown , as the real part of @xmath71 is zero for the particle - hole symmetric case . despite the discrete nature of functions of matsubara frequency",
    ", we present the curves as continuous here .",
    "it is also interesting to note ( not shown ) that the curves for @xmath1 obtained from the matsubara frequency representation correspond exactly to the curves of @xmath188 in fig .",
    "[ fig : alpha_gl ] as expected .",
    "one very important result is that @xmath188 is , from quite a low frequency , a smooth function of @xmath70 .",
    "this opens up the possibility of using interpolation from @xmath188 curves as a another way to perform very efficient ml .    for completeness , the other representation that directly learns @xmath62 is the imaginary time one and thus @xmath190 is really a representation of @xmath128 in the ml space .",
    "we thus show the @xmath190 curve for both examples in fig .",
    "[ fig : alpha_tau ] .",
    "once again the curves are smooth and in this case fundamentally continuous .",
    "we have proposed a machine learning scheme to learn the electron green s function .",
    "our method should apply to any other correlation function of interest .",
    "we have reduced the problem of learning a function of a single variable into one of learning a relatively small set of independents numbers , either direct slices of the function or coefficients used to define it . for the green s function of the single - site anderson impurity model ,",
    "we have tested four different representations of @xmath62 : the continued fraction , the matsubara frequency , imaginary time and legendre polynomial expansion . directly learning",
    "the function in imaginary time is a well - defined operation as long as the learning set is not too small and random .",
    "however , replacing the direct function in imaginary time @xmath16 by its legendre polynomial expansion is clearly superior because the need to learn only a small number of coefficients improves the accuracy .",
    "this way is even more promising for the context of learning correlation functions for real materials as new powerful ctqmc algorithms directly measure these coefficients@xcite .",
    "we also observe that the matsubara frequency representation may be problematic for more general dmft calculations . in the aim studied here ,",
    "the system is always a metal and thus the dataset of solved problems in @xmath70 is very homogeneous . in the general case of the dmft , where the aim serves as an intermediate problem",
    ", there is an interaction driven metal to insulator transition . in matsubara frequency",
    ", the green s function at lower frequencies changes qualitatively from the metallic to the insulating phase .",
    "this creates a dataset much less homogeneous at low @xmath70 .",
    "this is why , even if this representation yields accurate predictions in the present study , we do not think that it can be efficiently used in general for ml+dmft .",
    "contrary to @xmath71 , @xmath128 changes much less drastically from a metal to an insulator .",
    "therefore , this representation should perform much better for ml . finally ,",
    "as we already discussed , the continued fraction representation can only be really used when the aim is solved via ed and thus is a very limited representation .",
    "+   + before handling real materials , learning dmft for model hamiltonians is the next logical step .",
    "concomitant to this , the next logical step for ml of a function itself is to look for a scheme where learned numbers are not considered to be totally independent .",
    "this is highly nontrivial and perhaps could be achieved by adding appropriating new constraints to the minimization problem inherent in kernel ridge regression of eq .  .",
    "for example , constraints concerning moments could perhaps be integrated in the ml scheme for learning functions that have a spectral representation .",
    "this research was supported by the office of science of the u.s .",
    "department of energy under subcontract no .",
    "l - f.a . thanks ara go for numerous discussions on implementing an exact diagonalization code for the anderson impurity model .",
    "we thank peter b. littlewood for discussions and critical reading of the manuscript .",
    "this research used resources of the argonne leadership computing facility at argonne national laboratory , which is supported by the office of science of the u.s .",
    "doe under contract no.de-ac02-06ch11357 .",
    "o.a.v.l acknowledges funding from the swiss national science foundation grant no .",
    "ppoop2 _ 138932 .",
    "in kernel ridge regression , the coefficients of the expansion in the kernel space of eq .   are found by minimization with respect to @xmath1 of the cost function ( eq .  ) @xmath191 the @xmath33 are determined as the solution of the set of equations @xmath192 . by using the definition of @xmath193 given by eq .   and also the fact that the kernel is a symmetric function , we obtain @xmath194 \\nonumber \\\\ & & \\times k_m(\\mathbf{d}_l,\\mathbf{d}_q)= 0 \\ ] ] a sufficient condition for eq .   to hold",
    "is that the quantity in the square brackets vanishes , i.e. @xmath195 in eq .",
    ", @xmath37 is a dummy index . for fixed @xmath37 , we can represent @xmath1 and @xmath3 as vectors @xmath43 and @xmath44 of length given by the number of examples @xmath6 and the kernel as a @xmath49 matrix @xmath47 so that @xmath196 with @xmath48 the identity matrix .",
    "in this appendix we describe what is meant by exact diagonalization ( ed ) in the case of the anderson impurity hamiltonian . here , eq .",
    "is the target hamiltonian @xmath197 . to solve the problem by ed",
    ", we map @xmath197 to a new hamiltonian where the effect of the continuous bath is approximated by a few poles and weights .",
    "this gives a hamiltonian with a finite number of site @xmath198 given by @xmath199 where the @xmath200 and @xmath201 are chosen to reproduce as much as possible the effect of eq .  .",
    "we have now a finite size hamiltonian with a hilbert space of size @xmath202 that can be solved using matrices diagonalization techniques .",
    "we use the usual lanczos approach@xcite .",
    "of course , by using a finite bath , it is impossible to reproduce the continuous case to perfect match . only in the case",
    "where the number of bath sites @xmath96 was equal to infinity could one recover the continuous case exactly .",
    "therefore , the idea proposed by caffarel and krauth@xcite is to define a distance function @xmath203 from the matsubara axis representation .",
    "note that the matsubara axis representation of a @xmath204 problem requires the use of a fictitious temperature , which we choose as @xmath100 . here",
    ", we use the inverse of the noninteracting green s function to define the distance function @xmath205 @xmath206 is the maximum number of frequencies used to define @xmath203 .",
    "it is important that @xmath207 .",
    "we typically use @xmath208 . since the bath is fixed with a half bandwidth of 1 , @xmath209 . at @xmath210",
    "such an energy corresponds to approximately @xmath211 and thus @xmath212 is large enough .",
    "finally , @xmath213 is the inverse of the noninteracting green s function of the hamiltonian of eq .   and",
    "is written as @xmath214 we specify the set of parameters @xmath215 as those that minimize @xmath203 .",
    "this is a problem of unconstrained optimization in several variables .      for our @xmath197 ( eq .  ) with hybridization fixed , the ground state is nondegenerate and always in the sector where @xmath216 and @xmath217 .",
    "once the energy @xmath218 and many - body wave function @xmath219 are obtained , the green s function can be calculated from the electron and hole part .",
    "@xmath220 it is most convenient to represent @xmath62 as a continued fraction @xmath221 +   + a second lanczos procedure can be used to obtained the coefficients . the algorithm to calculate these coefficients from the ground state is as follows@xcite : +   +    * we construct a starting vector @xmath222 * we construct the so - called lanczos space with the recursion relation of orthogonal vectors @xmath223 * it is easy to obtain the coefficients as @xmath224 and @xmath225 with @xmath226 .",
    "the equivalent can be done for the @xmath227 and @xmath228 .",
    "the @xmath229 are orthogonal by construction if evaluated exactly . with double precision floating point arithmetic and a recursion relation ( eq .  )",
    "that only forces three vectors to be orthogonal , at some point ( typically after 50 ) the orthogonality will be lost . for the green s function",
    ", it does not matter much . if necessary",
    ", there are slightly more complicated algorithms that do partial orthogonalization to keep a good orthogonality of our set of vectors@xcite .",
    "note that for proper numerical calculation eqs .",
    ", and must be modified so that the states in eq .",
    "are normalized .",
    "the legendre polynomials @xmath230 are defined on an interval @xmath9 @xmath137 @xmath138 $ ] and thus an arbritrary function @xmath231 in @xmath232 can be written as @xmath233 the coefficients @xmath234 are found by multiplying both sides of eq .   by @xmath136 and then integrating over @xmath9 . the orthogonality relation for the legendre polynomials",
    "is used , @xmath235 .",
    "@xmath236 hence , @xmath237 we define @xmath238 to be consistent with ref .",
    "[ ] . finally , we can also change the integration variable and therefore get the result of eq .   and the expansion of eq .  .",
    "in the legendre polynomial expansion , the green s function on the matsubara axis is given by eq .",
    "@xmath239 in this approach , the ml procedure is for the coefficients @xmath146 and not @xmath71 itself .",
    "therefore , using eq   we may write @xmath240 therefore , putting eq .   in eq .",
    "we obtain @xmath241 in the most general case , this is as far as we can go .",
    "however , in our approach , we have chosen to consider that the parameters of @xmath50 are such that @xmath50 is independent of @xmath38 , i.e. , @xmath242 .",
    "this enables us to factor out @xmath50 in eq . and thus obtain @xmath243 we therefore obtain in this approximation a renormalized @xmath1 parameter in the ml expansion .",
    "this can be compared with the @xmath1 obtained from the ml directly on @xmath71 .",
    "this is not possible for the other two representations since the reconstruction of @xmath71 is done using highly nonlinear relations ( fourier transform and the continued fraction ) ."
  ],
  "abstract_text": [
    "<S> machine learning methods are applied to finding the green s function of the anderson impurity model , a basic model system of quantum many - body condensed - matter physics . </S>",
    "<S> different methods of parametrizing the green s function are investigated ; a representation in terms of legendre polynomials is found to be superior due to its limited number of coefficients and its applicability to state of the art methods of solution . </S>",
    "<S> the dependence of the errors on the size of the training set is determined . </S>",
    "<S> the results indicate that a machine learning approach to dynamical mean - field theory may be feasible . </S>"
  ]
}