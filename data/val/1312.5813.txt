{
  "article_text": [
    "deep neural networks ( dnns ) have found many successful applications in recent years . however , it is well - known that if one trains such networks with the standard back - propagation algorithm from randomly initialized parameters , one typically ends up with models that have poor prediction performance .",
    "a major progress in dnns research is the invention of pretraining techniques for deep learning @xcite .",
    "the main strategy is to employ layer - wise unsupervised learning procedures to initialize the dnn model parameters .",
    "a number of such unsupervised training techniques have been proposed , such as restricted boltzmann machines ( rbms ) in @xcite , and denoising autoencoders ( daes ) in @xcite .",
    "although these methods show strong empirical performance , the reason of their success has not been fully understood .",
    "two reasons were offered in the literature to explain the advantages of unsupervised learning procedure @xcite : the regularization effect and the optimization effect .",
    "the regularization effect says that pretraining provides regularization which initialize the parameters in the basin of attraction to a `` good '' local minimum .",
    "the optimization effect says that the pretraining leads to better optimization so that the initial value is close to a local minimum with a lower objective value than that can be achieved with random initialization .",
    "based on experimental evidences , some researchers confirm that the pretraining can learn invariant representations and selective units @xcite .",
    "* our contributions : * we study why the pretraining encourages moderate - sparseness .",
    "the main reason is that the pretraining models can be interpreted as an adaptive sparse coding .",
    "this coding is approximated by a sparse encoder , which is implemented by adaptively filtering out a lot of features that are not present in the input and suppressing the responses of some features that are not significant in the input .",
    "we further conduct experiments to demonstrate that it is a sparse regularization ( the hidden units become more sparsely activated ) .",
    "in this part we review some advantages of the pretraining methods .    distributed representations and deep architectures play an important role in deep learning methods .",
    "a distributed representation ( an old idea ) can capture a very great number of possible input configurations @xcite .",
    "deep architectures can promote the re - use of features and lead to abstract more invariant features for most local changes of the inputs @xcite .",
    "however , it is hard to use the _ back - propagation _ to train dnns with two traditional activation functions ( the sigmoid function @xmath0 and the hyperbolic tangent @xmath1 ) .",
    "luckily , @xcite proposes a unsupervised pretraining method to initialize the dnns model parameters and learn good representations . the regularization effect and the optimization effect",
    "are used to explain the main advantages of the pretraining method @xcite .    to better understand what the pretraining models learn in deep architectures ,",
    "@xcite find that the pretraining methods can learn invariant representations and selective units .",
    "some researchers use the linear combination of previous units @xcite and the maximizing activation @xcite to visualize the feature detectors ( or invariance manifolds or filters ) in an arbitrary layer .",
    "fig . 1 of @xcite and fig .",
    "3 of @xcite show that the first , second and third layer can learn edge detectors , object parts , and objects respectively . based the distributed and invariant representations , @xcite",
    "further confirm that the pretraining methods tend to do a better job at disentangling the underlying factors of variation , such as objects or object parts .",
    "compared to dnns with sigmoid function , we confirm that the pretraining methods encourage moderate - sparseness as the detectors filter out a lot of features that are not present in the input . in general , there is an illusion that unsupervised pretraining methods tend to learn non - sparse representations because it does not meet the conventional sparse methods @xcite .",
    "the conventional methods consider the idea of introducing a form of sparsity regularization .",
    "most ways have been proposed by directly penalizing the outputs of hidden units , such as @xmath2 penalty , @xmath3 penalty and student - t penalty .",
    "but , the pretraining methods implement sparseness by filtering out a lot of irrelevant features .",
    "there is a classic pretraining models : rbms .",
    "an rbms is an energy - based generative model defined over a visible layer and a hidden layer .",
    "the visible layer is fully connected to the hidden layer via symmetric weights @xmath4 , while there have no connections between units of the same layer .",
    "the number of visible units @xmath5 and hidden units @xmath6 are denoted by @xmath7 and @xmath8 , respectively",
    ". additionally , visible units and hidden units receive input from bias - @xmath9 and @xmath10 respectively .",
    "the energy function is denoted by @xmath11 : @xmath12    the probability that the network assigns to visible units @xmath5 is @xmath13 where @xmath14 is the partition function or normalizing constant .",
    "because there are no direct connections between hidden ( or visible ) units , it is very easy to sample from the conditional functions taking the form : @xmath15 where @xmath16 , @xmath17 and @xmath18 is a logistic sigmoid functions : @xmath19 .",
    "the training is to use @xmath20 algorithm @xcite to minimize the likelihood of the data : @xmath21 .",
    "in this section , we denote that the sparse regularization with more overlapping groups in low layer or less in high layer is called the moderate - sparseness .",
    "we mainly consider the multi - class problem to explain why the unsupervised pretraining encourages moderate - sparseness since dnns with the pretraining has been used to achieve state - of - the - art results on classification tasks .",
    "there are two reasons .",
    "first , we show a new viewing that the pretraining model is an adaptive sparse coding .",
    "second , because the pretraining can train the `` good '' feature detectors , we discuss that how the feature detectors can lead to moderate - sparseness . finally , we measure the moderate - sparseness .    to start off the discussion , there are two natural assumptions to @xmath22-class training set .",
    "_ assumption 1 : _ every class has a balanced number of samples and there are a lot of common raw features ( pixels or mfcc features ) among samples of the same class @xcite . _",
    "assumption 2 : _ there are some similar raw features among samples of different classes since they share some common ones @xcite .",
    "pretraining model ( such as rbms ) is an adaptive sparse coding .",
    "the explanation is as follow . by the results of @xcite , the pretraining model ( rbms training is to minimize @xmath21 )",
    "is also approximated by minimizing a reconstruction error criterion : @xmath23 where @xmath24 $ ] is the mean - field output of the hidden units given the observed input @xmath5 and @xmath25 $ ] is the mean - field output of the visible units given the representation @xmath6 sampled from @xmath16 .",
    "the @xmath26 and @xmath27 can be regard as a decoder and an encoder , respectively .    from the second parts of and",
    ", every hidden unit @xmath28 can be further interpreted as a feature detector ( or invariance manifolds or filters ) because the hidden unit is active ( or non - active ) , that means , the detector should respond strongly ( or weakly ) when the corresponding feature is present ( or absent ) in the input @xcite .",
    "amazedly the pretraining can train edge feature detectors in low layer and objects ( or object parts ) in high layer @xcite . _",
    "given an input , the feature detectors naturally filter out a lot of features that are not present in the input and suppress the responses of some features that are not significant in the input .",
    "_ clearly , those detectors result in sparseness .",
    "* relationship with sparse coding * : sparse coding is to find the dictionary @xmath29 and the sparse representation @xmath6 to minimize the most popular form : @xmath30 where @xmath31 also is a hyper - parameter . obviously , the first part of rbms is similar to the first part of sparse coding as they are decoders .",
    "the sparse coding is directly to penalize the @xmath2 norm of the hidden representation @xmath6 .",
    "but in rbms the @xmath6 is approximated by the sparse encoders ( feature detectors ) , which filter out a lot of irrelevant features . in next subsection",
    "we shall discuss that how the feature detectors can lead to moderate - sparseness .      * low - layer * : based on the assumptions the pretraining models averagely distribute all edge feature detectors to the hidden units in low layer as every class has a same number of samples .",
    "assumption 1 shows that every class has a same number of edge feature detectors and there are a lot of common edge ones in the same class .",
    "clearly , the edge feature detectors find out the edge features belonged to self - class , suppress the responses of some nonsignificant edge features , and filter out a lot of edge features related to the other classes .",
    "suppose that there are @xmath32 hidden units ( edge feature detectors ) and a @xmath22-class dataset , every class ideally has @xmath33 ones .",
    "given input samples of a class , thus , the @xmath33 hidden units belonged to the class are activated or weakly responded and the remaining @xmath34 units are not activated ( corresponding sparseness that is measured by ) .",
    "moreover , there are the common activation units ( corresponding group ) .",
    "simultaneously , assumption 2 shows that there are some similar edge feature detectors among different classes .",
    "different classes share some edge feature detectors corresponded to hidden units , which are also activated . the activated units results in more overlapping activation units in low layer .",
    "the activation overlapping degree is measured by .",
    "combined with regularization effect @xcite , therefore , we obtain the first result ( a1 ) that _ the unsupervised pretraining is a sparse regularization with more - overlapping groups in low layer_.    * high - layer * : in high layer the pretraining goes on to train object or object part features detectors from the edge features .",
    "similarly to the analysis in low layer , the hidden units are more sparsely activated or weakly responded in high layer . moreover , the activation overlapping degree is lower than one in low layer because the pretraining can potentially lead to do a better job at disentangling the objects or object parts @xcite .",
    "thus , we obtain the second result ( a2 ) that _ the unsupervised pretraining is a sparse regularization with less ( or no)-overlapping groups in high layer_.    in dnns without the pretraining the most hidden units of every layer are always activated and correspond to terrible feature detectors , which are the important causes of difficult classification problems . for classification tasks , it is always desirable to extract features that are most effective for preserving class separability @xcite and collaboratively representing objects @xcite .",
    "the pretraining firmly grasps the those benefits .",
    "the more activation overlapping units can capture the collaborative features in low layer and the less or no activation overlapping units can capture the separability in high layer .      for better understanding the pretraining , we tried to find sparseness , more - overlapping and no - overlapping characteristics of dnns with or without the pretraining .",
    "so , hoyer s sparseness measure and activation overlapping degree are defined as followings .",
    "the hoyer s sparseness measure ( hspm ) @xcite is based on the relationship between the @xmath2 norm and the @xmath35 norm .",
    "the hspm of a @xmath36 dimensional vector @xmath6 is defined as follows : @xmath37 this measure has good properties , which is in the interval @xmath38 $ ] and on a normalized scale .",
    "it s value more close to @xmath39 means that there are more zero components in the vector @xmath6 .",
    "we denote @xmath40 absolute value of a real number and give the following definitions about aod .",
    "* definition 1 : * a hidden unit @xmath41 is said to be active if the absolute value of its activation @xmath42 is above a threshold @xmath43 , that is @xmath44 . and a hidden unit @xmath41 is called un - active if @xmath45 .",
    "* definition 2 : * a vector @xmath46 is said to be an activation binary - vector of a @xmath47 dimensional representation @xmath6 if some representation units are active when the corresponding features are present in @xmath5 , and otherwise are not active when they are absent .",
    "formally , the activation binary - vector @xmath48 is defined as : @xmath49 where @xmath43 is a threshold .",
    "to indicate the present feature in the input , we select a threshold @xmath43 that does not change the reconstruction data , that is @xmath50 because it is small enough . ] , where @xmath51 and the vector @xmath52 of a sample @xmath5 is defined as : @xmath53    * definition 3 : * an activation binary - vector @xmath54 of a sample set @xmath55 is an activation binary - vector of the mean value @xmath56 among all samples @xmath57 .",
    "it is defined as : @xmath58 where @xmath59 is defined in and @xmath22 is the number of sample in the set @xmath55 . the activation overlapping degree ( aod )",
    "simply calculates the percentage of activation unites that are simultaneously selected by different classes @xmath60 .",
    "aod among a set @xmath61 is defined as : @xmath62 where @xmath63 , @xmath46 is a binary - vector that is a logical conjunction on all activation binary - vectors @xmath64 and @xmath65 is defined in .",
    "aod , which is in the interval @xmath38 $ ] , is used to measure the percentage of activation overlapping units in different classes .",
    "it s value more close to @xmath66 means that there are few activation overlapping units and it is easier to separate the different classes .",
    "in this section , we use deep neural networks to do experiments . a standard architecture for dnns consists of multiple layers of units in a directed graph , with each layer fully connected to the next one .",
    "the nodes of the inter - layers are called hidden units .",
    "each hidden unit is passed through a standard sigmoid functions .",
    "the objective of learning is to find the optimal network parameters so that the network output matches the target closely .",
    "the output can be compared to a target vector through a squared loss function or an negative log - likelihood loss function .",
    "we employ the standard _ back - propagation _ algorithm to train the model parameters ( the connected weights ) @xcite .",
    "we denote that dsigm : dnns with standard sigmoid functions , dprbms : dnns only pretrained with rbms and dbns : deep belief networks pretrained with rbms and finely tuned .",
    "* datasets * : we present experimental results on standard benchmark datasets : mnist and birdsong the pixel intensities of all datasets are normalized to @xmath67 $ ] . *",
    "mnist * dataset has 60,000 training samples and 10,000 test samples with @xmath68 pixel greyscale images of handwritten digits 0 - 9 . * * birdsong * * 35 bird recordings and test set has 150sec @xmath69 3 mics @xmath69 90 recordings .",
    "there are not labels in test set .",
    "so we divide the train set to a new train set and a new test set(we randomly select 3,000 train samples with 16 mfcc features , the rest are test samples in every recording . ) . ]",
    "dataset has 70,000 training samples and 200,690 test samples with 16 mfcc features .    to speed - up training",
    ", we subdivide training sets into mini - batches , each containing 100 cases , and the model parameter is updated after each minibatch using the averages .",
    "weights are initialized with small random values sampled from a normal distribution with zero mean and standard deviation of @xmath70 .",
    "biases are initialized with zeros . for simplicity",
    ", we use a constant learning rate chosen from @xmath71 .",
    "momentum is also used to speed up learning .",
    "the momentum starts at a value of @xmath72 and linearly increases to @xmath73 over half epochs , and stays at @xmath74 thereafter .",
    "the @xmath35 regularization parameter for the weights is fixed at @xmath75 .",
    "-0 in -0.3 in      -0.1 in        -0.15 in    -0.2 in    -0.1 in        -0.15 in    -0.4 in    before presenting the comparison of activation overlapping units , we first show the sparseness of pretraining compared to the more traditional sigmoid activation function .",
    "the sparseness metric hspm is the averaged value over the definition of .",
    "we perform comparisons on mnist , and results after fine - tuning training for 200 epochs are reported in table  [ tab : hspm552 ] .",
    "the results show that compared to dsigm the pretraining leads to models with higher sparseness , and smaller test errors .",
    "table  [ tab : hspm552 ] compares the network hspm of dbns and dprbms to that of dsigm . from table  [",
    "tab : hspm552 ] , we observe that the average sparseness of three layer dprbms is about @xmath76 ; the resulting dbns has similar sparseness . in fig",
    "[ fig : spon ] , ( * a * ) also shows that the feature of every layer rbms is more sparse as the train epoch increases . in contract ,",
    "the hspm of dsigm is on average below @xmath77 .",
    "when the pretraining are trained longer enough and the number of hidden unites increases , hspm of the pretraining models will become more sparse and also has an upper bound . in fig",
    "[ fig : spon ] , ( * b * ) shows that when the number of hidden units changes from 500 to 5000 , an upper bounds of rbms is 0.68 after 1000 training epochs .",
    "as the number of layers increases , hspm of the pretraining models also has an upper bound . from fig  [ fig : spon ] , (",
    "* c * ) shows that upper bounds of five hidden ( 500 and 1000 ) layers rbms are @xmath78 and @xmath79 , respectively .",
    "we observe that the hspm of the third layer pretraining is lower than one of the second layer .",
    "we empirically obtain the high hspm by increasing the number of the hidden units of high layer , for example , dbns ( 500 - 500 - 2000 ) .",
    "this observation maybe can explain why the top layer should be big .",
    "-0 in -0.3 in    we perform comparisons on birdsong , results after fine - tuning training for 100 epochs are reported in table  [ tab : hspm511bird ] .",
    "the results also show that the pretraining leads to models with higher sparseness , and smaller test errors . from table  [",
    "tab : hspm511bird ] , we observe that the sparseness of three layer dprbms is higher than that of dsigm and database . although after tuning the sparseness is close to dsigm , the pretraining learn `` good '' initial values to initialize the dnn model parameters .",
    "this illustrates that the pretraining also is an optimization effect .",
    "the hspm in 3th layer are lower than 2nd layer .",
    "when training 2 layers networks , the resulting has similar test errors .",
    "so , there is a inspiration that the hspm can be used to guide the number of layers and the number of hidden units .",
    "we perform comparisons on the test set of mnist . for convenience ,",
    "the test set @xmath80 is denoted by @xmath81 , where @xmath82 represents a set of all digits @xmath41 .",
    "@xmath83-combinations of the set @xmath80 is denoted by a set @xmath84 , where @xmath85 is the number of @xmath83-combinations and @xmath86 is a subset of @xmath83 distinct elements of @xmath80 .",
    "the average aod among @xmath83 classes is an average of all aod among a subset @xmath87 .",
    "we compare the average aod of dprbms to that of dsigm .",
    "we found that the pretraining can capture the characteristics _",
    "( a1 ) _ that there are many overlapping units in low layer and _ ( a2 ) _ that there are few ( or no ) overlapping units in high layer . from fig",
    "[ fig : aodmnist ] , the results show that average aod among @xmath83 classes ( @xmath83 changes from @xmath88 to @xmath89 ) is high in low layer and is low ( or zero ) in high layer .",
    "the average aod gets closer to @xmath66 as the number of layer increases in dprbms .",
    "particularly , the average aod gets closer to @xmath66 than data - self in the third layer .",
    "this reveals that it is easier to classify .",
    "but it is very approximate to @xmath39 in every layer of dsigm .",
    "since the pretraining is known to perform well on mnist , this paper mainly discusses why the unsupervised pretraining encourages moderate - sparseness .",
    "our observations make us suspect that sparseness and activation overlapping degree play more important roles in deep neural networks . from table  [",
    "tab : hspm552 ] , table  [ tab : hspm511bird ] and fig [ fig : aodmnist ] , the pretraining can capture the sparse hidden units , the more activation overlapping units in low layer and the less ( or no ) activation overlapping units in high layers ."
  ],
  "abstract_text": [
    "<S> it is well known that direct training of deep neural networks will generally lead to poor results . </S>",
    "<S> a major progress in recent years is the invention of various pretraining methods to initialize network parameters and it was shown that such methods lead to good prediction performance . </S>",
    "<S> however , the reason for the success of pretraining has not been fully understood , although it was argued that regularization and better optimization play certain roles . </S>",
    "<S> this paper provides another explanation for the effectiveness of pretraining , where we show pretraining leads to a sparseness of hidden unit activation in the resulting neural networks . </S>",
    "<S> the main reason is that the pretraining models can be interpreted as an adaptive sparse coding . </S>",
    "<S> compared to deep neural network with sigmoid function , our experimental results on mnist and birdsong further support this sparseness observation . </S>"
  ]
}