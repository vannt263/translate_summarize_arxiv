{
  "article_text": [
    "the use of high - dimensional data has become very popular these days , especially in search , natural language processing ( nlp ) , and computer vision .",
    "for example , winner of 2009 pascal image classification challenge used 4 million ( non - binary ) features  @xcite .",
    "@xcite discussed datasets with billions or even trillions of features .    for text data ,",
    "the use of extremely high - dimensional representations ( e.g. , @xmath0-grams ) is the standard practice , for example , @xcite .",
    "binary representations could be sufficient if the order of @xmath0-grams is high enough .",
    "on the other hand , in current practice of computer vision , it is still more common to use non - binary feature representations , for example , _ local coordinate coding ( lcc ) _",
    "it is often the case that in practice high - dimensional non - binary visual features might be appropriately sparsified without hurting the performance .",
    "however simply binarizing the features will often incur loss of accuracies , sometimes significantly so .",
    "see table  [ tab_linearsvm ] for an illustration .",
    "our contribution in this paper is the proposal of two types of ( nonlinear ) `` core '' kernels , where `` core '' stands for `` correlation - resemblance '' , for non - binary sparse data .",
    "interestingly , using core kernels leads to improvement in classification accuracies ( in some cases significantly so ) on a variety of datasets .    for",
    "practical large - scale applications , naive implementations of nonlinear kernels may be too costly ( time and/or memory ) , while linear learning methods ( e.g. , linear svm or logistic regression ) are extremely popular in industry .",
    "the proposed core kernels would be facing the same challenge . to address this critical issue",
    ", we also develop efficient hashing algorithms which approximate the core kernels by linear kernels .",
    "these new hashing algorithms allow us to take advantage of highly efficient ( batch or stochastic ) linear algorithms , e.g. ,  @xcite .",
    "in the rest of this section , we first review the definitions of correlation and resemblance , then we provide an experimental study to illustrate the loss of classification accuracies when sparse data are binarized .",
    "we assume a data matrix of size @xmath1 , i.e. , @xmath0 observations in @xmath2 dimensions .",
    "consider , without loss of generality , two data vectors @xmath3 .",
    "the correlation is simply the normalized inner product : @xmath4 it is well - known that @xmath5 constitutes a positive definite and linear kernel , which is one of the reasons why correlation is very popular in practice .      for binary data ,",
    "the resemblance is commonly used : @xmath6 it was shown in  @xcite that the resemblance defines a type of positive definite kernel . in this study",
    ", we will combine correlation and resemblance to define two new types of nonlinear kernels .",
    "table  [ tab_linearsvm ] lists the datasets , which are non - binary and sparse .",
    "the table also presents the test classification accuracies using linear svm on both the original ( non - binary ) data and binarized data .",
    "the results in the table illustrate the noticeable drop of accuracies by using only binary data .",
    ".classification accuracies ( in % ) using linear svm ( liblinear  @xcite ) on sparse non - binary datasets .",
    "as we always normalize the data ( to unit norm ) , the correlation kernel @xmath7 is naturally used here .",
    "we experiment with the @xmath8-regularized linear svm and report the best test accuracies from a wide range of `` @xmath9 '' values ( where @xmath9 is the parameter in linear svm ) . using binarized data ( i.e. , the last column ) ,",
    "the test accuracies drop quite noticeably in most cases .",
    "+ available at the uci repository , * youtube * is a multi - view dataset , and we choose the largest set of features ( audio ) for our experiment .",
    "* m - basic * , * m - rotate * , and * mnist10k * were used in  @xcite for testing _",
    "abc - logitboost _ and _ abc - mart _  @xcite ( and comparisons with deep learning  @xcite ) . for * rcv1",
    "* , we use a subset of the original testing examples ( to facilitate efficient kernel computation later needed in the paper ) . [ cols=\"<,>,>,>,>,>\",options=\"header \" , ]     [ tab_linearsvm ]    figure  [ fig_linearsvm ] provides more detailed results for a wide range of @xmath9 values , where @xmath9 is the usual @xmath8-regularization parameter in svm .    while linear svm is extremely popular in industrial practice , it is often not as accurate .",
    "our proposed core kernels will be able to produce noticeably more accurate results than linear svm .",
    "we propose two types of core kernels , which combine resemblance with correlation , for sparse non - binary data .",
    "both kernels are positive definite",
    ". we will demonstrate the effectiveness of the two core kernels using the same datasets in table  [ tab_linearsvm ] and figure  [ fig_linearsvm ] .",
    "the first type of core kernel is basically the product of correlation @xmath7 and the resemblance @xmath10 .",
    "@xmath11 later we will express @xmath12 as an ( expectation of ) inner product , i.e. , @xmath12 is obviously positive definite .    if the data are fully dense , then @xmath13 and @xmath14 . on the other hand , if the data are binary , then @xmath15 and @xmath16 .",
    "recall the definitions of @xmath17 in ( [ eqn_r ] ) .",
    "the second type of core kernel perhaps appears less intuitive than the first type : @xmath18 if the data are binary , then @xmath19 . we will , later in the paper , also write @xmath20 as an expectation of inner product to confirm it is also positive definite",
    ".      figure  [ fig_kernelsvm ] presents the classification accuracies on the same six datasets as in figure  [ fig_linearsvm ] , using nonlinear kernel svm with three different kernels : core type 1 , core type 2 , and resemblance .",
    "we can see that resemblance ( which only uses binary information of the data ) does not perform as well as core kernels .",
    "[ tab_kernelsvm ]    the best results are summarized in table  [ tab_kernelsvm ] .",
    "it is interesting to compare them with linear svm results in table  [ fig_linearsvm ] and figure  [ fig_linearsvm ] .",
    "we can see that core kernels perform very well , without using additional tuning parameters .",
    "in fact , if we compare the best results in  @xcite ( e.g. , rbf svm , abc - boosting , or deep learning ) on mnist10k , m - rotate , and m - basic , we will see that core kernels ( with no tuning parameters ) can achieve the same ( or similar ) performance .    we should mention that our experiments can be fairly easily reproduced because all datasets are public and we use standard svm packages ( libsvm and liblinear ) without any modifications .",
    "we also provide the results for wide range of @xmath9 values in figure  [ fig_linearsvm ] and figure  [ fig_kernelsvm ] .",
    "because we use pre - computed kernel functionality of libsvm ( which consumes very substantial amount of memory to store the kernel matrices ) , we only experiment with training data of moderate sizes , to ensure repeatability .",
    "* section 1.4.3 ) mentioned three main computational issues of kernels summarized as follows :    1 .",
    "_ computing kernels is very expensive _ + computing kernels can account for more than half of the total computing time",
    "computing the full kernel matrix is wasteful _ + this is because not all pairwise kernel values will be used during training .",
    "the kernel matrix does not fit in memory _",
    "+ the cost of storing the full kernel matrix in the memory is @xmath21 , which is not realistic for most pcs even for merely @xmath22 , while the industry has used training data with billions of examples .",
    "thus , kernel evaluations are often conducted _ on the fly _ , which means the computational cost is dominated by kernel evaluations .    in fact , evaluating kernels on - demand would encounter another serious ( and often common ) issue if the datasets themselves are too big for the memory .",
    "all these crucial issues motivate us to develop hashing algorithms to approximate core kernels .",
    "our goal is to develop good hashing algorithms to ( approximately ) transform nonlinear kernels into linear kernels .",
    "once we have the new data representations ( i.e. , the hashed data ) , we can use highly efficient batch or stochastic linear methods for training svm ( or logistic regression )  @xcite .",
    "another benefit would be in the context of approximate near neighbor search because probabilistic hashing provides a ( often good ) strategy for space partitioning which will help reduce the search time ( i.e. , no need to scan all data points ) .",
    "our proposed hashing methods can be modified to become an instance of _ locality sensitive hashing ( lsh ) _",
    "@xcite in the space of core kernels .    in this study , we will present hashing algorithms for core kernels based on standard random projection and minwise hashing methods .",
    "we first provide a review of these two methods .",
    "consider two vectors @xmath23 .",
    "the idea of random projection is simple .",
    "we first generate a random vector of i.i.d .",
    "entries @xmath24 , @xmath25 to @xmath2 , and then compute the inner products as the hashed values : @xmath26 for the convenience of theoretical analysis , we adopt @xmath27 , which is a typical choice in the literature .",
    "several variants of random projections like  @xcite are essentially equivalent , as analyzed in  @xcite .",
    "we always assume the data are normalized , i.e. , @xmath28 .",
    "note that computing the @xmath8 norms of all the data points only requires scanning the data once which is anyway needed during data collection / processing . for normalized data , it is known that @xmath29 = \\rho$ ] . in order to estimate @xmath7",
    ", we need to use @xmath30 random projections to generate @xmath31 to @xmath30 , and estimate @xmath7 by @xmath32 , which is also an inner product .",
    "this means we can directly use the projected data to build a linear classifier .      the method of minwise hashing  @xcite is very popular for computing set similarities , especially for industrial applications , for example ,  @xcite .",
    "consider the space of the column numbers : @xmath33 .",
    "we assume a random permutation @xmath34 and apply @xmath35 on the coordinates of both vectors @xmath36 and @xmath37 .",
    "for example , consider @xmath38 , @xmath39 $ ] and @xmath40 .",
    "then the permuted vector becomes @xmath41 $ ] . in this example , the first nonzero column of @xmath42 is 1 , and the corresponding value of the coordinate is 0.45 . for convenience ,",
    "we introduce the following notation : @xmath43 in this example , we have @xmath44 and @xmath45 .    the following well - known _ collision probability _",
    "@xmath46 can be used to estimate the resemblance @xmath10 . to do so , we need to generate @xmath30 permutations @xmath47 , @xmath48 to @xmath30 .",
    "the proposed hashing algorithms for core kernels combine random projections and minwise hashing .",
    "the goal is to develop unbiased estimators of @xmath12 and @xmath20 which can be written as inner products .",
    "we assume that we have conducted random projections and minwise hashing @xmath30 times .",
    "in other words , for each data vector @xmath36 , we have the hashed values @xmath49 , @xmath50 , @xmath51 , @xmath48 to @xmath30 .",
    "recall the definitions of @xmath52 , @xmath53 , @xmath54 in ( [ eqn_p ] ) , ( [ eqn_l ] ) , and ( [ eqn_v ] ) , respectively .      our proposed estimator of @xmath12 is @xmath55    [ thm_kc1 ] @xmath56 @xmath57 * proof : *  see appendix  [ app_thm_kc1].@xmath58    a simple argument can show that @xmath59 could be written as an inner product ; and hence @xmath12 is positive definite . although this fact is obvious since @xmath12 is a product of two positive definite kernels , we would like to present a constructive proof because the construction is basically the same procedure for expanding the hashed data before feeding them to an svm solver .",
    "recall , @xmath53 is the location of the first nonzero after minwise hashing . basically , we can view @xmath50 equivalently as a vector of length @xmath2 whose coordinates are all zero except the @xmath50-th coordinate .",
    "the value of the only nonzero coordinate will be @xmath49 .",
    "for example , suppose @xmath38 , @xmath60 , @xmath61",
    ". then the equivalent vector would be @xmath62 $ ] .",
    "this way , we can write @xmath59 as an inner product of two @xmath2-dimensional sparse vectors .    note that the input data format of standard svm packages is the sparse format . for linear svm",
    ", the cost is essentially determined by the number of nonzeros ( in this case , @xmath30 ) , not much to do with the dimensionality ( unless it is too high ) .",
    "if @xmath2 is too high , then we can adopt the standard trick of _ @xmath63-bit minwise hashing _",
    "@xcite by only using the lowest @xmath63 bits of @xmath50 .",
    "our second proposal is @xmath64 recall that we always assume the data ( @xmath36 , @xmath37 ) are normalized .",
    "for example , if the data are binary , then we have @xmath65 , @xmath66 .",
    "hence the values @xmath51 and @xmath67 are small ( and we need the term @xmath68 ) .    [ thm_kc2 ] @xmath69 @xmath70 * proof : *  see appendix  [ app_thm_kc2].@xmath58    once we understand how to express @xmath59 as an inner product , it should be easy to see that @xmath71 can also be written as an inner product . again , suppose @xmath38 , @xmath60 , and @xmath72 .",
    "we can consider an equivalent vector @xmath73 $ ] .",
    "in other words , the difference between @xmath59 and @xmath71 is what value we should put in the nonzero location .",
    "one advantage of @xmath71 is that it only requires the permutations and thus eliminates the cost of random projections .",
    "as expected , the variance of @xmath71 would be large if the data are heavy - tailed .",
    "however , when the data are binary or appropriately normalized ( e.g. , tf - idf ) , @xmath74 is actually quite small .",
    "for example , when the data are binary , i.e. , @xmath65 , @xmath75 , we have @xmath76 , which is ( considerably ) smaller than @xmath77 .      to validate the theoretical results in theorem  [ thm_kc1 ] and theorem  [ thm_kc2 ]",
    ", we provide a set of experiments in figure  [ fig_mse ] .",
    "two pairs of word vectors are selected : `` a ",
    "the '' and `` hong  kong '' , from a chuck of web crawls .",
    "for example , the vector `` hong '' is a vector whose @xmath78-th entry is the number of occurrences of the word `` hong '' in the @xmath78-th document . for each pair , we apply the two proposed hashing algorithms on the two corresponding vectors to estimate @xmath12 and @xmath20 . with sufficient repetitions",
    ", we can empirically compute the mean square errors ( mse = var + bias@xmath79 ) , which should match the theoretical variances if the estimators are indeed unbiased and the variance formulas , ( [ eqn_varkc1 ] ) and ( [ eqn_varkc2 ] ) , are correct .",
    "the number of word occurrences is a typical example of highly heavy - tailed data .",
    "usually when text data are used in machine learning tasks , they have to be appropriately weighted ( e.g. , tf - idf ) or simply binarized .",
    "figure  [ fig_mse ] presents the results on the original raw data as well as binarized data , to verify the formulas for these two extreme cases , for @xmath80 to 1000 .    indeed ,",
    "the plots show that the empirical mses essentially overlap the theoretical variances .",
    "in addition , the mses of @xmath71 is significantly larger than the mses of @xmath59 on the raw data , as expected .",
    "once the data are binarized , the mses of @xmath71 become smaller .",
    "in this section , we provide a set of experiment for using the hashed data as input for a linear svm solver ( liblinear ) . our goal is to approximate the performance of ( nonlinear ) core kernels with linear kernels . in section  [ sec_hashing ] ,",
    "we have explained how to express the estimators @xmath59 and @xmath71 as inner products by expanding the hashed data . with @xmath30 permutations and @xmath30 random projections ,",
    "the number of nonzeros of the expanded data is precisely @xmath30 . to reduce the dimensionality ,",
    "we use only the lowest @xmath63 bits of the locations . in this study , we experiment with @xmath81 , 2 , 4 , 8 .",
    "figure  [ fig_mrotateacc ] presents the results on the * m - rotate * dataset . as shown in figure  [ fig_linearsvm ] and table  [ tab_linearsvm ] , using the linear kernel can only achieve an accuracy of @xmath82 .",
    "this means , if we use random projections ( or variants , e.g. ,  @xcite ) , which approximate inner products , then the most we can achieve would be about @xmath82 .",
    "for this dataset , the performance of core kernels ( and resemblance kernel ) is astonishing , as shown in figure  [ fig_kernelsvm ] and table  [ tab_kernelsvm ] .",
    "thus , we choose this dataset to demonstrate that our proposed hashing algorithms combined with linear svm can also achieve the performance of ( nonlinear ) core kernels .",
    "+ to better explain the procedure , we use the same examples as in section  [ sec_hashing ] .",
    "suppose we apply @xmath30 minwise hashing and @xmath30 random projections on the data and we consider without loss of generality the data vector @xmath36 .",
    "for the @xmath83-th projection and @xmath83-th minwise hashing , suppose @xmath84 .",
    "recall @xmath53 and @xmath54 are , respectively , the location and the value of the first nonzero entry after minwise hashing .",
    "@xmath52 is projected value obtained from random projection .    in order to use linear svm to approximate kernel svm with type 1 core kernel , we expand the @xmath83-th hashed data as a vector @xmath85 $ ] if @xmath86 , or @xmath87 $ ] if @xmath81 .",
    "we then concatenate @xmath30 such vectors to form a vector of length @xmath88 ( with exactly @xmath30 nonzeros ) .",
    "before we feed the expanded hashed data to liblinear , we always normalize the vectors to have unit norm .",
    "the experimental results are presented in the right panels of figure  [ fig_mrotateacc ] .",
    "to approximate type 2 core kernel , we expand the @xmath83-th hashed data of @xmath36 as @xmath89 $ ] if @xmath86 , or @xmath90 $ ] if @xmath81 , where @xmath91 is the number of nonzero entries in the original data vector @xmath36 .",
    "again , we concatenate @xmath30 such vectors . the experimental results are presented in the middle panels of figure  [ fig_mrotateacc ] .    to approximate resemblance kernel , we expand the @xmath83-th hashed data of @xmath36 as @xmath92 $ ] if @xmath86 or @xmath93 $ ] if @xmath81 and we concatenate @xmath30 such vectors .",
    "the results in figure  [ fig_mrotateacc ] are exciting because linear svm on the original data can only achieve an accuracy of @xmath82 .",
    "our proposed hashing methods + linear svm can achieve @xmath94 . using only the original @xmath63-bit minwise hashing , the accuracy can still reach about @xmath95 . again , we should mention that other hashing algorithms which aim at approximating the inner product ( such as random projections and variants ) can at most achieve the same result as using linear svm on the original data .",
    "there is a line of work called _ conditional random sampling ( crs ) _",
    "@xcite which was also designed for sparse non - binary data . basically , the idea of crs is to keep the first @xmath30 nonzero entries after applying one permutation on the data .",
    "@xcite developed the trick to construct an ( essentially ) equivalent random sample for each pair . although crs is applicable to non - binary data , it is not suitable for training linear svm ( or other applications which require the input data to be in a metric space ) , because the hashed data of crs are not appropriately aligned , unlike our method .",
    "table  [ tab_kernelsvm ] shows that type 1 core kernel can often achieve better results than type 2 core kernel , in some cases quite substantially .",
    "this helps justify the need for developing hashing methods for type 1 core kernel , which require random projections in addition to random permutations .",
    "there are many promising extensions .",
    "for example , we can construct new kernels based on core kernels ( which currently do not have tuning parameters ) , by using the exponential function and introducing an additional tuning parameter @xmath96 , just like rbf kernel .",
    "this will allow more flexibility and potentially further improve the performance .",
    "another interesting line of extensions would be applying other hashing algorithms on our generated hashed data .",
    "this is possible again because we can view our estimators as inner products and hence we can apply other hashing algorithms which approximate inner products on top of our hashed data .",
    "the advantage is the potential further data compression",
    ". another advantage would be in the context of sublinear time approximate near neighbor search ( when the target similarity is the core kernels ) .",
    "for example , we can apply another layer of random projections on top of the hashed data and then store the signs of the new projected data  @xcite .",
    "these signs , which are bits , provide good indexing & space partitioning capability to allow sublinear time approximate near neighbor search under the framework of lsh  @xcite .",
    "this way , we can search for near neighbors in the space of core kernels ( instead of inner products ) .",
    "current popular hashing methods , such as random projections and variants , often focus on approximating inner products and large - scale linear classifiers ( e.g. , linear svm ) .",
    "however , linear kernels often do not achieve good performance . in this paper , we propose two types of core kernels which outperform linear kernels , sometimes by a large margin , on sparse non - binary data ( which are common in practice ) . because core kernels are nonlinear , we accordingly develop new hash methods to approximate core kernels .",
    "the hashed data can be fed into highly efficient linear classifiers .",
    "our experiments confirm the findings .",
    "we expect this work will inspire a new line of research on hashing algorithms and large - scale learning .",
    "to compute the expectation and variance of the estimator @xmath97 , we need the first two moments of @xmath98 .",
    "the first moment is @xmath99\\\\\\notag = & e\\left[p_j(u)p_j(v)\\right]\\mathbf{pr}\\left(l_j(u)=l_j(v)\\right ) = \\rho r\\end{aligned}\\ ] ] which implies that @xmath100 .",
    "the second moment is @xmath101\\\\\\notag = & e\\left[p_j^2(u)p_j^2(v)\\right ] \\mathbf{pr}\\left(l_j(u)=l_j(v)\\right)\\\\\\notag = & \\left(1 + 2\\rho^2\\right ) \\rho r\\end{aligned}\\ ] ] here , we have used the result in the prior work  @xcite : @xmath102 = 1 + 2\\rho^2 $ ] .",
    "therefore , the variance is @xmath103    this completes the proof .",
    "we need the first two moments of the estimator @xmath104    because @xmath105\\\\\\notag = & e\\left[v_j(u ) v_j(v)1\\{l_j(u ) = l_j(v)\\}|l_j(u)=l_j(v)\\right]\\\\\\notag & \\times\\mathbf{pr}\\left(l_j(u)=l_j(v)\\right)\\\\\\notag = & \\frac{\\sum_{i=1}^d u_iv_i}{a}r = \\rho \\frac{1}{f_1+f_2-a}\\end{aligned}\\ ] ] we know @xmath106 and @xmath107\\\\\\notag = & e\\left[v_j^2(u ) v_j^2(v)\\right]\\mathbf{pr}\\left(l_j(u)=l_j(v)\\right)\\\\\\notag = & \\frac{\\sum_{i=1}^d u_i^2v_i^2}{a}r = \\frac{\\sum_{i=1}^d u_i^2v_i^2}{f_1+f_2-a}\\end{aligned}\\ ] ] therefore , @xmath108                                      h.  larochelle , d.  erhan , a.  c. courville , j.  bergstra , and y.  bengio . an empirical evaluation of deep architectures on problems with many factors of variation . in _",
    "icml _ , pages 473480 , corvalis , oregon , 2007 ."
  ],
  "abstract_text": [
    "<S> the term `` core kernel '' stands for _ correlation - resemblance kernel_. in many applications ( e.g. , vision ) , the data are often high - dimensional , sparse , and non - binary . </S>",
    "<S> we propose two types of ( nonlinear ) core kernels for non - binary sparse data and demonstrate the effectiveness of the new kernels through a classification experiment . </S>",
    "<S> core kernels are simple with no tuning parameters . </S>",
    "<S> however , training nonlinear kernel svm can be ( very ) costly in time and memory and may not be suitable for truly large - scale industrial applications ( e.g. search ) . in order to make the proposed core kernels more practical , </S>",
    "<S> we develop basic probabilistic hashing algorithms which transform nonlinear kernels into linear kernels . </S>"
  ]
}