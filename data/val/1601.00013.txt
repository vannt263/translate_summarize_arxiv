{
  "article_text": [
    "neural networks are being successfully applied across an extraordinary range of problem domains , in fields as diverse as computer science , finance , medicine , engineering , physics , etc .",
    "the main reason for such popularity is their ability to approximate arbitrary functions . for the last 30 years",
    "a number of results have been published showing that the artificial neural network called a _ feedforward network with one hidden layer _ can approximate arbitrarily well any continuous function of several real variables .",
    "these results play an important role in determining boundaries of efficacy of the considered networks .",
    "but the proofs are usually do not state how many neurons should be used in the hidden layer .",
    "the purpose of this paper is to prove constructively that a neural network having only one neuron in its single hidden layer can approximate arbitrarily well all continuous functions defined on any compact subset of the real axis .",
    "the building blocks for neural networks are called _",
    "neurons_. an artificial neuron is a device with @xmath2 real inputs and an output .",
    "this output is generally a superposition of a univariate function with an affine function in the @xmath2-dimensional euclidean space , that is a function of the form @xmath3 .",
    "the neurons are organized in layers .",
    "each neuron of each layer is connected to each neuron of the subsequent ( and thus previous ) layer .",
    "information flows from one layer to the subsequent layer ( thus the term feedforward ) .",
    "a feedforward neural network with one hidden layer has three layers : input layer , hidden layer , and output layer .",
    "a feedforward network with one hidden layer consisting of @xmath4 neurons computes functions of the form @xmath5 here the vectors @xmath6 , called _ weights _ , are vectors in @xmath7 ; the _ thresholds _ @xmath8 and the coefficients @xmath9 are real numbers and @xmath1 is a univariate activation function .",
    "the following are common examples of activation functions : @xmath10    in many applications , it is convenient to take the activation function @xmath1 as a _ sigmoidal function _ which is defined as @xmath11 the literature on neural networks abounds with the use of such functions and their superpositions .",
    "note that all the above activation functions are sigmoidal .",
    "in approximation by neural networks , there are two main problems .",
    "the first is the _ density _ problem of determining the conditions under which an arbitrary target function can be approximated arbitrarily well by neural networks .",
    "the second problem , called the _ complexity _ problem , is to determine how many neurons in hidden layers are necessary to give a prescribed degree of approximation .",
    "this problem is almost the same as the problem of degree of approximation ( see @xcite ) .",
    "the possibility of approximating a continuous function on a compact subset of the real line ( or @xmath2-dimensional space ) by a single hidden layer neural network with a sigmoidal activation function has been well studied in a number of papers .",
    "different methods were used .",
    "carroll and dickinson @xcite used the inverse radon transformation to prove the universal approximation property of single hidden layer neural networks .",
    "gallant and white @xcite constructed a specific continuous , nondecreasing sigmoidal function , called a cosine squasher , from which it was possible to obtain any fourier series .",
    "thus their activation function had the density property .",
    "cybenko @xcite and funahashi , @xcite independently from each other , established that feedforward neural networks with a continuous sigmoidal activation function can approximate any continuous function within any degree of accuracy on compact subsets of @xmath12 cybenko s proof uses the functional analysis method , combining the hahn  banach theorem and the riesz representation theorem , whiles funahashi s proof applies the result of irie and miyake @xcite on the integral representation of @xmath13 functions , using a kernel which can be expressed as a difference of two sigmoidal functions .",
    "hornik , stinchcombe and white @xcite applied the stone  weierstrass theorem , using trigonometric functions .",
    "krkov @xcite proved that staircase like functions of any sigmoidal type can approximate continuous functions on any compact subset of the real line within an arbitrary accuracy .",
    "this is effectively used in krkov s subsequent results , which show that a continuous multivariate function can be approximated arbitrarily well by two hidden layer neural networks with a sigmoidal activation function ( see  @xcite ) .",
    "chen , chen and liu @xcite extended the result of cybenko by proving that any continuous function on a compact subset of @xmath7 can be approximated by a single hidden layer feedforward network with a bounded ( not necessarily continuous ) sigmoidal activation function .",
    "almost the same result was independently obtained by jones @xcite .",
    "costarelli and spigler @xcite reconsidered cybenko s approximation theorem and for a given function @xmath14 $ ] constructed certain sums of the form ( [ eq : intro ] ) , which approximate @xmath15 within any degree of accuracy . in their result ,",
    "similar to @xcite , @xmath1 is bounded and sigmoidal . therefore , when @xmath16 , the result can be viewed as a density result in @xmath17 $ ] for the set of all functions of the form ( [ eq : intro ] ) .",
    "chui and li @xcite proved that a single hidden layer network with a continuous sigmoidal activation function having integer weights and thresholds can approximate an arbitrary continuous function on a compact subset of @xmath0 .",
    "ito @xcite established a density result for continuous functions on a compact subset of @xmath0 by neural networks with a sigmoidal function having only unit weights .",
    "density properties of a single hidden layer network with a restricted set of weights were studied also in other papers ( for a detailed discussion see @xcite ) .",
    "in many subsequent papers , which dealt with the density problem , nonsigmoidal activation functions were allowed . among them",
    "are the papers by stinchcombe and white @xcite , cotter @xcite , hornik @xcite , mhaskar and micchelli @xcite , and other researchers .",
    "the more general result in this direction belongs to leshno , lin , pinkus and schocken @xcite .",
    "they proved that the necessary and sufficient condition for any continuous activation function to have the density property is that it not be a polynomial . for a detailed discussion of most of the results in this section ,",
    "see the review paper by pinkus @xcite .",
    "it should be remarked that in all the above mentioned works the number of neurons @xmath4 in the hidden layer is not fixed . as such to achieve a desired precision one",
    "may take an excessive number of neurons .",
    "this , in turn , gives rise to the problem of complexity ( see above ) .",
    "our approach to the problem of approximation by single hidden layer feedforward networks is different and quite simple .",
    "we consider networks ( [ eq : intro ] ) defined on @xmath0 with a limited number of neurons ( @xmath4 is fixed ! ) in a hidden layer and ask the following fair question : is it possible to construct a well behaved ( that is , sigmoidal , smooth , monotone , etc . ) universal activation function providing approximation to arbitrary continuous functions on any compact set in @xmath0 within any degree of precision ? we show that this is possible even in the case of a feedforward network with only one neuron in its hidden layer ( that is , in the case @xmath18 ) .",
    "the basic form of our theorem claims that there exists a smooth , sigmoidal , almost monotone activation function @xmath1 with the property : for each univariate continuous function @xmath15 on the unit interval and any positive @xmath19 one can chose three numbers @xmath20 , @xmath21 and @xmath22 such that the function @xmath23 gives @xmath19-approximation to @xmath15 .",
    "it should be remarked that we prove not only the existence result but also give an algorithm for constructing the mentioned universal sigmoidal function .",
    "for a wide class of lipschitz continuous functions we also give an algorithm for evaluating the numbers @xmath20 , @xmath21 and @xmath22 .    for numerical experiments we used sagemath @xcite .",
    "we wrote a code for creating the graph of @xmath1 and computing @xmath24 at any reasonable @xmath25 .",
    "the code is open - source and available at http://sites.google.com/site/njguliyev/papers/sigmoidal .",
    "we begin this section with the definition of a @xmath26-increasing ( @xmath26-decreasing ) function .",
    "let @xmath26 be any nonnegative number .",
    "a real function @xmath15 defined on @xmath27 is called @xmath26-increasing ( @xmath26-decreasing ) if there exists an increasing ( decreasing ) function @xmath28 such that @xmath29 , for all @xmath30 .",
    "if @xmath31 is strictly increasing ( or strictly decreasing ) , then the above function @xmath15 is called a @xmath26-strictly increasing ( or @xmath26-strictly decreasing ) function .",
    "clearly , @xmath32-monotonicity coincides with the usual concept of monotonicity and a @xmath33-increasing function is @xmath34-increasing if @xmath35 .",
    "the following theorem is valid .    for",
    "any positive numbers @xmath36 and @xmath26 , there exists a @xmath37 , sigmoidal activation function @xmath38 which is strictly increasing on @xmath39 , @xmath26-strictly increasing on @xmath40 , and satisfies the following property : for any finite closed interval @xmath41 $ ] of @xmath0 and any @xmath42 $ ] and @xmath43 there exist three real numbers @xmath20 , @xmath21 and @xmath22 for which @xmath44 for all @xmath45 $ ] .",
    "let @xmath36 be any positive number .",
    "divide the interval @xmath40 into the segments @xmath46 $ ] , @xmath47 $ ] , @xmath48 .",
    "let @xmath49 be any strictly increasing , infinitely differentiable function on @xmath40 with the properties    1 .",
    "@xmath50 for all @xmath51 ; 2 .",
    "@xmath52 ; 3 .",
    "@xmath53 , as @xmath54 .",
    "the existence of a strictly increasing smooth function satisfying these properties is easy to verify .",
    "note that from conditions ( 1)(3 ) it follows that any function @xmath55 satisfying the inequality @xmath56 for all @xmath51 , is @xmath26-strictly increasing and @xmath57 , as @xmath54 .",
    "we are going to construct @xmath1 obeying the required properties in stages .",
    "let @xmath58 be the sequence of all polynomials with rational coefficients defined on @xmath59.$ ] first , we define @xmath1 on the closed intervals @xmath60 $ ] , @xmath61 , as the function @xmath62,\\ ] ] or equivalently , @xmath63,\\ ] ] where @xmath64 and @xmath65 are chosen in such a way that the condition @xmath66 holds for all @xmath67 $ ] .    at the second stage",
    "we define @xmath1 on the intervals @xmath68 $ ] , @xmath61 , so that it is in @xmath37 and satisfies the inequality ( [ eq : h_sigma_1 ] ) .",
    "finally , in all of @xmath39 we define @xmath1 while maintaining the @xmath69 strict monotonicity property , and also in such a way that @xmath70 .",
    "we obtain from the properties of @xmath71 and the condition ( [ eq : h_sigma_1 ] ) that @xmath24 is a @xmath26-strictly increasing function on the interval @xmath40 and @xmath72 , as @xmath54 .",
    "note that the construction of a @xmath1 obeying all the above conditions is feasible .",
    "we show this in the next section .",
    "from ( [ eq : sigma ] ) it follows that for each @xmath73 , @xmath74 , @xmath48 , @xmath75    let now @xmath76 be any continuous function on the unit interval @xmath77 $ ] . by the density of polynomials with rational coefficients in the space of continuous functions over any compact subset of @xmath0 , for any @xmath43 there exists a polynomial @xmath78 of the above form such that @xmath79 for all @xmath80 $ ] .",
    "this together with ( [ eq : u_m ] ) means that @xmath81 for some @xmath20 , @xmath21 , @xmath82 and all @xmath80 $ ] .",
    "note that ( [ eq : g_epsilon ] ) proves our theorem for the unit interval @xmath59 $ ] . using linear transformation",
    "it is not difficult to go from @xmath59 $ ] to any finite closed interval @xmath83 $ ] .",
    "indeed , let @xmath14 $ ] , @xmath1 be constructed as above and @xmath19 be an arbitrarily small positive number .",
    "the transformed function @xmath84 is well defined on @xmath59 $ ] and we can apply the inequality ( [ eq : g_epsilon ] ) . now using the inverse transformation @xmath85 ,",
    "we can write that @xmath86 where @xmath87 and @xmath88 .",
    "the last inequality ( [ eq : f_epsilon ] ) completes the proof .    the idea of using a limited number of neurons in hidden layers of a feedforward network was first implemented by maiorov and pinkus @xcite .",
    "they proved the existence of a sigmoidal , strictly increasing , analytic activation function such that two hidden layer neural networks with this activation function and a fixed number of neurons in each hidden layer can approximate any continuous multivariate function over the unit cube in @xmath7 .",
    "note that the result is of theoretical value and the authors do not suggest constructing and using their sigmoidal function . using the techniques developed in @xcite",
    ", we showed theoretically that if we replace the demand of analyticity by smoothness and monotonicity by @xmath26-monotonicity , then the number of neurons in hidden layers can be reduced substantially ( see @xcite ) .",
    "we stress again that in both papers the algorithmic implementation of the obtained results is not discussed nor illustrated by numerical examples .    in the next section ,",
    "we propose an algorithm for computing the above sigmoidal function @xmath1 at any point of the real axis .",
    "the code of this algorithm is available at http://sites.google.com/site/njguliyev/papers/sigmoidal . as examples ,",
    "we include in the paper the graph of @xmath1 ( see figure 1 ) and a numerical table ( see table 1 ) containing several computed values of this function .",
    "* step 1 . * _ definition of @xmath49 . _    set @xmath89 note that this function satisfies the conditions 1)3 ) in the proof of theorem 2.1 .",
    "* step 2 . *",
    "_ enumerating the rationals .",
    "_    let @xmath90 be stern s diatomic sequence : @xmath91 it should be remarked that this sequence first appeared in print in 1858 @xcite and has been the subject of many papers ( see , e.g. , @xcite and the references therein ) .",
    "the calkin ",
    "wilf @xcite sequence @xmath92 contains every positive rational number exactly once and hence the sequence @xmath93 is the enumeration of all the rational numbers .",
    "it is possible to calculate @xmath94 and @xmath95 directly .",
    "let @xmath96 be the binary code of @xmath2 . here",
    ", @xmath97 show the number of 1-digits , 0-digits , 1-digits , etc .",
    ", respectively , starting from the end of the binary code .",
    "note that @xmath98 can be zero .",
    "then @xmath94 equals the continued fraction @xmath99 : = f_0 + \\dfrac1{f_1 + \\dfrac1{f_2 + \\dfrac1{\\ddots + \\dfrac1{f_k}}}}.\\ ] ] the calculation of @xmath95 is reduced to the calculation of @xmath100 , if @xmath2 is even and @xmath101 , if @xmath2 is odd .",
    "* step 3 . *",
    "_ enumerating the polynomials with rational coefficients . _",
    "it is clear that every positive rational number determines a unique finite continued fraction @xmath102 $ ] with @xmath103 , @xmath104 and @xmath105 .",
    "since each non - zero polynomial with rational coefficients can uniquely be written as @xmath106 , where @xmath107 ( i.e. @xmath108 ) , we have the following bijection between the set of all non - zero polynomials with rational coefficients and the set of all positive rational numbers : @xmath109\\ ] ] we define @xmath110 and @xmath111 where @xmath112 $ ] .",
    "* step 4 . * _ construction of @xmath1 on @xmath113 $ ] .",
    "_    set @xmath114 . besides , for each polynomial @xmath115 , set @xmath116 and @xmath117 it is not difficult to verify that @xmath118.\\ ] ]    if @xmath119 is constant , then we put @xmath120 otherwise , we define @xmath121 as the function @xmath122,\\ ] ] where @xmath123 note that @xmath124 , @xmath125 are the coefficients of the linear function @xmath126 mapping the closed interval @xmath127 $ ] onto the closed interval @xmath128 $ ] .",
    "thus , @xmath129 for all @xmath67 $ ] .     on",
    "@xmath130 $ ] ( @xmath131 , @xmath132),scaledwidth=75.0% ]    .some computed values of @xmath1 ( @xmath131 , @xmath132 ) [ cols=\"^,^,^,^,^,^,^,^,^,^\",options=\"header \" , ]     * step 5 . * _ construction of @xmath1 on @xmath68 $ ] . _    to define @xmath1 on the intervals @xmath68 $ ] we will use the _ smooth transition function _",
    "@xmath133 where @xmath134 it is easy to see that @xmath135 for @xmath136 , @xmath137 for @xmath138 and @xmath139 for @xmath140 .",
    "set @xmath141 since both @xmath142 and @xmath143 belong to the interval @xmath144 , we obtain that @xmath145 .",
    "first we extend @xmath1 smoothly to the interval @xmath146 $ ] .",
    "take @xmath147 and choose @xmath148 such that @xmath149.\\ ] ] let us show how one can choose this @xmath150 . if @xmath119 is constant , it is sufficient to take @xmath151 . if @xmath119 is not constant , we take @xmath152 where @xmath153 is a number satisfying @xmath154 for @xmath155 .",
    "now define @xmath1 on the first half of the interval @xmath68 $ ] as the function @xmath156 .",
    "\\end{split}\\ ] ]    let us verify that @xmath24 satisfies the condition ( [ eq : h_sigma_1 ] ) .",
    "indeed , if @xmath157 , then there is nothing to prove , since @xmath158 .",
    "if @xmath159 , then @xmath160 and hence from ( [ eq : sigma_left ] ) it follows that for each @xmath161 , @xmath24 is between the numbers @xmath162 and @xmath163 . on the other hand , from ( [ eq : epsilon ] ) we obtain that @xmath164 which together with ( [ eq : sigma_again ] ) and ( [ eq : h_m_sigma_1 ] ) yields that @xmath165 $ ] , for @xmath161 . since @xmath166 ,",
    "the inclusion @xmath167 is valid .",
    "now since both @xmath162 and @xmath168 belong to @xmath144 , we finally conclude that @xmath169.\\ ] ]    we define @xmath1 on the second half of the interval in a similar way : @xmath170 , \\end{split}\\ ] ] where @xmath171 } |u'_{m+1}(t)|.\\ ] ] one can easily verify , as above , that the constructed @xmath24 satisfies the condition ( [ eq : h_sigma_1 ] ) on @xmath146 $ ] and @xmath172    * step 6 . * _ construction of @xmath1 on @xmath39 .",
    "_    finally , we put @xmath173 it is not difficult to verify that @xmath1 is a strictly increasing , smooth function on @xmath39 . note also that @xmath174 ( see step 4 ) , as @xmath175 tends to @xmath36 from the left and @xmath176 , for @xmath177 .",
    "step 6 completes the construction of the universal activation function @xmath1 , which satisfies theorem 2.1 .",
    "although theorem 2.1 is valid for all continuous functions , in practice it is quite difficult to calculate algorithmically @xmath178 , @xmath179 and @xmath22 in theorem 2.1 for badly behaved continuous functions .",
    "the main difficulty arises while attempting to design an efficient algorithm for the construction of a best approximating polynomial within any given degree of accuracy .",
    "but for certain large classes of well behaved functions , the computation of the above numbers is doable . in this section ,",
    "we show this for the class of lipschitz continuous functions .",
    "assume that @xmath15 is a lipschitz continuous function on @xmath41 $ ] with a lipschitz constant @xmath180 . in order to find the parameters @xmath178 , @xmath179 and @xmath22 algorithmically , it is sufficient to perform the following steps .      consider the function @xmath181 , which is lipschitz continuous on @xmath59 $ ] with a lipschitz constant @xmath182 .",
    "denote by @xmath183 the @xmath2-th bernstein polynomial of the function @xmath76 .",
    "let @xmath43 be given .",
    "define the functions @xmath184 which return the positions of a positive rational number and a rational number in the sequences @xmath185 and @xmath186 , respectively ( see section 3 ) .",
    "we start with the computation of @xmath187 .",
    "let @xmath188 be a positive rational number .",
    "if ( [ eq : fraction ] ) is the continued fraction representation of @xmath188 with @xmath189 even ( we may always consider @xmath190 $ ] instead of @xmath191 $ ] if needed ) , then the binary representation of the position @xmath192 of @xmath188 in the calkin  wilf sequence is @xmath193 now we can easily find @xmath194 by the formula @xmath195                  set @xmath220 . if @xmath221 is constant then we put @xmath222 if @xmath221 is not constant then we put @xmath223 where @xmath124 and @xmath125 are computed using the formulas ( [ eq : a_1 ] ) , ( [ eq : a_2 ] ) and ( [ eq : a_m , b_m ] ) .      in this step , we return to our original function @xmath15 and calculate the numbers @xmath178 , @xmath179 and @xmath22 ( see theorem 2.1 ) .",
    "the numbers @xmath178 and @xmath179 have been calculated above ( they are the same for both @xmath76 and @xmath15 ) . in order to find @xmath22",
    ", we use the formula @xmath224 .    note that some computational difficulties may arise while implementing the above algorithm in standard computers .",
    "for some functions , the index @xmath225 of a polynomial @xmath119 in step 5 may be extraordinarily large . in this case , a computer is not capable of producing this number , hence the numbers @xmath178 , @xmath179 and @xmath22 .",
    "s. m. carroll and b. w. dickinson , construction of neural nets using the radon transform , in _ proceedings of the ieee 1989 international joint conference on neural networks _ , 1989 , vol .",
    "1 , ieee , new york , pp .",
    "607611 .",
    "a. r. gallant and h. white , there exists a neural network that does not make avoidable mistakes , in _ proceedings of the ieee 1988 international conference on neural networks _ , 1988 , vol .",
    "1 , ieee , new york , pp . 657664 .",
    "m. stinchcombe and h. white , approximating and learning unknown mappings using multilayer feedforward networks with bounded weights , in _ proceedings of the ieee 1990 international joint conference on neural networks _ , 1990 , vol .",
    "3 , ieee , new york , 716 ."
  ],
  "abstract_text": [
    "<S> the possibility of approximating a continuous function on a compact subset of the real line by a feedforward single hidden layer neural network with a sigmoidal activation function has been studied in many papers . </S>",
    "<S> such networks can approximate an arbitrary continuous function provided that an unlimited number of neurons in a hidden layer is permitted . in this paper </S>",
    "<S> , we consider constructive approximation on any finite interval of @xmath0 by neural networks with only one neuron in the hidden layer . we construct algorithmically a smooth , sigmoidal , almost monotone activation function @xmath1 providing approximation to an arbitrary continuous function within any degree of accuracy . </S>",
    "<S> this algorithm is implemented in a computer program , which computes the value of @xmath1 at any reasonable point of the real axis . </S>"
  ]
}