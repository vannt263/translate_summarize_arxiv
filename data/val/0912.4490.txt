{
  "article_text": [
    "during the last three decades , cosmology has made tremendous progress : from order of magnitude estimates to measurements of key cosmological parameters approaching percent level accuracy .",
    "the standard model of cosmology , based on the growth of structure by gravitational instability , has been impressively validated on large scales where the perturbations to the friedman model are small .",
    "while the model is successful at predicting or reproducing a wide array of observations , it contains several mysterious elements with perhaps the most mysterious being the accelerated expansion of the universe @xcite .",
    "the accelerated expansion may be caused by a dark energy or hinting at a modification of general relativity on the largest scales .",
    "to date most of the best known cosmological parameters have been constrained primarily from the study of anisotropies in the cosmic microwave background ( cmb ) radiation . however , large - scale structure probes play an important role in breaking parameter degeneracies and in constraining conditions in the late - time universe .",
    "such probes are becoming ever more precise , with future surveys aiming at measurements approaching percent level accuracy to better characterize the universe in which we live .",
    "techniques based on the observation and analysis of cosmic structure include the use of baryon acoustic oscillations , redshift space distortions , weak lensing measurements and the abundance of clusters of galaxies ; they stand to play a pivotal role in improving our understanding of the dynamics of the universe .",
    "( for a recent discussion regarding improvements on dark energy constraints from combining different probes , see , e.g.  @xcite ) .",
    "the large scale structure of the universe contains information about both the geometry , as well as the dynamics of structure formation . in combination",
    "these two pieces of information can help distinguish between dark energy or a modification of general relativity as the prime cause of cosmic acceleration .    on small scales ,",
    "the cosmological interpretation of structure formation probes is complicated due to the nonlinear physics involved .",
    "commonly - used fitting functions for e.g. , the power spectrum @xcite have poorly characterized systematics and are no longer adequate for precision work . absent a controlled theoretical framework , direct use of simulations ( augmented with phenomenological parameters as appropriate ) is essential if the physics is to be more correctly captured .",
    "the simulation codes need to be adequately tested to ensure they meet the new demands being placed upon them .",
    "the simulations which pass these goals are often very expensive and only a restricted number of runs can be performed .",
    "this in turn puts a premium on developing very efficient strategies to constrain parameters from limited observations and simulations .",
    "high - precision prediction schemes for different statistics are essential to succeed in this task .    [",
    "cols=\"^,^,^,^,^,^,^,^,^,^,^,^,^,^,^,^,^,^\",options=\"header \" , ]     [ tab : basic ]    this paper is the third in a series aimed at addressing this question in the context of the nonlinear matter power spectrum on mpc scales .",
    "current observations in weak lensing are quickly becoming theory limited due to the lack of precise theoretical estimates of this quantity for a wide range of cosmological models .",
    "this is a pressing problem .",
    "it is also relatively simple , allowing us to work through  in a concrete setting  the many steps which will be routinely required in the future . in some ways",
    "the problem is one of the simplest currently confronting theorists , but as we shall discuss below even it has demanded collaboration with other communities , the development of a significant infrastructure , new modes of working , and large amounts of manpower and computational capacity .    in paper",
    "i of this series ( heitmann et al .",
    "2009a ) we demonstrated that it is possible to obtain nonlinear matter power spectra with percent level accuracy out to @xmath2 and derived a set of requirements for such simulations .",
    "paper  ii @xcite described the construction of an emulation scheme to predict the nonlinear matter power spectrum and the underlying cosmological models , building on the `` cosmic calibration framework '' @xcite . here ,",
    "in paper  iii , we present results from the complete simulation suite based on the cosmologies presented in the second paper and publicly release a precision power spectrum emulator  .",
    "the simulation suite is called the `` coyote universe '' after the cluster it has been carried out on .",
    "we will extend our work to include other measurements , such as the mass function or higher order statistics , in future publications .",
    "the outline of this paper is as follows . in  [ sec : sims ] we describe the simulations we have run in support of this program , and the codes with which they were run . ",
    "[ sec : power ] describes how we put together the different estimations of the power spectra from our multi - scale runs while  [ sec : emulator ] presents the details of the emulator and tests .",
    "we describe some lessons learned in  [ sec : lessons ] before concluding in  [ sec : conclusions ] .",
    "the coyote universe simulation suite encompasses nearly 1,000 simulations of varying force and mass resolution .",
    "the simulation volume is the same in all cases , a periodic cube of side length @xmath3mpc .",
    "we consider 37 + 1 cosmological models , listed in table [ tab : basic ] , which we select with two aspects in mind : our statistical framework and current constraints from a variety of cosmological measurements ( see paper ii for further discussions and see below for a short summary ) .",
    "the 37 models , labeled 1 - 37 , are used to construct the emulator while model m000 is used as an independent check on the power spectrum accuracy in the parameter regime of most interest .",
    "( other tests are described in  [ sec : emulator ] and in paper  ii . )    for each cosmology we run 20 realizations , 16 lower resolution simulations covering the low-@xmath4 regime ( which we refer to as the `` l - series '' ) , 4 medium resolution runs to provide good statistics in the quasi - linear to mildly nonlinear regime ( the `` h - series '' ) , and one high resolution run to extend to @xmath2 ( the `` g - series '' ) .",
    "the high - resolution run uses the same realization as one of the medium resolution runs .",
    "the low- and medium - resolution runs are performed with a particle - mesh ( pm ) code .",
    "the code evolves @xmath5 particles in the low - resolution runs and @xmath6 particles in the medium - resolution runs , in each case with a force mesh twice as large in each dimension as the particle load .",
    "densities and forces are computed using cloud - in - cell ( cic ) interpolation and a fast fourier transform ( fft)-based poisson solver .",
    "the potential is determined from the density using a @xmath7 green s function and the force is computed by @xmath8 order differencing .",
    "particles are advanced with second order ( global ) symplectic time - stepping with @xmath9 . in order to resolve the high-@xmath4 part of the power spectrum , we evolve one of the @xmath6 particle initial distributions with the tree - pm code gadget-2 @xcite .",
    "we use a pm grid twice as large , in each dimension , as the number of particles , and a ( gaussian ) smoothing of @xmath10 grid cells .",
    "the force matching is set to @xmath11 times the smoothing scale , the tree opening criterion to @xmath12 and the softening length to @xmath13kpc .",
    "the starting redshift for each simulation is @xmath14 .",
    "further details regarding the simulations and choices of simulation parameters can be found in paper i.    all the simulations are carried out on _ coyote _ , a large hpc linux cluster consisting of 2580 amd opterons running at @xmath15ghz .",
    "the low - resolution pm runs are carried out on 64 processors , the medium - resolution pm runs and gadget-2 runs on 256 .",
    "for the billion particle runs we store particle and halo information at 11 different redshifts between @xmath16 and @xmath17 .",
    "this leads to a final simulation database of size roughly @xmath18 tb .",
    "the selection of the cosmological model suite depends on two considerations : the statistical framework we use to construct the emulator and current parameter constraints from the cmb as set by wmap-5  @xcite .",
    "we do not insist on a formal methodology to make the model selection , but instead apply some practical and conservative arguments to justify our decisions .",
    "for an in - depth discussion we refer the reader to paper ii . in this paper",
    "we briefly summarize some of the considerations since these will define the region for which the emulator will be valid .    our aim is to find a distribution of the parameter settings  the simulation design  which provides optimal coverage of the parameter space , using only a limited number of sampling points .",
    "simulation designs well - suited for this task are latin - hypercube ( lh)-based designs , a type of stratified sampling scheme .",
    "latin hypercube sampling generalizes the latin square for two variables , where only one sampling point can exist in each row and each column . a latin hypercube sample  in arbitrary dimensions  consists of points stratified in each ( axis - oriented ) projection .",
    "very often lh designs are combined with other design strategies such as orthogonal array ( oa)-based designs or are optimized in other ways , e.g. , by symmetrizing them ( more details below ) . by intelligently melding design strategies , different attributes of the individual sampling strategies",
    "can be combined to lead to improved designs , and shortcomings of specific designs can be eliminated . as a last step ,",
    "optimization schemes are often applied to spread out the points evenly in a projected space .",
    "one such optimization scheme is based on minimizing the maximal distance between points in the parameter space , which will lead to more even coverage .",
    "two design strategies well suited to cosmological applications in which the number of parameters is much less than the number of simulations that can be performed are optimal oa - lh design strategies and optimal symmetric lh design strategies .",
    "for this project , we have generated forty different designs following different strategies and chose the best design from these ( where `` best '' refers to best coverage in parameter space with respect to specific distance criteria explained in paper ii ) .    in order to restrict the number of necessary simulation runs to an as small number as possible , it is helpful to keep the number of cosmological parameters and their prior ranges small .",
    "current observations of the cmb and the large scale structure are consistent with a @xmath19cdm model with constant dark energy equation of state , @xmath20 .",
    "we therefore concentrate on the following five cosmological parameters : @xmath21 , @xmath22 , @xmath23 , @xmath20 , and @xmath24 where @xmath25 contains the contributions from the dark matter and the baryons .",
    "we restrict ourselves to power - law models ( no running of the spectral index ) and to spatially flat models without massive neutrinos . for each cosmology",
    ", @xmath26 is determined by the angular scale of the acoustic peaks in the cmb ( paper ii ) which is known to very high accuracy ( 0.3% ; * ? ? ?",
    "* ) . from wmap 5-year data , in combination with bao",
    ", we have @xmath27 current data restrict a constant equation of state for the dark energy to @xmath28 with roughly 10% accuracy and recent determinations put the normalization in the range @xmath29 with still rather large uncertainties .",
    "considering all these constraints and their uncertainties , we choose our sample space boundaries for the 37 + 1 models to lie within the range(s ) @xmath30 over which the emulator is designed to produce reliable results .",
    "we verified in paper ii that 37 models spanning these parameter ranges are indeed enough to generate an emulator at the 1% accuracy .",
    "we emphasize that our emulator is valid for the _ complete _ parameter space defined by these priors and not restricted to some values in a band around the best - fit cosmology ( for current observational data ) . obviously , the emulator quality will be slightly worse on the edges of the hypercube but the emulator is always accurate within @xmath311% anywhere within the priors .      in paper i",
    "we describe in detail how we obtain the matter power spectrum from a snapshot of the simulation .",
    "we briefly summarize the salient points here .",
    "we compute the dimensionless power spectrum , @xmath32 which is the contribution to the variance of the density perturbations per @xmath33 .",
    "we obtain @xmath34 by binning the particles onto a 2048@xmath35 grid using cic assignment , applying an fft , correcting for the charge - assignment window function , and averaging the result in fine bins in @xmath36 spaced linearly with width @xmath37 . as discussed in paper",
    "i , we do not correct for particle discreteness as our particle loading is high enough to make such corrections unnecessary and there are some indications that a simple poisson shot - noise form is not correct .",
    "when creating a nonlinear power spectrum emulator , we prefer that the underlying training data set be smooth .",
    "we describe here how we construct a smooth power spectrum from the 20 realizations of each cosmology and from cosmological perturbation theory .    in each simulation",
    "the modes in the initial density field are a single realization of a gaussian random field and this introduces large run - to - run scatter .",
    "this scatter is reduced at higher @xmath4 by the relatively large number of modes which are averaged . however , at low @xmath4 the estimates of the power spectrum exhibit significant scatter which is expensive to reduce by brute force , i.e. running a very large number of realizations in large volumes .",
    "in addition , the approach to linear theory at low @xmath4 can be quite slow for many currently popular models around @xmath19cdm ( if @xmath38 accuracy is the desired goal ) so simply replacing the n - body results with the input linear model can be relatively inaccurate .",
    "this is however an area where perturbation theory can be of help , since the real - space mass power spectrum is computable in perturbation theory and there is a small but non - negligible range of scales where perturbation theory improves upon linear theory .",
    "for a recent overview of the performance of different perturbation theory approaches , see @xcite .",
    "while there has been a resurgence of interest in perturbative methods in recent years , we stick to the simplest and oldest method : `` standard perturbation theory '' @xcite .",
    "we consider only the first correction to linear theory , which in standard perturbation theory can be written in terms of a simple integral over the linear theory power spectrum ( we use the specific form given in @xcite ) . when compared to our simulations , we find that standard perturbation theory is accurate at the percent level for @xmath39 where @xmath40 can be defined as @xcite : @xmath41 the values for @xmath40 at @xmath17 and @xmath42 are listed for all models in table  [ tab : basic ] .",
    "for example , for model m000 , @xmath43 at @xmath17 .",
    "similar results have been reported in @xcite and @xcite .",
    "almost any scheme that switches smoothly from standard perturbation theory to our n - body results around @xmath44 produces results that agree at the percent level . at @xmath17 ,",
    "this would lead to a matching point between @xmath45  mpc@xmath46 ( m036 ) and @xmath47mpc@xmath46 ( m019 ) . for simplicity",
    "we keep the matching point the same for all cosmologies .",
    "since we have good statistics from our simulations at @xmath48  mpc@xmath46 already , we choose this @xmath4 value as a very conservative matching point for all models .",
    "to verify this point , we compare our simulation results to perturbation theory for the different cosmologies .",
    "the simplest approach for this comparison is `` brute force '' : simply take the ratio of the simulation result to the perturbation theory prediction .",
    "the major obstacle here is the run - to - run scatter in the simulations . in order to overcome this problem",
    ", one can follow two routes : either incorporate fluctuations from the realization into the perturbation theory prediction , as done in @xcite , or average over a large number of large volume simulations .",
    "we follow the second approach to avoid any ambiguities and systematic errors resulting from finite box size effects .",
    "an example of our approach is shown in figure  [ fig : compare ] for a random subset of 3 models from the total of 37 used to build the emulator .",
    "although at the lower end of the @xmath4 range , the large but finite sampling volume becomes clearly evident , the matching to perturbation theory at @xmath49mpc@xmath46 works extremely well .",
    "a similar result can also be found in @xcite .",
    "next , we show how we can combine perturbation theory and results from @xmath0-body simulations to generate smooth power spectra which will be the foundation for building our emulator .",
    "two problems have to be solved for this : ( 1 ) we have to eliminate the scatter in the @xmath0-body results for the nonlinear power spectrum without erasing subtle features and match results from different resolution simulations . ( 2 ) we have to match very accurately between perturbation theory and simulation results . in the following ,",
    "we discuss an approach based on process convolution to solve these problems .      in this section",
    ", we discuss the procedure for estimating the smooth power spectrum for each cosmology based on the simulation results which possess inherent scatter .",
    "figure [ fig : m001 ] shows the power spectrum from the simulations for model m001 at @xmath17 .",
    "the data have three notable features that are important for the modeling procedure .",
    "( 1 ) the non - standard representation for the power spectrum @xmath50 is chosen to accentuate the baryon acoustic oscillations .",
    "we will account for this feature when we choose our function class for the smooth power spectra ( discussed later ) .",
    "( 2 ) the three series of simulations ; g , h , and l with 1 , 4 , and 16 realizations respectively .",
    "because the h and l series do not have enough force resolution to resolve the nonlinear regime at high @xmath4 , we need to restrict the use of these runs to small and intermediate @xmath4 ranges .",
    "( 3 ) the simulation variance at any given @xmath4 is known .",
    "we treat each simulated ( `` noisy '' ) realization of the ( large volume or averaged ) spectrum as a draw from a multivariate gaussian distribution whose mean is given by an unknown smooth spectrum .",
    "thus , for a given cosmology @xmath51 , a given series @xmath52 , and a given replicate @xmath53 ( where @xmath54 is the number of simulations for the series ) , we have a multivariate gaussian density for the simulated spectrum @xmath55 , @xmath56 here , @xmath57 is the smooth power spectrum ; @xmath58 is a projection matrix of zeros and ones that is used to remove the high-@xmath4 values for which the h and l series are not used ( thus , @xmath59 is an identity matrix ) ; and @xmath60 is a diagonal matrix of the known precisions ( inverse variances ) .    the model is completed by specifying a class of functions for the smooth power spectrum @xmath57 .",
    "we choose a flexible class of smooth functions called a process convolution @xcite .",
    "these functions are best described constructively .",
    "a process convolution builds a smooth function as a moving average of a simple stochastic process like independent and identically - distributed ( _ i.i.d .",
    "_ )  gaussian variates or brownian motion .",
    "the moving average uses a smoothing kernel whose width is allowed to vary over the domain to account for nonstationarity ( smoother in some regions , more wiggly in others ) .",
    "figure [ fig : proc - conv ] shows a simple example of a process convolution built on white noise smoothed with a gaussian kernel .",
    "we build the process convolution for @xmath57 on brownian motion @xmath61 , with marginal variance @xmath62 , realized on a sparse grid ( relative to the power spectrum ) of evenly spaced points , @xmath63 ( the number of points is not important so long as it is large enough ; we use 100 ) , @xmath64 where @xmath65 is the brownian precision matrix with diagonal equal to @xmath66 $ ] and @xmath67 on the first off - diagonals ( this matrix can not actually be inverted , but never has to be in the estimation ) .",
    "the brownian motion is transformed into @xmath57 by the smoothing matrix @xmath68 , @xmath69 the smoothing matrix @xmath68 is built using gaussian kernels whose width varies smoothly across the domain .",
    "thus , we have @xmath70 where @xmath71 is the @xmath72-th value of @xmath4 for which the power spectrum is computed .    in the description of @xmath68 ,",
    "@xmath73 is indexed to indicate that it changes over the domain",
    ". intuitively , we want @xmath73 to be small in the middle of the domain in order to capture the oscillations , but large elsewhere to smooth away the noise . in order to estimate this varying bandwidth parameter",
    ", we build a second process convolution model .",
    "this model is built on _ i.i.d .",
    "_ gaussian variates @xmath74 , with mean zero and variance @xmath75 , observed on an even sparser grid of evenly spaced points , @xmath76 ( length @xmath77 ; we use 10 , but any large enough number will suffice ) , @xmath78 the process @xmath74 is transformed into @xmath73 by the smoothing matrix @xmath79 , @xmath80        this matrix is also built using gaussian smoothing kernels , but with a constant bandwidth , @xmath81 .",
    "thus , we have @xmath82    combining all of this , we get a distribution for the simulated power spectra for a given cosmology , @xmath83 where @xmath84 and @xmath85 .",
    "note that @xmath68 is a function of the parameters @xmath74 and @xmath81 .",
    "we choose noninformative priors for @xmath86 , @xmath75 , and @xmath81 so as to impart little or no information about their values , @xmath87 which are inverse gamma , inverse gamma , and uniform , respectively .",
    "equations ( [ eq : uprior ] ) ( for all @xmath51 ) , ( [ eq : vprior ] ) , and ( [ eq : priors ] ) are multiplied together with eqn .",
    "( [ eq : likelihood ] ) ( for all @xmath51 ) to produce a posterior distribution for the unknown parameters and stochastic processes . obtaining an estimate for the smooth power spectra",
    "requires an estimate for each of the parameters , the process @xmath74 , and each of the processes @xmath61 for all @xmath51 .",
    "markov chain monte carlo ( mcmc ) via the metroplis - hastings algorithm @xcite produces a sample from the posterior distribution by drawing each parameter individually .",
    "the stochastic processes @xmath61 can be integrated out of the distribution , so the mcmc produces samples of the parameters as well as @xmath74 .",
    "we use the posterior mean of the parameters to obtain a conditional mean for each of the @xmath61 which is then transformed into each of the @xmath57 .    the perturbation results are included by setting the low @xmath4 values for each of the simulated spectra in a given cosmology to the perturbation results and setting the precisions for these values to be very large .",
    "this is done prior to the estimation described here .",
    "the replication of these values in every simulation , combined with the large precisions , nearly forces the estimated result through these points .",
    "further , the transition from the n - body results to the perturbation results will be relatively smooth as long as the two line up fairly well .",
    "the next step is to test the matching procedure between theoretical and simulation results to ensure high accuracy at the perturbation theory matching point . since we do not know the exact answer for the nonlinear power spectrum , we first carry out a test using linear theory . in this test",
    "we use the power spectra from the initial conditions and show how well we can smooth out the run - to - run scatter . knowing the exact answer here will allow us to assess how well the matching procedure actually works .    for these spectra",
    ", the entire estimation procedure is applied to the initial condition spectra .",
    "the only difference is that fewer grid points were used for the latent processes @xmath88 and @xmath74 to account for the reduced range of the spectra ( we use 70 and 7 , respectively ) .",
    "the final results are shown in figure  [ fig : lin_pred ] for the same set of three random models as in figure  [ fig : compare ] .",
    "we verified that the results hold for all of the remaining models .",
    "the upper panels of each sub - panel show the theoretical linear power spectrum in black and the prediction from the simulations in red .",
    "the vertical line marks the matching point to linear theory at @xmath89mpc@xmath46 .",
    "the lower panels show the ratio of the predicted power spectra to the theoretical power spectra in red . below the matching point",
    ", the agreement is  by construction  perfect . beyond the matching point ,",
    "the smoothed prediction in red is accurate at the 1% level .",
    "this test shows that we can obtain a smooth , high - accuracy prediction for the power spectrum by combining a suite of realizations with perturbation theory .",
    "the process convolution procedure is applied to the power spectrum realizations for each of the 37 cosmologies at six values of the scale factor @xmath90 , where @xmath91 is the redshift .",
    "the results are shown in figure  [ fig : smooths ] , again for a subset of three models .",
    "although we have no known truth to use as a comparison in this case , the resulting estimates continue to fit the simulation realizations very well .    in place of comparing with known truth",
    ", we can examine some tests of our modeling assumptions .",
    "we assume that the simulation spectra on the modified scale are independently and normally distributed about a smooth mean with a variance that changes with @xmath4 . given this",
    ", we can compute standardized residuals in which the estimated smooth mean is subtracted from the simulations and the result is scaled by the known standard deviation .",
    "these standardized residuals should look like _ i.i.d .",
    "_ standard normal variables .",
    "figure [ fig : qq ] shows quantile - quantile plots for the g simulations of three cosmologies at each of the six scale factors .",
    "theoretical quantiles from a standard normal distribution are given on the @xmath63-axis and the sample quantiles of the standardized residuals are shown on the @xmath92-axis .",
    "the nearly straight line at @xmath93 indicates an extremely good distributional fit .",
    "figure [ fig : res ] shows the standardized residuals plotted against @xmath4 for the same simulations .",
    "this plot verifies the independence assumption .",
    "further , there is an almost complete lack of evidence of any structure in these plots , suggesting that we would not greatly improve the fit by relaxing the smoothness assumption .",
    "it is also interesting to examine the plot of the kernel width function as estimated by the mcmc process .",
    "figure [ fig : sigma ] shows the median draw for @xmath73 as a function of @xmath4 . as expected , the kernel width is small in the vicinity of the baryon wiggles .",
    "this means the values of the latent process @xmath88 in this region receive large weights , with the contributions for values further away dropping off very quickly . on both ends ,",
    "the kernel width is large , which results in a smooth function in these regions .",
    "having extracted the smooth power spectra from our simulation suite , we can now build an emulator to predict the nonlinear matter power spectrum within the priors specified in eqns .",
    "( [ priors ] ) .",
    "we will use only models 1 - 37 for the emulator construction ; model m000 will serve as an independent check of the emulator accuracy , along with hold - out tests described below .    in order to construct the emulator , we model the 37 power spectra using an @xmath94 dimensional basis representation : @xmath95^{n_\\mathcal{\\theta}},\\ ] ] where the @xmath96 are the basis functions , the @xmath97 are the corresponding weights , and the @xmath98 represent the cosmological parameters .",
    "the dimensionality @xmath94 refers to the number of orthogonal basis vectors @xmath99 .",
    "the parameter @xmath100 is the dimensionality of our parameter space  with 5 cosmological parameters we have @xmath101 .",
    "the power spectrum @xmath102 depends on the wavenumber @xmath4 , the redshift @xmath91 , and the five cosmological input parameters @xmath98 ( note that we rescale the range of each parameter to @xmath103 $ ] ) .",
    "examination of the results indicates that @xmath104 is a good choice for the number of basis vectors @xmath96 ( that @xmath105 here is a coincidence ) .",
    "the task is now to ( 1 ) construct a suitable set of orthogonal basis vectors @xmath96 and ( 2 ) model the weights @xmath97 ) .",
    "for the first task we use principal components , and for the second , gaussian process ( gp ) models .",
    "our choice of gp modeling is based on their success in representing functions that change smoothly with parameter variation , e.g. , the variation of the power spectrum as a function of cosmological parameters .",
    "both steps are explained in great detail in paper ii ; we refer the interested reader to that publication .",
    "paper ii also contains various error control tests of the gp - based interpolation method which we do not repeat here .    since the details of how to build an emulator",
    "are already provided in paper ii we can immediately turn to our final product : the emulator itself . to facilitate use of the emulator",
    ", we are releasing the fully trained emulator with this paper .",
    "it provides nonlinear power spectra at a set of redshifts between @xmath17 and @xmath42 out to @xmath106mpc@xmath46 , for any cosmological model specified within the priors given in eqns .",
    "( [ priors ] ) .      in order to verify the accuracy of the emulator",
    ", we perform two important checks ( other tests on the methodology were carried out in paper ii ) . for the first , we compare the simulation results for model m000 with the emulator prediction .",
    "figure [ fig : m000holdout ] shows the results of this true out - of - sample prediction .",
    "the plot shows the ratio of the emulated power spectrum to the actual simulation for the m000 cosmology at six values of the scale factor @xmath107 .",
    "this cosmology is completely interior in the design and , since m000 was not used to estimate the smoothing or the emulator , this test provides a good indication of the actual performance of the emulator within its parameter bounds .",
    "overall the agreement between the simulations and the emulator is excellent and for most of the @xmath4-range well below the 1% error bound .",
    "the second check consists of a sequence of holdout tests . in a holdout test",
    ", the emulator is built from 36 cosmological models and the emulator prediction can be then compared to the result from the extra model .",
    "the drawback of this test is that if we have only a very small number of models , each of them is important for capturing some part of the parameter space and taking it out for building the emulator degrades the emulator precision . in order to keep this problem to a minimum , we only perform holdout tests for models which are interior simulations , meaning that none of the five parameters are near the extreme limits of the chosen prior range",
    ". there are six such models : m004 , m008 , m013 , m016 , m020 , and m026 .",
    "the ratio of the emulated prediction to the actual simulation for these models is shown in figure  [ fig : holdout ] .",
    "for each cosmology , there are six ratios for each value of the scale factor .",
    "the lines are quite well behaved with errors largely within the 1% bounds .",
    "the advent of precision cosmology and the prospect of very large surveys such as lsst and jdem poses an enormous challenge to the theory community in the field of large scale structure predictions .",
    "future progress not only requires _ very accurate _ predictions but also the ability to produce predictions for different cosmological models _ very fast_. the aim of the coyote universe project was to take a first step in attacking this problem , focusing on the matter power spectrum at intermediate scales out to @xmath108  @xmath26mpc@xmath46 .",
    "while predicting the matter power spectrum on these scales at high accuracy may appear to be a moderately difficult task , it turned out to be a technical and computational challenge in several respects .",
    "it is generally hard to imagine problems and pitfalls in advance , which is why fully working through an example is so helpful .",
    "since the community has to follow a similar path in the future to create predictive capabilities for different cosmological probes , we summarize here some of the lessons learned during this project .",
    "the major differences between a project like this ( including all three papers of the series ) and previous numerical studies of large scale structure probes are :    \\(1 ) _ computational and storage capacity : _ the coyote universe simulation suite encompasses roughly 60 tb of data .",
    "the computational cost is of the order of a million cpu - hours and including waiting times in submission queues , downtimes of the machine , and so on , carrying out these simulations took roughly six months .",
    "the simulation size of one billion particles in a gpc@xmath35 volume was barely enough to resolve the scales of interest and for future work will certainly not be sufficient .",
    "such simulations will need larger volumes and better mass resolution .",
    "for example , to resolve a 10@xmath109m@xmath110 halo with 100 particles in a ( 3  gpc)@xmath35 volume , we would need 300 billion particles . while supercomputers will get faster and larger in the future ,",
    "generating many simulations at the edge of machine capabilities will always be a challenge .",
    "in addition , archiving the outputs of such simulations will become very expensive in terms of storage . from the coyote universe",
    "runs we stored 11 time snapshots plus the initial conditions ( particle positions and velocities and halo information ) leading to 250  gb of data per run . for the 300 billion particle run this",
    "would increase to 75  tb . only very few places worldwide would be able to manage the resulting large databases .",
    "\\(2 ) _ simulation infrastructure : _ running a very large number of simulations makes it necessary to integrate the major parts of the analysis steps into the simulation code and to automate as much of the mechanics of running the code ( submission , restarts ) as possible . for the coyote universe project we developed several scripts to generate the input files of the codes , to structure the directories where different runs are performed in , and to submit the simulations to the computing queue system . for future efforts of this kind the adoption and development of dedicated workflow capabilities for these tasks must be considered .",
    "the number of tasks to be carried out will become too large to keep track of without such tools .",
    "in addition , since large projects will require extensive collaborations , software tools will make it easier to work in a team environment since each collaborator will have information about previous tasks and results .",
    "an example of such a tool for cosmological simulation analysis and visualization is given in @xcite .",
    "we carried out the data analysis after the runs were finished .",
    "for very large simulations this is not very practical , and on - the - fly analysis tools are required to minimize read and write times and failures .",
    "this in turn requires that the code infrastructure be tailored to the problem under consideration .",
    "\\(3 ) _ serving the data : _ clearly , large simulation efforts can not be carried out by a few individuals , and require possibly community wide coordination .",
    "the simulation data will be valuable for many different projects .",
    "it is therefore necessary to make the data from such simulation efforts publicly available and serve them in a way that new science can be extracted from different groups of simulations .",
    "transferring large amounts of data is difficult because of limitations in communication bandwidth and also because of the large storage requirements",
    ". it would therefore be desirable to have computational resources dedicated to the database .",
    "in such a situation , researchers would be able to run their analysis codes on machines with direct access to the database and perform queries on the data easily .",
    "we are planning to make the coyote universe database available in the future and use it as a manageable testbed for such services .",
    "\\(4 ) _ communication with other communities : _ the complexity of the analysis task makes it necessary to efficiently collaborate and communicate with other communities , for example statisticians , computer scientists , and applied mathematicians .",
    "many tools that will be essential for precision cosmology in the future have already been invented  the task is to find them and use them in the best way possible .",
    "this paper is the last of the coyote universe series of publications .",
    "paper  i was concerned with demonstrating that percent level accuracy in the ( gravity only ) nonlinear power spectrum could be attained out to @xmath111 mpc@xmath46 .",
    "paper  ii showed that with only a relatively small number of simulations , interpolation across a high - dimensional space was possible at close to the same level of accuracy as that attained in the individual runs .",
    "paper  iii takes this work to the final conclusion : based on almost 1000 simulations spanning 38 @xmath20cdm cosmologies , we present a fast and very accurate prediction scheme  an emulator  for the nonlinear matter power spectrum .",
    "the emulator is accurate at the percent level , improving over commonly used fitting functions by almost an order of magnitude .",
    "the emulator construction  as explained in paper  ii  is based on gp modeling . in order to carry this out ,",
    "a major challenge is to produce a smooth power spectrum from a finite set of simulations for each cosmology . in order to minimize run - to - run scatter on very large scales",
    ", we performed several medium resolution simulations and matched these at sufficiently low @xmath4 to perturbation theory . on quasi - linear scales we used medium resolution simulations and matched those to high resolution",
    "runs at small spatial scales . matching the different resolution runs and perturbation theory",
    "accurately was carried out using process convolution .",
    "this technique allowed us to construct a smooth power spectrum for each cosmological model .",
    "the results were then used to construct the emulator via gp modeling as described in detail in paper  ii .",
    "we are releasing the power spectrum emulator as a c code , which allows the user to specify a cosmology within our priors and returns the power spectrum at six different redshifts between @xmath17 and @xmath42 out to @xmath111 mpc@xmath46 .",
    "these power spectra can now be used for further analysis of cosmological data .",
    "we are planning to extend the emulator in the near future to a larger @xmath4-range and will provide a smooth interpolation between results from different redshifts .    a major challenge will be to ensure that a certain level of accuracy is reached with the simulations when going to small scales . besides being computationally very expensive ( high force and mass resolution being required ) the physics at smaller scales is far more complicated .",
    "the inclusion of gasdynamics and feedback effects ( along with other physics ) is far from being straightforward .",
    "the impossibility of carrying out a direct simulation effort is more or less certain ; a good number of phenomenological / subgrid modeling parameters will be required . as simulation complexity and the number of modeling and cosmological parameters increases",
    ", it becomes even more important to develop efficient and controlled sampling schemes as described in paper  ii , so that data can be used to determine both cosmological and modeling parameters ( self - calibration ) .    with the series of three coyote universe papers we have demonstrated that it is possible to extract cosmological statistics such as the power spectrum at high accuracy and that one can build an accurate prediction scheme based on a limited set of simulations .",
    "this line of work will be important for interpreting results of future cosmological surveys .",
    "it will also have to be extended in several ways : ( 1 ) the cosmological model space has to be opened up ; ( 2 ) we have to ensure high accuracy at scales smaller than those considered here ; ( 3 ) we have to include more physics in order to capture those small scales correctly ; ( 4 ) we have to include different cosmological probes , e.g. , the cluster mass function and the shear power spectrum , to be able to build a complete framework for analyzing future survey data . we have shown here that such a program can in principle be established , though it will demand a large concerted effort between different communities .",
    "a special acknowledgment is due to supercomputing time awarded to us under the lanl institutional computing initiative .",
    "part of this research was supported by the doe under contract w-7405-eng-36 and by a doe hep dark energy r&d award . s.h .",
    ", e.l . , and c.w .",
    "acknowledge support from the ldrd program at los alamos national laboratory .",
    "k.h . was supported in part by nasa .",
    "m.w . was supported in part by nasa and the doe .",
    "we would like to thank dragan huterer , nikhil padmanabhan , adrian pope , and michael schneider for useful discussions .",
    "we thank volker springel for making the @xmath0-body code gadget-2 publicly available ."
  ],
  "abstract_text": [
    "<S> many of the most exciting questions in astrophysics and cosmology , including the majority of observational probes of dark energy , rely on an understanding of the nonlinear regime of structure formation . in order to fully exploit the information available from this regime and to extract cosmological constraints , </S>",
    "<S> accurate theoretical predictions are needed . </S>",
    "<S> currently such predictions can only be obtained from costly , precision numerical simulations . </S>",
    "<S> this paper is the third in a series aimed at constructing an accurate calibration of the nonlinear mass power spectrum on mpc scales for a wide range of currently viable cosmological models , including dark energy . </S>",
    "<S> the first two papers addressed the numerical challenges , and the scheme by which an interpolator was built from a carefully chosen set of cosmological models . in this paper </S>",
    "<S> we introduce the `` coyote universe '' simulation suite which comprises nearly 1,000 @xmath0-body simulations at different force and mass resolutions , spanning 38 @xmath1cdm cosmologies . </S>",
    "<S> this large simulation suite enables us to construct a prediction scheme , or emulator , for the nonlinear matter power spectrum accurate at the percent level out to @xmath2 . </S>",
    "<S> we describe the construction of the emulator , explain the tests performed to ensure its accuracy , and discuss how the central ideas may be extended to a wider range of cosmological models and applications . </S>",
    "<S> a power spectrum emulator code is released publicly as part of this paper .    </S>",
    "<S> to 0pt to 0ptla - ur-09 - 06131 </S>"
  ]
}