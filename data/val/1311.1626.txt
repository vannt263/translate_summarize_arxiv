{
  "article_text": [
    "queries over graph databases can be classified broadly into whole graph at - a - time , and node at - a - time processing , and framed as a subgraph isomorph computation problem ( e.g. , @xcite ) under a set of label mapping constraints , generally known as graph matching .",
    "techniques such as graphql @xcite , quicksi @xcite and earlier research such as vflib @xcite and ullmann @xcite fall in the former category while tale @xcite , and sapper @xcite are representative of the latter .",
    "the advantage of the node at - a - time graph processing approach is its inherent ability to prune search space based on target node matching conditions .",
    "node indices are the most common pruning aid used in most of these processing methods although indices on paths @xcite , frequent structures @xcite , node distances @xcite , etc .",
    "have also been used .",
    "the key difference is in the ways indices are exploited for the construction of the target database graphs from their parts ( i.e. , the edges ) .",
    "the effectiveness of the node at - a - time methods , however , largely depends on the query type such as subgraph isomorphism , approximate matching , path queries and so on , as well as on the index structure used . in other words ,",
    "universal indexing methods are not always suitable for all queries , and therefore specialized indices are often constructed to process a query ( e.g. , graphgrep @xcite , tale , and sapper ) and never maintained .",
    "thus , it is not apparent if the index structure is switched , how an algorithm will perform leaving an open question if generic index structures can be leveraged in a way similar to relational query processing with popular indices such as b+ trees , extendible hashing and inverted files .    in order to decouple the index selection from the query expressions , and to subsequently use indices as a strategic instrument to compute alternative query plans , we focus on a representation method for graphs that is independent of the underlying access structures .",
    "our goal is to propose the  hub \" as the unit of graph representation that tells us all we need to know about a node or vertex of a graph .",
    "intuitively , each node in a graph as hub  covers \" all the edges involving its neighbors and itself .",
    "for example , the hub @xmath0 in figure [ fig : example](a ) covers the edges @xmath1 and @xmath2 as a unit structure ( shown as the purple edges ) .",
    "the concept of hub we have in mind can be thought of as a convenient extension of ullmann s adjacency matrix @xcite and feature structure indexing @xcite in that we localize the adjacency matrix at the node level and consider only a single feature , edges among the neighbors .",
    "consequently only structures that are part of a hub are stars ( neighbors with no shared edge with other neighbors ) and triangles ( neighbors sharing edge with other neighbors ) . in figure",
    "[ fig : example](a ) , vertex @xmath0 has two triangles @xmath3 and @xmath4 , and a star @xmath2 ( in this case just an edge ) . whereas vertices @xmath5 and @xmath6 in figure [ fig : example](b ) have a star ( with two edges @xmath7 and @xmath8 with @xmath5 as their center ) , and three triangles ( @xmath9 , @xmath10 , and @xmath11 ) respectively .",
    "our goal is to use these atomic structural cues to match shapes for the purpose of graph matching .",
    "for example , to match the query graph @xmath12 in figure [ fig :",
    "example](a ) with the data graph @xmath13 in figure [ fig : example](b ) , we look for individual node structures that are identically connected and depending on the matching requirement , have identical labels . the next step is to piece together these individual matches to see if the composed structure is the target graph .",
    "the cost of this search usually is dominated by the cost of piecing together the components and testing if the process is yielding the target graph .    in this example , we can contemplate several different types of graph matching that can be conceived as the variants of subgraph isomorphism though in the literature , only the structural isomorphs and match isomorphs defined below are prevalent .",
    "we therefore will consider only these two types of matching in the remainder of this paper .    *",
    "_ structural subgraph isomorph _ , where only the node ids ( not the labels ) are mapped from query graphs to data graphs using and injective function .",
    "* _ label subgraph isomorph _",
    ", on the other hand , requires an injective mapping of both node ids and node labels from query graph to data graph .",
    "* _ full subgraph isomorph",
    "_ extends label subgraph isomorphic matching to include edge labels in the mapping . *",
    "_ match subgraph isomorph _ uses an equality function on the definition of full subgraph isomorph to achieve exact matching of node and edge labels while maps node ids using an injective function .    among the above four modes of matching , structural subgraph isomorphism is the least restrictive or selective , and so most computationally expensive .",
    "while the match subgraph isomorphic matching ( in the literature it is known as labeled graph matching ) is the most restrcitive / selective , full and and label subgraph isomorphism are increasingly less so .",
    "the idea here is that by combining different selection and mapping constraints , called _ matching mode _ , we can capture most popular graph matching concepts and go beyond current definitions .",
    "traditional deep equality @xmath14 operator @xcite in object - oriented databases can be used to test if two graphs ( or a subgraph ) are equal ( or contained in the other graph ) by requiring that node ids and labels be identical . finally , by requiring that the two graphs have equal number of nodes",
    ", we can also achieve graph isomorphism for each case above .",
    "the vertex labeled graphs in figure [ fig : example ] show two query graphs @xmath12 and @xmath15 , and a data graph @xmath13 respectively . in these graphs ,",
    "each node has a unique node i d such as @xmath16 , @xmath17 and @xmath18 , and a label such as @xmath19 , @xmath20 and @xmath21 ( shown in uppercase with unique color codes ) .",
    "if all the labels are empty , the graph is considered unlabeled .",
    "if we matched query graph @xmath12 with the data graph @xmath13 for structural subgraph isomorphism , we will compute the green , brown and red matches , among others , as we are only required to map node ids .",
    "however , if we were to compute match subgraph isomorphs , only solution we can compute then is the green subgraph .",
    "if label subgraph isomorphs were being sought instead , the solution will include the brown subgraph where we map pink ( c ) to pink , green ( d ) to blue ( a ) , mustard ( b ) to mustard , and blue to green , along with the obvious green subgraph .",
    "let us illustrate this naive matching process using figures [ fig : example ] and [ fig : cand ] .    since there are six nodes in the graph @xmath12 , a total of six hubs are possible .",
    "similarly , the graph @xmath13 can be represented by a total of eleven hubs .",
    "let us first match @xmath12 with @xmath13 in match subgraph isomorphism mode . under this constraint , for hub @xmath0 ( shown clock - wise rotated by @xmath22 in purple in figure [ fig : cand](a ) )",
    ", we can only find one hub , hub @xmath23 shown in figure [ fig : cand](b ) , in @xmath13 which is a supergraph of hub @xmath0 and can produce a structure identical to @xmath0 ( shown as red edges ) on proper mapping of the node ids .",
    "these structures are called candidate graphlets . however ,",
    "if we choose to match in structural or label subgraph isomorphism mode , we can find more hubs as capable of generating candidates structures .",
    "for example , in label subgraph isomorphism mode , hubs @xmath24 and @xmath25 can also generate candidates ( possible candidate structures are shown in red , purple and green ) as shown in figures [ fig : cand](c ) and [ fig : cand](d ) .    to complete the matching , we can continue to match all the other hubs in @xmath12 in a similar way , and by applying the substitutions generated for each set of previous matches to the candidate hubs to eventually compute the green match as shown in figure [ fig : example](b ) .",
    "these graphlets can be joined or pieced together in both bottom - up fashion using a process similar to natural join , or in top - down manner using a depth - first search . in this algorithm @xmath26",
    "is the structure mapping function that generates the mapping , @xmath27 is the substitution list from previous steps , @xmath28 $ ] is the application of the substitution , and @xmath29 is the composition function of two substitutions . ] . in both cases , the graphlets that do not stitch to form the target graph will eventually be eliminated .",
    "clearly , the dominant cost in this naive algorithm is in candidate generation .",
    "therefore , it would be prudent to seek opportunities to curtail the candidates that do not have a realistic chance of contributing to the result , or will produce redundant candidates .",
    "for example , we can be smarter and choose to match @xmath30 or @xmath0 only without compromising the outcome .",
    "we can further speed up the process by noticing that @xmath30 is a green d node and there are four such nodes in @xmath13 , all of which will generate a total of twelve candidates even though only four will survive the node mapping , and only one will join with the first candidate to complete the computation . on the other hand ,",
    "if we decided to use @xmath31 , we will generate six candidates ( @xmath32 is not one of them ) of which only one will survive the mapping and eventually the response .",
    "furthermore , if we started initially with @xmath30 , and then try to map @xmath0 , the number of candidates generated will be even higher although the response computed will still be the same .",
    "interestingly though , if the query is @xmath15 ( note the similarity of the two queries except that @xmath33 is now purple ) , it is definitely better to start matching with @xmath33 , because there is only one candidate and it will fail to produce the response in the next step , as expected .    these observations",
    "lead us to devise the following query processing strategy .",
    "first , we reduce the number of hubs or graphlets in the query graph that we must match based on a new notion of edge covering in graph theory , called the _ minimum hub cover _ ( mhc ) .",
    "a minimum hub cover essentially means a subset of the nodes in a graph accounts for all the edges in a graph .",
    "secondly , the concept of mhc helps exploit available meta - data on nodes to order the nodes in priority order based on their selectivity to prune search space , that we call a _ query plan_. in ordering the nodes , we explore the nodes that will most likely produce the least number of candidates first . ] .",
    "given the fact that a query graph may have multiple mhcs , it also offers us the opportunity to choose the best query plan for a database instance . finally , we are now able to use access structures such as hash index and set index to find only the nodes that are relevant for expanding nodes at a given point in a query plan .",
    "in fact , the matching algorithm [ alg : find ] uses two such indices @xmath34 and @xmath35 . the query plan can be implemented as a top - down or bottom - up procedure based on the expected number of candidates and a choice can be made based on the expected cost .",
    "in particular , it is also possible to reorder the query plan in a top - down procedure to prune search space dynamically in a way similar to best - first search .",
    "covering based graph matching is proving to be an interesting and emerging research direction although we are aware of only sigma @xcite which used set covering directly for matching very small graphs , while @xcite indirectly used covering for graph matching tangentially .",
    "our goal in this paper , however , is to formally introduce the idea of graph representation using graphlets and graph query processing using the minimum hub cover of query graph graphlets .",
    "our focus is to convince the skeptics that these two concepts help achieve the separation in graph representation and storage , indexing , query plan generation , and query optimization conveniently .",
    "once this model is accepted in principle , two main computational problem emerge both of which are computationally hard  computing mhc and graph matching using subgraph isomorphism as the primary vehicle . in this paper",
    ", we only address the first issue , that is the computational aspects of mhc .",
    "but for the sake of completeness , we also briefly present an outline of the cost - based optimization strategy for the ordering of graphlets in the mhc as a candidate query plan , and a query processing algorithm that uses indices for the execution of a query plan . by doing so",
    ", we demonstrate that cost - based query optimization is feasible if we are able to compute the mhc of a query graph .",
    "finally , we believe that even if algorithms such as summa @xcite , nova @xcite , tale , and sapper do not use a notion similar to hubs as we do , they do use the notion of neighborhood and will benefit from the development presented in this paper if they consider a similar covering , i.e. , edge or vertex covers , of the queries .",
    "the results in this paper then becomes directly relevant to those research as well because we show how the cost of covering computation may vary depending on the type and parameters of the graphs being considered .",
    "the remainder of the paper is organized as follows .",
    "we discuss background of the research related to mhc in section [ sec : related ] . in this section",
    ", we also discuss related research in covering computation based on which we formulate our characterization of mhc .",
    "the formal treatment of mhc and its application in query plan generation is discussed in section [ sec : prel ] .",
    "similar to other covering problems such as set cover , and minimum vertex cover , mhc turns out to be an np - complete problem as well .",
    "therefore , it can be framed as an optimization problem and made a candidate for heuristic solutions . in section [ sec :",
    "mhc ] , we discuss an integer programming formulation of the mhc problem as a prelude to our main results on its computability .",
    "we have implemented the algorithm using the ibm ilog optimization engine cplex .",
    "the experimental results in section [ subsec : eval ] based on the design in section [ sec : comp ] suggest that solving mhc to optimality is not a concern for many graph types . a summary of interesting and possible future research issues that are still outstanding is discussed in section [ sec : fut ] .",
    "we finally conclude in section [ sec : conl ] .",
    "in our earlier research on isosearch @xcite , we have shown that the notion of _ structural unification _ helps to extract all possible matches of two hubs under a mapping function or a substitution list .",
    "while this atomic matching process generates a potentially large candidate pool , we were able to avoid the large cost related to testing for conformity of the candidate target structure with that of the query graph that most other algorithms incur . in our case",
    ", conformity is an eventuality and automatic if a match exists .",
    "we have also shown that isosearch performs significantly better than traditional algorithms such as ullmann and vflib , have a significantly low memory footprint , and is able to handle arbitrary sized query and data graphs ( because we handle only pairs of graphlets at a time ) .",
    "the concepts of hubs and minimum hub covers also help model various definitions of exact graph matching along the lines of @xcite , as well as approximate graph matching in the spirit of tale .    tangentially to this research , in our recent top-@xmath36 graph matching algorithm tram @xcite , we have explored the idea of hub matching as a unit of comparison and computed structural distance of attributed hubs without the need for explicit use of indices . in this approach , we have developed a quantification for a hub s structural feature as a random walk score @xcite .",
    "since random walk scores encompass the global topological properties of a node as a hub , from the standpoint of graph matching , it can be used to compare topological orientation and relative importance of graph nodes .",
    "these scores thus effectively capture the topological likeness and structural cues shared among the hubs , and were effectively exploited for approximate graph matching in tram .",
    "a similar method was used in @xcite to compute computation of similarities of nodes of a graph for collaborative recommendation .",
    "the random walk based approach , however , does not offer much opportunity for cost - based query optimization based on the evolving states of the database extension in ways similar to @xcite because it is largely similar in nature to many algorithmic counterparts such as summa , nova , tale , and sapper .    as we shall elaborate in the subsequent part of this section , the mhc problem is closely related to two well - known combinatorial optimization problems , the _ set covering problem _ ( scp ) and the _ minimum vertex cover _ ( mvc ) problem , which are , in turn , share a similar mathematical programming model .",
    "we next discuss the relationship between the mhc problem and the mvc problem , and then tie this discussion to a general set covering formulation that we also adopt in this work .",
    "let us start with the integer programming ( ip ) model for the mvc problem : @xmath37 where @xmath38 is a binary variable that is equal to 1 , if vertex @xmath39 is in the cover and 0 , otherwise .",
    "the objective function evaluates the total number of vertices in the cover .",
    "constraints ensure that every edge is covered by at least one vertex , and constraints enforce binary restrictions on the variables . to give a concrete example ,",
    "suppose that we are trying to find the optimal mvc in the graph shown in figure [ fig : example](a ) . the corresponding ip model is then given by    @xmath40\\nonumber\\\\                        x_1 & { }    & { }      & { }    & { }      & { }    & { }      & { } + & { } x_5 & { }    & { }      & { } \\geq{1 } , \\nonumber \\ \\ \\ [ ( u_1 , u_5)]\\\\                            & { }    & { } x_2 & { } + & { } x_3 & { }    & { }      & { }    & { }      & { }    & { }      & { } \\geq{1 } , \\nonumber \\ \\ \\ [ ( u_2 , u_3)]\\\\                            & { }    & { } x_2 & { }    & { }      & { }    & { }      & { } + & { } x_5 & { }    & { }      & { } \\geq{1 } , \\nonumber \\ \\ \\ [ ( u_2 , u_5)]\\\\                            & { }    & { }      & { }    & { } x_3 & { } + & { } x_4 & { }    & { }      & { }    & { }      & { } \\geq{1 } , \\nonumber \\ \\ \\ [ ( u_3 , u_4)]\\\\                            & { }    & { }      & { }    & { } x_3 & { }    & { }      & { } + & { } x_5 & { }    & { }      & { } \\geq{1 } , \\nonumber \\ \\ \\ [ ( u_3 , u_5)]\\\\                            & { }    & { }      & { }    & { }      & { }    & { }      & { }    & { } x_5 & { } + & { } x_6 & { } \\geq{1 } , \\nonumber \\ \\ \\ [ ( u_5 , u_6)]\\\\                        x_1 & { } , & { } x_2 & { } , & { } x_3 & { } , & { } x_4 & { } , & { } x_5 & { } , & { } x_6 & { } \\in\\{0,1\\}. \\nonumber\\end{aligned}\\ ] ]    the model minimizes the total number of selected vertices while satisfying the coverage constraints written for each edge . for the sake of clarity , the edge corresponding to a constraint is designated at the end of each line within the brackets .",
    "the first constraint , for example , implies that the edge @xmath41 can be covered by vertices @xmath42 and @xmath43 . since an edge can be covered only by the vertices incident to it , each constraint in the ip formulation of the mvc problem",
    "involves exactly two variables . in this particular example",
    ", @xmath44 is the unique optimal solution .",
    "when it comes to the mathematical programming model of the mhc problem , we need to pay attention to the fact that a vertex ( as a hub ) covers not only the edges incident to itself but also those edges between its immediate neighbors . using this fact",
    ", we obtain the following ip formulation of the mhc problem for the graph in figure [ fig : example](a ) : @xmath45 notice that unlike the ip formulation of the mvc problem , the number of constraints reduces since multiple edges can be covered by the same set of vertices .",
    "for instance , the second constraint shows that vertices @xmath43 , @xmath31 and @xmath0 cover the edges @xmath46 , and @xmath47 . as a consequence of this hub property , the number of variables appearing in a constraint is greater than or equal to two .",
    "in fact , this number can easily go up to the number of vertices , because the vertices incident to an edge may be connected to all other vertices forming an abundant number of triangles .",
    "clearly , the cardinality of the mhc can be far less than that of the cardinality of the mvc due to the additional non - incident edges covered by those vertices in a triangle .",
    "therefore , for triangle - free graphs , the optimal solutions for the mhc problem and the mvc problem naturally coincide .",
    "the above formulations of the mvc and mhc problems actually show that both problems are just the special cases of the scp .",
    "given a fixed number of items and a family of sets collectively including ( covering ) all these items , the objective of the scp is to select the least number of sets ( minimum cardinality collection ) such that each item is in at least one of these selected sets . if an edge corresponds to an item and a set is formed with the edges that can be covered by each vertex , then the connection between the scp and the mhc problem as well as the mvc problem can easily be established . to formalize this discussion , below",
    "we give the generic ip formulation for the mhc problem : @xmath48 again , the binary variable @xmath38 is equal to 1 , if vertex @xmath39 is in the hub cover , and the objective is to minimize the number of vertices used in the cover .",
    "constraints ensure that every edge is covered by at least one hub node in the cover . finally , the constraints enforce the binary restrictions on the variables .",
    "although the number of constraints seems equal to the number of edges , we remind that multiple edges can be covered by the same set of vertices ( see mhc example above ) .",
    "the introduction of the hub cover concept to the literature is quite recent @xcite .",
    "thus , to the best of our knowledge , the solution methods for the mhc problem have not been examined in the literature .",
    "however , closely related problems , the mvc problem and the scp , have been extensively studied before .",
    "take the mvc problem ; approximation algorithms @xcite , heuristic solutions @xcite , evolutionary algorithms @xcite can be listed among those numerous solution methods .",
    "the scp is no different . from approximation algorithms",
    "@xcite that have good empirical performances to randomized greedy algorithms @xcite , and from local search heuristics @xcite to different meta - heuristics @xcite have been proposed for solving the scp .",
    "as mentioned earlier , in graph @xmath12 , node or hub @xmath0 covers the edges @xmath49 , @xmath50 , @xmath51 , @xmath52 , @xmath47 and @xmath2 .",
    "similarly , the hub @xmath31 covers edges @xmath53 , @xmath54 , @xmath47 and @xmath55 .",
    "since the set of hubs @xmath56 covers all the edges of @xmath12 , this set is called a _",
    "hub cover _ of @xmath12 , denoted @xmath57 , and so are the sets @xmath58 , @xmath59 , and @xmath60 .",
    "however , for the purposes of graph matching , it is sufficient if we matched the set @xmath56 with another graph as any super set this will not add any more new node or edge matching .",
    "we thus call this set the _ minimum hub cover _ of @xmath12 , denoted @xmath61 and formalized in the following definition .    _ for a given graph @xmath62 , @xmath63 is a minimum hub cover of @xmath64 , denoted @xmath65 , if @xmath66 is the smallest set , and for every edge @xmath67 , either @xmath68 or @xmath69 , or there exists edges @xmath70 and @xmath71 , and @xmath72 .",
    "the set of all @xmath65 is denoted as @xmath73 .",
    "_    it should be apparent that given a graph @xmath74 , the number of hub nodes in @xmath66 lies within the range @xmath75 .",
    "for example , both @xmath56 and @xmath76 are in @xmath77 , and @xmath78 .",
    "it should also be apparent that hubs in a given mhc ensures edge connectivity because all edges are covered .",
    "but it is likely that a given sequence of hubs may not retain edge connectivity , and thus ordering of the elements in @xmath65 matters and as such this ordering relationship is a key to processing queries efficiently . intuitively , the query optimization approach we present essentially is the problem of computing the set @xmath79 when computable , then ordering the elements in each set @xmath80 using a cost function @xmath81 ( discussed in detail in section [ sec : cost ] ) such that @xmath82 is minimized , and then choose mhc @xmath83 such that @xmath84 is the minimum among all mhcs in @xmath79 under @xmath81 .",
    "the least cost ordering relationship implied by @xmath84 is then taken as the query plan to be executed using a depth - first matching algorithm .",
    "traditionally , graphs are represented as a pair @xmath85 where @xmath86 is a set of vertices and @xmath87 is a set of edges over @xmath86 .",
    "such a representation does not carry any structural information of which vertices are a part of , and they are not visible until structures are constructed from the set of edges . to ease computational hurdles and aid analysis ,",
    "some models have used vertices and their neighbors as a unit of representation @xcite , i.e. , @xmath88 where @xmath89 is vertex in @xmath86 , and @xmath90 is a set of neighbors such that @xmath91 .",
    "a graph is then modeled as a set of such units .",
    "while this representation captures some structural cues , it still is pretty basic .",
    "the decision on the degree of structural information that can be captured and exploited is very hard . in one extreme",
    ", we have the traditional representation @xmath92 with no structural information other than the edges , and the graphql model where pretty much the entire graph structure is represented as a single xml document .",
    "while graphql offers most selectivity due to its representation , taking advantage of this in cost - based query optimization is extremely difficult because meta - data now has to be at the largest structural level .",
    "the models that are in between are the star structures @xcite and dynamically mined frequent feature structures @xcite which we believe can generally be difficult to index and maintained with the evolution of the database .",
    "we believe representing a graph as a set of hubs is a prudent compromise because it assures a deterministic model , and yet offers a realistic chance of efficient storage and processing of graph queries .",
    "it is deterministic because each hub can be represented as a triple of the form @xmath93 , called a _ graphlet _",
    ", where @xmath94 is a node in graph @xmath95 , and @xmath96 , and @xmath97 such that for each @xmath94 all @xmath98 are its immediate neighbors , and every edge @xmath99 are edges involving neighbors in @xmath100 . for unlabeled and undirected graphs ,",
    "this representation model is sufficient .",
    "but for labeled and directed graphs , this simple model can be extended without any structural overhaul where @xmath101 additionally represents the node label .",
    "the hubs in a fully labeled graphs can be modeled as yet another extension as @xmath102 where @xmath103 are a set of pairs @xmath104 and @xmath105 are triples @xmath106 such that @xmath107 and @xmath108 are edge labels for edges between the hub and the neighbors , and among the neighbors respectively .",
    "the directionality of the edges can be captured by partitioning the sets @xmath100 and @xmath109 to imply directions .",
    "for example , the expression @xmath110 means that ( i ) there are edges from @xmath89 to every node in @xmath111 , and from @xmath112 to @xmath89 , and ( ii ) for each edge @xmath113 in @xmath114 , the sink node is @xmath115 , and for edges in @xmath116 , it is reversed . ] .",
    "we can now simply define a graph as a set of graphlets , i.e. , @xmath117 .",
    "in fact , in nyql , we have shown that such a representation can be conveniently captured using traditional storage structures such as nested relations or xml documents .      given two graphlets @xmath119 and @xmath120 , and a graph @xmath121 , we define an ordering relation @xmath122 , called the _ selectivity ordering _ , between them to mean @xmath119 precedes @xmath120 ( or @xmath119 costs less than @xmath120 in processing time ) in two principal ways .",
    "* first , when the selectivity of @xmath119 , denoted @xmath123 , is higher than @xmath124 .",
    "technically , @xmath125 means the number of graphlet instances @xmath126 in graph @xmath64 that satisfy the condition @xmath127 and @xmath128 , and selectivity of @xmath119 is higher than that of @xmath120 in a graph @xmath64 if @xmath129 .",
    "we call it _ node selectivity _ , denoted @xmath130 . *",
    "second , when the selectivity of @xmath100 and @xmath109 , called the _ structural selectivity _ and denoted @xmath131 , is higher than the selectivity of @xmath132 and @xmath133 , denoted respectively as @xmath134 and @xmath135 , i.e. , @xmath136 , where @xmath137 is the number of graphlet instances @xmath126 in graph @xmath64 that satisfy the condition @xmath138 and @xmath128 . * finally ,",
    "the _ neighbor selectivity _ ( denoted @xmath139 ) @xmath140 holds where @xmath141 represents the number of graphlet instances @xmath126 in graph @xmath64 that satisfy the condition @xmath138 .",
    "the query plan as the total ordering @xmath118 then can be induced from the precedence relationship @xmath142 of graphlets in @xmath64 using the procedure * graphlet ordering * in algorithm [ alg : order ] .",
    "algorithm [ alg : order ] is intuitively explained in figure [ g - exmp2 ] where we show that the dot nodes are identified first followed by the star nodes and ordered in terms of their selectivity . in figure",
    "[ top ] , the three components of a graphlet are shown in brown , green and yellow , where the neighbors @xmath100 are partitioned in to two sets @xmath143 $ ] and unmapped subsets ( shown in uppercase ) . since an instantiated dot node",
    "may only fetch a single hub at the most , this ordering is likely to result in a least cost plan .",
    "set @xmath144 .",
    "+ choose the most selective graphlet as @xmath145 using node selectivity .",
    "+      let us also assume that we have a hash index @xmath34 that given a node i d ( i.e. , @xmath94 ) , returns the graphlet corresponding to @xmath89 , i.e. , @xmath146 .",
    "furthermore , we also have another index @xmath35 similar to @xcite such that it returns a set of graphlets @xmath147 given a set of nodes @xmath148 , and an integer @xmath149 , i.e. , @xmath150 is now a set of graphlets , or basically a set of nested tuples . ] , such that for each @xmath151 , @xmath152 , and @xmath153 . in other words",
    ", @xmath35 returns all graphlets that contain the set of nodes in @xmath148 and have a minimum of @xmath154 neighbors .",
    "the graph query processor for exact matching is simple .",
    "it is composed of a depth - first search algorithm * find solutions * that maps a sequence of graphlets in a query graph presented in @xmath118 order to a set of graphlets of a data graph using a composable term mapping function @xmath26 .",
    "algorithm [ alg : find ] is assumed to have access to a data graph @xmath121 , one of four graph matching modes , and the indices @xmath34 and @xmath35 .",
    "let @xmath155 . + apply substitution @xmath27 to @xmath119 , i.e. , @xmath156 $ ] .",
    "+      we formally present the computational characterization of mhc in terms of its complexity class , and using the decision version of the mhc problem ( mhc - d ) show that the mhc problem is indeed difficult and that it is np - complete .",
    "mhc - d is np - complete .    to show that mhc - d is np - complete , we answer the following question : given a graph @xmath157 and an integer @xmath158 ,",
    "does there exist a subset @xmath159 with @xmath160 such that for every edge @xmath161 , @xmath162 or @xmath163 or there exists a vertex @xmath164 such that both @xmath165 and @xmath166 ?",
    "first , we argue that mhc - d is in np because for a given yes - instance and @xmath167 with @xmath168 , we can verify in polynomial time that every edge in @xmath87 is covered by @xmath169 .",
    "we now complete the proof by a reduction from the mvc problem on triangle free graphs @xcite . in triangle - free graphs , for all @xmath161 there does not exist a vertex @xmath170 such that @xmath165 and @xmath166 .",
    "thus , either @xmath171 , or @xmath13 must be in @xmath169 to cover the edge @xmath172 .",
    "consequently , the mhc and the mvc of a graph are equivalent in this class of graphs and mvc is np - complete for this class of graphs @xcite .",
    "the np - completeness of mhc speaks to the hardness of the solvability in general .",
    "the optimization strategy and query plan generation presented in section [ sec : prel ] , however , calls for the computation of all mhcs of a given query @xmath173 , i.e. , @xmath79 , which is indeed a hard problem . in this section ,",
    "our goal is to design a set of experiments using different graph types , size and graph density parameters to study how the solvability and the quality of mhc solutions depend on these parameters .",
    "the goal is to experimentally identify problem classes for which available solutions are practical and acceptable , and the classes for which new heuristic solutions are warranted .",
    "we therefore employ different algorithms to show the trade - offs between optimality and computation time over different graph types .",
    "although our experiments and analysis involve producing only one optimal solution for a graph , the insights gained can be leveraged to develop new algorithms to compute @xmath79 or to choose other strategies when computing @xmath79 is infeasible .",
    "the optimal linear programming ( lp ) and ip solutions are obtained by ilog ibm cplex 12.4 on a personal computer with an intel core 2 dual processor and 3.25 gb of ram . in all problem instances , the upper limit on the computation is set at 3,600 seconds .",
    "the batch processing of the instances is carried out through simple c++ scripts .",
    "our data set includes a total of 830 instances .",
    "we have 5 different instances for each combination of a graph type , size , and density parameter to be able to draw conclusions .",
    "we have chosen to use the benchmark database graph instances in @xcite and our own synthetically generated data set for our numerical study .",
    "this is a very large database of different graph types and sizes designed specifically to test the sophistication of ( sub)graph isomorphism algorithms . since we are using subgraph isomorphism as a basic vehicle for graph matching , the instances selected are thus representative of the class of queries we are likely to handle when we solve the mhc problem .",
    "the descriptions of the graph instances we have chosen from this collection are listed below .",
    "[ [ randomly - connected - graphs ] ] randomly connected graphs + + + + + + + + + + + + + + + + + + + + + + + + +    these graphs have no special structure and the number of vertices range from 20 to 1000 ( @xmath174= 20 , 60 , 100 , 200 , 600 , 1000 ) . the parameter @xmath175 denotes the probability of having an edge between any pair of vertices .",
    "thus , this parameter , in a sense , specifies the sparsity of a graph . in the database , three different values of @xmath175 ( 0.01 , 0.05 , and 0.10 )",
    "are considered .",
    "our data set includes a set of graphs of different sizes for each value of @xmath175 .",
    "[ [ bounded - valence - graphs ] ] bounded valence graphs + + + + + + + + + + + + + + + + + + + + + +    the vertices of the graphs in this class have the same degree ( _ fixed valence _ ) .",
    "the sizes of the instances are similar to those of the problem class ( a ) .",
    "we use three different values of valence  3 , 6 and 9 to obtain graphs of different size and valence .",
    "[ [ irregular - bounded - valence - graphs ] ] irregular bounded valence graphs + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    these graphs are generated by introducing irregularities in the problem class ( b ) .",
    "irregularity comes from randomly deleting edges and adding them elsewhere in the graph . with this modification ,",
    "the average degree is again bounded but some of the vertices may have higher degrees .",
    "the sizes of the instances are similar to those of problem classes ( a ) and ( b ) .",
    "[ [ regular - meshes - with-2d-3d - and-4d ] ] regular meshes with 2d , 3d , and 4d + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in graphs with 2d , 3d , and 4d meshes , each vertex has connections with 4 , 6 , and 8 neighbors , and the numbers of vertices range from 16 to 1024 , 27 to 1000 , and 16 to 1296 , respectively .",
    "similar to the problem classes ( a ) ,",
    "( b ) , and ( c ) , we have a set of graphs for each combination of size and dimension .    [",
    "[ irregular - meshes ] ] irregular meshes + + + + + + + + + + + + + + + +    as in class ( c ) , irregular meshes are generated by introducing small irregularities to the regular meshes .",
    "irregularity comes from the addition of a certain number of edges to the graph . the number of edges added to the graph is @xmath176 scale - free graphs + + + + + + + + + + + + + + + + +    this problem class includes the graphs that follow a power - law distribution of the form @xmath177 where @xmath178 is the probability that a randomly selected vertex has exactly @xmath36 edges , @xmath170 is the normalization constant , and @xmath179 is a fixed parameter .",
    "we employed the scale - free graph generator of c++ boost graph library . the generator ( power law out degree algorithm )",
    "takes three inputs .",
    "these are the number or vertices , @xmath180 and @xmath181 . increasing the value of @xmath181 increases the average degree of vertices . on the other hand , increasing the value of @xmath180 decreases the probability of observing vertices with high degrees .",
    "the sizes of the instances range from 20 to 1000 ; @xmath182 to be precise .",
    "we considered two values for @xmath183 and three values of @xmath184 .",
    "graphs in social networks , protein - protein interaction networks , and computer networks are examples of this class .",
    "we choose three solution methods to compute mhc ",
    "( i ) an exact method to solve the problem to optimality , ( ii ) adapt two approximation algorithms from the vertex cover literature capable of computing feasible solutions fast , and ( iii ) a mathematical programming - based heuristic originally proposed for solving the scp .",
    "the ip formulation - is solved by an off - the - shelf solver to optimality .",
    "since the mhc problem is shown to be np - hard , this approach may have practical value only for small - to - medium - scale graphs .",
    "however , it sets a definitive benchmark for comparing the performances of various heuristics .",
    "we implemented two different approximation algorithms .",
    "first algorithm selects the vertex with the highest degree at each iteration .",
    "the aim is to cover as many edges as possible .",
    "next , all covered edges as well as the vertices in the cover are removed from the graph .",
    "the algorithm ends when there is no uncovered edge in the graph .",
    "the algorithm is called the _ @xmath185-approximation algorithm _ ( gr1 ) for the mvc problem . here",
    ", @xmath186 is the maximum degree in the graph , and @xmath185 is evaluated by @xmath187 the second algorithm ( gr2 ) , the _",
    "2-approximation algorithm _ , is an adaptation of @xcite originally proposed for computing a near - optimal solution for the mvc problem .",
    "unlike the previous algorithm , it selects an edge arbitrarily , then both vertices incident to that edge are added to the cover .",
    "yelbay et al .",
    "@xcite propose a heuristic ( mbh ) that uses the dual information obtained from the lp relaxation of the ip model of scp .",
    "they show the efficacy of the heuristic on a large set of scp instances . in their work ,",
    "the dual information is used to identify the most promising columns and then form a restricted problem with those columns .",
    "then , an integer feasible solution is found by one of the two approaches . in the first approach ( mbh ) , the exact ip optimal solution is obtained by solving _ the restricted problem_. in the second approach , a metaraps @xcite local search heuristic ( lslp ) is applied over those promising columns .",
    "we use both of these approaches .",
    "we focus on analyzing and understanding the mhc solution methods in section [ subsec : methods ] on the instances in section [ subsec : probclass ] in three different axes : ( i ) optimal solvability of mhc , ( ii ) quality of the solutions , and ( iii ) computational cost of optimal solution .",
    "these analyses are aimed at understanding which problem classes are inherently more difficult relative to others so that depending on the application and query , a suitable algorithm can be selected to compute mhc .",
    "we also discuss the factors that increase the complexity of the problems .",
    "figure [ fig : opt ] shows how the optimal solution time of cplex , an exact method , varies depending on the problem size , class , and structure .",
    "the @xmath188-axis and the @xmath189-axis represent the number of vertices and the average computation time , respectively .",
    "the right - most data point on a line shows the size of the largest instance that can be solved to optimality in a group . in general , its performance is good for small to medium scale graphs .",
    "however , in our study , 39 out of 90 , 73 out of 285 and 39 out of 180 instances in problem classes ( a ) , ( e ) and ( f ) , respectively , could not be solved optimally using cplex within the time limit .",
    "this observation opens the door for heuristics to find acceptable but possibly suboptimal solutions .",
    "[ [ random - graphs ] ] random graphs + + + + + + + + + + + + +    from figure [ fig : randopt ] we conclude that for randomly connected graphs with more than 200 nodes , optimal solution is not achievable within the bounded time .",
    "it also suggests that the density of graphs is a factor that affects the solvability .",
    "the solver does increasingly better as the density @xmath175 goes down ( up to 0.01 ) for the same number of vertices .",
    "its sensitivity with respect to the size and density is apparent in the plots for @xmath175 equal to 0.05 and 0.10 , i.e. , a 16 fold increase in solution time .",
    "[ [ bounded - valance - graphs ] ] bounded valance graphs + + + + + + + + + + + + + + + + + + + + + +    compared to random graphs , figure [ fig : boundopt ] shows an improved performance on bounded valence graphs solving all instances under 0.3 seconds .",
    "the reason for the performance difference may be due to the considerably higher number of edges in a randomly connected graph ( which forces the number constraints in the ip model to go higher ) than that of a bounded valence graph . however , although we expect higher solution time for graphs with larger valence , figure [ fig : boundopt ] shows substantially higher time for valence 3 than valences 6 and 9 suggesting other factors may also be playing a role .    [ [ irregular - bounded - valance - graphs ] ] irregular bounded valance graphs + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    although the degree distribution is neither constant nor fully randomly distributed , cplex performs similarly to bounded valence graphs .",
    "as figure [ fig : irboundopt ] shows , all solutions are computed in less than .25 seconds , and that the computation time increases with the increase in valence .",
    "[ [ regular - mesh - graphs ] ] regular mesh graphs + + + + + + + + + + + + + + + + + + +    figure [ fig : meshopt ] shows that the size of meshes ( 2d , 3d or 4d ) usually does not have any influence on the performance barring the abrupt behavior of the 4d mesh graph . in general , the solution time appears to linearly increase with the increase in graph size , though the increase in time is extremely small .",
    "[ [ irregular - mesh - graphs ] ] irregular mesh graphs + + + + + + + + + + + + + + + + + + + + +    unlike the irregular bounded valence graphs , mesh graphs are more susceptible to irregularity and the computation time substantially increases with the degree of irregularity .",
    "figures [ fig : irmeshopt2 ] through [ fig : irmeshopt4 ] show that the sizes of the problems that can be solved to optimality decrease and the computation times increase with increasing degree of irregularity .",
    "this result is quite reasonable and expected because increasing irregularity increases the number of edges , and thus the computation time as well .",
    "this is also because randomly adding edges to a mesh graph makes it structurally more similar to random graphs , which , as discussed earlier , is inherently hard to solve .",
    "[ [ scale - free - graphs-1 ] ] scale - free graphs + + + + + + + + + + + + + + + + +    we consider the effect of the two parameters @xmath180 and @xmath181 on the solvability of the problems .",
    "on one hand , increasing @xmath180 makes the degree distribution sharper , i.e , we observe smaller number of vertices with high degrees . on the other hand , increasing the value of @xmath181 increases the degrees of non - hub nodes .",
    "figures [ fig : scalefree1 ] and [ fig : scalefree2 ] represent the optimal solution times of scale - free instances .",
    "it is clear that the difficulty of the problem is closely related to parameters @xmath180 and @xmath181 .",
    "the figures show that computation times decrease significantly with increasing values of @xmath180 . when @xmath190 , the instances with more than 100 vertices can not be solved to optimality within the time limit .",
    "when @xmath191 , however , all of the instances can be solved optimally in less than 0.06 seconds .",
    "these figures also show that the computation time increases with increasing values of @xmath181 .",
    "this means that increasing degrees of non - hub nodes makes the problem more difficult .",
    "to study the quality of solutions generated by other solution methods with respect to the optimal solutions computed using cplex , we refer to figures [ fig : perfrand ] through [ fig : perfirmesh ] .",
    "these plots are called performance profiles of algorithms that depict the fraction of problems for which the algorithm is within a factor of the best solution @xcite .",
    "thus , they compare the performance of an algorithm @xmath171 on an instance @xmath192 with the best performance observed by any other algorithm on the same instance .",
    "the x - axis represents the _ performance ratio _ given by @xmath193 where @xmath194 is the number of hub nodes in the hub cover when the instance @xmath192 is solved by algorithm @xmath171 and @xmath195 is the set of all benchmark algorithms .",
    "the y - axis shows the percentage of the instances that gives a solution that is less than or equal to @xmath196 times the best solution .",
    "recall that cplex can not solve all of the instances in problem classes ( a ) , ( e ) and ( f ) to optimality .",
    "however , the solver is able to find feasible solutions for some of those unsolved instances ( 11 out of 39 , 44 out of 73 , and 24 out of 39 in ( a ) , ( e ) , and ( f ) respectively ) .",
    "figure [ fig : perfpro ] includes all instances except those for which cplex can not find either feasible or optimal solutions within the time limit .",
    "we first analyze how much we sacrifice from the optimality by employing mathematical programming - based heuristics mbh and lslp . recall that mbh and lslp solve the same restricted problem . while mbh tries to solve the problem to optimality , the latter visits alternate solutions in the feasible region .",
    "figure [ fig : perfrand ] shows that 12% of the instances in class ( a ) where both mbh and lslp find better feasible solutions than that of cplex .",
    "note that , this can happen if and only if cplex returns a feasible solution rather than an optimal solution within the time limit . for other problem classes ,",
    "the performances of the cplex and mbh are quite similar .",
    "moreover , these figures show that lslp is outperformed by mbh and cplex on almost 40% and 30% of instances in problem classes ( b ) and ( c ) respectively . for the remaining problem classes ,",
    "the performance of lslp is also comparable to mbh and cplex .    the greedy algorithms return feasible but sub - optimal solutions quickly . except the scale - free networks ,",
    "the performances of the greedy algorithms do not change with respect to problem classes .",
    "gr2 is known as 2-approximation algorithm for mvc .",
    "figures [ fig : perfirbound ] and [ fig : perfscale ] show that there are some instances for which performance ratios of gr2 are higher than 2 .",
    "obviously , the approximation ratio of gr1 for mhc problem is higher than 2 . intuitively , the performance of gr1 is supposed to be better when the degree distribution of the vertices is not uniform .",
    "since the average degrees of the vertices are identical or are quite similar for the instances in problem classes ( a)-(e ) , the performance of gr1 does not vary for these problem classes .",
    "however , figure [ fig : perfscale ] shows that gr1 finds the optimal or the best solution in 30% of the scale - free instances .",
    "this means that the performance of the gr1 is better for the graphs that follow the power - law distribution .      the previous two analyses focused on the optimal solvability and the quality of the solutions .",
    "in this section we turn our attention to the cost of computing a feasible or optimal mhc solutions in terms of time .",
    "figures [ fig : timerand ] through [ fig : timescale ] summarize the distribution of the average computation times of the algorithms over the problem classes . in these plots , the instances for which feasible solutions were not found by any of the algorithm within a time limit are excluded .",
    "each bar in the figure represents the percentage of the instances that are solved within the time interval stated in the legend , e.g. , the blue bar for 0.0 to 0.05 seconds . since lslp is a local search algorithm , we show both the total computation time and the first time when the best solution is found .",
    "the results clearly show that the solution times of greedy algorithms ( gr1 , gr2 ) are much shorter than that of the other algorithms .",
    "mbh can solve the restricted problem to optimality in a reasonable amount of time for a great majority of the instances .",
    "we have already discussed earlier that the performance of the mbh is good in terms of its solution quality .",
    "however , the main drawback for mbh is its inability to solve the restricted problem to optimality . in such cases",
    ", lslp may serve as an alternative to mbh as it is comparable to mbh in terms of both solution quality and time , and because lslp is a local search algorithm , it is also guaranteed to produce a feasible solution .",
    "however , the performance of lslp is dependent upon prudent selection of algorithmic parameters , e.g. , the total number of iterations , the number of improvement iterations ( see @xcite for details ) .",
    "there is a trade - off between solution time and the solution quality .",
    "decreasing the total number of iterations may result in a decrease in the total solution time .",
    "however , it may increase the optimality gap .",
    "constraint solvers such as cplex usually do not offer all optimal solutions .",
    "such solutions also do not exploit database meta - data in computing the most desired solution .",
    "for example , for the graph in figure [ fig : example](c ) , there is no guarantee that the solver will produce the desirable solution @xmath197 when the data graph is known to be @xmath13 .",
    "therefore , we are required to compute all possible mhcs of @xmath15 , i.e. , @xmath198 , so that we are able to identify this least cost query plan . unfortunately , it is not guaranteed that solvers can even always compute a solution , let alone the whole family of solutions @xmath79 of @xmath173 .",
    "the discussion in section [ subsec : eval ] also suggests that although the cost of computing mhc is significantly low for small graphs , it still remains high for many graph types when the query graph is large .",
    "therefore , even though some of the existing solvers may be useful for applications involving small scale - free graphs such as protein - protein interaction networks , they may not be a great candidate for big data applications in social networks and world wide web .",
    "though it may be challenging , we believe the low mhc solution time for many graph types offers hope that designing algorithms for @xmath79 is feasible for most practical applications , but remains as an interesting problem .",
    "it is also important to recognize that while developing the least cost mhc using meta - data may be feasible for a single data graph , devising such algorithm for a large set of data graphs may not be feasible .",
    "it is thus worth investigating if a general but a single optimal query plan ( without computing @xmath79 ) , for which we have a solution , can be dynamically adjusted for best performance over a set of graphs .",
    "finally , it remains an open question if a suboptimal mhc produced by a greedy algorithm can be improved enough to defeat or match the overall processing performance using an optimal mhc solution , i.e. , total cost of mhc , plan generation , plan selection and execution .",
    "in this paper we have formally introduced the idea of graphlets as a basic unit for graph representation in a way similar to rdf triple store , and the concept of minimum hub cover of graphlets as a basic ingredient toward graph query optimization .",
    "we have demonstrated on intuitive grounds that such an approach can leverage generic access structures such as hash @xcite and set indices @xcite for query optimization .",
    "though computationally hard , we have also demonstrated that query processing and optimization using mhc and subgraph isomorphism is computationally feasible and intellectually intriguing .",
    "in particular , we have shown that for many application domains of current interest such as social networks , and protein - protein interaction networks , existing constraint solvers are capable of delivering optimal solutions for mhc , and therefore can be used to develop optimization strategies .",
    "it is our thesis that covering based graph processing we have presented opens up new research directions and holds enormous promise .",
    "the logical next step is to develop a query processor by integrating the algorithms in sections [ sec : cost ] and [ sec : qp - alg ] , with new algorithms outlined in section [ sec : fut ] .",
    "these are some of the tasks we plan to continue as our future research .",
    "this study was supported partially by tubitak 2214 ph.d .",
    "research scholarship program .",
    "m.  mongiov , r.  d. natale , r.  giugno , a.  pulvirenti , a.  ferro , and r.  sharan , `` sigma : a set - cover - based inexact graph matching algorithm , '' _ j. bioin . and comp . bio .",
    "_ , vol .  8 , no .  2 , pp .",
    "199218 , 2010 .",
    "f.  fouss , a.  pirotte , j .-",
    "m . renders , and m.  saerens , `` random - walk computation of similarities between nodes of a graph with application to collaborative recommendation , '' _ tkde _ , vol .",
    "19 , no .  3 , pp .",
    "355369 , 2007 .",
    "f.  gomes , c.  meneses , p.  pardalos , and g.  viana , `` experimental analysis of approximation algorithms for the veretx cover and set covering problems , '' _ computers and operations research _ , vol .",
    "35203534 , 2006 .",
    "m.  haouari and j.  s. chaouachi , `` a probabilistic greedy search algorithm for combinatorial optimization with application to the set covering problem , '' _ journal of the operational research society _ ,",
    "792799 , 2002 ."
  ],
  "abstract_text": [
    "<S> as techniques for graph query processing mature , the need for optimization is increasingly becoming an imperative . </S>",
    "<S> indices are one of the key ingredients toward efficient query processing strategies via cost - based optimization . due to the apparent absence of a common representation model , it is difficult to make a focused effort toward developing access structures , metrics to evaluate query costs , and choose alternatives . in this context , </S>",
    "<S> recent interests in covering - based graph matching appears to be a promising direction of research . in this paper </S>",
    "<S> , our goal is to formally introduce a new graph representation model , called _ minimum hub cover _ , and demonstrate that this representation offers interesting strategic advantages , facilitates construction of candidate graphs from graph fragments , and helps leverage indices in novel ways for query optimization . </S>",
    "<S> however , similar to other covering problems , minimum hub cover is np - hard , and thus is a natural candidate for optimization . </S>",
    "<S> we claim that computing the minimum hub cover leads to substantial cost reduction for graph query processing . </S>",
    "<S> we present a computational characterization of minimum hub cover based on integer programming to substantiate our claim and investigate its computational cost on various graph types . </S>"
  ]
}