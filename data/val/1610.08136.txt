{
  "article_text": [
    "neural text embedding models have recently gained significant popularity for both natural language processing ( nlp ) and information retrieval ( ir ) tasks . in ir ,",
    "a significant number of these works have focused on word embeddings @xcite and modelling short - text similarities @xcite . in traditional web search ,",
    "the query consists of only few terms but the body text of the documents typically has at least few hundred sentences . in the absence of click information , such as for newly - published or infrequently - visited documents , the body text can be a useful signal to determine the relevance of the document for the query .",
    "therefore , extending existing neural text representation learning approaches to long body text for document ranking is an important challenge in ir .",
    "however , as was noted during a recent workshop @xcite , in spite of the recent surge in interests towards applying deep neural network ( dnn ) models for retrieval , their success on ad - hoc retrieval tasks has been rather limited .",
    "some recent papers @xcite report worse performance of neural embedding models when compared to traditional term - based approaches , such as bm25 @xcite .",
    "traditional ir approaches consider terms as discrete entities .",
    "the relevance of the document to the query is estimated based on , amongst other factors , the number of matches of query terms in the document , the parts of the document in which the matches occur , and the proximity between the matches .",
    "in contrast , latent semantic analysis ( lsa ) @xcite , probabilistic latent semantic analysis ( plsa ) @xcite and latent dirichlet allocation ( lda ) @xcite learn low - dimensional vector representations of terms , and match the query against the document in the latent semantic space .",
    "retrieval models can therefore be classified based on what representations of text they employ at the point of matching the query against the document . at the point of match , if each term is represented by a unique identifiers ( _ local _ representation @xcite ) then the query - document relevance is a function of the pattern of occurrences of the exact query terms in the document .",
    "however , if the query and the document text is first projected into a continuous latent space , then it is their distributed representations that are compared . along these lines ,",
    "@xcite classify recent dnn models for short - text matching as either _",
    "interaction_-focused @xcite or _",
    "representation_-focused @xcite .",
    "they claim that ir tasks are different from nlp tasks , and that it is more important to focus on exact matching for the former and on learning text embeddings for the latter .",
    "@xcite , on the other hand , claim that models that compare the query and the document in the latent semantic space capture a different sense of relevance than models that focus on exact term matches , and therefore the combination of the two is more favorable .",
    "our work is motivated by the latter intuition that it is important to match the query and the document using both local and distributed representations of text .",
    "we propose a novel ranking model comprised of two separate dnns that model query - document relevance using local and distributed representations , respectively .",
    "the two dnns , referred to henceforth as the _ local model _ and the _ distributed model _",
    ", are jointly trained as part of a single neural network , that we name as a _ duet _ architecture because the two networks co - operate to achieve a common goal .",
    "figure [ fig : wordimportance ] demonstrates how each subnetwork models the same document given a fixed query .",
    "while the local model captures properties like exact match position and proximity , the distributed model detects property synonyms ( e.g. ` obama ' ) , related terms ( e.g. ` federal ' ) , and even wellformedness of content ( e.g. ` the ' , ` of ' ) .    in this paper",
    ", we show that the duet of the two dnns not only outperforms the individual local and distributed models , but also demonstrate large improvements over traditional baselines and other recently proposed models based on dnns on the document ranking task .",
    "unlike many other work , our model significantly outperforms classic ir approaches by using a dnn to learn text representation .",
    "dnns are known for requiring significant training data , and most of the state - of - the - art performances achieved by these deep models are in areas where large scale corpora are available for training @xcite .",
    "some of the lack of positive results from neural models in the area of ad - hoc retrieval is likely due to the scarce public availability of large quantity of training data necessary to learn effective representations of text . in section [ sec : discussion ]",
    ", we will present some analysis on the effect of training data on the performance of these dnn models .",
    "in particular , we found that  unsurprisingly  the performance of the distributed model improves drastically in the presence of more data .",
    "unlike some previous work @xcite that train on clickthrough data with randomly sampled documents as negative examples , we train our model on human - judged labels . our candidate set for every query",
    "consists of documents that were retrieved by the commercial search engine bing , and then labelled by crowdsourced judges .",
    "we found that training with the documents that were rated irrelevant by the human judges as the negative examples is more effective than randomly sampling them from the document corpus .",
    "to summarize , the key contributions of this work are :    1 .",
    "we propose a novel duet architecture for a model that jointly learns two deep neural networks focused on matching using local and distributed representations of text , respectively .",
    "we demonstrate that this architecture out - performs state - of - the - art neural and traditional non - neural baselines .",
    "we demonstrate that training with documents judged as irrelevant as the negative examples is more effective than randomly sampling them from the corpus .",
    "before describing our ranking model , we first present three properties found across most effective retrieval systems .",
    "we will then operationalize these in our architecture in section [ sec : model ] .",
    "first , _ exact term matches _ between the query and the document are fundamental to all information retrieval models @xcite .",
    "traditional ir models such as bm25 @xcite are based on counts of exact matches of query terms in the document text .",
    "they can be employed with minimal ( or no ) need for training data , sometimes directly on new tasks or corpora .",
    "exact matching can be particularly important when the query terms are new or rare .",
    "for example , if new documents appear on the web with the television model number ` sc32mn17 ' then bm25 can immediately retrieve these pages containing precisely that model number without adjusting any parameters of the ranking model .",
    "a good ranking model needs to take advantage of exact matches to perform reliably on fresh and rare queries .",
    "second , _ match positions _ of query terms in the document not only reflect where potentially relevant parts of the document are localized ( e.g. title , first paragraph , closing paragraph ) but also how clustered individual query term matches are with each other .",
    "figure [ fig : interactionmatrix ] shows the position of matches on two different queries and a sample of relevant and non - relevant documents . in the first query , we see that query term matches in the relevant document are much more clustered than in the non - relevant documents .",
    "we observe this behavior in the second query but also notice that the clustered matches are localized near the beginning of the relevant document .",
    "match proximity serves as a foundation for effective methods such as sequential dependence models @xcite .",
    "finally , _ inexact term matches _ between the query and the document refer to techniques for addressing the vocabulary mismatch problem .",
    "the main disadvantage of term matching is that related terms are ignored , so when ranking for the query ` australia ' then only the term frequency of ` australia ' is considered , even though counting terms like ` sydney ' and ` koala ' can be good positive evidence .",
    "@xcite pointed out that under the probabilistic model of ir there is , in fact , no good justification for ignoring the non - matching terms in the document .",
    "furthermore , @xcite demonstrated that a distributed representation based retrieval model that considered _ all _ document terms is able to better distinguish between a passage that is truly relevant to the query `` cambridge '' from a passage on a different topic ( e.g. , giraffes ) with artificially injected occurrences of the term `` cambridge '' .",
    "any ir model that considers the distribution of non - matching terms is therefore likely to benefit from this additional evidence , and be able to tell `` cambridge '' apart from `` an african even - toed ungulate mammal '' .    in practice , the most effective techniques leverage combinations of these techniques .",
    "dependence models combine exact matching with proximity @xcite .",
    "lda - based document models combine exact matching with inexact matching @xcite .",
    "query hypergraphs capture all three @xcite .",
    "our method also combines these techniques but , unlike prior work , jointly learns all free parameters of the different components .",
    "figure [ fig : architecture ] provides a detailed schematic view of the duet architecture .",
    "the distributed model projects the query and the document text into an embedding space before matching , while the local model operates over an interaction matrix comparing every query term to every document term .",
    "the final score under the duet setup is the sum of scores from the local and the distributed networks , @xmath0 where both the query and the document are considered as ordered list of terms , @xmath1 $ ] and @xmath2 $ ] .",
    "each query term @xmath3 and document term @xmath4 is a @xmath5 vector where @xmath6 is the input representation of the text ( e.g. the number of terms in the vocabulary for the local model ) .",
    "we fix the length of the inputs across all queries and documents such that we consider only the first 10 terms in the query and the first 1000 in the document . if the either is shorter than the target dimension , the input vectors are padded with zeros .",
    "the truncation of the document body text to the first 1000 terms is performed only for our model variants .",
    "for all the neural and non - neural baseline models we consider the full body text .",
    "the local model estimates document relevance based on patterns of exact matches of query terms in the document . to this end , each term is represented by its one - hot encoding in a @xmath7-dimensional space , where @xmath7 is the size of the vocabulary .",
    "the model then generates the @xmath8 binary matrix @xmath9 , capturing every exact match ( and position ) of query terms in the document .",
    "this interaction matrix , without the zero - padding , is analogous to the visual representation of term matches in figure [ fig : interactionmatrix ] , and therefore captures both exact term matches and match positions .",
    "it is also similar to the indicator matching matrix proposed previously by @xcite .",
    "while interaction matrix @xmath10 perfectly captures every query term match in the document , it does not retain information about the actual terms themselves .",
    "therefore , the local model can not learn term - specific properties from the training corpus , nor model interactions between dissimilar terms .",
    "the interaction matrix @xmath10 is first passed through a convolutional layer with @xmath11 filters , a kernel size of @xmath12 , and a stride of @xmath13 .",
    "the output @xmath14 corresponding to the @xmath15 convolutional window over @xmath10 is a function of the match between the @xmath16 term against the whole document , @xmath17 where @xmath18 is row @xmath19 of @xmath10 , @xmath20 is performed elementwise , and the @xmath21 matrix @xmath22 contains the learnable parameters of the convolutional layer . the output @xmath23 of the convolutional layer is a matrix of dimension @xmath24 .",
    "we use a filter size ( @xmath11 ) of 300 for all the evaluations reported in this paper .",
    "the output of the convolutional layer is then passed through two fully - connected layers , a dropout layer , and a final fully - connected layer that produces a single real - valued output .",
    "all the nodes in the local model uses the hyperbolic tangent function for non - linearity .",
    "the distributed model learns dense lower - dimensional vector representations of the query and the document text , and then computes the positional similarity between them in the learnt embedding space . instead of a one - hot encoding of terms , as in the local model",
    ", we use a character @xmath25-graph based representation of each term in the query and document .",
    "our @xmath25-graph based input encoding is motivated by the trigraph encoding proposed by @xcite . for each term , we count all of the @xmath25-graphs present for @xmath26 .",
    "we then use this @xmath25-graph frequency vector of length @xmath27 to represent the term .    instead of directly computing the interaction between the @xmath28 matrix @xmath29 and the @xmath30 matrix @xmath31",
    ", we first learn a series of nonlinear transformations to the character - based input . for both query and document",
    ", the first step is convolution .",
    "the @xmath32 convolution window has filter size of 300 .",
    "it projects 3 consecutive terms to a 300 dimensional vector , then takes a stride by 1 position , and projects the next 3 terms , and so on . for the query ,",
    "the convolution step generates a tensor of dimensions @xmath33 . for document it generates one of dimensions @xmath34 .    following this",
    ", we conduct a max - pooling step . for the query",
    "the pooling kernel dimensions are @xmath35 . for document",
    "it is @xmath36 . as a result",
    ", we get one @xmath37 matrix @xmath38 for the query and a @xmath39 matrix @xmath40 for the document .",
    "the document matrix @xmath40 can be interpreted as @xmath41 separate embeddings , each corresponding to different equal - sized spans of text within the document .",
    "our choice of a window - based max - pooling strategy , instead of global max - pooling as employed by cdssm @xcite , is motivated by the fact that the window - based approach allows the model to distinguish between matches in different parts of the document .",
    "as posited in section [ sec : motivation ] , a model that is aware of match positions may be more suitable when dealing with long documents , especially those containing mixture of many topics .",
    "the output of the max - pooling layer for the query is then passed through a fully - connected layer .",
    "for the document , the @xmath39 dimensional matrix output is operated on by another convolutional layer with filter size of 300 , kernel dimensions of @xmath37 , and a stride of 1 .",
    "the combination of these convolutional and max - pooling layers enable the distributed model to learn suitable representations of text for effective inexact matching .    in order to perform the matching",
    ", we conduct the element - wise or hadamard product between the embedded document matrix and the extended or broadcasted query embedding ,    @xmath42    after this , we pass the matrix through fully connected layers , and a dropout layer until we arrive at a single score .",
    "similar to the local model , we use hyperbolic tangent function here for non - linearity .",
    "each training sample consists of a query @xmath29 , a relevant document @xmath43 and a set of irrelevant documents @xmath44 .",
    "we use a softmax function to compute the posterior probability of the positive document given a query based on the score .",
    "@xmath45    and we maximize the log likelihood @xmath46 using stochastic gradient descent .",
    "we conducted three experiments to test :    the effectiveness of our duet model compared to the local and distributed models separately , and    the effectiveness of our duet model compared to existing baselines for content - based web ranking ,    the effectiveness of training with judged negative documents compared to random negative documents .    in this section",
    ", we detail our experiment setup and baseline implementations .      the training dataset consisted of 199,753 instances in the format described in section [ sec :",
    "experiment : training ] .",
    "the queries in the training dataset were randomly sampled from bing s search logs from a period between january , 2012 and september , 2014 .",
    "human judges rated the documents on a five - point scale ( _ perfect _ , _ excellent _ , _ good _ , _ fair _ and _ bad _ ) .",
    "the document body text was retrieved from bing s web document index .",
    "we used proprietary parsers for extracting the body text from the raw html content .",
    "all the query and the document text were normalized by down - casing and removing all non - alphanumeric characters .",
    "we considered two different test sets , both sampled from bing search logs .",
    "the _ weighted _ set consisted of queries sampled according their frequency in the search logs . as a result , frequent queries were well - represented in this dataset .",
    "queries were sampled between october , 2014 and december , 2014 . the _ unweighted _ set consisted of queries sampled uniformly from the entire population of unique queries .",
    "the queries in this samples removed the bias toward popular queries found in the weighted set .",
    "the unweighted queries were sampled between january , 2015 and june , 2015 .    because all of our datasets were derived from sampling real query logs and because queries will naturally repeat",
    ", there was some overlap in queries between the training and testing sets .",
    "specifically , 14% of the testing queries in the weighted set occurred in the training set , whereas only 0.04% of the testing queries in the unweighted set occurred in the training set .",
    "we present both results for those who may be in environments with repeated queries ( as is common in production search engines ) and for those who may be more interested in cold start situations or tail queries .",
    "table [ tbl : testsets ] summarizes statistics for the two test sets .",
    "lrrr & queries & documents & @xmath47 + training & 199,753 & 998,765 & 5 +   + weighted & 7,741 & 171,302 & 24.9 + unweighted & 6,808 & 71,722 & 10.6 +      besides the architecture ( figure [ fig : architecture ] ) , our model has the following free parameters : the maximum order of the character - based representation for the distributed model ( @xmath48 ) , the number of negative documents to sample at training time ( @xmath49 ) , the dropout rate , and the learning rate .",
    "we used a maximum order of five for our character @xmath25-graphs in the distributed model . instead of using the full 62,193,780-dimensional vector , we only considered top 2,000 most popular @xmath25-graphs , resulting 36 unigraphs ( a - z and 0 - 9 ) , 689 bigraphs , 1149 trigraphs , 118 4-graphs , and eight 5-graphs .    when training our model ( section [ sec : training ] ) , we sampled four negative documents for every one relevant document .",
    "more precisely , for each query we generated a maximum of one training sample of each form ,    one _ excellent _ document with four _ fair _ documents    one _ excellent _ document with four _ bad _ documents    one _ good _ document with four _ bad _ documents    .",
    "pilot experiments showed that treating documents judged as _ fair _ or _ bad _ as the negative examples resulted in significantly better performance , than when the model was trained with randomly sampled negatives . for training , we discarded all documents rated as _ perfect _ because a large portion of them fall under the navigational intent , which can be better satisfied by historical click based ranking signals .",
    "the dropout rate and the learning rate were set to 0.20 and 0.01 , respectively , based on a validation set .",
    "we implemented our model using cntk @xcite and trained the model with stochastic gradient descent based optimization ( with automatic differentiation ) on a single gpu .",
    "it was necessary to use a small minibatch size of 8 to fit the whole data in gpu memory .",
    "our baselines capture the individual properties we outlined in section [ sec : motivation ] .",
    "exact term matching is effectively performed by many classic information retrieval models .",
    "we used the okapi bm25 @xcite and query likelihood ( ql ) @xcite models as representative of this class of model .",
    "we used indri for indexing and retrieval .",
    "match positions are handled by substantially fewer models .",
    "metzler s dependence model ( dm ) @xcite provides an inference network approach to modeling term proximity .",
    "we used the indri implementation for our experiments .",
    "inexact term matching received both historic and modern treatments in the literature .",
    "originally presented latent semantic analysis ( lsa ) @xcite as a method for addressing vocabulary mismatch by projecting words and documents into a lower - dimension latent space . the dual embedding space model ( desm )",
    "@xcite computes a document relevance score by comparing every term in the document with every query term using pre - trained word embeddings .",
    "we used the same pre - trained word embeddings dataset that the authors made publicly available online for download .",
    "these embeddings , for approximately 2.8 m words , were previously trained on a corpus of bing queries . in particular",
    ", we use the @xmath50 model , which was reported to have the best performance on the retrieval task , as a baseline in this paper . both the deep structured semantic model ( dssm ) @xcite and its convolutional variant cdssm @xcite consider only the document title for matching with the query . while some papers have reported negative performances for title - based dssm and cdssm on the _ ad hoc _ document retrieval tasks @xcite , we included document - based variants appropriately retrained on the same set of positive query and document pairs as our model .",
    "as with the original implementation we choose the irrelevant documents for training by randomly sampling from the document corpus . for the cdssm model , we concatenated the trigraph hash vectors of the first @xmath51 terms of the body text followed by a vector that is a sum of the trigraph hash vectors for the remaining terms .",
    "the choice of @xmath51 was constrained by memory requirements , and we pick 499 for our experiments .",
    "the drmm model @xcite uses a dnn to perform term matching , with few hundred parameters , over histogram - based features .",
    "the histogram features , computed using exact term matching and pre - trained word embeddings based cosine similarities , ignoring the actual position of matches .",
    "we implemented the @xmath52 variant of the model on cntk @xcite using word embeddings trained on a corpus of 341,787,174 distinct sentences randomly sampled from bing s web index , with a corresponding vocabulary of 5,108,278 words . every training sample for our model",
    "was turned into four corresponding training samples for drmm , comprised of the query , the positive document , and each one of the negative documents .",
    "this guaranteed that both models observed the exact same pairs of positive and negative documents during training .",
    "we adopted the same loss function as proposed by guo _",
    "et al . _",
    "all evaluation and empirical analysis used the normalized discounted cumulative gain ( ndcg ) metric computed at positions one and ten @xcite .",
    "all performance metrics were averaged over queries for each run . whenever testing for significant differences in performance , we used the paired @xmath53-test with a bonferroni correction .",
    "table [ tbl : results - main ] reports ndcg based evaluation results on two test datasets for our model and all the baseline models .",
    "our main observation is that the duet model performs significantly better than the individual local and distributed models .",
    "this supports our underlying hypothesis that matching in a latent semantic space can complement exact term matches in a document ranking task , and hence a combination of the two is more appropriate .",
    "note that the ndcg numbers for the local and the distributed models correspond to when these dnns are trained individually , but for the ` duet ' the two dnns are trained together as part of a single neural network .    among the baseline models , including both traditional and neural network based models , cdssm and desm achieve the highest ndcg at position one and ten , respectively , on the weighted test set .",
    "on the unweighted test set drmm is our best baseline model at both rank positions .",
    "the duet model demonstrates significant improvements over all these baseline models on both test sets and at both ndcg positions .",
    "we also tested our independent local and distributed models against their conceptually closest baselines . because our local model captures both matching and proximity , we compared performance to dependence models ( dm ) .",
    "while the performance in terms of ndcg@1 is statistically indistinguishable , both of the ndcg@10 results are statistically significant ( @xmath54 ) .",
    "we compared our distributed model to the best neural model for each test set and metric .",
    "we found no statistically significant difference except for ndcg@10 for the weighted set .",
    "we were interested in testing our hypotheses that training with labeled negative documents is superior to training with randomly sampled documents presumed to be negative .",
    "we conducted an experiment training with negative documents following each of the two protocols .",
    "figure [ fig : negsampling ] shows the results of these experiments .",
    "we found that , across all of our models , using judged nonrelevant documents was more effective than randomly sampling documents from the corpus and considering them as negative examples .",
    "our results demonstrated that our joint optimization of local and distributed models provides substantial improvement over all baselines .",
    "although the independent models were competitive with existing baselines , the combination provided a significant boost .",
    "we also confirmed that using judged negative documents should be used when available .",
    "we speculate that training with topically - similar ( but nonrelevant ) documents allows the model to better discriminate between the confusable documents provided by an earlier retrieval stage .",
    "this sort of staged ranking , first proposed by @xcite , is now a common web search engine architecture .",
    "in section [ sec : baselines ] we described our baseline models according to which of the properties of effective retrieval systems , that we outlined in section [ sec : motivation ] , they incorporate .",
    "it is reasonable to expect that models with certain properties are better suited to deal with certain segments of queries .",
    "for example , the relevant web page for the query `` what channel are the seahawks on today '' may contain the name of the actual channel ( e.g. , `` espn '' or `` fox '' ) and the actual date for the game , instead of the terms `` channel '' or `` today '' . a retrieval model that only counts repetitions of query terms is likely to retrieve less relevant documents for this query  compared to a model that considers `` espn '' and `` fox '' to be relevant document terms .",
    "in contrast , the query `` pekarovic land company '' , which may be considered as a tail navigational intent , is likely to be better served by a retrieval model that simply retrieves documents containing many matches for the term `` pekarovic '' .",
    "a representation learning model is unlikely to have a good representation for this rare term , and therefore may be less equipped to retrieve the correct documents .",
    "these anecdotal examples agree with the results in in table [ tbl : results - main ] that show that on the weighted test set all the neural models whose main focus is on learning distributed representations of text ( duet model , distributed model , desm , dssm , and cdssm ) perform better than the models that only look at patterns of term matches ( local model and drmm ) .",
    "we believe that this is because the dnns are able to learn better representations for more popular queries , and perform particularly well on this segment .",
    "figure [ fig : slicendice ] provides further evidence towards this hypothesis by demonstrating that the distributed model has a larger ndcg gap with the local model for queries containing more popular terms , and when the number of terms in the query is small .",
    "the duet model , however , is found to perform better than both the local and the distributed models across all these segments .    in order to better understand the relationship of our models to existing baselines , we compared the per - query performance amongst all models .",
    "we conjecture that similar models should performance similarly for the same queries .",
    "we represented a retrieval model as a vector where each position of the vector contains the performance of the model on a different query .",
    "we randomly sample two thousand queries from our weighted test set and represent all ranking models as vectors of their ndcg values against these two thousand queries .",
    "we visualized the similarity between models by projecting using principal component analysis on the set of performance vectors .",
    "the two - dimensional projection of this analysis is presented in figure [ fig : modelspace ] .",
    "the figure largely confirms our intuitions about properties of retrieval models .",
    "models that use only local representation of terms are closer together in the projection , and further away from models that learn distributed representations of text .",
    "interestingly , the plot does not distinguish between whether the underlying model is based on a neural network based or not , and only focuses on the retrieval properties of the model .    ) , proximity ( @xmath55 ) , and inexact matches ( @xmath56 ) are presented .",
    "our models are presented as black squares.,width=288 ]    another interesting distinction between deep neural models and traditional approaches is the effect of the training data size on the performance of the model .",
    "bm25 has very few parameters and can be applied to new corpus or task with almost no training .",
    "on the other hand , dnns like ours demonstrate significant improvements when trained with larger datasets .",
    "figure [ fig : trainingdata ] shows that the effect of training data size particularly pronounced for the duet and the distributed models that learns representations of text .",
    "the trends in these plots indicate that training on even larger datasets may result in further improvements in model performance over what is reported in this paper .",
    "we believe this should be a promising direction for future work .    a last consideration",
    "when comparing these models is runtime efficiency .",
    "web search engines receive tens of thousands of queries per second . running a deep neural model on raw body text at that scale",
    "is a hard problem .",
    "the local sub - network of our model operates on the term interaction matrix that should be reasonable to generate using an inverted index . for the distributed model ,",
    "it is important to note that the @xmath39 dimensional matrix representation of the document , that is used to compute the hadamard product with the query , can be pre - computed and stored as part of the document cache . at runtime",
    ", only the hadamard product and the subsequent part of the network needs to be executed .",
    "such caching strategies , if employed effectively , can mitigate large part of the runtime cost of running a dnn based document ranking model at scale .",
    "representations of data can be local or distributed . in a local representation",
    "a single unit represents an entity , for example there is a particular memory cell that represents the concept of a grandmother . that cell should be active if and only if the concept of a grandmother is present .",
    "by contrast , in a distributed representation , the concept of grandmother would be represented by a pattern of active cells .",
    "@xcite provides an overview contrasting distributed and local representations , listing their good and bad points . in a distributed representation , an activation pattern that has some errors or other differences from past data",
    "can still be mapped to the entity in question and to related entities , using a similarity function .",
    "a local representation lacks this robustness to noise and ability to generalize , but is better at precisely storing a large set of data .",
    "this paper considers local and distributed representations of queries and documents for use in web page ranking .",
    "our measure of ranking quality is ndcg @xcite , which rewards a ranker for returning documents with higher gain nearer to the top , where gain is determined according to labels from human relevance assessors .",
    "we describe different ranking methods in terms of their representations and how this should help them achieve good ndcg .",
    "exact term matching models such as bm25 @xcite and query likelihood @xcite tend to rank a document higher if it has a greater number of query term matches , while also potentially employing a variety of smoothing , weighting and normalization approaches .",
    "such exact matching is done with a local representation of terms .",
    "exact match systems do not depend on a large training set , since they do not need to learn a distributed representation of queries and documents .",
    "they are useful in cases where the relevant documents contain exactly the query terms entered by the user , including very rare or new vocabulary , since new terms can be incorporated with no adjustments to the underlying model .",
    "they can also be extended to reward matches of query phrases and proximity @xcite .    to deal with the vocabulary mismatch problem that arises with local representations , it is possible to do document ranking using a distributed representation of terms .",
    "@xcite developed the popular word2vec embedding approach that has been used in a number of retrieval studies .",
    "@xcite use term embeddings as evidence for term weighting , learning regression models to optimize weighting in a language modeling and a bm25 retrieval model .",
    "@xcite used term embeddings for smoothing in the language modeling approach of information retrieval .",
    "@xcite used dual embeddings , one for document terms and one for query terms , then ranked according to the all - pairs similarity between vectors .",
    "@xcite used term embeddings to generate query expansion candidates in the language modeling retrieval framework , also finding better performance when training a specialized term embedding .",
    "other papers incorporating word embeddings include @xcite .",
    "@xcite propose the use of matching matrices to represent the similarity of short texts , then apply a convolutional neural network inspired by those in computer vision .",
    "they populate the matching matrix using both local and distributed term representations . in the local representation , an exact match",
    "is used to generate binary indicators of whether the @xmath19th term of one text and @xmath57th term of the other are the same , as in our local model . in the distributed representation , a pre - trained term embedding",
    "is used instead , populating the match matrix with cosine or inner product similarities .",
    "the method works for some problems with short text , but not for document ranking @xcite . however , by using the match matrix to generate summary statistics it is possible to make the method work well @xcite , which is our drmm baseline .",
    "these term embeddings are a learned representation of language , but in most cases they are not learned on query - document relevance labels",
    ". more often they are trained based on a a corpus , where a term s representation is learned from its surrounding terms or other document context .",
    "the alternative , learning a representation based on ndcg labels , is in keeping with recent progress in deep learning .",
    "deep models have multiple layers that learn distributed representations with multiple levels of abstraction . this kind of representation learning , along with other factors such as the availability of large labeled data sets , has yielded performance improvements on a variety of tasks such as speech recognition , visual object recognition and object detection @xcite .",
    "this paper learns a text representation end - to - end based on query - document ranking labels .",
    "this has not been done often in related work with document body text , but we can point to related papers that use short text such as title , for document ranking or related tasks . @xcite learn a distributed representation of query and title , for document ranking .",
    "the input representation is character trigraphs , the training procedure asks the model to rank clicked titles over randomly chosen titles , and the test metric is ndcg with human labels .",
    "@xcite developed a convolutional version of the model .",
    "these are our dssm and cdssm baselines .",
    "other convolutional models that match short texts using distributed representations include @xcite , also showing good performance on short text ranking tasks .    outside of document ranking , learning text representations for",
    "the target task has been explored in the context of other ir scenarios , including query classification @xcite , query auto - completion @xcite , next query prediction @xcite , and entity extraction @xcite .",
    "we propose a novel document ranking model composed of two separate deep neural networks , one that matches using a local representation of text , and another that learns a distributed representation before matching .",
    "the duet of these two neural networks demonstrated a higher performance than the solo models on the document ranking task as well as significant improvements over all baselines , including both traditional ir baselines and other recently proposed models based on shallow and deep neural networks .",
    "our analysis indicate that these models may achieve even more substantial improvements in the future with much larger datasets .",
    "the authors are grateful to abdelrahman mohamed , pushmeet kohli , emine yilmaz , filip radlinski , david barber , david hawking , and milad shokouhi for the insightful discussions and feedback during the course of this work , and to frank seide and dong yu for their incredible support with cntk ."
  ],
  "abstract_text": [
    "<S> models such as latent semantic analysis and those based on neural embeddings learn _ distributed _ representations of text , and match the query against the document in the latent semantic space . in traditional information retrieval models , on the other hand , terms have discrete or _ local _ representations , and the relevance of a document is determined by the exact matches of query terms in the body text . </S>",
    "<S> we hypothesize that matching with distributed representations complements matching with traditional local representations , and that a combination of the two is favorable . </S>",
    "<S> we propose a novel document ranking model composed of two separate deep neural networks , one that matches the query and the document using a local representation , and another that matches the query and the document using learned distributed representations . </S>",
    "<S> the two networks are jointly trained as part of a single neural network . </S>",
    "<S> we show that this combination or ` duet ' performs significantly better than either neural network individually on a web page ranking task , and also significantly outperforms traditional baselines and other recently proposed models based on neural networks . </S>"
  ]
}