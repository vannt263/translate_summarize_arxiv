{
  "article_text": [
    "the empirical analysis of interactions between the elements of a complex system is fundamental to understand both the collective structures and the basic rules inducing the emergent behavior of complex systems .",
    "the monitoring of several complex systems nowadays produces large sets of multivariate data .",
    "examples of these sets of data are present in physical @xcite , biological @xcite and economic systems @xcite and their analysis is an important and challenging task in the investigation of complex systems .",
    "many efforts have been done in the analysis of multivariate data series and most of them focus on the study of pair cross - correlations .",
    "the analysis of cross - correlation is precious in order to elicit the emergence of collective structures from multivariate data .",
    "classical spectral methods such as the principal component analysis @xcite , recent related techniques based on concepts of random matrix theory @xcite , hierarchical clustering @xcite , factor analysis @xcite and graph theory @xcite are fruitful approaches to the analysis of correlations among elements of complex systems elicited by multivariate data .",
    "cross - correlations estimated from real data are unavoidably affected by the statistical uncertainty due to the finite size of the sample . in most cases ,",
    "the length of data is unavoidably limited whereas in other cases the length of data needs to be limited to avoid that sizable non - stationary effects might introduce large errors in the estimation of correlations .",
    "cross - correlations might also be affected by noise due to measurement errors and to the interaction of the system with the environment . in order to at least partially overcome these problems , it is advisable to select statistically reliable information from the correlation matrix .",
    "we address the selection of the most statistically reliable part of the correlation matrix with the locution _ filtering of the correlation matrix_.    several techniques have been proposed in the literature in order to filter out information from the correlation matrix and therefore it is important to have at hand a method for comparing the performance of such different techniques in a quantitative way .    in this paper , we propose to measure the performance of filtering procedures by using the kullback - leibler distance @xcite which is a measure of distance between probability distributions and it is widely used in information theory ( see for instance @xcite ) .",
    "specifically , for multivariate gaussian variables , we explicitly compute the analytical form of the kullback - leibler distance and we show how it depends on the correlation matrices of the considered sets of data or of filtered versions of them . under the same assumptions we analytically obtain the expected values of the kullback - leibler distance between the correlation matrix of a multivariate model and a sample correlation matrix obtained with the pearson estimator from a finite set of data .",
    "one of our key results is that these expected values are model independent .",
    "this result shows that the kullback - leibler distance is very good in quantifying the amount of information present in a sample correlation matrix with respect to an hypothetical reference model also in the cases when the specific nature of the model is unknown .",
    "we are also able to compute the expected value of the kullback - leibler distance between two distinct samples of the correlation matrix obtained from the same random source .",
    "this last quantity is very useful in quantifying the stability associated with any sample estimation and specifically with the stability of the correlation matrices obtained from filtering procedures .",
    "we show the effectiveness of the use of the kullback - leibler distance in comparing data and models and in assessing the stability of the estimation of the sample correlation matrix by investigating four different filtering methods .",
    "two of them are based on spectral analysis , while the other two are generated by hierarchical clustering procedures .",
    "a good filtered correlation matrix is supposed to be informative about the sample correlation matrix and , at the same time , to be statistically more robust than the sample matrix itself with respect to statistical uncertainty . in our investigation",
    "we consider in a quantitative way both these aspects .",
    "the paper is organized as follows . in section [ kullbacktheor ]",
    "we present the analytical results of the expected values of the kullback - leibler distance and we show how the kullback - leibler distance can be used as an estimator of the goodness of filtering procedures . in section [ methods ] we describe the four filtering procedures that we quantitatively compare in section [ comparison ] both by investigating simulations and real data .",
    "finally , in section [ conclusions ] we draw our conclusions .",
    "the kullback - leibler distance ( see for instance @xcite ) or _ mutual entropy _ is a measure of the distance between two probability densities , say @xmath0 and @xmath1 , which is defined as @xmath2,\\ ] ] where @xmath3 $ ] indicates the expectation value with respect to the probability density @xmath0 .",
    "the kullback - leibler distance is asymmetric . in eq.([kullbackeqgen ] ) the expectation value is evaluated according to the distribution @xmath0 . since the property of symmetry is sometimes important a symmetrization of the kullback - leibler distance , called jefferys - kullback - leibler j - divergence has been introduced @xcite . in other cases",
    "the asymmetry could also be an useful feature of a distance measure .",
    "this is the case when objects of different nature ( or simply with different statistical meaning ) are compared .",
    "the kullback - leibler distance is widely used in information theory .",
    "the mutual information between two random variables @xmath4 and @xmath5 is defined as @xmath6 ( see for instance @xcite ) , where @xmath7 is the joint probability density function of @xmath4 and @xmath5 , whereas @xmath8 and @xmath9 are the corresponding marginal probabilities . in this case , the asymmetry is important because the mutual information is measuring the error one commits in considering two random variables as independent variables .",
    "accordingly , this measure is performed by evaluating the distance between the correct joint probability @xmath7 and the product @xmath10 , averaging the result over @xmath7 .",
    "here we consider the kullback - leibler distance between multivariate gaussian random variables .",
    "we consider variables with zero mean and unit variance without loss of generality because we are interested in the comparison of the correlation matrices of the two set of variables . in this case , the gaussian multivariate distribution associated with the random vector @xmath4 is completely defined by the correlation matrix @xmath11 of the system . in the following",
    "we indicate the probability density function with @xmath12 . given two different probability density functions @xmath13 and @xmath14 , we have    @xmath15 = \\int{p({\\bf \\sigma_1},x ) \\log\\left [ \\frac{p({\\bf \\sigma_1},x)}{p({\\bf \\sigma_2},x)}\\right ] dx},\\ ] ]    by performing the integral in eq .",
    "( [ kullbackmulti ] ) one obtains : @xmath16,\\end{aligned}\\ ] ] where @xmath17 is the dimension of the space spanned by the @xmath4 variable and @xmath18 indicates the determinant of @xmath11 . in appendix",
    "a we show how to derive the last equation from eq .",
    "( [ kullbackmulti ] ) .",
    "( [ kullbackgaussian ] ) shows that the kullback - leibler distance is an explicit function of only the correlation matrices @xmath19 and @xmath20 for multivariate normal distributions .",
    "therefore , from now on we indicate @xmath21 simply with @xmath22 .",
    "it is worth noting that the kullback - leibler distance takes naturally into account the statistical nature of correlation matrices .",
    "indeed @xmath22 is well defined only provided that the matrices @xmath19 and @xmath20 are positive definite .",
    "this property is not common to other measures of distance between matrices which are based essentially on the isomorphism between the matrix space and a vector space , e.g. the frobenius distance ( see below ) .",
    "however this property can also be a limitation .",
    "the kullback - leibler distance can not be used to quantify the distance between semi - positive correlation matrices that are observed when the length @xmath23 of data series is smaller than the number @xmath17 of elements of the system .",
    "the kullback - leibler distance is also related to the maximum likelihood factor analysis ( mlfa ) @xcite .",
    "in fact , the log - likelihood function to be maximized in order to describe a system of @xmath17 elements with sample correlation matrix @xmath24 estimated from data series of length @xmath23 , with a certain _ k_-factor model with correlation matrix @xmath25 is given by : @xmath26.\\ ] ] in the mlfa , @xmath27 is maximized with respect to @xmath25 .",
    "this maximization is therefore equivalent to minimize the kullback - leibler distance @xmath28 with respect to @xmath25 , because the other terms in eq .",
    "( [ loglikekull ] ) are independent of @xmath25 .",
    "it is to notice that in eq .",
    "( [ loglikekull ] ) the empirical correlation matrix @xmath24 is the one estimated from the investigated data and one calibrates the correlation matrix @xmath25 of the model by maximizing @xmath27 .",
    "this fact explains why the log - likelihood is depending on @xmath28 instead of @xmath29 . in this paper",
    "we want to apply the kullback - leibler distance to sample correlation matrices obtained with the pearson estimator .",
    "since different realizations of the process give rise to different samples , a kullback - leibler distance having one or two sample correlation matrices as arguments is a function of one or two random matrices .",
    "it is known that sample covariance matrices of finite variance variables belong to the ensemble of wishart random matrices and many statistical properties of wishart matrices are known @xcite .",
    "it is therefore useful to investigate the statistical properties of kullback - leibler distance involving sample correlation matrices of multivariate gaussian random variables .",
    "these properties will be useful in the next section as absolute terms of comparison of filtering procedures of the correlation matrix .",
    "let us consider a multinormally distributed random vector @xmath4 of dimension @xmath17 with correlation matrix @xmath11 .",
    "let @xmath30 and @xmath31 be two sample correlation matrices obtained from two independent realizations of the system both of length @xmath23 . by making use of the theory of wishart matrices",
    "@xcite we obtain that @xmath32=\\frac{1}{2 } \\left \\{n\\log{\\left(\\frac{2}{t}\\right)}+\\right .",
    "+ \\sum_{p = t - n+1}^{t}{\\left[\\frac{\\gamma^{\\prime}(p/2)}{\\gamma(p/2)}\\right]}+\\frac{n ( n+1)}{t - n-1}\\right\\},\\end{aligned}\\ ] ] @xmath33=\\frac{1}{2 } \\left \\{n\\log{\\left(\\frac{t}{2}\\right)}-\\right .",
    "\\nonumber \\\\ \\left . -\\sum_{p = t - n+1}^{t}{\\left[\\frac{\\gamma^{\\prime}(p/2)}{\\gamma(p/2)}\\right]}\\right\\}\\end{aligned}\\ ] ] and @xmath34=\\frac{1}{2 } \\frac{n ( n+1)}{t - n-1},\\ ] ] where @xmath35 is the usual gamma function and @xmath36 is the derivative of @xmath35 . in appendix",
    "b we show how to derive these expectation values .",
    "finally , it is possible to give the asymptotic expectation value of the standard deviation of @xmath37 by using the bartlett statistics @xcite . specifically if @xmath38 , @xmath39 and @xmath40 we infer that the standard deviation of @xmath37 is @xmath41 . + it is important to observe that all the expectation values given in eq.s ( [ kullexpecsigs1]-[kullexpecs1s2 ] ) are independent of @xmath11 , i.e. they are independent of the specific model .",
    "this fact implies that ( i ) the kullback - leibler distance is a good measure of the statistical uncertainty of correlation matrix which is due to the finite length of data series and ( ii ) the expected value of the kullback - leibler distance is known also when the underlying model hypothesized to describe the system is unknown .",
    "this fact has important consequences .",
    "suppose one knows that the observed data are well approximated by a multivariate gaussian distribution and that one measures a sample correlation matrix @xmath24 .",
    "in order to remove some unavoidably present statistical uncertainty , the experimenter applies a filtering procedure to the data obtaining the filtered correlation matrix @xmath42 .",
    "if the filtering technique is able to recover the model correlation matrix , i.e. @xmath43 , the kullback - leibler distance @xmath44 must be equal on average to the value given in eq .",
    "( [ kullexpecs1sig ] ) .",
    "this expected value is independent on the ( unknown ) model correlation matrix @xmath11 .",
    "therefore large deviations from this expectation value indicate that the filtered matrix is not consistent with the true matrix of the system .",
    "if @xmath44 is significantly smaller ( in terms of the error @xmath41 ) than the expectation value of eq .",
    "( [ kullexpecs1sig ] ) , it means that the filtering procedure has at most partially removed the statistical uncertainty , i.e. the filtered matrix is keeping some of the statistical uncertainty due to the finite length @xmath23 . if , on the other hand , @xmath44 is significantly larger than the value of eq .",
    "( [ kullexpecs1sig ] ) , it means that the filtered matrix is either filtering too much information or distorting the signal .",
    "the distance between @xmath44 and the expected value of eq .",
    "( [ kullexpecs1sig ] ) is a measure of the goodness of the filtering procedure in keeping the maximal amount of information which can be present in sample correlation matrices estimated with a finite number of records .",
    "a second aspect concerns the stability of the filtered correlation matrix obtained from a sample matrix .",
    "let us suppose to apply a certain filtering procedure to the correlation matrices @xmath30 and @xmath31 of two independent realizations of the system , obtaining two filtered correlation matrices @xmath45 and @xmath46 .",
    "if it turns out that @xmath47 is larger than the expected value of @xmath48 described by eq .",
    "( [ kullexpecs1s2 ] ) , one can conclude that the filtering procedure produces correlation matrices less reproducible than the sample correlation matrices and therefore the procedure is not suitable for the purpose of filtering robust information from the empirical correlation matrices @xmath30 and @xmath31 .",
    "in summary we have shown that the kullback - leibler distance is very good for comparing correlation matrices because ( i ) it is an asymmetric distance and therefore it can distinguish between quantities observed in real systems and used to model the empirical observations , e.g. the sample correlation matrix and the filtered correlation matrix respectively ; ( ii ) the expectation values of the kullback - leibler distance given in eq.s ( [ kullexpecsigs1]-[kullexpecs1s2 ] ) are model independent , indicating that this distance is a good estimator of the statistical uncertainty due to the finite size of the empirical sample ; ( iii ) the kullback - leibler distance is intimately related to the log - likelihood function used in mlfa and ( iv ) it is deeply related with concepts of information theory , such as the the mutual information .",
    "these properties are not observed in other widespread distances between matrices .",
    "for example , we shall show that we do not find these properties in the frobenius distance , which is a standard measure of the distance between matrices .",
    "the frobenius distance between two @xmath49 matrices @xmath19 and @xmath20 , of real elements @xmath50 and @xmath51 respectively , is defined as @xmath52}\\end{aligned}\\ ] ] we note that the frobenius distance is symmetric .",
    "therefore it can not assign a different role to a model correlation matrix @xmath11 with respect to some sample @xmath24 of @xmath11 .",
    "we also observe that this distance is well defined independently of the statistical nature of matrices @xmath19 and @xmath20 , i.e. these matrices can also be non positive definite . finally and more important , we want to show , for a simple system of two variables , that the expectation value of the frobenius distance between a true correlation matrix and its pearson estimator is model dependent , i.e. this expectation value depends on the true correlation matrix . +",
    "let us consider a bivariate normal distribution @xmath53 , where @xmath11 is a @xmath54 correlation matrix and @xmath55 is the null vector of dimension @xmath56 .",
    "we indicate the only entry of @xmath11 different from 1 with @xmath57 . the sample correlation matrix @xmath24 is defined as @xmath58 where @xmath59 is the pearson correlation coefficient estimated from a realization of @xmath53 of length @xmath23 .",
    "it results that @xmath60 the distribution of @xmath59 is approximately gaussian for large values of @xmath23 .",
    "the mean value of @xmath59 is @xmath57 and the standard deviation is @xmath61 @xcite .",
    "accordingly , the expectation value of the frobenius distance between the two matrices is : @xmath62=\\frac{2}{\\sqrt{\\pi \\ , t } } ( 1-\\rho^2)\\ ] ] this result shows that the frobenius distance is model dependent and therefore it is not a good estimator of the statistical uncertainty of correlation matrix due to the finite length of data series .",
    "in this section we describe four procedures that can be used to filter correlation matrices .",
    "two procedures are based on spectral techniques , i.e. they are based on the comparison between the spectrum of the sample correlation matrix and the spectrum expected for a random matrix .",
    "these procedures are described in some detail in subsection [ spectral ] .",
    "the other two techniques that we consider here are hierarchical clustering procedures . specifically , we obtain two different filtered matrices by applying the single linkage cluster analysis ( slca ) and the average linkage cluster analysis ( alca ) to the sample correlation matrix of the system .",
    "the alca and slca are standard procedures of hierarchical clustering and we describe how these techniques generate filtered correlation matrices in subsection [ hctheor ] .",
    "random matrix theory @xcite was originally developed in nuclear physics and then applied to many different fields .",
    "let us consider @xmath17 independent random variables with finite variance and @xmath23 records each .",
    "the sample correlation matrix of the system in the limit @xmath63 is simply the identity matrix .",
    "when @xmath23 is finite the correlation matrix will in general be different from the identity matrix .",
    "random matrix theory allows to prove that in the limit @xmath64 , with a fixed ratio @xmath65 , the eigenvalues of the sample correlation matrix @xmath24 can not be larger than @xmath66 where @xmath67 for correlation matrices .",
    "the idea underlying both the spectral filtering procedures considered here is that of reducing the impact of eigenvalues smaller than @xmath68 on the structure of an empirical correlation matrix , in order to remove the effects of those eigenvalues that are consistent with the null hypothesis of uncorrelated random variables . in some practical cases , such as for example in finance",
    ", one finds that the largest eigenvalue @xmath69 of the empirical correlation matrix is definitely inconsistent with random matrix theory . in these cases , the null hypothesis is modified so that correlations can be explained in terms of a one factor model .",
    "accordingly , when @xmath70 we set @xmath71 in eq .",
    "( [ lmax ] ) @xcite .",
    "+ the first filtering procedure we consider here has been used by rosenow _",
    "the technique consists in replacing the eigenvalues smaller than @xmath68 in the diagonal matrix @xmath72 of eigenvalues of @xmath24 with @xmath73 s , thus obtaining a new diagonal matrix @xmath74 .",
    "one can therefore compute the matrix @xmath75 of elements @xmath76 , where @xmath77 is the matrix of eigenvectors of @xmath24 .",
    "finally , the filtered correlation matrix @xmath78 of elements @xmath79 is obtained by forcing the diagonal elements of @xmath80 to 1 , i.e. @xmath81 , where @xmath82 is the standard kronecker symbol .",
    "the second procedure we apply has been considered by potters _",
    "@xcite . here",
    ", eigenvalues smaller than @xmath68 in @xmath72 are replaced with their average value in the diagonal matrix @xmath83 . as in the previous case ,",
    "one rotates the matrix @xmath83 getting the matrix @xmath84 of elements @xmath85 , where again @xmath77 is the matrix of eigenvectors of @xmath24",
    ". finally , the filtered correlation matrix @xmath86 is the matrix of elements @xmath87 . both the matrices @xmath78 and @xmath86 satisfy the properties of a correlation matrix , i.e. ( i ) they are positive definite ; ( ii ) their diagonal elements are equal to 1 and ( iii ) their off - diagonal elements are in absolute value smaller or equal to 1 .",
    "another approach used to filter the information associated with the correlation matrix is given by hierarchical clustering analysis @xcite .",
    "let us consider a set of @xmath17 objects and suppose that a similarity measure , e.g. the correlation coefficient , between pairs of elements is defined .",
    "similarity measures can be written in a @xmath88 similarity matrix .",
    "the hierarchical clustering methods allow to hierarchically organize the elements in clusters .",
    "a result of the procedure is a rooted tree or dendrogram giving a quantitative description of the clusters thus obtained .",
    "another result of the procedure is a filtered correlation matrix .",
    "indeed the whole information about the rooted tree can be stored in a @xmath49 matrix @xmath89 @xcite .",
    "we have recently shown @xcite that , when the entries of @xmath89 are non negative numbers , this matrix is the correlation matrix of a suitable factor model , that we have named hierarchically nested factor model ( hnfm ) .",
    "this result ensures that , under the condition of non negative entries of @xmath89 ( typically satisfied in many empirical applications ) , this matrix is a true correlation matrix , i.e. it is positive definite .",
    "a large number of hierarchical clustering procedures can be found in the literature .",
    "for a review about the classical techniques see for instance ref . @xcite . in this paper",
    "we focus our attention on the slca and the alca .",
    "+ the starting point of both the procedures is the empirical correlation matrix @xmath24 .",
    "the following procedure performs the alca giving as an output a rooted tree and a filtered correlation matrix @xmath90 of elements",
    "@xmath91 :    1 .   set @xmath92 .",
    "2 .   select the maximum correlation @xmath93 in the correlation matrix @xmath94 .",
    "note that after the first step of construction @xmath95 and @xmath96 can be simple elements ( i.e. clusters of one element each ) or clusters ( sets of elements ) .",
    "@xmath97 and @xmath98 one sets the elements @xmath91 of the matrix @xmath90 as @xmath99 .",
    "3 .   merge cluster @xmath95 and cluster @xmath96 into a single cluster , say @xmath1 .",
    "the merging operation identifies a node in the rooted tree connecting clusters @xmath95 and @xmath96 at the correlation @xmath93 .",
    "4 .   redefine the matrix @xmath94 : @xmath100 where @xmath101 and @xmath102 are the number of elements belonging respectively to the cluster @xmath95 and to the cluster @xmath96 before the merging operation . note that if the dimension of @xmath94 is @xmath103 then the dimension of the redefined @xmath94 is @xmath104 because of the merging of clusters @xmath95 and @xmath96 into the cluster @xmath1 .",
    "5 .   if the dimension of @xmath94 is larger than 1 then go to step 2 , else stop .",
    "we have applied the four filtering procedures described in the previous section to both real and artificial systems .",
    "we have considered the real system of daily returns of the 100 most capitalized stocks traded at new york stock exchange ( nyse ) in the time period from january 2001 to december 2003 . in this case , the length of the @xmath111 time series is @xmath112 records .",
    "we have also considered the system of daily returns of 92 highly capitalized stocks traded at london stock exchange in 2002 .",
    "the length of the @xmath113 time series is @xmath114 for this system . we have also applied the filtering procedures to two artificial systems of @xmath111 elements each .",
    "both these systems are described by a factor model @xcite .",
    "a factor model is a mathematical model which describes the correlation among a set of elements that we indicate with @xmath115 @xmath116 , in terms of a certain number of common factors @xmath117 @xmath118 .",
    "the linear dependence of elements from factors is mathematically expressed as @xmath119 where @xmath120 , @xmath121^{1/2}$ ] .",
    "the @xmath122 factor @xmath123 and @xmath124 are independent identically distributed random variables with zero mean and unit variance . in our simulations , the factors @xmath123 @xmath118 and the idiosyncratic noises",
    "@xmath124 @xmath116 are gaussian random variables . in the first artificial system that we consider here",
    ", elements are grouped in @xmath125 orthogonal clusters . in terms of factor models , this orthogonal grouping of elements",
    "is expressed by the fact that elements belonging to different clusters depend on different ( independent ) factors , i.e. if @xmath115 belongs to the group @xmath96 then @xmath126 .",
    "the dimension of groups is heterogeneous to mimic typical conditions observed in some real systems .",
    "specifically the number of elements belonging to each group ranges from a minimum of 3 elements to a maximum of 17 .",
    "the other artificial system that we have considered is described by a hnfm with @xmath127 factors .",
    "this empirically based model has been introduced in ref .",
    "we have chosen these two models because they are conceptually very different one from the other .",
    "in fact , in the hnfm elements can not be straightforwardly divided in groups because they depend on factors in a nested hierarchical way whereas in the other model the groups of elements are clearly distinguished because elements belonging to different groups depend on different and mutually independent factors .",
    "roughly speaking we can say that the block diagonal model describes a `` separable '' system whereas the hnfm represents a `` nested '' system . in a first analysis , both the considered factor models are degenerate models , i.e. the coefficient @xmath128 , which expresses the dependence of the element @xmath129 on the factor @xmath96 in the model of eq .",
    "( [ facgen ] ) , is only depending on the factor and not on the element .",
    "it is to notice that by applying either the alca or the slca to the correlation matrix of the two considered models one obtains back the correlation matrix of the models .",
    "this fact is due to the degeneracy of the models and it gives a certain advantage to hierarchical clustering procedures with respect to spectral techniques in reconstructing the true correlation matrix of these systems .",
    "in fact both the considered spectral techniques can not reconstruct the true correlation matrix @xmath11 of the system when applied to @xmath11 itself .",
    "this is the first reason why we have decided to perform other simulations of the systems by removing the degeneracy from models .",
    "the second reason is that the true correlation matrix of the system is in general unknown for real data : we have only one correlation matrix obtained from a single realization of the system with finite time series length @xmath23 .",
    "accordingly , we have decided to perform one single realization , say @xmath130 , with length @xmath131 of data series of each model and we have assumed that the correlation matrix @xmath132 of this single realization of each model represents the true correlation matrix of the corresponding system .",
    "this approach removes the degeneracy of the @xmath133-parameters of models and at the same time allows to treat models in a way more similar to the one used for real data . in order to test the stability of filtering procedures with respect to statistical uncertainty ( as discussed in subsection [ samplesfilt ] ) , we have constructed bootstrap replicas of the single realization @xmath130 of each model .",
    "the bootstrap approach has the advantage that it does not require to make assumptions about the data distribution . instead of the bootstrap approach , in order to obtain different realizations of the non degenerate systems .",
    "the cholesky decomposition approach @xcite allows to construct mutually independent realizations of the system .",
    "however results obtained with the cholesky decomposition are in complete agreement with results obtained by using the bootstrap technique that we report in the paper .",
    "it is also to notice that by using the cholesky decomposition to perform simulations it is necessary to know the data distribution ( e.g. gaussian or student - t ) , whereas the bootstrap approach does not require to make assumptions about such distribution . ]",
    "we have simulated @xmath134 independent sets of data for the artificial systems described by the degenerate models and we have constructed @xmath134 bootstrap replicas @xcite of the empirical data .",
    "we have also considered 1000 bootstrap replicas of the single realization with series length @xmath131 of both the artificial systems , in order to treat the models more similarly to real data .",
    "we have applied all the filtering procedures described above to the correlation matrix @xmath135 of each simulation or replica @xmath129 of the artificial systems and to each replica @xmath129 of the real systems .",
    "therefore , we have obtained four filtered correlation matrices that we indicate with @xmath136 associated with each realization or replica @xmath129 of the systems .",
    "the label @xmath137 in @xmath136 stands for alca , slca , b and s depending on the filtering procedure .      the first question we want to ask is which filtering procedure performs better in detecting the correlation matrix of the model .",
    "we can ask this question only for the simulations where we know the model correlation matrix used to generate the data . in order to evaluate the ability of filtering procedures in reconstructing the correlation matrix of the model @xmath11",
    ", we have evaluated the average kullback - leibler distance @xmath138 between the correlation matrix of the model and the correlation matrix filtered from the samples .",
    "averages have been performed over @xmath134 realizations of the models .",
    "the smaller @xmath138 the larger is the amount of information about the model that is detected by the filtered matrix .",
    "in tables i and ii we distinguish between degenerate models that we indicate with `` block diagonal '' and `` hnfm '' and non degenerate models that we indicate with `` block diagonal ( n.d . ) '' and `` hnfm ( n.d . ) '' . in table",
    "i we report results obtained for all the considered models when the length of simulated normally distributed time series is @xmath112 . in the table , we observe that the alca outperforms all the other filtering procedures both for degenerate and non degenerate models .",
    "it is also to notice that the performance of slca is better than both the spectral filtering procedures for all the models with the exception of the non degenerate block diagonal model .",
    "such a good performance of hierarchical clustering filtering procedures was expected for the degenerate models .",
    "indeed , as we have discussed above , such models give a certain advantage to hierarchical clustering filtering procedures because of the degeneracy of coefficients .",
    "the fact that alca outperforms all the other filtering procedures also in the case of non degenerate models can be explained by taking into account both the length of data series and the way in which model degeneracy has been removed .",
    "the correlation matrix of the non degenerate models is by construction the correlation matrix of a single realization of the corresponding degenerate models with series length @xmath139 .",
    "this fact implies that the dispersion of the non degenerate correlations from the corresponding values in the degenerate model is of the order of @xmath140 . in table",
    "i , the length of simulated data series is also @xmath112 , i.e. @xmath141 .",
    "this fact implies that the statistical uncertainty associated with the sample correlations is of the order @xmath142 .",
    "this value is equal to @xmath143 , implying that for series length @xmath112 the non degeneracy of model parameters is of the same order of the statistical uncertainty . in other words , details about specific correlation values",
    "can not be distinguished from statistical uncertainty for such short data series .",
    "only the global structure of the correlation matrix is important and hierarchical clustering procedures results to be more capable than spectral techniques in reconstructing the correlation structure of the models . in order to better understand the effect of the non degeneracy of model parameters on the ability of filtering procedures in reconstructing the model , we consider also a case with time series of length longer than in the prevoius case .",
    "specifically , in table ii we report results obtained for time series of length @xmath144 , which is ten times the length considered in table i. in the case of @xmath144 , we continue to observe a better performance of hierarchical clustering filtering procedures and in particular of alca with respect to spectral techniques for the degenerate models .",
    "this fact was expected because of the degeneracy of the models . however , in table ii we observe that the spectral technique producing @xmath86 as result of the filtering outperforms hierarchical clustering procedures for the non degenerate models .",
    "the method producing @xmath78 provides a result which is of the same order than @xmath145 for the block diagonal ( n.d . ) model whereas still underperform with respect to both hierarchical clustering procedures for the hnfm ( n.d . ) .",
    "the success of @xmath86 can be explained by the fact that for @xmath144 the statistical uncertainty of sample correlations is of the order @xmath146 which is smaller than @xmath143 .",
    "therefore , for @xmath144 the non degeneracy of models becomes relevant as compared with the statistical uncertainty affecting sample correlations and spectral techniques result to be more capable than hierarchical clustering in taking into account such non degeneracy .",
    "this aspect is related to the fact that alca and slca are filtering procedures characterized by @xmath147 free parameters whereas spectral methods have a variable number of free parameters which is scaling as @xmath148 when t tends to infinity .    in summary",
    ", we have shown that hierarchical clustering procedures better reconstruct the degenerate models both for short and long time series , whereas for the non degenerate models the length of data series becomes relevant in the comparison .",
    "specifically , for short time series ( @xmath112 ) , such that the statistical uncertainty of correlations hides the heterogeneity of model parameters , we have observed that hierarchical clustering procedures , and in particular the alca , outperform spectral techniques . on the contrary , for data series long enough ( @xmath144 ) that the heterogeneity of model parameters is relevant with respect to the statistical uncertainty of sample correlations , spectral procedures result typically to be more efficient than hierarchical clustering procedures in reconstructing the correlation matrix of models .",
    "@xmath149 & @xmath150 & @xmath151 & @xmath152 + block diagonal & @xmath153 & @xmath154 & @xmath155 & @xmath156 + hnfm & @xmath157 & @xmath158 & @xmath159 & @xmath160 + block diagonal ( n.d . ) & @xmath161 & @xmath162 & @xmath163 & @xmath164 + hnfm ( n.d . ) & @xmath165 & @xmath166 & @xmath167 & @xmath168 +     @xmath149 & @xmath150 & @xmath151 & @xmath152 + block diagonal & @xmath169 & @xmath170 & @xmath171 & @xmath172 + hnfm & @xmath173 & @xmath174 & @xmath175 & @xmath176 + block diagonal ( n.d . ) & @xmath177 & @xmath178 & @xmath179 & @xmath180 + hnfm ( n.d . ) & @xmath181 & @xmath182 & @xmath183 & @xmath184 +      in this subsection we quantify the amount of information that different filtering procedures preserve when applied to sample correlation matrices .",
    "this is important in all those real cases when one does not know the model correlation matrix .",
    "moreover we investigate the stability of the filtered correlation matrices with respect to different realizations of the process .",
    "we use two quantities in order to evaluate the performance of the filtering procedures . the first quantity that we have measured is the kullback - leibler distance @xmath185 between the correlation matrix @xmath135 of the @xmath129-th sample and the filtered correlation matrix @xmath136 obtained by applying one of the filtering procedure to @xmath135 .",
    "@xmath185 is a measure of the information about @xmath135 that is stored in @xmath136 : the smaller @xmath185 , the larger is the amount of information about @xmath135 which is retained in the filtered matrix .",
    "the second quantity that we have considered is the kullback - leibler distance @xmath186 between two filtered matrices @xmath136 and @xmath187 obtained by applying the same filtering procedure to two different simulations ( or replicas ) @xmath129 and @xmath188 of the system .",
    "@xmath186 measures the statistical robustness of filtered matrices .",
    "the smaller @xmath186 , the greater is the stability of the filtering procedure with respect to the statistical uncertainty . in our estimations , we have averaged both @xmath185 and @xmath186 over the @xmath134 independent realizations or replicas of each system .",
    "axis ) against the amount of information about the correlation matrix that is retained in the filtered matrix ( @xmath189 axis ) .",
    "small values of @xmath190 and @xmath191 correspond to large stability and large amount of information preserved by the filtering respectively .",
    "the analysis is performed for a system of 100 elements divided in 12 orthogonal groups , each one depending on a specific gaussian factor , i.e. a block diagonal model .",
    "averages have been performed over 1000 independent realizations of the system and error bars correspond to one standard deviation.,scaledwidth=48.0% ]        in fig .",
    "[ infostabnohdeg ] , we show the results obtained for the block diagonal model with degenerate coefficients . in the figure we plot @xmath191 versus @xmath190 for all the described filtering procedures .",
    "averages that we indicate with the notation @xmath192 are performed over 1000 realizations and the series length is @xmath112 .",
    "error bars are one standard deviation . in all the cases presented in this paper",
    "we have verified that the error interval indicated around the mean value of plus and minus one standard deviation includes approximately the 67% of the realizations used to compute the mean value . in the figure we also report the result of an hypothetic perfect filtering procedure , i.e. a filtering techniques which is able to recover exactly the model from each realization . in the figure",
    ", we indicate the corresponding correlation matrix with @xmath11 .",
    "such a filtering is maximally stable , because it recovers always the correlation matrix of the block diagonal factor model .",
    "accordingly , it is @xmath193 .",
    "this perfect filtering procedure removes completely the noise due to the finite length of data series and therefore the quantity @xmath194 .",
    "instead , it is equal to the expectation value of eq .",
    "( [ kullexpecs1sig ] ) , i.e. @xmath195 for @xmath111 and @xmath112 .",
    "note that we know the position in the plane of the optimal filtering even if we do not know the underlying model .",
    "this is due to the important characteristic that the mean value of the kullback - leibler distance is independent from the model correlation matrix ( at least in the multivariate gaussian case ) . in the figure",
    ", we observe that all the filtering procedures , except the slca , retain in average more information about the sample correlation matrix than the true model , i.e. @xmath196 for @xmath42 equal to @xmath145 , @xmath197 and @xmath198 .",
    "this fact indicates that these filtering procedures do not discard completely the noise present in the sample correlation matrix as a consequence of the finite length of time series .",
    "the slca algorithm is the only one which is retaining less information about the sample correlation matrix than the true model .",
    "moreover the slca is more stable than all the other filtering procedures .    in fig .",
    "[ infostabhnfmdeg ] , we show the results obtained by applying the considered filtering procedures to the system described by the hnfm with @xmath127 factors and with degenerate coefficients . in this case , only the alca is retaining more information about the sample correlation matrix than the true model .",
    "however it is interesting to note that both the spectral techniques are at the same time less informative about the sample correlation matrix and less stable than both hierarchical clustering filtering procedures . in other words , for the degenerate hnfm , hierarchical clustering procedures clearly outperform spectral techniques .",
    "this fact is a consequence of the pure hierarchical nature of the hnfm . indeed in ref .",
    "@xcite , we have shown that when the hierarchical features of a system are prominent with respect to the details of specific correlation values , the spectral procedures have problems in filtering information about the system .",
    "such problems do not appear for separable systems , like the block diagonal model considered above .    in summary , for both the considered models we observe that hierarchical clustering techniques produce more stable filtered correlation matrices than spectral procedures .",
    "concerning the information about the sample correlation matrix that is stored in the filtering we observe that results obtained for hierarchical clustering procedures are closer to the perfect filtering ( giving as output the true model of the system ) than spectral techniques . finally , it is to notice that the slca is the most stable within the considered filtering procedures .",
    "such an excellent performance of hierarchical clustering techniques can be due to the degenerate nature of models as discussed in the first part of this section .",
    "in fact when we remove the degeneracy of coefficients from the models we observe a different behavior of filtering procedures . in fig .",
    "[ infostabnoh ] we plot @xmath191 versus @xmath199 for the artificial system obtained from a single realization @xmath130 with time series length @xmath139 of the factor model with 12 orthogonal factors .",
    "this is equivalent to consider a block model with non degenerate coefficients . in fig .",
    "[ infostabhnfm ] , we plot results obtained for the single realization with length @xmath139 of time series of the hnfm with 23 factors . also in this case our investigation is equivalent to consider a hnfm with non degenerate coefficients .",
    "mean values and error bars in the figures correspond to the average and the standard deviation respectively both estimated over 1000 bootstrap replicas of the single realization of the models . from figures [ infostabnoh ] and [ infostabhnfm ]",
    "we note that @xmath200 in both the figures , we observe that none of the filtering procedures is more informative about the sample correlation matrix than the true correlation matrix @xmath201 of both the models , i.e. @xmath202\\simeq 3.54 $ ] is smaller than any @xmath203 reported in the figures .    concerning the stability of the filtered matrices , from the figures we observe that the slca filtered matrix outperforms all the other techniques , although the filtered matrix given by alca has a stability of the same order of magnitude of the slca matrix .",
    "a good filtered correlation matrix should be at least more stable than the sample correlation matrix with respect to the statistical uncertainty .",
    "this sentence can be translated in the following inequality @xmath204 for gaussian variables we know the expected value of @xmath205 from eq .",
    "( [ kullexpecs1s2 ] ) and thus , for @xmath111 and @xmath112 , the last inequality becomes @xmath206 this condition is satisfied by all the considered filtered matrices .",
    "however we stress the fact that the matrices obtained from hierarchical clustering techniques and in particular the one obtained by slca have a value of @xmath190 of an order of magnitude smaller than the one expected for the pearson estimator of correlations .    in summary , our investigation of considered models shows that spectral filtering techniques are slightly more informative about the sample correlation matrix than hierarchical clustering filtering techniques when details about specific correlation values are relevant , like in the case of non degenerate models . on the contrary , from the point of view of stability of filtered matrices , hierarchical clustering procedures , and in particular the slca , outperform spectral techniques .      in this subsection",
    ", we compare the filtering procedures when applied to real data .",
    "we have considered the system of daily returns of the 100 most capitalized stocks traded at nyse in the time period from january 2001 to december 2003 . in this case , the length of the @xmath111 time series is @xmath112 records .",
    "we have also considered the system of daily returns of 92 highly capitalized stocks traded at london stock exchange in 2002 . for this system",
    "the record length of the @xmath113 time series is @xmath114 .     for this system .",
    "averages have been performed over 1000 bootstrap replicas of data series and error bars correspond to one standard deviation.,scaledwidth=48.0% ]     for this system .",
    "averages have been performed over 1000 bootstrap replicas of data series and error bars correspond to one standard deviation.,scaledwidth=48.0% ]    in fig .",
    "[ infostabnyse100 ] , we report the results obtained by applying all the considered filtering procedures to the system of @xmath111 stocks traded at nyse , while in fig .",
    "[ infostablse92 ] we show the results obtained for the system of @xmath113 stocks traded at lse . in both the figures , we observe that hierarchical clustering procedures are more stable than spectral techniques , whereas the latter are more informative about the sample correlation matrix than hierarchical clustering .",
    "these facts are in agreement with results obtained for simulations in the case of non degenerate models .",
    "however this agreement is only qualitative . indeed , both the values of @xmath191 and @xmath190 observed for the real systems are larger than the corresponding values obtained in the case of simulations .",
    "this fact can be due to two effects .",
    "the first one is related to the fact that the real systems can be characterized by a structure of correlations more complex than the one considered in the models .",
    "for example , the role of the complexity of correlation structures onto the performance of filtering procedures was observed in the simulations of the degenerate models of subsection [ filtmod ] for the spectral techniques . indeed",
    "the performance of such procedures was rather unsatisfactory for the hnfm with respect to the block diagonal model .",
    "the second effect that can be responsible for the quantitative difference between results obtained for simulations and results obtained for real data can be related to the fact that we have considered gaussian variables in the simulations , whereas the distribution of returns is fat tailed @xcite .",
    "some quantitative differences are also evident in the comparison of the two real systems .",
    "specifically , both the values of @xmath191 and @xmath190 are larger in the lse data with respect to the nyse data .",
    "this difference is mainly due to the different length of data series , i.e. @xmath112 at nyse and @xmath114 at lse .",
    "the smaller @xmath23 , the larger is the statistical uncertainty of the sample correlation matrix .",
    "for instance , we can make quantitative this difference by using the expectation values of the kullback - leibler distance of eq.s ( [ kullexpecs1sig ] ) and ( [ kullexpecs1s2 ] ) . for a system of @xmath111 elements with data",
    "series of length @xmath112 we have @xmath207\\simeq3.54 $ ] and @xmath208\\simeq7.81 $ ] , whereas for a system of @xmath113 elements and series length @xmath114 is @xmath207\\simeq9.86 $ ] and @xmath208\\simeq27.2 $ ] . a comparison of the results obtained for gaussian random models in subsection [ samplesfilt ] with the results obtained for the real systems investigated in this subsection shows that the kullback - leibler distance provides results on real data about the relative effectiveness of the considered filtering procedures which are in agreement with those observed for models .",
    "in conclusion we have shown that the kullback - leibler distance can be fruitfully used to compare correlation matrices of multivariate data .",
    "we have shown that this distance is more appropriate to achieve this objective than the standard frobenius distance .",
    "this fact is due to some properties of the kullback - leibler distance such as the asymmetry , the model independence of expectation values and its relation with the maximum likelihood factor analysis .",
    "sample correlation matrices can be compared in pairs among them and/or with respect to model matrices or to filtered matrices .",
    "we have used the kullback - leibler distance to compare four different techniques used to obtain a filtered correlation matrix from the empirical one .",
    "two of the four techniques that we have analyzed are spectral filtering procedures based on random matrix theory whereas the other two techniques are based on hierarchical clustering methods , specifically alca and slca .",
    "results obtained for simulations are consistent with those obtained for real data .",
    "these results can be summarized as follows : both the considered spectral techniques are slightly more informative about the sample correlation matrix than the other two techniques based on hierarchical clustering . on the other hand both the techniques based on hierarchical clustering",
    "are producing filtered correlation matrices which are more stable than those obtained with spectral procedures .",
    "these results show that the kullback - leibler distance is very useful in characterizing multivariate systems described by real data , factor models and matrices filtered from the sample one . in conclusion ,",
    "the kullback - leibler distance is a powerful and accurate tool able to characterize the information and stability of sample , model and filtered correlation matrices and it is a useful quantitative indicator for the relative amount of information and the relative stability of correlation matrices of multivariate data .",
    "we acknowledge partial support from miur research project `` dinamica di altissima frequenza nei mercati finanziari '' and nest - dysonet 12911 eu project .",
    "in this appendix , we show how to derive eq .",
    "( [ kullbackgaussian ] ) from eq .",
    "( [ kullbackmulti ] ) .",
    "let us consider the multivariate gaussian distributions @xmath13 and @xmath14 describing the same random vector @xmath4 .",
    "we have @xmath209 by substituting eq .",
    "( [ multigauss ] ) into eq .",
    "( [ kullbackmulti ] ) we get @xmath210 where @xmath211    the integral @xmath212 can be solved by using the linear transformation @xmath213 , where @xmath214 is the orthogonal matrix which diagonalizes @xmath215 .",
    "it results that @xmath216 where @xmath217 @xmath218 are the elements of the diagonal matrix @xmath219 , whereas @xmath220 @xmath218 are the diagonal elements of the matrix @xmath221 .",
    "we can further simplify the expression of @xmath212 by taking into account the fact that the matrix @xmath219 is diagonal . indeed @xmath222 $ ]",
    "@xmath223 @xmath224 $ ] due to the orthogonality of @xmath214 and to the invariance of the trace with respect to rotations .",
    "accordingly , we obtain that @xmath225.\\ ] ]    finally , we obtain the expression of @xmath21 given in eq .",
    "( [ kullbackgaussian ] ) by substituting the last expression of @xmath212 into eq .",
    "( [ pass1kull ] ) and noting that @xmath226=n$ ] .",
    "in this appendix , we derive the expectation values of the kullback - leibler distance given in eq.s ( [ kullexpecsigs1]-[kullexpecs1s2 ] ) .",
    "we shall use two known results from the theory of wishart matrices .",
    "let us consider a multinormally distributed random vector @xmath4 of dimension @xmath17 with correlation matrix @xmath11 .",
    "let @xmath30 and @xmath31 be two sample correlation matrices obtained from two independent realizations of the system , @xmath227 and @xmath228 respectively both of length @xmath23 .",
    "the first result from the theory of wishart matrices that we shall use hereafter is that @xmath229 , @xmath230 is equal to @xmath231 plus the sum of the logarithms of @xmath17 mutually independent chi - squared random variables @xmath232 with degrees of freedom @xmath233 respectively ( see for instance @xcite ) .",
    "this fact implies that the expectation value of @xmath229 is @xmath234.\\ ] ] because @xmath235=\\gamma^{\\prime}(p/2)/\\gamma(p/2)+\\log(2)$ ] ( see for instance @xcite ) we obtain that : @xmath236            where we have again used the linearity of the trace and the fact that @xmath240 . by using eq.s ( [ logexpecsec ] ) and ( [ expectrace ] )",
    "it is now straightforward to obtain both the expression of @xmath241 $ ] as given in eq .",
    "( [ kullexpecsigs1 ] ) and the expectation value @xmath208 $ ] as given in eq .",
    "( [ kullexpecs1s2 ] ) .",
    "finally , by using results of eq.s ( [ logexpecsec ] ) and ( [ expectracesigmas1 ] ) we obtain the expectation value of @xmath37 as given in eq .",
    "( [ kullexpecs1sig ] ) ."
  ],
  "abstract_text": [
    "<S> we show that the kullback - leibler distance is a good measure of the statistical uncertainty of correlation matrices estimated by using a finite set of data . for correlation matrices of multivariate gaussian variables we analytically determine the expected values of the kullback - leibler distance of a sample correlation matrix from a reference model and </S>",
    "<S> we show that the expected values are known also when the specific model is unknown . </S>",
    "<S> we propose to make use of the kullback - leibler distance to estimate the information extracted from a correlation matrix by correlation filtering procedures . </S>",
    "<S> we also show how to use this distance to measure the stability of filtering procedures with respect to statistical uncertainty . </S>",
    "<S> we explain the effectiveness of our method by comparing four filtering procedures , two of them being based on spectral analysis and the other two on hierarchical clustering . </S>",
    "<S> we compare these techniques as applied both to simulations of factor models and empirical data . </S>",
    "<S> we investigate the ability of these filtering procedures in recovering the correlation matrix of models from simulations . </S>",
    "<S> we discuss such an ability in terms of both the heterogeneity of model parameters and the length of data series . </S>",
    "<S> we also show that the two spectral techniques are typically more informative about the sample correlation matrix than techniques based on hierarchical clustering , whereas the latter are more stable with respect to statistical uncertainty . </S>"
  ]
}