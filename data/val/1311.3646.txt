{
  "article_text": [
    "the demand for video streaming services such as those offered by netflix , youtube , amazon , and others , is growing rapidly .",
    "this places a significant burden on networks .",
    "one way to mitigate this burden is to place memories into the network that can be used to cache files that users may request . in this paper , we investigate how to optimally use these caches . in particular , we are interested in _ online _ algorithms for this problem , in which the operations of the cache have to be performed on the fly and without knowledge of future requests .",
    "the online caching problem ( also known as the paging problem in the context of virtual memory systems ) has a long history , dating back to the work by belady in 1966  @xcite .",
    "this problem has been investigated both for systems with a single cache  @xcite as well as for systems with multiple distributed caches @xcite .",
    "one solution to the caching problem that is popular in practice and for which strong optimality guarantees can be proved  @xcite is the _ least - recently used _",
    "( lru ) eviction policy . in lru , each cache is continuously updated to hold the most recently requested files , allowing it to exploit the temporal locality of content requests .",
    "the figure of merit adopted by the papers mentioned so far is the cache - miss rate ( or page - fault rate in the context of the paging problem ) , sometimes weighted by the file size .",
    "this cache - miss rate is used as a proxy for the network load . for systems with a _ single _",
    "cache , the weighted cache - miss rate and the network load are indeed proportional to each other , and hence minimizing the former also minimizes the latter .",
    "however , this proportionality no longer holds for systems with _ multiple _ caches . for such systems with multiple caches , a fundamentally different so - called _ coded caching",
    "approach is required .",
    "this coded caching approach has been recently introduced in  @xcite for the _ offline _ caching problem .    in this paper",
    ", we investigate _ online _ coded caching , focusing on a basic content distribution scenario consisting of a single origin server connected through a shared ( bottleneck ) link to a number of users each equipped with a cache of finite memory ( see fig .  [",
    "fig : setting ] ) . the users issue a sequence of content requests from a set of popular files , and the goal is to operate the caches as well as the server such as to satisfy these requests with the minimum number of bits sent over the shared link .",
    "we consider the case where the set of popular files evolve according to a markov model and users select their demand uniformly from this set .",
    "files of size @xmath0 bits each is connected through a shared link to @xmath1 users each with a cache of size @xmath2 bits . in the figure , @xmath3 and @xmath4 . ]",
    "we approximately characterize the optimal long - term average rate of the shared link for this setting .",
    "we show further that the optimal online scheme performs approximately the same as the optimal offline scheme .",
    "this is perhaps surprising , since in the offline scheme caches are allowed to be updated in an offline fashion each time the set of popular files changes , whereas in the online scheme caches are updated in an online fashion based solely on the limited observations they have through the sequence of requests .    to evaluate the gain of coded caching in practical scenarios , we propose an online coded caching scheme termed _ coded least - recently sent _",
    "( lrs ) and simulate it on a demand time series derived from the dataset made available by netflix for the netflix prize . for this time series , we show that the proposed coded lrs algorithm significantly outperforms the baseline lru algorithm .",
    "the remainder of this paper is organized as follows .",
    "section  [ sec : background ] provides background information on coded caching .",
    "section  [ sec : problem ] formally introduces the problem setting .",
    "section  [ sec : main ] contains the main results .",
    "the proof of these results are provided in section [ sec : proof1 ] .",
    "coded caching , recently introduced in @xcite , is a novel approach to the distributed caching problem .",
    "it can achieve a significant reduction in network load by creating and exploiting coded multicasting opportunities between users with different demands .",
    "we make essential use of the offline coded caching scheme from  @xcite in the present paper .",
    "therefore , we now briefly overview that algorithm and illustrate it with an example .",
    "the setting in  @xcite is the offline version of the one depicted in fig .",
    "[ fig : setting ] in section  [ sec : intro ] . in particular , a single origin server is connected to @xmath1 users through a shared link .",
    "there is a fixed set of @xmath5 files of length @xmath0 bits , and each user has a memory of size @xmath2 bits with @xmath6 .",
    "the cache memories are prefetched in an _ offline _ fashion during a placement phase ( during a period of low network load , say the early morning ) so as to minimize the peak load @xmath7 over the shared link during a later delivery phase ( say in the evening ) during which each user requests a single file .",
    "we refer to the normalized peak load @xmath8 as the peak rate .",
    "the offline coded caching scheme proposed in  @xcite achieves a peak rate of @xmath9 which is shown to be within a constant factor of the optimal rate .    in the placement phase of the algorithm in  @xcite , each user caches a random subset of @xmath10 bits of each of the @xmath11 files . in the delivery phase , the server sends an appropriate linear combination of those bits over the shared link to enable all users to recover the requested files , as is illustrated in the following example .",
    "[ eg : dec ] consider the caching problem with @xmath12 files say @xmath13 and @xmath14 users , each with a cache of size @xmath2 . in the placement phase",
    ", each user caches @xmath15 bits of each file independently at random , satisfying the memory constraint .",
    "we partition @xmath16 where @xmath17 denotes the bits of file @xmath18 that are stored at the users in the set @xmath19 , and similarly for @xmath20 . for @xmath0 large enough",
    ", the size of the subfile @xmath17 tends to @xmath21 bits by the law of large numbers .    in the delivery phase , suppose that users one and two request files @xmath18 and @xmath20 , respectively .",
    "user one already has access to the file parts @xmath22 and @xmath23 of its requested file , and needs @xmath24 and @xmath25 , which are not cached its memory .",
    "similarly user two already has access to @xmath26 and @xmath27 of its requested file and needs @xmath28 and @xmath29 .",
    "the server can then satisfy these user requests by sending @xmath24 , @xmath28 , and @xmath30 over the shared link , where @xmath31 denotes the xor operation applied element - wise to to @xmath25 and @xmath32 treated as vectors of bits .",
    "observe that user one has @xmath32 stored in its cache . from this and",
    "the output @xmath30 of the shared link , user one can recover the desired file part @xmath25 .",
    "similarly , user two has @xmath25 stored in its cache and can use this to recover the desired file part @xmath32 from the output @xmath30 of the shared link .",
    "thus , using the contents of their caches and the outputs of the shared link , both user can recover all the required subfiles .",
    "the rate over the shared link is @xmath33 where @xmath8 is defined in  .",
    "while here the delivery phase is explained for a specific set of user requests , one can verify that this rate is achievable for all other possible requests as well .    the main gain from using this scheme derives from the coded multicasting opportunities between users with different demands .",
    "these coded multicasting opportunities are created in the placement phase and are exploited in the delivery phase . as the size @xmath34 of the cache memories increases , this coded multicasting gain increases as well .",
    "this gain , called the _",
    "gain in  @xcite , is captured by the factor @xmath35 in @xmath8 .",
    "there is a second , _ local _ , gain deriving from having part of the requested file available at a users local cache .",
    "this local gain is captured by the factor @xmath36 in @xmath8 . as is shown in  @xcite ,",
    "this local gain is usually less significant than the global coded multicasting gain .",
    "we consider a content distribution system with a server connected through a shared , error - free link to @xmath1 users as illustrated in fig .",
    "[ fig : setting ] in section  [ sec : intro ] . at time @xmath37",
    ", each user @xmath38 requests a file @xmath39 from a time - varying set @xmath40 of popular files with cardinality @xmath5 .",
    "the @xmath1 user s requests , collectively denoted by the vector @xmath41 , are chosen uniformly at random without replacement from @xmath40 .",
    "each file has size @xmath0 bits , and each user is equipped with a cache memory of size @xmath2 bits .",
    "the content distribution system operates as follows . at the beginning of each time slot @xmath42 ,",
    "the users reveal their requests @xmath41 to the server .",
    "the server , having access to the database of all the files in the system , responds by transmitting a message of size @xmath43 bits over the shared link . using their cache contents and the message received over the shared link , each user @xmath38 aims to reconstruct its requested file @xmath39 .    the goal is to design the actions of the users and the server such as to minimize the long - term average rate @xmath44 of the system , i.e. , @xmath45 while satisfying the memory and reconstruction constraints .",
    "observe that the rate @xmath44 is the long - term average load @xmath46 over the shared link normalized by the file size @xmath0 . in order to obtain a rate @xmath44 independent of the file size and to simplify the analysis",
    ", we allow the file size @xmath0 to be as large as needed .    in this paper , we are interested in _ online _ caching schemes , which place additional restrictions on the actions of the server and the caches . in such online schemes , the cache content at user @xmath38 at the beginning of time",
    "slot @xmath42 is a function of the cache content at the same user at the previous time @xmath47 , the output of the shared link at time @xmath47 , and the requests @xmath48 up until time @xmath47 .",
    "in particular , the cache content may _ not _ be a function of the outputs of the shared link at times prior to @xmath47 .",
    "furthermore , for an online scheme , the message sent by the server over the shared link at time @xmath42 is a function of only the demands @xmath49 and the cache contents of the users at that time .",
    "we denote by @xmath50 the long - term average rate over the shared link of the optimal online caching scheme .",
    "[ eg : lru ] a popular online caching scheme is _ least - recently used _ ( lru ) . in this scheme ,",
    "each user caches @xmath34 entire files .",
    "when a user requests a file that is already contained in its cache , that request can be served out of the cache without any communication from the server .",
    "when a user requests a file that is not contained in the cache , the server sends the entire file over the link .",
    "the user then evicts the least - recently requested ( or used ) file from its cache and replaces it with the newly requested one .",
    "observe that this is a valid online caching strategy .",
    "indeed , the content of a user s cache at the beginning of time slot @xmath42 is a function of the cache content at time @xmath47 , the output of the shared link at time @xmath47 , and the past requests ( in order to determine which file was least - recently used ) .",
    "moreover , the message sent by the server at time @xmath42 is only a function of the demands @xmath41 and the cache contents of the users at that time ( in order to decide if a file needs to be transmitted at all ) .",
    "we will adopt lru as the baseline scheme throughout this paper .",
    "we next provide a formal description of the dynamics of the set of popular files @xmath40 .",
    "the initial set @xmath51 consists of @xmath11 distinct files .",
    "the set @xmath52 at time @xmath42 evolves from the set @xmath53 at time @xmath47 using an arrival / departure process . with probability @xmath54",
    ", there is no new arrival and @xmath55 . with probability @xmath56 ,",
    "there is a new arrival , and the set @xmath52 is constructed by choosing a file uniformly at random from @xmath53 and replacing it with a new , so far unseen , file .",
    "note that this guarantees that @xmath57 for all @xmath42 .",
    "[ eg : popular ] consider a system with @xmath12 popular files .",
    "a possible evolution of the set @xmath40 of popular files is as follows .",
    "@xmath58 : : :    the initial set of popular files is @xmath59 .",
    "@xmath60 : : :    there is an arrival .",
    "the file @xmath61 is randomly chosen and    replaced with the new file @xmath62 , so that    @xmath63 . @xmath64 : : :    there is no arrival , and @xmath65 .",
    "we start by introducing a new , online , coded caching algorithm in section  [ sec : main_algo ] .",
    "a simplified variant of this algorithm is shown in section  [ sec : main_theory ] to have performance close to the optimal online caching scheme .",
    "section  [ sec : main_sim ] provides simulation results comparing the proposed coded caching algorithm to the baseline lru scheme for an empirical demand time series derived from the netflix prize database .      in this section ,",
    "we propose an online version of the caching algorithm in @xcite , which we term _ coded least - recently sent _ ( lrs ) .",
    "the coded lrs algorithm is presented formally in the listing algorithm  [ alg:1 ] .",
    "the statement of the algorithm uses the shorthand @xmath66 $ ] for @xmath67 .",
    "[ alg:1_sloop ] [ alg:1_sloop ] server sends @xmath68 [ alg : send ] user @xmath38 replaces the least recently sent file in its cache with a random subset of @xmath69 bits of file @xmath70 [ alg:1_cache ]    algorithm  [ alg:1 ] consists of a delivery procedure and a cache update procedure .",
    "we now explain those two procedures in detail .",
    "the delivery procedure is formally similar to the delivery procedure of the decentralized caching algorithm in @xcite .",
    "@xmath71 denotes the bits of the file @xmath39 requested by user @xmath38 cached exclusively at users in @xmath19 . in other words , a bit of file",
    "@xmath39 is in @xmath71 if it is present in the cache of every user in @xmath19 and if it is absent from the cache of every user outside @xmath19 .",
    "the xor operation @xmath31 in line  [ alg : send ] is to be understood as being applied element - wise to @xmath71 treated as a vector of bits .",
    "if those vectors do not have the same length , they are assumed to be zero padded for the purposes of the xor .",
    "we thus see that the delivery procedure of the coded lrs algorithm consists of sending one linear file combination for each subset @xmath19 of users .",
    "it is worth pointing out that , whenever a requested file is not cached at any user , then @xmath72 is equal to the entire requested file , and hence when @xmath73 in line  [ alg:1_sloop ] the delivery procedure sends in this case the entire requested file uncoded over the shared link .",
    "consider next the cache update procedure . in each time slot @xmath42 , the users maintain a list of @xmath74 partially cached files for some @xmath75 .",
    "the parameter @xmath76 can be chosen to optimize the caching performance ; a simple and reasonable choice is @xmath77 . at time",
    "@xmath42 , after the delivery procedure is executed , the caches are updated as follows .",
    "if a requested file @xmath70 of _ any _ user @xmath78 is not currently partially cached , _ all _ users evict the least - recently used file and replace it with @xmath79 randomly chosen bits from file @xmath70 .",
    "this is feasible since the uncached file @xmath70 was sent uncoded over the shared link during the delivery procedure .",
    "note that this update procedure guarantees that the number of partially cached files remains @xmath80 , and that the same files ( but not necessarily the same bits ) are partially cached at each of the @xmath1 users .",
    "we illustrate the proposed coded lrs algorithm with an example .",
    "this example also illustrates that the rate of the proposed scheme can be related to the rate @xmath8 defined in   of the decentralized caching algorithm from  @xcite .",
    "we consider again a system with @xmath12 popular files and assume the same popular - file dynamics as in example  [ eg : popular ] in section  [ sec : problem ] .",
    "assume there are @xmath14 users with a cache memory of @xmath4 .",
    "let @xmath81 so that each user caches a fraction @xmath82 of @xmath83 files .",
    "we assume that initially each user partially caches the files @xmath84 .",
    "@xmath58 : : :    the set of popular files is @xmath59 .",
    "assume    the users request @xmath85 .",
    "both of the requested    files are partially cached at the users . in the delivery procedure",
    ",    the server sends @xmath86 , @xmath87 ,    and @xmath88 . for @xmath0 large enough ,",
    "so    that each of these file parts has close to expected size ( as discussed    in example  [ eg : dec ] in section  [ sec : background ] ) , this results in a    rate of    @xmath89 with    @xmath8 as defined in  .",
    "since all the requested files    are already partially cached , the set of cached files stays the same    in the cache update procedure .",
    "in other words , each user still    partially caches @xmath90 . @xmath60 : : :    the set of popular files changes to @xmath91 . assume the users request    @xmath92 . here , file @xmath20 is partially    cached at the users but file @xmath62 is not .",
    "the server sends    @xmath86 , @xmath93 , and    @xmath94 .",
    "since @xmath62 is not cached at    any of the users , we have in this case that    @xmath95 and @xmath96 .",
    "hence , the transmission of the server is    equivalently @xmath86 , @xmath62 and    @xmath29 .",
    "this results in a rate of    @xmath97    since @xmath18 is the least - recently sent file , it is evicted    from each cache and replaced by a random third of the file    @xmath62 .",
    "the new set of partially cached files is    @xmath98 . @xmath64",
    ": : :    the set of popular files stays the same @xmath99 .",
    "assume the users request    @xmath100 , both of which are now partially cached at    the users .",
    "the server now sends @xmath86 ,    @xmath93 , and @xmath94 . unlike the    previous time step , @xmath101 is now no longer empty , and the    resulting rate is @xmath102 as calculated    before .",
    "the set of partially cached files stays the same , namely    @xmath103 .",
    "it is worth comparing the proposed coded lrs algorithm with the well - known lru algorithm described in example  [ eg : lru ] in section  [ sec : problem ] .",
    "both of them are online algorithms . however , there are three key differences .",
    "first , coded lrs uses a coded delivery procedure whereas the transmissions in lru are uncoded .",
    "second , coded lrs caches many ( @xmath80 ) partial files whereas lru caches fewer ( @xmath34 ) whole files .",
    "third , coded lrs uses a least - recently sent eviction rule , taking into account the files requested by all users jointly , compared to the least - recently used eviction rule , taking into account only the files requested by every user individually .",
    "the impact of these differences will be explored in more detail later .",
    "the main result of this paper is the following theorem .",
    "[ thm1 ] the long - term average rate @xmath50 of the optimal online caching scheme satisfies @xmath104 where @xmath105    the proof of theorem  [ thm1 ] is presented in section [ sec : proof1 ] .",
    "the upper bound in theorem  [ thm1 ] results from the analysis of a simplified version of the proposed coded lrs caching scheme , showing that this algorithm is approximately optimal .",
    "for the lower bound , we use the rate of the optimal offline scheme , whose rate is approximately @xmath8 .",
    "the theorem thus implies that the rate of the optimal online caching scheme is approximately the same as the rate of the optimal offline scheme .",
    "recall that in an offline scheme , the cache memories are given access to the entire set of popular files each time it changes .",
    "moreover , these cache updates are performed offline , meaning that the data transfer needed to update and maintain the caches is not counted towards the load of the shared link .",
    "in contrast , in the online scenario , caches are updated based on the limited observations they have through the sequence of demands . moreover , the cache updates is performed through the same shared link , and therefore affects the average rate .",
    "theorem  [ thm1 ] thus indicates that these significant restrictions on the online problem have only a small effect on the rate compared to the offline scheme .",
    "popular files , @xmath106 users , and arrival probability @xmath107 .",
    "the figure plots the long - term average rate @xmath44 over the shared link as a function of cache memory size @xmath34 for lru ( dashed green ) , uncoded lrs ( dashed black ) , and the proposed coded lrs ( solid blue ) . ]",
    "we now compare the proposed coded lrs scheme to the baseline lru scheme .",
    "the performances of these two schemes are shown in fig .",
    "[ fig : synthetic ] for a system with @xmath108 popular files , @xmath106 users , and arrival probability @xmath107 . as is visible from the figure , coded lrs provides significant gains over lru both for small and large memory sizes . for example , for @xmath109 ( meaning that the cache is large enough to hold @xmath110 of the popular files ) , lru results in a rate of @xmath111 ( meaning that we need to send the equivalent of @xmath111 files over the shared link on average ) , whereas coded lrs results in a rate of @xmath112 .",
    "similarly , for @xmath113 , lru results in a rate of @xmath114 , whereas coded lrs results in a rate of @xmath115 .",
    "as mentioned in section  [ sec : main_algo ] , the three main differences between coded lrs and lru are coded delivery , partial caching , and lrs eviction . to get a sense of the impact of these three differences , fig .",
    "[ fig : synthetic ] also depicts the performance of the uncoded lrs scheme . in this scheme ,",
    "@xmath34 whole files are cached and uncoded delivery is used ( as in lru ) , however the lrs eviction rule is used ( unlike in lru ) .    comparing ( uncoded ) lrs to lru",
    ", we see that the two schemes perform quite similarly for small and moderate values of @xmath34 , say @xmath116 . for large values of @xmath34 ,",
    "say @xmath117 , lrs provides a significant improvement over lru .",
    "this is because when @xmath34 is close to the number of popular files @xmath11 , the rate is dominated by the arrival of new popular files , and lrs eviction allows the caches to learn these new files with fewer cache misses than lru .    comparing uncoded lrs to coded lrs , we see that only when @xmath34 is very close to the number of popular files @xmath11 are the performances of the schemes similar . for all other values ,",
    "coded lrs significantly outperforms uncoded lrs .",
    "this implies that , except for large values of @xmath34 , the main gain of the coded lrs scheme derives from the partial caching of many files and from the coded delivery .      to validate the theoretical analysis in section  [ sec : main_theory ] as well as our model for the evolution of popular files",
    ", we now evaluate the performance of the proposed coded lrs and the baseline lru schemes for a real - life time series of demands .",
    "this time series is derived from the dataset made available by netflix for the netflix prize as follows .",
    "each entry in the netflix dataset consists of a user i d , a movie i d , a time stamp , and a rating that the user gave to the movie at the specified time .",
    "we are not interested in the rating here , but would like to use the time of rating a movie as a proxy for the time of viewing that movie .",
    "this approach is problematic for old movies , which users may rate long after they have seen them .",
    "however , it is reasonable to expect that the rating time is close to the viewing time for recently released movies . to ensure that this is the case , we selected all user ratings in the database from the year 2005 ( the last full year for which ratings are available ) , and kept only those that are for movies released in either 2004 or 2005 .",
    "the resulting filtered time series contains about @xmath118 user ratings for @xmath119 unique movies .        to validate this approach , fig .",
    "[ fig : troy_treasure ] plots the number of ratings for the two most - rated movies ( `` troy '' and `` national treasure '' ) as a function of time measured in weeks .",
    "the movie `` troy '' was released on dvd on january 4 , 2005 ( at which time it was likely also available on netflix ) , corresponding to week 1 .",
    "the movie `` national treasure '' was released on dvd on may 3 , 2005 , corresponding to week 18 .",
    "as can be seen from fig .",
    "[ fig : troy_treasure ] , the number of ratings increases strongly on the dvd release week , stays relatively high for a number of weeks , and then drops .",
    "this suggests that the rating time is indeed a valid proxy for the viewing time when applied to recently released movies .",
    "it also suggests that the model of time - varying popular files described in section  [ sec : problem ] and used for the theoretical analysis in section  [ sec : main_theory ] is a reasonable model for the viewing behavior of users .",
    "the figure plots the long - term average rate @xmath44 over the shared link as a function of cache memory size @xmath34 for lru ( dashed green ) and the proposed coded lrs ( solid blue ) . ]",
    "[ fig : netflix ] compares the performance of the proposed coded lrs scheme to the baseline lru scheme for the netflix demand time series for a system with @xmath120 caches ( each in this case corresponding to many users that are attached to it ) .",
    "as is visible from the figure , coded lrs again significantly outperforms lru .",
    "in particular , for a cache size of @xmath121 , lru achieves a rate of @xmath122 compared to a rate of @xmath123 for coded lrs .",
    "to prove theorem  [ thm1 ] , we establish an upper bound ( section  [ sec : proof1_upper ] ) and a lower bound ( section  [ sec : proof1_lower ] ) on the optimal long - term average rate @xmath50 .      for the upper bound on @xmath50 , we analyze a simplified version of the proposed coded lrs scheme .",
    "we refer to this simplified scheme as _ coded random eviction_. in coded random eviction , if @xmath124 files are requested by users at time @xmath42 that are not currently partially cached , then @xmath125 of the cached files are randomly chosen , evicted from _ all _ cache memories , and replaced with @xmath79 randomly chosen bits from each of the @xmath124 newly requested files .",
    "observe that this guarantees that the same collection of files is partially cached at each user .",
    "the remainder of the algorithm is the same as coded lrs as listed in algorithm  [ alg:1 ] .",
    "in particular the coded random - eviction algorithm partially caches @xmath126 files for a positive constant @xmath75 at each user , where @xmath11 is the cardinality of the set of popular files @xmath40 .",
    "recall that , according to the system model described in section [ sec : problem ] , at each time slot @xmath42 , the users request @xmath1 randomly chosen files from the set of popular files @xmath40 without replacement . at any time @xmath42 , not all files in @xmath40 may be partially cached at the users . as in the previous paragraph , we denote by @xmath124 the ( random ) number of uncached files requested by the users at time @xmath42 . by definition",
    ", @xmath124 takes value in set @xmath127 .    the delivery procedure in algorithm  [ alg:1 ]",
    "transmits these @xmath124 files uncoded over the shared link . to send the remaining @xmath128 files that are partially cached at the users , the delivery procedure of algorithm  [ alg:1 ] uses coding",
    "this requires a rate of @xmath129 as described in section  [ sec : background ] , and with @xmath130 as defined in theorem  [ thm1 ] and in  .",
    "thus , the rate @xmath131 over the shared link at time @xmath42 is @xmath132    the long - term average rate @xmath44 of coded random eviction is therefore upper bounded by @xmath133 to prove theorem  [ thm1 ] , we show that the first term in   is approximately @xmath134 and that the second term is upper bounded by a constant .",
    "this second upper bound is perhaps surprising , since @xmath124 itself can take any value up to @xmath1 and is hence not upper bounded by a constant independent of the problem parameters .",
    "we start with the analysis of the second term in  .",
    "let the random variable @xmath135 denote the number of files in @xmath40 that are partially stored in the caches at the beginning of time slot @xmath42 .",
    "note that @xmath135 takes value in @xmath136 .",
    "conditioned on @xmath135 , the random variable @xmath124 has expected value @xmath137 therefore , @xmath138 in what follows , we investigate the random process @xmath139 .",
    "[ thm : ergodic ] @xmath139 is a markov process and has a unique stationary distribution @xmath140 .",
    "moreover , @xmath141 where @xmath142 is distributed according to @xmath143 .    due to the nature of the random - eviction algorithm , and due to the memoryless arrivals and departures to the set of popular files @xmath40 , @xmath144 is a markov process .",
    "it is easy to see that this markov process has a single ergodic recurrent class consisting of the states @xmath145 and has transient states @xmath146 .",
    "therefore , @xmath144 has a unique stationary distribution @xmath143 .    from fubini s theorem and the properties of @xmath147 , we have @xmath148 since @xmath149 by ergodicity , we obtain from the cesro - mean theorem that @xmath150 this implies that @xmath151 completing the proof .    applying lemma  [ thm : ergodic ] to   yields that @xmath152 to establish the upper bound on theorem  [ thm1 ]",
    ", it thus remains to lower bound @xmath153 .",
    "this is done in the next lemma .",
    "[ thm : quad ] let @xmath142 be as in lemma  [ thm : ergodic ]",
    ". then @xmath154    to analyze @xmath153 , we will need a more detailed understanding of the random process @xmath144 .",
    "we define two auxiliary processes @xmath155 , and @xmath156 , both for @xmath157 .",
    "@xmath155 is the number of randomly evicted files from the caches at the end of time slot @xmath42 that are in @xmath52 . in other words",
    ", @xmath155 counts the number of wrongly evicted files .",
    "@xmath156 is the indicator random variable of the event that , at the end of time slot @xmath42 , there is a departure from the set @xmath40 of popular files and that the departing file is partially stored in the caches at the end of time slot @xmath42 ( i.e. , after the cache update ) .    using these auxiliary processes , we can write the following update equation for process @xmath135 : @xmath158 in words ,   states that the number of correctly cached files at time @xmath159 is equal to the number @xmath135 of correctly cached files at time @xmath42 , plus the number of newly requested and cached files @xmath124 , minus the number of wrongly evicted files @xmath155 , minus the number of files @xmath156 that were correctly cached at the end of time slot @xmath42 but are no longer popular at time @xmath159 .",
    "[ eg : update ] consider a scenario with @xmath160 users and @xmath161 files .",
    "let @xmath162 and assume at time @xmath58 the files @xmath84 are partially cached .",
    "then @xmath163 , since the overlap is file @xmath61 .",
    "assume the users request @xmath164 at time @xmath58 .",
    "then @xmath165 , since two uncached files @xmath62 and @xmath166 are requested . to accommodate the two new files , we randomly evict two cached files .",
    "assume those files are @xmath20 and @xmath61 so that the cached files at the end of time slot @xmath42 are @xmath167 .",
    "then @xmath168 , since file @xmath61 in @xmath51 is evicted from the caches .",
    "finally assume that file @xmath62 is randomly selected to depart from @xmath51 and is replaced by the new file @xmath0 , so that @xmath169",
    ". then @xmath170 , since the departed file @xmath62 is cached at the end of time slot @xmath42 .",
    "finally , @xmath171 , since file @xmath166 is both popular and cached .",
    "this satisfies @xmath172    to establish a lower bound on @xmath153 we use the update equation   instead of directly computing the stationary distribution @xmath143 , which is not tractable .",
    "assume that the process @xmath144 is started in steady - state .",
    "in other words , @xmath135 has distribution @xmath143 for every @xmath37 .",
    "then , taking expectations on both sides of , @xmath173 since @xmath174 , this simplifies to @xmath175    we now calculate the two expectations in  .",
    "we start with the left expectation .",
    "consider the number of correctly cached files @xmath176 at the end of time slot @xmath42 just after the caches are updated but before any departures from the set of popular files @xmath40 .",
    "note that @xmath177 conditioned on @xmath176 , @xmath178 since a departure happens with probability @xmath56 and since there are @xmath11 files out of which @xmath176 are popular .",
    "now , @xmath179 so that @xmath180 solving for @xmath181 yields @xmath182    we then consider the right expectation in  . conditioned on @xmath124 and @xmath135 , the random variable @xmath155 has expected value @xmath183 since @xmath124 files are evicted and @xmath135 of the @xmath184 partial files in memory are correctly cached .",
    "thus , @xmath185 and @xmath186 finally , the right expectation in   can be evaluated as @xmath187    for ease of notation , define @xmath188 substituting   and   into and rearranging yields then @xmath189    this is a quadratic equation in the expected value @xmath190 of @xmath191 . in appendix",
    "[ sec : appendix_quadratic ] , we show that the solutions to this quadratic equation can be lower bounded as @xmath192 observe that , crucially , this lower bound does not depend on the variance @xmath193 of @xmath191 . using this lower bound on @xmath190 , we obtain after some algebra , @xmath194 concluding the proof of lemma  [ thm : quad ] .",
    "applying lemma  [ thm : quad ] to   and substituting into   shows that @xmath195 in appendix  [ sec : appendix_ineq ] , we show that @xmath196 is upper bounded as , @xmath197 for @xmath198 .",
    "hence , @xmath199 setting @xmath200 , we obtain for @xmath201 , @xmath202 on the other hand , for @xmath203 , we trivially have @xmath204    since the long - term average of the optimal scheme is less than or equal to @xmath44 , this implies @xmath205 concluding the proof of the upper bound in theorem  [ thm1 ] .",
    "we consider a offline scenario in which all cache memories are fully aware of the set of popular files @xmath40 .",
    "in addition , at the beginning of each time slot @xmath42 , before users decide on their requests , the caches are given full access to all the files in @xmath40 to update their stored content at no cost .",
    "however , the cache memories are not aware of future requests . clearly , the rate of the optimal scheme for this offline setting is a lower bound on the optimal rate for the online setting .",
    "this offline problem is in fact equal to the prefetching problem investigated in  @xcite , where it is shown that the instantaneous rate , and therefore also the long - term average rate , is lower bounded by @xmath206 .",
    "thus , @xmath207 as needed to be shown .",
    "set @xmath208 then , can be written as @xmath209 with solutions @xmath210 . since @xmath190 is the average of a real random sequence and",
    "satisfies the above quadratic equation , this equation has real solutions . in this case , the smaller solution is with the negative sign .",
    "thus , @xmath211 where for the last inequality we have used that @xmath212 .",
    "assume first that @xmath215 .",
    "then @xmath216 where the last inequality holds since @xmath217 for @xmath215 and @xmath198 .",
    "assume then that @xmath218 .",
    "then @xmath219 combining those two inequalities shows that @xmath220"
  ],
  "abstract_text": [
    "<S> we consider a basic content distribution scenario consisting of a single origin server connected through a shared bottleneck link to a number of users each equipped with a cache of finite memory . </S>",
    "<S> the users issue a sequence of content requests from a set of popular files , and the goal is to operate the caches as well as the server such that these requests are satisfied with the minimum number of bits sent over the shared link . assuming a basic markov model for renewing the set of popular files , we characterize approximately the optimal long - term average rate of the shared link . </S>",
    "<S> we further prove that the optimal online scheme has approximately the same performance as the optimal offline scheme , in which the cache contents can be updated based on the entire set of popular files before each new request . to support these theoretical results </S>",
    "<S> , we propose an online coded caching scheme termed _ coded least - recently sent _ </S>",
    "<S> ( lrs ) and simulate it for a demand time series derived from the dataset made available by netflix for the netflix prize . for this time series , </S>",
    "<S> we show that the proposed coded lrs algorithm significantly outperforms the popular least - recently used ( lru ) caching algorithm . </S>"
  ]
}