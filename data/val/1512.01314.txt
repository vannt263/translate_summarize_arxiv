{
  "article_text": [
    "the wta is a computational framework in which a group of recurrent neurons cooperate and compete with each other for activation .",
    "the computational power of wta @xcite and its function in cortical processing @xcite have been studied in detail .",
    "various models and hardware implementations of wta have been proposed for both rate @xcite and spike based @xcite neural networks . in recent past , researchers have looked into the application of stdp learning rule on wta circuits . the performance of competitive spiking neurons trained with stdp has been studied for different types of input such as discrete spike volleys @xcite , periodic inputs @xcite and inputs with random intervals @xcite .    in this paper , for the first time we are proposing a winner - take - all ( wta ) network which uses neurons with nonlinear dendrites ( nnld ) and binary synapses as the basic computational units .",
    "this architecture , which we refer to as winner - take - all employing neurons with nonlinear dendrites ( wta - nnld ) , uses a novel branch - specific spike timing dependent plasticity based network rewiring ( stdp - nrw ) learning rule for its training .",
    "we have earlier presented @xcite a branch - specific stdp rule for batch learning of a supervised classifier constructed of nnlds .",
    "the primary differences of our current approach with @xcite are :    * we present an unsupervised learning rule for training a wta network . *",
    "we propose an online learning scheme where connection modifications occur after presentation of each pattern .    in this article",
    "we consider spike train inputs with patterns occurring at random order which is the same type presented in @xcite .",
    "the primary differences between our work and the one proposed in @xcite are :    * our wta network is composed of neurons with nonlinear dendrites instead of traditional neurons with no dendrites .",
    "* unlike the network proposed in @xcite that requires high resolution weights , the proposed network uses low - resolution non - negative integer weights and trains itself through modifying connections of inputs to dendrites .",
    "hence , change of the ` morphology ' or structure of the neuron ( in terms of connectivity pattern ) reflects the learning .",
    "this results in easier hardware implementation since a low - resolution non - negative integer weight of @xmath0 can be implemented by activating a shared binary synapse @xmath0 times through time multiplexing schemes like address event representation ( aer)@xcite . * in @xcite , though the neurons were allowed to learn and respond to subpatterns , there was no actual guideline or control parameter to set the number of subpatterns to be learned . here",
    "we utilize the slow time constant of the inhibitory signal to select the number of subpatterns we want to divide a pattern into .    in the following section",
    ", we will present a overview of nnld , propose the wta - nnld architecture and stdp - nrw learning rule and show how the inhibitory slow time constant can be used to select subpatterns within a pattern .",
    "then we shall provide guidelines on selecting the parameters associated with wta - nnld and stdp - nrw . in section iv we will describe the classification task considered in this article which will be followed by the results .",
    "we will also present the robustness of the proposed method to variations of parameters in section v , a quality that is essential for its implementation in low - power , subthreshold neuromorphic designs that are plagued with mismatch .",
    "we will conclude the paper by discussing the implications of our work and future directions in the last section .",
    "in this section , we shall first present the working principle of a nnld .",
    "this will be followed by a description of the wta - nnld architecture and stdp - nrw learning rule .",
    "lastly , we will throw some light on the role of inhibitory time constant in balancing the specificity and sensitivity of the network .      the computational model of nnld",
    "was first proposed by mel et al . in @xcite .",
    "they showed that such neurons have higher storage capacity than their non - dendritic counterparts .",
    "they used two such nnlds to construct a supervised classifier and demonstrated its performance in pattern memorization .",
    "recently nnld has also been employed to develop computationally powerful rate @xcite and spike based @xcite supervised classifiers .",
    "the structure of nnld is isomorphic to a feedforward spiking neural network with a single layer of hidden neurons and one output neuron @xcite .",
    "the lumped dendritic nonlinearities @xmath1 are equivalent to the hidden neurons interposed between the input and output layers . however",
    ", spiking neurons implement nonlinear thresholding , integration , refractory period etc .",
    "hence , it is typically a much larger circuit compared to the square law nonlinearity of a dendrite , which makes the nnld an area efficient architecture .",
    ", scaledwidth=50.0% ]    as depicted in fig .",
    "[ fig : nnld ] , a nnld consists of @xmath2 dendritic branches having lumped nonlinearities , with each branch containing @xmath3 excitatory synaptic contact points of weight 1 .",
    "if we consider a @xmath4 dimensional input pattern , then each synapse is driven by any one of these input dimensions where @xmath5 .",
    "we use the leaky integrate - and - fire ( lif ) model to generate output spikes .",
    "thus , the neuronal membrane voltage is guided by the following differential equation : @xmath6 where @xmath7 , @xmath8 , @xmath9 and @xmath10 are the membrane voltage , threshold voltage , input current and output spikes of the nnld respectively .",
    "let us denote the input spike train arriving at the @xmath11 input line as @xmath12 which is given by :    @xmath13    where @xmath14 is the label of the spike .",
    "then , the input current @xmath9 to the neuron can be calculated as : @xmath15 where @xmath16 is the nonlinear activation function of the dendritic branch characterized by @xmath17 , @xmath18 is the output current of the @xmath19 dendrite and @xmath20 is the input current to the dendritic nonlinearity . here",
    ", @xmath21 describes the behavior of the dendritic nonlinear function by setting a limit on the minimum number of coactive synapses required to produce a supra - linear response .",
    "@xmath22 denotes the post - synaptic current kernel given by : @xmath23 where @xmath24 and @xmath25 are the fast and slow time constants governing the rise and fall times respectively and @xmath26 is a normalizing constant . in this article",
    ", we consider low resolution non - negative integer weights associated with the input lines .",
    "so @xmath27 means the number of times the @xmath11 input line is connected to the @xmath19 dendritic branch . like our earlier work @xcite",
    ", we allow multiple connections of one input dimension to a single dendrite but restrict the number of connections per dendrite to @xmath3 by enforcing @xmath28 for each @xmath29 .",
    "the output of the nnld is a spike train and can be denoted as @xmath30 where @xmath31 is the spike index .",
    "we propose a spike based wta network , depicted in fig .",
    "[ fig : wta_model ] , which is composed of @xmath32 such nnlds .",
    "each nnld is composed of @xmath2 dendrites , where each dendrite chooses ( repetition allowed ) @xmath3 of the @xmath4 available input lines and connects to @xmath3 synapses having weight 1 .",
    "the membrane voltage , threshold voltage and input current of the @xmath33 nnld are denoted by @xmath34 , @xmath8 and @xmath35 respectively and their dynamics is governed by equation [ eq : lif ] . for the @xmath33 nnld , while the pre - synaptic spike - train arriving at the @xmath11 input line is denoted by @xmath12 as before , the emitted output spike train is given by @xmath36 .",
    "note that for any applied input pattern , @xmath37 is measured from its beginning .",
    "we have modelled the effect of lateral inhibition by providing each nnld with a global inhibitory current signal @xmath38 supplied by a single inhibitory neuron @xmath39 through synapses .",
    "the signal @xmath38 is provided by the inhibitory neuron to all the nnld whenever any one of them fires an output spike .",
    "@xmath38 is modeled as @xmath40 , when the last post - synaptic spike is produced by the @xmath33 nnld at @xmath41 .",
    "the inhibitory post - synaptic kernel , @xmath42 , is given by : @xmath43 where @xmath44 and @xmath45 are the fast and slow time constants dictating the rise and fall times of the inhibitory current respectively and @xmath46 sets its amplitude .",
    "since we consider binary synapses with weight @xmath47 or @xmath48 , we do not have the provision to keep real valued weights associated with them .",
    "hence , to guide the unsupervised learning , we define a correlation coefficient based fitness value @xmath49 for the @xmath50 synaptic contact point on the @xmath19 dendrite of the @xmath33 nnld of the wta network , as a substitute for its weight . in the proposed algorithm , structural plasticity or connection modifications happen in longer timescales ( at the end of patterns ) which is guided by the fitness function @xmath49 updated by a stdp inspired rule in shorter timescales ( at each pre- and post - synaptic spike ) .",
    "the operation of the network and learning process comprises the following steps whenever a pattern is presented :    * @xmath49 is initialized as @xmath51 . *",
    "the value of @xmath49 is depressed at pre - synaptic and potentiated at post - synaptic spikes according to the following rule : 1 .",
    "depression : if the pre - synaptic spike occurs at the @xmath50 synapse on the @xmath19 dendritic branch of the @xmath33 nnld at time @xmath52 , then the value of @xmath49 at @xmath53 is updated by a quantity @xmath54 given by : @xmath55 where @xmath56 is the post - synaptic trace of the @xmath33 nnld and @xmath57 denotes derivative of the nonlinear function @xmath1 .",
    "potentiation : if the @xmath33 nnld of the wta - nnld network fires a post - synaptic spike at time @xmath58 then @xmath49 at @xmath59 @xmath60 i.e. for each synapse connected to the @xmath33 nnld is updated by @xmath61 given by : @xmath62 where @xmath63 is the pre - synaptic trace of the corresponding input line connected to it . + a pictorial explanation of this update rule of @xmath49 is shown in fig .",
    "[ fig : pre_post_pre ] .",
    "note that for a square law nonlinearity , @xmath64 and hence can be easily computed in hardware without requiring any extra circuitry to calculate the derivative .",
    "* during the presentation of the pattern whenever a spike is produced by any of the @xmath32 excitatory nnlds , the inhibitory neuron @xmath39 sends an inhibitory signal to all the nnlds of the wta .",
    "* after the network has been integrated over the current pattern of duration @xmath65 , the synaptic connections of the nnlds which have produced at least one spike are modified . * if we consider that @xmath66 out of @xmath32 nnlds have produced post - synaptic spike / spikes for the current pattern , then the connectivity of the @xmath67 nnld @xmath68 is updated by tagging the synapse ( @xmath69 ) having the lowest value of correlation coefficient at @xmath70 out of the @xmath71 synapses connected to it for possible replacement . * to aid the unsupervised learning process , randomly chosen sets @xmath72 containing @xmath73 of the @xmath4 input dimensions are forced to make silent synapses of weight 1 on the dendritic branch of @xmath69 @xmath74 .",
    "we term these synapses as  silent \" since they do not contribute to the computation of @xmath34 - so they do not alter the classification when the same pattern set is re - applied .",
    "the value of @xmath75 is calculated for synapses in @xmath72 and the synapse having maximum @xmath76 in @xmath72 denoted by @xmath77 @xmath74 is identified .",
    "next , the input line connected to @xmath69 is swapped with the input line connected to @xmath77 .",
    "hence , instead of the traditional method of training by changing of high - resolution synaptic weights , our learning rule modifies the connections between the inputs and dendrites based on the fitness values . *",
    "all the @xmath78 values are reset to zero and the above mentioned steps are repeated whenever a pattern is presented . here , we define an epoch for @xmath79 class classification as a set of patterns consisting of one pattern from each of the @xmath79 classes- in random order .",
    "we define another term @xmath80 as the average of the latencies of the post - synaptic spikes in the network over time period of the last epoch which is given by : @xmath81 where @xmath82 denotes averaging over one epoch .",
    "we note the value of @xmath80 for every epoch and the learning is considered to converge when the value of a ` convergence measure ' ( @xmath83 ) based on @xmath80 reaches saturation .",
    "we define our ` convergence measure ' in section [ params ] .    )",
    "when a post - synaptic spike occurs at @xmath84 the value of @xmath49 increases by @xmath85 . due to the appearance of a pre - synaptic spike at @xmath86 , @xmath49 reduces by @xmath87 as shown in the figure.,scaledwidth=50.0% ]      when a pattern is presented to the wta - nnld and any one of the @xmath32 nnlds produce an output spike , a global inhibition current @xmath38 is injected into all the @xmath32 nnlds .",
    "the slow time constant @xmath45 of this signal controls the output firing activity of the wta - nnld .",
    "typically , a large value of @xmath45 ( w.r.t to @xmath65 ) is set , and only one nnld produces an output spike i.e. patterns of same class are encoded by a single nnld . the post - synaptic spike latency for a pattern @xmath88 is defined as the time difference between the start of the pattern and the first spike produced by any one of the @xmath32 neurons of wta - nnld . during training of wta - nnld for this case ,",
    "different nnlds get locked onto different classes of pattern and the latency gradually decreases until the end of the training .",
    "thus , after completion of training , the unique nnlds which have learned different classes of pattern rely only on the first few spikes ( determined by the latency at the end of training ) to predict the pattern s class thereby significantly reducing the prediction time @xcite .",
    "so , the sensitivity of the network is increased",
    ". however , the problems with this approach are :    * the percentage of successful classifications can be less due to the strict requirement of different neurons firing based only on first few spikes of different patterns ( shown in section [ exp_res ] ) .",
    "* though the prediction time of a pattern s class is significantly reduced , this method neglects most part of the pattern after the first few spikes which may lead to a lot of false detections .     to respond.,scaledwidth=50.0% ]    we demonstrate the limitation mentioned in the above point by a simple example in fig .",
    "[ fig : eg_nsub1 ] .",
    "let us consider we are performing @xmath79 class classification and assume that after the training phase is complete , nnld @xmath89 responds to patterns belonging to class 1 .",
    "nnld @xmath89 has trained itself to provide an output spike depending on the position of the first few spikes ( red spikes in dashed box of fig .",
    "[ fig : eg_nsub1 ] ) of the pattern .",
    "it neglects the rest of the pattern while providing a prediction .",
    "however , for longer patterns there is a chance that this spike set can occur anywhere inside a random pattern ( not belonging to any class or to another class ) .",
    "the same nnld @xmath89 responds to such patterns by producing a post - synaptic spike .",
    "thus , we see that though trained wta - nnld is very sensitive in this case , it loses specificity . on the other hand ,",
    "if we set a moderate value of @xmath45 , then for a single pattern multiple nnlds are capable of producing output spikes .",
    "hence , patterns of the same class are now encoded by a sequence of successive firing of few nnlds where each nnld fires for one subpattern .",
    "let @xmath90 be the number of subpatterns that is set by a proper choice of @xmath45 .",
    "thus the original case of one nnld firing for each pattern corresponds to @xmath91 . in this article",
    ", for a @xmath79 class classification we define a successful trial as one in which ( a ) during the training phase wta - nnld learns different unique representations for patterns of different classes and ( b ) after completion of training and achieving success in ( a ) , the network produces the same representation , when presented with testing patterns corresponding to classes that it had learned during the training phase .",
    "when @xmath91 i.e. no pattern subdivisions are made , this unique representation is a different neuron firing for different classes of patterns . when @xmath92 1 , the unique representation is a different sequence of successive nnlds firing for different classes of patterns .",
    "when , @xmath93 1 , we allow the nnlds to detect subpatterns within patterns .",
    "since in this approach the wta - nnld gives weightage to the entire pattern before predicting its class , the number of false detections can be largely reduced .",
    "however , this method has a limitation of being less jitter resilient @xmath94 one of the many subpatterns can be easily corrupt by noisy jitters in spike ( shown in section [ exp_res ] ) and fail to produce a unique identifier during testing phase . hence , the choice of @xmath90 and consequently the inhibitory time constant depends on the amount of temporal jitter in the application .",
    "the following is an exhaustive list of the parameters used by wta - nnld and stdp - nrw :    1 .",
    "@xmath65 : duration of a pattern 2 .",
    "@xmath4 : dimension of the input 3 .",
    "@xmath2 : number of dendrites per nnld 4 .",
    "@xmath3 : number of synapses per dendrite 5 .",
    "@xmath73 : number of input dimensions in replacement set 6 .",
    "@xmath25 and @xmath24 : slow and fast time constant of excitatory current kernel 7 .",
    "@xmath26 : normalization constant of excitatory current kernel 8 .",
    "@xmath45 and @xmath44 : slow and fast time constant of inhibitory current kernel 9 .",
    "@xmath46 : normalization constant of inhibitory current kernel 10 .",
    "@xmath21 : threshold of dendritic nonlinearity 11 .",
    "@xmath8 : firing threshold voltage of nnld 12 .",
    "@xmath32 : number of nnlds in wta 13 .",
    "@xmath79 : number of classes of patterns    we will now provide some guidelines on choosing the key parameters :    [ [ total - number - of - synapses - per - nnld - s ] ] total number of synapses per nnld ( s ) + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the number of synapses allocated to each neuronal cell of wta - nnld are kept as equal to the dimension ( @xmath4 ) of the input patterns .",
    "this is done to ensure nnld uses the same amount of synaptic resources as the simplest neuron  a perceptron .",
    "thus , if the proposed network is comprised of @xmath32 such neuronal cells then the total number of synaptic resources required are @xmath95 .    [ [ number - of - dendrites - per - nnld - m ] ] number of dendrites per nnld ( m ) + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in @xcite a measure of the pattern memorization capacity,@xmath96 , of the nnld ( fig.[fig : model_nl ] ) has been provided by counting all possible functions realizable as : @xmath97 where @xmath2 , @xmath3 and @xmath4 are the number of dendrites , number of synapses per dendrites and dimension of the input respectively for this neuronal cell .",
    "when a new classification problem is encountered , we first note down the value of @xmath4 , which in turn sets our @xmath98 since we have considered @xmath99 . since @xmath100 , for a fixed @xmath98 all possible values which @xmath2 can take are factors of @xmath98 .",
    "we calculate @xmath101 for these values of @xmath2 by equation [ eq : bn ] .",
    "the value of @xmath2 for which @xmath101 attains its maxima is set as @xmath2 in our experiment .",
    "as an example , we show in fig . [",
    "fig : capacity ] the variation of @xmath101 with @xmath2 when @xmath102 .",
    "it is evident from the curve that the capacity is maximum when @xmath103 and so in our simulations for classifying @xmath104 dimensional patterns we employ neuronal cells having 25 dendrites .    )",
    "is plotted as function of the number of dendrites ( @xmath2 ) for a fixed number of input dimensions ( @xmath102 ) and synapses ( @xmath105).,scaledwidth=40.0% ]    [ [ number - of - synapses - per - branch - k ] ] number of synapses per branch ( k ) + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    after @xmath98 and @xmath2 have been set , the value of @xmath3 can be computed as @xmath106 .",
    "[ [ the - normalization - constant - i_0-slow - tau_s - and - fast - time - constant - tau_f - of - excitatory - psc - kernel ] ] the normalization constant ( @xmath26 ) , slow ( @xmath25 ) and fast time constant ( @xmath24 ) of excitatory psc kernel + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the fast time constant ( @xmath24 ) and slow time constant ( @xmath25 ) have been defined in section [ sec : stdp - nrw ] . in hardware implementation of a synapse @xcite @xmath24",
    "usually takes a small positive value and is typically not tuned .",
    "the slow time constant , @xmath25 , is responsible for integration across temporally correlated spikes and the performance of the network is dependent on its value .",
    "if @xmath25 takes too small a value , then the post synaptic current due to individual spikes dies down rapidly and thus temporal integration of separated inputs does not take place . on the other hand large values of @xmath25",
    "render all spikes effectively simultaneous .",
    "so , in both extremes the extraction of temporal features from the input pattern is hampered .",
    "in @xcite we have provided a mathematical formula for calculating @xmath107 , the optimal value of @xmath25 , with respect to the inter spike interval ( isi ) of the input pattern for which optimal performance of the network is obtained .",
    "if we are considering @xmath4 dimensional patterns and the mean firing rate of each dimension is @xmath108 , then the mean isi across the entire pattern is given by @xmath109 .",
    "we can then set @xmath107 according to the formula : @xmath110    in our simulations , we keep @xmath24 as @xmath111 . since the weights of all the active synapses are @xmath48 , we set @xmath112 to normalize the amplitude of the psc to be @xmath48 .",
    "[ [ threshold - of - nonlinearity - x_thr ] ] threshold of nonlinearity ( @xmath21 ) + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    during the training of wta - nnld , the stdp - nrw rule preferably selects those connection topologies where correlated inputs for synaptic connections are connected to the same branch .",
    "thus , the lumped dendritic nonlinearity @xmath113 should give a supra - linear output only when correlated input dimensions are connected to the dendrite . to ensure this we keep the value of @xmath21 equal to the average input to the nonlinear function in case of random connections .",
    "we create numerous instances of dendrites having @xmath3 synapses and calculate the average input to the nonlinear function , @xmath114 , for the pattern set at hand .",
    "then we set the value of @xmath21 as , @xmath115 .",
    "[ [ v_thr - of - nnld ] ] @xmath8 of nnld + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the nnld should provide a post - synaptic spike only when correlated inputs have been connected to its dendrites .",
    "we consider a nnld having @xmath2 dendrites and @xmath3 synapses and create numerous instances of random connections to these synapses .",
    "we measure the average value of the maximum membrane voltage ( @xmath116 ) produced when this nnld is integrated over the pattern duration for all these instances and set @xmath117 .",
    "[ [ the - normalization - constant - i_0inh - slow - tau_sinh - and - fast - tau_finh - time - constant - of - i_inht ] ] the normalization constant ( @xmath46 ) , slow ( @xmath45 ) and fast ( @xmath44 ) time constant of @xmath38 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the post - synaptic firing activity of the wta - nnld network is dependent on @xmath45 and @xmath46 . to simulate the hardware scenario we set @xmath44 to a small value given by @xmath118 . to set @xmath46 and @xmath45 , we first excite wta - nnld with @xmath119 epochs of patterns prior to training and calculate the average excitatory current ( @xmath120 ) to the nnlds as : @xmath121 where @xmath122 denotes averaging over @xmath119 epochs .",
    "the idea is to generate a @xmath38 which , if provided by @xmath39 at the beginning of a subpattern , decays exponentially to @xmath120 at the end of the subpattern i.e. after time @xmath123 has elapsed .",
    "this ensures that once a post - synaptic spike is generated by a nnld in a particular @xmath123 time window , other nnlds are unable to fire during that same @xmath123 time window .",
    "assuming @xmath124 , we can derive that the required @xmath38 is implemented by setting @xmath45 as : @xmath125    note that @xmath45 has an inverse logarithmic relation to @xmath46 .",
    "[ [ convergence - measure - cm ] ] convergence measure ( @xmath83 ) + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the formula for calculating @xmath83 , applicable to both @xmath126 and @xmath127 , for detecting the convergence of learning is given by : @xmath128    note that for @xmath126 , @xmath129 and so it computes the time - to - first spike for patterns averaged over an epoch .",
    "for @xmath127 , @xmath83 calculates the average time - to - first spike from the beginning of each subpattern of @xmath79 patterns of an epoch .",
    "we consider the learning has converged when the value of @xmath83 saturates .",
    "in this section , we will describe the classification task considered in this article . to show how the classification performance generalizes to multi - class",
    "we will consider two , four and six class classification .",
    "we will be showing the performance of wta - nnld and stdp - nrw for three values of @xmath90 given by @xmath90 = 1 , 5 and 10 .",
    "the benchmark task we have selected to analyze the performance of the proposed method is the spike train classification problem @xcite . in the generalized spike train classification problem ,",
    "@xmath79 arrays of @xmath130 poisson spike trains having frequency @xmath131 and length @xmath65 are present which are labeled as classes 1 to @xmath79 .",
    "jittered versions of these templates are created by altering the position of each spike within the templates by a random amount that is randomly drawn from a gaussian distribution with zero mean and standard deviation @xmath132 .",
    "the network is trained by these jittered versions of spike trains , and the task is to correctly identify a pattern s class . in this article ,",
    "unless otherwise mentioned , we have considered @xmath130 = 100 and poisson spike trains are present in each afferent , @xmath131 = 20 and @xmath65 = 0.5 sec and varied @xmath79 and @xmath132 .",
    "inspired by @xcite , we also consider the scenario when @xmath133 randomly chosen afferents do not contain any spikes , while the remaining @xmath133 afferents are poisson spike trains .",
    "+      in this case we have @xmath134 , so one nnld is capable of firing only once when a pattern is presented .",
    "considering @xmath132 = 0 , we have varied the number of nnlds and noted the percentage of successful trials which is depicted in fig . [",
    "n_sub_1](a ) .",
    "to make the horizontal axis invariant of the number of classes , we have taken @xmath135 as the horizontal axis . from the figure we can conclude that the percentage of successful trials gradually increases with an increase in @xmath135 and finally becomes constant after @xmath136 .",
    "thus , unless otherwise mentioned , we will keep @xmath137 when @xmath91 .",
    "it can be seen from fig .",
    "[ n_sub_1](a ) that the percentage of successful trials can not go beyond 92% , 88% and 82% for two , four and six class classification respectively .    earlier in section [ params ]",
    "we have mentioned that learning converges when @xmath83 saturates . for @xmath126",
    ", @xmath138 is the average of the time - to - first spikes for patterns in an epoch . as an example we consider a particular trial of four - class classification and show in fig . [",
    "fig : nojit_latency_vs_epoch__classes_experi_1 ] that during training , the latencies of the four nnlds , @xmath89 , @xmath139 , @xmath140 and @xmath141 which uniquely recognize the four class of patterns gradually reduce until reaching a saturation point .",
    "moreover , in fig .",
    "[ n_sub_1](b ) we show the epochwise evolution of @xmath83 averaged over 50 trials for two - class , four - class and six - class classification .",
    "it is evident from the figure that the value of @xmath83 decreases thereby showing that the algorithm is favoring correlated inputs such that the post - synaptic spikes can occur faster and finally saturates after some epochs have passed .",
    "we denote the number of epochs taken by the algorithm for saturation of @xmath83 as @xmath142 and note its value for 50 trials . the average value of @xmath142 for 50 trials , @xmath143 , is then computed to be @xmath143 = 149 , 157 and 165 for two - class , four - class and six - class classification respectively .",
    "moreover , this phenomenon clearly indicates that while the wta - nnld network is being trained by stdp - nrw learning rule , @xmath79 unique nnlds which have locked onto the @xmath79 different classes of pattern , are trying to recognize the start of repeating patterns for different classes .",
    "[ n_sub_1](b ) also suggests that after the training has stopped , these @xmath79 unique nnlds , instead of looking at the whole pattern of duration @xmath144 , can now look only at the starting @xmath145 , @xmath146 and @xmath147 of the patterns for @xmath148 2 , 4 and 6 respectively to predict its class .",
    "let us now consider the effect of jitter and we show in fig .",
    "[ n_sub_1](c ) the performance of the proposed method when the intensity of jitter is varied .",
    "next , we look into the performance of the proposed method when patterns with 50% empty afferents are considered .",
    "[ nsub_more_1](a ) depicts the results obtained by our network for this case with varying amounts of jitter .",
    "next , we consider @xmath149 i.e. we divide each pattern into 5 subpatterns by setting @xmath45 and @xmath46 as per equation [ tau_inh_i0_inh ] . for @xmath79 class classification ,",
    "the maximum number of subpatterns can be @xmath150 so we set @xmath151 in this case i.e. we keep @xmath32 = 10 , 20 and 30 for two , four and six class classification respectively . considering @xmath152 , the evolution of @xmath83 with epochs for two , four and six class classification averaged over 50 trials is shown in fig .",
    "[ nsub_more_1](b ) .",
    "moreover , the value of @xmath143 ( averaged over 50 trials ) is found out to be 210 , 221 and 230 when @xmath79 = 2 , 4 and 6 respectively .",
    "unlike case 1 , here we consider the response to a pattern as a unique firing sequence of few nnlds . as an example",
    ", we consider a particular trial of four class classification and look into the first and last 3 epochs during its training .",
    "it is evident from fig .",
    "[ first3_last3_epochs ] that during the first 3 epochs , wta - nnld produces arbitrary sequences of spikes .",
    "however , it can be seen that after the training of the network is complete , wta - nnld produces different firing sequences for different patterns while producing the same sequence when same patterns are encountered .",
    "wta - nnld trained by this method produces a 100% accuracy in recognizing different patterns by producing its unique firing sequence for two , four and six class classification . the performance of the network with varying intensity of jitter is provided in fig .",
    "[ n_sub_1](c ) ( spikes present in all afferents ) and fig .",
    "[ nsub_more_1](a ) ( spikes present in only half of the afferents ) which depict that the @xmath90 = 5 case is less jitter resilient than the @xmath90 = 1 case .",
    "we further increase the resolution of pattern subdivision by decreasing @xmath45 .",
    "we consider @xmath90 = 10 and following the principle of @xmath90 = 5 , the number of nnlds employed for @xmath153 are 20 , 40 and 60 for two - class , four - class and six - class classification .",
    "this approach also provides 100% accuracy in providing a unique sequence of firing whenever a particular pattern is encountered when @xmath152 .",
    "however , the performance of the network falls rapidly with the increase of @xmath132 as shown in fig . [ n_sub_1](c ) and fig . [ nsub_more_1](a ) .",
    "we also show the evolution of @xmath83 with the epochs in fig . [ nsub_more_1](b ) .",
    "furthermore , the number of epochs needed for convergence of @xmath83 in this case is much more than the previous cases as depicted in fig . [ nsub_more_1](c ) .",
    "we conclude that dividing a pattern into too many subpatterns hampers the network performance .",
    "next we delve a bit further and show the statistics of causes for the failure of the system in producing successful trials .",
    "a trial may fail if either condition ( a ) or ( b ) ( described in section [ spec_sens ] ) is not satisfied .",
    "we denote the failure of condition ( a ) as @xmath154 .",
    "note that for a trial , condition ( b ) can fail if a pattern is misclassified as a pattern of another class ( denoted as @xmath155 ) or as a random pattern ( denoted as @xmath156 ) .",
    "table [ fail_ana ] shows the statistics of failed trials for @xmath157 1 , 5 and 10 when @xmath158 .",
    "note that @xmath154 is high for @xmath157 1 since a unique nnld might lock onto multiple classes of patterns .",
    "@xmath154 reduces for @xmath157 5 and increases again for @xmath157 10 since sometimes the network fails to produce a unique 10 indices long representation for all patterns of the same class .",
    "moreover , we test our network with random patterns and note the cases where a learnt unique representation is produced for a random input pattern i.e. a false positive error occurs .",
    "the percentage of false positive errors produced for @xmath157 1 , 5 and 10 when @xmath158 are 8% , 0% and 0% respectively .",
    "note that no false positive errors occur for @xmath1575 and 10 since it is highly unlikely for a random pattern to make a sequence of neuron firing same as any of the learnt representation .",
    ".analysis of failure statistics [ cols=\"^,^,^,^,^\",options=\"header \" , ]     [ fail_ana ]",
    "in this section we analyze the stability of our algorithm to hardware nonidealities by incorporating the statistical variations of the key subcircuits .",
    "the primary subcircuits needed to implement our architecture are synapse , dendritic squaring block , neuron and @xmath159 calculator . while the variabilities of the synapse circuit are modeled by mismatch in the amplitude ( @xmath26 ) and time constant ( @xmath25 ) of the synaptic kernel function , the variabilities of the squaring block are captured by a multiplicative constant ( @xmath160 ) @xcite .",
    "we do not consider the variation of inhibitory current kernel since it is global and only a single instance is present in the architecture . in our earlier work @xcite , we proposed the circuits for implementing the synapse and squaring block of nnld and performed monte carlo analysis to find their variabilities .",
    "we presented that the @xmath161 of @xmath26 , @xmath25 and @xmath160 for the worst case scenario are 13% , 10.1% and 18% respectively .",
    "the mismatch of the lif neuron circuit proposed in @xcite was captured by variations in the firing threshold @xmath8 , the @xmath161 of which was computed to be 12.5% .",
    "lastly , the nonidealities of the @xmath159 calculator block , described in @xcite , are modeled as a multiplicative constant ( @xmath162 ) .",
    "monte carlo analysis of the @xmath159 calculator block revealed that its @xmath161 for the worst case is 18% .",
    "[ unsup_nonid ] shows the performance of the proposed method when these nonidealities are included in the model for @xmath126 and @xmath163 keeping @xmath164 0.1 .",
    "the bars corresponding to @xmath26 , @xmath25 , @xmath160 , and @xmath162 denote the performance degradation when statistical variations of @xmath26 , @xmath25 , @xmath160 , and @xmath162 are included individually .",
    "the results of fig .",
    "[ unsup_nonid](a ) and fig .",
    "[ unsup_nonid](b ) depict that the performance of the proposed algorithm is most affected by @xmath25 and @xmath162 and least by @xmath160 . finally , to mimic the proper hardware scenario we consider the simultaneous implementations of all the nonidealities , which is marked by ( ... ) .",
    "the ( ... ) bars show that there is an 8% and 6% decrease in performance for @xmath126 and @xmath163 respectively .",
    "we have proposed a new neuro - inspired winner - take - all architecture ( wta - nnld ) and a stdp inspired dendrite specific structural plasticity based learning rule ( stdp - nrw ) for its training .",
    "motivated by recent biological evidences and models suggesting nonlinear processing properties of neuronal dendrites we employ neurons with nonlinear dendrites to construct our wta architecture .",
    "moreover , we consider binary synapses instead of high resolution synaptic weights . thus our learning rule , instead of weight updates , trains the network by modification of the connections between input and synapses .",
    "we have also provided a method by which the number of subpatterns per pattern learned by wta - nnld can be controlled .",
    "wta - nnld encodes patterns of different classes by either activity of distinct nnlds or by a distinct sequence of nnld firings . to demonstrate the performance of wta - nnld and stdp - nrw ,",
    "we have considered two , four and six class classification of 100 dimensional poisson spike trains .",
    "we can conclude from the result that the slow time constant of inhibitory signal ( @xmath45 ) can be properly set to obtain a tradeoff between specificity and sensitivity of the network .",
    "our immediate future work will include studying the effects of connection changes after the network gets integrated over multiple patterns .",
    "this will reduce the number of required computations . on another note",
    ", we will look into the classification of spike based mnist @xcite datasets by our method .",
    "our network can be immediately scaled to learn the digits of mnist dataset , the only requirement being additional simulation time and computational memory compared to the tasks considered in this article .",
    "furthermore , to achieve invariance to scaling and rotation during image classification , we will be constructing nnld based convolutional neural networks @xcite trained by structural plasticity .",
    "we will also implement the proposed network in hardware and apply it for real time online unsupervised classification of spatio - temporal spike trains .",
    "t.  serrano and b.  l.  barranco , `` a modular current - mode high - precision winner - take - all circuit , '' in _ proceedings of the ieee international symposium on circuits and systems ( iscas ) _ , may 1994 , vol .  5 , pp . 557560 .",
    "a.  delorme , l.  perrinet , and s.  j. thorpe , `` networks of integrate - and - fire neurons using rank order coding b : spike timing dependent plasticity and emergence of orientation selectivity , '' , vol .",
    "40 , pp . 539545 , 2001 .",
    "s.  brink , s.  nease , p.  hasler , s.  ramakrishnan , r.  wunderlich , a.  basu , and b.  degnan , `` a learning - enabled neuron array ic based upon transistor channel models of biological phenomena , '' , vol .",
    "1 , pp . 7181 , feb . 2013 .",
    "s.  hussain , r.  gopalakrishnan , a.  basu , and s.  c. liu , `` morphological learning : increased memory capacity of neuromorphic systems with binary synapses exploiting aer based reconfiguration , '' in _ ieee intl .",
    "joint conference on neural networks ( ijcnn ) _ , aug .",
    "2013 , pp . 1  7 .",
    "s.  roy , a.  basu , and s.  hussain , `` hardware efficient , neuromorphic dendritically enhanced readout for liquid state machines , '' in _ proceedings of the ieee biomedical circuits and systems ( biocas ) _ , nov 2013 , pp . 302305 .",
    "jadi , b.f .",
    "behabadi , a.  poleg - polsky , j.  schiller , and b.w .",
    "mel , `` an augmented two - layer model captures nonlinear analog spatial integration effects in pyramidal neuron dendrites , '' , vol .",
    "782798 , may 2014 .",
    "a.  banerjee , a.  bhaduri , s.  roy , s.  kar , and a.  basu , `` a current - mode spiking neural classifier with lumped dendritic nonlinearity , '' in _ proceedings of the ieee international sympoisum on circuits and systems ( iscas ) _",
    ", 2015 , number may .",
    "s.  roy , s.k .",
    "kar , and a.  basu , `` architectural exploration for on - chip , online learning in spiking neural networks , '' in _",
    "14th international symposium on integrated circuits ( isic ) _ , dec 2014 , pp . 128131 ."
  ],
  "abstract_text": [
    "<S> in this article , we propose a novel winner - take - all ( wta ) architecture employing neurons with nonlinear dendrites and an online unsupervised structural plasticity rule for training it . </S>",
    "<S> further , to aid hardware implementations , our network employs only binary synapses . </S>",
    "<S> the proposed learning rule is inspired by spike time dependent plasticity ( stdp ) but differs for each dendrite based on its activation level . </S>",
    "<S> it trains the wta network through formation and elimination of connections between inputs and synapses . to demonstrate the performance of the proposed network and learning rule , we employ it to solve two , four and six class classification of random poisson spike time inputs . </S>",
    "<S> the results indicate that by proper tuning of the inhibitory time constant of the wta , a trade - off between specificity and sensitivity of the network can be achieved . </S>",
    "<S> we use the inhibitory time constant to set the number of subpatterns per pattern we want to detect . </S>",
    "<S> we show that while the percentage of successful trials are 92% , 88% and 82% for two , four and six class classification when no pattern subdivisions are made , it increases to 100% when each pattern is subdivided into 5 or 10 subpatterns </S>",
    "<S> . however , the former scenario of no pattern subdivision is more jitter resilient than the later ones . </S>"
  ]
}