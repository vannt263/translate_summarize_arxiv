{
  "article_text": [
    "in order to restore a corrupted image , one needs a model of how uncorrupted ( i.e.  natural ) images appear . in the markov random field bayesian paradigm for image restoration @xcite ,",
    "natural images are modeled via an _",
    "image prior_. this is a probabilistic model that encodes how natural images behave _ locally _ ( in the vicinity of every pixel ) .",
    "an inference algorithm is then used to restore the image , whose aim is to find a consensus among all vicinities on which global solution is most compatible with a natural image ( while still similar to the corrupted image  a corrupted tree should be restored as an uncorrupted tree , not as an uncorrupted house ) .",
    "the simplest example of an image prior is perhaps the pairwise model presented in @xcite  which simply expresses that neighboring pixels are likely to share similar gray - levels .",
    "however , it is easy to see that such a model fails to capture a great deal of important information about natural images .",
    "for example , ` edges ' are highly penalized by such a prior , and it is unable to encode any information about ` texture ' . only by using higher - order priors will one be able to capture this important information .",
    "an example of a high - order prior is the _ field of experts _ model @xcite , which is parameterized as the product of a selection of filters ( or ` experts ' ) .",
    "each of these filters is typically a patch of @xmath0 or @xmath1 pixels , resulting in a @xmath2 or @xmath3-dimensional prior respectively",
    ". unfortunately , such a high - dimensional prior limits the practicality of many inference algorithms .",
    "even though it may be possible to use smaller ( for example , @xmath4 ) patches @xcite , we are still limited by the number of gray - levels used to properly represent natural images ( typically @xmath5 ) .",
    "updating a single pixel using a gibbs sampler ( for example ) requires us to consider all @xmath5 possible gray - levels .",
    "since gibbs samplers may typically take hundreds ( or thousands ) of iterations to converge , they are simply impractical in this setting .    while belief - propagation techniques tend to converge in fewer iterations @xcite , they are often equally impractical .",
    "since adjacent cliques may share as many as @xmath6 nodes ( using a @xmath0 model ) , the size of the messages passed between them may be as large as @xmath7 .",
    "even if we only use @xmath4 cliques , the size of our messages may still be as large as @xmath8 , which remains impractical for many purposes .",
    "although inpainting has been previously approached using belief - propagation techniques in @xcite , they do not deal with @xmath4 ( or larger ) models , and their approach can therefore only capture limited textural information .    to avoid the above problems ,",
    "image restoration is typically performed using gradient - ascent , thereby eliminating the need to deal with many discrete gray - levels , and avoiding expensive sampling @xcite .",
    "while gradient - based approaches are generally considered to be fast , they may still require several thousand iterations in order to converge , and even then will converge only to a _ local _ optimum .    in this paper",
    ", we propose a method that gives us the best of both worlds : we manage to render belief - propagation practical using a high - order ( @xmath4 ) model , and use it for the task of image inpainting . by using a nonparametric prior",
    ", we avoid the need to discretize images , resulting in much smaller messages being passed between cliques .",
    "our experiments show that belief - propagation techniques are able to produce competitive results after only a single iteration , rendering them faster than many gradient - based approaches , while retaining similar visual quality of the restoration .",
    "in this section , we define the markov random field ( mrf ) image prior to be used in our model .",
    "although we shall not present any significant results in terms of _ learning _ the prior , we have nevertheless made a number of modifications to the ` standard ' image prior in order to render inference tractable .",
    "the hammersley - clifford theorem states that the joint probability distribution of a markov random field with clique set @xmath9 ( assuming maximal cliques ) is given by @xmath10 ( where @xmath11 is the set of variables in @xmath12 belonging to the @xmath13 clique ; @xmath14 is a normalization constant ) @xcite .",
    "when dealing with images , the @xmath15s are often assumed to be homogeneous @xcite , meaning that the prior can be defined entirely in terms of a single potential function , @xmath16 . in the field of experts model @xcite , this potential function is assumed to take the form of a _ product of experts _ @xcite , in which each ` expert ' is the response of the image patch \\{@xmath11 } to a particular filter \\{@xmath17}. that is , the potential function takes the form @xmath18 ( where the @xmath19 s are simply weighting coefficients controlling the importances of the filters ) . specifically , each expert is assumed to take the form of a student s t - distribution , namely @xmath20 although @xcite use contrastive divergence learning to select the filters and alphas , it has been shown that the filters can more easily be selected using principal component analysis ( pca ) @xcite .",
    "this leaves only the problem of learning the alphas , which we shall deal with in section [ sec : approx ] .",
    "inference in the mrf setting can be formulated as a message passing problem .",
    "two common message passing algorithms exist , namely the _ junction - tree algorithm _ , and _ loopy belief - propagation _ @xcite . in our case , which algorithm should be applied depends upon the ` shape ' of the region being inpainted",
    ". we will give only a brief overview of these algorithms in order to explain why it is infeasible to apply them directly when using the above prior .",
    "a more complete specification is given in @xcite ; similar ideas are also used in an image inpainting setting in @xcite .    belief - propagation algorithms work by having cliques pass ` messages ' to other cliques which share one or more nodes in common .",
    "if we denote by @xmath21 the intersection of the two cliques @xmath22 and @xmath23 , and denote by @xmath24 the neighbors of @xmath22 ( i.e.  those cliques which share one or more nodes with @xmath22 ) , then the message , @xmath25 , sent from @xmath22 to @xmath23 is given by @xmath26 that is , the outgoing message from @xmath22 to @xmath23 is defined as the product of the local potential \\{@xmath27 } with the incoming messages from all neighbors _ except _ @xmath23 , marginalized over the variables in @xmath22 but not in @xmath23 .",
    "once all messages have been sent , the final distribution of @xmath22 \\{@xmath28 } is given by @xmath29 even when using the @xmath4 model , evaluating @xmath27 requires us to consider @xmath30 possible gray - level combinations .",
    "although it may be possible to approximate the marginal being computed in equation ( [ eq : message ] ) without computing @xmath27 explicitly @xcite , the message itself still contains @xmath8 elements .",
    "this problem is dealt with in @xcite by using a _ factor - graph _",
    "@xcite , which requires only that one dimensional marginals are computed ; however the running time of their method is still linear in the number of gray - levels , in addition to the fact that the factor - graph fails to fully capture the conditional independencies implied by the model .    as a result",
    ", we seek @xmath16 in such a form that the sum in equation ( [ eq : message ] ) may be replaced by an integral . in @xcite ,",
    "the authors defined such a model in which the potential function takes the form of a gaussian mixture , that is , with @xmath16 taking the form @xmath31 ( this is sometimes known as a _ gaussian random field _",
    "@xcite ) . unfortunately , the method they use to learn these mixtures appears to be applicable only to low - order models ( the largest mixture models they learn are 3-dimensional ) .    in the remainder of this paper",
    ", we will show that the experts \\{@xmath32s } can be approximated as a gaussian mixture , resulting in a high - order model which closely matches the one given in equations ( [ eq : pot1 ] ) and ( [ eq : pot2 ] ) .",
    "we will describe belief - propagation in this setting , and show how this approach can be used for fast image inpainting .",
    "in @xcite , the authors showed that the filters \\{@xmath17s } in equation ( [ eq : pot1 ] ) can be learned by performing a pca on a collection of natural image patches . here",
    "we follow this idea . to learn our filters ( for a @xmath4 model )",
    ", we randomly cropped 50,000 @xmath4 patches from images in the berkeley segmentation database @xcite , and used their principal components as our filters .",
    "it was found in @xcite that the first component of such a pca always corresponds to a uniform gray patch , which should be ignored in order to obtain a model invariant to intensity .",
    "hence we only use the _ last three _ filters for our @xmath4 model .",
    "the resulting patches appear to make sense visually , and are shown in figure [ fig : filters ] .",
    "this technique can also be used to learn @xmath0 or @xmath1 models ( resulting in 8 or 24 filters , respectively ) , which are not shown .",
    "model.,title=\"fig : \" ]   model.,title=\"fig : \" ]   model.,title=\"fig : \" ]    this model requires also that we learn the ` importances ' @xmath33 of each filter . from equation ( [ eq : pot2 ] ) , it can be seen that the @xmath19s simply control the shape ( or ` peakedness ' ) of the student s t - distribution . rather than try to learn the @xmath19s explicitly",
    ", we will learn them implicitly through our approximation .    in order to approximate the experts @xmath34 , we first calculated the inner products @xmath35 for a random selection of 5,000 image patches ( again cropped from the berkeley segmentation database @xcite ) .",
    "a normal probability plot of the data ( against the first filter ) is shown in figure [ fig : normplot ]  this plot reveals that the data is more heavily tailed than would be suggested by a normal distribution , indicating that the student s t - distribution may indeed be valid .",
    "however , rather than assume that this data is generated according to a student s t - distribution , we simply tried to approximate it directly using a mixture of gaussians .        in order to estimate the distribution governing this data , we used the expectation - maximization ( em ) algorithm @xcite , assuming that the set of inner products for each filter was generated by a mixture of three gaussians .",
    "all of our parameters to be learned \\{@xmath36 } were initialized by using a k - means clustering @xcite on the original inner products .",
    "we used this approach to learn a separate mixture model for each expert .",
    "this algorithm produces an approximation of the form @xmath37 the alpha terms are no longer relevant  the ` shape ' of the distribution is implicitly controlled by the other parameters .",
    "however , the expression in equation ( [ eq : approximation ] ) is not yet in the same form as equation ( [ eq : mog ] ) .",
    "hence we need to solve the system @xmath38 that is , we are trying to solve for @xmath39 ( a matrix ) and @xmath40 ( a vector ) , in terms of @xmath41 ( a vector ) and @xmath42 ( a scalar ) .",
    "it is not difficult to see that the only solution for @xmath39 is @xmath43 \\label{eq : covinv}\\ ] ] ( where @xmath44 is the size of the filter @xmath41  in our case , @xmath45 ) . alternately , there are infinite solutions for @xmath40",
    ". one obvious solution is @xmath46 however , we found for all of our filters that @xmath47 , meaning that this solution would be highly unstable . a more stable solution ( which we used ) is given by @xmath48    our potential function is now of the form @xmath49 in order to expand the above product , we use the following result about the product of gaussian distributions @xcite : given @xmath50 gaussians ( with means @xmath51 , and covariances @xmath52 ) , the covariance of the product \\{@xmath53 } is given by @xmath54 ( although each @xmath55 is singular in our case , their sum is not ) .",
    "the mean of the product \\{@xmath56 } is given by @xmath57 the corresponding beta term for the product is just @xmath58 . in our case",
    "this results in a final approximation which is a mixture of @xmath59 gaussians .",
    "in order to perform belief - propagation , we must first be able to express equations ( [ eq : message ] ) and ( [ eq : messageproduct ] ) in terms of the gaussian mixtures we have defined . in our setting , the sum in equation ( [ eq : message ] ) becomes an integral , resulting in the new equation @xmath60 we have already suggested how to perform the above multiplication in equations ( [ eq : prod1 ] ) and ( [ eq : prod2 ] ) .",
    "the only difference in this case is that the mixtures for each message may contain fewer variables ( and smaller covariance matrices ) than the local distribution \\{@xmath27}. in such a case , the inverse covariance matrices for each message \\{@xmath39s } are simply assumed to be zero in all missing variables .    to compute the marginal distribution of a gaussian mixture with mean @xmath42 and covariance matrix @xmath61 ( i.e.  the integral in equation ( [ eq : newmessage ] ) )",
    ", we simply take the elements of @xmath42 and @xmath61 corresponding to the variables whose marginals we want .",
    "the importances for each gaussian in the mixture remain the same .",
    "of course , when we compute the products in equation ( [ eq : newmessage ] ) , we produce a model with an exponentially increasing number of gaussians . as a simple solution to this problem",
    ", we restrict the maximum number of gaussians to a certain limit ( see section [ sec : results ] ) , by including only those with the highest importances .",
    "when solving an inpainting problem , we only wish to treat some of the variables in each clique as unknowns ( for example , the ` scratched ' sections ) .",
    "hence the potential function for these cliques should be conditioned upon the ` observed ' regions of the image .",
    "suppose that for a clique @xmath62 we have unknowns @xmath63 , and observed variables @xmath64 ( i.e.  @xmath65 ) .",
    "then we may partition the mean and covariance matrix ( for a particular gaussian in the mixture ) as @xmath66 and @xmath67.\\ ] ] the mean of the conditional distribution \\{@xmath68 } is now given by @xmath69 and the covariance matrix \\{@xmath70 } is given by @xmath71    finally , once all messages have been propagated , we are able to compute the marginal distribution for a given node ( or pixel , belonging to clique @xmath62 ) by marginalizing @xmath72 ( equation ( [ eq : messageproduct ] ) ) in terms of that node . in order to estimate the ` most likely ' configuration for this pixel , we simply consider each of the 256 possible gray - levels .",
    "as we mentioned in section [ sec : bprop ] , the two propagation techniques we will deal with are the junction - tree algorithm and loopy belief - propagation . although we will not cover these in great detail ( see @xcite for a more complete exposition ) , we will explain the differences between the two in terms of image inpainting .    both algorithms work by passing messages between those cliques with non - empty intersection .",
    "however , when using the junction - tree algorithm , we connect only enough cliques to form a maximal spanning tree . now",
    ", suppose that two cliques @xmath73 and @xmath74 have intersection @xmath75 .",
    "if each clique along the path between them also contains @xmath75 , we say that this spanning tree obeys the ` junction - tree property ' .",
    "if this property holds , it can be proven that exact inference is possible ( subject only to the approximations used by our gaussian model ) , and requires that messages be passed only for a single iteration @xcite .",
    "technically , the graphs which obey this property are the so - called__triangulated _ _ or _ chordal _ graphs .",
    "the tractability of exact inference in these graphs depends on their _ tree - width _ : graphs that are more ` tree - like ' are better suited to efficient exact inference .",
    "see @xcite for details",
    ".    if this property _ does nt _ hold , then we may resort to using loopy belief - propagation , in which case we simply connect _ all _ cliques with non - empty intersection .",
    "there is no longer any message passing order for which equation ( [ eq : newmessage ] ) is well defined ( i.e.  we must have a criterion to initialize some messages , and the common choice is to assume they have a uniform distribution ) , meaning that messages must be passed for many iterations in the hope that they will converge .",
    "figure [ fig : animal ] shows an inpainting problem for which a junction - tree exists , and two problems for which one does not ( assuming @xmath4-pixel cliques ) . since the regions being inpainted are usually thin lines ( or ` scratches ' ) , we may often observe graphs which do in fact obey the junction - tree property in practice .",
    "the graphs formed from the white pixels in the other two images do not.,title=\"fig : \" ]   model ) .",
    "the graphs formed from the white pixels in the other two images do not.,title=\"fig : \" ]    model ) .",
    "the graphs formed from the white pixels in the other two images do not.,title=\"fig : \" ]    fortunately , we found that even in those cases where no junction - tree existed , loopy belief - propagation tended to converge in very few iterations .",
    "although there are few theoretical results to justify this behavior , loopy - belief propagation typically converges quickly in those cases where the graph _ almost _ forms a tree ( as is usually the case for the regions being inpainted ) .",
    "in order to perform image inpainting , we used a high - level ( python ) implementation of the junction - tree algorithm and loopy belief - propagation , which is capable of constructing markov random fields with any topology . despite being written in a high - level language ,",
    "our implementation is able to inpaint images within a reasonably short period of time .",
    "since it is difficult to assess the quality of our results visually , we have reported both the peak signal - to - noise ratio ( psnr ) , and the structured similarity ( ssim ) @xcite .",
    "we were not able to directly compare our psnr results to those in @xcite , since they only presented @xmath0 and @xmath1 models .",
    "while it is certainly true that their @xmath0 model produces a _ much _ higher psnr than our technique ( e.g.  a psnr of @xmath76 for the image in figure [ fig : text ] ) , its execution time is simply impractical .",
    "fortunately , it is still possible to measure approximate execution times of a @xmath4 model using their approach , even without reporting psnrs .",
    "these results are presented in the next section .    while it is true that the difference between the two models being compared makes meaningful comparison difficult , it is most important to note that there is little _ visual _ difference between the two models . in the next section",
    ", we will show that our @xmath4 model is faster than a similar model using gradient - ascent  it is the combination of these two results which we believe makes our technique viable .",
    "figure [ fig : text ] shows a corrupted image from which we want to remove the text .",
    "the image has been inpainted using a model containing only a single gaussian ( although the learned mixtures contained three gaussians  see below ) .",
    "after a single iteration , most of the text has been removed , and after two iterations it is almost completely gone .",
    "although the current state - of - the - art inpainting techniques produce superior results in terms of psnr @xcite , they give similar visual results and take several thousand iterations to converge , compared to ours which takes only two ( no further improvement was observed after a third iteration ) .",
    "+   +    ' '' ''     +   +     figure [ fig : results ] compares models of various sizes , varying both the number of gaussians used to approximate each mixture , as well as the maximum number of gaussians allowed during the inference stage .",
    "the same results are summarized in table [ tab : results ] .",
    "model , after three iterations .",
    "see table [ tab : results ] for more detail.,title=\"fig : \" ] model , after three iterations .",
    "see table [ tab : results ] for more detail.,title=\"fig : \" ] +    ' '' ''     +   model , after three iterations .",
    "see table [ tab : results ] for more detail.,title=\"fig : \" ] model , after three iterations .",
    "see table [ tab : results ] for more detail.,title=\"fig : \" ] +   model , after three iterations .",
    "see table [ tab : results ] for more detail.,title=\"fig : \" ] model , after three iterations .",
    "see table [ tab : results ] for more detail.,title=\"fig : \" ]    .comparison of inpainting performance for several models . here",
    "we vary the number of gaussians used to compute the initial mixture , as well as the maximum number of gaussians allowed during propagation . [ cols=\"<,<,<,<,<\",options=\"header \" , ]     as a simple experiment , we timed these operations in matlab ( using random matrices and vectors ) .",
    "we found that computing 69948 inner products was approximately 10 times faster than computing the matrix operations shown in table [ tab : runtime ] .",
    "this leads us to believe that a low - level implementation of our belief - propagation algorithms may be significantly faster even than the results we have shown .",
    "our results have shown than even a @xmath4 model is able to produce very satisfactory inpainting performance .",
    "we believe that even this small model is able to capture much of the important information about natural images . while higher - order models exist @xcite",
    ", the improvements appear to be quite incremental , despite a significant increase in their execution time .",
    "while it is certainly the case that our results fall short of the state - of - the - art in terms of psnr , the differences are difficult to distinguish visually .",
    "it is therefore pleasing that we are able to produce competitive results within only a short period of time .",
    "we have not yet fully explored the possibility of using the junction - tree algorithm to inpaint images .",
    "unfortunately , determining whether a graph obeys the junction - tree property ( see section [ sec : propmethods ] ) is very expensive , meaning we simply used loopy belief - propagation in all cases , without even performing this test .",
    "however , there are many cases in which we can be _ sure _ that a junction - tree exists  for example , if the inpainting region is a scratch which is only one or two pixels wide . in such cases ,",
    "optimal results can be produced after only a single iteration , which would render our algorithm several times faster again .    in spite of this",
    ", we found that loopy belief - propagation tended to converge in very few iterations .",
    "while we believe it helped that the regions we are inpainting appear to be fairly ` tree - like ' , there is very little theory to support this claim . on the other hand",
    ", loopy belief - propagation often converges far slower when dealing with large regions , meaning that we can inpaint a ` scratch ' much faster than a ` coffee stain ' .",
    "we have also not considered the possibility that the corrupted pixels may contain some information about the original image .",
    "many gradient - ascent approaches implicitly exploit this possibility by initializing their algorithms using the corrupted pixels .",
    "if the restored image is ` close to ' the corrupted image , this can result in faster convergence .",
    "our approach is also able to deal with this possibility by augmenting the graphical model with an observation layer with the respective noise model for the damaged pixels .",
    "in this paper , we have developed a model for inpainting images quickly using belief - propagation .",
    "while image inpainting has previously been performed using low - order models by belief - propagation , and high - order models by gradient - ascent , we have presented new methods which manage to exploit the benefits of both , while avoiding their shortcomings .",
    "we have shown these algorithms to give satisfactory visual results and to be faster than existing gradient - based techniques , even in spite of our high - level implementation .",
    "anat levin , assaf zomet , and yair weiss .",
    "learning how to inpaint from global image statistics . in _",
    "iccv 03 : proceedings of the ninth ieee international conference on computer vision _",
    ", page 305 , washington , dc , usa , 2003 .",
    "ieee computer society .",
    "j.  b. macqueen .",
    "some methods of classification and analysis of multivariate observations . in _ proceedings of the fifth berkeley symposium on mathemtical statistics and probability _ , pages 281297 , 1967 .",
    "d.  martin , c.  fowlkes , d.  tal , and j.  malik .",
    "a database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics . in _ proc .",
    "8th intl conf . computer vision _",
    ", volume  2 , pages 416423 , july 2001 .",
    "julian  j. mcauley , tibrio  s. caetano , alex  j. smola , and matthias  o. franz .",
    "learning high - order mrf priors of color images . in _",
    "icml 06 : proceedings of the 23rd international conference on machine learning _ , pages 617624 , new york , ny , usa , 2006 .",
    "acm press ."
  ],
  "abstract_text": [
    "<S> in this paper , we use belief - propagation techniques to develop fast algorithms for image inpainting . unlike traditional gradient - based approaches , which may require many iterations to converge , our techniques achieve competitive results after only a few iterations . on the other hand , while belief - propagation techniques are often unable to deal with high - order models due to the explosion in the size of messages , we avoid this problem by approximating our high - order prior model using a gaussian mixture . by using such an approximation , we are able to inpaint images quickly while at the same time retaining good visual results . </S>"
  ]
}