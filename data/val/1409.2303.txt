{
  "article_text": [
    "recently questions like , _ why go to so much effort to acquire all the data when most of what we get will be thrown away?_;_can we not just directly measure the part that will not end up being thrown away ? _ , that were paused by donoho@xcite and others , triggered a new way of sampling or sensing called compact ( `` compressed '' ) sensing ( cs ) .    in cs",
    "the task is to estimate or recover a sparse or compressible vector @xmath0 from a measurement vector @xmath1 .",
    "these are related through the linear transform @xmath2 . here",
    ", @xmath3 is a sparse vector and @xmath4 . in the seminal papers@xcite -@xcite , @xmath3",
    "is estimated from @xmath5 , by solving a convex optimization problem @xcite,@xcite .",
    "others have used greedy algorithms , like subspace pursuit ( sp)@xcite , orthogonal matching pursuit ( omp ) @xcite to solve the problem . in this paper",
    "the focus is rather on the convex optimization methods .",
    "and we consider the noisy measurement system and the linear relation becomes    @xmath6    here , @xmath5 and @xmath3 are as in above where as the noise term , @xmath7 .",
    "there exists a large body of work on how to efficiently obtain an estimate for @xmath3 . and",
    "the performances of such estimators are measured using metrics like restricted isometric property ( rip ) @xcite , mutual coherence ( mc ) @xcite , yet there is apparently no consensus on the bounds in using such metrics .",
    "the tool used in this paper gives performance bounds of large size cs systems @xcite .",
    "+ generally the linear model @xmath8 is used to describe a multitude of linear systems like code division multiple access ( cdma ) and multiple antenna systems like mimo , to mention just a few .",
    "tools from statistical mechanics have been employed to analyze large cdma @xcite and mimo systems @xcite @xcite , and on in this paper the same wisdom is applied to analyze the performance of estimators used in cs .",
    "guo and et al in @xcite used a bayesian framework for statistical inference with noisy measurements and characterize the posterior distribution of individual elements of the sparse signal by describing the mean mean square error(mse ) exactly . to do so , they consider @xmath8 in a large system and applied the decoupling principle using tools from statistical mechanics .",
    "+ one can find also works that have used the tools from statistical mechanics to analyse cs system performances . to mention some , in @xcite as stated above , guo and et",
    "al used the tools to describe the minimum mean square error ( mmse ) estimator , in @xcite rangan and others used the maximum a posterior(map ) estimator of cs systems .",
    "these are referred as replica mmse claim and replica map claim in @xcite .    in @xcite",
    "-@xcite authors have used belief propagation and message passing algorithms for probabilistic reconstruction in cs using replica methods including rs .",
    "especially , in @xcite one finds excellent work about phase diagrams in cs systems while @xcite generalizes replica analysis using free random matrices .",
    "kabashima and et .",
    "al in @xcite , ganguli and sompolinsky in @xcite and takeda and kabashima @xcite -@xcite have shown statistical mechanical analysis of the cs by considering the noiseless recovery problem and they indicated that rsb analysis is needed in the phase regimes where the rs solution is not stable . in this paper",
    "the performance of those cs estimators , considered as map estimator , is shown for the noisy problem by using the replica method including rs and rsb as in @xcite -@xcite , where the rsb ansatz gives better solution when the replica symmetry ( rs ) solution is unstable .",
    "this work is kind of an extension of @xcite from mimo systems to the cs systems .",
    "+ the paper is organized as follows . in section [ sec : problem setting ] the estimator in cs system are presented and redefined using the bayesian framework , and based on that we present our basis of analysis in section [ sec : analysis ] which is the replica method from the statistical physics and apply it on the different cs estimators which are presented generally as a map estimator . in section [ sec : particular example ] we showed our analysis using a paricular example , and section [ sec : conclusion ] presents conclusion and of future work .",
    "beginning with a given vector of measurements @xmath1 and measurement matrix @xmath9 , assuming noisy measurement with @xmath10 being i.i.d .",
    "gaussian random variables with zero mean and covariance matrix @xmath11 , estimating the sparse vector @xmath12 is the problem that we are considering where these variables are related by the linear model .",
    "various methods for estimating @xmath3 may be used . the classical approach to solving inverse problems of such type is by least squares ( ls ) estimator in which no prior information is used and its closed form is @xmath13 which performs very badly for the cs estimation problem we are considering since it does not find the sparse solution . another approach",
    "to estimate @xmath3 is via the solution of the unconstrained optimization problem    @xmath14    where @xmath15 is a regularizing term , for some non - negative @xmath16 . by taking @xmath17 ,",
    "emphasis is made on a solution with lp norm , and @xmath18 is defined as a penalizing norm .",
    "when @xmath19 , we get    @xmath20    this is penalizing the least square error by the l2 norm and this performs badly as well , since it does not introduce sparsity into the problem . when @xmath21 , we get the l0 norm , which is defined as    @xmath22    the number of the non zero intries of @xmath3 , which actually is a partial norm since it does not satisfy the triangle inequality property , but can be treated as norm by defining it as in @xcite , and get the l0 norm regularizing estimator @xmath23 which gives the best solution for the problem at hand since it favors sparsity in @xmath3 . nonetheless , it is an np- hard combinatorial problem .",
    "instead , it has been a practice to approximate it using l1 penalizing norm to get the estimator @xmath24 which is a convex approximation to the l0 penalizing solution [ l0 ] . the best solution for estimate of the sparse vector @xmath25",
    "is given by the zero - norm regularized estimator which is a hard combinatorial problem .",
    "these estimators , - , can equivalently be presented as solutions to constrained optimization problem @xcite-@xcite .",
    "this constrained optimization version of is known as the l1 penalized l2 minimization called lasso ( least absolute shrinkage and selection operator ) or bpdn(basis persuit denoising ) , which can be set as quadratic programing ( qp ) and quadratic constrained linear programing ( qcpl ) optimization problems . in the following subsection the above estimators are presented as a map estimator in bayesian framework .",
    "equivalently , the estimator of @xmath26 in can generally be presented as map estimator under the bayesian framework .",
    "assume a prior probability distribution for @xmath27 to be @xmath28 where the cost function @xmath29 is some scalar - valued , non negative function with @xmath30 and @xmath31 such that for sufficiently large @xmath16 , @xmath32 is finite as in @xcite . and",
    "let the assumed variance of the noise be given by @xmath33 where @xmath34 is system parameter which can be taken as @xmath35 where @xmath36 is the assumed variance for each component of @xmath37 .",
    "note that we incorporate the sparsity in the prior pdf via @xmath38 . by the probability density function of @xmath39 given @xmath40",
    "is given by @xmath41 and prior distribution of @xmath40 by , the posterior distribution for the measurement channel according to bayes law is @xmath42    then the map estimator can be shown to be @xmath43 now , as we choose different penalizing function in ( [ map ] ) we get the different estimators defined above in equations , , and but this time under the bayesian framework as a map estimator @xcite .    1 .",
    "linear estimators : when @xmath44 reduces to @xmath45 which is the lmmse estimator .",
    "lasso estimator : when @xmath46 we get the lasso estimator and becomes @xmath47 3 .",
    "zero - norm regularization estimator : when @xmath48 , we get the zero - norm regularization estimator and becomes @xmath49    whether these minimization problems are solvable or not the replica analysis results can provide the asymptotic performances of all the above estimators via replica method as showed in @xcite , @xcite , @xcite , @xcite and @xcite .",
    "we apply rs ansatz as used by mller and et al in @xcite and rsb ansatz as used by zaidel and et al @xcite on vector precoding for mimo .",
    "actually , this work is an extension of the rsb analysis to mimo systems done in @xcite to the cs system .",
    "the performance of the bayesian estimators like mmse and map can be done using the pdf of the error vector .",
    "the error is random and it should be centered about zero for the estimator to perform well .",
    "kay showed in that way ( see section 11.6 in @xcite ) the performance analysis of mmse estimator .",
    "we believe in general that inference for the asymptotic performance of map estimators is best done with statistical mechanical tools including rsb assumption and this is done in the sense of the mean square error ( mse ) .",
    "the posterior distribution is a sufficient statistics to estimate @xmath3 @xcite and the denominator is called the normalizing factor or evidence in bayesian inference according to @xcite and partition function in statistical mechanics . actually , it is this connection , which gives the ground to apply the tools , which are used in statistical mechanics .",
    "so the task of evaluating the above estimators for the sparse vector @xmath26 can be translated to the statistical physics framework . and let us justify first how the analysis using statistical mechanical tool is able to do it .",
    "+   + define the gibbs - boltzmann distribution as    @xmath50    where @xmath51 is a constant known as the inverse temperature in the terminology of physical systems . for small @xmath51 , the prior probability becomes flat , and for large @xmath51 , the prior probability has sharp modes .",
    "@xmath52 , which is an expression of the total energy of the system , is called the hamiltonian in physics literature and @xmath53 is the partition function given by    @xmath54    often the hamiltonian can be given by a quadratic form like    @xmath55    with @xmath56 being a random matrix of dimension @xmath57 .",
    "then the minimum average energy per component of @xmath58 can be given by @xmath59 for our system that we considered to address , which is given by or equivalently by , the hamiltonian becomes @xmath60 compared to , the hamiltonian in has regularizing term in addition to the quadratic form , which is the energy of the error , in which the regularizing term @xmath61 is accountable for addressing the problem in the cs .",
    "the gibbs - boltzman distribution is a solution to or to in general , after plugging and since they are equivalent problems .",
    "the normalizing factor ( aslo called the partition function ) of this distribution is central for calculating many important variables and we shall begin from this term to analyse the cs estimators performance . + assuming that @xmath3 and @xmath27 being drawn from the same discrete set ( we shall later provide an example from such a set )",
    ". the partition function of the posterior distribution given in becomes @xmath62},\\ ] ] by using and .",
    "the posterior distribution depends on the predetermined random variables @xmath5 and @xmath63 called quenched states in physics literature @xcite , @xcite .",
    "that is , we use fixed states @xmath64 instead of @xmath65 for the large system limit , as @xmath66 , while maintaining @xmath67 fixed .",
    "we then calculate the nth moment of the partition function @xmath68 with respect to the predetermined variables , @xmath69 replicas , hence the name replica method came from .",
    "the replicated partition function is then given by @xmath70},\\ ] ] where @xmath71 . and",
    "after substituting @xmath5 , it becomes @xmath72 } .\\ ] ]    averaging over the noise @xmath73 first , we get @xmath74 } ,    \\end{aligned}\\ ] ] where @xmath75 and it is assumed to decompose into @xmath76 and @xmath77 is a diagonal matrix while @xmath78 is @xmath57 orthogonal matrix assumed to be drawn randomly from the uniform distribution defined by the haar measure on the orthogonal group . for more clarity on this one can see  in @xcite . and",
    "@xmath79 is given by @xmath80 further averaging what we get on the right side of over the cross correlation matrix @xmath81 , by assuming the eigenvalue spectrum of @xmath81 to be self - averaging , we get @xmath82 } \\biggr ) \\\\&=   \\sum\\limits_{\\{\\x^{a}\\}}e^ {   \\frac { -\\beta\\gamma } { \\sigma_{u}^2 }   \\sum\\limits_{a=1}^{n }   f(\\x^{a } ) } \\underset{\\j }    { \\operatorname{e } } \\biggl(e^{-\\beta \\bigl [ \\frac { 1}{2 } tr \\j \\l(n )    \\bigr ] } \\biggr )   \\end{aligned}\\ ] ]    the inner expectation in is the harish -chandra -itzykoson - zuber integral ( again see in @xcite and @xcite and the references therein ) .",
    "the plan here is to evaluate the fixed - rank matrices @xmath83 as @xmath84 .",
    "further following the explanation in @xcite becomes @xmath85 where @xmath86 is the r - transform of the limiting eigenvalue distribution of the matrix j ( see , definition 1 in @xcite of r - transform or in @xcite and @xcite for better understanding of r - transform ) and @xmath87 denote the eigenvalues of the @xmath88 matrix @xmath89 , with @xmath90 defined through @xmath91 } ,    % \\q_{ab}\\equiv - \\frac{1}{n } \\sum\\limits_{i=1}^{n }    { x_{i}^{a}}^t   x_{i}^{b } \\hspace { 5 mm } for    \\hspace { 5 mm }   a , b=0,1 , \\cdots , n\\ ] ] for @xmath92 . after applying replica trick ,",
    "the average free energy can be given by @xmath93 and the energy of the error can be calculated from the average free energy as @xmath94 where we get by using one of the assumptions used in replica calculations , after interchanging the order of the limits we assumed we get the same result .",
    "further , for @xmath95 we have @xmath96 since the additive exponential terms of order @xmath97 have no effect on the results when taking saddle point integration in the limiting regime as @xmath84 due to the factor @xmath98 outside the logarithm in any such terms are dropped further for notational simplicity as in @xcite .    in order to find",
    "the summation in we employed the procedure in @xcite and the @xmath99 dimensional space spanned by the replicas is split into subshells , defined through @xmath100 matrix @xmath101 @xmath102 the limit @xmath84 able us to use saddle point integration .",
    "hence we can have the following general result as similar to @xcite but extended in this work with the term , which pertains to cs , where we have given the expression that helps to evaluate the performances of the cs estimators using equation .",
    "_ [ prop : the limiting energy ] the energy @xmath103 from , for any inverse temperature @xmath104 , any structure of @xmath90 consistent with , and any r - transform @xmath105 such that @xmath106 is well - defined , is given by @xmath107 , \\ ] ] where @xmath90 is the solution to the saddle point equation @xmath108    see appendix [ app : prof of propostion the limiting energy ] .",
    "further , to get specific results we need to assume simple structure onto the @xmath109 cross correlation matrix @xmath90 at the saddle point .",
    "so we assume two different assumptions for the entries of @xmath90 called ansatz : replica symmetry(rs ) and replica symmetric breaking ( rsb ) ansatz .",
    "then compare the above limiting energy for the different estimators considered in this paper using the two types of ansatz for the cs system . that is the main purpose that we want to show in this paper .",
    "and we took the structures similar to@xcite :    1 .",
    "replica symmetry ansatz : @xmath110 2 .",
    "one replica symmetry breaking ansatz : + @xmath111    applying these assumptions we found some results as given in the following subsections . in the first subsection",
    "we assume the rs ansatz which can be considered as the extension of @xcite . in the last two subsections we assume rsb ansatz as an extension of @xcite to cs .",
    "consider the lasso estimator given in , which is equivalent to the solution of the main unconstrained optimization problem in @xmath112 penalized sense .",
    "its performance can be expressed in terms of the limiting energy penalty per component using two macroscopic variables @xmath113 and @xmath114 given by @xmath115 @xmath116 where @xmath117 @xmath118    @xmath119    and @xmath120 is refering about integration over gaussian measure , while @xmath121 refers to integration over the pdf of @xmath122 ( see appendix b ) . under rs",
    "ansatz assumptions we then get the following statement .    _ _ [ prop : the limiting energy for the lasso estimator in rs ] given the lasso estimator in and the macroscopic variables @xmath113 and @xmath114 , in addition given the conditions in proposition 1 , the energy in simplifies to @xmath123    see appendix b.      moving to the very purpose of the present paper , we use rsb ansatz instead of rs and we repeat what has been done in the above subsections . the limiting energy in this case involves four macroscopic variables like @xmath124 , @xmath125 , @xmath126 , and @xmath127 , which can be given by the following fixed point equations as @xmath128 and @xmath129 , as showed in appendix d , and using the compact notation as in @xcite",
    ". let @xmath130 and its normalized version @xmath131    @xmath132    @xmath133    where@xmath134 and @xmath135 where the other variables @xmath136 , @xmath137 , and @xmath138 , are given by @xmath139},\\\\ f_{1}&\\displaystyle_{\\longrightarrow } ^{n\\rightarrow 0 }    \\frac { 1}{\\sigma_{u}^2 } \\sqrt { q_{1 } r'(\\frac{-b_{1}-\\mu_{1}p_{1 } } { \\sigma_{u}^2 } ) } % f_{1}&\\displaystyle_{\\longrightarrow } ^{n\\rightarrow 0}\\sqrt{\\frac{1}{\\beta } \\biggl[\\frac{\\sigma_{0}^2}{\\sigma_{u}^4}r(\\frac{-b_{1}-\\mu_{1 } p_{1 } } { \\sigma_{u}^2 } ) + \\frac{(\\sigma_{u}^2\\beta q_{1}+\\sigma_{0}^2(b_{1}+\\mu_{1}p_{1}))}{\\sigma_{u}^6}r'(\\frac{-b_{1}-\\mu_{1}p_{1 } } { \\sigma_{u}^2 } ) \\biggr]},\\end{aligned}\\ ] ] then the following two statements are the extensions of the propositions in @xcite to cs problems .",
    "_ _ [ prop : the limiting energy for the lasso estimator in rsb ] given the lasso estimator in and suppose the random matrix @xmath81 satisfies the decomposability property .",
    "then under some technical assumptions , including one - step replica symmetry breaking , and the macroscopic variables given by the above fixed point equations , the effective energy penalty per component converges in probability as @xmath140 , @xmath141 , @xmath142 , to    @xmath143    see appendices d.      the lasso estimation is considered as the convex relaxation of the zero - norm regularizing estimation . since the latter is a non - convex problem its performance is better evaluated when we use rsb ansatz .",
    "so extending proposition to this estimator we get the following statement .",
    "_ _ [ prop : zero - norm regularizing estimator with 1rsb ansatz ] given the zero - norm regularizing estimator in and suppose the random matrix @xmath81 satisfies the decomposability property . then under some technical assumptions , including one - step replica symmetry breaking , the effective energy penalty per component converges in probablity as @xmath140 , @xmath141 , @xmath142 , to @xmath144    see appendix d.",
    "assume the original vector @xmath0 follows a bernoulli - gaussian mixture distribution .",
    "so following the bayesian framework analysis in section [ sec : analysis ] , let @xmath27 be composed of random variables with each component obeying the pdf @xmath145 where @xmath146 , with @xmath147 being the number of non zero entries of @xmath27",
    ". with out loss of generality , let @xmath148 , @xmath149 and @xmath150 vary between @xmath151 and @xmath152 . also lets assume that the entries of the measurement matrix @xmath63 follow i.i.d .",
    "gaussian random variable of mean zero and variance 1/m .",
    "in addition let @xmath36 be such that the signal to noise ratio is @xmath153 .",
    "we have simulated equations ( 2.7 ) and ( 2.8 ) .",
    "figure 1 shows mse versus @xmath149 of the two estimators , where we se that the @xmath154 penalizing estimator , lmmse , is not as good as the @xmath112 penalizing estimator in general .",
    "figure 2 shows mse versus @xmath150 of the two estimators and we see that lmmse is not sensitive to the sparsity of the vector as compared to the @xmath112 penalizing estimator .",
    "note that we have plotted the @xmath112-penalizing estimator using different algorithms : lasso , l1-ls , log - bar .    in both figures",
    ", we see that the least square estimator is not good for the compressive sensing problem .",
    "in addition , we also observed that simulating the @xmath155 penalizing estimator is hard .",
    "however , it is possible to apply statistical physics tools , including replica methods , to analayze the performances of all the estimators mentioned above , including zero norm estimator . in @xcite ,",
    "median square error was used to compare the different estimators given by - as shown here in figure 3 .",
    "what we do here is that we include 1rsb ansatz analysis of the performance of the cs estimators as each of them are presented here as a map estimator .",
    "actually it is one of the conjuctures made by mller and others that the performance of map estimators is best done using one step rsb .",
    "and we showed it here via the minimized energy expressions as given in the propositions by the equations , , and .",
    "considering the macroscopic variables given by and and pluging the assumed distributions above and simplyfying it one more step , the fixed point equations become @xmath156 using these macroscopic variables in we find the limiting energy numerically which is given under propostion [ prop : the limiting energy for the lasso estimator in rs ] and the result is shown in figure [ fig : the limitting energy comparison ] .",
    "considering the same bernoulli - gaussian mixture distribution assumed in this section we consider the macroscopic variables which arises from one step replica symmetry breaking ( 1rsb ) ansatz . then the minimum energy per component as @xmath157 , while @xmath149 is finite ratio , which are given by and are dependent up on four macroscopic variables given by - .",
    "the ther first are simplified further as follows :    we can further simplify - as follows @xmath158 @xmath159    it is possible to simplify these results further and give numerical results .",
    "but this is deferred for further work .",
    "we expect that the free energy from the rsb ansatz to be greater than the free energy from the rs ansatz for the zero - norm regularizing , which can be seen from the analytical terms which have more parameters in",
    ". however , for lasso these free energy , hence the energy error , will be quite similar since for convex minimization problems there is one global minimum and rs ansats is sufficient enough to produce the solution .",
    "in this paper we have used the replica method to analyze the performance of the estimators used in compressed sensing which can be generalized as map estimators . and the performance of map estimators can well be shown using replica method including one - step replica breaking ansatz . it is a philosophical standpoint that 1rsb enough to analyze the estimators like map .",
    "we have only showed here for one particular example for the cs problem , i.e. for bernoulli - gaussian distribution .",
    "one may be interested to verify it using different examples .",
    "in addition we have only compared the estimators performance based on the free energy , but one can also use other metrics such as comparing the input / out put distribution using replica analysis as it is done in @xcite . the main result of this paper is analytical analysis for the performance of the estimators used in cs and many things can be extended including efficient algorithms in implementing the numerical analysis .",
    "we are grateful to lars lundheim , rodrigo vicente de miguel and benjamin m. zaidel for interesting discussions and suggestions .",
    "in classical probability theory ( cpt ) one is concerned with the densities , moments and comulants of elements of random matrices . where as in random matrix theory ( rmt ) also called ( free random variable calculus ) , one is engaed in finding the spectral densities , moments and cumilants ( by professor maciej a. novak ) . as",
    "fourier transfom is the generating function for the moments in cpt , green s function ( also called stieltjes transform ) is the generating function for the spectral moments defined as @xmath160 where @xmath161 is @xmath57 random matrix and @xmath162 is of the same size unit matrix , @xmath163 are the eigenvalues , and @xmath164 is the spectral moment .",
    "the integral is over the support set of the eigenvalues .",
    "the generating function for the cumulants of the cpt is given by the logarithm of the fourier transfom .",
    "in similar maner to the above section we can define the generating function for spectral cumulants .",
    "it is called the r - transform ( voiculescu,1986 ) .",
    "it is given by    @xmath165    where @xmath166 are the spectral cumulants of the random matrix @xmath167 .",
    "we can relate r - transform with greens s function as follows : @xmath168 the spectral density of the matrix @xmath169 converges almost surely to the marchenko - pastur law as @xmath170 @xcite . and",
    "the r - transform of this matrix is given by    @xmath171    and its derivative with respect to z becomes @xmath172 where @xmath173 is system load .",
    "the avarage energy penality can be derived from the average free energy given in @xmath174 where @xmath95 is given by .",
    "using as the splitting of the space , we get @xmath175 where @xmath176 is the integration measure , @xmath177dw   \\end{aligned}\\ ] ] @xmath178 @xmath179 denotes probability weight of the subshell composed of dirac - functions in the real line .",
    "this procedure is a change of integration variables in multiple dimensions where the integration of an exponential function over the replicas has been replaced by integration over the variables @xmath90 . to evaluate @xmath180 we follow @xcite , @xcite and represent the dirac measure using the fourier transform as @xmath181 where @xmath182 and this gives      where @xmath184 assuming @xmath185 , which is the sparsity enforcer as described above in lasso estimator , and after doing some rearrangements , the inner expectation of can be given by @xmath186 now defining @xmath187 we can get @xmath188 following the i.i.d .",
    "assumption for the component of the sparse vector @xmath27 , and applying the strong law of large numbers as @xmath189 we get @xmath190 where , @xmath191 is vector of dimention @xmath69 .",
    "next we apply the saddle point integration concept on the remaining part of , i.e. , as @xmath192 the integrand will be dominated by the exponential term with maximal exponent . hence in only the subshell that corresponds to this extremal value of the correlation between the vectors",
    "@xmath193 is relevant for the calculation of the integral .",
    "@xmath194 therefore , at the saddle point we have the following equations with partial derivatives being zero ( see the proof in appendix b of @xcite ) : @xmath195=\\0    \\hspace{7 mm }    and\\ ] ] @xmath196=\\0 .\\ ] ] and from the former we get @xmath197 and from the later , using we finally get @xmath198          the variables @xmath113 , @xmath202 , @xmath126 , @xmath125,@xmath124 , @xmath203,@xmath204,@xmath137,@xmath138,@xmath136 , and @xmath127 are called the macroscopic variables and they are all functions of n. they all can be calculated from the saddel point equations that we shortly will derive .",
    "first let us try to prove propostion [ prop : the limiting energy for the lasso estimator in rs ] using the ansatz in and .",
    "we do it using equations , and and we apply the saddelpoint integration rule . what matters most becomes the argument of the exponential in .",
    "so we first find @xmath205 , @xmath206 , @xmath207 and in addition we will find the macroscopic parametrs mentioned before since our limiting energy penality expressions for the different estimators considered in this paper are calculated interms of the macroscopic variables .",
    "hence using and we get @xmath208 and using and again we get @xmath209 from ( b.4 ) to ( b.7 ) we apply completing the square on the exponential of the argument and the hubbard - stratonovich transform , @xmath210 where @xmath211 is gaussian measure defined as before , to linearize the exponential argument .",
    "and we finally transformed the problem to a singele integral and a single summation problem . to evaluate @xmath206 we should first find the eigenvalues of the matrix l(n ) . under the rs",
    "ansatz the matrix l(n ) has three types of eigenvalues : @xmath212 @xmath213 and @xmath214 , and the numbers of degeneracy for each are 1 , n-1 , and n - n , respectively.thus we get @xmath215 the integral in is dominated by the maximum argument of the exponential function .",
    "therefore , the derivative of @xmath216 with respect to @xmath113 and @xmath202 must vanish as @xmath84 . plugging and into and taking the partial derivatives we get @xmath217 @xmath218 respectively . after algebraic simplification and solving for @xmath204 and @xmath203",
    "we get @xmath219 @xmath220}.\\end{aligned}\\ ] ] and with the limit for @xmath128 @xmath221}.\\end{aligned}\\ ] ] by substituting into and doing the partial derivative of @xmath222 with respect to @xmath204 and @xmath203 and equating to zero we get , @xmath223 @xmath224 where @xmath225 so collecting the macroscopic variables in , , and and sending @xmath226 we have @xmath227 @xmath228}\\end{aligned}\\ ] ] @xmath229 @xmath230 and the fixed point equations , and further can be simplified via the saddle point integration rule in the limit @xmath231 as @xmath232 putting together the results above we have @xmath233 and the average free energy becomes @xmath234 r\\bigl ( \\frac{-(b_{0}+n\\beta q_{0 } ) } { \\sigma_{u}^2+n\\sigma_{0}^2 } \\bigr )   \\\\",
    "\\nonumber & + \\frac{-(b_{0}+n\\beta q_{0 } ) } { ( \\sigma_{u}^2+n\\sigma_{0}^2)}\\bigl [ -\\frac{\\bigl(\\beta q_{0}(\\sigma_{u}^2+n\\sigma_{0}^2 ) -(b_{0}+n\\beta q_{0 } ) \\sigma_{0}^2\\bigr ) } { ( \\sigma_{u}^2+n\\sigma_{0}^2)^2 } \\bigr]r'\\bigl ( \\frac{-(b_{0}+n\\beta q_{0 } ) } { ( \\sigma_{u}^2+n\\sigma_{0}^2 ) } \\bigr ) \\\\ & + \\int_{0}^{\\frac{b_{0}}{\\sigma_{u}^2 } } r(-w)dw   -     \\int_{\\mathbb{r } }    \\int_{\\mathbb{r } }    \\frac { \\zeta^n \\ln \\zeta } { \\zeta^n }   dz df_{x^{0}}(x^{0 } ) \\bigg\\}\\\\ \\nonumber & = \\frac{-b_{0 } } { \\sigma_{u}^2 }   r\\bigl ( \\frac{-b_{0 } } { \\sigma_{u}^2 } \\bigr)+   \\frac{b_{0}\\bigl ( \\beta q_{0}\\sigma_{u}^2 -b_{0}\\sigma_{0}^2    \\bigr )   } { \\sigma_{u}^6 }   r'\\bigl ( \\frac{-b_{0 } } { \\sigma_{u}^2 } \\bigr ) \\\\ & + \\int_{0}^{\\frac{b_{0}}{\\sigma_{u}^2 } } r(-w)dw   - \\int_{\\mathbb{r } } \\int_{\\mathbb{r } }    \\ln \\zeta   dz df_{x^{0}}(x^{0}).\\end{aligned}\\ ] ] coming back to the main goal , the solution for the main unconstrained optimization problem is given by the extremum of , it is calculated through the free energy by sending @xmath231 as follows @xmath235 this proves propostion [ prop : the limiting energy for the lasso estimator in rs ] . and to prove propostion [ prop : the limiting energy for the zero - norm estimator in rs ] what we need is to use the zero norm regularizing term instead of the l1 norm , i.e. using @xmath236 in , and the result will be as in which differ from through the calculation of the macroscopic varables which depend on the distributions of the components of @xmath27 .      turning to lasso estimator with rsb ansatz we first use and to get @xmath237 to evaluate @xmath238 we should first find the eigenvalues of the matrix @xmath83 . under the rsb ansatz the matrix @xmath83 has four types of eigenvalues : @xmath239 @xmath240 , @xmath241 and @xmath242 , and the numbers of degeneracy for each are 1 , @xmath243 , @xmath244 , and @xmath245 , respectively .",
    "hence @xmath246 further with entries of @xmath199 being rsb ansatz will have more involved terms than the rs ansatzs .",
    "i.e. , @xmath247 using the hubbard - stratonovich transform we can express as in ( c.f .",
    "[ @xcite , ( 66)- ( 70 ) ] ) as follows @xmath248",
    "+ \\beta^2g_{1}^2\\sum\\limits_{l=0}^{\\frac{n \\beta}{\\mu}-1}\\bigl| \\sum\\limits_{a=1}^{\\frac{\\mu}{\\beta } } ( x^{0}- x_{a+\\frac{l \\mu_{1}}{\\beta}})\\bigr|^2    }   \\nonumber\\\\ & \\hspace{50 mm }   \\cdot dz df_{x^{0}}(x^{0})\\nonumber\\\\   & =    \\int \\log   \\int_{\\mathbb c } \\biggl [ \\int_{\\mathbb c }    \\biggl (   \\sum\\limits_{\\{\\x \\in \\chi \\ } } \\mbox{$\\cal{k}$ } \\mbox { ( $ x$ , $ y$ , $ z$ ) }",
    "\\biggr ) ^{\\frac{\\mu_{1}}{\\beta } }   \\mbox{$dy$ } \\biggr]^ { \\frac { n \\beta } { \\mu_{1 } } }   \\mbox{$ dz df_{x^{0}}(x^{0 } ) $ } \\end{aligned}\\ ] ] where @xmath249 due to the partial dervative of @xmath250 with respect to the macroscopic variables @xmath126 , @xmath125 , and @xmath124 vanishes as @xmath189 by definition of the sadel point approximation . and pluging and in and calculating the partial derivatives and seting them to zero and after some algebraic manipulation we get the folowing set of equations @xmath251 solving for @xmath136 , @xmath138 , @xmath137 we get @xmath252},\\\\ f_{1}&=\\sqrt{\\frac{1}{n\\beta } \\biggl[\\frac{1}{\\sigma_{u}^2}r(\\frac{-b_{1}-\\mu_{1 } p_{1 } } { \\sigma_{u}^2 } ) -\\frac{1}{\\sigma_{u}^2+n\\sigma_{0}^2}r(\\frac{-b_{1}-\\mu_{1}p_{1 } -n\\beta q_{1 } } { \\sigma_{u}^2+n\\sigma_{0}^2 } ) \\biggr]},\\end{aligned}\\ ] ] and further with the limits @xmath226 @xmath253}.\\end{aligned}\\ ] ] and as @xmath254 we can simplify it further as @xmath255 also due to the partial derivatives of @xmath256 with respect to @xmath137 , @xmath138 , and @xmath136 , must also vanish as @xmath192 .",
    "this produces the following set of equations while taking @xmath257 .",
    "@xmath258 @xmath259 @xmath260 in addition when we take",
    "the partial derivative of @xmath261 with respect of @xmath127 is vanishes and yields at the limit as @xmath226 @xmath262 \\nonumber\\\\ & \\hspace{60 mm }   \\cdot \\mbox{$   dz   df_{x^{0}}(x^{0 } ) $ } \\end{aligned}\\ ] ] so as @xmath129 these fixed point equations can be simplified as follows : @xmath263 @xmath264 @xmath265 where @xmath266 puting together the results again as in and doing again the steps ( b.34 ) to ( b.38 ) for the rsb case @xmath267 \\nonumber\\\\   & -\\log m ( q_{1 } , p_{1 } , f_{1 } , \\mu_{1 } ) \\biggr\\}\\\\ & = \\underset{\\beta \\rightarrow \\infty } { \\operatorname{lim } } \\frac{1 } { \\beta } \\biggl\\ {    ( \\frac{b_{1}+\\mu_{1 } p_{1 }   } { \\sigma_{u}^2 } ) r(\\frac{-b_{1}-\\mu_{1 } p_{1 }   } { \\sigma_{u}^2 } ) \\\\   & +   ( \\frac{b_{1}+\\mu_{1 } p_{1 }   } { \\sigma_{u}^2})\\frac { ( \\beta q_{1 } \\sigma_{u}^2 -(b_{1}+\\mu_{1 } p_{1 } ) \\sigma_{0}^2 ) } { \\sigma_{u}^4 }   r'(\\frac{-b_{1}-\\mu_{1 } p_{1 }   } { \\sigma_{u}^2 } )   \\nonumber   \\\\ & + \\frac{\\beta}{\\mu_{1}}\\int_{0}^{\\frac{b_{1}+\\mu_{1 } p_{1}}{\\sigma_{u}^2 }   } r(-w)dw +   ( 1-\\frac { \\beta}{\\mu_{1}})\\int_{0}^{\\frac{b_{1}}{\\sigma_{u}^2 }   } r(-w)dw   \\nonumber\\\\ & + \\bigl [ b_{1}(\\beta f_{1}^2+\\beta g_{1}^2- e_{1 } ) + \\mu_{1 } ( q_{1 }   + p_{1 } ) ( \\beta f_{1}^2+\\beta g_{1}^2-\\frac{\\beta}{\\mu_{1 } } e_{1 } ) - \\mu_{1 } q_{1}\\beta f_{1}^2   \\bigr ]    \\nonumber\\\\   &      - \\frac{\\beta}{\\mu_{1 } } \\int \\log   \\int_{\\mathbb c }   \\int_{\\mathbb c }    \\biggl (   \\sum\\limits_{\\{\\x \\in \\chi \\ } } \\mbox{$\\cal{k}$ } \\mbox { ( $ x$ , $ y$ , $ z$ ) }   \\biggr ) ^{\\frac{\\mu_{1}}{\\beta } }   \\mbox{$dy$ }    \\mbox{$ dz   df_{x^{0}}(x^{0 } ) $ } \\biggr \\ }   \\\\ & = \\frac {   q_{1 } } { \\sigma_{u}^2 } ( \\frac{b_{1}+\\mu_{1 } p_{1 }   } { \\sigma_{u}^2 } ) r(\\frac{-b_{1}-\\mu_{1 } p_{1 }   } { \\sigma_{u}^2 } )    + \\frac{1}{\\mu_{1}}\\int_{0}^{\\frac{b_{1}+\\mu_{1 } p_{1}}{\\sigma_{u}^2 }   } r(-w)dw -   \\frac { 1}{\\mu_{1}}\\int_{0}^{\\frac{b_{1}}{\\sigma_{u}^2 }   } r(-w)dw   \\nonumber \\nonumber \\\\ & + \\bigl [ ( b_{1}+ \\mu_{1 } ( q_{1 }   + p_{1 } ) ) ( f_{1}^2 + g_{1}^2)- e_{1}(q_{1 }   + p_{1 } ) - \\mu_{1 } q_{1 } f_{1}^2   \\bigr ] \\nonumber\\\\ &   -\\underset{\\beta \\rightarrow \\infty } { \\operatorname{lim } } \\frac{1 } { \\beta } \\biggl\\{\\frac{\\beta}{\\mu_{1}}\\int   \\log   \\int_{\\mathbb c }   \\int_{\\mathbb c }    \\biggl (   \\sum\\limits_{\\{\\x \\in \\chi \\ } } \\mbox{$\\cal{k}$ } \\mbox { ( $ x$ , $ y$ , $ z$ ) }   \\biggr ) ^{\\frac{\\mu_{1}}{\\beta } }   \\mbox{$dy$ }    \\mbox{$ dz   df_{x^{0}}(x^{0 } ) $ } \\biggr \\ } \\\\ % \\frac{f_{1}+\\mu p_{1 }   } { \\sigma_{u}^2 } r(\\frac{-f_{1}-\\mu p_{1 }   } { \\sigma_{u}^2})+ \\frac{\\beta   q_{1 } } { \\sigma_{u}^2 } r'(\\frac{-f_{1}-\\mu p_{1 }   } { \\sigma_{u}^2 } ) % % & = \\underset{\\beta \\rightarrow \\infty } { \\operatorname{lim } } \\frac{1 } { \\beta } \\biggl\\{+   - \\frac{(f_{1}+\\mu p_{1 } ) \\sigma_{0}^2 } { \\sigma_{u}^4 }   r'(\\frac{-f_{1}-\\mu p_{1 }   } { \\sigma_{u}^2 } )    + \\beta n q_{1 } } { \\sigma_{u}^2+n\\sigma_{0}^2 } } r(-w)dw + \\frac{\\beta}{\\mu_{1}}\\int_{0}^{\\frac{b_{1}+\\mu_{1 } p_{1}}{\\sigma_{u}^2 }   } r(-w)dw   % &   +   ( 1-\\frac { \\beta}{\\mu_{1}})\\int_{0}^{\\frac{b_{1}}{\\sigma_{u}^2 }   } r(-w)dw - \\underset{n \\rightarrow 0}{\\operatorname{lim } }   \\frac{\\partial } { \\partial n}\\bigl\\{\\log m ( q_{1 } , p_{1 } , f_{1 } , \\mu_{1 } ) \\bigr\\ } \\nonumber\\\\&+ \\bigl[\\beta^2 f_{1}^2 ( b_{1}+\\mu_{1 } p_{1 } )    + \\beta^2 g_{1}^2 ( b_{1}+\\mu_{1}(q_{1 }   + p_{1 } ) -\\beta e_{1}(q_{1 }   + p_{1 } + \\frac{b_{1}}{\\beta } )    \\bigr ] \\biggr\\ }    \\\\ & = \\frac {   1 } { \\sigma_{u}^2 } ( q_{1 } + p_{1 } + \\frac{b_{1}}{\\mu_{1}})r(\\frac{-b_{1}-\\mu_{1 } p_{1 }   } { \\sigma_{u}^2 } )   -\\frac{b_{1}}{\\mu_{1}\\sigma_{u}^2}r(-\\frac{b_{1 } } { \\sigma_{u}^2 } ) \\nonumber   \\\\ &     \\hspace{10mm}+    q_{1 } ( \\frac{b_{1}+\\mu_{1 } p_{1 }   } { \\sigma_{u}^2 } )   r'(\\frac{-b_{1}-\\mu_{1 } p_{1 }   } { \\sigma_{u}^2})\\\\ & = \\scriptstyle{\\frac {   1 } { \\sigma_{u}^2 } ( q_{1 } + p_{1 } + \\frac{b_{1}}{\\mu_{1}})r(\\frac{-b_{1}-\\mu_{1 } p_{1 }   } { \\sigma_{u}^2 } )   -\\frac{b_{1}}{\\mu_{1}\\sigma_{u}^2}r(-\\frac{b_{1 } } { \\sigma_{u}^2 } ) }   \\nonumber\\\\ &   \\scriptstyle { +    q_{1 } ( \\frac{b_{1}+\\mu_{1 } p_{1 }   } { \\sigma_{u}^2 } )   r'(\\frac{-b_{1}-\\mu_{1 } p_{1 }   } { \\sigma_{u}^2 } )      }   \\end{aligned}\\ ] ]    1 d. donoho,_compressed sensing_,1em plus 0.5em minus 0.4emieee trans . inform .",
    "theory , vol .",
    "4 , pp . 1289 - 1306 , 2006 .",
    "e. candes , j. romberg , and t. tao,_robust uncertainty principles : exact signal reconstruction from highly incomplete frequency information , _",
    "1em plus 0.5em minus 0.4emieee trans .",
    "theory , vol .",
    "489 - 509 , feb . 2006 .",
    "m. figueiredo , r. nowak , and s. wright,_gradient projection for sparse reconstruction : application to compressed sensing and other inverse problems _",
    ", 1em plus 0.5em minus 0.4emieee j. sel .",
    "top . signal process .",
    "586 - 597 , 2007 .",
    "d. guo , d. baron , and s. shamai,_a single - letter characterization of optimal noisy compressed sensin _ ,",
    "1em plus 0.5em minus 0.4emin proceedings of the forty- .",
    "seventh annual allerton conference on communication , control , and .",
    "computing , monticello , il , october 2009 .",
    "tanaka , t.,_a statistical mechanics approach to large system analysis of cdma multiuser detectors_,1em plus 0.5em minus 0.4em information theory , ieee transactions on , vol.48 , no.11 , pp.2888 - 2910 , nov 2002 .",
    "ralf r. mller , giusi alfano , benjamin m. zaidel , rodrigo de miguel,_applications of large random matrices in communications engineering_,1em plus 0.5em minus 0.4em invited for arxiv:1310.5479v1 [ cs.it ] 21 oct 2013 .",
    "s. sarvotham , d. baron , and r. g. baraniuk , , _ _ compressed sensing reconstruction via belief propagation,__1em plus 0.5em minus 0.4emtech .",
    "tree0601 , rice university , houston , tx 77005 , usa sundeep rangan , _ estimation with random linear mixing , belief propagation and compressed sensing_,1em plus 0.5em minus 0.4emarxiv:1001.2228v2 [ cs.it ] 18 may 2010 .",
    "florent krzakala , marc m ezard , francois sausset , yifan sun and lenka zdeborov a,_probabilistic reconstruction in compressed sensing : algorithms , phase diagrams , and threshold achieving matrices _ ,",
    "1em plus 0.5em minus 0.4emarxiv:1206.3953v1 [ cond-mat.stat-mech ] 18 jun 2012 .",
    "david l. donoho , adel javanmard and andrea montanari : _ information - theoretically optimal compressed sensing via spatial coupling and approximate message passing _",
    "arxiv:1112.0708v2 [ cs.it ]",
    "19 jan 2013 arian maleki , laura anitori , zai yang , and richard baraniuk,_asymptotic analysis of complex lasso via complex approximate message passing_,1em plus 0.5em minus 0.4emarxiv:1108.0477v2 [ cs.it ] 6 mar 2013 .",
    "antonia tulino , giuseppe caire , sergio verdu  and shlomo shamai ( shitz),_support recovery with sparsely sampled free random matrices_,1em plus 0.5em minus 0.4emarxiv:1208.5269v1 [ cs.it ] 27 aug 2012 .",
    "y. kabashima , t. wadayama , and t. tanaka , j. stat .",
    "( 2009 ) l09003 .",
    "s ganguli , h sompolinsky - physical review letters , 2010 - keck.ucsf.edu k takeda , y kabashima - arxiv preprint arxiv:1001.4361 , 2010 - arxiv.org k. takeda , s. uda and y. kabashima1 _ analysis of cdma systems that are characterized by eigenvalue spectrum _ ,",
    "1em plus 0.5em minus 0.4em europhys .",
    "76 1193 , 2006 .",
    "koujin takeda , atsushi hatabu and yoshiyuki kabashima _ statistical mechanical analysis of the linear vector channel in digital communication_,1em plus 0.5em minus 0.4em j. phys . a : math .",
    "40 14085 , 2007 .",
    "r. r. muller , d. guo , and a. l. moustakas,_vector precoding in high dimensions : a replica analysis_,1em plus 0.5em minus 0.4em ieee journal on selected areas in communications , vol .",
    "530 - 540 , april 2008 .",
    "rodrigo de miguel , ralf mueller , _ on convex vector precoding for multiuser mimo broadcast channels_,1em plus 0.5em minus 0.4emieee transactions on signal processing , vol . 57 , issue:11 , june 2009 .",
    "benjamin zaidel , ralf mueller , aris moustakas , rodrigo de miguel _ vector precoding for gaussian mimo broadcast channels : impact of replica symmetry breaking._,1em plus 0.5em minus 0.4emarxiv:1001.3790v3 [ cs.it ] j 25 feb , 2011",
    ". s. m. kay , _ fundamentals of statistical signal processing : estimation theory_,1em plus 0.5em minus 0.4emenglewood cliffs , nj : prentice - hall , 1993 .",
    "david j.c .",
    "mackay,_information theory , inference , and learning algorithms._,1em plus 0.5em minus 0.4emuniversity of cambridge , 2003 ."
  ],
  "abstract_text": [
    "<S> compressive sensing ( cs ) is a new methodology to capture signals at lower rate than the nyquist sampling rate when the signals are sparse or sparse in some domain . </S>",
    "<S> the performance of cs estimators is analyzed in this paper using tools from statistical mechanics , especially called replica method . </S>",
    "<S> this method has been used to analyze communication systems like code division multiple access ( cdma ) and multiple input multiple output ( mimo ) systems with large size . </S>",
    "<S> replica analysis , now days rigorously proved , is an efficient tool to analyze large systems in general . </S>",
    "<S> specifically , we analyze the performance of some of the estimators used in cs like lasso ( the least absolute shrinkage and selection operator ) estimator and zero - norm regularizing estimator as a special case of maximum a posteriori ( map ) estimator by using bayesian framework to connect the cs estimators and replica method . </S>",
    "<S> we use both replica symmetric ( rs ) ansatz and one - step replica symmetry breaking ( 1rsb ) ansatz , clamming the latter is efficient when the problem is not convex . </S>",
    "<S> this work is more analytical in its form . </S>",
    "<S> it is deferred for next step to focus on the numerical results .    </S>",
    "<S> solomon a.tesfamicael ( tesfamic@iet.ntnu.no - spring 2010 </S>"
  ]
}