{
  "article_text": [
    "given data @xmath1 , for @xmath2 , with a @xmath3-dimensional real - valued predictor variable @xmath4 , where @xmath5 , and a real - valued response @xmath6 , a typical goal of regression analysis is to find an estimator @xmath7 , such that the expected loss @xmath8 is minimal , under a given loss function @xmath9 .",
    "for the following , the standard squared error loss is used .",
    "if the predictor can be of the ` black - box ' type , tree ensembles have proven to be very powerful .",
    "random forests @xcite is a prime example , as are boosted regression trees @xcite .",
    "there are many interesting tools available for interpretation of these tree ensembles , see for example @xcite and the references therein .",
    "while tree ensembles often have very good predictive performance , an advantage of a linear model is better interpretability .",
    "measuring variable importance and performing variable selection are more easier to formulate and understand in the context of linear models . for high - dimensional data with @xmath10 ,",
    "regularization is clearly imperative and the lasso @xcite has proven to be very popular in recent years , since it combines a convex optimization problem with variable selection .",
    "a precursor to the lasso was the nonnegative garrote @xcite .",
    "a disadvantage of the nonnegative garrote is the reliance on an initial estimator , which could be the least squares estimator or a regularized variation . on the positive side ,",
    "important variables incur less penalty and bias under the regularization than they do with the lasso . for a deeper discussion of the properties of the nonnegative garrote see @xcite .    here",
    ", it is proposed to use random forest as an initial estimator for the nonnegative garrote .",
    "the idea is related to the rule ensemble approach of @xcite , who used the lasso instead of the nonnegative garrote .",
    "a crucial distinction is that rules fulfilling the same functional role are grouped in our approach .",
    "this is similar in spirit to the group lasso @xcite .",
    "this produces a very accurate predictor that uses just a few functional groups of rules , discarding many variables in the process as irrelevant .",
    "a unique feature of the proposed method is that is seems to work very well in the absence of a tuning parameter .",
    "it just requires the choice of an initial tree ensemble .",
    "this makes the procedure very simple to implement and computationally efficient .",
    "the idea and the algorithm is developed in section  [ section:2 ] , while a detailed numerical study on 15 datasets makes up section  [ section:3 ] .",
    "a tree @xmath11 is seen here as a piecewise - constant function @xmath12 derived from the tree structure in the sense of @xcite .",
    "@xcite proposed ` rules ' as a name for simple rectangular - shaped indicator functions .",
    "every node @xmath13 in a tree is associated with a @xmath14 in @xmath15-dimensional space , defined as the set of all values @xmath16 that pass through node @xmath13 if passed down the tree .",
    "all values @xmath16 that do not pass throuh node @xmath13 are outside of @xmath14 .",
    "the way rules are used here , they correspond to indicator functions @xmath17 , @xmath18 i.e.  @xmath19 is the indicator function for box @xmath14 . for a more detailed discussion",
    "see @xcite .    to give an example of a rule , take the well - know dataset on abalone @xcite as an example .",
    "the goal is to predict age of abalone from physical measurements . for each of the 4177 abalone in the dataset , eight predictor variables ( sex , length , diameter , height , while weight , shucked weight , viscera weight and shell weight ) are available .",
    "an example of a rule is @xmath20 and the presence of such a rule in a final predictor is easy to interpret , comparable to interpreting coefficients in a linear model . for the following",
    ", it is assumed that rules contain at most a single inequality for each variable . in other words ,",
    "the boxes defined by rules in @xmath3-dimensional spaces are defined by at most a single hyperplane in each variable .",
    "if a rule violates this assumption , it can easily be decomposed into two or several rules satisfying the assumption .",
    "every regression tree can be written as a linear superposition of rules .",
    "suppose a tree @xmath11 has @xmath21 nodes in total .",
    "the regression function @xmath22 of this tree ( ensemble ) can then be written as @xmath23 for some @xmath24 .",
    "the decomposition is not unique in general .",
    "we could , for example , assign non - zero regression coefficients @xmath25 only to leaf nodes . here",
    ", we build the regression function incrementally instead , assigning non - zero regression coefficients to _ all _ nodes .",
    "the value @xmath26 are defined as @xmath27 where @xmath28 is the empirical mean across the @xmath29 observations and @xmath30 is the parent node of @xmath13 in the tree .",
    "rule ( [ ruleex ] ) , in the abalone example above , receives a regression weight @xmath31 .",
    "the contribution of rule ( [ ruleex ] ) to the random forest fit is thus to increase the fitted value if and only if the diameter is larger than 0.537 and shell weight is larger than 0.135 .",
    "the random forest fit is the sum of the contribution from all these rules , where each rule corresponds to one node in the tree ensemble .    to see that ( [ treehat ] ) and ( [ eq : hatbetatree ]",
    ") really correspond to the original tree ( ensemble ) solution , consider just a single tree for the moment .",
    "denote the predicted value for predictor variable @xmath4 by @xmath32 .",
    "let @xmath33 be the rectangular area in @xmath3-dimensional space that corresponds to the leaf node @xmath34 of the tree in which predictor variable @xmath4 falls .",
    "the predicted value follows then by adding up all relevant nodes and obtaining with ( [ treehat ] ) and ( [ eq : hatbetatree ] ) , @xmath35 i.e.  the predicted values is just the empirical mean of @xmath6 across all observations @xmath2 which fall into the same leaf node as @xmath4 , which is equivalent to the original prediction of the tree .",
    "the equivalence for tree ensembles follows by averaging across all individual trees as in ( [ treehat ] ) .",
    "the idea of @xcite is to modify the coefficients @xmath24 in ( [ treehat ] ) , increasing sparsity of the fit by setting many regression coefficient to 0 and eliminating the corresponding rules from the fit , while hopefully not degrading the predictive performance of the tree ensemble in the process .",
    "the rule ensemble predictors are thus of the form @xmath36 sparsity is enforced by penalizing the @xmath0-norm of @xmath37 in lasso - style @xcite , @xmath38 this enforces sparsity in terms of rules , i.e.  the final predictor will have typically only very few rules , at least compared to the original tree ensemble .",
    "the penalty parameter @xmath39 is typically chosen by cross - validation . @xcite",
    "recommend to add the linear main effects of all variables into ( [ ruleens ] ) , which were omitted here for notational simplicity and to keep invariance with respect to monotone transformations of predictor variables .",
    "it is shown in @xcite that the rule ensemble estimator maintains in general the predictive ability of random forests , while lending itself more easily to interpretation .       _",
    "first column : combined variable interaction rules between predictor variable _ tch _ ( x - axis to the right ) and _ ltg _ ( y - axis to the left ) in the random forest fit for the diabetes data .",
    "second column : the interaction rules can be decomposed into the effects @xmath40 ( plotted on z - axis ) of four interaction patterns @xmath41 on top , @xmath42 on second from top , @xmath43 on second from bottom and @xmath44 on the bottom .",
    "third column : applying the garrote correction , three of the four interaction patterns are set to 0 .",
    "fourth column : adding the four interaction patterns with garrote correction up , the total interaction pattern between the two variables _ ltg _ and _ tch _ in the forest garrote fit . _ _ , title=\"fig:\",scaledwidth=25.0% ]   _ first column : combined variable interaction rules between predictor variable _ tch _ ( x - axis to the right ) and _ ltg _ ( y - axis to the left ) in the random forest fit for the diabetes data .",
    "second column : the interaction rules can be decomposed into the effects @xmath40 ( plotted on z - axis ) of four interaction patterns @xmath41 on top , @xmath42 on second from top , @xmath43 on second from bottom and @xmath44 on the bottom .",
    "third column : applying the garrote correction , three of the four interaction patterns are set to 0 .",
    "fourth column : adding the four interaction patterns with garrote correction up , the total interaction pattern between the two variables _ ltg _ and _ tch _ in the forest garrote fit . _ _ , title=\"fig:\",scaledwidth=24.0% ]   _ first column : combined variable interaction rules between predictor variable _ tch _ ( x - axis to the right ) and _ ltg _ ( y - axis to the left ) in the random forest fit for the diabetes data .",
    "second column : the interaction rules can be decomposed into the effects @xmath40 ( plotted on z - axis ) of four interaction patterns @xmath41 on top , @xmath42 on second from top , @xmath43 on second from bottom and @xmath44 on the bottom . third column : applying the garrote correction , three of the four interaction patterns are set to 0 .",
    "fourth column : adding the four interaction patterns with garrote correction up , the total interaction pattern between the two variables _ ltg _ and _ tch _ in the forest garrote fit .",
    "_ _ , title=\"fig:\",scaledwidth=24.0% ]   _ first column : combined variable interaction rules between predictor variable _ tch _ ( x - axis to the right ) and _ ltg _ ( y - axis to the left ) in the random forest fit for the diabetes data .",
    "second column : the interaction rules can be decomposed into the effects @xmath40 ( plotted on z - axis ) of four interaction patterns @xmath41 on top , @xmath42 on second from top , @xmath43 on second from bottom and @xmath44 on the bottom . third column : applying the garrote correction , three of the four interaction patterns are set to 0 .",
    "fourth column : adding the four interaction patterns with garrote correction up , the total interaction pattern between the two variables _ ltg _ and _ tch _ in the forest garrote fit . _ _ , title=\"fig:\",scaledwidth=25.0% ]    the rule ensemble approach is treating all rules equally by enforcing a @xmath0-penalty on all rules extracted from a tree ensemble .",
    "it does not take into account , however , that there are typically many very closely related rules in the fit .",
    "take the rf fit to the abalone data as an example .",
    "several hundred rules are extracted from the rf , two of which are @xmath45 with regression coefficient @xmath46 and @xmath47 with regression coefficient @xmath48 .",
    "the effect of these two rules , measured by @xmath49 and @xmath50 , are clearly very similar .",
    "in total , there are 32 rules ` interaction ' rules that involve variables diameter and shell weight in the rf fit to the abalone data .",
    "selecting some members of this group , it seems artificial to exclude others of the same ` functional ' type .",
    "sparsity is measured purely on a rule - by - rule basis in the @xmath0-penalty term of rule ensembles ( [ ruleens ] ) .",
    "selecting the two rules mentioned above incurs the same sparsity penalty as if the second rule involved two completely different variables .",
    "an undesirable side - effect of not taking into account the grouping of rules is that many or even all original predictor variables might still be involved in the rules ; sparsity is not explicitly enforced in the sense that many irrelevant original predictor variables are completely discarded in the selected rules .",
    "it seems natural to let rules form functional groups .",
    "the question then turns up which rules form useful and interpretable groups .",
    "there is clearly no simple right or wrong answer to this question . here",
    ", a very simple yet hopefully intuitive functional grouping of rules is employed . for the @xmath13-th rule , with coefficient @xmath25 ,",
    "define the interaction pattern @xmath51 for variables @xmath52 by    @xmath53    the meaning of interaction patterns is best understood if looking at examples for varying degrees , where the degree of a interaction pattern @xmath54 is understood to be the number of non - zero entries in @xmath54 and corresponds to the number of variables that are involved in a rule .",
    "[ [ first - degree - main - effects . ] ] first degree ( main effects ) .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + +    the simplest interaction patterns are those involving a single variable only , which correspond in some sense to the main effects of variables . the interaction pattern @xmath55 for example collects all rules that involve the 2nd predictor variable _ length _ only and lead to a monotonically increasing fit ( are thus of the form @xmath56 for some real - valued @xmath57 if the corresponding regression coefficient were positive or @xmath58 if the regression coefficient were negative ) . the interaction pattern @xmath59 collects conversely all those rules that yield a monotonically decreasing fit in the variable _ diameter _ , the third variable .",
    "[ [ second - degree - interactions - effects . ] ] second degree ( interactions effects ) .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    second degree interaction patterns are of the form ( [ ruleex1 ] ) or ( [ ruleex2 ] ) .",
    "as diameter is the 3rd variable and shell weight the 8th , the interaction pattern of both rules ( [ ruleex1 ] ) and ( [ ruleex2 ] ) is @xmath60 making them members of the same functional group , as for both rules the fitted value is monotonically increasing in both involved variables .",
    "in other words , either a large value in both variables increases the fitted value or a very low value in both variables decreases the fitted value .",
    "second degree interaction patterns thus form four categories for each pair of variables .",
    "a case could be made to merge these four categories into just two categories , as the interaction patterns do not conform nicely with the more standard multiplicative form of interactions in linear models .",
    "however , there is no reason to believe that nature always adheres to the multiplicative form of interactions typically assumed in linear models .",
    "the interaction patterns used here seemed more adapted to the context of rule - based inference .",
    "factorial variables can be dealt with in the same framework ( [ sigma ] ) by converting to dummy variables first .",
    "in contrast to the group lasso approach of @xcite , the proposed method does not only start with knowledge of natural groups of variables or rules .",
    "a very good initial estimator is available , namely the random forest fit .",
    "this is exploited in the following .",
    "let @xmath40 be the part of the fit that collects all contributions from rules with interaction pattern @xmath54 , @xmath61 let @xmath62 be the collection of all possible interaction patterns @xmath54 .",
    "the tree ensemble fit ( [ treehat ] ) can then be re - written as a sum over all interaction patterns @xmath63 a interaction pattern @xmath54 is called _ active _ if the corresponding fit in the tree ensemble is non - zero , i.e.  if and only if @xmath40 is not identically @xmath64 .",
    "the random forest fit contains very often a huge number of active interaction pattern , involving interactions up to fourth and higher degrees .",
    "most of those active patterns contribute just in a negligible way to the overall fit .",
    "the idea proposed here is to use ( [ tdecomp ] ) as a starting point and modify it to enforce sparsity in the final fit , getting rid of as many unnecessary predictor variables and associated interaction patterns as possible .",
    "the lasso of @xcite was used in the rule ensemble approach of @xcite . here , however , the starting point is the functional decomposition ( [ tdecomp ] ) , which is already a very good initial ( yet not sparse ) estimator of the underlying regression function .",
    "hence it seems more appropriate to use breiman s nonnegative garrote @xcite , penalizing contributions of interaction patterns less if their contribution to the initial estimator is large and vice versa .",
    "the beneficial effect of this bias reduction for important variables has been noted in , amongst others , @xcite and @xcite .",
    "the garrote - style forest estimator @xmath65 is defined as @xmath66 each contribution @xmath40 of an interaction pattern @xmath54 is multiplied by a factor @xmath67 .",
    "the original tree ensemble fit is obtained by setting all factors equal to 1 .",
    "the multiplicative factor @xmath68 is chosen by least squares , subject to the constraint that the total @xmath0-norm of the multiplying coefficients is less than 1 , @xmath69 the normalizing factor @xmath70 divides the @xmath0-norm of @xmath68 by the total number of interaction patterns and is certainly not crucial here but simplifies notation .",
    "the estimation of @xmath71 is an application of breiman s nonnegative garrote @xcite . as for the garrote ,",
    "the original predictors @xmath40 are not rescaled , thus putting effectively more penalty on unimportant predictors , with little variance of @xmath40 across the samples @xmath72 and less penalty on the important predictors with higher variance , see @xcite for details .",
    "algorithmically , the problem can be solved with an efficient lars algorithm @xcite , which can easily be adapted to include the positivity constraint .",
    "alternatively , quadratic programming can be used .",
    "it might be surprising to see the @xmath0-norm constrained by 1 instead of a tuning parameter  @xmath39 .",
    "yet this is indeed one of the interesting properties of forest garrote .",
    "the tree ensemble is in some sense selecting a good level of sparsity .",
    "it seems maybe implausible that this would work in practice , but some intuitive reasons for its empirical success are given further below and ample empirical evidence is provided in the section with numerical results .",
    "a drawback of the garrote in the linear model setting is the reliance on the ols estimator ( or another suitable estimator ) , see also @xcite .",
    "the ols estimator is for example not available if @xmath73 .",
    "the tree ensemble estimates are , in contrast , very reasonable estimators in a wide variety of settings , certainly including the high - dimensional setting @xmath10 .",
    "the entire forest garrote algorithm works thus as follows    1 .",
    "fit random forest or another tree ensemble approach to the data .",
    "2 .   extract @xmath40 from the tree ensemble for all @xmath74 by first extracting all rules @xmath75 and corresponding regression coefficients @xmath25 and grouping them via ( [ tsigma ] ) for each interaction pattern .",
    "estimate @xmath71 as in ( [ gammahat ] ) from the data , using for example the lars algorithm @xcite .",
    "4 .   the fitted forest garrote function @xmath65 is given by ( [ hatgar ] ) .",
    "the whole algorithm is very simple and fast , as there is no tuning parameter to choose .      in most regularization problems , like the lasso @xcite",
    ", choosing the regularization parameter is very important and it is usually a priori not clear what a good choice will be , unless the noise level is known with good accuracy ( and it usually it is not ) . the most obvious approach would be cross - validation",
    ". cross - validation can be computationally expensive and is usually not guaranteed to lead to optimal sparsity of the solution , selecting many more variables or interaction patterns than necessary , as shown for the lasso in @xcite and @xcite .    since the starting point is a very useful predictor , the original tree ensemble",
    ", there is a natural tuning parameter for the forest garrote estimator .",
    "as noted before , @xmath76 corresponds to the original tree ensemble solution .",
    "the original tree ensemble solution @xmath22 is thus contained in the feasible region of the optimization problem ( [ gammahat ] ) under a constraint on the @xmath0-norm of exactly 1 .",
    "the solution ( [ hatgar ] ) will thus be at least as sparse as the original tree ensemble solution in the sense if sparsity is measured in the same @xmath0-norm sense as in ( [ gammahat ] ) .",
    "some variables might not be used at all even though they appear in the original tree ensemble .    on the other hand ,",
    "the empirical squared error loss of @xmath65 is at least as low ( and typically lower ) than for the original tree ensemble solution as @xmath71 will reduce the empirical loss among all solutions in the feasible region of ( [ gammahat ] ) , which contains the original tree ensemble solution .",
    "the latter point does clearly not guarantee better generalization on yet unseen data , but a constraint of 1 on the @xmath0-norm turns out to be an interesting starting point and is a very good default choice of the penalty parameter .    sometimes one might still be interested in introducing a tuning parameter .",
    "one could replace the constraint of 1 on the @xmath0-norm of @xmath68 in ( [ gammahat ] ) by a constraint @xmath39 , @xmath77 the range over which to search , with cross - validation , to over @xmath39 can then typically be limited to @xmath78 $ ] . empirically , it turned out that the default choice @xmath79 is very reasonable and actually achieves often better predictive and selection performance than the cross - validated solution , since the latter suffers from possibly high variance for finite sample sizes .     _ upper right diagonal : ` main effects ' and ` interactions ' of second degree for the random forest fit on the diabetes data between the main 6 variables ( not showing all variables ) .",
    "lower left diagonal : corresponding functions for the forest garrote .",
    "some main effects and interactions are set exactly to zero .",
    "vanishing interactions are not plotted , leaving some entries blank . _ _ , scaledwidth=99.0% ]      the method is illustrated on the diabetes data from @xcite with @xmath80 predictor variables , age , sex , body mass index , average blood pressure and six blood serum measurements .",
    "these variables were obtained for each of @xmath81 diabetes patients , along with the response of interest , a ` quantitative measure of disease progression one year after baseline ' . applying a random forest fit to this dataset",
    ", the main effects and second - order interactions effects , extracted as in ( [ tsigma ] ) , are shown in the upper right diagonal of figure  [ fig : diabetes ] , for 6 out of the 10 variables ( chosen at random to facilitate presentation ) .",
    "all of these variables have non - vanishing main effects ( on the diagonal ) and the interaction patterns can be quite complex , making them somewhat difficult to interpret .    now applying a forest garrote selection to the random forest fit",
    ", one obtains the main effects and interaction plots shown in the lower left diagonal of figure  [ fig : diabetes ] .",
    "note that the x - axis in the interaction plots corresponds to the variable in the same column , while the y - axis refers to the variable in the same row .",
    "interaction plots are thus rotated by 90 degrees between the upper right and the lower left diagonal . some main effects and interactions",
    "are set to 0 by the forest garrote selection .",
    "interaction effects that are not set to 0 are typically ` simplified ' considerably .",
    "the same effect was observed and explained in figure  [ fig:38 ] .",
    "the interaction plot of forest garrote seems thus much more amenable for interpretation .",
    "the unexplained variance on test data , as a fraction of the total variance . left : comparison of unexplained variance for forest garrote versus random forests .",
    "middle : forest garrote ( cv ) versus random forests .",
    "right : rule ensembles versus random forests .",
    "_ , scaledwidth=99.0% ]    .",
    "_ [ table : timetable ] the relative cpu time spent on forest garrote , forest garrote ( cv ) and rule ensembles for the various datasets .",
    "forest garrote uses the least computational resources since ( i ) it starts from a relative small set of dictionary elements ( all @xmath40 for @xmath82 as opposed to all rules ) , ( ii ) the solution has to be computed only for a single regularization parameter and there is hence ( iii ) also no need for expensive cross - validation .",
    "note that the times above are only for the rule selection steps ( [ ruleens ] ) and ( [ gammahat ] ) respectively and the overall relative speed difference is typically smaller as a tree ensemble fit needs to be computed as an initial estimator in all settings .",
    "_ [ cols=\"<,<,<,>,>,>\",options=\"header \" , ]     forest garrote ( cv ) has also obviously a computational disadvantage compared with the recommended forest garrote estimator , as shown in table  [ table : timetable ] which is comparing relative cpu times necessary to compute the relative estimators .",
    "all three methods could be speeded up considerably by clever computational implementation .",
    "any such improvement would most likely be applicable to any of these three compared methods as they have very similar optimization problems at their heart .",
    "only relative performance measurements seem to be appropriate and only the time it takes to solve the respective optimization problems ( [ ruleens ] ) , ( [ gammahat ] ) and ( [ gammahatlambda ] ) is reported , including time necessary for cross - validation , if so required .",
    "rule ensembles is faring by far the worst here , since the underlying optimization problem is very high - dimensional .",
    "the dimensionality @xmath21 in ( [ ruleens ] ) is the total number of rules , which corresponds to the number of all nodes in the random forest fit .",
    "the total number @xmath83 of interaction patterns in the optimization underlying ( [ gammahat ] ) in the forest garrote fit is , on the other hand , very much smaller than the number @xmath21 of all rules , since many rules are typically combined in each interaction patterns .",
    "the lack of cross - validation for the forest garrote estimator clearly also speeds computation up by an additional factor between 5 and 10 , depending on which form of cross - validation is employed .",
    "finally , the number of variables selected by either method is examined .",
    "a variable is said to be selected for this purpose if it appears in any node in the forest or in any rule that is selected with a non - zero coefficient .",
    "in other words , selected variables will be needed to compute predictions , not selected variables can be discarded .",
    "the results are shown in table  [ table : var ] .",
    "many variables are typically involved in a random forest fit and both rule ensembles as well as forest garrote can cut down this number substantially . especially for higher - dimensional data with large number @xmath3 of variables , the effect can be pronounced . between rule ensembles and forest garrote ,",
    "the differences are very minor with a slight tendency of forest garrote to produce sparser results .",
    "[ section : numerical ]",
    "balancing interpretability and predictive power for regression problems is a difficult act .",
    "linear models lend themselves more easily to interpretation but suffer often in terms of predictive power .",
    "random forests ( rf ) , on the other hand , are known deliver very accurate prediction .",
    "tools exist to extract marginal variable importance measures from rf .",
    "however , the interpretability of rf could be improved if the very large number of nodes in the hundreds of trees fitted for rf could be reduced .    here",
    ", the forest garrote was proposed as such a pruning method for rf or tree ensembles in general .",
    "it collects all rules or nodes in the forest that belong to the same functional group . using a garrote - style penalty ,",
    "some of these functional groups are then shrunken to zero , while the signal of other functional groups is enhanced .",
    "this leads to a sparser model and rather interpretable interaction plots between variables .",
    "predictive power is similar or better to the original rf fit for all examined datasets .",
    "the unique feature of forest garrote is that it seems to work very well without the use of a tuning parameter , as shown on multiple well known and less well known datasets .",
    "the lack of a tuning parameter makes the method very easy to implement and computationally efficient ."
  ],
  "abstract_text": [
    "<S> variable selection for high - dimensional linear models has received a lot of attention lately , mostly in the context of @xmath0-regularization . </S>",
    "<S> part of the attraction is the variable selection effect : parsimonious models are obtained , which are very suitable for interpretation . in terms of predictive power , however , these regularized linear models are often slightly inferior to machine learning procedures like tree ensembles . </S>",
    "<S> tree ensembles , on the other hand , lack usually a formal way of variable selection and are difficult to visualize . a garrote - style convex penalty for trees ensembles , in particular random forests , is proposed . </S>",
    "<S> the penalty selects functional groups of nodes in the trees </S>",
    "<S> . these could be as simple as monotone functions of individual predictor variables . </S>",
    "<S> this yields a parsimonious function fit , which lends itself easily to visualization and interpretation . </S>",
    "<S> the predictive power is maintained at least at the same level as the original tree ensemble . </S>",
    "<S> a key feature of the method is that , once a tree ensemble is fitted , no further tuning parameter needs to be selected . </S>",
    "<S> the empirical performance is demonstrated on a wide array of datasets . </S>"
  ]
}