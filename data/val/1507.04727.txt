{
  "article_text": [
    "analyses of spiking activity recorded from sensory neurons have revealed three main features : first , neuronal activity is stochastic and exhibits significant variability across trials ; second , the spiking statistics often undergo rapid changes referred to as neuronal plasticity , in order to adapt to changing stimulus salience and behavioral context ; and third , the tuning characteristics of sensory neurons to the stimuli exhibit a degree of sparsity .",
    "examples include place cells in the hippocampus @xcite and spectrotemporally tuned cells in the primary auditory cortex @xcite .",
    "hence , in order to gain insight into the functional mechanism of the underlying neural system , it is crucial to have a mathematical theory to simultaneously capture the stochasticity , dynamicity and sparsity of neuronal activity .",
    "on one hand , the theory of point processes @xcite has been recently adopted as a mathematical framework to model the stochasticity of neuronal data .",
    "traditionally , these models have been used to predict the likelihood of self - exciting processes such as earthquake occurrences @xcite , but have recently found significant applications in the analysis of neuronal data @xcite .    on the other hand , classic results in signal processing such as the least mean squares ( lms ) and recursive least squares ( rls )",
    "algorithms @xcite have created a framework to efficiently capture the dynamics of the parameters in linear observation models .",
    "existing solutions in computational neuroscience have adopted this framework to estimate the dynamics of neuronal activity .",
    "for instance , in @xcite an lms - type point process filter was introduced to study plasticity in hippocampal neurons . in @xcite ,",
    "more general adaptive filtering solutions based on approximations to the chapman - kolmogorov equation were introduced .",
    "although quite powerful in analyzing neuronal data , these solutions do not account for the sparsity of the underlying parameters .",
    "finally , the theory of compressed sensing ( cs ) has provided a novel methodology for measuring and estimating statistical models governed by sparse underlying parameters @xcite . in particular , for _ static _ linear and generalized linear models ( glm ) with random covariates and sparsity of the parameters , the cs theory characterizes sharp trade - offs between the number of measurement , sparsity , and estimation accuracy @xcite .",
    "the sparse solutions of cs are typically achieved using batch - mode convex programs and greedy techniques . in online settings ,",
    "sparse adaptive filters have only been introduced in the context of linear systems governed by sparse parameters such as communication channels @xcite .    despite significant progress in all these research fronts , a unified framework to simultaneously capture",
    "the stochasticity , dynamicity and sparsity of neuronal data is lacking . in this paper",
    ", we close this gap by integrating techniques from point process theory , adaptive filtering , and compressed sensing . to this end , we consider the problem of estimating time - varying stimulus modulation coefficients ( e.g. , receptive fields ) from a sequence of binary observations in an online fashion . we model the spiking activity by a conditional bernoulli point process , where the conditional intensity is a logistic function of the stimulus and its time lags .",
    "we will then design a novel objective function by incorporating the forgetting factor mechanism of rls - type algorithms into the @xmath0-regularized maximum likelihood estimation of the point process parameters .",
    "we will present theoretical guarantees that extend those of cs theory and characterize fundamental trade - offs between the number of measurements , forgetting factor , model compressibility , and estimation error of the underlying point processes in the non - asymptotic regime .",
    "we will next develop two adaptive filters for recursive estimation of the objective function based on proximal gradient techniques , as well as a filter for recursive computation of statistical confidence regions .    in order to validate our algorithms ,",
    "we provide simulation studies which reveal that the proposed adaptive filtering algorithms significantly outperform existing point process filters in terms of goodness - of - fit , mean square error and trackability .",
    "we finally apply our proposed filters to multi - unit spiking data from ferret primary auditory cortex ( a1 ) during passive stimulus presentation and during performance of a click rate discrimination task @xcite in order to characterize the spectrotemporal receptive field ( strf ) plasticity of a1 neurons .",
    "application of our algorithm to these data provides new insights into the time course of attention - driven strf plasticity , with over 3 orders of magnitude increase in temporal resolution from minutes to centiseconds , while capturing the underlying sparsity in a robust fashion . aside from their theoretical significance ,",
    "our results are particularly important in light of the recent technological advances in neural prostheses , which require real - time robust neuronal system identification from limited data .",
    "the rest of the paper is organized as follows : in section [ prelim ] , we present our notational conventions , preliminaries and problem formulation . in section",
    "[ sec : main ] , we introduce the main theoretical results of this paper , including the construction and stability analysis of the objective function , recursive filter development , and computation of confidence regions .",
    "section [ sec : applications ] provides numerical simulations as well as application to real data , followed by our concluding remarks in section [ sec : conclusion ] .",
    "technical details of section [ sec : main ] are presented in appendices [ proof][app : conf ] .",
    "we first give a brief introduction to point process models ( see @xcite for a detailed treatment ) .",
    "we will use the following notation throughout the paper .",
    "parameter vectors are denoted by bold - face greek letters .",
    "for example , @xmath1'$ ] denotes an @xmath2-dimensional parameter vector , with @xmath3'$ ] denoting the transpose operator .",
    "consider a stochastic process defined by a sequence of discrete events at random points in time , noted by @xmath4'$ ] , and a counting measure given by @xmath5 where @xmath6 is the dirac s measure . the conditional intensity function ( cif ) for this process , denoted by @xmath7 , is defined as @xmath8 where @xmath9 denotes the history of the process as well as the covariates up to time @xmath10 .",
    "the cif can be interpreted as the _ instantaneous rate _ given the history of the process and the covariates .",
    "a point process with a cif given by @xmath11 is defined as :    1 .",
    "@xmath12 2 .   given @xmath13 , the random variables @xmath14",
    "are _ conditionally _ mutually independent .",
    "3 .   for any @xmath15",
    ", @xmath16 is a poisson random variable with probability distribution    @xmath17    a point process model is fully characterized by its cif . for instance , @xmath18 corresponds to the homogenous poission process with rate @xmath19 .",
    "a discretized version of this process can be obtained by binning @xmath20 within an observation interval of @xmath21 $ ] by bins of length @xmath22 , that is @xmath23 where @xmath24 . throughout this paper , @xmath25 will be considered as the observed spiking sequence , which will be used for estimation purposes . also , by approximating eq .",
    "( [ cif_def ] ) for small @xmath26 , and defining @xmath27 , we have : @xmath28 in discrete time , the orderliness of the process is equivalent to the requirement that with high probability not more than one event fall into any given bin . in practice , this can always be achieved by choosing @xmath22 small enough",
    ". an immediate consequence of eq .",
    "( [ bernoulli ] ) is that @xmath29 can be approximated by a sequence of bernoulli random variables with success probabilities @xmath30 .    a popular class of models for the cif is given by generalized linear models ( glm ) . in its general form ,",
    "a glm consists of two main components : an observation model ( which is given by ( [ bernoulli ] ) in this paper ) and an equation expressing some ( possibly nonlinear ) function of the observation mean as a _ linear _ combination of the covariates . in neuronal systems ,",
    "the covariates consist of extrinsic covariates ( e.g. , neural stimuli ) as well as intrinsic covariates ( e.g. , the history of the process ) . in this paper , we only consider glm models with purely extrinsic covariates , although most of our results can be generalized to incorporate intrinsic covariates as well .",
    "let @xmath31 denote the stimulus at time bin @xmath10 , @xmath32'$ ] denote the vector of stimulus modulation parameters , and @xmath33 denote the baseline firing rate .",
    "we adopt a logistic regression model for the cif as follows : @xmath34 by defining @xmath35'$ ] and @xmath36 $ ] , we can equivalently write : @xmath37    the model above is also known as the logistic - link cif model .",
    "another popular model in the computational neuroscience literature is the log - link model where @xmath38 .",
    "the significance of the logistic - link model is that @xmath39 maps the real line @xmath40 to the unit probability interval @xmath41 , making it a feasible model for describing statistics of binary events independent of the scaling of the covariates and modulation parameters .    despite capturing the stimulus dependence in quite a general form , the glm model in ( [ logit2 ] )",
    "represents a static model .",
    "we therefore generalize this model to the dynamic setting by allowing temporal variability of the modulation parameters : @xmath42 where @xmath43'$ ] represents the time - varying parameter vector at time @xmath10 . throughout the rest of the paper",
    ", we refer to @xmath44 and @xmath45 as the covariate vector and the modulation parameter vector at time @xmath10 , respectively .    in our applications of interest ,",
    "the modulation parameter vector @xmath46 exhibits a degree of sparsity @xcite .",
    "that is , only certain components in the stimulus modulation have significant contribution in determining the statistics of the process .",
    "these components can be thought of as the preferred or intrinsic tuning features of the underlying neuron . to be more precise , for a sparsity level @xmath47 ,",
    "we denote by @xmath48 the support of the @xmath49 highest elements of @xmath46 in absolute value , and by @xmath50 the best @xmath49-term approximation to @xmath46 .",
    "we also define @xmath51 to capture the compressibility of the parameter vector @xmath46 .",
    "recall that for @xmath52 , the @xmath0-norm is defined as @xmath53 .",
    "when @xmath54 , the parameter @xmath46 is called @xmath49-sparse , and when @xmath55 is small compared to @xmath56 , the parameter is called @xmath49-compressible @xcite .    finally , the main estimation problem of this paper can be stated as follows : _ given binary observations @xmath57 and covariates @xmath58 from a point process with a cif given by eq .",
    "( [ glmdynamic ] ) , the goal is to estimate the @xmath2-dimensional @xmath49-compressible parameter vectors @xmath59 in an online and stable fashion . _",
    "in this section , we will first describe the construction of an appropriate objective function for addressing our main estimation problem .",
    "we will then present a rigorous analysis of the maximizers of the objective function , which extends the results of cs to our setting .",
    "next , we will introduce two adaptive filters to recursively maximize the objective function based on proximal gradient techniques .",
    "finally , we will outline how statistical confidence bounds can also be constructed in a recursive fashion for our estimates .      before proceeding with the construction of the objective function",
    ", we need to introduce more notational conventions . in order to have a framework allowing multi - timescale dynamics",
    ", we consider piece - wise constant dynamics for the parameter @xmath45 .",
    "that is , we assume that @xmath45 remains constant over windows of arbitrary length @xmath60 samples , for some integer @xmath61 . by segmenting the corresponding spiking data @xmath62 into @xmath63 windows of length @xmath61 samples each , we assume that the cif for each time point @xmath64 is governed by @xmath65 , for @xmath66 .",
    "note that number of spiking samples @xmath67 is assumed to be an integer multiple of window size @xmath61 , without loss of generality .    invoking the bernoulli approximation to the poisson statistics for @xmath26 , the log - likelihood of the observation @xmath68 at time",
    "@xmath10 can be expressed as : @xmath69 assuming conditional independence of the spiking events , the joint log - likelihood of the observations within window @xmath70 is given by : @xmath71    in order to explicitly enforce adaptivity in the log - likelihood function , we adopt the forgetting factor mechanism of the rls algorithm , where the log - likelihood of each window is exponentially weighted regressively in time , with a forgetting factor @xmath72 .",
    "that is , the effective data log - likelihood up to and including window @xmath70 is taken to be : @xmath73 for some @xmath72 .",
    "note what for @xmath74 , @xmath75 coincides with the natural data log - likelihood .",
    "moreover , if we replace the bernoulli log - likelihood with the gaussian log - likelihood , then @xmath76 coincides with the conventional rls objective function .",
    "next , in order to explicitly enforce sparsity , we adopt the @xmath0-regularization mechanism of cs .",
    "that is , at window @xmath70 , we seek an estimate of the form :    @xmath77    where @xmath78 is a regularization parameter controlling the trade off between the log - likelihood fit and the sparsity of estimated parameters .",
    "our theoretical analysis in the next subsection reveals appropriate choices for @xmath78 , @xmath79 and the trade - offs therein .      in order to quantify the trade - offs involving our choice of the objective function in eq .",
    "( [ mainprob ] ) , we proceed in the tradition of performance analysis result of the rls algorithm @xcite by characterizing the geometric properties of the estimates @xmath80 in a stationary environment where @xmath81 for all @xmath70 .",
    "our analysis , however , is quite general and avoids ad hoc assumptions such as direct averaging or covariate independence which are usually invoked in the analysis of least squares problems .",
    "we need to make the following technical assumptions for our analysis :    \\1 ) the stimulus sequence @xmath82 consists of independent ( but not necessarily identically distributed ) random variables with a variance of @xmath83 which are uniformly bounded by a constant @xmath84 in absolute value .",
    "note that with this assumption , two successive covariate vectors , say at times @xmath10 and @xmath85 , given respectively by @xmath86 $ ] and @xmath87 $ ] are highly _ dependent _ , as they have @xmath88 random variables in common .",
    "hence , the independent assumption used in studying least squares problem is violated .",
    "\\2 ) we further assume that for all times @xmath10 , @xmath89 , for some constants @xmath90 and @xmath91 , i.e. , the probability of spiking does not reach its extremal values of @xmath92 and @xmath93 , but can get arbitrarily close .",
    "this assumption can be realized due to the boundedness of the covariates and appropriate normalization of the stimulus modulation coefficients , and does not result in any practical loss of generality .",
    "we have the following theoretical result regarding the stability of the maximizers of the objective function :    [ thm : main ] _ suppose that binary observations from a point process with a cif given by eq .",
    "( [ glmdynamic ] ) are given over @xmath67 windows of length @xmath61 each .",
    "consider a stationary environment with @xmath94 for all @xmath70 and suppose that @xmath46 is @xmath49-compressible . then",
    ", under assumptions ( 1 ) and ( 2 ) , for a fixed positive constant @xmath95 , there exist constants @xmath96 , @xmath97 , and @xmath98 such that for @xmath99 , @xmath100 and a choice of @xmath101 , any solution @xmath102 to ( [ mainprob ] ) satisfies the bound @xmath103{\\!(1\\!-\\!\\beta ) l \\log m},\\end{aligned}\\ ] ] with probability at least @xmath104 .",
    "the constants @xmath96 , @xmath97 , and @xmath98 are only functions of @xmath105 , @xmath90 , @xmath91 , @xmath83 , @xmath106 , and @xmath61 , and are explicitly given in appendix [ proof ] . _    the proof is given in appendix [ proof ] .",
    "* remarks . *",
    "the result of theorem [ thm : main ] has four major implications .",
    "first , the error bound scales with @xmath107 , the sparsity level , as opposed to @xmath2 , the ambient dimension of the parameter vector , which is consistent with results from cs , and results in the robustness of the estimate when the underlying parameter is sparse . note that the bounds holds for general non - sparse @xmath46 , but is sharpest when @xmath55 is negligible , i.e. , the parameter vector is nearly @xmath49-sparse .",
    "second , the theorem prescribes a lower bound on the forgetting factor akin to the bounds obtained in cs theory for the total number of observations .",
    "for instance , the result of @xcite for cs under toeplitz sensing measurements for the linear model requires @xmath108 number of measurements to achieve a similar scaling of the error bound . in our case , the role of the number of measurements is transferred to forgetting factor by taking @xmath109 as the _ effective _ length of the measurements . in the absence of the forgetting factor ( @xmath74 ) , by a careful limiting process ,",
    "our results require @xmath108 measurements .",
    "the latter case can be compared to the result of @xcite for point process models with independent and identically distributed covariate vectors , which requires @xmath110 for stability .",
    "the loss of @xmath111 is incurred due to the shift structure and hence high dependence of the covariate vectors in our case , as exemplified in assumption ( 1 ) .",
    "third , the theorem reveals the scaling of the regularization parameter in terms of @xmath2 and @xmath79 .",
    "in particular , this scaling is significant as it reveals another role for the forgetting factor mechanism : not only the forgetting factor mechanism allows for adaptivity of the estimates , it also controls the scaling of the @xmath0-regularization term with respect to the log - likelihood term .",
    "fourth , unlike conventional results in the analysis of adaptive filters which concern the expectation of the error in the asymptotic regime , our result holds for a single realization with probability polynomially approaching 1 , in the non - asymptotic regime .",
    "note that the objective function is clearly concave , and assuming that the matrix of the covariate vectors is full - rank , will be strictly concave with a unique maximizer .",
    "however , the result of theorem [ thm : main ] does not require the uniqueness of the maximizer and holds for any maximizer of the objective function .",
    "in the next section , we will proceed with the development of recursive filters to track the maximizer of the objective function in the more general time - varying setting .",
    "several standard optimization techniques , such as interior point methods , can be used to find the maximizer of ( [ mainprob ] ) . however , most of these techniques operate in batch mode and do not meet the real - time requirements of the adaptive filtering setting where the observations arrive in a streaming fashion . in order to avoid the increasing runtime complexity and memory requirements of the batch - mode computation , we seek a recursive approach which can perform low - complexity updates in an online fashion upon the arrival of new data in order to form the estimates . to this end , we adopt the proximal gradient approach . a version of the proximal gradient algorithm is given in appendix [ proximal ] .",
    "each iteration of the algorithm moves the previous iterate along the gradient of the log - likelihood function , which will then pass through a shrinkage operator .    before describing further details",
    ", we introduce a more compact notation for convenience .",
    "let @xmath112'$ ] denote the vector of observed spikes within window @xmath70 , for @xmath113 .",
    "similarly , let @xmath114'$ ] denote the vector of cifs within window @xmath70 . by extending the domain of the @xmath115 to vectors in a component - wise fashion",
    ", we can express @xmath116 as : @xmath117    where @xmath118'$ ] is the data matrix of size @xmath119 with rows corresponding to the covariate vectors in window @xmath70 .",
    "suppose that at window @xmath70 , we have an iterate denoted by @xmath120 , for @xmath121 , with @xmath122 being an integer denoting the total number of iterations .",
    "the gradient of @xmath123 evaluated at @xmath120 can be written as : @xmath124    where @xmath125 represents the innovation vector of the point process at window @xmath126 .",
    "the innovation vector @xmath127 can be thought of as the counterpart of the conventional innovation vector in adaptive filtering of linear models .",
    "the proximal gradient iteration for the @xmath0-regularization can be written in the compact form as : @xmath128 where @xmath129 is the element - wise soft thresholding operator at a level of @xmath130 given in appendix [ proximal ] .",
    "the final estimate at window @xmath70 is obtained following the @xmath122th iteration , and is denoted by @xmath131 . in order to achieve a recursive updating rule for @xmath132 ,",
    "we can rewrite eq .",
    "( [ gradserie ] ) as : @xmath133 however , in an adaptive setting , we only have access to values of @xmath134 evaluated at @xmath135 ! in order to turn eq .",
    "( [ eq : rec ] ) into a fully recursive updating rule , all the previous cif vectors @xmath136 should be recalculated at the most recent set of iterates @xmath137 . in order to overcome this computational burden",
    ", we exploit the smoothness of the logistic function and employ the taylor series expansion of the cif to approximate the required recursive update . in what follows , we consider the zeroth order and first order expansions , which result in two distinct , yet fully recursive , updating rules for eq .",
    "( [ eq : rec ] ) .    _ * zeroth order expansion : * _ by retaining only the first term in the taylor series expansion of the cif @xmath138 around @xmath139 , we get : @xmath140 )    where @xmath141 . substituting this approximation in eq .",
    "( [ gradserie ] ) , we can express the zeroth order approximation to the gradient at window @xmath70 , denoted by @xmath142 , as : @xmath143    it is then straightforward to obtain a recursive form as : @xmath144    .",
    "@xmath145 @xmath146 @xmath147 @xmath148 $ ] @xmath131 .",
    "the shrinkage step will be then given by : @xmath149    we refer to the resulting filter as the @xmath0-regularized point process filter of the zeroth order ( @xmath0-ppf@xmath150 ) .",
    "a pseudo - code is given in algorithm [ alg0 ] .",
    "_ * first order expansion : * _ if instead , we retain the first two terms in the taylor expansion , eq .",
    "( [ eq:0 ] ) will be replaced by : @xmath151 where @xmath152 is a diagonal @xmath153 matrix with the @xmath154th diagonal element given by @xmath155 . using the first order approximation above",
    ", we can improve the resulting approximation to the gradient , denoted by @xmath156 , as : @xmath157 by defining : @xmath158    we can express @xmath159 as : @xmath160    .",
    "@xmath145 @xmath146 @xmath161 , @xmath162 @xmath163 @xmath164 @xmath165 @xmath148 $ ] @xmath131 .",
    "it is then straightforward to check that both @xmath166 and @xmath167 can be updated recursively @xcite as : @xmath168 note that the update rules for both @xmath169 and @xmath170 involve simple rank-@xmath61 operations .",
    "the shrinkage step is then given by : @xmath171 we refer to the resulting filter as the @xmath0-regularized point process filter of the first order ( @xmath0-ppf@xmath172 ) .",
    "a pseudo - code is given in algorithm [ alg1 ] .",
    "* remark*. the computational complexity of @xmath0-ppf@xmath150 and @xmath0-ppf@xmath172 algorithms can be shown to be linear and quadratic in @xmath2 per iteration , respectively .",
    "our results in section [ sec : applications ] will reveal that both filters outperform existing filters of the same complexity , respectively .",
    "furthermore , @xmath0-ppf@xmath172 exhibits superior performance over @xmath0-ppf@xmath150 as expected , although with a cost of @xmath173 in computational complexity per iteration .      characterizing the statistical confidence bounds for the estimates",
    "is of utmost importance in neural data analysis , as it allows to test the validity of various hypotheses . although construction of confidence bounds for linear models in the absence of regularization is well understood and widely applied , regularized ml estimates are usually deemed as point estimates for which the construction of statistical confidence regions is not straightforward .",
    "a series of remarkable results in high - dimensional statistics @xcite have recently addressed this issue by providing techniques to construct confidence intervals for @xmath0-regularized ml estimates of linear models as well as glms .",
    "these approaches are based on a careful inspection of the karush - kuhn - tucker ( kkt ) conditions for the regularized estimates , which admits a procedure to decompose the estimates into a bias term plus an asymptotically gaussian term ( referred to as de - sparsifying in @xcite ) , which can be computed using a nodewise regression @xcite of the covariates .",
    "@xmath174 @xmath175 @xmath176 @xmath177 , and @xmath178 @xmath179 @xmath180 @xmath181 @xmath182 $ ]    in what follows , we give a brief description of how the methods of @xcite apply to our setting , and leave the details to appendix [ app : conf ] . using the result of @xcite , the estimate @xmath80 as the maximizer of ( [ mainprob ] )",
    "can be decomposed as : @xmath183 where @xmath184 is an approximate inverse to the hessian of @xmath185 evaluated at @xmath186 , @xmath132 is the gradient of @xmath185 previously defined in eq .",
    "( [ gradserie ] ) , and @xmath187 is an unbiased and asymptotically gaussian random vector with a covariance matrix of @xmath188 , with @xmath189 the first term in eq .",
    "( [ eq : decomp ] ) is a bias term which can be directly computed given @xmath184 .",
    "given @xmath190 , statistical confidence bounds for the second term at desired levels can be constructed in a standard way .",
    "the main technical issue in the aforementioned procedure in our setting is the computation of @xmath184 in a recursive fashion .",
    "since the rows of @xmath184 are computed using @xmath0-regularized least squares , we use the sparls algorithm @xcite as an efficient method to carry out the computation in a recursive fashion .",
    "algorithm [ alg3 ] summarized the recursive computation of confidence intervals for the @xmath191th component of @xmath187 .",
    "in this section , we will apply the proposed algorithms to simulated data as well as real spiking data from the ferret primary auditory cortex . in our simulation studies ,",
    "we compare the performance of our proposed filters with two of the state - of - the - art point process filters , namely the steepest descent point process filter ( sdppf ) @xcite and the stochastic state point process filter ( ssppf ) @xcite .",
    "these adaptive filters are based on approximate solutions to the chapman - kolmogorov forward equation obtained by a steepest descent and a gaussian approximation procedure , respectively .",
    "first , we consider a stationary environment where @xmath46 is constant over time .",
    "we use a bin size of @xmath192 and window size of @xmath193 sample , for a total observation window of @xmath194 ( @xmath195 ) .",
    "the length of the parameter vector @xmath196 $ ] is chosen as @xmath197 .",
    "for each realization , we draw a sparse parameter vector @xmath198 of fixed length @xmath199 and sparsity @xmath200 .",
    "the support @xmath201 and values of the nonzero components of @xmath198 are chosen randomly and the values are normalized so that @xmath202 .",
    "the stimulus input sequence @xmath203 is drawn from an i.i.d .",
    "gaussian distribution @xmath204 .",
    "the binary spike train @xmath205 is generated as a single realization of conditionally independent bernoulli trials with success rate @xmath206 .",
    "the stimulus variance @xmath83 is chosen as @xmath207 small enough so that the average spiking rate @xmath208 to ensure that the bernoulli approximation is valid .",
    "all the simulations are done with @xmath209 iteration per time step .",
    "the step size @xmath210 is chosen as @xmath211 ( see appendix [ proximal ] for details ) .",
    "for a given pair of @xmath212 parameters , we select an optimal value for the regularization parameter @xmath78 by performing a two - fold even - odd cross validation procedure : first , the data are split into two sets of even and odd samples in an interleaved manner .",
    "then , one set is used as the training set for estimation of the parameter vector @xmath80 and the other is used to assess the goodness - of - fit of the estimates @xmath186 with respect to the log - likelihood of the observations .",
    "we repeat the process switching the role of the two sets and take the average as the overall measure of fit .",
    "let @xmath213 denote the averaging operator with respect to realizations .",
    "we consider two performance metrics : the normalized mean squared error ( mse ) defined as @xmath214 to evaluate mse performance at time step @xmath70 ; and the out - of - support energy defined as @xmath215 to represent a sparsity metric ( spm ) .",
    "ideally , @xmath216 must be equal to zero at all times .",
    "the averaging is carried out over a sufficiently large number of runs .",
    "figure [ fig : f1 ] shows the corresponding learning curves for the four algorithms . according to figure [ fig : f1]a , the @xmath0-ppf@xmath172 achieves the lowest stationary mse measure of @xmath217 db , followed @xmath0-ppf@xmath150 which achieves an mse of @xmath218  db .",
    "the ssppf and sdppf algorithms respectively achieve an mse of @xmath219 db and @xmath220 db , which reveals a gap of @xmath221  db with respect to our proposed filters .",
    "note that this result is consistent with the prediction of theorem [ thm : main ] and highlights the mse gain achieved by @xmath0-regularization , as opposed to ml , when the underlying parameter is sparse .          in the second simulation scenario ,",
    "we consider a more realistic setting where @xmath80 evolves in time . furthermore , as in the case of real data applications , we assume that the support of @xmath80 is not available as a performance benchmark and resort to statistical goodness - of - fit test . these tests for point process models",
    "have been developed as an application of the time - rescaling theorem @xcite and consist of the kolmogorov - smirnov ( ks ) test for assessing the conditional intensity estimation accuracy , and the autocovariance function ( acf ) test to assess the conditional independence assumption .",
    "we skip the details in the interest of space , and refer the readers to the aforementioned references for a detailed treatment .    as in the previous case",
    ", we consider a bin size of @xmath222 , window size of @xmath223 , and a total observation window of @xmath224 ( @xmath225 bins ) . the stimulus is generated as in the previous case . for the parameter vector @xmath80",
    ", we choose a fixed baseline rate of @xmath226 to set the baseline spiking rate to @xmath227 , and select a sparse modulation vector @xmath228 of length @xmath229 with a support @xmath230 of size @xmath231 , and respective values of @xmath232 for @xmath233 .",
    "halfway through the test , at @xmath234 , the largest component @xmath235 , drops rapidly and linearly to @xmath92 , within a window of length @xmath236 and remains zero for the rest of the run .",
    "figure [ fig : f3 ] shows the performance of all four algorithms in the aforementioned setting .",
    "each row ( a through d ) shows the true time - varying parameter vector ( dashed traces ) as well as the filtered estimates ( solid traces ) in the left panel . in particular",
    ", the gray solid traces show the out - of - support components which must ideally be equal to zero .",
    "the colored hulls around @xmath237 show the @xmath238 confidence intervals ( note that confidence intervals for sdppf can not be directly obtained and require averaging over multiple realizations ) .",
    "the middle and right panels show the ks and acf test results at a @xmath238 confidence , respectively . for the quadratic algorithms @xmath0-ppf@xmath172 and ssppf , a forgetting factor of @xmath239",
    "is chosen .",
    "the regularization parameter for @xmath0-ppf@xmath172 is chosen as @xmath240 , obtained by the aforementioned two - fold even - odd cross validation .",
    "for the first order algorithm @xmath0-ppf@xmath150 , a smaller forgetting factor of @xmath241 is chosen to ensure stability , and a value of @xmath242 is used based on cross validation . these settings",
    "ensure that all the algorithms are tuned in their optimal operating point for fairness of comparison .",
    "figure [ fig : f3]a and [ fig : f3]b reveal three striking performance gaps between the two second - order algorithms ( with the same computational complexity , quadratic in @xmath2 ) : first , the out - of - support components ( gray traces ) of @xmath0-ppf@xmath172 are significantly smaller than those of ssppf ; second , the confidence regions of @xmath0-ppf@xmath172 are narrower than those of ssppf ; and third , @xmath0-ppf@xmath172 fully passes the ks test , while ssppf marginally does so .",
    "similarly , comparing the two first order algorithms ( with the same computational complexity , linear in @xmath2 ) figure [ fig : f3]c and [ fig : f3]d reveal that the @xmath0-ppf@xmath150 significantly suppresses the out - of - support components as compared to sdppf .",
    "moreover , @xmath0-ppf@xmath150 provides confidence bounds , which can not be directly obtained for sdppf .",
    "finally , @xmath0-ppf@xmath150 marginally fails the ks test , whereas sdppf does so significantly .",
    "both algorithms fail the acf test , which shows that the second - order corrections embedded in @xmath0-ppf@xmath172 and ssppf is necessary to achieve a better goodness - of - fit , which a price of higher computational complexity .",
    "we also inspect the estimated firing probability @xmath243 for the four algorithms in figure [ fig : f4 ] . in addition , we include the probability estimated by the normalized reverse correlation ( nrc ) method , which is commonly used in neural data analysis , and fits the modulation parameters using a linear model .",
    "figure [ fig : f4 ] shows the true spiking probability ( blue solid trace ) and the resulting spikes ( black vertical lines ) . in the subsequent rows ( b through f ) ,",
    "the true and estimated probabilities are shown by dashed blue and solid red traces , respectively .",
    "a comparison of all the rows reveals that @xmath0-ppf@xmath172 and @xmath0-ppf@xmath150 outperform ssppf and sdppf , respectively , in terms of estimating the true probability .",
    "the nrc method is inferior to the preceding four algorithms , and results in negative estimates of the probability due to its use of a linear model ( as opposed to logistic ) .",
    "the responses of neurons in the primary auditory cortex ( a1 ) can be characterized by their spectrotemporal receptive fields ( strfs ) , where each neuron is tuned to a specific region in the time - frequency plane , and only significantly spikes when the acoustic stimulus contains spectrotemporal contents matching its tuning region @xcite ( see figure [ fig : f5 ] , top row , leftmost panel ) .",
    "several experimental studies have revealed that receptive fields undergo rapid changes in their characteristics during attentive behavior in order to capture salient stimulus modulations @xcite . in @xcite , it is suggested that this rapid plasticity has a significant role in the functional processes underlying active listening .",
    "however , most of the widely - used estimation techniques ( e.g. , normalized reverse correlation ) provide static estimates of the receptive field with a a temporal resolution of the order of minutes .",
    "moreover , they do not systematically capture the inherent sparsity manifested in the receptive field characteristics .    in the context of our model , the strf can be modeled as an @xmath244-dimensional matrix , where @xmath245 and @xmath246 denote the number of time lags and frequency bands , respectively . by vectorizing this matrix",
    ", we obtain an @xmath247-dimensional vector @xmath228 at window @xmath70 , where @xmath248 . augmenting the baseline rate parameter @xmath249",
    ", we can model the activity of the a1 neurons using the logistic cif with a parameter @xmath250'$ ] . the stimulus vector at time @xmath10 , @xmath251 is given by the vectorized version of the spectrogram of the acoustic stimulus with @xmath246 frequency bands and @xmath245 lags .",
    "in order to capture the sparsity of the strf in the time - frequency plane , we further represent @xmath228 over a gabor time - frequency dictionary consisting of gaussian windows centered around a regular subset of the @xmath252 time - frequency plane .",
    "that is , for @xmath253 , where @xmath254 is the dictionary matrix and @xmath255 is the sparse representation of the strf",
    ". the estimation procedures of this paper can be applied to @xmath255 , by absorbing the dictionary matrix into the data matrix @xmath256 at window @xmath70 .",
    "we apply our proposed adaptive filter @xmath0-ppf@xmath172 to multi - unit spike recordings from the ferrets a1 during a series of passive listening conditions and active auditory task conditions . during each active task",
    ", ferrets attended to the temporal dynamics of the sounds , and discriminated the rate of acoustic clicks @xcite .",
    "the strfs were estimated from the passive condition , where the quiescent animal listened to a series of broadband noise - like acoustic stimuli known as temporally orthogonal ripple combinations ( torc ) .",
    "the experiment consisted of 2 active and 11 passive blocks . within each passive block",
    ", 30 torcs were randomly repeated a total of 4 - 5 times each . in our analysis , we pool the spiking data corresponding to the same repeated torc within each block",
    ". therefore , the time axis corresponds to the experiment time modulo repetitions within each block .",
    "we discretize the resulting duration of @xmath257 to time bins of size @xmath192 , and segment data to windows of size @xmath258 samples ( @xmath259 ) .",
    "the strf dimensions are @xmath260 , regularly spanning lags of 1 to @xmath261 and frequency bands of @xmath262 to @xmath263 ( in logarithmic scale ) .",
    "the dictionary @xmath254 consists of @xmath264 gabor atoms , evenly spaced within the strf domain .",
    "each atom is a two - dimensional gaussian kernel with a variance of @xmath265 per dimension , where @xmath266 denotes the spacing between the atoms .",
    "we selected a forgetting factor of @xmath267 , a step size of @xmath268 , where @xmath269 is the average variance of the spectrogram components , @xmath209 iterations per sample , and a regularization parameter of @xmath270 via two - fold even - odd cross validation .",
    "figure [ fig : f5 ] , top row , depicts five snapshots taken at @xmath271 corresponding to the end - points of the @xmath272th passive tasks .",
    "the bottom row shows the time - course of five selected points ( marked as a through d in the leftmost panel of the top row ) of the strf during the experiment .",
    "the strf snapshots at times @xmath273 and @xmath274 correspond to @xmath275 after the two active tasks , respectively , and verify the sharpening effect of the excitatory region ( @xmath276 ) due to the animal s attentive behavior following the active task reported in @xcite . moreover",
    ", the strf snapshots at times @xmath277 and @xmath278 reveal the weakening of the excitatory region long after the active task and returning to the pre - active state , highlighting the plasticity of a1 neurons .",
    "previous studies have revealed the strf dynamics with a resolution of the order of minutes @xcite .",
    "our result in figure [ fig : f5 ] provides a temporal resolution of the order of centiseconds ( 3 orders of magnitude increase ) , while capturing the strf sparsity in a robust fashion .",
    "in this paper , we considered recursive estimation of the time - varying parameter vectors in a logistic regression model for binary time series driven by continuous input . to this end , we integrated several techniques from compressed sensing , adaptive filtering , optimization and statistics .",
    "we constructed an objective function which enjoys from the trackability features of the rls - type algorithms , sparsifying features of @xmath0-minimization , and unlike the rate - based linear models commonly used to analyze spiking data , takes into account the binary statistics of the observations .",
    "we analyzed the maximizers of the objective function in a rigorous fashion , revealing novel trade - offs between various model parameters .",
    "we constructed two adaptive filters , with respective linear and quadratic complexity requirements , for recursive maximization of the objective function in an online setting .",
    "moreover , we characterized the statistical confidence regions for our estimates , and devised a recursive procedure to compute them efficiently .",
    "although we specialized our treatment to logistic statistics and @xmath0-regularization , our approach to algorithm development has a plug - and - play feature : other glm link functions ( e.g. , log - link ) with possibly history dependent covariates and other regularization schemes ( e.g. , re - weighted @xmath0 , or group - sparse regularization ) can be used instead and result in a large class of adaptive filters for sparse point process regression .",
    "we tested the performance of our algorithms on simulated as well as experimentally recorded spiking data .",
    "our simulation studies revealed that the proposed filters outperform several existing point process filters .",
    "application of our filters to real data from the ferret primary auditory cortex provided a high - resolution characterization of the time - course of spectrotemporal receptive field plasticity , with 3 orders of magnitudes increase in temporal resolution .",
    "although we focused on auditory neurons , we expect a similar superior performance of our filters when applied to other sensory or motor neurons ( e.g. , cells in primary or supplementary motor cortex @xcite ) .",
    "the proof is mainly based on the beautiful treatment of negahban et al . @xcite . the major difficulty in our case lies in the high inter - dependence of the covariates , which form a toeplitz structure due to the setup of adaptive filtering .",
    "we address the latter issue by adopting techniques from another remarkable paper by haupt et al .",
    "@xcite to deal with the underlying interdependence . in the process",
    ", we also employ concentration inequalities for dependent random variables due to van de geer @xcite .    in order to proceed ,",
    "we adopt the notion of strong restricted convexity ( rsc ) introduced in @xcite . for a twice differentiable log - likelihood with respect to @xmath46 , the rsc property or order @xmath49 implies the existence of a lower quadratic bound on the negative log - likelihood : @xmath279 for a positive constant @xmath280 and all @xmath281 satisfying : @xmath282 for any index set @xmath283 of cardinality @xmath49 .",
    "the following key lemma establishes the rsc for @xmath185 :    _ let @xmath284 denote a sequence of covariates and let @xmath46 denote the corresponding logistic parameters .",
    "then , for a fixed positive constant @xmath95 , there exist constants @xmath97 and @xmath280 such that for @xmath285 and @xmath286 the negative log - likelihood @xmath287 satisfies the rsc of order @xmath49 with constant @xmath288 with probability greater than @xmath289 .",
    "the constants @xmath97 and @xmath290 are only functions of @xmath105 , @xmath90 , @xmath91 , @xmath83 , @xmath106 , @xmath61 , and are explicitly given in the proof . _    the proof is inspired by the elegant treatment of negahban et al . @xcite .",
    "the major difficulty in our setting is the high interdependence of successive covariates due to the shift structure induced by the adaptive setting , whereas in @xcite , the matrix of covariates is composed of i.i.d .",
    "rows . using the taylor s theorem ,",
    "@xmath291 can be written as : @xmath292 with @xmath293 for some @xmath294 . since by hypothesis @xmath295",
    ", we have : @xmath296 we can therefore further lower bound @xmath291 by : @xmath297 where @xmath298 , and @xmath299 note that the matrix @xmath300 has highly inter - dependent elements due to the toeplitz structure in the adaptive design . in order to establish the rsc condition",
    ", we show the stronger restricted eigenvalue ( re ) property , which in turn implies rsc @xcite .",
    "let @xmath301 be fixed .",
    "to do so , we need to bound the eigenvalues of @xmath302 , the restriction of @xmath300 to a subset of columns and rows corresponding to indices in @xmath283 with @xmath303 , for some integer @xmath304 .    without loss of generality , we assume that the first entry of the covariate vectors @xmath44 is replaced by @xmath305 instead of @xmath93 , for presentational simplicity of the following treatment . for @xmath306 , we have : @xmath307 for @xmath308 , @xmath309 and for @xmath310 , @xmath311 we also have @xmath312 . using hoeffding s inequality @xcite we get : @xmath313 since @xmath314 , for @xmath315 $ ]",
    ". similarly , @xmath316 by adopting the partitioning technique of theorem 4 in @xcite , we also get for @xmath317 : @xmath318 let @xmath319 . now , using the union bound we have : @xmath320 where we have used @xmath321 and , @xmath322 now , by invoking the gershgorin s disc theorem , the eignevalues of any sub - matrix of @xmath300 restricted to an index set @xmath201 with @xmath303 , lie in the interval @xmath323 $ ] with probability at least : @xmath324 hence , by choosing @xmath325 , the probability above is greater than @xmath326 .    next , by invoking lemma 4.1 ( ii ) of @xcite , we have that @xmath300 satisfies the rsc condition over the set given by eq .",
    "( [ eq : cone ] ) with a constant given by : @xmath327 hence , the negative log - likelihood satisfies the rsc with a constant given by @xmath328 . finally , by taking @xmath329 , we have that @xmath330 , which makes @xmath290 independent of @xmath67 and @xmath79 , given by : @xmath331 and @xmath285 with @xmath332 .",
    "next , the result of theorem 1 of @xcite implies : @xmath333 for @xmath334 .",
    "we have , for @xmath310 , @xmath335 now , let @xmath336 be the @xmath305-field generated by @xmath337 , i.e. , @xmath338 .",
    "we have that @xmath339 & = \\mathbb{e } \\left [ \\mathbb{e } \\left [ \\left \\ { n_{t } - \\lambda_{t } \\delta\\right\\ } { s}_{t } | \\mathcal{f}_{t } \\right ] \\right ] \\\\",
    "\\nonumber & = \\mathbb{e } \\left [ s_t \\mathbb{e } \\left [ \\left \\ { n_{t } - \\lambda_{t } \\delta\\right\\ } | \\mathcal{f}_{t } \\right ] \\right ] \\\\",
    "\\nonumber & = \\mathbb{e } \\big [ s_t \\mathbb{e } \\big [ \\underbrace{\\left \\ { \\lambda_{t } \\delta - \\lambda_{t } \\delta\\right\\}}_{=0 } | \\mathcal{f}_{t } \\big ] \\big ] = 0.\\end{aligned}\\ ] ] hence for all @xmath191 , @xmath340 .",
    "we next invoke the following result for concentration of dependent random variables :    [ hoeff_dep ] consider a sequence of @xmath305-fields @xmath341 .",
    "suppose that @xmath342 is @xmath343-measurable with @xmath344 for some constant @xmath345 , @xmath346 and @xmath347 .",
    "then for all @xmath348 , @xmath349    this result is a special case of theorem 2.5 of @xcite for _ bounded _ and possibly dependent random variables , which generalizes hoeffding s inequality .    in our case",
    ", we can similarly show that @xmath350 = \\mathbb{e } \\left [ s_t \\mathbb{e } \\left [ \\left \\ { n_{t } - \\lambda_{t } \\delta\\right\\ } | \\mathcal{f}_{t-1 } , \\mathcal{f}_t \\right ] \\right ] = 0 $ ] .",
    "moreover , each summand is bounded by @xmath351 .",
    "hence , using the result of proposition [ hoeff_dep ] , by taking @xmath352 and @xmath353 $ ] , we get : @xmath354 using the union bound , we have : @xmath355 by choosing @xmath356 , we have that @xmath357 with probability at least @xmath358 .    hence , for a fixed @xmath359 , @xmath95 , and @xmath304 , by taking @xmath285 with @xmath332 and @xmath360 with @xmath361 , any maximizer @xmath102 satisfies : @xmath362{\\!(1\\!-\\!\\beta ) l \\log m},\\end{aligned}\\]]with probability at least @xmath363 , where @xmath96 is given by @xmath364",
    "in this appendix , we give an overview of the proximal gradient algorithm for minimization of convex function . the corresponding algorithm for maximization of concave functions",
    "can be obtained by negating the objective functions .",
    "consider the general optimization problem @xmath365 where functions @xmath366 and @xmath367 are assumed to be closed proper convex functions .",
    "suppose that @xmath368 is differentiable with a lipschitz continuous gradient @xmath369 with constant @xmath370 .",
    "the function @xmath371 can be possibly non - smooth .",
    "a wide range of practical optimization problems can be cast in this form , particularly in the context of machine learning @xcite , where the objective function can be decomposed into a loss function and a regularization term .",
    "the proximal gradient algorithm provides an iterative procedure for solving ( [ genprob ] ) in the following form : @xmath372 ,   \\label{proxgrad}\\end{aligned}\\ ] ] where the parameter @xmath373 is an appropriately chosen step size at iteration @xmath374 so that @xmath375 , and the _ proximal operator _ @xmath376 of function @xmath371 with parameter @xmath210 is defined as @xmath377 among the several interpretations available for the proximal gradient method , we have adopt a quadratic approximation - based model to derive the main iterative scheme in ( [ proxgrad ] ) .",
    "this interpretation @xcite , is based on the majorization - minimization algorithm ( see @xcite for a detailed discussion ) . in the approximation - based derivation , the @xmath374-th iteration for solving the general problem ( [ genprob ] )",
    "can be written in the following form : @xmath378 where the original objective function @xmath368 is replaced with a quadratically - regularized linear approximation around the previous iterate @xmath379 , given by @xmath380 where the quadratic term is referred to as the _ trust region penalty_. modulo constants , the objective function in ( [ approxprob ] ) can be rearranged to get the proximal gradient form @xmath381 . \\end{aligned}\\ ] ]    the proximal operator often admits closed form expressions . as for @xmath0-regularization , the proximal operator takes the simple form of the _ soft thresholding shrinkage operator _ @xmath382 whose @xmath126th component is given by @xmath383 with @xmath384 denoting the standard signum function , and @xmath385 . in this case",
    ", the proximal algorithm leads to a family of algorithms referred to as iterative shrinkage algorithms @xcite , where each iteration involves a simple gradient descent step followed by a shrinkage operation .",
    "finally , in our setting , the function @xmath368 is taken to be the exponentially weighted log - likelihood @xmath386 . due to the smoothness of the logistic function",
    ", the lipschitz constant for @xmath387 can be upper bounded by the trace of the hessian @xmath388 given in eq .",
    "( [ eq : hess ] ) . noting that the elements of @xmath389 are at most equal to @xmath390",
    ", we get @xmath391 . using assumption ( 1 ) of section [ maintheorem ] and an application of hoeffding s inequality , shows that the sum is concentrated around its mean given by @xmath392 , for large enough @xmath70 .",
    "therefore , we choose the step size @xmath393 , for some constant @xmath394 .",
    "the @xmath0-regularized ml estimate of eq .",
    "( [ mainprob ] ) can be written in the following form @xmath395 where @xmath396 denotes the log - likelihood function over a generic window with spiking vector @xmath397 , data matrix @xmath398 and parameter vector @xmath46 , and the operator @xmath399 is defined for a function @xmath400 as the empirical expectation exponentially weighted with a forgetting factor @xmath79 : @xmath401 where we have suppressed the dependence of @xmath368 on @xmath397 and @xmath398 on the left hand side for notational simplicity . following the treatment of theorem 3.1 of @xcite , the corresponding empirical gradient vector and hessian are respectively given by : @xmath402 the kkt conditions for the estimator @xmath186 can be then written as : @xmath403 where @xmath404 is a subgradient vector from the subdifferential of the @xmath0 norm , with components @xmath405 for @xmath406 and @xmath407 otherwise , for @xmath408 .",
    "substituting @xmath409 by its quadratic approximation around the true parameter vector @xmath80 , and inverting the corresponding kkt conditions , the de - sparsified estimator @xmath187 can be obtained as : @xmath410 where the matrix @xmath184 is the approximate inverse of hessian matrix @xmath411 , and can be computed using the following node wise regression procedure @xcite . to compute the @xmath191th row of @xmath412 , first the solution to the following lasso problem",
    "is obtained : @xmath413 where the dependence of @xmath167 on @xmath186 is suppressed for notational convenience , and the subscript notations are the same as those described in the footnote of algorithm [ alg3 ] .",
    "then , we define the vector @xmath414 as : @xmath415 and the scaling constant @xmath416 as @xmath417 finally , the @xmath191th row of @xmath184 is given by @xmath418 .",
    "the variance and the confidence interval at a level of @xmath210 for the @xmath191th component of @xmath419 can then be computed as given in lines 9 and the output of algorithm [ alg3 ] @xcite , where @xmath420 using taylor expansion similar to that in the development of @xmath0-ppf@xmath172 , the matrix @xmath421 can be recursively updated as given in line 2 of algorithm [ alg3 ] .",
    "finally , the node wise regression can be recursively computed using the sparls algorithm @xcite , which is given in lines 35 of algorithm [ alg3 ] .",
    "the parameter @xmath422 can be chosen to be in the same order of @xmath78 in ( [ mainprob ] ) ."
  ],
  "abstract_text": [
    "<S> we consider the problem of estimating the sparse time - varying parameter vectors of a point process model in an online fashion , where the observations and inputs respectively consist of binary and continuous time series . </S>",
    "<S> we construct a novel objective function by incorporating a forgetting factor mechanism into the point process log - likelihood to enforce adaptivity and employ @xmath0-regularization to capture the sparsity . </S>",
    "<S> we provide a rigorous analysis of the maximizers of the objective function , which extends the guarantees of compressed sensing to our setting . </S>",
    "<S> we construct two recursive filters for online estimation of the parameter vectors based on proximal optimization techniques , as well as a novel filter for recursive computation of statistical confidence regions . </S>",
    "<S> simulation studies reveal that our algorithms outperform several existing point process filters in terms of trackability , goodness - of - fit and mean square error . </S>",
    "<S> we finally apply our filtering algorithms to experimentally recorded spiking data from the ferret primary auditory cortex during attentive behavior in a click rate discrimination task . </S>",
    "<S> our analysis provides new insights into the time - course of the spectrotemporal receptive field plasticity of the auditory neurons .    </S>",
    "<S> adaptive filtering ; point process models ; compressed sensing ; neural signal processing ; receptive field plasticity . </S>"
  ]
}