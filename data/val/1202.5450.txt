{
  "article_text": [
    "multivariate statistical methods have been used for many decades to deal with situations in which two or more variables are measured or recorded for each unit .",
    "a classical example of this situation is guerry s data set , in which several variables meant to capture `` moral qualities '' ( e.g. , literacy , crime rate , suicide rate ) were tabulated for each of the departments in which france was divided at the time ( 1833 ) .",
    "this data set suggests that one can be interested in how the variables change as one moves around in france , or one can be interested in how the departments compare to each other based on the measured characteristics .",
    "it is of special interest how these two approaches can be combined ; this is considered in detail in @xcite .    besides having a combination of two essentially multidimensional sources of information , like geographic location plus recorded data , another layer of complexity",
    "is added when one more variable like time is added , leading to what essentially are two or more data cubes .",
    "a typical example of this is ecological data , where species abundances are measured at different , specified locations , over the course of time .",
    "the different approaches used in this setting are reviewed in @xcite , using the duality diagram setting as a unifying framework .",
    "such an approach is not limited to animal or plant species spread over a  geographic area ; the advent of _ metagenomics _ has made it possible to study the abundances of bacterial species in locations like ocean or pond water , or even the human gut . in this case",
    "it becomes important to incorporate information not only about location in space , but also in the phylogenetic landscape , by using established or inferred phylogenetic trees for the bacterial species detected .",
    "this problem is addressed , using the duality diagram formalism , in @xcite .    as an outgrowth of methods favored by french statisticians",
    ", @xcite proposed a unifying framework capable of including many methods reinvented and used by different groups in different countries as special cases .",
    "this framework is based on the analysis of certain linear operators between inner - product spaces which can be naturally associated to a data matrix , in the same way kernel matrices are used today in machine learning @xcite .",
    "this is explained in detail in works like @xcite and @xcite . in this article",
    "we present some motivations behind the choices made for this approach in the accompanying papers .",
    "the notion of duality is everywhere in mathematics , appearing under different guises in the most diverse fields ; and it is often remarkably useful .",
    "the idea of duality was introduced in the analysis of multivariate data by the french school of data analysts as a way to unify a suite of methods that turned out to be exactly or almost exactly equivalent to methods known by a different name , and the duality - diagram formalism provides a simple way to put all these methods in the same context .",
    "since this approach is the basis of the special articles presented together here [ @xcite , @xcite , @xcite ] , this short introduction aims to establish the basic facts and notation .",
    "the abstract approach in the duality diagram setup is often intimidating and it possibly turns away some interested readers ; we hope we can show here that these notions are actually natural , and that the overhead due in understanding the notation pays off handsomely in the breadth and complexity of applications .",
    "today the distinction between the space of rows of the matrix as a sample from a population and the space of columns as the fixed variables on which the observations were measured has been softened and we often hear the term ` transposable ' data .",
    "the definitions presented here explain this row - column duality .    by dispensing of the traditional probabilistic sample - population interpretation",
    ", european data analysts in the 1970s [ @xcite , @xcite , @xcite ] can be seen in hindsight as precursors of the current machine learning schools .",
    "it is interesting to remember that all these schools had precursors who spent time at the at&t laboratories in new jersey at a time when john tukey was active there .",
    "consider an @xmath0 matrix @xmath1 containing data for variables @xmath2 collected from @xmath3 individuals or units .",
    "this matrix defines an operator @xmath4 by the rule @xmath5 .",
    "what interpretation can we give to such a map ?",
    "the vector @xmath6 can be considered to contain the coefficients for linearly combining the variables @xmath2 into a new , synthetic , variable . in that sense",
    ", it becomes apparent that actually we should consider this a map from @xmath7 , the dual space of @xmath8 , into @xmath9 .",
    "the map @xmath10 provides a way to fill in the @xmath3 values for the new synthetic variable @xmath11 , which could have been defined even before collecting the data . from now on we will abuse notation and identify the operator @xmath12 and the matrix @xmath1 ( and will do the same with other similarly defined operators and matrices ) .",
    "we have then the following portion of the diagram : @xmath13^{\\mathbf{x } } & \\mathbb{r}^n . } \\ ] ]      recall that the adjoint of a linear transformation @xmath14 between inner product spaces is defined as the mapping @xmath15 that satisfies @xmath16 ( for simplicity , we will only consider spaces with scalars in @xmath17 in this article ; this is enough for most data analyses ) .",
    "this can be seen as just a clever way of extending the notion of matrix transpose to a more general setting , but it is actually a powerful formalism , especially when dealing with multiple inner products on the same spaces ( notice that @xmath18 depends not only on @xmath19 but also on @xmath20 and @xmath21 ) .",
    "it is convenient sometimes to think of @xmath18 as a map from @xmath22 to @xmath23 ; this matches the corresponding situation when it is generalized to banach spaces . in our setting this distinction might be considered moot , since all spaces considered are naturally isomorphic to their duals , but we will continue using the star notation ; the diagrams , and all the matrix operations obtained from them , work equally well if the stars are dropped from the spaces .",
    "then we have the following : @xmath24^{\\mathbf{x } } & \\mathbb{r}^n \\ar@{=}[d]_{\\wr}\\\\ % { x}^*(=\\mathbf{x}^t ) } % } \\ ] ] since we are considering the standard inner products on @xmath8 and @xmath9 ( and their dual spaces ) , @xmath25 corresponds just to the transpose @xmath26 of @xmath1 .",
    "thus , @xmath27 , @xmath28 , these two symmetric matrices have the same eigenvalues ( except possibly for zeros to account for the difference between  @xmath29 and @xmath3 ) , and the two sets of eigenvectors can be used to form the singular value decomposition ( svd ) of @xmath1 [ @xcite ] .",
    "consider now the situation where the inner products ( i.e. , the geometries ) on @xmath8 and @xmath9 are not standard .",
    "that is , assume that there are symmetric , positive definite matrices  @xmath30 and  @xmath31 such that the inner products @xmath32 and @xmath33 somehow make more sense for a particular data analysis than the standard inner products .",
    "a typical example is when @xmath34 is a diagonal matrix of ( positive ) weights , one for each individual , down - weighting individuals that are known to have been measured with a larger error ; another example is when @xmath35 is the diagonal matrix containing the reciprocals of the sample variances for the columns of @xmath1 , which corresponds to standardizing the variables ( assuming they are already centered ) ; a  related example is when @xmath35 is the inverse of the sample variance  covariance matrix obtained from @xmath1 , in which case the new geometry corresponds to the _ mahalanobis distance_. often , we want to consider the case in which @xmath36 and we are interested in a set @xmath37 of transformed variables obtained from @xmath38 by multiplication by @xmath39 , leading to a transformed data matrix @xmath40 .",
    "different multivariate procedures can be obtained by appropriately choosing @xmath35 and @xmath34 ; see section [ section : particular - cases ] for some examples .    instead of @xmath1 and its adjoint",
    ", consider now the transformation @xmath41 .",
    "that is , a vector @xmath42 of coefficients is first transformed into @xmath43 , which is in the scale of the transformed data matrix @xmath40 , and then used to create a linear combination of the variables @xmath44 .",
    "then , for all @xmath45 , @xmath46 , @xmath47 so @xmath48 . then , @xmath49 and @xmath50 are self - adjoint operators on @xmath51 and @xmath52 , respectively , but they are not necessarily symmetric matrices .",
    "nevertheless , they have real eigenvalues ( which match , except for zeros to account for the difference between @xmath29 and @xmath3 ) , because they are similar to symmetric matrices by way of positive definite matrices ; for example , @xmath53 where @xmath54 is obtained by replacing each eigenvalue @xmath55 of @xmath35 with @xmath56 .",
    "the eigenvectors of @xmath57 and @xmath58 are also real ; however , they need not be orthogonal .",
    "nevertheless , the eigenvectors of @xmath59 can be taken to be orthogonal with respect to @xmath60 , and those of @xmath61 to be orthogonal with respect to @xmath62 .",
    "( this can be interpreted as leading to a  generalized version of the svd . )    in diagram form , we have the following : @xmath63^{\\mathbf{x } } & \\mathbb{r}^n \\ar[d]^{\\mathbf{d } } % & { \\mathbb{r}^n}^ * \\ar[l]^{\\mathbf{x}^t } % } \\ ] ] this way , the triplet of matrices @xmath64 defines a multivariate data analysis setup , in which the main strategy is the computation of the eigendecompositions of the matrices @xmath57 and @xmath61 .",
    "it is also customary to denote @xmath65 and @xmath66 , so that the two operators of interest become @xmath67 and @xmath68 .",
    "the eigendecomposition can be computed for the smaller of the two matrices ( which is usually much smaller than the other ) , and , if needed , the eigenvectors for the other one can be easily obtained : for example , if @xmath69 , then @xmath70 satisfies @xmath71 furthermore , orthogonality is also preserved among eigenvectors : if @xmath72 are @xmath35-orthogonal eigenvectors for @xmath57 , then @xmath73 thus , whole eigendecompositions are easily transferred .",
    "the operator @xmath61 can be seen as a precursor of the more general kernel matrices used in today s kernel pca type methods [ @xcite ] .    in the kernel approach to data analysis",
    "one assumes that the data is provided as a @xmath74 matrix @xmath75 containing proximity scores for each pair of individuals ; these scores might have been computed from measured variables ( as in the case of the matrix @xmath58 , the _ linear kernel _ ; nonlinear functions of the variables offer a great variety of other possibilities ) , or by directly comparing the individuals ; see @xcite for a review of the theory and examples of applications in computational biology .",
    "kernel matrices are similarity matrices , that is , the proximity score for two individuals is high when they are similar and low ( even negative ) when they are dissimilar .",
    "( distance matrices , on the other hand , have higher values for dissimilar pairs of individuals . )",
    "what is the meaning of @xmath75 as an operator on @xmath9 ?",
    "a useful interpretation is to consider it as a smoothing operator acting on real - valued functions @xmath76 defined on the set of individuals : the value of @xmath77 at the @xmath78th individual is a weighted sum of all the values of @xmath76 , with higher weights for those individuals more similar to the @xmath78th ; in other words , @xmath76 is averaged locally ( up to a multiplicative constant ) .",
    "this offers one explanation of why the eigendecomposition of @xmath75 is useful , since repeated application of @xmath75 to @xmath79 ( which should produce a very smooth function ) converges toward an eigenvector of the leading eigenvalue , and `` very smooth '' functions of the data points can be used as coordinates .",
    "@xcite explores the similarities between the duality diagram and kernel approaches in appendix b , for the case of kernel canonical correspondence analysis .",
    "here we briefly describe how some well - known multivariate methods can be expressed as particular cases of the duality diagram , by appropriately choosing @xmath35 and @xmath34 .",
    "we will assume that @xmath1 is centered by columns ( i.e. , the mean has been subtracted for each variable ) .",
    "pca seeks to find linear combinations of the variables that explain most of the variability in the data ; see @xcite , for example , for more details .",
    "take @xmath80 , and @xmath81 .",
    "this corresponds to pca in the original scales ; it is equivalent to a straightforward svd on @xmath1 ( except for the factor  @xmath82 ) .",
    "if one standardizes the variables , as it is often appropriate to eliminate unit scale effects , then @xmath35 is taken to be the diagonal matrix containing the reciprocals of the sample variances of the columns of @xmath1 ( so @xmath39 contains the reciprocals of the standard deviations ) . while the @xmath78th eigenvector @xmath83 of the ( @xmath34-weighted ) sample covariance matrix @xmath84 provides the loadings of the variables @xmath44 for the @xmath78th principal component ( so that the actual components have to be obtained by @xmath85 ) , @xmath86 can be obtained directly as an eigenvector for @xmath61 : indeed , @xmath87 computing the principal components @xmath86 does not require the explicit decomposition of @xmath35 as @xmath88 .",
    "a total of @xmath89 observations are classified according to two categorical variables , one with @xmath3 categories or levels , and the other with @xmath29 , producing a @xmath0 matrix @xmath90 of counts for each combination of levels ( a _ contingency table _ ) .",
    "one wants to study how the counts differ from the expected counts under the assumption of independence between the two variables . to cast ca as a duality diagram",
    ", we first define the frequency matrix @xmath91 and the marginal frequency vectors @xmath92 and @xmath93 ; then , the expected counts ( conditionally on the marginals ) are given by @xmath94 .",
    "using the matrices @xmath95 and @xmath96 , we can standardize @xmath97 by @xmath98 the matrix @xmath1 seems like a reasonable choice to study by eigendecomposition .",
    "however , all rows and columns have been reduced to the same importance , while , heuristically , categories with larger marginal counts should provide more accurate information on the distribution of the other variable , and thus should be given greater weight .",
    "this can be achieved by defining the triplet @xmath99 .",
    "notice that actually @xmath1 is centered by rows and by columns with respect to the inner products given by @xmath100 and @xmath101 .",
    "this approach matches the traditional definition of ca .",
    "@xcite shows how the information about relationships between the rows of the contingency table can be incorporated into the duality diagram in the special case where there are binary trees that connects the rows of the abundance matrix .",
    "the study of variability of one continuous variable is done through the use of the variance ; this notion is generalized in several different directions to accommodate the complexities of dealing with multiple tables , graphs , etc .",
    ", through the concept of inertia . as in physics",
    ", we define inertia as a weighted sum of squared distances of the weighted points . for each of the diagrams studied above",
    ", the inertia designates the trace of the operator @xmath102 , and we have @xmath103 . as pointed out in @xcite , in the case of ca , the inertia is proportional to the @xmath104 statistic , whereas in ordinary pca it is just the total variance of all the variables . in discriminant analysis ,",
    "the inertia is decomposed into between - groups and within - group components ; these are also used in the bca analysis [ @xcite , @xcite ] .",
    "the weighted distances between columns have another interpretation in ecology and @xcite shows how they can be associated to different measures of diversity .",
    "the decomposition of total inertia can be seen as a generalization to manova which is the special case of a variance decomposition .",
    "@xcite uses this effectively to show how to decompose the total diversity across all locations into the average diversity of individual locations and plus the average of pairwise dissimilarities of locations .",
    "@xcite use similar decompositions to show what part of the inertia can be assigned to spatially local variation in their bca approach to multivariate spatial data .",
    "they also show how the graphical relationships between rows can be encoded in a special metric @xmath34 built from the weighted connectivity matrix .",
    "( in their paper , they call these weights @xmath105 . )",
    "interesting results can be obtained by combining two or more triplets .",
    "the usual assumption is that two ( or more ) sets of variables are measured on the same set of  @xmath3 individuals ; thus , the matrix @xmath34 is assumed to be common , but each set of variables has its own version of @xmath35 , of the appropriate size .    for example , one of the triplets might contain data from variables measured on each of the individuals , while the other might encode known relationships between the individuals .",
    "a key element in the comparison of the operators arising from two duality diagrams is the rv coefficient .",
    "it can be considered as a generalization of the squared correlation coefficient by using the froebenius matrix product .    given two symmetric matrices @xmath106 of the same size ,",
    "we define @xmath107 , and @xmath108 whenever @xmath109 . many nice properties of these definitions arise from the fact that @xmath110 defines an inner product on the vector space of symmetric matrices of a given size .",
    "this can be adapted to the general setting of multiple duality diagrams : having @xmath34 fixed , call @xmath111 the vector space of @xmath34-symmetric matrices , that is , matrices satisfying @xmath112 ( equivalently ,  @xmath113 is self - adjoint with respect to @xmath114 ) .",
    "then @xmath110 defines an inner product on @xmath111 .",
    "when comparing two duality diagrams @xmath115 , then numbers @xmath116 of variables might be different , yielding matrices @xmath117 of different size ; however , we will be comparing the matrices ( operators )  @xmath118 and @xmath119 , which are of the same size and @xmath34-symmetric .",
    "we define the rv coefficient of the two diagrams as @xmath120 .",
    "some immediate properties of the rv coefficient are as follows : its values are always in @xmath121 $ ] ; it equals 1 only when @xmath122 , for some nonzero scalar  @xmath123 ; and it equals 0 only when @xmath124 ( provided @xmath125 are nonsingular ) .",
    "the proofs are not too hard ; more details can be found in @xcite .    the rv coefficient between diagrams ( or triplets )",
    "can be used for justifying the use of eigenvalues and eigenvectors in this setting .",
    "for example , performing pca based on @xmath64 and selecting the @xmath126 top components is equivalent to finding a matrix @xmath127 such that the rv coefficient between @xmath64 and @xmath128 is maximized .",
    "when one data set  @xmath129 has the special status of a response that we would like to predict or explain from the other data set @xmath1 of explanatory variables , we can generalize ordinary regression to a multivariate response through the same diagram framework .",
    "this is called _ pca with respect to instrumental variables _ , abbreviated pca - iv ( also known as redundancy analysis , rda ) , first described by @xcite . in terms of the comparison of duality diagrams and rv coefficients , this problem can be rephrased as that of finding the metric @xmath130 to associate to @xmath1 so that @xmath131 is _ as close as possible _ to @xmath132 in the rv sense .",
    "that is , we want to maximize @xmath133 .",
    "we abbreviate the cross - products by writing @xmath134 and @xmath135 then for any @xmath136 @xmath137 the first term on the right - hand side does not depend on @xmath130 , and the second term will be zero for the choice @xmath138 .    if we add the extra constraint that we only allow ourselves a rank @xmath126 approximation , with @xmath139 , the optimal choice of a  positive definite matrix @xmath130 is to take @xmath140 where the columns of  @xmath141 are the eigenvectors of @xmath142 with @xmath143 the pca with regards to instrumental variables of rank @xmath126 is equivalent to the pca of rank @xmath126 of the triple @xmath144 where @xmath145      consider @xmath146 diagrams @xmath147 .",
    "this could correspond , for example , to @xmath146 different studies on the same subjects , using different variables , or the same variables measured on the same units at different points in time ( time course study ) ; a review of this problem in the setting of community ecology is found in @xcite .",
    "it is often important to summarize the relationships between the diagrams in a compact and intelligible way .",
    "the rv coefficients in fact allow us to consider this as performing a pca of the pcas .",
    "we compute the multivariate correlation coefficients between tables and use those as the matrix to be diagonalized , similarly to what happens in ordinary pca .",
    "the values of the pairwise computations of the covv and rv coefficients are arranged into @xmath148 symmetric matrices @xmath149 and @xmath136 , respectively , and the eigendecomposition of these matrices can lead to useful low - dimensional representations , just as in the case of pca using the covariance or correlation matrices , respectively . in this case , a 2- or 3-dimensional plot can be created in which each point represents one of the studies ( diagrams ) .    furthermore , since @xmath149 and @xmath136 have nonnegative entries , the eigenvector @xmath150 corresponding to the largest eigenvalue can be taken to have only nonnegative entries , adding up to 1 . then , defining @xmath151 , the operator @xmath68 can be taken as a compromise or summary of all the diagrams , and one can study how far , in the rv sense , different studies are from the compromise .",
    "these steps are part of the so - called statis procedure [ @xcite ] .",
    "one can think of these data sets as a data cube , with three indices ; then a  similar procedure can be used to compare two or more such cubes .",
    "the duality diagram is a useful formalism that allows one to easily compare many classical multivariate methods , revealing what they have in common and where they differ .",
    "but , furthermore , it has become a valuable tool for dealing with two problems that have become very common : ( 1 ) combining and amalgamating data which , although collected from different sources and using different methods , shed light on different aspects of the same phenomenon ; and ( 2 ) taking advantage of complex , nontraditional data types , like tree and network information .",
    "these two problems are closely related , as the data to be amalgamated are often of complex type .",
    "the overhead in effort to understand the abstract definitions in the duality diagram approach to data analysis is amply offset by the clearer picture that is gained and by the wealth of applications that become available . in this article",
    "we have tried to reduce that overhead by laying out arguments that show that those definitions are actually quite natural .",
    "the three articles [ @xcite , @xcite , @xcite ] in this group are excellent examples of the power of this approach , but are only a small sample from a large and growing body of work .",
    "most of the methods presented in these papers have been coded into functions for the statistical computation environment r [ @xcite ] , many available in the library ` ade4 ` , for which exemplary presentations have been published [ see @xcite , @xcite , @xcite ] . in the case of @xcite , you can even run in an interactive way through all the commands generating each and every plot through the reproducible website at http://pbil.univ - lyon1.fr / saoasopet/.      baty , f. , facompr , m. , wiegand , j. , schwager , j. and brutsche , m. ( 2006 ) .",
    "analysis with respect to instrumental variables for the exploration of microarray data structures .",
    "_ bmc bioinformatics _ * 7 * 422 .",
    "baty , f. , jaeger , d. , preiswerk , f. , schumacher , m. and brutsche , m. ( 2008 ) .",
    "stability of gene contributions and identification of outliers in multivariate analysis of microarray data .",
    "_ bmc bioinformatics _ * 9 * 289 .",
    "shinkareva , s. , mason , r. , malave , v. , wang , w. , mitchell , t. and just , m. ( 2008 ) . using fmri brain activation to identify cognitive states associated with perception of tools and dwellings .",
    "_ plos one _ * 3 * e1394 ."
  ],
  "abstract_text": [
    "<S> today s data - heavy research environment requires the integration of different sources of information into structured data sets that can not be analyzed as simple matrices . </S>",
    "<S> we introduce an old technique , known in the european data analyses circles as the duality diagram approach , put to new uses through the use of a variety of metrics and ways of combining different diagrams together . </S>",
    "<S> this issue of the annals of applied statistics contains contemporary examples of how this approach provides solutions to hard problems in data integration . </S>",
    "<S> we present here the genesis of the technique and how it can be seen as a precursor of the modern kernel based approaches .    and    . </S>"
  ]
}