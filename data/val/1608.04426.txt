{
  "article_text": [
    "unsupervised neural networks assume unlabeled data to be generated from a neural network structure , and have been applied extensively to pattern analysis and recognition .",
    "the most basic one is the restricted boltzmann machine ( rbm ) [ 12 ] , an energy - based model with a layer of hidden nodes and a layer of visible nodes .",
    "with such a basic structure , we can stack multiple layers of rbms to create a unsupervised deep neural network structure , such as the deep belief network ( dbn ) and the deep boltzmann machine ( dbm ) [ 7 , 13 ] .",
    "these models can be calibrated with a combination of the stochastic gradient descent and the contrastive divergence ( cd ) algorithm or the pcd algorithm [ 12 , 19 ] .",
    "once we learn the parameters of a model , we can retrieve the values of the hidden nodes from the visible nodes , thus applying unsupervised neural networks for feature selection .",
    "alternatively , we may consider applying the parameters obtained from a unsupervised deep neural network to initialize a deep feedforward neural network ( ffnn ) , thus improving supervised learning .",
    "one essential question for such models is to adjust for the high - dimensionality of their parameters and avoid overfitting . in ffnns ,",
    "the simplest regularization is arguably the early stopping method , which stops the gradient descent algorithm before the validation error rate goes up .",
    "the weight decay method , or @xmath0 regularization , is also commonly used [ 23 ] .",
    "recently dropout is proposed , which optimizes the parameters over an average of exponentially many models with a subset of all nodes [ 17 ] .",
    "it has been shown to outperform weight decay regularization in many situations .    for regularizing unsupervised neural networks ,",
    "sparse - rbm - type models encourage a smaller proportion of @xmath1-valued hidden nodes [ 4 , 9 ] .",
    "dbns are regularized in [ 5 ] with outcome labels .",
    "while these works tend to be goal - specific , we consider regularization for unsupervised neural networks in a more general setting . our work and contributions",
    "are as follows : ( 1 ) we extend common regularization methods to unsupervised deep neural networks , and explain their underlying mechanisms ; ( 2 ) we propose partial dropout / dropconnect which can improve the performance of dropout / dropconnect ; ( 3 ) we compare the performance of different regularization methods on real data sets , thus providing suggestions on regularizing unsupervised neural networks .",
    "we note that this is the very first study illustrating the mechanisms of various regularization methods for unsupervised neural nets with model convergence and likelihood bounds , including the effective newly proposed partial dropout / dropconnect .",
    "section 2 reviews recent works for regularizing neural networks , and section 3 exhibits rbm regularization as a basis for regularizing deeper networks .",
    "section 4 discusses the model convergence of each regularization method .",
    "section 5 extends regularization to unsupervised deep neural nets .",
    "section 6 presents a numerical comparison of different regularization methods on rbm , dbn , dbm , rsm [ 14 ] and gaussian rbm [ 12 ] .",
    "section 7 discusses potential future research and concludes the paper .",
    "to begin with , we consider a simple ffnn with a single layer of input @xmath2 and a single layer of output @xmath3 .",
    "the weight matrix @xmath4 is of size @xmath5 .",
    "we assume the relation @xmath6 where @xmath7 is the activation function , such as the sigmoid function @xmath8 applied element - wise .",
    "equation ( 1 ) has the modified form in [ 17 ] , @xmath9 where @xmath10 denotes element - wise multiplication , thereby achieving the dropout ( do ) regularization for neural networks . in dropout",
    ", we minimize the objective function @xmath11,\\ ] ] which can be achieved by a stochastic gradient descent algorithm , sampling a different mask @xmath12 per data example @xmath13 and per iteration .",
    "we observe that this can be readily extended to deep ffnns .",
    "dropout regularizes neural networks because it incorporates prediction based on any subset of all the nodes , therefore penalizing the likelihood .",
    "a theoretical explanation is provided in [ 20 ] for dropout , noting that it can be viewed as feature noising for glms , and we have the relation @xmath14 here @xmath15 for simplicity , and @xmath16 , where @xmath17 is the log - partition function of a glm .",
    "therefore , dropout can be viewed approximately as the adaptive @xmath18 regularization [ 2 , 20 ] .",
    "a recursive approximation of dropout is provided in [ 2 ] using normalized weighted geometric means to study its averaging properties .",
    "an intuitive extension of dropout is dropconnect ( dc ) [ 21 ] , which has the form below @xmath19 and thus masks the weights rather than the nodes .",
    "the objective @xmath20 has the same form as in ( 3 ) .",
    "there are a number of related model averaging regularization methods , each of which averages over subsets of the original model .",
    "for instance , standout varies dropout probabilities for different nodes which constitute a binary belief network [ 1 ] .",
    "shakeout adds additional noise to dropout so that it approximates elastic - net regularization [ 8 ] .",
    "drop - in considers the skipping of layers for deep ffnns [ 16 ] .",
    "fast dropout accelerates dropout with gaussian approximation [ 22 ] .",
    "we note that while dropout has been discussed for rbms [ 17 ] , to the best of our knowledge , there is no literature extending common regularization methods to rbms and unsupervised deep neural networks ; for instance , adaptive @xmath0 regularization and dropconnect as mentioned .",
    "therefore , we discuss their implementations and examine their empirical performance below .",
    "in addition to studying model convergence and likelihood bounds , we propose partial dropout / dropconnect which randomly drops a subset of nodes or edges based on a first calibrated model , therefore improving robustness in many situations .",
    "for a restricted boltzmann machine , we assume @xmath21 denotes the visible vector , and @xmath22 denotes the hidden vector .",
    "each @xmath23 , @xmath24 is a visible node and each @xmath25 , @xmath26 is a hidden node .",
    "the joint probability is @xmath27 we let @xmath28 , i.e. a vector containing all components of @xmath29 , @xmath30 , and @xmath4 . to calibrate",
    "the model is to find @xmath31 .",
    "an rbm is a neural network because we have the following relations @xmath32 where @xmath33 and @xmath34 represent , respectively , the @xmath35-th row and @xmath36-th column of @xmath4 . the gradient descent algorithm is applied to calibration , noting that @xmath37 where @xmath38 is the free energy .",
    "the right - hand side of ( 8) is approximated by contrastive divergence with @xmath39 steps of gibbs sampling ( cd-@xmath39 ) [ 12 ] .",
    "weight decay , or @xmath0 regularization , adds the term @xmath40 to the negative log - likelihood of an rbm .",
    "the most commonly used is @xmath18 ( ridge regression ) , or @xmath41 ( lasso ) . in all situations",
    ", we do not regularize biases for simplicity .",
    "here we consider a more general form .",
    "suppose we have a trained set of weights @xmath4 from cd with no regularization . instead of adding the term @xmath40",
    ", we add the term @xmath42 to the negative log - likelihood .",
    "apparently this adjusts for the different scales of the components of @xmath4 .",
    "we refer to this approach as adaptive @xmath0 .",
    "we note that adaptive @xmath41 is the adaptive lasso [ 24 ] , and adaptive @xmath18 plus @xmath41 is the elastic - net [ 25 ] .",
    "we consider the performance of @xmath18 regularization plus adaptive @xmath41 regularization ( @xmath43 ) below .      as discussed in [ 17 ] , to characterize a dropout ( do ) rbm , we simply need to apply the following conditional distributions @xmath44 therefore , given a fixed mask @xmath45 , we actually have an rbm with all visible nodes @xmath46 and hidden nodes @xmath47 . hidden nodes @xmath48 are fixed to zero",
    "so they have no influence on the conditional rbm .",
    "apart from replacing ( 7 ) with ( 9 ) , the only other change needed is to replace @xmath49 with @xmath50 . in terms of training , we suggest sampling a different mask per data example @xmath51 and per iteration as in [ 17 ] .    a dropconnect ( dc ) rbm is closely related ; given a mask @xmath52 on weights @xmath4 , @xmath4 in a plain rbm is replaced by @xmath53 everywhere .",
    "we suggest sampling a different mask @xmath12 per mini - batch since it is usually much larger than a mask in a dropout rbm .",
    "there are typically many nodes or weights which are of little importance in a neural network . in network",
    "pruning , such unimportant nodes or weights are discarded , and the neural network is retrained .",
    "this process can be conducted iteratively [ 11 ] .",
    "now we consider two variants of network pruning for rbms .",
    "for an trained set of weights @xmath54 with no regularization , we consider implementing a fixed mask @xmath55 where @xmath56 i.e. @xmath57 is the @xmath58-th left percentile of all @xmath59 , and @xmath60 is some fixed proportion of retained weights .",
    "we then recalibrate the weights and biases fixing mask @xmath12 , leading to a simple network pruning ( snp ) procedure which deletes @xmath58 of all weights .",
    "we may also consider deleting @xmath61 of all weights at a time , and conduct the above process @xmath62 times , leading to an iterative network pruning ( inp ) procedure .",
    "we may consider combining some of the above approaches .",
    "for instance , [ 17 ] considered a combination of @xmath0 and dropout .",
    "we introduce two such hybrid approaches , namely partial dropconnect ( pdc ) and partial dropout ( pdo ) , which generalizes dropconnect and dropout , and borrows from network pruning .",
    "the rationale comes from some of the model convergence results below .    again , suppose we have a trained set of weights @xmath54 with no regularization .",
    "instead of implementing a fixed mask @xmath12 , we perform dropconnect regularization with different retaining probabilities @xmath63 for each weight @xmath64 .",
    "we let @xmath65 , and @xmath66    therefore , we sample a different @xmath67 per mini - batch , which means we always keep @xmath68 of all the weights , and randomly drop the remaining weights with probability @xmath58 . the mask @xmath12 can be adjusted iteratively .",
    "intuitively , we are trying to maximize the following @xmath69.\\ ] ] + we propose this technique because we hypothesize that some weights could be more important than others _ a posteriori _ , so dropping them could cause much variation among the models being averaged .",
    "thus , it could be preferable not to drop them .",
    "we demonstrate this point empirically later .",
    "we also consider partial dropout which is analogous to partial dropconnect and does not drop some nodes rather than weights .",
    "we let a mask for nodes @xmath70 , @xmath71 , where @xmath72 this algorithm protects more important hidden nodes from being dropped in order to reduce variation .",
    "we also evaluate its empirical performance later .",
    "here we discuss the model convergence properties of different regularization methods when the number of data examples @xmath73 .",
    "we mark all regularization coefficients and parameter estimates with @xmath74 when there are @xmath75 data examples .",
    "we assume @xmath76 , which is compact , @xmath77 , @xmath78 is unique for each @xmath79 , and @xmath80 are i.i.d . generated from an rbm with a `` true '' set of parameters @xmath81 .",
    "we denote each regularized calibrated set of parameters as @xmath82 .",
    "let @xmath83 and @xmath84 .",
    "[ 24 ] showed that @xmath85 guarantees asymptotic normality and identification of set @xmath86 for linear regression .",
    "we demonstrate that similar results hold for @xmath43 for rbms .",
    "we let @xmath87 and @xmath88 be the @xmath18 and @xmath41 regularization coefficients for each component .",
    "we have the following .",
    "_ proposition 1_. ( a ) if @xmath89 , @xmath90 as @xmath73 , then the estimate @xmath91 ; ( b ) if also , @xmath92 , @xmath93 , then @xmath94 , where @xmath95 is the fisher information matrix ; @xmath96 , where @xmath97 .    _ proof_. for all proofs , see appendix .",
    "@xmath98    for dropout and dropconnect rbms , we also assume that the data is generated from a plain rbm structure .",
    "we assume @xmath99 is of size @xmath100 as in ( 12 ) for dropconnect and of length @xmath95 as in ( 13 ) for dropout , therefore covering the cases of both original and partial dropout / dropconnect . with a decreasing dropping rate @xmath101 with @xmath73 , the following result holds .",
    "_ proposition 2_. if @xmath102 as @xmath73 , then @xmath91 .",
    "@xmath98    for network pruning , we show that as the number of data examples increase , if the retained proportion of parameters @xmath103 can cover all nonzero components of @xmath81 , we will not miss any important component .",
    "_ proposition 3_. assume @xmath104 . then for simple network pruning , as @xmath73 , ( a ) @xmath91 ; ( b ) for sufficiently large @xmath75 , there exists @xmath105 such that @xmath106 .",
    "@xmath98    _ corollary 1_. the above results also hold for iterative network pruning",
    ". @xmath98",
    "we note that for all regularization methods , under the above conditions , the calibrated weights converge to the `` true '' set of parameters @xmath81 , which indicates consistency . also , adding @xmath41 regularization guarantees that we can identify components of zero value with infinitely many examples .",
    "the major benefits of dropout come from the facts that it makes @xmath18 regularization adaptive , and also encourages more confident prediction of the outcomes [ 22 ] .",
    "we propose partial dropconnect also base on proposition 3 , i.e. we do not drop the more important components of @xmath81 , therefore possibly reducing variation caused by dropping influential weights .",
    "partial dropout follows from the same reasoning .",
    "we consider the multilayer network below , @xmath107 where each probability on the right - hand side is from an rbm .",
    "to train the weights of @xmath108 , @xmath109 , @xmath110 , we only need to carry out a greedy layer - wise training approach , i.e. we first train the weights of @xmath108 , and then use @xmath111 to train @xmath112 , etc . the weights of the rbms are used to initialize a deep ffnn which is finetuned with gradient descent .",
    "apparently rbm regularization can be extended to each layer of a dbn .    here",
    "we show that adding layers to a dropout / drop - connect dbn improves the likelihood given symmetry of the weights of two adjacent layers .",
    "similar results for plain dbn are in [ 3 , 7 ] .",
    "we demonstrate this by using likelihood bounds .",
    "we let @xmath113 denote an @xmath114-layer dbn and @xmath115 denote an @xmath116-layer dbn with the first @xmath114 layers being the same as in @xmath113 . for a data example of a visible vector @xmath46",
    ", the log - likelihood is bounded as follows , @xmath117 \\\\ & \\ge e_m[h_{p_{dbn_l}(h^l|v , m ) } ] + \\sum_{h^l } e_{m , m^\\ast}\\{p_{dbn_l}(h^l|v , m)\\\\   & { \\hspace{1em } } \\cdot [ \\log p_{rbm_{l+1}}(h^l|m^\\ast)+\\log p_{dbn_l}(v|h^l , m)]\\}. { \\refstepcounter{equation}\\tag{\\theequation}}\\end{aligned}\\ ] ]    here , @xmath118 is the entropy function , and the derivation is analygous to section 11 in [ 3 ] .",
    "mask @xmath12 is for @xmath113 , and mask @xmath119 is for the new @xmath116-th layer .",
    "note that after we have trained the first @xmath114 layers , and initialized the @xmath116-th layer symmetric to the @xmath114-th layer , assuming a constant dropping probability , @xmath120 = e_{m}[\\log p_{dbn_l}(h^l|m)],\\ ] ] so @xmath115 has the same log - likelihood bound as @xmath113 .",
    "training @xmath121 , @xmath122 $ ] is guaranteed to increase , and therefore the likelihood of @xmath115 is expected to improve . therefore , for regularized unsupervised deep neural nets , adding layers also tend to elevate the explanatory power of the network . adding nodes has the same effect , providing a rationale for deep and large - scale networks .",
    "we present the following preposition .",
    "_ proposition 4_. adding nodes or layers ( preserving weight symmetry ) to a dropout / dropconnect dbn continually improves the likelihood ; also , adding layers of size @xmath123 continually improves the likelihood .",
    "@xmath98      unlike a dbn which trains model ( 14 ) ,",
    "a dbm [ 13 ] trains an undirected graphical model .",
    "the major difference is that in the training procedure for a dbm , both the visible and hidden nodes in the deepest hidden layer are doubled .",
    "the weights for the doubled nodes are tied to the original weights .    for a replicated softmax model ( rsm ) [ 14 ] , the energy function @xmath124 and for a gaussian rbm",
    "[ 12 ] , the energy function @xmath125 where @xmath126 is element - wise .",
    "they are simple extensions of an rbm to count outcomes and real - valued outcomes .",
    "rbm regularization can be easily extended to all these situations .",
    "we compare the empirical performance of the aforementioned regularization methods on the following data sets : mnist , norb ( image recognition ) ; 20 newsgroups , reuters21578 ( text classification ) ; isolet ( speech recognition ) .",
    "all results are obtained using geforce gtx titan x in theano .",
    "we consider the following unsupervised neural network structures : dbn / dbm for mnist ; dbn for norb ; rsm plus logistic regression for 20 newsgroups and reuters21578 ; grbm for isolet .",
    "cd-@xmath1 is performed for the rest of the paper .",
    "the following regularization methods are considered : none ( no regularization ) ; do ; dc ; @xmath18 ; @xmath43 ; snp ; inp(@xmath127 ) ; pdo ; pdc .",
    "the number of pretraining epochs is @xmath128 per layer and the number of finetuning epochs is @xmath129 , with a finetuning learning rate of @xmath130 . for @xmath43 , snp , and inp which need re - calibration , we cut the @xmath128 epochs into two halves ( @xmath131 quarters for inp ) .",
    "for regularization parameters , we apply the following ranges : @xmath132 for do / dc / snp / inp ; @xmath133 for @xmath18 , similar to [ 6 ] ; @xmath134 for @xmath43 ; @xmath135 , @xmath136 or the reverse for pdo / pdc .",
    "we only make one update to the `` partial '' dropping rates to keep simple . from the results",
    ", we note that unsupervised neural networks tend to need less regularization than ffnns .",
    "we choose the best iteration and regularization parameters over a fixed set of parameter values according to the validation error rates .",
    "the http://yann.lecun.com/exdb/mnist/[mnist ] data set consists of @xmath137 pixels of handwritten @xmath138-@xmath139 digits .",
    "there are @xmath140 training examples , @xmath141 validation and @xmath141 testing examples .",
    "we first consider the likelihood of the testing data of an rbm with @xmath142 nodes for mnist .",
    "there are two model fitting evaluation criteria : pseudo - likelihood and ais - likelihood [ 15 ] .",
    "the former is a sum of conditional likelihoods , while the latter directly estimates @xmath143 with ais .",
    "figure 1 is using log - scale .",
    "pretraining epochs .",
    "right : ais - likelihood of the rbm over @xmath142 pretraining epochs . ]",
    "here , @xmath144 for do , and @xmath145 for @xmath18 .",
    "these figures tend to be representative of the model fitting process .",
    "pseudo - likelihood is a more optimistic estimate of the model fitting .",
    "we observe that dropout outperforms the other two after about @xmath146 epochs , and @xmath18 does not improve pseudo - likelihood . in terms of ais - likelihood , which is a much more conservative estimate of the model fitting , the fitting process seems to have three stages : ( 1 ) initial fitting ; ( 2 ) overfitting ; ( 3 ) re - fitting .",
    "we observe that @xmath18 improves the likelihood significantly , while dropout catches up at about @xmath129 epochs .",
    "therefore , both dropout and @xmath18 can improve model fitting according to the likelihood .",
    "classification error rates tend to be a more practical measure .",
    "we first consider a @xmath147-hidden - layer dbn with @xmath148 nodes per layer , pretraining learning rate @xmath149 , and batch size @xmath150 ; see table 1 .",
    "we tried dbns of @xmath1 , @xmath151 , and @xmath131 hidden layers and found the aforementioned structure to perform best with none as baseline .",
    "the same was done for all other structures .",
    "we calculate the means of the classification errors for each regularization method averaged over @xmath152 random replicates and their standard deviations . in each table",
    ", we stress in bold the top @xmath147 performers with ties broken by deviation .",
    "we note that most of the regularization methods tend to improve the classification error rates , with dc and pdo yielding slightly higher error rates than no regularization .",
    ".classification errors for a @xmath147-layer dbn for the mnist data set .",
    "[ cols=\"^,^,^,^,^,^,^,^,^,^\",options=\"header \" , ]      from the above results , we observe that regularization does improve the structure of unsupervised deep neural networks and yields lower classification error rates for each data set studied herein .",
    "the most robust methods which yield improvements for all six instances are @xmath18 , @xmath43 , and pdc .",
    "snp is also acceptable , and preferable over inp .",
    "pdo can yield improvements for dropout when dropout is unsuitable for the network structure .",
    "pdc turns out to be the most stable method of all , and thus the recommended choice .",
    "regularization for deep learning has aroused much interest , and in this paper , we extend regularization to unsupervised deep learning , i.e. for dbns and dbms .",
    "we proposed several approaches , demonstrated their performance , and empirically compared the different techniques .",
    "for the future , we suggest that it would be of interest to consider more variants of model averaging regularization for supervised deep learning as well as better hybrid regularization methods ."
  ],
  "abstract_text": [
    "<S> unsupervised neural networks , such as restricted boltzmann machines ( rbms ) and deep belief networks ( dbns ) , are powerful tools for feature selection and pattern recognition tasks . </S>",
    "<S> we demonstrate that overfitting occurs in such models just as in deep feedforward neural networks , and discuss possible regularization methods to reduce overfitting . </S>",
    "<S> we also propose a `` partial '' approach to improve the efficiency of dropout / dropconnect in this scenario , and discuss the theoretical justification of these methods from model convergence and likelihood bounds . </S>",
    "<S> finally , we compare the performance of these methods based on their likelihood and classification error rates for various pattern recognition data sets .    </S>",
    "<S> content areas : deep learning , neural networks , unsupervised learning </S>"
  ]
}