{
  "article_text": [
    "quantum mechanics gives the most accurate description of many physical systems of interest . in turn",
    ", the most accurate characterization of a quantum device is given by its quantum mechanical model .",
    "thus , efficient methods for the honest estimation of the distribution of parameters in a quantum mechanical model are of utmost importance , not only for building robust quantum technologies , but to reach new regimes of physics .",
    "bayesian experimental design ( see , e.g. @xcite ) is a methodology to ascertain the utility of a proposed experiment .",
    "bayesian experimental design has been successfully applied to problems in experimental physics , such as in the recent examples of @xcite and @xcite . in classical theories of physics and statistics , the measurement simply reveals the state of the system at that instant .",
    "by contrast , quantum theory presents with the following physical ( and conceptual ) barrier : no single measurement can reveal the state .",
    "rather , each potential kind of experiment admits a probability distribution from which we draw our data .",
    "thus , the methodology of experimental design seems tailor - made for quantum theory .",
    "the structure of the paper is as follows .",
    "we begin by reviewing the general outline of bayesian experimental design .",
    "we then apply the technique to devise an algorithm for the estimation of quantum hamiltonian parameters .",
    "we show that in a particular case , this strategy is nearly globally optimal and demonstrate its improvement over standard algorithms numerically . finally we conclude with a discussion on the applicability of this technique to real experiments on more complex quantum systems .",
    "we assume some initial experiment @xmath0 has been performed and data @xmath1 has been obtained .",
    "the goal is to determine @xmath2 , the probability distribution of the model parameters @xmath3 given the experimental data . to achieve this we use bayes rule @xmath4 where @xmath5 is the _ likelihood function _ ,",
    "which is determined through the process of modeling the experiment , and @xmath6 is the _ prior _ , which encodes any _ a priori _ knowledge of the model parameters .",
    "the final term @xmath7 can simply be thought as a normalization factor .    at this stage",
    "we can stop or obtain further data .",
    "experimental design is well suited to quantum theory since an arbitrary fixed measurement procedure does not give maximal knowledge as is often assumed in the statistical modeling of classical system .",
    "we conceive , then , of possible future data @xmath8 obtained from a , possibly different , experiment @xmath9 .",
    "the probability of obtaining this data can be computed from the distributions at hand via marginalizing over model parameters @xmath10 we can use this distribution to calculate the expected _ utility _ of an experiment @xmath11 where @xmath12 is the utility we would derive if experiment @xmath9 gave result @xmath8 .",
    "this could in principle be any function tailored to the specific problem .",
    "however , for scientific inference , a generally well motivated measure of utility is _ information gain _ @xcite . in information theory ,",
    "information is measured by the entropy @xmath13 thus , we search for the experiment which maximizes the expected information in the final distribution . that is , an optimal experiment @xmath14 is one which satisfies @xmath15",
    "as an example of how to apply the bayesian experimental design formalism to problems in quantum information , we consider a simple situation with a single qubit .",
    "in particular , we suppose that the qubit evolves under an internal hamiltonian @xmath16 here @xmath17 is an unknown parameter whose value we want to estimate .",
    "an experiment consists of preparing a single known input state @xmath18 , the @xmath19 eigenstate of @xmath20 , evolving under the hamiltonian @xmath21 for a controllable time @xmath22 and performing a measurement in the @xmath20 basis .",
    "this is the simplest problem where adaptive hamiltonian estimation can be used and is the problem studied in reference @xcite .    in the language of bayesian inference , the data @xmath23 is the outcome of the measurement .",
    "an experiment @xmath0 consists of a specification of time the @xmath22 that the hamiltonian is on , while the model parameter @xmath3 is simply @xmath17 .",
    "the likelihood function is given by the born rule @xmath24 experimental design is a decision theoretic problem based on the utility function @xmath25 the optimal design is any value of @xmath22 which maximizes this quantity .",
    "we proceed by performing the optimal experiment and obtaining data @xmath8 . using bayesian inference we update our prior @xmath26 via bayes rule : @xmath27",
    "if we are not satisfied , we can repeat the process where this distribution becomes the prior for the new experimental design step .",
    "this algorithm is depicted in figure [ fig : overview ] .",
    "overview of a step in the online adaptive algorithm for finding locally optimal experiments .",
    "top : method for calculating the utility function @xmath28 , given a simulator and a prior distribution @xmath29 over model parameters @xmath3 .",
    "bottom : method for updating prior distribution with results @xmath1 from chosen actual experiment . ]",
    "the preceding problem had a single unknown variable .",
    "if we desire an estimate @xmath30 of the true value @xmath3 , the most often used figure of merit is the _ squared error loss _ :",
    "@xmath31    the _ risk _ of an estimator @xmath32 is its expected performance with respect to the loss function : @xmath33 for squared error loss , the risk is also called the _",
    "mean squared error_. the average of this quantity with respect to some prior @xmath34 is the _ bayes risk _ of @xmath35 , @xmath36 and the estimator which minimizes this quantity is called a _ bayes estimator_. in this case the bayes estimator is the mean of the posterior distribution . let us assume then that the estimators we choose are bayes .",
    "let us also choose a uniform prior for @xmath3 .",
    "then , the final figure of merit is the average mean squared error ( amse ) : @xmath37 we would like a strategy which minimizes this quantity .",
    "non - adaptive fourier and bayesian strategies were investigated and compared to an adaptive strategy in reference @xcite .",
    "their adaptive strategy fits into the bayesian experimental design framework when the utility is measured by the _ variance _ of the posterior distribution : @xmath38 where @xmath39 is the mean of the posterior .",
    "recall that the mean is a bayes estimator of amse , so @xmath40 . for a single measurement",
    "this utility function satisfies @xmath41 . that is",
    ", maximizing the utility _ locally _ at each step of the algorithm is equivalent to minimizing the amse at each step .",
    "hence , when using the negative variance as our utility function , the adaptive strategy summarized in figure [ fig : overview ] is an example not only of a local optimization , but also a _ greedy algorithm _ with respect to the amse risk . in the future , we shall refer to this choice of utility function together with the local optimization algorithm as the greedy algorithm for this problem .",
    "we can write the risk of this strategy recursively as follows .",
    "suppose at the @xmath42th , and final , measurement we have the updated distribution @xmath43 .",
    "then , the risk of the local strategy is @xmath44 where @xmath45 is the locally optimal design satisfying @xmath46 the expected risk at any other stage is @xmath47 where @xmath48 is , again , the locally optimal design satisfying @xmath49 then , the bayes risk of the greedy strategy is @xmath50    again , it is clear that the greedy algorithm is globally optimal on the final decision , as there is no further hypothetical data to consider .",
    "that is , the optimal solution at the @xmath42th measurement is @xmath51 where @xmath45 is the locally optimal design satisfying @xmath46 however , the globally optimal risk at any other stage @xmath52 where now @xmath53 is the globally optimal design satisfying @xmath54 then , the bayes risk of the greedy strategy is @xmath55    in general , @xmath56 . nor is it the case that @xmath57 for an arbitrary prior .",
    "however , for the special case of the uniform prior , we have found numerically that the bayes risk of the greedy strategy and the bayes risk of the global strategy are similar enough that the greedy strategy is useful .      in reference",
    "@xcite , it was shown via simulation that the posterior variance of the greedy strategy is best fit by an exponentially decreasing function of @xmath42 , the total number of measurements .",
    "in contrast , all off - line strategies decrease at best as a linear function of @xmath42 .    in figure",
    "[ fig : results ] , we show that the local information gain optimizing algorithm also enjoys an exponential improvement in accuracy over naive off - line methods",
    ". moreover , we show nyquist rate sampling is unnecessary and , indeed , sub - optimal .",
    "all results stated are obtained using a uniform prior on @xmath58 $ ] and are computed numerically by exploring every branch of the the decision tree , in contrast to simulation .",
    "in order to be `` fair '' to the off - line methods , we restricted the adaptive methods to explore the same experimental design specifications .",
    "that is , for this particular problem , the adaptive algorithm was allowed to select measurement times from @xmath59 $ ] , where @xmath60 is the total number of measurements . in principle , these methods could only do better with a larger design specification .",
    "performance of the estimation strategies .",
    "the bayesian sequential and the strategy labeled `` nyquist '' sample at the nyquist rate .",
    "the `` optimized '' strategies find the global maximum utility ( using matlab s `` fmincon '' starting with the optimal nyquist time ) . in each case , @xmath61 measurements are considered .",
    "left : the ideal model discussed in the text .",
    "right : a more realistic model with @xmath62 noise and an addition relaxation process ( known as @xmath63 ) which exponentially decays the signal ( to half its value at @xmath64 ) .",
    ", title=\"fig : \" ]   performance of the estimation strategies . the bayesian sequential and",
    "the strategy labeled `` nyquist '' sample at the nyquist rate .",
    "the `` optimized '' strategies find the global maximum utility ( using matlab s `` fmincon '' starting with the optimal nyquist time ) . in each case , @xmath61 measurements are considered .",
    "left : the ideal model discussed in the text .",
    "right : a more realistic model with @xmath62 noise and an addition relaxation process ( known as @xmath63 ) which exponentially decays the signal ( to half its value at @xmath64 ) .",
    ", title=\"fig : \" ]",
    "summarizing , we have shown for the problem of estimating the parameter in a simple hamiltonian model of qubit dynamics an adaptive measurement strategy can exponentially improve the accuracy over offline estimation strategies .",
    "moreover , we have shown that sampling at the nyquist rate is not optimal in the case of strong measurement .",
    "we have derived a recursive solution to the risks for both the local and global optimal strategies . using this solution",
    ", we numerically found that the local strategy is nearly optimal in the special case of a uniform prior . that the greedy algorithm is nearly optimal in a case relevant to experiment demonstrates that an adaptive bayesian method may be computationally feasible , in that an implementation need not consider all possible future data when choosing each experiment .",
    "together , these results demonstrate the usefulness of an adaptive bayesian algorithm for parameter estimation in quantum mechanical systems , especially in comparison with other algorithms in common use . in the presence of noise ,",
    "this improvement becomes still more stark , as demonstrated by the results shown in figure [ fig : results ] .",
    "why is it the case that the nyquist times are not optimal ?",
    "first , why should we expect them to be optimal ?",
    "the nyquist theorem states that a signal which contains no frequencies higher than @xmath65 is completely and unambiguously characterized by a discrete set of samples taken at a rate greater than or equal to @xmath66 .",
    "however , the classical notion of sampling fails for the strong - measurement case that we consider here .",
    "what we have is a periodic _ probability _ distribution which can be sampled , not a periodic function whose _ values _ can be ascertained .",
    "that is there is no _ signal _ , in the classical sense of the word , which can be reconstructed .",
    "the failure of the nyquist rate sampling is exemplified in figure [ fig : utils ] .     the information gain ( left ) and variance ( right ) utilities for the prior followed by three simulated measurements .",
    "the vertical grid lines indicate the nyquist times .",
    "note that the times at which the utilities are maximized do not necessarily increase with the number of measurements.,title=\"fig : \" ]   the information gain ( left ) and variance ( right ) utilities for the prior followed by three simulated measurements .",
    "the vertical grid lines indicate the nyquist times .",
    "note that the times at which the utilities are maximized do not necessarily increase with the number of measurements.,title=\"fig : \" ]    in this paper , we have chosen to measure success via the squared error loss .",
    "although this is a standard metric , note that it is not practically useful in the context of estimating the parameters of a quantum mechanical system .",
    "we motivate this claim as follows .",
    "a typical application of our algorithm is to inform control theory algorithms , which can achieve significantly higher fidelities if given a distribution over hamiltonians rather than a single best estimate . indeed , in the case of nuclear magnetic resonance , the physical ensemble of qubits produces a real distribution of hamiltonians to which control theory algorithm must be robust against @xcite .",
    "any single estimate of the hamiltonian parameters will thus artificially exclude dynamics which will appear as decoherence in the resultant pulses .",
    "thus , we must measure the success of our algorithm via a loss function of the true _ distribution _ and estimated posterior . noting that relative entropy is broadly considered the correct loss function for probability estimators , our algorithm , which maximizes expected information gain , becomes the optimal solution .",
    "we expect that in more complicated systems , the bayesian adaptive method will remain useful , especially in applications such as optimal control theory , where having a distribution over hamiltonians is significantly more useful than a single best estimate .",
    "u.  von toussaint , t.  schwarz - selinger , m.  mayer , and s.  gori , _ nuclear instruments and methods in physics research section b : beam interactions with materials and atoms _ * 268 * , 21152118 ( 2010 ) , issn 0168 - 583x , http://dx.doi.org/10.1016/j.nimb.2010.02.062 ."
  ],
  "abstract_text": [
    "<S> using bayesian experimental design techniques , we have shown that for a single two - level quantum mechanical system under strong ( projective ) measurement , the dynamical parameters of a model hamiltonian can be estimated with exponentially improved accuracy over offline estimation strategies . to achieve this </S>",
    "<S> , we derive an adaptive protocol which finds the optimal experiments based on previous observations . </S>",
    "<S> we show that the risk associated with this algorithm is close to the global optimum , given a uniform prior . </S>",
    "<S> additionally , we show that sampling at the nyquist rate is not optimal .     </S>",
    "<S> address = institute for quantum computing , university of waterloo , waterloo , ontario , canada , altaddress = department of applied mathematics , university of waterloo , ontario , canada , email=csferrie@uwaterloo.ca     address = institute for quantum computing , university of waterloo , waterloo , ontario , canada , altaddress = department of physics , university of waterloo , ontario , canada , email=cgranade@cgranade.com     address = institute for quantum computing , university of waterloo , waterloo , ontario , canada , altaddress = perimeter institute for theoretical physics , waterloo , ontario , canada , altaddress = department of chemistry , university of waterloo , ontario , canada </S>"
  ]
}