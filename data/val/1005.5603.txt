{
  "article_text": [
    "a sequence @xmath0 of discrete - valued observations ( @xmath3 , @xmath4 is finite ) is generated according to some unknown probabilistic law ( measure ) .",
    "that is , @xmath1 is a probability measure on the space @xmath5 of one - way infinite sequences ( here @xmath6 is the usual borel @xmath7-algebra ) . after each new outcome @xmath8",
    "is revealed , it is required to predict conditional _ probabilities _ of the next observation @xmath9 , @xmath10 , given the past @xmath11 . since a predictor @xmath12 is required to give conditional probabilities @xmath13 for all possible histories @xmath11 , it defines itself a probability measure on the space @xmath14 of one - way infinite sequences . in other words , a probability measure on @xmath14 can be considered both as a data - generating mechanism and as a predictor .    therefore , given a set @xmath15 of probability measures on @xmath14 , one can ask two kinds of questions about it . first , does there exist a predictor @xmath12 , whose forecast probabilities converge ( in a certain sense ) to the @xmath1-conditional probabilities , if an arbitrary @xmath16 is chosen to generate the data ? here",
    "we assume that the `` true '' measure that generates the data belongs to the set @xmath2 of interest , and would like to construct a predictor that predicts all measures in @xmath2 .",
    "the second type of questions is as follows : does there exist a predictor that predicts at least as well as any predictor @xmath17 , if the measure that generates the data comes possibly from outside of @xmath2 ?",
    "therefore , here we consider elements of @xmath2 as predictors , and we would like to combine their predictive properties , if this is possible . note that in this setting the two questions above concern the same object : a set @xmath2 of probability measures on @xmath14 .",
    "each of these two questions , the realizable and non - realizable one , have enjoyed much attention in the literature ; the setting for the non - realizable case is usually slightly different , which is probably why it has not ( to the best of the author s knowledge ) been studied as another facet of the realizable case .",
    "the realizable case traces back to laplace , who has considered the problem of predicting outcomes of a series of independent tosses of a biased coin .",
    "that is , he has considered the case when the set @xmath2 is that of all i.i.d .  process measures .",
    "other classical examples studied are the set of all computable ( or semi - computable ) measures @xcite , the set of @xmath18-order markov and finite - memory processes ( e.g. @xcite ) and the set of all stationary processes @xcite . the general question of finding predictors for an arbitrary given set @xmath2 of process measures has been addressed in @xcite ; the latter work shows that when a solution exists it can be obtained as a bayes mixture over a countable subset of  @xmath2 .",
    "the non - realizable case is usually studied in a slightly different , non - probabilistic , setting .",
    "we refer to @xcite for a comprehensive overview .",
    "it is usually assumed that the observed sequence of outcomes is an arbitrary ( deterministic ) sequence ; it is required not to give conditional probabilities , but just deterministic guesses ( although these guesses can be selected using randomisation ) .",
    "predictions result in a certain loss , which is required to be small as compared to the loss of a given set of reference predictors ( experts ) @xmath2 .",
    "the losses of the experts and the predictor are observed after each round . in this approach , it is mostly assumed that the set @xmath2 is finite or countable .",
    "the main difference with the formulation considered in this work is that we require a predictor to give probabilities , and thus the loss is with respect to something never observed ( probabilities , not outcomes ) .",
    "the loss itself is not completely observable in our setting . in this sense",
    "our non - realizable version of the problem is more difficult . assuming that the data generating mechanism is probabilistic , even if it is completely unknown , makes sense in such problems as , for example , game playing , or market analysis . in these cases",
    "one may wish to assign smaller loss to those models or experts who give probabilities closer to the correct ones ( which are never observed ) , even though different probability forecasts can often result in the same action .",
    "aiming at predicting probabilities of outcomes also allows us to abstract from the actual use of the predictions ( e.g. making bets ) and thus from considering losses in a general form ; instead , we can concentrate on the form of losses ( measuring the discrepancy between the forecast and true probabilities ) which are more convenient for the analysis . in this latter respect ,",
    "the problems we consider are easier than those considered in prediction with expert advice .",
    "( however , in principle nothing restricts us to considering the simple losses that we chose ; they are just a convenient choice . )",
    "noteworthy , the probabilistic approach also makes the machinery of probability theory applicable , hopefully making the problem easier . in this work",
    "we consider two measures of the quality of prediction .",
    "the first one is the total variation distance , which measures the difference between the forecast and the `` true '' conditional probabilities of all future events ( not just the probability of the next outcome ) .",
    "the second one is expected ( over the data ) average ( over time ) kullback - leibler divergence . requiring that predicted and true probabilities",
    "converge in total variation is very strong ; in particular , this is possible if @xcite and only if @xcite the process measure generating the data is absolutely continuous with respect to the predictor .",
    "the latter fact makes the sequence prediction problem relatively easy to analyse .",
    "here we investigate what can be paralleled for the other measure of prediction quality ( average kl divergence ) , which is much weaker , and thus allows for solutions for the cases of much larger sets @xmath2 of process measures ( considered either as predictors or as data generating mechanisms ) .",
    "having introduced our measures of prediction quality , we can further break the non - realizable case into two problems .",
    "the first one is as follows . given a set @xmath2 of predictors , we want to find a predictor whose prediction error converges to zero if there is at least one predictor in @xmath2 whose prediction error converges to zero ; we call this problem simply the `` non - realisable '' case , or problem  2 ( leaving the name `` problem  1 '' to the realizable case ) .",
    "the second problem is the `` fully agnostic '' problem : it is to make the prediction error asymptotically as small as that of the best ( for the given process measure generating the data ) predictor in @xmath2 ( we call this problem  3 ) .",
    "thus , we now have three problems about a set of process measures @xmath2 to address .",
    "we show that if the quality of prediction is measured in total variation , then all the three problems coincide : any solution to any one of them is a solution to the other two . for the case of expected average kl divergence ,",
    "all the three problems are different : the realizable case is strictly easier than non - realizable ( problem 2 ) , which is , in turn , strictly easier than the fully agnostic case ( problem 3 ) .",
    "we then analyse which results concerning prediction in total variation can be transferred to which of the problems concerning prediction in average kl divergence . it was shown in @xcite that , for the realizable case , if there is a solution for a given set of process measures @xmath2 , then a solution can also be obtained as a bayesian mixture over a countable subset of @xmath2 ; this holds both for prediction in total variation and in expected average kl divergence .",
    "here we show that this result also holds true for the ( non - realizable ) case of problem 2 , for prediction in expected average kl divergence . for the fully agnostic case of problem 3 ,",
    "we show that separability with respect to a certain topology given by kl divergence is a sufficient ( though not a necessary ) condition for the existence of a predictor .",
    "this is used to demonstrate that there is a solution to this problem for the set of all finite - memory process measures , complementing similar results obtained earlier in different settings . on the other hand ,",
    "we show that there is no solution to this problem for the set of all stationary process measures , in contrast to a result of @xcite which gives a solution to the realizable case of this problem ( that is , a predictor whose expected average kl error goes to zero if any stationary process is chosen to generate the data ) .",
    "let @xmath4 be a finite set .",
    "the notation @xmath19 is used for @xmath20 .",
    "we consider stochastic processes ( probability measures ) on @xmath21 where @xmath6 is the sigma - field generated by the cylinder sets @xmath22 $ ] , @xmath23 and @xmath22 $ ] is the set of all infinite sequences that start with @xmath19 . for",
    "a finite set @xmath24 denote @xmath25 its cardinality .",
    "we use @xmath26 for expectation with respect to a measure @xmath1 .",
    "next we introduce the measures of the quality of prediction used in this paper . for two measures @xmath1 and @xmath12 we are interested in how different the @xmath1- and @xmath12-conditional probabilities are , given a data sample @xmath19 .",
    "introduce the _ ( conditional ) total variation _",
    "distance @xmath27 if @xmath28 and @xmath29 , and @xmath30 otherwise .",
    "we say that @xmath12 predicts @xmath1 in total variation if @xmath31    this convergence is rather strong . in particular",
    ", it means that @xmath12-conditional probabilities of arbitrary far - off events converge to @xmath1-conditional probabilities .",
    "moreover , @xmath12 predicts @xmath1 in total variation if @xcite and only if @xcite @xmath1 is absolutely continuous with respect to @xmath12 . denote @xmath32 the relation of absolute continuity ( that is ,",
    "@xmath33 if @xmath1 is absolutely continuous with respect to @xmath12 ) .",
    "thus , for a class @xmath2 of measures there is a predictor @xmath12 that predicts every @xmath16 in total variation if and only if every @xmath16 has a density with respect to @xmath12 .",
    "although such sets of processes are rather large , they do not include even such basic examples as the set of all bernoulli i.i.d . processes .",
    "that is , there is no @xmath12 that would predict in total variation every bernoulli i.i.d .",
    "process measure @xmath34 , @xmath35 $ ] , where @xmath36 is the probability of @xmath37 .",
    "therefore , perhaps for many ( if not most ) practical applications this measure of the quality of prediction is too strong , and one is interested in weaker measures of performance .    for two measures @xmath1 and @xmath12 introduce the _ expected cumulative kullback - leibler divergence ( kl divergence ) _ as @xmath38 in words , we take the expected ( over data ) cumulative ( over time ) kl divergence between @xmath1- and @xmath12-conditional ( on the past data ) probability distributions of the next outcome .",
    "we say that @xmath12 predicts @xmath1 in expected average kl divergence if @xmath39    this measure of performance is much weaker , in the sense that it requires good predictions only one step ahead , and not on every step but only on average ; also the convergence is not with probability 1 but in expectation .",
    "with prediction quality so measured , predictors exist for relatively large classes of measures ; most notably , @xcite provides a predictor which predicts every stationary process in expected average kl divergence .",
    "we will use the following well - known identity @xmath40 where on the right - hand side we have simply the kl divergence between measures @xmath1 and @xmath12 restricted to the first @xmath41 observations .",
    "thus , the results of this work will be established with respect to two very different measures of prediction quality , one of which is very strong and the other rather weak .",
    "this suggests that the facts established reflect some fundamental properties of the problem of prediction , rather than those pertinent to particular measures of performance . on the other hand ,",
    "it remains open to extend the results below to different measures of performance .",
    "introduce the following classes of process measures : @xmath42 the set of all process measures , @xmath43 the set of all degenerate discrete process measures , @xmath44 the set of all stationary processes , and @xmath45 the set of all stationary measures with memory not greater than @xmath18 ( @xmath18-order markov processes , with @xmath46 being the set of all i.i.d .",
    "processes ) : @xmath47 @xmath48 @xmath49    abusing the notation , we will sometimes use elements of @xmath43 and @xmath50 interchangeably . the following simple statement (",
    "whose proof is obvious ) will be used repeatedly in the examples .",
    "[ th : disc ] for every @xmath51 there exists @xmath52 such that @xmath53 for all @xmath54 .",
    "for the two notions of predictive quality introduced , we can now start stating formally the sequence prediction problems .",
    "+ * problem 1*(realizable case ) . given a set of probability measures",
    "@xmath2 , find a measure @xmath12 such that @xmath12 predicts in total variation ( expected average kl divergence ) every @xmath16 , if such a @xmath12 exists .",
    "thus , problem  1 is about finding a predictor for the case when the process generating the data is known to belong to a given class @xmath15 .",
    "the set @xmath2 here is a set of measures generating the data .",
    "next let us formulate the questions about @xmath2 as a set of predictors .",
    "* problem 2 * ( non - realizable case ) . given a set of process measures ( predictors ) @xmath2 ,",
    "find a process measure @xmath12 such that @xmath12 predicts in total variation ( in expected average kl divergence ) every measure @xmath55 such that there is @xmath16 which predicts ( in the same sense ) @xmath56 .    while problem  2 is already quite general , it does not yet address what can be called the fully agnostic case : if nothing at all is known about the process @xmath56 generating the data , it means that there may be no @xmath16 such that @xmath1 predicts @xmath56 , and then , even if we have a solution @xmath12 to the problem  2 , we still do not know what the performance of @xmath12 on @xmath56 is going to be , compared to the performance of the predictors from @xmath2 . to address the fully agnostic case",
    ", we have to introduce the notion of loss .",
    "[ def : loss ] introduce the almost sure total variation loss of @xmath12 with respect to  @xmath1 @xmath57 : \\limsup_{n\\to\\infty } v(\\mu,\\rho , x_{1 .. n})\\le\\alpha\\ \\mu\\text{--a.s.}\\},\\ ] ] and the asymptotic kl loss @xmath58    we can now formulate the fully agnostic version of the sequence prediction problem .    * problem 3 . *",
    "given a set of process measures ( predictors ) @xmath2 , find a process measure @xmath12 such that @xmath12 predicts at least as well as any @xmath1 in @xmath2 , if any process measure @xmath55 is chosen to generate the data : @xmath59 for every @xmath55 and every @xmath16 , where @xmath60 is either @xmath61 or @xmath62 .",
    "the three problems just formulated represent different conceptual approaches to the sequence prediction problem .",
    "let us illustrate the difference by the following informal example .",
    "suppose that the set @xmath2 is that of all ( ergodic , finite - state ) markov chains .",
    "markov chains being a familiar object in probability and statistics , we can easily construct a predictor @xmath12 that predicts every @xmath16 ( for example , in expected average kl divergence , see @xcite ) .",
    "that is , if we know that the process @xmath1 generating the data is markovian , we know that our predictor is going to perform well .",
    "this is the realizable case of problem 1 . in reality , rarely can we be sure that the markov assumption holds true for the data at hand .",
    "we may believe , however , that it is still a reasonable assumption , in the sense that there is a markovian model which , for our purposes ( for the purposes of prediction ) , is a good model of the data .",
    "thus we may assume that there is a markov model ( a predictor ) that predicts well the process that we observe , and we would like to combine the predictive qualities of all these markov models .",
    "this is the `` non - realizable '' case of problem  2 .",
    "note that this problem is more difficult than the first one ; in particular , a process @xmath56 generating the data may be singular with respect to any markov process , and still be well predicted ( in the sense of expected average kl divergence , for example ) by some of them .",
    "still , here we are making some assumptions about the process generating the data , and if these assumptions are wrong , then we do not know anything about the performance of our predictor .",
    "thus we may ultimately wish to acknowledge that we do not know anything at all about the data ; we still know a lot about markov processes , and we would like to use this knowledge on our data . if there is anything at all markovian in it ( that is , anything that can be captured by a markov model ) , then we would like our predictor to use it .",
    "in other words , we want to have a predictor that predicts any process measure whatsoever ( at least ) as well as any markov predictor .",
    "this is the `` fully agnostic '' case of problem  3 .    of course",
    ", markov processes were just mentioned as an example , while in this work we are only concerned with the most general case of arbitrary unknown ( uncountable ) sets @xmath2 of process measures .",
    "the following statement is rather obvious .",
    "[ th:1 ] any solution to problem  3 is a solution to problem  2 , and any solution to problem  2 is a solution to problem  1 .    despite the conceptual differences in formulations , it may be somewhat unclear whether the three problems are indeed different .",
    "it appears that this depends on the measure of predictive quality chosen : for the case of prediction in total variation distance , all the three problems coincide , while for the case of prediction in expected average kl divergence , they are different .",
    "as it was mentioned , a measure @xmath1 is absolutely continuous with respect to a measure @xmath12 if and only if @xmath12 predicts @xmath1 in total variation distance .",
    "this reduces studying at least problem  1 for total variation distance to studying the relation of absolute continuity . introduce the notation @xmath33 for this relation .",
    "let us briefly recall some facts we know about @xmath32 ; details can be found , for example , in @xcite . let @xmath63_{tv}$ ] denote the set of equivalence classes of @xmath64 with respect to @xmath32 , and for @xmath65 denote @xmath66 $ ] the equivalence class that contains @xmath1 .",
    "two elements @xmath67_{tv}$ ] ( or @xmath68 ) are called disjoint ( or singular ) if there is no @xmath69_{tv}$ ] such that @xmath70 and @xmath71 ; in this case we write @xmath72 .",
    "we write @xmath73 + [ \\mu_2]$ ] for @xmath74 $ ] .",
    "every pair @xmath67_{tv}$ ] has a supremum @xmath75 . introducing into @xmath63_{tv}$ ] an extra element @xmath37 such that @xmath76 for all @xmath77_{tv}$ ]",
    ", we can state that for every @xmath78_{tv}$ ] there exists a unique pair of elements @xmath79 and @xmath80 such that @xmath81 , @xmath82 and @xmath83 .",
    "( this is a form of lebesgue decomposition . )",
    "moreover , @xmath84 .",
    "thus , every pair of elements has a supremum and an infimum .",
    "moreover , every bounded set of disjoint elements of @xmath63_{tv}$ ] is at most countable .",
    "furthermore , we introduce the ( unconditional ) total variation distance between process measures .",
    "the ( unconditional ) total variation distance is defined as @xmath85    known characterizations of those sets @xmath2 that are bounded with respect to @xmath32 can now be related to our prediction problems 1 - 3 as follows .",
    "[ th : tv ] let @xmath86 .",
    "the following statements about @xmath2 are equivalent .",
    "* there exists a solution to problem 1 in total variation .",
    "* there exists a solution to problem 2 in total variation .",
    "* there exists a solution to problem 3 in total variation .",
    "* @xmath2 is upper - bounded with respect to @xmath32 .",
    "* there exists a sequence @xmath87 , @xmath88 such that for some ( equivalently , for every ) sequence of weights @xmath89 $ ] , @xmath88 such that @xmath90 , the measure @xmath91 satisfies @xmath92 for every @xmath16 .",
    "* @xmath2 is separable with respect to the total variation distance .",
    "* let @xmath93 .",
    "every disjoint ( with respect to @xmath32 ) subset of @xmath94 is at most countable .    moreover , every solution to any of the problems 1 - 3 is a solution to the other two , as is any upper bound for @xmath2 .",
    "the sequence @xmath95 in the statement ( v ) can be taken to be any dense ( in the total variation distance ) countable subset of @xmath2 ( cf .",
    "( vi ) ) , or any maximal disjoint ( with respect to @xmath32 ) subset of @xmath94 of statement ( vii ) , in which every measure that is not in @xmath2 is replaced by any measure from @xmath2 that dominates it .    the implications @xmath96 are obvious ( cf",
    ".  proposition  [ th:1 ] ) .",
    "the implication @xmath97 is a reformulation of the result of @xcite .",
    "the converse ( and hence @xmath98 ) was established in @xcite .",
    "@xmath99 follows from the equivalence @xmath100 and the transitivity of @xmath32 ; @xmath101 follows from the transitivity of @xmath32 and from lemma  [ th:01 ] below : indeed , from lemma  [ th:01 ] we have @xmath102 if @xmath103 and @xmath104 otherwise . from this and the transitivity of @xmath105",
    "it follows that if @xmath106 then also @xmath107 for all @xmath55 . the equivalence of @xmath108 , @xmath109 , and @xmath110 was established in @xcite .",
    "the equivalence of @xmath111 and @xmath112 was proven in @xcite .",
    "the concluding statements of the theorem are easy to demonstrate from the results cited above .",
    "the following lemma is an easy consequence of @xcite .",
    "[ th:01 ] let @xmath113 be two process measures .",
    "then @xmath114 converges to either 0 or 1 with @xmath1-probability  1 .",
    "assume that @xmath1 is not absolutely continuous with respect to @xmath12 ( the other case is covered by @xcite ) .",
    "by lebesgue decomposition theorem , the measure @xmath1 admits a representation @xmath115 where @xmath116 $ ] and the measures @xmath80 and @xmath79 are such that @xmath80 is absolutely continuous with respect to @xmath12 and @xmath79 is singular with respect to @xmath12 .",
    "let @xmath117 be such a set that @xmath118 and @xmath119 .",
    "note that we can take @xmath120 and @xmath121 .",
    "from @xcite we have @xmath122 @xmath80-a.s . , as well as @xmath123 @xmath80-a.s .  and @xmath124 @xmath79-a.s",
    "moreover , @xmath125 so that @xmath126 @xmath79-a.s . furthermore , @xmath127 and @xmath128 we have @xmath129 @xmath80-a.s .  and hence @xmath130-a.s . , as well as @xmath131 @xmath79-a.s .  and hence @xmath132-a.s",
    "thus , @xmath133 , which concludes the proof .    * remark . *",
    "using lemma  [ th:01 ] we can also define _ expected _",
    "( rather than almost sure ) total variation loss of @xmath12 with respect to @xmath1 , as the @xmath1-probability that @xmath134 converges to  1 : @xmath135 then problem  3 can be reformulated for this notion of loss .",
    "however , it is easy to see that for this reformulation theorem  [ th : tv ] holds true as well .",
    "thus , we can see that , for the case of prediction in total variation , all the sequence prediction problems formulated reduce to studying the relation of absolute continuity for process measures and those families of measures that are absolutely continuous ( have a density ) with respect to some measure ( a predictor ) .",
    "on the one hand , from a statistical point of view such families are rather large : the assumption that the probabilistic law in question has a density with respect to some ( nice ) measure is a standard one in statistics .",
    "it should also be mentioned that such families can easily be uncountable .",
    "( in particular , this means that they are large from a computational point of view . ) on the other hand , even such basic examples as the set of all bernoulli i.i.d .",
    "measures does not allow for a predictor that predicts every measure in total variation ( as explained in section  [ s : pre ] ) .",
    "that is why we have to consider weaker notions of predictions ; from these , prediction in expected average kl divergence is perhaps one of the weakest .",
    "the goal of the next sections is to see which of the properties that we have for total variation can be transferred ( and in which sense ) to the case of expected average kl divergence .",
    "first of all , we have to observe that for prediction in kl divergence problems 1 , 2 , and 3 are different , as the following theorem shows .",
    "while the examples provided in the proof are artificial , there is a very important example illustrating the difference between problem  1 and problem  3 for expected average kl divergence : the set @xmath136 of all stationary processes , given in theorem  [ th : st ] in the end of this section .",
    "[ th : comp ] for the case of prediction in expected average kl divergence , problems 1 , 2 and 3 are different : there exists a set @xmath137 for which there is a solution to problem 1 but there is no solution to problem  2 , and there is a set @xmath138 for which there is a solution to problem 2 but there is no solution to problem  3 .",
    "we have to provide two examples .",
    "fix the binary alphabet @xmath139 . for each deterministic sequence",
    "@xmath140 construct the process measure @xmath141 as follows : @xmath142 and for @xmath143 let @xmath144 , for all @xmath54 .",
    "that is , @xmath141 is bernoulli i.i.d .",
    "1/2 process measure strongly biased towards a specific deterministic sequence , @xmath145 .",
    "let also @xmath146 for all @xmath147 , @xmath54 ( the bernoulli i.i.d .",
    "1/2 ) . for the set",
    "@xmath148 we have a solution to problem 1 : indeed , @xmath149 .",
    "however , there is no solution to problem  2 .",
    "indeed , for each @xmath150 we have @xmath151 ( that is , for every deterministic measure there is an element of @xmath152 which predicts it ) , while by lemma  [ th : disc ] for every @xmath51 there exists @xmath153 such that @xmath154 for all @xmath54 ( that is , there is no predictor which predicts every measure that is predicted by at least one element of @xmath152 ) .",
    "the second example is similar . for each deterministic sequence @xmath155 construct the process measure @xmath141 as follows : @xmath156 and for @xmath143 let @xmath157 , for all @xmath54 .",
    "it is easy to see that @xmath158 is a solution to problem 2 for the set @xmath159 . indeed ,",
    "if @xmath55 is such that @xmath160 then we must have @xmath161 . from this and the fact that @xmath158 and @xmath162 coincide ( up to @xmath163 ) on all other sequences we conclude @xmath164 .",
    "however , there is no solution to problem 3 for @xmath165 . indeed , for every @xmath153 we have @xmath166 .",
    "therefore , if @xmath12 is a solution to problem 3 then @xmath167 which contradicts lemma  [ th : disc ] .",
    "thus , prediction in expected average kl divergence turns out to be a more complicated matter than prediction in total variation .",
    "the next idea is to try and see which of the facts about prediction in total variation can be generalized to some of the problems concerning prediction in expected average kl divergence .",
    "first , observe that , for the case of prediction in total variation , the equivalence of problems  1 and  2 was derived from the transitivity of the relation @xmath32 of absolute continuity . for the case of expected average kl divergence , the relation `` @xmath12 predicts @xmath1 in expected average kl divergence '' is not transitive ( and problems  1 and  2 are not equivalent ) .",
    "however , for problem  2 we are interested in the following relation : @xmath12 `` dominates '' @xmath1 if @xmath12 predicts every @xmath56 such that @xmath1 predicts @xmath56 .",
    "denote this relation by @xmath168 :    we write @xmath169 if for every @xmath55 the equality @xmath170 implies @xmath171 .",
    "the relation @xmath168 has some similarities with @xmath32 .",
    "first of all , @xmath168 is also transitive ( as can be easily seen from the definition ) . moreover , similarly to @xmath32",
    ", one can show that for any @xmath172 any strictly convex combination @xmath173 is a supremum of @xmath174 with respect to  @xmath168 .",
    "next we will obtain a characterization of predictability with respect to @xmath168 similar to one of those obtained for @xmath32 .",
    "the key observation is the following .",
    "if there is a solution to problem  2 for a set @xmath2 then a solution can be obtained as a bayesian mixture over a countable subset of @xmath2 . for total variation",
    "this is the statement @xmath108 of theorem  [ th : tv ] .",
    "[ th:2 ] let @xmath2 be a set of probability measures on @xmath14 .",
    "if there is a measure @xmath12 such that @xmath169 for every @xmath16 ( @xmath12 is a solution to problem  2 ) then there is a sequence @xmath87 , @xmath88 , such that @xmath175 for every @xmath16 , where @xmath176 are some positive weights .",
    "the proof is deferred to appendix . an analogous result for problem  1 was established in @xcite .",
    "( the proof of theorem  [ th:2 ] is based on similar ideas , but is more involved . )    for the case of problem  3 , we do not have results similar to theorem  [ th:2 ] ( or statement @xmath108 of theorem  [ th : tv ] ) ; in fact , we conjecture that the opposite is true : there exists a ( measurable ) set @xmath2 of measures such that there is a solution to problem  3 for @xmath2 , but there is no bayesian solution to problem  3 , meaning that there is no probability distribution on @xmath2 ( discrete or not ) such that the mixture over @xmath2 with respect to this distribution is a solution to problem  3 for @xmath2 .    however",
    ", we can take a different route and extend another part of theorem  [ th : tv ] to obtain a characterization of sets @xmath2 for which a solution to problem  3 exists .",
    "we have seen that , in the case of prediction in total variation , separability with respect to the topology of this distance is a necessary and sufficient condition for the existence of a solution to problems 1 - 3 . in the case of expected average kl divergence",
    "the situation is somewhat different , since , first of all , ( asymptotic average ) kl divergence is not a metric .",
    "while one can introduce a topology based on it , separability with respect to this topology turns out to be a sufficient but not a necessary condition for the existence of a predictor , as is shown in the next theorem .",
    "[ def : dinf ] define the distance @xmath177 on process measures as follows @xmath178 where we assume @xmath179 .",
    "clearly , @xmath180 is symmetric and satisfies the triangle inequality , but it is not exact . moreover , for every @xmath181 we have @xmath182 the distance @xmath177 measures the difference in behaviour of @xmath183 and @xmath184 on all individual sequences .",
    "thus , using this distance to analyse problem  3 is most close to the traditional approach to the non - realizable case , which is formulated in terms of predicting individual deterministic sequences .",
    "[ th : dinf ]    * let @xmath2 be a set of process measures .",
    "if @xmath2 is separable with respect to @xmath180 then there is a solution to problem  3 for @xmath2 , for the case of prediction in expected average kl divergence .",
    "* there exists a set of process measures @xmath2 such that @xmath2 is not separable with respect to @xmath180 , but there is a solution to problem  3 for this set , for the case of prediction in expected average kl divergence .    for the first statement , let @xmath2 be separable and let @xmath185 be a dense countable subset of @xmath2 .",
    "define @xmath186 , where @xmath176 are any positive summable weights .",
    "fix any measure @xmath187 and any @xmath16 .",
    "we will show that @xmath188 .",
    "for every @xmath189 , find such a @xmath88 that @xmath190 .",
    "we have @xmath191 from this , dividing by @xmath41 taking @xmath192 on both sides , we conclude @xmath193 since this holds for every @xmath194 the first statement is proven .",
    "the second statement is proven by the following example .",
    "let @xmath2 be the set of all deterministic sequences ( measures concentrated on just one sequence ) such that the number of 0s in the first @xmath41 symbols is less than @xmath195 , for all @xmath54 . clearly , this set is uncountable .",
    "it is easy to check that @xmath196 implies @xmath197 for every @xmath198 , but the predictor @xmath56 , given by @xmath199 independently for different @xmath41 , predicts every @xmath16 in expected average kl divergence . since all elements of @xmath2 are deterministic , @xmath56 is also a solution to problem  3 for @xmath2 .",
    "although simple , theorem  [ th : dinf ] can be used to establish the existence of a solution to problem  3 for an important class of process measures : that of all processes with finite memory , as the next theorem shows",
    ". results similar to theorem  [ th : mark ] are known in different settings , e.g. , @xcite and others .",
    "[ th : mark ] there exists a solution to problem  3 for prediction in expected average kl divergence for the set of all finite - memory process measures @xmath200 .",
    "we will show that the set @xmath201 is separable with respect to @xmath180 .",
    "then the statement will follow from theorem  [ th : dinf ] .",
    "it is enough to show that each set @xmath45 is separable with respect to @xmath180 .    for simplicity ,",
    "assume that the alphabet is binary ( @xmath202 ; the general case is analogous ) .",
    "observe that the family @xmath45 of @xmath18-order stationary binary - valued markov processes is parametrized by @xmath203 @xmath204$]-valued parameters : probability of observing @xmath37 after observing @xmath205 , for each @xmath206 .",
    "note that this parametrization is continuous ( as a mapping from the parameter space with the euclidean topology to @xmath45 with the topology of @xmath180 ) .",
    "indeed , for any @xmath207 and every @xmath208 such that @xmath209 , @xmath210 , it is easy to see that @xmath211 so that the right - hand side of  ( [ eq : m3 ] ) also upper - bounds @xmath177 , implying continuity of the parametrization .",
    "it follows that the set @xmath212 , @xmath213 of all stationary @xmath18-order markov processes with rational values of all the parameters ( @xmath214 $ ] ) is dense in @xmath45 , proving the separability of the latter set .",
    "another important example is the set of all stationary process measures @xmath136 .",
    "this example also illustrates the difference between the prediction problems that we consider .",
    "for this set a solution to problem  1 was given in @xcite .",
    "in contrast , here we show that there is no solution to problem  3 for @xmath136 .",
    "[ th : st ] there is no solution to problem  3 for the set of all stationary processes @xmath136 .",
    "this proof is based on the construction similar to the one used in @xcite to demonstrate impossibility of consistent prediction of stationary processes without cesaro averaging .",
    "let @xmath215 be a markov chain with states @xmath216 and state transitions defined as follows . from each sate @xmath217 the chain passes to the state @xmath218 with probability 2/3 and to the state 0 with probability 1/3 .",
    "it is easy to see that this chain possesses a unique stationary distribution on the set of states ( see , e.g. , @xcite ) ; taken as the initial distribution it defines a stationary ergodic process with values in @xmath219 .",
    "fix the ternary alphabet @xmath220 . for",
    "each sequence @xmath221 define the process @xmath222 as follows .",
    "it is a deterministic function of the chain @xmath215 . if the chain is in the state 0 then the process @xmath222 outputs @xmath223 ; if the chain @xmath215 is in the state @xmath224 then the process outputs @xmath225 .",
    "that is , we have defined a hidden markov process which in the state 0 of the underlying markov chain always outputs @xmath223 , while in other states it outputs either @xmath37 or @xmath226 according to the sequence  @xmath145 .    to show that there is no solution to problem 3 for @xmath136",
    ", we will show that there is no solution to problem  3 for the smaller set @xmath227 .",
    "indeed , for any @xmath228 we have @xmath229 .",
    "then if @xmath12 is a solution to problem  3 for @xmath2 we should have @xmath230 for every @xmath153 , which contradicts lemma  [ th : disc ] .    from the proof theorem  [ th :",
    "st ] one can see that , in fact , the statement that is proven is stronger : there is no solution to problem  3 for the set of all functions of stationary ergodic countable - state markov chains .",
    "we conjecture that a solution to problem  2 exists for the latter set , but not for the set of all stationary processes .",
    "it has been long realized that the so - called probabilistic and agnostic ( adversarial , non - stochastic , deterministic ) settings of the problem of sequential prediction are strongly related .",
    "this has been most evident from looking at the solutions to these problems , which are usually based on the same ideas .",
    "here we have proposed a formulation of the agnostic problem as a non - realizable case of the probabilistic problem . while being very close to the traditional one",
    ", this setting allows us to directly compare the two problems . as a somewhat surprising result",
    ", we can see that whether the two problems are different depends on the measure of performance chosen : in the case of prediction in total variation distance they coincide , while in the case of prediction in expected average kl divergence they are different . in the latter case",
    ", the distinction becomes particularly apparent on the example of stationary processes : while a solution to the realizable problem has long been known , here we have shown that there is no solution to the agnostic version of this problem",
    ". the new formalization also allowed us to introduce another problem that lies in between the realizable and the fully agnostic problems : given a class of process measures @xmath2 , find a predictor that is predicts asymptotically optimal every measure for which at least one of the measures in @xmath2 is asymptotically optimal ( problem  2 ) .",
    "this problem is less restrictive then the fully agnostic one ( in particular , it is not concerned with the behaviour of a predictor on every deterministic sequence ) but at the same time the solutions to this problem have performance guarantees far outside the model class considered .    since the problem formulations presented here are mostly new ( at least , in such a general form ) , it is not surprising that there are many questions left open .",
    "a promising route to obtain new results seems to be to first analyse the case of prediction in total variation , which amounts to studying the relation of absolute continuity and singularity of probability measures , and then to try and find analogues in less restrictive ( and thus more interesting and difficult ) cases of predicting only the next observation , possibly with cesaro averaging .",
    "this is the approach that we took in this work .",
    "here it is interesting to find properties common to all or most of the prediction problems ( in total variation as well as with respect to other measures of the performance ) .",
    "a candidate is the `` countable bayes '' property of theorem  [ th:2 ] : if there is a solution to a given sequence prediction problem for a set @xmath2 , then a solution can be obtained as a mixture over a suitable countable subset of @xmath2 .    another direction for future research concerns finite - time performance analysis . in this work",
    "we have adopted the asymptotic approach to the prediction problem , ignoring the behaviour of predictors before asymptotic .",
    "while for prediction in total variation it is a natural choice , for other measures of performance , including average kl divergence , it is clear that problems 1 - 3 admit non - asymptotic formulations .",
    "it is also interesting what are the relations between performance guarantees that can be obtained in non - asymptotic formulations of problems  13 .",
    "define the weights @xmath231 , where @xmath232 is the normalizer @xmath233 . define the sets @xmath234 as the set of all measures @xmath235 such that @xmath1 predicts @xmath187 in expected average kl divergence",
    "let @xmath236 .",
    "for each @xmath237 let @xmath238 be any ( fixed ) @xmath16 such that @xmath239 .",
    "in other words , @xmath94 is the set of all measures that are predicted by some of the measures in @xmath2 , and for each measure @xmath187 in @xmath94 we designate one `` parent '' measure @xmath238 from @xmath2 such that @xmath238 predicts @xmath187 .",
    "_ for each @xmath240 let @xmath241 be any monotonically increasing function such that @xmath242 and @xmath243 .",
    "define the sets @xmath244 @xmath245 and @xmath246 we will upper - bound @xmath247 .",
    "first , using markov s inequality , we derive @xmath248 next , observe that for every @xmath54 and every set @xmath249 , using jensen s inequality we can obtain @xmath250 moreover , @xmath251 where in the inequality we have used  ( [ eq : v ] ) for the first summand and  ( [ eq : jen ] ) for the second .",
    "thus , @xmath252 from  ( [ eq : t ] ) , ( [ eq : markk ] ) and  ( [ eq : del ] ) we conclude @xmath253    _ step 2n : a countable cover , time @xmath41 .",
    "_ fix an @xmath54 .",
    "define @xmath254 ( since @xmath255 are finite all suprema are reached ) .",
    "find any @xmath256 such that @xmath257 and let @xmath258 . for @xmath259 , let @xmath260 .",
    "if @xmath261 , let @xmath262 be any @xmath16 such that @xmath263 , and let @xmath264 ; otherwise let @xmath265 . observe that ( for each @xmath41 ) there is only a finite number of positive @xmath266 , since the set @xmath255 is finite ; let @xmath267 be the largest index @xmath18 such that @xmath268 .",
    "let @xmath269 as a result of this construction , for every @xmath54 every @xmath270 and every @xmath271 using the definitions  ( [ eq : t ] ) , ( [ eq : u ] ) and  ( [ eq : v ] ) we obtain @xmath272    _ step 2 : the resulting predictor .",
    "_ finally , define @xmath273 where @xmath158 is the i.i.d .",
    "measure with equal probabilities of all @xmath274 ( that is , @xmath275 for every @xmath54 and every @xmath208 ) .",
    "we will show that @xmath56 predicts every @xmath276 , and then in the end of the proof ( step  r ) we will show how to replace @xmath158 by a combination of a countable set of elements of @xmath2 ( in fact , @xmath158 is just a regularizer which ensures that @xmath56-probability of any word is never too close to  0 ) .",
    "_ step 3 : @xmath56 predicts every @xmath276 . _",
    "fix any @xmath276 .",
    "introduce the parameters @xmath277 , @xmath54 , to be defined later , and let @xmath278 .",
    "observe that @xmath279 , for any @xmath259 and any @xmath54 , by definition of these sets .",
    "since the sets @xmath280 , @xmath88 are disjoint , we obtain @xmath281 .",
    "hence , @xmath282 for some @xmath283 , since otherwise @xmath284 so that @xmath285 , which is a contradiction . thus , @xmath286 we can upper - bound @xmath287 as follows .",
    "first , observe that @xmath288 then , from  ( [ eq : t ] ) and  ( [ eq : u ] ) we get @xmath289 from  ( [ eq : jen ] ) and  ( [ eq : tm ] ) we get @xmath290 furthermore , @xmath291 where the first inequality is obvious , in the second inequality we have used the fact that entropy is maximized when all events are equiprobable and in the third one we used @xmath292 . combining  ( [ eq : mut ] ) with the bounds  ( [ eq : e1 ] ) , ( [ eq : e2 ] ) and  ( [ eq : e3 ] ) we obtain @xmath293 so that @xmath294 from the fact that @xmath295 and  ( [ eq : mark ] ) it follows that the term in brackets is @xmath296 , so that we can define the parameters @xmath297 in such a way that @xmath298 while at the same time the bound  ( [ eq : mu2 ] ) gives @xmath299 .",
    "fix such a choice of @xmath297 . then , using  ( [ eq : mark ] ) , we conclude @xmath300    we proceed with the proof of @xmath301 . for any @xmath302 we have @xmath303 where the first inequality follows from  ( [ eq : nu ] ) , the second from  ( [ eq : ext ] ) , and in the equality we have used @xmath304 and @xmath305 .",
    "next we use the decomposition @xmath306 from  ( [ eq : i ] ) we find @xmath307 where in the second inequality we have used @xmath308 , @xmath295 and @xmath242 , in the last inequality we have again used the fact that the entropy is maximized when all events are equiprobable , while the last equality follows from  ( [ eq : xt ] ) . moreover , from  ( [ eq : nu ] ) we find @xmath309 where in the last inequality we have used @xmath275 and @xmath310 , and the last equality follows from  ( [ eq : xt ] ) .      _ step r : the regularizer @xmath158_. it remains to show that the i.i.d .",
    "regularizer @xmath158 in the definition of @xmath56  ( [ eq : nu ] ) , can be replaced by a convex combination of a countably many elements from @xmath2 .",
    "indeed , for each @xmath54 , denote @xmath312 and let for each @xmath147 the measure @xmath313 be any measure from @xmath2 such that @xmath314 .",
    "define @xmath315 for each @xmath316 , @xmath54 , and let @xmath317 . for every @xmath16 we have @xmath318 for every @xmath54 and every @xmath319 , which clearly suffices to establish the bound @xmath320 as in  ( [ eq:2 ] ) .",
    "this work has been partially supported by the french ministry of higher education and research , nord - pas de calais regional council and feder through the `` contrat de projets etat region ( cper ) 2007 - 2013 '' , by the french national research agency ( anr ) , project explo - ra anr-08-cosi-004 , and by pascal2 ."
  ],
  "abstract_text": [
    "<S> a sequence @xmath0 of discrete - valued observations is generated according to some unknown probabilistic law ( measure ) @xmath1 . </S>",
    "<S> after observing each outcome , it is required to give the conditional probabilities of the next observation . the realizable case is when the measure @xmath1 belongs to an arbitrary but known class @xmath2 of process measures . </S>",
    "<S> the non - realizable case is when @xmath1 is completely arbitrary , but the prediction performance is measured with respect to a given set @xmath2 of process measures . </S>",
    "<S> we are interested in the relations between these problems and between their solutions , as well as in characterizing the cases when a solution exists , and finding these solutions . </S>",
    "<S> we show that if the quality of prediction is measured by total variation distance , then these problems coincide , while if it is measured by expected average kl divergence , then they are different . for some of the formalizations </S>",
    "<S> we also show that when a solution exists , it can be obtained as a bayes mixture over a countable subset of @xmath2 . as an illustration to the general results obtained , </S>",
    "<S> we show that a solution to the non - realizable case of the sequence prediction problem exists for the set of all finite - memory processes , but does not exist for the set of all stationary processes .    </S>",
    "<S> the framework is completely general : the processes measures considered are not required to be i.i.d . , mixing , stationary , or to belong to any parametric family . </S>"
  ]
}