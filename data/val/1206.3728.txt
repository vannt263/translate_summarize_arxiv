{
  "article_text": [
    "work examines the dynamics that results when adaptive nodes are allowed to interact with each other . through cooperation",
    ", some interesting behavior occurs that is not observed when the nodes operate independently .",
    "for example , if one adaptive agent has worse performance than another independent adaptive agent , can both agents cooperate with each other in such a manner that the performance of _ both _ agents improves ?",
    "what if @xmath0 agents are interacting with each other ? can all agents improve their performance relative to the non - cooperative case even when some of them are noisier than others ? does cooperation need to be performed in a centralized manner or is distributed cooperation sufficient to achieve this goal ? starting with two adaptive nodes , we derive analytical expressions for the mean - square performance of the nodes under some conditions on the measurement data .",
    "the expressions are then used to compare the performance of various ( centralized and distributed ) adaptive strategies .",
    "the analysis reveals a useful fact that arises as a result of the cooperation between the nodes ; it establishes that , by optimizing over the combination weights , diffusion least - mean - squares ( lms ) strategies for distributed estimation can deliver lower excess - mean - square - error ( emse ) than a centralized solution employing traditional block or incremental lms strategies .",
    "we first study in some detail the situation involving combinations of two adaptive nodes for which the performance levels can be characterized analytically .",
    "subsequently , we extend the conclusion to @xmath0-node ad - hoc networks .",
    "reference @xcite provides an overview of diffusion strategies for adaptation and learning over networks .",
    "it is worth noting that the performance of diffusion algorithms was already studied in some detail in the earlier works @xcite .",
    "these works derived expressions for the network emse and mean - square - deviation ( msd ) in terms of the combination weights that are used during the adaptation process .",
    "the results in @xcite were mainly concerned in comparing the performance of diffusion ( i.e. , distributed cooperative ) strategies with _ non - cooperative _ strategies . in the cooperative case , nodes share information with each other , whereas they behave independently of each other in the non - cooperative case . in the current work ,",
    "we are instead interested in comparing diffusion or _ distributed _ cooperative strategies against _ centralized _ ( as opposed to non - cooperative ) strategies . in the centralized framework , a fusion center has access to all data collected from across the network , whereas in the non - cooperative setting nodes have access only to their individual data .",
    "therefore , finding conditions under which diffusion strategies can perform well in comparison to centralized solutions is generally a demanding task .",
    "we start our study by considering initially the case of two interacting adaptive agents .",
    "though structurally simple , two - node networks are important in their own right . for instance , two - antenna receivers are prevalent in many communication systems .",
    "the data received by the antennas could either be transferred to a central processor for handling or processed cooperatively and locally at the antennas .",
    "which mode of processing can lead to better performance and how ?",
    "some of the results in this article help provide answers to these questions .",
    "in addition , two - node adaptive agents can serve as good models for how estimates can be combined at master nodes that connect larger sub - networks together .",
    "there has also been useful work in the literature on examining the performance of combinations of two adaptive filters @xcite .",
    "the main difference between two - node adaptive networks and combinations of two adaptive filters is that in the network case the measurement and regression data are fully distributed and also different across nodes , whereas the filters share the same measurement and regression data in filter combinations @xcite .",
    "for this reason , the study of adaptive networks is more challenging and their dynamics is richer .    the results in this work will reveal that distributed diffusion lms strategies can outperform centralized block or incremental lms strategies through proper selection of the combination weights .",
    "the expressions for the combination weights end up depending on knowledge of the noise variances , which are generally unavailable to the nodes .",
    "nevertheless , the expressions suggest a useful adaptive construction . motivated by the analysis , we propose an adaptive method for adjusting the combination weights by relying solely on the available data .",
    "simulation results illustrate the findings .",
    "_ notation _ : we use lowercase letters to denote vectors , uppercase letters for matrices , plain letters for deterministic variables , and boldface letters for random variables .",
    "we also use @xmath1 to denote transposition , @xmath2 for conjugate transposition , @xmath3 for matrix inversion , @xmath4 for the trace of a matrix , @xmath5 for the spectral radius of a matrix , @xmath6 for the kronecker product , @xmath7 for stacking the columns of @xmath8 on top of each other , and @xmath9 for constructing a vector by using the diagonal entries of @xmath8 .",
    "all vectors in our treatment are column vectors , with the exception of the regression vectors , @xmath10 , which are taken to be row vectors for convenience of presentation .",
    "we refer to the two nodes as nodes 1 and 2 .",
    "both nodes are assumed to measure data that satisfy a linear regression model of the form : @xmath11 for @xmath12 , where @xmath13 is a deterministic but unknown @xmath14 vector , @xmath15 is a random measurement datum at time @xmath16 , @xmath10 is a random @xmath17 regression vector at time @xmath16 , and @xmath18 is a random noise signal also at time @xmath16 .",
    "we adopt the following assumptions on the statistical properties of the data @xmath19 .",
    "[ asm : all ] 0.5em    1 .",
    "the regression data @xmath10 are temporally white and spatially independent random variables with zero mean and uniform covariance matrix @xmath20 .",
    "the noise signals @xmath18 are temporally white and spatially independent random variables with zero mean and variances @xmath21 .",
    "the regressors @xmath10 and noise signals @xmath22 are mutually - independent for all @xmath23 and @xmath24 , @xmath16 and @xmath25 .",
    "it is worth noting that we do not assume gaussian distributions for either the regressors or the noise signals .",
    "we note that the temporal independence assumption on the regressors may be invalid in general , especially for tapped - delay implementations where the regressions at each node would exhibit a shift structure .",
    "however , there have been extensive studies in the stochastic approximation literature showing that , for stand - alone adaptive filters , results based on the temporal independence assumption , such as and further ahead , still match well with actual filter performance when the step - size is sufficiently small @xcite .",
    "thus , we shall adopt the following assumption throughout this work .",
    "[ asm : smallstepsize ] the step - sizes are sufficiently small , i.e. , @xmath26 , so that terms depending on higher - order powers of the step - sizes can be ignored , and such that the adaptive strategies discussed in this work are mean - square stable ( in the manner defined further ahead ) .",
    "we are interested in the situation where one node is less noisy than the other .",
    "thus , without loss of generality , we assume that the noise variance of node 2 is less than that of node 1 , i.e. , @xmath27 the nodes are interested in estimating the unknown parameter @xmath13 .",
    "assume initially that each node @xmath23 independently adopts the famed lms algorithm @xcite to update its weight estimate ( as illustrated in fig .",
    "[ fig : indlms ] ) according to the following rule : @xmath28\\end{aligned}\\ ] ] for @xmath12 , where @xmath29 is a positive constant step - size parameter .",
    "the steady - state performance of an adaptive algorithm is usually assessed in terms of its mean - square error ( mse ) , emse , and msd , which are defined as follows . if we introduce the error quantities : @xmath30 then the mse , emse , and msd for node @xmath23 are defined as the following steady - state values : @xmath31 where the notation @xmath32 denotes the euclidean norm of its vector argument . substituting expression into the definition for @xmath33 in ,",
    "it is easy to verify that the errors @xmath34 are related as follows : @xmath35 for @xmath12 .",
    "since the terms @xmath36 and @xmath37 are independent of each other , it readily follows that the mse and emse performance measures at each node are related to each other through the noise variance : @xmath38 therefore , it is sufficient to examine the emse and msd as performance metrics for adaptive algorithms . under assumption [ asm : smallstepsize ] ,",
    "the emse and msd of each lms filter in are known to be well approximated by @xcite : @xmath39 and @xmath40 for @xmath12 . to proceed , we further assume that both nodes employ the same step - size and observe data arising from the same underlying distribution .",
    "[ asm : uniform ] it is assumed that both nodes employ identical step - sizes , i.e. , @xmath41 , and that they observe data arising from the same statistical distribution , i.e. , @xmath42 .    under assumption",
    "[ asm : uniform ] , expression confirms the expected result that node 2 will achieve a lower emse than node 1 because node 2 has lower noise variance than node 1 .",
    "the interesting question that we would like to consider is whether it is possible to improve the emse performance for _ both _ nodes if they are allowed to cooperate with each other in some manner .",
    "the arguments in this work will answer this question in the affirmative and will present _ distributed _ cooperative schemes that are able to achieve this goal , not only for two - node networks but also for @xmath0-node ad - hoc networks ( see sec .",
    "one form of cooperation can be realized by connecting the two nodes to a fusion center , which would collect the data from the nodes and and use them to estimate @xmath13 .",
    "fusion centers are generally more powerful than the individual nodes and can , in principle , implement more sophisticated estimation procedures than the individual nodes . in order to allow for a fair comparison between implementations of similar nature at the fusion center and remotely at the nodes ,",
    "we assume that the fusion center is limited to implementing lms - type solutions as well , albeit in a centralized manner . in this work",
    ", the fusion center is assumed to operate on the data in one of two ways .",
    "the first method is illustrated in fig .",
    "[ fig : veclms ] and we refer to it as _ block lms_. in this method , the fusion center receives data from the nodes and updates its estimate for @xmath13 according to the following : @xmath43 with a constant positive step - size @xmath44 .",
    "the second method is illustrated in fig .",
    "[ fig : inclms ] and we refer to it as _ incremental lms_. in this method , the fusion center still receives data from the nodes but operates on them sequentially by incorporating one set of measurements at a time as follows : @xmath45\\\\ { \\bs{w}}_{i}&={\\bs{\\phi}}_{i}+\\mu'{\\bs{u}}_{2,i}^*\\left[{\\bs{d}}_{2}(i)-{\\bs{u}}_{2,i}{\\bs{\\phi}}_{i}\\right]\\\\ \\end{aligned}\\right.\\ ] ] we see from that the fusion center in this case first uses the data from node 1 to update @xmath46 to an intermediate value @xmath47 , and then uses the data from node 2 to get @xmath48 .",
    "method is a special case of the incremental lms strategy introduced and studied in @xcite and is motivated by useful incremental approaches to distributed optimization @xcite .",
    "we observe from and that in going from @xmath46 to @xmath49 , the block and incremental lms algorithms employ two sets of data for each such update ; in comparison , the conventional lms algorithm used by the stand - alone nodes in employs one set of data for each update of their respective weight estimates .",
    "we define the emse and msd for block lms and incremental lms as follows : @xmath50 where the a priori error @xmath51 is now a @xmath52 vector : @xmath53 note that in we are scaling the definition of the emse by @xmath54 because the squared euclidean - norm in involves the sum of the two error components from .",
    "we shall explain later in sec .",
    "vi that in order to ensure a fair comparison of the performance of the various algorithms ( including non - cooperative , distributed , and centralized ) , we will need to set the step - size as ( see ) @xmath55 this normalization will help ensure that the rates of convergence of the various strategies that are being compared are similar .    now , compared to the non - cooperative method where the nodes act individually , it can be shown that the two centralized algorithms and lead to improved mean - square performance ( the arguments further ahead in sec .",
    "iv - f establish this conclusion among several other properties ) .",
    "specifically , the emse obtained by the two centralized algorithms and will be smaller than the average emse obtained by the two non - cooperative nodes in .",
    "the question that we would like to explore is whether distributed cooperation between the nodes can lead to superior performance even in comparison to the centralized algorithms and . to address this question ,",
    "we shall consider distributed lms algorithms of the diffusion - type from @xcite , and which are further studied in @xcite .",
    "reference @xcite provides an overview of diffusion strategies .",
    "adaptive diffusion strategies have several useful properties : they are scalable and robust , enhance stability , and enable nodes to adapt and learn through localized interactions .",
    "there are of course other useful algorithms for distributed estimation that rely instead on consensus - type strategies , e.g. , @xcite .",
    "nevertheless , diffusion strategies have been shown to lead to superior mean - square - error performance in comparison to consensus - type strategies ( see , e.g. , @xcite ) .",
    "for this reason , we focus on comparing adaptive diffusion strategies with the centralized block and incremental lms approaches .",
    "the arguments further ahead will show that diffusion algorithms are able to exploit the spatial diversity in the data more fully than the centralized implementations and can lead to better steady - state mean - square performance than the block and incremental algorithms and , when all algorithms converge in the mean - square sense at the same rate .",
    "we shall establish these results initially for the case of two interacting adaptive agents , and then discuss the generalization for @xmath0-node networks in sec .",
    "diffusion lms algorithms are distributed strategies that consist of two steps @xcite : updating the weight estimate using local measurement data ( the adaptation step ) and aggregating the information from the neighbors ( the combination step ) . according to the order of these two steps ,",
    "diffusion algorithms can be categorized into two classes : combine - then - adapt ( cta ) ( as illustrated in fig .",
    "[ fig : difflmscta ] ) : @xmath56 \\end{aligned}\\right.\\ ] ] and adapt - then - combine ( atc ) ( as illustrated in fig .",
    "[ fig : difflmsatc ] ) : @xmath57\\\\ { \\bs{w}}_{k , i}&=a_{1k}{\\bs{\\psi}}_{1,i}+a_{2k}{\\bs{\\psi}}_{2,i } \\end{aligned}\\right.\\ ] ] for @xmath12 , where the @xmath58 are positive step - sizes and the @xmath59 denote convex combination coefficients used by nodes 1 and 2 .",
    "the coefficients are nonnegative and they satisfy @xmath60 for @xmath12 .",
    "we collect these coefficients into a @xmath61 matrix @xmath8 and denote them more compactly by @xmath62 for node 1 and @xmath63 for node 2 : @xmath64 where @xmath65 $ ] .",
    "note that when @xmath66 , both cta algorithm and the atc algorithm reduce to the non - cooperative lms update given by ; we shall exclude this case for diffusion algorithms .",
    "observe that the order of adaptation and combination steps are different for cta and atc implementations .",
    "the atc algorithm is known to outperform the cta algorithm because the former shares updated weight estimates in comparison to the latter , and these estimates are expected to be less noisy ; see the analysis further ahead and also @xcite .",
    "an important factor affecting the mean - square performance of diffusion lms algorithms is the choice of the combination coefficients @xmath67 and @xmath68 .",
    "different combination rules have been proposed in the literature , such as uniform , laplacian , maximum degree , metropolis , relative degree , relative degree - variance , and relative variance ( which were listed in table iii of reference @xcite ; see also @xcite ) .",
    "apart from these static combination rules , where the coefficients are kept constant over time , adaptive rules are also possible . in the adaptive case , the combination weights can be adjusted regularly so that the network can respond to real - time node conditions @xcite .    now that we have introduced the various strategies ( non - cooperative lms , block lms , incremental lms , atc and cta diffusion lms ) , we proceed to derive expressions for",
    "the _ optimal _ mean - square performance of the diffusion algorithms and .",
    "the analysis will highlight some useful properties for distributed algorithms in comparison to centralized counterparts .",
    "for example , the results will establish that the diffusion strategies using optimized combination weights perform better than the centralized solutions and . obviously , by assuming knowledge of the network topology",
    ", a fusion center can implement the optimized diffusion strategies centrally and therefore attain the same performance as the distributed solution .",
    "we are not interested in such situations where the fusion center implements solutions that are fundamentally distributed in nature .",
    "we are instead interested in comparing truly distributed solutions of the diffusion type and with traditional centralized solutions of the block and incremental lms types and ; all with similar levels of lms complexity .",
    "we rely on the energy conservation arguments @xcite to conduct the mean - square performance analysis of two - node lms adaptive networks .",
    "we first compute the individual and network emse and msd for the cta and atc algorithms and , and then deal with the block and incremental algorithms and . the analysis in the sequel",
    "is carried out under assumptions [ asm : all][asm : uniform ] and condition .",
    "assumption [ asm : smallstepsize ] helps ensure the mean - square convergence of the various adaptive strategies that we are considering here  see , e.g. , @xcite . by mean - square convergence of the distributed and centralized algorithms , we mean that @xmath69 , @xmath70 , and @xmath71 and @xmath72 tend to constant bounded values as @xmath73 .",
    "in addition , assumption [ asm : uniform ] and condition will help enforce similar convergence rates for all strategies .      under assumptions [ asm : all][asm : uniform ] and as mentioned before , it is known that the emse and msd of the two stand - alone lms filters in , which operate independently of each other , are given by @xmath74 and @xmath75 for @xmath12 .",
    "using , the average emse and msd of both nodes are @xmath76 and @xmath77      rather than study cta and atc algorithms separately , we follow the analysis in @xcite and consider a more general algorithm structure that includes cta and atc as special cases .",
    "we derive expressions for the node emse and msd for the general structure and then specialize the results for cta and atc . thus , consider a diffusion strategy of the following general form : @xmath78\\\\ \\label{eqn : idealdiffusionpostdiff } { \\bs{w}}_{k , i}&=q_{1k}{\\bs{\\psi}}_{1,i}+q_{2k}{\\bs{\\psi}}_{2,i}\\end{aligned}\\ ] ] where @xmath79 are the nonnegative entries of @xmath61 matrices @xmath80 .",
    "the cta algorithm corresponds to the special choice @xmath81 and @xmath82 while the atc algorithm corresponds to the special choice @xmath83 and @xmath84 , where @xmath85 denotes the @xmath61 identity matrix . from",
    ", it can be verified that the eigenvalues of @xmath8 are @xmath86 . in the cooperative case ,",
    "we rule out the choice @xmath66 so that the two eigenvalues of @xmath8 are distinct and , hence , @xmath8 is diagonalizable .",
    "let @xmath87 denote the eigen - decomposition of @xmath8 : @xmath88 and let @xmath89 denote the @xmath90th eigenvalue of @xmath91 whose size is @xmath92 .",
    "we derive in appendix [ app : derivationemse ] the following expression for the emse at node @xmath23 : @xmath93 for @xmath12 , where @xmath94 and @xmath95 are @xmath61 matrices that are given by @xmath96 likewise , we can derive the msd at node @xmath23 : @xmath97 for @xmath12 . comparing and we note that @xmath98 in is replaced by @xmath89 in ; all the other factors are identical .",
    "setting @xmath82 , we specialize to obtain the emse expression for the cta algorithm : @xmath99 for @xmath12 . substituting into",
    ", some algebra will show that the network emse for the cta algorithm , which is defined as the average emse of the individual nodes , is given by @xmath100}{1-\\xi_m(\\alpha+\\beta-1)}\\right.\\nonumber\\\\ { } & \\qquad\\qquad\\qquad\\qquad\\qquad\\left.+\\frac{(\\sigma_{v,1}^2+\\sigma_{v,2}^2)[(1-\\alpha)^2+(1-\\beta)^2]}{2[1-\\xi_m(\\alpha+\\beta-1)^2]}\\right]\\end{aligned}\\ ] ] we argue in appendix [ app : minimizeemsecta ] that , under assumption [ asm : smallstepsize ] ( i.e. , for sufficiently small step - sizes ) , the network emse in is essentially minimized when @xmath101 are chosen as @xmath102 this choice coincides with the relative degree - variance rule proposed in @xcite . in the sequel",
    "we will compare the performance of the diffusion strategies that result from this choice of combination weights against the performance of the block and incremental lms strategies and .    the value of that corresponds to the choice is then given by @xmath103 and the corresponding emse values at the nodes are @xmath104 for @xmath12 .",
    "similarly , the network msd is approximately minimized for the same choice and its value is given by @xmath105 the corresponding msd values at the nodes are @xmath106 for @xmath12 .",
    "we shall refer to the cta diffusion algorithm that uses as the optimal cta implementation .",
    "note that selecting the coefficients as in requires knowledge of the noise variances at both nodes .",
    "this information is usually unavailable . nevertheless , it is possible to develop adaptive strategies to adjust the coefficients @xmath101 on the fly based on the available data without requiring the nodes to know beforehand the noise profile in the network ( see @xcite and and further ahead ) .",
    "we therefore continue the analysis by assuming the nodes are able to determine ( or learn ) the coefficients .",
    "likewise , setting @xmath84 , we specialize to obtain the emse expression for the atc algorithm : @xmath107 for @xmath12 . following similar arguments to the cta case ,",
    "the network emse is given by @xmath108 } { 1 - \\xi_m(\\alpha + \\beta - 1)}\\nonumber\\\\ { } & \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad\\left.+\\frac{(\\sigma_{v,1}^2 + \\sigma_{v,2}^2 ) ( \\alpha + \\beta - 1)^2[(1 - \\alpha)^2 + ( 1 - \\beta)^2]}{2\\,[1 - \\xi_m(\\alpha + \\beta-1)^2]}\\right]\\end{aligned}\\ ] ] we can again verify that , under assumption [ asm : smallstepsize ] , expression is approximately minimized for the same choice ( see appendix [ app : minimizeemsecta ] ) .",
    "the resulting network emse value is given by @xmath109 and the corresponding emse values at the nodes are @xmath110 for @xmath12 . similarly , the network msd is approximately minimized for the same choice ; its value is given by @xmath111 and the corresponding msd values at the nodes are @xmath112 for @xmath12 .",
    "we shall refer to the atc diffusion algorithm that uses as the optimal atc implementation .",
    "uniform cta and atc diffusion lms correspond to the choice @xmath113 , which means that the two nodes equally trust each other s estimates .",
    "this situation coincides with the uniform combination rule @xcite . according to and ,",
    "the network emse and msd for uniform cta are @xmath114 and @xmath115 similarly , according to and , the network emse and msd for uniform atc are @xmath116 and @xmath117      in appendix [ app : derivationemsevec ] , we derive the emse and msd for the block lms implementation and arrive at @xmath118 and @xmath119 with regards to the incremental lms algorithm , we note from assumption [ asm : smallstepsize ] that the step - size @xmath44 is sufficiently small so that we can assume @xmath120 .",
    "then , from we get @xmath121 + \\underbrace{\\mu'^2\\|{\\bs{u}}_{2,i}\\|^2{\\bs{u}}_{1,i}^*({\\bs{d}}_{1}(i)-{\\bs{u}}_{1,i}{\\bs{w}}_{i-1})}_{o(\\mu'^2)}\\nonumber\\\\ { } & \\approx{\\bs{w}}_{i-1}+\\mu'\\begin{bmatrix } { \\bs{u}}_{1,i } \\\\ { \\bs{u}}_{2,i } \\\\",
    "\\end{bmatrix}^*\\left(\\begin{bmatrix } { \\bs{d}}_{1}(i ) \\\\ { \\bs{d}}_{2}(i ) \\\\",
    "\\end{bmatrix}-\\begin{bmatrix } { \\bs{u}}_{1,i } \\\\ { \\bs{u}}_{2,i } \\\\",
    "\\end{bmatrix}{\\bs{w}}_{i-1}\\right)\\end{aligned}\\ ] ] which means that the incremental lms update can be well approximated by the block lms update .",
    "then , the emse of incremental lms can be well approximated by ( reference @xcite provides a more detailed analysis of the performance of adaptive incremental lms strategies ) : @xmath122 and its msd as @xmath123 it is worth noting that although and are similar for small step - sizes , incremental lms actually outperforms block lms @xcite because the former uses the intermediate estimate @xmath47 during one step of the update in while the latter does not .",
    "the intermediate estimate @xmath47 is generally `` less noisy '' than @xmath46 so that incremental lms generally outperforms block lms .",
    "however , we shall not distinguish between incremental lms and block lms in this work , when we compare their performance with other strategies in the sequel .",
    "[ cols=\"^,^,>,^ , > \" , ]      we compare the network emse for various strategies in fig .",
    "[ fig : network2nodes ] .",
    "the length of @xmath13 is @xmath124 and its entries are randomly selected .",
    "the regression data @xmath125 and noise signals @xmath126 are i.i.d .",
    "white gaussian distributed with zero mean and @xmath127 , @xmath128 , and @xmath129 .",
    "the results are averaged over 500 trials . from the simulation results",
    ", we can see that although centralized algorithms like and can offer a better estimate than the non - cooperative lms algorithms , they can be outperformed by the diffusion strategies and .",
    "when the combination coefficients of atc or cta algorithms are chosen according to the relative degree - variance rule , these diffusion strategies can achieve lower emse by a significant margin .",
    "in addition , we compare the emse of each node in the network for various strategies in figs .",
    "[ fig : node1][fig : node2 ] .",
    "in the previous sections , we focused on two - node networks and were able to analytically characterize their performance , and to establish the superiority of the diffusion strategies over the centralized block or incremental lms implementations .",
    "we now extend the results to @xmath0-node ad - hoc networks .",
    "first , we establish that for sufficiently small step - sizes and for any _ doubly - stochastic _ combination matrix @xmath8 , i.e. , its rows and columns add up to one , the atc diffusion strategy matches the performance of centralized block lms .",
    "second , we argue that by optimizing over the larger class of left - stochastic combination matrices , which include doubly - stochastic matrices as well , the performance of atc can be improved relative to block lms .",
    "third , we provide a fully - distributed construction for the combination weights in order to minimize the network emse for atc .",
    "we illustrate the results by focusing on atc strategies but they apply to cta strategies as well .    thus , consider a connected network consisting of @xmath0-nodes .",
    "each node @xmath23 collects measurement data that satisfy the linear regression model .",
    "the noise variance at each node @xmath23 is @xmath21 .",
    "we continue to use assumptions [ asm : all][asm : uniform ] .",
    "each node @xmath23 runs the following atc diffusion strategy @xcite : @xmath130\\\\ { \\bs{w}}_{k , i}&=\\sum_{l\\in{\\mc{n}}_k}a_{lk}{\\bs{\\psi}}_{l , i}\\\\ \\end{aligned}\\right.\\ ] ] where @xmath131 denotes the positive weight that node @xmath23 assigns to data arriving from its neighbor @xmath24 ; these weights are collected into an @xmath132 combination matrix @xmath8 , and @xmath133 consists of all neighbors of node @xmath23 including @xmath23 itself .",
    "the weights @xmath59 satisfy the following properties : @xmath134      observe that @xmath8 is a left - stochastic matrix ( the entries on each of its columns add up to one ) .",
    "let @xmath87 denote the eigen - decomposition of @xmath8 , where @xmath135 is a real and invertible matrix and @xmath136 is in the real jordan canonical form @xcite .",
    "we assume that @xmath8 is a primitive / regular matrix , meaning that there exists an integer @xmath90 such that all entries of @xmath137 are strictly positive @xcite .",
    "this condition essentially states that for any two nodes in the network , there is a path of length @xmath90 linking them .",
    "since we assume a connected network and allow for loops because of , it follows that @xmath8 satisfies the regularity condition @xcite .",
    "then , from the perron - frobenius theorem @xcite , the largest eigenvalue in magnitude of @xmath8 is unique and is equal to one .",
    "therefore , @xmath136 has the following form : @xmath138 where the @xmath139 matrix @xmath140 consists of real stable jordan blocks . from appendix [ app : derivationemse ] ,",
    "the network emse and msd for atc diffusion are given by @xmath141 where @xmath142 is given by and @xmath143 from and , we get @xmath144 where , to simplify the notation , we are omitting the subscripts of the identity matrices .",
    "since @xmath140 is stable and @xmath145 under assumption [ asm : smallstepsize ] , we have @xmath146 therefore , by assumption [ asm : smallstepsize ] , we can ignore all blocks on the diagonal of with the exception of the left - most corner entry so that : @xmath147 where @xmath148 now denotes the @xmath132 matrix given by @xmath149 .",
    "then , @xmath150{\\mathrm{vec}}(t^{-1}t^{-\\t})\\nonumber\\\\ { } & \\qquad\\qquad=(2\\lambda_m)^{-1}{\\mathrm{vec}}(r_v)^{\\t}(te_{11}t^{-1}\\otimes te_{11}t^{-1}){\\mathrm{vec}}(i_n)\\end{aligned}\\ ] ] where we used the fact that @xmath151 because of and @xmath152 for matrices @xmath153 of compatible dimensions .",
    "now , note that @xmath154 is a rank - one matrix determined by the outer product of the left- and right - eigenvectors of @xmath8 corresponding to the unique eigenvalue at one .",
    "since @xmath8 is left - stochastic , this left - eigenvector can be selected as the all - one vector @xmath155 , i.e. , @xmath156 . let us denote the right - eigenvector by @xmath157 and normalize its element - sum to one , i.e. , @xmath158 and @xmath159 .",
    "it follows from the perron - frobenius theorem @xcite that all entries of @xmath157 are nonnegative and located within the range @xmath160 $ ] .",
    "we then get @xmath161 .",
    "thus , from , the network emse can be rewritten as @xmath162 that is , @xmath163 similarly , the network msd can be rewritten as @xmath164      for @xmath0-nodes , the block lms recursion is replaced by @xmath165\\end{aligned}\\ ] ] and the incremental lms recursion is replaced by @xmath166\\\\ \\quad\\mbox{set } \\;\\ ; { \\bs{w}}_{i}={\\bs{\\psi}}_n \\\\ \\mbox{end } \\end{cases}\\end{aligned}\\ ] ] in order for block and incremental lms to converge at the same rate as diffusion atc , we must set their step - sizes to @xmath167 ( compare with ) .",
    "following an argument similar to the one presented in appendix [ app : derivationemsevec ] , we can derive the emse and msd for the block lms strategy as @xmath168 and @xmath169 respectively . a similar argument to ( see also expression ( 84 ) in @xcite ) leads to the conclusion that the performance of incremental lms can be well approximated by that of block lms for small step - sizes .",
    "therefore , @xmath170 and @xmath171 for this reason , we shall not distinguish between block lms and incremental lms in the sequel .",
    "observe that the emse expression for atc diffusion lms and for block and incremental lms only differ by a scaling factor , namely , @xmath172 versus @xmath173 .",
    "then , atc diffusion would outperform block lms and incremental lms when @xmath174 where @xmath175 is diagonal and given by .",
    "we assume that the noise variance of at least one node in the network is different from the other noise variances to exclude the case in which the noise profile is uniform across the network ( in which case @xmath175 would be a scaled multiple of the identity matrix ) .",
    "thus , note that , if we select the combination matrix @xmath8 to be _ doubly - stochastic _",
    ", i.e. , @xmath176 and @xmath156 , then it is straightforward to see that @xmath177 so that @xmath178 this result means that , for sufficiently small step - sizes and for any doubly - stochastic matrix @xmath8 , the emse performance of atc diffusion and block lms match each other .",
    "however , as indicated by , the diffusion lms strategy can employ a broader class of combination matrices , namely , left - stochastic matrices .",
    "if we optimize over the larger set of left - stochastic combination matrices and in view of , we would expect @xmath179 where @xmath180 is the optimal combination matrix that solves the following optimization problem : @xmath181 where @xmath182 denotes the set consisting of all @xmath132 left - stochastic matrices whose entries @xmath59 satisfy the conditions in .",
    "we show next how to determine left - stochastic matrices that solve .",
    "first note that the optimization problem is equivalent to the following non - convex problem : @xmath183 where @xmath184 denotes the @xmath185 nonnegative vector space .",
    "we solve this problem in two steps .",
    "first we solve the _ relaxed _ problem : @xmath186 since @xmath175 is positive definite and diagonal , the closed - form solution for is given by @xmath187 next , if we can determine a primitive left - stochastic matrix @xmath8 whose right eigenvector associated to eigenvalue 1 coincides with @xmath188 , then we would obtain a solution to . indeed ,",
    "note that any primitive left - stochastic matrix @xmath8 can be regarded as the probability transition matrix of an irreducible aperiodic markov chain ( based on the connected topology and condition on the weights ) @xcite . in that case , a vector @xmath188 that satisfies @xmath189 would correspond to the stationary distribution vector for the markov chain .",
    "now given an arbitrary vector @xmath188 , whose entries are positive and add up to one , it is known how to construct a left - stochastic matrix @xmath8 that would satisfy @xmath189 .",
    "a procedure due to hastings @xcite was used in @xcite to construct such matrices . applying the procedure to our vector @xmath188 given by , we arrive at the following combination rule , which we shall refer to as the hastings rule",
    "( we may add that there are many other choices for @xmath8 that would satisfy the same requirement @xmath189 ) : @xmath190 where @xmath191 denotes the cardinality of @xmath133 .",
    "it is worth noting that the hastings rule is a _ fully - distributed _ solution  each node @xmath23 only needs to obtain the degree - variance product @xmath192 from its neighbor @xmath24 to compute the corresponding combination weight @xmath131 . by using the hastings rule ,",
    "the vector @xmath188 in is attained and the emse expression is therefore minimized .",
    "the minimum value of is then given by @xmath193 compared to the emse of block and incremental lms and , we conclude that diffusion strategies using the hastings rule achieve a lower emse level under assumption [ asm : smallstepsize ] .",
    "this is because , from the cauchy - schwarz inequality @xcite , we have @xmath194 when the entries on the diagonal of @xmath175 are not uniform ( as we assumed at the beginning of this subsection ) .    in real applications , where the noise variances are unavailable",
    ", each node can estimate its own noise variance recursively by using the following iteration : @xmath195 _ remark _ : in the two - node case , we determined the combination weights by seeking coefficients that essentially minimize the emse expressions and . the argument in appendix [ app : minimizeemsecta ] expressed the emse as the sum of two factors : a dominant factor that depends on @xmath196 and a less dominant factor that depends on higher powers of @xmath196 . in the @xmath0-node network case , we instead used the small step - size approximation to arrive at expressions and , which correspond only to the dominant terms in the emse and msd expressions and depend on @xmath196 .",
    "we can regard and as first - order approximations for the performance of the network for sufficiently small step - sizes .",
    "we simulate atc diffusion lms versus block lms over a connected network with @xmath197 nodes .",
    "the unknown vector @xmath13 of length @xmath198 is randomly generated .",
    "we adopt @xmath199 , @xmath200 for atc diffusion lms , and @xmath201 for block lms .",
    "the network topology and the profile of noise variances @xmath202 are plotted in fig .",
    "[ fig : profiles ] . for atc algorithms ,",
    "we simulate three different combination rules : the first one is the ( left - stochastic ) adaptive hastings rule using and without the knowledge of noise variances , the second one is the hastings rule with the knowledge of noise variance , and the third one is the ( doubly - stochastic ) metropolis rule @xcite ( which is a simplified version of the hastings rule ) : @xmath203 we adopted @xmath204 and @xmath205 for the adaptive hastings rule  to match the convergence rate of the other algorithms .",
    "we also consider the non - cooperative lms case for comparison purposes .",
    "the emse learning curves are obtained by averaging over 50 experiments and are plotted in fig .",
    "[ fig : emse ] .",
    "it can be seen that atc diffusion lms with metropolis weights exhibits almost the same convergence behavior as block lms in transient phase and attains a steady - state value that is less than 1 db worse than block lms . in comparison ,",
    "atc diffusion lms using adaptive hastings weights ( where the noise variances are estimated through ) has almost the same learning curve as atc using hastings weights with the knowledge of the noise variances ; both of them are able to attain about 7 db gain over block lms at steady - state .",
    "in this work we derived the emse levels for different strategies over lms adaptive networks and compared their performance .",
    "the results establish that diffusion lms strategies can deliver lower emse than centralized solutions employing traditional block or incremental lms strategies .",
    "we first studied the case of networks involving two cooperating nodes , where closed - form expressions for the emse and msd can be derived .",
    "subsequently , we extended the conclusion to generic @xmath0-node networks and established again that , for sufficiently small step - sizes , diffusion strategies can outperform centralized block lms strategies by optimizing over left - stochastic combination matrices .",
    "it is worth noting that although the optimized combination rules rely on knowledge of the noise statistics , it is possible to employ adaptive strategies like to adjust these coefficients on the fly without requiring explicit knowledge of the noise profile  in this way , the hastings rule can be implemented in a manner similar to the adaptive relative variance rule @xcite . clearly , the traditional block and incremental implementations and can be modified to incorporate information about the noise profile as well . in that case , it can be argued that diffusion strategies are still able to match the emse performance of these modified centralized algorithms .",
    "under assumptions [ asm : all][asm : uniform ] , the emse expression for node @xmath23 of the general diffusion strategy  is given by eq .",
    "( 39 ) from reference @xcite ( see also @xcite ) : @xmath206^\\t(i_{n^2m^2 } - { \\mc{f}})^{-1}\\vecm(e_{kk}\\otimes r_{u})\\end{aligned}\\ ] ] where @xmath207 and @xmath208 is an @xmath132 all - zero matrix except for the @xmath23th entry on the diagonal , which is equal to one . since for atc algorithms , @xmath209 and @xmath84 , and for cta algorithms , @xmath81 and @xmath210 , we know that @xmath211 for both cases .",
    "therefore , we get @xmath212 we can reduce into the form , which is more suitable for our purposes , by introducing the eigen - decompositions of @xmath91 and @xmath8 .",
    "thus , let @xmath213 denote the eigen - decomposition of @xmath91 , where @xmath214 is unitary and @xmath215 is diagonal with positive entries .",
    "let also @xmath87 denote the eigen - decomposition of the real matrix @xmath8 , where @xmath135 is real and invertible and @xmath136 is in the _ real _ jordan canonical form @xcite .",
    "then , the eigen - decomposition of @xmath216 is given by @xmath217\\nonumber\\\\ { } & = ( t^{-\\t}\\otimes{u})[d^{\\t}\\otimes(i_m-\\mu\\lambda)](t^{\\t}\\otimes{u}^*)\\end{aligned}\\ ] ] and the eigen - decomposition of @xmath218 is then given by @xmath219(t^{\\t}\\otimes{u}^*)\\}^{\\t } \\otimes\\{(t^{-\\t}\\otimes{u})[d^{\\t}\\otimes(i_m-\\mu\\lambda)](t^{\\t}\\otimes{u}^*)\\}^*\\nonumber\\\\ { } & = { \\mc{x}}\\{[d^{\\t}\\otimes(i_m-\\mu\\lambda)]^{\\t}\\otimes[d^{\\t}\\otimes(i_m-\\mu\\lambda)]^*\\}{\\mc{x}}^{-1}\\nonumber\\\\ { } & = { \\mc{x}}\\{[d\\otimes(i_m-\\mu\\lambda)]\\otimes[d\\otimes(i_m-\\mu\\lambda)]\\}{\\mc{x}}^{-1}\\nonumber\\\\ { } & = { \\mc{x}}({\\mc{g}}\\otimes{\\mc{g}}){\\mc{x}}^{-1}\\end{aligned}\\ ] ] where we used the facts that @xmath220 are real and @xmath215 is diagonal , and introduced the matrices : @xmath221 then , from  , we get @xmath222\\cdot\\mu^2\\vecm(q^{\\t}r_vq\\otimes r_u^\\t)\\nonumber\\\\ & = \\mu^2\\vecm\\left[(t^{*}\\otimes{u}^\\t)(q^{\\t}r_vq\\otimes r_u^\\t)(t\\otimes{u}^{*\\t})\\right]\\nonumber\\\\ & = \\mu^2\\vecm(t^{\\t}q^{\\t}r_vqt\\otimes\\lambda)\\end{aligned}\\ ] ] where we used the fact that @xmath135 is real .",
    "likewise , we get @xmath223\\cdot\\vecm(e_{kk}\\otimes r_{u})\\nonumber\\\\ & = \\vecm(t^{-1}e_{kk}t^{-\\t}\\otimes\\lambda)\\end{aligned}\\ ] ] then , from  , the emse expression in can be rewritten as @xmath224^\\t{\\mc{x}}(i_{n^2m^2}-{\\mc{g}}\\otimes{\\mc{g}})^{-1 } { \\mc{x}}^{-1}\\vecm(e_{kk}\\otimes r_{u})\\nonumber\\\\ { } & = \\mu^2[\\vecm(t^{\\t}q^{\\t}r_vqt\\otimes\\lambda)]^{\\t}(i_{n^2m^2}-{\\mc{g}}\\otimes{\\mc{g}})^{-1 } \\vecm(t^{-1}e_{kk}t^{-\\t}\\otimes\\lambda)\\end{aligned}\\ ] ] using the fact that @xmath225 in is stable under assumption [ asm : smallstepsize ] , we can further obtain @xmath226^{\\t } \\left(\\sum_{j=0}^{\\infty}{\\mc{g}}^j\\otimes{\\mc{g}}^j\\right ) \\vecm(t^{-1}e_{kk}t^{-\\t}\\otimes\\lambda)\\nonumber\\\\ { } & = \\mu^2[\\vecm(t^{\\t}q^{\\t}r_vqt\\otimes\\lambda)]^{\\t } \\sum_{j=0}^{\\infty}\\vecm\\left[{\\mc{g}}^j(t^{-1}e_{kk}t^{-\\t}\\otimes\\lambda){\\mc{g}}^{\\t j}\\right]\\nonumber\\\\ { } & = \\mu^2\\sum_{j=0}^{\\infty}\\tr\\left[(t^{\\t}q^{\\t}r_vqt\\otimes\\lambda){\\mc{g}}^j ( t^{-1}e_{kk}t^{-\\t}\\otimes\\lambda){\\mc{g}}^{\\t j}\\right]\\end{aligned}\\ ] ] where we used the identities @xmath152 and @xmath227^\\t\\vecm(b)$ ] for matrices @xmath153 of compatible dimensions . from , we get @xmath228\\nonumber\\\\ { } & \\quad=\\tr\\left\\{(t^{\\t}q^{\\t}r_vqt\\otimes\\lambda)[d^j\\otimes(i_m-\\mu\\lambda)^j ] ( t^{-1}e_{kk}t^{-\\t}\\otimes\\lambda)[d^{\\t j}\\otimes(i_m-\\mu\\lambda)^{j}]\\right\\}\\nonumber\\\\ { } & \\quad=\\tr\\left[t^{\\t}q^{\\t}r_vqtd^jt^{-1}e_{kk}t^{-\\t}d^{\\t j } \\otimes\\,\\lambda(i_m-\\mu\\lambda)^j\\lambda(i_m-\\mu\\lambda)^{j}\\right]\\nonumber\\\\ { } & \\quad=\\tr\\left[\\lambda(i_m-\\mu\\lambda)^j\\lambda(i_m-\\mu\\lambda)^{j } \\otimes\\,t^{\\t}q^{\\t}r_vqtd^jt^{-1}e_{kk}t^{-\\t}d^{\\t j}\\right]\\nonumber\\\\ { } & \\quad=\\sum_{m=1}^{m}\\lambda_m^2(1-\\mu\\lambda_m)^{2j } \\tr(t^{\\t}q^{\\t}r_vqtd^jt^{-1}e_{kk}t^{-\\t}d^{\\t j})\\nonumber\\\\ { } & \\quad=\\sum_{m=1}^{m}\\lambda_m^2(1-\\mu\\lambda_m)^{2j}[\\vecm(t^{\\t}q^{\\t}r_vqt)]^\\t ( d^{j}\\otimes d^j)\\vecm(t^{-1}e_{kk}t^{-\\t})\\end{aligned}\\ ] ] where we used the identity @xmath229 for square matrices @xmath230 and the fact that @xmath231 is diagonal . substituting back into leads to @xmath232^\\t \\left[\\sum_{j=0}^{\\infty}(1 - \\mu\\lambda_m)^{2j}d^{j}\\otimes d^j\\right ] \\vecm(t^{-1}e_{kk}t^{-\\t})\\nonumber\\\\ { } & = \\mu^2\\sum_{m=1}^{m}\\lambda_m^2[\\vecm(t^{\\t}q^{\\t}r_vqt)]^{\\t } [ i_{n^2 } - ( 1 - \\mu\\lambda_m)^2d\\otimes d]^{-1}{\\mathrm{vec}}(t^{-1}e_{kk}t^{-\\t})\\nonumber\\\\ { } & \\approx\\mu^2\\sum_{m=1}^{m}\\lambda_m^2[\\vecm(t^{\\t}q^{\\t}r_vqt)]^{\\t } [ i_{n^2 } - ( 1 - 2\\mu\\lambda_m)d\\otimes d]^{-1}{\\mathrm{vec}}(t^{-1}e_{kk}t^{-\\t})\\end{aligned}\\ ] ] where @xmath233 due to assumption [ asm : smallstepsize ] .",
    "to minimize the network emse for cta given by , we introduce two auxiliary variables @xmath234 and @xmath235 such that @xmath236 and @xmath237 , where @xmath238 and @xmath239 .",
    "the network emse can be rewritten as @xmath240\\end{aligned}\\ ] ] where @xmath241 and @xmath145 is given by under assumption [ asm : smallstepsize ] .",
    "minimizing expression in closed - form over both variables @xmath242 is generally non - trivial .",
    "we exploit the fact that the step - size is sufficiently small to help locate the values of @xmath235 and @xmath234 that approximately minimize the value of . for this purpose , we first substitute into and use assumption [ asm : smallstepsize ] to note that @xmath243 expression writes the emse as the sum of two factors : the first factor is linear in the step - size and depends only on @xmath235 , and the second factor depends on higher - order powers of the step - size . for sufficiently small step - sizes , the first factor is dominant and we can ignore the second factor .",
    "doing so allows us to estimate the value of @xmath235 that minimizes .",
    "observe that the first factor on rhs of is minimized at @xmath244 because @xmath245 we now substitute @xmath244 back into the original expression for the network emse to find that : @xmath246\\end{aligned}\\ ] ] we use this form to minimize the higher - order terms of @xmath196 over the variable @xmath234 .",
    "it is obvious that expression is minimized at @xmath247 .",
    "the value of emse under @xmath244 and @xmath247 is then given by @xmath248\\end{aligned}\\ ] ] similarly , we can employ the same approximate argument to find that the solution @xmath249 essentially minimizes the network msd under assumption [ asm : smallstepsize ] ; the corresponding value of the msd is @xmath250\\end{aligned}\\ ] ] the solution @xmath251 translates into , where @xmath252 .    in a similar manner , in order to minimize the network emse of atc given by , we introduce two auxiliary variables @xmath234 and @xmath235 such that @xmath236 and @xmath237 , where @xmath238 and @xmath239 .",
    "then , from we have @xmath253\\end{aligned}\\ ] ] for which we can again motivate the selection @xmath251 . the value of the network emse at @xmath244 and @xmath247 is then given by @xmath254",
    "we start from . to simplify the notation , we rewrite and as @xmath255 where @xmath256 the error recursion is then given by @xmath257",
    "let @xmath258 be an arbitrary @xmath92 positive semi - definite matrix that we are free to choose .",
    "using , we can evaluate the weighted square quantity @xmath259 .",
    "doing so and taking expectations under assumption [ asm : all ] , we arrive at the following weighted variance relation @xcite : @xmath260 where @xmath261 where , in view of assumption [ asm : smallstepsize ] , we are dropping higher - order terms in @xmath196 .",
    "let again @xmath262 denote the eigen - decomposition of @xmath91 .",
    "we then introduce the transformed quantities : @xmath263 relation is accordingly transformed into @xmath264 where @xmath265 since we are free to choose @xmath258 , or equivalently , @xmath266 , let @xmath266 be diagonal and nonnegative . then , it can be verified that @xmath267 is also diagonal and nonnegative under assumptions [ asm : all][asm : uniform ] so that @xmath268 under assumption [ asm : all ] , the second term on the right - hand side of evaluates to @xmath269\\end{aligned}\\ ] ] where @xmath270 therefore , we get @xmath271 when the filter is mean - square stable , taking the limit as @xmath73 of both sides of and selecting @xmath272 , we get @xmath273 likewise , by selecting @xmath274 and taking the limit of both sides of as @xmath73 , we arrive at @xmath275",
    "the authors would like to acknowledge useful feedback from ph.d .",
    "student z. towfic on sec .",
    "vi - c .",
    "j.  arenas - garcia , v.  gomez - verdejo , and a.  r. figueiras - vidal , `` new algorithms for improved adaptive convex combination of lms transversal filters , '' _ ieee trans .",
    "_ , vol .",
    "54 , no .  6 , pp .",
    "22392249 , dec .",
    "j.  arenas - garcia , a.  figueiras - vidal , and a.  h. sayed , `` mean - square performance of a convex combination of two adaptive filters , '' _ ieee trans .",
    "signal process .",
    "_ , vol .",
    "54 , no .  3 , pp . 10781090 ,",
    ". 2006 .",
    "d.  mandic , p.  vayanos , c.  boukis , b.  jelfs , s.  l. goh , t.  gautama , and t.  rutkowski , `` collaborative adaptive learning using hybrid filters , '' in _ proc .",
    "speech , signal process .",
    "( icassp ) _ , honolulu , hi , apr .",
    "2007 , pp . 921924 .",
    "r.  candido , m.  t.  m. silva , and v.  h. nascimento , `` transient and steady - state analysis of the affine combination of two adaptive filters , '' _ ieee trans .",
    "signal process .",
    "_ , vol .",
    "58 , no .  8 , pp . 40644078 , aug .",
    "2010 .",
    "b.  widrow , j.  m. mccool , m.  g. larimore , and c.  r. johnson  jr . ,",
    "`` stationary and nonstationary learning characterisitcs of the lms adaptive filter , '' _ proc .",
    "ieee _ , vol .",
    "64 , no .",
    "11511162 , aug .",
    "1976 .",
    "s.  jones , r.  c. iii , and w.  reed , `` analysis of error - gradient adaptive linear estimators for a class of stationary dependent processes , '' _ ieee trans .",
    "inf . theory _",
    "28 , no .  2 , pp .",
    "318329 , mar . 1982 .",
    "j.  b. foley and f.  m. boland , `` a note on the convergence analysis of lms adaptive filters with gaussian data , '' _ ieee trans .",
    "speech , signal process .",
    "_ , vol .",
    "36 , no .  7 , pp . 10871089 , jul .",
    "1988 .",
    "a.  nedic and d.  bertsekas , `` convergence rate of incremental subgradient algorithms , '' in _",
    "stochastic optimization : algorithms and applications _ , s.  uryasev and p.  m. pardalos , eds.1em plus 0.5em minus 0.4em kluwer academic publishers , 2000 , pp . 263304 .",
    "l.  li and j.  a. chambers , `` distributed adaptive estimation based on the apa algorithm over diffusion netowrks with changing topology , '' in _ proc .",
    "ieee workshop stat . signal process .",
    "( ssp ) _ , cardiff , uk , aug .",
    "2009 , pp .",
    "757760 .",
    "n.  takahashi , i.  yamada , and a.  h. sayed , `` diffusion least - mean squares with adaptive combiners : formulation and performance analysis , '' _ ieee trans .",
    "signal process .",
    "_ , vol .",
    "58 , no .  9 , pp . 47954810 , sep .",
    "n.  takahashi and i.  yamada , `` link probability control for probabilistic diffusion least - mean squares over resource - constrained networks , '' in _ proc .",
    "speech , signal process .",
    "( icassp ) _ , dallas , tx , mar .",
    "2010 , pp . 35183521 .",
    "tu and a.  h. sayed , `` optimal combination rules for adaptation and learning over netowrks , '' in _ proc .",
    "workshop comput .",
    "advances multi - sensor adapt . process .",
    "( camsap ) _ , san juan , puerto rico , dec . 2011 , pp .",
    "317320 .",
    " , `` diffusion strategies outperform consensus strategies for distributed estimation over adaptive netowrks , '' available online at http://http://arxiv.org / abs/1205.3993 as manuscript arxiv:1205.3993v1 [ cs.it ] , may 2012",
    ".    x.  zhao , s .- y .",
    "tu , and a.  h. sayed , `` diffusion adaptation over netowrks under imperfect information exchange and non - stationary data , '' _ to appear in _ ieee trans",
    ". signal process .",
    "_ , vol .  60 , no .  7 , jul",
    "2012 , also available online at http://arxiv.org/abs/1112.6212 as manuscript arxiv:1112.6212v3 [ math.oc ] , dec 2011 .",
    "s.  kar and j.  m.  f. moura , `` convergence rate analysis of distributed gossip ( linear parameter ) estimation : fundamental limits and tradeoffs , '' _ ieee j. sel .",
    "top . signal process .",
    "_ , vol .  5 , no .  4 , pp .",
    "674690 , aug ."
  ],
  "abstract_text": [
    "<S> in this work we analyze the mean - square performance of different strategies for distributed estimation over least - mean - squares ( lms ) adaptive networks . </S>",
    "<S> the results highlight some useful properties for distributed adaptation in comparison to fusion - based centralized solutions . </S>",
    "<S> the analysis establishes that , by optimizing over the combination weights , diffusion strategies can deliver lower excess - mean - square - error than centralized solutions employing traditional block or incremental lms strategies . </S>",
    "<S> we first study in some detail the situation involving combinations of two adaptive agents and then extend the results to generic @xmath0-node ad - hoc networks . in the later case </S>",
    "<S> , we establish that , for sufficiently small step - sizes , diffusion strategies can outperform centralized block or incremental lms strategies by optimizing over left - stochastic combination weighting matrices . </S>",
    "<S> the results suggest more efficient ways for organizing and processing data at fusion centers , and present useful adaptive strategies that are able to enhance performance when implemented in a distributed manner .    </S>",
    "<S> adaptive networks , distributed estimation , centralized estimation , diffusion lms , fusion center , incremental strategy , diffusion strategy , energy conservation . </S>"
  ]
}