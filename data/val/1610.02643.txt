{
  "article_text": [
    "one of the most challenging tasks when looking for a new phenomenon is the assessment of the background model and its uncertainties . in many cases the knowledge of the underlying background model",
    "is limited , and the model is estimated and tested against a control measurement . in other cases ,",
    "a reliable functional description of the background model exists , but the relevant parameters fail to cover the true model .",
    "this can happen , for example , when the model is based on monte - carlo ( mc ) simulation with overly constrained parameters , or with too many parameters , lacking the ability to track the effect of each of their combinations .",
    "when using likelihood based statistical tests , it is assumed that the model is sufficiently flexible such that there exists a set of values of the parameters which can be regarded as `` true '' .",
    "however , if the model does not reflect the truth accurately enough it might lead to an enhanced false discovery rate , or to an artificially enhanced exclusion limit .",
    "there are several realistic cases in which it is challenging to provide a complete description of the background : ( i ) the background model depends on `` too many '' parameters and it becomes computationally challenging to perform the minimization .",
    "( ii ) the relevant parameters are hidden deep inside the mc unreachable by the user ( iii ) the shape is modeled based on a special dataset ( i.e. , calibration runs ) with no underlying physical model that reliably accounts for the systematic uncertainties .",
    "( iv ) the models , parameters or uncertainties used do not cover the true model .",
    "the excess events previously observed in some dark matter experiments @xcite have recently been demonstrated to be entirely attributable to underestimated backgrounds @xcite , adding once more the importance of proper treatment of background uncertainties in these experiments and others .",
    "a widely used procedure to establish discovery or exclusion in particle physics is based on a frequentist significance test using the profile likelihood ( pl ) ratio as the test statistic  @xcite .",
    "the pl method has been proven useful to incorporate model uncertainties into the likelihood test .",
    "in addition to the parameter of interest ( e.g. , the cross section or number of signal events ) , the signal and background models contain nuisance parameters whose values are not taken as known a - priori .",
    "the additional flexibility introduced to parameterize systematic effects through the nuisance parameters results in a loss in sensitivity , according to the uncertainty on the parameters and their influence on the interpretation of the data .    in this paper",
    "we propose a procedure that provides a natural protection against mismodeling of the background , we use the unbinned pl ratio as a test statistic , and investigate its behavior .",
    "this paper is organized as follows : in sec .",
    "[ sec : likelihood ] we briefly discuss the pl method and motivate the use of unbinned likelihood over the binned likelihood . in sec .",
    "[ sec : theproblem ] we illustrate how mismodeling of the background causes loss of asymptoticness that enhances the false discovery rate ( in case of underestimated background ) or produces overly constrained exclusion limits ( in case of overestimated background ) . in sec .",
    "[ sec : thesolution ] we present a universal method to recover both the discovery potential and the exclusion sensitivity . in sec .",
    "[ sec : examples ] we present three realistic use cases , with user - defined parametric background models and empirical non - parametric models using kernel density estimates ( kde ) .",
    "in high energy physics and astroparticle physics , we wish to identify a small signal over a non - negligible background . in order to achieve",
    "that , the expected signal and background signatures in a detector have to show different behavior in some observable _ x_. this is modeled by the probability density functions ( pdfs ) , @xmath0 for signal and @xmath1 for background , where @xmath2 and @xmath3 represent one or more model parameters .",
    "if @xmath4 events were observed , the dataset is fully characterized be the vecotr @xmath5 . in a binned analysis ,",
    "the observed space is divided to bins , so each bin contains @xmath6 observed events , and @xmath7 .",
    "the probability for a signal ( or background ) event to be found in bin @xmath8 depends on the appropriate pdf , the bin location and its width @xmath9    and the expected number of events per bin is @xmath10 = n_s \\epsilon_s^j + n_b \\epsilon_b^j,\\ ] ] where @xmath11 and @xmath12 are the total number of signal and background events expected over all bins .",
    "the _ binned likelihood _",
    "function is the product of the poisson probabilities over all bins @xmath13    the binned scheme is simple , well defined and widely used , see e.g. , @xcite .",
    "however , the projection of events into bins is associated with information loss .",
    "the choice of bins affects the sensitivity and the optimal choice varies for different hypotheses , detector performance and sample size ( see chapter 11 in  @xcite ) .",
    "an alternative is the unbinned extended likelihood function ( e.g. , @xcite ) @xmath14    eq .",
    "[ eq : ublikelihood ] is the infinitesimal version of eq .",
    "[ eq : blikelihood ] , and therefore contains all its information , and avoids the need of optimizing bins in order to enhance the sensitivity , as we show in appendix  [ appendix : abinned ] .",
    "the unbinned scheme is invariant under coordinate transformation , as demonstrated in appendix  [ appendix : atx ] .    throughout this paper",
    "we will work with the unbinned likelihood function described above , with @xmath11 as the _ parameter of interest _ , and @xmath15 corresponding to the null , background only hypothesis .",
    "@xmath12 is treated as a nuisance parameter .",
    "though not explicitly shown here , additional nuisance parameters and their constraint terms can be added to the likelihood function in the usual way .",
    "the pl statistical inference approach is widely used for quantifying the level of agreement between a dataset and a null or signal hypothesis and its asymptotic formulae have been well studied  @xcite .    in this approach , given a likelihood function @xmath16 which is a function of a _",
    "parameter of interest _",
    "( @xmath17 in our case ) and a set of _ nuisance parameters _ , @xmath18",
    ", the following test statistic is constructed @xmath19    @xmath20 are the values of the nuisance parameters that maximize @xmath16 for a given @xmath11 , and @xmath21 and @xmath22 are the maximum likelihood estimators ( mle ) of the maximized ( unconditional ) likelihood function .",
    "if for a specific dataset , @xmath23 was observed when testing a specific @xmath11 , the level of disagreement between the data and the @xmath11 hypothesis can be quantified using the @xmath24-value or the equivalent significance , @xmath25 , @xmath26    where @xmath27 is the distribution of @xmath28 under the assumption of signal strength @xmath11 , and @xmath29 is the quantile of the standard gaussian . according to wilks  @xcite , if @xmath21 has a gaussian distribution around its true value , and certain regularity conditions are met , @xmath27 asymptotically approaches a @xmath30 distribution with one degree of freedom .    for the statistical inference of discovery claim or exclusion limits",
    "we follow the procedure described in  @xcite :    for a discovery claim the null hypothesis @xmath15 is to be rejected only when the data deviates from it with a positive signal @xmath31",
    ". cases in which @xmath32 are not considered as an evidence against the null hypothesis    @xmath33    and @xmath34 can be asymptotically described by half a @xmath30 distribution plus half a delta function at zero .",
    "r0.5        for an exclusion limit we aim at rejecting a signal hypothesis @xmath11 , and do not consider data with @xmath35 as being less compatible with the @xmath11 hypothesis    @xmath36    and @xmath27 can be asymptotically described by half a @xmath30 distribution plus half a delta function at zero .",
    "improper modeling of background can cause devastating consequences such as a false discovery claim or overly constraint limit . to illustrate this , toy datasets were generated using a `` true '' background model @xmath37 , and unbinned analysis was performed as in eq .",
    "[ eq : ublikelihood ] alas using a `` slightly different '' background model @xmath38 .",
    "[ fig : fb_models ] shows @xmath37and its four variations .",
    "two of the variations overestimate the background in the signal region ( which can cause overly constraint exclusion limits ) , and two underestimate it ( which can lead to false discovery claims ) . while this modeling scenario is somewhat artificial , it aims at illustrating the challenges in a coherent way .",
    "more realistic examples will be presented in sec .",
    "[ sec : examples ] .    to test the false discovery rate eq .",
    "[ eq : discoveryteststat ] was used on background only datasets using the true background shape @xmath38=@xmath37 , and each one of the four @xmath38variations .",
    "[ fig : discovery_user](a ) shows for each @xmath38the cumulative distribution function ( cdf ) of @xmath39 ( @xmath40 ) .",
    "the deviation from the assumed ( cumulative ) @xmath41 distribution is evident . in case of underestimated background model the false discovery rate",
    "is enhanced , and the overestimated background model artificially strengthens the exclusion limit as will be discussed in more details along with the proposed solution in sec .",
    "[ sec : thesolution ] .     using the true background model ( solid ) and 4 variations as shown in fig .",
    "[ fig : fb_models ] . * ( a ) * with the regular procedure showing under ( over ) coverage when using background models that underestimate ( overestimate ) the background in the signal region . * ( b ) * with the correction the asymptotic behavior is regained .",
    "the expected @xmath41 asymptotic behavior is shown in gray.,title=\"fig:\",scaledwidth=50.0% ]   using the true background model ( solid ) and 4 variations as shown in fig .",
    "[ fig : fb_models ] .",
    "* ( a ) * with the regular procedure showing under ( over ) coverage when using background models that underestimate ( overestimate ) the background in the signal region . *",
    "( b ) * with the correction the asymptotic behavior is regained .",
    "the expected @xmath41 asymptotic behavior is shown in gray.,title=\"fig:\",scaledwidth=50.0% ]    in the next section we will show how to account for these unmodeled mismatches of the background , in a way that preserves the coordinate invariance and the optimal sensitivity .",
    "in this section we present a poka - yoke method to handle mismodeled or overlooked uncertainties in unbinned analyses , verify its asymptotic behavior , and discuss its implementation for discovery and exclusion .    starting with a benchmark background model @xmath38 , and a background only calibration measurement",
    "we quantify the level of `` signal likeness '' that can be overlooked in the calibration sample by introducing a _ contamination parameter _ , @xmath42 .",
    "the benchmark background model will be added a signal like component quantified by @xmath42 which will serve as a nuisance parameter ,    @xmath43    this variation is motivated in appendix  [ appendix : b ] using two approaches .",
    "this variation is universal and it does not depend on the way the background is modeled .",
    "in addition , it is unique with respect to other functional variations of the background , as it is the most conservative one with respect to the tested hypothesis .    as a result of this extra variation the likelihood function ( i.e. , eq .",
    "[ eq : ublikelihood ] ) is promoted to @xmath44 the signal - free calibration dataset provides additional constraints on @xmath42 , @xmath45 combined to @xmath46    using this equation with eqs .",
    "[ eq : discoveryteststat ] and  [ eq : exclusionteststat ] this method can be used for discovery or exclusion tests .",
    "the method can be made more conservative by limiting @xmath42 to be only positive ( for conservative discovery tests ) or only negative ( for conservative exclusion tests ) . however , as will be soon demonstrated this does not seem to be necessary , and the asymptotic behavior is recovered without any constraints on @xmath42 .",
    "while summarizing this paper , we became aware that a similar approach is used by the atlas collaboration . in  @xcite",
    ", the `` spurious signal '' method was used to systematically compare background models , in a similar way to the method presented here . however , in this paper , we show that it is a valid and necessary addition to the likelihood in order to model shape uncertainties . in addition , we motivate the method ( appendix  [ appendix : b ] ) , and investigate its test statistic asymptotic behavior in a systematic way .",
    "we also demonstrate how it can be used in different types of analyses , in order to restore the assumed asymptotic behavior ( sec .",
    "[ sec : examples ] ) , both for exclusion and discovery of a new signal .      to test the asymptotic behavior we repeat the procedure described in sec .",
    "[ sec : theproblem ] in conjunction with the procedure described here . as can be seen in fig .",
    "[ fig : discovery_user ] ( b ) , using the new procedure the test statistic distribution regains its expected asymptotic behavior , for both over-  and underestimation of the background in the signal region . in both cases , the @xmath41 is an excellent approximation and therefore the significance ( or @xmath24-value ) estimated using the pl will be reliable , and will not lead to false discovery .    to see how this procedure affects the discovery sensitivity we present it using fake data samples with 15 signal events over 100 background events .",
    "this is done using the true background model , and the 4 background model variations described in fig .",
    "[ fig : fb_models ] .    in fig .",
    "[ fig : discovery_significance ] on the upper panel ( in black ) the sensitivity using the true background is shown .",
    "we would like to have a distribution as similar as possible to this .",
    "if a distribution is shifted to the right , it means that we get an artificial enhancement of the sensitivity and we are exposed to false discoveries .",
    "alternatively , a left shifted distribution with respect the true one signifies a sensitivity loss .",
    "if the background is _ overestimated _ in the signal region the discovery potential is artificially reduced ( lower two panels , solid lines ) . however , using the new method ( dashed lines ) it is raised and becomes closer to the sensitivity with the true model in hand . if one wishes to be more conservative , one can force @xmath42 to be positive , and the original distribution will remain almost untouched . in case of _ underestimation",
    "_ , the sensitivity is artificially enhanced .",
    "looking at the second and third panels in fig .",
    "[ fig : discovery_significance ] , it can be seen that using the method of this paper ( dashed line ) the sensitivity is reduced to the level of the true model .",
    "this is due to the fact that the test statistic distribution is restored and can be described by the @xmath41 distribution ( fig .  [ fig : discovery_user ] ) . as expected , when applying this correction a loss of sensitivity is apparent and in the examples shown here is less than @xmath47 .",
    "it can be seen that the proposed construction compensates for the unaccounted background shape without making any assumptions on its magnitude or functional shape .",
    "this is done naturally , without an inclusion of additional nuisance parameters .",
    "it is important to note that the distribution of the test statistic using the wrong model can not be estimated using mc as the true model out of which the data is really generated is unknown .      to illustrate the usage for an exclusion test",
    "we compute the 90% confidence level ( cl ) upper limit distribution , using the true and false background models . in this case",
    "we present the results for two background variations : the most overestimated and the most underestimated ones . for each model",
    "we compute the sensitivity with and without the correction , and compare it with the sensitivity using the true model .",
    "we use the same setup as before with 100 events in the physics dataset , and 1000 of background calibration events . as for discovery",
    ", the improper modeling will cause a breakdown of the @xmath41 distribution of the test statistic , leading to enhanced or reduced exclusion limits .",
    "[ fig : exclusion_user ] presents the tension between the limits using the wrong and the correct models .",
    "one can see that using the procedure defined in this paper cures the discrepancy . if the model is overestimated ( fig .",
    "[ fig : exclusion_user ] a ) , the sensitivity is artificially enhanced , and regions in the parameter space in which the experiment has no sensitivity might be excluded .",
    "the correction restores the true sensitivity of the experiment .",
    "alternatively , if the background model is underestimated ( fig .   [ fig : exclusion_user ] b ) , the limits will be higher than the true one , and can be restored using the correction .",
    "as before , to be more conservative , @xmath42 can be constrained to be negative throughout the procedure , and as a result the limits will remain practically unfixed in case of underestimation .",
    "for overestimation though , the limits will restore the true sensitivity .",
    "prevents too good limits ( left ) in case of overestimated background , and prevents sensitivity loss in case of underestimated background ( right).,title=\"fig:\",scaledwidth=50.0% ]   prevents too good limits ( left ) in case of overestimated background , and prevents sensitivity loss in case of underestimated background ( right).,title=\"fig:\",scaledwidth=50.0% ]",
    "in this section we present advanced examples in which the method defined in the previous section can be used .",
    "many times , the functional shape of the background is defined by either a mc or by some analytic expression , and the parameters of the background are extracted using a signal - free calibration sample . in the first example , we show how the method accounts for the uncertainties on the functional shape , and fix the asymptotic behavior of the test statistic .    in the second example , the background is estimated in a nonparametric way , this is done in order to dodge wrong assumptions on the model . however , in case the calibration size is not large enough the asymptotic behavior will be broken , and can be restored using the method suggested in this paper .    the third example is a 2-dimensional case study . we show that the method works in higher dimensions as well , and we present a breakdown due to nonparametric estimation with small a calibration sample .    as a general rule , all the applications depend on several aspects .",
    "a major aspect is the quality of the shape discrimination which can be quantified by @xmath48 ( see eq .",
    "[ eq : alphad ] in appendix [ appendix : atx ] ) .",
    "we give this measure for each of the examples as reference .",
    "in many experiments the background model @xmath38is constructed by fitting a function ( analytic  @xcite or mc based  @xcite ) to a control measurement ( calibration sample ) with one or more free parameters . this procedure assumes that the true background , @xmath37 , can be described by the chosen function and its parameters range . in case the functional space spanned by @xmath38does not cover @xmath37 , the results of the inference will be biased . in the following example we demonstrate how the method presented in this paper can resolve this issue .",
    "this is illustrated by constructing the background model using a fit to a calibration sample using different functional representations for @xmath37and @xmath38 .",
    "[ fig : functional ] ( a ) shows an instance of a calibration dataset randomly generated from a tail of a gaussian ( @xmath37 ) , and its best fit to a second order polynomial ( @xmath38 ) .",
    "it is noted that @xmath38describes very well the background . in this example @xmath49 .",
    "we generate 1000 trials of calibration dataset ( 100 events ) and physics dataset ( 100 events ) , where the physics dataset does not contain any signal . for each dataset pair",
    ", the likelihood function was calculated using eq .",
    "[ eq : discoveryteststat ] . to enhance the effect we did not include the fit uncertainties in the likelihood function as nuisance parameters .",
    "this represents the case in which the model s parameter do not cover the true model .",
    "[ fig : functional ] ( b ) shows the test statistic distribution and demonstrates the breakdown of the @xmath41 distribution .",
    "in addition , it shows how the method of this paper corrects the undercoverage introduced by the faulty background model . before applying the correction",
    "there is a clear undercoverage causing 4 times as many @xmath50 false discoveries than assumed , and around 2% of false @xmath51 discovery rate . after applying the correction the undercoverage",
    "is gone , and the @xmath41 distribution is reproduced .    for completeness , the discovery potential for 15 signal events over 100 background events is shown in fig .",
    "[ fig : functional ] ( c ) .",
    "it can be seen , by comparing the sensitivity to the one computed with @xmath37 , that the artificial enhancement of the sensitivity is relaxed when using the method of this paper at the price of a small sensitivity loss .",
    "( in black ) and a the best fit to a different function @xmath38(in red ) .",
    "the signal model is shown in green . in this instance",
    "the background is underestimated in the signal region . * ( b ) * cumulative distribution function ( cdf ) of discovery significance for a signal - less dataset , @xmath40 .",
    "the expected @xmath41 distribution is shown in gray , with the calculated distribution using @xmath37(black ) .",
    "the red lines show the cdf using a best fit @xmath38as described in the text , before applying the correction ( red dashed ) and after ( blue solid ) .",
    "the undercoverage that causes enhanced false discovery rate is completely removed with the method . *",
    "( c ) * the discovery potential for 15 signal events .",
    "the distributions are presented for normal pl using the wrong model ( solid red ) and using the method presented in sec .",
    "[ sec : thesolution ] ( blue dashed ) .",
    "the distribution using @xmath37is presented for comparison ( gray dotted).,title=\"fig:\",scaledwidth=33.0% ] ( in black ) and a the best fit to a different function @xmath38(in red ) .",
    "the signal model is shown in green . in this instance",
    "the background is underestimated in the signal region . * ( b ) * cumulative distribution function ( cdf ) of discovery significance for a signal - less dataset , @xmath40 .",
    "the expected @xmath41 distribution is shown in gray , with the calculated distribution using @xmath37(black ) .",
    "the red lines show the cdf using a best fit @xmath38as described in the text , before applying the correction ( red dashed ) and after ( blue solid ) . the undercoverage that causes enhanced false discovery rate",
    "is completely removed with the method . *",
    "( c ) * the discovery potential for 15 signal events .",
    "the distributions are presented for normal pl using the wrong model ( solid red ) and using the method presented in sec .",
    "[ sec : thesolution ] ( blue dashed ) .",
    "the distribution using @xmath37is presented for comparison ( gray dotted).,title=\"fig:\",scaledwidth=33.0% ] ( in black ) and a the best fit to a different function @xmath38(in red ) .",
    "the signal model is shown in green . in this instance",
    "the background is underestimated in the signal region . * ( b ) * cumulative distribution function ( cdf ) of discovery significance for a signal - less dataset , @xmath40 .",
    "the expected @xmath41 distribution is shown in gray , with the calculated distribution using @xmath37(black ) .",
    "the red lines show the cdf using a best fit @xmath38as described in the text , before applying the correction ( red dashed ) and after ( blue solid ) .",
    "the undercoverage that causes enhanced false discovery rate is completely removed with the method .",
    "* ( c ) * the discovery potential for 15 signal events .",
    "the distributions are presented for normal pl using the wrong model ( solid red ) and using the method presented in sec .  [ sec : thesolution ] ( blue dashed ) .",
    "the distribution using @xmath37is presented for comparison ( gray dotted).,title=\"fig : \" ]      there are cases in which a calibration sample is available and one wants to construct an empirical model based on this sample without any prior assumptions .",
    "the problem is that in those cases , there are no natural nuisance parameters that can model the uncertainties . in this section",
    "we show how the uncertainties can be modeled with the procedure described in this paper . in addition , we demonstrate how low calibration samples can lead to false discoveries if the procedure is not used .    in case",
    "a calibration sample is available we can use the kernel density estimation ( kde ) in order to produce a nonparametric estimation of the background pdf . let @xmath52 be a calibration sample , the kde is @xmath53 where @xmath54 is the _ kernel function _ ,",
    "a function that is normalized to one with a mean zero , and @xmath55 is a scale parameter called the _",
    "bandwidth_. there are many different ways to choose the bandwidth while throughout this paper we choose to work with a variable bandwidth , inversely proportional to the events density ( a thorough introduction of kde can be found e.g. , in  @xcite ) . the difficulty in using kde for inference , and nonparametric estimators in general , is the lack of natural nuisance parameters .",
    "then , any downward fluctuation of the calibration data in the signal region will result an enhanced probability for false discovery , due to the underestimated background model ( similarly to subsection  [ subsec : usermodel ] ) .",
    "an example of such instance is shown in fig .",
    "[ fig : kde_example_pl ] ( a ) , with @xmath49 . where a small downward fluctuation in the calibration dataset causes an underestimation of the background in the signal region .",
    "this results in an overestimation of the profile likelihood function at @xmath15 as can be seen in fig .",
    "[ fig : kde_example_pl ] ( b ) .",
    "( red dashed ) , constructed using 1000 calibration events .",
    "it can be compared with @xmath37(solid ) .",
    "the signal distribution is also shown ( blue dotted ) .",
    "in this instance @xmath56 is underestimated in the signal region . * ( b ) * the pl function using the the true background ( black solid ) , and using the underestimated @xmath56 before applying the correction ( red dashed ) .",
    "the dataset does not contain any signal events .",
    "the overestimation at @xmath15 is interpreted as a hint towards a signal like data , and is caused because of the underestimation of @xmath56 in the signal region .",
    "this can be avoided by the inclusion of the kde uncertainties as explained in this section and illustrated by the corrected curve ( dashed - dotted blue).,title=\"fig:\",scaledwidth=50.0% ]   ( red dashed ) , constructed using 1000 calibration events .",
    "it can be compared with @xmath37(solid ) .",
    "the signal distribution is also shown ( blue dotted ) . in this instance",
    "@xmath56 is underestimated in the signal region . *",
    "( b ) * the pl function using the the true background ( black solid ) , and using the underestimated @xmath56 before applying the correction ( red dashed ) .",
    "the dataset does not contain any signal events .",
    "the overestimation at @xmath15 is interpreted as a hint towards a signal like data , and is caused because of the underestimation of @xmath56 in the signal region .",
    "this can be avoided by the inclusion of the kde uncertainties as explained in this section and illustrated by the corrected curve ( dashed - dotted blue).,title=\"fig:\",scaledwidth=50.0% ]    the false discovery presented in fig .",
    "[ fig : kde_example_pl ] is caused by the breakdown of the @xmath41 approximation .",
    "as for the previous example , this assumption on the test statistic distribution does not hold due to the improper model .",
    "this breakdown causes an enhanced false discovery rate .",
    "the 3@xmath57 false discovery rate is presented in fig .",
    "[ fig : kde_discovery_comparison ] ( a ) as a function of the calibration size . as the calibration size",
    "is increased the kde approaches @xmath37and the false discovery rate is reduced . however , using the method described in this paper the false discovery can be corrected also for insufficient calibration sample size .",
    "discovery rate using kde background model , @xmath56 ( red line ) .",
    "the false discovery is removed using the method described in this paper as shown by the blue dashed line . * ( b ) * exclusion sensitivity using @xmath56 ( red circles ) , and using @xmath56 plus correction ( blue squares ) .",
    "the median 90% cl limit is shown along with its @xmath581@xmath57 .",
    "the 1@xmath57 sensitivity band using the true background model is shown for comparison ( gray band ) . *",
    "( c ) * discovery potential using pl with the true background ( gray band ) , using @xmath56 ( red circle ) , and using @xmath56 plus correction ( blue squares ) .",
    "it is computed for 15 expected signal events over 100 expected background events .",
    ", title=\"fig:\",scaledwidth=45.0% ]   discovery rate using kde background model , @xmath56 ( red line ) .",
    "the false discovery is removed using the method described in this paper as shown by the blue dashed line .",
    "* ( b ) * exclusion sensitivity using @xmath56 ( red circles ) , and using @xmath56 plus correction ( blue squares ) .",
    "the median 90% cl limit is shown along with its @xmath581@xmath57 .",
    "the 1@xmath57 sensitivity band using the true background model is shown for comparison ( gray band ) . * ( c ) * discovery potential using pl with the true background ( gray band ) , using @xmath56 ( red circle ) , and using @xmath56 plus correction ( blue squares ) .",
    "it is computed for 15 expected signal events over 100 expected background events .",
    ", title=\"fig:\",scaledwidth=45.0% ]    [ fig : kde_exclusion_comparison ]    another important property of the procedure is its discovery potential .",
    "ideally one would like to have a procedure that punishes lack of information by reduction in sensitivity . on the other hand",
    ", one does not wish to lose sensitivity in case there exists enough knowledge .",
    "our method is compared with the naive procedure , and with a pl using the @xmath37 in fig .   [ fig : kde_discovery_comparison ] ( b ) .",
    "the discovery potential is shown for various calibration sizes .",
    "it is computed by the injection of 15 signal events over 100 background events . for large calibration sample",
    "the method approaches the sensitivity of the true model and of the kde .",
    "however , for small calibration samples the kde artificially enhances the sensitivity , while the correction reduces it .",
    "this is the penalty for lack of knowledge .",
    "last but not least , we examine the behavior of the new method for setting exclusion limits . in fig .  [",
    "fig : kde_exclusion_comparison ] we compare the 90% cl exclusion limit of the method with pl using kde ( red squares ) , and with pl using the true model ( gray band ) .",
    "it can be seen that the sensitivity is comparable to kde for small calibration sizes , however it approaches the sensitivity of the true model for large calibration sizes .",
    "a third example is motivated by experiments trying to directly detect dark matter @xcite .",
    "these experiments typically expect signal in the order of a few events per year or less , and determining the background pdf lacks in general a complete physically motivated parametric model . therefore the pdf is , in many cases , estimated by a dedicated calibration dataset , see e.g. , @xcite .",
    "in other cases a model driven background model is used , as in @xcite , where a complementary 5 dimensional unbinned analysis is performed .    for this example",
    "we present the true pdfs of a two dimensional discrimination space in fig .",
    "[ fig:2d ] . in the case",
    "presented here @xmath59 .",
    "the two dimensions can represent two measured channels ( e.g. , charge and light , or heat and charge ) or a combination of them .",
    "the natural invariance under transformations of the unbinned pl makes the specific choice of axes irrelevant .     and @xmath60 axes .",
    "the two dimensional discrimination between the two is based on the combination of them .",
    ", scaledwidth=50.0% ]    we use a two dimensional adaptive kde procedure to estimate the background model , similarly to the previous example . for each instance",
    ", we draw a number of calibration events and infer a background model , and then use this model for statistical inference of a 100 background events , with and without added signal of 15 events , both numbers subject to poisson fluctuations .",
    "[ fig:2dresults ] summarizes the results of this study . on the left panel",
    "one can see the case of no signal , and how often a @xmath51 false detection may occur with and without the correction presented in this paper , as a function of the calibration size .",
    "comparing this to the one dimensional example , it is pointing to a larger calibration sample needed when increasing dimensionality - even though the discrimination is somewhat better in the 2d case ( @xmath48 of 7 vs. 4.9 ) .",
    "the right panel shows the discovery potential of an injected signal , which for low calibration statistics exceeds the expected one by the true pdf .",
    "again , the discrepancy for two dimensions is larger than in one dimension for a given calibration size . in both cases",
    "we see that the corrected pl is curing the false or overly confident discovery , but reduces accordingly the sensitivity - a natural result of less knowledge regarding the true nature of the background .",
    "discovery rate using kde background model , @xmath56 ( red line ) .",
    "the false discovery is removed using the method described in this paper as shown by the blue dashed line . * ( b ) * discovery potential using pl with the true background ( gray band ) , using @xmath56 ( red circles ) , and using @xmath56 plus correction ( blue squares ) .",
    "it is computed for 15 expected signal events over 100 expected background events .",
    ", title=\"fig:\",scaledwidth=48.0% ]   discovery rate using kde background model , @xmath56 ( red line ) .",
    "the false discovery is removed using the method described in this paper as shown by the blue dashed line . * ( b ) * discovery potential using pl with the true background ( gray band ) , using @xmath56 ( red circles ) , and using @xmath56 plus correction ( blue squares ) .",
    "it is computed for 15 expected signal events over 100 expected background events .",
    ", title=\"fig:\",scaledwidth=48.0% ]",
    "in this paper we have presented a new method that protects against biases created by mismodeling of the background .",
    "we show that statistical inference without this addition is prone to risks of false discovery and artificially enhanced sensitivity , which are cured when using eqs .",
    "[ eq : correctedlikelihood ] and  [ eq : epsilonlikelihood ] .",
    "we work within the pl framework and present the asymptotic behavior restoration and explore a number of examples , using parametric models , non - parametric models and two dimensional parameter space .",
    "additional complexity and dimensionality of the data would increase the dangers of mismodeling , and make this addition an essential part of unbinned pl usage .",
    "application of the method to more complicated scenarios such as external data coming from other experiments can be done with proper care .",
    "one must bear in mind that uncertainties attached to detector performance or yet - unknown physics effects can not be eliminated by a model or simulation , however numerically accurate it is .",
    "the authors would like to thank jan conrad , eilam gross and daniel lellouch for fruitful discussions .",
    "we are also grateful to teresa marrodn undagoitia and manfred lindner for their scientific support .",
    "this work was supported by isf i - core grant 1937/12 `` the quantum universe '' and the dfg research training group `` particle physics beyond the standard model ''",
    ".    9    w.  a.  rolke , a.  m.  lopez and j.  conrad , nucl .",
    "instrum .",
    "meth .  a * 551 * , 493 ( 2005 ) doi:10.1016/j.nima.2005.05.068 [ physics/0403059 ] .    c.  e.  aalseth _",
    "[ cogent collaboration ] , phys .",
    "d * 88 * , 012002 ( 2013 ) doi:10.1103/physrevd.88.012002 [ arxiv:1208.5737 [ astro-ph.co ] ] .",
    "g.  angloher _ et al .",
    "_ , eur .",
    "j.  c * 72 * , 1971 ( 2012 ) doi:10.1140/epjc / s10052 - 012 - 1971 - 8 [ arxiv:1109.0702 [ astro-ph.co ] ] .",
    "g.  angloher _ et al . _",
    "[ cresst - ii collaboration ] , eur .",
    "j.  c * 74 * , no .",
    "12 , 3184 ( 2014 ) doi:10.1140/epjc / s10052 - 014 - 3184 - 9 [ arxiv:1407.3146 [ astro-ph.co ] ] .",
    "j.  h.  davis , int .",
    "j.  mod .",
    "a * 30 * , no .",
    "15 , 1530038 ( 2015 ) doi:10.1142/s0217751x15300380 [ arxiv:1506.03924 [ hep - ph ] ] .",
    "j.  h.  davis , c.  mccabe and c.  boehm , jcap * 1408 * , 014 ( 2014 ) doi:10.1088/1475 - 7516/2014/08/014 [ arxiv:1405.0495 [ hep - ph ] ] .",
    "e.  aprile _ et al .",
    "_ [ xenon100 collaboration ] , phys .  rev .",
    "d * 84 * , 052003 ( 2011 ) doi:10.1103/physrevd.84.052003 [ arxiv:1103.0303 [ hep - ex ] ] .",
    "f.  james , hackensack , usa : world scientific ( 2006 ) 345 p    r.  j.  barlow , nucl .",
    "instrum .",
    "a * 297 * , 496 ( 1990 ) .",
    "doi:10.1016/0168 - 9002(90)91334 - 8    g.  cowan , k.  cranmer , e.  gross and o.  vitells , eur .",
    "j.  c * 71 * , 1554 ( 2011 ) erratum : [ eur .",
    "j.  c * 73 * , 2501 ( 2013 ) ] doi:10.1140/epjc / s10052 - 011 - 1554 - 0 ,",
    "10.1140/epjc / s10052 - 013 - 2501-z [ arxiv:1007.1727 [ physics.data-an ] ] .",
    "s.  s.  wilks , annals math .  statist .   *",
    "9 * , no . 1 , 60 ( 1938 ) .",
    "doi:10.1214/aoms/1177732360    g.  aad _ et al .",
    "_ [ atlas collaboration ] , phys .",
    "d * 90 * , no .",
    "11 , 112015 ( 2014 ) doi:10.1103/physrevd.90.112015 [ arxiv:1408.7084 [ hep - ex ] ] .",
    "d.  s.  akerib _ et al .",
    "_ , [ arxiv:1608.07648 [ astro-ph.co ] ]",
    ".    k.  s.  cranmer , comput .",
    "commun .   * 136 * , 198 ( 2001 ) doi:10.1016/s0010 - 4655(00)00243 - 5 [ hep - ex/0011057 ] .",
    "marrodn undagoitia teresa and rauch ludwig , j. phys .",
    "g43 1 013001 ( 2016 ) .",
    "doi : 10.1088/0954 - 3899/43/1/013001    w.  verkerke and d.  p.  kirkby , econf c * 0303241 * , molt007 ( 2003 ) [ physics/0306116 ] .",
    "here we will demonstrate the similarity between the binned and the unbinned likelihood function .    starting with the binned - likelihood function defined in eq .",
    "[ eq : blikelihood ] we can obtain the log - likelihood function    @xmath61=\\\\ & \\quad \\sum_j^{n_{bins } } \\left [ -\\epsilon_s^j n_s - \\epsilon_b^j n_b +   n_j \\mathrm{log}(n_s \\epsilon^j_{s } + n_b \\epsilon^j_{b})\\right]=\\\\ & \\quad - n_s - n_b + \\sum_j^{n_{bins } } n_j \\mathrm{log}(n_s \\epsilon^j_{s } + n_b \\epsilon^j_{b}),\\end{aligned}\\ ] ]    where we use the fact that @xmath62 , and neglect constant terms .    alternatively , starting from eq .  [ eq : ublikelihood ] , the unbinned log - likelihood function can be projected into bins .",
    "this is done by the replacement of @xmath63 with a binned probability density function , namely it can be replaced by @xmath64 , the probability to be found in the bin , divided by the bin width , @xmath65 .",
    "@xmath66 = \\\\ &",
    "\\quad -n_s - n_b+\\sum_{i=1}^n \\mathrm{log}(n_s f_s(x_i ) + n_b f_b(x_i))= \\\\ &",
    "\\quad -n_s -n_b + \\sum_{i=1}^n \\mathrm{log}(n_s \\epsilon^{bin(x_i)}_{s}/w + n_b \\epsilon^{bin(x_i)}_{b}/w ) = \\\\   & \\quad - n_s - n_b + \\sum_j^{bins } \\sum_{i=1}^{n_j } \\mathrm{log}(n_s\\epsilon^j_{s } + n_b \\epsilon^j_{b } ) + constant=\\\\ & \\quad - n_s - n_b + \\sum_j^{n_{bins } } n_j \\mathrm{log}(n_s \\epsilon^j_{s } + n_b \\epsilon^j_{b } ) + constant,\\end{aligned}\\ ] ]    which yields an identical expression to the binned case , up to a constant .      in general",
    ", it is quite common to measure more than one variable for each event . out of those variables",
    "one would usually like to get the maximum discrimination between signal and background .",
    "namely , to have the best sensitivity on the parameter of interest , @xmath11 .",
    "this question can be phrased in the following way : assuming that we have some data space @xmath67 in which the measurement has been done .",
    "we can apply transformation on the variables and work in other space @xmath68 .",
    "what is the space in which we get the smallest @xmath69 .",
    "working with eq .",
    "[ eq : ublikelihood ] , @xmath70 can be estimated by calculating the expectation values of the second derivatives of the log - likelihood function @xmath71},\\\\   \\beta \\equiv -e\\big{[}\\frac{\\partial^2 { { \\mathrm{log}(\\mathcal{l})}}}{\\partial n_s \\partial n_b}\\big{]},\\\\   \\gamma \\equiv   -e\\big{[}\\frac{\\partial^2 { { \\mathrm{log}(\\mathcal{l})}}}{\\partial { n_b}^2}\\big{]}.    \\end{gathered}\\ ] ]    the resolution of @xmath11 can be computed to be @xmath72 .",
    "for the unbinned likelihood it can be easily shown that @xmath73 , @xmath74 and @xmath75 are invariant under coordinate transformation - hence we can not gain or loose sensitivity by choice of coordinates    @xmath76 } = \\int \\frac{f_s^2(x ) } { n_s f_s(x ) + n_b f_b(x)}dx \\equiv \\alpha,\\\\   -e\\big{[}\\frac{\\partial^2 { { \\mathrm{log}(\\mathcal{l})}}}{\\partial n_s \\partial n_b}\\big { ] } = \\int \\frac{f_s(x ) f_b(x)}{n_s f_s(x ) + n_b f_b(x)}dx \\equiv \\beta,\\\\   -e\\big{[}\\frac{\\partial^2 { { \\mathrm{log}(\\mathcal{l})}}}{\\partial { n_b}^2}\\big { ] } = \\int \\frac{f_b^2(x ) } { n_s f_s(x ) + n_b f_b(x)}dx \\equiv \\gamma .",
    "\\end{gathered}\\ ] ]    the resolution on @xmath11 can be computed to be @xmath72 , and we see that @xmath73 , @xmath74 and @xmath75 are invariant under transformation of @xmath67 .",
    "this means that for an unbinned analysis the sensitivity , @xmath69 , is optimal and invariant in any space we work in .",
    "this is another advantage of this procedure .    when rejecting the null hypothesis with eqs .",
    "[ eq : sensitivitynsnb2 ] , @xmath77 and we are left with the estimator for _ shape discrimination _",
    "@xmath78 ranging from 1 to infinity , where @xmath79 if and only if @xmath80",
    ". the higher @xmath48 , the better the shape discrimination between the background and the signal .",
    "this is again an invariant under transformations of the measured space . in the limit @xmath81 ,",
    "pl does not contribute to the sensitivity beyond simple counting , as the background leakage goes to zero .",
    "lets assume that we have a control measurement consists of @xmath82 background events . the unbinned likelihood ( eq .  [ eq : ublikelihood ] ) can be written in order to constrain the background model .",
    "the calibration likelihood function is @xmath83 where @xmath84 and @xmath85 are the expected number of signal and background events .",
    "the @xmath84 component is introduced in order to quantify the level of signal like behavior of the background model , as we do nt expect to have any signal events in the calibration dataset . in the case of good background over signal discrimination",
    "@xmath86 as @xmath82 increases",
    ".    the log likelihood , after neglecting constant offsets , is @xmath87 with @xmath88 , and @xmath89 .",
    "we identify the term in the logarithm to be the background model .",
    "namely instead of keeping @xmath90 fixed we let it be contaminated with a signal like component , where @xmath42 is the contamination parameter , and the calibration dataset pose a constraint on it .    for a ` physics ' dataset , which might contain a signal we will use eq .  [",
    "eq : ublikelihood ] , with one change .",
    "we will propagate the background model as follows @xmath91 , in order to reflect the uncertainty of the model .      where @xmath11 is the parameter of interest , and @xmath94 , @xmath42 , and @xmath12 are nuisance parameters .",
    "@xmath94 is constrained to be @xmath95 independently of the other variables , hence the last two terms can be neglected .",
    "lets take the unbinned likelihood function @xmath96 ( eq .",
    "[ eq : ublikelihood ] ) and treat the background model itself as a nuisance parameter .",
    "now , we have a semiparametric likelihood functional @xmath97 , with two parameters , @xmath11 and @xmath12 , and one nuisance function , @xmath90 , where the parameter of interest is @xmath11 .    for simplicity",
    "we assume that @xmath12 is known , and check the first order variation with respect to @xmath11 and @xmath98 in order to identify functional variation on the background model that can be used .      for discovery",
    "we take a negative variation of @xmath11 and we are scanning below @xmath21 .",
    "therefore the absolute value of the second term is greater than the value of the first term .",
    "then , if we allow variation of the background model , by setting @xmath100 we relax the difference between the two terms for any given instance of the data @xmath101 ."
  ],
  "abstract_text": [
    "<S> we present a universal method to include residual un - modeled background shape uncertainties in likelihood based statistical tests for high energy physics and astroparticle physics . </S>",
    "<S> this approach provides a simple and natural protection against mismodeling , thus lowering the chances of a false discovery or of an over constrained confidence interval , and allows a natural transition to unbinned space . </S>",
    "<S> unbinned likelihood allows optimal usage of information for the data and the models , and enhances the sensitivity .    </S>",
    "<S> we show that the asymptotic behavior of the test statistic can be regained in cases where the model fails to describe the true background behavior , and present 1d and 2d case studies for model - driven and data - driven background models . </S>",
    "<S> the resulting penalty on sensitivities follows the actual discrepancy between the data and the models , and is asymptotically reduced to zero with increasing knowledge . </S>"
  ]
}