{
  "article_text": [
    "mixture models are noted for their flexibility in modeling complex data and are widely used in the statistical literature ( see @xcite ) .",
    "such models provide a natural framework for the modeling of heterogeneity in a population .",
    "moreover , due to the large class of functions that can be approximated by mixture models , they are attractive for describing non - standard distributions and have been adopted in many areas , as genetics , ecology , computer science , economics , biostatistics and many others .",
    "for instance , as stated in @xcite , in genetics , location of quantitative traits on a chromosome and interpretation of microarrays are both related to mixtures , while , in computer science , spam filters and web context analysis start from a mixture assumption to distinguish spams from regular emails and group pages by topic , respectively .    statistical analysis of mixtures has not been straightforward and the bayesian paradigm has been particularly suited to their analysis . this framework allows the complicated structure of a mixture model to be decomposed into a set of simpler structures through the use of hidden or latent variables . according to @xcite , when the number of components is unknown , the bayesian paradigm is the only sensible approach to its estimation .",
    "then , the bayesian approach contributed to mixture models become increasingly popular in many areas . in real applications ,",
    "the number of components can arise important conclusions about the problem , so it has to be well specified or estimated , despite we usually have little theoretical guidance . on the other hand ,",
    "even if prior theory suggests a particular number of components we may not be able to reliably distinguish between some of the components . in some cases additional components",
    "may simply reflect the presence of outliers in the data .    when the number of subpopulations is assumed known , markov chain monte carlo methods ( mcmc ) can be used for bayesian estimation of the subpopulation parameters .",
    "nevertheless , this method , as originally formulated , requires the posterior distribution to have a density with respect to some fixed measure .",
    "when the number of components is considered unknown , i.e. , the size of the parametric space is also a parameter , it appears a problem with variable dimension , thus mcmc can not be used alone in this case and more sophisticated methods are required to perform the bayesian analysis .",
    "one alternative in this case is the approach based on reversible jump mcmc ( rjmcmc ) , which was first proposed in @xcite and applied in univariate normal mixture models with unknown numbers of components by @xcite .",
    "the method basically consists of jumps between the parameter subspaces corresponding to different numbers of components in the mixture .    whilst mcmc provides a convenient way to draw inference from complicated statistical models ,",
    "there are still many , perhaps under appreciated , problems associated with the mcmc analysis of mixtures .",
    "the problems are mainly caused by the nonidentifiability of the components under symmetric priors , which leads to the so called label - switching in the mcmc output , discussed in @xcite .",
    "the term describes the invariance of the likelihood under relabelling of the mixture components , which can lead to the posterior distribution of the parameters being highly symmetric and multimodal , making it difficult to summarize .",
    "in particular , the usual practice of estimating parameters by their posterior mean , and summarizing joint posterior distributions by marginal distributions is often inappropriate .",
    "one frequent response to this problem is to remove the symmetry by using artificial identifiability constraints .",
    "this and other alternative classes of approaches to this problem are described by @xcite .",
    "the aim of this work is to review and discuss the application of mixture models , in particular the normal mixture models , to heterogeneous populations under the bayesian approach .",
    "the main purpose is to evaluate the model s performance in different settings of heterogeneity and considering the number of components known and unknown .",
    "we verify if the label - switching phenomena generally persists when the subpopulations are not well separated , then , if the more the population is heterogeneous , the more the model parameters should be better estimated .",
    "furhermore , we evaluate the label - switching assuming more informative prior distributions for the mixture weights .",
    "the paper is organized as follows .",
    "section [ model ] presents the general definition of a mixture model and discusses some aspects of the inference .",
    "a simulation study for assessing the estimation of model parameters under different levels of heterogeneity is presented in section [ simul_study ] .",
    "additionally , a prior sensitivity analysis of the mixture proportions is presented .",
    "it also discusses the model fit when the number of components is known and unknown . in section [ real_data ]",
    "the performance of the methodology is assessed through an application to a real data set .",
    "finally , section [ sec : concl ] presents some conclusions and suggestions for further research .",
    "the basic mixture model for independent scalar or vector observations @xmath0 , @xmath1 is a convex combination given by : @xmath2 where @xmath3 is a given parametric family of densities indexed by a scalar or a vector @xmath4 .",
    "in general , the objective of the analysis is to make inferences about the unknowns : the number of components , @xmath5 ; the parameters @xmath6 with @xmath7 being specific to component @xmath8 ; and the components weights , @xmath9 , @xmath10 , @xmath11 .",
    "let @xmath12 be the parametric vector of the model ( [ mixt ] ) .    for a random sample",
    "@xmath13 observed , the likelihood function of @xmath14 is given by : @xmath15 the likelihood function leads to @xmath16 terms , what brings a computational difficulty .",
    "a context in which the model ( [ mixt ] ) can arise and we are interested in this paper is when we postulate a heterogeneous population consisting of heterogeneous groups @xmath17 of sizes proportional to @xmath18 , from which a random sample is drawn .",
    "the label of the group from which each observation is drawn is unknown and it is natural to regard the group label @xmath19 , for the @xmath20-th observation as a latent variable and rewrite ( [ mixt ] ) as the following hierarchical model : for @xmath21 , @xmath22 @xmath23    integrating @xmath24 out from ( [ mixt2 ] ) we return to model ( [ mixt ] ) .",
    "the formulation given by ( [ mixt2 ] ) is convenient for interpretation and calculation , decreasing the computational cost .",
    "a bayesian approach to inference requires the specification of a prior distribution @xmath25 for the parameters of the mixture model ( [ mixt ] ) .",
    "in particular , the prior elicitation is an important question .",
    "being fully non - informative and obtaining proper posterior distributions are not possible in a mixture content .",
    "an alternative on this case is to define weakly informative priors , which may or may not be data dependent",
    ".    the mixture model in ( [ mixt2 ] ) is invariant to permutation of the labels @xmath26 .",
    "some implications of this for likelihood analyses are discussed by @xcite .",
    "if we have no prior information that distinguishes between the components of the mixture , so the prior distribution @xmath25 is the same for all permutations of @xmath4 , then the posterior distribution will be similarly symmetric and , there will be @xmath27 symmetric modes of the posterior distribution .",
    "therefore , the component labels are mixed up and can not be distinguished from each other . as a result ,",
    "the marginal on the parameters for all components is identical and the posterior expectation for the parameters is identical too , and so estimating the parameters on the basis of the mcmc output is not straightforward .",
    "there are some suggested solutions to this problem , see @xcite for details .",
    "one common response to the label - switching problem is to impose an identifiability constraint on the parameter space .",
    "this breaks the symmetry of the prior and thus , of the posterior distribution of the parametric vector .",
    "for example , we can impose an ordering constraint on @xmath28 s , such as @xmath29 , if it is a scalar .",
    "however , for any given data set , many choices of identifiability constraint may be ineffective in removing the symmetry in the posterior distribution .",
    "as we are in a bayesian framework , the inference consists in obtain the posterior distribution of the parametric vector @xmath30 of model ( [ mixt2 ] ) . in general , this distribution can not be obtained in closed form .",
    "therefore , it is necessary to use some numerical approximation methods .",
    "one alternative , which is often used and is feasible to implement , is to generate samples from the marginal distributions of the parameters based on the mcmc algorithm .",
    "a comprehensive bayesian treatment using mcmc methods was presented in @xcite for finite mixture models .",
    "nevertheless , this method , as originally formulated , requires the posterior distribution to have a density with respect to some fixed measure .",
    "thus , in the mixture context , the method can only be applied when the number of components @xmath5 in the model ( [ mixt2 ] ) is considered known",
    ".    however , rarely the number @xmath5 is known , and fix it on an incorrect value can bring important consequences to the posterior distribution .",
    "other times , the target of the study is exactly the estimation of @xmath5 .",
    "the approach based on rjmcmc is an alternative in this case , which was proposed on this context by @xcite .",
    "it operates on the augmented parameter space , where the allocation variables @xmath31 are included as unknown parameters .",
    "the method basically consists of jumps between the parameter subspaces corresponding to different numbers of components in the mixture , after updating them .",
    "if the current model is a mixture with @xmath32 components , then it is usual to reduce the searching strategy to moves that either preserve the number of components , or lead to a mixture with @xmath33 or @xmath34 components .",
    "the idea is then to supplement each of the spaces with adequate artificial spaces in order to create a bijection between them , most often by augmenting the space of the smaller model .",
    "jumps are achieved by adding new components , deleting existing components , and splitting or merging these .",
    "these moves are randomly chosen and after being drawn makes necessary corresponding changes to @xmath35 .",
    "with respect to the label - switching problem , during mcmc computation , the sampler should switch from modes to modes between the iterations , however the failure to visit the identical posterior expectations reveals that the mcmc sampler has not converged and the posterior distribution surface is multimodal . if the value of @xmath27 is very large , it would be hard for the regular mcmc sampler to thoroughly and explore the high multimodality .",
    "thus , the mcmc samples might be trapped into local modes and it would require an enormous number of iterations to escape from it and the label switch would cause very poor estimates and the results might be very different from different runs of the mcmc .",
    "in this work we are particularly interested in the univariate normal case presented in @xcite , then @xmath7 in ( [ mixt ] ) becomes a vector with expectation and variance parameters @xmath36 .",
    "the model is stated below : for @xmath37 and @xmath38 , @xmath39    assuming that the parameters in @xmath30 are prior independent and identically distributed and that @xmath5 is unknown , the prior distribution is given by : for @xmath37 and @xmath38 , @xmath40 where @xmath41 generically denotes the symmetric dirichlet distribution with parameter @xmath42 .",
    "the symmetric dirichlet distributions are often used , since there typically is no prior knowledge favoring one component over another . since all elements of the parameter vector",
    "have the same value , the distribution alternatively is parametrized by a single scalar value @xmath42 .",
    "@xmath43 represents the gamma distribution with mean @xmath44 and variance @xmath45 and @xmath46 is the uniform distribution defined on the integers @xmath47 .",
    "moreover , for identifiability , we can use for example that the @xmath48 are in increasing numerical order , thus the joint prior distribution of @xmath30 is @xmath27 times the product of their marginal prior distributions .    in this paper , as treated in @xcite",
    "we considered the bayesian estimation in the set - up where we do not have strong prior information on the mixture parameters . on the other hand , being fully non - informative and obtaining proper posterior distributions are not possible in a mixture content .",
    "an alternative on this case is to keep the simple independence and define weakly informative priors , which may or may not be data dependent .",
    "therefore , the default hyperparameter choices can be viewed with further details in @xcite .",
    "furthermore , in a normal mixture model the posterior distribution of the means for example could overlap , but the extent of the overlap depends on its separation and the sample size .",
    "when the means are well separated , labels of the realizations from the posterior by ordering their means generally coincide with the population ones . as the separation reduces",
    ", label - switching may occur .",
    "this problem can be also minimized by choosing to order other parameters of the mixture components , for example , the variance , weights or some combination of all three parameters .    in this work",
    "the unique labeling will be achieved by imposing a restriction on @xmath48 .",
    "we will use that in which the @xmath48 are in increasing numerical order ; thus the joint prior distribution of the parameters is @xmath27 times the prior density , restricted to the set @xmath49",
    "to assess the convergence of the mcmc and rjmcmc estimation , we generated some samples under different settings .",
    "we obtained samples from the posterior distributions of the model parameters , supposing @xmath5 known and estimating it .",
    "the population estimates were then compared with the true values to evaluate the model s performance .",
    "the aim is to evaluate the performance of the normal mixture model varying the level of heterogeneity and the prior information elicited for the mixture proportions .",
    "furthermore , we also compared the results obtained under each simulation method considered , mcmc and rjmcmc .",
    "to check the convergence of the rjmcmc and mcmc estimations , we generated one sample with @xmath50 observations under two levels of heterogeneity , the first one with groups well separated , which we call as the more heterogeneous sample and the other with groups less well separated , which represents the more homogeneous one . on both scenarios we fixed @xmath51 , @xmath52 and @xmath53 . with this value fixed for @xmath54 we expected to have groups with a reasonable number of observations , thus we did not consider scenarios with groups outliers .",
    "the heterogeneous scenario was obtained fixing @xmath55 and the homogeneous fixing @xmath56 .",
    "figure [ data1 ] presents the distribution of both data generated .",
    "the aim of this study is to verify if the level of heterogeneity of the population affects the results , mainly the label - switching problem .",
    "cc    the prior distribution considered are described in ( [ prior ] ) , and we elicited the prior for @xmath48 and @xmath57 using the same idea of weakly informative prior suggested by @xcite .",
    "first , we assumed @xmath5 unknown and in its prior distribution presented in ( [ prior ] ) we assumed @xmath58 , thus rjmcmc was used to obtain samples from the posterior distribution .",
    "we also did a brief prior sensitivity analysis assuming two values on the dirichlet prior distribution for each data : @xmath59 for the heterogeneous case and @xmath60 for the homogeneous case .",
    "to assume @xmath61 is equivalent to a uniform distribution over all points in its support .",
    "on the other hand , the parameter value above @xmath62 gives some information that all sample proportions in subpopulations are similar to each other .",
    "for the rjmcmc simulations , we generated , respectively for the homogeneous sample and the heterogeneous one , 350,000 and 70,000 samples from the posterior distribution , discarded the first 10,000 and 20,000 , and then thinned the chain by taking every 10th sample value .",
    "figure [ kpost1 ] displays the histogram with the posterior densities of @xmath5 for some values of @xmath63 .",
    "it shoud be noted that for the heterogeneous case the parameter @xmath5 is well estimated , but when @xmath64 the estimate is more accurate . on the other hand , @xmath5 is underestimated when assuming @xmath64 with the homogeneous sample .",
    "the same happen with @xmath61 or any value less than @xmath65 .",
    "in this case , when @xmath66 the value of @xmath5 is well estimated .",
    "cc +    figure [ cadeia1 ] shows the trace plot with the posterior distribution of parameters @xmath48 conditional on the posterior samples , whose estimated value of @xmath5 is the one with higher posterior probability . here",
    ", we also considered the value of @xmath5 known and fixed it on the true value used to generate the samples , so mcmc was also used to generate samples from the posterior distribution . for the mcmc simulations , we generated 70,000 samples from the posterior distribution , discarded the first 20,000 , and then thinned the chain by taking every 10th sample value , for both data generated .",
    "all the results were obtained for each scenario and value of @xmath63 considered .",
    "the black trace represents the posterior density fixing @xmath61 , the blue trace when @xmath64 in both scenarios and @xmath66 is represented by the red trace in the homogeneous case .",
    "the gray line represents the true value of each @xmath48 .",
    "note that on the homogeneous case , when rjmcmc is used , there is only a red trace for @xmath67 , that is because the posterior for @xmath5 favors the value 5 , only when @xmath66 .",
    "when analyzing figure [ cadeia1 ] it can be seen the effects of label - switching in the sampled values of the component means for many cases , even in the heterogeneous case .",
    "however this behavior improves when giving some prior information about the mixture proportions .",
    "it is also possible to observe that on the homogeneous case to obtain better results it is needed to increase even more the value of @xmath63 , this is , to give more prior information that the proportion observed in groups are similar .",
    "l +   +   +   +    the results obtained indicate that the rjmcmc and mcmc chains have converged for some cases , but for others the label - switching phenomena appears significantly , and so estimating the means on the basis of the rjmcmc and mcmc output is not straightforward .",
    "however , as the value of @xmath63 increases this behavior improves .",
    "if the number of iterations increases and then , the lag of the chain , the convergence may also improve , but it would require a high computational effort , thus we suggest here the carefully elicitation of the prior distribution to have better estimates .",
    "almost all the mean parameters are well estimated when @xmath64 and @xmath66 in the heterogeneous and homogeneous case , respectively .",
    "traces and density estimates for the mixture proportions and variances present this same behavior .",
    "in general , mcmc and rjmcmc present a similar behavior , mainly for the heterogeneous sample generated .",
    "a more interesting comparison between both approaches is presented in the next subsection .",
    "figure [ summary1 ] shows summary statistics of the posterior distributions of the mean parameters after reaching the supposed convergence for each of the scenarios and prior assumed , when assuming @xmath5 unknown .",
    "the crosses represent the true value , the lines the 95% credibility interval and the points are the posterior mean .",
    "also , the results in black are obtained assuming @xmath61 , the blue one when @xmath64 and the red when @xmath66 . in almost all the cases",
    "the intervals contain the true value .",
    "it is possible to observe the impact of the label - switching , which difficults the parameters estimation , but also the improvement of the results when assuming a more informative prior to @xmath54 .",
    "cc    therefore , we conclude that in those cases considered here the identifiability problem can be minimized under more informative priors and it was not necessary to use other alternative class of approaches to deal with the identifiability problem , as those described in @xcite .",
    "the prior distribution of @xmath54 seems to have strong impact on the posterior distribution , improving the results , even in a homogeneous case .",
    "furthermore , as the degree of heterogeneity increases , the mixture model performs considerably better even under less prior information .",
    "the same conclusions were attained when estimating the value of @xmath5 or considering it known .",
    "additionally , figure [ pred1 ] shows the predictive densities for the two data sets generated , for all the prior distributions considered for @xmath54 , represented by the solid ( @xmath61 ) , dashed ( @xmath64 ) and dotted ( @xmath66 ) lines , respectively .",
    "the predictive densities in black are those obtained when the value of @xmath5 is estimated , so rjmcmc was used , and the red ones are obtained when the value of @xmath5 is fixed on the true value , so mcmc was used . the densities obtained under rjmcmc are conditional on the posterior samples whose sampled value of @xmath5 is equal to the value with high posterior probability among all the samples .",
    "in contrast with the above results , an estimate of the predictive density based on the rjmcmc and mcmc outputs is unaffected by the label - switching problem , since it does not depend on how the components are labeled . it should be noted that the predictive density is better estimated on the heterogeneous sample than the homogeneous one and that the prior distribution does not affect the estimates .",
    "moreover , the results obtained in the estimation considering @xmath5 unknown and fixed are very similar to each other .",
    "cc      to examine the performance of the bayes estimators obtained under each simulation method , we generated two artificial samples of size @xmath68 , fixing @xmath5 into two different values , @xmath69 and @xmath51 , in order to also evaluate the results when varying the value of @xmath5",
    ". then , we obtained samples from the posterior distribution of the parametric vector , supposing @xmath5 known ( mcmc ) and estimating it ( rjmcmc ) . in the mcmc simulation",
    "we particularly fixed @xmath5 for each case in three different values : we assumed it @xmath70 and @xmath65 for the first sample and @xmath71 and @xmath72 for the second one .",
    "we assumed here the same prior distribution used in section [ sens ] .",
    "thus , we are interested in evaluate the method s performance when we fix @xmath5 on its true value , on a smaller and a greater value than the true one and when it is estimated . for the rjmcmc and mcmc simulations , we generated 350,000 samples from the posterior distribution , discarded the first 10,000 , then thinned the chain by taking every 50th sample value , and the convergence was supposed achieved .",
    "figure [ preditivacomp ] presents the posterior distribution of @xmath5 obtained under the rjmcmc simulation and the predictive densities obtained for each sample generated .",
    "it should be noted that @xmath5 is well estimated and all predictive densities are very similar , except when we fixed @xmath5 in a value lower than the true one .",
    "moreover , fixing @xmath5 in a greater value than the truth does not affect the results .",
    "+    table [ dic ] presents the deviance information criterion ( dic ) , introduced by @xcite , for each approach considered in this study .",
    "dic evaluates the goodness of fit of the model , thus the model with the smallest dic should be the one that would best fit .",
    "the model with @xmath5 known seems to fit the data better than its counterparts .",
    "however , the results are very similar , even when @xmath5 is estimated , increasing the size of the parametric vector , except when @xmath5 is fixed in a smaller value than the truth .",
    "[ cols=\"^,^,^,^,^,^,^,^,^,^,^\",options=\"header \" , ]     the model fit estimating @xmath5 and fixing it in @xmath69 results a dic of 3692.59 and 3693.99 , respectively .",
    "thus , as dic increases with the number of parameters , it is expected that rjmcmc presents a higher dic .",
    "however , since both dics were very similar , it is possible to conclude that both methods are efficient in this case .",
    "we have considered the problem of the fit of mixture models for heterogeneous populations under different levels of heterogeneity .",
    "we have discussed the improvement of the convergence when assigning a weakly informative prior distribution for the mixture proportions even for more homogeneous populations .",
    "finally , we have also evaluated the inference for the model when the number of components is unknown ( rjmcmc ) and when it is fixed in a known value ( mcmc ) .",
    "we have concluded that when the number of mixture components is unknown , the rjmcmc is a feasible alternative , achieving similar results when this number is fixed in the true value .",
    "nevertheless , it requires slightly bigger computational effort than mcmc . on the other hand ,",
    "if we are not interesting in estimating this number , fixing it in a smaller value than the truth will generate poor estimates , however , similar results are obtained when fixing it in the true value or greater than this .",
    "spiegelhalter , d. j. , best , n. g. , carlin , b. p. and van der linde , a. ( 2002 ) bayesian measures of model complexity and fit .",
    "journal of the royal statistical society : series b ( statistical methodology ) , * 64 * , 583 - 639 ."
  ],
  "abstract_text": [
    "<S> mixture models provide a flexible representation of heterogeneity in a finite number of latent classes . from the bayesian point of view </S>",
    "<S> , markov chain monte carlo methods provide a current way to draw inference from these models . </S>",
    "<S> in particular , when the number of subpopulations is considered unknown , more sophisticated methods are required to perform the bayesian analysis . </S>",
    "<S> the reversible jump markov chain monte carlo is an alternative method for computing the posterior distribution by simulation in this case . </S>",
    "<S> some problems associated with the bayesian analysis of these class of models are frequent , as the so - called  label - switching \" . however , as the level of the heterogeneity in the population increases , it is expected that these problems become less frequent and the model s performance improves . </S>",
    "<S> thus , the aim of this work is to evaluate the normal mixture model fit using simulated data under different settings of heterogeneity and prior information about the mixture proportions . </S>",
    "<S> a simulation study was also carried out to evaluate the model s performance considering the number of components known and estimating it . </S>",
    "<S> finally , the model is applied to a real data set that consists of antibody levels of cytomegalovirus in individuals .    * keywords * : identifiability , sensitivity analysis , subpopulations , frequentist properties , nhanes </S>"
  ]
}