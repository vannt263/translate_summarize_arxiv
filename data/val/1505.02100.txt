{
  "article_text": [
    "the probability density function ( pdf ) is a key concept in statistics with many practical applications , see for example @xcite , @xcite .",
    "constructing the most adequate pdf from the observed data is still an important and interesting research problem , especially for large datasets .",
    "pdfs are often calculated using nonparametric data - driven methods .",
    "one of the most popular nonparametric method is the kernel density estimation ( kde ) @xcite .",
    "however , a very serious drawback of using kde is the large number of calculations required to compute them as well as to find the optimal bandwidth ( smoothing ) parameter ( time complexity @xmath0 ) .    in this paper",
    "we investigate the possibility of utilizing field - programmable gate arrays ( fpga ) to accelerate finding of such the optimal bandwidth . towards the needs of the paper",
    "we have selected one popular and often used algorithm called in literature the plugin @xcite .",
    "this work can be considered as a continuation and extension of the paper @xcite , where the authors utilize gpus for speeding up optimal bandwidth selection .",
    "one of the algorithm analyzed in that paper was the above mentioned plugin one .    generally , there are two methodologies for speeding up complex numerical algorithms : software - based and hardware - based . in this paper",
    "we concentrate only on hardware - based methods .",
    "the commonly known approaches are as follows : ( a ) computing on general purpose multicore cpu microprocessors , ( b ) computing on distributed environments ( e.g. clusers , grids , etc . ) , ( c ) computing on gpus , @xcite ( d ) computing on dsp units and ( e ) computing on fpga chips @xcite .    in the paper we are concerned with fpga approach . in @xcite",
    "the author considers a problem how to use fpga for fast computing of pdfs using direct vhdl programming approach .",
    "however , the problem we are concerning is of different nature as we concentrate our attention for computing the optimal bandwidth for pdf ( see chapter [ sec : kde - band ] ) .    to develop the final fpga design we use the high level synthesis ( hls ) approach @xcite , @xcite , where no direct hardware description language ( hdl ) coding is needed ( typically in vhdl or verilog languages ) .",
    "the remainder of the paper is organized as follows . in section [ sec : kde - band ] we turn our attention to give the reader some preliminary information on kde and bandwidth selection . in section [ sec : plugin ] we give detailed mathematical formulas for calculating optimal bandwidth using the plugin method . in section [ sec : fpga - impl ] we cover all the necessary details on our fpga - based implementation .",
    "we also present practical experiments we carried out and discuss the results . in section [ sec : concluding ] we conclude the paper .",
    "let us consider a continuous _ univariate _ random variable @xmath1 and let assume its probability density function @xmath2 exists but is unknown .",
    "its estimate , usually denoted by @xmath3 , will be determined on the basis of a random sample of size @xmath4 , that is @xmath5 ( our experimental data ) . in such a case , a @xmath6-dimensional kernel density estimator @xmath7 of a real density @xmath8 for random sample @xmath9",
    "is given by the following formula @xmath10 @xmath11 is a positive real number called _ smoothing parameter _ or _ bandwidth _ and @xmath12 is the _ kernel function _  a symmetric function that integrates to one . in practical applications",
    "@xmath12 has often the gaussian normal form , that is @xmath13 if we have the bandwidth @xmath11 , we can determine the estimator @xmath7 of the unknown density function @xmath8 using formula ( [ eq : kde - def ] ) .",
    "the bandwidth @xmath11 is the parameter which exhibits a strong influence on the resulting kde .",
    "formula ( [ eq : kde - def ] ) can be easily extended to the _ multivariate _ case . in the most general variant",
    "the scalar bandwidth @xmath11 is replaced by the _ unconstrained _ bandwidth matrix @xmath14 ( which is symmetric and positive definite ) . also ( [ eq : gaussian ] )",
    "is generalized to the multivariate case .",
    "two commonly used kernel types are _ product _ and _ radial _ ( also known as _ spherically symmetric _ ) ones .",
    "as an example of how kde works consider a toy dataset of 8 data points : @xmath15 three different kdes based on these data are depicted in figure [ fig : kernel - demo ] .",
    "it is easy to notice how the bandwidth @xmath11 influences the shape of the kde curve .",
    "lines in bold show the estimated pdfs , while normal lines show the shapes of individual kernel functions @xmath16 ( gaussians ) .",
    "dots represent the data points @xmath17 .        choosing the best value of @xmath11 is not a trivial task and this problem was and still is extensively studied in literature @xcite , @xcite .",
    "currently available selectors can be roughly divided into 3 classes @xcite , @xcite .",
    "the first class uses very simple and easy to calculate mathematical formulas .",
    "they were developed to cover a wide range of situations , but do not guarantee being enough close to the optimal bandwidth .",
    "they are often called rules - of - thumb methods .",
    "the second class contains methods based on least squares and cross - validation ideas with more precise mathematical arguments , but they require much more computational effort .",
    "however , in reward for it , we get bandwidths more accurate for a wider range of density functions",
    ".    the third class contains methods based on plugging in estimates of some unknown quantities that appear in formulas for the asymptotically optimal bandwidth .",
    "one selected method from the third class is investigated in the paper .",
    "the method is briefly presented in chapter [ sec : plugin ] and from now on it will abbreviated as the plugin .",
    "in algorithm  [ alg : plugin ] we give recipe for calculation of the optimal bandwidth using the plugin method ( the symbols used are exactly such as in the book @xcite ) .",
    "all the necessary details on the method , as well as details on deriving of particular mathematical formulas can be found in many source materials , see for example books @xcite .",
    "it is important to stress that the plugin algorithm is a strictly _ sequential _ computational process ( see figure [ flowchart : of : plugin ] ; parallel processing is possible only internally in steps iv and vi ) as every step depends on the results obtained in the previous steps .",
    "first we calculate the variance and the standard deviation estimators of the input data , see step i in algorithm [ alg : plugin ] .",
    "then we calculate some more complex formulas from step ii to step vi .",
    "finally , we can substitute them into equation given in step vii to get the searched optimal bandwidth value @xmath11 .",
    "* step i : * _ calculate the estimates of variance ( @xmath18 ) and standard deviation ( @xmath19 _ ) : @xmath20    * step ii : * _ calculate the estimate @xmath21 of functional @xmath22 _ : @xmath23    * step iii : * _ calculate the bandwidth of the kernel estimator of function @xmath24 ( 4th derivative of function @xmath2 , that is @xmath25 ) _ : @xmath26 * step iv : * _ calculate the estimate @xmath27 of functional @xmath28 _ : @xmath29 ,   \\\\",
    "k^{6}(x )     & = \\frac{1}{\\sqrt{2\\pi}}\\left ( x^6 - 15x^4 + 45x^2 -15 \\right ) e^{-\\frac{1}{2}x^2}.    \\end{split}\\ ] ]    * step v : * _ calculate the bandwidth of the kernel estimator of function @xmath30 _ : @xmath31 * step vi : * _ calculate the estimate @xmath32 of functional @xmath33 _ :",
    "@xmath34 ,   \\\\",
    "k^{4}(x )     & =     \\frac{1}{\\sqrt{2\\pi}}\\left ( x^4 - 6x^2 + 3 \\right ) e^{-\\frac{1}{2}x^2}.    \\end{split}\\ ] ]    * step vii : * _ calculate the final value of the bandwidth @xmath11 _ : @xmath35        our implementation of the algorithm [ alg : plugin ] is carried out in fixed - point arithmetic ( see section [ sec : impl - prelim ] ) . unfortunately , using the raw data while conducting the required calculations , threatens a potential problems with overflow , especially while calculating the value of @xmath21 , see step ii in algorithm  [ alg : plugin ] .",
    "note that the estimate of standard deviation in @xmath21 is raised to the power of @xmath36 . for large values of @xmath37",
    "it results in extremely small values of @xmath21 .",
    "the above problems can be successfully overcome if the input datasets are _ standardized _ using the _ z - score _ formula , that is @xmath38 where @xmath39 and @xmath37 are mean and standard deviation of the original vector @xmath1 respectively .",
    "_ z - score _ guarantees that @xmath40 in @xmath21 and , consequently , @xmath21 entity has simply a constant value . applying the data standardization",
    "requires an extra operation on the @xmath11 value in step vii in algorithm  [ alg : plugin ] , that is @xmath41 where @xmath11 is the bandwidth calculated for the standardized dataset and @xmath19 is the standard deviation of the original vector @xmath1 .",
    "the correctness of the above equation can be easily proofed algebraically .    to reduce the calculation burden we can also slightly change equations @xmath27 and @xmath32 in algorithm  [ alg : plugin ] .",
    "it is easy to notice a symmetry , that is @xmath42    so , the double summations can be changed and , consequently , the final formula for @xmath27 has now the following form @xmath43 \\label{eq : plugin - psi6estimate-2}\\ ] ] ( note for different summation ranges , the , , 2  before sums and an extra factor added , that is @xmath44 ) . obviously , the same concerns @xmath45 and @xmath32 @xmath46 .",
    "\\label{eq : plugin - psi4estimate-2}\\ ] ]    computation complexity of steps iv and vi ( double summations ) , where the symmetry property is used , still belongs to @xmath0 complexity class @xmath47 where @xmath48 , and @xmath49 represents computation time for the differences , @xmath50 represents division time , and @xmath51 represents time for computing @xmath52 and @xmath45 polynomials .",
    "high level synthesis ( hls ) is an automated design process that interprets an algorithmic description of a problem ( given in high level languages c / c++ ) and translates this problem into a so called register - transfer level ( rtl ) hdl code .",
    "then in turn this hdl code can be easily synthesized to the gate level by the use of a logic synthesis tool , like for example _ xilinx ise design suite _ , _ xilinx vivado design suite _ , _ altera quartus ii_. in this paper we discuss results obtained using a tool called _ xilinx vivado high level synthesis _ , a feature of _ vivado design suite_. this tool supports c / c++ inputs , and generates vhdl / verilog / systemc outputs .",
    "other solutions are offered by _ scala _ programming language @xcite and a specialised high level synthesis language called _",
    "cx _ @xcite .",
    "it should also be mentioned that a similar tool called a++ is also available for altera fpga devices .      before implementing the plugin algorithm [ alg : plugin ] it is important to take some assumptions affecting both performance and resource consumption .    _",
    "the first assumption _ is about a proper arithmetic used .",
    "the floating - point one gives very good range and precision .",
    "unfortunately , from fpga s point of view , this representation is very resource demanding .",
    "in contrast , the fixed - point arithmetic is much less resource demanding but its range and precision are more limited .",
    "hence , the exact fixed point representation was determined based on a careful analysis of the particular intermediate values taken during calculations . if the input dataset does nt contain extremely large outliers ( which suggests that such dataset should be first carefully analyzed before any statistical analysis taken ) and if the _ z - score _ standardization is used , @xmath53 fixed point representation is sufficient for all calculations ( that is : integer part length @xmath54 , fractional part length @xmath55 , word length @xmath56 and the first bit represents the sign ) .",
    "note also that as a result of the _ z - score _ standardization , the vales of @xmath18 , @xmath19 , @xmath21 are constant and this significantly simplifies the calculations .",
    "the fractional part does give the required precision .",
    "however , the integer part must also be sufficiently large , as @xmath57 factors are present in the plugin algorithm . _ the second assumption _ is about choosing the most adequate methods for calculating individual steps in algorithm  [ alg : plugin ] .",
    "now it needs to be stressed that programming for fpga devices differs considerably from programming for cpus / gpus devices .",
    "fpga devices are built from a large number of simple logical blocks like : look up tables ( lut ) , flip - flops ( ff ) , block rams ( bram ) , specialized dsp units ( dsp ) .",
    "these blocks can be connected each other and can implement only relatively low - level logical functions ( the so called gates level ) . as a consequence , even very basic operations , like for examples the adder for adding two numbers must be implemented from scratch . in description of the plugin algorithm [ alg : plugin ]",
    "one can easily indicate such operators like ( a ) addition , ( b ) subtraction , ( c ) multiplication , ( d ) division , ( e ) reciprocal , ( f ) exponent , ( g ) logarithm .",
    "] , ( h ) power , ( i ) square roots , ( j ) higher order roots .",
    "our implementation utilizes the following methods : _ cordic _ @xcite,@xcite for calculating exponents and logarithms ,",
    "divisions were replaced by multiplications and reciprocals , difference operators were replaced by addition of negative operands . additionally , one extra implementation of the exponent function was used for calculations of @xmath52 and @xmath45 in algorithm [ alg : plugin ] .",
    "this implementation is based on the remez algorithm @xcite , @xcite , @xcite and is open to pipelining . as a consequence ,",
    "a significant speedup can be achieved during calculations of steps iv and vi in algorithm [ alg : plugin ] .",
    "it is also worth to note that the authors implementation of the division operator ( base on multiplications and reciprocals ; the reciprocal is based on the newton method ) is significantly faster than the default division operator available in vivado hls .",
    "moreover , the another advantage of using our own operators , is that no ipcore ( xilinx s library of many specialized functions available for fpga projects ) is needed . as a consequence ,",
    "the generated vhdl codes are more portable for fpga chips from different than xilinx vendors .    _ the third assumption _ during implementing of the plugin algorithm was to enable the nominal clock frequency of an fpga chip used ( see chapter [ sec : impl - results ] for details ) . during experiments",
    "it was turned out that the usage of the original division operator resulted in problems with reaching the required frequency .",
    "the authors original implementation of the division operator ( base on multiplications and reciprocals ) solved this problem .    _ the forth assumption _ was that all the input datasets must be stored in the bram memory , which are available in almost all current fpga chips .",
    "they have enough capacity to store truly large data , like even 500,000 elements or more .      in figure",
    "[ fig : plugin - unitis ] we show the scheme of the plugin implementation where all the main components are presented .",
    "they correspond literally to the seven steps shown in algorithm  [ alg : plugin ] .        figure  [ fig : one - block - architecture ] presents general architecture of the functional unit for computing @xmath32 ( step vi in algorithm [ alg : plugin ] ) .",
    "it is worth to note that the proper architecture of this unit must be reached during careful coding in vivado hls , using techniques like listed in section [ sec : impl - prelim ] .",
    "unit at the block - level view .",
    "the extra frame called _ ul - part _ shows the part of the step vi in algorithm  [ alg : plugin ] where _ loop unrolling _ can be used , width=491 ]    we developed three different versions of the plugin algorithm .    _",
    "the first implementation _ , called _ literal _ , is just a literal rewriting of algorithm  [ alg : plugin ] ( with the improvements ( [ eq : plugin - psi6estimate-2 ] ) and ( [ eq : plugin - psi4estimate-2 ] ) ) .",
    "no additional actions were taken toward optimization of both execution time and resource requirements .",
    "this version can operate with any unscaled input data ( assuming that all the inputs as well as all the internal results fulfil the fixed - point ranges that have been set ) .",
    "this version automatically ( vivado decides ) utilizes pipelining .",
    "however , the pipelining does nt make the implementation enough fast and additionally , large number of dsp blocks is used .",
    "ffs and luts usage is also quite big ( see table [ tab : resource - usage ] ) .    _ the second implementation _ , called _ minimal _ , is written so that it is optimized for resource utilization , mainly the dsp units . to reduce the number of the dsp units some dedicated functions for addition and multiplication",
    "are required .",
    "using vivado hls compiler s pragmas ( _ # pragma hls inline off _ ) pipelining can be disabled ( on default , during translation of the high level codes into hdl ones pipelining is enabled whenever it is possible ) .",
    "as can be observed in table [ tab : resource - usage ] , a significant reduction of the dsp units was achieved .",
    "it confirms the fact that vivado hls is very sensitive for the structure of the high level codes being translated into hdl ones .",
    "so that , to achieve good performance and resource usage many modifications of the high level codes are required .    _ the third implementation _ , called _ fast _ , is written so that it is optimized for time execution .",
    "addition and multiplication functions were implemented in two ways . in the first way ( similar as in _ minimal _ implementation )",
    "the pipelining is disabled , while in the second way it is enabled .",
    "the pipelined versions of the functions are used in steps iv and vi in algorithm [ alg : plugin ] as these two steps are crucial for the final performance .",
    "additionally , in these two steps a dedicated implementation of the exponent function was used ( based on remez algorithm which is more likely to pipelining ) .",
    "also , a technique known as _ loop unrolling _ was used in a manual manner ( see sample codes in figure [ fig : sample - loops ] ) .",
    "although vivado hls uses automatic loop unrolling , this feature does nt work correctly in our algorithm ( as it can operate with datasets of any size and the exact number of loops is not known in advance ) .",
    "the _ forth and fifth implementations _ used during experiments ( called _ cpu _ and _ gpu _ respectively ) are the ones implemented and investigated in @xcite .",
    "cpu implementation utilizes the sse ( streaming simd extensions ) of the current multicore cpus .      during all practical experiments the target _ xilinx virtex-7 xc7vx690tffg1761 - 2 _",
    "device was used .",
    "its nominal working frequency is 200mhz ( or 5 ns for a single clock tact ) .",
    "cpu implementation was run on _ intel processor i7 4790k 4.0 ghz_. _ geforce 480gtx _ graphics card was used for gpu implementation .",
    "_ vivado hls ver .",
    "2015.2 _ was used for developing all the fpga implementations .",
    "the summary of the _ resource consumption _ is given in table  [ tab : resource - usage ] .",
    "additionally , _ power consumption _ is included .",
    "it is a real power ( in watts ) taken by the fpga chip after physical implementation of the plugin algorithm using vivado design suite .",
    "the power consumption of the fpga implementations is significantly smaller comparing with the power consumption of the cpu and gpu implementations .",
    "the power consumption for the cpu and gpu used in our experiments are an average ( catalogue - like ) values .",
    ".resources usage for three different fpga implementations of the plugin algorithms as well as cpu and gpu implementations .",
    "additionally power consumption is included . [",
    "cols=\"^,^,^,^,^,^ \" , ]     the summary of the _ accuracy _ for three different implementations of the plugin algorithm is given in table  [ tab : accuracy ] .",
    "@xmath58 is the reference bandwidth calculated in double floating point arithmetic ( in c++ program , 1517 significant decimal digits ) .",
    "it is worth to note that the relative errors for _ literal _ , _ minimal _ and _ fast _ implementations are very small ( not more than @xmath59 ) . in practical applications",
    "such small values can be in fact neglected .",
    "cccc * n & @xmath60 & @xmath58 & * + 128 & 0.304902711650357 & 0.304902701728222 & 3.25e-06 + 256 & 0.227651247521862 & 0.227651285449348 & 1.67e-05 + 384 & 0.202433198224753 & 0.202433187549741 & 5.27e-06 + 512 & 0.242707096505910 & 0.242707026022425 & 2.9e-05 + 640 & 0.190442902734503 & 0.190443702342891 & 0.00042 + 768 & 0.175199386896566 & 0.175199406819444 & 1.14e-05 + 896 & 0.172251554206014 & 0.172251524317464 & 1.74e-05 + 1024 & 0.174044180661440 & 0.174044236921001 & 3.23e-05",
    "+ * *    * n & @xmath61 & @xmath58 & * + 128 & 0.304902980336919 & 0.304902701728222 & 9.14e-05 + 256 & 0.227651586290449 & 0.227651285449348 & 0.000132 + 384 & 0.202433346537873 & 0.202433187549741 & 7.85e-05 + 512 & 0.242707266006619 & 0.242707026022425 & 9.89e-05 + 640 & 0.190443017752841 & 0.190443702342891 & 0.000359 + 768 & 0.175199396442622 & 0.175199406819444 & 5.92e-06 + 896 & 0.172251742798835 & 0.172251524317464 & 0.000127 + 1024 & 0.174044403014705 & 0.174044236921001 & 9.54e-05 + *",
    "n & @xmath62 & @xmath58 & * + 128 & 0.304901758907363 & 0.304902701728222 & 0.000309 + 256 & 0.227651913650334 & 0.227651285449348 & 0.000276 + 384 & 0.202433891594410 & 0.202433187549741 & 0.000348 + 512 & 0.242707268567756 & 0.242707026022425 & 9.99e-05 + 640 & 0.190443484811112 & 0.190443702342891 & 0.000114 + 768 & 0.175199736841023 & 0.175199406819444 & 0.000188 + 896 & 0.172251721611246 & 0.172251524317464 & 0.000115 + 1024 & 0.174044031649828 & 0.174044236921001 & 0.000118 + * * * *    the summary of the _ scalability _ of different plugin algorithm implementations is presented in figure [ fig : fpga - implementation - speedup ] .",
    "scalability of the fpga implementations is nearly linear , which is a very welcome behavior .",
    "the corresponding results for _ cpu _ and _ gpu _ implementations can be found in @xcite .",
    "the figure is in fact a graphical summary of data given in table [ tab : execution - times ] .",
    "axis is used),width=453 ]    simplified source codes of the three fpga implementations are presented in figure [ fig : sample - loops ] .",
    "complete source codes ( c++ and resulted vivado hls translations into vhdl ) are available on request .",
    "the first version is just the literal implementation of the step vi in algorithm [ alg : plugin ] in c language .",
    "unfortunately , as can be observed in table [ tab : execution - times ] and in figure [ fig : fpga - implementation - speedup ] such implementation is very slow . in the second version multiplications and additions are realized using dedicated functions ( _ fadd _ , _ fmul _ ) .",
    "also a dedicated function for reciprocal operator was implemented . in the third version",
    "much more modification was implemented .",
    "first , loop unrolling was used , second , vivado hls pragmas were used and third , multiplications and additions were realized using dedicated functions with pipelining enabled ( _ pfadd _ , _ pfmul _ ) .    .... //",
    "literal implementation psi4_f1 : for ( i=0 ; i < n ; i++ ) {      psi4_f2 : for ( j = i+1 ; j < n ; j++ ) {          s = s + k4 ( ( ( x[i ] - x[j ] ) / g2 ) ) ;      } }    // minimal implementation rg2 = reciprocal ( g2 ) ; psi4_f1 : for ( i=0 ; i <",
    "n ; i++ ) {      psi4_f2 : for ( j = i+1 ; j <",
    "n ; j++ ) {          s = fadd ( s , k4 ( fmul ( fadd ( x[i ] , -x[j ] ) , rg2 ) ) ) ;      } }    // fast implementation rg2 = reciprocal ( g2 ) ; psi4_f1 : for ( i=0 ; i < n ; i++ ) {      psi4_f2 : for ( j = i+1 ; j < n ; j+=2 ) {                   # pragma hls expression_balance          # pragma hls pipeline                   if ( j = = i+1 ) tmp = 0.0 ;          if ( j < n ) { tmp1 = 0.0 ; tmp2 = 0.0 ; }          psi4_f1_b0 : {              tmp1a = pfadd ( x[i ] , -x[j ] ) ;              tmpva = pfmul ( tmp1a , rg2 ) ;              tmp1 = k4 ( tmpva ) ;          }          psi4_f1_b1 : {              if ( ( j+1 ) <",
    "n ) {                  tmp1b = pfadd ( x[i ] , -x[j+1 ] ) ;                  tmpvb = pfmul ( tmp1b , rg2 ) ;                  tmp2 = k4 ( tmpvb ) ;              }          }          if ( j < n ) { tmp = pfadd ( tmp , tmp1 ) ; tmp = pfadd ( tmp , tmp2 ) ; }          if ( j+2>=n ) s = pfadd ( s , tmp ) ;      } } ....",
    "hls tools are competitive with manual design techniques using hdls .",
    "implementation time of complex numerical algorithms can be essentially reduced ( comparing to direct coding in hdl languages ) .",
    "unfortunately , to obtain efficient fpga implementations , many changes to source codes are required , comparing to equivalent implementations for cpus and/or gpus .",
    "this is because fpga devices use specific primitives like dsp , bram , ff and lut blocks and programmers should control their utilization manually .",
    "however , this control is performed on the level of c / c++ codes , not the hdl ones .",
    "it is also worth to stress that using the hls approach allows to obtain implementations which are often faster than cpu and/or gpu counterparts .",
    "another crucial motivation for replacing gpu or cpu solutions by their fpga equivalents is power consumption .",
    "fpga can settle for single watts , while cpu or gpu counterparts typically take tens / hundreds of watts or even more .",
    "we would like to thank for useful discussions with colleagues at the institute of control and computation engineering ( issi ) of the university of zielona gra , poland .",
    "we would like also to thank to anonymous referees for useful comments on the preliminary version of this paper .",
    "the numerical results were done using the hardware and software available at the  fpga / gpu @xmath39-lab  located at the institute of control and computation engineering of the university of zielona gra , poland .",
    "andrzejewski , w. , gramacki , a. , gramacki , j. : graphics processing units in acceleration of bandwidth selection for kernel density estimation .",
    "869885 ( 2013 )    bachrach , j. vo , h. , richards , b. lee , y. , waterman , a. , avizienis , r. , wawrzynek , j. , asanovi , k. : chisel : constructing hardware in a scala embedded language , in : design automation conference , ieee , pp .",
    "1212  1221 , san francisco ( 2012 )                      lei , y. , dou , y. , dong , y. , zhou , j. , xia , f. : fpga implementation of an exact dot product and its application in variable - precision floating - point arithmetic .",
    "j. supercomput .",
    "64 , issue 2 , pp .  580605 ( 2013 )        pedro ferlin e. , silvrio lopes h. , erig lima , c.r . ,",
    "perretto , m. : prada : a high - performance reconfigurable parallel architecture based on the dataflow model , int . j. of high performance systems architecture , vol .  3 , no .  1 ,",
    "41 - 55 ( 2011 )                taherkhani , s. ; ever , e. ; gemikonakli , o. , : implementation of non - pipelined and pipelined data encryption standard ( des ) using xilinx virtex-6 fpga technology , computer and information technology ( cit ) , in : 10th international conference on computer and information technology , pp .",
    "1257  1262 , ieee press , bradford ( 2010 )"
  ],
  "abstract_text": [
    "<S> fpga technology can offer significantly higher performance at much lower power consumption than is available from cpus and gpus in many computational problems . </S>",
    "<S> unfortunately , programming for fpga ( using hardware description languages , hdl ) is a difficult and not - trivial task and is not intuitive for c / c++/java programmers . to bring the gap between programming effectiveness and difficulty the high level synthesis ( hls ) approach is promoting by main fpga vendors . </S>",
    "<S> nowadays , time - intensive calculations are mainly performed on gpu / cpu architectures , but can also be successfully performed using hls approach . in the paper </S>",
    "<S> we implement a bandwidth selection algorithm for kernel density estimation ( kde ) using hls and show techniques which were used to optimize the final fpga implementation . </S>",
    "<S> we are also going to show that fpga speedups , comparing to highly optimized cpu and gpu implementations , are quite substantial . </S>",
    "<S> moreover , power consumption for fpga devices is usually much less than typical power consumption of the present cpus and gpus .    </S>",
    "<S> * keywords : * fpga , high level synthesis , kernel density estimation , bandwidth selection , plug - in selector </S>"
  ]
}