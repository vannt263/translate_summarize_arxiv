{
  "article_text": [
    "the orthogonal procrustes problem ( opp ) is the least square problem on the stiefel manifold .",
    "the opp originates from the factor analysis in psychometrics during 1950s and 1960s @xcite .",
    "the major purpose is to determine an orthogonal matrix that rotates the factor matrix to best fit some hypothesis matrix .",
    "the balanced case of the opp was surveyed in multiple introductory textbooks such as @xcite .",
    "cccc + & rsr @xcite&lsr @xcite&sp @xcite + order of the & & & + complexity & & & + & lr @xcite&eb @xcite&gpi ( our ) + order of the & & & + complexity & & & +    recently , due to the wide applications of the orthogonal regression in computer science , see @xcite , solving the unbalanced opp ( uopp ) is under increasing concern .",
    "multiple approaches are proposed to solve uopp such as the expansion balanced algorithm ( eb ) , the right hand side and the left hand side relaxation ( rsr ) , ( lsr ) , the successive projection ( sp ) and the lagrangian relaxation ( lr ) . in @xcite , the eb method employs the expanded balanced opp as its objective function . in @xcite and @xcite respectively , the rsr and the lsr approaches update the solution row by row or column by column iteratively based on solving the least square regression with a quadratic equality constraint ( lsqe ) . in @xcite ,",
    "the sp method updates the solution column by column by virtue of the projection method combined with correction techniques ( pmct ) discussed by @xcite , which is efficient to solve lsqe . in @xcite ,",
    "the lr method solves uopp by selecting different lagrangian multipliers .",
    "all the approaches mentioned above could converge to the solution of uopp successfully , whereas they deal with more complex procedures , which represent high orders of complexity .",
    "furthermore , all these methods initialize the parameters deliberately to optimize their proposed algorithms .",
    "last but not least , all these approaches are unable to deal with a more general problem known as the quadratic problem on the stiefel manifold ( qpsm ) .    to address the referred deficiencies , we derive a novel generalized power iteration method ( gpi ) for qpsm in order to efficiently solve the orthogonal least square regression ( olsr ) and uopp with a random initial guess and concise computational steps . in sum , the proposed gpi method can deal with a more general problem known as qpsm than other approaches .",
    "furthermore , the experimental results show that the proposed gpi method not only takes much less cpu time for the convergence but becomes more efficient dealing with the data matrix of large dimension as well .",
    "* notations : * for any matrix @xmath2 , frobenius norm is defined as @xmath3 , where @xmath4 is the trace operator . for any positive integer @xmath5 , @xmath6 denotes a @xmath7 identity matrix .",
    "the power iteration method is an iterative algorithm to seek the dominant eigenvalue and the related eigenvector of any given symmetric matrix @xmath8 , where the dominant eigenvalue is defined as the greatest eigenvalue in magnitude .",
    "the power iteration can be performed as the following steps :    \\1 .",
    "random initialize a vector @xmath9 , which has a nonzero    component in the direction of the dominant eigenvector .",
    "update @xmath10 .",
    "calculate @xmath11 .",
    "update @xmath12 .",
    "iteratively perform the step 2 - 4 until convergence .",
    "+ the power iteration could be further extended to the orthogonal iteration ( also called subspace iteration or simultaneous iteration ) method to find the first @xmath13 dominant eigenvalues and their associated eigenvectors for the given matrix @xmath14 .",
    "the orthogonal iteration method could be described as the following iterative algorithm :    \\1 . initialization .",
    "random initialize @xmath15 .",
    "update @xmath16 .",
    "calculate @xmath17 via the compact qr factorization of    @xmath2 , where @xmath18 and @xmath19 .",
    "update @xmath20 .",
    "iteratively perform the step 2 - 4 until convergence .",
    "+ apparently , the orthogonal iteration method above indicates a normalization process , which is similar as the normalization in the power iteration method . when the matrix @xmath14 is positive semi - definite ( psd ) , the orthogonal iteration method is equivalent to solving the following optimization problem @xmath21 therefore , the orthogonal iteration method is equivalent to the following steps under the psd matrix @xmath14 :    \\1 .",
    "random initialize @xmath15 .",
    "update @xmath22 .",
    "calculate @xmath23 via the compact svd method of @xmath2 ,    where @xmath24 , @xmath25 and @xmath26 .",
    "update @xmath27 .",
    "iteratively perform the step 2 - 4 until convergence .",
    "+ from the observation , the solution of the above algorithm as @xmath28 differs from the solution of the orthogonal iteration method as @xmath29 by the form , where @xmath30 .",
    "however , the difference between the solutions of these two algorithms does nt affect the objective value of the problem ( [ e000 ] ) due to the following derivation @xmath31",
    "the stiefel manifold @xmath32 is a set of the matrices @xmath15 , which have orthonormal columns as @xmath33 .    in this section , a novel approach",
    "is derived to unravel the following quadratic problem on the stiefel manifold ( qpsm ) @xcite as @xmath34 where @xmath15 , @xmath35 and the symmetric matrix @xmath8 . in order to solve the problem ( [ e01 ] ) , qpsm in ( [ e01 ] ) can be further relaxed into @xmath36 where @xmath37 .",
    "the relaxation parameter @xmath38 is an arbitrary constant such that @xmath39 is a positive definite ( pd ) matrix . instead of the method of the lagrangian multipliers to deal with an optimization problem with orthogonal constraints",
    ", one may use a geometric optimization algorithm tailored to the stiefel manifold , such as , for example , the one surveyed in @xcite .",
    "accordingly , the lagrangian function for the problem ( [ e02 ] ) can be written as @xmath40    from eq .",
    "( [ e09 ] ) , we could obtain the kkt condition for the problem ( [ e02 ] ) as @xmath41   + which is difficult to solve directly .",
    "thus , motivated by @xcite and the power iteration method mentioned in section 2 , we could propose the following iterative algorithm :    \\1 .",
    "random initialize @xmath15 such that @xmath42 .",
    "update @xmath43 .",
    "calculate @xmath44 by solving the following problem @xmath45    \\4 .",
    "update @xmath46 .",
    "iteratively perform the step 2 - 4 until convergence .",
    "+ besides , a closed form solution of the problem ( [ e05 ] ) can be achieved by the following derivation .",
    "suppose the full svd of @xmath2 is @xmath47 with @xmath48 , @xmath49 and @xmath50 , then we have @xmath51 where @xmath52 with @xmath53 and @xmath54 being the @xmath55-th elements of the matrix @xmath56 and @xmath57 , respectively .",
    "note that @xmath58 , thus @xmath59 . on the other hand , @xmath60 since @xmath54 is a singular value of the matrix @xmath2 .",
    "therefore , we have @xmath61    apparently , the equality holds when @xmath62 . that is to say , @xmath63 reaches the maximum when the matrix @xmath64\\in\\mathbb{r}^ { k\\times m}$ ] . recall that @xmath65 , thus the optimal solution to the problem ( [ e05 ] ) can be represented as @xmath66\\mathbb{v}^t.\\label{eq1}\\ ] ] since eq .",
    "( [ eq1 ] ) is based upon the full svd of the matrix @xmath2 , eq .",
    "( [ eq1 ] ) can be rewritten as @xmath67 via the compact svd of the matrix @xmath2 , where @xmath68 with @xmath24 , @xmath25 and @xmath26 .",
    "m0.32m0.32m0.4 & & +    based on the above analysis , the generalized power iteration method ( gpi ) can be summarized in the algorithm [ alg1 ] .",
    "we will prove that the proposed algorithm [ alg1 ] converges monotonically to the local minimum of qpsm ( [ e01 ] ) .",
    "* input : * the symmetric matrix @xmath8 and the matrix @xmath69 . *",
    "initialize * a random @xmath15 satisfying @xmath42 and @xmath38 such that @xmath37 is a positive definite matrix .",
    "update @xmath70 .",
    "calculate @xmath23 via the compact svd method of @xmath2 where @xmath24 , @xmath25 and @xmath26 .",
    "update @xmath27 .",
    "iteratively perform the step 3 - 5 until the algorithm converges .",
    "step 5 of algorithm [ alg1 ] is an instance of a class of methods , called manifold retractions , to update a matrix on the stiefel manifold , that were discussed in details in @xcite .",
    "if the symmetric matrix @xmath71 is positive definite ( pd ) , then @xmath72 where @xmath73 and @xmath15 are arbitrary matrices.[lem1 ]    proof : since the matrix @xmath39 is positive definite ( pd ) , we could rewrite @xmath74 via cholesky factorization .",
    "therefore , we have the following proof for lemma [ lem1 ] as @xmath75 @xmath76    the algorithm [ alg1 ] decreases the value of the objective function in ( [ e01 ] ) monotonically in each iteration until it converges .",
    "[ thm2 ]    proof : suppose the updated @xmath29 is @xmath77 in the algorithm [ alg1 ] , then we have @xmath78 since @xmath77 is the optimal solution of the problem ( [ e05 ] ) .",
    "based on the fact that @xmath79 , eq . ( [ e06 ] ) can be further illustrated as @xmath80    based on lemma [ lem1 ] and eq .",
    "( [ e07 ] ) , we could infer that @xmath81 which indicates that the algorithm [ alg1 ] decreases the objective value of qpsm in ( [ e01 ] ) in each iteration until the algorithm converges .",
    "@xmath76    the algorithm [ alg1 ] converges to a local minimum of the qpsm problem ( [ e01 ] ) .    proof : since the algorithm [ alg1 ] performs based on solving the problem ( [ e05 ] ) in each iteration , the lagrangian function for the solution of the algorithm [ alg1 ] can be represented as @xmath82 therefore , the solution of the algorithm [ alg1 ] satisfies the following kkt condition @xmath83    generally speaking , the matrix @xmath2 will be updated by @xmath77 in each iteration under the algorithm [ alg1 ] . since the algorithm [ alg1 ] converges to the optimal solution @xmath29 i.e. @xmath84 due to theorem [ thm2 ] , eq .",
    "( [ e012 ] ) can be further formulated by substituting @xmath79 as @xmath85 by comparing eq .",
    "( [ e010 ] ) and ( [ e013 ] ) , we could draw the conclusion that the solution of the algorithm [ alg1 ] and the problem ( [ e02 ] ) satisfy the same kkt condition .",
    "therefore , the algorithm [ alg1 ] converges to a local minimum of qpsm ( [ e01 ] ) since the problems ( [ e01 ] ) and ( [ e02 ] ) are equivalent .",
    "@xmath76    m0.32m0.32m0.4 & & +    besides , the problem ( [ e05 ] ) has an unique solution under full column - rank matrix @xmath2 due to the uniqueness of the svd method .",
    "on the other hand , the experimental results in section 5 represent that the proposed gpi method uniformly converges to the same objective value with a large amount of random initial guesses .",
    "based on the unique solution of the problem ( [ e05 ] ) and the associated experimental results , it is rational to conjecture that the proposed gpi method converges to the global minimum of qpsm .",
    "the orthogonal least square regression ( olsr ) can be written as @xmath86 where the data matrix @xmath87 and the hypothesis matrix @xmath88 with @xmath89 .",
    "moreover , @xmath15 is the regression matrix and @xmath90 is the bias vector .",
    "obviously , @xmath91 is free from any constraint . by virtue of the extreme value condition w.r.t .",
    "@xmath91 , we can derive as @xmath92    by substituting the above result as @xmath93 , eq .",
    "( [ e00 ] ) can be simplified to the following form as @xmath94 where @xmath95 .",
    "accordingly , the problem ( [ eee ] ) can be further reformulated into @xmath96 in which @xmath97 apparently , eq . ( [ eeee ] ) is in the exact same form as qpsm in ( [ e01 ] ) .",
    "therefore , olsr in ( [ e00 ] ) can be solved via the algorithm [ alg1 ] .      with @xmath98 , @xmath99 and @xmath100",
    ", we name the optimization problem    @xmath101    \\1 .",
    "balanced orthogonal procrustes problem ( opp ) if and only if @xmath102 .    \\2 . unbalanced orthogonal procrustes problem ( uopp ) if and only if @xmath103 .",
    "[ 1 ] especially when @xmath104 serves as a column vector @xmath105 , the problem ( [ e1 ] ) degenerates to @xmath106 which is known as the least square problem with a quadratic equality constraint ( lsqe ) .      to solve the balanced opp ( @xmath102 ) ,",
    "we could expand eq .",
    "( [ e1 ] ) into @xmath107   + which is same as the problem ( [ e05 ] ) with treating @xmath108 .",
    "thus , the balanced opp has the analytical solution of the closed form ( [ eq1 ] ) .      when @xmath103 , uopp ( [ e1 ] ) can be expanded into @xmath109    denote @xmath110 and @xmath111 , then eq .",
    "( [ e3 ] ) is in the exact same form as qpsm ( [ e01 ] ) .",
    "based on the algorithm [ alg1 ] , the algorithm [ alg2 ] can be proposed to converge to a local minimum of uopp monotonically due to the theoretical supports proved in section 3 .",
    "* input : * the matrix @xmath99 and the matrix @xmath100 where @xmath103 . *",
    "initialize * @xmath18 and @xmath112 such that @xmath113 and the matrix @xmath114 is positive definite , respectively .",
    "* while * not converge * do * update matrix @xmath115 .",
    "calculate @xmath24 and @xmath116 via the compact svd of @xmath2 as @xmath68 .",
    "update @xmath117 .",
    "* end while * * return * @xmath104 .",
    "[ alg2 ]    m0.45m0.5 & +    cccccccc +    & rsr@xcite&lsr@xcite&sp@xcite&lr@xcite&eb@xcite&gpi(our ) + @xmath118&cpu time&64.940s&23.426s&2.386s&0.541s&0.337s&*0.228s * + @xmath119&cpu time&136.020s&21.635s&3.221s&1.134s & 0.347s&*0.226s * + @xmath120&cpu time&229.851s&20.560s&5.054s&1.806s & 0.445s&*0.273s * + & rsr@xcite&lsr@xcite&sp@xcite&lr@xcite&eb@xcite&gpi(our ) + @xmath118&cpu time&-&842.849s&132.232s&3.869s&11.440s&*1.290s * + @xmath119&cpu time&-&851.231s&196.761s&5.180s&12.534s&*1.434s * + @xmath120&cpu time&-&860.746s&260.132s&7.700s&12.625s&*1.575s * +    generally speaking , qpsm can not be reformulated into uopp while uopp could always be rewritten into qpsm .",
    "therefore , the gpi method is more general than other approaches , which can only cope with uopp .",
    "based on the experimental results involved in the next section , the proposed gpi method takes much less time to converge to the solution of uopp .",
    "in this section , we analyze and report the numerical results of the generalized power iteration method ( gpi ) represented by both the algorithm [ alg1 ] and the algorithm [ alg2 ] .",
    "we randomly choose the test data matrix with normally distributed singular values .",
    "m0.5    besides , the computer we use is macbook air , whose cpu is 1.4 ghz intel core i5 , ram is 4 gb 1600 mhz ddr3 and operating system is os x yosemite 10.10.5 .",
    "_ case 1 : ( parameter dependence ) _ firstly , we try to investigate the gpi method in the algorithm [ alg2 ] via varying the relaxation parameter @xmath112 .",
    "suppose @xmath121 is the largest eigenvalue of @xmath122 , then we can let @xmath123 such that @xmath114 is a positive definite matrix , where @xmath124 is an arbitrary constant .",
    "\\1 ) from the figure [ fig3 ] , we can further notice that although the convergence rate for the algorithm [ alg2 ] is inversely proportional to the value of @xmath112 , the relaxation parameter @xmath112 does not affect the uniform convergence of the gpi method .",
    "cccccccc + dimension & & & & & & & + @xmath125 & & & & & & & + & cpu & & & & & & + & time & & & & & & + & cpu & & & & & & + & time & & & & & & + & cpu & & & & & & + & time & & & & & & + & cpu & & & & & & + & time & & & & & & + & cpu & & & & & & + & time & & & & & & + & cpu & & & & & & + & time & & & & & & +    [ tab4 ]    _ case 2:(cpu time comparison for solving uopp ) _ secondly , we further investigate the proposed gpi method in the algorithm [ alg2 ] by comparing it with five existing approaches mentioned in section 1 as eb @xcite , rsr @xcite , lsr @xcite , sp @xcite and lr @xcite .    based on solving lsqe problem , rsr @xcite and lsr @xcite respectively",
    "update the solution row by row and column by column iteratively .",
    "eb @xcite utilizes the expanded balanced opp as the objective function .",
    "sp @xcite employs the projection method combined with correction techniques ( pmct ) @xcite .",
    "lr @xcite solves uopp by fixing different lagrangian multipliers .",
    "the proposed gpi method includes two terms as @xmath122 outside the loop and @xmath126 within the loop , whose orders of complexity are @xmath127 and @xmath128 , respectively . besides , these two terms have the highest orders of complexity for the proposed gpi method . besides",
    ", the order of the complexity for each method is shown in the table [ tab2.5 ] .",
    "the comparative results are based on fixing @xmath129 as the square matrix at first hand ( table [ tab3 ] ) and then extend @xmath129 to a more general case ( table [ tab4 ] ) afterwards .",
    "( mark @xmath130 in the table [ tab3 ] and the table [ tab4 ] represents that it takes too much time to record in the tables . )",
    "\\1 ) from the figure [ fig4 ] , we notice that the existing methods as eb @xcite , rsr @xcite , lsr @xcite , sp @xcite lr @xcite and the proposed gpi method converge to the same objective value under the same input data .",
    "besides , our proposed gpi method converges faster than other approaches during iteration .",
    "\\2 ) from the table [ tab2.5 ] , the proposed gpi method has the lowest order of complexity due to its succinct computational process to obtain the optimal solution . during the experiments , we observe that the iteration number @xmath131 for the lr method is usually very large for the convergence .",
    "thus , the time consumption for lr method is much larger than that for the proposed gpi method though orders of complexity for these two approaches seem close . besides",
    ", the gpi method becomes more efficient when @xmath5 ( the number of data ) is large .",
    "\\3 ) from the table [ tab3 ] , the proposed algorithm [ alg2 ] ( gpi ) serves as the most efficient method under the square matrix case .",
    "\\4 ) from the table [ tab4 ] , we can observe that lsr @xcite , sp @xcite and rsr @xcite are unable to compete with lr @xcite , eb @xcite and gpi due to the complex updating procedures including the expanded opp and solving lsqe .",
    "especially when the dimension increases , the superiority of our proposed gpi method would be more obvious .",
    "_ case 3:(cpu time comparison for solving lsqe ) _ finally , the projection method combined with correction techniques ( pmct ) @xcite is compared to the gpi method in the algorithm [ alg2 ] targeting at solving the least square regression with a quadratic equality constraint ( lsqe ) in ( [ ls ] ) .",
    "actually , solving lsqe ( [ ls ] ) is no different from solving uopp ( [ e1 ] ) under @xmath132 .",
    "\\1 ) from the figure [ fig9 ] , we can notice that pmct @xcite and the algorithm [ alg2 ] ( gpi ) converge to the same objective value though in terms of the different patterns .",
    "\\2 ) from the figure [ fig10 ] , the algorithm [ alg2 ] ( gpi ) takes much less time for convergence than pmct @xcite does .",
    "in this paper , we analyze the quadratic problem on the stiefel manifold ( qpsm ) by deriving a novel generalized power iteration ( gpi ) method . based on the proposed gpi method , two special and significant cases of qpsm known as the orthogonal least square regression and the unbalanced orthogonal procrustes problem are under further investigation . with the theoretical supports ,",
    "the gpi method decreases the objective value of the qpsm problem monotonically to a local minimum until convergence .",
    "eventually , the effectiveness and the superiority of the proposed gpi method are verified empirically . in sum",
    ", the proposed gpi method not only takes less cpu time to converge to the optimal solution with a random initial guess but becomes much more efficient especially for the data matrix of large dimension as well .",
    "green , b. : the orthogonal approximation of an oblique simple structure in factor analysis . psychometrika . * 17 * ( 1952 ) 429 - 440 .",
    "hurley , j. , cattell , r. : the procrustes program : producing direct rotation to test a hypothesized factor structure .",
    "behavioural science .",
    "* 6 * ( 1962 ) 258 - 262 .",
    "golub , g. h. , van loan , c. f. : matrix computations .",
    "the johns hopkins university press ( 1989 ) .",
    "thomas , v. : algorithms for the weighted orthogonal procrustes problem and other least squares problems : [ d ] .",
    "( 2006 ) ume  university , sweden .",
    "souza , p. , leite , c. , borges , h. , fonseca , r. : online algorithm based on support vectors for orthogonal regression .",
    "pattern recognition letters * 34 * ( 2013 ) 1394 - 1404 .",
    "chu , m. , trendafilov , n. : the orthogonally constrained regression revisted .",
    "j. comput . graph",
    ". stat . * 10 * ( 2001 ) 746 - 771 .",
    "green , b. , goers , j. : a problem with congruence .",
    "the annual meeting of the psychometric society .",
    "monterey , california ( 1979 ) .",
    "park , h. : a parallel algorithm for the unbalanced orthogonal procrustes problem .",
    "parallel computing .",
    "* 17 * ( 1991 ) 913 - 923 .",
    "bojanczyk , a. , lutoborski , a. : the procrustes problem for orthogonal stiefel matrices .",
    "* 21 * ( 1999 ) 1291 - 1304 .",
    "zhang , z. , du , k. : successive projection method for solving the unbalanced procrustes problem .",
    "science in china : series a mathematics .",
    "* 49 * ( 2006 ) 971 - 986 .",
    "xia , y. , han , y. : partial lagrangian relaxation for the unbalanced orthogonal procrustes problem .",
    "* 79 * ( 2014 ) 225 - 237 .",
    "zhang , z. , huang , y. : a projection method for least square problems with quadratic equality constraint .",
    "* 25 * ( 2003)188 - 212 .",
    "journe , m. , nesterov , y. , richtrik , p. , sepulchre , r. : generalized power method for sparse principal component analysis .",
    "journal of machine learning research .",
    "* 11 * ( 2008 ) 517 - 553 .",
    "fiori , s. : formulation and integration of learning differential equations on the stiefel manifold .",
    "ieee transactions on neural networks .",
    "* 16 * ( 2005 ) 1697 - 1701 .",
    "kaneko , t. , fiori , s. , tanaka , t. : empirical arithmetic averaging over the compact stiefel manifold .",
    "ieee transactions on signal processing .",
    "* 61 * ( 2013 ) 883 - 894 .",
    "nie , f. , yuan , j. , huang , h. : optimal mean robust principal component analysis . in proc .",
    "icml . ( 2014 ) 2755 - 2763 ."
  ],
  "abstract_text": [
    "<S> in this paper , we first propose a novel generalized power iteration method ( gpi ) to solve the quadratic problem on the stiefel manifold ( qpsm ) as @xmath0 @xmath1 along with the theoretical analysis . </S>",
    "<S> accordingly , its special case known as the orthogonal least square regression ( olsr ) is under further investigation . </S>",
    "<S> based on the aforementioned studies , we then cast major focus on solving the unbalanced orthogonal procrustes problem ( uopp ) . as a result , not only a general convergent algorithm is derived theoretically but the efficiency of the proposed approach is verified empirically as well .    </S>",
    "<S> quadratic problem , stiefel manifold , power iteration , procrustes problem , orthogonal least square regression . </S>"
  ]
}