{
  "article_text": [
    "in the era of big data , traditional statistical learning methods are facing two significant challenges : ( 1 ) how to scale current machine learning methods to the large - scale data ? and",
    "( 2 ) how to obtain accurate inference results when the data are noisy and may even contain malicious outliers ?",
    "these two important challenges naturally lead to a need for developing _ scalable robust _ learning methods .",
    "traditional robust learning methods generally rely on optimizing certain robust statistics  @xcite or applying some sample trimming strategies  @xcite , whose calculations require loading all the samples into the memory or going through the data multiple times  @xcite .",
    "thus , the computational time of those robust learning methods is usually at least linearly dependent on the size of the sample set , @xmath0 .",
    "for example , in rpca  @xcite , the computational time is @xmath1 where @xmath2 is the intrinsic dimension of the subspace and @xmath3 is the ambient dimension . in robust linear regression  @xcite ,",
    "the computational time is super - linear on the sample size : @xmath4 .",
    "this rapidly increasing computation time becomes a major obstacle for applying robust learning methods to big data in practice , where the sample size easily reaches the terabyte or even petabyte scale .",
    "online learning and distributed learning are natural solutions to the scalability issue .",
    "most of existing online statistical learning methods propose to optimize a surrogate function in an online fashion , such as employing stochastic gradient descent @xcite to update the estimates , which however can not handle the outlier samples in the streaming data @xcite .",
    "similarly , most of existing distributed learning approaches ( _ e.g. _ , mapreduce  @xcite ) are not robust to contamination from outliers , communication errors or computation node breakdown .    in this work",
    ", we propose an online robust learning ( orl ) framework to efficiently process big data with outliers while preserving robustness and statistical consistency of the estimates .",
    "the core technique is based on two - level online learning procedure , one of which employs a novel median filtering process .",
    "the robustness of median has been investigated in statistical estimations for heavy - tailed distributions @xcite . however , to our best knowledge , this work is among the first to employ such estimator to deal with outlier samples in the context of online learning .",
    "the implementation of orl follows mini - batch based online optimization which is popular in a wide range of machine learning problems ( _ e.g. _ , deep learning , large - scale svm ) from large - scale data . within each mini - batch",
    ", orl computes an independent estimate .",
    "however , outliers may be heterogeneously distributed on the mini - batches and some of them may contain overwhelmingly many outliers . the corresponding estimate will be arbitrarily bad and break down the overall online learning .",
    "therefore , on top of such streaming estimates orl performs another level of robust estimationmedian filteringto obtain reliable estimate .",
    "the orl approach is general and compatible with many popular learning algorithms . besides its obvious advantage of enhancing the computation efficiency for handling big data",
    ", orl incurs negligible robustness loss compared to centralized ( and computationally unaffordable ) robust learning methods .",
    "in fact , we provide analysis and demonstrate that orl is robust to a constant fraction of `` bad '' estimates generated in the streaming mini - batches that are corrupted by outliers .",
    "we specify the orl approach for two concrete problemsonline robust principal component analysis ( pca ) and linear regression .",
    "comprehensive experiments on both synthetic and real large scale datasets demonstrate the efficiency and robustness advantages of the proposed orl approach .",
    "in addition , orl can be adapted straightforwardly to distributed learning setting and offers additional robustness to corruption of several computation nodes or communication errors , as demonstrated in the experiments .",
    "in short , we make following contributions in this work .",
    "first , we develop an outlier robust online learning framework which is the first one with provable robustness to a constant fraction of outliers .",
    "secondly , we introduce two concrete online robust learning approaches , one for unsupervised learning and the other for supervised learning .",
    "other examples can be developed in a similar way easily .",
    "finally , we also present the application of the orl approach to distributed learning setting which is equally attractive for learning from large scale data .",
    "we consider a set of @xmath5 observation samples @xmath6 , which contains a mixture of @xmath7 authentic samples @xmath8 and @xmath9 outliers @xmath10 . the authentic samples are generated according to an underlying model ( _ i.e. _ , the ground truth ) parameterized by @xmath11 .",
    "the target of a statistical learning procedure is to estimate the model parameter @xmath12 according to the provided observations @xmath13 . throughout the paper , we assume the authentic samples are sub - gaussian random vectors in @xmath14 , which thus satisfy that @xmath15 for some @xmath16 . here",
    "@xmath17 denotes the unit sphere . in this work ,",
    "we focus on the case where a constant fraction of the observations are outliers , and we use @xmath18 to denote this outlier fraction . in the context of online learning , samples are provided in a sequence of @xmath19 mini batches , each of which contains @xmath20 observations .",
    "denote the sequence as @xmath21 .",
    "the target of online statistical learning is to estimate the parameter @xmath12 , only based on the observations revealed so far .",
    "we first introduce the _ geometric median _ herea core concept underlying the median filtering procedure that is important for developing the proposed online robust learning approach .",
    "[ def : median ] given a finite collection of i.i.d .",
    "estimates @xmath22 , their geometric median is the point which minimizes the total @xmath23 distance to all the given estimates , _",
    "i.e. _ , @xmath24    an important property of the geometric median is that it indeed aggregates a collection of independent estimates into a single estimate @xmath25 with strong concentration guarantees , even in presence of a constant fraction of outlying estimates in the collection .",
    "the following lemma , straightforwardly derived from lemma 2.1 in  @xcite , characterizes such robustness property of the geometric median .",
    "[ lemma : median ] let @xmath25 be the geometric median of the points @xmath26 .",
    "fix @xmath27 and @xmath28 .",
    "suppose there exists a subset @xmath29 of cardinality @xmath30 such that for all @xmath31 and any point @xmath32 , @xmath33 .",
    "then we have @xmath34 .    in words , given a set of points",
    ", their geometric median will be close to the `` true '' @xmath35 as long as at least half of them are close to @xmath35 .",
    "in particular , the geometric median will not be skewed severely even if some of the points deviate significantly away from @xmath35 .",
    "in this section , we present how to scale up robust learning algorithms to process large - scale data ( containing outliers ) through online learning without losing robustness .",
    "we term the proposed approach as online robust learning ( orl ) .",
    "the idea behind orl is intuitiveinstead of equally incorporating generated estimates at each time step , orl aggregates the sequentially generated estimates by mini - batch based learning methods via an _ online _ computation of the robust geometric median .",
    "basically , orl runs online learning at two levels : at the bottom level , orl employs appropriate robust learning procedures @xmath36 with parameter @xmath37 ( _ e.g. _ , robust pca algorithms on a mini - batch of samples ) to obtain a sequence of estimates @xmath38 of @xmath12 based on the observation mini - batch @xmath39 ; at the top level , orl updates the running estimate @xmath40 ( @xmath41 ) through a geometric median filtering algorithm ( explained later ) over @xmath42 and outputs a _ robust _ estimate after going through all the mini - batches .",
    "intuitively , according to lemma [ lemma : median ] , as long as a majority of mini - batch estimates are not skewed by outliers , the produced @xmath43 would be robust and accurate .",
    "this new two - level robust learning gives orl stronger robustness to outliers compared with ordinary online learning .    to develop the top level geometric median filtering procedure , recall definition of the geometric median in  .",
    "a natural estimate of the geometric median @xmath25 is the minimum @xmath44 of the following empirical loss function @xmath45 : @xmath46 the empirical function @xmath47 is differentiable everywhere except for the points @xmath48 , and can be optimized by applying stochastic gradient descent ( sgd ) @xcite . more concretely , at the time step @xmath49 ,",
    "given a new estimate @xmath50 ( based on the @xmath51-st mini - batch ) and the current estimate @xmath52 , orl computes the gradient at point @xmath53 of the empirical function @xmath47 in eqn .",
    "evaluated only at @xmath50 : @xmath54 then orl updates the estimate @xmath40 by following filtering : @xmath55 here @xmath56 is a predefined step size parameter which usually takes the form of @xmath57 with a constant @xmath58 characterizing convexity of the empirical function to optimize . besides , @xmath59 controls contribution of each new estimate @xmath50 conservatively in updating the global estimate @xmath25 .",
    "details of orl are provided in algorithm  [ alg : rol ] .",
    "* input * : mini - batch sequence @xmath60 , convexity parameter @xmath58 , robust learning procedure parameter @xmath37 . * initialization * : @xmath61 . call the robust learning procedure @xmath62 ; compute weight @xmath63 with @xmath64 . update the estimate : @xmath65 .",
    "* output * : final estimate @xmath66 .",
    "another level of filtering is important .",
    "certain mini - batches may contain overwhelming outliers .",
    "therefore , even though a robust learning procedure is employed on each mini - batch , the resulted estimate can not be guaranteed to be accurate .",
    "in fact , a mini - batch containing over @xmath67 outliers would corrupt any robust learning procedurethe resulted estimate can be arbitrarily bad and breakdown the overall online learning . to address this critical issue , orl performs another level of online learning for updating the `` global '' estimate with adaptive weights for the new estimate and `` filters out '' possibly bad estimates .",
    "we provide the performance guarantees for orl in this section . throughout this section ,",
    "we use following asymptotic inequality notations : for positive numbers @xmath68 and @xmath69 , the asymptotic inequality @xmath70 means that @xmath71 where @xmath72 is a constant only depending on @xmath73 .",
    "suppose @xmath74 samples , a constant fraction of which are authentic ones and have sub - gaussian distributions as specified in for some @xmath16 , are evenly divided to @xmath19 mini - batches and outlier fractions on the @xmath19 mini - batches are @xmath75 respectively .",
    "let @xmath76 be a collection of independent estimates of @xmath12 output by implementing the robust learning procedure @xmath77 on the @xmath19 mini - batches independently .",
    "we assume an estimate or the robust learning procedure provides following composite deviation bound , @xmath78 where @xmath79 is the size of each mini - batch whose value can be tuned by the desired accuracy ( _ e.g. _ , through data augmentation ) .",
    "we will specify value of the constant depending on @xmath80 and @xmath81 explicitly in concrete applications .",
    "the above bound indicates the estimation error depends on the standard statistically error and the outlier fraction .",
    "if @xmath82 is overwhelmingly large , the estimate will be arbitrarily bad .",
    "we now proceed to demonstrate that the orl approach is robust to outlierseven on a constant fraction of mini - batches , the obtained estimates are not good , orl can still provide reliable estimate with bounded error . given a sequence of estimates @xmath83 produced internally in orl , we analyze and provide guarantee on performance of the orl through following two steps .",
    "we first demonstrate the geometric median function @xmath84 is in fact strongly convex and thus geometric median filtering provides a good estimate of the `` true '' geometric median of @xmath85 .",
    "then we derive following performance guarantee for orl by invoking the robustness property of geometric median .",
    "[ prop : rol ] suppose in total @xmath74 samples , a constant fraction of which have sub - gaussian distribution as in , are divided into @xmath19 sequential mini batches of size @xmath79 with outlier fraction @xmath86 . here",
    "we run a base robust learning algorithm having a deviation bound as in on each mini batch .",
    "denote the ground truth of the parameter to estimate as @xmath12 and the output of orl ( alg .",
    "[ alg : rol ] ) as @xmath44 . then with probability at least @xmath88",
    ", @xmath44 satisfies : @xmath89 here @xmath90 and @xmath91 denotes the @xmath92 smallest outlier fraction in @xmath93 with @xmath94 .",
    "the above results demonstrate the estimation error of orl consists of two components .",
    "the first term accounts for the deviation between the solution @xmath44 and the `` true '' geometric median of the @xmath95 sequential estimations .",
    "when @xmath96 is sufficiently large , _",
    "i.e. _ , after orl seeing sufficiently many mini batches of observations , this error vanishes at a rate of @xmath97 .",
    "the second term explains the deviation of geometric median of estimates from the ground truth .",
    "the significant part of this result is that the error of orl only depends on the @xmath98 smallest outlier fraction among @xmath19 mini - batches , no matter how severely the other estimates are corrupted .",
    "this explains why orl is _ robust _ to outliers in the samples .",
    "in this section , we provide two concrete examples of the orl approach , including one unsupervised learning algorithmprincipal component analysis ( pca ) and one supervised learning onelinear regression ( lr ) .",
    "both algorithms are popular in practice but their online learning versions with robustness guarantees are still absent .",
    "finally , we also discuss an extension of orl for distributed robust learning .",
    "classical pca is known to be fragile to outliers and many robust pca methods have been proposed so far ( see  @xcite and references therein ) .",
    "however , most of those methods require to load all the data into memory and have computational cost ( super-)linear in the sample size , which prevents them from being applicable for big data . in this section ,",
    "we first develop a new robust pca method which robustifies pca via a robust sample covariance matrix estimation , and then demonstrate how to implement it with the orl approach to enhance the efficiency .    given a sample matrix @xmath99 \\in \\mathbb{r}^{p \\times n}$ ] ,",
    "the standard covariance matrix is computed as @xmath100 , _",
    "i.e. _ , @xmath101 . here",
    "@xmath102 denotes the @xmath103th row vector of matrix @xmath104 . to obtain a robust estimate of the covariance matrix",
    ", we replace the vector inner product by a trimmed inner product , @xmath105 , as proposed in @xcite for linear regressor estimation .",
    "intuitively , the trimmed inner product removes the outliers having large magnitude and the remaining outliers are bounded by inliers .",
    "thus , the obtained covariance matrix , after proper symmetrization , is close to the authentic sample covariance . how to calculate the trimmed inner product for a robust estimation of sample covariance matrix is given in algorithm [ alg : trimmed_ip ] .",
    "two vectors @xmath106 and @xmath107 , trimmed parameter @xmath9 .",
    "compute @xmath108 .",
    "sort @xmath109 in ascending order and select the smallest @xmath110 ones .",
    "let @xmath111 be the set of selected indices .",
    "@xmath112 .",
    "then we perform a standard eigenvector decomposition on the covariance matrix to produce the principal component estimations .",
    "the details of the new robust covariance pca ( rc - pca ) algorithm are provided in algorithm [ alg : rpca ] .",
    "sample matrix @xmath113 \\in \\mathbb{r}^{p\\times n}$ ] , subspace dimension @xmath114 , outlier fraction @xmath115 .",
    "compute the trimmed covariance matrix @xmath116 : @xmath117 .",
    "perform eigen decomposition on @xmath118 and take the eigenvectors corresponding to the @xmath114 largest eigenvalues @xmath119 $ ] .",
    "column subspace projector @xmath120 .",
    "applying the proposed orl approach onto the above rc - pca develops a new online robust pca algorithm , called orl - pca , as explained in algorithm [ alg : rol_pca ] .",
    "* input * : sequential mini - batches @xmath121 with size @xmath79 , subspace dimension @xmath122 , rc - pca parameter @xmath123 . * initialization * : @xmath124 .",
    "perform rc - pca on @xmath125 : @xmath126 ; compute covariance matrix : @xmath127 ; compute @xmath128 with @xmath64 ; update the estimate @xmath129 .",
    "* output * : @xmath130 @xmath131 .    based on the above result ,",
    "along with proposition [ prop : rol ] , we provide the following performance guarantee for orl - pca .",
    "[ theo : rol_pca ] suppose samples are divided into @xmath19 mini - batches of size @xmath79 .",
    "authentic samples satisfy the sub - gaussian distribution with parameter @xmath81 .",
    "let @xmath132 where @xmath91 is the @xmath133 smallest outlier fraction out of the @xmath95 mini - batches .",
    "let @xmath134 denote the projection operator given by orl - pca , and @xmath135 denotes the projection operator to the ground truth @xmath122 dimensional subspace .",
    "then , with a probability at least @xmath136 , we have , @xmath137 here @xmath138 are positive constants .",
    "we then showcase another example of the application of orlonline robust regression . as aforementioned",
    ", the target of linear regression is to estimate the parameter @xmath12 of linear regression model @xmath139 given the observation pairs @xmath140 where @xmath141 samples are corrupted .",
    "here @xmath142 is additive noise .",
    "similar to orl - pca , we use the robustified thresholding ( rotr ) regression proposed in algorithm [ alg : rotr ] ( ref .",
    "@xcite ) as the robust learning procedure for parameter estimation within each mini - batch .",
    "covariate matrix @xmath143 \\in \\mathbb{r}^{p\\times ( n+n_1)}$ ] and response @xmath144 , outlier fraction @xmath115 ( set as @xmath145 if unknown ) . for @xmath146 ,",
    "compute @xmath147 . @xmath25 .    due to the blessing of online robust learning framework , orl - lr has the following performance guarantee .",
    "[ theo : rol_sr ] adopt the notations in theorem [ theo : rol_pca ] .",
    "suppose the authentic samples have the sub - gaussian distribution as in with noise level @xmath148 , are divided into @xmath19 sequential mini - batches .",
    "let @xmath44 be the output of orl - lr and @xmath12 be the ground truth .",
    "then , with probability at least @xmath136 , the following holds : @xmath149      following the spirit of orl , we can also develop a distributed robust learning ( drl ) approach .",
    "suppose in a distributed computing platform , @xmath150 machines are usable for parallel computation .",
    "then for processing a large scale dataset , one can evenly distribute them onto the @xmath150 machines and run robust learning procedure @xmath151 in parallel .",
    "each machine provides an independent estimate @xmath152 for the parameter of interest @xmath35 . aggregating these estimates via geometric median ( ref .",
    "eqn .  ) can provide additional robustness to the inaccuracy , breakdown and communication error for a fraction of machines in the computing cluster , as stated in lemma [ lemma : median ] . of particular interest",
    ", drl can provide much stronger robustness than the commonly used averaging over the @xmath150 estimates , as average or mean is notoriously fragile to corruption .",
    "even a single corrupted estimate out of the @xmath150 estimates can make the final estimate arbitrarily bad .",
    "in this section , we investigate robustness of the orl approach by evaluating the orl - pca and orl - lr algorithms and comparing them with their centralized and non - robust counterparts .",
    "we also perform similar investigation on drl ( ref .",
    "section [ sec : drl ] ) considering robustness is also critical for distributed learning in practice . in the simulations , we report the results with the outlier fraction which is computed as @xmath153",
    ".    * data generation * in simulations of the pca problems , samples are generated according to @xmath154 . here",
    "the signal @xmath155 is sampled from the normal distribution : @xmath156 .",
    "the noise @xmath157 is sampled as : @xmath158 .",
    "the underlying matrix @xmath159 is randomly generated whose columns are then orthogonalized .",
    "the entries of outliers @xmath160 are i.i.d .",
    "random variables from uniform distribution @xmath161 $ ] .",
    "we use the distance between two projection matrices to measure the subspace estimation error for pca : @xmath162 . here",
    "@xmath120 is the output estimates and @xmath163 is the ground truth .    in simulations of the lr problems , samples",
    "@xmath164 are generated according to @xmath165 . here",
    "the model parameter @xmath35 is randomly sampled from @xmath166 , and @xmath167 is also sampled from normal distribution : @xmath168 .",
    "the noise @xmath169 is sampled as : @xmath170 .",
    "the entries of outlier @xmath171 are also i.i.d .  randomly sampled from uniform distribution",
    "@xmath161 $ ] .",
    "the response of outlier is generated by @xmath172 .",
    "we use @xmath173 to measure the error . here",
    "@xmath25 is the output estimate .    *",
    "online setting * results shown in figure [ subfig : pca_non_uniform ] give following observations .",
    "first , orl - pca converges to comparable performance with batch rc - pca with accesses to the entire data set .",
    "this demonstrates the rapid convergence of orl - pca .",
    "it is worth noting that orl - pca saves considerable memory cost than batch rc - pca ( @xmath174 mb _ vs. _",
    "@xmath175 mb ) and computation time ( @xmath176 seconds vs.  @xmath177 hours ) since orl - pca performs svd on much smaller data .",
    "secondly , orl - pca offers much stronger robustness than naively averaged aggregation when outlier order is adversarial to corrupt a fraction of mini - batches .",
    "as shown in figure [ subfig : pca_non_uniform ] , when some batches have overwhelming outliers ( outlier fraction @xmath178 ) , base rc - pca fails on these batches and outputs completely corrupted estimations .",
    "the corruption of mini - batches also fails online averaging rpca .",
    "in contrast , orl - pca still offers correct estimation , even when a fraction of @xmath179 of estimates from mini batches are corrupted .",
    "we also report the results of orl - lr and comparison with online - averaging baselines in figure [ subfig : online_lr ] .",
    "similar to orl - pca , one observes that orl - lr offers outperforming robustness to the sample outliers and batch corruptions , in contrast to the naive averaging algorithm .    *",
    "distributed setting * all the simulations are implemented on a pc with @xmath180ghz quad cpu and @xmath181 gb ram .",
    "it takes centralized rpca around @xmath182 seconds to handle @xmath183 samples with dimensionality of @xmath184 .",
    "in contrast , distributed rpca only costs @xmath185 seconds by using @xmath186 parallel procedures .",
    "the communication cost here is negligible since only eigenvector matrices of small sizes are communicated . for rlr simulations",
    ", we also observe about efficiency enhancement .    as for the performance , from fig .",
    "[ fig : drpca ] , we observe that when @xmath187 , drl - rpca , rpca with division - averaging ( div .- avg .",
    "rpca ) and centralized rpca ( _ i.e. _ , the rc - pca ) achieve similar performances , which are much better than non - robust standard pca .",
    "when @xmath188 , _",
    "i.e. _ , when there are no outliers , the performances of drl - rpca and div .- avg .",
    "rpca are slightly worse than standard pca as the quality of each mini - batch estimate deteriorates due to the smaller sample size .",
    "however , distributed algorithms of course offer significant higher efficiency .",
    "similar observations also hold for lr simulations from fig .  [",
    "fig : drlr ] . actually , standard pca and lr begin to break down when @xmath189 .",
    "these results demonstrate that drl preserves the robustness of centralized algorithms well .",
    "when outlier fraction @xmath115 increases to @xmath185 , centralized ( blue lines ) and division - averaging algorithms ( green lines ) break down sharply , as the outliers outnumber their maximal breakdown point of @xmath190 .",
    "in contrast , drl - rpca and drl - rlr still present strong robustness and perform much better , which demonstrate that the drl framework is indeed robust to computing nodes breaking down , and even enhances the robustness of the base robust learning methods under favorable outlier distributions across the machines .",
    "* comparison with averaging * taking the average instead of the geometric median is a natural alternative to drl .",
    "here we provide more simulations for the rpca problem to compare these two different aggregation strategies in the presence of different errors on the computing nodes .    in distributed computation of learning problems , besides outliers ,",
    "significant deterioration of the performance may result from unreliabilities , such as latency of some machines or communication errors .",
    "for instance , it is not uncommon that machines solve their own sub - problem at different speed , and sometimes users may require to stop the learning before all the machines output the final results . in this case ,",
    "results from the slow machines are possibly not accurate enough and may hurt the quality of the aggregated solution .",
    "similarly , communication errors may also damage the overall performance .",
    "we simulate the machine latency by stopping the algorithms once over half of the machines finish their computation . to simulate communication error , we randomly sample @xmath191 estimations and flip the sign of @xmath192 of the elements in these estimations .",
    "the estimation errors of the solution aggregated by averaging and drl are given in table [ tab : avg - compare ] .",
    "clearly , drl offers stronger resilience to unreliability of the computing nodes .",
    ".comparisons of the estimation error for pca between division - averaging ( div .- avg . ) and drl , with machine latency and communication errors . under the same parameter setting as figure  [ fig : drpca ] .",
    "outlier fraction @xmath193 .",
    "the average and std of the error from @xmath194 repetitions are reported . [",
    "cols=\"^,^,^\",options=\"header \" , ]     we perform experiments with the online learning setting , and compare the performance of the proposed orl - lr with the online averaging lr .",
    "we also implement a non - robust baseline  ",
    "stochastic gradient descent to solve the lr problem .",
    "the size of min - batch is fixed as @xmath195 images . from the results in table [ tab : flickr - acc_orl ]",
    ", one can observe that orl - lr achieves significantly higher accuracy than non - robust baseline algorithms , with a margin of more than @xmath196 .",
    "[ lemma : hoeffding ] let @xmath197 be independent random variables taking values in @xmath198 $ ] .",
    "let @xmath199 and @xmath200 .",
    "then for @xmath201 , @xmath202    [ lemma : coupling ] let @xmath203 be independent random variables , let @xmath204 be a real number and let @xmath205 .",
    "let @xmath206 $ ] such that , for all @xmath207 , @xmath208 and let @xmath209 be a random variable with a binomial law @xmath210 .",
    "there exists a coupling @xmath211 such that @xmath212 has the same distribution as @xmath213 , @xmath214 has the same distribution as @xmath209 and such that @xmath215 . in particular , for all @xmath216 , @xmath217 .",
    "the following lemma demonstrates that aggregating estimates via their geometric median can enhance the confidence significantly .",
    "[ lemma : median_aggregation ] given @xmath218 independent estimates of @xmath12 satisfying @xmath219 , for all @xmath220 .",
    "let @xmath221 , then we have , @xmath222 here @xmath223 and @xmath224 are defined in lemma [ lemma : median ] .    according to lemma [ lemma : median ] , we have @xmath225 let @xmath226 , and let @xmath227 have binomial distribution @xmath228 , then @xmath229 according to lemma [ lemma : coupling ] . applying the hoeffding s inequality in lemma [ lemma : hoeffding ] ( with @xmath230 , @xmath231 , @xmath232 and @xmath233 )",
    "gives @xmath234        we suppose from now on following conditions hold .",
    "[ cond1 ] assume that @xmath235 is the parameter of interest .",
    "let @xmath236 be a collection ofindependent estimates of @xmath12 , which are not concentrated on a straight line : for all @xmath237 , there is @xmath238 such that @xmath239 and @xmath240 with @xmath241 .",
    "as noted in  @xcite , condition [ cond1 ] ensures that geometric median @xmath242 of the @xmath19 estimates is uniquely defined .",
    "[ cond2 ] the distribution of the independent estimates of @xmath12 is a mixing of two `` nice '' distributions : @xmath243 .",
    "here @xmath244 is not strongly concentrated around single points : if @xmath245 is the ball @xmath246 , and @xmath247 is a random variable with distribution @xmath244 , then for any constant @xmath248 ,",
    "@xmath249 \\leq c_a.\\ ] ] in addition , @xmath250 is a discrete measure , @xmath251 . here @xmath252 is a dirac measure at point @xmath253 .",
    "we denote by @xmath254 the support of @xmath250 and assume that the median @xmath255 .",
    "conditions [ cond1 ] and [ cond2 ] are only technical conditions to avoid pathologies in the convergence analysis for algorithm [ alg : rol ] . in practical implementations , we can simply set the sub - gradient of @xmath256 at @xmath257 as zero ( a valid sub - gradient as proved in @xcite ) when @xmath258 .",
    "given the definition of geometric median in  , we can define following population geometric median loss function , @xmath259 , that we want to minimize to compute the geometric median : @xmath260.\\ ] ]    in this subsection , we first show that the geometric median function in is indeed strongly convex under conditions [ cond1 ] and [ cond2 ] .",
    "thus the sgd optimization is able to provide solutions with a convergence rate of @xmath97 to the true geometric median @xmath242 , given @xmath19 independent estimates .",
    "[ def : strong - convex ] a function @xmath261 is @xmath262-strongly convex , if for all @xmath263 and any sub - gradient @xmath264 of @xmath261 at @xmath265 , we have @xmath266    the following theorem establishes the strong convexity of the geometric median function in .",
    "[ theo : convexity ] let @xmath264 be the sub - gradient of @xmath256 at @xmath265 . under conditions [ cond1 ] and [ cond2 ] , there is a strictly positive constant @xmath267 , such that : @xmath268 and thus @xmath256 is @xmath58-strongly convex .",
    "the proof can be derived from the proof for the proposition 2.1 in  @xcite straightforwardly and we omit details here .",
    "given the strong convexity property of geometric median function @xmath256 , we can apply the convergence argument of sgd for strongly convex functions ( _ e.g. _ , proposition 1 in @xcite ) , and obtain the following convergence rate for online geometric median filtering .",
    "[ theo : converge_rate ] assume conditions [ cond1 ] and [ cond2 ] hold , and @xmath269 .",
    "then @xmath270 with probability @xmath271 .",
    "assume @xmath87 .",
    "let @xmath272 .",
    "pick @xmath273 in algorithm [ alg : rol ] and let @xmath43 denote the output at time step @xmath274 . furthermore , let @xmath242 be the geometric median of @xmath275 . then for any @xmath276 , @xmath277 with probability at least @xmath136 . here",
    "@xmath278 .",
    "the bound on the gradient @xmath279 is from the definition of the gradient in  , condition [ cond2 ] and the assumption that all the estimates are bounded .      from now on ,",
    "we slightly abuse the notation and use @xmath280 to denote the geometric median of a collection of estimates .",
    "proposition [ prop : rol ] can be derived by following triangle inequality : @xmath281 where @xmath280 denotes the `` true '' geometric median of estimates @xmath282 .",
    "we now proceed to bound the above two terms separately . based on theorem [ theo : converge_rate ] , we have @xmath283 with a probability at least @xmath136 .",
    "the second term can be bounded as follows by applying lemma [ lemma : median_aggregation ] : @xmath284 where @xmath285 and @xmath91 denotes the @xmath286 smallest outlier fraction in @xmath287 with @xmath94 .",
    "combining these two bounds together gives : @xmath288      before proving the performance guarantee for orl - pca and orl - lr , we provide robustness analysis for the base robust learning procedurethe rc - pca and rotr .",
    "[ theo : rpca ] suppose in total @xmath74 samples are provided with @xmath7 authentic samples and @xmath9 outliers .",
    "let @xmath289 .",
    "assume the authentic samples follow sub - guassian design with parameter @xmath16 .",
    "let @xmath290 , where @xmath291 denotes the @xmath292 largest eigenvalue of ground - truth sample covariance matrix @xmath293 .",
    "let @xmath294 be the output @xmath114-dimensional subspace projector from rc - pca .",
    "then for a constant @xmath295 , we have with probability @xmath88 , @xmath296    according to the proof of theorem 4 in @xcite and deviation bound on the empirical covariance matrix estimation in @xcite , when the authentic samples are from sub - gaussian distribution with parameter @xmath16 , we have , for the covariance matrix constructed in algorithm  [ alg : rpca ] , @xmath297 with a probability at least @xmath136 . here",
    "@xmath298 is a constant , @xmath7 is the number of authentic samples and @xmath9 is the number of outliers .",
    "let @xmath290 be the eigenvalue gap , where @xmath291 denotes the @xmath114-th largest eigenvalue of @xmath293 . then , applying the davis - kahan perturbation theorem @xcite , we have , whenever @xmath299 , @xmath300 .",
    "thus , @xmath301 with a probability at least @xmath136",
    ".      theorem [ theo : rol_pca ] can be derived directly from following triangle inequality : @xmath302 and we bound the above two terms separately .",
    "the first term can be bounded by theorem [ theo : converge_rate ] as , @xmath303 with a probability @xmath136 .",
    "the second term can be bounded as in theorem [ theo : rpca ] that with a probability @xmath88 , @xmath304 combining the above two bounds ( with union bound ) proves the theorem .      before proving theorem  [ theo : rol_sr ] ,",
    "we first show the following performance guarantee for rotr algorithm from @xcite . the estimation error of the rotr is bounded as in lemma  [ lemma : rotr ] .",
    "[ lemma : rotr ] suppose the samples @xmath305 are from sub - gaussian design with @xmath306 , with dimension @xmath3 and noise level @xmath148 , then with probability at least @xmath88 , the output of rotr satisfies the @xmath307 bound : @xmath308 here @xmath295 is a constant independent of @xmath309 .",
    "based on the results in the lemma [ lemma : rotr ] and lemma [ lemma : median_aggregation ] , it is straightforward to get : @xmath310 where @xmath311 with @xmath295 being the constant in lemma  [ lemma : rotr ] , @xmath312 , and @xmath313 with @xmath314 being the @xmath315 smallest outlier fraction in @xmath287 .    as proving theorem [ theo : rol_pca ] , theorem [ theo : rol_sr ]",
    "can be derived based on the results in theorem [ theo : converge_rate ] .",
    "for simplicity , we omit the details here .",
    "we developed a generic online robust learning ( orl ) approach with provable robustness guarantee and we also demonstrate its application for distributed robust learning ( drl ) .",
    "the proposed approaches not only significantly enhance the time and memory efficiency of robust learning but also preserve the robustness of the centralized learning procedures . moreover , when the outliers are not uniformly distributed , the proposed approaches are still robust to adversarial outliers distributions .",
    "we provided two concrete examples , online and distributed robust principal component analysis and linear regression ."
  ],
  "abstract_text": [
    "<S> we consider the problem of learning from noisy data in practical settings where the size of data is too large to store on a single machine . </S>",
    "<S> more challenging , the data coming from the wild may contain malicious outliers . to address the scalability and robustness issues </S>",
    "<S> , we present an _ online _ </S>",
    "<S> robust learning ( orl ) approach . </S>",
    "<S> orl is simple to implement and has provable robustness guaranteein stark contrast to existing online learning approaches that are generally fragile to outliers . </S>",
    "<S> we specialize the orl approach for two concrete cases : online robust principal component analysis and online linear regression . </S>",
    "<S> we demonstrate the efficiency and robustness advantages of orl through comprehensive simulations and predicting image tags on a large - scale data set . </S>",
    "<S> we also discuss extension of the orl to distributed learning and provide experimental evaluations . </S>"
  ]
}