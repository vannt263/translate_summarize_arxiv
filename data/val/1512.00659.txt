{
  "article_text": [
    "a support vector machine ( svm ) is a distinguishing classifier conventionally defined by a separating hyperplane . in other words ,",
    "given labeled training data ( supervised learning ) , the algorithm outputs an optimal hyperplane which classifies the unseen examples[1 - 2 ] .",
    "svm is not limited statistics or machine learning but can be applied in wide number of applications .",
    "svm proved to be the best classifier in different applications from handwritten digit recognition to text categorization .",
    "svm does nt have any effect on classification due to the curse of dimensionality .",
    "it works well with high dimensional data .",
    "literature survey witnesses that the support vector machines are the best classifiers for 2-class classification problems .",
    "the real time applications are not limited to binary classification but multiclass classification[3 ] .",
    "for example , to classify an astronomical object as a star , a galaxy or a quasar requires a multi - class classifier but not binary classifier .",
    "there are two major approaches in solving the n - class problem one is a single large optimization problem [ 4 - 5 ] , and the alternative is to decompose n - class problem into multiple 2-class problems . but solving a single large optimization problem will be expensive in terms of computational time and not suitable for practical applications .",
    "there are several algorithms based on the second approach like ovo(one - versus - one),ova(one - versus - all ) , dag(directed acyclic graph)[6 - 7 ] . in this paper",
    "we propose a new algorithm which decomposes the single n - class problem into multiple 2-class problems . and",
    "it requires less number of binary classifiers when compared with the above mentioned algorithms and gives a better accuracy too .",
    "the rest of the paper is organized as follows .",
    "the related work is discussed in section 2.the proposed approach is presented in section 3 .",
    "section 4 contains the experimental results and comparison of the proposed approach with the existing algorithms .",
    "finally , section 5 contains conclusion and future work .",
    "the learning task in binary svm can be represented as the following @xmath0    subject to @xmath1 where @xmath2 and @xmath3 are the parameters of the model for total @xmath4 number of instances .",
    "+ using lagrange multiplier method the following equation is to be solved ,    @xmath5 + the dual version of the above problem is + @xmath6 subject to @xmath7 @xmath8 where @xmath9 are known as the lagrange multipliers . + by solving this dual problem , svm will be found .",
    "once the svm model is built , the class label of a testing object z can be predicted as follows .",
    "+ @xmath10 if @xmath11 z will be predicted as + class else -ve class .",
    "the simple approach is to decompose the problem of classifying n classes into n binary problems , where each problem differentiates a given class from the other n-1 classes [ 8 ] . in this approach ,",
    "we require k = n binary classifiers , where the nth classifier is trained with positive examples belonging to class n and negative examples belonging to the other n-1 classes .",
    "when an unknown example is to be predicted , the classifier achieving the maximum output is considered as the best choice , and the corresponding class label is assigned to that test object .",
    "though this approach is simple [ 8 ] , it provides performance that is comparable to other more complicated approaches when the binary classifier is tuned well .      in this approach , each class is compared to every other class [ 9 - 10 ] .",
    "a binary classifier is built to differentiate between each pair of classes , while discarding the rest of the classes .",
    "this requires building n(n-1)/2 binary classifiers .",
    "when testing a new object , a voting is performed among the classifiers and the class with the maximum number of votes will be considered as the best choice .",
    "results [ 6,11]show that this approach is in general better than the one - versus - all approach .",
    "the n - class problem is decomposed into multiple 2-class problems in a binary tree structured manner .",
    "k - means clustering is used as a preprocessing step to get the rough estimation of similarity between the class labels .",
    "this will let us divide the class labels into two disjoint sets and build the svm for the root node .",
    "thereafter every node is divided at the mid point for creating disjoint sets .",
    "the order of the class labels is computed based on the sse .",
    "the least sse will be first in the list and the highest sse will appear last in the list .",
    "this way ( n-1 ) binary svms will be built , and hence needs only ( n-1)/2 svms evaluation to classify the unclassified record .",
    "this is better than the worst case ( n-1 ) of ova and n(n-1)/2 of ovo . at the same time ,",
    "our experimental results show that the accuracy is comparable with ovo .",
    "but when cbts is compared with ova it shows a better accuracy with reduced training time and testing time .",
    "the algorithm for training and testing is illustrated below .",
    "input the training objects .",
    "add all the training objects to the root node .",
    "let the class labels are from 1 .... n + * preprocessing step : * divide the training objects i.e.the root node into two clusters / nodes ( @xmath12 ) and ( @xmath13 ) using k - means clustering ( centroid based ) .",
    "+    1 .   the objects will be adjusted to ( @xmath12 ) as positive class or ( @xmath13 ) as negative class based on the majority of their class labels from the two clusters .",
    "+ 2 .   for ( @xmath12 ) and ( @xmath13 ) calculate the sse of all objects based on the same class labels and sort them in the ascending order .",
    "the sse is given by @xmath14 + 3 .",
    "for both ( @xmath12 ) and ( @xmath13 ) repeat + 1 .",
    "if the number of class labels of the node are two , construct the binary classifier and return . if the number of class labels are more than two + 2 .",
    "divide the each node exactly at the mid point , construct the binary svm and repeat this till we reach only two class labels .      1 .",
    "the test object should be evaluated on the root node the binary tree of svms .",
    "repeat : * if the value is positive traverse to the left node ( @xmath12 ) else to the right node ( @xmath13 ) . + * until we reach the leaf node .",
    "classify the test object into the class label of leaf node .",
    "+    * let s run through an example .",
    "* consider figure 1 , suppose if we have 8 classes , we first run k - means clustering with k=2 to divide the data objects according to their distribution .",
    "then , through the cluster distribution and based on the majority we get to know which class labels fall on one side ( positive ) and which ones on the other side ( negative ) . in the example shown , set @xmath12 \\{1,3,7 } belong to the positive class and set of rest of class labels , @xmath13 \\{4,6,8,2,5 } belong to the negative class",
    ". the order of class labels within the node will be in the ascending order of sse .",
    "we then build a svm model by constructing a binary svm between data objects belonging to @xmath12 as belonging to positive class and those belonging to @xmath13 as negative class .",
    "now , for the left child of the root node , sample space is all the data objects that belong to class labels in @xmath12 i.e. \\{1,3,7}. we divide exactly at the mid point ( remember , clustering is done at the first step only ! ) and hence make new @xmath12 and @xmath13 sets for this node .",
    "so , new @xmath12 is \\{1,3 } and @xmath13 is \\{7}. we now consider all data objects having class labels in @xmath12 as having positive class and all in @xmath13 as negative class and build a svm model .",
    "this svm model acts as left child of the root node .    this way we build the whole binary tree of svm functions recursively for both left and right nodes",
    "till we reach a leaf node .",
    "when an unseen object has to be classified , the search starts from the root node and then it moves on to the left or right based on the evaluation function value recursively till the leaf node and assigns the corresponding class label to the test object .            in figure 2 , the hyperplanes are shown for different class labels based on the above approach .",
    "the order of the construction of the hyperplanes will be based on the binary tree structure .",
    "we implemented the algorithm on twelve classification data sets and compared the training time , testing time and accuracy with ovo and ova .",
    "glass , iris , letter , mfeat - fac , mfeat - fou , mfeat - kar and mfeat - mor , pendigits , satimage , segment , shuttle , vowel are uci data sets and taken from https://archive.ics.uci.edu/ml/datasets.html . the detailed properties of the data sets are given in table i. for letter , shuttle and satimage dataset , a separate training and testing file was used .",
    "but , for the rest of all the data sets 2/3 part of data is considered for training and 1/3 is considered for testing .",
    "all the datasets were scaled to [ 0,1 ] and have been randomized ( similar class labels data will not appear together ) . in our implementation , we make use of radial basis function(rbf ) kernel and the range of value of @xmath15 @xmath16 @xmath17 to @xmath18 , c @xmath16 @xmath19 to @xmath20 .",
    "rbf kernel works best for any kind of problem [ 16 ] . in all the three approaches ovo , ova ,",
    "cbts the best combination of @xmath15 and c is chosen from the above mentioned range so as to provide the higher accuracy and lower testing and training time [ 17 ] .",
    "all the computations are done on a computer with 1.60ghz intel i5 - 4200u dual core processor and ram of 6 gb and using the software libsvm [ 17 ] .    in table",
    "ii , the training time and testing time of cbts is compared with ovo and ova .",
    "cbts performs very well when compared with ova .",
    "though the training time and testing time of cbts is little more than ovo , @xmath15 and cost values are justified to have a good model .    in table iii ,",
    "the results show that the accuracy of all twelve datasets by cbts are comparable with ovo with less cost c. a higher value of c means to choose more samples as support vectors . but limiting the support vectors i.e.limiting the value of c will use the minimum memory possible and the prediction will be faster . even when the @xmath15 is compared , cbts is choosing the intermediate values as for ovo it is too small . for a good model @xmath15 ca nt be too small or too large . if @xmath15 is too small the model is restricted and can not capture the complexity or `` shape '' of the data . if @xmath15 is too large",
    "there is a possibility that model will become over fit the data [ 18 ] . coming to the comparison with ova",
    ", cbts gives a better accuracy .    in table iv , the number of binary classifiers required for one classification is shown for ovo , ova and cbts .",
    "if we observe cbts requires the minimum number of binary classifiers when compared with ovo and ova .",
    "in addition to above data sets we analyzed the sloan digital sky survey ( sdss ) data set also .",
    "sdss is a major multi - filter imaging and spectroscopic redshift survey using a dedicated 2.5-m wide - angle optical telescope at apache point observatory in new mexico , united states , astronomical telescope survey .",
    "it has 6 class labels and only 5 features ( u , g , r , i , z ) are considered ( it has many ) .",
    "the data can be downloaded from sql interface on http://skyserver.sdss.org/dr7/en/tools/search/sql.asp    the results are shown in table v for sdss data set .",
    "cbts outperforms both ova and ovo in the aspects of accuracy and training time with the best gamma and cost values .",
    "the size of the instances are varied from 30,000 to 75,000 .",
    "ova could nt build the binary classifiers when the data size is 75,000 because of scalability issue whereas ovo and cbts could build the required models with out any problem .",
    "hence cbts is scalable . and it gives a better accuracy with reduced training time .",
    ".the complete details of the data sets used in our implementation . [ cols=\"^,^,^,^\",options=\"header \" , ]",
    "we propose a new algorithm cbts ( centroid based binary tree structured svm ) , a binary tree structure in which the root node contains all the class labels and is splitted based on the k - means clustering at the first step .",
    "and then the left and right nodes are recursively splitted into half till we have only two class labels . in this n-1 svms",
    "are required to construct but the required svms in ovo , ova are n(n-1)/2 , n respectively .",
    "the testing time for all ovo , ova , dag is linear on n i.e. o(n ) but for our algorithm it is o(log n ) as it needs only ( n-1)/2 classifiers to predict the class label for the test object .",
    "experimental results show that the accuracy of cbts is comparable with ovo approach and it out performs with ova both in accuracy and training , testing time .",
    "cbts also capable of handling large data sets , hence scalable . by using a more optimized approach of implementing cbts , testing time may further be reduced . instead of k - means",
    "clustering alternative partition techniques can also be explored at the root level .",
    "furthermore , a parallel / distributed version of our algorithm can be taken up as a future work . in our algorithm",
    "there are independent svm models that can be constructed simultaneously which results in reducing both training and testing time of cbts even further .",
    "we are thankful for the resources provided by the department of computer science and information systems , bits , pilani , k.k .",
    "birla goa campus to carry out the experimental analysis .",
    "c.cortes and v. vapnik .",
    "_ support vector network .",
    "1em plus 0.5em minus 0.4emseptember 1995 , volume 20 , issue 3 , pp 273 - 297 v. vapnik . _",
    "the nature of statistical learning theory_. ny : springer - verlag .",
    "k.crammer,y.singer , _ on the learnability and design of output codes for multiclass problems_. 1em plus 0.5em minus 0.4emproceedings of the thirteenth annual conference on computational learning theory , morgan kaufmann publishers inc . , san francisco , ca , usa,2000,pp.35 - 46 v.n.vapnik _ statistical learning theory_. 1em plus 0.5em minus 0.4emjohn wiley and sons , new york,1998 .",
    "y.guermeur , combining discriminant models with new - multi - class svms , 1em plus 0.5em minus 0.4empatt.anal.appl.5(2002 ) 168 - 179 chih - wei hsu and chih - jen lin .",
    "_ a comparison of methods for multi class support vector machines_. 1em plus 0.5em minus 0.4emieee transactions on neural networks , vol",
    ". 13 , no .",
    "2 , march 2002 abe , shigeo _ analysis of multi support vector machines_. 1em plus 0.5em minus 0.4emproc .",
    "international conference on computational intelligence for modelling control and automation ( cimca2003 ) , : 385 - 396 ryan rifkin , aldebano klautau , _ in defense of one  vs - all classification _",
    "1em plus 0.5em minus 0.4emjournal of machine learning research 5 ( 2004 ) 101 - 141 .",
    "_ another approach to polychotomous classification . technical report _ ,",
    "1em plus 0.5em minus 0.4em stanford university , 1996 .",
    "trevor hastie and robert tibshirani .",
    "_ classification by pairwise coupling .",
    "_ 1em plus 0.5em minus 0.4em advances in neural information processing systems , volume 10 .",
    "the mit press,1998 erin allwein , robert shapire , and yoram singer . _ reducing multiclass to binary : a unifying approach for margin classifiers_. 1em plus 0.5em minus 0.4em journal of machine learning research , pages 113141 , 2000    l.bottou , c. cortes , j. denker , h. drucker , i. guyon , l. jackel , y. lecun , u. muller , e. sackinger , p. simard , and v. vapnik .",
    "_ comparison of classifier methods : a case study in handwriting digit recognition_. 1em plus 0.5em minus 0.4emin international conference on pattern recognition , pages 77 - 87 .",
    "ieee computer society press , 1994 .",
    "s. knerr , l. personnaz , and g. dreyfus , _ single - layer learning revisited : a stepwise procedure for building and training a neural network_. 1em plus 0.5em minus 0.4em neurocomputing : algorithms , architectures and applications .",
    "springer - verlag , 1990 .",
    "_ another approach to polychotomous classification . technical report ,",
    "department of statistics , stanford university,1996_. 1em plus 0.5em minus 0.4emavailable at http://www-stat.stanford.edu/reports/friedman/poly.ps.z u. krebel .",
    "_ pairwise classification and support vector machines_. 1em plus 0.5em minus 0.4emadvances in kernel methods  support vector learning , pages 255 - 268 , cambrdige , ma , 1999 . mit press .",
    "chih - chung chang and chih - jen lin ._libsvm : a library for support vector machines .",
    "_ 1em plus 0.5em minus 0.4em acm transactions on intelligent systems and technology , 2:27:127:27 , 2011 .",
    "software available at http://www.csie.ntu.edu.tw/  cjlin / libsvm"
  ],
  "abstract_text": [
    "<S> support vector machines ( svms ) were primarily designed for 2-class classification . </S>",
    "<S> but they have been extended for n - class classification also based on the requirement of multiclasses in the practical applications . </S>",
    "<S> although n - class classification using svm has considerable research attention , getting minimum number of classifiers at the time of training and testing is still a continuing research . </S>",
    "<S> we propose a new algorithm cbts - svm ( centroid based binary tree structured svm ) which addresses this issue . in this we build a binary tree of svm models based on the similarity of the class labels by finding their distance from the corresponding centroids at the root level . </S>",
    "<S> the experimental results demonstrates the comparable accuracy for cbts with ovo with reasonable gamma and cost values . on the other hand when cbts is compared with ova </S>",
    "<S> , it gives the better accuracy with reduced training time and testing time . </S>",
    "<S> furthermore cbts is also scalable as it is able to handle the large data sets .    </S>",
    "<S> k - means clustering / centroid based clustering ; svm ; multi - classification ; binary tree ; </S>"
  ]
}