{
  "article_text": [
    "graphics processing units ( gpus ) are specialized for math - intensive highly parallel computation , thus more transistors are devoted to data processing rather than data caching and flow control like in cpu .",
    "so the potential tremendous performance of general non - graphics computations on gpus has recently motivated a lot of research activities on general - purpose gpu ( gpgpu ) computing ( see .",
    "e.g. owens et al .",
    "2007 for a review ) .",
    "nvidia introduced the tesla unified graphics and computing architecture in november 2006 .",
    "the tesla architecture is built around a scalable array of multithreaded streaming multiprocessors ( sm ) .",
    "a sm consists of eight streaming processor ( sp ) cores .",
    "the tesla sm uses a new processor architecture called single - instruction , multiple - thread ( simt ) .",
    "the simt unit creates , manages and executes up to @xmath0 concurrent threads in hardware with zero scheduling overhead .",
    "the sm also implements barrier synchronization intrinsic with a single instruction . the fast barrier synchronization , together with lightweight thread creation and zero - overhead thread scheduling support very fine - grained parallelism allowing thousands and even millions of threads to be invoked in kernel calls to achieve highly scalable parallel programming .",
    "high performance supercomputing has been important in modern astrophysical research since it became available .",
    "simulations allow astronomers to perform  experiments \" on astronomical objects , collide stars , galaxies , or model the entire visible universe ; all situations are clearly impossible to recreate in a terrestrial laboratory . studying the formation of stars , black holes and galaxies in the universe is particularly challenging computationally .",
    "their formation involves the nonlinear interplay of a range of physical processes including gravity , turbulence , magnetic field , shocks , radiation , chemistry , etc .",
    "those questions motivated the astrophysical community to develop robust and efficient fluid codes with all the relevant physics .",
    "studies involving astrophysical fluid dynamics in general are benefitting tremendously from using spatial and temporal adaptive mesh refinement ( amr ) .",
    "this is especially so in the studies of structure formation .",
    "for example , the radius of a star is 8 orders of magnitude smaller than the size of a molecular cloud .",
    "a uniform grid code is hopeless . on the other hand ,",
    "the amr technique has been demonstrated to work well in resolving the large dynamical range involved in those problems ( e.g. abel et al .",
    "2002 ; wang & abel 2009 ) .",
    "the mapping of computational fluid algorithms to gpu however is still at an early stage of development .",
    "harris et al .",
    "( 2003 ) performed cloud simulations using stam s method ( stam 1999 ) .",
    "this method is also used by liu et al .",
    "( 2004 ) for 3d flow calculations .",
    "using finite difference methods , brandvik & pulla ( 2008 ) solved uniform grid 3d euler equations , elsen , legresley & darve ( 2008 ) solved 3d euler equations on a multi - block meshes and zink ( 2008 ) solved einstein s equation with uniform grid . as far as we are aware of , this work is the first on mapping an adaptive mesh finite volume solver to gpu .",
    "in 2007 , nvidia released cuda for gpu computing as a language extension to c ( nvidia 2009 ) .",
    "cuda makes gpu computing application development much easier and more efficient than earlier attempts to gpgpu using various shading languages which need to translate the computation to a graphics language .",
    "cuda s parallelization model is based on abstraction of the geforce 8-series hardware .",
    "it allows programmer to define kernels which can be executed in parallel by many threads on gpu .",
    "threads are organized into 1d , 2d or 3d thread blocks , where each block is executed on one sm .",
    "the sm maps the threads inside a block to the streaming processor ( sp ) cores and each thread executes independently with its own instruction address and register state .",
    "synchronization is possible only within a block whereas global synchronization between blocks is impossible .",
    "blocks are in turn organized into 1d or 2d grid of blocks .",
    "each thread can access its thread and block indices by two built - in variables threadidx and blockidx .",
    "the sm s simt unit creates , manages , schedules and executes threads in groups of @xmath1 parallel threads called warps .",
    "the threads in a warp always execute a common instruction at a time , but different warps execute independently . as a result",
    ", different warps can execute on different branches .",
    "this is an enormous improvement for branching code compared to previous - generation gpus as the @xmath1-thread warps are much narrower than the simd ( single - instruction multiple - data ) width of prior gpus .",
    "however , if threads of a warp diverge via a conditional branch , different execution path have to be serialized , increaing the total number of instructions executed for this warp .",
    "so branching inside a warp should still be minimized to achieve good efficiency .",
    "cuda exposes the hardware memory hierarchy by allowing threads to access data from multiple memory spaces .",
    "all threads have access to the same global memory .",
    "each thread block has a shared memory visible to all threads of the block and with the same lifetime as the block .",
    "each thread has a private local memory and a set of registers .",
    "there are also two additional read - only memories accessible by all threads : the constant and texture memory .",
    "the shared memory is much smaller than global memory , typically @xmath2 kb , but it is on - chip so it has very high register - level bandwidth . a typical programming pattern utilizing this fact is to stage data from global memory into shared memory , process the data there and then write the results back to global memory .",
    "in the berger & colella block - structured amr ( berger & colella 1989 ) , a subgrid will be created in regions of its parent grid needing higher resolution .",
    "the hierarchy of grids is organized in a tree structure .",
    "each grid is evolved as a separate initial boundary value problem .",
    "the whole grid hierarchy is evolved recursively . in this framework ,",
    "a single grid is a natural unit to be sent to gpu for computing .",
    "since the grids are dynamically created , arbitrary grid dimensions can arise in real applications . thus _",
    "the key issue for a parallelization scheme to couple efficiently to amr is to let it work for arbitrary grid dimensions_. many parallelization models previously proposed for uniform grid gpu fluid solvers are thus not appropriate to be coupled with amr .    in this work",
    ", we use the implementation of berger & colella amr in the publicly available hydrodynamics code enzo ( enzo 2009 ) .",
    "previously we have implemented our own hydrodynamics and magnetodynamics code using the enzo amr framework ( wang et al .",
    "2008 ; wang & abel 2009 ) . often , the hydrodynamics / magnetohydrodynamics solver is the computationally dominant part ( typically more than 10 times expensive than the amr part in single core run ) .",
    "so in this work we consider the mapping of hydrodynamics solver onto gpu while leaving the creation and refinement of the grid hierarchy on cpu .",
    "the flows involved in star and galaxy formation are highly supersonic and the reynolds number in those flows are very large ( spitzer 1978 ) .",
    "thus we are interested in solving the equations of compressible inviscid hydrodynamics .",
    "however , the parallelization scheme we implemented can be applied to any hyperbolic conservation laws .",
    "we will demonstrate this explicitly in section [ sec : mhd ] by implementing a amr magnetohydrodynamics ( mhd ) solver on gpu .",
    "the equations of compressible inviscid hydrodynamics can be written in the form of conservation laws , @xmath3 the conserved variable @xmath4 is given by @xmath5 where @xmath6 is density , @xmath7 are the three components of velocity for @xmath8 , @xmath9 is the total energy and @xmath10 is the internal energy .",
    "the fluxes are given by @xmath11 where @xmath12 is the enthalpy . in this work",
    "we assume the ideal gas equation of state : @xmath13 where @xmath14 is the adiabatic index .",
    "many numerical schemes have been developed to solve hyperbolic conservation laws of the form ( [ hydro ] ) .",
    "we are interested in a class of finite volume methods called high - resolution shock - capturing ( hrsc ) schemes developed since the mid-1980s .",
    "those schemes are designed to capture the correct shock speed even with very low resolutions ( see e.g. leveque 2002 for a comprehensive review ) .",
    "this property makes it ideal for adaptive mesh fluid simulations where shocks outside the refined regions may not be well resolved .",
    "hrsc schemes represent the most popular scheme in modern astrophysical codes ( e.g. stone et al .",
    "2008 ; mignone et al . 2007 ;",
    "wang et al . 2008 ) .",
    "thus we will focus on mapping the hrsc schemes onto gpu .",
    "we use the method of lines ( mol ) to discretize the system ( [ hydro ] ) spatially , @xmath15 where @xmath16 refers to the discrete cell index in @xmath17 directions , respectively .",
    "@xmath18 , @xmath19 and @xmath20 are the fluxes at the cell interface .",
    "as discussed by shu & osher ( 1988 ) , if one uses a high order scheme to reconstruct flux spatially , one must also use the appropriate multi - level total variation diminishing ( tvd ) runge - kutta schemes to integrate the ode system ( [ ode ] ) .",
    "so instead of the forward euler time integration in the original berger - collela amr ( berger & colella 1989 ) , we have implemented the second order tvd runge - kutta scheme : @xmath21 see wang et al .",
    "( 2008 ) for further details of the implementation of runge - kutta scheme in an amr framework .",
    "generally speaking , there are two classes of spatial reconstruction schemes ( leveque 2002 ) .",
    "one is reconstructing the unknown variables at the cell interfaces and then use exact or approximate riemann solver to compute the fluxes .",
    "another is direct flux reconstruction , in which we reconstruct the flux directly using the fluxes at cell center .",
    "we will adopt the first class in this work .",
    "but our framework can also be applied directly to the second class .     by equations ( [ ul ] ) to ( [ flux]).,width=211,height=57 ]    for example , to calculate the flux @xmath22 at cell interface @xmath23 ( see fig.[grid1d ] ) , we need to reconstruct the left and right states @xmath24 and @xmath25 . in this work , we use the piecewise linear method ( plm ) ( van leer 1979 ) for reconstruction , in which the values of primitive variable @xmath26 at @xmath27 are needed .",
    "the formula for plm reconstruction is @xmath28 where @xmath29 and the minmod function is given as @xmath30    with those two states , we use the harten - lax - van leer ( hll ) approximate riemann solver ( harten et al .",
    "1983 ) to calculate the flux @xmath22 by , @xmath31 where @xmath32 and @xmath33 for @xmath34 .",
    "the important property of this mol approach is that the cells required for flux calculation in any direction are all along that direction .",
    "as we will see below , this property is crucial for a efficient parallelization scheme in 2d and 3d .",
    "in this section , we discuss a parallelization scheme for the 1d case . in the next section we will extend this scheme to 2d and 3d .",
    "first , we note that one drawback of putting all the steps of the 2nd order runge - kutta scheme ( [ rk2 ] ) to gpu is that it needs to send both the old value and intermediate value to gpu in the second step . from our experience ,",
    "data transfer between cpu and gpu is quite expensive so it needs to be minimized .",
    "this would also double the gpu memory usage .",
    "however , the 2nd order runge - kutta scheme can also be written as @xmath35 thus , if we only put the first and second step to gpu , which is the computationally intensive part , and leave the third part on cpu , which is just the addition of two vectors , we may still get large speed - up",
    ". this will be the strategy we take .",
    "since the first and second steps have exactly the same form , we only need to write one routine computing @xmath36 and call it twice .",
    "the pseudocode for this routine reads :     + allocate memory for primitives and fluxes on gpu + copy primitives to gpu + call kernel for flux computation + call kernel for @xmath37 computation + call kernel for time update @xmath36 + copy @xmath38 back to cpu + free memory on gpu +    since the computation of flux vector @xmath37 and time update use only local information and involve only one read and one computation , their implementations are simple : just launch one thread for every active cell .",
    "the most computational intensive and tricky part is the flux computation , which happens at cell interfaces and needs a different treatment compared to the other two kernels .",
    "our basic scheme is that every flux is calculated by a single thread ( see fig .",
    "[ gridthread ] ) .",
    "so for a problem with @xmath39 grid cells ( including the ghost cells ) , the number of threads should be @xmath40 .",
    "we take the number of threads per block to be a fixed number @xmath41 .",
    "thus the number of blocks to be launched is @xmath42 .",
    "the final block will have some threads that do not correspond to flux computation , we add a conditional statement in the kernel to just let those threads do nothing .",
    "since this affect only the last one or two warps , this remains efficient .",
    "for example , in the @xmath43th block , we want to calculate @xmath44 fluxes at cell interfaces @xmath45 with @xmath46 . according to eqs .",
    "( [ ul ] ) to ( [ flux ] ) , every thread needs 4 cells around it to calculate the flux .",
    "so in total , @xmath47 cells @xmath48 are needed for the calculation .    .",
    "every thread , except the first and last , loads the data in the cell to the right of it .",
    "the first thread loads in two more cells to the left of it while the last thread loads in one more cell to the right of it.,width=384,height=96 ]    as discussed in section [ sec : cuda ] , the memory bandwidth is much higher on the shared memory .",
    "so in the computation of a block , we will first load the @xmath47 cells from global memory to a shared memory array primline[n+3 ] .",
    "after the computation is finished , we will then write the flux back to global memory . as shown in fig.[gridthread ] , our memory loading scheme goes as follows : thread @xmath43 will read in primitive variables in cell @xmath49 .",
    "to read in all the necessary data , the zeroth thread will then need to read in two more cells @xmath50 and @xmath51 and the ( n-1)th thread will read in one more cell @xmath52 .",
    "the kernel code for this flux computation are : +    # define cuda_block_size 64 + # define neq_hydro 5 + ... +   + //",
    "get thread and block index + const long tx = threadidx.x ; + const long bx = blockidx.x ; +   + int igrid = tx + bx*cuda_block_size ; //",
    "array index in the data field + int idx_prim , idx_prim1 ; +   + _ _ shared _ _ float primline[neq_hydro*(cuda_block_size+3 ) ] ; // input primitive variable + _ _ shared _ _ float fluxline[neq_hydro*cuda_block_size ] ; // output flux +   + if = ( igrid @xmath53 2 & & igrid @xmath54 size - 2 ) \\ { // only do flux computation for active cells +   + // load data from device to shared . + idx_prim1 = ( tx+2)*neq_hydro ; + primline[idx_prim1++ ] = rho[igrid ] ; + primline[idx_prim1++ ] = eint[igrid ] ; + primline[idx_prim1++ ] = vx[igrid ] ; + primline[idx_prim1++ ] = vy[igrid ] ; + primline[idx_prim1 ] = vz[igrid ] ; + // if the first , load in two more cells for boundary condition + if ( tx = = 0 @xmath55 igrid = = 2 ) \\ { +  for ( int i = -2 ; i @xmath54 - 1 ; i++ ) \\ { +",
    "idx_prim = igrid + i ; +  idx_prim1 = ( i+tx+2)*neq_hydro ; +  primline[idx_prim1++ ] = rho[idx_prim ] ; +  primline[idx_prim1++ ] = eint[idx_prim ] ; +  primline[idx_prim1++ ] = vx[idx_prim ] ; +  primline[idx_prim1++ ] = vy[idx_prim ] ; +  primline[idx_prim1 ] = vz[idx_prim ] ; +  } + } +   + //",
    "if the last , load in one more cell for boundary condition + if ( tx = = cuda_block_size - 1 @xmath55 igrid = = size - 2 ) \\ { +  idx_prim = igrid + 1 ; +  idx_prim1 = ( tx+3)*neq_hydro ; +  primline[idx_prim1++ ] = rho[idx_prim ] ; +  primline[idx_prim1++ ] = eint[idx_prim ] ; +  primline[idx_prim1++ ] = vx[idx_prim ] ; +  primline[idx_prim1++ ] = vy[idx_prim ] ; +  primline[idx_prim1 ] = vz[idx_prim ] ; + } + } +   + // synchronize to ensure all the data are loaded + _ _ syncthreads ( ) ; +   + if ( igrid @xmath53 2 & & igrid @xmath54 size - 2 ) \\ { + // the main computation : calculating the flux at tx + llf_plm(primline , fluxline , tx ) ; +   + //",
    "copy 1d flux back to flux + idx_prim1 = tx*neq_hydro ; + fluxd[igrid ] = fluxline[idx_prim1++ ] ; + fluxs1[igrid ] = fluxline[idx_prim1++ ] ; + fluxs2[igrid ] = fluxline[idx_prim1++ ] ; + fluxs3[igrid ] = fluxline[idx_prim1++ ] ; + fluxtau[igrid ] = fluxline[idx_prim1 ] ; + }      in higher dimension , the main trick is that in the mol , 2d and 3d flux computation can be reduced to 1d problems and thus the parallelization scheme discussed above for 1d problem can be applied directly to 2d and 3d problems .     and @xmath56 sweeps.,width=451,height=240 ]    for example , in 2d application of mol , one sweeps through a 1d grid of lines extending in the other direction , calculating the fluxes on every line in every sweep . to compute flux in all two directions , one does two sweeps .",
    "the trick is that in every sweep , we conceptually regard the 2d grid to be a large 1d grid , including the ghost cells , which is continuous in the direction we are sweeping . thus ,",
    "when we apply thread decomposition to the grid , we send the large 1d array to it and use exactly the same scheme discussed in previous section for 1d problems .",
    "this is illustrated in fig .",
    "[ sweep ] for a 2d problem , where we show the original 2d grid and the 1d grid sent to the kernel for @xmath57 and @xmath56 sweeps .",
    "the 3d case is exactly the same .",
    "the additional cost of this method is that we will also calculate fluxes for all the ghost cells since now we regard them as normal cells so some calculation is wasted .",
    "however , this is only a small cost .",
    "but the gain is that the parallelization scheme now works for any grid dimensions .",
    "one only need to use two  if \" statements to handle the first and last points in the grid , which will lead to execution branching only in the first and last two warps . note that in @xmath56 sweeping , when reading from global to shared memory , the reading is from non - continuous locations .",
    "thus reading is non - coalesced in those cases .",
    "we have also experimented reshuffling the large 3d array in cpu so that the reading can be coalesced but we found this leads to lower performance because of the reshuffling cost on cpu and additional data transfer .",
    "[ quadro ]    .technical specifications of nvidia s quadro fx 5600 graphics card . [ cols=\"^,^ \" , ]",
    "all the problems in this work are run on a quadro fx 5600 card . for reader",
    "s convenience , the technical specifications of quadro fx 5600 card are listed in table 1 .",
    "the corresponding cpu comparison cases are run on a single @xmath58 ghz core .       for the sod test problem with @xmath59 .",
    "solid line is the cpu solution and squares are the gpu solution.,height=240 ]    to evaluate the accuracy of the gpu solution , we first run a 1d sod shock tube problem ( sod 1978 ) with both the cpu code and gpu code .",
    "the initial condition for this problem is two uniform states separated at @xmath60 .",
    "the left and right density and pressure are @xmath61 and @xmath62 .",
    "the initial velocity is zero everywhere and the adiabatic index is @xmath63 .",
    "we use @xmath64 grid points for this test .",
    "the result is shown in fig.[sod ] .",
    "it can be seen that the gpu solution agrees with the cpu solution very well .",
    "this validates our gpu implementation of fluid solver .      .",
    "the diamonds show the ratio of the cpu and gpu running time.,height=288 ]    next , we study the performance of gpu code in 3d using the sedov - taylor blast wave problem ( sedov 1959 ) .",
    "this section will use a uniform grid .",
    "an amr example is given in the next section .",
    "to set up the problem , we deposit a total energy @xmath65 into a spherical region of radius @xmath66 ( the  explosion region \" ) at the center of the simulation box which has length @xmath67 .",
    "the pressure inside the explosion region can then be calculated as @xmath68 , where @xmath63 for this problem .",
    "the initial density is uniform throughout the box with @xmath69 and the pressure is set to a small value @xmath70 outside the explosion region .",
    "[ sedovtime ] shows the performance comparison of the cpu and gpu code for uniform grid sizes @xmath71 @xmath72 .",
    "it can be seen that we get fairly uniform @xmath73 times speed - up for two orders of magnitude difference in grid sizes .",
    "this is very encouraging as a wide variety of grid sizes can arise in real amr applications and a uniform speed - up for a large range of grid sizes is a necessary condition for good performance in amr applications .      in this section",
    "we show a case of cloud disruption by blast wave , demonstrating that our implementation also lead to good speed - up on amr applications .",
    "we put a uniform density cloud with radius @xmath74 at distance @xmath75 away from the center of the blast wave .",
    "the cloud is @xmath76 times denser than the medium and is in pressure equilibrium with the medium .",
    "the topgrid has resolution @xmath77 and we use six levels of refinement to resolve the cloud disruption , which correspond to an effective uniform resolution @xmath78 .    for this problem , the running time of a single timestep in gpu case is @xmath79 times faster than the cpu case .",
    "this is consistent with the behavior we saw in last section for uniform grid tests with various grid sizes .    .",
    "six levels of refinement is used to resolve the cloud disruption .",
    "the dotted lines show the boundary of subgrids.,width=480,height=240 ]",
    "magnetic field has been known to play a very important role in the formation of stars ( shu et al .",
    "1987 ) and its role in galaxy formation has also been investigated recently ( wang & abel 2009 ) .",
    "thus for astrophysical applications , a mhd solver is of great interest .",
    "our parallelization scheme discussed above can be easily extended to the case of mhd . in this section , after discussing the mhd equations , we will show some results of this gpu mhd solver .",
    "mhd equations can also be written in the form of conservation laws .",
    "thus any schemes for hydrodynamics in principle can also be applied to mhd .",
    "the main numerical problem of solving mhd equations is cleaning up the numerically generated magnetic monopoles as magnetic field is divergence - free physically .",
    "various schemes have been proposed for this purpose and there is still no universal agreement which scheme is the best ( see toth 2000 for a comparison of various schemes ) . in this work",
    ", we will adopt the so - called hyperbolic clean approach ( dedner et al .",
    "2002 ) , which nicely fit in our developed framework .",
    "this may not be the best scheme for all applications .",
    "but other schemes , like the projection scheme , requires the solution of a poisson equation . thus mapping them to gpu requires additional work on sparse matrix solvers .",
    "following dedner et al .",
    "( 2002 ) , we consider the generalized lagrange multiplier ( glm ) formulation of the mhd equations , which can be written in the conservative form ( [ hydro ] ) with the conserved variables given by @xmath80 where @xmath81 with @xmath8 are the three components of magnetic fields and @xmath82 is the additional scalar field introduced in the glm formulation for the divergence cleaning .    and the fluxes are given by @xmath83 where @xmath84 is a constant controlling the propagation speed and damping rate of @xmath85 ( dedner et al . 2002 ) .",
    "solving the glm - mhd system in our framework is straightforward .",
    "all we need to do is to add the additional primitive variables to our hydrodynamics solver .       for the brio - wu test problem with @xmath86 .",
    "solid line is the cpu solution and squares are the gpu solution.,height=240 ]    first , to evaluate the accuracy of the gpu solution , we run a 1d brio - wu shock tube problem ( brio & wu 1988 ) with both the cpu code and gpu code .",
    "the setup of brio - wu problem is similar to the sod problem discussed in section [ sec : sod ] , with two uniform states separated at @xmath60 .",
    "but here there is non - zero magnetic field in the initial condition .",
    "the left and right states are @xmath87 and @xmath88 .",
    "the adiabatic index is taken to be @xmath89 .",
    "we use @xmath90 grid points for this test .",
    "the result is shown in fig.[briowu ] .",
    "it can be seen that the gpu solution agrees with the cpu solution very well .",
    "this validates our gpu implementation of mhd solver .      .",
    "the diamonds are the ratio of the two running times.,height=288 ]    to compare the performance of the mhd solver to the hydrodynamics solver , we apply the mhd solver to the sedov - taylor blast wave problem discussed in section [ sec : sedov ] .",
    "[ mhdsedovtime ] shows the performance comparison of the cpu and gpu mhd solvers for uniform grid sizes @xmath91 .",
    "note that for the mhd case , the memory requirement for a @xmath92 run is @xmath93 gb ( three  primitives , flux and @xmath37copies of the nine mhd fields in a grid size @xmath92 ) .",
    "thus it can not fit in the @xmath94 gb memory of a quadro fx 5600 card .    from fig.[mhdsedovtime ] we can see that in the mhd case we also get @xmath73 speedup for a large range of grid sizes .",
    "this demonstrates the efficiency of our scheme as applied to more complicated physical problems .              finally , we present the results of an application of our gpu mhd solver to a problem that is of great astrophysical interest related to star formation : the decay of mhd turbulence ( stone et al .",
    "1998 ; mac low et al . 1998 ) . following stone",
    "et al . ( 1998 ) , we set initial density to be uniform . an isotropic turbulent velocity with burger - like power spectrum @xmath95 is imposed .",
    "the initial mach number is @xmath96 .",
    "we use an isothermal equation of state by setting @xmath97 .",
    "the resolution is taken to be uniform @xmath98 .",
    "[ turbulence ] shows the results of density field after one dynamical time .",
    "the code takes about @xmath76 minutes to evolve to this point .",
    "[ mach ] shows the time evolution of the average mach number and total magnetic energy .",
    "consistent with the findings in stone et al .",
    "( 1998 ) , a significant fraction of the kinetic energy decays in one dynamical time , and the presence of magnetic field can not delay it .",
    "those results are not new",
    ". however , the much shorter running time of the gpu code makes it possible to do many runs in a reasonable time to build up statistics , which is crucial for some aspects of star formation such as the core mass function .",
    "currently , this has only been possible in 2d ( e.g. basu & ciolek 2009 ) .",
    "resolution sedov - taylor blast wave simulations.,height=240 ]    to be competitive with current state of the art simulations which are usually run on large cpu clusters , a gpu implementation should also be mpi parallel .",
    "a simple setup would be building a small cpu cluster with each cpu node containing one or a few gpu cards . as a simple experiment",
    ", we built a 4 nodes cpu cluster with 4 mac pro workstations and a nvidia geforce 8800 gt graphics card on each node .",
    "the cpu nodes communicate with each other using the standard mpi library over gigabit ethernet .",
    "as discussed above , in our scheme , a grid is the basic unit of gpu computing .",
    "this makes it trivial to combine our scheme with the standard mpi parallel scheme of berger - collela amr , which also use grids as the basic unit for distribution among processors .",
    "the code first calls mpi library to distribute grids to different processors .",
    "then each processor sends its grids , one by one , to its gpu for computing .",
    "we have tested the resulting mpi - cuda hybrid code on up to 4 gpus .",
    "the results of the sedov - taylor blast wave simulations with a uniform @xmath98 resolution using 1 , 2 and 4 gpus are shown in fig .",
    "[ speed ] .",
    "it shows that our code archived very good , close to ideal speedup for up to four gpus .",
    "thus it seems realistic that large gpu clusters may offer significant cost saving as compared to a cpu clusters giving the same performance the magneto hydrodynamical problems our implementation can study .",
    "in this work we described how to map hrsc schemes for hyperbolic conservation laws to gpu using nvidia s cuda .",
    "we demonstrated that our framework as applied to the equations of inviscid compressible hydrodynamics and mhd can lead to a significant speedup .",
    "specifically , on a quadro fx 5600 card , saw approximately a factor of ten speedup compared to a single 3 ghz cpu core .",
    "an important purpose of our gpu parallel scheme design is achieving good scalability . in typical fluid simulations with as many as millions of cells per grid ,",
    "our parallelization scheme will launch millions of threads at the same time on gpu to perform the computation .",
    "this exceeds the quadro fx 5600 s ability of running @xmath99 threads concurrently by more than two orders of magnitude .",
    "thus we expect that the speedup factor of our parallelization scheme will increase linearly with the number of sms on the gpu .",
    "this makes our scheme highly scalable to future generation of graphics hardwares .",
    "one important topic we would like to concentrate on in the near future is sparse matrix solvers for poisson equation .",
    "if one can also speed up poisson solvers by a similar factor on gpu , then coupled with the fluid solvers we implemented in this work , a whole range of astrophysical simulations will be open to processing on the gpu as we can then model astrophysical fluids with self - gravity , viscosity and other non - ideal effects .",
    "an implementation of poisson solver will also make it possible to use projection method for divergence cleaning and implicit fluid solvers .",
    "we would like to thank sean treichler for helpful discussions .",
    "we are also grateful to the slac computing group , especially adeyemi adesanya , stuart marshall , ken zhou , for technical support on gpu hardware .",
    "zink , b. , 2008 : a general relativistic evolution code on cuda architectures , in the 9th lci international conference on high - performance clustered computing at the national center for supercomputing applications in urbana , il , usa"
  ],
  "abstract_text": [
    "<S> we describe an implementation of compressible inviscid fluid solvers with block - structured adaptive mesh refinement on graphics processing units using nvidia s cuda . </S>",
    "<S> we show that a class of high resolution shock capturing schemes can be mapped naturally on this architecture . using the method of lines approach with the second order total variation diminishing runge - kutta time integration scheme , piecewise linear reconstruction , and a harten - lax - van leer riemann solver , we achieve an overall speedup of approximately 10 times faster execution on one graphics card as compared to a single core on the host computer . </S>",
    "<S> we attain this speedup in uniform grid runs as well as in problems with deep amr hierarchies . our framework </S>",
    "<S> can readily be applied to more general systems of conservation laws and extended to higher order shock capturing schemes . </S>",
    "<S> this is shown directly by an implementation of a magneto - hydrodynamic solver and comparing its performance to the pure hydrodynamic case . </S>",
    "<S> finally , we also combined our cuda parallel scheme with mpi to make the code run on gpu clusters . </S>",
    "<S> close to ideal speedup is observed on up to four gpus .    </S>",
    "<S> multi - scale methods , finite volume methods , hydrodynamics , magnetohydrodynamics and plasma 47.11.st , 47.11.df , 95.30.lz , 95.30.qd </S>"
  ]
}