{
  "article_text": [
    "during the past four decades , the decrease in transistor size and the increase in integration factor have led to very small , fast , and power efficient chips .",
    "as the demand for power efficiency continues , a wide range of new nano - scale technologies is being actively investigated for processing and storage of digital data .",
    "although it is difficult to discern which of these approaches will become a technological basis for computers in the future , it is widely recognized that due to their miniature size and variations in technological process , the nano - components will be inherently unreliable . even in more traditional semiconductor technologies , reducing transistor size has already started affecting circuit reliability , and it is widely believed that transistor failures ( both transient and permanent ) will become one of the main technological obstacles as the trend of increasing the integration factor continues . in this paper , we consider storage circuits built from such unreliable ( faulty ) components .",
    "we consider an unreliable component ( a logic gate or a memory element ) to be a component that is subject to _ transient faults _",
    ", i.e. , faults that manifest themselves at particular time steps but do not necessarily persist for later times @xcite .",
    "von neumann @xcite was the first to study computation using faulty gates . in @xcite , he showed that , under certain conditions , increased gate redundancy can lead to increased reliability of a circuit .",
    "however , it was shown that , in general , computation by faulty gates with non - zero computational capacity is not possible ( see @xcite ) .",
    "the study of storage circuits made of unreliable components led to much more optimistic results .",
    "taylor in @xcite proved that a memory has an associated information storage capacity , @xmath0 , such that arbitrarily reliable information storage is possible for all memory redundancies greater than @xmath1 .",
    "the methodology of the proof , however , does not allow one to explicitly calculate the storage capacity .",
    "taylor considered two models of component failures and proposed construction of fault - tolerant memories based on low - density parity - check ( ldpc ) codes . in the first model ,",
    "the failures of a particular component are assumed to be statistically independent from one use to another and is referred to as the independent failure model . in the second model ,",
    "the components fail permanently but bad components are replaced with good ones at regular intervals .",
    "the failures in different components are assumed to be independent in both the models .",
    "this construction was further studied by kuznetsov in @xcite and we will refer to it as the taylor - kuznetsov ( tk ) scheme .",
    "hadjicostis @xcite was able to generalize taylor s scheme to fault tolerant linear finite state machines .",
    "spielman @xcite obtained the best result for a general model of computation , by marrying the ideas of von neumann with reed - solomon ( rs ) codes .",
    "the fundamental contribution of this paper is to show existence of reliable memories built entirely from unreliable components and which have finite redundancies .",
    "we consider the adversarial failure model in which only a fixed fraction of the components fail at any given time and extend our results to the independent failure model using chernoff bounds @xcite .",
    "our memory architecture has lower redundancy compared to the tk scheme .",
    "our fault tolerant memory architecture is also based on ldpc codes but differs from the tk scheme in the decoding algorithm employed .",
    "the tk scheme can be shown to be an implementation of the gallager b decoding algorithm for ldpc codes ( the proof will be given in section [ tkscheme ] ) .",
    "we use the parallel bit flipping decoding algorithm proposed in the context of expander codes by sipser and spielman @xcite .",
    "expander codes are a class of asymptotically good error correcting codes with linear time decoding algorithms which can correct a linear fraction of errors .",
    "expander graph based arguments have been successfully applied for message passing algorithms by burshtein and miller in @xcite as well as for linear programming decoding by feldman _",
    "et.al _ in @xcite .",
    "at the time of their discovery , explicit construction of graphs with expansion required for parallel bit flipping algorithm were not known .",
    "et al_. @xcite recently gave an explicit construction of expander graphs based on randomness constructors .",
    "hence , our method can be seen as a constructive proof in contrast to taylor s method which is an existence proof .",
    "the rest of the paper is organized as follows . in section [ system ]",
    "we provide the necessary definitions and a brief overview of ldpc codes .",
    "we explain the proposed memory architecture and characterize it in terms of complexity and redundancy . in section",
    "[ main ] we introduce the model of failure of the components and prove our main result showing the existence of memories which can tolerate failures in all the components . in section [ results ]",
    "we provide a few numerical examples . in section [ tkscheme ]",
    "we establish an equivalence between the tk scheme and gallager b algorithm and extend our results to the independent failure model . in section [ discussion ]",
    "we discuss open questions and conclude with some interesting remarks .",
    "in this section , we give a detailed description of the memory system .",
    "we start by introducing the terminology used to characterize memories and proceed to discuss the importance of ldpc codes .",
    "we explain the coding scheme and the error correction scheme employed in the proposed memory architecture .",
    "we then calculate the redundancy and complexity associated with the memories .",
    "a memory is a device in which information is stored at some time and retrieved at a later time @xcite .",
    "the memories under consideration store information in form of bits and are built from registers ( memory elements ) each of which can store a single bit .",
    "the information storage capability of a memory is the number of information bits it stores .",
    "consider a memory built out of reliable registers . to build a memory with information storage capability of @xmath2 bits",
    "requires @xmath2 registers .",
    "such a memory is termed as an irredundant memory .",
    "now , consider the problem of information storage with unreliable memory elements . due to the component failures , the information read out of the memory",
    "may not be identical to the information stored originally .",
    "hence , to ensure reliable storage , the information needs to be stored in coded form ( see @xcite for an excellent discussion on the importance of coded form ) . initially , a codeword from some error correcting code is stored in the memory .",
    "the unreliable nature of the memory elements introduces errors in the registers and the contents of the memory differ from the initial state . to ensure reliability ,",
    "a correcting circuit is employed which performs error correction and updates the contents of the registers with an estimate of the original codeword .",
    "hence , a fault - tolerant memory system ( referred to as memory system or simply memory henceforth ) consists of memory elements ( referred to as storage circuit ) and a correcting circuit .",
    "the correcting circuit is also built of unreliable components .",
    "the coding of information along with the correcting circuit introduce redundancy into the memory system .",
    "such redundant memories are characterized by two closely related parameters , namely , complexity and redundancy .",
    "the _ complexity _ of a memory is the number of components within the memory ( a component is a device which either performs an elementary operation or stores a single bit where an elementary operation is any boolean function of two binary operands @xcite ) .",
    "the _ redundancy _ of a memory is the ratio of the complexity of the memory to the complexity of an irredundant memory which has the same information storage capability @xcite .",
    "it should be noted that there can be many memory architectures with different complexities but the same information storage capability .",
    "another important characteristic of a memory is reliability .",
    "we say that arbitrarily reliable information storage is possible in a memory if the probability of memory failure can be made arbitrarily small .",
    "to quantify the reliability of a memory system , it is important to first define what constitutes a memory failure .",
    "let a memory failure be defined as an event in which the word read out of memory is not equal to the original codeword .",
    "arbitrarily reliable information storage is not possible with such a definition of memory failure .",
    "this is due to the fact that the probability of failure is lower bounded by the probability of failure of components in the final step of extracting the information bits .",
    "hence , we define a failure in the following manner . associated with each codeword in a code is a decoding equivalence class , i.e. , the set of words which decode to that particular codeword when decoded with a decoder built of reliable components .",
    "if the contents of the memory do not belong to the decoding equivalence class of the original codeword , we say a memory failure has occurred . the _ storage capacity",
    ", @xmath0 , of a memory is a number such that for all memory redundancies greater than @xmath1 , arbitrarily reliable information storage is possible @xcite .",
    "the memories under consideration store information in form of bits and therefore we restrict our attention to binary codes in this paper .",
    "an @xmath3 binary block code maps a message block of @xmath2 information bits to a binary @xmath4-tuple @xcite .",
    "the rate @xmath5 of the code is given by @xmath6 .",
    "an @xmath3 binary linear block code , @xmath7 , is a subspace of @xmath8 of dimension @xmath2 @xcite .",
    "a parity check matrix @xmath9 of @xmath7 is a matrix whose columns generate the orthogonal complement of @xmath7 , i.e. , an element @xmath10 of @xmath8 is a codeword of @xmath7 iff @xmath11 @xcite .",
    "the information storage capability of a memory depends on the type of the code employed in the correcting circuit .",
    "hence , a memory employing an @xmath3 block code has information storage capability of @xmath2 bits .",
    "taylor in @xcite argues that no decoding scheme other than iterative decoding of ldpc codes can achieve non - zero storage capacity .",
    "ldpc codes @xcite are a class of linear block codes which can be defined by sparse bipartite graphs @xcite .",
    "let @xmath12 be a bipartite graph with two sets of nodes : @xmath4 variable ( bit ) nodes and @xmath13 check ( constraint ) nodes .",
    "the check nodes ( variable nodes ) connected to a variable node ( check node ) are referred to as its neighbors .",
    "the degree of a node is the number of its neighbors .",
    "this graph defines a linear block code of length @xmath4 and dimension at least @xmath14 in the following way : the @xmath4 variable nodes are associated to the @xmath4 coordinates of codewords .",
    "a vector @xmath15 is a codeword if and only if for each check node , the sum of its neighbors is zero .",
    "such a graphical representation of an ldpc code is called the tanner graph @xcite of the code .",
    "the adjacency matrix of @xmath12 gives @xmath9 , a parity check matrix of @xmath7 .",
    "an @xmath16 regular ldpc code has a tanner graph with @xmath4 variable nodes each of degree @xmath17 and @xmath18 check nodes each of degree @xmath19 .",
    "this code has length @xmath4 and rate @xmath20 @xcite .",
    "it should be noted that the tanner graph is not uniquely defined by the code and when we say the tanner graph of an ldpc code , we only mean one possible graphical representation .      the complexity and redundancy of a fault - tolerant memory depend on the coding scheme as well as the decoding algorithm employed in updating the contents of the memory .",
    "we now explain our memory architecture in detail .    at time @xmath21 , a codeword from an @xmath16 regular binary ldpc code is written into the storage circuit consisting of @xmath4 registers each of which can store a single bit .",
    "the @xmath4 bits of the codeword correspond to the @xmath4 variable nodes in the tanner graph , @xmath12 , of the code .",
    "the contents of the registers are updated at times @xmath22 , @xmath23 .",
    "the update rules can be explained by defining messages along the edges in @xmath12 . for a variable node @xmath24 ( check node @xmath25 ) ,",
    "let @xmath26 ( @xmath27 ) denote the edges incident on @xmath24 ( @xmath25 ) .",
    "each edge @xmath28 is associated with a variable node @xmath24 and a check node @xmath25 .",
    "let @xmath29 and @xmath30 represent the messages passed on an edge @xmath28 from variable node to check node and check node to variable node at time @xmath31 respectively .",
    "let @xmath32 denote the value of variable node @xmath24 at time @xmath31 .",
    "then the update at time @xmath31 is given by the following algorithm :    * algorithm a *    * for each edge @xmath28 and corresponding variable node @xmath24 @xmath33 * for each edge @xmath28 and corresponding check node @xmath25 @xmath34 * for each variable node @xmath24 + @xmath35    the algorithm can be interpreted in the following manner .",
    "every variable node sends an estimate of its value to the neighboring check nodes .",
    "a check node calculates an estimate of a neighboring variable node by computing the modulo two sum of all the remaining @xmath36 neighboring variable nodes .",
    "each variable node receives @xmath17 estimates , one from each neighboring check and the majority of these estimates is the updated value of the node .",
    "_ remarks : _ we assume that the update is instantaneous and use @xmath37 and @xmath38 to denote the value of variable @xmath24 just before and after the update respectively .",
    "we note that the algorithm presented above is a slight modification of the parallel bit flipping algorithm proposed in @xcite .",
    "ldpc codes can achieve non - zero capacity due to the fact that the redundancy of the ldpc codes memory increases linearly with the information storage capability .",
    "the complexity of the logic gates needed to perform decoding depend only on @xmath17 and @xmath19 and not on the length of the code .",
    "so the redundancy remains bounded even as the code length tends to infinity .",
    "we now calculate the complexity and redundancy associated with our fault - tolerant memory architecture .",
    "the storage circuit consists of @xmath4 registers each of which can store a single bit and hence has complexity @xmath4 .",
    "the correcting circuit consists of logic gates ( built from components ) needed to implement the update algorithm .",
    "the message sent from a check node to variable node involves computing the modulo two sum of @xmath39 bits which requires a @xmath36-input xor gate which can be implemented using @xmath40 two input xor gates ( a two input xor gate calculates modulo two sum of two bits ) .",
    "each check node needs to compute @xmath19 such estimates .",
    "therefore , the total number of two input xor gates is @xmath41 each variable node is updated based on the majority of the @xmath17 estimates received from its neighbors .",
    "this requires a @xmath17-input majority logic gate for every variable node whose complexity we denote by @xmath42 .",
    "hence , the complexity of the memory system is @xmath43 the memory has information storage capability of @xmath44 bits and the complexity of an irredundant memory with the same information storage capability is @xmath44 .",
    "the redundancy of the fault - tolerant memory is therefore @xmath45",
    "the storage capacity of a memory depends on the type of failures in the components .",
    "a logic gate is said to have failed if its output is flipped .",
    "a register is said to have failed if the bit stored in it is flipped . in this paper",
    "we consider the adversarial failure model also referred to as bit flipping channel model . in the adversarial model ,",
    "the failures occur in the worst case fashion but no more than a fixed fraction of the components fail at any given time .",
    "in other words , the number of failures is bounded for a given number of components . as the number of components increases",
    "so does the number of failures .",
    "we denote the fraction of memory element failures in a time interval @xmath46 by @xmath47 , fraction of two input xor gate failures for every use by @xmath48 and fraction of @xmath17-input majority logic gate failures for every use by @xmath49 . as mentioned before",
    ", the component failures are transient and independent from one use to another .",
    "a memory system is said to tolerate a constant fraction of errors in all components if at any time at most a constant fraction of components can fail and no memory failure occurs in the system at all times @xmath50 .",
    "recall that , from our definition , a memory failure occurs if the contents in the memory do not belong to the decoding class of the originally stored codeword . in this section , we prove that the memory architecture proposed in section [ system ] can tolerate a constant fraction of failures in all the components .",
    "our proof is based on the expansion property of the underlying tanner graph , @xmath12 , of the code .",
    "@xcite a tanner graph @xmath12 of a @xmath16 ldpc code is a @xmath51 expander if for every subset @xmath52 of at most an @xmath53 variable nodes , at least @xmath54 check nodes are incident to @xmath52 .",
    "the definition of expander is much more general but we restrict our attention to tanner graphs of ldpc codes .",
    "sipser and spielman in @xcite proposed a class of asymptotically good error correcting codes based on expander graphs known as expander codes .",
    "they proposed two simple bit flipping algorithms , namely , serial and parallel and showed that when the underlying graph has sufficient expansion , these algorithms can correct a fixed fraction of errors .",
    "ldpc codes are a special case of expander codes in which the expander graph is the tanner graph of the ldpc code .",
    "we describe the parallel bit flipping algorithm and interested readers are referred to @xcite for details about serial bit flipping .",
    "we say that a constraint is satisfied by a setting of variables if the sum of the variables in the constraint is even ; otherwise , the constraint is unsatisfied .",
    "the set of variable nodes ( bits ) which differ from their original value are known as corrupt variables .    * parallel bit flipping algorithm *    * in parallel , flip each variable that is in more unsatisfied than satisfied constraints . * repeat until no such variable remains .",
    "the following theorem from @xcite gives the sufficient conditions for the parallel bit flipping algorithm to correct a constant fraction of errors .",
    "[ thm1 ] let @xmath12 be a @xmath55 expander over @xmath4 variable nodes , for any @xmath56 .",
    "then , the simple parallel decoding algorithm will correct any @xmath57 fraction of error after @xmath58 decoding rounds .",
    "also , if @xmath59 denotes the set of corrupt variables in the input and @xmath60 , then the parallel decoding algorithm produces a word with at most @xmath61 corrupt variables after one decoding round .",
    "see @xcite    from lemma [ thm1 ] , it is clear that a word belongs to the decoding class of a codeword as long as the fraction of corrupt variables ( bits ) is less than @xmath62 .",
    "note that algorithm a is a slight modification of one iteration of the parallel bit flipping algorithm of @xcite . in the parallel bit flipping algorithm , every check node indicates to its neighboring variable node if it is satisfied or not . in algorithm",
    "a every check node gives an estimate of the variable node .",
    "theoretically , both the algorithms are equivalent but we use the algorithm a as it has lesser redundancy .",
    "we now state and prove our main theorem .",
    "[ thm2 ] let @xmath12 be a @xmath63 expander for any @xmath56 .",
    "the proposed memory architecture can tolerate constant fraction of errors in all the components if @xmath64    at @xmath21 , a codeword from an @xmath16 ldpc code with tanner graph @xmath12 is written into the memory .",
    "the contents are updated at times @xmath65 , @xmath23 , by running algorithm a. we bound the number of corrupt variables at time @xmath31 .",
    "let @xmath66 denote the fraction of corrupt variables at time @xmath31 .",
    "we establish bounds on @xmath66 for all @xmath31 .",
    "we first prove the following .",
    "let @xmath67 , denote an infinitesimal duration of time .",
    "if @xmath68 then , @xmath69 let @xmath70 denote the set of corrupt variables at time @xmath31 .",
    "@xmath71 since @xmath72 , a decoder built with reliable gates outputs a word with at most @xmath73 corrupt variables ( by lemma [ thm1 ] ) .",
    "we now bound the number of errors introduced due to the faulty nature of the decoder .",
    "each xor gate failure can corrupt at most one variable and each majority logic gate failure can corrupt at most one variable .",
    "so , @xmath74 eq . [ eq2 ] bounds the number of corrupt variables at the end of @xmath75 correcting cycle .",
    "however , in the time interval @xmath76 , at most @xmath77 variables can get corrupted due to failures in memory elements .",
    "therefore , the time at which there are maximum number of corrupt variables is just before the start of a correcting cycle , i.e. , @xmath78 hence , it suffices to bound @xmath79 for @xmath80 .",
    "@xmath81 dividing eq .",
    "[ eq1 ] by @xmath4 gives @xmath82 since @xmath83 it follows that @xmath84 hence , @xmath85 since , the fraction of corrupt variables is less than @xmath86 , the contents of storage circuit correspond the decoding class of original codeword and hence , by our definition , no memory failure occurs .",
    "it is instructive to see the behavior of the memory in the absence of the correcting circuit . in any time interval of @xmath46 seconds , at most @xmath87 fraction of the memories may fail .",
    "after sufficiently long time , the fraction of corrupt variables becomes more than @xmath86 and a memory failure occurs .",
    "the presence of a correcting circuit ensures that at any time the number of corrupt variables remains less than the correcting capability of the code .",
    "however , for a given expander there is a loss in the tolerable memory failure due to the faulty nature of the gates as well as the iterative nature of the decoder . consider the case of where decoder is reliable and failures occur only once .",
    "the tolerable fraction of errors for a given expander is close to @xmath86 . in the case of memories with unreliable memory elements but reliable logic gates ,",
    "the tolerable fraction of memory errors is close to @xmath88 .",
    "the reduction by a factor of @xmath89 occurs due to the fact that decoder is iterative in nature and needs multiple rounds to converge to the codeword .",
    "one round of error correction decreases the errors by a factor of @xmath90 and @xmath77 new errors might be introduced due to memory failures . in the extreme case of",
    "@xmath91 we have a decoder which takes just one step to correct all the corrupt variables , in which case the tolerable failure rate is arbitrarily close to @xmath86 .",
    "the faulty nature of the decoder further reduces the tolerable memory failure rate .",
    "given the values of @xmath92 , a code based on graph with sufficient expansion can be chosen to build a fault tolerant memory .",
    "it is well known that a random graph is a good expander with high probability ( see @xcite and references therein ) . in the next section ,",
    "we illustrate this fact with a few examples .",
    "in this section , we illustrate with specific numerical examples the redundancies and tolerable failure rates associated with different values of @xmath17 and @xmath19 .",
    "we first make the following observations .",
    "the redundancy of a memory system depends on the parameters @xmath17 and @xmath19 of the ldpc code used .",
    "different values of @xmath17 and @xmath19 can result in same redundancy . to compare across different values of @xmath17 and @xmath19",
    ", the values of @xmath93 and @xmath94 have to be chosen consistently .",
    "how @xmath93 and @xmath94 scale with @xmath17 depends on the technology and implementation .",
    "assuming that all gates are built out of universal nand gates also does not answer the question fully as different implementations can lead to different values .",
    "hence for the sake of illustration we consider a specific implementation .",
    "it should be noted that the subsequent discussion is for illustration purpose only .",
    "accurate analysis for a given case can be carried out along the lines of the method we present in this section . for a given implementation",
    ", we fix the the values of @xmath17 and @xmath19 thereby fixing the redundancy as well as @xmath94 and @xmath95 .",
    "we then use the bounds on the achievable expansion of a @xmath96 regular bipartite graph to find bounds on the value of @xmath97 .",
    "this in turn provides bounds on the value of @xmath98 for fixed @xmath17 and @xmath19 .",
    "[ redundancy9 ]        [ redundancy34 ]        [ alpha9 ]        [ alpha34 ]          recall that the redundancy of a memory system is given by @xmath45 for a fixed @xmath17 , @xmath99 is minimum for a certain @xmath19 depending on the value of @xmath93 .",
    "for example , if @xmath100 , then it can be shown that @xmath101 minimizes the redundancy .",
    "this implies that a rate @xmath102 code has the least redundancy for a given @xmath17 .",
    "[ redundancy9 ] and fig .",
    "[ redundancy34 ] show the dependence of the redundancy on @xmath19 for a given value of @xmath17 .",
    "we make use of the following theorem from @xcite to find an upper bound @xmath103 for a given @xmath17 and @xmath19 .    [",
    "theorem 25 , @xcite ] : let @xmath104 be a bipartite graph between @xmath4 @xmath25-regular vertices and @xmath105 @xmath106-regular vertices . for all @xmath107 , there exists a set of @xmath53 @xmath25-regular vertices with at most @xmath108 it should be noted that the upper bound is tight for higher values of @xmath25 .    using this theorem",
    ", we can find an upper bound on @xmath103 for a given @xmath17 and @xmath19 .",
    "it should also be noted that we look for graphs which expand by at least a factor of @xmath109 .    the following proposition from @xcite addresses the issue of existence of expanders .    [ proposition 6 , @xcite ] : let @xmath110 and @xmath111 be any fixed constants , and let @xmath25 be such that @xmath112 is an integer which is at least 2 .",
    "then for any @xmath113 such that @xmath114 there is a tanner graph with @xmath4 variable nodes , @xmath13 check nodes , and regular left degree @xmath25 which is an @xmath115-expander , where @xmath116 it should be noted that the notation for expanders is different in @xcite .",
    "also , the proof does not guarantee that all the check nodes have same degree .",
    "this proposition guarantees the existence of graphs with sufficient expansion and can be used to derive a lower bound on @xmath103 for given @xmath17 and @xmath19 .",
    "this in turn proves existence of memories which can tolerate @xmath117 and @xmath94 fraction of failures in respective components as long as @xmath118",
    "[ alpha9 ] and [ alpha34 ] illustrate the upper bounds and lower bounds on @xmath103 for @xmath119 and @xmath120 respectively .",
    "we remark that the bounds have been derived numerically and we do not attempt to give closed form expressions for the bounds as the results are for illustration purpose only .",
    "as mentioned in the introduction , taylor @xcite , @xcite was the first to investigate the capacity and fault - tolerant architectures of storage systems built entirely from unreliable components .",
    "his results were refined by kuznetsov @xcite .",
    "the aim of taylor and kuznetsov ( tk ) was to derive results analogous to the ones derived by shannon on the capacity of communication systems .",
    "the spirit and methodology of taylor and kuznetsov s work @xcite , @xcite is similar to gallager s",
    "results @xcite on ldpc codes .",
    "the bounds on probability of error are given for an ensemble of regular random ldpc codes of infinitely large length used in the correcting circuit .",
    "they are obtained under the assumptions that the bits in memory elements remain independent during the process of correction , i.e. , under the assumption that the girth ( the length of the shortest cycle ) of the tanner graph corresponding to a code is infinitely large .",
    "taylor and kuznetsov considered a failure model in which a faulty component , generally a logic gate or a memory element , is subject to transient faults , i.e. , faults that manifest themselves at particular time steps but do not necessarily persist for later times @xcite .",
    "it is also assumed that gates fail independently of each other , and that the defects are not permanent , i.e. , a gate that malfunctioned at some point in time may give correct output subsequently and that failure occurs by flipping the correct result with some probability @xmath121 , i.e. , if the correct result is `` 1 '' , the gate gives `` 0 '' and vice versa .",
    "such failure mechanism is referred to as von neumann type of error or as independent failure model .",
    "a faulty gate or memory element in this case can be modeled as as a binary symmetric channel ( bsc ) with crossover probability @xmath121 .",
    "the information to be stored is first encoded by a @xmath16 regular binary ldpc code .",
    "the stored codeword @xmath122 consist of bits @xmath123 referred also as variables .",
    "each variable bit @xmath123 is involved in @xmath17 parity - check equations by @xmath124 , where @xmath9 is an @xmath125 parity check matrix and all operations are in binary field .",
    "the degree of each check node is @xmath19 .",
    "the vector @xmath126 is called syndrome and @xmath127 corresponds to the value of @xmath128 parity - check sum for @xmath129 .",
    "parity check @xmath127 is said to be satisfied if @xmath130 and unsatisfied if @xmath131 . a set of parity checks involving bit @xmath132 is @xmath133 .",
    "after encoding , every coded bit @xmath132 is replaced with @xmath17 bit - copies of itself @xmath134 and stored in @xmath17 registers .",
    "all bit - copies initially have the same value .",
    "new estimates of each of these copies are obtained by using one combination of @xmath135 checks .",
    "note that there are exactly @xmath136 combinations .",
    "the estimates are obtained as follows .    1 .",
    "evaluate parity checks for each bit - copy ( exclude one distinct parity check from the original set of checks for each bit - copy ) .",
    "2 .   flip the value of a particular bit - copy if half or more of the parity checks are unsatisfied .",
    "3 .   iterate ( 1 ) and ( 2 ) .",
    "the tanner graph description of ldpc codes was unknown at the time of taylor s paper .",
    "it is easy to see that each bit copy corresponds to an edge in the tanner graph .",
    "the variable node corresponding to the edge is the corresponding bit and the check node is the parity check that is excluded in the estimation of that bit copy .",
    "if the update scheme is modified so that the check nodes indicate an estimate of the bit copy , then the update rule is an exact implementation of the hard decision message passing algorithm ( known as gallager b algorithm ) for iterative decoding of ldpc codes ( see @xcite for a more detailed discussion ) .",
    "such an equivalence is of great significance as expander graph arguments have been applied to message passing algorithms @xcite and allows us to extend these results to the case of unreliable gates also .",
    "the complexity and redundancy of the original tk scheme are given by @xmath137      in this section , we extend our results to the independent failure model . by chernoff bounds",
    "@xcite , it follows that a code which can correct a fraction of @xmath138 errors achieves exponentially small probability of error on the bsc with crossover probability @xmath121 @xcite . in other words , if there are @xmath4 components which can fail independently with probability @xmath121 , then the probability that more than @xmath139 fraction of the components fail at any time is bounded by @xmath140 where @xmath141 is the kullback - leibler divergence between bernoulli random variables with parameters @xmath142 and @xmath143 respectively .",
    "now consider a memory architecture built from unreliable components subject to independent failures .",
    "let @xmath144 denote the probability of failure of memory element in time interval @xmath46 , @xmath145 denote the probability of failure per use of an xor gate and a @xmath17-input majority logic gate respectively .",
    "also , let @xmath146 be such that @xmath147 , @xmath148 and @xmath149 .",
    "let @xmath150 denote the probability of memory failure at time @xmath31 .",
    "for @xmath151 and @xmath12 satisfying the conditions in theorem [ thm2 ] , we now have the following theorem    [ thm3 ] the proposed memory architecture has the following parameters for the independent failure model :    1 .",
    "information storage capability @xmath152 [ one ] 2 .",
    "@xmath153 [ two ] 3 .",
    "@xmath154 [ three ]    ( [ one ] ) and ( [ two ] ) follow from our discussion in section [ system ] .",
    "a memory failure may occur if the fraction of components which fail at a time is more than the tolerable fraction of errors . in @xmath155 time steps ,",
    "the correcting circuit is run for @xmath155 times .",
    "the memory registers can fail @xmath155 times .",
    "hence , we have @xmath156    the bound on the probability of memory failure given in theorem [ thm3 ] is a very weak bound and we do not try to improve it .",
    "theorem [ thm3 ] establishes the fact that in the proposed memory architecture , probability of memory failures decreases exponentially with the code length while the redundancy remains bounded .",
    "theorem [ thm3 ] has been stated in the same form as the main theorem in kuznetsov s paper @xcite .",
    "hence , the proposed memory architecture has exponentially decreasing probability of memory failure in code length and redundancy which is roughly @xmath17 times less the tk scheme .",
    "taylor in @xcite remarks that memories have an associated non - zero storage capacity but an explicit calculation of the capacity is , in general , a difficult problem . for a given failure mechanism ,",
    "finding storage capacity involves calculating the minimum redundancy to achieve arbitrarily low probability of error .",
    "the redundancy is a function of the coding scheme as well as the decoding algorithm .",
    "the tk scheme as well as the proposed memory architecture have finite redundancies and only give bounds on the storage capacity . in this paper , we have shown that there exist reliable memories with redundancies less than that of the tk scheme .",
    "this implies that the proposed memory architecture improves the bound by a factor of @xmath17 at least in a few cases .",
    "the explicit calculation of the storage capacity still remains an unsolved problem .",
    "while the proposed architecture has less redundancy , the tk scheme may achieve better error exponents as it employs message passing algorithm which is in general more powerful than the parallel bit flipping algorithm .",
    "it is worth noting that taylor in @xcite describes the parallel bit flipping algorithm as a scheme for the update rule .",
    "he remarks that such an algorithm leads to complex interrelation between the errors as on successive iterations the values of the bits involved in the estimation of new value of each bit depend on previous value of the bit .",
    "we overcome this problem in this paper by using expander arguments . also , extending the results from the adversarial model to the independent failure model using chernoff bounds results in very weak bounds on the probability of memory failure . using expander arguments directly for the independent failure model for both the proposed architecture and the tk scheme might result in better error exponents as well as lead to tighter bounds on the capacity .",
    "another problem which needs to be investigated is the bounds on the probabilities of failures of components , i.e. , what are the upper bounds on the probability of failure of various components .",
    "sipser and spielman in @xcite provided explicit construction of codes which can correct a certain fraction of errors .",
    "the fraction was later improved by zemor in @xcite .",
    "barg and zemor in @xcite proved that expander codes achieve capacity on the bsc under iterative decoding .",
    "guruswami and indyk in @xcite proposed linear time encodable and decodable codes which achieve optimal error correction performance .",
    "study of fault - tolerant memory architectures based on these codes can provide the required bounds .",
    "however , these codes do not directly imply a specific implementation as is the case with parallel bit flipping algorithm .",
    "we noted earlier that capalbo _",
    "et al_. @xcite gave an explicit construction of expanders .",
    "however , the redundancies associated with such expanders are typically very high .",
    "this serves as another reason to consider expander codes and other linear time decodable codes based on expanders .",
    "the proposed architecture as well as the tk scheme employ coding scheme based on regular ldpc codes .",
    "the works of richardson , urbanke and shokrollahi @xcite and luby , mitzenmacher , shokrollahi , and spielman @xcite show that well designed irregular codes perform close to capacity .",
    "burshtein and miller s work on expander graph arguments for message passing @xcite is also based on irregular graphs .",
    "investigating memory architectures based on irregular codes may serve as another avenue to study the storage capacity problem .",
    "the authors would like to thank milos ivkovic for fruitful discussions .",
    "j.  v. neumann , _ probabilistic logics and the synthesis of reliable organisms from unreliable components _ , ser .",
    "automata studies.1em plus 0.5em minus 0.4emprinceton : princeton university press , 1956 , pp . 4398 .",
    "m.  capalbo , o.  reingold , s.  vadhan , and a.  wigderson , `` randomness conductors and constant - degree lossless expanders , '' in _",
    "stoc 02 : proceedings of the thiry - fourth annual acm symposium on theory of computing_.1em plus 0.5em minus 0.4emnew york , ny , usa : acm press , 2002 , pp . 659668 .",
    "a.  shokrollahi , `` an introduction to low - density parity - check codes , '' in _ theoretical aspects of computer science : advanced lectures_.1em plus 0.5em minus 0.4emnew york , ny , usa : springer - verlag new york , inc . , 2002 , pp .",
    "175197 .",
    "b.  vasic and s.  k. chilappagari , `` an information theoretical framework for analysis and design of nano - scale fault - tolerant memories based on low - density parity - check codes , '' _ ieee trans .",
    "circuits syst .",
    "i , reg . papers _ , accepted for publication .",
    "m.  g. luby , m.  mitzenmacher , m.  a. shokrollahi , and d.  a. spielman , `` improved low - density parity - check codes using irregular graphs , '' _ ieee trans .",
    "inform . theory _",
    "47 , no .  2 ,",
    "585598 , feb ."
  ],
  "abstract_text": [
    "<S> in this paper , memories built from components subject to transient faults are considered . </S>",
    "<S> a fault - tolerant memory architecture based on low - density parity - check codes is proposed and the existence of reliable memories for the adversarial failure model is proved . </S>",
    "<S> the proof relies on the expansion property of the underlying tanner graph of the code . </S>",
    "<S> an equivalence between the taylor - kuznetsov ( tk ) scheme and gallager b algorithm is established and the results are extended to the independent failure model . </S>",
    "<S> it is also shown that the proposed memory architecture has lower redundancy compared to the tk scheme . </S>",
    "<S> the results are illustrated with specific numerical examples . </S>"
  ]
}