{
  "article_text": [
    "there are many facets to the analysis of data in high dimensions , depending on the type of application , some relying on dimension reduction , others relying on variable selection and a  few employing both tactics .",
    "there has been considerable interest in dimension - reduction methods for the regression of a  real response @xmath0 on a  random vector of predictors @xmath1 since the introduction of sliced inverse regression [ sir ; @xcite ] and sliced average variance estimation [ save ; @xcite ] .",
    "a common goal of these and many other methods is to reduce the dimension of the predictor vector without loss of information about the response .",
    "the aim is to estimate a  reduction @xmath2 , @xmath3 , with the property that @xmath4 or , equivalently , @xmath5 [ @xcite ] . in this way",
    "@xmath6 is sufficient because it captures all the information about @xmath0 that is available from @xmath7 .",
    "sufficient reductions are not determined uniquely by this definition because any bijective transformation of @xmath6 is also sufficient .",
    "nearly all methods for sufficient dimension reduction ( sdr ) restrict attention to the class of linear reductions , which arise naturally in many contexts .",
    "linear reduction can be represented conveniently in terms of the projection @xmath8 of @xmath7 onto a  subspace @xmath9 . if @xmath10 , then @xmath11 is called a  dimension - reduction subspace . under mild conditions",
    "the intersection of any two dimension - reduction subspaces is again a  dimension - reduction subspace and that being so the central subspace @xmath12 , defined as the intersection of all dimension - reduction subspaces , is taken as the inferential target [ cook ( @xcite , @xcite ) ] .",
    "a minimal sufficient linear reduction is then of the form @xmath13 , where @xmath14 is any basis for @xmath12 .",
    "sdr has a  long history of successful application and is still an active research area .",
    "recent novel sdr methods include likelihood - based sufficient dimension reduction [ @xcite ] , kernel dimension reduction [ @xcite ] , shrinkage inverse regression estimation [ @xcite ] , dimension reduction for nonelliptically distributed predictors [ @xcite , @xcite ] , cumulative slicing estimation [ @xcite ] , dimension reduction for survival models [ @xcite ] and dimension reduction for spatial point processes [ @xcite ] .",
    "this body of work reflects three different but related frontiers in sdr : extensions that require progressively fewer assumptions , development of likelihood - based methods and adaptations for specific areas of application .",
    "almost all sdr methods rely on traditional asymptotic reasoning for support , letting the sample size @xmath15 with @xmath16 fixed .",
    "they nearly all require the inverse of a  @xmath17 sample covariance matrix and thus application is problematic when @xmath18 .",
    "since accurate estimation of a  general @xmath17 covariance matrix can require @xmath19 observations , it has seemed inevitable that sdr methods would encounter estimation problems when @xmath20 is not sufficiently large .",
    "@xcite , @xcite and others circumvented these issues by performing reduction in two stages , first replacing the @xmath16 predictors with @xmath21 principal components and then applying an sdr method to the regression of the response on the selected @xmath22 components .",
    "however , recent results on the eigenvectors of sample covariance matrices in high - dimensional settings raise questions on the value of such two - stage methods [ see , e.g. , @xcite ] .",
    "@xcite proposed an sdr method that avoids computation of inverses and reduces to partial least squares in a  special case .",
    "@xcite showed recently that the partial least squares estimator of the coefficient vector in the linear regression of @xmath0 on @xmath7 is inconsistent unless @xmath23 and this raises questions about the behavior of the cook et al .",
    "sdr estimator when @xmath20 is not large relative to @xmath16 .",
    "@xcite used the least squares formulation of sliced inverse regression originated by @xcite to develop a  regularized version that allows @xmath18 and achieves simultaneous predictor selection and dimension reduction .",
    "this seems to be a  promising method , but its asymptotic properties are unknown and it may not work well when the regression is not sparse .",
    "@xcite studied the asymptotic properties of a  family of sdr estimators , using a  scad - type penalty for variable selection in sparse regressions where the number of relevant variables is fixed as @xmath24 .",
    "their method requires that @xmath23 for consistency .    while sparsity is an important concept in high - dimensional regression , not all high - dimensional regressions are sparse .",
    "for example , near - infrared reflectance is often measured at many wavelengths to predict the composition of matter , like the protein content of a  grain .",
    "there is not normally an expectation that only a  few wavelengths are needed to predict content . while some wavelengths may be better predictors than others , it is the cumulative information provided by many wavelengths that is often relevant .",
    "partial least squares has been the dimension - reduction method of choice in this type of regression .",
    "the regressions implied by this and other nonsparse applications share similar characteristics : ( 1 ) the predictor vectors are high - dimensional and typical area - specific analyses have employed some type of dimension reduction ; ( 2 ) while assessing the relative importance of the predictors may be of interest , prediction is the ultimate goal ; ( 3 ) information on the response is thought to accumulate , albeit perhaps slowly , as predictors are added , and ( 4 ) sparsity is not a  driving notion .    in this article",
    "we introduce a  family of sdr methods for studying high - dimensional regressions that differs from past approaches in at least three important ways .",
    "first , we do not require sparsity but rather we emphasize _ abundant regressions _ where most of the predictors contribute some information about the response . in the logic of @xcite ,",
    "the bet - on - sparsity principle arose because , to continue the metaphor , there is otherwise little chance of a  reasonable payoff .",
    "we show in contrast that reasonable payoffs can be obtained in abundant regressions with prediction as the ultimate goal , leading to a  contrasting _ bet - on - abundance _ principle .",
    "second , sdr studies have largely focused on properties of estimators of @xmath12 .",
    "we bypass this step and instead consider the limiting behavior of estimators of the sufficient reduction @xmath25 itself , assuming that the dimension @xmath26 of @xmath6 is fixed . more specifically , letting @xmath27 denote an estimated reduction",
    ", we establish rates of convergence in the following sense .",
    "let @xmath28 denote a  new observation on  @xmath7 . if @xmath29 and if @xmath30 as @xmath31 , then @xmath32 is consistent for @xmath33 and its convergence rate is at least @xmath34 .",
    "third , we integrate recent work on the estimation of high - dimensional covariance matrices into our approach .",
    "in particular , we estimate a  critical matrix of weights by using sparse permutation invariant covariance estimation ( spice ) as developed by @xcite .    in sum , by considering the reduction",
    "@xmath6 itself rather than the central subspace @xmath12 , we both introduce a  novel viewpoint for addressing dimension reduction and develop theoretically grounded sdr methodology for @xmath18 regressions where other methods either have no asymptotic support or must necessarily fail .",
    "we describe the model for our study and its sufficient reduction in section  [ secmodelstate ] . the class of estimators that we use is described in section  [ secestimation ] , and stabilizing restrictions are presented in section  [ secsignal ]",
    ". sections  [ secwhat ] and [ secnormalerrors ] contain theoretical conclusions for selected estimators from the class described in section  [ secestimation ] .",
    "simulation results are presented in sections  [ secsimw ] and [ secsimulations ] .",
    "we turn to a  spectroscopy application in section  [ secdata ] and a  concluding discussion is given in section  [ secdiscussion ] . all proofs and",
    "additional simulation results are available in a  supplemental article [ @xcite ] .",
    "the following notational conventions will be used in our exposition .",
    "we use @xmath35 to denote the collection of all real @xmath36 matrices .",
    "we use @xmath37 and @xmath38 to denote the spectral and frobenius norms of @xmath39 .",
    "the largest and smallest eigenvalues of @xmath40 are denoted @xmath41 and @xmath42 .",
    "if @xmath40 , then @xmath43 is the diagonal matrix with diagonal elements equal to those of @xmath39 .",
    "@xmath44 is the operator that maps @xmath45 to  @xmath46 by stacking its columns .",
    "if @xmath47 and @xmath40 is symmetric and positive definite , then the operator that projects in the @xmath39 inner product onto @xmath48 , the subspace spanned by the columns of @xmath49 , has the matrix representation @xmath50 , and @xmath51 .",
    "@xmath52 indicates the projection onto @xmath48 in the usual inner product .",
    "a basis matrix for a  subspace @xmath53 of dimension @xmath26 is any matrix @xmath54 whose columns form a  basis for @xmath11 . for nonstochastic sequences @xmath55 and  @xmath56",
    ", we write @xmath57 if there are constants",
    "@xmath58 , @xmath59 and @xmath60 such that @xmath61 for all @xmath62 .",
    "similarly , for stochastic sequences @xmath55 and  @xmath63 , we write @xmath64 if @xmath65 and @xmath66 . for @xmath67 ,",
    "@xmath68 means that @xmath69 is positive definite .",
    "@xmath70 denotes the kronecker product , and @xmath71 means @xmath72 and @xmath0 are equal in distribution . deviating slightly from convention , we do not index quantities by @xmath20 and @xmath16 , preferring instead to avoid notation proliferation by giving reminders from time to time .",
    "we assume throughout that the data consist of independent observations @xmath73 , @xmath74 , and that @xmath16 is increased by taking additional measurements on each of @xmath20 sampling units .",
    "we treat the process of adding predictors as conditional on the response and so our approach is based on inverse reduction , @xmath75 , which is a  standard sdr paradigm .",
    "limits as @xmath76 are thus conditional on the responses unless specifically indicated otherwise .      the model that we employ is engendered by standard sdr assumptions .",
    "we first review those assumptions briefly and then give a  statement of the model .",
    "classical methods like sir require two predominant and well - known conditions for estimation of @xmath12 .",
    "the first , called the _ linearity condition _",
    ", insists that @xmath77 be a  linear function of @xmath78 , where @xmath14 is a  basis matrix for  @xmath12 .",
    "it must be valid only at a  true basis and not for all @xmath79 .",
    "this condition is generally regarded as mild since it holds to a  good approximation when @xmath16 is large [ @xcite ] .",
    "let @xmath80 denote the subspace spanned by @xmath81 as @xmath82 varies in the sample space of @xmath0 .",
    "then the linearity condition implies that @xmath83 , which is used as a  basis for estimating a  subspace of @xmath12 .",
    "the second , called the _ coverage condition _ , requires that @xmath84 and it too is generally regarded as mild in many applications . while the linearity and coverage conditions are standard requirements for root-@xmath20 consistent methods based on the inverse mean function @xmath85 , the actual performance of those methods depends also on the inverse variance function @xmath86 .",
    "for instance , @xcite concluded based on simulations that sir works best when @xmath86 is nonstochastic , and @xcite demonstrated analytically that sir can be quite inefficient when @xmath86 varies .",
    "we refer to the requirement that @xmath86 be nonstochastic as the _ covariance condition_. if the linearity and covariance conditions hold , then @xmath87 must be nonstochastic as well .",
    "this is related to the usual covariance condition@xmath88 is constant  used by save and other second - order methods .",
    "see @xcite , @xcite and @xcite for further discussion of these conditions .",
    "we assume the linearity , coverage and covariance conditions as a  basis for our study . under these conditions",
    "it can be shown straightforwardly that @xmath89 , where @xmath90 . consequently , letting @xmath91 be a  basis matrix for @xmath92 , we are led to the model @xmath93 where @xmath94 , the error vectors @xmath95 are independent copies of a  random vector @xmath96 with mean 0 and variance @xmath97 , and @xmath98 is the @xmath99th instance of an unknown vector - valued function @xmath100 with @xmath101 that gives the coordinates of @xmath102 in terms of @xmath103 and is independent of @xmath104 . in this representation @xmath105 .",
    "neither @xmath103 nor @xmath106 is identified in model  ( [ origmodel ] ) , because for any conforming nonsingular matrix @xmath39 , @xmath107 , leading to a  different parameterization .",
    "this nonuniqueness has been mitigated in past studies with @xmath16 fixed by requiring @xmath108 .",
    "however , that parameterization is not workable when allowing @xmath24 , and for this reason we adopt different restrictions .",
    "the next step is to reparameterize model  ( [ origmodel ] ) to satisfy constraints that will facilitate our development .",
    "specifically , we construct an equivalent model @xmath109 where @xmath110 , @xmath111 is the coordinate vector relative to the new basis matrix @xmath112 , and we center the @xmath113 s in the sample so that @xmath114 .",
    "let the rows of @xmath115 and @xmath116 be @xmath117 and @xmath118 , @xmath74 .",
    "without loss of generality , we impose on model ( [ vecmodel ] ) the constraints that ( 1 ) @xmath119 , where @xmath120 is a  _ scaling matrix _ that is defined in section  [ scaling ] , and ( 2 ) @xmath121 is a  diagonal matrix where @xmath122 is a  symmetric @xmath17 population _ weight matrix _ that is discussed in section [ secestimation ] . to see how this",
    "is done starting from model ( [ origmodel ] ) , let @xmath123 and let @xmath124 be an orthogonal matrix so that @xmath125 is a  diagonal matrix .",
    "then @xmath126 , where @xmath127 and @xmath128 satisfy the constraints by construction . model  ( [ vecmodel ] ) can be represented also in matrix form as @xmath129 where @xmath130 and @xmath131 have rows @xmath132 and @xmath133 , and @xmath134 has mean @xmath135 and variance @xmath136 .",
    "since bijective transformations of a  sufficient reduction are themselves sufficient , we define @xmath6 to be the coordinates of the projection of @xmath137 onto @xmath138 in the @xmath139 inner product : @xmath140 where the first factor @xmath141 stabilizes @xmath142 as @xmath24 .",
    "without further structure it is not possible to use model  ( [ vecmodel ] ) to estimate @xmath25 since the coordinate vectors @xmath143 are unknown . however , estimation is possible by approximating the coordinate vectors as @xmath144 , where @xmath145 , @xmath146 , and @xmath147 is the @xmath99th realization of a  known user - selected vector - valued function @xmath148 . without loss of generality",
    "we center the sample @xmath149 .",
    "let @xmath150 be the matrix with rows @xmath151 , and assume that @xmath152 and @xmath153 , unless @xmath154 and then of course @xmath155 is nil .",
    "we next describe the class of estimators we use , postponing discussion of possible choices for @xmath156 until section  [ choose - f ] .",
    "the class of reduction estimators @xmath27 that we study is based on using estimators of @xmath157 from a  subclass of the family of inverse regression estimators proposed by @xcite : @xmath158 where the minimization is over @xmath159 , @xmath160 and @xmath161 subject to the constraints that @xmath162 and that @xmath163 is a  diagonal matrix .",
    "a  particular estimator is determined by the choice of the sample weight matrix @xmath164 with @xmath165 being a  population version .",
    "some instances of  @xmath166 that we introduce later correspond to @xmath167 , @xmath139 and @xmath168 .",
    "we next report the estimators from this family .",
    "a sketch of the derivation is available in the supplemental article [ @xcite ] .",
    "it is easily seen that @xmath169 .",
    "let @xmath170 , and let @xmath171 denote the matrix of regression coefficients from the least squares fit of @xmath7 on @xmath156 , assuming that @xmath172 .",
    "also , let the columns of @xmath173 be the first @xmath26 eigenvectors of @xmath174 assuming that the @xmath26th eigenvalue of @xmath175 is strictly larger than the @xmath176st eigenvalue .",
    "this assumption will be true with probability 1 for the choices of  @xmath166 considered here .",
    "then the estimators are @xmath177 , @xmath178 , @xmath179 and @xmath180 the diagonal elements of the diagonal matrix @xmath181 consist of the first ( largest ) @xmath26 eigenvalues of @xmath175 , and @xmath182 is the coefficient matrix from the ols fit of @xmath183 on  @xmath184 .      clearly , we should strive to choose an @xmath185 so that , for some @xmath161 , @xmath113 is well approximated by @xmath186 for @xmath74 .",
    "this intuition is manifested in various calculations via the requirement that @xmath187 for all  @xmath20 . as an extreme first instance , suppose that @xmath188 . then @xmath189 and consequently  @xmath190 provides no information on @xmath138 .",
    "since @xmath191 , this requirement is essentially that the true coordinate vector @xmath192 is sufficiently correlated with its approximation , and it is equivalent to the condition derived by @xcite under a  pfc model with normal errors and @xmath16 fixed . in the remainder of this article",
    "we assume that , for all @xmath20 , @xmath193 where the order is nonstochastic because we condition on the observed values of @xmath0 in our formal asymptotic calculations .",
    "there are many ways to choose an appropriate @xmath156 in practice . under model  ( [ vecmodel ] ) , each coordinate @xmath194 , @xmath195 , of @xmath7 follows a  univariate linear model with predictor vector @xmath185 .",
    "when @xmath0 is quantitative we can use inverse response plots [ @xcite , chapter 10 ] of @xmath194 versus @xmath0 , @xmath195 , to gain information about suitable choices for @xmath156 .",
    "when @xmath16 is too large for a  thorough graphical investigation , which is the case in the context of this article , we can consider @xmath156 s that contain a  reasonably flexible set of basis functions , like polynomial terms in @xmath0 . in some regressions",
    "there may be a  natural choice for @xmath156 .",
    "for example , suppose that @xmath0 is categorical , taking values in one of @xmath58 categories @xmath196 , @xmath197 .",
    "we can then set @xmath198 and specify the @xmath199th coordinate of @xmath156 to be the indicator function @xmath200 .",
    "another option consists of `` slicing '' the observed range of a  continuous @xmath0 into @xmath58 bins ( categories ) @xmath196 , @xmath197 . a rudimentary set of basis functions",
    "can then be constructed by specifying the @xmath199th coordinate of @xmath156 as for the case of a  categorical @xmath0 .",
    "this has the effect of approximating each conditional mean @xmath201 as a  step function of @xmath0 with @xmath58 steps .",
    "this option is of particular interest because it exactly reproduces sirs estimate of @xmath12 in the traditional large-@xmath20 setting [ @xcite ] .",
    "cubic splines with the endpoints of the bins as the knots are a  more responsive option that is less prone to loss of intra - slice information .",
    "our goal is to study the limiting behavior of@xmath202 , where @xmath6 and @xmath27 are given by  ( [ popred ] ) and  ( [ samplered ] ) .",
    "this difference depends on the choice of @xmath166 and the behavior of the true reduction @xmath6 as @xmath24 . in this section",
    "we measure and constrain the interaction between  @xmath165 and the model .",
    "we also place weak constraints on @xmath6 to help ensure well - behaved limits .",
    "the context described in this section will be assumed to hold throughout this article , without necessarily being declared in formal statements .",
    "we will revisit these constraints occasionally during the discussion , particularly when some of them are implied by other structure .",
    "since bijective transformations of sufficient reductions are also sufficient , we need to have @xmath32 and @xmath33 in the same scale to ensure that their difference @xmath202 is a  useful measure of agreement .",
    "this can be accomplished by choosing the scaling matrix @xmath203 so that the first model constraint stated following  ( [ vecmodel ] ) becomes @xmath204 , which is a  rank-@xmath26 matrix by ( [ rank ] ) .",
    "this choice ensures that @xmath112 is the lead term in the asymptotic expansion of @xmath205 , which places @xmath32 and @xmath33 on the same scale .",
    "we define the _ signal rate _ to be a  positive monotonically increasing function @xmath206 that measures the rate of increase in the population signal : let @xmath207 , which is a  diagonal matrix because @xmath121 is diagonal by construction .",
    "then @xmath208 is chosen to meet the requirement @xmath209 where @xmath210 is a  diagonal matrix that is assumed to be positive definite with finite elements .",
    "the signal rate is not needed for computation of @xmath27 but it will play a  key role in later developments .",
    "it depends via @xmath165 on the specific estimator selected , although this is not indicated notationally . when @xmath211 the bounds @xmath212 can be used to aid intuition on the magnitude of @xmath208 by presuming properties of @xmath112 and @xmath165 .",
    "for example , consider regressions in which @xmath213 and @xmath214 are bounded away from 0 and @xmath215 as @xmath216 and a  positive fraction @xmath217 , @xmath218 , of the rows of @xmath112 is sampled from a  multivariate density with finite second moments and the other rows of @xmath112 are all zero vectors .",
    "then the number of nonzero rows @xmath219 , @xmath220 , and we say that the regression has an _ abundant signal_. regressions with _ near abundant signals _ like @xmath221 may often perform similarly in practice to regressions with abundant signals .",
    "it is technically possible to have regressions in which @xmath222 , but we would not normally expect this in practice . on the other extreme , sparse regressions are those in which @xmath223 , so only finitely many predictors are relevant or the signal accumulates very slowly as @xmath24 .",
    "regressions with _ near sparse signals _ have , say , @xmath224 . typically we will use @xmath208 in definitions and formal statements , but use the abbreviated form @xmath225 otherwise .",
    "we assume in the rest of this article that  ( [ h ] ) holds .",
    "our second requirement is that , as @xmath24 , the spectral norm @xmath226 converges to a  finite constant , which may be 0 .",
    "if this is not so , then the variance of some linear combinations @xmath227 will diverge as @xmath216 , and the very notion of dimension reduction for large-@xmath16 regressions becomes problematic .",
    "a reduction @xmath25 is stable if @xmath228 .",
    "the following lemma gives a  sufficient condition for stability that incorporates the signal rate . in preparation , define @xmath229 which is manifested in various asymptotic expansions as a  measure of the agreement between @xmath165 and @xmath139 .",
    "[ lemmastable ] if @xmath230 , then reduction ( [ popred ] ) is stable .    according to lemma  [ lemmastable ] , if @xmath231 , then the reduction is stable regardless of @xmath225 .",
    "all regressions in which @xmath232 is bounded yield stable reductions .",
    "bounded eigenvalues have been required in various studies to avoid ill - conditioned covariance matrices [ see , e.g. , @xcite and @xcite ] . more generally , the requirement for a  stable reduction is that @xmath208 must increase at a  rate that is no less than the rate at which @xmath232 increases . in particular , if @xmath233 , then the sufficient condition of lemma  [ lemmastable ] requires that @xmath232 is bounded .",
    "the rates that we develop depend on the functions @xmath234^{1/2}$ ] and @xmath235 .",
    "the interpretation and roles of these functions will be discussed later ; for now we state across - the - board requirements that , as @xmath76 , @xmath236 these functions will nearly always be written without their arguments .",
    "we assume in the rest of this article that all regressions are stable and that the orders given in  ( [ kappa ] ) hold .",
    "so far we have assumed only that the errors @xmath95 are independent copies of the random vector  @xmath104 which has mean 0 and variance @xmath237 .",
    "we impose two additional constraints to ensure well - behaved limits : as @xmath24 ,    @xmath238 , and    @xmath239 .    condition ( w.1 ) , which is equivalent to @xmath240 , seems quite mild .",
    "for example , if we use unweighted fits with @xmath241 , then this condition is simply that the average error variance @xmath242 is bounded .",
    "condition  ( w.2 ) can be seen as placing constraints on @xmath165 and on the fourth moments of  @xmath104 .",
    "it too seems mild , although it is more constraining than the first condition .",
    "the following lemma describes a  few settings in which condition ( w.2 ) holds .",
    "[ lemmaw ] let @xmath243 so that @xmath244 and @xmath245 .",
    "let @xmath246 , @xmath247 . then :    if @xmath248 , then condition implies condition .",
    "if @xmath231 and if @xmath249 as @xmath24 , then condition holds .    if the elements @xmath250 of @xmath251 have symmetric distributions or are independent and if @xmath252 as @xmath24 , then condition implies condition",
    "we assume in the rest of this article that conditions ( w.1 ) and ( w.2 ) hold .",
    "in this section we describe our results for the limiting reduction when @xmath166 converges in the spectral norm . specifically ,",
    "we require the following two conditions :    there exists a  population weight matrix @xmath253 so that the spectral norm of @xmath254 converges to 0 in probability at rate at most @xmath255 as @xmath256 ; equivalently , @xmath257 as @xmath258 .    @xmath259 as @xmath260 .",
    "these conditions are implied by the stronger condition that @xmath261 , ( s.1 ) following from chebyshev s inequality and ( s.2 ) arising since @xmath262 .",
    "all of the weight matrices discussed in this article can satisfy these conditions , as well as other weight matrices that we have considered , so these conditions do not seem burdensome . for ease of reference we denote the corresponding estimator as @xmath263 .      we state and discuss one of our main results in this section . in preparation , let @xmath264 where @xmath265 is as defined in  ( [ h ] ) , let @xmath266 and define the random vector @xmath267 , where @xmath268 is the population reduction using @xmath165 applied to the error @xmath269 of a  new observation and @xmath270 is the targeted population reduction ( [ popred ] ) applied to the same error .",
    "the vector @xmath271 measures the population - level agreement between the user - selected reduction @xmath272 and the ideal reduction .",
    "it has mean and variance @xmath273 where the semi - orthogonal matrix @xmath274 .",
    "the next proposition describes asymptotic properties of @xmath275 and  @xmath263 , where @xmath175 was defined in  ( [ khat ] ) .",
    "[ propwhat1 ] [ propwhat2 ] assume conditions and .",
    "then :    if @xmath276 , then @xmath277 .",
    "@xmath278 .    if , in addition , @xmath230 , then @xmath279 where @xmath280 and @xmath281 were defined in  ( [ kappa ] ) .",
    "this proposition shows that the asymptotic properties of @xmath175 and @xmath263 depend on four quantities , each with a  different role .",
    "we defer discussion of the order @xmath282 that measures the asymptotic behavior of @xmath166 to later sections .",
    "in the rest of this section we concentrate on the remaining and @xmath283by assuming that @xmath166 is nonstochastic , so @xmath284 , @xmath285 and @xmath286 .",
    "this involves no loss since none of these terms depends on @xmath287 .",
    "terms involving @xmath280 affect both estimation @xmath175 and prediction @xmath288 .",
    "if @xmath280 were to diverge , then , from proposition  [ propwhat1](i ) , eventually @xmath289 will look like a  diagonal matrix and there will be little signal left , which is why we imposed the universal condition  ( [ kappa ] ) that @xmath290 .",
    "if @xmath291 diverges , then again @xmath289 will resemble a  diagonal matrix , but this is prohibited by universal condition ( w.1 ) .",
    "here we also see the role of the rank condition  ( [ rank ] ) introduced at the beginning of section  [ choose - f ] . if @xmath292 , then @xmath293 and again some signal will lost . in the extreme ,",
    "if @xmath294 , then @xmath295 and all information on @xmath138 is lost .",
    "if @xmath296 , then @xmath297 increases faster than @xmath16 increases . in effect",
    "there is a  synergy between the signal rate and the sample size . if the regression is abundant , so @xmath298 , then @xmath299 .",
    "useful results can also be obtained when @xmath300 , because then @xmath301 , which may be small in some regressions . in sparse regressions where @xmath302 , @xmath303 , and we are back to the usual requirement that @xmath23 for consistency of @xmath175 . equally important , since @xmath280 does not depend on the weight matrix , the results indicate that it may not be possible to develop from the family of estimators considered rates of convergence that are faster than @xmath304 .",
    "the @xmath271 and @xmath283 terms arise from prediction . the first term @xmath305 has a  characteristic that is different from the others because it does not depend on @xmath20 . since @xmath306 , the sufficient stability requirement @xmath307 of lemma  [ lemmastable ] guarantees that @xmath308 is bounded as @xmath309 .",
    "while the contribution of @xmath305 will be negligible if @xmath308 is sufficiently small , for the best results we should have @xmath310 as @xmath311 .",
    "let @xmath312 be an orthogonal matrix . using a  result from cook and forzani [ ( @xcite ) , eq .",
    "( a.4 ) ] , @xmath313 consequently , @xmath314 when @xmath315 is a  reducing subspace of @xmath316 , even if @xmath317 .",
    "this result is similar to zyskind s ( @xcite ) classical findings about conditions for equality of the best and simple least squares linear estimators in linear models .",
    "if @xmath231 , then @xmath318 and @xmath315 is trivially a  reducing subspace of @xmath316 . if @xmath237 is a  generalized inverse of @xmath165 , @xmath319 , and if @xmath320 , then again @xmath321 reduces @xmath316 and @xmath314 .",
    "turning to @xmath283 , since @xmath322 , it follows that @xmath323 .",
    "consequently , if @xmath324 , then @xmath325 and @xmath326 . in a  worst - case scenario ,",
    "if @xmath327 and @xmath328 diverges , then @xmath329 because of the requirement @xmath330 in proposition  [ propwhat1](iii ) , and these terms reduce to the usual condition that @xmath23 .    in sparse regressions",
    ", the condition @xmath296 reduces to @xmath331 and @xmath330 means that @xmath232 must be bounded .",
    "these two conditions imply that .",
    "consequently , the best results in sparse regressions will be achieved",
    "when ( a ) @xmath315 is a  reducing subspace of @xmath316 , ( b ) @xmath331 and ( c ) @xmath232 is bounded .",
    "in nonsparse settings @xmath332 will yield the best results when @xmath232 is bounded and the regression is abundant or near abundant .",
    "we summarize some implications of these conditions in the following corollary .",
    "[ corw21 ] assume that @xmath333 and that @xmath285 .",
    "then :    @xmath334 .",
    "if the regression is abundant , then @xmath335 .",
    "if @xmath315 is a  reducing subspace of @xmath316 , then @xmath336 . if , in addition , the regression is abundant , then @xmath337 .",
    "the conclusions of proposition  [ propwhat2 ] and its corollary  [ corw21 ] hold as @xmath338 and @xmath24 in the required relationships and as @xmath338 with @xmath16 fixed . in the latter case the results simplify to those given in the next corollary .",
    "[ corw22 ] if @xmath339 and @xmath285 , then :    @xmath340 .    if , in addition , @xmath315 is a  reducing subspace of @xmath316 , then @xmath337 .",
    "it may be clear from the previous discussion that the best possible rate is achieved when @xmath341 and then @xmath342 .",
    "consequently , we refer to @xmath304 as the _ oracle rate_.      we conducted a  simulation study with @xmath241 to show the importance of @xmath305 , to demonstrate that proposition  [ propwhat2 ] and corollaries  [ corw21 ] and  [ corw22 ] give good qualitative characterizations of the limiting behavior of @xmath343 , and to provide some intuition into the nonasymptotic behavior of @xmath344 .",
    "the simulated data were generated using a  simple version of model  ( [ vecmodel ] ) : @xmath345 , @xmath346 , @xmath347 , was constructed as a  vector of standard normal random variables and @xmath237 is a  diagonal matrix .",
    "specific scenarios may differ on @xmath20 , @xmath16 and the choice of @xmath237 . in any case",
    ", the true model then has @xmath348 .",
    "we confined attention to regressions with @xmath348 because we found that there is nothing in principle to be gained from settings with @xmath349 . for each sample size",
    "@xmath20 we used the estimators described at the end of section  [ secestimation ] with @xmath350 and @xmath351 , @xmath352 , as the elements of @xmath156 , so @xmath353 and @xmath354 .    for each data set constructed in this way , we determined @xmath355and  @xmath356 at @xmath357 new data points generated from the original model .",
    "we summarized each data set by computing the absolute sample correlation between @xmath344 and @xmath6 and the sample standard deviation of the difference @xmath358 over the 100 new data points .",
    "this process was repeated at least 400 times for @xmath359 , 200 times for @xmath360 and 50 times for @xmath361 .",
    "the average absolute correlation and average standard deviation were used as overall summary statistics . our decision to use a  diagonal matrix for  @xmath237 was based on the observation that the asymptotic results depend on  @xmath237 largely via @xmath232 and with @xmath350 , @xmath362 .    [ cols=\"^,^ \" , ]      since there are only @xmath363 predictors it was straightforward , albeit somewhat tedious , to inspect inverse response plots of @xmath194 versus @xmath0 , @xmath357 , to gain information about the likely structure of  @xmath156 .",
    "we performed two analyses ; the first consisted of 54 pork samples and the second consisted of 103 meat samples of beef and pork . for a  specified value of @xmath26 , we assessed the fitted model by using the residual mean square @xmath364 , where the fitted values were determined using  ( [ ehat ] ) with the indicated combination of @xmath26 , @xmath365 and reduction  @xmath366 .",
    "we selected the value of @xmath26 by adapting the permutation scenario developed by @xcite .",
    "the hypothesis @xmath367 was tested sequentially , starting at @xmath368 and estimating @xmath26 as the first hypothesized value that was not rejected .",
    "the test statistic @xmath369 was compared to the distribution of @xmath370 induced by @xmath371 random permutations  @xmath372 applied to the rows of the predictor matrix @xmath373 as follows : @xmath374 where @xmath205 and @xmath166 were computed under the null hypothesis .",
    "this scheme leaves the signal @xmath375 intact while permuting the uninformative part of the predictors @xmath376 .",
    "the multicollinearity of the predictors made computing the weight matrix estimator for @xmath377 difficult for small values of its tuning parameter , values for which our cross - validation procedure recommended .",
    "we subsequently set its tuning parameter to @xmath378 for both analyses , since this was the smallest value for which a  numerically stable solution was available .      in this case",
    "we concluded that a  cubic polynomial @xmath379 would be adequate ; a  representative plot is shown in figure  [ figporkinvresp](a ) . performing the permutation test to select @xmath26 ,",
    "the test statistic for @xmath380 using @xmath381 was @xmath382 which was smaller than the smallest value @xmath383 of @xmath384 observed among the @xmath371 random permutations under the hypothesis that @xmath380 .",
    "since the test statistic is much smaller than can be accounted for by chance under the null hypothesis , we concluded that @xmath385 . similarly , to test @xmath348 , we observed @xmath386 which fell at the 80th quantile of the permutation distribution of @xmath387 under the hypothesis .",
    "consequently , we used @xmath348 for the model .",
    "comparisons with other values of @xmath365 figured in our choice @xmath388 .",
    "cross - validation using @xmath389 as the criterion might also be used to select @xmath26.=-1    @cc@    ,  @xmath377 ,  @xmath390 and @xmath344 , using the pork samples.,title=\"fig : \" ] & ,  @xmath377 ,  @xmath390 and @xmath344 , using the pork samples.,title=\"fig : \" ] + ( a ) & ( b ) + ,  @xmath377 ,  @xmath390 and @xmath344 , using the pork samples.,title=\"fig : \" ] & ,  @xmath377 ,  @xmath390 and @xmath344 , using the pork samples.,title=\"fig : \" ] + ( c ) & ( d )    fitting each of the four estimators with @xmath391 , we found that @xmath392 , @xmath393 , @xmath394 and @xmath395 for @xmath396 , @xmath390 , @xmath381 and @xmath377 .",
    "plots of @xmath0 versus @xmath397 are shown in figure  [ figpork](a)(d ) .",
    "the predictors in this illustration are highly collinear , as is typical in spectral data , and the relative performance of the four estimators is qualitatively similar to that shown in previous simulations ; however , numerical instability degraded the performance of @xmath377 .",
    "the relative signal rates for the four estimators are reflected by the values of @xmath398 , @xmath399 , @xmath400 and @xmath401 for @xmath402 , @xmath403 , @xmath404 and @xmath405 .",
    "in this case we concluded that a  second - order polynomial and the indicator function of beef would be adequate , @xmath406 ; a  representative plot is shown in figure  [ figporkinvresp](b ) . using the permutation test approach to select @xmath26 ,",
    "the test statistic for @xmath380 using @xmath381 was @xmath407 which fell at the 0.003 quantile of @xmath384 observed among the @xmath371 random permutations under the hypothesis that @xmath380 .",
    "since the test statistic is much smaller than can be accounted for by chance under the null hypothesis , we concluded that @xmath385 .",
    "similarly , to test @xmath348 , we observed @xmath408 which fell at the 0.70 quantile of the permutation distribution of @xmath387 under the hypothesis .",
    "consequently , we used @xmath348 for the model .",
    "@cc@    ,  @xmath377 ,  @xmath390 and @xmath344 .",
    "circles represent pork and plus signs represent beef.,title=\"fig : \" ] & ,  @xmath377 ,  @xmath390 and @xmath344 .",
    "circles represent pork and plus signs represent beef.,title=\"fig : \" ] + ( a ) & ( b ) + ,  @xmath377 ,  @xmath390 and @xmath344 .",
    "circles represent pork and plus signs represent beef.,title=\"fig : \" ] & ,  @xmath377 ,  @xmath390 and @xmath344 .",
    "circles represent pork and plus signs represent beef.,title=\"fig : \" ] + ( c ) & ( d )    fitting each of the four estimators with @xmath391 , we found that @xmath409 , @xmath410 , @xmath411 and @xmath412 for @xmath396 , @xmath390 , @xmath381 and @xmath377 .",
    "plots of  @xmath0 versus  @xmath397 are shown in figure  [ figboth](a)(d ) .",
    "the relative signal rates for the four estimators are reflected by the values of @xmath413 , @xmath414 , @xmath415 and @xmath416 for @xmath417 , @xmath403 , @xmath418 and @xmath405 . the relatively large signal for @xmath418",
    "is reflected in the plots of figure  [ figboth ] .",
    "the class of estimators that we studied is limited in scope relative to the range of sdr methods presently available for @xmath419 regressions . however , in the broader context of this article , which does not require @xmath420 , we introduced the concept of an abundant regression and presented what may be the first @xmath421 asymptotic analysis of a  class of sdr methods when the focus is on estimating a  reduction @xmath6 rather than on the underlying central subspace .",
    "these ideas can in principle be extended for other sdr methods , and we expect that the same general issues will be encountered . while each of the methods that we studied can perform usefully in the right situation , we judge the spice and @xmath422 weight matrices to be the best overall , although improvements for nonsparse weight matrices and for regressions with very highly correlated predictors are still needed .",
    "our simulation results were all based on normal errors to focus the presentation and save space .",
    "however , we have also conducted a  variety of parallel simulations using uniform @xmath423 , @xmath424 and @xmath425 errors , each centered and appropriately scaled .",
    "the results were essentially the same as those with normal errors [ @xcite ] .",
    "alternative penalty functions may be used for estimating the weight matrix , particularly in scenarios when the inverse error covariance matrix is not sparse .",
    "for example , the sparse - seeking penalty function in  ( [ optomega2 ] ) , @xmath426 , could be replaced with the quadratic penalty function , @xmath427 where @xmath428 controls whether or not the diagonal of @xmath429 , the inverse error correlation matrix , is penalized . if @xmath430 , the general spice algorithm developed by @xcite can efficiently solve  ( [ optomega2 ] ) with the penalty defined in ( [ quadpenalty ] ) . if @xmath431 , @xcite derived an noniterative solution to an equivalent problem to  ( [ optomega2 ] ) with the penalty defined in ( [ quadpenalty ] ) .",
    "recalling that @xmath432 denotes the @xmath433th eigenvalue of a  matrix @xmath39 , let @xmath434 .",
    "witten and tibshirani showed that the eigenvectors of @xmath435 are equivalent to the eigenvectors of @xmath436 and that @xmath437 .",
    "the general rates given in proposition  [ propwhat1 ] are not very sensitive to the choice of @xmath156 since they hold when @xmath156 satisfies the minimal rank condition  ( [ rank ] ) .",
    "nevertheless , assuming normality and a  correct @xmath156 , we obtained the oracle rates of proposition  [ propwdelta1 ] , which indicates that there are advantages to pursuing good choices .",
    "the methods sketched in section  [ choose - f ] are often useful in practice , but it is also possible to develop semiparametric methods to estimate @xmath438 directly rather than passing through approximations  @xmath439 .",
    "this might be accomplished iteratively : choose an initial @xmath156 and construct the corresponding estimates @xmath440 , @xmath441 and @xmath442 .",
    "a new estimate of @xmath192 can be obtained by smoothing the coordinates of @xmath442 against  @xmath0 , leading to a  second reduction estimate @xmath443 .",
    "the process can now be continued until some convergence criterion is met .",
    "while we did not incorporate screening or variable selection into our reduction methodology , the potential benefits of those procedures are manifested in our results .",
    "consider , for instance , a  regression in which @xmath444 of the predictors are inactive .",
    "then the oracle rate @xmath445 .",
    "however , if we remove @xmath446 of the inactive predictors , the oracle rate is increased to @xmath447 , which should be worthwhile in most applications .",
    "work along these lines is in progress .",
    "the authors are grateful to the referees whose helpful comments led to significant improvements in this article ."
  ],
  "abstract_text": [
    "<S> we study the asymptotic behavior of a  class of methods for sufficient dimension reduction in high - dimension regressions , as the sample size and number of predictors grow in various alignments . </S>",
    "<S> it is demonstrated that these methods are consistent in a  variety of settings , particularly in abundant regressions where most predictors contribute some information on the response , and oracle rates are possible . </S>",
    "<S> simulation results are presented to support the theoretical conclusion .    ,    .    . </S>"
  ]
}