{
  "article_text": [
    "identifying complex dependence structures among predictors and responses is an important problem in statistics and machine learning , since these structures reveal hidden domain knowledge about the data .",
    "for example , in bioinformatics , identifying gene regulatory networks is crucial for understanding gene regulatory paths and gene functions , which helps disease prediction and diagnosis .",
    "similarly , in social media analysis , inferring the influence networks from user activities ( that is , _ diffusion network inference _",
    "problem @xcite ) is an important problem , and it has found applications in social media marketing @xcite and crisis management @xcite . in these big data applications , inferring the dependence structures is challenging since the responses and predictors may be related through a few latent pathways and/or associated through only a subset of responses and predictors . moreover , the curse of dimensionality and massive amounts of data , that is , scalability issues make the dependence structure discovery problem even harder to solve .",
    "recently , regularization methods such as lasso @xcite and group lasso @xcite , and reduced - rank regression approaches @xcite have been proposed to recover sparse response - predictor associations and latent predictors , respectively . @xcite and @xcite have proposed sparse reduced - rank regression approaches by combining the regularization and reduced - rank regression techniques to find the complex dependence structures between responses and predictors .",
    "sparse reduced - rank regression works by modeling the associations between the predictor and response variables via a sparse and low - rank representation of the coefficient matrix .",
    "it not only enhances the interpretability of the estimated matrix by eliminating irrelevant features @xcite , but also reduces the number of free parameters of the model and thus the number of observations required for desired estimation consistency @xcite .",
    "sparse reduced - rank regression has found applications in micro - array biclustering @xcite , subspace clustering @xcite , social network community discovery @xcite , and motion segmentation @xcite . in these applications ,",
    "joint sparsity and low - rankness has been used to enforce a clustered dependence structure among data points . in particular ,",
    "the key idea is to estimate a similarity matrix among data points that is simultaneously sparse and low - rank and then permute the rows and columns of the matrix to yield approximately block - diagonal structures , which naturally lead to clustering of data points into several groups .",
    "note that @xcite , @xcite , and the references therein have considered estimating matrices with a low - rank plus sparse representation which is different from our work as we are interested in estimating a matrix that is jointly low - rank and sparse .",
    "a natural approach to solving the sparse reduced - rank regression problem is to simultaneously penalize the parameter matrix using the @xmath1 and nuclear norm regularizers , as they are convex relaxations to sparsity and low - rankness of a matrix , respectively .",
    "the resulting optimization problem is convex and can be solved using the alternating direction method of multipliers ( admm ) @xcite as proposed by @xcite and @xcite . in @xcite , an alternative approach ,",
    "called rank constrained group lasso ( rcgl ) , was proposed which directly penalizes the rank and the number of nonzero rows of the parameter matrix .",
    "they showed oracle rates for the estimated matrix and also provided a practical algorithm which iteratively and jointly solves a @xmath1-regularization and low - rank estimation problem .",
    "to further improve the estimation accuracy , @xcite borrowed ideas from adaptive lasso @xcite and proposed the iterative exclusive extraction algorithm ( ieea ) which finds a locally optimal solution in the neighborhood of the initial value",
    ". they also showed model selection consistency and asymptotic normality results along with the improved empirical performance of ieea on microarray biclustering analysis data .",
    "all the above approaches for sparse reduced - rank regression achieve both desirable theoretical properties and strong empirical results . however , they can not scale to large matrix estimation problems in many big data applications .",
    "the admm algorithm of @xcite and @xcite uses iterative singular value thresholding @xcite for solving the joint @xmath1 and nuclear norm regularization .",
    "iterative singular value thresholding is known to be computationally expensive since it performs a full singular value decomposition of the parameter matrix in each iteration . on the other hand ,",
    "rcgl @xcite is computationally much faster than admm since it only performs top-@xmath0 singular value decomposition for estimating a rank-@xmath0 matrix in each iteration . despite a lower computational cost per iteration ,",
    "it is unclear how many iterations rcgl needs for convergence .",
    "ieea @xcite performs nested loops of alternating @xmath1-penalized regression for each singular vector which can be expensive , especially on parallel computing devices .",
    "the iterative nature of these three approaches makes them not scalable and renders them inefficient for large matrix estimation even on high performance computing devices .    to overcome the scalability issues of the previous approaches , we propose a simple and scalable sparse reduced - rank regression procedure called sequential estimation with eigen - decomposition ( seed ) .",
    "seed is designed for high - performing computing platforms .",
    "it converts the sparse and low - rank regression problem to a sparse generalized eigenvalue problem , and then solves the problem using the recent algorithms for sparse eigenvalue decomposition @xcite . as a pure learning algorithm , seed is expected to perform only a single top-@xmath0 sparse eigenvalue decomposition for estimating a rank-@xmath0 matrix , which makes it truly scalable and efficient for large matrix estimation problems .",
    "the main contributions of our paper are threefold .",
    "first , for the sparse reduced - rank regression problem , our proposed procedure seed provides a scalable approach to uncovering the sparse predictor - response association network while simultaneously achieving dimension reduction and variable selection .",
    "second , for the high - dimensional settings , our theoretical analysis shows that seed can consistently estimate the singular vectors , latent factors as well as the regression coefficient matrix , identify the correct rank of the matrix , accurately predict the multivariate response vector , and recover the support of the singular vectors under mild conditions . note that , compared to @xcite , we do not make any assumption on the positive definiteness of the design matrix for proving our consistency results .",
    "third , we empirically demonstrate that seed can not only be efficiently implemented on both central processing units ( cpu ) and graphics processing units ( gpu ) for large - scale applications , but it also outperforms the state - of - the - art sparse reduced - rank regression approaches .",
    "the rest of this paper is organized as follows .",
    "section [ sec : method ] introduces our seed method .",
    "we discuss the implementation details of seed in section [ sec : imp ] and present its asymptotic properties in section [ sec : analysis ] .",
    "we demonstrate the advantages of seed on both synthetic and real data sets in section [ sec : exp ] , and in section [ sec : dis ] we discuss some extensions of our seed method . all technical proofs and details are provided in the appendix .",
    "denote by @xmath2 @xmath3 observations for the fixed design setting where @xmath4 and @xmath5 represent the @xmath6th predictor and the corresponding response vectors , respectively . given a predictor vector @xmath7 ,",
    "the corresponding response vector @xmath8 is drawn from the following model @xmath9 where the noise vector @xmath10 is a @xmath11-dimensional zero mean multivariate gaussian random vector with the covariance matrix @xmath12 , and @xmath13 is the regression coefficient matrix .",
    "we can rewrite the data model in the matrix form as follows @xmath14 where @xmath15{^{t}}$ ] , @xmath16{^{t}}$ ] , and @xmath17{^{t}}$ ] denote the matrices of stacked response , predictor and noise vectors , respectively .",
    "let @xmath18 be the gram matrix of the predictors .",
    "we consider the case where the true regression coefficient matrix @xmath19 is jointly low - rank and sparse , the same as in @xcite and @xcite . in particular , the matrix rank @xmath20 is assumed to be small with @xmath21 , and @xmath19 follows a decomposition that @xmath22 where the left singular vectors @xmath23 are @xmath24-orthogonal with unit length , that is , @xmath25 if @xmath26 and @xmath27 , while the right singular vectors @xmath28 are orthogonal , that is , @xmath29 for @xmath26 , and @xmath30 is the layer @xmath31 unit rank matrix of @xmath19 .",
    "the singular vectors are sorted by the magnitudes of the singular values @xmath32 in descending order , which correspond to their contributions to the prediction of @xmath33 .",
    "we consider the left singular vectors ( both the population and estimated ones ) in the constraint space @xmath34 , where @xmath35 denotes the null space of @xmath24 , to guarantee the model identifiability as otherwise @xmath36 would contain certain component @xmath37 such that @xmath38 , which does not contribute to the prediction of @xmath33 . the @xmath24-orthogonality of @xmath39 is not necessary in the algorithm but facilitates the theoretical analysis .",
    "in fact , the above decomposition ( [ eq : c * ] ) for @xmath19 coincides with the singular value decomposition of @xmath40 through different scalings on the singular vectors .",
    "we defer the discussion on the existence and identifiability of decomposition ( [ eq : c * ] ) to supplementary material [ sec : ident ] .",
    "the aforementioned modeling of the regression coefficient matrix indeed gives a latent factor model with @xmath20 latent factors , where @xmath41 is the @xmath31th latent factor and @xmath42 describes the impact of the @xmath31th factor on the response variables .",
    "as illustrated in @xcite , the low - rankness of @xmath19 renders dimension reduction such that all responses can be predicted by a relatively small set of common factors .",
    "furthermore , the left singular vectors @xmath39 correspond to the selection of predictors , and we impose a sparsity assumption such that @xmath43 for @xmath44 .",
    "similar sparsity assumptions were made in @xcite and @xcite to enhance model interpretability by removing irrelevant features in high dimensions , where @xcite assumed that both the left and right singular vectors are sparse while @xcite imposed restriction on the number of nonzero rows of the coefficient matrix . in this paper , we are interested in two cases : ( i ) when the right singular vectors are not required to be sparse and ( ii ) when it is desirable to have sparse right singular vectors , which entails the response selection",
    ". we will show that both cases are efficiently accommodated by our procedure .",
    "our goal is to accurately estimate the singular vectors @xmath45 and @xmath46 , and the true rank @xmath20 such that we can recover the latent factors as well as their impacts , and at the same time , identify the underlying number of latent factors and the significant predictors . as a singular vector",
    "can have two opposite directions , we always assume that the estimated left singular vector takes the correct one , that is , the angles between estimated and population left singular vectors are no more than a right angle . once the estimated rank and singular vectors are obtained",
    ", the estimate @xmath47 of the true matrix @xmath19 follows immediately from ( [ eq : c * ] ) . for theoretical analysis",
    ", we consider the estimation of the true rank within certain upper bound @xmath48 , that is , @xmath49 . in practice",
    ", the upper bound @xmath0 can be controlled by our algorithm . unlike most existing sparse and low - rank estimation methods",
    "which adopt the regularization framework of a loss function plus certain penalties and are generally not scalable , the proposed procedure seed is indeed a pure learning algorithm that predicts @xmath33 using @xmath50 with some low - rank @xmath47 .",
    "the following proposition provides us insight into estimating the top-@xmath20 left and right singular vectors of @xmath19 .",
    "[ lem : rank1opt ] consider the noiseless case where @xmath51 and @xmath52 as defined in ( [ eq : c * ] ) .",
    "then @xmath53 are the @xmath20 non - degenerate left singular vectors of @xmath19 if and only if they are the eigenvectors of the following generalized eigenvalue problem @xmath54 with respect to the nonzero eigenvalues @xmath55 , where @xmath56 is the @xmath31th largest eigenvalue of @xmath57 with the singular values @xmath58 defined in section [ sec2.1 ] .",
    "furthermore , given the left singular vector @xmath45 , the corresponding right singular vector @xmath46 can be written as @xmath59    proposition [ lem : rank1opt ] shows that the problem of estimating the singular vectors can be transformed into the generalized eigenvalue problem in ( [ eq : geneig ] ) , thanks to the @xmath24-orthogonality of the left singular vectors .",
    "in the noisy case where @xmath60 , it motivates us to estimate the left singular vectors by solving the corresponding generalized eigenvalue problem as follows @xmath61 the estimation consistency in the noisy case will be guaranteed by the matrix perturbation theory , see section [ sec : analysis ] for details .",
    "on the other hand , it is not difficult to see that the eigenvectors with respect to different eigenvalues of problem ( [ eq : geneig1 ] ) are @xmath24-orthogonal , which further gives the orthogonality of the right singular vectors estimated by ( [ eq : solu1 ] ) .",
    "it implies that the right and left singular vectors obtained by solving the generalized eigenvalue problem ( [ eq : geneig1 ] ) will automatically be orthogonal and @xmath24-orthogonal , respectively .",
    "related results of principal component analysis in low dimensions can be found in @xcite , @xcite , and @xcite .",
    "note that in the high - dimensional setting , the regime of interest for this paper , the gram matrix @xmath24 can be rank deficient and the generalized eigenvalue problem is potentially challenging to solve .",
    "we will address the implementation challenges for high - dimensional settings in section [ sec : imp ] .",
    "motivated by proposition [ lem : rank1opt ] , our proposed procedure seed performs a two - step estimation for the regression coefficient matrix : it first solves the generalized eigenvalue problem in ( [ eq : geneig1 ] ) to obtain the estimated left singular vectors @xmath62 with unit length ; then it finds the estimated right singular vectors @xmath63 according to ( [ eq : solu1 ] ) , that is , @xmath64 the maximum rank @xmath0 depends on the magnitude of the estimated singular value @xmath65 with @xmath66 ( whether it is larger than a threshold @xmath67 ) . and the optimal rank will be tuned by the information criterion described in section [ sec : analysis ] .",
    "the details of the procedure are provided in algorithm [ algo : all ] . to achieve a sparse solution",
    ", we need to find the optimal rank-@xmath0 sparse matrix via a sparse eigenvalue decomposition procedure in line [ line : eig ] of [ algo : all ] . for theoretical analysis",
    ", we assume that there exists a sparse eigenvalue decomposition procedure that solves ( [ eq : geneig1 ] ) and practical methods will be provided in section [ sec : imp ] .",
    "the practical methods need a sparsity parameter @xmath68 , such as a threshold @xcite or a sparsity size @xcite .",
    "we will show in section [ sec : exp ] that seed is robust to the choices of parameters @xmath68 and @xmath67 .",
    "if the right singular vectors are also required to be sparse , we perform a simple element - wise thresholding on @xmath69 after we obtain it in line [ line : th ] .",
    "compute matrices : @xmath70 , @xmath71 @xmath72    given condition [ as : rsc ] in section [ sec : analysis ] , the following proposition shows that with significant probability , [ algo : all ] will stop after exactly @xmath20 iterations if the termination criterion @xmath67 is properly chosen .",
    "suppose that condition [ as : rsc ] holds and @xmath73 , then with probability at least @xmath74 for any @xmath75 , [ algo : all ] will stop after @xmath20 iterations if the termination criterion @xmath67 is chosen from the following interval : @xmath76 where @xmath77 is assumed to be larger than @xmath78 with positive constant @xmath79 defined in supplementary material [ sec : rankproof ] .",
    "[ prop : termin ]    proposition [ prop : termin ] is mainly for theoretical purposes since the interval above is generally unknown to us .",
    "we will need to tune the optimal rank by certain information criterion in practice .",
    "the tail probability @xmath80 can decay to zero quickly as @xmath81 and @xmath11 grow with rates such as @xmath82 for some positive constant @xmath83 .",
    "this is due to the fact that when @xmath82 , we have @xmath84 under the assumption that @xmath73 .",
    "the lower bound of @xmath67 excludes the case that extra latent factors are involved due to noises while the upper bound guarantees the important factors will not be missed .",
    "[ algo : all ] requires a sparse solution of ( [ eq : geneig1 ] ) which is a generalized eigenvalue problem with a rank deficient matrix @xmath24 . in this section ,",
    "we study multiple practical aspects of seed and propose two different ways to solve ( [ eq : geneig1 ] ) , one with enhanced stability and the other using a fast procedure to accelerate it .      [ [ stability . ] ] stability .",
    "+ + + + + + + + + +    for numerical stability purposes , we can solve the following modified problem of ( [ eq : geneig1 ] ) with a very small positive @xmath85 : @xmath86 note that @xmath87 is invertible since the eigenvalues of @xmath88 are nonnegative .",
    "denote by @xmath89 the modified predictor matrix such that @xmath90 , which can be obtained via the cholesky decomposition , and @xmath91 the modified response matrix . then the above equation ( [ eq : geneig2 ] )",
    "can be rewritten as @xmath92 which adopts the same form as ( [ eq : geneig1 ] ) .",
    "the formulation of @xmath93 and @xmath94 can be regarded as a generalization of the ridge regression to the multivariate response setting .",
    "in fact , since @xmath19 is the minimizer of @xmath95 , we can enhance the stability by adding a small frobenius norm regularization as follows : @xmath96 where the frobenius norm is defined as @xmath97 for any matrix @xmath98 .",
    "after completing the squares , we get @xmath99 which means that @xmath93 and @xmath94 are the corresponding predictor and response matrices that take into account the shrinkage effects @xcite .",
    "a computationally efficient technique for solving equation ( [ eq : regu3 ] ) is to solve the sparse eigenvalue problem @xmath100 , where the modified gram matrix @xmath101 and @xmath102 is defined accordingly as in algorithm [ algo : all ] .",
    "we also use the sherman  morrison ",
    "woodbury formula to compute @xmath103 as follows : @xmath104 the above equation requires inversion of an @xmath105 matrix instead of a @xmath106 matrix which is significantly faster in the high - dimensional setting when @xmath107 .",
    "the corresponding right singular vectors will then be obtained by ( [ eq : solu ] ) .",
    "[ [ refitting . ] ] refitting .",
    "+ + + + + + + + + +    in [ algo : all ] , a further refitting can be performed during the eigenvalue decomposition in line [ line : eig ] to enhance the stability .",
    "the refitting procedure is as follows . in the @xmath31th step ,",
    "we compute the residual @xmath108 and solve the generalized eigenvalue problem ( [ eq : geneig1 ] ) with @xmath33 replaced by @xmath109 to obtained the unit rank matrix @xmath110 .",
    "then we perform the top-@xmath31 singular value decomposition @xmath111 for @xmath112 and refit the solution by finding @xmath113 . the estimate with refitting",
    "is defined as @xmath114 . in practice",
    ", we find this approach more stable and report the results based on this variation of seed in numerical studies .",
    "[ [ speedup . ] ] speedup .",
    "+ + + + + + + +    the bottleneck in speeding up algorithm [ algo : all ] is line [ line : eig ] where we need to solve a sparse generalized eigenvalue problem . to overcome this bottleneck",
    ", we propose a new solution to estimating the left singular vectors by rewriting equation ( [ eq : geneig1 ] ) as @xmath115 similar to proposition [ lem : rank1opt ] , when @xmath116 is of full row rank ( which is easy to satisfy in the high - dimensional setting ) , the above equation shares the same nonzero eigenvalues with @xmath117 .",
    "even if @xmath116 is row rank deficient , the nonzero solution of @xmath36 is ensured by the perturbation theory in lemma [ lem : pert ] .",
    "thus , we propose the following two - step procedure for line [ line : eig ] of [ algo : all ] :    * @xmath118 . *",
    "@xmath119 .",
    "similarly as before , we compute the residual @xmath108 in the @xmath31th step and replace @xmath33 with @xmath109 in the above two - step procedure to obtain the @xmath31th left singular vector @xmath120 .",
    "overall , the first step requires calculation of the top-@xmath0 eigenvalues for an @xmath105 matrix ( or @xmath121 if @xmath122 ) while the second step finds the corresponding eigenvectors by solving a regular sparse eigenvalue problem .",
    "thus , the above procedure significantly accelerates the speed of seed as it converts a degenerate sparse generalized eigenvalue problem to two simpler regular sparse eigenvalue problems .",
    "the truncated power method @xcite can be used to compute both eigenvalue problems efficiently .",
    "[ [ sparse - eigenvector - estimation . ] ] sparse eigenvector estimation .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the previous two approaches for solving the generalized eigenvalue decomposition problem indicate that we can solve the problem via regular eigenvalue decomposition .",
    "this allows us to reuse the existing procedures for sparse eigenvalue decomposition such as @xcite , @xcite , @xcite , and @xcite to solve the problem in ( [ eq : geneig1 ] ) . in numerical studies",
    ", we use the iterative thresholding method @xcite for estimating the sparse eigenvectors of a matrix .",
    "[ [ parallel - implementation . ] ] parallel implementation .",
    "+ + + + + + + + + + + + + + + + + + + + + + + +    in order to scale up the procedures , we often need to utilize the parallel computing tools .",
    "given the fact that seed only uses basic matrix operations , we can employ parallel implementation of the large matrix operations to accelerate seed . in section [ sec : exp ]",
    ", our experiments with gpu which contain thousands of processing units show that the matrix operations in seed can be efficiently parallelized and it significantly enhances the speed of seed . whenever the data can not be loaded into the memory of a single device , efficient distributed algorithms can be used ,",
    "see for example , @xcite and the references therein .",
    "in this section , we will analyze the statistical properties of seed . define the maximum sparsity level of the left singular vectors as @xmath123 , which is assumed mainly for theoretical analysis ( see condition [ as : rsc ] below ) and will not be used directly in the algorithm .",
    "we consider the estimated left singular vectors with the number of nonzero elements less than certain sparsity level @xmath124 , that is , @xmath125 for @xmath126 .      here",
    "we list a few technical conditions and discuss their relevance in detail .",
    "[ restricted isometry ] there exists a positive constant @xmath127 such that the gram matrix @xmath24 satisfies @xmath128 for some @xmath124 .",
    "[ as : rsc ]    [ minimum singular value separation ] the non - zero singular values @xmath58 satisfy @xmath129 for some constant @xmath130 and @xmath131 .",
    "[ as : separtion ]    [ bounded eigenvalues ] the eigenvalues of the population covariance matrix of the noise vector @xmath132 satisfy @xmath133 for @xmath134 , where @xmath135 and @xmath136 are positive constants with @xmath137 for some positive constant @xmath138 .",
    "[ as : lowerbound ]    [ minimum signal strength ] [ as : supp ] there exists some positive constant @xmath139 such that the following lower bounds on the magnitudes of the non - zero elements of @xmath45 and @xmath46 hold for any @xmath140 : @xmath141 where @xmath142 and @xmath143 are constants defined in theorem [ th : rankm ]",
    ".    condition [ as : rsc ] imposes bounds on the @xmath144-sparse eigenvalues of @xmath24 , which is weaker than the regular bounded eigenvalue assumption since the sparse eigenvalues do not grow as fast as the regular eigenvalues when the dimensionality @xmath81 grows . as a typical condition in high dimensions",
    ", it restricts the correlations between small numbers of features and thus guarantees the identifiability of the true sparse support .",
    "see , for instance , @xcite and @xcite for more discussion on it . recall that @xmath58 is the @xmath31th largest singular value of @xmath145 .",
    "condition [ as : separtion ] requires a non - zero separation among the singular values such that the true left singular vectors are distinguishable .",
    "for ease of presentation , we assume @xmath130 to be a constant .",
    "in fact , @xmath130 can be allowed to converge to zero asymptotically , so we indicate the effect of @xmath130 clearly in the constants of the theoretical results .",
    "the elements of the unobserved noise vector @xmath132 were assumed to be independent and identically distributed ( i.i.d . ) in @xcite .",
    "we relax it a bit in condition [ as : lowerbound ] by imposing bounded eigenvalues for the noise covariance matrix to recover the true rank in theorem [ th : rankconsist ] .",
    "our technical argument still applies when either @xmath146 or @xmath147 as long as their rates of convergence can be controlled within certain magnitudes .",
    "the two inequalities in condition [ as : supp ] are imposed for the model selection consistency of the predictors and responses , respectively .",
    "the magnitude of the minimum signal strength is @xmath148 , which is relatively mild as it converges to zero in our setting .",
    "since @xmath39 is assumed to have unit length , the scaling of @xmath19 is put on @xmath42 such that there is an extra factor @xmath149 in the second inequality .",
    "denote by @xmath150 and @xmath151 the maximum diagonal component of the gram matrix and noise covariance matrix , respectively . without loss of generality , we assume that @xmath152 is finite .",
    "moreover , it is clear that under conditions [ as : rsc ] and [ as : lowerbound ] , @xmath153 and @xmath154 are also finite constants . throughout this section",
    ", we assume that the top-@xmath0 eigenvectors are obtained by solving the generalized eigenvalue problem ( [ eq : geneig1 ] ) with the maximum sparsity level @xmath155 such that condition [ as : rsc ] is satisfied .",
    "the estimated regression coefficient matrix is given by @xmath156 , where @xmath157 is the optimal rank tuned by information criterion ( [ inform ] ) and @xmath158 .",
    "the following theorem bounds the estimation errors of seed with the estimated left singular vectors @xmath120 taking the correct signs as discussed before .    [ estimation and prediction consistency ] [ th : rankm ] suppose that conditions [ as : rsc ] and [ as : separtion ] hold , @xmath154 is finite , and @xmath159 , then with probability at least @xmath160 for any @xmath75 and uniformly over @xmath44 , we have @xmath161 where the constants @xmath162 and @xmath163 .    theorem [ th : rankm ]",
    "shows that the uniform estimation error bounds for both top-@xmath20 singular vectors @xmath164 and @xmath165 , the top-@xmath20 latent factors @xmath166 and unit rank matrices @xmath167 , and the uniform prediction error bounds of the top-@xmath20 latent factors are all in the same order of @xmath168 .",
    "similar to proposition [ prop : termin ] , by setting @xmath169 with @xmath83 , the tail probability will decay to zero quickly as the dimensionality @xmath81 and @xmath11 grow .",
    "furthermore , the estimation and prediction accuracy would then be within the rate of @xmath170 , where the factor @xmath171 reflects the curse of dimensionality as there are @xmath172 parameters in total from the regression coefficient matrix @xmath19 .",
    "if the true rank @xmath20 can be correctly identified , it is not difficult to see that the estimation accuracy for @xmath173 will be within the rate of @xmath174 ( see corollary [ corr1 ] below ) , which coincides with the minimax error bound for estimating the regression coefficient vector in the univariate response setting @xcite with the dimensionality @xmath81 and sparsity size @xmath155 replaced by the overall dimensionality @xmath172 and true rank @xmath20 , respectively . in view of this , the true rank @xmath20 , instead of the maximum sparsity level @xmath175 , plays the same role in multivariate regression as the sparsity size in the univariate response setup . with the true rank @xmath20 , the prediction accuracy for the multivariate response vector @xmath176 will also follow from the prediction accuracy of the top-@xmath20 latent factors .    based on",
    "the discussion before , a desirable statistical property of any low - rank estimation procedure is to accurately recover the true rank of the parameter matrix .",
    "similar to lasso , the nuclear norm regularization needs to be enhanced by techniques such as adaptive regularization to accurately recover the rank of the matrix @xcite .",
    "in contrast , in seed we can directly control the rank of the solution by limiting the number of steps .",
    "in particular , we propose a gic - type @xcite information criterion that guarantees rank recovery by seed when the optimal rank is tuned according to it .",
    "[ consistency of rank recovery ] suppose conditions [ as : rsc][as : lowerbound ] hold , @xmath159 , @xmath177^{1/4}\\right)$ ] , and @xmath178^{1/4}\\right)$ ] .",
    "define the following information criterion : @xmath179 where @xmath180 . under the above information criterion , with probability at least @xmath181 for some positive constant @xmath83 and sufficiently large @xmath3",
    ", seed will select the true rank , that is , @xmath182 .",
    "[ th : rankconsist ]    in the high - dimensional setting where the number of predictors can increase exponentially with the sample size , it is demonstrated in @xcite that we need some power of the logarithmic factor of dimensionality ( @xmath183 for our setting ) in the model complexity penalty of the information criterion to consistently identify the true model , and the slow diverging rate @xmath184 is set to prevent underfitting .",
    "the proof of theorem [ th : rankconsist ] indeed shows that information criterion ( [ inform ] ) will keep decreasing until the estimated rank reaches the true rank @xmath20 , where in each step the amount of decrease in the objective function @xmath185 equals to the squared singular value obtained by solving the generalized eigenvalue problem ( [ eq : geneig1 ] ) . after reaching the true rank ,",
    "the estimated singular value becomes small such that the model complexity penalty will overweight the decrease and then information criterion ( [ inform ] ) would start increasing .",
    "therefore , in the sequence of solutions generated by seed , the estimate @xmath186 with rank @xmath20 will be the minimizer of ( [ inform ] ) such that the true rank can be correctly identified .    as discussed before , correct identification of the true rank will yield the estimation accuracy of @xmath19 as well as the prediction accuracy of @xmath40 .",
    "therefore , combined with theorem [ th : rankconsist ] , it is immediate that the results in theorem [ th : rankm ] give the following corollary .",
    "[ overall estimation and prediction consistency ] given conditions [ as : rsc][as : lowerbound ] , @xmath159 , @xmath177^{1/4}\\right)$ ] , and @xmath178^{1/4}\\right)$ ] , if the optimal rank is tuned by information criterion ( [ inform ] ) , then with probability at least @xmath181 for any constant @xmath187 and sufficiently large @xmath3 , we have @xmath188 [ corr1 ]    besides estimation consistency and rank recovery , seed is also able to find the true support of the singular vectors . to achieve this goal , after selecting the optimal rank , we need to further refine the model selection procedure by performing a hard - thresholding .",
    "see , for example , @xcite for more general implications of thresholding in high - dimensional sparse modeling .",
    "specifically , denote by @xmath189 the estimator after the hard - thresholding operation on every element of @xmath190 such that @xmath191 based on the results of theorems [ th : rankm ] and [ th : rankconsist ] and the signal strength assumption in condition [ as : supp ] , we have the following properties for the estimator with a further thresholding .",
    "[ support recovery of the singular vectors ] given conditions [ as : rsc][as : supp ] , @xmath159 , @xmath177^{1/4}\\right)$ ] , and @xmath178^{1/4}\\right)$ ] , for every pair of singular vectors , @xmath192 , @xmath44 , the following results hold :    * if the threshold @xmath193 with @xmath194 , then with probability at least @xmath160 , we have @xmath195 ; * if the threshold @xmath196 with @xmath197 , then with probability at least @xmath160 , we have @xmath198 .",
    "[ cor : supp ]    theorem [ cor : supp ] shows that both supports of the left and right singular vectors can be accurately recovered with properly chosen tuning parameter @xmath68 .",
    "together with the correctly identified true rank @xmath20 , the above results indeed yield consistent selection of both predictors and responses . in practice , this threshold @xmath68 can be tuned by criteria such as cross - validation .",
    "our simulation studies in section [ sec : exp ] show that seed is robust to the choice of tuning parameter @xmath68 .    besides the statistical properties established before , the proposed procedure seed enjoys great flexibility in the sense that it does not rely on exact eigenvalue decomposition and the perturbation errors in the generalized eigenvalue problem ( [ eq : geneig1 ] )",
    "will be linearly incorporated into the estimated singular vectors @xmath199 and @xmath69 .",
    "furthermore , our analysis does not reply on the positive definiteness of the gram matrix @xcite in high dimensions .",
    "in this section , we conduct experiments on three data sets , including two simulation data sets ( one for a medium - scale experiment and one for a large - scale experiment ) and one application data set in social media analysis , to examine the empirical performance of seed .        we generate a medium - scale synthetic data set as follows : the predictors @xmath7 are drawn from a multivariate gaussian distribution as @xmath200 , where @xmath201 is the @xmath106 covariance matrix with auto - regressive structure , that is , @xmath202 for some @xmath203 which will be specified later . the responses @xmath8",
    "are drawn according to conditional distribution @xmath204 where the noise covariance matrix @xmath205 is also selected to have the autoregressive structure with @xmath206 and we set @xmath207 .",
    "we generate the parameter matrix @xmath98 as follows : first we generate a block - sparse matrix @xmath208 with @xmath209 non - zero elements .",
    "each non - zero element of @xmath208 is drawn from a @xmath210 . to achieve a low - rank structure , we find the top-@xmath0 singular value decomposition of @xmath208 as @xmath211 , and then set the elements of @xmath212 and @xmath213 whose magnitude is smaller than @xmath214 to zero to obtain @xmath215 and @xmath216 .",
    "the final parameter matrix is obtained as @xmath217 where the first @xmath0 diagonal elements of the diagonal matrix @xmath218 are set to @xmath219 . without loss of generality ,",
    "we add a few vectors to the design matrix to ensure the orthogonality condition of section [ sec : conds ] . in all of the simulation experiments , we generate 100 data sets and report the mean and standard error of performance for different methods .",
    "we compare the performance of seed with two state - of - art methods : ( 1 ) rcgl @xcite and ( 2 ) penalized regression with simultaneous @xmath1 and nuclear - norm penalization",
    ". the optimization problem is solved by the popular alternating direction method of multipliers @xcite and we will refer to this baseline as the `` ln  admm '' algorithm .",
    "all model parameters are set based on a separate validation set with size @xmath220 .",
    "the quality of the estimator @xmath47 is evaluated via four performance metrics listed as follows .",
    "( 1 ) _ normalized prediction error _ defined as : @xmath221 ( 2 ) _ normalized parameter estimation error _ defined as : @xmath222 where @xmath98 is the true parameter matrix .",
    "\\(3 ) _ rank recovery error _ defined as : @xmath223 since the solution of the nuclear norm always leads to small non - zero singular values ( which prevents @xmath47 from being low - rank ) , we threshold the singular values of @xmath47 that are more than 100 times smaller than its largest singular value to have a fair comparison .",
    "\\(4 ) _ support recovery auc _ , that is , the area under the receiver operating characteristic ( roc ) curve of comparing support of @xmath47 with the ground truth , which is always between 0 and 1 .",
    "it is computed by varying the decision threshold and obtaining the false positive and true positive curve .",
    "then the area under the false positive and true positive curve is reported as auc .",
    "the value of auc indicates the probability that a procedure assigns a higher value to a randomly chosen non - zero element than a randomly chosen zero element @xcite .",
    "it is an appropriate metric for measuring support recovery accuracy because in sparse support recovery we have more zeros than non - zeros which inflates the result of the simple 0 - 1 accuracy measure .",
    "in contrast , auc is more robust to imbalanced positive / negative prediction labels .",
    "table [ tab : synth ] shows the results of all algorithms on a variety of regimes by varying the dimensionality @xmath81 and the rank @xmath0 .",
    "we can see that seed is superior or comparable to the baseline algorithms across all four measures .",
    "as the results show and the theory predicts , in most high - dimensional cases , nuclear norm usually overestimates the true rank of the matrix .",
    "furthermore , we find that the iterative svd procedure in the rcgl algorithm often results in significant underestimation of the true rank , when the true rank is large . note that in addition to accuracy , seed also significantly reduces the variance of the estimation .",
    "@l|c | c |c| c|c & algorithm & normalized prediction & normalized estimation & rank recovery & support recovery + & & error ( @xmath224 ) & error ( @xmath224 ) & error & auc +   + & seed & @xmath225 & @xmath226 & @xmath227 & @xmath228 + & ln  admm & @xmath229 & @xmath230 & @xmath231 & @xmath232 + & rcgl & @xmath233 & @xmath234 & @xmath235 & @xmath236 + & seed & @xmath237 & @xmath238 & @xmath227 & @xmath239 + & ln  admm & @xmath240 & @xmath241 & @xmath242 & @xmath243 + & rcgl & @xmath244 & @xmath245 & @xmath246 & @xmath247 + & seed & @xmath248 & @xmath249 & @xmath227 & @xmath250 + & ln  admm & @xmath251 & @xmath252 & @xmath253 & @xmath254 + & rcgl & @xmath255 & @xmath256 & @xmath257 & @xmath258 + & seed & @xmath259 & @xmath260 & @xmath227 & @xmath261 + & ln  admm & @xmath262 & @xmath263 & @xmath264 & @xmath265 + & rcgl & @xmath266 & @xmath267 & @xmath268 & @xmath269 + & seed & @xmath270 & @xmath271 & @xmath227 & @xmath272 + & ln  admm & @xmath273 & @xmath274 & @xmath275 & @xmath276 + & rcgl & @xmath277 & @xmath278 & @xmath279 & @xmath280 +   + & seed & @xmath281 & @xmath282 & @xmath227 & @xmath283 + & ln  admm & @xmath229 & @xmath230 & @xmath231 & @xmath232 + & rcgl & @xmath233 & @xmath234 & @xmath235 & @xmath236 + & seed & @xmath284 & @xmath285 & @xmath227 & @xmath286 + & ln  admm & @xmath240 & @xmath241 & @xmath242 & @xmath287 + & rcgl & @xmath244 & @xmath245 & @xmath246 & @xmath247 + & seed & @xmath288 & @xmath289 & @xmath227 & @xmath290 + & ln  admm & @xmath251 & @xmath252 & @xmath253 & @xmath254 + & rcgl & @xmath255 & @xmath256 & @xmath257 & @xmath258 + & seed & @xmath291 & @xmath292 & @xmath227 & @xmath293 + & ln  admm & @xmath262 & @xmath263 & @xmath264 & @xmath265 + & rcgl & @xmath266 & @xmath267 & @xmath268 & @xmath269 + & seed & @xmath294 & @xmath295 & @xmath227 & @xmath293 + & ln  admm & @xmath273 & @xmath274 & @xmath275 & @xmath276 + & rcgl & @xmath277 & @xmath278 & @xmath279 & @xmath280 +    [ tab : synth ]    figure [ fig : rankpath ] shows the solution path for seed on one example data set ( @xmath296 , @xmath297 , @xmath298 , @xmath299 , and @xmath300 ) .",
    "the corresponding singular values are set to @xmath301 , and @xmath302 . in the horizontal axis",
    ", we show the termination parameter @xmath67 normalized by @xmath303 .",
    "we can see that seed can identify the correct rank with medium values of @xmath67 .",
    "figure [ fig : spsolpath ] shows the solution path for the top left singular vector @xmath304 of @xmath98 on an example data set ( @xmath305 , @xmath306 , @xmath307 , @xmath299 , and @xmath308 ) .",
    "both of the solution paths indicate that seed is robust to the particular choice of parameters and in a large range of parameters seed is able to successfully recover the true rank of the matrix and the support of the singular vectors .",
    "in order to study scalability of seed , we conduct the experiments on two computing environment , including : ( 1 ) an off - the - shelf personal computer ( pc ) and ( 2 ) a graphics processing unit ( gpu ) , to demonstrate the runtime efficiency and the parallelization capability of seed .",
    "first , we run our experiments on an off - the - shelf pc with intel i7 at 3.4ghz and 8 gb of memory .",
    "the system runs matlab r2013b on the windows operating system .",
    "we generate 5 data sets with @xmath309 , non - zero ratio of @xmath310 , @xmath311 , and @xmath312 .",
    "figure [ fig : speedupcpu ] shows the average cpu runtime of three algorithms as the dimension @xmath81 increases .",
    "we can see that seed can achieve a speed up of 10 - 100 times in runtime compared with baseline methods .",
    "next , in order to test scalability of seed in extremely large data sets , we use a machine that is equipped with a tesla k40 gpu which has 2880 processing cores at 745mhz and 12 gb of memory .",
    "we perform our experiments with matlab r2013b on a debian linux operating system .",
    "gpus are built to have many less - powerful processing units which makes them ideal for parallel implementation ( * ? ? ?",
    "* chapter 5 ) .",
    "therefore we apply the two - step fast eigenvalue decomposition described in section [ sec : imp ] , which involves only simple matrix operation and can be paralleled easily .",
    "the experiment results shown in figures [ fig : speedupgpu ] and [ fig : scalability ] are obtained under the setting of @xmath313 , @xmath314 , @xmath309 and non - zero ratio of @xmath310 .",
    "the results indicate that while seed is fast on the gpu , it also achieves reasonable accuracy .",
    "note that the results show that seed is able to estimate a sparse and low - rank matrix with @xmath315 elements in less than a minute , confirming its extreme scalability .",
    "_ diffusion network inference _ , that is , the task of inferring influence networks from user activities , is one of the central tasks in social networks analysis @xcite because it helps improve social marketing by finding the influential users in a network .",
    "it is a challenging problem because : ( i ) in many social networks the influence is expressed implicitly @xcite and ( ii ) empirical studies show that common metrics such as number of friends or followers fail to accurately measure the social influence of the users @xcite .",
    "a popular computational approach in estimating social influence among users is to count the number of users activities over a time span ( in regularly or irregulary spaced intervals ) and analyze the resulting time series data @xcite .",
    "many different models have been developed , among which the vector auto - regressive model arises as a simple and robust solution @xcite .",
    "that is , every user is described by a time series @xmath316 for @xmath317 .",
    "next , the vector auto - regressive model with @xmath318 lags is fitted to the time series as follows : @xmath319 where @xmath320 $ ] and @xmath321 is the evolution matrix at @xmath322th lag .",
    "the influence network can be built from the evolution matrices by establishing an edge from node @xmath323 to node @xmath6 if @xmath324 is significantly larger than zero .    in this experiment",
    ", we gather a twitter data set with tweets on the `` haiti earthquake '' and apply vector auto - regressive model to identify the potential top influencers on this topic ( that is , those twitter accounts with the largest impact on the others ) .",
    "we divide the 17 days after the haiti earthquake on jan . 12 , 2010 into 1000 uniformly spaced intervals and generate a multivariate time series data set by counting the number of tweets on this topic for the top 1000 users who tweeted most about it .",
    "for accurate modeling , we remove the users that were highly correlated with each other , most of which were operated by the same users and tweeted exactly the same content .",
    "we also remove robot - like user - accounts which tweeted on very regular intervals , which in total led to a subset of 270 users .",
    "we analyze this data with a var(@xmath325 ) model which requires estimation of a @xmath326 dimensional response vector using @xmath327 predictors while we have only @xmath328 observations . the number of lags is chosen based on the intuition about the maximum retweeting delay .",
    "since we do not have access to the true influence network , we use the retweet network as a surrogate of the ground truth following the evaluation convention in the social networks community .",
    "the retweet network is constructed by adding an edge from user @xmath6 to user @xmath323 if user @xmath323 has retweeted at least @xmath329 of the tweets of user @xmath6 .",
    "clearly , the retweet network is not the actual underlying temporal dependency graph , mainly because there are possible implicit influence patterns as well .",
    "however , it is the best possible metric that we could obtain for graph estimation accuracy evaluation in our data set @xcite .",
    "the retweet network for the 270 selected users is sparse ; it has only @xmath330 of possible edges .",
    "we apply seed , ln ",
    "admm , and rcgl algorithms to uncover the influence network in our twitter data set .",
    "figure [ fig : accuracytwitter ] shows the accuracy of the procedures in uncovering the true influence network in terms of auc . for every value of the rank parameter",
    ", we tune the sparsity by 5-fold cross - validation .",
    "given the fact that exact rank constraint can not be enforced directly in the ln ",
    "admm algorithm , we find the best value of the nuclear norm regularization parameter @xmath331 by 5-fold cross - validation .",
    "then , we compute the low - rank approximations of the parameter matrix and evaluate the accuracy at each rank .        the results in figure [ fig : accuracytwitter ] show that seed significantly outperforms the baseline procedures .",
    "they also indicate that , in all of the algorithms , as we increase the rank of the solution matrix , the accuracy is improved initially and then quickly saturates .",
    "seed achieves the highest accuracy when the rank is 4 .",
    "note that this result also confirms other studies that the social network connections may be strongly influenced by a few unobserved exogenous variables @xcite .",
    "the results in table [ tab : speedtwitter ] demonstrate the significant speedup achieved by seed compared to the baselines .",
    ".run time ( in seconds ) of the algorithms on the application data set .",
    "[ cols=\"^,^,^\",options=\"header \" , ]     [ tab : speedtwitter ]",
    "in this paper , we propose to convert the problem of sparse reduced - rank regression into a sparse generalized eigenvalue problem , which allows us to efficiently employ the recently developed sparse eigenvalue decomposition techniques .",
    "after this transformation , the left singular vectors can be estimated in two simple steps , and the estimation of both sparse and dense right singular vectors is unified in a single framework . as a pure learning algorithm",
    ", seed deviates from traditional regularization frameworks ( i.e. , a loss function plus certain penalties ) , leading to computational efficiency and scalability .",
    "furthermore , we prove that seed achieves nice estimation and prediction accuracy that coincides with the minimax error bound in the univariate regression setting @xcite .",
    "some interesting problems for future research include extending the current formulation of the regression coefficient matrix in ( [ eq : c * ] ) to the case where the singular values can be repeated such that the left singular vectors ( which correspond to latent factors ) are not identifiable",
    ". then we will need to estimate the eigenspaces spanned by important singular vectors and characterize the estimation accuracy by some new criterion , such as the one in @xcite and @xcite .",
    "another research direction is to explore the theory of random design matrices and this can be addressed by using an extended version of perturbation theory where the perturbation in @xmath24 is also included in the analysis .",
    "moreover , it is computationally straightforward to extend seed to the generalized linear models by adapting the sequential quadratic programming framework . for this extension",
    ", we first approximate the loss function by the quadratic loss function and find the optimal unit rank matrix . then we can add the unit rank matrix to the solution and re - approximate the loss function with another quadratic function around this new solution . by performing these three steps sequentially , we can efficiently estimate the low - rank coefficient matrix .",
    "statistical properties of such estimator can be analyzed by extending the results in @xcite for greedy sparse procedures to reduced - rank regression .",
    "chandrasekaran , v. , parrilo , p. , willsky , a.  s. , et  al .",
    "latent variable graphical model selection via convex optimization . in _ communication , control , and computing ( allerton ) , 2010 48th annual allerton conference on _ , pages 16101613 .",
    "ieee .",
    "embar , v.  r. , pasumarthi , r.  k. , and bhattacharya , i. ( 2014 ) .",
    "a bayesian framework for estimating properties of network diffusions . in _ proceedings of the 20th acm sigkdd international conference on knowledge discovery and data mining _ , pages 12161225 .",
    "acm .",
    "myers , s.  a. , zhu , c. , and leskovec , j. ( 2012 ) .",
    "information diffusion and external influence in networks . in _ proceedings of the 18th acm sigkdd international conference on knowledge discovery and data mining _ , pages 3341 .",
    "acm .",
    "starbird , k. and palen , l. ( 2012 ) .",
    "( how ) will the revolution be retweeted ? : information diffusion and the 2011 egyptian uprising . in _ proceedings of the acm 2012 conference on computer",
    "supported cooperative work _ , pages 716 ."
  ],
  "abstract_text": [
    "<S> sparse reduced - rank regression is an important tool to uncover meaningful dependence structure between large numbers of predictors and responses in many big data applications such as genome - wide association studies and social media analysis . despite the recent theoretical and algorithmic advances , </S>",
    "<S> scalable estimation of sparse reduced - rank regression remains largely unexplored . in this paper , we suggest a scalable procedure called sequential estimation with eigen - decomposition ( seed ) which needs only a single top-@xmath0 singular value decomposition to find the optimal low - rank and sparse matrix by solving a sparse generalized eigenvalue problem . </S>",
    "<S> our suggested method is not only scalable but also performs simultaneous dimensionality reduction and variable selection . under some mild regularity conditions , </S>",
    "<S> we show that seed enjoys nice sampling properties including consistency in estimation , rank selection , prediction , and model selection . </S>",
    "<S> numerical studies on synthetic and real data sets show that seed outperforms the state - of - the - art approaches for large - scale matrix estimation problem .    </S>",
    "<S> # 1    0    0    1    0    _ running title _ : seed _ key words _ : reduced - rank regression ; scalability ; sparsity ; high dimensionality ; greedy algorithm ; sparse eigenvector estimation </S>"
  ]
}