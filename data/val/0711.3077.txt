{
  "article_text": [
    "we study the algorithms that reduce the average complexity of maximum likelihood ( ml ) decoding of convolutional codes . by ml decoding ,",
    "we mean the decoder uses code - search to find , and to guarantee the output of , the most likely codeword .",
    "forney showed that ml decoding of convolutional codes is equivalent to finding the most probable path taken through a markov graph @xcite .",
    "denote the codeword length by @xmath0 and the coding memory by @xmath1 . for each time index ,",
    "the number of markov states in the markov graph is exponential in @xmath1 .",
    "the total number of markov states is therefore exponential in @xmath1 but linear in @xmath0 .",
    "define the complexity of a decoder as the number of visited markov states normalized by the codeword length @xmath0 .",
    "practical ml decoding is often achieved using the viterbi algorithm ( va ) @xcite@xcite , whose complexity does not scale in @xmath0 but scales exponentially in @xmath1 .",
    "well known decoders such as the list decoders @xcite , the sequential decoders @xcite , and the iterative decoders @xcite are able to achieve near optimal error performance with low average complexity .",
    "however , these decoders do not guarantee the output of the ml codeword @xcite .",
    "if obtaining the ml codeword is strictly enforced ( see section [ discussions ] for justification ) , to avoid exhaustive path search , the decoder must develop certain criterion or bound that can be used to disprove the optimality of a markov path set .",
    "this is equivalent to developing an optimality test criterion ( otc ) @xcite to test whether the ml path ( or codeword ) belongs to the complementary path set ( or codeword set ) .",
    "two major otcs have been used in the ml decoding of convolutional codes .",
    "the first one is the  path covering criterion \" ( pcc ) ( explained in @xcite and in appendix [ pcc ] ) used in the va @xcite@xcite .",
    "va visits _ all _ markov states in chronological order @xcite . for each time index ,",
    "the decoder maintains a set of  cover \" ( defined in appendix [ pcc ] ) markov paths each passing one of the markov states @xcite .",
    "according to the pcc , the  cover \" markov path passing a markov state disproves the optimality of all other markov paths passing the same state .",
    "the second otc is the sum log likelihood ( sll)-based otcs used extensively in the sphere decoder @xcite@xcite .",
    "sphere decoder models ml decoding as finding the lattice point closest to the channel output in the signal space @xcite .",
    "hence the distance between the channel output and an arbitrary lattice point upper bounds the distance from the channel output to the ml codeword .",
    "such distance bound is based on the sll of the corresponding codeword , and is used in the sphere decoder @xcite@xcite as well as other ml decoders @xcite as the key means to avoid exhaustive codeword search . in @xcite@xcite ,",
    "vikalo and hassibi showed that pcc - based and sll - based optimality tests can be combined to find the ml codeword without visiting all markov states .",
    "assume pcc - based optimality test is always implemented .",
    "in this paper , we first show that _ additional _ complexity reduction brought by the sll - based optimality test diminishes as one fixes the coding memory @xmath1 and takes the codeword length @xmath0 to infinity .",
    "such inefficiency is due to the fact that sll - based otc does not exploit the structure of the convolutional code . searching the ml codeword is equivalent to finding the ml source message , which contains a sequence of source symbols .",
    "we show whether the ml message contains a particular symbol at a given time index can be tested using an otc that depends only on the log likelihood of channel output symbols in a _ fixed - sized _ time neighborhood .",
    "we call such test the neighboring log likelihood ( nll)-based optimality test , and show its efficiency does not depend on the codeword length .",
    "we theoretically demonstrate that nll - based optimality test can bring significant complexity reduction to ml decoding when the communication system has a high signal to noise ratio ( snr ) .",
    "complexity of the decoder using sll - base optimality test , on the other hand , remains the same as the va for all snr if the codeword length is taken to infinity .",
    "the results are also generalized to ml sequence detection in a class of discrete - time hidden markov systems @xcite .",
    "let @xmath2 be an @xmath3 convolutional code over @xmath4 defined by a polynomial generater matrix @xmath5 @xcite , @xmath6+\\mbox{\\boldmath   $ g$}[1]d+ \\dots + \\mbox{\\boldmath   $ g$}[\\nu-1]d^{\\nu-1},\\ ] ] where @xmath7 is the delay operator ; @xmath1 is the coding memory ; @xmath8 $ ] , @xmath9 , are @xmath10 matrices over @xmath4 .",
    "assume @xmath5 is a minimal encoder @xcite .",
    "denote the source message by a _ sequence _ of vector _ symbols _",
    ", @xmath11d^d+\\mbox{\\boldmath   $ x$}[d+1]d^{d+1}+\\dots,\\ ] ] where @xmath12 is the time index , possibly negative ; @xmath13 $ ] , @xmath14 , are row vectors of dimension @xmath15 over @xmath4 . the encoded message , or the corresponding codeword , is given by @xmath16\\mbox{\\boldmath   $ g$}[l]d^d.\\ ] ] to simplify the presentation , we assume time index @xmath12 takes all integer values .",
    "we assume @xmath13=\\mbox{\\boldmath   $ 0$}$ ] for @xmath17 and @xmath18 .",
    "we term @xmath0 the codeword length .",
    "define a function @xmath19 that maps @xmath20 from @xmath4 to @xmath21 ( the set of real numbers ) in one - to - one sense .",
    "if @xmath22 is a vector sequence , @xmath23 applies the mapping to each of the elements of @xmath22 , respectively is a vector sequence of the same length and dimension as @xmath22 . ] .",
    "assume the codeword is transmitted over a memoryless gaussian channel .",
    "the channel output symbol sequence is given by @xmath24 where @xmath25d^d+\\mbox{\\boldmath   $ n$}[d+1]d^{d+1}+\\dots$ ] is the noise sequence with @xmath26 \\sim n(\\mbox{\\boldmath   $ 0 $ } , \\sigma^2\\mbox{\\boldmath   $ i$ } ) $ ] being i.i.d . gaussian .",
    "without loss of generality , we define the scaled signal to noise ratio of the system as @xmath27 . in section [ hmm ]",
    ", we show that the results are generalizable not only to other channel models , but also to a class of hidden markov systems .    given the channel output , for any source message @xmath28 and its corresponding codeword @xmath29",
    ", we define the  negative sll \" as @xmath30-g_q\\left(\\mbox{\\boldmath   $ y$}[d]\\right)\\right\\|^2 . \\label{negativesll}\\ ] ] the objective of ml decoding is to find the ml message @xmath31 that minimizes the negative sll , @xmath32 , 0\\le d <",
    "n}s_x(\\mbox{\\boldmath   $ x$}(d)).\\ ] ]    throughout this paper , we assume pcc - based optimality test is always implemented . for the sake of completeness , a description of pcc - based optimality test is given in appendix [ pcc ] .",
    "for ml decoders using sll - based optimality test , the decoder first obtains a quick guess of the source message without solving the ml decoding problem .",
    "sll of the obtained message is then used to help disproving the optimality of certain markov path sets and consequently to avoid exhaustive path search .",
    "we make an ideal assumption that the  guessed \" message equals the transmitted message .",
    "we show in this section that , even under this ideal assumption , complexity reduction brought by the sll - based optimality tests still diminishes as we take @xmath0 to infinity .",
    "let @xmath28 be the actual source message , which is also the message  guessed \" by the decoder .",
    "let @xmath29 be the transmitted codeword .",
    "the corresponding negative sll is given by @xmath33-g_d\\left(\\mbox{\\boldmath   $ y$}[d]\\right)\\right\\|^2=\\sum_{d=0}^{n+\\nu-1}\\left\\|\\mbox{\\boldmath   $ n$}[d]\\right\\|^2 .",
    "\\label{sll}\\ ] ]    now consider a subset of time indices @xmath34 .",
    "let @xmath35|d\\in d_d^x \\right\\}$ ] be a _",
    "partial message _ defined only at time indices in @xmath36 .",
    "denote by @xmath37 the set of messages satisfying @xmath38=\\tilde{\\mbox{\\boldmath   $ x$}}[d ] , \\forall d\\in d_d^x , \\mbox{\\boldmath   $ x$}_0(d)\\ne \\mbox{\\boldmath   $ x$}(d ) \\}.\\ ] ] suppose the decoder wants to test whether it can disprove the optimality of @xmath37 , i.e. , whether @xmath39 . a common practice @xcite@xcite@xcite is to find a lower bound , denoted by @xmath40 , of the negative slls of the messages in @xmath37 .",
    "@xmath41 if the lower bound @xmath40 is larger than @xmath42 obtained in ( [ sll ] ) , then we have @xmath43 for all @xmath44 , which means the ml message is not in @xmath37 .    in appendix",
    "[ snllowerbound ] , we show that the sll lower bounds appeared in the literature satisfy the following assumption .",
    "[ assumption1 ] given @xmath37 , let @xmath45 be the maximum time index set , over which we can find a _",
    "partial codeword _",
    "@xmath46 such that for all @xmath44 with @xmath47 , we have @xmath48=\\tilde{\\mbox{\\boldmath   $ y$}}[d]$ ] for all @xmath49 . note that @xmath50 and @xmath46 are uniquely determined by @xmath37 .",
    "we also have @xmath51 .",
    "we assume the existence of a positive constant @xmath52 $ ] , whose value does _ not _ depend on @xmath0 , such that @xmath53-g_q\\left(\\tilde{\\mbox{\\boldmath   $ y$}}[d]\\right)\\right\\|^2    + ( n+\\nu-|d_d^y|)(1-\\epsilon)n\\sigma^2.\\end{aligned}\\ ] ] @xmath54    as demonstrated in @xcite@xcite , if we fix @xmath0 , using @xmath55 as the otc to disprove the optimality of message set @xmath37 can bring significant complexity reduction to ml decoding , especially under high snr .",
    "however , if we define @xmath56 as the subset of time indices corresponding to the erroneous codeword symbols , i.e. , @xmath57 the following proposition shows that sll - based optimality tests become inefficient if @xmath58 is taken to infinity while @xmath59 is kept finite .",
    "[ proposition1 ] assume the generater matrix @xmath5 is fixed , and therefore the constraint length @xmath1 is fixed .",
    "consider message sets characterized by @xmath37 for arbitrary @xmath36 but under the constraint of a fixed @xmath60 , where @xmath61 is defined in ( [ de ] ) and the derivation of @xmath50 is specified in assumption [ assumption1 ] .",
    "if we fix snr and take @xmath58 to infinity , we have @xmath62 if we first take @xmath58 to infinity and then take snr to infinity , we have @xmath63 @xmath54    since @xmath51 , taking @xmath58 to infinity implies taking @xmath64 to infinity .",
    "according to assumption [ assumption1 ] , we have @xmath65-g_q\\left(\\tilde{\\mbox{\\boldmath   $ y$}}[d]\\right)\\right\\|^2 \\right )    + ( 1-\\epsilon)n\\sigma^2   \\nonumber \\\\ & & - \\frac{1}{n+\\nu-|d_d^y|}\\left(\\sum_{d\\in d_e}\\|\\mbox{\\boldmath   $ n$}[d]\\|^2 \\right )   -\\frac{1}{n+\\nu-|d_d^y|}\\sum_{d\\not\\in d_d^y}\\|\\mbox{\\boldmath   $ n$}[d]\\|^2 . \\label{sllbound2}\\end{aligned}\\ ] ]    since @xmath26 $ ] are i.i.d .",
    "gaussian with covariance matrix @xmath66 , @xmath67\\|^2 $ ] are i.i.d .",
    "@xmath68 with mean @xmath69 and variance @xmath70",
    ". therefore @xmath71\\|^2\\to n\\sigma^2 $ ] , @xmath72-g_q\\left(\\tilde{\\mbox{\\boldmath   $ y$}}[d]\\right)\\right\\|^2 \\right)\\to 0 $ ] , and @xmath73\\|^2\\to 0 $ ] with probability one as @xmath74 .",
    "consequently , denote the right hand side of ( [ sllbound2 ] ) by @xmath75 , we have with probability one , @xmath76 this yields @xmath77    since ( [ ineffsllotc ] ) holds for all snr , the conclusion remains true if we take snr to infinity after @xmath58 is taken to infinity ) is important .",
    "if we fix @xmath0 and take snr to infinity first , we can get @xmath78 . ] .    with the help of lemma [ proposition1 ] , inefficiency of sll - based optimality tests",
    "is characterized by the following lemma .",
    "[ proposition2 ] let @xmath79 be the complexity of an ml decoder that only uses pcc- and sll - based optimality tests for complexity reduction .",
    "let @xmath80 be the complexity of the viterbi decoder , in which , only pcc - based optimality test is used .",
    "for any @xmath81 , we have , @xmath82 @xmath54    the proof of lemma [ proposition2 ] is given in appendix [ proofproposition2 ] .",
    "we propose in theorem [ theorem1 ] a class of nll - based optimality tests , whose efficiency does not depend on the codeword length @xmath0 .",
    "we show in section [ sectionv ] that these nll - based optimality tests can significantly reduce the average complexity of ml decoding under high snr .",
    "this is in contrast to the inefficiency of sll - based optimality tests which are not able to bring meaningful complexity reduction if @xmath0 is taken to infinity first .",
    "[ theorem1 ] define @xmath83 , @xmath84 by @xmath85 where @xmath86 , @xmath87 are @xmath88-dimensional row vectors over @xmath89 .",
    "let @xmath90 be an arbitrary constant , @xmath91 be an arbitrary integer , satisfying @xmath92 let @xmath93 be a source message whose corresponding codeword is @xmath94 .",
    "for any time index @xmath95 , if the following inequality is satisfied for all @xmath96 , @xmath97-g_q(\\mbox{\\boldmath   $ y$}_0[d ] ) \\| < \\frac{d_{\\min}^2}{2}-\\xi , \\label{plldiference1}\\ ] ] and the following inequalities hold , @xmath98-g_q(\\mbox{\\boldmath   $ y$}_0[d ] ) \\|^2 \\le m\\xi -\\nu d_{\\max}^2 \\nonumber \\\\ & & \\sum_{d = m-(2m+1)\\nu}^{m-2m\\nu-1 } \\|\\mbox{\\boldmath   $ r$}[d]-g_q(\\mbox{\\boldmath   $ y$}_0[d ] ) \\|^2 \\le m\\xi -\\nu d_{\\max}^2 , \\label{plldiference2}\\end{aligned}\\ ] ] then we must have @xmath99=\\mbox{\\boldmath   $ x$}_{ml}[\\tilde{m}]$ ] , @xmath100 .",
    "@xmath54    we skip the proof of theorem [ theorem1 ] since the result is implied by theorem [ theorem3 ] presented in section [ hmm ] .",
    "note that the values of @xmath101 and @xmath102 only depend on the @xmath103 function .",
    "hence , as long as @xmath103 and @xmath1 are given , the values of @xmath90 and @xmath91 can be fixed , e.g. , @xmath104 and @xmath105 .",
    "given @xmath91 , the optimality test presented in theorem [ theorem1 ] testifies the optimality of @xmath106| \\tilde{m}\\in [ m , m+\\nu ) \\}$ ] using the log likelihood of channel output symbols within a _ fixed - sized _ time interval @xmath107 .",
    "it is quite intuitive to see , efficiency of the test does not depend on the codeword length if all other parameters are fixed .",
    "efficiency of the otc proposed in theorem [ theorem1 ] is characterized by the following lemma .",
    "[ proposition3 ] assume @xmath90 and @xmath91 are chosen to satisfy ( [ ximconstraint ] ) .",
    "let @xmath95 be an arbitrary time index .",
    "let @xmath94 equal the transmitted codeword within time interval @xmath107 .",
    "define @xmath108 as the event that ( [ plldiference2 ] ) is satisfied and ( [ plldiference1 ] ) is satisfied for all @xmath96 .    fix all other parameters and take snr to infinity , we have @xmath109 the same conclusion holds if we first take @xmath0 to infinity , then take snr to infinity",
    ". @xmath110    if @xmath94 equals the transmitted codeword within time interval @xmath107 , for @xmath111 , we have @xmath112-g_q(\\mbox{\\boldmath   $ y$}_0[d])=\\mbox{\\boldmath   $ n$}[d].\\ ] ] consequently , ( [ pllotcefficiency ] ) and ( [ pllotcefficiencyn ] ) hold because @xmath67\\|^2 $ ] are i.i.d .",
    "@xmath68 , whose mean , @xmath113 , and variance , @xmath114 , converge to @xmath115 as snr goes to infinity .",
    "lemma [ proposition3 ] implies , if there is a suboptimal decoder whose probability of _ symbol _ detection error ( as opposed to sequence detection error ) is low under high snr , then nll - based optimality tests can help transforming the suboptimal detector to an ml detector with only marginal increase in average decoding complexity .",
    "an example of such transformation is presented in the following section .",
    "the communication system given in section [ sectionii ] follows a discrete - time hidden markov model @xcite , where each markov state at time index @xmath12 corresponds to a possible combination of source symbols in time interval @xmath116 $ ] .",
    "if a decoder obtains the ml codeword using the va , all markov states within time interval @xmath117 $ ] have to be visited .",
    "alternatively , if one can use a low complexity algorithm to disprove the optimality of most of the markov states , then the va can limit its search by visiting only a small subset of markov states .",
    "following this idea , the three - step ml decoding framework is given as follows .    * step 1 : the decoder uses a suboptimal algorithm ( denoted by @xmath118 ) to obtain a quick guess of the codeword @xmath119 and its corresponding source message @xmath120 .",
    "* step 2 : an nll - based optimality test ( specified in theorem [ theorem1 ] ) is applied to each of the source symbols of @xmath120 .",
    "the decoder maintains a source symbol set sequence @xmath121 , with @xmath122 $ ] being the source symbol set of time index @xmath12 .",
    "if @xmath123=\\mbox{\\boldmath   $ x$}_{ml}[d]$ ] can be confirmed by the optimality test , we let @xmath122=\\{\\tilde{\\mbox{\\boldmath   $ x$}}[d]\\}$ ] ; otherwise , we let @xmath122 $ ] be the set of all possible source symbol vectors at time index @xmath12 . *",
    "step 3 : the decoder uses a modified va to search for the ml source message . the only difference between the modified va and the conventional va is that , the modified va visits a markov state only if all source symbols corresponding to the markov state belong to the source symbol sets @xmath122 $ ] of the corresponding time indices .",
    "implementing the modified va is quite straightforward . hence its further description is skipped .",
    "comparing to the three - step decoding algorithm studied in @xcite , the key advantage of using an nll - based optimality test is that the test can be applied to an individual source symbol rather than the whole source message .",
    "[ theorem2 ] let @xmath124 be the probability of _ symbol _ detection error of @xmath118 .",
    "assume , while fixing all other parameters , @xmath125 let @xmath126 be the average number of markov states per time unit visited by the modified va in the third step of the ml decoder .",
    "for any @xmath81 , we have @xmath127    let @xmath28 , @xmath22 be the actual source message and the transmitted codeword , respectively .",
    "let @xmath120 , @xmath119 be the source message and the codeword output by @xmath118 . according to ( [ symbolerror ] ) , for any time index @xmath95",
    ", we have @xmath128=\\mbox{\\boldmath   $ y$}[d ] , \\\\ \\forall d\\in [ m-2(m-1)\\nu , m+(2m+1)\\nu)\\end{array}\\right\\}=1 . \\label{correctdetection}\\ ] ] where @xmath91 is the parameter of the nll - based optimality test , specified in theorem [ theorem1 ] . according to ( [ correctdetection ] ) , lemma [ proposition2 ] , and theorem [ theorem1 ] , for any @xmath95 , if @xmath129=\\mbox{\\boldmath   $ y$}[d ] , \\forall d\\in [ m-2(m-1)\\nu , m+(2m+1)\\nu)$ ] , then the probability that the nll - based optimality test can confirm @xmath123=\\mbox{\\boldmath   $ x$}_{ml}[d ] , \\forall d\\in [ m , m+\\nu)$ ] converges to one as @xmath130",
    "consequently , letting @xmath122 $ ] be the source symbol set maintained by the ml decoder in the second step , we have @xmath131|=1 , \\forall d\\in [ m , m+\\nu)\\right\\}=1 , \\qquad \\forall m \\label{lowcomplexityinprob}\\ ] ] since the worst case complexity of the modified va is bounded , ( [ lowcomplexityinprob ] ) implies , for any @xmath81 , @xmath132 .",
    "since all derivations hold if we first take @xmath0 to infinity , we also have @xmath133 .    by sharing computations among optimality tests",
    ", it is easy to see that the complexity of the second step of the ml decoder is equivalent , in order , to visiting one markov state per time unit . therefore , if @xmath118 satisfies ( [ symbolerror ] ) , as @xmath130 , the complexity of the three - step ml decoder converges to the complexity of @xmath118 , which can be significantly lower than the complexity of the va .",
    "moreover , the three steps of the ml decoder can be implemented in a parallelized manner in the sense that each step can process some of the source symbols without waiting for the previous step to _ completely _ finish its work .",
    "an example of such parallelized implementation can be found in ( * ? ? ?",
    "* ; * ? ? ?",
    "* the simple mlsd algorithm ) .",
    "in this section , we generalize the results of section [ sectioniv ] to ml sequence detection ( mlsd ) in a class of first order discrete - time hidden markov systems @xcite .",
    "we demonstrate in appendix [ hmmmodelverification ] that the communication system presented in section [ sectionii ] satisfies the model and the key assumptions given in this section .",
    "let @xmath134d^d+\\mbox{\\boldmath   $ u$}[d+1]d^{d+1}+ ... $ ] be a first order markov sequence , where @xmath12 is the time index , possibly negative ; @xmath135 $ ] represents the markov state ( at time @xmath12 ) , which is a @xmath136-dimensional row vector defined over @xmath89 .",
    "we assume @xmath135=\\mbox{\\boldmath   $ 0$}$ ] for @xmath17 and @xmath18 , with @xmath0 being the sequence length .",
    "define @xmath137=\\mbox{\\boldmath   $ y$}(\\mbox{\\boldmath   $ u$}[d])$ ] as the  processed state \" , which is a",
    "_ deterministic _ function of @xmath135 $ ] .",
    "@xmath137 $ ] is a @xmath88-dimensional row vector defined over @xmath89 .",
    "we term @xmath138d^d+\\mbox{\\boldmath   $ y$}[d+1]d^{d+1}+ ... $ ] the processed state sequence .",
    "let @xmath139d^d+\\mbox{\\boldmath   $ r$}[d+1]d^{d+1}+ ... $ ] be the observation sequence , where @xmath140 $ ] is a @xmath88-dimensional row vector with real - valued elements .",
    "denote the state transition probability of the hidden markov system by @xmath141=\\mbox{\\boldmath   $ u$}_1|\\mbox{\\boldmath   $ u$}[d]=\\mbox{\\boldmath   $ u$}_2\\}.\\ ] ] define the transition probability ratio bound @xmath142 by @xmath143 we assume the markov chain is ergodic and homogeneous .",
    "therefore , there exists a positive integer @xmath1 , such that @xmath144=\\mbox{\\boldmath   $ u$}_1| \\mbox{\\boldmath   $ u$}[d]=\\mbox{\\boldmath   $ u$}_2\\ } \\ne 0 , \\quad \\forall \\mbox{\\boldmath   $ u$}_1 , \\mbox{\\boldmath   $ u$}_2 .",
    "\\label{homogeneous}\\ ] ]    denote the observation distribution function by @xmath145\\le \\mbox{\\boldmath   $ r$}|\\mbox{\\boldmath   $ y$}[d]=\\mbox{\\boldmath   $ y$}_1\\}. \\label{observationfunction}\\ ] ] let the corresponding probability density function ( or probability mass function ) be @xmath146 .",
    "we also make the following two key assumptions .",
    "[ assumption2 ] we assume state processing @xmath137=\\mbox{\\boldmath   $ y$}(\\mbox{\\boldmath   $ u$}[d])$ ] does not compromise the observability of the markov states in the sense that there exists a positive integer @xmath1 satisfying the following property . given two markov state sequences @xmath147 and @xmath148 .",
    "for any time index @xmath12 , if @xmath135\\ne \\tilde{\\mbox{\\boldmath   $ u$}}[d]$ ] , then we can find a time index @xmath149 , such that @xmath150)\\ne \\mbox{\\boldmath   $ y$}(\\tilde{\\mbox{\\boldmath   $ u$}}[m])$ ] .",
    "note that we used the same constant @xmath1 in ( [ homogeneous ] ) and in assumption [ assumption2 ] .",
    "this is valid because if ( [ homogeneous ] ) is satisfied for @xmath151 , then it is also satisfied for all @xmath152 ; similar property applies to assumption [ assumption2 ] .",
    "consequently , if assumption [ assumption2 ] holds , a common integer @xmath1 satisfying both ( [ homogeneous ] ) and assumption [ assumption2 ] can always be found .",
    "[ assumption3 ] assume the existence of two functions : @xmath153 and @xmath154 , both are functions of the channel output symbol @xmath155 and the processed state @xmath86 .",
    "assume @xmath153 and @xmath154 have the following two properties .",
    "first , the following inequalities hold for all @xmath155 and @xmath86",
    ". @xmath156 \\nonumber \\\\ & & l_u(\\mbox{\\boldmath   $ r$ } , \\mbox{\\boldmath   $ y$}_1 ) \\ge \\max_{\\mbox{\\scriptsize \\boldmath   $ y$}_2\\ne \\mbox{\\scriptsize \\boldmath   $ y$}_3}\\left[-\\log(f_o(\\mbox{\\boldmath",
    "$ r$}|\\mbox{\\boldmath   $ y$}_2))+\\log(f_o(\\mbox{\\boldmath   $ r$}|\\mbox{\\boldmath   $ y$}_3))\\right ] .",
    "\\label{nllu}\\end{aligned}\\ ] ]    second , the complexity of evaluating @xmath153 and @xmath154 is low in the sense that they do not require the search of any processed state other than @xmath86 .",
    "@xmath54    note that validity of the results presented in this section does not depend on the second property imposed in assumption [ assumption3 ] .",
    "however , we still include the property in the assumption since the key motivation of posing assumption [ assumption3 ] is to use the two functions @xmath153 and @xmath154 as tools to avoid exhaustive markov state search and hence to reduce the complexity of ml decoding .",
    "also note that the right hand side of the second inequality in ( [ nllu ] ) is not a function of @xmath86 .",
    "however , the upper bound on the left hand side is a function of a processed state @xmath86 since one often needs a  reference state \" in order to upper bound the right hand side of ( [ nllu ] ) .",
    "further explanation is given in appendix [ hmmmodelverification ] .",
    "given the observation sequence @xmath157 , the negative sll of a state sequence @xmath147 is obtained by @xmath158|\\mbox{\\boldmath   $ y$}[d])p_t(\\mbox{\\boldmath   $ u$}[d]|\\mbox{\\boldmath   $ u$}[d-1])).\\ ] ] the objective of mlsd is to find the ml sequence that minimizes the negative sll , @xmath159 , 0\\le d < n}s_u(\\mbox{\\boldmath   $ u$}(d)).\\ ] ]    the following theorem gives a class of nll - based optimality tests .",
    "[ theorem3 ] assume the discrete - time markov system satisfies assumptions [ assumption2 ] and [ assumption3 ] .",
    "let @xmath160 be a positive constant . given a markov state sequence @xmath147 and the corresponding processed states @xmath22 .",
    "let @xmath142 be defined by ( [ ptr ] ) . for any time index @xmath95 , if there is an integer @xmath161 such that for all @xmath96 @xmath162 , \\mbox{\\boldmath   $ y$}[d ] ) > 3\\nu(\\rho-\\log p_{tr } ) , \\label{lbound}\\ ] ] and @xmath163 ) \\le",
    "3m\\nu\\rho + ( \\nu+1)\\log p_{tr } \\nonumber \\\\ & & \\sum_{d = m-(2m+1)\\nu}^{m-2m\\nu-1}l_u(\\mbox{\\boldmath   $ r$ } , \\mbox{\\boldmath   $ y$}[d ] ) \\le 3m\\nu\\rho + \\nu\\log p_{tr } , \\label{lmbound}\\end{aligned}\\ ] ] then @xmath164=\\mbox{\\boldmath   $ u$}_{ml}[m+\\nu-1]$ ] must be true . @xmath54    the proof of theorem [ theorem3 ] is given in appendix [ prooftheorem3 ] .",
    "note that theorem [ theorem3 ] implies theorem [ theorem1 ] if we set the parameters in theorem [ theorem1 ] at the corresponding values given in appendix [ hmmmodelverification ] .    for communication systems following a discrete - time hidden markov model",
    ", @xmath146 often belongs to an ensemble of density ( or probability ) functions , with the actual realization determined by the snr . in other words",
    ", we can write the observation density ( or probability ) @xmath165 as a function of the snr . assume the discrete - time markov system satisfies assumption [ assumption3 ] , where both functions @xmath153 and @xmath154 can be functions of the snr . we make the following assumption .    [ assumption4 ] assume the observation density ( or probability ) @xmath165 is a function of the snr .",
    "assume the discrete - time markov system satisfies assumption [ assumption3 ] .",
    "let the actual state sequence and the processed state sequence be @xmath147 and @xmath22 , respectively .",
    "define two positive numbers @xmath83 and @xmath84 as follows @xmath166 , \\mbox{\\boldmath   $ y$}[d])\\ge \\gamma \\mbox{snr}\\}=1 \\right\\ } , \\nonumber \\\\ & & d_{\\max}^2=     \\inf\\left\\{\\gamma\\ge 0 ; \\lim_{\\mbox{\\scriptsize snr}\\to \\infty}p\\{l_u(\\mbox{\\boldmath   $ r$}[d ] , \\mbox{\\boldmath   $ y$}[d])\\le \\gamma \\mbox{snr}\\}=1 \\right\\}.\\end{aligned}\\ ] ] we assume @xmath167    the following lemma characterizes the efficiency of the otc proposed in theorem [ theorem3 ] .    [ proposition4 ] assume the discrete - time markov system satisfies assumptions [ assumption2 ] and [ assumption4 ] .",
    "let the state sequence be @xmath147 .",
    "let @xmath90 be an arbitrary constant , @xmath91 be an arbitrary integer , satisfying @xmath168 let @xmath169 .",
    "given an arbitrary time index @xmath95 , define @xmath108 as the event that ( [ lmbound ] ) is satisfied and ( [ lbound ] ) is satisfied for all @xmath96 .",
    "if we fix all other parameters except the snr , we have @xmath170 if we fix all other parameters except the snr and the sequence length @xmath0 , we have @xmath171    we skip the proof of lemma [ proposition4 ] since it is quite straightforward .    note that in lemma [ proposition4 ] , when we take @xmath0 and snr to infinity , @xmath91 can be fixed at a constant .",
    "this indicates that , when testing the optimality of a markov state at a given time index , the nll - based optimality test only uses observation symbols in a fixed - sized time neighborhood . based on theorem [ theorem3 ] and lemma [ proposition3 ] , a three - step ml sequence detector similar to the one presented in section [ sectionv ] can be developed to transform a suboptimal sequence detector to a low complexity ml sequence detector .",
    "the detailed discussion is skipped since it does not essentially differ from the one presented in section [ sectionv ] .",
    "in a practical system , suboptimal decoders such as the belief - propagation - based iterative decoders @xcite@xcite can achieve near optimal error performance with low complexity . it is natural to ask : if suboptimal decoding only causes a negligible performance loss , why one should even bother with enforcing the ml solution ?",
    "note that this question does not suggest a default answer since the argument can also be presented in the opposite direction , i.e. , if ml decoding only causes a negligible complexity increase , why one should not use an ml decoder ?",
    "nevertheless , the purpose of our work is not to participate in the debate whether ml decoding is practically useful .",
    "rather , one should interpret theorem [ theorem2 ] as , for convolutional codes , the existence of a well - performed low complexity suboptimal algorithm implies that ml decoding can be carried out with a similar complexity under high snr .",
    "more importantly , such conclusion holds irrespective of the codeword length .",
    "although the efficiency of sll - based optimality tests does not depend on the codeword length , nll - based optimality tests are inefficient only when the codeword length is large .",
    "lemma [ proposition1 ] and theorem [ theorem2 ] suggest that complexity reduction brought by nll - based optimality tests can be superior to sll - based optimality tests even for moderate snr if the codeword length is large enough .",
    "assume the discrete - time hidden markov model given in section [ hmm ] that the model is satisfied by the communication system given in section [ sectionii ] . ] .",
    "given the observation sequence @xmath157 .",
    "let @xmath148 and @xmath147 be two markov state sequences whose corresponding processed state sequences are @xmath119 and @xmath22 , respectively .",
    "if we can find two time indices @xmath172 , such that @xmath173=\\mbox{\\boldmath   $ u$}[d_1]$ ] , @xmath174=\\mbox{\\boldmath   $ u$}[d_2]$ ] , and @xmath175|\\tilde{\\mbox{\\boldmath   $ y$}}[d-1])p_t(\\tilde{\\mbox{\\boldmath   $ u$}}[d]|\\tilde{\\mbox{\\boldmath   $ u$}}[d-1])}{f_o(\\mbox{\\boldmath   $ r$}[d]|\\mbox{\\boldmath   $ y$}[d-1]))p_t(\\mbox{\\boldmath   $ u$}[d]|\\mbox{\\boldmath   $ u$}[d-1 ] ) } < 0 , \\label{pathcovering}\\ ] ] we say @xmath147  covers \" @xmath148 .",
    "we say @xmath147 is a  cover \" path with respect to markov states @xmath176 $ ] and @xmath177 $ ] at time indices @xmath172 if , among all markov paths passing @xmath176 $ ] and @xmath177 $ ] , @xmath147 maximizes @xmath178|\\mbox{\\boldmath   $ y$}[d-1])p_t(\\mbox{\\boldmath   $ u$}[d]|\\mbox{\\boldmath   $ u$}[d-1]))$ ] .",
    "assume all markov paths start from @xmath179=\\mbox{\\boldmath   $ 0$}$ ] .",
    "we say @xmath147 is a  cover \" path with respect to markov state @xmath176 $ ] at time index @xmath180 if , among all markov paths passing @xmath176 $ ] , @xmath147 maximizes @xmath181|\\mbox{\\boldmath   $ y$}[d-1])p_t(\\mbox{\\boldmath   $ u$}[d]|\\mbox{\\boldmath   $ u$}[d-1]))$ ] .      in @xcite@xcite ,",
    "when the decoder branches a markov path at time index @xmath182 , the branch is characterized by a _ partial message _",
    "@xmath183 , \\tilde{\\mbox{\\boldmath   $ x$}}[1 ] , \\dots , \\tilde{\\mbox{\\boldmath   $ x$}}[m]\\}$ ] .",
    "for any codeword @xmath119 associated to the branch , we have @xmath184=\\sum_{l=0}^{\\nu-1}\\tilde{\\mbox{\\boldmath   $ x$}}[d - l]\\mbox{\\boldmath   $ g$}[l].\\ ] ] in other words , @xmath185 $ ] .",
    "the negative sll lower bound is given by @xmath186-g_q\\left(\\tilde{\\mbox{\\boldmath   $ y$}}[d]\\right)\\right\\|^2     \\ge \\sum_{d=0}^{m}\\left\\|\\mbox{\\boldmath   $ r$}[d]-g_q\\left(\\sum_{l=0}^{\\nu-1}\\tilde{\\mbox{\\boldmath   $ x$}}[d - l]\\mbox{\\boldmath   $ g$}[l]\\right)\\right\\|^2 , \\label{spherebound}\\end{aligned}\\ ] ] which satisfies assumption [ assumption1 ] with @xmath187 .",
    "in @xcite , several sll - based otcs were presented for decoding block codes .",
    "the decoder obtains a first guess @xmath22 of the codeword .",
    "a negative sll lower bound @xmath188 is then developed for the codeword set @xmath189 , which corresponds to the case of @xmath190 being an empty set in the context of section [ sectioniii ] .",
    "@xmath22 is optimal if the optimality test @xmath191 gives a positive answer @xcite .",
    "the lower bounds @xmath192 presented in ( * ? ? ?",
    "* ; * ? ? ?",
    "* section iii ) satisfy the following inequality , @xmath193\\right)-g_q\\left(\\mbox{\\boldmath   $ y$}[d]\\right)\\right\\|^2 \\label{distancebound}\\ ] ] since the coding constraint is @xmath1 , we can always find a codeword @xmath194 with @xmath119 differing from @xmath22 at no more than @xmath1 codeword symbols .",
    "this implies that the right hand side of ( [ distancebound ] ) can be upper bounded by a constant , denoted by @xmath195 , which is not a function of @xmath0 .",
    "@xmath193\\right)-g_q\\left(\\mbox{\\boldmath   $ y$}[d]\\right)\\right\\|^2\\le u_1\\ ] ] consequently , given @xmath196 and @xmath197 , there exists a constant @xmath198 such that assumption [ assumption1 ] is satisfied for @xmath199 .",
    "assume , in searching the ml codeword , the decoder successfully avoided visiting a markov state specified by @xmath200 , \\dots , \\mbox{\\boldmath   $ x$}_0[d]\\}$ ] .",
    "this implies that we can find two time index sets , @xmath201 $ ] and @xmath36 , @xmath202=\\phi$ ] , such that the optimality of _ all _ message sets @xmath203 with @xmath204=\\mbox{\\boldmath   $ x$}_0[\\tilde{d}]$ ] , @xmath205 is disproved .",
    "we choose @xmath36 with _ the maximum cardinality _ while make sure that , in disproving the optimality of @xmath200 , \\dots , \\mbox{\\boldmath   $ x$}_0[d]\\}$ ] , the detector visited _ all _ the markov states @xmath206 , \\dots , \\tilde{\\mbox{\\boldmath   $ x$}}[\\tilde{d}]\\}$ ] satisfying @xmath207\\subseteq d_d^x$ ] .    according to the definitions of @xmath208 and @xmath36 , the decoder needs to disprove the optimality of a special message set @xmath209 defined by @xmath210=\\mbox{\\boldmath   $ x$}_0[\\tilde{d}]$ ] , @xmath205 and @xmath210=\\mbox{\\boldmath   $ x$}[\\tilde{d}]$ ] , @xmath211 .",
    "the definition of @xmath36 also implies that the decoder needs to obtain a lower bound @xmath212 of the negative slls of the messages in @xmath203 .",
    "the lower bound @xmath212 should only be a function of the partial message @xmath213 , but should not depend on any source message symbol whose time index is outside @xmath214 .",
    "however , since the corresponding @xmath60 ( defined in ( [ de ] ) ) of @xmath209 satisfies @xmath215 , according to lemma [ proposition1 ] , the probability of disproving the optimality of @xmath209 ( using sll - based optimality test ) is low if @xmath216 .    to make the argument explicit , the fact that the decoder visits _ all _ markov states @xmath206 , \\dots , \\tilde{\\mbox{\\boldmath   $ x$}}[\\tilde{d}]\\}$ ] with @xmath207\\subseteq d_d^x$ ] implies @xmath217 according to lemma [ proposition1 ] , for any positive constant @xmath81 , if we fix all other parameters and take @xmath0 to infinity , we have ) is , if @xmath218 , as @xmath219 , the probability of disproving the optimality of all message sets @xmath203 with @xmath204=\\mbox{\\boldmath   $ x$}_0[\\tilde{d}]$ ] , @xmath205 , using sll - based optimality test goes to zero . ]",
    "@xmath220 combining ( [ complexityva ] ) and ( [ complexityprob2 ] ) , we get @xmath221          consider a communication system modeled in section [ sectionii ]",
    ". define @xmath135=[\\mbox{\\boldmath   $ x$}[d-\\nu+1 ] , \\dots , \\mbox{\\boldmath   $ x$}[d]]$ ] .",
    "it is easy to see @xmath147 is a markov sequence .",
    "the processed state @xmath137=\\mbox{\\boldmath   $ y$}(\\mbox{\\boldmath   $ u$}[d])$ ] is only a function of the corresponding markov state .",
    "if two markov states in successive time indices take the form @xmath223=[\\tilde{\\mbox{\\boldmath   $ x$}}[d-\\nu+1 ] , \\dots , \\tilde{\\mbox{\\boldmath   $ x$}}[d ] ] \\nonumber \\\\ & & \\mbox{\\boldmath   $ u$}[d+1]=[\\tilde{\\mbox{\\boldmath   $ x$}}[d-\\nu+2 ] , \\dots , \\tilde{\\mbox{\\boldmath   $ x$}}[d+1]],\\end{aligned}\\ ] ] for some @xmath120 , then we have @xmath224|\\mbox{\\boldmath   $ u$}[d])=\\frac{1}{q^k}.\\ ] ] otherwise @xmath225|\\mbox{\\boldmath   $ u$}[d])=0 $ ] .",
    "according to ( [ ptr ] ) , we have @xmath226 .    since @xmath135=[\\mbox{\\boldmath   $ x$}[d-\\nu+1 ] , \\dots , \\mbox{\\boldmath   $ x$}[d]]$ ] does not depend on source symbols at time indices @xmath227 , we know @xmath228|\\mbox{\\boldmath   $ u$}[d-\\nu])\\ne 0 , \\qquad \\forall \\mbox{\\boldmath   $ u$}[d ] , \\mbox{\\boldmath   $ u$}[d-\\nu].\\ ] ]      next , we show assumption [ assumption2 ] is satisfied .",
    "let @xmath147 and @xmath148 be two markov state sequences .",
    "let @xmath28 and @xmath22 be the source message and the codeword corresponding to @xmath147 .",
    "let @xmath120 and @xmath119 be the source message and the codeword corresponding to @xmath148 . for a time index @xmath12 , if @xmath135\\ne \\tilde{\\mbox{\\boldmath   $ u$}}[d]$ ] , we can find a time index @xmath230 $ ] such that @xmath231\\ne \\tilde{\\mbox{\\boldmath   $ x$}}[m]$ ] .",
    "consequently , according to ( * ? ? ?",
    "* ; * ? ? ?",
    "* corollary 2 ) , we can find a time index @xmath232 , such that @xmath233\\ne \\tilde{\\mbox{\\boldmath   $ y$}}[\\tilde{m}]$ ] .",
    "therefore , assumption [ assumption2 ] holds because @xmath234 .",
    "let @xmath83 and @xmath84 be defined in theorem [ theorem1 ] .",
    "let @xmath235 be two arbitrary codeword symbols .",
    "we have the following triangle inequalities , @xmath236 the first inequality in ( [ triangleinequality ] ) implies @xmath237+\\log(f_o(\\mbox{\\boldmath   $ r$}|\\mbox{\\boldmath   $ y$}_1 ) )   & & =   \\min_{\\mbox{\\scriptsize \\boldmath   $ y$}_2 , \\mbox{\\scriptsize \\boldmath   $ y$}_2\\ne \\mbox{\\scriptsize \\boldmath   $ y$}_1}\\left[\\frac{\\mbox{\\scriptsize snr}}{2}(\\|\\mbox{\\boldmath   $ r$}-g_q(\\mbox{\\boldmath   $ y$}_2 ) \\|^2-\\|\\mbox{\\boldmath   $ r$}-g_q(\\mbox{\\boldmath   $ y$}_1 ) \\|^2 ) \\right ] \\nonumber \\\\ & & \\ge   \\frac{\\mbox{\\scriptsize",
    "snr}}{2}d_{\\min}(d_{\\min}-2 \\|\\mbox{\\boldmath   $ r$}-g_q(\\mbox{\\boldmath   $ y$}_1 ) \\|).\\end{aligned}\\ ] ] the second inequality in ( [ triangleinequality ] ) implies @xmath238   & & = \\max_{\\mbox{\\scriptsize \\boldmath   $ y$}_2\\ne \\mbox{\\scriptsize \\boldmath   $ y$}_3}\\left[\\frac{\\mbox{\\scriptsize snr}}{2}(\\|\\mbox{\\boldmath   $ r$}-g_q(\\mbox{\\boldmath   $ y$}_2 ) \\|^2-\\|\\mbox{\\boldmath   $ r$}-g_q(\\mbox{\\boldmath   $ y$}_3 ) \\|^2 ) \\right ] \\nonumber \\\\ & & \\le \\max_{\\mbox{\\scriptsize \\boldmath   $ y$}_2}\\left[\\frac{\\mbox{\\scriptsize snr}}{2}\\|\\mbox{\\boldmath   $ r$}-g_q(\\mbox{\\boldmath   $ y$}_2 ) \\|^2\\right ] \\nonumber \\\\ & & \\le \\max_{\\mbox{\\scriptsize \\boldmath   $ y$}_2}\\left[\\mbox{snr}(\\|\\mbox{\\boldmath   $ r$}-g_q(\\mbox{\\boldmath   $ y$}_1 ) \\|^2+\\|g_q(\\mbox{\\boldmath   $ y$}_2)-g_q(\\mbox{\\boldmath   $ y$}_1 ) \\|^2)\\right ] \\nonumber \\\\ & & \\le \\mbox{snr}(\\|\\mbox{\\boldmath   $ r$}-g_q(\\mbox{\\boldmath   $ y$}_1 ) \\|^2+d_{\\max}^2).\\end{aligned}\\ ] ] therefore , assumption [ assumption3 ] is satisfied by defining @xmath239 note that evaluating @xmath153 and @xmath154 does not involve visiting any processed state other than @xmath86 .",
    "if @xmath137 $ ] and @xmath140 $ ] are the actual codeword symbol and the channel output at time index @xmath12 , @xmath240-g_q(\\mbox{\\boldmath   $ y$}[d])\\|=\\|\\mbox{\\boldmath   $ n$}[d]\\|$ ] is a @xmath68 random variable with mean @xmath113 and variance @xmath241 . from ( [ lowerandupperbounds ] ) , it is easily seen that assumption [ assumption4 ] is satisfied with @xmath242 and @xmath243 .",
    "let @xmath148 be an arbitrary markov state sequence with corresponding processed state sequence being @xmath119 .",
    "assume @xmath244 \\ne \\mbox{\\boldmath   $ u$}[m+\\nu-1 ] \\label{prooftheorem3.1}\\ ] ] theorem [ theorem3 ] holds if we can prove that any @xmath148 satisfying ( [ prooftheorem3.1 ] ) can not be the ml state sequence .",
    "let @xmath15 denote a positive integer .",
    "define two integers @xmath245 and @xmath246 as follows .",
    "@xmath247 = \\mbox{\\boldmath   $ u$}[m+\\nu-1-k\\nu]\\ } \\nonumber \\\\ & & k_r=\\mathop{\\mbox{argmin}}_{k > 0 } \\ { \\tilde{\\mbox{\\boldmath   $ u$}}[m+\\nu-1+k\\nu ] = \\mbox{\\boldmath   $ u$}[m+\\nu-1+k\\nu ] \\}.",
    "\\label{prooftheorem3.2}\\end{aligned}\\ ] ] we consider respectively the following four cases based on the values of @xmath245 and @xmath246 . in all the four cases , we show @xmath148 can not be the ml sequence .      since @xmath250 \\ne \\mbox{\\boldmath   $ u$}[m+\\nu-1+k\\nu]$ ] for all @xmath251 , according to assumption [ assumption2 ] , @xmath119 and @xmath22 differ at no less than @xmath252 time indices in the time interval @xmath253 , where @xmath254 denotes the maximum integer no larger than @xmath255 . according to ( [ nllu ] ) and ( [ lbound ] ) , for @xmath256 ,",
    "if @xmath129 \\ne \\mbox{\\boldmath   $ y$}[d]$ ] , we have @xmath257|\\tilde{\\mbox{\\boldmath   $ y$}}[d])}{f_o(\\mbox{\\boldmath   $ r$}[d]|\\mbox{\\boldmath   $ y$}[d])}\\ge l_l(\\mbox{\\boldmath   $ r$}[d ] , \\mbox{\\boldmath   $ y$}[d ] ) > 3\\nu(\\rho-\\log p_{tr}).\\ ] ] consequently , we get @xmath258|\\tilde{\\mbox{\\boldmath   $ y$}}[d])p_t(\\tilde{\\mbox{\\boldmath   $ u$}}[d]|\\tilde{\\mbox{\\boldmath   $ u$}}[d-1])}{f_o(\\mbox{\\boldmath   $ r$}[d]|\\mbox{\\boldmath   $ y$}[d])p_t(\\mbox{\\boldmath   $ u$}[d]|\\mbox{\\boldmath   $ u$}[d-1 ] ) }   \\nonumber \\\\ & & \\ge \\left\\lfloor\\frac{k_l+k_r}{2}\\right\\rfloor 3\\nu(\\rho-\\log p_{tr})+ ( k_r+k_l)\\nu\\log p_{tr }   \\ge \\left\\lfloor\\frac{k_l+k_r}{2}\\right\\rfloor 3\\nu\\rho > 0 \\label{prooftheorem3.3}\\end{aligned}\\ ] ]          @xmath260 is constructed as follows .",
    "@xmath261=\\mbox{\\boldmath   $ u$}[d ] , \\quad \\mbox{for } d < m+2m\\nu \\nonumber \\\\ & & \\mbox{\\boldmath",
    "$ u$}_c[d]=\\tilde{\\mbox{\\boldmath   $ u$}}[d ] , \\quad \\mbox{for } d\\ge m+(2m+1)\\nu . \\label{prooftheorem3.4}\\end{aligned}\\ ] ] according to ( [ homogeneous ] ) , we can always construct @xmath262 $ ] for @xmath263 so that ( [ prooftheorem3.4 ] ) is satisfied .",
    "let @xmath264 be the processed state sequence corresponding to @xmath260 .    from ( [ nllu ] ) and the first inequality in ( [ lmbound ] ) , we get @xmath265|\\tilde{\\mbox{\\boldmath   $ y$}}[d])p_t(\\tilde{\\mbox{\\boldmath   $ u$}}[d]|\\tilde{\\mbox{\\boldmath   $ u$}}[d-1])}{f_o(\\mbox{\\boldmath   $ r$}[d]|\\mbox{\\boldmath   $ y$}_c[d])p_t(\\mbox{\\boldmath   $ u$}_c[d]|\\mbox{\\boldmath   $ u$}_c[d-1 ] ) } \\nonumber \\\\ & & \\ge -\\sum_{d = m+2m\\nu}^{m+(2m+1)\\nu-1}l_u(\\mbox{\\boldmath   $ r$}[d ] , \\mbox{\\boldmath   $ y$}[d])+(\\nu+1)\\log p_{tr }   \\ge -3m\\nu \\rho \\label{prooftheorem3.5}\\end{aligned}\\ ] ]    since @xmath250 \\ne \\mbox{\\boldmath   $ u$}_c[m+\\nu-1+k\\nu]$ ] for all @xmath266 , according to assumption [ assumption2 ] , @xmath119 and @xmath22 differ at no less than @xmath267 time indices in the time interval @xmath268 . according to ( [ nllu ] ) and ( [ lbound ] )",
    ", we have @xmath269|\\tilde{\\mbox{\\boldmath   $ y$}}[d])p_t(\\tilde{\\mbox{\\boldmath   $ u$}}[d]|\\tilde{\\mbox{\\boldmath   $ u$}}[d-1])}{f_o(\\mbox{\\boldmath   $ r$}[d]|\\mbox{\\boldmath   $ y$}_c[d])p_t(\\mbox{\\boldmath   $ u$}_c[d]|\\mbox{\\boldmath   $ u$}_c[d-1 ] ) } \\nonumber \\\\ & & >   \\left\\lfloor\\frac{k_l+2m-1}{2}\\right\\rfloor 3\\nu(\\rho-\\log p_{tr } )   + ( k_l+2m-1)\\nu\\log p_{tr } \\nonumber \\\\ & & \\ge   3m\\nu(\\rho-\\log p_{tr } ) + 2m\\nu\\log p_{tr } \\ge   3m\\nu\\rho \\label{prooftheorem3.6}\\end{aligned}\\ ] ]    combining ( [ prooftheorem3.5 ] ) and ( [ prooftheorem3.6 ] ) , we obtain @xmath270|\\tilde{\\mbox{\\boldmath   $ y$}}[d])p_t(\\tilde{\\mbox{\\boldmath   $ u$}}[d]|\\tilde{\\mbox{\\boldmath   $ u$}}[d-1])}{f_o(\\mbox{\\boldmath   $ r$}[d]|\\mbox{\\boldmath   $ y$}_c[d])p_t(\\mbox{\\boldmath   $ u$}_c[d]|\\mbox{\\boldmath   $ u$}_c[d-1 ] ) } > 0 \\label{prooftheorem3.7}\\ ] ] ( [ prooftheorem3.7 ] ) implies that @xmath260 covers @xmath148 . hence according to the pcc",
    ", @xmath148 can not be the ml sequence .",
    "@xmath260 is constructed as follows .",
    "@xmath261=\\mbox{\\boldmath   $ u$}[d ] , \\quad \\mbox{for } d\\ge m-2m\\nu \\nonumber \\\\ & & \\mbox{\\boldmath   $ u$}_c[d]=\\tilde{\\mbox{\\boldmath   $ u$}}[d ] , \\quad \\mbox{for } d < m-(2m+1)\\nu .",
    "\\label{prooftheorem3.8}\\end{aligned}\\ ] ] according to ( [ homogeneous ] ) , we can always construct @xmath262 $ ] for @xmath272 so that ( [ prooftheorem3.8 ] ) is satisfied .",
    "let @xmath264 be the processed state sequence corresponding to @xmath260 .    from ( [ nllu ] ) and the second inequality in ( [ lmbound ] ) , we get @xmath273|\\tilde{\\mbox{\\boldmath   $ y$}}[d])p_t(\\tilde{\\mbox{\\boldmath   $ u$}}[d]|\\tilde{\\mbox{\\boldmath   $ u$}}[d-1])}{f_o(\\mbox{\\boldmath   $ r$}[d]|\\mbox{\\boldmath   $ y$}_c[d])p_t(\\mbox{\\boldmath   $ u$}_c[d]|\\mbox{\\boldmath   $ u$}_c[d-1 ] ) } \\nonumber \\\\ & & \\ge -\\sum_{d = m-(2m+1)\\nu}^{m-2m\\nu-1}l_u(\\mbox{\\boldmath   $ r$}[d ] , \\mbox{\\boldmath   $ y$}[d])+\\nu\\log p_{tr }   \\ge -3m\\nu \\rho . \\label{prooftheorem3.9}\\end{aligned}\\ ] ]    since @xmath250 \\ne \\mbox{\\boldmath   $ u$}_c[m+\\nu-1+k\\nu]$ ] for all @xmath274 , according to assumption [ assumption2 ] , @xmath119 and @xmath22 differ at no less than @xmath275 time indices in the time interval @xmath276 . according to ( [ nllu ] ) and ( [ lbound ] ) , we have @xmath277|\\tilde{\\mbox{\\boldmath   $ y$}}[d])p_t(\\tilde{\\mbox{\\boldmath   $ u$}}[d]|\\tilde{\\mbox{\\boldmath   $ u$}}[d-1])}{f_o(\\mbox{\\boldmath   $ r$}[d]|\\mbox{\\boldmath   $ y$}_c[d])p_t(\\mbox{\\boldmath   $ u$}_c[d]|\\mbox{\\boldmath   $ u$}_c[d-1 ] ) } \\nonumber \\\\ & & > \\left\\lfloor\\frac{2m+1+k_r}{2}\\right\\rfloor 3\\nu(\\rho-\\log p_{tr } )    + ( 2m+1+k_r)\\nu\\log p_{tr } \\ge 3(m+1)\\nu\\rho .",
    "\\label{prooftheorem3.10}\\end{aligned}\\ ] ]    combining ( [ prooftheorem3.9 ] ) and ( [ prooftheorem3.10 ] ) , we obtain @xmath278|\\tilde{\\mbox{\\boldmath   $ y$}}[d])p_t(\\tilde{\\mbox{\\boldmath   $ u$}}[d]|\\tilde{\\mbox{\\boldmath   $ u$}}[d-1])}{f_o(\\mbox{\\boldmath   $ r$}[d]|\\mbox{\\boldmath   $ y$}_c[d])p_t(\\mbox{\\boldmath   $ u$}_c[d]|\\mbox{\\boldmath   $ u$}_c[d-1 ] ) } < 0 \\label{prooftheorem3.11}\\ ] ] ( [ prooftheorem3.11 ] ) implies that @xmath260 covers @xmath148 . hence according to the pcc",
    ", @xmath148 can not be the ml sequence .",
    "we construct a markov state sequence @xmath260 as follows .",
    "@xmath261=\\mbox{\\boldmath   $ u$}[d ] , \\quad \\mbox{for } m-2m\\nu \\le d < m+2m\\nu \\nonumber \\\\ & & \\mbox{\\boldmath",
    "$ u$}_c[d]=\\tilde{\\mbox{\\boldmath   $ u$}}[d ] , \\quad \\mbox{for } d\\ge m+(2m+1)\\nu \\nonumber \\\\ & & \\mbox{\\boldmath   $ u$}_c[d]=\\tilde{\\mbox{\\boldmath   $ u$}}[d ] , \\quad \\mbox{for } d < m-(2m+1)\\nu .",
    "\\label{prooftheorem3.12}\\end{aligned}\\ ] ] let the processed state sequence corresponding to @xmath260 be @xmath264 .",
    "since @xmath250 \\ne \\mbox{\\boldmath   $ u$}_c[m+\\nu-1+k\\nu]$ ] for all @xmath279 , according to assumption [ assumption2 ] , @xmath119 and @xmath22 differ at no less than @xmath280 time indices in the time interval @xmath281 . according to ( [ nllu ] ) and ( [ lbound ] ) , we have @xmath282|\\tilde{\\mbox{\\boldmath   $ y$}}[d])p_t(\\tilde{\\mbox{\\boldmath   $ u$}}[d]|\\tilde{\\mbox{\\boldmath   $ u$}}[d-1])}{f_o(\\mbox{\\boldmath   $ r$}[d]|\\mbox{\\boldmath   $ y$}_c[d])p_t(\\mbox{\\boldmath",
    "$ u$}_c[d]|\\mbox{\\boldmath   $ u$}_c[d-1 ] ) } > \\left\\lfloor\\frac{4m+1}{2}\\right\\rfloor 3\\nu(\\rho-\\log p_{tr } ) + 4m\\nu\\log p_{tr } \\ge 6m\\nu\\rho .",
    "\\label{prooftheorem3.13}\\ ] ]    meanwhile , it is easily seen that ( [ prooftheorem3.5 ] ) and ( [ prooftheorem3.9 ] ) hold .",
    "combine ( [ prooftheorem3.5 ] ) , ( [ prooftheorem3.9 ] ) and ( [ prooftheorem3.13 ] ) , we obtain @xmath283|\\tilde{\\mbox{\\boldmath   $ y$}}[d]))p_t(\\tilde{\\mbox{\\boldmath   $ u$}}[d]|\\tilde{\\mbox{\\boldmath   $ u$}}[d-1])}{f_o(\\mbox{\\boldmath   $ r$}[d]|\\mbox{\\boldmath   $ y$}_c[d])p_t(\\mbox{\\boldmath   $ u$}_c[d]|\\mbox{\\boldmath   $ u$}_c[d-1 ] ) }   > -3m\\nu \\rho -3m\\nu \\rho + 6m\\nu\\rho = 0 .",
    "\\label{prooftheorem3.14}\\end{aligned}\\ ] ] ( [ prooftheorem3.14 ] ) implies that @xmath260 covers @xmath148 . hence according to the pcc",
    ", @xmath148 can not be the ml sequence ."
  ],
  "abstract_text": [
    "<S> this paper considers the average complexity of maximum likelihood ( ml ) decoding of convolutional codes . </S>",
    "<S> ml decoding can be modeled as finding the most probable path taken through a markov graph . </S>",
    "<S> integrated with the viterbi algorithm ( va ) , complexity reduction methods such as the sphere decoder often use the sum log likelihood ( sll ) of a markov path as a bound to disprove the optimality of other markov path sets and to consequently avoid exhaustive path search . in this paper , it is shown that sll - based optimality tests are inefficient if one fixes the coding memory and takes the codeword length to infinity . </S>",
    "<S> alternatively , optimality of a source symbol at a given time index can be testified using bounds derived from log likelihoods of the neighboring symbols . </S>",
    "<S> it is demonstrated that such neighboring log likelihood ( nll)-based optimality tests , whose efficiency does not depend on the codeword length , can bring significant complexity reduction to ml decoding of convolutional codes . </S>",
    "<S> the results are generalized to ml sequence detection in a class of discrete - time hidden markov systems .    </S>",
    "<S> coding complexity , convolutional code , hidden markov model , maximum likelihood decoding , viterbi algorithm </S>"
  ]
}