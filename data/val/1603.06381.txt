{
  "article_text": [
    "_ anomalous diffusion _ , where the associated underlying stochastic process is not brownian motion , has recently attracted considerable attention @xcite .",
    "this case is interesting because there may be long - range correlations , among other reasons .",
    "anomalous superdiffusion can be be related to fractional laplacian operators and/or so - called _ nonlocal _ operators @xcite , defined point - wise by their operation on a function @xmath0 as @xmath1   dx ' .",
    "\\label{eq : ell}\\ ] ] the fractional laplacian is actually a special case of this equation .",
    "the present work will focus on these operators and in particular the associated stationary equation , analogous to the local elliptic equation .",
    "it should be noted that these nonlocal operators and the associated equations can be applied not only to problems of anomalous diffusion , but to a wide range of phenomena , including peridynamic models of continuum mechanics which allow crack nucleation and propagation @xcite .    in this work",
    "it will be assumed that the kernel @xmath2 appearing above is parametrized by a ( possibly infinite - dimensional ) parameter @xmath3 ; @xmath4 is assumed random .",
    "furthermore , some partial noisy observations of the solution @xmath5 will be available . in a probabilistic framework",
    ", a prior distribution is placed on @xmath6 and the posterior distribution @xmath7 results from conditioning on the observations .",
    "the joint distribution can often trivially be derived and evaluated in closed form for a given pair @xmath8 , as @xmath9 .",
    "hence , the posterior for a given observed value of @xmath10 can be evaluated , up to a normalizing constant .",
    "one aims to approximate _ quantities of interest _ @xmath11 for some @xmath12 , where @xmath13 for the forward problem and @xmath14 for the inverse problem . the likelihood @xmath15 is often concentrated in a small , possibly nonlinear , subspace of @xmath16 .",
    "this posterior concentration generically precludes the naive application of standard forward approximation algorithms independently to the numerator , @xmath17 , and denominator , @xmath18 .",
    "it is noted that , typically , will have to be approximated numerically , and this will lead to an approximate posterior density .",
    "monte carlo ( mc ) and sequential monte carlo ( smc ) methods are amongst the most widely used computational techniques in statistics , engineering , physics , finance and many other disciplines . in particular , if i.i.d .",
    "samples @xmath19 may be obtained , the mc sampler simply iterates this @xmath20 times and approximates @xmath21 .",
    "smc samplers @xcite are designed to approximate a sequence @xmath22 of probability distributions on a common space , whose densities are only known up - to a normalising constant .",
    "the method uses @xmath23 samples ( or particles ) that are generated in parallel , and are propagated with importance sampling ( often ) via markov chain monte carlo ( mcmc ) and resampling methods .",
    "several convergence results , as @xmath20 grows , have been proved ( see e.g.  @xcite ) . for problems which must first be approximated at finite resolution , as is the case in this article , and must subsequently be sampled from using mc - based methods , a multilevel ( ml ) framework may be used .",
    "this can potentially reduce the cost to obtain a given level of mean square error ( mse ) @xcite , relative to performing i.i.d .",
    "sampling from the approximate posterior at a given ( high ) resolution .",
    "a telescopic sum of successively refined approximation increments are estimated instead of a single highly resolved approximation .",
    "the convergence of the refined approximation increments allows one to balance the cost between levels , and optimally obtain a cost @xmath24 for mse @xmath25 .",
    "this is shown in the context of the models in this article .",
    "specifically , it is shown that the cost of mlmc , to provide a mse of @xmath26 , is less than i.i.d",
    ".  sampling from the most accurate prior approximation .",
    "smc within the ml framework has been primarily developed in @xcite .",
    "these methodologies , some of which consider a similar context to this article , have been introduced where the ml approach is thought to be beneficial , but i.i.d .",
    "sampling is not possible .",
    "indeed , one often has to resort to advanced mcmc or smc methods to implement the ml identity ; see for instance @xcite for mcmc .",
    "smc for nonlocal problems however , is a very sensible framework to implement the ml identity , as it will approximate a sequence of related probabilities of increasing complexity ; in some cases , exact simulation from couples of the probabilities is not possible . as a result , smc is the computational framework which we will primarily pursue in this article .",
    "it is shown that the cost of mlsmc , to provide a mse of @xmath26 , is less than i.i.d .",
    "sampling from the most accurate posterior approximation .",
    "it is noted , however , that some developments both with regards to theoretical analysis and implentation of the algorithm , must be carefully considered in order to successfully use mlsmc for nonlocal models .",
    "this article is structured as follows . in section [ sec : theo ] the nonlocal elliptic equations are given , along with the bayesian inverse problem , and well - posedness of both are established . in particular , it is shown that the posterior has a well - defined radon - nikodym derivative w.r.t .   prior measure . in section [ sec : expec ] we consider how one can approximate expectations w.r.t .",
    "the prior and the posterior , specifically using mlmc and mlsmc methods .",
    "our complexity results are given also .",
    "section [ sec : num ] provides some numerical implementations of mlmc and mlsmc for the prior , and posterior , respectively .",
    "consider the following equation @xmath27 where @xmath28 is given by , the domain @xmath29 is simply connected , and its `` boundary '' @xmath30 is sufficiently regular and _ nonlocal _ , in the sense that it has non - zero volume in @xmath31 ,    under appropriate conditions on @xmath2 , @xmath32 , for @xmath33 , with @xmath34 only for the limiting local version in which @xmath35 ( or a similar uniformly elliptic form ) @xcite .",
    "following @xcite @xmath36 denotes the volume constrained space of functions , and the fractional sobolev space @xmath37 is defined as follows , for @xmath38 , @xmath39 where @xmath40 we define @xmath41 for ease of notation .    for @xmath42 , the dual with respect to @xmath43 of some space @xmath44",
    ", the weak formulation of can be defined as follows .",
    "integrate the equation against an arbitrary test function @xmath45 .",
    "now , find @xmath46 ( satisfying the boundary conditions ) such that @xmath47 where @xmath48 b(x , x ' ) dx ' v(x ) dx , \\\\",
    "f(v ) & : =   \\int_{{\\mathbb{r}}^d } b(x ) v(x ) dx . \\end{split}\\ ] ]      let @xmath49 denote the maximum diameter of an element of @xmath50 .",
    "the finite approximation of is stated as follows .",
    "identify some @xmath51 such that @xmath52 assuming the spaces @xmath53 are spanned by elements @xmath54 , then one can substitute the ansatz @xmath55 into the above equation , resulting in the finite linear system @xmath56    in particular , the spaces @xmath57 will be comprised of discontinuous elements , so that the method described is a discontinuous galerkin finite element method ( fem ) @xcite .",
    "piecewise polynomial discontinuous element spaces are dense in @xmath58 for @xmath59 , and are therefore _ conforming _ when @xmath60 for @xmath59 , so there is no need to impose penalty terms at the boundaries as one must do for smoother problems in which the discontinuous elements are non - conforming .",
    "the following assumption will be made for simplicity    @xmath61 , with norm @xmath62 and inner product @xmath63 .",
    "@xmath64 is continuous with @xmath65 , and there exist @xmath66 such that for all @xmath67 @xmath68 [ as : kernel ]    as shown in section 6 of @xcite , this implies that for all @xmath69    * @xmath70 is continuous : @xmath71 ; * @xmath72 is continuous : @xmath73 ; * @xmath72 is coercive : @xmath74 for some @xmath75 .",
    "this actually follows from the poincar - type inequality derived in proposition 2.5 of @xcite .    hence , lax - milgram lemma ( @xcite , thm .",
    "1.1.3 ) can be invoked , guaranteeing existence of a unique solution @xmath76 such that @xmath77 in other words , the map @xmath78 is continuous .",
    "the system inherits solvability since the bilinear is a fortiori coercive on @xmath53 , so that @xmath79    let @xmath80 be a parametrization of @xmath2 , where @xmath81 and @xmath3 , and either @xmath82 or @xmath83 compact",
    ". then the following theorem holds .",
    "if assumption [ as : kernel ] holds almost surely for @xmath84 and the map @xmath85 is continuous from @xmath43 to @xmath86 , then the map @xmath87 is almost surely continuous .",
    "hence @xmath88 for all @xmath89 , i.e. all moments exist .",
    "@xmath90 here denotes the space of random variables @xmath91 such that @xmath92 .",
    "[ thm : fwd ]    since assumption [ as : kernel ] holds almost surely for @xmath84 , then @xmath93 uniformly .",
    "so , the quantity of interest @xmath94 for all @xmath89 . therefore , for any @xmath95 such that @xmath96 , the result follows , since then @xmath97 for all @xmath4 .",
    "if assumption [ as : kernel ] holds almost surely for @xmath84 and the map @xmath98 is continuous from @xmath43 to @xmath86 , then the map through the _ discrete _ system @xmath99 is almost surely continuous and @xmath100 .",
    "[ thm : finfwd ]      for the inverse problem , let us assume that some data is given in the form @xmath101 where @xmath102^\\top , \\quad g_i\\in v.   \\label{eq : g}\\ ] ] then the following theorem holds .",
    "the posterior distribution of @xmath103 is well - defined and takes the form @xmath104 with @xmath105 .",
    "[ thm : inv ]    the form of the posterior is obtained by a change of variables to @xmath106 } , which are independent by assumption . note the change of variables has jacobian 1 .",
    "so , changing variables back , this also gives the joint density .",
    "the posterior is obtained by normalizing for the observed value of @xmath10 .",
    "the form of the observation operator guarantees @xmath107 , where @xmath108 and @xmath109 is defined as in the proof of theorem [ thm : fwd ] , and @xmath110 is the matrix norm .    now define @xmath111 , \\quad g_i\\in l^2(\\omega).$ ]    the finite approximation of the posterior distribution of @xmath103 is well - defined and takes the form @xmath112 with @xmath113 .",
    "[ cor : fininv ]    the forcing may also be taken as uncertain , although the uniformity will require it to be defined on a compact space .",
    "the probability space @xmath16 will be taken as compact for simplicity , as it is then easy to verify assumption [ as : kernel ] .",
    "the objective here is to approximate expectations of some functional @xmath114 with respect to a probability measure @xmath115 ( @xmath116 or @xmath117 ) , denoted @xmath118 the solution @xmath5 of above must be approximated by some @xmath119 , with a degree of accuracy which depends on @xmath120 .",
    "indeed there exists a hierarchy of levels @xmath121 ( where @xmath122 may be arbitrarily large ) of increasing accuracy and increasing cost . for the inverse problem",
    "this manifests in a hierarchy of target probability measures @xmath123 via the approximation of . for the forward problem @xmath124 for all @xmath120 , but it will be assumed that the function @xmath125 requires evaluation of the solution of , as in theorem [ thm : fwd ] , and the corresponding approximations will be denoted by @xmath126 .",
    "one may then compute the estimator @xmath127 , where for the forward problem it may be that @xmath128 for all @xmath122 . the mean square error ( mse )",
    "is given by @xmath129 - \\hat{y}_l^n \\right |^2   =   \\underbrace{{\\mathbb{e}}\\left\\ { { \\mathbb{e}}_{\\eta_l}[\\varphi_l(\\lambda ) ] - \\hat{y}_l^n \\right\\}^2}_{\\rm variance }    + \\underbrace{\\{{\\mathbb{e}}_{\\eta_l } [ \\varphi_l(\\lambda ) ] - { \\mathbb{e}}_{\\eta } [ \\varphi(\\lambda)]\\}^2}_{\\rm bias}\\   .",
    "\\label{eq : mseml}\\ ] ] now assume there is some discretization level , say of diameter @xmath130 , which gives rise to an error estimate on the output of size @xmath131 , for example arising from the deterministic numerical approximation of a spatio - temporal problem .",
    "this also translates to a number of degrees of freedom proportional to @xmath132 , where @xmath133 is the spatio - temporal dimension .",
    "now the complexity of typical forward solves will range from a dot product @xmath134 ( linear ) to a full gaussian elimination @xmath135 ( cubic ) . in this problem one",
    "aims to find @xmath136 , so one would find the cost controlled by @xmath137 , for @xmath138 .",
    "if one can obtain independent , identically distributed samples @xmath139 , then the necessary number of samples to obtain a variance of size @xmath25 is given by @xmath140 . the total cost to obtain a mean - square error tolerance of @xmath25 is therefore @xmath141 .",
    "for the forward uq problem , in which one can sample _ directly _ from @xmath142 , one very popular methodology for improving the efficiency of solution to such problems is the multilevel monte carlo ( mlmc ) method @xcite .",
    "indeed there has been an explosion of recent activity @xcite since its introduction in @xcite . in this methodology",
    "the simple estimator @xmath143 above for a given desired @xmath122 is replaced by a telescopic sum of unbiased increment estimators @xmath144 where @xmath145 are i.i.d .",
    "samples , with marginal laws @xmath146 , @xmath142 , respectively , carefully constructed on a joint probability space .",
    "this is repeated independently for @xmath147 .",
    "the overall multilevel estimator will be @xmath148 under the convention that @xmath149 .",
    "a simple error analysis shows that the mean squared error ( mse ) in this case is given by @xmath150 \\}^2 & =    \\underbrace { \\sum_{\\ell=0}^l { \\mathbb{e}}\\left \\ { y^{n_\\ell}_{\\ell } - [ { \\mathbb{e}}_{\\eta_{\\ell}}{\\varphi_\\ell}(\\lambda ) - { \\mathbb{e}}_{\\eta_{\\ell-1}}{\\varphi_{\\ell-1}}(\\lambda ) ] \\right\\}^2}_{\\rm variance }   \\\\   & + \\underbrace{\\{{\\mathbb{e}}_{\\eta_l } [ { \\varphi_l}(\\lambda ) ] - { \\mathbb{e}}_{\\eta } [ \\varphi(\\lambda)]\\}^2}_{\\rm bias}\\   .",
    "\\label{eq : msemls}\\end{aligned}\\ ] ] notice that the bias is given by the _",
    "finest _ level , whilst the variance is decomposed into a sum of variances of the _ increments_. the variance of the @xmath151 increment estimator has the form @xmath152 , where the terms @xmath153 decay , following from refinement of the approximation of @xmath115 and/or @xmath125 .",
    "one can therefore balance the variance at a given level @xmath154 with the number of samples @xmath155 . as the level increases , the corresponding cost increases , but the variance _ decreases _ , allowing fewer samples to achieve a given variance .",
    "this can be optimized , and results in a total cost of @xmath156 , in the optimal case .    to be explicit , denote by @xmath157 the level @xmath120 approximation of the quantity of interest @xmath158 .",
    "introduce the following assumptions    [ hyp : d ] there exist @xmath159 , and a @xmath160 such that @xmath161 where @xmath162 denotes the cost to evaluate @xmath163 .",
    "we have the following classical mlmc theorem @xcite    [ theo : mlmc ] assume ( a[hyp : d ] ) and @xmath164 .",
    "then for any @xmath165 , there exist @xmath166 and @xmath160 such that @xmath167\\big)^2\\big ] \\leq c \\varepsilon^2 , \\label{eq : mlmse}\\ ] ] for the following cost @xmath168      now the inverse problem will be considered .",
    "there is a sequence of probability measures @xmath169 on a common measurable space @xmath170 , and for each @xmath171 there is a measure @xmath172 , such that @xmath173 where the normalizing constant @xmath174 is unknown .",
    "the objective is to compute : @xmath175 : = \\int_e \\varphi(\\lambda)\\eta_\\infty(d\\lambda)\\ ] ] for potentially many measurable @xmath176integrable functions @xmath177 .",
    "let @xmath170 be a measurable space .",
    "the notation @xmath178 denotes the class of bounded and measurable real - valued functions , and the supremum norm is written as @xmath179 .",
    "consider non - negative operators @xmath180 such that for each @xmath3 the mapping @xmath181 is a finite non - negative measure on @xmath182 and for each @xmath183 the function @xmath184 is measurable ; the kernel @xmath185 is markovian if @xmath186 is a probability measure for every @xmath3 .",
    "for a finite measure @xmath116 on @xmath170 , and a real - valued , measurable @xmath187 , we define the operations : @xmath188 we also write @xmath189 .",
    "in addition @xmath190 , @xmath191 , denotes the @xmath192norm , where the expectation is w.r.t .",
    "the law of the appropriate simulated algorithm .      as described in section [ sec",
    ": intro ] , the context of interest is when a sequence of densities @xmath193 , as in ( [ eq : target ] ) , are associated to an ` accuracy ' parameter @xmath194 , with @xmath195 as @xmath196 , such that @xmath197 . in practice",
    "one can not treat @xmath198 and so must consider these distributions with @xmath199 .",
    "the laws with large @xmath49 are easy to sample from with low computational cost , but are very different from @xmath200 , whereas , those distributions with small @xmath49 are hard to sample with relatively high computational cost , but are closer to @xmath200 .",
    "thus , we choose a maximum level @xmath201 and we will estimate @xmath202 : = \\int_e \\varphi(\\lambda)\\eta_l(\\lambda)d\\lambda\\ .\\ ] ] by the standard telescoping identity used in mlmc , one has @xmath203 & =   \\mathbb{e}_{\\eta_0}[\\varphi(\\lambda ) ] + \\sum_{\\ell=1}^{l}\\big\\ { \\mathbb{e}_{\\eta_\\ell}[\\varphi(\\lambda ) ] - \\mathbb{e}_{\\eta_{\\ell-1}}[\\varphi(\\lambda)]\\big\\ } \\nonumber \\nonumber \\\\   & = \\mathbb{e}_{\\eta_0}[\\varphi(\\lambda ) ] + \\sum_{\\ell=1}^{l}\\mathbb{e}_{\\eta_{\\ell-1}}\\big [ \\big(\\frac{\\gamma_\\ell(\\lambda)z_{\\ell-1}}{\\gamma_{\\ell-1}(\\lambda)z_\\ell } - 1\\big)\\varphi(\\lambda)\\big]\\ .",
    "\\label{eq : ml_approx}\\end{aligned}\\ ] ]    suppose now that one applies an smc sampler @xcite to obtain a collection of samples ( particles ) that sequentially approximate @xmath204 .",
    "we consider the case when one initializes the population of particles by sampling i.i.d",
    ".  from @xmath205 , then at every step resamples and applies a mcmc kernel to mutate the particles .",
    "we denote by @xmath206 , with @xmath207 , the samples after mutation ; one resamples @xmath208 according to the weights @xmath209 , for indices @xmath210",
    ". we will denote by @xmath211 the sequence of mcmc kernels used at stages @xmath212 , such that @xmath213 .",
    "for @xmath177 , @xmath214 , we have the following estimator of @xmath215 $ ] : @xmath216 we define @xmath217 the joint probability distribution for the smc algorithm is @xmath218 if one considers one more step in the above procedure , that would deliver samples @xmath219 , a standard smc sampler estimate of the quantity of interest in ( [ eq : ml_approx ] ) is @xmath220 ; the earlier samples are discarded . within a multilevel context , a consistent smc estimate of is @xmath221 and this will be proven to be superior than the standard one , under assumptions .",
    "the relevant mse error decomposition here is : @xmath222 \\}^2\\big ] \\le 2\\,\\mathbb{e}\\big[\\{\\widehat{y}-\\mathbb{e}_{\\eta_l}[\\varphi(\\lambda)]\\}^2\\big ] +   2\\,\\ { \\mathbb{e}_{\\eta_l}[\\varphi(\\lambda ) ] - \\mathbb{e}_{\\eta_\\infty}[\\varphi(\\lambda ) ]   \\}^2\\ .\\ ] ]      we will now restate an analytical result from @xcite that controls the error term @xmath223\\}^2]$ ] in expression . for any @xmath224 and @xmath225",
    "we write : @xmath226 the following standard assumptions will be made ; see @xcite .",
    "[ hyp : a ] there exist @xmath227 such that @xmath228    [ hyp : b ] there exists a @xmath229 such that for any @xmath230 , @xmath231 , @xmath232 : @xmath233    under these assumptions the following theorem is proven in @xcite    [ theo : main_error ] assume ( a[hyp : a]-[hyp : b ] ) .",
    "there exist @xmath234 such that for any @xmath235 , with @xmath236 , @xmath237\\}^2\\big ]   \\leq   c\\,\\bigg(\\frac{1}{n_0 } + & \\sum_{\\ell=1}^{l}\\bigg ( \\tfrac{{\\mathcal{v}}_l}{n_{l-1 } } +   \\tfrac{{\\mathcal{v}}_\\ell^{1/2}}{n_{\\ell-1}^{1/2 } } \\sum_{q=\\ell+1}^{l}\\tfrac{{\\mathcal{v}}_q^{1/2}}{n_{q-1 } } \\bigg)\\bigg)\\ , \\end{aligned}\\ ] ] where @xmath238 .",
    "the following additional assumption will now be made    [ hyp : c ] there exist @xmath159 , and a @xmath160 such that @xmath239 where @xmath240 denotes the cost to evaluate @xmath241 .",
    "the following theorem may now be proven    [ theo : mlsmc ] assume ( a[hyp : a]-[hyp : c ] ) and @xmath164 .",
    "then for any @xmath165 , there exist @xmath166 and @xmath160 such that @xmath242\\big)^2\\big ] \\leq c \\varepsilon^2 , \\label{eq : mlsmcmse}\\ ] ] for the following cost @xmath243    the mse can be bounded using .",
    "following from ( a[hyp : c])(ii ) , the second term requires that @xmath244 , and assuming @xmath245 for some @xmath246 , this translates to @xmath247 . as in @xcite , the additional error term is dealt with by first ignoring it and optimizing @xmath248 , for a given @xmath249 , where @xmath250 .",
    "this requires that @xmath251 .",
    "the constraint then requires that @xmath252 , where @xmath253 , so one has @xmath254 it was shown in @xcite that the result follows , provided @xmath255 .",
    "in fact , re - examining the additional term as in section 3.3 of @xcite , one has @xmath256 where the second line follows from the inequality @xmath257 and the definition of @xmath258 . now substituting @xmath259 for the 3 cases and recalling the assumption @xmath164 gives @xmath249 for the costs in .",
    "assume a uniform prior @xmath116 .",
    "following from , the unnormalized measures will be given by @xmath260 where @xmath261 , and @xmath262^\\top ,   \\quad g_i\\in l^2(\\omega),\\ ] ] and @xmath263 is the solution of the numerical approximation of .",
    "notice that these are uniformly bounded , over both @xmath3 and over @xmath120 , following from corollary [ cor : fininv ] for finite @xmath120 , and theorem [ thm : inv ] in the limit .",
    "it is shown in @xcite section 4 that @xmath264 one proceeds similarly to that paper , and finds that @xmath265    note that @xmath266 , so inserting the bound into , and observing the boundedness of the @xmath267 noted above , one can see that assumption ( a[hyp : a ] ) holds . now inserting and into",
    ", one can see that in order to establish assumption ( a[hyp : c ] ) , it suffices to establish rates of convergence for @xmath268 .",
    "in particular , assumption ( a[hyp : a ] ) and the c2 inequality ( minkowski plus young s ) provide that @xmath269    therefore , the rate of convergence of @xmath268 is the required quantity for both the forward ( for lipschitz @xmath158 ) and the inverse multilevel estimation problems . by the triangle inequality it suffices to consider the approximation of the truth @xmath270 , and by ca s lemma ( @xcite , thm .",
    "2.4.1 ) , assumption [ as : kernel ] guarantees the existence of a @xmath160 such that @xmath271    theorem 6.2 of @xcite provides rates of convergence of the best approximation above , in the case that @xmath272 and the fem triangulation is shape regular and quasi - uniform .",
    "in particular , their case 1 corresponds to the case of a singular kernel , i.e. not even integrable , and @xmath273 , for @xmath274 , so the solution operator is smoothing .",
    "their case 2 corresponds to a slightly more regular kernel than ours , where in fact @xmath275 , rather than @xmath276 as given by assumption [ as : kernel ] and consequently @xmath277 .",
    "it is shown that in case 2 , if the solution @xmath278 , for polynomial elements of order @xmath279 and some @xmath280 $ ] , then convergence with respect to the @xmath43 norm is in fact @xmath281 .",
    "so , for linear elements , second order convergence can be obtained , leading to @xmath282 in .",
    "recall that [ as : kernel ] ensures well - posedness of the solution from @xmath283 , and so discontinuities are allowed .",
    "this is actually a strength of nonlocal models and one impetus for their use .",
    "it is reasonable to expect that if the the nodes of the discontinuous elements match up with the discontinuities , i.e. there is a node at each point of discontinuity for @xmath284 or there are nodes all along a curve / surface of discontinuity for @xmath285 , and if the solution is sufficiently smooth in the subdomains , then the rates of convergence should match those of the smooth subproblems .",
    "for example , if there is a point discontinuity such that the domain can be separated at that point into @xmath286 and one has @xmath287 and @xmath288 , where @xmath289 is the restriction of @xmath5 to the set @xmath290 , then one expects the convergence rate of @xmath291 to be preserved .",
    "this is postulated and verified numerically in @xcite .",
    "it is also illustrated numerically in that work that even with discontinuous elements , if there is no node at the discontinuity then the convergence rate reverts to @xmath292 , so @xmath293 in .",
    "the particular nonlocal model which will be of interest here is that in which the kernel is given by @xmath294 where @xmath295 , and @xmath296 is the characteristic function which takes the value 1 if @xmath297 and zero otherwise , and @xmath298 .",
    "notice that @xmath299 is scalar , but @xmath300 may be defined on either a finite - dimensional subspace of function - space , or in principle in the full infinite dimensional space .",
    "this may be of interest to incorporate spatial dependence of material properties .",
    "following @xcite we consider the following model .",
    "let @xmath301 $ ] and @xmath302 $ ] .",
    "for @xmath303 , let @xmath304 .",
    "the kernel is defined by , @xmath305 the following prior is used for the parameters , @xmath306 this is an extension of an example used in @xcite , which is identical to this model with @xmath307 and @xmath308 ( various values of @xmath309 are used there ) . to solve the equation with fem , a uniform partition on @xmath29 is used with @xmath310 , @xmath311 .",
    "thus for each level @xmath120 , @xmath312 . the same discontinuous galerkin fem as in @xcite is applied to solve the system .",
    "similar methods to those in @xcite are used to estimate the convergence rates for both the forward and inverse problems .",
    "the estimated rates are @xmath313 and @xmath314 .",
    "the rates @xmath315 and @xmath316 are used for the simulations .    for the inverse problem ,",
    "data is generated with @xmath317 , @xmath318 and @xmath319 .",
    "the observations are @xmath320 , where @xmath321 and @xmath322^\\top$ ] .",
    "the quantity of interest for both the forward and inverse problem is @xmath323 .",
    "the forward problem is solved in the standard multilevel fashion by generating coupled particles from the prior at each level .",
    "the main result , the cost vs. mse of the estimates is shown in figure  [ fig : mlmc ] . the bayesian inverse problem is also solved for this model , and the main result is shown in figure  [ fig : mlsmc ] .",
    "this is the first systematic treatment of uq for nonlocal models , to the knowledge of the authors .",
    "natural extensions include obtaining rigorous convergence results for piecewise smooth solutions , exploring higher - dimensional examples , spatial parameters , time - dependent models , and parameters defined on non - compact spaces .",
    "* acknowledgements .",
    "* kjhl was supported by the darpa formulate project .",
    "aj & yz were supported by ministry of education acrf tier 2 grant , r-155 - 000 - 161 - 112 .",
    "we express our gratitude to marta delia , pablo seleson , and max gunzburger for useful discussions .      , f. , mazon , j. m. , jose , m. rossi , j. & toledo , j.  ( 2009 ) . a nonlocal p - laplacian evolution equation with nonhomogeneous dirichlet boundary conditions .",
    "_ siam journal on mathematical analysis _ , * 40 * , 18151851 ."
  ],
  "abstract_text": [
    "<S> this paper considers uncertainty quantification for an elliptic nonlocal equation . in particular </S>",
    "<S> , it is assumed that the parameters which define the kernel in the nonlocal operator are uncertain and a priori distributed according to a probability measure . </S>",
    "<S> it is shown that the induced probability measure on some quantities of interest arising from functionals of the solution to the equation with random inputs is well - defined ; as is the posterior distribution on parameters given observations . as the elliptic nonlocal equation can not be solved </S>",
    "<S> approximate posteriors are constructed . </S>",
    "<S> the multilevel monte carlo ( mlmc ) and multilevel sequential monte carlo ( mlsmc ) sampling algorithms are used for a priori and a posteriori estimation , respectively , of quantities of interest . </S>",
    "<S> these algorithms reduce the amount of work to estimate posterior expectations , for a given level of error , relative to monte carlo and i.i.d .  sampling from the posterior at a given level of approximation of the solution of the elliptic nonlocal equation . </S>",
    "<S> +    * key words * : uncertainty quantification , multilevel monte carlo , sequential monte carlo , nonlocal equations , bayesian inverse problem + * ams subject classification * : 82c80 , 60k35 . </S>"
  ]
}