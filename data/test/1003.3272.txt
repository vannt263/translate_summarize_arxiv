{
  "article_text": [
    "statisticians , like all scientists , are acutely aware that the clock speeds on their desktops and laptops have stalled .",
    "does this mean that statistical computing has hit a wall ?",
    "the answer fortunately is no , but the hardware advances that we routinely expect have taken an interesting detour .",
    "most computers now sold have two to eight processing cores .",
    "think of these as separate cpus on the same chip .",
    "naive programmers rely on sequential algorithms and often fail to take advantage of more than a single core .",
    "sophisticated programmers , the kind who work for commercial firms such as matlab , eagerly exploit parallel programming .",
    "however , multicore cpus do not represent the only road to the success of statistical computing .",
    "graphics processing units ( gpus ) have caught the scientific community by surprise .",
    "these devices are designed for graphics rendering in computer animation and games . propelled by these nonscientific markets ,",
    "the old technology of numerical ( array ) coprocessors has advanced rapidly .",
    "highly parallel gpus are now making computational inroads against traditional cpus in image processing , protein folding , stock options pricing , robotics , oil exploration , data mining , and many other areas @xcite .",
    "we are starting to see orders of magnitude improvement on some hard computational problems .",
    "three companies , intel , nvidia , and amd / ati , dominate the market .",
    "intel is struggling to keep up with its more nimble competitors .",
    "modern gpus support more vector and matrix operations , stream data faster , and possess more local memory per core than their predecessors .",
    "they are also readily available as commodity items that can be inserted as video cards on modern pcs .",
    "gpus have been criticized for their hostile programming environment and lack of double precision arithmetic and error correction , but these faults are being rectified .",
    "the cuda programming environment @xcite for nvidia chips is now easing some of the programming chores .",
    "we could say more about near - term improvements , but most pronouncements would be obsolete within months .",
    "oddly , statisticians have been slow to embrace the new technology . @xcite",
    "first demonstrated the potential for gpus in fitting simple bayesian networks .",
    "recently @xcite have seen greater than @xmath0-fold speed - ups in mcmc simulations in molecular phylogeny . @xcite and @xcite are following suit with bayesian model fitting via particle filtering and slice sampling . finally , work is under - way to port common data mining techniques such as hierarchical clustering and multi - factor dimensionality reduction onto gpus @xcite .",
    "these efforts constitute the first wave of an eventual flood of statistical and data mining applications .",
    "the porting of gpu tools into the r environment will undoubtedly accelerate the trend @xcite .",
    "not all problems in computational statistics can benefit from gpus .",
    "sequential algorithms are resistant unless they can be broken into parallel pieces .",
    "even parallel algorithms can be problematic if the entire range of data must be accessed by each gpu . because they have limited memory , gpus are designed to operate on short streams of data .",
    "the greatest speedups occur when all of the gpus on a card perform the same arithmetic operation simultaneously .",
    "effective applications of gpus in optimization involves both separation of data and separation of parameters .    in the current paper , we illustrate how gpus can work hand in glove with the mm algorithm , a generalization of the em algorithm . in many optimization problems ,",
    "the mm algorithm explicitly separates parameters by replacing the objective function by a sum of surrogate functions , each of which involves a single parameter .",
    "optimization of the one - dimensional surrogates can be accomplished by assigning each subproblem to a different core .",
    "provided the different cores each access just a slice of the data , the parallel subproblems execute quickly . by construction the new point in parameter space improves the value of the objective function . in other words , mm algorithms",
    "are iterative ascent or descent algorithms .",
    "if they are well designed , then they separate parameters in high - dimensional problems .",
    "this is where gpus enter .",
    "they offer most of the benefits of distributed computer clusters at a fraction of the cost . for this reason alone ,",
    "computational statisticians need to pay attention to gpus .",
    "@xmath1{rosenbrocksurface.eps } & \\includegraphics[width=2.3in]{rosenbrockmm - q0.eps } \\end{array}\\ ] ]    before formally defining the mm algorithm , it may help the reader to walk through a simple numerical example stripped of statistical content .",
    "consider the rosenbrock test function @xmath2 familiar from the minimization literature . as we iterate toward the minimum at @xmath3",
    ", we construct a surrogate function that separates parameters .",
    "this is done by exploiting the obvious majorization @xmath4 where equality holds when @xmath5 and the current iterate @xmath6 coincide .",
    "it follows that @xmath7 itself is majorized by the sum of the two surrogates @xmath8 x_1 ^ 2 - 2x_1 + 1 \\\\",
    "g_2(x_2 \\mid { \\bf x}_n ) & = & 200 x_2 ^ 2 - 200 ( x_{n1}^2+x_{n2 } ) x_2 + ( x_{n1}^2 + x_{n2})^2.\\end{aligned}\\ ] ] the left panel of figure  [ fig : rosenbrock ] depicts the rosenbrock function and its majorization @xmath9 at the point @xmath10 .",
    "according to the mm recipe , at each iteration one must minimize the quartic polynomial @xmath11 and the quadratic polynomial @xmath12 .",
    "the quartic possesses either a single global minimum or two local minima separated by a local maximum these minima are the roots of the cubic function @xmath13 and can be explicitly computed .",
    "we update @xmath14 by the root corresponding to the global minimum and @xmath15 via @xmath16 .",
    "the right panel of figure  [ fig : rosenbrock ] displays the iterates starting from @xmath17 .",
    "these immediately jump into the rosenbrock valley and then slowly descend to @xmath18 .",
    "separation of parameters in this example makes it easy to decrease the objective function .",
    "this almost trivial advantage is amplified when we optimize functions depending on tens of thousands to millions of parameters . in these settings , newton s method and variants such as fisher s scoring",
    "are fatally handicapped by the need to store , compute , and invert huge hessian or information matrices . on the negative side of the balance sheet , mm",
    "algorithms are often slow to converge .",
    "this disadvantage is usually outweighed by the speed of their updates even in sequential mode .",
    "if one can harness the power of parallel processing gpus , then mm algorithms become the method of choice for many high - dimensional problems .",
    "we conclude this introduction by sketching a roadmap to the rest of the paper .",
    "section  [ sec : mm ] reviews the mm algorithm .",
    "section  [ sec : examples ] discusses three high - dimensional mm examples .",
    "although the algorithm in each case is known , we present brief derivations to illustrate how simple inequalities drive separation of parameters .",
    "we then implement each algorithm on a realistic problem and compare running times in sequential and parallel modes .",
    "we purposefully omit programming syntax since many tutorials already exist for this purpose , and material of this sort is bound to be ephemeral .",
    "section [ discussion_section ] concludes with a brief discussion of other statistical applications of gpus and other methods of accelerating optimization algorithms .",
    "the mm algorithm like the em algorithm is a principle for creating optimization algorithms . in minimization the acronym mm stands for majorization - minimization ; in maximization it stands for minorization - maximization .",
    "both versions are convenient in statistics . for the moment we will concentrate on maximization .",
    "let @xmath19 be the objective function whose maximum we seek .",
    "its argument @xmath20 can be high - dimensional and vary over a constrained subset @xmath21 of euclidean space .",
    "an mm algorithm involves minorizing @xmath19 by a surrogate function @xmath22 anchored at the current iterate @xmath23 of the search .",
    "the subscript @xmath24 indicates iteration number throughout this article .",
    "if @xmath25 denotes the maximum of @xmath22 with respect to its left argument , then the mm principle declares that @xmath25 increases @xmath19 as well .",
    "thus , mm algorithms revolve around a basic ascent property .",
    "minorization is defined by the two properties @xmath26 in other words , the surface @xmath27 lies below the surface @xmath28 and is tangent to it at the point @xmath29 .",
    "construction of the minorizing function @xmath22 constitutes the first m of the mm algorithm . in our examples @xmath22",
    "is chosen to separate parameters .    in the second m of the mm algorithm ,",
    "one maximizes the surrogate @xmath22 rather than @xmath19 directly .",
    "it is straightforward to show that the maximum point @xmath25 satisfies the ascent property @xmath30 .",
    "the proof @xmath31 reflects definitions ( [ minorization_definition1 ] ) and ( [ minorization_definition2 ] ) and the choice of @xmath25 . the ascent property is the source of the mm algorithm s numerical stability and remains valid if we merely increase @xmath22 rather than maximize it . in many problems mm updates are delightfully simple to code , intuitively compelling , and automatically consistent with parameter constraints . in minimization",
    "we seek a majorizing function @xmath22 lying above the surface @xmath28 and tangent to it at the point @xmath29 . minimizing @xmath22 drives",
    "@xmath19 downhill .",
    "the celebrated expectation - maximization ( em ) algorithm @xcite is a special case of the mm algorithm .",
    "the @xmath32-function produced in the e step of the em algorithm constitutes a minorizing function of the loglikelihood .",
    "thus , both em and mm share the same advantages : simplicity , stability , graceful adaptation to constraints , and the tendency to avoid large matrix inversion . the more general mm perspective",
    "frees algorithm derivation from the missing data straitjacket and invites wider applications .",
    "for example , our multi - dimensional scaling ( mds ) and non - negative matrix factorization ( nnfm ) examples involve no likelihood functions .",
    "@xcite briefly summarize the history of the mm algorithm and its relationship to the em algorithm .",
    "the convergence properties of mm algorithms are well - known @xcite .",
    "in particular , five properties of the objective function @xmath19 and the mm algorithm map @xmath33 guarantee convergence to a stationary point of @xmath19 : ( a ) @xmath19 is coercive on its open domain ; ( b ) @xmath19 has only isolated stationary points ; ( c ) @xmath34 is continuous ; ( d ) @xmath35 is a fixed point of @xmath34 if and only if @xmath35 is a stationary point of @xmath19 ; and ( e ) @xmath36 \\ge f({\\boldsymbol{\\theta}}^*)$ ] , with equality if and only if @xmath35 is a fixed point of @xmath34 .",
    "these conditions are easy to verify in many applications .",
    "the local rate of convergence of an mm algorithm is intimately tied to how well the surrogate function @xmath37 approximates the objective function @xmath19 near the optimal point @xmath35 .",
    "in this section , we compare the performances of the cpu and gpu implementations of three classical mm algorithms coded in c++ : ( a ) non - negative matrix factorization ( nnmf ) , ( b ) positron emission tomography ( pet ) , and ( c ) multidimensional scaling ( mds ) . in each case",
    "we briefly derive the algorithm from the mm perspective . for the cpu version ,",
    "we iterate until the relative change @xmath38 of the objective function @xmath19 between successive iterations falls below a pre - set threshold @xmath39 or the number of iterations reaches a pre - set number @xmath40 , whichever comes first . in these examples ,",
    "we take @xmath41 and @xmath42 . for ease of comparison",
    ", we iterate the gpu version for the same number of steps as the cpu version .",
    "overall , we see anywhere from a 22-fold to 112-fold decrease in total run time .",
    "the source code is freely available from the first author .",
    "table  [ table : system - config ] shows how our desktop system is configured .",
    "although the cpu is a high - end processor with four cores , we use just one of these for ease of comparison . in practice , it takes considerable effort to load balance the various algorithms across multiple cpu cores . with 240 gpu cores , the gtx 280 gpu card delivers a peak performance of about 933 gflops in single precision .",
    "this card is already obsolete .",
    "newer cards possess twice as many cores , and up to four cards can fit inside a single desktop computer .",
    "it is relatively straightforward to program multiple gpus .",
    "because previous generation gpu hardware is largely limited to single precision , this is a worry in scientific computing . to assess the extent of roundoff error , we display the converged values of the objective functions to ten significant digits .",
    "only rarely is the gpu value far off the cpu mark .",
    "finally , the extra effort in programming the gpu version is relatively light . exploiting the standard cuda library @xcite",
    ", it takes 77 , 176 , and 163 extra lines of gpu code to implement the nnmf , pet , and mds examples , respectively .",
    ".configuration of the desktop system [ cols=\"^,^,^\",options=\"header \" , ]      coordinates on the 2005 house of representatives roll call data.,width=432 ]",
    "the rapid and sustained increases in computing power over the last half century have transformed statistics . every advance has encouraged statisticians to attack harder and more sophisticated problems .",
    "we tend to take the steady march of computational efficiency for granted , but there are limits to a chip s clock speed , power consumption , and logical complexity .",
    "parallel processing via gpus is the technological innovation that will power ambitious statistical computing in the coming decade .",
    "once the limits of parallel processing are reached , we may see quantum computers take off . in the meantime",
    "statisticians should learn how to harness gpus productively .",
    "we have argued by example that high - dimensional optimization is driven by parameter and data separation .",
    "it takes both to exploit the parallel capabilities of gpus .",
    "block relaxation and the mm algorithm often generate ideal parallel algorithms . in our opinion",
    "the mm algorithm is the more versatile of the two generic strategies .",
    "unfortunately , block relaxation does not accommodate constraints well and may generate sequential rather than parallel updates .",
    "even when its updates are parallel , they may not be data separated .",
    "the em algorithm is one of the most versatile tools in the statistician s toolbox .",
    "the mm principle generalizes the em algorithm and shares its positive features .",
    "scoring and newton s methods become impractical in high dimensions . despite these arguments in favor of mm algorithms",
    ", one should always keep in mind hybrid algorithms such as the one we implemented for nnmf .",
    "although none of our data sets is really large by today s standards , they do demonstrate that a good gpu implementation can easily achieve one to two orders of magnitude improvement over a single cpu core .",
    "admittedly , modern cpus come with 2 to 8 cores , and distributed computing over cpu - based clusters remains an option .",
    "but this alternative also carries a hefty price tag .",
    "the nvidia gtx280 gpu on which our examples were run drives 240 cores at a cost of several hundred dollars .",
    "high - end computers with 8 or more cpu nodes cost thousands of dollars .",
    "it would take 30 cpus with 8 cores each to equal a single gpu at the same clock rate .",
    "hence , gpu cards strike an effective and cost efficient balance .",
    "the simplicity of mm algorithms often comes at a price of slow ( at best linear ) convergence .",
    "our mds , nnmf , and pet ( without penalty ) examples are cases in point .",
    "slow convergence is a concern as statisticians head into an era dominated by large data sets and high - dimensional models .",
    "think about the scale of the netflix data matrix .",
    "the speed of any iterative algorithm is determined by both the computational cost per iteration and the number of iterations until convergence .",
    "gpu implementation reduces the first cost .",
    "computational statisticians also have a bag of software tricks to decrease the number of iterations @xcite .",
    "for instance , the recent paper  @xcite proposes a quasi - newton acceleration scheme particularly suitable for high - dimensional problems .",
    "the scheme is off - the - shelf and broadly applies to any search algorithm defined by a smooth algorithm map .",
    "the acceleration requires only modest increments in storage and computation per iteration .",
    "tables  [ table : pet ] and  [ table : mds ] also list the results of this quasi - newton acceleration of the cpu implementation for the mds and pet examples .",
    "as the tables make evident , quasi - newton acceleration significantly reduces the number of iterations until convergence .",
    "the accelerated algorithm always locates a better mode while cutting run times compared to the unaccelerated algorithm .",
    "we have tried the quasi - newton acceleration on our gpu hardware with mixed results .",
    "we suspect that the lack of full double precision on the gpu is the culprit .",
    "when full double precision becomes widely available , the combination of gpu hardware acceleration and algorithmic software acceleration will be extremely potent .",
    "successful acceleration methods will also facilitate attacking another nagging problem in computational statistics , namely multimodality .",
    "no one knows how often statistical inference is fatally flawed because a standard optimization algorithm converges to an inferior mode .",
    "the current remedy of choice is to start a search algorithm from multiple random points .",
    "algorithm acceleration is welcome because the number of starting points can be enlarged without an increase in computing time . as an alternative to multiple starting points",
    ", our recent paper  @xcite suggests modifications of several standard mm algorithms that increase the chance of locating better modes .",
    "these simple modifications all involve variations on deterministic annealing  @xcite .",
    "our treatment of simple classical examples should not hide the wide applicability of the powerful mm+gpu combination .",
    "a few other candidate applications include penalized estimation of haplotype frequencies in genetics @xcite , construction of biological and social networks under a random multigraph model @xcite , and data mining with a variety of models related to the multinomial distribution @xcite .",
    "many mixture models will benefit as well from parallelization , particularly in assigning group memberships .",
    "finally , parallelization is hardly limited to optimization .",
    "we can expect to see many more gpu applications in mcmc sampling . given the computationally intensive nature of mcmc",
    ", the ultimate payoff may even be higher in the bayesian setting than in the frequentist setting .",
    "of course realistically , these future triumphs will require a great deal of thought , effort , and education .",
    "there is usually a desert to wander and a river to cross before one reaches the promised land .",
    "m.s . acknowledges support from nih grant r01 gm086887 .",
    "k.l . was supported by united states public health service grants gm53275 and mh59490 .      berry , m.  w. , browne , m. , langville , a.  n. , pauca , v.  p. , and plemmons , r.  j. ( 2007 ) . algorithms and applications for approximate nonnegative matrix factorization .",
    "statist . data anal . _ * 52 * 155173 .          de  leeuw , j. ( 1977 ) . applications of convex analysis to multidimensional scaling .",
    "_ recent developments in statistics ( proc .",
    "european meeting statisticians , grenoble , 1976 ) _ , 133145 .",
    "north - holland , amsterdam .",
    "lee , a. , yan , c. , giles , m.  b. , doucet , a. , and holmes , c.  c. ( 2009 ) . on the utility of graphics cards to perform massively parallel simulation of advanced monte carlo methods .",
    "_ technical report _",
    ", department of statistics , oxford university .",
    "nvidia ( 2008 ) .",
    "nvidia cublas library .",
    "nvidia ( 2008 ) .",
    "nvidia cuda compute unified device architecture : programming guide version 2.0 .",
    "owens , j.  d. , luebke , d. , govindaraju , n. , harris , m. , krger , j. , lefohn , a.  e. , and purcell , t.  j. ( 2007 ) . a survey of general - purpose computation on graphics hardware .",
    "_ computer graphics forum _ * 26 * 80113 .",
    "roland , c. , varadhan , r. , and frangakis , c.  e. ( 2007 ) . squared polynomial extrapolation methods with cycling : an application to the positron emission tomography problem .",
    ". algorithms _ * 44 * 159172 .    silberstein , m. , schuster , a. , geiger , d. , patney , a. , and owens , j.  d. ( 2008 ) .",
    "efficient computation of sum - products on gpus through software - managed cache .",
    "_ proceedings of the 22nd annual international conference on supercomputing _",
    ", pages 309318 , acm ."
  ],
  "abstract_text": [
    "<S> this paper discusses the potential of graphics processing units ( gpus ) in high - dimensional optimization problems . </S>",
    "<S> a single gpu card with hundreds of arithmetic cores can be inserted in a personal computer and dramatically accelerates many statistical algorithms . to exploit these devices </S>",
    "<S> fully , optimization algorithms should reduce to multiple parallel tasks , each accessing a limited amount of data . </S>",
    "<S> these criteria favor em and mm algorithms that separate parameters and data . to a lesser extent block relaxation and coordinate descent and ascent also qualify . </S>",
    "<S> we demonstrate the utility of gpus in nonnegative matrix factorization , pet image reconstruction , and multidimensional scaling . </S>",
    "<S> speedups of 100 fold can easily be attained . over the next decade , gpus will fundamentally alter the landscape of computational statistics . </S>",
    "<S> it is time for more statisticians to get on - board .    , </S>"
  ]
}