{
  "article_text": [
    "vector auto ",
    "regressive ( var ) models play an important role in contemporary macro  economics , being an example of an approach called the `` dynamic stochastic general equilibrium '' ( dsge ) , which is superseding traditional large  scale macro  econometric forecasting methodologies  @xcite .",
    "the motivation behind them is based on the assertion that more recent values of a variable are more likely to contain useful information about its future movements than the older ones . on the other hand , a standard tool in multivariate time series analysis is vector moving average ( vma ) models , which is really a linear regression of the present value of the time series w.r.t .",
    "the past values of a white noise . a broader class of stochastic processes used in macro ",
    "economy comprises both these kinds together in the form of vector auto ",
    "regressive moving average ( varma ) models",
    ". these methodologies can capture certain spatial and temporal structures of multidimensional variables which are often neglected in practice ; including them not only results in more accurate estimation , but also leads to models which are more interpretable .",
    "they are widely used by academia and central banks ( _ cf .",
    "_ the european central bank s smets ",
    "wouters model for the euro zone  @xcite ) , as they constitute quite a simple version of the dsge equations .",
    "varma models are constructed from a number of univariate arma ( box  jenkins ; see for example  @xcite ) processes , typically coupled with each other . in this paper",
    ", we investigate only a significantly simplified circumstance when there is no coupling between the many arma components .",
    "one may argue that this is too far fetched and will be of no use in describing an economic reality",
    ". however , one may also treat it as a `` zeroth  order hypothesis , '' analogously to the idea of  @xcite in finance , namely that the case with no cross  covariances is considered theoretically , and subsequently compared to some real  world data modeled by a varma process ; any discrepancy between the two will reflect nontrivial cross ",
    "covariances present in the system , thus permitting their investigation .",
    "this latter route is taken in this communication .",
    "a challenging and yet increasingly important problem is the estimation of large covariance matrices generated by these stationary processes , since high dimensionality of the data as compared to the sample size is quite common in many statistical problems ( the `` dimensionality curse '' ) .",
    "therefore , an appropriate `` noise cleaning '' procedure has to be implemented , and random matrix theory ( rmt ) provides a natural and efficient outfit for doing that .",
    "in particular , the mean spectral densities ( a.k.a .",
    "`` limiting spectral distributions , '' lsd ) of the pearson estimators of the cross ",
    "covariances for the @xmath7 and @xmath8 models , in the relevant high  dimensionality sector and under the full decoupling , have been derived in  @xcite by applying the framework proposed by  @xcite .    in this paper , we suggest that such calculations can be considerably simplified by resorting to a mathematical concept of the free random variables ( frv ) calculus  @xcite , succinctly introduced in sec .  [ s : doublycorrelatedwishartensemblesandfreerandomvariables ] .",
    "our general frv formula  @xcite allows not only to rediscover , which much less strain , the two fourth  order polynomial equations obtained in  @xcite in the @xmath7 and @xmath8 cases , but also to derive a sixth  order equation ( [ eq : varmaoneonemainequation ] ) which produces the mean spectral density for a more involved @xmath6 model .",
    "the results are verified by numerical simulations , which show a perfect agreement .",
    "this is all done in sec .",
    "[ s : varmafromfreerandomvariables ] .",
    "@xmath2 stochastic processes , as we will see below , fall within quite a general set  up encountered in many areas of science where a probabilistic nature of multiple degrees of freedom evolving in time is relevant , for example , multivariate time series analysis in finance , applied macro  econometrics and engineering . to describe this framework ,",
    "consider a situation of @xmath3 time ",
    "dependent random variables which are measured at @xmath4 consecutive time moments ( separated by some time interval @xmath9 ) ; let be the value od the @xmath10th ( @xmath11 ) random number at the @xmath12th time moment ( @xmath13 ) ; together , they make up a rectangular @xmath14 matrix @xmath15 . in what usually would be the first approximation ,",
    "each is supposed to be drawn from a gaussian probability distribution .",
    "we will also assume that they have mean values zero , .",
    "these degrees of freedom may in principle display mutual correlations .",
    "a set of correlated zero ",
    "mean gaussian numbers is fully characterized by the two  point covariance function , if the underlying stochastic process generating these numbers is stationary .",
    "linear stochastic processes , including @xmath2 , belong to this category .",
    "we will restrict our attention to an even narrower class where the cross  correlations between different variables and the auto  correlations between different time moments are factorized , _",
    "i.e. _ , @xmath16 in this setting , the inter ",
    "variable covariances do not change in time ( and are described by an @xmath17 cross  covariance matrix @xmath18 ) , and also the temporal covariances are identical for all the numbers ( and are included in a @xmath19 auto  covariance matrix @xmath20 ; both these matrices are symmetric and positive  definite ) . the gaussian probability measure with this structure of covariances",
    "is known from textbooks , @xmath21_{i j } y_{j b } \\left [ \\mathbf{a}^{- 1 } \\right]_{b a } \\right ) { \\mathrm{d}}\\mathbf{y } = \\ ] ] @xmath22 where the normalization constant , and the integration measure , while the letters `` c.g . ''",
    "stand for `` correlated gaussian . ''",
    "now , a standard way to approach correlated gaussian random numbers is to recall that they can always be decomposed as linear combinations of uncorrelated gaussian degrees of freedom ; indeed , this is achieved through the transformation @xmath23 where the square roots of the covariance matrices , necessary to facilitate the transition , exist due to the positive  definiteness of @xmath18 and @xmath20 ; the new normalization reads .      an essential problem in multivariate analysis is to determine ( estimate ) the covariance matrices @xmath18 and @xmath20 from given @xmath3 time series of length @xmath4 of the realizations of our random variables . for simplicity",
    ", we do not distinguish in notation between random numbers , _",
    "i.e. _ , the population , and their realizations in actual experiments , _ i.e. _ , the sample .",
    "since the realized cross  covariance between degrees @xmath10 and @xmath24 at the same time @xmath12 is , the simplest method to estimate the today s cross  covariance is to compute the time average , @xmath25 this is usually named the `` pearson estimator '' , up to the prefactor which depending on the context is @xmath26 or @xmath27 .",
    "other estimators might be introduced , such as between distinct degrees of freedom at separate time moments ( `` time  delayed estimators '' ) , or with certain decreasing weights given to older measurements to reflect their growing obsolescence ( `` weighted estimators '' ) , but we will not investigate them in this article .",
    "furthermore , in the last equality in ( [ eq : estimatorcdefinition ] ) , we cast @xmath28 through the uncorrelated gaussian numbers contained in , the price to pay for this being that the covariance matrices now enter into the expression for @xmath28 , making it more complicated ; this will be the form used hereafter .",
    "the random matrix @xmath28 is called a `` doubly correlated wishart ensemble ''  @xcite .",
    "let us also mention that the auto  covariance matrix @xmath20 can be estimated through .",
    "however , it is verified that this object carries identical information to the one contained in @xmath28 ( it is `` dual '' to @xmath28 ) , and therefore may safely be discarded . indeed , these two estimators have same non  zero eigenvalues ( modulo an overall rescaling by @xmath29 ) , and the larger one has @xmath30 additional zero modes .",
    "any historical estimator is inevitably marred by the measurement noise ; it will reflect the true covariances only to a certain degree , with a superimposed broadening due to the finiteness of the time series .",
    "more precisely , there are @xmath31 independent elements in @xmath18 , to be estimated from @xmath32 measured quantities @xmath15 , hence the estimation accuracy will depend on the `` rectangularity ratio , '' @xmath33 the closer @xmath29 to zero , the more truthful the estimate .",
    "this is a cornerstone of classical multivariate analysis .",
    "unfortunately , a practical situation will typically feature a large number of variables sampled over a comparably big number of time snapshots , so that we may approximately talk about the `` thermodynamical limit , '' @xmath34 on the other hand , it is exactly this limit in which the frv calculus ( see the subsection below for its brief elucidation ) can be applied ; hence , the challenge of de  noising is somewhat counterbalanced by the computationally powerful frv techniques .",
    "any study of a ( real symmetric @xmath36 ) random matrix @xmath37 will most surely include a fundamental question about the average values of its ( real ) eigenvalues .",
    "they are concisely encoded in the `` mean spectral density , '' @xmath38 here the expectation map @xmath39 is understood to be taken w.r.t .",
    "the probability measure @xmath40 of the random matrix .",
    "we will always have this distribution rotationally ( _ i.e. _ , , with @xmath41 orthogonal ) invariant , and hence the full information about @xmath37 resides in its eigenvalues , distributed on average according to ( [ eq : meanspectraldensitydefinition ] ) .    on the practical side , it is more convenient to work with either of the two equivalent objects , @xmath42 referred to as the `` green s function '' ( or the `` resolvent '' ) and the `` @xmath35transform '' of @xmath37 .",
    "the latter is also called the `` moments generating function , '' since if the `` moments '' of @xmath37 exist , it can be expanded into a power series around @xmath43 as .",
    "it should however be underlined that even for probability measures disallowing such an expansion ( heavy  tailed distributions , preeminent in finance , being an example ) , the quantities ( [ eq : greenfunctionandmtransformdefinition ] ) still manage to entirely capture the spectral properties of @xmath37 ; hence the name `` @xmath35transform '' more appropriate , in addition to being more compact .",
    "we will show that for our purposes ( multiplication of random matrices ; see par .",
    "[ sss : thentransformandfreerandomvariables ] ) the @xmath35transform serves better than the green s function .",
    "however , it is customary to write the relationship between ( [ eq : meanspectraldensitydefinition ] ) and ( [ eq : greenfunctionandmtransformdefinition ] ) in terms of this latter , @xmath44 resulting from a well  known formula for generalized functions , .",
    "the doubly correlated wishart ensemble @xmath28 ( [ eq : estimatorcdefinition ] ) may be viewed as a product of several random and non  random matrices .",
    "the general problem of multiplying random matrices seems formidable .",
    "in classical probability theory , it can be effectively handled in the special situation when the random terms are independent : then , the exponential map reduces it to the addition problem of independent random numbers , solved by considering the logarithm of the characteristic functions of the respective pdfs , which proves to be additive . in matrix probability theory ,",
    "a crucial insight came from d.  voiculescu and coworkers and r.  speicher  @xcite , who showed how to parallel the commutative construction in the noncommutative world .",
    "it starts with the notion of `` freeness , '' which basically comprises probabilistic independence together with a lack of any directional correlation between two random matrices .",
    "this nontrivial new property happens to be the right extension of classical independence , as it allows for an efficient algorithm of multiplying free random variables ( frv ) , which we state below :    step 1 : : :    suppose we have two random matrices , and , mutually free .",
    "their    spectral properties are best wrought into the    @xmath35transforms    ( [ eq : greenfunctionandmtransformdefinition ] ) , and .",
    "step 2 : : :    the critical maneuver is to turn attention to the functional inverses    of these @xmath35transforms , the so  called    `` @xmath3transforms , ''    @xmath45 step 3 : : :    the @xmath3transforms submit to a very straightforward rule    upon multiplying free random matrices ( the `` frv multiplication    law '' ) , @xmath46 step 4 : : :    finally , it remains to functionally invert the resulting    @xmath3transform to gain the @xmath35transform of the    product , , and consequently , all the spectral properties via    formula  ( [ eq : sokhotskyformula ] ) .",
    "it is stunning that such a simple prescription ( relying on the choice of the @xmath35transform as the carrier of the mean spectral information , and the construction of its functional inverse , the @xmath3transform , which essentially multiplies under taking the free product ) resolves the multiplication problem for free random noncommutative objects .",
    "let us just mention that the addition problem may be tackled along similar lines : in this case , the green s function should be exploited , its functional inverse considered (; it is sometimes called the `` blue s function ''  @xcite ) , which obeys the `` frv addition law , '' , for two free random matrices . in this paper , we do not resort to using this addition formula , even though our problem could be approached through it as well .",
    "let us also remark that in the original mathematical formulations  @xcite of these frames , a slightly different language is employed : instead of the @xmath3transform , the `` @xmath47transform '' is found convenient , , while in place of the blue s function , one engages the `` @xmath48transform , '' .",
    "they fulfil simpler laws , and , respectively .      the innate potential of the frv multiplication algorithm ( [ eq : frvmultiplicationlaw ] )",
    "is surely revealed when inspecting the doubly correlated wishart random matrix ( [ eq : estimatorcdefinition ] ) .",
    "this has been done in detain in  @xcite , so we will only accentuate the main results here , referring the reader to the original paper for a thorough explanation .",
    "the idea is that one uses twice the cyclic property of the trace ( which permits cyclic shifts in the order of the terms ) , and twice the frv multiplication law ( [ eq : frvmultiplicationlaw ] ) ( to break the @xmath3transforms of products of matrices down to their constituents ) , in order to reduce the problem to solving the uncorrelated wishart ensemble .",
    "this last model is further simplified , again by the cyclic property and the frv multiplication rule applied once , to the standard @xmath49 random matrix squared ( and the projector , designed to chip the rectangle off the square @xmath49 ) , whose properties are firmly established .",
    "let us sketch the derivation , @xmath50 @xmath51 @xmath52    this is the basic formula .",
    "since the spectral properties of @xmath28 are given by its @xmath35transform , , it is more pedagogical to recast ( [ eq : doublycorrelatedwishartensemblethemainequation1 ] ) as an equation for the unknown @xmath35 , @xmath53 it provides a means for computing the mean spectral density of a doubly correlated wishart random matrix once the `` true '' covariance matrices @xmath18 and @xmath20 are given .    in this communication ,",
    "only a particular instance of this fundamental formula is applied , namely with an arbitrary auto  covariance matrix @xmath20 , but with trivial cross ",
    "covariances , . using that , equation ( [ eq : doublycorrelatedwishartensemblethemainequation2 ] ) thins out to @xmath54 which will be strongly exploited below .",
    "let us mention that these equalities ( [ eq : doublycorrelatedwishartensemblethemainequation2 ] ) , ( [ eq : doublycorrelatedwishartensemblethemainequation3 ] ) have been derived through other , more tedious , techniques ( the planar feynman ",
    "diagrammatic expansion , the replica trick ) in  @xcite .",
    "in what follows , we will assume that the @xmath0 , @xmath1 , or @xmath2 stochastic processes are covariance ( weak ) stationary ; for details , we refer to  @xcite .",
    "it implies certain restrictions on their parameters , but we will not bother with this issue in the current work .",
    "another consequence is that the processes display some interesting features , such as invertibility .",
    "for all this , we must in particular take both @xmath3 and @xmath4 large from the start , with their ratio @xmath55 fixed ( [ eq : thermodynamicallimit ] ) .",
    "more precisely , we stretch the range of the @xmath12index from minus to plus infinity .",
    "this means that all the finite  size effects ( appearing at the ends of the time series ) are readily disregarded .",
    "in particular , there is no need to care about initial conditions for the processes , and all the recurrence relations are assumed to continue to the infinite past .",
    "we consider a situation when @xmath3 stochastic variables evolve according to identical independent @xmath0 ( vector moving average ) processes , which we sample over a time span of @xmath4 moments .",
    "this is a simple generalization of the standard univariate weak  stationary moving average @xmath56 . in such a setting , the value of the @xmath10th ( @xmath11 ) random variable at time moment @xmath12 ( @xmath13 ) can be expressed as @xmath57 here all the s are iid standard ( mean zero , variance one ) gaussian random numbers ( white noise ) , .",
    "the s are some @xmath58 real constants ; importantly , they do not depend on the index @xmath10 , which reflects the fact that the processes are identical and independent ( no `` spatial '' covariances among the variables ) .",
    "the rank @xmath59 of the process is a positive integer .      in order to handle such a process ( [ eq : vmaqdefinition ] ) , notice that the s , being linear combinations of uncorrelated gaussian numbers , must also be gaussian random variables , albeit displaying some correlations",
    "therefore , to fully characterize these variables , it is sufficient to calculate their two  point covariance function ; this is straightforwardly done ( see appendix  [ ss : theautocovariancematrixforvmaq ] for details ) , @xmath60 where @xmath61 in other words , the cross  covariance matrix is trivial , ( no correlations between different variables ) , while the auto  covariance matrix , responsible for temporal correlations , can be called `` @xmath62diagonal . ''",
    "in the course of this article , we will use several different auto  covariance matrices , and for brevity , we decide to label them with superscripts ; their definitions are all collected in appendix  [ ss : alistofthevariousautocovariancematricesused ] .",
    "for example , in the simplest case of @xmath7 , it is tri  diagonal , @xmath63      such an infinite matrix ( [ eq : vmaqautocovariancematrix ] ) is translationally invariant ( as announced , it is one of the implications of the weak stationarity ) , _ i.e. _ , the value of any of its entries depends only on the distance between its indices , ; specifically , , for @xmath64 , and .",
    "hence , it is convenient to rewrite this matrix in the fourier space , @xmath65    in this representation , the @xmath35transform of is readily obtained  @xcite , @xmath66 this integral can be evaluated by the method of residues for any value of @xmath59 , which we do in appendix  [ ss : themtransformoftheautocovariancematrixforvmaq ] , where also we print the general result ( [ eq : vmaqgreenfunctionoftheautocovariancematrix ] ) . in particular , for @xmath67 , @xmath68 where the square roots are principal .      we will be interested in investigating the spectral properties of the pearson estimator ( [ eq : estimatorcdefinition ] ) . the @xmath35transform of this correlated wishart random matrix , , can be retrieved from equation ( [ eq : doublycorrelatedwishartensemblethemainequation3 ] ) .",
    "we could write it for any @xmath59 using ( [ eq : vmaqgreenfunctionoftheautocovariancematrix ] ) , but we will restrict ourselves only to @xmath67 , in which case the substitution of ( [ eq : vmaonemomentsgeneratingfunctionfora ] ) leads to a fourth  order polynomial ( ferrari ) equation for the unknown @xmath35 ,",
    "@xmath69 @xmath70 @xmath71 the frv technique allowed us therefore to find this equation in a matter of a few lines of a simple algebraic computation .",
    "it has already been derived in  @xcite , and ( [ eq : vmaonemainequation ] ) may be verified to coincide with the version given in that paper . in  @xcite",
    ", the pertinent equation is printed before ( a.6 ) , and to compare the two , one needs to change their variables into ours according to @xmath72 , @xmath73 , and .",
    "the last equality means that and @xmath74 of  @xcite correspond in our language to the green s functions and , respectively , where is the pearson estimator dual to @xmath28 .",
    "as mentioned , a quick extension to the case of arbitrary @xmath59 is possible , however the resulting equations for @xmath35 will be significantly more complicated ; for instance , for @xmath75 , a lengthy ninth  order polynomial equation is discovered .        a set  up of @xmath3 identical and independent @xmath1 ( vector auto  regressive ) processes",
    "is somewhat akin to ( [ eq : vmaqdefinition ] ) , _ i.e. _ , we consider @xmath3 decoupled copies of a standard univariate @xmath76 process , @xmath77 it is again described by the demeaned and standardized gaussian white noise ( which triggers the stochastic evolution ) , as well as @xmath58 real constants , , with @xmath78 . as announced before , the time stretches to the past infinity , so no initial condition is necessary .",
    "although at first sight ( [ eq : varqdefinition ] ) may appear to be a more involved recurrence relation for the s , it is actually easily reduced to the @xmath0 case : it remains to remark that if one exchanges the s with the s , one precisely arrives at the @xmath0 process with the constants , , @xmath78 .",
    "in other words , the auto  covariance matrix of the @xmath1 process ( [ eq : varqdefinition ] ) is simply the inverse of the auto  covariance matrix of the corresponding @xmath0 process with the described modification of the parameters , @xmath79 this inverse exists thanks to the weak stationarity supposition .",
    "the fourier transform of the auto  covariance matrix of @xmath1 is therefore a ( number ) inverse of its counterpart for @xmath0 with its parameters appropriately changed , @xmath80 where @xmath81 and where we define .    in order to find the @xmath35transform of the inverse matrix , , one employs a general result , true for any ( real symmetric ) random matrix @xmath37 , and obtainable through an easy algebra , @xmath82 since the quantity is known for any @xmath59 ( [ eq : vmaqgreenfunctionoftheautocovariancematrix ] ) , hence is via ( [ eq : mhinversefrommh ] ) , but we will not print it explicitly .",
    "let us just give it for @xmath67 , in which case ( [ eq : mhinversefrommh ] ) and ( [ eq : vmaonemomentsgeneratingfunctionfora ] ) yield @xmath83      despite being somewhat outside of the main line of thought of this article , an interesting question would be to search for an explicit expression for the auto  covariance matrix from its fourier transform ( [ eq : varqautocovariancematrixfourier ] ) , @xmath84 where we exploited the fact that must be translationally invariant , .",
    "this computation would shed light on the structure of temporal correlations present in a var setting .",
    "this integral is evaluated by the method of residues in a very similar manner to the one shown in appendix  [ ss : themtransformoftheautocovariancematrixforvmaq ] , and we do this in appendix  [ ss : theautocovariancematrixforvarq ] .",
    "we discover that the auto  covariance matrix is a sum of @xmath59 exponential decays , @xmath85 where are constants , and are the characteristic times ( [ eq : varqautocovariancematrixcharacteristictimes ] ) , @xmath86 ; these constituents are given explicitly in ( [ eq : varqautocovariancematrix2 ] ) .",
    "this is a well  known fact , nevertheless we wanted to establish it again within our approach .",
    "for example , for @xmath67 , the auto  covariance matrix of @xmath8 is one exponential decay , @xmath87 where we assumed for simplicity ( the formula can be easily extended to all values of ) .",
    "having found an expression for the @xmath35transform of the auto  covariance matrix of a @xmath1 ( [ eq : mhinversefrommh ] ) , ( [ eq : vmaqgreenfunctionoftheautocovariancematrix ] ) , we may proceed to investigate the equation ( [ eq : doublycorrelatedwishartensemblethemainequation3 ] ) for the @xmath35transform of the correlated wishart random matrix ( [ eq : estimatorcdefinition ] ) .",
    "we will do this explicitly only for @xmath67 , when ( [ eq : varonemomentsgeneratingfunctionfora ] ) leads to a fourth  order ( ferrari ) polynomial equation for the unknown @xmath35 , @xmath88 @xmath89 this equation has been derived by another method in  @xcite , and our result confirms their equation ( a.8 ) , with the change in notation , @xmath72 , @xmath73 , @xmath90 .",
    "the two types of processes which we elaborated on above , and , can be combined into one stochastic process called , @xmath91 now it is a straightforward and well  known observation ( which can be verified by a direct calculation ) that the auto  covariance matrix of this process is simply the product ( in any order ) of the auto  covariance matrices of the var and vma pieces ; more precisely , @xmath92 where corresponds to the generic model ( [ eq : vmaqautocovariancematrix ] ) , while denotes the auto  covariance matrix of with a slightly different modification of the parameters compared to the previously used , namely , , for .",
    "we have thus already made use here of the fact that the auto  covariance matrix of a var process is the inverse of the auto  covariance matrix of a certain corresponding vma process ( [ eq : varqfromvmaq ] ) , but the new change in parameters necessary in moving from var to vma has effectively w.r.t .",
    "what we had before ( [ eq : varqfromvmaq ] ) ; it is understandable : this `` missing '' is now included in the matrix of the other process .     of the cross  covariances in the @xmath6 process computed numerically from the sixth  order polynomial equation ( [ eq : varmaoneonemainequation ] ) , for various values of the process parameters .",
    "the scale of these parameters is determined by choosing everywhere .",
    "recall that the theoretical formula ( [ eq : varmaoneonemainequation ] ) is valid in the thermodynamical limit ( [ eq : thermodynamicallimit ] ) of @xmath93 , with @xmath94 kept finite . + up left : we set the remaining varma parameters to , , while the rectangularity ratio takes the values @xmath95 ( the purple line ) , @xmath96 ( red ) , @xmath97 ( magenta ) , @xmath98 ( pink ) ; each one is @xmath99 times smaller than the preceding one .",
    "we observe how the graphs become increasingly peaked ( narrower and taller ) around @xmath100 as @xmath29 decreases , which reflects the movement of the estimator @xmath28 toward its underlying value .",
    "+ up right : we fix @xmath101 and consider the two varma parameters equal to each other , with the values ( purple ) , @xmath102 ( red ) , @xmath103 ( magenta ) , @xmath104 ( pink ) .",
    "+ down left : we hold @xmath101 and , and modify ( purple ) , @xmath102 ( red ) , @xmath103 ( magenta ) , @xmath105 ( pink ) ; for this last value , the @xmath6 model reduces to @xmath8 . + down right : similarly , but this time we assign @xmath101 and , while changing ( purple ) , @xmath102 ( red ) , @xmath103 ( magenta ) , @xmath105 ( pink ) ; this last value corresponds to @xmath7.,title=\"fig:\",width=321 ]   of the cross  covariances in the @xmath6 process computed numerically from the sixth  order polynomial equation ( [ eq : varmaoneonemainequation ] ) , for various values of the process parameters .",
    "the scale of these parameters is determined by choosing everywhere . recall that the theoretical formula ( [ eq : varmaoneonemainequation ] ) is valid in the thermodynamical limit ( [ eq : thermodynamicallimit ] ) of @xmath93 , with @xmath94 kept finite . + up left : we set the remaining varma parameters to , , while the rectangularity ratio takes the values @xmath95 ( the purple line ) , @xmath96 ( red ) , @xmath97 ( magenta ) , @xmath98 ( pink ) ; each one is @xmath99 times smaller than the preceding one .",
    "we observe how the graphs become increasingly peaked ( narrower and taller ) around @xmath100 as @xmath29 decreases , which reflects the movement of the estimator @xmath28 toward its underlying value .",
    "+ up right : we fix @xmath101 and consider the two varma parameters equal to each other , with the values ( purple ) , @xmath102 ( red ) , @xmath103 ( magenta ) , @xmath104 ( pink ) .",
    "+ down left : we hold @xmath101 and , and modify ( purple ) , @xmath102 ( red ) , @xmath103 ( magenta ) , @xmath105 ( pink ) ; for this last value , the @xmath6 model reduces to @xmath8 .",
    "+ down right : similarly , but this time we assign @xmath101 and , while changing ( purple ) , @xmath102 ( red ) , @xmath103 ( magenta ) , @xmath105 ( pink ) ; this last value corresponds to @xmath7.,title=\"fig:\",width=321 ]   of the cross  covariances in the @xmath6 process computed numerically from the sixth  order polynomial equation ( [ eq : varmaoneonemainequation ] ) , for various values of the process parameters .",
    "the scale of these parameters is determined by choosing everywhere .",
    "recall that the theoretical formula ( [ eq : varmaoneonemainequation ] ) is valid in the thermodynamical limit ( [ eq : thermodynamicallimit ] ) of @xmath93 , with @xmath94 kept finite . + up left : we set the remaining varma parameters to , , while the rectangularity ratio takes the values @xmath95 ( the purple line ) , @xmath96 ( red ) , @xmath97 ( magenta ) , @xmath98 ( pink ) ; each one is @xmath99 times smaller than the preceding one .",
    "we observe how the graphs become increasingly peaked ( narrower and taller ) around @xmath100 as @xmath29 decreases , which reflects the movement of the estimator @xmath28 toward its underlying value .",
    "+ up right : we fix @xmath101 and consider the two varma parameters equal to each other , with the values ( purple ) , @xmath102 ( red ) , @xmath103 ( magenta ) , @xmath104 ( pink ) .",
    "+ down left : we hold @xmath101 and , and modify ( purple ) , @xmath102 ( red ) , @xmath103 ( magenta ) , @xmath105 ( pink ) ; for this last value , the @xmath6 model reduces to @xmath8 . + down right : similarly , but this time we assign @xmath101 and , while changing ( purple ) , @xmath102 ( red ) , @xmath103 ( magenta ) , @xmath105 ( pink ) ; this last value corresponds to @xmath7.,title=\"fig:\",width=321 ]   of the cross  covariances in the @xmath6 process computed numerically from the sixth  order polynomial equation ( [ eq : varmaoneonemainequation ] ) , for various values of the process parameters .",
    "the scale of these parameters is determined by choosing everywhere .",
    "recall that the theoretical formula ( [ eq : varmaoneonemainequation ] ) is valid in the thermodynamical limit ( [ eq : thermodynamicallimit ] ) of @xmath93 , with @xmath94 kept finite . + up left : we set the remaining varma parameters to , , while the rectangularity ratio takes the values @xmath95 ( the purple line ) , @xmath96 ( red ) , @xmath97 ( magenta ) , @xmath98 ( pink ) ; each one is @xmath99 times smaller than the preceding one .",
    "we observe how the graphs become increasingly peaked ( narrower and taller ) around @xmath100 as @xmath29 decreases , which reflects the movement of the estimator @xmath28 toward its underlying value .",
    "+ up right : we fix @xmath101 and consider the two varma parameters equal to each other , with the values ( purple ) , @xmath102 ( red ) , @xmath103 ( magenta ) , @xmath104 ( pink ) .",
    "+ down left : we hold @xmath101 and , and modify ( purple ) , @xmath102 ( red ) , @xmath103 ( magenta ) , @xmath105 ( pink ) ; for this last value , the @xmath6 model reduces to @xmath8 . + down right : similarly , but this time we assign @xmath101 and , while changing ( purple ) , @xmath102 ( red ) , @xmath103 ( magenta ) , @xmath105 ( pink ) ; this last value corresponds to @xmath7.,title=\"fig:\",width=321 ]    ) ( the dashed red lines ) .",
    "the conformity is nearly perfect .",
    "we generate the matrices @xmath15 of sizes @xmath106 , @xmath107 ( _ i.e. _ , @xmath101 ) from the @xmath6 process with the parameters , ,",
    ". the monte carlo simulation is repeated @xmath108 ( left ) or @xmath109 ( right ) times ; in this latter case , a significant improvement in the quality of the agreement is seen .",
    "one notices finite  size effects at the edges of the spectrum ( `` leaking out '' of eigenvalues ) : in the numerical simulations , @xmath3 and @xmath4 are obviously finite , while equation ( [ eq : varmaoneonemainequation ] ) is legitimate in the thermodynamical limit ( [ eq : thermodynamicallimit ] ) only , hence the small discrepancies ; by enlarging the chosen dimensions @xmath110 one would diminish this fallout.,title=\"fig:\",width=321 ] ) ( the dashed red lines ) .",
    "the conformity is nearly perfect .",
    "we generate the matrices @xmath15 of sizes @xmath106 , @xmath107 ( _ i.e. _ , @xmath101 ) from the @xmath6 process with the parameters , ,",
    ". the monte carlo simulation is repeated @xmath108 ( left ) or @xmath109 ( right ) times ; in this latter case , a significant improvement in the quality of the agreement is seen .",
    "one notices finite  size effects at the edges of the spectrum ( `` leaking out '' of eigenvalues ) : in the numerical simulations , @xmath3 and @xmath4 are obviously finite , while equation ( [ eq : varmaoneonemainequation ] ) is legitimate in the thermodynamical limit ( [ eq : thermodynamicallimit ] ) only , hence the small discrepancies ; by enlarging the chosen dimensions @xmath110 one would diminish this fallout.,title=\"fig:\",width=321 ]      the fourier transform of the auto  covariance matrix of ( [ eq : varmaq1q2fromvarq1vmaq2 ] ) is simply the product of the respective fourier transforms ( [ eq : vmaqautocovariancematrixfourier ] ) and ( [ eq : varqautocovariancematrixfourier ] ) , @xmath111 where @xmath112 where we recall . for instance , for @xmath6",
    "( it is described by three constants , , , ) , one explicitly has @xmath113    the @xmath35transform of can consequently be derived from the general formula ( [ eq : momentsgeneratingfunctionforatranslationallyinvariant ] ) .",
    "we will evaluate here the pertinent integral only for the simplest @xmath6 process , even though an arbitrary case may be handled by the technique of residues , @xmath114      one might again attempt to track the structure of temporal covariances in a varma process .",
    "this can be done either by the inverse fourier transform of ( [ eq : varmaq1q2autocovariancematrixfourier ] ) , or through a direct computation based on the recurrence relation ( [ eq : varmaq1q2definition ] ) ( importantly , adhering to the assumption that it stretches to the past infinity ) .",
    "let us print the result just for @xmath6 , @xmath115 where for simplicity .",
    "this is an exponential decay , with the characteristic time of the var piece , with an additional term on the diagonal .",
    "expression ( [ eq : varmaoneonemomentsgeneratingfunctionforafive ] ) , along with the fundamental frv formula ( [ eq : doublycorrelatedwishartensemblethemainequation3 ] ) , allow us to write the equation satisfied by the @xmath35transform of the pearson estimator ( [ eq : estimatorcdefinition ] ) of the cross  covariances in the @xmath6 process ; it happens to be polynomial of order six , and we print it ( [ eq : varmaoneonemainequation ] ) in appendix  [ ss : theequationforthemtransformofthepearsonestimatorofthecovariancesforvarmaoneone ] .",
    "it may be solved numerically , a proper solution chosen ( the one which leads to a sensible density : real , positive  definite , normalized to unity ) , and finally , the mean spectral density derived from ( [ eq : sokhotskyformula ] ) .",
    "we show the shapes of this density for a variety of the values of the parameters @xmath29 , , , in fig .",
    "[ fig : varmaoneonetheory ] .",
    "moreover , in order to test the result ( [ eq : varmaoneonemainequation ] ) , and more broadly , to further establish our frv framework in the guise of formula ( [ eq : doublycorrelatedwishartensemblethemainequation3 ] ) , the theoretical form of the density is compared to monte carlo simulations in fig .",
    "[ fig : varmaoneonetheoryplusmc ] ; they remain in excellent concord .",
    "these are the main findings of this article .",
    "in this paper we attempted to advertise the power and flexibility of the free random variables calculus for multivariate stochastic processes of the varma type .",
    "the frv calculus is ideally suited for multidimensional time series problems , provided the dimensions of the underlying matrices are large .",
    "the operational procedures are simple , algebraic and transparent .",
    "the structure of the final formula which relates the moments generating function of the population covariance and the sample covariance allows one to easily derive eigenvalue density of the sample covariance .",
    "we in detail illustrated how this procedure works for @xmath6 , confronted the theoretical prediction with numerical data obtained by monte carlo simulations of the varma process and observed a perfect agreement .",
    "the frv calculus is not restricted to gaussian variables .",
    "it also works for non ",
    "gaussian processes , including those with heavy  tailed increments belonging to the lvy basin of attraction , where the moments do not exist . since the majority of data collected nowadays is naturally stored in the form of huge matrices , we believe that the frv technique is the most natural candidate for the matrix  valued `` probability calculus '' that can provide efficient algorithms for cleaning ( de  noising ) large sets of data and unraveling essential but hidden correlations .    this work has been supported by the polish ministry of science grant no .",
    "n  n202  229137 ( 20092012 ) .",
    "aj acknowledges the support of clico ltd .",
    "in this appendix , we sketch a proof of the formula ( [ eq : vmaqautocovariancematrix ] ) for the auto  covariance matrix of the @xmath0 process .",
    "as mentioned , since the random variables are centered gaussian , this matrix alone suffices to completely capture all their properties .",
    "we set @xmath116 ; the dependence on this index may be dropped as there are no correlations here .",
    "we use the definition ( [ eq : vmaqdefinition ] ) of @xmath0 , as well as the auto  covariance structure of the white noise , .",
    "this leads to @xmath117 the double sum is symmetrized , the index @xmath118 replaced by @xmath119 , @xmath120 and the order of the sums interchanged ( an elegant method for this is explained in  @xcite ) , @xmath121 which , upon splitting the sum over @xmath122 into three pieces ( from @xmath123 to @xmath124 , @xmath125 , and from @xmath126 to @xmath59 ) , is quickly seen to coincide with ( [ eq : vmaqautocovariancematrix ] ) .        * by we denote the auto  covariance matrix of the @xmath0 process with the generic constants , with @xmath127 , as defined in ( [ eq : vmaqdefinition ] ) . * by we denote the auto  covariance matrix of the @xmath0 process with the constants , , where @xmath78 . * by we denote the auto  covariance matrix of the @xmath1 process with the generic constants , , with @xmath78 , as defined in ( [ eq : varqdefinition ] )",
    ". there holds ( [ eq : varqfromvmaq ] ) . * by we denote the auto  covariance matrix of the @xmath128 process with the constants , , where . * by we denote the auto  covariance matrix of the @xmath2 process with the generic constants , , with and , according to the definition ( [ eq : varmaq1q2definition ] ) .",
    "there is ( [ eq : varmaq1q2fromvarq1vmaq2 ] ) , where in the latter piece .",
    "[ [ ss : themtransformoftheautocovariancematrixforvmaq ] ] the @xmath35transform of the auto  covariance matrix for @xmath0 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    we will derive here the @xmath35transform ( [ eq : momentsgeneratingfunctionforatranslationallyinvariant ] ) of the auto  covariance matrix of an arbitrary @xmath0 process , using the expression for its fourier transform ( [ eq : vmaqautocovariancematrixfourier ] ) .",
    "it is a little simpler to consider the green s function , @xmath129 where the integration range has been halved due to the evenness of the integrand .",
    "this integral is performed with help of the change of variables @xmath130 .",
    "the measure , when @xmath131 $ ] , reads .",
    "a basic observation is that the denominator of the integrand is a linear combination of @xmath132 , for @xmath133 , and each such a cosine can be cast as a polynomial of order @xmath122 in @xmath134 through the de moivre formula .",
    "hence , the denominator is a polynomial of order @xmath59 in @xmath134 , @xmath135 where the s are the @xmath59 roots ( which we assume to be single ) , and @xmath136 is the coefficient at . using the method of residues ,",
    "one readily finds @xmath137 where the two square roots on the r.h.s .",
    "are principal .",
    "this is an explicit formula for the green s function of , provided one has factorized the order@xmath59 polynomial ( [ eq : vmaqgreenfunctionoftheautocovariancematrixdenominator ] ) .",
    "let us argue now that the fourier transform ( [ eq : varqautocovariancematrixfourier ] ) leads to the auto  covariance matrix of @xmath1 ( [ eq : varqautocovariancematrixfromitsfouriertransform ] ) of the form of a sum of exponential decays ( [ eq : varqautocovariancematrix1 ] ) , and let us give precise expressions for the constants and the characteristic times , @xmath86 .",
    "we proceed by the technique of residues , analogously to appendix  [ ss : themtransformoftheautocovariancematrixforvmaq ] , however this time with aid of another variable , , related to the previously used through @xmath138 .",
    "the integration measure is @xmath139 , and the integration path is counterclockwise around the centered unit circle .",
    "the denominator of the integrand is a polynomial of order @xmath59 in the variable @xmath134 , having thus some @xmath59 roots , @xmath78 .",
    "therefore , there are @xmath140 corresponding solutions for the variable @xmath141 , with a half of them inside the integration path and a half outside ; let be the solutions to with the absolute values less than @xmath126 .",
    "only them contribute to the integral , and their residues straightforwardly give @xmath142 this is indeed @xmath59 exponents , @xmath86 .",
    "remark that the solutions may be complex , hence this is really @xmath59 different exponential decays , with the characteristic times @xmath143 ( these times are positive as the roots have the absolute values less than @xmath126 ) , possibly modulated by sinusoidal oscillations when a root has an imaginary part .    for example , for @xmath67",
    "there is one exponential decay ( [ eq : varoneautocovariancematrix ] ) , while for @xmath75 , one obtains either two exponential decays ( the two roots are real and different ) , or one exponential decay modulated by oscillations ( the two roots are complex and mutually conjugate ) , _ etc . _",
    "[ [ ss : theequationforthemtransformofthepearsonestimatorofthecovariancesforvarmaoneone ] ] the equation for the @xmath35transform of the pearson estimator of the covariances for @xmath6 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    the sixth  order polynomial equation obeyed by in the case of @xmath6 reads , @xmath144 @xmath145 @xmath146 @xmath147 @xmath148 @xmath149 @xmath150 @xmath151 @xmath152 @xmath153 @xmath154 @xmath155 @xmath156 @xmath157 @xmath158 @xmath159 @xmath160 @xmath161 @xmath162 @xmath163 this equation in a ` mathematica ` file can be obtained from the authors upon request .",
    "99 sims c. a. , _ macroeconomics and reality _ , econometrica * 48 * ( 1980 ) 1 .",
    "smets f. , wouters r. , _ an estimated stochastic dynamic general equilibrium model of the euro area _ , european central bank working paper series , working paper * 171 * , august 2002 [ ` http://www.ecb.int ` ] .",
    "box g. , jenkins g. m. , reinsel g. , _ time series analysis : forecasting and control _",
    ", prentice  hall , englewood cliffs , nj , 1994 .",
    "laloux l. , cizeau p. , bouchaud j.p . , potters m. , _ noise dressing of financial correlation matrices _ , phys .",
    "* 83 * ( 1999 ) 1467 [ ` arxiv : cond - mat/9810255 ` ] .",
    "plerou v. , gopikrishnan p. , rosenow b. , amaral l. a. n. , stanley h. e. , _ universal and non  universal properties of cross  correlations in financial time series _ , phys .",
    "* 83 * ( 1999 ) 1471 [ ` arxiv : cond - mat/9902283 ` ] .",
    "jin b. , wang c. , miao b. , huang m.n .",
    "l. , _ limiting spectral distribution of large  dimensional sample covariance matrices generated by varma _",
    ", journal of multivariate analysis * 100 * ( 2009 ) 2112 .",
    "bai z. d. , silverstein j. w. , _ spectral analysis of large dimensional random matrices _ , science press , beijing , 2006 .",
    "voiculescu d. v. , dykema k. j. , nica a. , _ free random variables _ , crm monograph series , vol .",
    "1 , am . math .",
    "providence , 1992 .",
    "speicher r. , _ multiplicative functions on the lattice of non  crossing partitions and free convolution _ , math .",
    "* 298 * ( 1994 ) 611 .",
    "burda z. , jarosz a. , jurkiewicz j. , nowak m. a. , papp g. , zahed i. , _ applying free random variables to random matrix analysis of financial data _ , submitted to quantitative finance .",
    "wishart j. , _ the generalized product moment distribution in samples from a normal multivariate population _",
    ", biometrika * a 20 * ( 1928 ) 32 .",
    "zee a. , _ law of addition in random matrix theory _ ,",
    "nucl . phys .",
    "* b 474 * ( 1996 ) 726 [ ` arxiv : cond - mat/9602146 ` ] .",
    "janik r. a. , nowak m. a. , papp g. , zahed i. , _ various shades of blue s functions _ , acta phys",
    "* b 28 * ( 1997 ) 2949 [ ` arxiv : hep - th/9710103 ` ] .",
    "burda z. , grlich a. , jarosz a. , jurkiewicz j. , _ signal and noise in correlation matrix _",
    ", physica * a 343 * ( 2004 ) 295 [ ` arxiv : cond - mat/0305627 ` ] .",
    "burda z. , jurkiewicz j. , _ signal and noise in financial correlation matrices _ , physica * a 344 * ( 2004 ) 67 [ ` arxiv : cond - mat/0312496 ` ] .",
    "burda z. , jurkiewicz j. , wacaw b. , _ spectral moments of correlated wishart matrices _ , phys .",
    "e 71 * ( 2005 ) 026111 [ ` arxiv : cond - mat/0405263 ` ] .",
    "burda z. , grlich a. , jurkiewicz j. , wacaw b. , _ correlated wishart matrices and critical horizons _ , eur .",
    "j. * b 49 * ( 2006 ) 319 [ ` arxiv : cond - mat/0508341 ` ] .",
    "burda z. , jurkiewicz j. , wacaw b. , _ eigenvalue density of empirical covariance matrix for correlated samples _",
    ", acta phys . pol .",
    "* b 36 * ( 2005 ) 2641 [ ` arxiv : cond - mat/0508451 ` ] .",
    "ltkepohl h. , _ new introduction to multiple time series analysis _ , springer verlag , berlin , 2005 .",
    "graham r. , knuth d. , patashnik o. , _ concrete mathematics : a foundation for computer science _",
    ", addison ",
    "wesley , 1994 ."
  ],
  "abstract_text": [
    "<S> we apply random matrix theory to derive spectral density of large sample covariance matrices generated by multivariate @xmath0 , @xmath1 and @xmath2 processes . in particular </S>",
    "<S> , we consider a limit where the number of random variables @xmath3 and the number of consecutive time measurements @xmath4 are large but the ratio @xmath5 is fixed . in this regime </S>",
    "<S> the underlying random matrices are asymptotically equivalent to free random variables ( frv ) . </S>",
    "<S> we apply the frv calculus to calculate the eigenvalue density of the sample covariance for several varma  type processes . </S>",
    "<S> we explicitly solve the @xmath6 case and demonstrate a perfect agreement between the analytical result and the spectra obtained by monte carlo simulations . </S>",
    "<S> the proposed method is purely algebraic and can be easily generalized to and . </S>"
  ]
}