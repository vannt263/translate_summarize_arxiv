{
  "article_text": [
    "recent work views lexical choice as the process of mapping from a set of concepts ( in some representation of knowledge ) to a word or phrase @xcite . when the same concept admits more than one lexicalization , it is often difficult to choose which of these ` synonyms ' is the most appropriate for achieving the desired pragmatic goals ; but this is necessary for high - quality machine translation and natural language generation .",
    "knowledge - based approaches to representing the potentially subtle differences between synonyms have suffered from a serious lexical acquisition bottleneck @xcite . statistical approaches , which have sought to explicitly represent differences between pairs of synonyms with respect to their occurrence with other specific words @xcite , are inefficient in time and space .",
    "this paper presents a new statistical approach to modeling context that provides a preliminary solution to an important sub - problem , that of determining the near - synonym that is _ most typical _ , or expected , if any , in a given context .",
    "although weaker than full lexical choice , because it does nt choose the ` best ' word , we believe that it is a necessary first step , because it would allow one to determine the effects of choosing a non - typical word in place of the typical word .",
    "the approach relies on a generalization of lexical co - occurrence that allows for an implicit representation of the differences between two ( or more ) words with respect to _ any _ actual context .",
    "for example , our implemented lexical choice program selects _ mistake _ as most typical for the ` gap ' in sentence  [ mistake72 ] , and _ error _ in [ error115 ] .",
    "[ mistake72 ] however , such a move also would run the risk of cutting deeply into u.s .",
    "economic growth , which is why some economists think it would be a big \\{error @xmath0 mistake @xmath0 oversight}.    [ error115 ] the \\{error @xmath0 mistake @xmath0 oversight }  was magnified when the army failed to charge the standard percentage rate for packing and handling .",
    "evidence - based models represent context as a set of features , say words , that are observed to co - occur with , and thereby predict , a word @xcite .",
    "but , if we use just the context surrounding a word , we might not be able to build up a representation satisfactory to uncover the subtle differences between synonyms , because of the massive volume of text that would be required .",
    "now , observe that even though a word might not co - occur significantly with another given word , it might nevertheless _ predict _ the use of that word if the two words are mutually related to a third word .",
    "that is , we can treat lexical co - occurrence as though it were moderately transitive .",
    "for example , in [ task113 ] , _ learn _ provides evidence for _ task _ because it co - occurs ( in other contexts ) with _ difficult _ , which in turn co - occurs with _ task _ ( in other contexts ) , even though _ learn _ is not seen to co - occur significantly with",
    "_ task_.    [ task113 ]",
    "the team s most urgent _ task _ was to _ learn _ whether chernobyl would suggest any safety flaws at kwu - designed plants .",
    "so , by augmenting the contextual representation of a word with such second - order ( and higher ) co - occurrence relations , we stand to have greater predictive power , assuming that we assign less weight to them in accordance with their lower information content . and as our results will show , this generalization of co - occurrence is necessary .",
    "we can represent these relations in a _ lexical co - occurrence network _",
    ", as in figure  [ task - net ] , that connects lexical items by just their first - order co - occurrence relations .",
    "second - order and higher relations are then implied by transitivity .",
    "we build a lexical co - occurrence network as follows : given a root word , connect it to all the words that significantly co - occur with it in the training corpus ; tokens .",
    "no lemmatization or sense disambiguation was done .",
    "stop words were numbers , symbols , proper nouns , and any token with a raw frequency greater than @xmath1 . ] then , recursively connect these words to their significant co - occurring words up to some specified depth .",
    "we use the intersection of two well - known measures of significance , mutual information scores and @xmath2-scores @xcite , to determine if a ( first - order ) co - occurrence relation should be included in the network ; however , we use just the @xmath2-scores in computing _ significance scores _ for all the relations . given two words , @xmath3 and @xmath4 , in a co - occurrence relation of order @xmath5 , and a shortest path @xmath6 between them ,",
    "the significance score is @xmath7 this formula ensures that significance is inversely proportional to the order of the relation .",
    "for example , in the network of figure  [ task - net ] , sig@xmath8/8 = 0.41 $ ] .",
    "a single network can be quite large .",
    "for instance , the complete network for _ task _ ( see figure  [ task - net ] ) up to the third - order has 8998 nodes and 37,548 edges .",
    "the amount of evidence that a given sentence provides for choosing a candidate word is the sum of the significance scores of each co - occurrence of the candidate with a word in the sentence .",
    "so , given a gap in a sentence @xmath9 , we find the candidate @xmath10 for the gap that maximizes @xmath11 for example , given @xmath9 as sentence  [ task113 ] , above , and the network of figure  [ task - net ] , @xmath12 .",
    "( using its own network ) matches best with a score of @xmath13 ; _ duty _ places third with a score of @xmath14 .",
    "to evaluate the lexical choice program , we selected several sets of near - synonyms , shown in table  [ syn - sets ] , that have low polysemy in the corpus , and that occur with similar frequencies .",
    "this is to reduce the confounding effects of lexical ambiguity .    [",
    "cols=\"<,<,<\",options=\"header \" , ]     for each set , we collected all sentences from the yet - unseen 1987 _ wall street journal _ ( part - of - speech - tagged ) that contained any of the members of the set , ignoring word sense . we replaced each occurrence by a ` gap ' that the program then had to fill .",
    "we compared the ` correctness ' of the choices made by our program to the baseline of always choosing the most frequent synonym according to the training corpus .",
    "but what are the ` correct ' responses ?",
    "ideally , they should be chosen by a credible human informant .",
    "but regrettably , we are not in a position to undertake a study of how humans judge typical usage , so we will turn instead to a less ideal source : the authors of the _ wall street journal_. the problem is , of course , that authors are nt always typical . a particular word might occur in a ` pattern ' in which another synonym was seen more often , making it the typical choice .",
    "thus , we can not expect perfect accuracy in this evaluation .",
    "table  [ table - results ] shows the results for all seven sets of synonyms under different versions of the program .",
    "we varied two parameters : ( 1 ) the window size used during the construction of the network : either narrow ( @xmath154 words ) , medium ( @xmath15 10 words ) , or wide ( @xmath15 50 words ) ; ( 2 ) the maximum order of co - occurrence relation allowed : 1 , 2 , or 3 .    rrrrrrrr set & 1 & 2 & 3 & 4 & 5 & 6 & 7 + size & 6665 & 1030 & 5402 & 3138 & 1828 & 10204 & 1568 + baseline & 40.1% & 33.5% & 74.2% & 36.6% & 62.8% & 45.7% & 62.2% + 1 & 31.3% & 18.7% & 34.5% & 27.7% & 28.8% & 33.2% & 41.3% + narrow 2 & 47.2% & 44.5% & 66.2% & 43.9% & 61.9%@xmath16 & 48.1% & 62.8%@xmath16 + 3 & * 47.9% * & * 48.9% * & * 68.9% * & 44.3% & * 64.6%@xmath16 * & * 48.6% * & * 65.9% * + 1 & 24.0% & 25.0% & 26.4% & 29.3% & 28.8% & 20.6% & 44.2% + medium 2 & 42.5% & 47.1% & 55.3% & * 45.3% * & 61.5%@xmath16 & 44.3% & 63.6%@xmath16 + 3 & 42.5% & 47.0% & 53.6% &  &  &  &  + wide 1 & 9.2% & 20.6% & 17.5% & 20.7% & 21.2% & 4.1% & 26.5% + 2 & 39.9%@xmath16 & 46.2% & 47.1% & 43.2% & 52.7% & 37.7% & 58.6% +    the results show that at least second - order co - occurrences are necessary to achieve better than baseline accuracy in this task ; regular co - occurrence relations are insufficient .",
    "this justifies our assumption that we need more than the surrounding context to build adequate contextual representations .",
    "also , the narrow window gives consistently higher accuracy than the other sizes .",
    "this can be explained , perhaps , by the fact that differences between near - synonyms often involve differences in short - distance collocations with neighboring words , e.g. , _ face the task_.    there are two reasons why the approach does nt do as well as an automatic approach ought to .",
    "first , as mentioned above , our method of evaluation is not ideal ; it may make our results just _ seem _ poor .",
    "perhaps our results _ actually _ show the level of ` typical usage ' in the newspaper .",
    "second , lexical ambiguity is a major problem , affecting both evaluation and the construction of the co - occurrence network .",
    "for example , in sentence [ task113 ] , above , it turns out that the program uses _ safety _ as evidence for choosing _ job _ ( because _ job safety _ is a frequent collocation ) , but this is the wrong sense of _",
    "job_. syntactic and collocational red herrings can add noise too .",
    "we introduced the problem of choosing the most typical synonym in context , and gave a solution that relies on a generalization of lexical co - occurrence .",
    "the results show that a narrow window of training context ( @xmath17 words ) works best for this task , and that at least second - order co - occurrence relations are necessary . we are planning to extend the model to account for more structure in the narrow window of context .",
    "for comments and advice , i thank graeme hirst , eduard hovy , and stephen green .",
    "this work is financially supported by the natural sciences and engineering council of canada .",
    "church , kenneth  ward , william gale , patrick hanks , donald hindle , and rosamund moon .",
    "lexical substitutability . in b.t.s .",
    "atkins and a.  zampolli , editors , _ computational approaches to the lexicon_. oxford university press , pages 153177 .",
    "dimarco , chrysanne , graeme hirst , and manfred stede .",
    "the semantic and stylistic differentiation of synonyms and near - synonyms . in _",
    "aaai spring symposium on building lexicons for machine translation _",
    ", pages 114121 , stanford , ca , march .",
    "golding , andrew  r. and yves schabes . 1996 . combining trigram - based and feature - based methods for context - sensitive spelling correction . in _ proceedings of the 34th annual meeting of the association for computational linguistics_.    hirst , graeme .",
    "near - synonymy and the structure of lexical knowledge . in _ aaai symposium on representation and acquisition of lexical knowledge : polysemy , ambiguity , and generativity _ , pages 5156 , stanford , ca , march .",
    "ng , hwee  tou and hian  beng lee .",
    "1996 . integrating multiple sources to disambiguate word sense : an exemplar - based approach . in _ proceedings of the 34th annual meeting of the association for computational linguistics_.      yarowsky , david .",
    "word - sense disambiguation using statistical models of roget s categories trained on large corpora . in _ proceedings of the 14th international conference on computational linguistics ( coling-92 )",
    "_ , pages 454460 ."
  ],
  "abstract_text": [
    "<S> this paper presents a partial solution to a component of the problem of lexical choice : choosing the synonym most typical , or expected , in context . </S>",
    "<S> we apply a new statistical approach to representing the context of a word through lexical co - occurrence networks . </S>",
    "<S> the implementation was trained and evaluated on a large corpus , and results show that the inclusion of second - order co - occurrence relations improves the performance of our implemented lexical choice program . </S>"
  ]
}