{
  "article_text": [
    "in this paper , we propose a clustering - based iterative algorithm to solve certain optimization problems in machine learning when data size is large and thus it becomes impractical to use out - of - the - box algorithms .",
    "we rely on the principle of data aggregation and then subsequent disaggregations . while it is standard practice to aggregate the data and then calibrate the machine learning algorithm on aggregated data , we embed this into an iterative framework where initial aggregations are gradually disaggregated to the extent that even an optimal solution is obtainable .    early studies in data aggregation consider transportation problems @xcite , where either demand or supply nodes are aggregated .",
    "zipkin @xcite studied data aggregation for linear programming ( lp ) and derived error bounds of the approximate solution .",
    "there are also studies on data aggregation for 0 - 1 integer programming @xcite .",
    "the reader is referred to rogers _",
    "et al _ @xcite and litvinchev and tsurkov @xcite for comprehensive literature reviews for aggregation techniques applied for optimization problems .    for support vector machines ( svm )",
    ", there exist several works using the concept of clustering or data aggregation .",
    "evgeniou and pontil @xcite proposed a clustering algorithm that creates large size clusters for entries surrounded by the same class and small size clusters for entries in the mixed - class area . the clustering algorithm is used to preprocess the data and the clustered data is used to solve the problem .",
    "the algorithm tends to create large size clusters for entries far from the decision boundary and small size clusters for the other case .",
    "et al _ @xcite developed screening rules for svm to discard non - support vectors that do not affect the classifier .",
    "et al _ @xcite and doppa _",
    "et al _ @xcite proposed a second order cone programming ( socp ) formulation for svm based on chance constraints and clusters .",
    "the key idea of the socp formulations is to reduce the number of constraints ( from the number of the entries to number of clusters ) by defining chance constraints for clusters .    after obtaining an approximate solution by solving the optimization problem with aggregated data ,",
    "a natural attempt is to use less - coarsely aggregated data , in order to obtain a finer approximation .",
    "in fact , we can do this iteratively : modify the aggregated data in each iteration based on the information at hand .",
    "this framework , which iteratively passes information between the original problem and the aggregated problem @xcite , is known as _ iterative aggregation disaggregation _ ( iad ) .",
    "the iad framework has been applied for several optimization problems such as lp @xcite and network design @xcite . in machine learning ,",
    "@xcite used hierarchical micro clustering and a clustering feature tree to obtain an approximate solution for support vector machines .    in this paper , we propose a general optimization algorithm based on clustering and data aggregation , and apply it to three common machine learning problems : least absolute deviation regression ( lad ) , svm , and semi - supervised support vector machines ( s@xmath0vm ) .",
    "the algorithm fits the iad framework , but has additional properties shown for the selected problems in this paper .",
    "the ability to report the optimality gap and monotonic convergence to global optimum are features of our algorithm for lad and svm , while our algorithm guarantees optimality for s@xmath0vm without monotonic convergence .",
    "our work for svm is distinguished from the work of yu _ et al _ @xcite , as we iteratively solve weighted svm and guarantee optimality , whereas they iteratively solve the standard unweighted svm and thus find only an approximate solution . on the other hand",
    ", it is distinguished from evgeniou and pontil @xcite , as our algorithm is iterative and guarantees global optimum , whereas they used clustering to preprocess data and obtain an approximate optimum .",
    "et al _ @xcite and doppa _ et al _ @xcite are different because we use the typical svm formulation within an iterative framework , whereas they propose an socp formulation based on chance constraints .",
    "our data disaggregation and cluster partitioning procedure is based on the optimality condition derived in this paper : relative location of the observations to the hyperplane ( for lad , svm , s@xmath0vm ) and labels of the observations ( for svm , s@xmath0vm ) .",
    "for example , in the svm case , if the separating hyperplane divides a cluster , the cluster is split .",
    "the condition for s@xmath0vm is even more involved since a single cluster can be split into four clusters . in the computational experiment",
    ", we show that our algorithm outperforms the current state - of - the - art algorithms when the data size is large .",
    "the implementation of our algorithms is based on in - memory processing , however the algorithms work also when data does not fit entirely in memory and has to be read from disk in batches .",
    "the algorithms never require the entire data set to be processed at once .",
    "our contributions are summarized as follows .    1 .",
    "we propose a clustering - based iterative algorithm to solve certain optimization problems , where an optimality condition is derived for each problem .",
    "the proposed algorithmic framework can be applied to other problems with certain structural properties ( even outside of machine learning ) .",
    "the algorithm is most beneficial when the time complexity of the original optimization problem is high .",
    "we present model specific disaggregation and cluster partitioning procedures based on the optimality condition , which is one of the keys for achieving optimality .",
    "3 .   for the selected machine learning problems ,",
    "i.e. , lad and svm , we show that the algorithm monotonically converges to a global optimum , while providing the optimality gap in each iteration . for s@xmath0vm",
    ", we provide the optimality condition .",
    "we present the algorithmic framework in section 2 and apply it to lad , svm , and s@xmath0vm in section 3 .",
    "a computational study is provided in section 4 , followed by a discussion on the characteristic of the algorithm and how to develop the algorithm for other problems in section 5 .",
    "we start by defining a few terms .",
    "a _ data matrix _ consists of _ entries _ ( rows ) and _ attributes _ ( columns ) .",
    "a machine learning optimization problem needs to be solved over the data matrix . when the entries of the original data are partitioned into several sub - groups , we call the sub - groups _ clusters _ and we require every entry of the original data to belong to exactly one cluster . based on the clusters , an _ aggregated entry _ is created for each cluster to represent the entries in the cluster .",
    "this aggregated entry ( usually the centroid ) represents one cluster , and all aggregated entries are considered in the same attribute space as the entries of the original data .",
    "the notion of the _ aggregated data _ refers to the collection of the aggregated entries .",
    "the _ aggregated problem _ is a similar optimization problem to the original optimization problem , based on the aggregated data instead of the original data . _",
    "declustering _ is the procedure of partitioning a cluster into two or more sub - clusters .",
    "we consider optimization problems of the type @xmath1 where @xmath2 is the number of entries of @xmath3 , @xmath4 is @xmath5 entry of @xmath3 , and arbitrary functions @xmath6 and @xmath7 are defined for every @xmath8 .",
    "one of the common features of such problems is that the data associated with @xmath3 is aggregated in practice and an approximate solution can be easily obtained .",
    "well - known problems such as lad , svm , and facility location fall into this category .",
    "the focus of our work is to design a computationally tractable algorithm that actually yields an optimal solution in a finite number of iterations .",
    "our algorithm needs four components tailored to a particular optimization problem or a machine learning model .    1 .   a _ definition of the aggregated data _",
    "is needed to create aggregated entries .",
    "2 .   _ clustering and declustering procedures ( and criteria ) _ are needed to cluster the entries of the original data and to decluster the existing clusters .",
    "an _ aggregated problem _",
    "( usually weighted version of the problem with the aggregated data ) should be defined .",
    "an _ optimality condition _ is needed to determine whether the current solution to the aggregated problem is optimal for the original problem .",
    "the overall algorithm is initialized by defining clusters of the original entries and creating aggregated data . in each iteration , the algorithm solves the aggregated problem .",
    "if the obtained solution to the aggregated problem satisfies the optimality condition , then the algorithm terminates with an optimal solution to the original problem .",
    "otherwise , the selected clusters are declustered based on the declustering criteria and new aggregated data is created .",
    "the algorithm continues until the optimality condition is satisfied .",
    "we refer to this algorithm , which is summarized in algorithm [ algo_aid ] , as _ aggregate and iterative disaggregate _",
    "observe that the algorithm is finite as we must stop when each cluster is an entry of the original data .",
    "in the computational experiment section , we show that in practice the algorithm terminates much earlier .",
    "create clusters and aggregated data * do * solve aggregated problem check optimality condition * if * optimality condition is violated * then * decluster the clusters and redefine aggregated data * while * optimality condition is not satisfied    in figure [ fig : ex_decluster ] , we illustrate the concept of the algorithm . in figure",
    "[ fig : ex_decluster_1 ] , small circles represent the entries of the original data .",
    "they are partitioned into three clusters ( large dotted circles ) , where the crosses represent the aggregated data ( three aggregated entries ) .",
    "we solve the aggregated problem with the three aggregated entries in figure [ fig : ex_decluster_1 ] .",
    "suppose that the aggregated solution does not satisfy the optimality condition and that the declustering criteria decide to partition all three clusters .",
    "in figure [ fig : ex_decluster_2 ] , each cluster in figure [ fig : ex_decluster_1 ] is split into two sub - clusters .",
    "suppose that the optimality condition is satisfied after several iterations .",
    "then , we terminate the algorithm with guaranteed optimality .",
    "figure [ fig : ex_decluster_3 ] represents possible final clusters after several iterations from figure [ fig : ex_decluster_2 ] .",
    "observe that some of the clusters in figure [ fig : ex_decluster_2 ] remain the same in figure [ fig : ex_decluster_3 ] , due to the fact that we selectively decluster .",
    "we use the following notation in subsequent sections .    1 .",
    "@xmath9 : index set of entries , where @xmath2 is the number of entries ( observations ) 2 .",
    "@xmath10 : index set of attributes , where @xmath11 is the number of attributes 3 .",
    "@xmath12 : index set of the clusters in iteration @xmath13 4 .",
    "@xmath14 : set of clusters in iteration @xmath13 , where @xmath15 is a subset of @xmath16 for any @xmath17 in @xmath18 5 .",
    "@xmath19 : last iteration of the algorithm when the optimality condition is satisfied",
    "the multiple linear least absolute deviation regression problem ( lad ) can be formulated as @xmath20 where @xmath21 \\in \\mathbb{r}^{n \\times m}$ ] is the explanatory variable data , @xmath22 \\in \\mathbb{r}^{n}$ ] is the response variable data , and @xmath23 is the decision variable . since the objective function of is the summation of functions over all @xmath24 in @xmath16 , lad fits , and we can use aid .    let us first define the clustering method .",
    "given target number of clusters @xmath25 , any clustering algorithm can be used to partition @xmath2 entries into @xmath25 initial clusters @xmath26 .",
    "given @xmath27 in iteration @xmath13 , for each @xmath28 , we generate aggregated data by      where @xmath32 and @xmath33 . to balance the clusters with different cardinalities , we give weight @xmath34 to the absolute error associated with @xmath15 .",
    "hence , we solve the aggregated problem @xmath35 observe that any feasible solution to is a feasible solution to .",
    "let @xmath36 be an optimal solution to . then",
    ", the objective function value of @xmath36 to with the original data is @xmath37        the above procedure keeps cluster @xmath15 if all original entries in the clusters are on the same side of the regression hyperplane .",
    "otherwise , the procedure splits @xmath15 into two clusters @xmath47 and @xmath48 , where the two clusters contain original entries on the one and the other side of the hyperplane .",
    "it is obvious that this rule implies a finite algorithm .    in figure",
    "[ fig : lad ] , we illustrate aid for lad . in figure",
    "[ fig : lad_1 ] , the small circles and crosses represent the original and aggregated entries , respectively , where the large dotted circles are the clusters associated with the aggregated entries .",
    "the straight line represents the regression line @xmath36 obtained from an optimal solution to . in figure",
    "[ fig : lad_2 ] , the shaded and empty circles are the original entries below and above the regression line , respectively .",
    "observe that two clusters have original entries below and above the regression line .",
    "hence , we decluster the two clusters based on the declustering criteria and obtain new clusters and aggregated data for the next iteration in figure [ fig : lad_3 ] .",
    "now we are ready to present the optimality condition and show that @xmath36 is an optimal solution to when the optimality condition is satisfied . the optimality condition presented in the following proposition",
    "is closely related to the clustering criteria .",
    "[ proposition_f_and_e_equivalent ] if @xmath41 for all @xmath42 have the same sign for all @xmath28 , then @xmath36 is an optimal solution to . in other words ,",
    "if all entries in @xmath15 are on the same side of the hyperplane defined by @xmath36 for all @xmath28 , then @xmath36 is an optimal solution to .",
    "further , @xmath49 ."
  ],
  "abstract_text": [
    "<S> we propose a clustering - based iterative algorithm to solve certain optimization problems in machine learning , where we start the algorithm by aggregating the original data , solving the problem on aggregated data , and then in subsequent steps gradually disaggregate the aggregated data . </S>",
    "<S> we apply the algorithm to common machine learning problems such as the least absolute deviation regression problem , support vector machines , and semi - supervised support vector machines . </S>",
    "<S> we derive model - specific data aggregation and disaggregation procedures . </S>",
    "<S> we also show optimality , convergence , and the optimality gap of the approximated solution in each iteration . a computational study is provided . </S>"
  ]
}