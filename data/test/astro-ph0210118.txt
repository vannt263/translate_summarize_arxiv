{
  "article_text": [
    "the primary purpose of the dls is to study the evolution of mass clustering over cosmic time using weak gravitational lensing .",
    "secondary goals include studying the relationship between mass and light through galaxy - galaxy lensing , galaxy and galaxy - quasar correlation functions , and optical searches for galaxy clusters ; time - domain astrophysics stemming from the transient search and followup ; and enabling a broad array of astrophysics by publicly releasing a large , uniform dataset of images and catalogs .",
    "the evolution of mass clustering over cosmic time is our primary goal because to date , most of what we know about the large - scale structure of the universe comes from the observed anisotropies in the cosmic microwave background ( cmb ) at a redshift @xmath2 , and from the distribution of galaxies at @xmath3 .",
    "the concordance model of cosmology adequately explains both , but to really test the paradigm , we must examine the evolution between these two extremes .",
    "weak gravitational lensing is our tool of choice because unlike many other tools , it is sensitive to all types of matter , luminous or dark , baryonic or not",
    ". furthermore , it provides constraints on cosmological parameters which are complementary to the cmb and to the expansion history of the universe as probed by type ia supernovae .",
    "for example , certain weak lensing statistics constrain @xmath4 very well while constraining @xmath5 only weakly .",
    "the cmb and supernovae results each constrain different combinations of @xmath4 and @xmath5 .",
    "the dls will test the validity of the foundation of the theoretical model by providing a third , precision measurement relying on different physics ",
    "weak gravitational lensing .",
    "the term `` weak lensing '' actually includes several independent measurements , all of which take advantage of the fact that shapes of distant galaxies are distorted when their light rays pass through intervening mass distributions on their way to us .",
    "the most straightforward measurement is that of counting mass concentrations above a certain mass threshold .",
    "( henceforth , we will call these `` clusters '' for simplicity , but it should be understood that weak lensing is sensitive to all mass concentrations , whether or not they contain light - emitting galaxies . ) the number of such clusters as a function of redshift between @xmath6 and @xmath7 is a sensitive function of @xmath4 and @xmath8 , the dark energy equation of state .    in a complementary measurement ,",
    "the power spectrum of mass fluctuations is derived from the statistics of the source shapes without the need to identify any particular mass concentration .",
    "this power spectrum is expected to grow with cosmic time in a certain way , so by measuring it at a series of different redshifts , the dls will provide sharp constraints on the models .    for visualization",
    ", the dls will produce maps of the mass distribution at a series of redshifts .",
    "assembled into a time series , these maps will show the growth of structure from @xmath9 , when the universe was about half its present age , to the present .",
    "these weak lensing projects imposed the following qualitative requirements on the data :    * imaging in 4 - 5 different filters with a wavelength baseline sufficient to derive photometric redshifts * deep enough to derive photometric redshifts for @xmath10 50100 galaxies arcmin@xmath11 * subarcsecond resolution , so that shape information about distant galaxies is retained even after dilution by seeing * at least 5 independent , well - separated fields , selected without regard to already - known structures , to get a handle on cosmic variance * in each field , coverage of an area of sky sufficient to include the largest structures expected    of course deep imaging is useful for a huge variety of other investigations , so we resolved to retain as much general utilty as possible while optimizing the survey for lensing . for example , we chose widely used filters ( described below ) rather than adopting a custom set .    at the same time , we realized that the data would be useful for a completely different application .",
    "in addition to coadding multiple exposures of the same field , we could subtract one epoch from another and find transient phenomena including moving objects in the solar system , bursting objects such as supernovae and possibly optical counterparts to gamma - ray bursts , and variable stars and agn .",
    "image subtraction , or difference image analysis ( dia ) , has been used quite successfully by supernova search groups . for that application ,",
    "however , all other types of transients are nuisances , and their elimination from the sample is part of the art of the supernova search .",
    "ironically , there are other groups using the same 4-meter telescopes and wide - field imagers , often during the same dark run , to look for asteroids while discarding supernovae ! we decided to try a new approach , in which we would do the difference image analysis while observing and release information about all types of transients on the web the same night , so that others could follow up any subsample of their choosing .",
    "this attempt to do parallel astrophysics is in many ways a precursor of the much larger survey projects now in the planning stage which seek to further open the time domain for deep imaging , and we will discuss some of the lessons learned .",
    "we chose the kitt peak national observatory ( kpno ) and cerro tololo inter - american observatory ( ctio ) 4-m telescopes for their good combination of aperture and field of view .",
    "each telescope is equipped with an 8k @xmath12 8k mosaic imager ( muller _ et al . _",
    "1998 ) , providing a 35@xmath13  field of view with 0.25@xmath14  pxels and very small gaps between three - edge - buttable devices .",
    "a further strength is that with some observing with each of two similar setups , we can provide a check on systematic errors due to optical distortions , seeing , and the like , without sacrificing homegeneity of the dataset in terms of depth , sampling , and so on .",
    "we set our goal for photometric redshift accuracy at 20% in @xmath15 .",
    "this is quite modest in comparison to the state of the art , but we do not need better accuracy . a bigger contribution to the error on a lensing measurement from any one galaxy is shape noise , the noise stemming from the random unlensed orientations of galaxies .",
    "thus we do not need great redshift accuracy on each galaxy , but we do need the redshifts to be unbiased in the mean .    to keep the total amount of telescope time reasonable , we wanted to use the absolute minimum number of filters necessary for estimating photometric redshifts to this accuracy .",
    "simulations showed that four was the minimum .",
    "we chose a wavelength baseline from harris @xmath16 to sloan @xmath17 , which is the maximum possible without taking a large efficiency efficiency hit from the prime focus correctors in @xmath18 or from a switch to infrared detectors on the other end . between these extremes , we chose harris @xmath19 and @xmath20 , where throughput is maximized ; simulations showed that photometric redshift performance did not depend critically on these choices .    our depth goal is a 1-@xmath21 sky noise of 29 mag arcsec@xmath11 in @xmath22 and 28 in @xmath17 , which required total exposure times of 12 ksec in @xmath23 and 18 ksec in @xmath20 according to exposure time calculators , for a total of 54 ksec exposure on each patch of sky ( see below for discussion ) .",
    "we divided this total time into 20 exposures of 600 ( 900 ) seconds each in @xmath23 ( @xmath20 ) .",
    "twenty exposures was a compromise between efficiency ( given a read time of @xmath24 seconds ) and the need to take many dithered exposures to obtain good dark - sky flats .",
    "the metric size of expected mass structures , and the redshift at which they most effectively lens faint background galaxies , sets the angular scale for a field . in order to study the mass clustering on linear scales ( out to @xmath25 mpc at z=0.3 ) , we must image @xmath26 regions of the sky . with mosaic",
    "providing a 40@xmath13field of view after dithering , we assemble each 2@xmath27 field from a 3@xmath123 array of mosaic - size subfields .",
    "thus each field requires a total exposure time of 9@xmath1254 ksec , or 486 ksec",
    ".    the number of such fields should be as large as possible to get a handle on cosmic variance . to fit the entire program into a reasonable amount of telescope time ,",
    "we decided on five fields observed from scratch plus two others in which we can build on a previous survey , the noao deep wide - field survey ( ndwfs ) .",
    "the ndwfs is a @xmath28 survey of two 9 deg@xmath29 fields with partial @xmath30 coverage as well .",
    "we decided to locate a dls field within the infrared - covered area of each of these fields , and simply add additional @xmath20 imaging ( 10 exposures of 900 seconds on each patch of sky ) to bring it to the same @xmath20 depth as the remainder of the dls .",
    "thus the total exposure time for piggybacking on the ndwfs fields is 9ksec times 18 subfields , or 162 ksec .",
    "the total exposure time for the entire survey is then 2592 ksec .",
    "the number of nights is then determined by the observing efficiency .",
    "the read time of the mosaic was @xmath10 140 seconds at the start of the survey , implying about 82% efficiency if the observers are able to focus and take standard star images during twilight .",
    "the read time was slated to decrease significantly after an upgrade from 8 to 16-amplifier readout , and so we felt 82% efficiency was a reasonable goal for the entire survey ( see below for discussion ) .",
    "thus we needed 3 msec of time , which is 86 10-hour nights .",
    "we split this between kpno and ctio by assigning two new and one ndwfs field to kpno and three new and one ndwfs field to ctio .",
    "the first consideration in field selection was availability of deep spectroscopy for calibrating photometric redshifts in at least one field .",
    "after choosing that one field , the gross location of the other fields fell into place through traditional considerations such as avoiding the plane of the galaxy and spacing the fields to share nights smoothly .",
    "the precise location was then chosen to avoid bright stars as much as possible .",
    "fields were not chosen with regard to known structures such as clusters .",
    "we chose the caltech faint galaxy redshift survey ( cohen _ et al . _  1999 ) for our deep spectroscopy , so we centered our first field ( f1 ) in the north at 00:53:25.3 + 12:33:55 ( all coordinates are j2000 ) . the second field ( f2 )",
    "must then be @xmath105 hours earlier or later to be able to share nights with this field .",
    "earlier is impractical because it would require summer observing during the monsoon season at kitt peak .",
    "a later field would have to be further north to avoid the plane of the galaxy .",
    "we chose 09:18:00 + 30:00:00 because it had few bright stars and low extinction .    in the south , we wanted our fields complementary in ra to those in the north , so that we would not have simultaneous observing runs in the same month .",
    "it was difficult to find three southern fields for the 515 h range evenly spaced in ra , while avoiding the galaxy .",
    "the middle of the three ( f4 ) was fixed at 10:52 -05:00 because redshifts would be available from the 2df galaxy redshift survey ( 2dfgrs , colless _",
    "et al . _",
    "the 2dfgrs covers vastly more area , but this happens to be a region of low extinction .",
    "( the 2dfgrs goes to @xmath31 , and thus is of much less importance than the cfgrs in calibrating our photometric redshifts , which we want to push to @xmath32 .",
    "however , we felt that any additional spectroscopy would help . ) the late field ( f5 ) was chosen to be very near the ecliptic to help the hunt for kuiper belt objects ( kbos ) ; 13:55 -10:00 was chosen for low extinction , and partial overlap with the las campanas distant cluster survey ( lcdcs ; gonzalez _ et al . _",
    "the early field ( f3 ) was the most difficult to place because it was up against the galaxy .",
    "we chose 05:20 -49:00 , which at a galactic latitude of -35 is the closest field to the plane .",
    "note that proximity to the plane is really shorthand for two different things : galactic extinction , which we extracted from the maps of schlegel et al , and bright stars , for which we consulted star atlases .",
    "stars in general are actually helpful for lensing , because they reveal the point - spread function ( psf ) , but they become useless when saturated ( at around 18.5@xmath33 in most of the @xmath20 exposures ) , and in fact become destructive when they are bright enough to prohibit detection and evaluation of galaxies projected nearby ( a tenth magnitude star causes a significant amount of lost area ) .",
    "these two aspects are not perfectly correlated , so while f3 is closest to the plane and has the most bright stars , it does not have the greatest extinction .    the ndwfs ( f6 and f7 ) is still completing their observations ,",
    "so we decided to postpone exact location of our fields within theirs until late in our survey , when we should know where the imaging quality and wavelength coverage are best .    [ cols=\"^,^,^,^,^\",options=\"header \" , ]",
    "subarcsecond resolution is common for individual exposures on these telescopes , but achieving this resolution in the final product of a large survey is another matter , as a large range of seeing conditions is to be expected .",
    "we address this problem by observing in @xmath20 when the seeing conditions are 0.9@xmath14  or better , and switching to @xmath16 , @xmath19 , or @xmath17 when the seeing becomes worse than 0.9@xmath14 .",
    "thus the final @xmath20 images are guaranteed to be 0.9@xmath14  or better .",
    "we will then make the weak lensing shape measurements in @xmath20 and use the other filters only for color information .",
    "this seeing dependence is the main element in deciding what to observe when at the telescope .",
    "the next most important consideration is moon ",
    "when there is significant moon illumination , we observe in @xmath17 , which is the least sensitive to such illumination .",
    "next , there is observing efficiency .",
    "we can offset , but do not wish to slew while reading out , so the most efficient tactic is to take as many consecutive dithered exposures as possible on a given subfield .",
    "dithers are up to @xmath34200@xmath14 , or half the width of a 2k@xmath124k device , to insure good superflats .",
    "although we could change filters rapidly while reading out , the filter is mostly dictated by atmospheric conditions as described above .",
    "thus we settled on a pattern of five dithered exposures in any given subfield / filter combination before slewing to another subfield ( and possible changing filters if conditions dictate ) .",
    "this still leaves several degrees of freedom , which we grant to the transient search .",
    "we wish to sample a range of timescales , from minutes to years . as described above",
    ", we spend of order one hour on a given subfield / filter combination .",
    "this timescale is perfect for discovering inner solar system objects ( in fact it is used by the supernova searches to eliminate asteroids ) .",
    "it is also largely unexplored for more exotic bursting objects ; supernova searches do take exposures on a given field one hour apart , but with only two data points , anything that does not behave as expected is simply thrown out .    the next priority is to capture the one - month timescale , which has a known population of interesting transients : supernovae .",
    "we schedule runs a month apart for sensitivity to supernovae , and make every attempt to observe subfields and filters which were observed the previous month . at kpno , we have two runs each year separated by a month , and at ctio , three runs each separated by a month , barring scheduling difficulties .    finally , we also have day ( suitable for searches for rans - neptunian objects and possible optical afterglows of gamma - ray bursts ) and year ( suitable for supernovae and agn ) timescales .",
    "we have runs of 46 nights each , so in the latter half of a run the priority is to revisit a subfield / filter combination done in the first half of the run . and one - year timescales are involved because we typically do not finish all twenty exposures in a given subfield / filter combination in a single observing season .",
    "the following steps are performed at the end of a run using iraf s _ mscred _ package :    * astrometric solution : each image comes is matched to the usno - a2 star catalog using the _ msccmatch _ task ) .",
    "after this step , the uncertainty in the global ra , dec coordinate system relative to the usno - a2 is less than 0.01@xmath14 .",
    "the rms error in each object s position is @xmath100.3@xmath14 , which is mostly limited by the a-2 catalog s internal accuracy . *",
    "crosstalk correction : we use the noao - determined corrections to mitigate the crosstalk which occurs as multiple amplifiers are read in parallel . this step and the overscan , zero , and flattening",
    "are done with the _ ccdproc _ task .",
    "* overscan subtraction * zero subtraction * flatfield pupil removal : in the kpno @xmath17 data , a ghost image of the pupil appears as an additive effect in the dome flats .",
    "we use the _ rmpupil _ task to remove it .",
    "* dome - flat correction * sky - flat correction : the sky flat is made from a stack of all the deep dome - flattened images in a given filter from the entire run .",
    "* pupil removal ( kpno @xmath17 data only ) * defringing ( @xmath17 data only ; _ rmfringe _ task )    these are performed in the traditional way , by manually running the tasks and inspecting the results . we feel that our mission is sufficiently small that every exposure should be inspected . at the same time , the tools to make an effective pipeline from existing iraf tasks were not in place when we started the survey . with the emergence of tools like pyraf , we leave open the possibility that a final re - reduction of all the inspected data may be done with an automated pipeline .",
    "conceptually , the next step is the lensing - specific step of making the psf round . because this is specific to lensing , this is where our custom software takes over ; we view the flatfielded data as `` raw '' data for our lensing pipeline .",
    "figure  [ fig - circ ] illustrates the problem : an anisotropic psf mimics a gravitational lensing effect in many ways , with shapes of objects correlated at least locally .",
    "psfs on real telescopes tend to be @xmath35 elliptical , which is much larger than the lensing signal we are looking for , and thus must be eradicated as much as possible .",
    "this is done by convolving the image with a kernel with ellipticity components opposite to that of the psf ( fischer & tyson 1997 ) . because the psf is position - dependent",
    ", the kernel must be also .",
    "however , this correction can not be applied immediately , as each image must be remapped onto a common coordinate system ( optical distortions reach 30@xmath14  at the corners of the mosaic ) , and this remapping itself changes the psf shape .",
    "therefore the software must catalog each image ; set up a coordinate system for the final image ; match catalogs and use the matches to determine the transformations to the final coordinate system , rejecting outliers ; select stars ; compute what the psf shape will be after remapping ( to avoid the computational expense of writing out intermediate images ) ; fit for the spatial variation of the stellar shapes , rejecting outliers ; determine the photometric scaling between images from the matched catalogs ; and combine the images using all these transformations .",
    "most of these tasks are implemented as standalone programs written in c , and they are executed in the proper order by a perl script , dlsmake , which also checks the return values , and often the output data , of the tasks for potential errors .",
    "human intervention is required only for inspection of the star selection , which sometimes goes awry .",
    "there is a graphical user interface which displays the magnitude - size plane and allows the user to adjust the stellar locus by dragging a box over it .",
    "precise registration is crucial , as a small registration error can mimic a shear .",
    "the rms departure of object centroids from the coordinate fit prediction is @xmath10 0.1 pixel , or 0.025@xmath14 . to nullify any psf shape systematics which might have been introduced by this",
    ", we select a new sample of stars from the coadded image , and convolve it with the appropriate spatially varying kernel .",
    "we make extensive use of existing tools . for example",
    ", we use sextractor ( bertin & arnouts 1996 ) to make quick catalogs of each input image for the pipeline , even though we have not settled on it for the science - grade cataloging of the final images . at the same time",
    ", we ask sextractor to produce a sky - subtracted version of each input image .",
    "we use these sky - subtracted versions in the combine , so that chip - to - chip sky variations do not produce artifacts in the final image .",
    "these are the only intermediate images written to disk ; the resampled and convolved versions reside only temporarily in buffers in the combine program , dlscombine .    throughout , attention is paid to masking .",
    "in addition to straightforward bad - pixel masking , we must deal with plumes of scattered light from very bright stars off the edge of the field in some individual exposures .",
    "we use ds9 to draw regions around these and save a link to the region file in the image header .",
    "dlsmake then makes a custom bad - pixel mask on the fly , but applies it only where relevant .",
    "for example , the region is still examined for sources which will help determine the coordinate transformation or the spatial variation of the psf ; otherwise an image could not be processed at all if a substantial part of it must be masked .",
    "in a separate pipeline , we use difference image analysis to subtract off the temporally constant parts of our images and reveal the variability .",
    "this pipeline is now managed using the opus data processing environment ( miller & rose 2001 and references therein ) , and runs while observing .",
    "thus it must use afternoon dome flats instead of sky flats . the difference in image quality",
    "is not great , but it is the main reason that these reductions are carried out separately from the deep imaging reductions .",
    "we use a modified version of the alard ( 2000 ) algorithm to register both the height and shape of the individual point - spread functions .",
    "we difference the individual dithers for short timescale events down to @xmath36rd magnitude , and co - add the images for comparison to a previous co - add for sensitivity down to @xmath37th mag .",
    "detection and some winnowing of spurious events ( cosmic rays , bleeds from bright stars ) is done automatically , but the observer is required to do the final classification .",
    "the main motivation here is that events are posted to the web as soon as they are approved , so human - caliber quality assurance is more important than maintaining strict objectivity , even if that makes efficiency calculations difficult .",
    "the website ( http://dls / bell - labs.com / transients.html ) lists positions , dates , magnitudes , classifications , and has time series images similar to the figures shown here .",
    "we find several hundred transients per run , so entries are color - coded according to classification . for each moving object ,",
    "the angular velocity is listed , and an ephemeris is available . for stationary objects ,",
    "a finding chart is available .",
    "figure [ fig - trans219 ] shows how well the image subtraction works , revealing a supernova otherwise enmeshed in the isophotes of an extended host galaxy . over the couse of the survey ,",
    "we have detected nearly 100 supernova candidates , and a similar number of transients close enough to the nuclei of galaxies to potentially be agns . over the last year",
    "we have made a concerted effort to reacquire supernova candidates on subsequent nights ( possibly in other filters ) to meet the requirements of the international astronomical union for designating official supernova .",
    "thus , 12 out of 14 of our designated supernova have come in the past year .",
    "we hope to coordinate and distribute upcoming supernovae to next - generation supernova projects .",
    "we have had suceess obtaining follow - up observations for the interesting cases where there is no apparent host for the transient . in the case of the transient in figure [ fig - trans139 ] , from jan 2002 , we were able to obtain both photometric and spectroscopic follow - up observations after disseminating information on this event over the variable star network , and expect to further our response by using the astronomer s telegram .    in the most interesting of the classes of transients , rapidly varying phenomena",
    ", we have a very small crop of candidates , none of which have proved to be extragalactic .",
    "figure  [ fig - trans337 ] shows an exaple , in which a burst was detected over what appears to be a background galaxy .",
    "this burst appeared in the third of five consecutive 600-second exposures , then faded rapidly .",
    "we immediately obtained follow - up spectroscopy on the las campanas observatory 6.5-m baade telescope , showing the spectral signatures of an m4 dwarf with h-@xmath38 in emission  a foreground galactic flare star  on top of a faint blue background galaxy .",
    "exotic - looking events with low event rates are not necessarily astrophysically interesting !",
    "we also have several thousand temporal sequences of solar system objects down to @xmath39rd magnitude .",
    "preliminary analysis of this sample confirms the finding by izevic _",
    "et al . _  ( 2001 ) of a break in the power - law slope of the size distribution .",
    "the minor planet center has awarded preliminary designations to approximately 200 of these objects .",
    "observing started in the fall of 1999 .",
    "after three observing seasons , we have completed 7 of the 63 subfields .",
    "the remaining coverage is fairly random due to the twin constraints of switching to @xmath20 in good seeing and maximizing the transient productivity .",
    "thus most subfields are not yet useful scientifically , because the coverage is too shallow and/or limited to a few filters . in the ndwfs fields , the @xmath20 data upon which we intended to piggyback",
    "is unsuitable due to poor seeing ( lensing is not one of the drivers of that survey ) , so those fields are very far behind .",
    "the full status of each subfield and filter is best visualized at http://dls.bell-labs.com/progress.html , which is updated after each run .",
    "we released deep images and catalogs of the first two completed subfields in august 2001 , and will release five more in august / september 2002 .",
    "refer to the dls website http://dls.bell-labs.com for the latest details .",
    "we found that the night sky was significantly brighter in @xmath20 and @xmath17 than assumed by the exposure time calculators , because we started observing near solar maximum while the calculators had been calibrated closer to solar minimum .",
    "we are maintaining the planned exposure times while assessing the impact on depth .",
    "it appears that we will still reach our goal of 50100 galaxies arcmin@xmath11 with photometric redshifts and shape measurements ; we are getting around 60 in the few subfields which are up to full depth .",
    "of course , there is a tradeoff between density of sources and accuracy of redshifts , and that is the subject of continuing refinement .",
    "continuing refinement should not obscure the overall good performance of the photometric redshifts .",
    "figure  [ fig - zphot ] shows that the rms error is less than 20% in @xmath15 , with very little bias ( @xmath401% ) , using the template - fitting method plus a magnitude prior .    the first lensing results , focusing on specific subfields rather than large areas , are coming out very soon .",
    "the first is a paper on the discovery and tomography of a cluster at @xmath41 ( wittman _ et al._2002 ) .",
    "this is only the second cluster discovered via weak lensing , and its redshift is more than twice that of the first such cluster .",
    "this bodes well for cluster searches over the entire dls area , and the tomography confirms that the photometric redshifts are useful .",
    "several other clusters have been discovered ; two are already - known abell clusters .",
    "careful attention is paid to quality assurance in the cataloging of the science - grade data , but we have found that there is no substitute for trying to do science with it .",
    "quality assurance as a self - contained task is bound to fail , no matter how assiduously carried out . to that end , work is starting on many different ways of stressing the data , including cosmic shear , angular correlation functions , searches for cool stars , quasar - mass correlations , and using the photometric redshifts to assess intrinsic ellipticity correlations .",
    "we have already gone through several iterations of refining our science - grade cataloging procedure .",
    "that procedure is still preliminary , and the final version will be described in future papers .",
    "we have learned many lessons , but only a few are applicable to future large surveys with dedicated instruments such as lsst ( tyson _ et al._2002 ; angel _ et al . _  2001",
    "; see also these proceedings ) .",
    "first , take a long - term view .",
    "this is obvious , but it is difficult to integrate into every aspect of the survey and every tool used to design the survey . a good example is the exposure time calculator having been calibrated at solar minimum . to catch this kind of error , a long - term mentality must be very deeply ingrained .",
    "second , every conceivable data quality check must be done automatically .",
    "our data volume was small enough to inspect manually , but we still encountered a variety of failure modes richer than our imaginations .",
    "each instrument has its own peculiarities , so it is vital to have some early data _ from the instrument _ to help refine the software .",
    "simulated data will not do .",
    "another aspect which simply will not scale is our manual classification of transients .",
    "automatic classification is a problem which has not been tackled because it hasnt been necessary , and therefore we really do nt know how difficult it will be .",
    "thus , work should be started now , well in advance of any flood of data , and we are planning to do so in the next year .",
    "the example of the flare star in front of a faint galaxy shows that exploring low - event - rate physics is complicated by all kinds of other low - event - rate processes .",
    "finally , we have been very successful at finding transients , but followup has been extremely difficult to coordinate . the problem scales roughly as the factorial of the number of different observatories involved .",
    "furthermore , it is easy to detect transients which are too faint to reach with spectroscopy",
    ". therefore , time - domain experiments like lsst must design in as much self - followup as possible by revisiting fields at a useful cadence .",
    "one thing that would help greatly is obtaining colors right away , which unfortunately was nt possible given the design of the dls .",
    "simply finding transients turned out to be the easy part ; the challenge is in followup and in quickly measuring features which enable effective followup .",
    "we thank the kpno and ctio staffs for their invaluable assistance .",
    "noao is operated by the association of universities for research in astronomy ( aura ) , inc . under cooperative agreement with the national science foundation .",
    "tyson , t. , wittman , d. , hennawi , j. , & spergel , d.  2002 , american physical society , april meeting , jointly sponsored with the high energy astrophysics division ( head ) of the american astronomical society april 20 - 23 , 2002 albuquerque convention center albuquerque , new mexico meeting i d : apr02 , abstract # y6.004 , 6004 , 2002"
  ],
  "abstract_text": [
    "<S> the deep lens survey ( dls ) is a deep @xmath0 imaging survey of seven @xmath1 degree fields , with all data to be made public . </S>",
    "<S> the primary scientific driver is weak gravitational lensing , but the survey is also designed to enable a wide array of other astrophysical investigations . </S>",
    "<S> a unique feature of this survey is the search for transient phenomena . </S>",
    "<S> we subtract multiple exposures of a field , detect differences , classify , and release transients on the web within about an hour of observation . here </S>",
    "<S> we summarize the scientific goals of the dls , field and filter selection , observing techniques and current status , data reduction , data products and release , and transient detections . </S>",
    "<S> finally , we discuss some lessons which might apply to future large surveys such as lsst . </S>"
  ]
}