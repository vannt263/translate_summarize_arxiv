{
  "article_text": [
    "hidden variable resultant methods are a popular class of algorithms for global multidimensional rootfinding  @xcite .",
    "they compute all the solutions to zero - dimensional polynomial systems of the form : @xmath0 where @xmath1 and @xmath2 are polynomials in @xmath3 with complex coefficients .",
    "mathematically , they are based on an elegant idea that converts the multidimensional rootfinding problem in   into one or more eigenvalue problems  @xcite . at",
    "first these methods appear to be a practitioner s dream as a difficult rootfinding problem is solved by the robust qr or qz algorithm .",
    "desirably , these methods have received considerable research attention from the scientific computing community  @xcite .    despite this significant interest ,",
    "hidden variable resultant methods are notoriously difficult , if not impossible , to make numerically robust .",
    "most naive implementations will introduce unwanted spurious solutions , compute roots inaccurately , and unpredictably miss zeros  @xcite .",
    "spurious solutions can be removed by manually checking that all the solutions satisfy  , inaccurate roots can usually be polished by newton s method , but entirely missing a zero is detrimental to a global rootfinding algorithm .",
    "the higher the polynomial degree @xmath4 and the dimension @xmath5 , the more pronounced the numerical difficulties become .",
    "though our conditioning bounds do hold for small @xmath4 and @xmath5 , this paper deals with a worst - case analysis .",
    "hence , our conclusions are not inconsistent with the observation that ( at least when @xmath4 and @xmath5 are small ) resultant methods can work very well in practice for some problems . when @xmath6 and real finite solutions are of interest , a careful combination of domain subdivision , regularization , and local refinement has been successfully used together with the cayley resultant ( also known as the dixon or bzout resultant ) for large @xmath4  @xcite .",
    "this is the algorithm employed by chebfun for bivariate global rootfinding  @xcite .",
    "moreover , for @xmath7 , randomization techniques and the qz algorithm have been combined fruitfully with the macaulay resultant  @xcite .",
    "there are also many other ideas  @xcite .",
    "however , these techniques seem to be less successful in higher dimensions .    in this paper",
    ", we show that any plain vanilla hidden variable resultant method based on the cayley or sylvester matrix is a numerically unstable algorithm for solving a polynomial system . in particular , we show that the hidden variable resultant method based on the cayley resultant matrix is numerically unstable for multidimensional rootfinding with a factor that grows exponentially with @xmath5 .",
    "we show that for @xmath7 the sylvester matrix leads to a hidden variable resultant method that can also square the conditioning of a root .",
    "we believe that this numerical instability has not been analyzed before because there are at least two other sources of numerical issues : ( 1 ) the hidden variable resultant method is usually employed with the monomial polynomial basis , which can be devastating in practice when @xmath4 is large , and ( 2 ) some rootfinding problems have inherently ill - conditioned zeros and hence , one does not always expect accurate solutions .",
    "practitioners can sometimes overcome ( 1 ) by representing the polynomials @xmath2 in another degree - graded polynomial basis for @xmath8 $ ] is degree - graded if the degree of @xmath9 is exactly @xmath10 for @xmath11 . ]",
    "however , the numerically instability that we identify can be observed even when the roots are well - conditioned and for degree - graded polynomial basis ( which includes the monomial , chebyshev , and legendre bases ) .",
    "we focus on the purely numerical , as opposed to symbolic , algorithm .",
    "we take the view that every arithmetic operation is performed in finite precision .",
    "there are many other rootfinders that either employ only symbolic manipulations  @xcite or some kind of symbolic - numerical hybrid  @xcite .",
    "similar careful symbolic manipulations may be useful in overcoming the numerical instability that we identify . for example",
    ", it may be possible to somehow transform the polynomial system   into one that the resultant method treats in a numerical stable manner .",
    "this paper may be considered as a bearer of bad news .",
    "yet , we take the opposite and more optimistic view",
    ". we are intrigued by the potential positive impact this paper could have on rootfinders based on resultants since once a numerical instability has been identified the community is much better placed to circumvent the issue .",
    "we use the following notation .",
    "the space of univariate polynomials with complex coefficients of degree at most @xmath4 is denoted by @xmath8 $ ] , the space of @xmath5-variate polynomials of maximal degree @xmath4 in the variables @xmath3 is denoted by @xmath12 $ ] , and if @xmath13 is a vector space then the cartesian product space @xmath14 ( d - times ) is denoted by @xmath15 .",
    "finally , we use @xmath16 to be the vectorization of the matrix or tensor @xmath17 to a column vector ( this is equivalent to ` v ( : ) ` in matlab ) .",
    "our setup is as follows .",
    "first , we suppose that a degree - graded polynomial basis for @xmath8 $ ] , denoted by @xmath18 , has been selected .",
    "all polynomials will be represented using this basis .",
    "second , a region of interest @xmath19 is chosen such that @xmath20 , where @xmath20 is the tensor - product domain @xmath21 ( @xmath5 times ) , contains all the roots that would like to be computed accurately .",
    "the domain @xmath22 can be a real interval or a bounded region in the complex plane . throughout",
    ", we suppose that @xmath23 for @xmath11 , which is a very natural normalization .",
    "our two main results are in theorem  [ thm : condtheorem ] and theorem  [ thm : condsylvester ] .",
    "together they show that there exist @xmath2 in   such that @xmath24 where @xmath25 is either the cayley ( for any @xmath26 ) or sylvester ( for @xmath6 ) resultant matrix .",
    "such a result shows that in the absolute sense the eigenvalue problem employed by these two resultant - based methods can be significantly more sensitive to perturbations than the corresponding root . together with results about relative conditioning , we conclude that these rootfinders are numerically unstable ( see section  [ sec : absrel ] ) .    in the next section",
    "we first introduce multidimensional resultants and describe hidden variable resultant methods for rootfinding . in section  [ sec : cayley ] we show that the hidden variable resultant method based on the cayley resultant suffers from numerical instability and in section  [ sec : sylvester ] we show that the sylvester matrix has a similar instability for @xmath7 . in section  [ sec : absrel ] we explain why our absolute conditioning analysis leads to an additional twist when considering relative conditioning .",
    "finally , in section  [ sec : futurework ] we present a brief outlook on future directions .",
    "this paper requires some knowledge of multidimensional rootfinding , hidden variable resultant methods , matrix polynomials , and conditioning analysis . in this section",
    "we briefly review this material .",
    "global rootfinding in high dimensions can be a difficult and computationally expensive task . here , we are concerned with the easiest situation where   has only simple finite roots .",
    "let @xmath27 be a solution to the zero - dimensional polynomial system  .",
    "then , we say that @xmath28 is a simple root of   if the jacobian matrix @xmath29 is invertible , where @xmath30 \\vdots & \\ddots & \\vdots \\\\[5pt ] \\frac{\\partial p_d}{\\partial x_1}(\\underline{x}^{\\ast } ) & \\ldots & \\frac{\\partial p_d}{\\partial x_d}(\\underline{x}^{\\ast})\\end{bmatrix}\\in\\mathbb{c}^{d\\times d}.    \\label{eq : jacobian}\\ ] ]    if @xmath29 is not invertible then the problem is ill - conditioned , and a numerically stable algorithm working in finite precision arithmetic may introduce a spurious solution or may miss a non - simple root entirely .",
    "we will consider the roots of   that are well - conditioned ( see proposition  [ def : conditioning ] ) , finite , and simple .",
    "our focus is on the accuracy of hidden variable resultant methods , not computational speed . in general",
    ", one can not expect to have a `` fast '' algorithm for global multidimensional rootfinding .",
    "this is because the zero - dimensional polynomial system in   can potentially have a large number of solutions .",
    "to say exactly how many solutions there can be , we first must be more precise about what we mean by the degree of a polynomial in the multidimensional setting  @xcite .",
    "a @xmath5-variate polynomial @xmath31 has total degree @xmath32 if @xmath33 for some tensor @xmath34 .",
    "it is of total degree @xmath4 if one of the terms @xmath35 with @xmath36 is nonzero .",
    "moreover , @xmath31 has maximal degree @xmath32 if @xmath37 for some tensor @xmath34 indexed by @xmath38 .",
    "it is of maximal degree @xmath4 if one of the terms @xmath35 with @xmath39 is nonzero .",
    "bzout s lemma says that if   involves polynomials of total degree @xmath4 , then there are at most @xmath40 solutions  ( * ? ? ?",
    "* chap .  3 ) . for polynomials of maximal degree",
    "we have the following analogous bound ( see also  ( * ? ? ?",
    "* thm .  5.1 ) ) .    the zero - dimensional polynomial system in  , where @xmath2",
    "are of maximal degree @xmath4 , can have at most @xmath41 solutions .",
    "[ lem : maxsolutions ]    this is the multihomogeneous bzout bound , see  ( * ? ? ?",
    "8.5.2 ) . for polynomials of maximal degree @xmath4 the bound",
    "is simply @xmath42 , where @xmath43 is the @xmath44 identity matrix and @xmath45 is the permanent of @xmath34 .",
    "we have selected maximal degree , rather than total degree , because maximal degree polynomials are more closely linked to tensor - product constructions and make later analysis in the multidimensional setting easier .",
    "we do not know how to repeat the same analysis when the polynomials are represented in a sparse basis set .",
    "suppose that the polynomial system   contains polynomials of maximal degree @xmath4 .",
    "then , to verify that @xmath46 candidate points are solutions the polynomials @xmath2 must be evaluated , costing @xmath47 operations .",
    "thus , the optimal worst - case complexity is @xmath47 . for many applications global rootfinding",
    "is computationally unfeasible and instead local methods such as newton s method and homotopy continuation methods  @xcite can be employed to compute a subset of the solutions . despite the fact that global multidimensional rootfinding is a computationally intensive task , we still desire a numerically stable algorithm .",
    "a survey of numerical rootfinders is given in  ( * ? ? ?",
    "* chap .  5 ) .",
    "when @xmath48 , global numerical rootfinding can be done satisfactorily even with polynomial degrees in the thousands .",
    "excellent numerical and stable rootfinders can be built using domain subdivision  @xcite , eigenproblems with colleague or comrade matrices  @xcite , and a careful treatment of dynamic range issues  @xcite .",
    "the first step of a hidden variable resultant method is to select a variable , say @xmath49 , and regard the @xmath5-variate polynomials @xmath2 in   as polynomials in @xmath50 with complex coefficients that depend on @xmath49 .",
    "that is , we `` hide '' @xmath49 by rewriting @xmath51 for @xmath52 as @xmath53(x_1,\\ldots , x_{d-1 } ) = \\sum_{i_1,\\ldots , i_{d-1}=0}^{n } c_{i_1,\\ldots , i_{d-1}}(x_d)\\prod_{s=1}^{d-1}\\phi_{i_s}(x_s),\\ ] ] where @xmath54 is a degree - graded polynomial basis for @xmath8 $ ] .",
    "this new point of view rewrites   as a system of @xmath5 polynomials in @xmath55 variables .",
    "we now seek all the @xmath56 such that @xmath57,\\ldots , p_d[x_d^{\\ast}]$ ] have a common root in @xmath58 .",
    "algebraically , this can be achieved by using a multidimensional resultant  ( * ? ? ?",
    "let @xmath1 and @xmath59 .",
    "a functional @xmath60)^{d}\\rightarrow\\mathbb{c}$ ] is a multidimensional resultant if , for any set of @xmath5 polynomials @xmath61 $ ] , @xmath62 is a polynomial in the coefficients of @xmath63 and @xmath64 if and only if there exists an @xmath65 such that @xmath66 for @xmath52 , where @xmath67 denotes the extended complex plane .",
    "[ def : resultantmulti ]    definition  [ def : resultantmulti ] defines @xmath68 up to a nonzero multiplicative constant  ( * ? ? ?",
    "1.6.1 ) . in the monomial basis",
    "it is standard to normalize @xmath68 so that @xmath69  ( * ? ? ?",
    "1.6.1(ii ) ) . for nonmonomial bases",
    ", we are not aware of any standard normalization .",
    "assuming   only has finite solutions , if @xmath68 is a multidimensional resultant then for any @xmath56 we have @xmath70,\\ldots , p_d[x_d^{\\ast}])\\!=\\!0 \\!\\quad \\!\\longleftrightarrow\\!\\quad\\",
    "! \\exists ( x_1^{\\ast},\\ldots , x_{d-1}^{\\ast})\\in\\mathbb{c}^{d-1}\\text { s.t . } p_1(\\underline{x}^{\\ast})\\!=\\!\\cdots\\!=\\!p_d(\\underline{x}^{\\ast})\\!=\\!0,\\ ] ] where @xmath71 .",
    "thus , we can calculate the @xmath5th component of all the solutions of interest by computing the roots of @xmath72,\\ldots , p_d[x_d])$ ] and discarding those outside of @xmath73 . in principle , since @xmath72,\\ldots , p_d[x_d])$ ] is a univariate polynomial in @xmath49 it is an easy task .",
    "however , numerically , @xmath68 is typically near - zero in large regions of @xmath74 , and spurious solutions as well as missed zeros plague this approach in finite precision arithmetic ( see figure  [ fig : resultanttrouble ] ) .",
    "thus , directly computing the roots of @xmath68 is spectacularly numerically unstable for almost all @xmath4 and @xmath5 .",
    "this approach is rarely advocated in practice .",
    "typicalresultants ( 15,52 ) missed zeros ( 30,10 ) spurious solution ( 50,25 ) inaccurate root ( 30,50 ) ( -1,-1)7 ( 42,15 ) ( -1,2)7 ( 70,29 ) ( 2,1)10 ( 50,-2.5 ) @xmath75    instead , one often considers an associated multidimensional resultant matrix whose determinant is equal to @xmath68 .",
    "working with matrices rather than determinants is beneficial for practical computations , especially when @xmath6  @xcite .",
    "occasionally , this variation on hidden variable resultant methods is called _ numerically confirmed eliminants _ to highlight its improved numerical behavior  ( * ? ? ?",
    "however , we will show that even after this significant improvement the hidden variable resultant methods based on the cayley and sylvester resultant matrices remain numerically unstable .",
    "let @xmath1 , @xmath59 , @xmath76 , and @xmath68 be a multidimensional resultant ( see defintion  [ def : resultantmulti ] ) .",
    "a matrix - valued function @xmath77)^d\\rightarrow \\mathbb{c}^{n\\times n}$ ] is a multidimensional resultant matrix associated with @xmath68 if for any set of @xmath5 polynomials @xmath61 $ ] we have @xmath78    there are many types of resultant matrices including cayley ( see section  [ sec : cayley ] ) , sylvester ( see section  [ sec : sylvester ] ) , macaulay  @xcite , and others  @xcite . in this paper",
    "we only consider two of the most popular choices : cayley and sylvester resultant matrices .",
    "theoretically , we can calculate the @xmath5th component of the solutions by finding all the @xmath56 such that @xmath79,\\ldots , p_d[x_d^{\\ast}]))=0 $ ] . in practice",
    ", our analysis will show that this @xmath5th component can not always be accurately computed .",
    "each entry of the matrix @xmath80,\\ldots , p_d[x_d])$ ] is a polynomial in @xmath49 of finite degree .",
    "in linear algebra such objects are called matrix polynomials ( or polynomial matrices ) and finding the solutions of @xmath81,\\ldots , p_d[x_d]))=0 $ ] is related to a polynomial eigenproblem  @xcite .      since multidimensional resultant matrices are matrices with univariate polynomial entries , matrix polynomials play an important role in the hidden variable resultant method . a classical reference on matrix polynomials",
    "is the book by gohberg , lancaster , and rodman  @xcite .",
    "let @xmath76 and @xmath82 .",
    "we say that @xmath83 is a ( square ) matrix polynomial of size @xmath84 and degree @xmath85 if @xmath83 is an @xmath86 matrix whose entries are univariate polynomials in @xmath87 of degree @xmath88 , where at least one entry is of degree exactly @xmath85 .",
    "in fact , since   is a zero - dimensional polynomial system it can only have a finite number of isolated solutions and hence , the matrix polynomials we consider are regular  @xcite .",
    "we say that a square matrix polynomial @xmath83 is regular if @xmath89 for some @xmath90 .",
    "a matrix polynomial @xmath83 of size @xmath84 and degree @xmath85 can be expressed in a degree - graded polynomial basis as @xmath91 when the leading coefficient matrix @xmath92 in   is invertible the eigenvalues of @xmath83 are all finite , and they satisfy @xmath93 .",
    "let @xmath83 be a regular matrix polynomial of size @xmath84 and degree @xmath85 .",
    "if @xmath90 is finite and there exists a non - zero vector @xmath94 such that @xmath95 ( resp .",
    "@xmath96 ) , then we say that @xmath97 is a _",
    "right ( resp .  left ) eigenvector _ of @xmath83 corresponding to the eigenvalue @xmath87 .",
    "[ def : matpolyeigenvector ]    for a regular matrix polynomial @xmath83 we have the following relationship between its eigenvectors and determinant  @xcite : for any finite @xmath90 , @xmath98    in multidimensional rootfinding , one sets @xmath99,\\ldots , p_d[\\lambda])$ ] and solves @xmath100 via the polynomial eigenvalue problem @xmath95 .",
    "there are various algorithms for solving @xmath95 including linearization  @xcite , the ehrlich ",
    "aberth method  @xcite , and contour integration  @xcite .",
    "however , regardless of how the polynomial eigenvalue problem is solved in finite precision , the hidden variable resultant method based on the cayley or the sylvester matrix is numerically unstable .    for the popular resultant matrices , such as cayley and sylvester , the first @xmath55 components of the solutions can be determined from the left or right eigenvectors of @xmath101,\\ldots , p_d[x_d^{\\ast}])$ ] .",
    "for instance , if linearization is employed , the multidimensional rootfinding problem is converted into one ( typically very large ) eigenproblem , which can be solved by the qr or qz algorithm .",
    "practitioners often find that the computed eigenvectors are not accurate enough to adequately determine the @xmath55 components .",
    "however , the blame for the observed numerical instability is not only on the eigenvectors , but also the eigenvalues .",
    "our analysis will show that the @xmath5th component may not be computed accurately either .",
    "not even a numerically stable algorithm can be expected to accurately compute a simple root of   if that root is itself sensitive to small perturbations .",
    "finite precision arithmetic almost always introduces roundoff errors and if these can cause large perturbations in a root then that solution is ill - conditioned .",
    "the absolute condition number of a simple root measures how sensitive the location of the root is to small perturbations in @xmath2 .",
    "let @xmath27 be a simple root of  .",
    "the absolute condition number of @xmath28 associated with rootfinding is @xmath102 , i.e. , the matrix @xmath103-norm of the inverse of the jacobian .",
    "[ def : conditioning ]    see  @xcite .    as a rule of thumb",
    ", a numerically stable rootfinder should be able to compute a simple root @xmath104 to an accuracy of @xmath105 , where @xmath106 is the unit machine roundoff .",
    "in contrast , regardless of the condition number of @xmath28 , a numerically unstable rootfinder may not compute it accurately . worse still , it may miss solutions with detrimental consequences .",
    "a hidden variable resultant method computes the @xmath5th component of the solutions by solving the polynomial eigenvalue problem @xmath80,\\ldots , p_d[x_d])v = 0 $ ] .",
    "the following condition number tells us how sensitive an eigenvalue is to small perturbations in @xmath25  ( * ? ? ?",
    "( 12 ) ) ( also see  @xcite ) :    let @xmath56 be a finite eigenvalue of @xmath107,\\ldots , p_d[x_d])$ ] .",
    "the condition number of @xmath108 associated with the eigenvalue problem @xmath109 is @xmath110 where the supremum is taken over the set of matrix polynomials @xmath111 such that @xmath112 .",
    "[ def : condeigenvalue ]    a numerical polynomial eigensolver can only be expected to compute the eigenvalue @xmath108 satisfying @xmath113 to an accuracy of @xmath114 , where @xmath106 is unit machine roundoff .",
    "we will be interested in how @xmath115 relates to the condition number , @xmath102 , of the corresponding root .",
    "it can be quite difficult to calculate @xmath115 directly from  , and is usually more convenient to use the formula below .",
    "( related formulas can be found in  ( * ? ? ?",
    "1 ) for symmetric matrix polynomials and in  ( * ? ? ?",
    "5 ) for general matrix polynomials . )    let @xmath116 be a regular matrix polynomial with finite simple eigenvalues .",
    "let @xmath56 be an eigenvalue of @xmath116 with corresponding right and left eigenvectors @xmath117 .",
    "then , we have @xmath118 where @xmath119 denotes the derivative of @xmath25 with respect to @xmath49 . [",
    "lem : conditionexpression ]    the first part of the proof follows the analysis in  @xcite .",
    "let @xmath116 be a regular matrix polynomial with a simple eigenvalue @xmath56 and corresponding right and left eigenvectors @xmath117 .",
    "a perturbed matrix polynomial @xmath120 will have a perturbed eigenvalue @xmath121 and a perturbed eigenvector @xmath122 such that @xmath123 , where @xmath124 .",
    "expanding , keeping only the first order terms , and using @xmath125 we obtain @xmath126 multiplying by @xmath127 on the left , rearranging , and keeping the first order terms , we obtain @xmath128 where the derivative in @xmath129 is taken with respect to @xmath49 .",
    "thus , from   we see that @xmath130 we now show that the upper bound in   can be attained .",
    "take @xmath131 .",
    "then , @xmath132 and @xmath133 the result follows by definition  [ def : condeigenvalue ] .    for the cayley resultant matrix ( see section  [ sec : cayley ] )",
    ", we will show that @xmath134 can be as large as @xmath135 ( see theorem  [ thm : condtheorem ] ) .",
    "thus , there can be an exponential increase in the conditioning that seems inherent to the methodology of the hidden variable resultant method based on the cayley resultant matrix .",
    "in particular , once the polynomial eigenvalue problem has been constructed , a backward stable numerical eigensolver may not compute accurate solutions to  .",
    "we now must tackle the significant challenge of showing that the cayley and sylvester resultant matrices do lead to numerical unstable hidden variable resultant methods , i.e. , for certain solutions @xmath28 the quantity @xmath134 can be much larger than @xmath102 .",
    "the hidden variable resultant method when based on the cayley resultant  @xcite finds the solutions to   by solving the polynomial eigenvalue problem given by @xmath136 , where @xmath137 is a certain matrix polynomial . to define it we follow the exposition in  @xcite and first introduce a related cayley function @xmath138 .",
    "the cayley function associated with the polynomials @xmath139 $ ] is a multivariate polynomial in @xmath140 variables , denoted by @xmath141 , and is given by @xmath142   q_1(t_1,s_2,\\ldots , s_{d-1 } ) & \\ldots & q_d(t_1,s_2,\\ldots , s_{d-1})\\\\[3pt ]   \\vdots & \\ddots & \\vdots\\\\[3pt ] q_1(t_1,t_2,\\ldots , t_{d-1 } ) & \\ldots & q_d(t_1,t_2,\\ldots , t_{d-1})\\\\[3pt ]    \\end{pmatrix}\\bigg/\\prod_{i=1}^{d-1 } ( s_i - t_i ) .",
    "\\label{eq : dixonresultant}\\ ] ]    in two dimensions the cayley function ( also known as the bzoutian function  @xcite ) takes the more familiar form of @xmath143 q_1(t_1 ) & q_2(t_1)\\end{pmatrix } = \\frac{q_1(s_1)q_2(t_1)-q_2(s_1)q_1(t_1)}{s_1-t_1},\\ ] ] which is of degree at most @xmath144 in @xmath145 and @xmath146 . by carefully applying laplace s formula for the matrix determinant in",
    ", one can see that @xmath138 is a polynomial of degree @xmath147 in @xmath148 and @xmath149 for @xmath150 .",
    "note that @xmath138 is not the multidimensional resultant ( except when @xmath151 for all @xmath10 ) .",
    "instead , @xmath138 is a function that is a convenient way to define the cayley resultant matrix .",
    "let @xmath152 be the selected degree - graded polynomial basis .",
    "the cayley resultant matrix depends on the polynomial basis and is related to the expansion coefficients of @xmath138 in a tensor - product basis of @xmath152 .",
    "that is , let @xmath153 be the tensor - product expansion of the polynomial @xmath138 , where @xmath34 is a tensor of expansion coefficients of size @xmath154 .",
    "the _ cayley resultant matrix _ is the following unfolding ( or matricization ) of @xmath34  ( * ? ? ?",
    "2.3 ) :    the cayley resultant matrix associated with @xmath155 $ ] with respect to the basis @xmath152 is denoted by @xmath156 and is the @xmath157 matrix formed by the unfolding of the tensor @xmath34 in  .",
    "this unfolding is often denoted by @xmath158 , where @xmath159 and @xmath160 _  ( * ? ? ?",
    "2.3)_.    for example ,",
    "when @xmath161 for @xmath150 we have for @xmath162 @xmath163 this is equivalent to n = factorial(d-1)*n^(d-1 ) ; r = reshape(a , n , n ) ; in matlab , except here the indexing of the matrix @xmath156 starts at @xmath164 .    for rootfinding , we set @xmath165,\\ldots , q_d = p_d[x_d]$ ] ( thinking of @xmath49 as the `` hidden '' variable ) .",
    "then , @xmath166 is a square matrix polynomial ( see section  [ sec : matrixpoly ] ) .",
    "if all the polynomials are of maximal degree @xmath4 , then @xmath156 is of size @xmath167 and of degree at most @xmath168 .",
    "the fact that @xmath169 is the maximum number of possible solutions that   can possess ( see lemma  [ lem : maxsolutions ] ) is a consequence of @xmath156 being a resultant matrix . in particular ,",
    "the eigenvalues of @xmath137 are the @xmath5th components of the solutions to   and the remaining @xmath55 components of the solutions can in principle be obtained from the eigenvectors .",
    "it turns out that evaluating @xmath138 at @xmath170 is equivalent to a matrix - vector product with @xmath156 .",
    "this relationship between @xmath156 and @xmath138 will be essential in section  [ subsec : cayleyeigenvectorstructure ] for understanding the eigenvectors of @xmath156 .",
    "let @xmath1 , @xmath171 , and @xmath138 and @xmath156 be the cayley function and matrix associated with @xmath139 $ ] , respectively . if @xmath17 is the tensor satisfying @xmath172 for @xmath173 , then we have @xmath174 where @xmath175 is the tensor that satisfies @xmath176 [ lem : rightcayleyvectors ]    the matrix - vector product @xmath177 is equivalent to the following sums : @xmath178 for some tensor @xmath175 .",
    "the result follows from  .      in this section",
    "we show that for systems of linear polynomials , i.e. , of total degree @xmath179 , the cayley resultant is precisely cramer s rule .",
    "we believe this connection is folklore , but we have been unable to find an existing reference that provides a rigorous justification .",
    "it gives a first hint that the hidden variable resultant method in full generality may be numerically unstable .",
    "let @xmath34 be a matrix of size @xmath44 , @xmath180 , and @xmath181 a vector of size @xmath182 .",
    "then , solving the linear polynomial system @xmath183 by the hidden variable resultant method based on the cayley resultant is equivalent to cramer s rule for calculating @xmath49 .",
    "[ thm : cayleycramer ]    let @xmath184 be the last column of @xmath34 and @xmath185 , where @xmath186 is the @xmath5th canonical vector .",
    "recall that cramer s rule computes the entry @xmath49 in @xmath187 via the formula @xmath188 .",
    "we will show that for the linear polynomial system @xmath183 we have @xmath189 .",
    "observe that this , in particular , implies that ( since @xmath138 has degree @xmath164 in @xmath190 for all @xmath191 ) @xmath192 .",
    "hence , the equivalence between cramer s rule and rootfinding based on the cayley resultant .",
    "first , using  , we write @xmath193 where the matrices @xmath194 and @xmath17 are @xmath195 s_2 & s_2 & t_2 & \\dots & t_2\\\\[3pt ] \\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\[3pt ] s_{d-1 } & s_{d-1 } & s_{d-1 } & \\dots & t_{d-1 } \\\\[3pt ] 1 & 1 & 1 & \\dots & 1 \\end{bmatrix } , \\qquad    m = b v + x_d a_d e^t,\\ ] ] where @xmath196 is the @xmath182 vector of all ones .",
    "( it can be shown by induction on @xmath5 that @xmath197 , as required . ) using the matrix determinant lemma , we have @xmath198 where @xmath199 is the algebraic adjugate matrix of @xmath200 .",
    "now , recall that @xmath201 and observe that @xmath202 . hence , we obtain @xmath203 using @xmath204 and the matrix determinant lemma one more time , we conclude that @xmath205 thus , @xmath189 and the resultant method calculates @xmath49 via cramer s formula .",
    "it is well - known in the literature that cramer s rule is a numerically unstable algorithm for solving @xmath206  ( * ? ? ?",
    "thus , theorem  [ thm : cayleycramer ] casts significant suspicion on the numerical properties of the hidden variable resultant method based on the cayley resultant .",
    "ultimately , we wish to use lemma  [ lem : conditionexpression ] to estimate the condition number of the eigenvalues of the cayley resultant matrix .",
    "to do this we need to know the left and right eigenvectors of @xmath156 .",
    "the following lemma shows that the eigenvectors of @xmath156 are in vandermonde form vector @xmath97 is in vandermonde form if there is an @xmath207 such that @xmath208 for @xmath209 . in higher dimensions ,",
    "the vector @xmath210 is in vandermonde form if @xmath211 for some @xmath212 . ] . to show this we exploit the convenient relationship between evaluation of @xmath138 and matrix - vector products with @xmath156 .",
    "suppose that @xmath213 is a simple root of  .",
    "let @xmath17 and @xmath214 be tensors of size @xmath215 and @xmath216 , respectively , defined by @xmath217 and @xmath218 then , the vectors @xmath219 and @xmath220 are the right and left eigenvectors of the matrix @xmath221,\\ldots , p_d[x_d^{\\ast}])$ ] that correspond to the eigenvalue @xmath108 . [",
    "lem : cayleyeigenvectorstructure ]    let @xmath222,\\ldots , p_d[x_d^{\\ast}])$ ] be the cayley function associated with @xmath57,\\ldots , p_d[x_d^{\\ast}]$ ] . from   we find that @xmath223 because the determinant of a matrix with a vanishing last row is zero .",
    "moreover , by lemma  [ lem : rightcayleyvectors ] we have @xmath224 since @xmath152 is a polynomial basis we must conclude that @xmath225 , and hence , @xmath226 with @xmath227 .",
    "in other words , @xmath97 is a right eigenvector of @xmath156 corresponding to the eigenvalue @xmath108 ( see definition  [ def : matpolyeigenvector ] ) .",
    "an analogous derivation shows that @xmath220 is a left eigenvector of @xmath156 .      to bound @xmath228",
    "we need to bound the absolute value of the generalized rayleigh quotient of @xmath229 ( see  lemma  [ lem : conditionexpression ] ) , whenever @xmath104 is such that @xmath108 is a simple eigenvalue of @xmath137 , i.e. , there are no other solutions to   with the same @xmath5th component . in a similar style to the proof of lemma  [ lem : cayleyeigenvectorstructure ] we show this by exploiting the relation between evaluating the derivative of @xmath138 and matrix - vector products with @xmath229 .",
    "let @xmath230 be the polynomials in  , @xmath231 a solution of  , and @xmath232 the cayley function associated with @xmath233,\\dots , q_d = p_d[x_d]$ ] .",
    "we have @xmath234 where @xmath29 is the jacobian matrix in  .",
    "that is , @xmath235 evaluated at @xmath236 for @xmath150 is equal to the determinant of the jacobian .",
    "[ thm : jacobianexpression ]    recall from   that @xmath232 is a polynomial in @xmath237 and @xmath238 written in terms of a matrix determinant , and set @xmath233,\\ldots , q_d = p_d[x_d]$ ] .",
    "the determinant in   for @xmath232 can be expanded to obtain @xmath239(t_1,\\dots , t_{i-1},s_i,\\dots , s_{d-1}),\\ ] ] where @xmath240 is the symmetric group of @xmath241 and @xmath242 is the signature of the permutation @xmath243 .",
    "when we evaluate @xmath232 at @xmath236 for @xmath150 the denominator vanishes , and hence , so does the numerator because @xmath232 is a polynomial .",
    "thus , by lhospital s rule , @xmath235 evaluated @xmath236 for @xmath150 is equal to @xmath244(t_1,\\dots , t_{i-1},s_i,\\dots , s_{d-1 } ) \\label{eq : diffdet}\\ ] ] evaluated at @xmath245 , @xmath246 , and @xmath247 . in principle",
    ", one could now apply the product rule and evaluate the combinatorially many terms in  .",
    "instead , we note that after applying the product rule a term is zero if it contains @xmath248 for any @xmath249 and @xmath250 ( since @xmath28 is a solution to  ) .",
    "there are precisely @xmath5 partial derivatives and @xmath5 terms in each product so that any nonzero term when expanding  [ eq : diffdet ] has each @xmath251 differentiated precisely once .",
    "finally , note that for each @xmath150 only the @xmath252 terms in the product depend on @xmath148 .",
    "hence , from   we obtain @xmath253 the result follows because the last expression is the determinant of the jacobian matrix evaluated at @xmath28 .    as a consequence of theorem  [ thm : jacobianexpression ] we have the following unavoidable conclusion that mathematically explains the numerical difficulties that practitioners have been experiencing with hidden variable resultant methods based on the cayley resultant .",
    "let @xmath1 . then",
    ", there exist @xmath230 in   with a simple root @xmath231 such that @xmath254 and @xmath255 .",
    "thus , an eigenvalue of @xmath137 can be more sensitive to perturbations than the corresponding root by a factor that grows exponentially with @xmath5 .",
    "[ thm : condtheorem ]    using lemma  [ lem : rightcayleyvectors ] , theorem  [ thm : jacobianexpression ] has the following equivalent matrix form : @xmath256 where @xmath227 , @xmath257 , and @xmath17 and @xmath214 are given in lemma  [ lem : cayleyeigenvectorstructure ] . since @xmath258 , we know that @xmath259 and @xmath260 . hence , by lemma  [ lem : conditionexpression ] @xmath261 denoting the singular values  ( * ? ? ?",
    "7.3 ) of the matrix @xmath29 by @xmath262 , select @xmath230 and @xmath231 such that @xmath263 .",
    "such polynomial systems do exist , for example , linear polynomial systems where @xmath264 and @xmath194 is a matrix with singular values @xmath265 . to ensure that @xmath255 we also require @xmath266 .",
    "then , we have @xmath267 the result follows .    [ exc ]",
    "let @xmath268 be a @xmath269 orthogonal matrix , @xmath270 , having elements @xmath271 for @xmath272 , and let @xmath273 .",
    "consider the system of polynomial equations @xmath274 the origin , @xmath275 , is a simple root of this system of equations .",
    "the jacobian of the system at @xmath164 is @xmath276 , and hence , the absolute conditioning of the problem is @xmath277 . constructing the cayley resultant matrix polynomial in the monomial basis ,",
    "one readily sees that for this example the right and left eigenvectors for the eigenvalue @xmath278 satisfy @xmath279 . as a consequence , @xmath280 .",
    "we emphasize that this numerical instability is truly spectacular , affects the accuracy of @xmath108 , and can grow exponentially with the dimension @xmath5 .",
    "moreover , theorem  [ thm : condtheorem ] holds for any degree - graded polynomial basis selected to represent @xmath2 as long as @xmath258 . in particular ,",
    "the associated numerical instability can not be resolved in general by a special choice of polynomial basis .",
    "theorem  [ thm : condtheorem ] is pessimistic and importantly does not imply that the resultant method always loses accuracy , just that it might . in general , one must know the solutions to   and the singular values of the jacobian matrix to be able to predict if and when the resultant method will be accurate .",
    "one should note that theorem  [ thm : condtheorem ] concerns absolute conditioning and one may may wonder if a similar phenomenon also occurs in the relative sense . in section  [ sec : absrel ]",
    "we show that the relative conditioning can also be increased by an exponential factor with @xmath5 .",
    "a popular alternative in two dimensions to the cayley resultant matrix is the sylvester matrix  ( * ? ? ?",
    "3 ) , denoted here by @xmath281 .",
    "we now set out to show that the hidden variable resultant based on @xmath281 is also numerically unstable .",
    "however , since @xmath7 the instability has only a moderate impact in practice as the conditioning can only be at most squared . with care ,",
    "practical bivariate rootfinders can be based on the sylvester resultant  @xcite though there is the possibility that a handful digits are lost .    a neat way to define",
    "the sylvester matrix that accommodates nonmonomial polynomial bases is to define the matrix one row at a time .",
    "let @xmath282 and @xmath283 be two univariate polynomials in @xmath284 $ ] of degree exactly @xmath285 and @xmath286 , respectively .",
    "then , the sylvester matrix @xmath287 associated with @xmath282 and @xmath283 is defined row - by - row as @xmath288 where @xmath289 is the row vector of coefficients such that @xmath290 and @xmath291 where @xmath292 is the row vector of coefficients such that @xmath293 .",
    "[ def : sylvestermatrix ]    in the monomial basis , i.e. , @xmath294 , definition  [ def : sylvestermatrix ] gives the sylvester matrix of size @xmath295 as  ( * ? ? ?",
    "3 ) : @xmath296    & \\ddots & \\ddots & \\ddots & \\ddots \\\\[3pt ] & & a_{0 } & a_{1 } & \\ldots & a_{\\tau_1 } \\\\[3pt ]   b_{0 } & b_{1 } & \\ldots & b_{\\tau_2 } &   & \\\\[3pt ]    &   \\ddots & \\ddots & \\ddots & \\ddots \\\\[3pt ] & & b_{0 } & b_{1 } & \\ldots & b_{\\tau_2 } \\\\[3pt ] \\end{pmatrix}\\begin{matrix }   { \\left.\\vphantom{\\begin{matrix } x\\\\[3pt]y\\vphantom{\\ddots}\\\\[3pt]y\\\\[3pt ] \\end{matrix}}\\right\\}\\\\ { \\left.\\vphantom{\\begin{matrix } x\\\\[3pt]y\\vphantom{\\ddots}\\\\[3pt]y\\\\[3pt ] \\end{matrix}}\\right\\}\\\\ \\end{matrix}\\label{eq : monomial}\\ ] ] where @xmath297 and @xmath298 .",
    "our goal is to use lemma  [ lem : conditionexpression ] to bound the condition number of the eigenvalues of the sylvester matrix .",
    "it turns out the right eigenvectors of @xmath281 are in vandermonde form .",
    "however , the left eigenvectors have a more peculiar structure and are related to the byproducts of a generalized clenshaw s algorithm for degree - graded polynomial bases ( see lemma  [ lem : sylvestereigenvectorstructure ] ) .",
    "we develop a clenshaw s algorithm for degree - graded bases in this section with derivations of its properties in appendix  [ sec : appendix ] .",
    "the selected polynomial basis @xmath299 is degree - graded and hence , satisfies a recurrence relation of the form @xmath300 where @xmath301 and @xmath302 .",
    "if @xmath299 is an orthogonal polynomial basis , then   is a three - term recurrence and it is standard to employ clenshaw s algorithm  @xcite to evaluate polynomials expressed as @xmath303 .",
    "this procedure can be extended to any degree - graded polynomial basis .",
    "let @xmath304 be expressed as @xmath303 , where @xmath305 is a degree - graded polynomial basis .",
    "one can evaluate @xmath304 via the following procedure : let @xmath306(x ) = 0 $ ] , and calculate @xmath307(x),\\ldots , b_1[p](x)$ ] from the following recurrence relation : @xmath308(x ) = a_k + ( \\alpha_kx + \\beta_k)b_{k+1}[p](x ) + \\sum_{j = k+1}^{n-1 } \\gamma_{j , k+1}b_{j+1}[p](x ) , \\qquad 1\\leq k\\leq n. \\label{eq : clenshawlikealg}\\ ] ] we refer to the quantities @xmath309(x),\\ldots , b_{n+1}[p](x)$ ] as _ clenshaw shifts _ ( in the monomial case they are called horner shifts  @xcite ) .",
    "the value @xmath304 can be written in terms of the clenshaw shifts is stated in a general form and holds for _ any _ degree - graded basis , in this paper we fix the normalization @xmath310 , that implies in particular @xmath311 simplifying . ] .",
    "let @xmath4 be a positive integer , @xmath207 , @xmath305 a degree - graded basis satisfying  , @xmath303 , and @xmath306(x),\\ldots , b_1[p](x)$ ] the clenshaw shifts satisfying  .",
    "then , @xmath312(x ) + \\sum_{i=1}^{n-1 } \\gamma_{i,1}b_{i+1}[p](x ) .",
    "\\label{eq : clenshawdegreegraded}\\ ] ] [ lem : clenshawdegreegraded ]    see appendix  [ sec : appendix ] .",
    "clenshaw s algorithm for degree - graded polynomial bases is summarized in figure  [ fig : clenshawalgorithm ] .",
    "we note that because of the full recurrence in   the algorithm requires @xmath313 operations to evaluate @xmath304 .",
    "though this algorithm may not be of significant practical importance , it is of theoretical interest for the conditioning analysis of some linearizations from the so - called @xmath314- or @xmath315-spaces  @xcite when degree - graded bases are employed  @xcite .",
    "there is a remarkable and interesting connection between clenshaw shifts and the quotient @xmath316 , which will be useful when deriving the left eigenvectors of @xmath281 .    with the same set up as lemma  [ lem : clenshawdegreegraded ] we have @xmath317(y)\\phi_i(x ) , \\quad x\\neq y \\label{eq : diffdegreegraded}\\ ] ] and @xmath318(x)\\phi_i(x ) .",
    "\\label{eq : diffclenshaw}\\ ] ] [ thm : diffdegreegraded ]    see appendix  [ sec : appendix ] .",
    "the relation between the derivative and clenshaw shifts in   has been noted by skrzipek for orthogonal polynomial bases in  @xcite , where it was used to construct a so - called _ extended clenshaw s algorithm _ for evaluating polynomial derivatives . using theorem  [ thm : diffdegreegraded ] and  @xcite an extended clenshaw s algorithm for polynomials expressed in a degree - graded basis is immediate .",
    "we now set @xmath319 $ ] and @xmath320 $ ] ( considering @xmath75 as the hidden variable ) , and we are interested in the eigenvectors of the matrix polynomial @xmath321 , when @xmath322 is a solution to   when @xmath6 .",
    "it turns out that the right eigenvectors of @xmath321 are in vandermonde form , while the left eigenvectors are related to the clenshaw shifts ( see section  [ subsec : generalizedclenshaw ] ) .",
    "suppose that @xmath323 is a simple root of   and that @xmath324 $ ] and @xmath325 $ ] are of degree @xmath285 and @xmath286 , respectively , in @xmath326 .",
    "the right eigenvector of @xmath321 corresponding to the eigenvalue @xmath327 is @xmath328 and the left eigenvector is defined as @xmath329(x_1^{\\ast } ) , & 0\\leq i\\leq \\tau_2 - 1,\\\\    \\alpha_{i-\\tau_2 } b_{i-\\tau_2 + 1}[q_{1}](x_1^{\\ast } ) , & \\tau_2\\leq i\\leq \\tau_1+\\tau_2 - 1 ,    \\end{cases}\\ ] ] where @xmath330 $ ] and @xmath331(x_1^{\\ast})$ ] are the clenshaw shifts with respect to @xmath152 , while the coefficients @xmath332 are defined as in . [ lem : sylvestereigenvectorstructure ]    by construction we have , for @xmath333 , @xmath334 and , for @xmath335 , @xmath336 thus , @xmath97 is a right eigenvector of @xmath321 corresponding to the eigenvalue @xmath327 .    for the left eigenvector , first note that for any vector @xmath337 of the form @xmath338 for @xmath339 we have by theorem  [ thm : diffdegreegraded ] @xmath340(x_1^{\\ast})\\phi_{i}(x)q_1(x ) + \\sum_{i=0}^{\\tau_1 - 1 } \\alpha_{i}b_{i+1}[q_1](x_1^{\\ast})\\phi_{i}(x)q_2(x)\\\\   & = -\\frac{q_2(x ) - q_2(x_1^{\\ast})}{x - x_1^{\\ast}}q_1(x ) + \\frac{q_1(x ) - q_1(x_1^{\\ast})}{x - x_1^{\\ast}}q_2(x)\\\\   & = -\\frac{q_2(x)}{x - x_1^{\\ast}}q_1(x ) + \\frac{q_1(x)}{x - x_1^{\\ast}}q_2(x ) = 0 , \\end{aligned}\\ ] ] where the second from last equality follows because @xmath341 . since   holds for any @xmath342 and @xmath343 is a basis of @xmath344 $ ]",
    ", we deduce that @xmath345 , and hence , @xmath346 is a left eigenvector of @xmath281 corresponding to the eigenvalue @xmath327 .      to bound @xmath347",
    "we look at the absolute value of the generalized rayleigh quotient of @xmath348 , whenever @xmath28 is such that @xmath327 is a simple eigenvalue of @xmath349 .",
    "lemma  [ lem : sylvestereigenvectorstructure ] allows us to show how the generalized rayleigh quotient of @xmath350 relates to the determinant of the jacobian .    with the same assumptions as in lemma  [ lem : sylvestereigenvectorstructure ] , we have @xmath351 where @xmath346 and @xmath97 are the left and right eigenvectors of @xmath281 , respectively , and @xmath29 is the jacobian matrix in  .",
    "[ lem : sylvesterjacobian ]    by lemma  [ lem : sylvestereigenvectorstructure ] we know the structure of @xmath97 and @xmath346 .",
    "hence , we have @xmath352(x_1^{\\ast})\\phi_{i}(x_1^{\\ast})\\frac{\\partial q_1}{\\partial x_2}(x_1^{\\ast } ) + \\!\\!\\sum_{i=0}^{\\tau_1 - 1}\\!\\ !",
    "\\alpha_{i}b_{i+1}[q_1](x_1^{\\ast})\\phi_{i}(x_1^{\\ast})\\frac{\\partial q_2}{\\partial x_2}(x_1^{\\ast})\\\\ & = -\\frac{\\partial q_1}{\\partial x_2}(x_1^{\\ast})\\frac{\\partial q_2}{\\partial x_1}(x_1^{\\ast } ) + \\frac{\\partial q_1}{\\partial x_1}(x_1^{\\ast})\\frac{\\partial q_2}{\\partial x_2}(x_1^{\\ast } ) , \\end{aligned}\\ ] ] where the last equality used the relation in  .",
    "the result now follows since this final expression equals @xmath353 and since @xmath258 we have @xmath354 .",
    "there exist @xmath355 and @xmath356 in   with a simple root @xmath357 such that @xmath358 and @xmath359 .",
    "thus , an eigenvalue of @xmath349 can be squared more sensitive to perturbations than the corresponding root in the absolute sense .",
    "[ thm : condsylvester ]    we give an example for which @xmath360 in lemma  [ lem : sylvesterjacobian ] . for some positive parameter @xmath106 and for some @xmath361 consider the polynomials @xmath362 one can verify that @xmath363 is a common root .",
    "since @xmath364(0 ) | = \\alpha_{n-1 } \\alpha_{n-1}^{-1 } = 1 $ ] we have @xmath365 .",
    "the result then follows from @xmath366 and lemma  [ lem : sylvesterjacobian ] .",
    "[ exs ] let us specialize example [ exc ] to @xmath7 , i.e. , for some @xmath273 and @xmath367 let us consider the system @xmath368 again , for the solution @xmath369 we have @xmath370 .",
    "building the sylvester matrix in the monomial basis , we obtain    @xmath371 as predicted by the theory , @xmath372 is an eigenvalue with corresponding right and left eigenvectors , respectively , @xmath373 and @xmath374 .",
    "moreover , it is readily checked that , as expected , @xmath375 .",
    "therefore , @xmath376    theorem  [ thm : condsylvester ] mathematically explains the numerical difficulties that practitioners have been experiencing with hidden variable resultant methods based on the sylvester resultant .",
    "there are successful bivariate rootfinders based on this methodology  @xcite for low degree polynomial systems and it is a testimony to those authors that they have developed algorithmic remedies ( not cures ) for the inherent numerical instability .",
    "we emphasize that theorem  [ thm : condsylvester ] holds for any normalized degree - graded polynomial basis .",
    "thus , the mild numerical instability can not , in general , be overcome by working in a different degree - graded polynomial basis .",
    "the example in the proof of theorem  [ thm : condsylvester ] is quite alarming for a practitioner since if @xmath106 is the unit machine roundoff , then we have @xmath377 and @xmath378 .",
    "thus , a numerical rootfinder based on the sylvester matrix may entirely miss a solution that has a condition number larger than @xmath379 .",
    "a stable rootfinder should not miss such a solution .",
    "when @xmath6 , we can use theorem  [ thm : condtheorem ] and lemma  [ lem : sylvesterjacobian ] to conclude that the ratio between the conditioning of the cayley and sylvester resultant matrices for the same eigenvalue @xmath380 is equal to @xmath381 , where @xmath97 and @xmath346 are the right and left eigenvector of @xmath321 associated with the eigenvalue @xmath380 .",
    "this provides theoretical support for the numerical observations in  @xcite .",
    "however , it seems difficult to predict _ a priori _ if the cayley or sylvester matrix will behave better numerically . for real polynomials and @xmath7 ,",
    "the cayley resultant matrix is symmetric and this structure can be exploited  @xcite . in the monomial basis ,",
    "the sylvester matrix is two stacked toeplitz matrices ( see  ) .",
    "it may be that structural differences like these are more important than their relatively similar numerical properties when @xmath6 .",
    "let @xmath382 be the solution of a mathematical problem depending on data @xmath383 .",
    "in general , with the very mild assumption that @xmath383 and @xmath384 lie in banach spaces , it is possible to define the absolute condition number of the problem by perturbing the data to @xmath385 and studying the behaviour of the perturbed solution @xmath386 : @xmath387 similarly , a relative condition number can be defined by looking at the limit ratios of _ relative _ changes .",
    "@xmath388    in this paper , we have compared two absolute condition numbers .",
    "one is given by proposition   [ def : conditioning ] : there , @xmath389 is a solution of while @xmath390 is the set of polynomials in  .",
    "the other is given by lemma  [ lem : conditionexpression ] , where @xmath383 is a matrix polynomial and @xmath391 is the @xmath5th component of @xmath392 .    to quote n.  j.  higham  @xcite : `` usually , it is the relative condition number that is of interest , but it is more convenient to state results for the absolute condition number '' .",
    "this remark applies to our analysis as well .",
    "we have found it convenient to study the absolute condition number , but when attempting to solve the rootfinding problem in floating point arithmetic it is natural to allow for relatively small perturbations , and thus to study the relative condition number .",
    "hence , a natural question is whether the exponential increase of the absolute condition number in theorem  [ thm : condtheorem ] and the squaring in theorem  [ thm : condsylvester ] causes a similar effect in the relative condition number .",
    "it is not immediate that the exponential increase of the absolute condition number leads to the same effect in the relative sense .",
    "we have found examples where the exponential increase of the absolute condition number is perfectly counterbalanced by an exponentially small cayley resultant matrix .",
    "for instance , linear polynomial systems , when the cayley resultant method is equivalent to cramer s rule , fall into this category . in the relative sense , it may be possible to show that the hidden variable resultant method based on cayley or sylvester is either numerically unstable during the construction of the resultant matrix or the resultant matrix has an eigenvalue that is more sensitive to small relative perturbations than hoped .",
    "we do not know yet how to make such a statement precise .    instead , we provide an example that shows that the hidden variable resultant method remains numerically unstable in the relative sense .",
    "let @xmath106 be a sufficiently small real positive parameter and @xmath1 .",
    "consider the following polynomial system : @xmath393 where if @xmath5 is odd then take @xmath394 . selecting @xmath395^d$ ]",
    ", we have that @xmath396 for @xmath250 , except possibly @xmath397 if @xmath5 is odd .",
    "it can be shown that the origin^d$ ] .",
    "] , @xmath28 , is a simple root , @xmath398 , @xmath399 , and that @xmath400 thus , neither the polynomials @xmath401 or the resultant matrix @xmath137 are small .",
    "in such an example , the relative condition number will exhibit the same behavior as the absolute condition number .",
    "in particular , the relative condition number of an eigenvalue of @xmath137 may be larger than the relative condition number of the corresponding solution by a factor that grows exponentially with @xmath5 .",
    "the same example ( for @xmath7 ) , and a similar argument , applies to the sylvester matrix showing the conditioning can be squared in the relative sense too .",
    "in this paper we have shown that two popular hidden variable resultant methods based on the sylvester and cayley matrices are numerically unstable .",
    "our analysis is for degree - graded polynomial bases and does not include the lagrange basis or certain sparse bases .",
    "we believe that the analysis of the cayley matrix in section  [ sec : cayley ] could be extended to include general polynomial bases , though the analysis in section  [ sec : sylvester ] for the sylvester matrix is more intimately connected to degree - graded bases .",
    "we hesitantly suggest that hidden variable resultant methods are inherently plagued by numerial instabilities , and that neither other polynomial bases nor other resultants can avoid a worst - case scenario that we have identified in this paper .",
    "we do not know exactly how to formulate such a general statement , but we note that practitioners are widely experiencing problems with hidden variable resultant methods",
    ". in particular , we do not know of a numerical multidimensional rootfinder based on resultants that is robust for polynomial systems of large degree @xmath4 and high @xmath5 .",
    "however , at the moment the analysis that we offer here is limited to the cayley and sylvester matrices . despite our doubts that it exists",
    ", we would celebrate the discovery of a resultant matrix that can be constructed numerically and that provably does not lead to a numerically unstable hidden variable resultant method .",
    "this would be a breakthrough in global rootfinding with significant practical applications as it might allow   to be converted into a large eigenproblem without confronting conditioning issues . solving high - dimensional and large degree polynomial systems",
    "would then be restricted by computational cost rather than numerical accuracy .",
    "finally , we express again our hope that this paper , while appearing rather negative , will have a positive long - term impact on future research into numerical rootfinders .",
    "we thank yuji nakatsukasa , one of our closest colleagues , for his insightful discussions during the writing of  @xcite that ultimately lead us to consider conditioning issues more closely .",
    "we also thank anthony austin and martin lotz for carefully reading a draft and providing us with excellent comments .",
    "while this manuscript was in a much earlier form martin lotz personally sent it to gregorio malajovich for his comments .",
    "gregorio s comprehensive and enthusiastic reply encouraged us to proceed with renewed vigor .",
    "this appendix contains the tedious , though necessary , proofs required in section  [ subsec : generalizedclenshaw ] for clenshaw s algorithm for evaluating polynomials expressed in a degree - graded basis .    by rearranging   we have @xmath402(x ) - ( \\alpha_kx + \\beta_k)b_{k+1}[p](x ) - \\sum_{j = k+1}^{n-1 } \\gamma_{j , k+1}b_{j+1}[p](x)$ ] .",
    "thus , @xmath403(x ) - ( \\alpha_kx+\\beta_k)b_{k+1}[p](x ) -\\!\\ ! \\sum_{j = k+1}^{n-1 } \\gamma_{j , k+1}b_{j+1}[p](x)\\right]\\!\\phi_k(x).\\ ] ] now , by interchanging the summations and collecting terms we have @xmath404(x ) - \\sum_{k=2}^n \\left(\\alpha_{k-1}x+\\beta_{k-1}\\right)\\phi_{k-1}(x)b_k[p](x)\\\\    & \\qquad\\quad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad -\\sum_{j=2}^{n-1}\\left[\\sum_{k=1}^{j-1}\\gamma_{j , k+1}\\phi_{k}(x)\\right]b_{j+1}[p](x)\\\\       & = a_0\\phi_0(x ) + \\phi_1(x)b_1[p](x)\\\\       & \\qquad\\qquad\\qquad+\\sum_{j=1}^{n-1}\\left[\\phi_{j+1}(x ) -(\\alpha_{j}x+\\beta_{j})\\phi_{j}(x ) - \\sum_{k=1}^{j-1 } \\gamma_{j , k+1}\\phi_{k}(x)\\right]b_{j+1}[p](x)\\\\   \\end{aligned}\\ ] ] finally , using   we obtain @xmath405(x ) + \\sum_{j=1}^{n-1 } \\gamma_{j,1}\\phi_0(x)b_{j+1}[p](x),\\ ] ] as required .",
    "section  [ subsec : generalizedclenshaw ] also shows that clenshaw s algorithm connects to the quotient @xmath316 . to achieve this we need an immediate result that proves a different recurrence relation on the clenshaw shifts to  .",
    "the proof involves tedious algebraic manipulations and mathematical strong induction .",
    "let @xmath4 be an integer , @xmath305 a degree - graded basis satisfying  , and @xmath306,\\ldots , b_1[p]$ ] the clenshaw shifts satisfying  .",
    "then , for @xmath406 , @xmath407(x ) = ( \\alpha_nx+\\beta_n)b_{j}[\\phi_{n}](x ) + \\sum_{s = j+1}^n\\gamma_{n , s}b_{j}[\\phi_{s-1}](x).\\ ] ] [ lem : newrecurrence ]    we proceed by induction on @xmath408 .",
    "let @xmath409 .",
    "we have , by  , @xmath410(x ) = ( \\alpha_nx + \\beta_n)b_{n+1}[\\phi_{n+1}](x ) = ( \\alpha_nx + \\beta_n)b_{n}[\\phi_{n}](x),\\ ] ] where the last equality follows because @xmath411(x ) = b_n[\\phi_n](x ) = 1 $ ] . now",
    ", suppose the result holds for @xmath412 .",
    "we have , by   and the inductive hypothesis , @xmath413(x ) & = ( \\alpha_kx + \\beta_k)b_{k+1}[\\phi_{n+1}](x ) + \\sum_{j = k+1}^{n } \\gamma_{j , k+1}b_{j+1}[\\phi_{n+1}](x)\\\\     & = ( \\alpha_kx + \\beta_k)\\left[(\\alpha_nx+\\beta_n)b_{k+1}[\\phi_{n}](x ) + \\sum_{s = k+2}^n \\gamma_{n , s}b_{k+1}[\\phi_{s-1}](x)\\right ] \\\\      & \\hspace{1cm}+ \\sum_{j = k+1}^{n-1 } \\gamma_{j , k+1}\\left[(\\alpha_nx+\\beta_n)b_{j+1}[\\phi_{n}](x ) + \\sum_{s = j+2}^n \\gamma_{n , s}b_{j+1}[\\phi_{s-1}](x)\\right ] \\\\      & \\hspace{2cm}+ \\gamma_{n , k+1}b_{n+1}[\\phi_{n+1}](x).\\\\ \\end{aligned}\\ ] ] by interchanging the summations and collecting terms we have @xmath414(x ) & = ( \\alpha_nx+\\beta_n)\\left[(\\alpha_kx + \\beta_k)b_{k+1}[\\phi_n](x ) + \\sum_{j = k+1}^{n-1}\\gamma_{j , k+1}b_{j+1}[\\phi_n](x)\\right ] \\\\      & \\hspace{.6 cm } + \\sum_{s = k+3}^n \\gamma_{n , s}\\left[(\\alpha_kx + \\beta_k)b_{k+1}[\\phi_{s-1}](x ) + \\sum_{j = k+1}^{s-2}\\gamma_{j , k+1}b_{j+1}[\\phi_{s-1}](x)\\right]\\\\      & \\hspace{2cm}+ \\gamma_{n , k+2}(\\alpha_kx+\\beta_k)b_{k+1}[\\phi_{k+1}](x ) + \\gamma_{n , k+1}b_{n+1}[\\phi_{n+1}](x)\\\\      & = ( \\alpha_nx+\\beta_n)b_k[\\phi_n ] + \\sum_{s = k+1}^n \\gamma_{n , s } b_k[\\phi_{s-1 } ] , \\end{aligned}\\ ] ] where in the last equality we used  , @xmath415(x ) = b_k[\\phi_{k+1}](x)$ ] , and @xmath411(x ) = b_k[\\phi_k](x ) = 1 $ ] .      * case 1 : @xmath416 . * since for a fixed @xmath417 the clenshaw shifts are linear , i.e. , @xmath418(y ) = c_1b_j[\\phi_i](y ) + c_2 b_j[\\phi_k](y)$ ] for constants @xmath419 and @xmath420 , it is sufficient to prove the theorem for @xmath421 for @xmath422 .",
    "assume that the result holds for @xmath425 .",
    "from the inductive hypothesis , we have @xmath426(y)\\phi_j(x ) \\\\ & \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+ \\sum_{j=1}^k\\gamma_{k , j}\\sum_{s=0}^{j-2}\\alpha_sb_{s+1}[\\phi_{j-1}](y)\\phi_s(x ) .",
    "\\end{aligned}\\ ] ] moreover , by interchanging the summations and collecting terms we have @xmath427(y)\\phi_{k-1}(x ) \\\\ & \\!\\!\\!+ \\sum_{j=0}^{k-2}\\alpha_j\\left[(\\alpha_kx+\\beta_k)b_{j+1}[\\phi_k](y ) + \\sum_{s = j+2}^k\\gamma_{k , s}b_{j+1}[\\phi_{s-1}](y)\\right]\\phi_j(x ) .",
    "\\end{aligned}\\ ] ] finally , since @xmath428(y ) = 1 $ ] , @xmath429(y ) = ( \\alpha_kx+\\beta_k)b_k[\\phi_k](y)$ ] , and by  , we have @xmath430(y)\\phi_k(x ) + \\alpha_{k-1}b_{k}[\\phi_{k+1}](y)\\phi_{k-1}(x ) \\\\    &",
    "\\qquad\\qquad\\qquad + \\sum_{j=0}^{k-2 } \\alpha_jb_{j+1}[\\phi_{k+1}](y)\\phi_j(x ) \\end{aligned}\\ ] ] and the result follows by induction .      3 , _ the method of resultants for computing real solutions of polynomial systems _ , siam j. numer .",
    ", 29 ( 1992 ) , pp .",
    "831844 . , _ a numerical method for polynomial eigenvalue problems using contour integral _ , japan j. indust .",
    "appl . math .",
    ", 27 ( 2010 ) , pp .",
    ", _ numerically solving polynomial systems with bertini _ , siam , 2013 . , _ computing curve intersection by means of simultaneous iterations _ , numer .",
    "algor . , 43 ( 2006 ) , pp .",
    "151175 . ,",
    "_ solving polynomial eigenvalue problems by means of the ehrlich - aberth method _ , linear algebra appl . , 439 ( 2013 ) , pp .  11301149",
    ". , _ solution of a polynomial system of equations via the eigenvector computation _ , linear algebra appl .",
    ", 319 ( 2000 ) , pp .",
    "193209 . , _ computing zeros on a real interval through chebyshev expansion and polynomial rootfinding _",
    ", siam j. numer .",
    ", 40 ( 2002 ) , pp .",
    "16661682 . ,",
    "_ solving transcendental equations : the chebyshev polynomial proxy and other numerical rootfinders _ , siam , 2014 . , _ an algorithm for finding the basis elements of the residue class ring of a zero dimensional polynomial ideal _ , j. symbolic comput . , 41 ( 2006 ) , pp",
    "475511 . , _ resultant - based methods for plane curves intersection problems _ , computer algebra in scientific computing , springer berlin heidelberg , 3718 ( 2005 ) , pp .  7592 . ,",
    "_ introduction to residues and resultants _ , in , a. dickenstein and i. z. emiris , editors , _ solving polynomial equations .",
    "foundations , algorithms , and applications _ , springer , 2005 .",
    ", _ on the theory of elimination _ , cambridge and dublin math .",
    "j. iii , ( 1848 ) , pp .",
    ", _ fast computation of the bzout and dixon resultant matrices _ , j.  symb .",
    "comput . , 33 ( 2002 ) ,",
    "1329 . , _ a note on the summation of chebyshev series _ , math .",
    "comput . , 9 ( 1955 ) ,",
    "118120 . , _ using algebraic geometry _ ,",
    "springer , 2013 . , _",
    "fiedler companion linearizations and the recovery of minimal indices _ , siam j. mat .",
    "appl . , 31 ( 2010 ) , pp .",
    "21812204 . , _ back to the roots : polynomial system solving , linear algebra , systems theory _ , proc .",
    "16th ifac symposium on system identification ( sysid ) , 2012 , pp .",
    "12031208 . , _ sparse elimination and applications in kinematics _ , dissertation , university of california , berkeley , 1994 . , _ symbolic and numeric methods for exploiting structure in constructing resultant matrices _ , j. symbolic comput . ,",
    "33 ( 2002 ) , pp .",
    "393413 . , _",
    "discriminants , resultants , and multidimensional determinants _ , springer , birkhuser , boston , 2008 . , _ the ehrlich  aberth method for palindromic matrix polynomials represented in the dickson basis _ , linear algebra appl . , 438 ( 2013 ) , pp .",
    ", _ matrix polynomials _ , siam , philadelphia , usa , 2009 , ( unabridged republication of book first published by academic press in 1982 ) . , _ the colleague matrix , a chebyshev analogue of the companion matrix _ , the quarterly journal of mathematics , 12 ( 1961 ) , pp .",
    ", _ accuracy and stability of numerical algorithms _ ,",
    "siam , 2nd edition , 2002 .",
    ", _ function of matrices : theory and computation _ , siam , 2008 . , _ matrix analysis _ , cambridge university press , 2nd edition , new york , 2013 .",
    ", _ accurate solution of polynomial equations using macaulay resultant matrices _ , math .",
    ", 74 ( 2005 ) , pp .",
    "221262 . ,",
    "_ comparison of various multivariate resultant formulations _ ,",
    "acm proceedings of the 1995 international symposium on symbolic and algebraic computation , 1995 . , _ complex algebraic curves _ , cambridge university press , cambridge , 1992 .",
    ", _ a simple solution to the six - point two - view focal - length problem _ , computer vision  eccv , springer berlin heidelberg , ( 2006 ) , pp .",
    "200213 . ,",
    "_ vector spaces of linearizations for matrix polynomials _ , siam j. matrix anal . appl . , 28 ( 2006 ) , pp .",
    ", _ multipolynomial resultants and linear algebra _ , papers from the international symposium on symbolic and algebraic computation .",
    "acm , 1992 . , _",
    "algorithms for intersecting parametric and algebraic curves i : simple intersections _ ,",
    "acm trans .",
    "graphics , 13 ( 1994 ) , pp .",
    "_ vector spaces of linearizations for matrix polynomials : a bivariate polynomial approach _ , submitted .",
    ", _ computing the common zeros of two bivariate functions via bzout resultants _ , numer .",
    ", 129 ( 2015 ) , pp .",
    ", _ block tensor unfoldings _ , siam j. mat . anal . appl . , 33 ( 2012 ) , pp .",
    "149169 . , _ polynomial evaluation and associated polynomials _ , numer .",
    ", 79 ( 1998 ) , pp .",
    "601613 . , _ the numerical solution of systems of polynomials arising in engineering and science _ , world scientific , 2005 . ,",
    "_ numerical solution of bivariate and polyanalytic polynomial systems _",
    ", siam j. numer .",
    ", 52 ( 2014 ) , pp .",
    "15511572 . , _",
    "finding all real zeros of polynomial systems using multi - resultant _",
    ", j. comput .",
    ", 167 ( 2004 ) , pp .",
    "417428 . ,",
    "_ an algorithm for quadratic eigenproblems with low rank damping _ , siam j. matrix anal .",
    "appl . , 36 ( 2015 ) , pp .",
    ", _ backward error and condition of polynomial eigenvalue problems _ , linear algebra appl .",
    ", 309 ( 2000 ) , pp .",
    "339361 . , _ the quadratic eigenvalue problem _ , siam review , 43 ( 2001 ) , pp .",
    "235286 . ,",
    "_ computing with functions of two variables _ , dphil thesis , the university of oxford , 2014 . , _ an extension of chebfun to two dimensions _",
    ", siam j. sci .",
    "comput . , 35 ( 2013 ) , c495c518 .",
    ", _ resultant methods for the inverse kinematics problem _ , computational kinematics , springer netherlands , 28 ( 1993 ) , pp ."
  ],
  "abstract_text": [
    "<S> hidden - variable resultant methods are a class of algorithms for solving multidimensional polynomial rootfinding problems . in two dimensions , </S>",
    "<S> when significant care is taken , they are competitive practical rootfinders . however , in higher dimensions they are known to miss zeros , calculate roots to low precision , and introduce spurious solutions . </S>",
    "<S> we show that the hidden variable resultant method based on the cayley ( dixon or bzout ) matrix is inherently and spectacularly numerically unstable by a factor that grows exponentially with the dimension . </S>",
    "<S> we also show that the sylvester matrix for solving bivariate polynomial systems can square the condition number of the problem . </S>",
    "<S> in other words , two popular hidden variable resultant methods are numerically unstable , and this mathematically explains the difficulties that are frequently reported by practitioners . </S>",
    "<S> regardless of how the constructed polynomial eigenvalue problem is solved , severe numerical difficulties will be present . along the way </S>",
    "<S> , we prove that the cayley resultant is a generalization of cramer s rule for solving linear systems and generalize clenshaw s algorithm to an evaluation scheme for polynomials expressed in a degree - graded polynomial basis .    </S>",
    "<S> resultants , rootfinding , conditioning , multivariate polynomials , cayley , sylvester    13p15 , 65h04 , 65f35 </S>"
  ]
}