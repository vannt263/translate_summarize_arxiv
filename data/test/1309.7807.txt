{
  "article_text": [
    "filtering is engineering terminology for extracting information about a signal from partial and noisy observations . in geophysics ,",
    "filtering is usually called data assimilation . in the last 50 years , filtering has been mainly studied in the framework of state space or hidden markov models , assuming a markovian time evolution of the signal and observations which are instantaneous functions of the signal subject to white observation noise .",
    "developments started in the 1960s with the kalman ",
    "bucy filter ( @xcite , @xcite ) for linear gaussian models and with the forward ",
    "backward algorithm due to baum and welch for models with a finite state space ( see p. 74 of @xcite for the history of this algorithm , including references ) .",
    "the essential feature of these methods is that they are recursive and thus suitable for online applications where the observations arrive sequentially and quantities of interest have to be recomputed with each new observation .",
    "probabilists started in the mid - sixties to develop a general theory of nonlinear filtering in continuous time . in statistics , state space models and filtering techniques took longer to take roots . in the seventies and eighties ,",
    "the relation between linear state space and arma models was studied and used .",
    "a breakthrough occurred with the paper @xcite which developed recursive monte carlo methods called particle filters .",
    "interestingly , @xcite had proposed much earlier to use monte carlo methods , but the idea of resampling was missing . however , this idea is essential to ensure that the required sample size for a given accuracy does not explode with the number of time steps .",
    "particle filters quickly became very popular . among other things",
    "they have also been used for continuous time filtering .",
    "nowadays , they are also applied outside the context of state space models as a complement to other , static mcmc methods . in the 1990s",
    ", geophysicists developed a different version of the particle filter , called the ensemble kalman filter which is more stable in high dimensions .",
    "after some delay , this idea has now also become part of the research in statistics .",
    "there are many presentations of the topic in books and in survey articles ( e.g. , @xcite , @xcite , @xcite , capp , moulines and rydn ( @xcite ) , @xcite , @xcite , @xcite ) .",
    "this paper gives a brief introduction for non - specialists , explaining the main algorithms , describing their scope and also their limitations and surveying some of the interesting current developments . because of limitations of space , many interesting topics and references that would deserve to be mentioned had to be omitted",
    "a state space model consists of an unobservable @xmath0-valued markov process @xmath1 , the state of a system or the signal , combined with partial and noisy @xmath2-valued observations @xmath3 of the state at discrete times @xmath4 . in order to simplify the notation ,",
    "we assume @xmath5 .",
    "we denote the initial distribution of @xmath6 by @xmath7 and the conditional distribution of @xmath8 given @xmath9 by @xmath10 .",
    "observations at different times are assumed to be conditionally independent given the states , and the conditional distributions of @xmath11 given @xmath8 are assumed to have densities @xmath12 with respect to some reference measure @xmath13 ( usually the lebesgue or the counting measure ) .",
    "time homogeneity of these conditional distributions is only assumed to simplify notation .",
    "the state process can be in continuous or discrete time . in the former case , the transition kernel @xmath14 is usually not available analytically .",
    "for some of the algorithms , this is not necessary , it is sufficient that we are able to simulate from @xmath15 for any value @xmath16 . because some applications have a deterministic or partially deterministic state evolution , we do not assume the existence of densities for @xmath14 . throughout , notation like @xmath17 for @xmath18 is used . by a slight abuse of notation , @xmath19  stands for any ( conditional ) density : the arguments of @xmath19 will indicate which random variables are involved .",
    "the ratio of two probability measures is an abbreviation for the radon  nikodym derivative .",
    "state space models have a wide range of applications in finance ( stochastic volatility , interest rates ) , engineering ( tracking , speech recognition , computer vision ) , biology ( genome sequence analysis , ion channels , stochastic kinetic models ) , geophysics ( meteorology , oceanography , reservoir modeling ) , analysis of longitudinal data and others .",
    "it is not possible here to describe these applications in detail or give references to all relevant pulications .",
    "some of these applications are discussed in @xcite and in @xcite .",
    "a  few references of more recent applications are @xcite and @xcite for biology , part ix in @xcite for financial mathematics , and @xcite , @xcite and @xcite for geophysical applications .",
    "by the assumptions on the state and observation process , we have the following joint distributions for @xmath20 @xmath21 the information about the state contained in the observations is expressed by the conditional distributions @xmath22 of @xmath23 given @xmath24 .",
    "of particular interest are @xmath25 , called here the joint smoothing distribution , and @xmath26 , called here the filter distribution ( the terminology is not unique ) . for @xmath20 , @xmath27 follows immediately from ( [ eqjoint ] ) and bayes formula .",
    "other cases are then obtained in principle by marginalization .",
    "we are however interested in methods to compute or approximate expectations with respect to these distributions in an explicit and efficient way . for this , recursive formulae are most useful .",
    "it is straightforward to verify that @xmath28 where @xmath29 by marginalization , we therefore also have the recursions @xmath30 in both cases , the recursions consist of a propagation step ( [ eqprop1 ] ) or ( [ eqprop2 ] ) , respectively , and an update or correction step , ( [ equpdate1 ] ) or ( [ equpdate2 ] ) , respectively .",
    "typically , one wants to compute these recursions for an arbitrary , but fixed sequence @xmath31 ( not necessarily a realization from the state space model ) .",
    "there are two important special cases where one can perform the above recursions exactly . in the first one ,",
    "the state space @xmath32 is finite and the integrals reduce to finite sums which can be computed with @xmath33 operations .",
    "the second special case are linear gaussian state space models where @xmath34 and @xmath35",
    ". if @xmath7 is also gaussian , then all @xmath36 are gaussian and ( [ eqprop2])([equpdate2 ] ) lead to recursions for the conditional means and covariances . for comparison with the ensemble kalman filter below ,",
    "we write down the update step for going from @xmath37 to @xmath38 : @xmath39 where @xmath40 is the so - called kalman gain .    in most other cases of practical interest",
    ", one has to approximate the integrals involved in ( [ eqprop2 ] ) and ( [ eqlikeli ] ) .",
    "numerical approximations are difficult to use because the region of main mass of @xmath36 changes with @xmath41 and is unknown in advance .",
    "the particle filter tries to generate values in this region adaptively as new observations arise .",
    "the particle filter recursively computes importance sampling approximations of @xmath42 , that is @xmath43 here the @xmath44 are random weights which sum to one , @xmath45 are random variables called `` particles '' and @xmath46 is the point mass at @xmath16 . at time @xmath47 , we draw particles from @xmath7 and set @xmath48 . at time @xmath41",
    "we start with @xmath49 and draw independently new particles @xmath50 from @xmath51 . by ( [ eqprop2 ] ) , the particles @xmath50 with weights @xmath52 provide an importance sampling approximation of @xmath53 .",
    "if we also update the weights with @xmath54 , we have closed the recursion by ( [ equpdate2 ] ) .",
    "this algorithm has the drawback that after a few iterations most particles are located at positions very far away from the region of main mass of @xmath36 and the weights are very unbalanced .",
    "this can be avoided by introducing a resampling step before propagation such that particles with low weights die and particles with high weights have much offspring that is independently propagated afterwards .",
    "thus the basic particle filter , also called the bootstrap filter or sir - filter ( sampling importance resampling ) , works as follows .",
    "resample : draw @xmath55 from @xmath56 .",
    "propagate : draw @xmath50 from @xmath57 , independently for different indices @xmath58 .",
    "reweight : set @xmath59 .    note that for any function @xmath60 , @xmath61 always has a larger variance than @xmath62 .",
    "the advantage of resampling is seen only after one or several propagation steps .",
    "because of this , we resample at the beginning and not at the end of a recursion .    as a byproduct",
    ", the particle filter gives also the following estimate of ( [ eqlikeli ] ) @xmath63 one can show by induction that the product @xmath64 is an exactly unbiased estimator of @xmath65 for any @xmath41 and any @xmath66 , see theorem 7.4.2 in @xcite .",
    "however , @xmath67 is in general not unbiased for @xmath68 .      because the numbering of particles is irrelevant",
    ", we only need to know the number of times @xmath69 that the @xmath58-th particle is selected in the resampling step .",
    "one can therefore reduce the additional variability introduced by resampling by a so - called balanced resampling scheme , meaning that @xmath70 and @xmath71 .",
    "the simplest such scheme uses a uniform@xmath72 random variable @xmath73 and takes as @xmath74 the number of points in the intersection of @xmath75 with @xmath76 $ ] .",
    "see @xcite for other balanced resampling schemes .",
    "since balanced resampling can always be used at little extra cost , it is widely used .",
    "a second improvement omits the resampling step whenever the weights are sufficiently uniform . as criterion",
    ", one often uses the so - called effective sample size which is defined as one over @xmath77 , see @xcite for a justification of the name of this criterion .    in the propagation step , we can draw @xmath50 not from @xmath57 , but from any other distribution @xmath78 which dominates @xmath57 .",
    "we then have to adjust the weights in the reweighting step .",
    "the correct weights are obtained by setting @xmath79 in step 4 of algorithm  [ alg2 ] below . by letting @xmath78",
    "depend not only on @xmath80 , but also on the new observation @xmath81 , we can make the propagated particles @xmath82 more compatible with @xmath83 and thus the weights more balanced . in the so - called auxiliary particle filter due to @xcite ,",
    "one uses the new observation @xmath83 not only in the propagation step , but also in an additional reweighting step before resampling .",
    "the goal of this additional reweighting is to bring @xmath49 closer to @xmath84 .",
    "thus , the auxiliary particle filter works as follows .",
    "[ alg2 ] 1 .",
    "reweight : set @xmath85 where @xmath86 .",
    "resample : draw @xmath55 from @xmath87 .    \\3 .",
    "propagate : draw @xmath50 from @xmath88 , independently for different indices @xmath58 .",
    "reweight : set @xmath89    in order to understand the formula for @xmath90 , note that @xmath91 has distribution proportional to @xmath92 and the distribution target is proportional to @xmath93 . because the average of the unnormalized weights @xmath90 estimates the ratio of the normalizing constants , the estimate of ( [ eqlikeli ] ) is now @xmath94 the product @xmath95 is again unbiased for @xmath96 .",
    "auxiliary particle filters can not be used if the state evolution is deterministic , or if the density @xmath97 is not available in closed form . in other cases ,",
    "the choices of @xmath98 and @xmath78 are up to the user . ideally , we take @xmath99 and @xmath100 , because then the weights @xmath101 in the fourth step are constant . in most cases , these choices are not possible , but one can try to find suitable approximations . with the ideal choices for @xmath98 and @xmath78 , the auxiliary particle filter therefore leads to a reweighting with @xmath102 instead of @xmath103 : although this usually reduces the variance of the weights , the gain may not be substantial . in principle",
    ", it is possible to go further back in time by computing particle filter approximations of @xmath104 for some @xmath105 .",
    "an auxiliary particle filter in this case uses @xmath106 to reweight the particles at time @xmath107 and to generate new particles at times @xmath108 to @xmath41 .",
    "the main difficulty with the particle filter is that often weights become unbalanced , even when we use the auxiliary particle filter in algorithm  [ alg2 ] or apply some of the other simple improvements discussed above . in such cases ,",
    "most resampled particles coincide ( `` sample depletion '' ) .",
    "if the state transitions are partially deterministic , this becomes especially drastic because the propagation will not create diversity .",
    "partially deterministic state transitions occur for instance if the model contains unknown parameters @xmath109 in the state transition @xmath14 or in the observation density @xmath12 and one proceeds by considering the enlarged state vector @xmath110 .",
    "the propagation step for @xmath109 is then simply @xmath111 .",
    "one can add some noise to create diversity , possibly combined with some shrinking towards the mean to keep the variance the same .",
    "still , this does not always work well .",
    "a second instance with partially deterministic state transitions occurs if one uses the particle filter algorithm to approximate not only @xmath36 , but the whole smoothing distribution  @xmath112 . in principle",
    ", this is straightforward : each particle at time @xmath41 is then a path of length @xmath113 that we write as @xmath114 .",
    "the propagation step concatenates a resampled path @xmath115 with a new value @xmath116 .",
    "if the weights at one time point become very unbalanced , the filter can be completely unreliable and it can lose track even though the propagation step later creates again diversity .",
    "unbalanced weights have been observed to occur easily if the dimension of the observations is large .",
    "a theoretical explanation of this phenomenon has been provided by @xcite .    in the following , we discuss some more advanced methods that have been proposed to overcome these difficulties .",
    "@xcite have proposed the following method to avoid sample depletion when the particle filter is used to produce an approximation of @xmath112 with particles @xmath114 and equal weights .",
    "let @xmath117 be a markov kernel on @xmath118 which has @xmath112 as invariant distribution , constructed for instance according to the general metropolis ",
    "hastings recipe .",
    "drawing new particles @xmath119 , independently for different @xmath58 s will then give a new approximation of @xmath112 which is expected to be at least as good as the old one .",
    "if @xmath117 modifies all components of @xmath114 , this method also removes ties , but since typically a single kernel can only update one or a few components of @xmath114 , the computational complexity increases with @xmath41 if one wants to get rid of all ties .",
    "this method is due to @xcite .",
    "it assumes linear observations with gaussian errors , that is , @xmath120 is a normal density with mean @xmath121 and variance @xmath122 .",
    "it uses particles with equal weights , the propagation step is the same as in the particle filter whereas the update step is a monte carlo implementation of the kalman filter update ( [ eqkalman ] ) with estimated first and second moment of @xmath53 :    \\1 .",
    "propagate : draw @xmath123 from @xmath51 .",
    "update : draw i.i.d . values @xmath124 and set @xmath125 where @xmath126 is the kalman gain computed with the sample covariance @xmath127 of the @xmath123 s .",
    "it is not difficult to show that the algorithm is consistent as @xmath128 for a linear gaussian state space model .",
    "however , for non - gaussian @xmath53 , this update typically has a systematic error because only the location , but neither the spread nor the shape of the sample @xmath129 change if @xmath81 changes .",
    "nevertheless , the ensemble kalman filter is extremely wide - spread in geophysical applications where the state evolution is usually complicated , making the propagation step the computational bottleneck .",
    "this forces one to use a sample size @xmath130 which is much smaller than the dimensions of the state or the observation .",
    "even in such cases , the ensemble kalman filter turns out to be surprisingly robust  provided we regularize the estimate @xmath127 of the covariance of @xmath53 .",
    "several attempts have been made to find algorithms which combine the robustness of the ensemble kalman filter with the nonparametric features of the particle filter .",
    "they either approximate @xmath53 by a mixture of gaussians or use the ensemble kalman filter as a proposal distribution @xmath78 in a particle filter . see @xcite for references and a new proposal which avoids both the fitting of a gaussian mixture to the forecast sample @xmath131 and the estimation of the density @xmath132 (",
    "which is usually not known analytically in these applications ) .",
    "an extension of the ensemble kalman filter to more general observation densities @xmath12 has been given in @xcite .",
    "in an offline application where all @xmath133 observations are available from the beginning , one can use smoothing algorithms which combine a forward filtering pass through the data from @xmath134 to @xmath135 with a backward recursion from @xmath136 to @xmath134 .",
    "we limit ourselves to approximations of the marginals @xmath137 , but the same methods apply also for joint distributions .    by bayes formula and conditional independence , we obtain the following relations @xmath138 this is also called the two - filter formula because we have the recursions @xmath139 which are dual to ( [ eqprop2 ] ) and ( [ equpdate2 ] ) . combining ( [ eqsmooth0])([eqsmooth1 ] ) with ( [ eqprop - back ] ) gives @xmath140    in order to be able to use monte carlo methods , we have to assume that for any @xmath141 the state transition kernel @xmath142 has density @xmath143 with respect to some measure @xmath144 on  @xmath32",
    ". then the filter distributions also have densities which we denote by the same symbol .",
    "the right - hand side of ( [ eqsmooth2 ] ) can then be considered as an integral with respect to @xmath145 .",
    "thus we obtain a marginal particle smoother @xmath146 which has the same particles as the filter , but different weights @xmath147 which are computed with the recursion @xmath148 the disadvantage is the complexity of the algorithm which is of the order @xmath149 .    the algorithm in @xcite",
    "computes first backward particle approximations of the distributions @xmath150 where @xmath151 is a known function such that @xmath152 is integrable .",
    "inserting a forward particle filter approximation for @xmath53 and a backward particle filter approximation for @xmath153 into ( [ eqsmooth0 ] ) gives then an approximation of @xmath137 which is concentrated on the particles approximating @xmath154 .",
    "@xcite have suggested to insert particle approximations into @xmath155 which follows by combining ( [ eqsmooth0 ] ) with ( [ eqprop - back ] ) and ( [ equpdate - back ] ) .",
    "this has the advantage that the support of @xmath146 is not constrained on the sampled particles from the forward or the backward recursion .",
    "moreover , one can sample from the approximation with an algorithm of complexity @xmath156 which may not be efficient , however .",
    "this is a recent innovation by @xcite which uses particle filters as a building block in an mcmc algorithm .",
    "assume that @xmath12 and the density of @xmath14 both depend on an unknown parameter @xmath109 with prior density @xmath157 and that we want to sample from the posterior @xmath158 . a gibbs sampler which updates single components of @xmath159 given the rest is usually too slow , and exact updates of the whole sequence @xmath159 are usually not possible .",
    "what the particle filter provides are _ random approximations _ @xmath160 of @xmath161 for any fixed @xmath109 .",
    "@xcite show that with these random approximations one can still construct markov chains which leave the correct posterior invariant without letting the number of particles go to infinity .",
    "the first such algorithm is called particle marginal metropolis  hastings sampler .",
    "it is an approximation of the sampler which jointly proposes @xmath162 from the distribution @xmath163 with the acceptance ratio @xmath164 the approximation occurs at two places : first @xmath165 is generated from @xmath166 instead of  @xmath167 , and second the unknown likelihoods @xmath168 and",
    "@xmath169 in the acceptance ratio are replaced by unbiased estimates from the particle filter .",
    "the surprising result is that the errors from these two approximations cancel and the algorithm has the exact posterior @xmath158 as invariant distribution for any @xmath130 .    instead of jointly proposing a parameter and a path of the state process",
    ", one can also use a gibbs sampler , alternating between updates of the parameter and the state process . updating the parameter given",
    "the state and the observations is usually feasible , but for the other update one samples again from a particle filter approximation @xmath160 and not from @xmath167 .",
    "@xcite show that this also gives a correct algorithm for any @xmath170 provided the particle filter approximation is modified such that the current path is equal to one of the particle paths @xmath171 in @xmath160 .",
    "laws of large numbers as well as central limit theorems have been shown for particle filter approximations .",
    "@xcite contains general results , @xcite gives an essentially self - contained short derivation .",
    "first , one can show that for every @xmath41 , every @xmath66 and a suitable class of functions @xmath172 , @xmath173 converges in probability or almost surely to @xmath174 .",
    "the proof works by induction on @xmath41 , assuming that @xmath49 is close to @xmath175 .",
    "this error propagates in the next particle filter iteration , but one can control by how much it grows in the worst case , and the additional monte carlo error in the @xmath41-th step can be bounded by standard methods , at least with multinomial ( independent ) resampling . for balanced sampling , there seems to be still no general proof .",
    "however , such a result is of limited use because the required sample size @xmath130 may grow exponentially with the number of steps @xmath41 . for applications , it is more relevant to find conditions under which the convergence is uniform in @xmath41",
    ". this is more difficult because  in contrast to the propagation step  the update step is in general not contractive and the above induction argument does not succeed .",
    "one has instead to study the error propagation over several time steps .",
    "this is equivalent to the question if and how fast the filter forgets its initial distribution @xmath7 which has been studied extensively , see e.g. @xcite .",
    "much of the probability literature on filtering considers both state and observation processes in continuous time .",
    "more precisely , @xmath176 is assumed to satisfy the following evolution equation @xmath177 where @xmath178 is a multivariate brownian motion .",
    "we again denote by @xmath179 the conditional distribution of @xmath180 given the @xmath181-field generated by the observations @xmath182 ( completed by all null sets ) .",
    "note that @xmath183 is a stochastic process which takes values in the set of probability measures on @xmath184 .",
    "the evolution equation for @xmath183 corresponding to the recursions ( [ eqprop2])([equpdate2 ] ) is a stochastic pde , the kushner ",
    "stratonovich equation .",
    "a particle filter approximation consists of interacting particles @xmath185 and associated weights @xmath186 : within an interval of length @xmath187 they evolve independently , whereas at multiples of @xmath187 there is a resampling step like in the discrete case , see @xcite for more details .",
    "particle filtering algorithms have found many applications outside the state space framework . in these cases ,",
    "the more general term sequential monte carlo is used .",
    "assume we have a complicated target distribution @xmath188 on @xmath184 which we can not sample directly .",
    "in such a situation , a  promising strategy consists of sampling recursively from a sequence @xmath189 where @xmath7 is a simple distribution , @xmath190 is the target one is interested in , and @xmath36 is close to @xmath175 .",
    "one example is the posterior distribution of a parameter with a large number @xmath133 of observations where @xmath36 the posterior for the first @xmath41 observations . in another example , the @xmath36 s are tempered approximations of @xmath191 : @xmath192    starting with a sample from @xmath7 , one wants to recursively generate samples @xmath193 from @xmath36 by resampling , propagation and reweighting as in the particle filter .",
    "if @xmath117 denotes the transition kernel in the @xmath41-th propagation step , then for reweighting we need the density of @xmath36 with respect to latexmath:[$\\int\\pi_{n-1}(dx')k_n(\\cdot    @xmath117 such that it leaves @xmath175 invariant .",
    "the idea in @xcite which allows more flexibility for the choice of @xmath117 is to consider the distributions @xmath195 and @xmath196 on the product space @xmath197 . here",
    "@xmath198 is an arbitrary kernel such that these two distributions are absolutely continuous .",
    "if @xmath199 is a ( weighted ) sample from @xmath175 and we draw @xmath45 from @xmath200 independently for different @xmath58 s , then @xmath201 is a ( weighted ) sample from @xmath202 .",
    "we can convert this into a weighted sample from @xmath196 because the radon  nikodym density can be computed without integration . by marginalization",
    ", we finally obtain the desired weighted sample from @xmath36 . in @xcite , the optimal choice of @xmath198 for given @xmath117 is determined .",
    "particle filters are also used in rare event simulation , see e.g. @xcite .",
    "assume @xmath203 is a markov process with fixed starting point @xmath204 , @xmath205 and @xmath206 are two stopping times and we are interested in @xmath207 which is small so simple monte carlo is inefficient . in a technique",
    "called `` importance splitting '' one introduces a sequence of stopping times @xmath208 and sets @xmath209 .",
    "moreover , we introduce `` observations '' @xmath210}$ ] . then for @xmath211 , @xmath36 is the conditional distribution of @xmath212 given @xmath213 , and @xmath214 is the probability we would like to estimate .",
    "hence , we can estimate this probability unbiasedly with a particle filter since it gives _ unbiased _ estimates of @xmath96 . because the observations are deterministic functions of the state",
    ", the resampling step simply duplicates the particles with @xmath213 until the sample size is again @xmath130 . @xcite",
    "propose to control precision instead of computational effort .",
    "this means that in the @xmath41-th step we do not propagate a fixed number of particles and see how many of them satisfy @xmath213 , but rather propagate particles until a fixed number of them satisfies @xmath213 .",
    "one can still obtain an unbiased estimator , and in addition this can increase the efficiency of the algorithm .",
    "also in other applications of the particle filter where all observations are available from the beginning , it can be worthwile to aim for a fixed precision instead of a fixed computational effort in each iteration , using for instance accept - reject methods instead of importance sampling ( see @xcite ) ."
  ],
  "abstract_text": [
    "<S> this is a short review of monte carlo methods for approximating filter distributions in state space models . </S>",
    "<S> the basic algorithm and different strategies to reduce imbalance of the weights are discussed . </S>",
    "<S> finally , methods for more difficult problems like smoothing and parameter estimation and applications outside the state space model context are presented . </S>"
  ]
}