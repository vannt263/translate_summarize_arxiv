{
  "article_text": [
    "graphical models are used in many application domains , running the gamut from computer vision and civil engineering to political science and epidemiology . in many applications , estimating the edge structure of an underlying graphical model is of significant interest .",
    "for instance , a graphical model may be used to represent friendships between people in a social network  @xcite or links between organisms with the propensity to spread an infectious disease  @xcite .",
    "it is a classical corollary of the hammersley ",
    "clifford theorem  @xcite that zeros in the inverse covariance matrix of a multivariate gaussian distribution indicate absent edges in the corresponding graphical model .",
    "this fact , combined with various types of statistical estimators suited to high dimensions , has been leveraged by many authors to recover the structure of a gaussian graphical model when the edge set is sparse ( see the papers  @xcite and the references therein ) .",
    "recently , liu et al .",
    "@xcite and liu , lafferty and wasserman @xcite introduced the notion of a  nonparanormal distribution , which generalizes the gaussian distribution by allowing for monotonic univariate transformations , and argued that the same structural properties of the inverse covariance matrix carry over to the nonparanormal ; see also the related work of xue and zou  @xcite on copula transformations .",
    "however , for non - gaussian graphical models , the question of whether a general relationship exists between conditional independence and the structure of the inverse covariance matrix remains unresolved . in this paper , we establish a number of interesting links between covariance matrices and the edge structure of an underlying graph in the case of discrete - valued random variables .",
    "( although we specialize our treatment to multinomial random variables due to their widespread applicability , several of our results have straightforward generalizations to other types of exponential families . ) instead of only analyzing the standard covariance matrix , we show that it is often fruitful to augment the usual covariance matrix with higher - order interaction terms .",
    "our main result has an interesting corollary for tree - structured graphs : for such models , the inverse of a generalized covariance matrix is always ( block ) graph - structured . in particular , for binary variables ,",
    "the inverse of the usual covariance matrix may be used to recover the edge structure of the tree .",
    "we also establish more general results that apply to arbitrary ( nontree ) graphs , specified in terms of graph triangulations .",
    "this more general correspondence exploits ideas from the geometry of exponential families  @xcite , as well as the junction tree framework  @xcite .",
    "as we illustrate , these population - level results have a number of corollaries for graph selection methods .",
    "graph selection methods for gaussian data include neighborhood regression  @xcite and the graphical lasso  @xcite , which corresponds to maximizing an @xmath0-regularized version of the gaussian likelihood .",
    "alternative methods for selection of discrete graphical models include the classical chow  liu algorithm for trees  @xcite ; techniques based on conditional entropy or mutual information  @xcite ; and nodewise logistic regression for discrete graphical models with pairwise interactions",
    "our population - level results imply that minor variants of the graphical lasso and neighborhood regression methods , though originally developed for gaussian data , remain consistent for trees and the broader class of graphical models with singleton separator sets .",
    "they also convey a cautionary message , in that these methods will be inconsistent ( generically ) for other types of graphs .",
    "we also describe a new method for neighborhood selection in an arbitrary sparse graph , based on linear regression over subsets of variables .",
    "this method is most useful for bounded - degree graphs with correlation decay , but less computationally tractable for larger graphs .",
    "in addition , we show that our methods for graph selection may be adapted to handle noisy or missing data in a seamless manner . naively applying nodewise logistic regression when",
    "observations are systematically corrupted yields estimates that are biased even in the limit of infinite data .",
    "there are various corrections available , such as multiple imputation  @xcite and the expectation - maximization ( em ) algorithm  @xcite , but , in general , these methods are not guaranteed to be statistically consistent due to local optima . to the best of our knowledge",
    ", our work provides the first method that is provably consistent under high - dimensional scaling for estimating the structure of discrete graphical models with corrupted observations .",
    "further background on corrupted data methods for low - dimensional logistic regression may be found in carroll , ruppert and stefanski @xcite and ibrahim et al .",
    "@xcite .",
    "the remainder of the paper is organized as follows . in section  [ secbackground ] , we provide brief background and notation on graphical models and describe the classes of augmented covariance matrices we will consider . in section  [ secresults ] , we state our main population - level result ( theorem  [ thmfull ] ) on the relationship between the support of generalized inverse covariance matrices and the edge structure of a discrete graphical model , and then develop a number of corollaries",
    ". the proof of theorem  [ thmfull ] is provided in section  [ secproofthmfull ] , with proofs of corollaries and more technical results deferred to the supplementary material @xcite . in section  [ secsample ] , we develop consequences of our population - level results in the context of specific methods for graphical model selection .",
    "we provide simulation results in section  [ secsims ] in order to confirm the accuracy of our theoretically - predicted scaling laws , dictating how many samples are required ( as a function of graph size and maximum degree ) to recover the graph correctly .",
    "in this section , we provide background on graphical models and exponential families .",
    "we then present a simple example illustrating the phenomena and methodology underlying this paper .",
    "an _ undirected graphical model _ or _ markov random field _ ( mrf ) is a family of probability distributions respecting the structure of a fixed graph .",
    "we begin with some basic graph - theoretic terminology .",
    "an undirected graph @xmath1 consists of a collection of vertices @xmath2 and a collection of unordered and the edge @xmath3 . in this paper , we forbid graphs with self - loops , meaning that @xmath4 for all @xmath5 . ] vertex pairs @xmath6 . a  _",
    "vertex cutset _ is a subset  @xmath7 of vertices whose removal breaks the graph into two or more nonempty components [ see figure  [ figgraphprops](a ) ] .",
    "clique _ is a subset @xmath8 such that @xmath9 for all distinct @xmath10 .",
    "the cliques in figure  [ figgraphprops](b ) are all _ maximal _ , meaning they are not properly contained within any other clique . for @xmath11 , we define the neighborhood @xmath12 to be the set of vertices connected to @xmath13 by an edge .    for an undirected graph @xmath14",
    ", we associate to each vertex @xmath15 a random variable  @xmath16 taking values in a space @xmath17 . for any subset @xmath18",
    ", we define @xmath19 , and for three subsets of vertices , @xmath20 , @xmath21 and @xmath7 , we write @xmath22 to mean that the random vector @xmath23 is conditionally independent of @xmath24 given @xmath25 . the notion of a markov random field",
    "may be defined in terms of certain _ markov properties _ indexed by vertex cutsets or in terms of a _ factorization property _",
    "described by the graph cliques .    [ cols=\"^,^ \" , ]",
    "still focusing on the single - cycle graph in panel  ( b ) , suppose that instead of considering the ordinary covariance matrix , we compute the covariance matrix of the _ augmented _ random vector @xmath26 , where the extra term @xmath27 is represented by the dotted edge shown in panel  ( c ) .",
    "the @xmath28 inverse of this generalized covariance matrix takes the form @xmath29.\\ ] ] this matrix safely separates nodes @xmath30 and @xmath31 , but the entry corresponding to the nonedge @xmath32 is _ not _ equal to zero .",
    "indeed , we would observe a similar phenomenon if we chose to augment the graph by including the edge @xmath33 rather than @xmath32 .",
    "this example shows that the usual inverse covariance matrix is not always graph - structured , but inverses of augmented matrices involving higher - order interaction terms may reveal graph structure .",
    "now let us consider a more general graphical model that adds the 3-clique interaction terms shown in panel  ( d ) to the usual ising terms .",
    "we compute the covariance matrix of the augmented vector @xmath34 empirically , one may show that the @xmath35 inverse @xmath36)^{-1}$ ] respects aspects of the graph structure : there are zeros in position @xmath37 , corresponding to the associated functions @xmath38 and @xmath39 , whenever @xmath40 and @xmath41 do not lie within the same maximal clique .",
    "[ e.g. , this applies to the pairs @xmath42 and @xmath43 . ]",
    "the goal of this paper is to understand when certain inverse covariances do ( and _ do not _ ) capture the structure of a graphical model . at its root is the principle that the augmented inverse covariance matrix @xmath44 , suitably defined , is _ always _ graph - structured with respect to a graph triangulation . in some cases [ e.g. , the dino graph in figure  [ figgraphs](e ) ]",
    ", we may leverage the block - matrix inversion formula  @xcite , namely , @xmath45 to conclude that the inverse of a sub - block of the augmented matrix ( e.g. , the ordinary covariance matrix ) is still graph - structured .",
    "this relation holds whenever @xmath20  and  @xmath21 are chosen in such a way that the second term in equation  ( [ eqnchai ] ) continues to respect the edge structure of the graph .",
    "these ideas will be made rigorous in theorem  [ thmfull ] and its corollaries in the next section .",
    "we now state our main results on the relationship between the zero pattern of generalized ( augmented ) inverse covariance matrices and graph structure . in section  [ secsample ] to follow , we develop some consequences of these results for data - dependent estimators used in structure estimation .    we begin with some notation for defining generalized covariance matrices , stated in terms of the sufficient statistics previously defined  ( [ eqnsuff ] ) . recall that a  clique @xmath46 is associated with the collection @xmath47 of binary - valued sufficient statistics .",
    "let @xmath48 , and define the random vector @xmath49 consisting of all the sufficient statistics indexed by elements of @xmath50 .",
    "as in the previous section , the set @xmath51 contains both maximal and nonmaximal cliques .    we will often be interested in situations where @xmath50 contains all subsets of a given set . for a subset @xmath18 ,",
    "let @xmath52 denote the collection of all @xmath53 nonempty subsets of @xmath20 .",
    "we extend this notation to @xmath50 by defining @xmath54      our first main result concerns a connection between the inverses of generalized inverse covariance matrices associated with the model  ( [ eqngeneralmultinomial ] ) and any triangulation of the underlying graph @xmath55 .",
    "the notion of a triangulation is defined in terms of chordless cycles , which are sequences of distinct vertices @xmath56 such that :    * @xmath57 for all @xmath58 , and also @xmath59 ; * no other nodes in the cycle are connected by an edge .    as an illustration ,",
    "the @xmath31-cycle in figure  [ figgraphs](b ) is a chordless cycle .",
    "given an undirected graph @xmath1 , a _",
    "triangulation _ is an augmented graph @xmath60 that contains no chordless cycles of length greater than 3 .",
    "note that a tree is trivially triangulated , since it contains no cycles . on the other hand ,",
    "the chordless @xmath31-cycle in figure  [ figgraphs](b ) is the simplest example of a nontriangulated graph . by adding the single edge @xmath32 to form the augmented edge set @xmath61",
    ", we obtain the triangulated graph @xmath62 shown in panel  ( c ) .",
    "one may check that the more complicated graph shown in figure  [ figgraphs](e ) is triangulated as well .",
    "our first result concerns the inverse @xmath63 of the matrix @xmath64 , where @xmath65 is the set of all cliques arising from some triangulation @xmath66 of @xmath55 .",
    "for any two subsets @xmath67 , we write @xmath68 to denote the sub - block of @xmath63 indexed by all indicator statistics on @xmath20 and @xmath21 , respectively .",
    "( note that we are working with respect to the exponential family representation over the triangulated graph @xmath66 . )",
    "given our previously - defined sufficient statistics  ( [ eqnsuff ] ) , the sub - block @xmath68 has dimensions @xmath69 , where @xmath70 for example , when @xmath71 and @xmath72 , the submatrix @xmath68 has dimension @xmath73 . with this notation",
    ", we have the following result :    [ thmfull ] consider an arbitrary discrete graphical model of the form  ( [ eqngeneralmultinomial ] ) , and let @xmath74 be the set of all cliques in any triangulation of @xmath55 .",
    "then the generalized covariance matrix @xmath64 is invertible , and its inverse @xmath63 is _ block graph - structured _ :    a.   for any two subsets @xmath75 that are not subsets of the same maximal clique , the block @xmath76 is identically zero .",
    "b.   for almost all parameters @xmath77 , the entire block @xmath68 is nonzero whenever @xmath20  and  @xmath21 belong to a common maximal clique .    in part ( b ) , `` almost all '' refers to all parameters @xmath77 apart from a set of lebesgue measure zero .",
    "the proof of theorem  [ thmfull ] , which we provide in section  [ secproofthmfull ] , relies on the geometry of exponential families  @xcite and certain aspects of convex analysis  @xcite , involving the log partition function @xmath78 and its fenchel  legendre dual @xmath79 .",
    "although we have stated theorem  [ thmfull ] for discrete variables , it easily generalizes to other classes of random variables .",
    "the only difference is the specific choices of sufficient statistics used to define the generalized covariance matrix .",
    "this generality becomes apparent in the proof .    to provide intuition for theorem  [ thmfull ]",
    ", we consider its consequences for specific graphs .",
    "when the original graph is a tree [ such as the graph in figure  [ figgraphs](a ) ] , it is already triangulated , so the set @xmath80 is equal to the edge set @xmath81 , together with singleton nodes .",
    "hence , theorem  [ thmfull ] implies that the inverse @xmath63 of the matrix of sufficient statistics for vertices and edges is graph - structured , and blocks of nonzeros in @xmath63 correspond to edges in the graph . in particular",
    ", we may apply theorem  [ thmfull](a ) to the subsets @xmath82 and @xmath72 , where @xmath13 and @xmath83 are distinct vertices with @xmath84 , and conclude that the @xmath73 sub - block @xmath68 is equal to zero .",
    "when @xmath14 is not triangulated , however , we may need to invert a larger augmented covariance matrix and include sufficient statistics over pairs @xmath85 as well .",
    "for instance , the augmented graph shown in figure  [ figgraphs](c ) is a triangulation of the chordless @xmath31-cycle in panel  ( b ) .",
    "the associated set of maximal cliques is given by @xmath86 ; among other predictions , our theory guarantees that the generalized inverse covariance @xmath63 will have zeros in the sub - block @xmath87 .",
    "in fact , it is not necessary to take sufficient statistics over _ all _ maximal cliques , and we may consider a slightly smaller augmented covariance matrix .",
    "( this simpler type of augmented covariance matrix explains the calculations given in section  [ secexamples ] . )    by classical graph theory , any triangulation @xmath66 gives rise to a _ junction tree _ representation of @xmath55 .",
    "nodes in the junction tree are subsets of @xmath88 corresponding to maximal cliques of @xmath66 , and the intersection of any two adjacent cliques @xmath89 and @xmath90 is referred to as a _ separator set _ @xmath91 .",
    "furthermore , any junction tree must satisfy the _ running intersection property _ , meaning that for any two nodes of the junction tree ",
    "say , corresponding to cliques @xmath92 and @xmath93the intersection @xmath94 must belong to every separator set on the unique path between @xmath92 and @xmath93 .",
    "the following result shows that it suffices to construct generalized covariance matrices augmented by separator sets :    [ corsep ] let @xmath50 be the set of separator sets in any triangulation of @xmath55 , and let @xmath63 be the inverse of @xmath95 .",
    "then @xmath96 whenever @xmath97 .",
    "note that @xmath98 , and the set of sufficient statistics considered in corollary  [ corsep ] is generally much smaller than the set of sufficient statistics considered in theorem  [ thmfull ] .",
    "hence , the generalized covariance matrix of corollary  [ corsep ] has a smaller dimension than the generalized covariance matrix of theorem  [ thmfull ] , which becomes significant when we consider exploiting these population - level results for statistical estimation .",
    "the graph in figure  [ figgraphs](c ) of example  [ exasgraph ] and the associated matrix in equation  ( [ eqnaugmented ] ) provide a concrete example of corollary  [ corsep ] in action . in this case , the single separator set in the triangulation is @xmath99 , so when @xmath100 , augmenting the usual covariance matrix with the additional sufficient statistic @xmath101 and taking the inverse yields a graph - structured matrix . indeed , since @xmath102 , we observe that @xmath103 in equation  ( [ eqnaugmented ] ) , consistent with the result of corollary  [ corsep ] .",
    "although theorem  [ thmfull ] and corollary  [ corsep ] are clean population - level results , however , forming an appropriate augmented covariance matrix requires prior knowledge of the graph , namely , which edges are involved in a suitable triangulation .",
    "this is infeasible in settings where the goal is to recover the edge structure of the graph .",
    "corollary  [ corsep ] is most useful for edge recovery when @xmath14 admits a triangulation with only singleton separator sets , since then @xmath104 .",
    "in particular , this condition holds when @xmath14 is a tree .",
    "the following corollary summarizes our result :    [ cortree ] for any graph with singleton separator sets , the inverse @xmath63 of the covariance matrix @xmath105 of vertex statistics is graph - structured .",
    "( this class includes trees as a special case . )    in the special case of binary variables , we have @xmath106 , so corollary  [ cortree ] implies that the inverse of the ordinary covariance matrix @xmath107 is graph - structured . for @xmath108-ary variables ,",
    "@xmath109 is a matrix of dimensions @xmath110 involving indicator functions for each variable .",
    "again , we may relate this corollary to example  [ exasgraph]the inverse covariance matrices for the tree graph in panel  ( a ) and the dino graph in panel  ( e ) are exactly graph - structured .",
    "although the dino graph is not a tree , it possesses the nice property that the only separator sets in its junction tree are singletons .",
    "corollary  [ corsep ] also guarantees that inverse covariances may be partially graph - structured , in the sense that @xmath111 for any pair of vertices @xmath112 separable by a singleton separator set , where @xmath113 .",
    "this is because for any such pair @xmath112 , we may form a junction tree with two nodes , one containing  @xmath13 and one containing @xmath83 , and apply corollary  [ corsep ] . indeed , the matrix @xmath63 defined over singleton vertices is agnostic to which triangulation we choose for the graph .    in settings where there exists a junction tree representation of the graph with only singleton separator sets , corollary  [ cortree ]",
    "has a number of useful implications for the consistency of methods that have traditionally only been applied for edge recovery in gaussian graphical models : for tree - structured discrete graphs , it suffices to estimate the support of @xmath114 from the data .",
    "we will review methods for gaussian graphical model selection and describe their analogs for discrete tree graphs in sections  [ secglobal ] and  [ secnodewise ] .",
    "theorem  [ thmfull ] also has a corollary , that is , relevant for nodewise neighborhood selection approaches to graph selection  @xcite , which are applicable to graphs with arbitrary topologies .",
    "nodewise methods use the basic observation that recovering the edge structure of @xmath55 is equivalent to recovering the neighborhood set @xmath115 for each vertex @xmath5 . for",
    "a given node @xmath15 and positive integer @xmath116 , consider the collection of subsets @xmath117 the following corollary provides an avenue for recovering @xmath118 based on the inverse of a certain generalized covariance matrix :    [ corneigh ] for any graph and node @xmath119 with @xmath120 , the inverse @xmath63 of the matrix @xmath121 is @xmath13-block graph - structured , that is , @xmath122 whenever @xmath123 .",
    "in particular , @xmath111 for all vertices @xmath124 .",
    "note that @xmath125 is the set of subsets of all candidate neighborhoods of  @xmath13 of size @xmath116 .",
    "this result follows from theorem  [ thmfull ] ( and the related corollary  [ corsep ] ) by constructing a particular junction tree for the graph , in which @xmath13 is separated from the rest of the graph by @xmath118 . due to the well - known relationship between the rows of an inverse covariance matrix and linear regression coefficients  @xcite , corollary  [ corneigh ] motivates the following neighborhood - based approach to graph selection : for a fixed vertex @xmath5 , perform a single _ linear regression _ of @xmath126 on the vector @xmath127 . via elementary algebra and an application of corollary  [ corneigh ]",
    ", the resulting regression vector will expose the neighborhood @xmath118 in an arbitrary discrete graphical model ; that is , the indicators @xmath128 corresponding to @xmath129 will have a nonzero weight only if @xmath130 .",
    "we elaborate on this connection in section  [ secnodewise ] .",
    "we now turn to the proof of theorem  [ thmfull ] , which is based on certain fundamental correspondences arising from the theory of exponential families  @xcite .",
    "recall that our exponential family  ( [ eqngeneralmultinomial ] ) has binary - valued indicator functions  ( [ eqnsuff ] ) as its sufficient statistics .",
    "let @xmath93 denote the cardinality of this set and let @xmath131 denote the multivariate function that maps each configuration @xmath132 to the vector @xmath133 obtained by evaluating the @xmath93 indicator functions on @xmath134 . using this notation ,",
    "our exponential family may be written in the compact form @xmath135 , where @xmath136 since this exponential family is known to be minimal , we are guaranteed  @xcite that @xmath137 \\quad\\mbox{and}\\quad\\nabla^2 \\phi(\\theta ) = { \\operatorname{cov}}_\\theta \\bigl[\\mathbb{i}(x ) \\bigr],\\ ] ] where @xmath138 and @xmath139 denote ( resp . )",
    "the expectation and covariance taken under the density @xmath140  @xcite .",
    "the conjugate dual  @xcite of the cumulant function is given by @xmath141 the function @xmath79 is always convex and takes values in @xmath142 . from known results",
    "@xcite , the dual function @xmath79 is finite only for @xmath143 belonging to the marginal polytope @xmath144 the following lemma , proved in the supplementary material @xcite , provides a  connection between the covariance matrix and the hessian of @xmath79 :    [ lementropy ] consider a regular , minimal exponential family , and define @xmath145 $ ] for any fixed @xmath146 . then @xmath147 \\bigr)^{-1 } & = & \\nabla^2 \\phi^*(\\mu).\\end{aligned}\\ ] ]    note that the minimality and regularity of the family implies that @xmath148 $ ] is strictly positive definite , so the matrix is invertible .",
    "for any @xmath149 , let @xmath150 denote the unique natural parameter @xmath77 such that @xmath151 .",
    "it is known  @xcite that the negative dual function @xmath152 is linked to the shannon entropy via the relation @xmath153 in general , expression  ( [ eqndualentropy ] ) does _ not _ provide a straightforward way to compute @xmath154 , since the mapping @xmath155 may be extremely complicated . however",
    ", when the exponential family is defined with respect to a triangulated graph , @xmath79  has an explicit closed - form representation in terms of the mean parameters @xmath156 . consider a junction tree triangulation of the graph , and let @xmath157 be the collection of maximal cliques and separator sets , respectively . by the junction tree theorem",
    ", we have the factorization @xmath158 where @xmath159 and @xmath160 are the marginal distributions over maximal clique @xmath92 and separator set @xmath161 .",
    "consequently , the entropy may be decomposed into the sum @xmath162 where we have introduced the clique- and separator - based entropies @xmath163 and @xmath164    given our choice of sufficient statistics  ( [ eqnsuff ] ) , we show that @xmath159 and @xmath160 may be written explicitly as `` local '' functions of mean parameters associated with @xmath92  and  @xmath161 . for each subset @xmath165 ,",
    "let @xmath166 be the associated collection of mean parameters , and let @xmath167 be the set of mean parameters associated with all nonempty subsets of @xmath20 .",
    "note that @xmath168 contains a total of @xmath169 parameters , corresponding to the number of degrees of freedom involved in specifying a marginal distribution over the random vector @xmath170 .",
    "moreover , @xmath168 uniquely determines the marginal distribution @xmath171 :    [ lemmapping ] for any marginal distribution @xmath171 in the @xmath172-dimensional probability simplex , there is a unique mean parameter vector @xmath168 and matrix @xmath173 such that @xmath174 .    for the proof , see the supplementary material @xcite .",
    "we now combine the dual representation  ( [ eqndualentropy ] ) with the decomposition  ( [ eqnplainentropy ] ) , along with the matrices @xmath175 from lemma  [ lemmapping ] , to conclude that @xmath176 now consider two subsets @xmath75 that are not contained in the same maximal clique .",
    "suppose @xmath20 is contained within maximal clique @xmath92 .",
    "differentiating expression  ( [ eqnentropyexp ] ) with respect to @xmath177 preserves only terms involving @xmath159 and @xmath160 , where @xmath161  is any separator set such that @xmath178 . since @xmath179 , we clearly can not have @xmath180 .",
    "consequently , all cross - terms arising from the clique @xmath92 and its associated separator sets vanish when we take a second derivative with respect to @xmath181 . repeating this argument for any other maximal clique @xmath182 containing @xmath20 but",
    "not @xmath21 , we have @xmath183 .",
    "this proves part ( a ) .    turning to part ( b ) ,",
    "note that if @xmath20 and @xmath21 are in the same maximal clique , the expression obtained by taking second derivatives of the entropy results in an algebraic expression with only finitely many solutions in the parameters @xmath156 ( consequently , also @xmath77 ) . hence , assuming the @xmath77 s are drawn from a continuous distribution , the corresponding values of the block @xmath68 are a.s . nonzero .",
    "moving beyond the population level , we now state and prove several results concerning the statistical consistency of different methods  both known and some novel  for graph selection in discrete graphical models , based on i.i.d",
    ". draws from a discrete graph . for sparse gaussian models ,",
    "existing methods that exploit sparsity of the inverse covariance matrix fall into two main categories : global graph selection methods ( e.g. ,  @xcite ) and local ( nodewise ) neighborhood selection methods  @xcite .",
    "we divide our discussion accordingly .",
    "we begin by describing how a combination of our population - level results and some concentration inequalities may be leveraged to analyze the statistical behavior of log - determinant methods for discrete graphical models with singleton separator sets , and suggest extensions of these methods when observations are systematically corrupted by noise or missing data . given a @xmath184-dimensional random vector @xmath185 with covariance @xmath186 , consider the estimator @xmath187 where @xmath188 is an estimator for @xmath186 . for multivariate gaussian data ,",
    "this program is an @xmath0-regularized maximum likelihood estimate known as the _ graphical lasso _ and is a well - studied method for recovering the edge structure in a gaussian graphical model  @xcite .",
    "although the program  ( [ eqnlogdet ] ) has no relation to the mle in the case of a discrete graphical model , it may still be useful for estimating @xmath189 .",
    "indeed , as shown in ravikumar et al .",
    "@xcite , existing analyses of the estimator  ( [ eqnlogdet ] ) require only tail conditions such as sub - gaussianity in order to guarantee that the sample minimizer is close to the population minimizer .",
    "the analysis of this paper completes the missing link by guaranteeing that the population - level inverse covariance is in fact graph - structured .",
    "consequently , we obtain the interesting result that the program  ( [ eqnlogdet])even though it is ostensibly derived from gaussian considerations  is a consistent method for recovering the structure of any binary graphical model with singleton separator sets .    in order to state our conclusion precisely ,",
    "we introduce additional notation .",
    "consider a general estimate @xmath188 of the covariance matrix @xmath190 such that @xmath191 & \\leq & c \\exp \\bigl(- \\psi(n , p ) \\bigr)\\end{aligned}\\ ] ] for functions @xmath192 and @xmath193 , where @xmath194 denotes the elementwise @xmath195-norm . in the case of fully - observed i.i.d .",
    "data with sub - gaussian parameter @xmath196 , where @xmath197 is the usual sample covariance , this bound holds with @xmath198 and @xmath199 .    as in past analysis of the graphical lasso  @xcite , we require a certain _ mutual incoherence _ condition on the true covariance matrix @xmath186 to control the correlation of nonedge variables with edge variables in the graph .",
    "let @xmath200 , where @xmath201 denotes the kronecker product .",
    "then @xmath202 is a @xmath203 matrix indexed by vertex pairs .",
    "the incoherence condition is given by @xmath204,\\ ] ] where @xmath205 is the set of vertex pairs corresponding to nonzero entries of the precision matrix @xmath206 , equivalently , the edge set of the graph , by our theory on tree - structured discrete graphs .",
    "for more intuition on the mutual incoherence condition , see ravikumar et al .",
    "@xcite .    with this notation , our global edge recovery algorithm proceeds as follows :    [ alggraphlasso ]    1 .",
    "form a suitable estimate @xmath188 of the true covariance matrix @xmath190 .",
    "2 .   optimize the graphical lasso program  ( [ eqnlogdet ] ) with parameter @xmath207 , and denote the solution by @xmath208 .",
    "3 .   threshold the entries of @xmath209 at level @xmath210 to obtain an estimate of @xmath206 .",
    "it remains to choose the parameters @xmath211 . in the following corollary",
    ", we will establish statistical consistency of @xmath209 under the following settings : @xmath212 where @xmath40 is the incoherence parameter in inequality  ( [ eqnmatincoherence ] ) and @xmath213 are universal positive constants .",
    "the following result applies to algorithm  [ alggraphlasso ] when @xmath188 is the sample covariance matrix and @xmath211 are chosen as in equations  ( [ eqnlambdatau ] ) :    [ corglobising ] consider an ising model  ( [ eqnising ] ) defined by an undirected graph with singleton separator sets and with degree at most @xmath116 , and suppose that the mutual incoherence condition  ( [ eqnmatincoherence ] ) holds . with @xmath214 samples , there are universal constants @xmath215 such that with probability at least @xmath216 , algorithm  [ alggraphlasso ] recovers all edges @xmath112 with @xmath217 .",
    "the proof is contained in the supplementary material @xcite ; it is a relatively straightforward consequence of corollary  [ corsep ] and known concentration properties of @xmath188 as an estimate of the population covariance matrix .",
    "hence , if @xmath217 for all edges @xmath218 , corollary  [ corglobising ] guarantees that the log - determinant method plus thresholding recovers the full graph exactly .    in the case of the standard sample covariance matrix ,",
    "a variant of the graphical lasso has been implemented by banerjee , el ghaoui and daspremont @xcite .",
    "our analysis establishes consistency of the graphical lasso for ising models on single separator graphs using @xmath219 samples .",
    "this lower bound on the sample size is unavoidable , as shown by information - theoretic analysis  @xcite , and also appears in other past work on ising models  @xcite .",
    "our analysis also has a _ cautionary message _ :",
    "the proof of corollary  [ corglobising ] relies heavily on the population - level result in corollary  [ cortree ] , which ensures that @xmath206 is graph - structured when @xmath14 has only singleton separators . for a general graph",
    ", we have no guarantees that @xmath206 will be graph - structured [ e.g. , see panel  ( b ) in figure  [ figgraphs ] ] , so the graphical lasso  ( [ eqnlogdet ] ) is _ inconsistent in general_.    on the positive side , if we restrict ourselves to tree - structured graphs , the estimator  ( [ eqnlogdet ] ) is attractive , since it relies only on an estimate @xmath188 of the population covariance @xmath186 that satisfies the deviation condition  ( [ eqnlogdettreecond ] ) .",
    "in particular , even when the samples @xmath220 are contaminated by noise or missing data , we may form a good estimate @xmath188 of @xmath186 . furthermore",
    ", the program  ( [ eqnlogdet ] ) is always convex regardless of whether @xmath188 is positive semidefinite .    as a concrete example of",
    "how we may correct the program  ( [ eqnlogdet ] ) to handle corrupted data , consider the case when each entry of @xmath221 is missing independently with probability @xmath222 , and the corresponding observations @xmath223 are zero - filled for missing entries .",
    "a natural estimator is @xmath224 where @xmath225 denotes elementwise division by the matrix @xmath226 with diagonal entries @xmath227 and off - diagonal entries @xmath228 , correcting for the bias in both the mean and second moment terms . the deviation condition  ( [ eqnlogdettreecond ] ) may be shown to hold w.h.p . , where @xmath229 scales with @xmath227 ( cf .",
    "loh and wainwright  @xcite ) .",
    "similarly , we may derive an appropriate estimator @xmath188 for other forms of additive or multiplicative corruption .",
    "generalizing to the case of @xmath108-ary discrete graphical models with @xmath230 , we may easily modify the program  ( [ eqnlogdet ] ) by replacing the elementwise @xmath0-penalty by the corresponding group @xmath0-penalty , where the groups are the indicator variables for a given vertex .",
    "precise theoretical guarantees follow from results on the group graphical lasso  @xcite .      turning to local neighborhood selection methods ,",
    "recall the neighborhood - based method due to meinshausen and bhlmann  @xcite . in a gaussian graphical model ,",
    "the column corresponding to node @xmath13 in the inverse covariance matrix @xmath231 is a scalar multiple of @xmath232 , the limit of the linear regression vector for @xmath16 upon  @xmath233 .",
    "based on @xmath234 i.i.d .",
    "samples from a @xmath184-dimensional multivariate gaussian distribution , the support of the graph may then be estimated consistently under the usual lasso scaling @xmath235 , where @xmath236 .    motivated by our population - level results on the graph structure of the inverse covariance matrix ( corollary  [ cortree ] ) ,",
    "we now propose a method for neighborhood selection in a tree - structured graph .",
    "although the method works for arbitrary @xmath108-ary trees , we state explicit results only in the case of the binary ising model to avoid cluttering our presentation .",
    "the method is based on the following steps . for each node @xmath11",
    ", we first perform @xmath0-regularized linear regression of @xmath16 against @xmath233 by solving the modified lasso program @xmath237 where @xmath238 is a constant , @xmath239 are suitable estimators for @xmath240 , and @xmath207 is an appropriate parameter .",
    "we then combine the neighborhood estimates over all nodes via an and operation [ edge @xmath112 is present if both @xmath13 and @xmath83 are inferred to be neighbors of each other ] or an or operation ( at least one of @xmath13 or @xmath83 is inferred to be a neighbor of the other ) .    note that the program  ( [ eqnnoisylasso ] ) differs from the standard lasso in the form of the constraint .",
    "indeed , the normal setting of the lasso assumes a linear model where the predictor and response variables are linked by independent sub - gaussian noise , but this is not the case for @xmath16 and @xmath233 in a discrete graphical model .",
    "furthermore , the generality of the program  ( [ eqnnoisylasso ] ) allows it to be easily modified to handle corrupted variables via an appropriate choice of @xmath241 , as in loh and wainwright  @xcite .",
    "the following algorithm summarizes our nodewise regression procedure for recovering the neighborhood set @xmath118 of a given node @xmath13 :    [ algnodewise ]    1 .",
    "form a suitable pair of estimators @xmath242 for covariance submatrices ( @xmath243 , @xmath244 ) .",
    "2 .   optimize the modified lasso program  ( [ eqnnoisylasso ] ) with parameter @xmath207 , and denote the solution by @xmath245 .",
    "3 .   threshold the entries of @xmath245 at level @xmath210 , and define the estimated neighborhood set @xmath246 as the support of the thresholded vector .    in the case of fully observed i.i.d .",
    "observations , we choose @xmath247 to be the recentered estimators @xmath248 and assign the parameters @xmath211 according to the scaling @xmath249 where @xmath250 and @xmath192 is some parameter such that @xmath251 is sub - gaussian with parameter @xmath252 for any @xmath116-sparse vector @xmath253 , and @xmath192 is independent of @xmath253 . the following result applies to algorithm  [ algnodewise ] using the pairs @xmath247 and @xmath254 defined as in equations  ( [ eqnlassopair ] ) and  ( [ eqnlambdatau2 ] ) , respectively .",
    "[ propising ] suppose we have i.i.d .",
    "observations @xmath220 from an ising model and that @xmath255 . then there are universal constants @xmath256 such that with probability greater than @xmath216 , for any node @xmath257 , algorithm  [ algnodewise ] recovers all neighbors @xmath130 for which @xmath258 .",
    "we prove this proposition in the supplementary material @xcite , as a corollary of a more general theorem on the @xmath195-consistency of the program  ( [ eqnnoisylasso ] ) for estimating  @xmath259 , allowing for corrupted observations .",
    "the theorem builds upon the analysis of loh and wainwright  @xcite , introducing techniques for @xmath195-bounds and departing from the framework of a linear model with independent sub - gaussian noise .    regarding the sub - gaussian parameter @xmath192 appearing in proposition  [ propising ] , note that we may always take @xmath260 , since @xmath261 when @xmath253 is @xmath116-sparse and @xmath221 is a binary vector .",
    "this leads to a sample complexity requirement of @xmath262 .",
    "we suspect that a tighter analysis , possibly combined with assumptions about the correlation decay of the graph , would reduce the sample complexity to the scaling @xmath263 , as required by other methods with fully observed data  @xcite .",
    "see the simulations in section  [ secsims ] for further discussion .",
    "for corrupted observations , the strength and type of corruption enters into the factors @xmath264 appearing in the deviation bounds  ( c.2a ) and  ( c.2b ) below , and proposition  [ propising ] has natural extensions to the corrupted case .",
    "we emphasize that although analogs of proposition  [ propising ] exist for other methods of graph selection based on logistic regression and/or mutual information , the theoretical analysis of those methods does not handle corrupted data , whereas our results extend easily with the appropriate scaling .    in the case of @xmath108-ary tree - structured graphical models with @xmath265",
    ", we may perform multivariate regression with the multivariate group lasso  @xcite for neighborhood selection , where groups are defined ( as in the log - determinant method ) as sets of indicators for each node .",
    "the general relationship between the best linear predictor and the block structure of the inverse covariance matrix follows from block matrix inversion , and from a population - level perspective , it suffices to perform multivariate linear regression of all indicators corresponding to a given node against all indicators corresponding to other nodes in the graph .",
    "the resulting vector of regression coefficients has nonzero blocks corresponding to edges in the graph .",
    "we may also combine these ideas with the group lasso for multivariate regression  @xcite to reduce the complexity of the algorithm .      moving on from tree - structured graphical models ,",
    "our method suggests a graph recovery method based on nodewise linear regression for general discrete graphs .",
    "note that by corollary  [ corneigh ] , the inverse of @xmath266 is @xmath13-block graph - structured , where @xmath116 is such that @xmath267 .",
    "it suffices to perform a single multivariate regression of the indicators @xmath268 corresponding to node @xmath13 upon the other indicators in @xmath269 .",
    "we again make precise statements for the binary ising model ( @xmath270 ) . in this case , the indicators @xmath271 corresponding to a subset of vertices @xmath7 of size @xmath272 are all @xmath273 distinct products of variables @xmath274 , for @xmath275 . hence , to recover the @xmath116 neighbors of node @xmath13 , we use the following algorithm . note that knowledge of an upper bound @xmath116 is necessary for applying the algorithm .",
    "[ algnontreenodecay ]    1 .",
    "use the modified lasso program  ( [ eqnnoisylasso ] ) with a suitable choice of @xmath247 and regularization parameter @xmath207 to perform a linear regression of @xmath16 upon all products of subsets of variables of @xmath233 of size at most @xmath116 .",
    "denote the solution by @xmath245 .",
    "threshold the entries of @xmath245 at level @xmath210 , and define the estimated neighborhood set @xmath246 as the support of the thresholded vector .",
    "our theory states that at the population level , nonzeros in the regression vector correspond exactly to subsets of @xmath118 .",
    "hence , the statistical consistency result of proposition  [ propising ] carries over with minor modifications . since algorithm  [ algnontreenodecay ] is essentially a version of algorithm  [ algnontree ] with the first two steps omitted , we refer the reader to the statement and proof of corollary  [ cornontree ] below for precise mathematical statements .",
    "note here that since the regression vector has @xmath276 components , of which are nonzero , the sample complexity of lasso regression in step  ( 1 ) of algorithm  [ algnontreenodecay ] is @xmath277 .    for graphs exhibiting correlation decay  @xcite",
    ", we may reduce the computational complexity of the nodewise selection algorithm by prescreening the nodes of @xmath278 before performing a lasso - based linear regression .",
    "we define the nodewise correlation according to @xmath279 and say that the graph exhibits _ correlation decay _ if there exist constants @xmath280 such that @xmath281 for all @xmath282 , where @xmath283 is the length of the shortest path between @xmath13 and @xmath83 . with this notation , we then have the following algorithm for neighborhood recovery of a fixed node @xmath13 in a graph with correlation decay :    [ algnontree ]    1 .",
    "compute the empirical correlations @xmath284 between @xmath13 and all other nodes @xmath285 , where @xmath286 denotes the empirical distribution .",
    "2 .   let @xmath287 be the candidate set of nodes with sufficiently high correlation .",
    "( note that @xmath51 is a function of both @xmath13 and @xmath288 and , by convention , @xmath289 . )",
    "use the modified lasso program  ( [ eqnnoisylasso ] ) with parameter @xmath207 to perform a linear regression of @xmath16 against @xmath290 , the set of all products of subsets of variables @xmath291 of size at most @xmath116 , together with singleton variables .",
    "denote the solution by @xmath245 .",
    "4 .   threshold the entries of @xmath245 at level @xmath210 , and define the estimated neighborhood set @xmath246 as the support of the thresholded vector .",
    "note that algorithm  [ algnontreenodecay ] is a version of algorithm  [ algnontree ] with @xmath292 , indicating the absence of a prescreening step .",
    "hence , the statistical consistency result below applies easily to algorithm  [ algnontreenodecay ] for graphs with no correlation decay .    for fully observed i.i.d .",
    "observations , we choose @xmath241 according to @xmath293 and parameters @xmath211 as follows : for a candidate set @xmath51 , let @xmath294 denote the augmented vector corresponding to the observation @xmath221 , and define @xmath295 .",
    "let @xmath296 .",
    "then set @xmath297 where @xmath192 is some function such that @xmath298 is sub - gaussian with parameter @xmath299 for any @xmath300-sparse vector @xmath253 , and @xmath192 does not depend on @xmath253 .",
    "we have the following consistency result , the analog of proposition  [ propising ] for the augmented set of vectors .",
    "it applies to algorithm  [ algnontree ] with the pairs @xmath239 and @xmath211 chosen as in equations  ( [ eqngamparam ] ) and  ( [ eqnlambdatau3 ] ) .",
    "[ cornontree ] consider i.i.d .",
    "observations @xmath220 generated from an ising model satisfying the correlation decay condition  ( [ eqncorrdecay ] ) , and suppose @xmath301 then there are universal constants @xmath256 such that with probability at least @xmath302 , and for any @xmath257 :    the set @xmath51 from step ( 2 ) of algorithm  [ algnontree ] satisfies @xmath303 .",
    "algorithm  [ algnontree ] recovers all neighbors @xmath304 such that @xmath305    the proof of corollary  [ cornontree ] is contained in the supplementary material @xcite . due to the exponential factor",
    "@xmath306 appearing in the lower bound  ( [ eqnham ] ) on the sample size , this method is suitable only for bounded - degree graphs .",
    "however , for reasonable sizes of @xmath116 , the dimension of the linear regression problem decreases from @xmath276 to @xmath307 , which has a significant impact on the runtime of the algorithm .",
    "we explore two classes of bounded - degree graphs with correlation decay in the simulations of section  [ secsims ] , where we generate erds  renyi graphs with edge probability @xmath308 and square grid graphs in order to test the behavior of our recovery algorithm on nontrees .",
    "when @xmath265 , corresponding to nonbinary states , we may combine these ideas with the overlapping group lasso  @xcite to obtain similar algorithms for nodewise recovery of nontree graphs .",
    "however , the details are more complicated , and we do not include them here .",
    "note that our method for nodewise recovery in nontree graphical models is again easily adapted to handle noisy and missing data , which is a clear advantage over other existing methods .",
    "in this section we report the results of various simulations we performed to illustrate the sharpness of our theoretical claims . in all cases , we generated data from binary ising models .",
    "we first applied the nodewise linear regression method ( algorithm [ algnodewise ] for trees ; algorithm [ algnontreenodecay ] in the general case ) to the method of @xmath0-regularized logistic regression , analyzed in past work for ising model selection by ravikumar , wainwright and lafferty @xcite .",
    "their main result was to establish that , under certain incoherence conditions of the fisher information matrix , performing regularized logistic regression with a sample size @xmath309 is guaranteed to select the correct graph w.h.p .",
    "thus , for any bounded - degree graph , the sample size @xmath234 need grow only logarithmically in the number of nodes @xmath184 . under this",
    "scaling , our theory also guarantees that nodewise _ linear regression _ with @xmath0-regularization will succeed in recovering the true graph w.h.p .",
    "@c@c@    -regularized logistic vs. linear regression methods for graph recovery .",
    "each panel plots of the probability of correct graph recovery vs. the rescaled sample size @xmath310 ; solid curves correspond to linear regression ( method in this paper ) , whereas dotted curves correspond to logistic regression  @xcite .",
    "curves are based on average performance over 500 trials .",
    "simulation results for two - dimensional grids with @xmath311 neighbors , and number of nodes @xmath184 varying over @xmath312 .",
    "consistent with theory , when plotted vs. the rescaled sample size @xmath313 , all three curves ( red , blue , green ) are well aligned with one another . both linear and logistic regression transition from failure to success at a similar point .",
    "analogous results for an erds ",
    "renyi graph with edge probability @xmath314 .",
    "analogous results for a chain - structured graph with maximum degree @xmath315.,title=\"fig : \" ] & -regularized logistic vs. linear regression methods for graph recovery .",
    "each panel plots of the probability of correct graph recovery vs. the rescaled sample size @xmath310 ; solid curves correspond to linear regression ( method in this paper ) , whereas dotted curves correspond to logistic regression  @xcite .",
    "curves are based on average performance over 500 trials .",
    "simulation results for two - dimensional grids with @xmath311 neighbors , and number of nodes @xmath184 varying over @xmath312 .",
    "consistent with theory , when plotted vs. the rescaled sample size @xmath313 , all three curves ( red , blue , green ) are well aligned with one another . both linear and logistic regression transition from failure to success at a similar point .",
    "analogous results for an erds ",
    "renyi graph with edge probability @xmath314 .",
    "analogous results for a chain - structured graph with maximum degree @xmath315.,title=\"fig : \" ] + & +   +    in figure  [ figsimsone ] we present the results of simulations with two goals : ( i ) to test the @xmath316 scaling of the required sample size ; and ( ii ) to compare @xmath0-regularized nodewise linear regression ( algorithms [ algnontreenodecay ] and [ algnontree ] ) to @xmath0-regularized nodewise logistic regression  @xcite .",
    "we ran simulations for the two methods on both tree - structured and nontree graphs with data generated from a binary ising model , with node weights @xmath317 and edge weights @xmath318 . to save on computation , we employed the neighborhood screening method described in section  [ secnontree ] to prune the candidate neighborhood set before performing linear regression .",
    "we selected a  candidate neighborhood set of size @xmath319 with highest empirical correlations , then performed a single regression against all singleton nodes and products of subsets of the candidate neighborhood set of size at most @xmath116 , via the modified lasso program  ( [ eqnnoisylasso ] ) .",
    "the size of the candidate neighborhood set was tuned through repeated runs of the algorithm . for both methods , the optimal choice of regularization parameter @xmath207 scales as @xmath320 , and we used the same value of @xmath207 in comparing logistic to linear regression . in each panel",
    "we plot the probability of successful graph recovery versus the rescaled sample size @xmath321 , with curves of different colors corresponding to graphs ( from the same family ) of different sizes .",
    "solid lines correspond to linear regression , whereas dotted lines correspond to logistic regression ; panels ( a ) , ( b ) and ( c ) correspond to grid graphs , erds ",
    "renyi random graphs and chain graphs , respectively .",
    "for all these graphs , the three solid / dotted curves for different problem sizes are well aligned , showing that the method undergoes a transition from failure to success as a function of the ratio @xmath321 .",
    "in addition , both linear and logistic regression are comparable in terms of statistical efficiency ( the number of samples @xmath234 required for correct graph selection to be achieved ) .",
    "@cc@     of missing data varying in @xmath322 .",
    "panel shows simulation results for nodewise regression applied to chain graphs for varying @xmath184 and @xmath222 .",
    "panel shows simulation results for nodewise regression applied to star graphs with maximal node degree @xmath323 and varying @xmath222.,title=\"fig : \" ] &   of missing data varying in @xmath322 .",
    "panel shows simulation results for nodewise regression applied to chain graphs for varying @xmath184 and @xmath222 .",
    "panel shows simulation results for nodewise regression applied to star graphs with maximal node degree @xmath323 and varying @xmath222.,title=\"fig : \" ] + & +   +    the main advantage of nodewise linear regression and the graphical lasso over nodewise logistic regression is that they are straightforward to correct for corrupted or missing data .",
    "figure  [ figsimstwo ] shows the results of simulations designed to test the behavior of these corrected estimators in the presence of missing data .",
    "panel  ( a ) shows the results of applying the graphical lasso method , as described in section  [ secglobal ] , to the dino graph of figure  [ figgraphs](e ) .",
    "we again generated data from an ising model with node weights @xmath324 and edge weights @xmath325 .",
    "the curves show the probability of success in recovering the 15 edges of the graph , as a function of the rescaled sample size @xmath321 for @xmath326 .",
    "in addition , we performed simulations for different levels of missing data , specified by the parameter @xmath327 , using the corrected estimator  ( [ eqnmissdata ] ) .",
    "note that all five runs display a transition from success probability 0 to success probability 1 in roughly the same range , as predicted by our theory .",
    "indeed , since the dinosaur graph has only singleton , corollary  [ cortree ] ensures that the inverse covariance matrix is exactly graph - structured , so our global recovery method is consistent at the population level .",
    "further note that the curves shift right as the fraction @xmath222 of missing data increases , since the recovery problem becomes incrementally harder .",
    "panels ( b ) and ( c ) of figure  [ figsimstwo ] show the results of the nodewise regression method of section  [ secnodewise ] applied to chain and star graphs , with increasing numbers of nodes @xmath328 and @xmath329 , respectively . for the chain graphs in panel",
    "( b ) , we set node weights of the ising model equal to @xmath324 and edge weights equal to @xmath325 .",
    "for the varying - degree star graph in panel ( c ) , we set node weights equal to @xmath324 and edge weights equal to @xmath330 , where the degree @xmath116 of the central hub grows with the size of the graph as @xmath331 .",
    "again , we show curves for different levels of missing data , @xmath332 .",
    "the modified lasso program  ( [ eqnnoisylasso ] ) was optimized using a form of composite gradient descent due to agarwal , negahban and wainwright @xcite , guaranteed to converge to a small neighborhood of the optimum even when the problem is nonconvex  @xcite . in both the chain and star graphs , the three curves corresponding to different problem sizes @xmath184 at each value of the missing data parameter @xmath222 stack up when plotted against the rescaled sample size .",
    "note that the curves for the star graph stack up nicely with the scaling @xmath333 , rather than the worst - case scaling @xmath334 , corroborating the remark following proposition  [ propising ] .",
    "since @xmath335 is fixed for the chain graph , we use the rescaled sample size @xmath321 in our plots , as in the plots in figure  [ figsimsone ] . once again",
    ", these simulations corroborate our theoretical predictions : the corrected linear regression estimator remains consistent even in the presence of missing data , although the sample size required for consistency grows as the fraction of missing data @xmath222 increases .",
    "the correspondence between the inverse covariance matrix and graph structure of a gauss  markov random field is a classical fact with numerous consequences for estimation of gaussian graphical models .",
    "it has been an open question as to whether similar properties extend to a broader class of graphical models . in this paper , we have provided a partial affirmative answer to this question and developed theoretical results extending such relationships to discrete undirected graphical models .    as shown by our results ,",
    "the inverse of the ordinary covariance matrix is graph - structured for special subclasses of graphs with singleton separator sets .",
    "more generally , we have considered inverses of _ generalized covariance matrices _ , formed by introducing indicator functions for larger subsets of variables . when these subsets are chosen to reflect the structure of an underlying junction tree , the edge structure is reflected in the inverse covariance matrix .",
    "our population - level results have a number of statistical consequences for graphical model selection .",
    "we have shown that our results may be used to establish consistency ( or inconsistency ) of standard methods for discrete graph selection , and have proposed new methods for neighborhood recovery which , unlike existing methods , may be applied even when observations are systematically corrupted by mechanisms such as additive noise and missing data",
    ". furthermore , our methods are attractive in their simplicity , in that they only involve simple optimization problems .",
    "thanks to the associate editor and anonymous reviewers for helpful feedback ."
  ],
  "abstract_text": [
    "<S> we investigate the relationship between the structure of a discrete graphical model and the support of the inverse of a generalized covariance matrix . </S>",
    "<S> we show that for certain graph structures , the support of the inverse covariance matrix of indicator variables on the vertices of a graph reflects the conditional independence structure of the graph . </S>",
    "<S> our work extends results that have previously been established only in the context of multivariate gaussian graphical models , thereby addressing an open question about the significance of the inverse covariance matrix of a non - gaussian distribution . </S>",
    "<S> the proof exploits a combination of ideas from the geometry of exponential families , junction tree theory and convex analysis . </S>",
    "<S> these population - level results have various consequences for graph selection methods , both known and novel , including a novel method for structure estimation for missing or corrupted observations . </S>",
    "<S> we provide nonasymptotic guarantees for such methods and illustrate the sharpness of these predictions via simulations . </S>"
  ]
}