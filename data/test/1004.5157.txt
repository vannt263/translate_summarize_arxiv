{
  "article_text": [
    "fifteen years , the area of channel coding has been revolutionized by the practical realization of capacity - approaching coding schemes , initiated by the invention of turbo codes and their associated decoding algorithms in 1993 @xcite .",
    "a few years after the invention of the turbo coding schemes , researchers became aware that gallager s low - density parity - check ( ldpc ) block codes and message - passing iterative decoding , first introduced in  @xcite , were also capable of capacity - approaching performance .",
    "the analysis and design of these coding schemes quickly attracted considerable attention in the literature , beginning with the work of wiberg  @xcite , mackay and neal @xcite , and many others .",
    "an irregular version of ldpc codes was first introduced by luby _",
    "_ in @xcite , and analytical tools were presented in  @xcite to obtain performance limits for graph - based message - passing iterative decoding algorithms , such as those suggested by tanner  @xcite . for many classes of channels ,",
    "these tools have been successfully employed to design families of irregular ldpc codes that perform very well near capacity  @xcite .",
    "moreover , for the binary erasure channel these tools have enabled the design of families of irregular ldpc codes that are not only capacity - approaching but in fact capacity - achieving ( see  @xcite and references therein ) .",
    "the convolutional counterparts of ldpc block codes are ldpc convolutional codes .",
    "analogous to ldpc block codes , ldpc convolutional codes are defined by sparse parity - check matrices , which allow them to be decoded using iterative message - passing algorithms .",
    "recent studies have shown that ldpc convolutional codes are suitable for practical implementation in a number of different communication scenarios , including continuous transmission and block transmission in frames of arbitrary size  @xcite .",
    "two major methods have been proposed in the literature for the construction of ldpc convolutional codes , two methods that in fact started the field of ldpc convolutional codes .",
    "the first method was proposed by tanner  @xcite ( see also @xcite ) and exploits similarities between quasi - cyclic block codes and time - invariant convolutional codes .",
    "the second method was presented by jimnez - feltstr \" om and zigangirov in  @xcite and relies on a matrix - based unwrapping procedure to obtain the parity - check matrix of a periodically time - varying convolutional code from the parity - check matrix of a block code .",
    "@xmath0    the aims of this paper are threefold .",
    "first , we show that these two ldpc convolutional code construction methods , once suitably generalized , are in fact tightly connected .",
    "we establish this connection with the help of so - called graph covers .",
    "a second aim is to discuss a variety of ldpc convolutional code constructions .",
    "although the underlying principles are mathematically quite straightforward , it is important to understand how they can be applied to obtain convolutional codes with good performance and attractive encoder and decoder architectures .",
    "a third aim is to make progress towards a better understanding of where the observed `` convolutional gain '' comes from , and what its costs are in terms of decoder complexity .",
    "the paper is structured as follows .",
    "after some notational remarks in section  [ sec : notation:1:basic ] , we discuss the basics of ldpc convolutional codes in section  [ sec : ldpc : convolutional : codes:1 ] . in particular ,",
    "in that section we give a first exposition of the ldpc convolutional code construction methods due to tanner and due to jimnez - feltstr \" om and zigangirov . in section  [ sec : graph : covers:1 ] we discuss two types of graph - cover code constructions and show how they can be used to connect the code construction methods due to tanner and due to jimnez - feltstr \" om and zigangirov . based on these insights , section  [ sec : variations : unwrapping:1 ] presents a variety of ldpc convolutional code constructions ( along with simulation results ) , and in section  [ sec : other : ldpc : code : constructions:1 ] we mention some similarities and differences of these constructions compared to other recent code constructions in the literature .",
    "afterwards , in section  [ sec : analysis ] we analyze some aspects of the constructed ldpc convolutional codes and discuss some possible reasons for the `` convolutional gain , '' before we conclude the paper in section  [ sec : discussion ] .",
    "we use the following sets , rings , and fields : @xmath1 is the ring of integers ; @xmath2 is the set of non - negative integers ; @xmath3 is the field of size two ; @xmath4 $ ] is the ring of polynomials with coefficients in @xmath3 and indeterminate @xmath5 ; and @xmath6 / \\langle x^r{-}1 \\rangle}$ ] is the ring of polynomials in @xmath4 $ ] modulo @xmath7 , where @xmath8 is a positive integer .",
    "we also use the notational short - hand @xmath9}$ ] for @xmath6 / \\langle x^r{-}1 \\rangle}$ ] .    by @xmath10 and @xmath9}^n$ ] , we mean , respectively , a row vector over @xmath3 of length @xmath11 and a row vector over @xmath9}$ ] of length @xmath11 . in the following ,",
    "if @xmath12 is some matrix , then @xmath13_{j , i}$ ] denotes the entry in the @xmath14-th row and @xmath15-th column of @xmath12 .",
    "note that we use the convention that indices of vector entries start at @xmath16 ( and not at @xmath17 ) , with a similar convention for row and column indices of matrix entries .",
    "( this comment applies also to semi - infinite matrices , which are defined such that the row and column index sets equal @xmath2 . )",
    "the only exception to this convention are bi - infinite matrices , where the row and column index sets equal @xmath1 .",
    "finally , @xmath18 will denote the kronecker product of the matrices @xmath19 and @xmath20 .",
    "this section defines ldpc convolutional codes and discusses why they are interesting from an implementation perspective .",
    "afterwards , we review two popular methods of obtaining ldpc convolutional codes by unwrapping block codes . later in this paper , namely in section  [ sec : graph : covers:1 ] , we will use graph covers to show how these two methods are connected , and in section  [ sec : variations : unwrapping:1 ] we will see how these two methods can be implemented and combined to obtain ldpc convolutional codes with very good performance .      a semi - infinite binary parity - check matrix as in   at the top of this page defines a convolutional code @xmath21 as follows .",
    "namely , it is the set of semi - infinite sequences given by @xmath22 where @xmath23 denotes the transpose of a vector or of a matrix .",
    "we comment on several important aspects and properties of the code @xmath21 and its parity - check matrix @xmath24 .    * if the submatrices @xmath25 , @xmath26 , @xmath27 , have size @xmath28 with @xmath29 , then @xmath21 is said to have ( design ) rate @xmath30 . * the parameter @xmath31 that appears in   is called the syndrome former memory .",
    "it is an important parameter of @xmath21 because the maximal number of non - zero submatrices per block row of @xmath24 is upper bounded by @xmath32 . * the quantity @xmath33",
    "is called the constraint length of @xmath21 .",
    "it measures the maximal width ( in symbols ) of the non - zero area of @xmath24 . gives only an upper bound on the maximal width ( in symbols ) of the non - zero area of @xmath24 , but this upper bound will be good enough for our purposes . ]",
    "* we do not require that for a given @xmath34 the submatrices @xmath35 are independent of @xmath36 , and so @xmath21 is in general a _ time - varying _ convolutional code .",
    "* if there is a positive integer @xmath37 such that @xmath38 for all @xmath34 and all @xmath39 , then @xmath37 is called the period of @xmath24 , and @xmath21 is _ periodically time - varying_. * if the period @xmath37 equals @xmath17 , then @xmath24 is called _ time - invariant _ , and the parity - check matrix can be simply written as @xmath40 .",
    "\\label{eq : tipcc }    \\end{aligned}\\ ] ] * if the number of ones in each row and column of @xmath24 is small compared to the constraint length @xmath41 , then @xmath21 is an ldpc convolutional code .",
    "* an ldpc convolutional code @xmath21 is called @xmath42-regular if , starting from the zeroth column , @xmath24 has @xmath43 ones in each column , and , starting from the @xmath44-th row , @xmath24 has @xmath45 ones in each row .",
    "if , however , there are no integers @xmath31 , @xmath43 , and @xmath45 such that @xmath21 is @xmath42-regular , then @xmath21 is called irregular .    of course , there is some ambiguity in the above definition .",
    "namely , a periodically time - varying ldpc convolutional code with parameters @xmath37 , @xmath46 , and @xmath47 can also be considered to be a periodically time - varying ldpc convolutional code with parameters @xmath48 , @xmath49 , @xmath50 , and @xmath51 for any integer @xmath52 that divides @xmath37 . in particular ,",
    "for @xmath53 we consider the code to be a time - invariant ldpc convolutional code with parameters @xmath54 and @xmath55 .",
    "an advantage of ldpc convolutional codes compared to their block code counterparts is the so - called `` fast encoding '' property . as a result of the diagonal shape of their parity - check matrices ,",
    "many ldpc convolutional codes enjoy simple shift register based encoders .",
    "even randomly constructed ldpc convolutional codes can be formed in such a way as to achieve this feature without any loss in performance ( see , e.g. , @xcite ) . on the other hand , in order to have a simple encoding procedure for ldpc block codes , either the block code must have some sort of structure @xcite or the parity - check matrix must be changed to a more easily `` encodable '' form  @xcite .",
    "the difficulty in constructing and decoding ldpc convolutional codes is dealing with the unbounded size of the parity - check matrix .",
    "this is overcome at the code construction step by considering only periodically time - varying or time - invariant codes .",
    "the code construction problem is therefore reduced to designing just one period of the parity - check matrix .",
    "for decoding , the most obvious approach is to terminate the encoder and to employ message - passing iterative decoding based on the complete tanner graph representation of the parity - check matrix of the code .",
    "although this would be straightforward to implement using a standard ldpc block code decoder , it would be wasteful of resources , since the resulting ( very large ) block decoder would not be taking advantage of two important aspects of the convolutional structure : namely , that decoding can be done continuously without waiting for an entire terminated block to be received and that the distance between two variable nodes that are connected to the same check node is limited by the size of the syndrome former memory .    in order to take advantage of the convolutional nature of the parity - check matrix , a continuous sliding window message - passing iterative decoder that operates on a window of size @xmath56 variable nodes , where @xmath57 is the number of decoding iterations to be performed ,",
    "can be implemented , similar to a viterbi decoder with finite path memory  @xcite .",
    "this window size is chosen since , in a single iteration , messages from variable ( or check ) nodes can be passed across a span of only one constraint length .",
    "thus , in @xmath57 iterations , messages can propagate only over a window of size @xmath57 constraint length .",
    "( see also the recent paper by papaleo _",
    "_  @xcite , which investigates further reducing the window size for codes operating on a binary erasure channel ( bec ) . ) another simplification is achieved by exploiting the fact that a single decoding iteration of two variable nodes that are at least @xmath32 time units apart can be performed independently , since the corresponding bits can not participate in the same parity - check equation .",
    "this allows the parallelization of the @xmath57 iterations by employing @xmath57 _ independent identical processors _ working on different regions of the parity - check matrix simultaneously , resulting in the parallel pipeline decoding architecture introduced in  @xcite .",
    "the pipeline decoder outputs a continuous stream of decoded data after an initial decoding delay of @xmath58 received symbols .",
    "the operation of this decoder on the tanner graph of a simple time - invariant rate-@xmath59 convolutional code with @xmath60 and @xmath61 is illustrated in figure  [ fig : decoder ] .",
    "is usually much larger than typical values of @xmath41 for `` classical '' convolutional codes .",
    "therefore the value @xmath61 of the convolutional code shown in figure  [ fig : decoder ] is not typical for the codes considered in this paper . ]",
    "although the pipeline decoder is capable of fully parallelizing the iterations by using @xmath57 independent identical processors , employing a large number of hardware processors might not be desirable in some applications . in such cases , fewer processors ( even one processor ) can be scheduled to perform subsets of iterations , resulting in a serial looping architecture @xcite with reduced throughput .",
    "this ability to balance the processor load and decoding speed dynamically is especially desirable where very large ldpc convolutional codes must be decoded with limited available on - chip memory .",
    "further discussion on the implementation aspects of the pipeline decoder can be found in @xcite .      in this subsection",
    "we discuss two approaches for deriving convolutional codes from block codes , in particular for deriving ldpc convolutional codes from ldpc block codes .",
    "the first technique will be the unwrapping due to tanner and the second will be the unwrapping due to jimnez - feltstrm and zigangirov ( jfz ) . in section  [ sec : graph : covers:1 ] we will see , with the help of graph covers , how these two  seemingly different ",
    "unwrapping techniques are connected with each other .",
    "the term _ unwrapping _ , in particular unwrapping a quasi - cyclic block code to obtain a time - invariant convolutional code , was first introduced in a paper by tanner  @xcite ( see also  @xcite ) . that paper describes a link between quasi - cyclic block codes and time - invariant convolutional codes and shows that the free distance of the unwrapped convolutional code can not be smaller than the minimum distance of the underlying quasi - cyclic code .",
    "this idea was later extended in  @xcite .",
    "consider the quasi - cyclic block code @xmath62 defined by the polynomial parity - check matrix @xmath63 of size @xmath64 , i.e. , @xmath65}^n           \\bigm|            { { { { \\bm{h}}}}_{\\mathrm{qc}}^{(r)}}(x ) \\cdot { { { { \\bm{v}}}}}(x)^{\\mathsf{t}}= { { { \\bm{0}}}}^{\\mathsf{t}}\\bigr\\}.\\end{aligned}\\ ] ] here the polynomial operations are performed modulo @xmath7 . the tanner unwrapping technique is simply based on dropping these modulo computations .",
    "more precisely , with a quasi - cyclic block code @xmath62 we associate the convolutional code @xmath66^n           \\bigm|           { { { { \\bm{h}}}}_{\\mathrm{conv}}}(d ) \\cdot { { { { \\bm{v}}}}}(d)^{\\mathsf{t}}= { { { \\bm{0}}}}^{\\mathsf{t}}\\bigr\\}\\end{aligned}\\ ] ] with polynomial parity - check matrix @xmath67 here the change of indeterminate from @xmath5 to @xmath68 indicates the lack of the modulo @xmath69 operations . (",
    "note that in   we assume that the exponents appearing in the polynomials in @xmath63 are between @xmath16 and @xmath70 inclusive . )",
    "it can easily be seen that any codeword @xmath71 in @xmath72 maps to a codeword @xmath73 in @xmath62 through @xmath74 a process which was described in  @xcite as the wrapping around of a codeword in the convolutional code into a codeword in the quasi - cyclic code .",
    "the inverse process was described as unwrapping .",
    "having introduced the unwrapping technique due to tanner , we move on to discuss the unwrapping technique due to jfz  @xcite , which is another way to unwrap a block code to obtain a convolutional code .",
    "the basic idea is best explained with the help of an example .",
    "[ example : unwrap:1 ]    consider the parity - check matrix    with size @xmath75 , of a rate-@xmath76 block code .",
    "as indicated above , we can take a pair of scissors and `` cut '' the parity - check matrix into two pieces , whereby the cutting pattern is such that we repeatedly move @xmath77 units to the right and then @xmath78 unit down .",
    "having applied this `` diagonal cut , '' we repeatedly copy and paste the two parts to obtain the bi - infinite matrix shown in figure  [ fig : jjj:1 ] .",
    "this new matrix can be seen as the parity - check matrix of ( in general ) a periodically time - varying convolutional code ( here the period is @xmath79 ) .",
    "it is worth observing that this new matrix has the same row and column weights as the matrix that we started with .",
    "rows are reduced . ]",
    "this example can be formalized easily .",
    "namely , starting with an @xmath64 parity - check matrix @xmath80 of some block code , let @xmath81 .",
    "then the `` diagonal cut '' is performed by alternately moving @xmath82 units to the right and then @xmath83 units down ( i.e. , @xmath84 .",
    "the resulting convolutional code has rate @xmath85 , syndrome former memory @xmath86 , constraint length @xmath87 , and period @xmath88 .",
    "analogous to the comment at the end of section  [ sec : definition : ldpc : conv : codes:1 ] , it is also possible to cut the matrix @xmath80 in larger step sizes , e.g. , moving @xmath89 units to the right and @xmath90 units down , for any integer @xmath52 that divides @xmath91 , thereby obtaining a periodically time - varying convolutional code with rate @xmath92 , syndrome former memory @xmath93 , constraint length @xmath94 , and period @xmath95 .",
    "( see also the discussion in section  [ sec : time : varying : ldpc : convolutional : codes:1 ] . )    in the rest of this paper , the term `` jfz unwrapping technique '' will also stand for the following generalization of the above procedure .",
    "namely , starting with a length-@xmath11 block code @xmath96 defined by some size-@xmath97 parity - check matrix @xmath80 , i.e. , @xmath98 we write @xmath80 as the sum @xmath99 of a collection of matrices @xmath100 .",
    "the convolutional code @xmath21 is then defined to be @xmath101 where @xmath102 referring to the notation introduced in section  [ sec : definition : ldpc : conv : codes:1 ] , the matrix @xmath24 is the parity - check matrix of a time - invariant convolutional code . however , depending on the decomposition of @xmath80 and the internal structure of the terms in that decomposition , the matrix @xmath24 can also be ( and very often is ) viewed as the parity - check matrix of a time - varying convolutional code with non - trivial period @xmath37 .    in order to illustrate the generalization of the jfz unwrapping technique that we have introduced in the last paragraph ,",
    "observe that decomposing @xmath80 from example  [ example : unwrap:1 ] as @xmath103 with    yields a convolutional code with parity - check matrix @xmath24 whose bi - infinite version equals the matrix shown in figure  [ fig : jjj:1 ] .",
    "having formally introduced ldpc convolutional codes in the previous section , we now turn our attention to the main tool of this paper , namely graph covers .",
    "[ def : graph : cover:1 ]    a _ cover _ of a graph @xmath104 with vertex set @xmath105 and edge set @xmath106 is a graph @xmath107 with vertex set @xmath108 and edge set @xmath109 , along with a surjection @xmath110 which is a graph homomorphism ( i.e. , @xmath111 takes adjacent vertices of @xmath107 to adjacent vertices of @xmath104 ) such that for each vertex @xmath112 and each @xmath113 , the neighborhood @xmath114 of @xmath115 is mapped bijectively to @xmath116 .",
    "a cover is called an _ @xmath117-cover _ , where @xmath117 is a positive integer , if @xmath118 for every vertex @xmath119 in @xmath105 .",
    "is also known as the degree of the cover .",
    "( not to be confused with the degree of a vertex . ) ]    these graph covers will be used for the construction of new tanner graphs from old tanner graphs , in particular for the construction of tanner graphs that represent ldpc convolutional codes .",
    "more specifically , this section starts by discussing two simple methods to specify a graph cover , which will be called graph - cover construction  1 ( * gcc1 * ) and graph - cover construction  2 ( * gcc2 * ) .",
    "although they yield isomorphic tanner graphs , and therefore equivalent codes , it is convenient to have both methods at hand . as we will see , interesting classes of tanner graphs can be obtained by repeatedly applying these graph - cover constructions , by mixing them , and by suitably shortening the resulting codes . moreover",
    ", these two graph - cover constructions will allow us to exhibit a connection between the tanner and the jfz unwrapping techniques .",
    "let @xmath120 be an @xmath121 matrix over @xmath2 .",
    "with such a matrix we can associate a tanner graph @xmath122 , where we draw @xmath123 variable nodes , @xmath124 check nodes , and where there are @xmath125_{j , i}$ ] edges from the @xmath15-th variable node to the @xmath14-th check node .",
    "given the role that the matrix @xmath120 will play subsequently , we follow  @xcite and call the matrix @xmath120 a proto - matrix and the corresponding graph @xmath122 a proto - graph .",
    "the next definition introduces * gcc1 *  and * gcc2 * , two ways to specify graph covers that will be used throughout the rest of the paper .. ]    [ def : graph : cover : construction:1 ]    for some positive integers @xmath124 and @xmath123 , let @xmath126 be a proto - matrix",
    ". we also introduce the following objects :    * for some finite set @xmath127 , let @xmath128 be a collection of matrices such that @xmath129 , @xmath130 , and such that @xmath131 . * for some positive integer @xmath8 , let @xmath132 be a collection of size-@xmath133 permutation matrices .",
    "i.e. , for every @xmath130 , the matrix @xmath134 is such that it contains one `` @xmath17 '' per column , one `` @xmath17 '' per row , and `` @xmath16 ' 's otherwise",
    ".    based on the collection of matrices @xmath135 and the collection of matrices @xmath136 , there are two common ways of defining a graph cover of the tanner graph @xmath122 .",
    "( in the following expressions , @xmath137 is the identity matrix of size @xmath138 . )    * * graph - cover construction 1 ( * gcc1 * ) .",
    "* consider the intermediary matrix @xmath139 whose tanner graph @xmath140 consists of @xmath8 disconnected copies of @xmath122 .",
    "this is an @xmath8-fold cover of @xmath122 , albeit a rather trivial one . in order to obtain an interesting @xmath8-fold graph cover of @xmath120 , for each @xmath130",
    ", we replace @xmath141 by @xmath142 , i.e. , we define @xmath143 * * graph - cover construction 2 ( * gcc2 * ) * consider the intermediary matrix @xmath144 whose tanner graph @xmath145 consists of @xmath8 disconnected copies of @xmath122 .",
    "this is an @xmath8-fold cover of @xmath122 , albeit a rather trivial one . in order to obtain an interesting @xmath8-fold graph cover of @xmath120 , for each @xmath130",
    ", we replace @xmath146 by @xmath147 , i.e. , we define @xmath148    if all the matrices @xmath136 are circulant matrices , then the graph covers @xmath149 and @xmath150 will be called cyclic covers of @xmath122 .    one can verify that the two graph - cover constructions in definition  [ def : graph : cover : construction:1 ] are such that the matrix @xmath151 , after a suitable reordering of the rows and columns , equals the matrix @xmath152 . and",
    "@xmath153 are permutation equivalent , i.e. , there is a pair of permutation matrices @xmath154 such that @xmath155",
    ". of course , for this to work , the pair @xmath156 must be independent of @xmath130 , i.e. , dependent only on the size of the matrices @xmath135 and @xmath157 . such a @xmath154 pair can easily be found .",
    "] this implies that @xmath149 and @xmath150 are isomorphic graphs ; nevertheless , it is helpful to define both types of constructions .",
    "the following examples will help us to better understand how * gcc1 *  and * gcc2 *  can be used to obtain interesting classes of tanner graphs , and , in particular , how the resulting graph - cover constructions can be visualized graphically .",
    "although these examples are very concrete , they are written such that they can be easily generalized .",
    "[ ex : graph : cover : construction:1 ]    consider the proto - matrix @xmath158 with @xmath159 and @xmath160 , and whose tanner graph @xmath122 is shown in figure  [ fig : graph : cover : constructions:1](a ) .",
    "let @xmath161 , and let the collection of matrices @xmath162 be given by @xmath163 , where for each @xmath164 and each @xmath165 the matrix @xmath166 is defined as follows @xmath167_{j',i ' }        & { \\triangleq}\\begin{cases }             [ { { { { \\bm{a}}}}}]_{j',i ' } & \\text{if $ ( j',i ' ) = ( j , i)$ } \\\\             0                 & \\text{otherwise }           \\end{cases}.    \\end{aligned}\\ ] ] moreover , let @xmath168 , and let the collection of matrices @xmath157 be given by @xmath169 , where @xmath170 , @xmath171 , @xmath172 , @xmath173 , @xmath174 , @xmath175 , and where @xmath176 is an @xmath177 times left - shifted identity matrix of size @xmath178 .",
    "* using * gcc1 * , we obtain the matrices @xmath179        { { { { \\bm{b}}}}}&= \\begin{bmatrix }               { { { { \\bm{i}}}}}_1 & { { { { \\bm{i}}}}}_2 & { { { { \\bm{i}}}}}_4 \\\\               { { { { \\bm{i}}}}}_6 & { { { { \\bm{i}}}}}_5 & { { { { \\bm{i}}}}}_3             \\end{bmatrix } ,               \\label{eq : ex : graph : cover : constructions:1:b:1 }      \\end{aligned}\\ ] ] whose tanner graphs @xmath140 and @xmath149 , respectively , are shown in figure  [ fig : graph : cover : constructions:1](b ) . * using * gcc2 * , we obtain the matrices @xmath180        \\hskip-0.20 cm        { \\overline{{{{\\bm{b}}}}}}&= \\left [             \\begin{array}{@{\\;}c@{\\;\\;}c@{\\;\\;}c@{\\;\\;}c@{\\;\\;}c@{\\;\\;}c@{\\;\\;}c@{\\ ; } }               { { { \\bm{0 } } } } & { { { { \\bm{a}}}}}_{1,0 } & { { { { \\bm{a}}}}}_{1,1 } & { { { { \\bm{a}}}}}_{0,2 } &               { { { { \\bm{a}}}}}_{1,2 } & { { { { \\bm{a}}}}}_{0,1 } & { { { { \\bm{a}}}}}_{0,0 } \\\\               { { { { \\bm{a}}}}}_{0,0 } & { { { \\bm{0 } } } } & { { { { \\bm{a}}}}}_{1,0 } & { { { { \\bm{a}}}}}_{1,1 } &               { { { { \\bm{a}}}}}_{0,2 } & { { { { \\bm{a}}}}}_{1,2 } & { { { { \\bm{a}}}}}_{0,1 } \\\\",
    "{ { { { \\bm{a}}}}}_{0,1 } & { { { { \\bm{a}}}}}_{0,0 } & { { { \\bm{0 } } } } & { { { { \\bm{a}}}}}_{1,0 } &               { { { { \\bm{a}}}}}_{1,1 } & { { { { \\bm{a}}}}}_{0,2 } & { { { { \\bm{a}}}}}_{1,2 } \\\\               { { { { \\bm{a}}}}}_{1,2 } & { { { { \\bm{a}}}}}_{0,1 } & { { { { \\bm{a}}}}}_{0,0 } & { { { \\bm{0 } } } } &               { { { { \\bm{a}}}}}_{1,0 } & { { { { \\bm{a}}}}}_{1,1 } & { { { { \\bm{a}}}}}_{0,2 } \\\\               { { { { \\bm{a}}}}}_{0,2 } & { { { { \\bm{a}}}}}_{1,2 } & { { { { \\bm{a}}}}}_{0,1 } & { { { { \\bm{a}}}}}_{0,0 } &               { { { \\bm{0 } } } } & { { { { \\bm{a}}}}}_{1,0 } & { { { { \\bm{a}}}}}_{1,1 } \\\\               { { { { \\bm{a}}}}}_{1,1 } & { { { { \\bm{a}}}}}_{0,2 } & { { { { \\bm{a}}}}}_{1,2 } & { { { { \\bm{a}}}}}_{0,1 } &               { { { { \\bm{a}}}}}_{0,0 } & { { { \\bm{0 } } } } & { { { { \\bm{a}}}}}_{1,0 } \\\\               { { { { \\bm{a}}}}}_{1,0 } & { { { { \\bm{a}}}}}_{1,1 } & { { { { \\bm{a}}}}}_{0,2 } & { { { { \\bm{a}}}}}_{1,2 } &               { { { { \\bm{a}}}}}_{0,1 } & { { { { \\bm{a}}}}}_{0,0 } & { { { \\bm{0 } } } }             \\end{array }             \\right]\\!\\ ! ,               \\label{eq : ex : graph : cover : constructions:1:ob:1 }      \\end{aligned}\\ ] ] whose tanner graphs @xmath145 and @xmath150 , respectively , are shown in figure  [ fig : graph : cover : constructions:1](c ) . note that all the block rows and all the block columns sum ( in @xmath1 ) to @xmath120 .",
    "( this observation holds in general , not just for this example . )",
    "we would like to add two comments with respect to the above example .",
    "first , instead of defining @xmath176 to be an @xmath177 times _ left_-shifted identity matrix of size @xmath138 , we could have defined @xmath176 to be an @xmath177 times _ right_-shifted identity matrix of size @xmath181 .",
    "compared to the matrices and graphs described above , such a change in definition would yield ( in general ) different matrices but isomorphic graphs .",
    "second , we note that * gcc2 *  was termed the `` copy - and - permute '' construction by thorpe and his co - workers .",
    "this terminology stems from the visual appearance of the procedure : namely , in going from figure  [ fig : graph : cover : constructions:1](a ) to figure  [ fig : graph : cover : constructions:1](c)(top ) we copy the graph several times , and in going from figure  [ fig : graph : cover : constructions:1](c)(top ) to figure  [ fig : graph : cover : constructions:1](c)(bottom ) we permute the edges of the graph , where the permutations are done within the sets of edges that have the same pre - image in figure  [ fig : graph : cover : constructions:1](a ) .",
    "[ rem : graph : cover : construction:1:variation:1 ]    consider again the matrices that were constructed in example  [ ex : graph : cover : construction:1 ] , in particular the matrix @xmath120 in   and its @xmath8-fold cover matrix @xmath151 in  . because all matrices in the matrix collection @xmath136 are circulant",
    ", @xmath149 represents a cyclic cover of @xmath182 .",
    "clearly , when seen over @xmath183 , the matrix @xmath184 is the parity - check matrix of a quasi - cyclic binary linear block code @xmath185 using the well - known isomorphism between the addition and multiplication of circulant matrices over @xmath183 and the addition and multiplication of elements of the ring @xmath9}$ ] , this code can be written equivalently as @xmath186}^{{n_{{{{\\bm{a } } } } } } }             \\bigm|              { { { { \\bm{h}}}}_{\\mathrm{qc}}^{(r)}}(x ) \\cdot { { { { \\bm{v}}}}}(x)^{\\mathsf{t}}= { { { \\bm{0}}}}^{\\mathsf{t}}\\bigr\\ }    \\end{aligned}\\ ] ] with @xmath187    as noted above , the graphs @xmath149 and @xmath150 that are constructed in definition  [ def : graph : cover : construction:1 ] are isomorphic .",
    "applying this observation to example  [ ex : graph : cover : construction:1 ] , the matrix @xmath188 with @xmath152 from   is therefore the parity - check matrix of a binary linear block code @xmath189 that is equivalent to @xmath62 , i.e. , the codewords of @xmath190 can be obtained from the codewords of @xmath62 by a suitable reordering of the codeword components . in terms of the matrices @xmath191 , which also appear in the matrix @xmath152 in  , one can verify that the polynomial parity - check matrix @xmath63 can be written as @xmath192 .",
    "besides defining finite graph covers , we can also define infinite graph covers , as illustrated in the following examples .",
    "these infinite graph covers will be crucial towards defining tanner graphs of convolutional codes .",
    "[ ex : graph : cover : construction:1:infinite:1 ]    we continue example  [ ex : graph : cover : construction:1 ] .",
    "however , besides keeping the proto - matrix @xmath120 and the collection of matrices @xmath193 , we consider a different collection of matrices @xmath194 .",
    "namely , we set @xmath195 , @xmath196 , @xmath197 , @xmath198 , @xmath199 , @xmath200 . here",
    "@xmath201 is a bi - infinite toeplitz matrix with zeros everywhere except for ones in the @xmath177-th diagonal below the main diagonal , i.e. , @xmath202_{j , i } = 1 $ ] if @xmath203 and @xmath202_{j , i } = 0 $ ] otherwise .",
    "e.g. , @xmath204 where for clarity we have underlined the entries of the main diagonal .    *",
    "using * gcc1 * , we obtain the matrices @xmath205 and @xmath206 the tanner graph @xmath140 , which is depicted in figure  [ fig : graph : cover : constructions:1:variation:2](b)(top ) , is similar to the corresponding tanner graph in figure  [ fig : graph : cover : constructions:1](b)(top ) , but with bi - infinitely many independent components .",
    "analogously , the tanner graph @xmath149 , which is depicted in figure  [ fig : graph : cover : constructions:1:variation:2](b)(bottom ) , is similar to the tanner graph shown in figure  [ fig : graph : cover : constructions:1](b)(bottom ) , but instead of cyclically wrapped edge connections , the edge connections are infinitely continued on both sides . * using * gcc2 * , we obtain the matrices @xmath207 and @xmath208\\!.               \\label{eq : ex : graph : cover : constructions:1:variation:2:ob:1 }      \\end{aligned}\\ ] ] the tanner graph @xmath145 , which is depicted in figure  [ fig : graph : cover : constructions:1:variation:2](c)(top ) , is similar to the corresponding tanner graph in figure  [ fig : graph : cover : constructions:1](c)(top ) , but with bi - infinitely many independent components .",
    "analogously , the tanner graph @xmath150 , which is depicted in figure  [ fig : graph : cover : constructions:1:variation:2](c)(bottom ) , is similar to the tanner graph shown in figure  [ fig : graph : cover : constructions:1](c)(bottom ) , but instead of cyclically wrapped edge connections , the edge connections are infinitely continued on both sides .",
    "although it is tempting to replace in example  [ ex : graph : cover : construction:1:infinite:1 ] the bi - infinite toeplitz matrices @xmath201 ( whose row and column index sets equal @xmath1 ) by semi - infinite toeplitz matrices ( whose row and column index sets equal @xmath2 ) , note that the resulting tanner graphs @xmath149 and @xmath150 would then in general not be graph covers of @xmath122 .",
    "this follows from the fact that semi - infinite toeplitz matrices are not permutation matrices ( except for @xmath209 ) , and so some vertex degrees of @xmath149 and @xmath150 would not equal the corresponding vertex degrees in @xmath122 .",
    "[ rem : graph : cover : connections:1 ]    it turns out that the tanner graphs in figure  [ fig : graph : cover : constructions:1:variation:2 ] are infinite graph covers of the tanner graphs in figure  [ fig : graph : cover : constructions:1 ] .",
    "more precisely , the tanner graphs @xmath140 , @xmath149 , @xmath145 , @xmath150 in figure  [ fig : graph : cover : constructions:1:variation:2 ] are graph covers of the corresponding tanner graphs @xmath140 , @xmath149 , @xmath145 , @xmath150 in figure  [ fig : graph : cover : constructions:1 ]",
    ". for the tanner graphs @xmath140 in figures  [ fig : graph : cover : constructions:1](b)(top ) and  [ fig : graph : cover : constructions:1:variation:2](b)(top ) and the tanner graphs @xmath145 in figures  [ fig : graph : cover : constructions:1](c)(top ) and  [ fig : graph : cover : constructions:1:variation:2](c)(top ) , this statement is easily verified by inspection .    to verify that the tanner graph @xmath150 in figure  [ fig : graph : cover : constructions:1:variation:2](c)(bottom ) is a graph cover of @xmath150 in figure  [ fig : graph : cover : constructions:1](c)(bottom )",
    ", we apply * gcc2 * with proto - matrix @xmath120 , with resulting matrix @xmath152 , with the set @xmath127 , with the collection of matrices @xmath210 , and with the collection of permutation matrices @xmath211 as follows .",
    "namely , we let the proto - matrix @xmath120 be the matrix from   ( there denoted by @xmath152 ) , we let the resulting matrix @xmath152 be the matrix in   ( there denoted by @xmath152 ) , we define @xmath212 , we select @xmath213 ,             \\label{eq : rem : graph : cover : connections:1:a:0 } \\\\[0.25 cm ]      { { { { \\bm{a}}}}}_1        & = \\left [           \\begin{array}{@{\\;}c@{\\;\\;}c@{\\;\\;}c@{\\;\\;}c@{\\;\\;}c@{\\;\\;}c@{\\;\\;}c@{\\ ; } }             { { { \\bm{0 } } } } & { { { { \\bm{a}}}}}_{1,0 } & { { { { \\bm{a}}}}}_{1,1 } & { { { { \\bm{a}}}}}_{0,2 } &             { { { { \\bm{a}}}}}_{1,2 } & { { { { \\bm{a}}}}}_{0,1 } & { { { { \\bm{a}}}}}_{0,0 } \\\\             { { { \\bm{0 } } } } & { { { \\bm{0 } } } } & { { { { \\bm{a}}}}}_{1,0 } & { { { { \\bm{a}}}}}_{1,1 } &             { { { { \\bm{a}}}}}_{0,2 } & { { { { \\bm{a}}}}}_{1,2 } & { { { { \\bm{a}}}}}_{0,1 } \\\\             { { { \\bm{0 } } } } & { { { \\bm{0 } } } } & { { { \\bm{0 } } } } & { { { { \\bm{a}}}}}_{1,0 } &             { { { { \\bm{a}}}}}_{1,1 } & { { { { \\bm{a}}}}}_{0,2 } & { { { { \\bm{a}}}}}_{1,2 } \\\\             { { { \\bm{0 } } } } & { { { \\bm{0 } } } } & { { { \\bm{0 } } } } & { { { \\bm{0 } } } } &             { { { { \\bm{a}}}}}_{1,0 } & { { { { \\bm{a}}}}}_{1,1 } & { { { { \\bm{a}}}}}_{0,2 } \\\\             { { { \\bm{0 } } } } & { { { \\bm{0 } } } } & { { { \\bm{0 } } } } & { { { \\bm{0 } } } } &             { { { \\bm{0 } } } } & { { { { \\bm{a}}}}}_{1,0 } & { { { { \\bm{a}}}}}_{1,1 } \\\\             { { { \\bm{0 } } } } & { { { \\bm{0 } } } } & { { { \\bm{0 } } } } & { { { \\bm{0 } } } } &             { { { \\bm{0 } } } } & { { { \\bm{0 } } } } & { { { { \\bm{a}}}}}_{1,0 } \\\\             { { { \\bm{0 } } } } & { { { \\bm{0 } } } } & { { { \\bm{0 } } } } & { { { \\bm{0 } } } } &             { { { \\bm{0 } } } } & { { { \\bm{0 } } } } & { { { \\bm{0 } } } }           \\end{array }           \\right ] ,             \\label{eq : rem : graph : cover : connections:1:a:1 }    \\end{aligned}\\ ] ] and we select @xmath214 and @xmath215 , where @xmath201 was defined in example  [ ex : graph : cover : construction:1:infinite:1 ] . clearly , @xmath216 . would be put in @xmath217 . ] with this we have @xmath218 \\!\\ ! ,    \\end{aligned}\\ ] ] and one can verify that this matrix equals the matrix in   ( there denoted by @xmath152 ) , which means that @xmath150 is indeed an infinite cover of @xmath122 .",
    "we remark that , interestingly , in this process we have shown how a certain * gcc2 *  graph cover of a proto - matrix can be written as a * gcc2 *  graph cover of a certain * gcc2 *  graph cover of that proto - matrix .",
    "finally , a similar argument shows that the tanner graph @xmath149 in figure  [ fig : graph : cover : constructions:1:variation:2](b)(top ) is a graph cover of the tanner graph in figure  [ fig : graph : cover : constructions:1](b)(top ) , also denoted by @xmath149 .",
    "there are many other ways of writing a proto - matrix @xmath120 as a sum of a collection of matrices @xmath135 .",
    "the next example discusses two such possibilities .",
    "[ ex : graph : cover : construction:2 ]    consider the proto - matrix @xmath219 that is shown in figure  [ fig : graph : cover : constructions:2](a ) , and that also appeared in example  [ example : unwrap:1 ] .",
    "its tanner graph @xmath122 is @xmath220-regular , i.e. , all variable nodes have degree @xmath221 and all check nodes have degree @xmath222 .",
    "let @xmath223 , and consider the collection of matrices @xmath136 with @xmath214 and @xmath215 , where the matrices @xmath209 and @xmath224 are defined as in example  [ ex : graph : cover : construction:1:infinite:1 ] . in the following ,",
    "we look at two different choices of the collection of matrices @xmath162 .",
    "* figure  [ fig : graph : cover : constructions:2](c ) shows a typical part of the matrix @xmath152 that is obtained when * gcc2 *  is used to construct a graph cover of @xmath120 with the collection of matrices @xmath225 defined as shown in figure  [ fig : graph : cover : constructions:2](a ) . * figure  [ fig : graph :",
    "cover : constructions:2](d ) shows a typical part of the matrix @xmath152 when * gcc2 *  is used to construct a graph cover of @xmath120 with the collection of matrices @xmath128 defined as shown in figure  [ fig : graph : cover : constructions:2](b ) .",
    "overall , because of the choice of the collection @xmath226 , the support of both matrices @xmath152 possesses a banded diagonal structure . moreover , the different choices of the collection @xmath162 leads to a somewhat narrower banded diagonal structure in the first case compared to the second case .",
    "the next example makes a crucial observation ; namely , it shows that the above graph - cover constructions can be applied repeatedly to obtain additional interesting classes of tanner graphs .",
    "[ ex : graph : cover : construction:3 ]    starting with the proto - matrix @xmath227 from example  [ ex : graph : cover : construction:1 ] , we consider two iterated graph - cover constructions . in the first case , we apply * gcc1 *  and then * gcc2 * , and in the second case we apply * gcc2 *  twice .    *",
    "consider the matrix @xmath151 obtained from the matrix @xmath120 using * gcc1 * , like in example  [ ex : graph : cover : construction:1 ] .",
    "the resulting matrix @xmath151 is shown in   and will be called @xmath228 in this example , since it is considered to be a proto - matrix by itself , cf .  figure  [ fig : graph :",
    "cover : construction:3](a ) . based on the `` cutting line '' shown in figure  [",
    "fig : graph : cover : construction:3](a ) , we define the matrices @xmath229 and @xmath230 as follows : the non - zero part of @xmath229 equals the non - zero part of the lower triangular part of @xmath228 and the non - zero part of @xmath230 equals the non - zero part of the upper triangular part of @xmath228 .",
    "( clearly , @xmath231 . )",
    "applying the procedure from example  [ ex : graph : cover : construction:2 ] , figure  [ fig : graph : cover : construction:3](c ) shows a typical part of the matrix @xmath232 that is obtained when * gcc2 *  is used to construct a graph cover of @xmath228 . *",
    "consider the graph - cover @xmath152 obtained from @xmath120 using * gcc2 * , like in example  [ ex : graph : cover : construction:1 ] .",
    "the resulting matrix @xmath152 is shown in   and will be called @xmath233 in this example , since it is considered to be a proto - matrix by itself , cf .  figure  [ fig : graph :",
    "cover : construction:3](b ) . based on the `` cutting line '' shown in figure  [",
    "fig : graph : cover : construction:3](b ) , we define the matrices @xmath234 and @xmath235 as follows : the non - zero part of @xmath234 equals the non - zero part of the lower triangular part of @xmath233 and the non - zero part of @xmath235 equals the non - zero part of the upper triangular part of @xmath233 .",
    "( clearly , @xmath236 . )",
    "applying the procedure from example  [ ex : graph : cover : construction:2 ] , figure  [ fig : graph : cover : construction:3](d ) shows a typical part of the matrix @xmath237 that is obtained when * gcc2 *  is used to construct a graph cover of @xmath233 .",
    "we observe a large difference in the positions of the non - zero entries in @xmath232 and @xmath237 .",
    "* in the first case , the two graph - cover constructions are `` incompatible '' and the positions of the non - zero entries in @xmath232 follow a `` non - simple '' or `` pseudo - random '' pattern . as we will see in example  [ ex : iterated : gcc : ldpc : code : construction:1 ] with the help of simulation results , such tanner graphs can lead to time - varying ldpc convolutional codes with very good performance . * in the second case , the two graph - cover constructions are `` compatible '' in the sense that @xmath237 can be obtained from the proto - matrix @xmath120 by applying * gcc2 *  with suitable matrix collections @xmath135 and @xmath238 . as such , the positions of the non - zero entries of @xmath237 follow a relatively `` simple '' or `` non - random '' pattern , which leads to a time - invariant ldpc convolutional code .",
    "the above procedure of obtaining two matrices that add up to a matrix is called `` cutting a matrix '' .",
    "actually , we will also use this term if there is no simple cutting line , as in the above examples , and also if the matrix is written as the sum of more than two matrices ( cf .",
    "example  [ example : unwrap:1 ] and the paragraphs after it ) .",
    "in section  [ sec : unwrapping:1 ] we introduced two techniques , termed the tanner and the jfz unwrapping techniques , to derive convolutional codes from block codes . in this subsection",
    "we revisit these unwrapping techniques .",
    "in particular , we show how they can be cast in terms of graph covers and how the two unwrapping techniques are connected .",
    "because of the importance of the coding - theoretic notion of shortening  @xcite for this subsection , we briefly revisit this concept .",
    "let @xmath239 be a parity - check matrix that defines some length-@xmath11 binary code @xmath240 .",
    "we say that the length-@xmath241 code @xmath242 is obtained by shortening @xmath240 at position @xmath15 if @xmath243 in terms of parity - check matrices , a possible parity - check matrix @xmath244 of @xmath242 is obtained by deleting the @xmath15-th column of @xmath239 . in terms of tanner graphs , this means that the tanner graph @xmath245 is obtained from @xmath246 by removing the @xmath15-th variable node , along with its incident edges . in the following",
    ", we will also use the term `` shortening '' to denote this graph modification procedure .",
    "now , to explain the tanner unwrapping technique in terms of graph covers , consider the quasi - cyclic block code @xmath62 defined by the polynomial parity - check matrix @xmath63 of size @xmath121 , i.e. , @xmath65}^{{n_{{{{\\bm{a } } } } } } }           \\bigm|            { { { { \\bm{h}}}}_{\\mathrm{qc}}^{(r)}}(x ) \\cdot { { { { \\bm{v}}}}}(x)^{\\mathsf{t}}= { { { \\bm{0}}}}^{\\mathsf{t}}\\bigr\\},\\end{aligned}\\ ] ] where the polynomial operations are performed modulo @xmath7 ( see also remark  [ rem : graph : cover : construction:1:variation:1 ] ) . as already mentioned in section  [ sec : unwrapping:1 ] , the tanner unwrapping technique is simply based on dropping these modulo computations .",
    "more precisely , with a quasi - cyclic block code @xmath62 , we associate the convolutional code @xmath66^{{n_{{{{\\bm{a } } } } } } }           \\bigm|           { { { { \\bm{h}}}}_{\\mathrm{conv}}}(d ) \\cdot { { { { \\bm{v}}}}}(d)^{\\mathsf{t}}= { { { \\bm{0}}}}^{\\mathsf{t}}\\bigr\\}\\end{aligned}\\ ] ] with polynomial parity - check matrix @xmath247 again , the change of indeterminate from @xmath5 to @xmath68 indicates the lack of modulo @xmath69 operations .",
    "in the following we will give , with the help of an example , two interpretations of the tanner unwrapping technique in terms of graph covers .",
    "[ ex : tanner : unwrapping:1:rev:1 ]    unwrapping the quasi - cyclic block code @xmath62 that was considered in remark  [ rem : graph : cover : construction:1:variation:1 ] , we obtain a rate-@xmath59 time - invariant convolutional code @xmath248^{{n_{{{{\\bm{a } } } } } } }             \\bigm|              { { { { \\bm{h}}}}_{\\mathrm{conv}}}(d ) \\cdot { { { { \\bm{v}}}}}(d)^{\\mathsf{t}}= { { { \\bm{0}}}}^{\\mathsf{t}}\\bigr\\ }    \\end{aligned}\\ ] ] with polynomial parity - check matrix @xmath249 consider now the infinite graph covers that were constructed in example  [ ex : graph : cover : construction:1:infinite:1 ] using * gcc1 * , in particular @xmath149 .",
    "let @xmath250 be the set of codewords defined by the tanner graph @xmath149 .",
    "then the convolutional code @xmath72 is a shortened version of @xmath251 where all codeword bits corresponding to negative time indices have been shortened .",
    "therefore , the tanner graph of @xmath72 is given by the tanner graph in figure  [ fig : graph : cover : constructions:1:variation:2](b)(bottom ) , where all bit nodes with negative time indices , along with their incident edges , are removed .",
    "clearly , this bit - node and edge removal process implies decreasing the degrees of some check nodes .",
    "in fact , some check nodes become obsolete , because their degree is decreased to zero .    therefore , one interpretation of the tanner unwrapping technique in terms of graph covers is that the tanner graph of the convolutional code is obtained by taking a suitable graph cover of the same proto - graph that was used to construct the quasi - cyclic ldpc code , along with some suitable shortening .",
    "[ ex : tanner : unwrapping:1:rev:2 ]    we continue remark  [ rem : graph : cover : construction:1:variation:1 ] and example  [ ex : tanner : unwrapping:1:rev:1 ] .",
    "clearly , in the same way as the block code @xmath190 is equivalent to the block code @xmath62 , we can define a code @xmath21 ( with parity - check matrix @xmath24 ) that is equivalent to @xmath72 .",
    "the observations in remark  [ rem : graph : cover : connections:1 ] and example  [ ex : tanner : unwrapping:1:rev:1 ] can then be used to show that the tanner graph of @xmath24 equals a graph cover of the tanner graph @xmath252 , along with some suitable shortening .",
    "therefore , the second interpretation of the tanner unwrapping in terms of graph covers is that the tanner graph of the convolutional code is obtained by taking a suitable graph cover of the tanner graph of the quasi - cyclic code , along with some suitable shortening .    now turning our attention to the jfz unwrapping technique , recall from section  [ sec : unwrapping:1 ] that this method is based on writing a parity - check matrix @xmath80 of some block code @xmath96 as the sum @xmath253 of a collection of matrices @xmath254 .",
    "the convolutional code is then defined to be @xmath255 where @xmath256 .",
    "\\label{eq : code : c : conv : jfz : unwrapping:1:rev : pcm:1}\\end{aligned}\\ ] ]    with the help of an example , we now explain how the jfz unwrapping technique can be cast in terms of graph - covers .    [",
    "ex : jfz : unwrapping:2:rev ]    consider the infinite graph covers that were constructed using * gcc2 *  in example  [ ex : graph : cover : construction:1:infinite:1 ] , in particular @xmath150 .",
    "let @xmath257 be the set of valid assignments to the tanner graph @xmath150 .",
    "moreover , let @xmath258 , and let @xmath21 be defined as in  . then the code @xmath21 is a shortened version of @xmath259 , where all codeword bits corresponding to negative time indices have been shortened .",
    "therefore , the tanner graph of @xmath21 is given by the tanner graph in figure  [ fig : graph : cover : constructions:1:variation:2](c)(bottom ) , where all the bit nodes with negative time indices are shortened .    in order to connect the unwrapping techniques due to tanner and due to jfz",
    ", we show now , with the help of an example , that in fact the unwrapping technique due to tanner can be seen as a _ special case _ of the unwrapping technique due to jfz .",
    "consider the quasi - cyclic block code defined by the parity - check matrix @xmath260 , where @xmath152 was defined in  . applying the jfz unwrapping technique with the matrix decomposition @xmath261 , with @xmath217 defined in   and @xmath262 defined in",
    ", @xmath24 turns out to equal a submatrix of @xmath152 in  , namely the submatrix of @xmath152 where the row and column index set are equal to @xmath2 .",
    "however , the code defined by @xmath24 is equivalent to the code defined by the tanner unwrapping technique applied to the quasi - cyclic code defined by @xmath252 .    therefore , the unwrapping technique due to jfz is more general .",
    "in fact , whereas the tanner unwrapping technique leads to _ time - invariant _ convolutional codes , the unwrapping technique due to jfz can , depending on the parity - check matrix decomposition and the internal structure of the terms in the decomposition , lead to _ time - varying _ convolutional codes with non - trivial period .    despite the fact that the unwrapping technique due to tanner is a special case of the unwrapping technique due to jfz , it is nevertheless helpful to have both unwrapping techniques at hand , because sometimes one framework can be more convenient than the other .",
    "we will use both perspectives in the next section .",
    "we conclude this section with the following remarks .    *",
    "although most of the examples in this section have regular bit node degree  @xmath263 and regular check node degree  @xmath221 , there is nothing special about this choice of bit and check node degrees ; any other choice would work equally well . *",
    "although all polynomial parity - check matrices that appear in this section contain only monomials , this is not required , i.e. , the developments in this section work equally well for polynomial parity - check matrices containing the zero polynomial , monomials , binomials , trinomials , and so on . *",
    "it can easily be verified that if the matrix @xmath120 in definition  [ def : graph : cover : construction:1 ] contains only zeros and ones , then the graph covers constructed in * gcc1 *  and * gcc2 *  never have parallel edges .",
    "in particular , if @xmath120 is the parity - check matrix of a block code ( like in most examples in this paper ) , then the constructed graph covers never have parallel edges .",
    "+ however , if @xmath120 contains entries that are larger than one , then there is the potential for the constructed graph covers to have parallel edges ; if parallel edges really appear depends then critically on the choice of the decomposition @xmath264 and the choice of the permutation matrices @xmath211 .",
    "an example of such a case is the tanner graph construction in section  [ sec : ldpc : code : construction : lentmaier : kudekar:1 ] , where @xmath265 $ ] and where @xmath135 and @xmath266 are chosen such that parallel edges are avoided in the constructed graph cover .",
    "+ we note that in the case of iterated graph - cover constructions it can make sense to have parallel edges in the intermediate graph covers .",
    "however , in the last graph - cover construction stage , parallel edges are usually avoided , because parallel edges in tanner graphs typically lead to a weakening of the code and/or of the message - passing iterative decoder .",
    "although the graph - cover constructions and unwrapping techniques that were discussed in sections  [ sec : ldpc : convolutional : codes:1 ] and  [ sec : graph : covers:1 ] are mathematically quite straightforward , it is important to understand how they can be applied to obtain ldpc convolutional codes with good performance and attractive encoder and decoder architectures . to that end",
    ", this section explores a variety of code design options and comments on some practical issues .",
    "it also proposes a new `` random '' unwrapping technique which leads to convolutional codes whose performance compares favorably to other codes with the same parameters .",
    "of course , other variations than the ones presented here are possible , in particular , by suitably combining some of the example constructions .",
    "the simulation results for the codes in this section plot the decoded bit error rate ( ber ) versus the signal - to - noise ratio ( snr ) @xmath267 and were obtained by assuming bpsk modulation and an additive white gaussian noise channel ( awgnc ) .",
    "all decoders were based on the sum - product algorithm  @xcite and were allowed a maximum of 100 iterations , with the block code decoders employing a syndrome - check based stopping rule . for comparing the performance of unwrapped convolutional codes with their underlying block codes we will use the following metric .    for a convolutional code constructed from an underlying block code ,",
    "we define its `` convolutional gain '' to be the difference in snr required to achieve a particular ber with the convolutional code compared to achieving the same ber with the block code .",
    "the rest of this section is structured as follows .",
    "first we discuss the construction of some _ time - invariant _ ldpc convolutional codes based on the tanner unwrapping technique . in this context",
    "we make a simple observation about how the syndrome former memory can sometimes be reduced without changing the convolutional code .",
    "secondly , we present a construction of _ time - varying _ ldpc convolutional codes based on iterated graph - cover constructions . an important sub - topic here",
    "will be an investigation of the influence of the `` diagonal cut '' ( which is used to define a graph cover ) on the decoding performance .      in this section",
    "we revisit a class of quasi - cyclic ldpc codes and their associated convolutional codes that was studied in  @xcite .",
    "as we will see , they are instances of the quasi - cyclic code construction in example  [ ex : graph : cover : construction:1 ] and remark  [ rem : graph : cover : construction:1:variation:1 ] , and the corresponding convolutional code construction based on tanner s unwrapping technique in example  [ ex : tanner : unwrapping:1:rev:1 ] .",
    "[ ex : tanner : qc : code:1 ]    consider the regular proto - matrix @xmath268 with @xmath269 and @xmath270 .",
    "we apply * gcc1 * , as in example  [ ex : graph : cover : construction:1 ] and remark  [ rem : graph : cover : construction:1:variation:1 ] , with an interesting choice of permutation matrices first suggested by tanner @xcite that yields the parity - check matrix @xmath271 where as before @xmath176 is an @xmath177 times left - circularly shifted identity matrix of size @xmath138 and @xmath272 .",
    "the corresponding polynomial parity - check is @xmath273 the resulting quasi - cyclic @xmath274-regular ldpc block codes have block length @xmath275 . in particular , for @xmath276 , @xmath277 , and @xmath278 , we obtain codes of length @xmath279 , @xmath280 , and @xmath281 , respectively , whose simulated ber performance results are shown in figure  [ fig : sims ] .",
    "the choice @xmath276 yields the well - known length-@xmath279 quasi - cyclic block code that was first introduced by tanner  @xcite ( see also the discussion in  @xcite ) .",
    "unwrapping these codes by the tanner unwrapping technique as in example [ ex : tanner : unwrapping:1:rev:1 ] , we obtain a rate-@xmath282 time - invariant convolutional code with @xmath283 defined by the polynomial parity - check matrix @xmath284 its decoding performance is also shown in figure  [ fig : sims ] under the label `` @xmath285 time - invariant conv .",
    "code with @xmath283 . ''",
    "we conclude this example with a few remarks .",
    "* figure  [ fig : sims ] shows that the convolutional code exhibits a `` convolutional gain '' of between @xmath286 and @xmath287 compared to the @xmath288 $ ] quasi - cyclic ldpc block code at moderate bers and that the gain remains between @xmath289 and @xmath290 at lower bers .",
    "* note that the polynomial parity - check matrix @xmath291 that is obtained by the tanner unwrapping technique is independent of the parameter @xmath8 of the polynomial parity - check matrix @xmath63 , as long as @xmath8 is strictly larger than the largest exponent appearing in @xmath63 .",
    "moreover , for @xmath292 , the tanner graph of @xmath63 is closely related to the tanner graph of @xmath291 , and so it is not surprising to see that , for larger @xmath8 , the decoding performance of quasi - cyclic ldpc block codes based on @xmath63 tends to the decoding performance of the ldpc convolutional based on @xmath291 , as illustrated by the two curves labeled `` @xmath293 $ ] qc code '' and `` @xmath294 $ ] qc code '' in figure  [ fig : sims ] . *",
    "the permutation matrices ( more precisely , the circulant matrices ) that were used for constructing the quasi - cyclic codes in this example were _ not _ chosen to optimize the hamming distance or the pseudo - weight properties of the code . in particular , a different choice of circulant matrices may result in better high - snr performance , i.e. , in the so - called `` error floor '' region of the ber curve . for choices of codes with better hamming distance properties ,",
    "we refer the reader to  @xcite . * the remaining curves in figure  [ fig : sims ]",
    "will be discussed in example  [ ex : iterated : gcc : ldpc : code : construction:1 ] .",
    "we conclude this subsection with some comments on the syndrome former memory @xmath31 of the convolutional codes obtained by the tanner unwrapping technique , in particular how this syndrome former memory @xmath31 can sometimes be reduced without changing the convolutional code .",
    "assume that we have obtained a polynomial parity - check matrix @xmath291 from @xmath63 according to the tanner method . clearly , the syndrome former memory @xmath31 is given by the largest exponent that appears in @xmath291 . in some instances there is a simple way of reducing @xmath31 without changing the convolutional code .",
    "namely , if @xmath295 is the _ minimal _ exponent that appears in the polynomials of a given row of @xmath291 , then the polynomials in this row of @xmath291 can be divided by @xmath296 .",
    "we illustrate this syndrome former memory reduction for the small convolutional code that appeared in example  [ ex : tanner : unwrapping:1:rev:1 ] .",
    "[ ex : syndrome : former : memory : reduction:1 ]    applying the tanner unwrapping technique to the polynomial parity - check matrix @xmath63 of the quasi - cyclic ldpc code with @xmath297 in remark  [ rem : graph : cover : construction:1:variation:1 ] , we obtain @xmath291 of a rate-@xmath59 time - invariant ldpc convolutional code , as shown in example  [ ex : tanner : unwrapping:1:rev:1 ] , with syndrome former memory @xmath298 .",
    "following the procedure discussed in the paragraph above , the first and second rows of @xmath291 can be divided by @xmath299 and @xmath300 , respectively , to yield an equivalent convolutional code with syndrome former memory @xmath301 and polynomial parity - check matrix @xmath302 figure [ fig : ex : syndrome : former : memory : reduction:1 ] shows parts of the corresponding scalar parity - check matrix @xmath24 for @xmath301 , together with the original scalar parity - check matrix for @xmath298 , and illustrates the equivalence of the two matrices in the sense that only the ordering of the rows is different , which does not affect the corresponding convolutional code . in this example , the order of the even - numbered rows stays the same , while the odd - numbered rows are shifted by four positions .",
    "the equivalence of the two parity - check matrices can be seen by noting that the parity - check matrix , outside of the diagonal structure , is filled with zeros .      as was seen in example",
    "[ ex : graph : cover : construction:3 ] , interesting graph covers can be obtained by combining *",
    "gcc1 *  with * gcc2 * , or vice - versa .",
    "inspired by that example , this subsection considers iterated graph - cover constructions for constructing tanner graphs of ldpc convolutional codes , in particular of time - varying ldpc convolutional codes .",
    "[ def : iterated : gcc : ldpc : code : construction:1 ]    based on a combination of * gcc1 * , * gcc2 * , and the code - shortening concept introduced in section  [ sec : unwrapping:1:rev ] , we propose the following construction of ldpc convolutional codes .    1 .",
    "we start with a proto - matrix @xmath120 of size @xmath121 .",
    "[ item : iterated : gcc : first : lifting:1 ] we apply * gcc1 *  to @xmath120 with finite - size permutation matrices and obtain the matrix @xmath303 .",
    "[ item : iterated : gcc : second : lifting:1 ] we apply * gcc2 *  to @xmath303 with permutation matrices that are bi - infinite toeplitz matrices and obtain the matrix @xmath304 .",
    "[ item : iterated : gcc : tanner : unwrapping:1 ] finally , looking at @xmath304 as the parity - check matrix of a bi - infinite convolutional code , we obtain the parity - check matrix of a convolutional code by shortening the code bit positions corresponding to negative time indices .    here ,",
    "steps  [ item : iterated : gcc : second : lifting:1 ] and  [ item : iterated : gcc : tanner : unwrapping:1 ] can be seen as an application of the jfz unwrapping method .     and",
    "( b ) @xmath301.,scaledwidth=50.0% ]    the following example shows how this construction can be applied to obtain ldpc convolutional codes with excellent performance .",
    "( in the example , where suitable , we will refer to the analogous matrices of example  [ ex : graph : cover : construction:3 ] and figure  [ fig : graph : cover : construction:3 ] that were used to illustrate the iterated graph - cover construction . )",
    "[ ex : iterated : gcc : ldpc : code : construction:1 ]    based on definition  [ def : iterated : gcc : ldpc : code : construction:1 ] , we construct an ldpc convolutional code by performing the following steps .    1 .",
    "we start with the same regular proto - matrix @xmath120 as in example  [ ex : tanner : qc : code:1 ] , for which @xmath269 and @xmath270 .",
    "we apply * gcc1 *  to @xmath120 with permutation matrices that are circulant matrices of size @xmath138 and obtain the parity - check matrix @xmath305 shown in  , which is the analogue of @xmath228 in figure  [ fig : graph : cover : construction:3](a ) .",
    "we apply * gcc2 *  to @xmath305 with permutation matrices that are bi - infinite toeplitz matrices and obtain a new parity - check matrix @xmath304 .",
    "this is analogous to the transition of the matrix @xmath228 in figure  [ fig : graph : cover : construction:3](a ) to the matrix @xmath232 in figure  [ fig : graph : cover : construction:3](c ) .",
    "the `` diagonal cut '' is obtained by alternately moving @xmath270 units to the right and then @xmath269 units down .",
    "4 .   finally , we obtain the desired convolutional code by shortening the code bit positions corresponding to negative time indices .    for the choices @xmath276 , @xmath306 , @xmath307 , this",
    "construction results in rate-@xmath282 time - varying convolutional codes with syndrome former memory @xmath308 , @xmath309 , @xmath310 , respectively , and with constraint length @xmath311 , @xmath280 , @xmath281 , respectively .",
    "the label `` time - varying '' is indeed justified because the convolutional codes constructed here can be expressed in the form of the parity - check matrix in   with a suitable choice of syndrome former memory @xmath31 , non - trivial period @xmath37 , and submatrices @xmath312 .",
    "the decoding performance of these codes is shown in figure  [ fig : sims ] , labeled `` r = 2/5 time - varying conv .",
    "code with @xmath313 '' .",
    "as originally noted in @xcite , we observe that these three ldpc convolutional codes achieve significantly better performance at a ber of @xmath314 than the other codes shown in this plot , namely with `` convolutional gains '' of @xmath315 for the @xmath316 convolutional code , @xmath317 for the @xmath318 convolutional code , and @xmath319 for the @xmath320 convolutional code , compared to the three respective underlying ldpc block codes .    in order to compare these codes based on a given decoding processor ( hardware ) complexity",
    ", we consider a block code of length @xmath321 ( see  @xcite and  @xcite ) .",
    "the above time - varying convolutional code for @xmath276 has constraint length @xmath322 , and hence approximately the same processor complexity as the quasi - cyclic block code of length @xmath323 in figure  [ fig : sims ] and the time - invariant convolutional code with @xmath283 in figure  [ fig : sims ] , but it achieves large gains compared to both of these codes .",
    "we note , in addition , that the performance of the time - varying convolutional code with constraint length @xmath320 is quite remarkable , since , at a ber of @xmath324 , it performs within @xmath325 of the iterative decoding threshold of @xmath326 , while having the same processor complexity as a block code of length only @xmath327 . in section [ sec : cost ] , we discuss some possible reasons for these `` convolutional gains , '' along with their associated implementation costs in terms of decoder memory and decoding delay .",
    "we make the following observations with respect to the above definition and example .    * the ldpc code construction in the above example yields time - varying ldpc convolutional codes with syndrome former memory @xmath328 and period @xmath329 .",
    "most importantly , varying @xmath8 in the above construction leads to different ldpc convolutional codes .",
    "this is in contrast to the tanner unwrapping technique discussed in section  [ sec : varying : circulant : size:1 ] , where the obtained ldpc convolutional code is independent of the parameter @xmath8 , as long as @xmath8 is strictly larger than the largest exponent in @xmath63 . *",
    "as mentioned previously in example  [ ex : graph : cover : construction:3 ] , the iterated graph - cover construction based on the combination of * gcc1 * and * gcc2 *  yields tanner graphs that have a `` pseudo - random '' structure , a structure that seems to be beneficial as indicated by the above simulation results .",
    "( we remark that the improved performance of the time - varying ldpc convolutional codes obtained by unwrapping a randomly constructed ldpc block code was first noted by lentmaier _",
    "_ in  @xcite . )",
    "* instead of constructing a first parity - check matrix as in step  [ item : iterated : gcc : first : lifting:1 ] of definition  [ def : iterated : gcc : ldpc : code : construction:1 ] , one can also start with any other ( randomly or non - randomly constructed , regular or irregular ) parity - check matrix , and still achieve a `` convolutional gain . ''",
    "the next example is an illustration of this point .",
    "[ example : divsalar : codes : derivatives:1 ]    as was done in  @xcite , one can replace the parity - check matrix that was constructed in step  [ item : iterated : gcc : first : lifting:1 ] of definition  [ def : iterated : gcc : ldpc : code : construction:1 ] by an irregular ldpc block code with optimized iterative decoding thresholds .",
    "in particular , one can start with the parity - check matrix of the rate-@xmath76 irregular proto - graph - based code from  @xcite with an iterative decoding threshold of @xmath330 , and several of its punctured versions .",
    "figure  [ fig : jplsims ] shows simulation results for the obtained block and convolutional codes .",
    "each simulated block code had a block length of about @xmath331 , with code rates ranging from @xmath76 to @xmath332 .",
    "we see that `` convolutional gains '' ranging from @xmath333 to @xmath334 at a ber of @xmath324 were obtained .",
    "similarly , it was shown in  @xcite that an ldpc convolutional code derived from a randomly constructed rate-@xmath76 irregular ldpc block code with block length @xmath335 outperformed the underlying code by almost @xmath336 at a ber of @xmath324 .",
    "the degree distribution of the underlying ldpc block code was fully optimized and had an iterative decoding threshold of @xmath337  @xcite .    of course , there are other ways of applying the `` diagonal cut '' in step  [ item : iterated : gcc : second : lifting:1 ] of example  [ ex : iterated : gcc : ldpc : code : construction:1 ] , and so it is natural to investigate the influence of different `` diagonal cuts '' on the decoding performance .",
    "we will do this in the next few paragraphs by extending the discussion that was presented right after example  [ example : unwrap:1 ] .",
    "we start by assuming that the matrix after step  [ item : iterated : gcc : first : lifting:1 ] of definition  [ def : iterated : gcc : ldpc : code : construction:1 ] has size @xmath338 , and define @xmath339 .",
    "then , for any positive integer @xmath52 that divides @xmath340 , we can perform a `` diagonal cut '' where we alternately move @xmath341 units to the right and then @xmath342 units down ( i.e. , @xmath343 . with this ,",
    "the obtained convolutional code is a periodically time - varying ldpc convolutional code with rate @xmath92 , syndrome former memory @xmath344 , period @xmath345 , and constraint length @xmath346 .",
    "( note that the syndrome former memory @xmath347 depends on @xmath52 , but the constraint length @xmath348 is independent of @xmath52 . )",
    "-regular ldpc block code using different step sizes . ]",
    "[ ex : diagonal : cuts : big : ldpc : block : codes:1 ]    here we simulate the performance of some ldpc convolutional codes obtained according to the above generalization of the `` diagonal cut . ''",
    "namely , we start with a randomly - constructed @xmath220-regular ldpc block code based on a parity - check matrix of size @xmath349 . therefore @xmath350 , @xmath351 , and @xmath352 .",
    "( note that @xmath353 and @xmath354 in this case . )",
    "figure [ fig : results400 ] shows the performance of the resulting family of ldpc convolutional codes , where @xmath52 varies in powers of @xmath263 from @xmath17 to @xmath355 , each with constraint length @xmath356 .",
    "we make the following observations .",
    "first , the case @xmath357 is not interesting because it results in @xmath358 , i.e. , it is a trivial concatenation of copies of the block code , and so the ber is the same as for the underlying block code .",
    "secondly , for all other choices of @xmath52 , the constructed codes perform very similarly , each exhibiting a sizable `` convolutional gain '' compared to the block code , although the syndrome former memory @xmath347 is different in each case .",
    "a special case of the above code construction deserves mention .",
    "when @xmath359 , i.e. , @xmath360 and @xmath11 are relatively prime , the only possible step size is obtained by choosing @xmath361 , which results in the above - mentioned uninteresting case of trivial concatenations of copies of the block code .",
    "however , all - zero columns can be inserted in the parity - check matrix such that a value of @xmath362 is obtained , which allows a step size to be chosen that results in a convolutional code with @xmath363 .",
    "the variable nodes corresponding to the all - zero columns are not transmitted , i.e. , they are punctured , so that the rate corresponds to the size of the original parity - check matrix .    for the `` diagonal cut '' ldpc convolutional code constructions discussed above , the unwrapped convolutional codes have the minimum possible constraint length @xmath348 , which is equal to the block length of the underlying block code .",
    "although this is a desirable property for practical implementation , we do not need to limit ourselves to diagonal cuts in general .",
    "inspired by the graph - cover construction of figures  [ fig : graph : cover : constructions:2](b ) and  [ fig : graph : cover : constructions:2](d ) in example  [ ex : graph : cover : construction:2 ] , instead of a `` diagonal cut '' we now consider a `` random cut , '' which we define as a partition of the parity - check matrix into two matrices that add up ( over @xmath1 ) to the parity - check matrix . despite the randomness of this approach ,",
    "several of the key unwrapping properties of the `` diagonal cut '' are preserved .",
    "for example , the computational complexity per decoded bit does not change , since the degree distributions of the resulting codes are all equal .",
    "however , the ldpc convolutional codes based on a `` random cut '' typically require larger decoding processor sizes as a result of increased code constraint lengths .",
    "-regular ldpc block code using random partitions . ]",
    "[ ex : random : cuts : big : ldpc : block : codes:1 ]    we continue example  [ ex : diagonal : cuts : big : ldpc : block : codes:1 ] ; however , instead of performing `` diagonal cuts , '' we perform `` random cuts . ''",
    "figure [ fig : partition ] shows the performance of five such ldpc convolutional codes , each with rate @xmath76 and constraint length @xmath364 , compared to the underlying block code and the ldpc convolutional code constructed in example  [ ex : diagonal : cuts : big : ldpc : block : codes:1 ] ( with parameters @xmath365 , @xmath366 , @xmath367 , and @xmath356 ) .",
    "we note that the increase in constraint length from @xmath356 to @xmath364 due to the `` random cut '' results in a small additional coding gain in exchange for the larger decoding processor size .    finally , we note that , for a size @xmath368 sparse parity - check matrix @xmath369 with @xmath370 nonzero entries , there are a total of @xmath371 possible ways of choosing a random cut .",
    "however , due to the sparsity , there are only @xmath372 distinct random cuts , where @xmath373 .",
    "in this section we briefly discuss some other graph - cover - based ldpc code constructions proposed in the literature , namely by ivkovic _",
    "et al . _",
    "@xcite , by divsalar _",
    "et al . _",
    "@xcite , by lentmaier _ _ et al.__@xcite , and by kudekar _",
    "et al . _",
    "@xcite .",
    "the ldpc code construction by ivkovic _ _ et al.__in  @xcite can be seen as an application of the graph - cover construction in figures  [ fig : graph : cover : constructions:2](b ) and  [ fig : graph : cover : constructions:2](d ) in example  [ ex : graph : cover : construction:2 ] .",
    "namely , in terms of our notation , ivkovic _ et al . _",
    "@xcite start with a parity - check matrix @xmath239 , choose the set @xmath212 , a collection of zero - one matrices @xmath374 such that @xmath375 , and the collection of permutation matrices @xmath376 , \\bigl [ \\begin{smallmatrix } 0 & 1 \\\\ 1 & 0 \\end{smallmatrix } \\bigr ] \\bigr\\}$ ] .",
    "most importantly , the decomposition of @xmath239 into @xmath377 and @xmath378 is done such that trapping sets that were present in the tanner graph of @xmath239 are not present in the tanner graph of the new parity - check matrix .",
    "in addition , ivkovic _ et al . _ give guarantees on the relationship between the minimum hamming distances of the old and new code .",
    "one of the ldpc code constructions by divsalar _ _",
    "et al.__in  @xcite is the so - called rate-@xmath76 ar4ja ldpc code construction , which was also considered earlier in example  [ example : divsalar : codes : derivatives:1 ] . a particularly attractive , from an implementation perspective , version of this code construction is obtained by an iterated graph - cover construction procedure , where each graph - cover construction is based on a cyclic cover , as in the application of * gcc1 *  in example  [ ex : graph : cover : construction:1 ] .",
    "although cyclic covers result in simplified encoding and decoding circuitry , codes based on cyclic covers are known to have the disadvantage that the minimum hamming distance is upper bounded by a number that is a function of the proto - graph structure  @xcite . however , because the cyclic cover of a cyclic cover of the proto - graph is _ not necessarily _ a cyclic cover of the proto - graph , such disadvantages are avoided to a certain extent in the ar4ja ldpc code construction .",
    "nevertheless , ultimately the minimum hamming distance of such codes will also be upper bounded by some number ; however , these bounds usually become relevant only beyond the code length of interest .",
    "the ldpc code constructions by lentmaier _",
    "et al . _",
    "@xcite and kudekar _ et al . _",
    "@xcite can also be seen as iterated graph - cover constructions .",
    "we now describe a specific instance of this construction .",
    "* it starts with a proto - matrix @xmath379 $ ] . *",
    "the first graph - cover construction is very similar to the bi - infinite graph - cover construction in example  [ ex : graph : cover : construction:1:infinite:1 ] and figure  [ fig : graph : cover : constructions:1:variation:2 ] .",
    "namely , in terms of our notation , we define the set @xmath380 , the collection of matrices @xmath135 with @xmath381 $ ] and @xmath382 $ ] , and the collection of permutation matrices @xmath136 with @xmath383 , @xmath384 , @xmath385 , @xmath386 , @xmath387 , @xmath388 , where as before @xmath201 is a bi - infinite toeplitz matrix with zeros everywhere except for ones in the @xmath177-th diagonal below the main diagonal .",
    "* the second graph - cover construction is a random graph - cover construction of cover - degree @xmath117 . *",
    "the code is shortened .",
    "namely , for some positive integer @xmath389 all codeword indices corresponding to values outside the range @xmath390 $ ] are shortened.$ ] , we have opted to present the code construction such that the shortening is done after the two graph - cover construction steps . in this way",
    ", the structure of the code construction description matches better the description in definition  [ def : iterated : gcc : ldpc : code : construction:1 ] . ]",
    "we now point out some differences between this code construction and the ldpc convolutional code construction in definition  [ def : iterated : gcc : ldpc : code : construction:1 ] .",
    "namely , the ldpc code ensemble constructed above has the following properties .",
    "* the first graph - cover construction is based on bi - infinite toeplitz permutation matrices , and the second graph - cover construction is based on finite - size permutation matrices . * the analysis focuses on the case where @xmath117 and @xmath389 go to infinity ( in that order ) , i.e. , for a fixed @xmath389 the parameter @xmath117 tends to infinity .",
    "afterwards , @xmath389 tends to infinity . *",
    "the number of check nodes with degree smaller than @xmath222 in the tanner graph is proportional to @xmath117 . * in  @xcite ,",
    "for the binary erasure channel , when @xmath117 and @xmath389 go to infinity ( in that order ) , kudekar _ et al .",
    "_ prove that the sum - product algorithm decoding threshold for a slight variation of the above - mentioned ensemble of codes equals the maximum a - posteriori decoding threshold for the ensemble of @xmath220-regular ldpc codes .",
    "this is a very remarkable property ! ( in  @xcite , using density evolution methods , lentmaier _ et al . _",
    "give numerical evidence that this statement might also hold for binary - input output - symmetric channels beyond the binary erasure channel . )    on the other hand , the codes constructed in definition  [ def : iterated : gcc : ldpc : code : construction:1 ] have the following properties .",
    "( we assume that the underlying block code is a @xmath220-regular ldpc code . )",
    "* the first graph - cover construction is based on finite - size permutation matrices , and the second graph - cover construction is based on bi - infinite toeplitz permutation matrices .",
    "* in a typical application of this construction , @xmath8 is fixed . *",
    "the number of check nodes with degree smaller than @xmath222 in the tanner graph of the ldpc convolutional code is proportional to @xmath8 .",
    "* for a binary - input output - symmetric channel , the performance of the unterminated ldpc convolutional code under the continuous sliding window sum - product algorithm decoding discussed in section [ sec : implement ] improves with increasing @xmath8 ( see , e.g. , fig .",
    "[ fig : sims ] ) , but the ultimate asymptotic threshold of such unterminated decoding is unknown .",
    "the differences between these two code families come mainly from the fact that the codes constructed by lentmaier _",
    "et al . _ and kudekar _ et al . _ are essentially block codes , although sophisticated ones , whereas the codes in definition  [ def : iterated : gcc : ldpc : code : construction:1 ] are convolutional codes , along with their advantages and disadvantages . in particular ,",
    "the way the limits of the parameters are taken , there is a significant difference in the fraction of check nodes with degree strictly smaller than @xmath222 .",
    "namely , in the case of the codes by lentmaier _ et al . _ and kudekar _ et al . _",
    "this fraction is a fixed non - zero function of @xmath389 ( here we assume fixed @xmath389 and @xmath391 ) , whereas in the case of the codes considered in this paper , this fraction is zero ( here we assume fixed @xmath8 and an unterminated convolutional code ) .",
    "we conclude this section with the following remarks .",
    "namely , although the convolutional codes in definition  [ def : iterated : gcc : ldpc : code : construction:1 ] may not enjoy the same asymptotic thresholds as the block code constructions by lentmaier _ et al . _ and by kudekar _ et al . _ , they lend themselves to a continuous decoding architecture , as described in section [ sec : implement ] , which can be advantageous in certain applications , such as data streaming , without a predetermined frame structure .",
    "more importantly , however , it is very encouraging that the simulation results reported in this paper indicate that sizable `` convolutional gains '' are already visible for very reasonable constraint / code lengths . in the next section",
    "we discuss some possible reasons for these gains .",
    "finally , it is worth noting that , as the block lengths and associated constraint lengths of the constructions presented in this section become larger , the observed `` convolutional gains '' will become smaller since the block code results will approach their respective thresholds .",
    "this section collects some analytical results about ldpc convolutional codes . in particular",
    ", we compare the existence  / non - existence of cycles in ldpc block and ldpc convolutional codes , we present some properties of pseudo - codewords , and we discuss the  mostly moderate  cost increase in decoder complexity that is incurred by going from ldpc block to ldpc convolutional codes .      it is well known that cycles in the tanner graph representation of a sparse code affect message - passing iterative decoding algorithms , with short cycles generally pushing the performance further away from optimum .",
    "( indeed , attempts to investigate and minimize these effects have been made in @xcite and @xcite , where the authors propose ldpc code construction procedures to maximize the connectivity of short cycles to the rest of the graph , thus also maximizing the independence of the messages flowing through a cycle . )",
    "hence it is common practice to design codes that do not contain short cycles , so as to obtain independent messages in at least the initial iterations of the decoding process .    avoiding cycles in tanner graphs",
    "also has the benefit of avoiding pseudo - codewords . to see this ,",
    "let the active part of a pseudo - codeword be defined as the set of bit nodes corresponding to the support of the pseudo - codeword , along with the adjacent edges and check nodes . with this",
    ", it holds that the active part of any pseudo - codeword contains at least one cycle and/or at least one bit node of degree one . and so , given that the typical tanner graph under consideration in this paper does not contain bit nodes of degree one , the active part of a pseudo - codeword must contain at least one cycle .",
    "therefore , avoiding cycles implicitly means avoiding pseudo - codewords .",
    "let @xmath392 and @xmath239 be two parity - check matrices such that @xmath393 is a graph cover of @xmath246 .",
    "it is a well - known result that any cycle in @xmath393 can be mapped into a cycle in @xmath246 .",
    "this has several consequences .",
    "in particular , the girth of @xmath393 is at least as large as the girth of @xmath246 , and more generally , @xmath393 contains fewer short cycles than @xmath246 . for the codes constructed in this paper",
    ", this means that the unwrapping process ( from block code to convolutional code ) can `` break '' some cycles in the tanner graph of the block code .",
    "we now revisit some codes that where discussed in earlier sections and analyze their graph cycle structure using a brute - force search algorithm .",
    "defined in  ( * ? ? ?",
    "* eq .  ( 3.1 ) ) .",
    "note that this search technique works only for counting cycles of length smaller than twice the girth of the graph . for searching longer cycles ,",
    "more sophisticated algorithms are needed . ]",
    "note that , in order to accurately compare the graph cycle distributions of two codes with different block / constraint lengths , we compute the total number of cycles of a given cycle length per block / constraint length , and divide this number by the block / constraint length .",
    ".average ( per bit node ) number @xmath394 of cycles of length @xmath52 for the tanner graphs of the block codes ( bcs ) of block length @xmath11 and convolutional codes ( ccs ) of constraint length @xmath41 discussed in example  [ ex : cyc1 ] .",
    "( all tanner graphs have girth @xmath395 . ) [ cols=\"<,^,^,^,^ \" , ]     [ ex : cyc3 ]    table  [ table : girth : average : cycle : lengths : example:2 ] shows the cycle analysis results for the rate-@xmath76 proto - graph - based codes that were discussed in example  [ example : divsalar : codes : derivatives:1 ] and whose ber performance was plotted in figure  [ fig : jplsims ] .    from examples [ ex : cyc1 ] and",
    "[ ex : cyc3 ] , we see that many of the short cycles in the tanner graphs of the ldpc block codes are `` broken '' to yield cycles of larger length in the tanner graphs of the derived ldpc convolutional codes .",
    "this section collects some comments concerning the pseudo - codewords of the parity - check matrices under consideration in this paper .",
    "we start by observing that many of the statements that were made in  @xcite about pseudo - codewords can be extended to the setup of this paper . in particular , if some parity - check matrices @xmath392 and @xmath239 are such that @xmath393 is a graph cover of @xmath246 , then a pseudo - codeword of @xmath392 can be `` wrapped '' to obtain a pseudo - codeword of @xmath239 , as is formalized in the next lemma .",
    "let the parity - check matrices @xmath392 and @xmath239 be such that @xmath393 is an @xmath117-fold graph cover of @xmath246 .",
    "more precisely , let @xmath396 for some set @xmath127 , for some collection of parity - check matrices @xmath254 such that @xmath397 ( in @xmath1 ) , and for some collection of @xmath398 permutation matrices @xmath266 .",
    "moreover , let @xmath399 be the set of column indices of @xmath239 and let @xmath400 with @xmath401 be the set of column indices of @xmath392 . with this , if @xmath402 is a pseudo - codeword of @xmath392 , then @xmath403 with @xmath404 is a pseudo - codeword of @xmath239 .",
    "( sketch . )",
    "there are different ways to verify this statement .",
    "one approach is to show that , based on the fact that @xmath405 satisfies the inequalities that define the fundamental polytope of @xmath392  @xcite , @xmath406 satisfies the inequalities that define the fundamental polytope of @xmath239 .",
    "( we omit the details . )",
    "another approach is to use the fact that pseudo - codewords with rational entries are given by suitable projections of codewords in graph covers  @xcite .",
    "so , for every pseudo - codeword @xmath405 of @xmath392 with rational entries , there is some graph cover of @xmath393 with a codeword in it , which , when projected down to @xmath393 , gives @xmath405 .",
    "however , that graph cover of @xmath393 is also a graph cover of @xmath246 , and so this codeword , when projected down to @xmath246 , gives @xmath406 as defined in  .",
    "( we omit the details ; see  @xcite for a similar , but less general , result . )",
    "one can then proceed as in  @xcite and show that the awgnc , the bsc , and the bec pseudo - weights  @xcite of @xmath405 will be at least as large as the corresponding pseudo - weights of @xmath406 . as a corollary , the minimum awgnc , bsc , and bec pseudo - weights of @xmath392 are , respectively , at least as large as the corresponding minimum pseudo - weights of @xmath239 .",
    "similar results can also be obtained for the minimum hamming distance .    because the high - snr behavior of linear programming decoding is dominated by the minimum pseudo - weight of the relevant parity - check matrix ,",
    "the high - snr behavior of linear programming decoding of the code defined by @xmath392 is at least as good as the high - snr behavior of linear programming decoding of the code defined by @xmath239 .",
    "in general , because of the observations made in section  [ sec : cycleanalysis ] about the `` breaking '' of cycles and the fact that the active part of a pseudo - codeword must contain at least one cycle , it follows that the unwrapping process is beneficial for the pseudo - codeword properties of an unwrapped code , in the sense that many pseudo - codewords that exist in the base code do not map to pseudo - codewords in the unwrapped code .",
    "it is an intriguing challenge to better understand this process and its influence on the low - to - medium snr behavior of linear programming and message - passing iterative decoders , in particular , to arrive at a better analytical explanation of the significant gains that are visible in the simulation plots that were shown in section  [ sec : variations : unwrapping:1 ] . to this end",
    ", the results of  @xcite and  @xcite with respect to some related code families ( see the discussion in section  [ sec : other : ldpc : code : constructions:1 ] ) will be very helpful , since they indicate that some of the features of the fundamental polytope deserve further analysis .      in this subsection",
    ", we investigate the cost of the convolutional gain by comparing several aspects of decoders for ldpc block and convolutional codes .",
    "in particular , we consider the computational complexity , hardware complexity , decoder memory requirements , and decoding delay .",
    "more details on the various comparisons described in this section can be found in @xcite .",
    "ldpc block code decoders and ldpc convolutional code decoders have the same computational complexity per decoded bit and per iteration since ldpc convolutional codes derived from ldpc block codes have the same node degrees ( row and column weights ) in their tanner graph representations , which determines the number of computations required for message - passing decoding .",
    "we adopt the notion of _ processor size _ to characterize the hardware complexity of implementing the decoder .",
    "a decoder s processor size is proportional to the maximum number of variable nodes that can participate in a common check equation .",
    "this is the block length @xmath11 for a block code , since any two variable nodes in a block can participate in the same check equation . for a convolutional code",
    ", this is the constraint length @xmath41 , since no two variable nodes that are more than @xmath41 positions apart can participate in the same check equation .",
    "the constraint lengths of the ldpc convolutional codes derived from ldpc block codes of length @xmath11 satisfy @xmath407 .",
    "therefore , the convolutional codes have a processor size less than or equal to that of the underlying block code .    on the other hand ,",
    "the fully parallel pipeline decoding architecture penalizes ldpc convolutional codes in terms of decoder memory requirements ( and decoding delay / latency ) as a result of the @xmath57 iterations being multiplexed in space rather than in time .",
    "the pipeline decoder architecture of figure [ fig : decoder ] consists of @xmath57 identical processors of size @xmath41 performing @xmath57 decoding iterations simultaneously on independent sections of a decoding window containing @xmath57 constraint lengths of received symbols .",
    "this requires @xmath57 times more decoder memory elements than an ldpc block code decoder that employs a single processor of size @xmath408 performing @xmath57 decoding iterations successively on the same block of received symbols .",
    "therefore , the decoder memory requirements and the decoding delay of the pipeline decoder are proportional to @xmath409 , whereas the block decoder s memory and delay requirements are only proportional to @xmath11 .",
    "another way of comparing the two types of codes , preferred by some researchers , is to equate the block length of a block code to the memory / delay requirements , rather than the processor size , of a convolutional code , i.e. , to set @xmath410 . in this case the block code , now having a block length many times larger than the constraint length of the convolutional code , will typically ( depending on @xmath57 ) outperform the convolutional code , but at a cost of a much larger hardware processor .",
    "finally , as noted in section [ sec : ldpc : convolutional : codes:1 ] , the parallel pipeline decoding architecture for ldpc convolutional codes can be replaced by a serial looping decoding architecture , resulting in fewer processors but a reduced throughput along with the same memory and delay requirements .    in summary ,",
    "the convolutional gain achieved by ldpc convolutional codes derived from ldpc block codes comes at the expense of increased decoder memory requirements and decoding delays .",
    "although this does not cause problems for some applications that are not delay - sensitive ( e.g. , deep - space communication ) , for other applications that are delay - sensitive ( e.g. , real - time voice / video transmission ) , design specifications may be met by deriving ldpc convolutional codes from shorter ldpc block codes , thus sacrificing some coding gain , but reducing memory and delay requirements , or by employing a reduced window size decoder , as suggested in the recent paper by papaleo _",
    "et al . _",
    "@xcite , with a resulting reduction in the `` convolutional gain . ''",
    "in this paper we showed that it is possible to connect two known techniques for deriving ldpc convolutional codes from ldpc block codes , namely the techniques due to tanner and due to jimnez - feltstr \" om and zigangirov .",
    "this connection was explained with the help of graph covers , which were also used as a tool to present a general approach for constructing interesting classes of ldpc convolutional codes . because it is important to understand how the presented code construction methods can be used",
    " and in particular combined  we then discussed a variety of ldpc convolutional code constructions , along with their simulated performance results .    in the future",
    ", it will be worthwhile to extend the presented analytical results , in particular to obtain a better quantitative understanding of the low - to - medium snr behavior of ldpc convolutional codes . in that respect , the insights in the papers by lentmaier _",
    "et al . _",
    "@xcite and kudekar _ et al . _",
    "@xcite on the behavior of related code families will be valuable guidelines for further investigation .",
    "the authors would like to thank chris jones , michael lentmaier , david mitchell , michael tanner , and kamil zigangirov for their valuable discussions and comments . we also gratefully acknowledge the constructive comments made by the reviewers .      c.  berrou , a.  glavieux , and p.  thitimajshima , `` near shannon limit error correcting coding and decoding : turbo codes , '' in _ proc .",
    "ieee international conference on communications _ , geneva , switzerland , may 1993 , pp",
    ". 10641070 .",
    "s.  y. chung , t.  j. richardson , and r.  l. urbanke , `` analysis of sum - product decoding of low - density parity - check codes using a gaussian approximation , '' _ ieee trans .",
    "inf . theory _",
    "47 , no .  2 ,",
    "657670 , feb .",
    "2001 .",
    "s.  y. chung , g.  d. forney , jr .",
    ", t.  j. richardson , and r.  l. urbanke , `` on the design of low - density parity - check codes within 0.0045 db of the shannon limit , '' _ ieee communications letters _ , vol .  5 , no .  2 , pp .",
    "5860 , feb .",
    "2001 .",
    "s.  bates , d.  elliot , and r.  swamy , `` termination sequence generation circuits for low - density parity - check convolutional codes , '' _ ieee trans .",
    "circuits and systems i _ , vol .",
    "53 , no .  9 , pp . 19091917 , sep .",
    "s.  bates , z.  chen , and x.  dong , `` low - density parity check convolutional codes for ethernet networks , '' in _ proc .",
    "ieee pacific rim conference on communications , computers and signal processing _ , victoria ,",
    "bc , canada , aug . 2005 .",
    "s.  bates , l.  gunthorpe , a.  e. pusane , z.  chen , k.  sh .",
    "zigangirov , and d.  j. costello , jr .",
    ", `` decoders for low - density parity - check convolutional codes with large memory , '' in _ proc .",
    "12th nasa symposium on vlsi design _ , coeur dalene , i d , usa , oct . 2005 .",
    "r.  m. tanner , d.  sridhara , a.  sridharan , t.  e. fuja , and d.  j. costello , jr .",
    ", `` ldpc block and convolutional codes based on circulant matrices , '' _ ieee trans .",
    "inf . theory _ ,",
    "50 , no .  12 , pp .",
    "29662984 , dec . 2004 .",
    "r.  m. tanner , `` on quasi - cyclic repeat - accumulate codes , '' in _ proc .  of the 37th allerton conference on communications , control , and computing _ , allerton house , monticello , il , usa , sep .",
    "22 - 24 1999 , pp . 249259 .",
    "o. vontobel and r.  koetter , `` graph - cover decoding and finite - length analysis of message - passing iterative decoding of ldpc codes , '' _ corr , available online under _ ` http://www.arxiv.org/abs/cs.it/ ` _ _ ` 0512078 ` _ _ , dec .",
    "a.  e. pusane , k.  sh .",
    "zigangirov , and d.  j. costello , jr .",
    ", `` construction of irregular ldpc codes with fast encoding property , '' in _ proc .",
    "ieee intl .",
    "conference on commun .",
    "_ , istanbul , turkey , jun .",
    "1115 , 2006 .",
    "m.  papaleo , a.  r. iyengar , p.  h. siegel , j.  wolf , and g.  corazza , `` windowed erasure decoding of ldpc convolutional codes , '' in _ proc .",
    "ieee inform .",
    "theory workshop _ , cairo , egypt , jan .",
    "68 2010 , pp . 7882 .",
    "s.  bates , z.  chen , l.  gunthorpe , a.  e. pusane , k.  sh .",
    "zigangirov , and d.  j. costello , jr .",
    ", `` a low - cost serial decoder architecture for low - density parity - check convolutional codes , '' _ ieee trans .",
    "circuits and systems i _ , vol .",
    "55 , no .  7 , pp . 19671976 , aug .",
    "a.  e. pusane , a.  jimnez - feltstrm , a.  sridharan , m.  lentmaier , k.  sh .",
    "zigangirov , and d.  j. costello , jr .",
    ", `` implementation aspects of ldpc convolutional codes , '' _ ieee trans . commun .",
    "_ , vol .",
    "56 , no .  7 , pp . 10601069 , jul .",
    "y.  levy and d.  j. costello , jr .",
    ", `` an algebraic approach to constructing convolutional codes from quasi - cyclic codes , '' in _ coding and quantization _",
    "( piscataway , nj , 1992 ) , vol .",
    "14 of dimacs ser",
    ". discrete math .",
    "sci . , pp .",
    "189198 , providence , ri : amer .",
    "soc . , 1993 .",
    "r.  smarandache , a.  e. pusane , p.  o. vontobel , and d.  j. costello , jr .",
    ", `` pseudo - codeword performance analysis of ldpc convolutional codes , '' _ ieee trans .",
    "inf . theory _",
    "55 , no .  6 , pp . 25772598 , jun . 2009",
    ".      f.  hug , i.  bocharova , r.  johannesson , b.  kudryashov , and r.  satyukov , `` new low - density parity - check codes with large girth based on hypergraphs , '' in _ proc .",
    "ieee intl .",
    "symposium on inform .",
    "austin , tx , usa , jun .",
    "1318 2010 .",
    "a.  e. pusane , r.  smarandache , p.  o. vontobel , and d.  j. costello , jr .",
    ", `` on deriving good ldpc convolutional codes from qc ldpc block codes , '' in _ proc .",
    "ieee intl .",
    "symposium on inform .",
    "theory _ , nice , france , jun .",
    "2429 , 2007 , pp .",
    "12211225 .",
    "d.  j. costello , jr .",
    ", a.  e. pusane , s.  bates , and k.  sh .",
    "zigangirov , `` a comparison between ldpc block and convolutional codes , '' in _ proc .",
    "information theory and applications workshop _ , san diego , ca , usa , feb . 610 , 2006 .",
    "d.  j. costello , jr .",
    ", a.  e. pusane , c.  r. jones , and d.  divsalar , `` a comparison of ara- and protograph - based ldpc block and convolutional codes , '' in _ proc .",
    "information theory and applications workshop _ , san diego , ca , usa , jan .",
    "29feb .  2 , 2007 .",
    "m.  lentmaier , d.  v. truhachev , and k.  sh .",
    "zigangirov , `` on the theory of low - density convolutional codes ii , '' _ problems of information transmission ( problemy peredachy informatsii ) _ , vol .",
    "288306 , oct .- dec .",
    "d.  divsalar , c.  r. jones , s.  dolinar , and j.  thorpe , `` protograph based ldpc codes with minimum distance linearly growing with block size , '' in _ proc .",
    "ieee global telecommun .",
    "_ , vol .  3 , st",
    "louis , mo , usa , nov .  28  dec .  5 2005 .",
    "m.  ivkovic , s.  k. chilappagari , and b.  vasic , `` eliminating trapping sets in low - density parity - check codes by using tanner graph covers , '' _ ieee trans .",
    "inf . theory _ ,",
    "54 , no .  8 , pp . 37633768 , aug .",
    "2008 .",
    "m.  lentmaier , g.  p. fettweis , k.  sh .",
    "zigangirov , and d.  j. costello , jr .",
    ", `` approaching capacity with asymptotically regular ldpc codes , '' in _ proc .",
    "information theory and applications workshop _ , san diego , ca , usa , feb .",
    "813 2009 .",
    "m.  lentmaier , d.  g.  m. mitchell , g.  p. fettweis , and d.  j. costello , jr .",
    ", `` asymptotically regular ldpc codes with linear distance growth and thresholds close to capacity , '' in _ proc .",
    "information theory and applications workshop _ , san diego , ca , usa , jan .  31  feb .  5 2010 .",
    "r.  smarandache and p.  o. vontobel , `` quasi - cyclic ldpc codes : influence of proto- and tanner - graph structure on minimum hamming distance upper bounds , '' _ submitted to ieee trans .",
    "theory , available online under _ ` http://arxiv.org/abs/0901.4129`_ _ , jan .",
    "2009 .",
    "m.  lentmaier , a.  sridharan , d.  j. costello , jr . , and k.  s. zigangirov , `` iterative decoding threshold analysis for ldpc convolutional codes , '' _ ieee trans",
    ". inf . theory _",
    "56 , no .",
    "10 , pp . 52745289 , oct .",
    "2010 .",
    "g.  d. forney , jr .",
    ", r.  koetter , f.  kschischang , and a.  reznik , `` on the effective weights of pseudocodewords for codes defined on graphs with cycles , '' in _ codes , systems , and graphical models _ , b.  marcus and j.  rosenthal , eds.1em plus 0.5em minus 0.4emnew york , usa : springer verlag , 2001 , vol ."
  ],
  "abstract_text": [
    "<S> low - density parity - check ( ldpc ) convolutional codes are capable of achieving excellent performance with low encoding and decoding complexity . in this paper </S>",
    "<S> we discuss several graph - cover - based methods for deriving families of time - invariant and time - varying ldpc convolutional codes from ldpc block codes and show how earlier proposed ldpc convolutional code constructions can be presented within this framework .    some of the constructed convolutional codes significantly outperform the underlying ldpc block codes . </S>",
    "<S> we investigate some possible reasons for this `` convolutional gain , '' and we also discuss the  mostly moderate  decoder cost increase that is incurred by going from ldpc block to ldpc convolutional codes .    </S>",
    "<S> pusane , smarandache , vontobel , costello    block codes , convolutional codes , low - density parity - check ( ldpc ) codes , message - passing iterative decoding , pseudo - codewords , pseudo - weights , quasi - cyclic codes , unwrapping , wrapping . </S>"
  ]
}