{
  "article_text": [
    "deep neural networks have significantly improved the state of the art for a wide variety of machine - learning problems and applications .",
    "unfortunately , these impressive gains in performance come only when massive amounts of labeled data are available for supervised training .",
    "since manual labeling of sufficient training data for diverse application domains on - the - fly should be prohibitive , for problems short of labeled data , there is strong incentive to establishing effective algorithms to reduce the labeling consumption , typically by leveraging off - the - shelf labeled data from a different but related source domain that is big enough for training large - scale deep networks .",
    "however , this learning paradigm suffers from the shift in data distributions across different domains , which poses a major obstacle in adapting predictive models for the target task .",
    "a typical example is to apply an object recognition model trained on manually tagged images to test images under substantial variations in pose , occlusion , or illumination @xcite .    learning a discriminative model in the presence of a shift between training and test distributions",
    "is known as domain adaptation , a special case of transfer learning @xcite .",
    "a rich line of approaches to domain adaptation have been proposed in the context of both shallow learning and deep learning , which bridge the source and target domains by learning domain - invariant feature representations without using target labels , such that the classifier learned from the source domain can also be applied to the target domain . in particular , recent studies have shown that deep neural networks can learn more transferable features for domain adaptation @xcite , by disentangling explanatory factors of variations underlying data samples , and grouping deep features hierarchically in accordance with their relatedness to invariant factors .",
    "the latest advances have been achieved by embedding domain adaptation in the pipeline of deep feature learning to extract domain - invariant representations @xcite .",
    "the previous papers on deep domain adaptation worked under the assumption that the source classifier can be directly transferred to the target domain upon the learned domain - invariant feature representations .",
    "this assumption is rather strong as in practical applications , it is often infeasible to check whether the source classifier and target classifier can be shared or not .",
    "hence , we focus in this paper on a more general ( and safe ) domain adaptation scenario in which the source classifier and target classifier differ by a small perturbation function .",
    "the goal of this paper is to simultaneously learn adaptive classifiers and transferable features from labeled data in the source domain and unlabeled data in the target domain by embedding the adaptations of both classifiers and features in an end - to - end deep architecture .",
    "this work is primarily motivated by he et al .  , the winner of the imagenet ilsvrc 2015 challenge .",
    "we thus propose a novel residual transfer network ( rtn ) approach to domain adaptation in deep networks which can simultaneously learn adaptive classifiers and transferable features , both are complementary to each other and can be combined .",
    "we relax the shared - classifier assumption made by previous methods and assume that the source and target classifiers differ by a small residual function .",
    "thus we enable classifier adaptation by plugging several layers into the deep network to explicitly learn the residual function with reference to the target classifier . in this way",
    ", the source classifier and target classifier can be bridged tightly in the back - propagation process .",
    "the target classifier is made to better fit the target structures by exploiting the low - density separation criterion .",
    "we embed features of multiple layers into reproducing kernel hilbert spaces ( rkhss ) and match feature distributions for feature adaptation .",
    "we will show that the adaptation behaviors can be achieved in most feed - forward models by extending them with new residual layers and loss functions , which can be trained efficiently using standard back - propagation .",
    "extensive empirical evidence suggests that the proposed rtn approach outperforms state of art methods on standard domain adaptation benchmarks .",
    "the contributions of this paper are summarized as follows .",
    "( 1 ) we propose a new residual transfer network for domain adaptation , where both classifiers and features are adapted .",
    "( 2 ) we explore a deep residual learning framework for classifier adaptation , which does not require target labeled data .",
    "our approach is generic since it can be used to add domain adaptation to almost all existing feed - forward architectures .",
    "this work is related to domain adaptation , a special case of transfer learning @xcite , which builds models that can bridge different domains or tasks , explicitly taking the cross - domain discrepancy into account .",
    "transfer learning is to mitigate the burden of manual labeling for machine learning  @xcite , computer vision @xcite and natural language processing @xcite .",
    "it is a consensus that the domain discrepancy in probability distributions of different domains should be formally reduced , either by shallow learning or deep learning methods .",
    "deep neural networks can learn abstract representations that disentangle different explanatory factors of variations behind data samples @xcite and manifest invariant factors underlying different populations that transfer well from original tasks to similar novel tasks @xcite .",
    "thus deep neural networks have been explored for domain adaptation @xcite , multimodal and multi - task learning @xcite , where significant performance gains have been witnessed relative to prior shallow transfer learning methods .",
    "however , recent advances show that deep neural networks can learn abstract feature representations that can only reduce , but not remove , the cross - domain discrepancy @xcite .",
    "dataset shift has posed a bottleneck to the transferability of deep features , resulting in statistically unbounded risk for target tasks @xcite . some recent work addresses the aforementioned problem by deep domain adaptation , which bridges the two worlds of deep learning and domain adaptation @xcite .",
    "they extend deep convolutional networks ( cnns ) to domain adaptation either by adding one or multiple adaptation layers through which the mean embeddings of distributions are matched @xcite , or by adding a fully connected subnetwork as a domain discriminator whilst the deep features are learned to confuse the domain discriminator in a domain - adversarial training paradigm @xcite .",
    "while performance was significantly improved , these state of the art methods may be restricted by the assumption that under the learned domain - invariant feature representations , the source classifier can be directly transferred to the target domain .",
    "in particular , this assumption may not hold when the source classifier and target classifier can not be shared . as theoretically studied in @xcite , when the combined error of the ideal joint hypothesis is large , then there is no single classifier that performs well on both source and target domains , so we can not find a good target classifier by directly transferring from the source domain .",
    "this work is primarily motivated by he et al .  , the winner of the imagenet ilsvrc 2015 challenge .",
    "they present a residual learning framework to ease the training of very deep networks ( up to 152 layers ) , termed _",
    "residual nets_. the residual nets explicitly reformulate the layers as learning residual functions @xmath0 with reference to the layer inputs @xmath1 , instead of directly learning the unreferenced functions @xmath2 as in traditional plain nets .",
    "the method focuses on standard deep learning in which training data and test data are drawn from identical distributions , hence it can not be directly applied to domain adaptation . in this paper",
    ", we propose to bridge the source classifier @xmath3 and target classifier @xmath4 by the residual layers such that the cross - domain classifier mismatch can be explicitly modeled by the residual functions @xmath0 in an end - to - end deep learning architecture .",
    "note that the idea of adapting source classifier to target domain by adding a perturbation function has been studied by @xcite .",
    "however , these methods require target labeled data to learn the perturbation function , which can not be applied to unsupervised domain adaptation , the focus of this study .",
    "another crucial distinction is that their perturbation function is defined in the input space @xmath1 , while the input to our residual function is the target classifier @xmath4 , which can better capture the mismatch between the source and target classifiers .",
    "finally , their source classifiers should be pre - learned in a separated step , and the prediction phase will be inefficient using nonlinear kernel functions .",
    "in unsupervised domain adaptation , we are provided with a _ source _ domain @xmath5 of @xmath6 labeled examples and a _ target _",
    "domain @xmath7 of @xmath8 unlabeled examples .",
    "the source domain and target domain are sampled from probability distributions @xmath9 and @xmath10 respectively , and note @xmath11 .",
    "the goal of this paper is to craft a deep neural network that enables learning of transfer classifiers @xmath12 and @xmath13 to bridge the source - target discrepancy , such that the target risk @xmath14 $ ] is minimized by leveraging the source domain supervision .",
    "the challenge of unsupervised domain adaptation arises in that the target domain has no labeled data , while the source classifier @xmath15 trained on source domain @xmath16 can not be directly applied to the target domain @xmath17 due to the distribution discrepancy @xmath18 .",
    "the distribution discrepancy may give rise to mismatches in both features and classifiers , i.e. @xmath19 and @xmath20 .",
    "both mismatches should be fixed by the adaptation of features and classifiers to enable effective domain adaptation .",
    "classifier adaptation is more difficult than feature adaptation because the target domain is fully unlabeled .",
    "note that current state of the art deep feature adaptation methods @xcite generally assume shared classifier on their adaptive deep features .",
    "this paper assumes @xmath21 and presents an end - to - end deep learning architecture for joint adaptation of classifiers and features .",
    "we will extend from the breakthrough alexnet architecture @xcite that comprises five convolutional layers ( @xmath22@xmath23 ) and three fully connected layers ( @xmath24@xmath25 ) , where @xmath22@xmath26 are feature layers and @xmath25 is the classifier layer . specifically , each fully connected layer @xmath27 will learn a nonlinear mapping @xmath28 , where @xmath29 is the @xmath30-layer hidden representation of example @xmath31 , @xmath32 and @xmath33 are the @xmath30-layer weight and bias parameters , and @xmath34 is the @xmath30-layer activation function , taken as rectifier units ( relu ) @xmath35 for the feature layers or softmax units @xmath36 for classifier layer .",
    "denote by @xmath37 the set of network parameters ( @xmath38 layers in total ) . the empirical error @xmath39 of source - classifier @xmath15 on source data @xmath40 is @xmath41 where @xmath42 is the conditional probability that cnn assigns point @xmath43 to label @xmath44 , @xmath45 is the cross - entropy loss function @xmath46",
    ", @xmath47 is the number of classes , and @xmath48 is the softmax function defined on the @xmath49-layer activation @xmath50 that computes the probability of predicting point @xmath43 to class @xmath51 .",
    "based on the quantification study of feature transferability @xcite , the convolutional layers @xmath22@xmath23 can learn generic features transferable across domains @xcite . hence , when adapting the pre - trained alexnet model from the source domain to the target domain , we opt to fine - tune @xmath22@xmath23 such that the efficacy of feature co - adaptation can be preserved .",
    "as we perform residual transfer and distribution matching only for fully connected layers and pooling layers , we will not elaborate the computational details of convolutional layers .",
    "although the source classifier and target classifier are different , @xmath20 , they should be related to ensure the feasibility of domain adaptation .",
    "hence it is reasonable to assume that @xmath52 and @xmath53 differ only by a small perturbation function @xmath54 .",
    "previous works @xcite assume that @xmath55 , where the perturbation @xmath56 is a function of input feature @xmath1 . unfortunately , these methods require target labeled data to learn the perturbation function , which can not be applied to unsupervised domain adaptation , the focus of this study .",
    "we argue that the difficulty arises because no constraint is imposed to the perturbation function @xmath56 , therefore it is independent on the source labeled data and source classifier @xmath52 so that it must be learned from target labeled data .",
    "how to bridge @xmath52 and @xmath53 in a tight way such that the perturbation @xmath54 can be constrained and learned effectively is a critical challenge for unsupervised domain adaptation .",
    "we are motivated by the deep residual learning framework presented by he et al . to win the imagenet ilsvrc 2015 challenge .",
    "consider fitting @xmath57 as an original mapping by a few stacked layers ( convolutional layers or fully connected layers ) in figure  [ fig : reslayer ] , where @xmath1 denotes the inputs to the first of these layers .",
    "if one hypothesizes that multiple nonlinear layers can asymptotically approximate complicated functions , then it is equivalent to hypothesize that they can asymptotically approximate the residual functions , i.e. @xmath58 . thus rather than expect stacked layers to approximate @xmath57 , one explicitly lets these layers approximate a residual function @xmath59 , with the original function becoming @xmath60 . the operation @xmath60 is performed by a shortcut connection and an element - wise addition , while the residual function is parameterized as @xmath61 by residual layers within each building block .",
    "although both forms are able to asymptotically approximate the desired functions , the ease of learning is different .",
    "in reality , it is unlikely that identity mappings are optimal , but it should be easier to find the perturbations with reference to an identity mapping , than to learn the function as new . the residual learning is the key contributor to the successful training of very deep networks @xcite .     and target classifier @xmath4 by @xmath62 , @xmath63 , and @xmath64 . ]    as elaborated above , the deep residual learning framework bridges the inputs and outputs of the residual layers by the shortcut connection ( identify mapping ) such that @xmath2 , which eases the learning of residual function @xmath0 ( similar to the perturbation function across the source and target classifiers ) .",
    "based on this observation , we extend the cnn architecture ( figure  [ fig : rtn ] ) by plugging in the residual block ( figure  [ fig : reslayer ] ) .",
    "we formulate the residual block to bridge the source classifier @xmath3 and target classifier @xmath4 by @xmath62 , @xmath63 , and @xmath64 .",
    "note that @xmath3 is the outputs of the source - classifier layer @xmath65 and @xmath4 is the outputs of the target - classifier layer @xmath25 , both before softmax activation @xmath66 , hence @xmath67 .",
    "we can bridge the source and target classifiers ( before activation ) by the residual block as @xmath68 where we have used the unactivated classifiers @xmath69 and @xmath70 to ensure the final classifier @xmath15 and @xmath71 will output well - defined probabilities as the softmax classifier in deep neural networks .",
    "both residual layers @xmath72@xmath65 are fully connected with @xmath73 units , where @xmath47 is the number of classes .",
    "we set the source classifier @xmath69 as the outputs of the residual block to guarantee that it can be trained from the source - labeled data by the deep residual learning framework .",
    "in other words , if we set @xmath70 as the outputs of the residual block , then we will be unable to learn it successfully as we do not have target labeled data and standard back - propagation will not work .",
    "the deep residual learning @xcite ensures to output valid classifiers @xmath74 .",
    "note that , the residual learning framework will make the perturbation function @xmath75 dependent on both the target classifier @xmath4 ( the functional dependency ) as well as the source classifier @xmath3 ( thanks to back - propagation ) .",
    "although we cast the classifier adaptation into the residual learning framework , while the residual learning framework guarantees that the target classifier @xmath71 can not deviate much from the source classifier @xmath15 , we still can not guarantee that @xmath71 will fit the target - specific structures well . to address this problem",
    ", we further exploit the entropy minimization principle @xcite for classifier adaptation , which favors the low - density separation between classes by minimizing the conditional - entropy @xmath76 of the class probability @xmath77 on target data @xmath17 as @xmath78 where @xmath79 is the conditional probability that cnn assigns unlabeled - point @xmath80 to pseudo - label @xmath81 , and @xmath82 is the conditional - entropy loss function defined as @xmath83 , @xmath47 is the number of classes , and @xmath84 is the probability of predicting point @xmath80 to class @xmath51 . by minimizing the entropy penalty  ,",
    "the target classifier @xmath71 is made directly accessible to target - unlabeled data and amend itself to pass through the target low - density regions .",
    "as classifier adaptation does not necessarily undo the mismatch in the feature distributions , we further perform feature adaptation to make domain adaptation more effective .",
    "the literature has revealed that the deep features learned by cnns can disentangle the explanatory factors of variations underlying data distributions and facilitate knowledge transfer @xcite .",
    "but deep features can only reduce , but not remove , the cross - domain distribution discrepancy @xcite , which thus motivates the state of the art deep feature adaptation methods @xcite . as feature adaptation has been addressed quite well",
    ", we adopt the deep adaptation network ( dan ) @xcite to fine - tune cnn on labeled examples and require the feature distributions of the source and target to become similar under feature representations of fully connected layers @xmath85 , which is realized by minimizing a _ multi - layer _ mk - mmd penalty as @xmath86 where @xmath87 and @xmath88 are @xmath30-layer hidden representations of the source and target points respectively , @xmath89 is the mk - mmd between source and target evaluated using the @xmath30-layer hidden representations , and @xmath90 is the indices of layers where mk - mmd penalty is active . let @xmath91 be the reproducing kernel hilbert space ( rkhs ) with kernel @xmath92 . the multi - kernel maximum mean discrepancy ( mk - mmd ) between @xmath9 and @xmath10 is @xcite @xmath93 - { { \\mathbb{e}}_q}\\left [ { \\phi \\left ( { { { \\mathbf{x}}^t } } \\right ) } \\right ] } \\right\\|_{{\\mathcal{h}_k}}^2,\\ ] ] where @xmath94 is the nonlinear feature mapping .",
    "an important property is @xmath95 iff @xmath96 @xcite .",
    "characteristic kernel @xmath97 is defined as the convex combination of @xmath98 psd kernels @xmath99 , @xmath100 where the kernel coefficients @xmath101 can be simply set to @xmath102 or can be learned by multiple kernel learning @xcite .      based on the aforementioned analysis , to enable effective unsupervised domain adaptation",
    ", we propose the _ residual transfer network _ ( rtn ) to be an integration of deep feature learning , classifier adaptation  , and feature adaptation in an end - to - end deep learning framework as @xmath103 where @xmath104 and @xmath105 are the tradeoff parameters for the entropy penalty   and multi - layer mk - mmd penalty   respectively .",
    "the proposed rtn model is able to learn both adaptive classifiers and transferable features .",
    "as classifier adaptation proposed in this paper and feature adaptation studied in @xcite are tailored to adapt different layers of the deep networks , they are well complementing to each other to establish better performance and can be combined wherever necessary .    as training deep cnns requires a large amount of labeled data that is prohibitive for many domain adaptation applications , we start with the alexnet model pre - trained on imagenet 2012 data and fine - tune it as @xcite .",
    "the training of rtn mainly follows standard back - propagation , including the residual layers for classifier adaptation @xcite .",
    "but the optimization of mk - mmd penalty requires carefully - designed algorithm as detailed in @xcite for linear - time training with back - propagation .",
    "* discussion : * the proposed idea of residual transfer is quite general and can be readily extended in several scenarios . in heterogeneous domain adaptation where the feature representations are different across domains @xcite ,",
    "the residual transfer can be applied to the feature layers ( e.g. @xmath24@xmath26 ) of the deep network to bridge the source and target representations .",
    "( 2 ) in multi - task learning @xcite that solves multiple potentially correlated tasks together to improve performance of all these tasks by sharing statistic strength , the residual transfer can also be extended to bridge the gap between specific task and the shared task .",
    "we will leave these possible extensions to the future work .",
    "we evaluate the proposed residual transfer network against state of the art transfer learning and deep learning methods on unsupervised domain adaptation benchmarks .",
    "datasets , codes , and configurations will be made available publicly .      * office-31 * @xcite is a standard benchmark for domain adaptation , comprising 4,652 images distributed in 31 classes collected from three distinct domains : _ amazon _ ( * a * ) , which contains images downloaded from amazon.com , _ webcam _ ( * w * ) and _ dslr _ ( * d * ) , which contain images taken by web camera and digital slr camera in an office environment with different photographical settings , respectively .",
    "we evaluate all methods across three transfer tasks * a * @xmath106 * w * , * d * @xmath106 * w * and * w * @xmath106 * d * , which are commonly adopted by previous deep transfer learning methods @xcite , and across another three transfer tasks * a * @xmath106 * d * , * d * @xmath106 * a * and * w * @xmath106 * a * as presented in @xcite .",
    "* office - caltech * @xcite is built by selecting the 10 common categories shared by the _ office-31 _ and _ caltech-256 _ ( * c * ) @xcite datasets , and is widely used by previous methods @xcite .",
    "we consider all domain combinations of this dataset , and build 12 transfer tasks : * a * @xmath106 * w * , * d * @xmath106 * w * , * w * @xmath106 * d * , * a * @xmath106 * d * , * d * @xmath106 * a * , * w * @xmath106 * a * , * a * @xmath106 * c * , * w * @xmath106 * c * , * d * @xmath106 * c * , * c * @xmath106 * a * , * c * @xmath106 * w * , and * c * @xmath106 * d*. while _",
    "office-31 _ has more categories and is more difficult for domain adaptation algorithms , _ office - caltech _ provides more transfer tasks to enable an unbiased look at the dataset bias issue @xcite .",
    "we adopt decaf@xmath107 @xcite features for shallow transfer methods and original images for deep transfer methods .",
    "we compare with state of the art transfer and deep learning methods : transfer component analysis ( * tca * ) @xcite , geodesic flow kernel ( * gfk * ) @xcite , deep domain confusion ( * ddc * ) @xcite , deep adaptation network ( * dan * ) @xcite , and reverse gradient ( * revgrad * ) @xcite .",
    "tca is a conventional transfer learning method based on mmd - regularized kernel pca .",
    "gfk is a widely - adopted manifold learning method that interpolates across an infinite number of intermediate subspaces to bridge the source and target .",
    "ddc is the first method that maximizes domain invariance by adding to alexnet an adaptation layer using linear - kernel mmd .",
    "dan learns more transferable features by embedding deep features of multiple task - specific layers to reproducing kernel hilbert spaces ( rkhss ) and matching different distributions optimally by multi - kernel mmd .",
    "revgrad boosts domain adaptation by making source and target indistinguishable for a discriminative domain classifier through adversarial training @xcite . to go deeper with the efficacy of _ residual transfer _ with _ entropy minimization _",
    ", we evaluate several variants of rtn : ( 1 ) * rtn - res * , by removing the residual layers from rtn ; ( 2 ) * rtn - ent * , by removing the entropy penalty from rtn ; ( 3 ) * rtn - mmd * , by removing the mmd penalty from rtn .",
    "note that rtn - mmd is no longer built upon dan , and this experiment will suggest that residual transfer of classifiers and mmd adaptation of features are well complementary .",
    "we follow standard evaluation protocols for unsupervised domain adaptation @xcite .",
    "dataset , we adopt the sampling protocol @xcite that randomly samples the source domain with 20 labeled examples per category for amazon ( @xmath108 ) and 8 labeled examples per category for webcam ( * w * ) and dslr ( * d * ) . for the _ office - caltech _ dataset",
    ", we adopt the full - sampling protocol @xcite that uses all labeled source examples and all unlabeled target examples .",
    "all unlabeled examples in the target domain are used for learning transfer classifiers .",
    "we compare the average classification accuracy of each method on five random experiments , and report the standard error of the classification accuracies by different experiments of the same transfer task .    for all methods , we either follow the procedures for model selection explained in their respective papers , or conduct cross - validation",
    "on labeled source data if parameter selection strategies are not explained . for mmd - based methods ( tca , ddc , dan , and rtn )",
    ", we use gaussian kernel with bandwidth @xmath109 set to median pairwise squared distances on training data , i.e. median heuristic @xcite .",
    "we follow @xcite and use multi - kernel mmd for dan and rtn , and a family of @xmath98 gaussian kernels by varying bandwidth @xmath110 $ ] with a multiplicative step - size of @xmath111 .",
    "we conduct cross - validation on labeled source data to select parameters of rtn @xcite .",
    "we implement all deep methods based on the * caffe * @xcite deep - learning framework , and fine - tune from caffe - trained models of alexnet @xcite pre - trained on imagenet @xcite . for rtn , we fine - tune convolutional layers @xmath22@xmath23 and fully connected layers @xmath24@xmath26 , and train classifier layer @xmath25 and residual layers @xmath72@xmath65 , all through standard back - propagation .",
    "since the classifier and residual layers are trained from scratch , we set their learning rate to be 10 times that of the lower layers .",
    "we use mini - batch sgd with momentum of 0.9 and the learning rate annealing strategy implemented in caffe , and cross - validate base learning rate between @xmath112 and @xmath113 by a multiplicative step - size @xmath114 . to suppress noisy predictions ( due to random initialization ) that bias the entropy penalty at early stages of the training procedure , we set parameter @xmath115 within 1000 mini - batch iterations and set it to the cross - validated value afterwards .    cccccccc method & a @xmath106 w & d @xmath106 w & w @xmath106 d & a @xmath106 d & d @xmath106 a & w @xmath106 a & avg + tca @xcite & 59.0@xmath1160.4 & 90.2@xmath1160.2 & 88.2@xmath1160.4 & 57.8@xmath1160.4 & 51.6@xmath1160.5 & 47.9@xmath1160.5 & 65.8 + gfk @xcite & 58.4@xmath1160.6 & 93.6@xmath1160.4 & 91.0@xmath1160.5 & 58.6@xmath1160.6 & 52.4@xmath1160.6 & 46.1@xmath1160.5 & 66.7 + alexnet @xcite & 60.4@xmath1160.5 & 94.0@xmath1160.3 & 92.2@xmath1160.3 & 58.5@xmath1160.4 & 46.0@xmath1160.6 & 49.0@xmath1160.5 & 66.7 + ddc @xcite & 62.7@xmath1160.5 & 94.1@xmath1160.3 & 92.9@xmath1160.3 & 60.0@xmath1160.4 & 47.3@xmath1160.5 & 48.8@xmath1160.6 & 67.6 + revgrad @xcite & 67.3@xmath1160.5 & 93.7@xmath1160.3 & 94.0@xmath1160.3 & - & - & - & - + dan @xcite & 66.0@xmath1160.5 & 94.3@xmath1160.3 & 95.2@xmath1160.3 & 63.2@xmath1160.4 & 50.0@xmath1160.5 & 51.1@xmath1160.5 & 70.0 + rtn - res ( no residual ) & 68.9@xmath1160.5 & 95.3@xmath1160.3 & 95.5@xmath1160.2 & 65.7@xmath1160.4 & 52.1@xmath1160.6 & 52.7@xmath1160.6 & 71.7 + rtn - ent ( no entropy ) & 64.5@xmath1160.4 & 94.2@xmath1160.3 & 94.2@xmath1160.3 & 63.0@xmath1160.5 & 53.8@xmath1160.5 & 51.7@xmath1160.4 & 70.2 + rtn - mmd ( no mmd ) & 65.6@xmath1160.4 & 96.1@xmath1160.3 & * 96.0@xmath1160.3 * & 59.3@xmath1160.4 & 52.1@xmath1160.4 & 51.0@xmath1160.3 & 70.1 + rtn ( all ) & * 70.2@xmath1160.6 * & * 96.6@xmath1160.5 * & 95.5@xmath1160.4 & * 66.3@xmath1160.3 * & * 54.9@xmath1160.6 * & * 53.1@xmath1160.4 * & * 72.8 * +    cccccccccccccc method & a@xmath106w & d@xmath106w & w@xmath106d & a@xmath106d & d@xmath106a & w@xmath106a & a@xmath106c & w@xmath106c & d@xmath106c & c@xmath106a & c@xmath106w & c@xmath106d & avg + tca @xcite & 84.4 & 96.9 & 99.4 & 82.8 & 90.4 & 85.6 & 81.2 & 75.5 & 79.6 & 92.1 & 88.1 & 87.9 & 87.0 + gfk @xcite & 89.5 & 97.0 & 98.1 & 86.0 & 89.8 & 88.5 & 76.2 & 77.1 & 77.9 & 90.7 & 78.0 & 77.1 & 85.5 + alexnet @xcite & 83.1 & 97.6 & * 100.0 * & 88.3 & 89.0 & 83.1 & 84.0 & 77.9 & 81.0 & 91.3 & 83.2 & 89.1 & 87.3 + ddc @xcite & 86.1 & 98.2 & * 100.0 * & 89.0 & 89.5 & 84.9 & 85.0 & 78.0 & 81.1 & 91.9 & 85.4 & 88.8 & 88.2 + dan @xcite & 93.8 & * 99.0 * & * 100.0 * & 92.4 & 92.0 & 92.1 & 85.1 & 84.3 & 82.4 & 92.0 & 90.6 & 90.5 & 91.2 + rtn - res ( no residual ) & 96.1 & * 99.0 * & * 100.0 * & 92.8 & 94.3 & * 93.4 * & 88.0 & 87.3 & 82.4 & 93.5 & 96.3 & 91.4 & 92.9 + rtn - ent ( no entropy ) & 94.6 & * 99.0 * & * 100.0 * & 93.2 & 92.8 & 92.9 & 85.9 & 85.1 & 83.2 & 92.8 & 91.4 & 91.3 & 92.0 + rtn - mmd ( no mmd ) & 92.2 & 98.9 & * 100.0 * & 90.3 & 92.6 & 88.8 & 86.3 & 83.7 & 82.3 & 93.1 & 94.3 & 90.5 & 91.1 + rtn ( all ) & * 97.0 * & 98.8 & * 100.0 * & * 94.6 * & * 95.5 * & 93.1 & * 88.5 * & * 88.4 * & * 84.3 * & * 94.4 * & * 96.6 * & * 92.9 * & * 93.7 * +      the classification accuracy results of unsupervised domain adaptation on the six transfer tasks generated from _",
    "office-31 _ are shown in table [ table : office31 ] , and the results on the twelve transfer tasks generated from _",
    "office - caltech _ are shown in table [ table : office - caltech ] .",
    "note that the results of revgrad under the sampling protocol @xcite are directly reported from @xcite , as the original paper only provided results under the full - sampling protocol @xcite . the rtn model based on alexnet ( figure  [ fig : rtn ] )",
    "outperforms all comparison methods on most transfer tasks .",
    "in particular , rtn substantially improves the classification accuracy on hard transfer tasks , e.g. * a @xmath106 w * and * a @xmath106 d * , where the source and target are substantially different , and achieves comparable classification accuracy on easy transfer tasks , * d @xmath106 w * and * w @xmath106 d * , where source and target are similar @xcite .",
    "the encouraging results suggest that rtn is able to learn more adaptive classifiers as well as more transferable features for effective domain adaptation .    from the results",
    ", we can make insightful observations .",
    "( 1 ) standard deep - learning methods ( alexnet ) perform comparably with traditional _ shallow _ transfer - learning methods with deep decaf@xmath107 features as input ( tca and gfk ) .",
    "the only difference between these two sets of methods is that alexnet can take the advantage of supervised fine - tuning on the source - labeled data , while tca and gfk can take benefits of their domain adaptation procedures .",
    "this result confirms the current practice that supervised fine - tuning is important for transferring source classifier to target domain @xcite , and sustains the recent discovery that deep neural networks learn abstract feature representation , which can only reduce , but not remove , the cross - domain discrepancy @xcite .",
    "this reveals that the two worlds of deep learning and domain adaptation are not compatible with each other in the two - step pipeline , which motivates carefully - designed deep adaptation architectures to unify them .",
    "( 2 ) deep - transfer learning methods that reduce the domain discrepancy by domain - adaptive deep networks ( ddc , dan and revgrad ) substantially outperform standard deep learning methods ( alexnet ) and traditional shallow transfer - learning methods with deep features as the input ( tca and gfk ) .",
    "this confirms that incorporating domain - adaptation modules to deep networks can improve domain adaptation performance . by adapting source - target distributions in multiple task - specific layers using optimal multi - kernel two - sample matching",
    ", dan performs the best in general among the prior deep - transfer learning methods .",
    "( 3 ) the proposed residual transfer network ( rtn ) method performs the best and sets a new state of the art record on these datasets .",
    "different from all the previous deep - transfer learning methods that only adapt the feature layers of deep neural networks to learn more transferable features , rtn further adapts the classifier layers to bridge the source and target classifiers in an end - to - end residual learning framework , which can correct the classifier mismatch effectively .    to go deeper into the modules of rtn",
    ", we show the results of rtn variants : rtn - res ( no residual transfer ) , rtn - ent ( no entropy penalty ) , and rtn - mmd ( no mmd penalty ) .",
    "the results in tables  [ table : office31 ] and [ table : office - caltech ] well validate our motivation .",
    "( 1 ) rtn - res achieves much better results than revgrad and dan , but substantially underperforms rtn .",
    "this highlights the importance of the _ residual transfer _ of classifier layers for learning more adaptive classifiers .",
    "this is critical as in practical applications , there is no guarantee that the source classifier and target classifier can be safely shared .",
    "( 2 ) rtn - ent also achieves much better results than dan and revgrad , but substantially underperforms rtn .",
    "this highlights the importance of _ entropy minimization _ for low - density separation , which exploits the cluster structure of target - unlabeled data such that the target - classifier can be better adapted to the target data .",
    "otherwise , the residual function may tend to learn a useless zero mapping such that the source and target classifiers are nearly identical @xcite .",
    "( 3 ) rtn - mmd performs comparably with the best baseline dan , but substantially underperforms rtn .",
    "this evidence suggests the residual transfer of classifiers devised in this paper is as effective as the mmd adaptation of features @xcite .",
    "since these two methods are tailored to adapt different layers of the deep networks , they are well complementing each other to establish better performance as witnessed by the best performance of rtn .",
    "* visualization of predictions : * we illustrate the adaptivity of classifiers by visualizing in figures  [ fig : dan_s][fig : rtn_t ] the t - sne embeddings @xcite of the predictions made by the classifiers of dan and rtn for transfer task * a * @xmath106 * w * , respectively .",
    "we can make the following observations .",
    "( 1 ) the predictions made by dan in figure  [ fig : dan_s][fig : dan_t ] show that the target categories are not well discriminated by the source classifier , which implies that target data is not compatible with the source classifier .",
    "hence the source and target classifiers should not be assumed to be identical , which , whatsoever , has been a common assumption made by all prior deep domain adaptation methods @xcite . ( 2 ) the predictions made by rtn in figures  [ fig : rtn_s][fig : rtn_t ] show that the target categories are discriminated much better by the target classifier , which suggests that residual transfer of classifiers proposed in this paper is a reasonable extension to the previous deep domain adaptation methods .",
    "rtn learns more adaptive classifiers as well as more transferable features to enable effective deep domain adaptation .",
    "* analysis of layer responses : * we illustrate in figure  [ fig : response ] the average magnitudes and standard deviations of the layer responses @xcite , which are the outputs of @xmath4 ( @xmath25 layer ) , @xmath117 ( @xmath65 layer ) , and @xmath3 ( sum operator ) before other nonlinearity ( relu / addition / softmax ) , respectively . for the residual transfer network ,",
    "this analysis exposes the response strength of the residual functions .",
    "the results reveal that the residual functions @xmath117 have generally much smaller responses than the shortcut functions @xmath4 .",
    "these results support our original motivation that the residual functions might be generally closer to zero than the non - residual functions , since they characterize the small gap between the source classifier and target classifier .",
    "the small residual function can be learned more effectively through the residual learning framework @xcite .",
    "* parameter sensitivity : * besides the mmd penalty parameter @xmath105 as dan @xcite , the rtn model involves another entropy penalty parameter @xmath104 .",
    "we perform sensitivity analysis for it on transfer tasks * a * @xmath106 * w * ( 31 classes ) and * c * @xmath106 * w * ( 10 classes ) by varying the parameter of interest in @xmath118 .",
    "the results are shown in figures  [ fig : sensitivity ] , with the best results of the baseline shown as dashed lines .",
    "we observe that the accuracy of rtn first increases and then decreases as @xmath104 varies and demonstrates a desirable bell - shaped curve .",
    "this justifies our motivation of jointly learning deep features and adapting classifiers using residual layers and entropy penalty in deep nets , as a good trade - off between them can promote transfer performance .",
    "this paper presented a novel approach to unsupervised domain adaptation of deep networks , which enables simultaneous learning of adaptive classifiers and transferable features .",
    "similar to many prior domain adaptation techniques , the feature adaptation is achieved through matching the distributions of features across domains . however , unlike all previous techniques , the proposed approach also supports classifier adaptation , which is accomplished through a new residual transfer module to bridge the source classifier and target classifier .",
    "this new ingredient makes the approach a good complement to existing techniques .",
    "the approach can be trained by standard back - propagation , which is scalable and can be implemented by any deep learning package .",
    "we will constitute further evaluations on larger - scale tasks and semi - supervised domain adaptation settings as future work ."
  ],
  "abstract_text": [
    "<S> the recent success of deep neural networks relies on massive amounts of labeled data . for a target task </S>",
    "<S> where labeled data is unavailable , domain adaptation can transfer a learner from a different source domain . in this paper </S>",
    "<S> , we propose a new approach to domain adaptation in deep networks that can simultaneously learn adaptive classifiers and transferable features from labeled data in the source domain and unlabeled data in the target domain . </S>",
    "<S> we relax a shared - classifier assumption made by previous methods and assume that the source classifier and target classifier differ by a residual function . </S>",
    "<S> we enable classifier adaptation by plugging several layers into the deep network to explicitly learn the residual function with reference to the target classifier . </S>",
    "<S> we embed features of multiple layers into reproducing kernel hilbert spaces ( rkhss ) and match feature distributions for feature adaptation . </S>",
    "<S> the adaptation behaviors can be achieved in most feed - forward models by extending them with new residual layers and loss functions , which can be trained efficiently using standard back - propagation . </S>",
    "<S> empirical evidence exhibits that the approach outperforms state of art methods on standard domain adaptation datasets . </S>"
  ]
}