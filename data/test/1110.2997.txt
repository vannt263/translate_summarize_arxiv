{
  "article_text": [
    "bayesian methods of inference are widely used in astronomy and cosmology and are gaining popularity in other fields , such as particle physics .",
    "they can generally be divided into the performance of two main tasks : parameter estimation and model selection .",
    "the former has traditionally been performed using markov chain monte carlo ( mcmc ) methods , usually based on the metropolis - hastings algorithm or one of its variants",
    ". these can be computationally expensive in their exploration of the parameter space and often need to be finely tuned in order to produce accurate results .",
    "additionally , sampling efficiency can be seriously affected by multimodal distributions and large , curving degeneracies .",
    "the second task of model selection is further hampered by the need to calculate the bayesian evidence accurately .",
    "the most common method of doing so is thermodynamic integration , which requires several chains to be run , thus multiplying the computational expense .",
    "fast methods of evidence calculation , such as assuming a gaussian peak , clearly fail in multimodal and degenerate situations .",
    "nested sampling  @xcite is a method of monte carlo sampling designed for efficient calculation of the evidence which also provides samples from the posterior distribution as a by - product , thus allowing parameter estimation at no additional cost .",
    "the multinest algorithm  @xcite is a generic implementation of nested sampling , extended to handle multimodal and degenerate distributions , and is fully parallelised .    at each point in parameter space",
    ", bayesian methods require the evaluation of a ` likelihood ' function describing the probability of obtaining the data for a given set of model parameters .",
    "for some cosmological and particle physics problems each such function evaluation takes up to tens of seconds .",
    "mcmc applications may require millions of these evaluations , making them prohibitively costly .",
    "multinest is able to reduce the number of likelihood function calls by an order of magnitude or more , but further gains can be achieved if we are able to speed up the evaluation of the likelihood itself .",
    "an artificial neural network ( nn ) is ideally suited for this task .",
    "a universal approximation theorem assures us that we can accurately and precisely approximate the likelihood with a nn of a given form .",
    "the training of nns is one of the most widely studied problems in machine learning , so techniques for learning the likelihood function are well established .",
    "we implement a variant of conjugate gradient descent to find the optimum set of weights for a nn , using regularisation of the likelihood and a hessian - free second - order approximation to improve the quality of proposed steps towards the best fit .    the blind accelerated multimodal bayesian inference ( bambi ) algorithm combines these two elements .",
    "after a specified number of new samples from multinest have been obtained , bambi uses these to train a network on the likelihood function .",
    "after convergence to the optimal weights , we test that the network is able to predict likelihood values to within a specified tolerance level .",
    "if not , sampling continues using the original likelihood until enough new samples have been made for training to be done again . once a network is trained that is sufficiently accurate , its predictions are used in place of the original likelihood function for future samples for multinest . using the network reduces the likelihood evaluation time from seconds to milliseconds , allowing multinest to complete the analysis much more rapidly . as a bonus",
    ", the user also obtains a network that is trained to easily and quickly provide more likelihood evaluations near the peak if needed , or in subsequent analyses .",
    "the structure of the paper is as follows . in section",
    "[ sec : bayesnest ] we will introduce bayesian inference and the use of nested sampling .",
    "section  [ sec : nnoptimiser ] will then explain the structure of a nn and how our optimiser works to find the best set of weights .",
    "we present some toy examples with bambi in section  [ sec : toys ] to demonstrate its capabilities ; we apply bambi to cosmological parameter estimation in section  [ sec : cosmology ] . in section  [ sec : nnanalysis ] we show the full potential speed - up from bambi , by using the trained nns in a follow - up analysis .",
    "section  [ sec : conclusion ] summarises our work and presents our conclusions .",
    "bayesian statistical methods provide a consistent way of estimating the probability distribution of a set of parameters @xmath0 for a given model or hypothesis @xmath1 given a data set @xmath2 .",
    "bayes theorem states that @xmath3 where @xmath4 is the posterior probability distribution of the parameters and is written as @xmath5 , @xmath6 is the likelihood and is written as @xmath7 , @xmath8 is the prior distribution and is written as @xmath9 , and @xmath10 is the bayesian evidence and is written as @xmath11 .",
    "the evidence is the factor required to normalise the posterior over  @xmath0 , @xmath12 where @xmath13 is the dimensionality of the parameter space .",
    "since the bayesian evidence is independent of the parameter values , @xmath0 , it can be ignored in parameter estimation problems and the posterior inferences obtained by exploring the un  normalized posterior .",
    "bayesian parameter estimation has achieved widespread use in many astrophysical applications .",
    "standard monte carlo methods such as the metropolis ",
    "hastings algorithm or hamiltonian sampling  ( see * ? ? ?",
    "* ) can experience problems with multimodal likelihood distributions , as they can get stuck in a single mode . additionally , long and curving degeneracies are difficult for them to explore and",
    "can greatly reduce sampling efficiency .",
    "these methods often require careful tuning of proposal jump distributions and testing for convergence can be problematic . additionally , calculation of the evidence for model selection often requires running multiple chains , greatly increasing the computational cost .",
    "nested sampling and the multinest algorithm implementation address these problems .",
    "nested sampling  @xcite is a monte carlo method used for the computation of the evidence that can also provide posterior inferences .",
    "it transforms the multi - dimensional integral of equation  [ eq : evidence ] into a one - dimensional integral over the prior volume .",
    "this is done by defining the prior volume @xmath14 as @xmath15 .",
    "therefore , @xmath16 this integral extends over the region of parameter space contained within the likelihood contour @xmath17 .",
    "the evidence integral , equation  [ eq : evidence ] , can then be written as @xmath18 where @xmath19 is the inverse of equation  [ eq : xdef ] and is a monotonically decreasing function of @xmath14 .",
    "thus , if we evaluate the likelihoods @xmath20 , where @xmath21 is a sequence of decreasing values , @xmath22 the evidence can then be approximated numerically as a weighted sum @xmath23 where the weights @xmath24 for the simple trapezium rule are given by @xmath25 .",
    "an example of a posterior in two dimensions and its associated function @xmath19 is shown in figure  [ fig : ns ] .",
    "the fundamental operation of nested sampling begins with the initial , ` live ' , points being chosen at random from the entire prior volume .",
    "the lowest likelihood live point is removed and replaced by a new sample with higher likelihood .",
    "this removal and replacement of live points continues until a stopping condition is reached ( multinest uses a tolerance on the evidence calculation ) .",
    "the difficult task lies in finding a new sample with higher likelihood than the discarded point .",
    "as the algorithm goes up in likelihood , the prior volume that will satisfy this condition decreases until it contains only a very small portion of the total parameter space , making this sampling potentially very inefficient .",
    "multinest tackles this problem by enclosing all of the active points in clusters of ellipsoids .",
    "new points can then be chosen from within these ellipsoids using a fast analytic function .",
    "since the ellipsoids will decrease in size along with the distribution of live points , their surfaces in effect represent likelihood contours of increasing value ; the algorithm climbs up these contours seeking new points .",
    "as the clusters of ellipsoids are not constrained to fit any particular distribution , they can easily enclose curving degeneracies and are able to separate out to allow for multimodal distributions .",
    "this separation also allows for the calculation of the ` local ' evidence associated with each mode .",
    "multinest has been shown to be of substantial use in astrophysics and particle physics  ( see * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) , typically showing great improvement in efficiency over traditional mcmc techniques .",
    "artificial neural networks are a method of computation loosely based on the structure of a brain .",
    "they consist of a group of interconnected nodes , which process information that they receive and then pass this along to other nodes via weighted connections .",
    "we will consider only feed - forward nn , for which the structure is directed , with a layer of input nodes passing information to an output layer , via zero , one , or many hidden layers in between .",
    "a network is able ` learn ' a relationship between inputs and outputs given a set of training data and can then make predictions of the outputs for new input data . further introduction can be found in  @xcite .",
    "a multilayer perceptron artificial neural network ( nn ) is the simplest type of network and consists of ordered layers of perceptron nodes that pass scalar values from one layer to the next .",
    "the perceptron is the simplest kind of node , and maps an input vector @xmath26 to a scalar output @xmath27 via @xmath28 where @xmath29 and @xmath30 are the parameters of the perceptron , called the ` weights ' and ` bias ' , respectively .",
    "we will focus mainly on 3-layer nns , which consist of an input layer , a hidden layer , and an output layer as shown in figure  [ fig : neuralnet ] .",
    "the outputs of the nodes in the hidden and output layers are given by the following equations : @xmath31 where @xmath32 runs over input nodes , @xmath33 runs over hidden nodes , and @xmath34 runs over output nodes .",
    "the functions @xmath35 and @xmath36 are called activation functions and must be bounded , smooth , and monotonic for our purposes .",
    "we use @xmath37 and @xmath38 ; the non - linearity of @xmath35 is essential to allowing the network to model non - linear functions .",
    "the weights and biases are the values we wish to determine in our training ( described in section  [ sec : nntrain ] ) .",
    "as they vary , a huge range of non - linear mappings from inputs to outputs is possible .",
    "in fact , a ` universal approximation theorem '  ( see * ? ? ?",
    "* ) states that a nn with three or more layers can approximate any continuous function as long as the activation function is locally bounded , piecewise continuous , and not a polynomial ( hence our use of @xmath39 , although other functions would work just as well , such as a sigmoid ) . by increasing the number of hidden nodes , we can achieve more accuracy at the risk of overfitting to our training data .",
    "as long as the mapping from model parameters to predicted data is continuous  and it is in many cases  the likelihood function will also be continuous .",
    "this makes a nn an ideal tool for approximating the likelihood .",
    "an important choice when using a nn is the number of hidden layer nodes to use .",
    "we consider here just the case of three - layer networks with only one hidden layer .",
    "the optimal number is a complex relationship between of the number of training data points , the number of inputs and outputs , and the complexity of the function to be trained .",
    "choosing too few nodes will mean that the nn is unable to learn the likelihood function to sufficient accuracy .",
    "choosing too many will increase the risk of overfitting to the training data and will also slow down the training process . as a general rule , a nn should not need more hidden nodes than the number of training data points .",
    "we choose to use @xmath40 or @xmath41 nodes in the hidden layer in our examples .",
    "these choices allow the network to model the complexity of the likelihood surface and its functional dependency on the input parameters .",
    "the toy examples ( section  [ sec : toys ] ) have fewer inputs that result in more complicated surfaces , but with simple functional relationships .",
    "the cosmological examples ( section  [ sec : cosmology ] ) have more inputs with complex relationships to generate a simple likelihood surface . in practice",
    ", it will be better to over - estimate the number of hidden nodes required .",
    "there are checks built in to prevent over - fitting and for computationally expensive likelihoods the additional training time will not be a large penalty if a usable network can be obtained earlier in the analysis .",
    "we wish to use a nn to model the likelihood function given some model and associated data .",
    "the input nodes are the model parameters and the single output is the value of the likelihood function at that point .",
    "the set of training data , @xmath42 , is the last ` updint ` number of points accepted by multinest , a parameter that is set by the user .",
    "this data is split into two groups randomly ; approximately @xmath43 is used for training the network and @xmath44 is used as validation data to avoid overfitting . if training does not produce a sufficiently accurate network , multinest will obtain @xmath45 new samples before attempting to train again .",
    "additionally , the range of log - likelihood values must be within a user - specified range or nn training will be postponed .      the weights and biases we will collectively call the network parameter vector @xmath46 .",
    "we can now consider the probability that a given set of network parameters is able to reproduce the known training data outputs  representing how well our nn model of the original likelihood reproduces the true values .",
    "this gives us a log - likelihood function for @xmath46 , depending on a standard @xmath47 error function , given by @xmath48 ^ 2},\\end{aligned}\\ ] ] where @xmath49 is the number of data points and @xmath50 is the nn s predicted output value for the inputs @xmath51 and network parameters @xmath46 .",
    "the value of @xmath52 is a hyper - parameter of the model that describes the standard deviation ( error size ) of the output .",
    "our algorithm considers the parameters @xmath46 to be probabilistic with a log - prior distribution given by @xmath53 @xmath54 is a hyper - parameter of the model , called the ` regularisation constant ' , that gives the relative influence of the prior and the likelihood .",
    "the posterior probability of a set of nn parameters is thus @xmath55    bambi s network training begins by setting random values for the weights , sampled from a normal distribution with zero mean .",
    "the initial value of @xmath52 is set by the user and can be set on either the true log - likelihood values themselves or on their whitened values ( whitening involves performing a linear transform such that the training data values have a mean of zero and standard deviation of one ) .",
    "the only difference between these two settings is the magnitude of the error used .",
    "the algorithm then calculates a large initial estimate for @xmath54 , @xmath56 where @xmath57 is the total number of weights and biases ( nn parameters ) and @xmath58 is a rate set by the user ( @xmath59 , default @xmath60 ) that defines the size of the ` confidence region ' for the gradient .",
    "this formula for @xmath54 sets larger regularisation ( ` damping ' ) when the magnitude of the gradient of the likelihood is larger .",
    "this relates the amount of `` smoothing '' required to the steepness of the function being smoothed .",
    "the rate factor in the denominator allows us to increase the damping for smaller confidence regions on the value of the gradient .",
    "this results in smaller , more conservative steps that are more likely to result in an increase in the function value .",
    "bambi then uses conjugate gradients to calculate a step , @xmath61 , that should be taken ( see section  [ sec : nntrain_step ] ) .",
    "following a step , adjustments to @xmath54 and @xmath52 may be made before another step is calculated .",
    "the methods for calculating the initial @xmath54 value and then determining subsequent adjustments of @xmath54 and/or @xmath52 are as developed for the memsys software package , described in  @xcite .      in order to find the most efficient path to an optimal set of parameters , we perform conjugate gradients using second - order derivative information .",
    "newton s method gives the second - order approximation of a function , @xmath62 where @xmath63 is the hessian matrix of second derivatives of @xmath64 at @xmath46 . in this approximation",
    ", the maximum of @xmath64 will occur when @xmath65 solving this for @xmath61 gives us @xmath66 iterating this procedure will bring us eventually to the global maximum value of @xmath64 . for our purposes , the function @xmath64 is the log - posterior distribution and hence equation   is a gaussian approximation to the posterior .",
    "the hessian of the log - posterior is the regularised ( ` damped ' ) hessian of the log - likelihood function , where the prior  whose magnitude is set by @xmath54  provides the regularisation . if we define the hessian matrix of the log - likelihood as @xmath67 , then @xmath68 ( @xmath69 being the identity matrix ) .",
    "using the second - order information provided by the hessian allows for more efficient steps to be made , since curvature information can extend step sizes in directions where the gradient varies less and shorten where it is varying more . additionally , using the hessian of the log - posterior instead of the log - likelihood adds the regularisation of the prior , which can help to prevent getting stuck in a local maximum by smoothing out the function being explored .",
    "it also aids in reducing the ` region of confidence ' for the gradient information which will make it less likely that a step results in a worse set of parameters .",
    "given the form of the log - likelihood , equation  , is a sum of squares ( plus a constant ) , we can also save computational expense by utilising the gauss - newton approximation of its hessian , given by @xmath70 where @xmath71    the drawback of using second - order information is that calculation of the hessian is computationally expensive and requires large storage , especially so in many dimensions as we will encounter for more complex networks .",
    "in general , the hessian is not guaranteed to be positive semi - definite and so may not be invertible ; however , the gauss - netwon approximation does have this guarantee .",
    "inversion of the very large matrix will still be computationally expensive . as noted in  @xcite however , we only need products of the hessian with a vector to compute the solution , never actually the full hessian itself .",
    "to calculate these approximate hessian - vector products , we use a fast approximate method given in  @xcite and  @xcite .",
    "combining all of these methods makes second - order information practical to use .",
    "following each step , the posterior , likelihood , and correlation values are calculated for the training data and the validation data that was not used in training ( calculating the steps ) .",
    "convergence to a best - fit set of parameters is determined by maximising the posterior , likelihood , or correlation of the validation data , as chosen by the user .",
    "this prevents overfitting as it provides a check that the network is still valid on points not in the training set .",
    "we use the correlation as the default function to maximise as it does not include the model hyper - parameters in its calculation .",
    "the optimal network possible with a given set of training data may not be able to predict likelihood values accurately enough , so an additional criterion is placed on when to use the trained network .",
    "this requirement is that the standard deviation of the difference between predicted and true log - likelihood values is less than a user - specified tolerance .",
    "when the trained network does not pass this test , then bambi will continue using the original log - likelihood function to obtain @xmath45 new samples to generate a new training data set of the last ` updint ` accepted samples .",
    "network training will then resume , beginning with the weights that it had found as optimal for the previous data set .",
    "since samples are generated from nested contours and each new data set contains half of the previous one , the saved network will already be able to produce reasonable predictions on this new data ; resuming therefore enables us to save time as fewer steps will be required to reach the new optimum weights .",
    "once a nn is in use in place of the original log - likelihood function , checks are made to ensure that the network is maintaining its accuracy .",
    "if the network makes a prediction outside of @xmath72 $ ] , then that value is discarded and the original log - likelihood function is used for that point .",
    "additionally , the central @xmath73 percentile of the output log - likelihood values from the training data used is calculated and if the network is making predictions mostly outside of this range then it will be re - trained .",
    "to re - train the network , bambi first substitutes the original log - likelihood function back in and gathers the required number of new samples from multinest .",
    "training then commences , resuming from the previously saved network .",
    "these criteria ensure that the network is not trusted too much when making predictions beyond the limits of the data it was trained on , as we can not be sure that such predictions are accurate .",
    "in order to demonstrate the ability of bambi to learn and accurately explore multimodal and degenerate likelihood surfaces , we first tested the algorithm on a few toy examples .",
    "the eggbox likelihood has many separate peaks of equal likelihood , meaning that the network must be able to make predictions across many different areas of the prior .",
    "the gaussian shells likelihood presents the problem of making predictions in a very narrow and curving region .",
    "lastly , the rosenbrock function gives a long , curving degeneracy that can also be extended to higher dimensions .",
    "they all require high accuracy and precision in order to reproduce the posterior truthfully and each presents unique challenges to the nn in learning the likelihood .",
    "it is important to note that running bambi on these problems required more time than the straightforward analysis ; this was as expected since the actual likelihood functions are simple analytic functions that do not require much computational expense .",
    "this is a standard example of a very multimodal likelihood distribution in two dimensions .",
    "it has many peaks of equal value , so the network must be able to take samples from separated regions of the prior and make accurate predictions in all peaks .",
    "the eggbox likelihood  @xcite is given by @xmath74,\\ ] ] where we take a uniform prior @xmath75 for both @xmath76 and @xmath77 .",
    "the structure of the surface can be seen in figure  [ fig : eggboxlike ] .",
    "we ran the eggbox example in both multinest and bambi , both using @xmath78 live points . for bambi",
    ", we used @xmath78 samples for training a network with @xmath40 hidden nodes . in table",
    "[ tab : eggboxev ] we report the evidences recovered by both methods as well as the true value obtained analytically from equation  .",
    "both methods return evidences that agree with the analytically determined value to within the given error bounds .",
    "figure  [ fig : eggboxpost ] compares the posterior probability distributions returned by the two algorithms via the distribution of lowest - likelihood points removed at successive iterations by multinest .",
    "we can see that they are identical distributions ; therefore , we can say that the use of the nn did not reduce the quality of the results either for parameter estimation or model selection . during the bambi analysis @xmath79 of the log - likelihood function evaluations",
    "were done using the nn ; if this were a more computationally expensive function , significant speed gains would have been realised .",
    ".the log - evidence values of the eggbox likelihood as found analytically and with multinest and bambi .",
    "[ cols=\"^,^,^\",options=\"header \" , ]     one possible way to avoid the computational cost of computing error bars on the predictions is that suggested by  @xcite .",
    "one can take the nn training data and add gaussian noise and train a new nn , using the old weights as a starting point .",
    "performing many realisations of this will quickly provide multiple nns whose average prediction will be a good fit to the original data and whose variance from this mean will measure the error in the prediction .",
    "this will reduce the time needed to compute an error bar since multiple nn predictions are faster than a single inverse hessian - vector product .",
    "investigation of this technique will be explored in a future work .",
    "we have introduced and demonstrated a new algorithm for rapid bayesian data analysis . the blind",
    "accelerated multimodal bayesian inference algorithm combines the sampling efficiency of multinest with the predictive power of artificial neural networks to reduce significantly the running time for computationally expensive problems .",
    "the first applications we demonstrated are toy examples that demonstrate the ability of the nn to learn complicated likelihood surfaces and produce accurate evidences and posterior probability distributions .",
    "the eggbox , gaussian shells , and rosenbrock functions each present difficulties for monte carlo sampling as well as for the training of a nn . with the use of enough hidden - layer nodes and training points , we have demonstrated that a nn can learn to accurately predict log - likelihood function values .    we then apply bambi to the problem of cosmological parameter estimation and model selection . we performed this using flat and non - flat cosmological models and incorporating only cmb data and using a more extensive data set . in all cases , the nn is able to learn the likelihood function to sufficient accuracy after training on early nested samples and then predict values thereafter . by calculating a significant fraction of the likelihood values with the nn instead of the full function , we are able to reduce the running time by a factor of @xmath80 to @xmath81",
    "this is in comparison to use of multinest only , which already provides significant speed - ups in comparison to traditional mcmc methods  ( see * ? ? ?",
    "through all of these examples we have shown the capability of bambi to increase the speed at which bayesian inference can be done .",
    "this is a fully general method and one need only change the settings for multinest and the network training in order to apply it to different likelihood functions . for computationally expensive likelihood functions ,",
    "the network training takes less time than is required to sample enough training points and sampling a point using the network is extremely rapid as it is a simple analytic function .",
    "therefore , the main computational expense of bambi is calculating training points while the sampling evolves until the network is able to reproduce the likelihood accurately enough . with the trained nn",
    ", we can now perform additional analyses using the same likelihood function but different priors and save large amounts of time in sampling points with the original likelihood and in training a nn .",
    "follow - up analyses using already trained nns provides much larger speed increases , with factors of @xmath82 to @xmath40 obtained for cosmological parameter estimation relative to bambi speeds .",
    "the limiting factor in these runs is the calculation of the error of predictions , which is a flat cost based on the size of the nn and data set , regardless of the original likelihood function .",
    "the nns trained by bambi for cosmology cover a larger range of log - likelihoods than the one trained for cosmonet .",
    "this allows us to use a wider range of priors for subsequent analysis and not be limited to the four - sigma region around the maximum likelihood point . by setting the tolerance for bambi s nns to a larger value , fewer nns with larger likelihood ranges can be trained , albeit with larger errors on the predictions .",
    "allowing for larger priors requires us to test the validity of our nns approximations , which ends up slowing the overall analysis .",
    "since bambi uses a nn to calculate the likelihood at later times in the analysis where we typically also suffer from lower sampling efficiency ( harder to find a new point with higher likelihood than most recent point removed ) , we are more easily able to implement hamiltonian monte carlo  @xcite for finding a proposed sample .",
    "this method uses gradient information to make better proposals for the next point that should be ` sampled ' .",
    "calculating the gradient is usually a difficult task , but with the nn approximation they are very fast and simple .",
    "this improvement will be investigated in future work .",
    "as larger data sets and more complicated models are used in cosmology , particle physics , and other fields , the computational cost of bayesian inference will increase .",
    "the bambi algorithm can , without any pre - processing , significantly reduce the required running time for these inference problems .",
    "in addition to providing accurate posterior probability distributions and evidence calculations , the user also obtains a nn trained to produce likelihood values near the peak(s ) of the distribution that can be used in even more rapid follow - up analysis .",
    "pg is funded by the gates cambridge trust and the cambridge philosophical society ; ff is supported by a trinity hall college research fellowship . the authors would like to thank michael bridges for useful discussions , jonathan zwart for inspiration in naming the algorithm , and natallia karpenko for helping to edit the paper .",
    "the colour scheme for figures  [ fig : eggboxlike ] , [ fig : gausslike ] , and  [ fig : rosenbrocklike ] was adapted to matlab from  @xcite .",
    "this work was performed on cosmos viii , an sgi altix uv1000 supercomputer , funded by sgi / intel , hefce and pparc and the authors thank andrey kaliazin for assistance .",
    "the work also utilised the darwin supercomputer of the university of cambridge high performance computing service ( ` http://www.hpc.cam.ac.uk/ ` ) , provided by dell inc . using strategic research infrastructure funding from the higher education funding council for england and",
    "the authors would like to thank dr .",
    "stuart rankin for computational assistance .",
    "1 auld t , bridges m , hobson m p & gull s f , 2007 , mnras , 376 , l11 , arxiv : astro - ph/0608174 auld t , bridges m & hobson m p , 2008 , mnras , 387 , 1575 , arxiv : astro - ph/0703445 betancourt m , 2011 , in mohammad - djafari a. , bercher j .- f . , bessire p. , eds , aip conf .",
    "proc . vol .",
    "1305 , nested sampling with constrained hamiltonian monte carlo .",
    "phys . , new york , p. 165 , arxiv:1005.0157 [ gr - qc ] bouland a , easther r & rosenfeld k , 2011 , j. cosmol .",
    ", 5 , 016 , arxiv:1012.5299 [ astro-ph.co ] fendt w a & wandelt b d , 2006 , apj , 654 , 2 , arxiv : astro - ph/0606709 feroz f & hobson m p , 2008 , mnras , 384 , 449 , arxiv:0704.3704 [ astro - ph ] feroz f , allanach b c , hobson m p , abdus salam s s , trotta r & weber a m , 2008a , j. high energy phys .",
    ", 10 , 64 , arxiv:0807.4512 [ hep - ph ] feroz f , marshall p j & hobson m p , 2008b , pre - print arxiv:0810.0781 [ astro - ph ] feroz f , hobson m p , & bridges m , 2009a , mnras , 398 , 1601 , arxiv:0809.3437 [ astro - ph ] feroz f , hobson m p , zwart j t l , saunders r d e & grainge k j b , 2009b , mnras , 398 , 2049 , arxiv:0811.1199 [ astro - ph ] feroz f , gair j , hobson m p & porter e k , 2009c , cqg , 26 , 215003 , arxiv:0904.1544 [ gr - qc ] feroz f , gair j , graff p , hobson m p & lasenby a , 2010 , classical quantum gravity , 27 , 075010 , arxiv:0911.0288 [ gr - qc ] gair j , feroz f , babak s , graff p , hobson m p , petiteau a & porter e k , 2010 , j. phys . :",
    "ser . , 228 , 012010 green d a , 2011 , bull .",
    "india , 39 , 289 , arxiv:1108.5083 [ astro-ph.im ] gull s f & skilling j , 1999 , quantified maximum entropy : memsys 5 users manual .",
    "maximum entropy data consultants ltd . bury st .",
    "edmunds , suffolk , uk . ` http://www.maxent.co.uk/ ` hornik k , stinchcombe m & white h , 1990 , neural networks , 3 , 359 larson d et al . , 2011 , apjs , 192 , 16 , arxiv:1001.4635 [ astro-ph.co ] lewis a & bridle s , 2002 , phys .",
    "d , 66 , 103511 , arxiv : astro - ph/0205436 . ` http://cosmologist.info/cosmomc/ ` lewis a , challinor a & lasenby a , 2000 , apj , 538 , 473 , arxiv : astro - ph/9911177 mackay d j c , 1995 , network : comput .",
    "neural syst . , 6 , 469 mackay d j c , 2003 ,",
    "information theory , inference , and learning algorithms .",
    "cambridge univ . press . ` www.inference.phy.cam.ac.uk/mackay/itila/ ` martens j , 2010 , in frnkranz j. , joachims t. , eds , proc .",
    "machine learning .",
    "omnipress , haifa , p. 735",
    "pearlmutter b a , 1994 , neural comput . , 6 ,",
    "147 schraudolph n n , 2002 , neural comput .",
    ", 14 , 1723 skilling j , 2004 , in fisher r. , preuss r. , von toussaint u. , eds , aip conf .",
    "735 , nested sampling .",
    "new york , p. 395 trotta r , feroz f , hobson m p , roszkowski l & ruiz de austri r , 2008 , j. high eenergy phys .",
    ", 12 , 24 , arxiv:0809.3792 [ hep - ph ]"
  ],
  "abstract_text": [
    "<S> in this paper , we present an algorithm for rapid bayesian analysis that combines the benefits of nested sampling and artificial neural networks . the blind accelerated multimodal bayesian inference ( bambi ) </S>",
    "<S> algorithm implements the multinest package for nested sampling as well as the training of an artificial neural network ( nn ) to learn the likelihood function . in the case of computationally expensive likelihoods </S>",
    "<S> , this allows the substitution of a much more rapid approximation in order to increase significantly the speed of the analysis . </S>",
    "<S> we begin by demonstrating , with a few toy examples , the ability of a nn to learn complicated likelihood surfaces . </S>",
    "<S> bambi s ability to decrease running time for bayesian inference is then demonstrated in the context of estimating cosmological parameters from _ wilkinson microwave anisotropy probe _ and other observations . </S>",
    "<S> we show that valuable speed increases are achieved in addition to obtaining nns trained on the likelihood functions for the different model and data combinations . </S>",
    "<S> these nns can then be used for an even faster follow - up analysis using the same likelihood and different priors . </S>",
    "<S> this is a fully general algorithm that can be applied , without any pre - processing , to other problems with computationally expensive likelihood functions .    </S>",
    "<S> [ firstpage ]    methods : data analysis  methods : statistical  cosmological parameters </S>"
  ]
}