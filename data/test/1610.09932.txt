{
  "article_text": [
    "machine learning methods are used widely within high energy physics ( hep ) . one promising approach , used extensively outside of hep for applications such as handwriting recognition , is that of support vector machines ( svms ) , a supervised learning model used with associated learning algorithms for multivariate analysis ( mva ) .",
    "developed originally in the 1960s , with the current standard version proposed in 1995  @xcite , svms aim to classify data points using a maximal margin hyperplane mapped from a linear classification problem to a possibly infinite dimensional hyperspace .",
    "however this means svms , like other mva classifiers , have a number of free parameters which need to be tuned on a case by case basis .",
    "this motivates a number methods for ensuring the classifier is sufficiently generalised such that when used on an unseen dataset the performance can be accurately predicted . in this paper a brief overview of svms is given in section  [ sec : svm ] , with an example using svms shown in section  [ sec : checker ] .",
    "generalisation is discussed in section  [ sec : gen ] with an illustrative example of how this can improve performance given in section  [ sec : check2 ] .",
    "consider the problem of linear classification with the svm where the training set , @xmath0 , is linearly separable .",
    "we define a separating hyperplane given by @xmath1 , where @xmath2 , the weight vector , is perpendicular to the hyperplane , and @xmath3 , the bias , determines the distance of the hyperplane from the origin ( fig .  ) . a decision function defined by @xmath4",
    "is used to positively and negatively classify @xmath5 , the points in the training set . without further constraint",
    "the choice of separating hyperplane is arbitrary and the geometric margin , @xmath6 , is introduced with the aim to maximise this .",
    "the functional margin , @xmath7 , for @xmath5 is defined as @xmath8 to impose the condition that no points lie within the margins .",
    "considering two points that lie closest to the hyperplane referred to as support vectors ( svs ) , @xmath9 and @xmath10 , with functional margins of @xmath11 and @xmath12 respectively , the geometric margin @xmath6 can be defined as @xmath13 .",
    "the problem is solved in dual space by minimising the lagrangian @xmath14 for the parameters @xmath15 ( lagrange multipliers ) subject to @xmath16 , where @xmath17 is only non - zero for svs , and @xmath18 , providing the constraint equation .",
    "the full derivation leading to this minimisation and further details can be found in @xcite .",
    "the hard margin svm is an important svm algorithm , however it is not applicable in most real world cases .",
    "issues arise if the data are not linearly separable due to being noisy , with data having small variations between different independent samples .",
    "this leads to the training becoming susceptible to over - fitting due to a few points in the training set , so some classification error must be allowed . to achieve this",
    "the hard margin constraint is relaxed and points are allowed on the incorrect side of the decision boundary , with the misclassification described by two additional parameters , @xmath19 ( slack ) and @xmath20 ( cost ) , where @xmath21 defines the distance from the correct margin to the @xmath22th incorrectly classified sv , and @xmath20 is a tunable weight parameter multiplying the sum of slack variables penalising the misclassified points ( fig .  ) .",
    "this reduces to the same dual space lagrangian problem as for the hard margin svm , eq .",
    "( [ eq : hardmarginlagrangian ] ) , however the constraint on the lagrange multipliers is now @xmath23 , while the constraint equation remains , @xmath18 .      to this point",
    "the discussion of the svm algorithm has been considered a linear decision boundary , however in most cases this is sub - optimal . in order to overcome this",
    "we can replace the inner product found in eqns  ( [ eq : functionalmargin ] ) and  ( [ eq : hardmarginlagrangian ] ) with a kernel function ( kf ) @xmath24 that maps the problem from the input space @xmath25 to a potentially higher dimensionality , implicit feature space @xmath26 , where the data may then be separable . in this process neither the feature map , @xmath27 , or the feature space , @xmath28 , need to be known explicitly ( known as the  kernel trick \"  @xcite ) .",
    "this leads to specific properties required to define proper kernel functions in order for them to be used in this way .",
    "the kernel function should satisfy @xmath29 and @xmath30 .",
    "these are not strong enough conditions to guarantee the existence of f and so a kf is also required to meet mercer s condition  @xcite . + an example of a kernel meeting these requirements is the polynomial kf of eq .",
    "( [ eq : polynomial ] ) with two tunable parameters , the order @xmath31 and offset @xmath32 .",
    "another common choice of kf is the radial basis function ( rbf ) , eq .",
    "( [ eq : rbf ] ) , where the distance between two svs is computed and used as the input to a gaussian with one tunable parameter @xmath33 .",
    "the rbf does not take into account potential bandwidth differences between dimensions in the input data which may cause tension and the norm between svs may result in loss of information .",
    "it can be extended to include a tunable parameter , @xmath34 for each dimension in the input space referred to as the multi - gaussian kf , eq .",
    "( [ eq : multigauss ] ) . the kfs implemented in tmva ( along with products and sums thereof )",
    "are :    @xmath35      the higgs machine learning challenge dataset  @xcite , with 6500 signal and 10000 background points is used to train svms with the kfs of eqns  ( [ eq : polynomial]-[eq : multigauss ] ) , as well as a boosted decision tree ( bdt )  @xcite .",
    "the six variables offering the most discriminating power were used ; @xmath37 , @xmath38 , @xmath39 , @xmath40 , @xmath41 centrality and @xmath42 , see  @xcite for definition .",
    "the classifiers are optimised , trained and tested with tmva  @xcite using the hold - out validation method , discussed in sec .",
    "[ sec : holdout ] . considering the receiver operating characteristic ( roc ) curves , signal efficiency versus background rejection shown in fig .",
    ", the classifiers all have similar performance for this problem .",
    "however it is not clear whether these solutions are fine tuned or optimal and so a further step is required to confirm if the results are generalised .",
    "due to the often non - transparent nature of multivariate techniques one requires confidence that the trained mva is robust against over - fitting , such that when used on an unseen data sample the performance of the mva is accurately predicted and therefore the mva is generalised . in order to ensure this ,",
    "validation techniques are required for both model selection , as most mvas have at least one free hyper - parameter ( hp ) , and for performance estimation , measured with a figure of merit such as the misclassification error . for an unlimited dataset these issues are trivial , as one could simply iterate through the hp space and models using a different subset of the data each time , evaluating the performance and choosing the model and configuration with the lowest error rate . in reality datasets",
    "are often much smaller than required .",
    "navely the entire dataset could be used to select and train the classifier and to estimate the error , however this leads to over - fitting or over - training as the classifier learns specific fluctuations in the dataset and subsequently performs worse on unseen data .",
    "this is more distinct in classifiers with a large number of tunable hps .",
    "this also gives an overly optimistic estimation of the error .",
    "a natural extension and potential way to overcome these issues is to split the dataset into two subsamples , one for training and one for testing .",
    "this is referred to as hold - out validation  @xcite .",
    "the training sample is used to select optimal hps , using a method such as back propagation for the multilayer perceptron  @xcite , and train the classifier .",
    "the testing sample is used to evaluate the performance .",
    "this method is not ideal as the performance differs depending on how the data is split , which may lead to misleading error estimates .      in circumstances where a relatively small dataset is available",
    "it may not be possible to reserve a large portion of data for testing and so the hold - out method may not be viable .",
    "instead the approach of @xmath43-fold cross - validation ( cv )  @xcite can be used , performed as follows :    1 .",
    "split the data into @xmath43 equally sized , randomly sampled , independent datasets ( or folds ) ; 2 .",
    "train the classifier with the data from @xmath44 of the folds ; 3 .",
    "test the classifier using the remaining fold ; 4",
    ".   repeat steps ( ii ) and ( iii ) @xmath43 times for all permutations .    the overall performance for the classifier",
    "is then given by the average error rate , @xmath45 , where @xmath46 is the misclassification rate for each training .",
    "this method has the advantage of using the entire dataset for testing and training , offering potentially improved performance over the previous methods .",
    "however it raises the question of the number of folds to be used",
    ". a large number of folds will give good estimation of the average error rate , as the bias of the estimator is small , but the variance is large and the computational time is large , whilst the reverse is true for a small number of folds .",
    "in reality the choice of number of folds should be motivated by the size of the dataset available for the problem , for example a sparse dataset may require the extreme of leave-@xmath47-out cv  @xcite in order to train on as much data as possible .",
    "+   + building on the @xmath43-fold method , ideally the data should be split into three statistically independent , randomly sampled datasets for training , testing and validation , with each set split into @xmath43-folds . the purpose of the training set is to choose the model and to optimise the hps for this model through iterative @xmath43-fold cv , with one set of hps for each fold .",
    "these hps are then tested via @xmath43-fold cv with the validation set to obtain their estimated performance .",
    "the training and validation samples are then combined and used to train the final classifier with the best performing hps . the test sample , which has been reserved until this point , is then used to assess the performance of this final fully trained classifier .",
    "+   + taking the best performing classifier from the optimisation process may not be the optimal solution or give the desired output , due to potential pathologies in the distributions .",
    "this also involves discarding a large number of trained classifiers from the process , so instead of using all hps sets from the optimisation step , @xmath43 classifiers can be trained and their performance estimated .",
    "these can then be used to give an averaged output on the testing sample which can help reduce fluctuations in the final distributions , improving agreement between the performance on the training and testing samples .      using the higgs machine learning challenge dataset , discussed in sec .",
    "[ sec : checker ] , svms with rbf kfs were optimised , trained and tested using @xmath48-fold cv to obtain the best performing and averaged svms , as well as using the hold - out validation method for comparison .",
    "the roc curve comparing the different methods , fig .  , shows improvement for the @xmath43-fold cv classifiers over the hold - out method , both in the performance and shape of the curves .",
    "it is also worth noting that for the best performing and averaged classifiers from the @xmath43-fold cv , fig .",
    ", show improved agreement between the training and test samples than for the hold - out validated method , fig .",
    ", illustrating how this can lead to a more generalised result .",
    "a brief overview of support vector machines was presented , with an example showing similar performance to that of a bdt .",
    "however it is not clear without further checks as to whether the mvas are sufficiently generalised .",
    "hence a multistage cross - validation procedure has been outlined , which for the same example shows better performance as well as better agreement between the training and testing samples in the output distributions .",
    "this functionality is now available in the github version of root .",
    "9 c. cortes and v. vapnik , machine learning volume * 20 * issue 3 ( 1995 ) .",
    "j. shawe - taylor and n. cristianini , support vector machines and other kernel - based learning methods , cambridge university press ( 2000 ) .",
    "j. mercer , philosophical transactions of the royal society a * 209 * issue 441@xmath49458 ( 1909 ) . c.  adam - bourdarios , g.  cowan , c.  germain - renaud , i.  guyon , b.  kgl and d.  rousseau , `` the higgs machine learning challenge , '' j.  phys .",
    "* 664 * ( 2015 ) no.7 , 072015 .",
    "see also url : http://higgsml.lal.in2p3.fr/documentation/ ( 2014 )",
    ". l. breiman , j. friedman , r. olshen , and c. stone , classification and regression trees , wadsworth international group ( 1984 ) .",
    "a. hoecker et al . , pos acat 040 ( 2007 ) 040 .",
    "s. arlot and a. celisse , statistics surveys vol . *",
    "4 * 4079 ( 2010 ) .",
    "r. rojas , neural networks : a systematic introduction , springer science @xmath50 business media ( 1996 ) ."
  ],
  "abstract_text": [
    "<S> we review the concept of support vector machines ( svms ) and discuss examples of their use . </S>",
    "<S> one of the benefits of svm algorithms , compared with neural networks and decision trees is that they can be less susceptible to over fitting than those other algorithms are to over training . </S>",
    "<S> this issue is related to the generalisation of a multivariate algorithm ( mva ) ; a problem that has often been overlooked in particle physics . </S>",
    "<S> we discuss cross validation and how this can be used to improve the generalisation of a mva in the context of high energy physics analyses . </S>",
    "<S> the examples presented use the toolkit for multivariate analysis ( tmva ) based on root and describe our improvements to the svm functionality and new tools introduced for cross validation within this framework . </S>"
  ]
}