{
  "article_text": [
    "to rank is a crucial issue in the field of information retrieval ( ir ) .",
    "the main goal of learning to rank is to learn automatically ranking functions using a machine learning algorithm , in order to optimize the ranking of documents or web pages .",
    "several algorithms have been proposed during the past decade @xcite that can combine a very large amount of features to learn ranking functions .    whereas the number of features that can be used by algorithms have increased ,",
    "the issue of feature selection in learning to rank has emerged , for two main reasons .",
    "first , as more and more features are incorporated into algorithms , not only the models become more difficult to understand , but also , they potentially have to deal with more and more noisy or irrelevant features . as",
    "feature selection is well - known in machine learning to deal with noisy and irrelevant features , it is seen as a quite natural way to solve this problem in learning to rank .",
    "second , the amount of training data used in learning to rank is substantial . as a consequence ,",
    "learning a ranking function using algorithms is generally costly and can be time - consuming . reducing",
    "the number of features , and thus the dimensionality of the problem , is a promising way to handle the issue of high computational cost .",
    "recent works have focused on the development of feature selection methods dedicated to learning to rank , which can be either preprocessing steps such as filter @xcite@xcite@xcite and wrapper approaches @xcite@xcite@xcite@xcite or integrated to the learning algorithm , such as embedded approaches @xcite@xcite @xcite . in the latter case ,",
    "the learning algorithm is called a sparse algorithm . in this paper , we consider an embedded approach for feature selection in learning to rank .",
    "we propose a general framework for feature selection in learning to rank using support vector machines ( svm ) and a regularization term to induce sparsity .",
    "we investigate both convex regularizaions such as @xmath0 @xcite and non - convex regularizations such as mcp @xcite , log or @xmath1 , @xmath2 @xcite . to the best of our knowledge ,",
    "this is the first work that investigates the use of non - convex penalties for feature selection in learning to rank .",
    "we first propose an accelerated forward backward splitting algorithm in order to solve the @xmath0-regularized problem .",
    "then , we propose a reweighted @xmath0 algorithm to handle the non - convex penalties that benefits from the first algorithm .",
    "we conduct intensive experiments on the letor 3.0 and 4.0 corpora .",
    "our convex algorithm leads to similar performance than the state - of - the - art methods .",
    "we show that the second algorithm that uses non - convex regularizations is a very competitive feature selection method , since it provides as good results as convex approaches but is much more performant in terms of sparsity .",
    "indeed , it provides similar values of evaluation measures while using half as many features in average .",
    "this paper is organized as follows .",
    "section 2 presents the state of the art for learning to rank algorithms , feature selection methods , sparse svm and forward backward splitting approaches .",
    "we formulate the optimization problem in section 3 .",
    "section 4 introduces the algorithms used to solve the optimization problems .",
    "we fully describe the datasets used and the experimental protocol in section 5 . in section 6 ,",
    "we firstly analyze the ability of our approach to induce sparsity into models .",
    "we secondly evaluate the performance of our framework in terms of map and ndcg@xmath3 . we confront these results to those obtained with two recent embedded feature selection methods .",
    "our work focuses on feature selection in learning to rank .",
    "we begin this section by presenting existing learning to rank algorithms .",
    "we provide an overview of feature selection methods dedicated to learning to rank and introduce feature selection using sparse regularized svm .",
    "the learning to rank process consists in a training phase and a prediction phase . in ir ,",
    "the training data are composed of query - documents pairs represented by feature vectors . a relevance judgement between the query and the document",
    "is given as ground truth .",
    "the purpose of the training phase is to learn a model that provides the optimum ranking of documents according to their relevance to the query . the ability of the model to correctly rank documents for new queries is then evaluated during the prediction phase , on test data .",
    "following is a short overview of learning to rank approaches and algorithms",
    ". a more complete introduction to learning to rank for ir can be found in the book by liu @xcite .",
    "+ three approaches , called _ pointwise _ , _ pairwise _ and _ listwise _ , have been proposed to solve the learning to rank problem . in the _ pointwise _ approach ,",
    "each instance is a vector of features @xmath4 which represents a query - document pair",
    ". the ground truth can be either a relevance score @xmath5 or a class of relevance ( such as `` not relevant '' , `` quite relevant '' , `` highly relevant '' ) .",
    "when dealing with a relevance score , learning to rank is seen as a regression problem .",
    "some algorithms such as subset ranking @xcite have been proposed to solve it .",
    "when dealing with classes of relevance , learning to rank is considered as a classification problem or as an ordinal regression problem , depending on whether there is an ordinal relation between the classes of relevance .",
    "some algorithms based on svm @xcite or on boosting @xcite deal with the classification problem .",
    "crammer and singer @xcite proposed an algorithm for ordinal regression . in the _ pairwise _",
    "approach , also referred as preference learning @xcite , each instance is a pair of feature vectors @xmath6 for a given query @xmath7 .",
    "the ground truth is given as a preference @xmath8 between the two documents .",
    "for a given couple @xmath6 , if @xmath4 is preferred to @xmath9 , we note @xmath10 and then @xmath11 is set to @xmath12 . in the contrary , if @xmath9 is preferred to @xmath4 , we note @xmath13 and then @xmath11 is set to @xmath14 .",
    "it is thus a classification problem .",
    "many algorithms have been developed to deal with this problem , such as ranknet @xcite based on neural networks , rankboost @xcite based on boosting or ranksvm - primal @xcite and ranksvm - struct @xcite based on svm .",
    "finally , the _ listwise _ approach considers the whole ranked list of documents as the instance of the algorithm .",
    "most works have focused on the proposal of new specific loss functions , based on the optimization of an ir metric or on permutations count in order to solve this kind of problem @xcite@xcite .",
    "+ these approaches have been shown to be both efficient and effective to learn functions that ensure high ranking performance in terms of ir measures .",
    "nevertheless , they may be suboptimal for use in real life with large scale data .",
    "ranking functions deal with a very large amount of features , which raises three critical issues .",
    "first , as features may take time to compute , preprocessing steps such as the creation of training data may become time - consuming .",
    "second , due to the high dimensionality of training data , algorithms may not be scalable or they may take too much time for computation . finally , there may be a significant amount of redundant or irrelevant features used by models , that can lead to suboptimal ranking performance .",
    "thus , how to reduce the number of features to be used by algorithms has emerged as a crucial issue . nevertheless , only few attempts have been made to solve this problem . in the following section ,",
    "we propose an overview of existing feature selection methods in classification and learning to rank .      in classification ,",
    "there are three kinds of feature selection methods called filter , wrapper and embedded . in filter methods , a subset of features",
    "is selected as a preprocessing step , independently of the predictor used for learning . in wrapper methods ,",
    "the machine learning algorithm is used as a black box to score subsets of features according to their predictive power .",
    "the subset with the highest score is then chosen . finally , in embedded methods",
    ", feature selection is performed within the training phase and incorporated to the algorithm .",
    "embedded methods are generally specific to a given machine learning algorithm . a wide introduction to feature selection for classification",
    "is presented in the work of guyon and elisseeff @xcite .",
    "feature selection methods for learning to rank have been developed in a similar way as in classification .",
    "we propose an overview of feature selection methods for learning to rank in the following section and we classify them into filter , wrapper and embedded categories in table [ featureselectionmethods ] .",
    "+ to the best of our knowledge , the first proposal of a feature selection method dedicated to learning to rank is the work of geng @xcite .",
    "their method is called greedy search algorithm for feature selection ( gas ) and belongs to filter approaches . for each feature",
    ", they first define its importance score : they rank instances according to feature values and evaluate the performance of the ranking list with a measure such as mean average precision ( map ) or normalized discounted cumulative gain ( ndcg ) .",
    "this evaluation measure is then used as the importance score for the feature . for each pair of features",
    ", they also define a similarity score , which is the value of the kendall s @xmath15 between the rankings induced by the features of the pair .",
    "the kendall s @xmath15 is defined as follow : if @xmath16 and @xmath11 are two features and @xmath17 the number of documents pairs , then @xmath18 where @xmath19 indicates that the document @xmath20 is ranked above the document @xmath21 according to the value of feature @xmath16 .",
    "an optimization problem is then formulated to select features by simultaneously maximizing the total importance score and minimizing the total similarity score .",
    "this optimization problem is solved by a greedy search algorithm .",
    "they show that the gas algorithm can significantly improve the performance in terms of map or ndcg while reducing the number of features .",
    "hua @xcite later proposed a two - phase feature selection strategy . in a first step , they define the similarity between features in the same way as in the gas algorithm .",
    "features are then clustered into groups according to their similarity , by using a k - means approach .",
    "the number of clusters to be used is chosen according to a quality measure defined by the authors . in a second step ,",
    "they propose to select a single representative feature from each cluster to learn the model .",
    "they use two delegation strategies for this purpose : a filter one based on evaluation measure ( bem ) and a wrapper one implied by the learning to rank method used ( iltr ) .",
    "the bem delegation method selects the feature which ranking has the best evaluation score .",
    "the iltr delegation method learns a linear model using a learning to rank algorithm . for each cluster ,",
    "the representative feature is then the one with the highest weight in the ranking function .",
    "they show that bem and iltr techniques can significantly improve the performance in terms of ndcg@10 compared to models with no feature selection .",
    "+ some other works have focused on the development of wrapper approaches for feature selection on learning to rank .",
    "pan @xcite proposed a method using boosted regression trees . in a similar way to @xcite",
    ", they define an importance score for each feature and a similarity score for each pair of features .",
    "the importance score is the relative importance score as defined by friedman @xcite for regression boosted trees .",
    "the similarity score is defined by the kendall s @xmath15 between the vectors of values for the features of the pairs .",
    "the authors investigate three optimization problems : ( 1 ) to maximize the importance score , ( 2 ) to minimize the similarity score and ( 3 ) to simultaneously maximize the importance score and minimize the similarity score .",
    "these optimization problems are solved by a greedy approach .",
    "experiments show that better results are obtained when only using the importance score than when using the importance and similarity scores .",
    "moreover , they point out that a 30 features model achieves similar performance in terms of ndcg@5 than the complete model with 419 features . in a second approach ,",
    "they propose a randomized feature selection with a feature - importance - based backward elimination . in practice",
    ", they create subsets of features , then iteratively train boosted trees and remove a percentage of features according to their ndcg@5 performance .",
    "the experimental results show that these methods achieve comparable performance than the complete model by using only 30 features .",
    "yu @xcite proposed two effective feature selection methods for ranking based on relief algorithms @xcite .",
    "relief algorithms are iterative methods that update the feature weights at each iteration , based on their importance .",
    "the authors propose rankwrapper , a wrapper approach for training data with relative orderings , andrankfilter , a filter approach from training data with multi - level relevance classes .",
    "they also define new updating rules for the weights for each algorithm .",
    "experiments on synthetic and benchmark datasets show that their method outperforms the gas algorithm and can be used with large scale datasets .",
    "dang and croft @xcite proposed a feature selection technique based on the wrapper approach defined in @xcite .",
    "they use a best - first search procedure to create subsets of features . for each subset",
    ", they train a model with a ranking algorithm .",
    "the output is defined as a new feature . a new feature vector is then created with the output of each subset and contains less features than the initial dataset .",
    "models are trained using this vector with four well - known learning to rank algorithms : ranknet , rankboost , adarank and coordinate ascent .",
    "their experiments on letor datasets show that they produce comparable performance in terms of ndcg@5 by using the smaller feature vector .    finally , pahikkala @xcite proposed an algorithm called greedy rankrls , which is a wrapper approach based on the existing rankrls algorithm .",
    "subsets of features are created on which a leave - query - out cross - validation is performed by using the rankrls algorithm .",
    "results on the letor 4.0 distribution show that the performance in terms of map and ndcg@10 are comparable to state - of - the - art algorithms with all the features .",
    "+ recently , embedded methods have been proposed to deal with the problem of feature selection .",
    "these approaches introduce a sparse regularization term in the formulation of the optimization problem .",
    "although sparse regularizations are widely used in classification to deal with feature selection , only a few attempts have been made to propose sparse - regularized learning to rank methods .",
    "sun @xcite implemented a sparse algorithm called rsrank to directly optimize the ndcg .",
    "they propose a framework to reduce ranking to importance weighted pairwise classification .",
    "to achieve sparsity , they introduce a @xmath0-regularization term and solve the optimization problem using truncated gradient descent .",
    "experiments on ohsumed and td2003 datasets show that only about a third of features remained after the selection . moreover , the performance of the learned model is comparable or significantly better than the baselines , depending on the dataset and the measure used .",
    "a more recent work of lai @xcite proposed a primal - dual algorithm for learning to rank called fenchelrank .",
    "the authors formulate the sparse learning to rank problem as a svm problem with a @xmath0-regularization term .",
    "they use the properties of the fenchel duality to solve the optimization problem .",
    "basically , fenchelrank is an iterative algorithm that works in three steps . at each iteration , it first checks whether the stopping criterion is satisfied .",
    "if not , the algorithm then greedily chooses a feature to update according to its value . finally , it updates the weights of the ranking model .",
    "experiments were conducted on several datasets from the letor 3.0 and letor 4.0 collections .",
    "the authors show that fenchelrank leads to a good sparsity with sparsity ratios from 0.1875 to 0.5 .",
    "it also provides comparable or significantly better results in terms of map and ndcg than state - of - art algorithms and rsrank .",
    "finally , lai @xcite recently proposed a new embedded algorithm for feature selection based on sparse svm .",
    "this algorithm solves a joint convex optimization problem in order to learn ranking functions while automatically selecting the best features .",
    "they use a nesterov approach to ensure fast convergence .",
    "they show that fsmrank can learn efficiently ranking models that outperform the gas algorithm .    in classification",
    ", a large panel of embedded methods have been developed to learn sparse models with svm .",
    "as far as we know , fenchelrank and fsmrank are the only ones to use sparse svm for feature selection in learning to rank .",
    "sparse svm could widely and efficiently be adapted in this purpose . in this paper , we focuses on svm methods with sparse - regularized term for which we propose a short overview in the following section .",
    ".classification of feature selection algorithms for learning to rank into filter , wrapper and embedded categories . [ cols=\"^,^,^\",options=\"header \" , ]",
    "in this work , we presented a general framework for feature selection in learning to rank , by using svm with sparse regularizations .",
    "we first proposed an accelerated proximal algorithm to solve the convex @xmath0 regularized problem .",
    "this algorithm has the same theoretical convergence rate than the state - of - the - art fenchelrank and fsmrank algorithm .",
    "we showed that a reweighted @xmath0 scheme can be used in order to solve non convex problems .",
    "this scheme is implemented into a second algorithm that solved problems with mcp , log and @xmath1 , @xmath2 penalties . to the best of our knowledge ,",
    "it is the first work that propose to consider non - convex penalties for feature selection in learning to rank we conducted experiments on two major benchmarks in learning to rank that include nine different datasets on which we evaluate the performance in terms of map and ndcg@xmath3 .",
    "we also evaluate the ability of our framework to induce sparsity into models .",
    "we pointed out that the non - convex penalties lead to similar prediction quality , whatever the evaluation measure is , while using only half as many features as convex methods .",
    "our framework is then a novel , competitive and effective embedded method for feature selection in learning to rank .",
    "its originality is to consider non - convex regularizations in order to induce more sparsity into models without degradation of the prediction quality .",
    "moreover , we will provide publicly available software for the two proposed algorithms in order to promote reproducible research .",
    "this work and the contributions of sun @xcite , lai @xcite @xcite show the effectiveness of embedded methods in the field of feature selection for learning to rank .",
    "more specifically , the use of sparse regularized svm seems to be a promising way to handle the issue of feature selection and dimensionality reduction in learning to rank . to the best of our knowledge ,",
    "our work is the first that propose a feature selection framework for learning to rank that is not restricted to the use of @xmath0-regularization .",
    "a wide range of issues still need to be explored . in future works",
    ", we plan to evaluate the impact of tuning the non - convex regularizations parameters on both sparsity and prediction quality",
    ". a large study of the computational times of the sparse leaning to rank algorithms could be conducted . finally , as feature selection can be used in order to learn ranking function specific to subset of queries , one of the most promising direction of work is the field of multitask learning .",
    "we plan to investigate the potential of a sparse regularized svm algorithm using a fast iterative shrinkage thresholding framework , to be confronted to existing multitask algorithm @xcite@xcite@xcite .",
    "the authors would like to thank calmip ( grant 2012 - 32 ) , the fremit federation and the rgion midi - pyrnes for their support .",
    "t.  pahikkala , a.  airola , p.  naula , and t.  salakoski , `` greedy rankrls : a linear time algorithm for learning sparse ranking models , '' in _ sigir 2010 workshop on feature generation and selection for information retrieval _ , 2010 , pp .",
    "z.  sun , t.  qin , q.  tao , and j.  wang , `` robust sparse rank learning for non - smooth ranking measures , '' in _ proc .",
    "acm sigir conf .",
    "research and development in information retrieval _ , 2009 , pp .",
    "259266 .",
    "m.  kloft , u.  brefeld , s.  sonnenburg , and a.  zien , `` lp - norm multiple kernel learning , '' _ j. mach .",
    "_ , vol .  12 , pp . 953997 , jul",
    "[ online ] .",
    "available : http://dl.acm.org/citation.cfm?id=1953048.2021033        p.  li , c.  j.  c. burges , and q.  wu , `` mcrank : learning to rank using multiple classification and gradient boosting . '' in _ nips _ , j.  c. platt , d.  koller , y.  singer , and s.  t. roweis , eds.1em plus 0.5em minus 0.4emcurran associates , inc .",
    ", 2007 .        c.  burges , t.  shaked , e.  renshaw , a.  lazier , m.  deeds , n.  hamilton , and g.  hullender , `` learning to rank using gradient descent , '' in _ proc .",
    "machine learning ( icml 05 ) _ , 2005 , pp .",
    "8996 .",
    "y.  yue , t.  finley , f.  radlinski , and t.  joachims , `` a support vector method for optimizing average precision , '' in _ proc .",
    "30th annu .",
    "acm sigir conf .",
    "research and development in information retrieval _ , 2007 , pp .",
    "271278 .",
    "g.  gasso , a.  rakotomamonjy , and s.  canu , `` recovering sparse signals with a certain family of nonconvex penalties and dc programming , '' _ ieee trans .",
    "signal process .",
    "_ , vol .",
    "57 , no .  12 , pp . 46864698 , 2009 .",
    "j.  bai , k.  zhou , g.  xue , h.  zha , g.  sun , b.  tseng , z.  zheng , and y.  chang , `` multi - task learning for learning to rank in web search , '' in _ proc .",
    "18th acm conf .",
    "information and knowledge management _",
    ", 2009 , pp .",
    "15491552 .",
    "o.  chapelle , p.  shivaswamy , s.  vadrevu , k.  weinberger , y.  zhang , and b.  tseng , `` multi - task learning for boosting with application to web search ranking , '' in _ proc .",
    "16th acm sigkdd int .",
    "knowledge discovery and data mining _ , 2010 ,",
    ". 11891198 ."
  ],
  "abstract_text": [
    "<S> feature selection in learning to rank has recently emerged as a crucial issue . whereas </S>",
    "<S> several preprocessing approaches have been proposed , only a few works have been focused on integrating the feature selection into the learning process . in this work , </S>",
    "<S> we propose a general framework for feature selection in learning to rank using svm with a sparse regularization term . </S>",
    "<S> we investigate both classical convex regularizations such as @xmath0 or weighted @xmath0 and non - convex regularization terms such as log penalty , minimax concave penalty ( mcp ) or @xmath1 pseudo norm with @xmath2 . </S>",
    "<S> two algorithms are proposed , first an accelerated proximal approach for solving the convex problems , second a reweighted @xmath0 scheme to address the non - convex regularizations . </S>",
    "<S> we conduct intensive experiments on nine datasets from letor 3.0 and letor 4.0 corpora . </S>",
    "<S> numerical results show that the use of non - convex regularizations we propose leads to more sparsity in the resulting models while prediction performance is preserved . </S>",
    "<S> the number of features is decreased by up to a factor of six compared to the @xmath0 regularization . </S>",
    "<S> in addition , the software is publicly available on the web .    </S>",
    "<S> feature selection , learning to rank , regularized svm , sparsity , fbs algorithms , non - convex regularizations . </S>"
  ]
}