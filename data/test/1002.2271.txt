{
  "article_text": [
    "let a memoryless additive white gaussian noise ( awgn ) channel be described by @xmath0 , where @xmath1 is independent of @xmath2 .",
    "if the input is imposed an average power constraint given by @xmath3 , the input distribution maximizing the mutual information is gaussian .",
    "this is due to the fact that under second moment constraint , the gaussian distribution maximizes the entropy , hence @xmath4 on the other hand , if we use a gaussian input distribution , i.e. , @xmath5 , the worst noise that can occur , i.e. , the noise minimizing the mutual information , among noises with bounded second moment , is again gaussian distributed .",
    "this can be shown by using the _ entropy power inequality _",
    "( epi ) , cf .",
    "@xcite , which reduces in this setting to @xmath6 and implies @xmath7 hence , in the single - user setting , when optimizing the mutual information as above , a gaussian input is the best input for a gaussian noise and a gaussian noise is the worst noise for a gaussian input .",
    "this provides a game equilibrium between user and nature , as defined in @xcite , p. 263 . with these results ,",
    "many problems in information theory dealing with gaussian noise can be solved . however , in gaussian networks , that is , in multi - user information theory problems where the external noise is gaussian distributed , several new phenomena make the search for the optimal input ensemble more complex . besides for some specific cases of gaussian networks , we still do not know how interference should be treated in general .",
    "let us consider two users interfering on each other in addition to suffering from gaussian external noise and say that the receivers treat interference as noise .",
    "then , if the first user has drawn its code from a gaussian ensemble , the second user faces a frustration phenomenon : using a gaussian ensemble maximizes its mutual information but minimizes the mutual information of the first user .",
    "it is an open problem to find the optimal input distributions for this problem .",
    "this is one illustration of the complications appearing in the network setting .",
    "another example is regarding the treatment of the fading . over a single - user awgn channel ,",
    "whether the fading is deterministic or random , but known at the receiver , does not affect the optimal input distribution . from",
    ", it is clear that maximizing @xmath8 or @xmath9 under an average power constraint is achieved by a gaussian input .",
    "however , the situation is different if we consider a gaussian broadcast channel ( bc ) . when there is a deterministic fading , using and , the optimal input distribution can be shown to be gaussian .",
    "however , it has been an open problem to show whether gaussian inputs are optimal or not for a gaussian bc with a random fading known at the receiver , even if the fading is such that it is a degraded bc .",
    "a reason for these open questions in the network information theoretic framework , is that gaussian ensembles are roughly the only ensembles that can be analyzed over gaussian networks , as non - gaussian ensembles have left most problems in an intractable form . in this paper ,",
    "a novel technique is developed to analyze a class of non - gaussian input distributions over gaussian noise channels .",
    "this technique is efficient to analyze the _ competitive situations _ occurring in the network problems described below .",
    "it allows in particular to find certain non - gaussian ensembles that outperform gaussian ones on a gaussian bc with coherent fading channel , a two user interference channel , and it allows to disprove the strong shamai - laroia conjecture on the gaussian intersymbol interference channel .",
    "this tool provides a new insight on gaussian networks and confirms that non - gaussian ensembles do have a role to play in these networks .",
    "we now introduce with more details the notion of competitive situations .        consider a degraded gaussian bc with coherent memoryless fading , where the fading is indeed the same for both receivers , i.e. @xmath10 but @xmath11 and @xmath12 , with @xmath13 .",
    "the input @xmath2 is imposed a power constraint denoted by @xmath14 . because the fading is coherent , each receiver also knows the realization of @xmath15 , at each channel use .",
    "the fading and the noises are memoryless ( iid ) processes .",
    "since this is a degraded broadcast channel , the capacity region is given by all rate pairs @xmath16 with @xmath17 .",
    "the optimal input distributions , i.e. , the distributions of @xmath18 achieving the capacity region boundary , are given by the following optimization , where @xmath19 , @xmath20 note that the objective function in the above maximization is given by @xmath21 now , each term in this expression is individually maximized by a gaussian distribution for @xmath22 and @xmath2 , but these terms are combined with different signs , so there is a competitive situation and the maximizer is not obvious .",
    "when @xmath23 , one can show that gaussian distributions are optimal .",
    "also , if @xmath15 is compactly supported , and if @xmath24 is small enough as to make the support of @xmath15 and @xmath25 non overlapping , the optimal distribution of @xmath18 is jointly gaussian ( cf .",
    "however , in general the optimal distribution is unknown .",
    "we do not know if it because we need more theorems , or if it is really that with fading , non - gaussian codes can actually perform better than the gaussian ones .",
    "we consider the symmetric memoryless interference channel ( ic ) with two users and white gaussian noise .",
    "the average power is denoted by @xmath14 , the interference coefficients by @xmath26 , and the respective noise by @xmath27 and @xmath28 ( independent standard gaussian ) .",
    "we define the following expression @xmath29 where @xmath30 and @xmath31 are independent random vectors of dimension @xmath32 with a covariance having a trace bounded by @xmath33 and @xmath34 , @xmath35 , are iid standard gaussian . for any dimension @xmath32 and",
    "any distributions of @xmath30 and @xmath31 , @xmath36 is a lower bound to the sum - capacity .",
    "moreover , it is tight by taking @xmath32 arbitrarily large and @xmath30 and @xmath31 maximizing .",
    "now , a similar competitive situation as for the fading broadcast problem takes place : gaussian distributions maximize each entropy term , but these terms are combined with different signs .",
    "would we then prefer to take @xmath37 and @xmath38 gaussian or not ?",
    "this should depend on the value of @xmath26 . if @xmath39 , we have two parallel awgn channels with no interference , and gaussian inputs are optimal",
    ". we can then expect that this might still hold for small values of @xmath26 .",
    "it has been proved recently in @xcite , that the sum - capacity is achieved by treating interference as noise and with iid gaussian inputs , as long as @xmath40 .",
    "hence , in this regime , the iid gaussian distribution maximizes for any @xmath32 .",
    "but if @xmath26 is above that threshold and below @xmath41 , the problem is open .",
    "let us now review the notion of `` treating interference as noise '' . for each user",
    ", we say that the decoder is _ treating interference as noise _ , if it does not require the knowledge of the other user s code book . however , we allow such decoders to have the knowledge of the distribution , under which the other user s code book may be drawn .",
    "this is for example necessary to construct a sum - capacity achieving code in @xcite , where the decoder of each user treats interference as noise but uses the fact that the other user s code book is drawn from an iid gaussian distribution .",
    "but , if we allow this distribution to be of arbitrarily large dimension @xmath32 in our definition of treating interference as noise , we can get a misleading definition .",
    "indeed , no matter what @xmath26 is , if we take @xmath32 large enough and a distribution of @xmath30 , @xmath31 maximizing , we can achieve rates arbitrarily close to the sum - capacity , yet , formally treating interference as noise .",
    "the problem is that the maximizing distributions in may not be iid for an arbitrary @xmath26 , and knowing it at the receiver can be as much information as knowing the other user s code book ( for example , if the distribution is the uniform distribution over a code book of small error probability ) .",
    "hence , one has to be careful when taking @xmath32 large . in this paper",
    ", we will only work with situations that are not ambiguous with respect to our definition of treating interference as noise .",
    "it is indeed an interesting problem to discuss what kind of @xmath32-dimensional distributions would capture the meaning of treating interference as noise that we want .",
    "this also points out that studying the maximizers of relates to studying the concept of treating interference as noise or information .",
    "since for any chosen distributions of the inputs we can achieve , the maximizers of must have a different structure when @xmath26 grows .",
    "for @xmath26 small enough , iid gaussian are maximizing distributions , but for @xmath42 , since we do not want to treat interference as noise , the maximizing distributions must have a `` heavy structure '' , whose characterization requires as much information as giving the entire code book .",
    "this underlines that an encoder can be drawn from a distribution which does not maximize for any value of @xmath32 , but yet , a decoder may exist in order to have a capacity achieving code .",
    "this happens if @xmath43 , iid gaussian inputs will achieve the sum - capacity if the receiver decodes the message of both users ( one can show that the problem is equivalent to having two mac s ) .",
    "however , if @xmath42 , the iid gaussian distribution does not maximize @xmath44 ( for the dimension 1 , hence for arbitrary dimensions ) .    in any cases",
    ", if the gaussian distribution does not maximize for the dimension 1 , it means that iid gaussian inputs and treating interference as noise is not capacity achieving , since a code which treats interference as noise and whose encoder is drawn from a distribution can be capacity achieving only if the encoder is drawn from a distribution maximizing .",
    "hence , understanding better how to resolve the competitive situation of optimizing is a consequent problem for the interference channel .",
    "[ sl ] let @xmath45 , @xmath46 and @xmath1 ( independent of @xmath47 ) . for all @xmath48 i.i.d . with mean 0 and variance @xmath14",
    ", we have @xmath49    this conjecture has been brought to our attention by shlomo shamai ( shitz ) , who referred to the strong conjecture for a slightly more general statement , where an arbitrary memory for the interference term is allowed , i.e. , where @xmath50 stands for @xmath51 .",
    "the strong conjecture then claims that picking all @xmath52 s gaussian gives a minimizer .",
    "however , we will show that even for the memory one case , the conjecture does not hold .",
    "the weak conjecture , also referred to as the shamai - laroia conjecture , corresponds to a specific choice of the @xmath53 s , which arises when using an mmse decision feedback equalizer on a gaussian noise isi channel , cf .",
    "this conjecture is investigated in a work in progress .",
    "+ there are many other examples in network information theory where such competitive situations occur . our goal in this paper is to explore the degree of freedom provided by non - gaussian input distributions .",
    "we show that the neighborhood of gaussian distributions can be parametrized in a specific way , as to simplify greatly the computations arising in competitive situations",
    ". we will be able to precisely quantify how much a certain amount of non - gaussianness , which we will characterize by means of the hermite polynomials , affects or helps us in maximizing the competitive entropic functional of previously mentioned problems .",
    "for the fading bc problem problem described in [ bc ] , we want to determine if / when the distribution of @xmath18 maximizing is gaussian or not .",
    "[ [ ic ] ] ic ~~    for the interference channel problem described in [ ic ] , we know from @xcite that treating interference as noise and using iid gaussian inputs is optimal when @xmath54 .",
    "we question when this coding scheme is no longer optimal .",
    "more generally , we want to analyze the maximizers of .",
    "we distinguish the implication of such a threshold in both the synchronized and asynchronized users setting , as there will be an interesting distinction between these two cases .",
    "we recall how the synch and asynch settings are defined here . in the synch setting",
    ", each user of the ic sends their code words of a common block length @xmath55 simultaneously , i.e. , at time 1 , they both send the first component of their code word , at time 2 the second component , etc . in the asynch",
    "setting , each user is still using code words of the same block length @xmath55 , however , there might be a shift between the time at which the first and second users start sending their code words .",
    "we denote this shift by @xmath56 , and assume w.l.o.g .",
    "that @xmath57 . in the totally asynch",
    "setting , we assume that @xmath56 is drawn uniformly at random within @xmath58",
    ". we may also distinguish the cases where @xmath56 is not known at the transmitter but at the receiver , and when @xmath56 is not known at both .",
    "note that if iid input distributions are used to draw the code books , and interference is treated as noise , whether the users are synch or asynch is not affecting the rate achievability and @xmath38 can still be defined for the totally asynch ic ] . however ,",
    "if the users want to time - share over the channel uses , such as to fully avoid their interference , they will need synchronization .",
    "[ ts ] time sharing over a block length @xmath55 ( assumed to be even ) with gaussian inputs refers to using @xmath37 gaussian with covariance @xmath59 and @xmath38 gaussian with covariance @xmath60 , where @xmath61 is a diagonal matrix with @xmath62 1 s and 0 s , and @xmath63 flips the 1 s and 0 s on the diagonal .",
    "we want to determine whether conjecture [ sl ] holds or not .",
    "our more general goal is to understand better the problem posed by any competitive situations . for this purpose ,",
    "we formulate the following mathematical problem",
    ".    we start by changing the notation and rewrite and as @xmath64 where @xmath65 denotes the gaussian density with zero mean and variance @xmath14 , and the functions @xmath66 are density functions on @xmath67 , i.e. , positive functions integrating to 1 , and having a well - defined entropy and second moment @xmath68 .",
    "+ we consider the local geometry by looking at densities of the form @xmath69 where @xmath70 satisfies @xmath71 with these two constraints on @xmath72 , @xmath73 is a valid density for @xmath74 sufficiently small .",
    "it is a perturbed gaussian density , in a `` direction '' @xmath72 .",
    "observe that , @xmath75    we are now interested in analyzing how these perturbations affect the output distributions through an awgn channel . note that , if the input distribution is a gaussian @xmath65 perturbed in the direction @xmath72 , the output is a gaussian @xmath76 perturbed in the direction @xmath77 since @xmath78 _ convention : _ @xmath79 refers to @xmath80 , i.e. , the multiplicative operator precedes the convolution one .",
    "+ for simplicity , let us assume in the following that the function @xmath72 is a polynomial satisfying , .",
    "[ appro ] we have @xmath81 where @xmath82    moreover , note that for any density @xmath66 , if @xmath83 and @xmath84 , we have @xmath85 hence , the extremal entropic results of and are locally expressed as @xmath86 where 0 denotes here the zero function . if is obvious , requires a proof which will be done in section [ proofs ] .",
    "let us define the following mapping , @xmath87 where @xmath88 denotes the space of real functions having a finite @xmath89 norm .",
    "this linear mapping gives , for a given perturbed direction @xmath72 of a gaussian input @xmath65 , the resulting perturbed direction of the output through additive gaussian noise @xmath90 .",
    "the norm of each direction in their respective spaces , i.e. , in @xmath88 and @xmath91 , gives how far from the gaussian distribution these perturbations are ( up to a scaling factor ) .",
    "note that if @xmath72 satisfies - , so does @xmath92 for the measure @xmath76 .",
    "the result in ( worst noise case ) tells us that this mapping is a contraction , but for our goal , what would be helpful is a spectral analysis of this operator , to allow more quantitative results than the extreme - case results of and .    in order to do so",
    ", one can express @xmath93 as an operator defined and valued in the same space , namely @xmath94 with the lebesgue measure @xmath95 , which is done by inserting the gaussian measure in the operator argument .",
    "we then proceed to a singular function / value analysis .",
    "formally , let @xmath96 which gives @xmath97 , and let @xmath98 which gives @xmath99 denoting by @xmath100 the adjoint operator of @xmath101 , we want to find the singular functions of @xmath101 , i.e. , the eigenfunctions @xmath102 of @xmath103 : @xmath104",
    "the following theorem gives the singular functions and values of the operator @xmath101 defined in previous section .",
    "[ hermite1 ] @xmath105 holds for each pair @xmath106 } , \\left(\\frac{p}{p+v}\\right)^k)\\}_{k \\geq 0},\\ ] ] where @xmath107}(x)=\\frac{1}{\\sqrt{k ! } } h_k(x/\\sqrt{p})\\ ] ] @xmath108    the polynomials @xmath109}$ ] are the normalized hermite polynomials ( for a gaussian distribution having variance @xmath14 ) and @xmath110}$ ] are called the hermite functions . for any @xmath111 , @xmath112}\\}_{k\\geq 0}$ ] is an orthonormal basis of @xmath88 , this can be found in @xcite .",
    "one can check that @xmath113 , respectively @xmath114 perturb a gaussian distribution into another gaussian distribution , with a different first moment , respectively second moment . for @xmath115 ,",
    "the @xmath116 perturbations are not modifying the first two moments and are moving away from gaussian distributions . since @xmath117}=1 $ ] , the orthogonality property implies that @xmath109}$ ] satisfies for any @xmath118 .",
    "however , it is formally only for even values of @xmath119 that is verified ( although we will see in section [ proofs ] that essentially any @xmath119 can be considered in our problems ) .",
    "the following result contains the property of hermite polynomials mostly used in our problems , and expresses theorem [ hermite1 ] with the gaussian measures .",
    "the following result contains the property of hermite polynomials mostly used in our problems , and expresses proposition [ hermite1 ] with the gaussian measures .",
    "[ hermite2 ] @xmath120}= \\frac{g_p h_k^{[p ] } \\star g_v}{g_{p+v}}=\\left(\\frac{p}{p+v}\\right)^{k/2 } h_k^{[p+v ] } , \\label{it } \\\\ & { \\gamma^{(-)}}h_k^{[p+v]}= h_k^{[p+v ] } \\star g_v = \\left(\\frac{p}{p+v}\\right)^{k/2 } h_k^{[p]}. \\end{aligned}\\ ] ]    last theorem implies theorem [ hermite1 ] , since @xmath121 for @xmath122    _ comment : _ the results that we have just derived are related to properties of the ornstein - uhlenheck process . _",
    "summary : _ in words , we just saw that @xmath123 is an eigenfunction of the input / output perturbation operator @xmath93 , in the sense that @xmath124 } = \\left(\\frac{p}{p+v } \\right)^{k/2 } h_k^{[p+v]}$ ] .",
    "hence , over an additive gaussian noise channel @xmath90 , if we perturb the input @xmath65 in the direction @xmath109}$ ] by an amount @xmath74 , we will perturb the output @xmath76 in the direction @xmath125}$ ] by an amount @xmath126 .",
    "such a perturbation in @xmath116 implies that the output entropy is reduced ( compared to not perturbing ) by @xmath127 ( if @xmath128 ) .",
    "the following result states that the capacity region of a degraded fading bc with gaussian noise is not achieved by a gaussian superposition code in general .    [ ce ] let @xmath129 with @xmath2 such that @xmath3 , @xmath130 , @xmath131 , @xmath132 and @xmath133 mutually independent .",
    "there exists a fading distribution and a value of @xmath24 for which the capacity achieving input distribution is non - gaussian .",
    "more precisely , let @xmath22 be any auxiliary random variable , with @xmath134 .",
    "then , there exists @xmath135 , a distribution of @xmath15 and @xmath136 such that @xmath137 is maximized by a non jointly gaussian distribution .    in the proof",
    ", we present a counter - example to gaussian being optimal for @xmath15 binary . in order to defeat gaussian distributions ,",
    "we construct input distributions using the hermite coordinates .",
    "the proof also gives a condition on the fading distribution and the noise variance @xmath24 for which a non - gaussian distribution strictly improves on the gaussian one .",
    "let @xmath138,\\ ] ] where @xmath139 , @xmath140 and @xmath141 , with @xmath142 defined in below ( as explain in section [ form ] , @xmath142 is a formal modification of @xmath116 to ensure the positivity of the perturbed densities ) .",
    "+ in other words , @xmath143 represents the gain ( positive or negative ) of using @xmath37 perturbed along @xmath116 and @xmath38 perturbed along @xmath144 with respect to using gaussian distributions . note that the distributions we chose for @xmath37 and @xmath38 are not the most general ones , as we could have chosen arbitrary directions spanned by the hermite basis to perturb the gaussian densities",
    "however , as explained in the proof of the theorem [ calc ] , this choice is sufficient for our purpose .",
    "[ calc ] we have for @xmath145 @xmath146^k - \\frac{(a^k-1)^2}{(pa^2+p+1)^k}.\\ ] ]    for any fixed @xmath14 , the function @xmath147 has a unique positive root , below which it is negative and above which it is positive .",
    "[ inter ] treating interference as noise with iid gaussian inputs does not achieve the sum - capacity of the symmetric ic ( synch or asynch ) and is outperformed by @xmath148 and @xmath149 , if @xmath150    this theorem is a direct consequence of theorem [ calc ] .",
    "[ f2 ] for the symmetric synch ic , time sharing improves on treating interference as noise with iid gaussian distribution if @xmath151    we now introduce the following definition .",
    "blind time sharing over a block length @xmath55 ( assumed to be even ) between two users , refers to sending non - zero power symbols only at the instances marked with a 1 in @xmath152 for the first user , and zero power symbols only at the instances marked with a 1 in @xmath153 for the second user .",
    "[ b2 ] for the symmetric totally asynch ic , if the receivers ( but not transmitters ) know the asynchronization delay , blind time sharing improves on treating interference as noise with iid gaussian distributions if @xmath154 where @xmath155 if the receivers do not know the asynchronization delay , blind time sharing can not improve on treating interference as noise with iid gaussian distributions if @xmath156 .    _",
    "how to read these results : _ we have four thresholds to keep track of :    * @xmath157 is when @xmath158 . if @xmath159 , we know from @xcite that iid gaussian inputs and treating interference as noise is sum - capacity achieving . *",
    "@xmath160 is when @xmath161 . if @xmath162 , we know from prop .",
    "[ f2 ] that , if synchronization is permitted , time sharing improves on treating interference as noise with iid gaussian inputs .",
    "this regime matches with the so - called moderate regime defined in @xcite .",
    "* @xmath163 is when @xmath164 . if @xmath165 , we know from prop .",
    "[ inter ] that treating interference as noise with iid non - gaussian distributions ( opposites in @xmath166 ) improves on the iid gaussian ones .",
    "* @xmath167 is when @xmath168 . if @xmath169 , we know from prop . [ b2 ] that , even if the users are totally asynchronized , but if the receivers know the asynchronization delay , blind time sharing improves on treating interference as noise with iid gaussian inputs .",
    "if the receivers do not know the delay , the threshold can only appear for larger values of @xmath26 .",
    "the question is now , how are these thresholds ranked .",
    "it turns out that @xmath170 and if @xmath171 , the above inequality reads as @xmath172 .",
    "this implies the following for a decoder that treats interference as noise . since @xmath173",
    ", it is first better to time share than using non - gaussian distributions along @xmath166 .",
    "but this is useful only if time - sharing is permitted , i.e. , for the synch ic .",
    "however , for the asynchronized ic , since @xmath174 , we are better off using the non - gaussian distributions along @xmath166 before a gaussian input scheme , even with blind time - sharing , and even if the receiver could know the delay .",
    "we notice that there is still a gap between @xmath157 and @xmath160 , and we can not say if , in this range , iid gaussian inputs are still optimal , or if another class of non - gaussian inputs ( far away from gaussians ) can outperform them . in @xcite , another technique ( which is related to ours but not equivalent ) is used to find regimes where non - gaussian inputs can improve on gaussian ones on the same problem that we consider here .",
    "the threshold found in @xcite is equal to 0.925 for @xmath171 , which is looser than the value of 0.680 found here .    finally , the following interesting and curious fact has also been noticed . in theorem [ calc ] , we require @xmath145 .",
    "nevertheless , if we plug @xmath175 in the right hand side of theorem [ calc ] and ask for this expressions to be positive , we precisely get @xmath176 , i.e. , the complement range delimited by @xmath157 . however , the right hand side of theorem [ calc ] for @xmath175 is not equal to @xmath177 ( this is explained in more details in the proof of theorem [ calc ] ) .",
    "indeed , it would not make sense that moving along @xmath113 , which changes the mean with a fixed second moment within gaussians , would allow us to improve on the iid gaussian scheme .",
    "yet , getting to the exact same condition , when working on the problem of improving on the iid gaussian scheme , seems to be a strange coincidence .",
    "we show in section [ proofs ] that conjecture [ sl ] does not hold . we provide counter - examples to the conjecture , pointing out that the range of @xmath178 for which the conjecture does not hold increases with snr .",
    "the hermite polynomial corresponding to @xmath179 is @xmath117}=1 $ ] and is clearly not a valid direction as it violates . using the orthogonality property of the hermite basis and since @xmath117}=1 $ ]",
    ", we conclude that @xmath109}$ ] satisfies for any @xmath118 .",
    "however , it is only for @xmath119 even that @xmath109}$ ] satisfies . on the other hand , for any @xmath180",
    ", we have that @xmath109 } + \\delta h_{4k}^{[p]}$ ] satisfies , whether @xmath119 is even or not ( we chose @xmath181 instead of @xmath182 for reasons that will become clear later ) .",
    "now , if we consider the direction @xmath183}$ ] , is not satisfied for both @xmath119 even and odd . but",
    "again , for any @xmath180 , we have that @xmath184 } + \\delta h_{4k}^{[p]}$ ] satisfies .",
    "hence , in order to ensure , we will often work in the proofs with @xmath185 } + \\delta h_{4k}^{[p]}$ ] , although it will essentially allow us to reach the performance achieved by any @xmath186}$ ] ( odd or even ) , since we will then take @xmath187 arbitrarily small and use continuity arguments .",
    "_ convention : _ we drop the variance upper script in the hermite terms whenever a gaussian density with specified variance is perturbed , i.e. , the density @xmath188 always denotes @xmath189 } ) $ ] , and @xmath190 always denotes @xmath191 } $ ] , no matter what @xmath14 is .",
    "same treatment is done for @xmath89 and @xmath192 .",
    "now , in order to evaluate the entropy of a perturbation , i.e. , @xmath193 , we can express it as the entropy of @xmath194 minus the divergence gap , as in , and then use lemma [ appro ] for the approximation .",
    "but this is correct _ if _ @xmath195 has the same first two moments as @xmath65 .",
    "hence , if @xmath72 contains only @xmath116 s with @xmath128 , the previous argument can be used .",
    "but if @xmath72 contains @xmath113 and/or @xmath114 terms , the situation can be different .",
    "next lemma describes this .",
    "[ corr ] let @xmath196 and @xmath197 } =     \\begin{cases }        b(h_k^{[p ] } + \\delta h_{4k}^{[p ] } ) , & \\text{if } b \\geq 0 ,   \\\\",
    "b(h_k^{[p ] } - \\delta h_{4k}^{[p ] } ) , & \\text{if } b < 0 .",
    "\\label{tild }     \\end{cases}\\end{aligned}\\ ] ] we have for any @xmath198 , @xmath199 , @xmath200 @xmath201    finally , when we convolve two perturbed gaussian distributions , we get @xmath202 + { \\varepsilon}^2 g_a h_j \\star g_b h_k.$ ] we already know from theorem [ hermite2 ] how to describe the terms in @xmath74 , what we still need is to describe the term in @xmath203 .",
    "we have the following .    [ new ] we have @xmath204 } \\star g_b h_l^{[b ] } = c g_{a+b } h_{k+l}^{[a+b]},\\ ] ] where @xmath205 is a constant depending only on @xmath206 and @xmath207 .",
    "in particular if @xmath208 , we have @xmath209 .",
    "we start by reviewing the proof of , as it brings interesting facts .",
    "we then prove the main result .",
    "_ proof of : _",
    "+ we first assume that @xmath73 has zero mean and variance @xmath14 . using the hermite basis , we express @xmath72 as @xmath210}$ ] ( @xmath72 must have such an expansion , since it must have a finite @xmath88 norm , to make sense of the original expressions ) . using , we can then express as @xmath211 which is clearly negative .",
    "hence , we have proved that @xmath212 and is maximized by taking @xmath213 .",
    "note that we can get tighter bounds than the one in previous inequality , indeed the tightest , holding for @xmath166 , is given by @xmath214 ( this clearly holds if written as a series like in ) .",
    "hence , locally the contraction property can be tightened , and locally , we have stronger epi s , or worst noise case .",
    "namely , if @xmath215 , we have @xmath216 and if @xmath217 , @xmath65 is outperformed by non - gaussian distributions .",
    "now , if we consider the constraint @xmath218 , which , in particular , allows to consider @xmath219 and @xmath220 , we get that if @xmath221 , @xmath222 and if @xmath223 , @xmath65 is outperformed by @xmath224 for some @xmath225 . it would then be interesting to study if these tighter results hold in a greater generality than for the local setting .",
    "_ proof of theorem [ hermite2 ] : _ + we want to show @xmath226 } \\star g_v = ( \\frac{p}{p+v})^{k/2 } h_k^{[p ] } , \\\\ & g_p h_k^{[p ] } \\star g_v=(\\frac{p}{p+v})^{k/2 } g_{p+v } h_k^{[p+v ] } , \\ ] ] which is proved by an induction on @xmath119 , using the following properties ( appell sequence and recurrence relation ) of hermite polynomials : @xmath227 } ( x)&= & \\sqrt{\\frac{k+1}{p } } h_{k}^{[p ] } ( x ) \\\\ \\frac{\\partial}{\\partial x } \\left ( g_p ( x)h_k^{[p]}(x ) \\right ) & = & -\\sqrt{\\frac{k+1}{p } } g_p(x ) h_{k+1}^{[p]}(x ) . \\ ] ]    _ proof of theorem [ ce ] : _ + we refer to as the mu - rate .",
    "let us first consider gaussian codes , i.e. , when @xmath18 is jointly gaussian , and see what mu - rate they can achieve . without loss of generality , we can assume that @xmath228 , with @xmath22 and @xmath229 independent and gaussian , with respective variance @xmath230 and @xmath231 satisfying @xmath232 . then , becomes @xmath233 now , we pick a @xmath136 and look for the optimal power @xmath231 that must be allocated to @xmath229 in order to maximize the above expression .",
    "we are interested in cases for which the optimal @xmath231 is not at the boundary but at an extremum of , and if the maxima is unique , the optimal @xmath231 is found by the first derivative check , which gives @xmath234 since we will look for @xmath136 , @xmath24 , with @xmath235 , previous condition can be written as @xmath236 we now check if we can improve on by moving away from the optimal jointly gaussian @xmath18 .",
    "there are several ways to perturb @xmath18 , we consider first the following case . we keep @xmath22 and @xmath229 independent , but perturb them away from gaussian s in the following way : @xmath237 } ( u ) + \\delta h_{4 } ) ) \\label{udef}\\\\ & p_{v_{\\varepsilon } } ( v ) = g_r(v ) ( 1-{\\varepsilon}(h_3^{[r]}(v ) - \\delta h_{4 } ) ) \\label{vdef}\\end{aligned}\\ ] ] with @xmath238 small enough .",
    "note that these are valid density functions and that they preserve the first two moments of @xmath22 and @xmath229 .",
    "the reason why we add @xmath239 , is to ensure that is satisfied , but we will see that for our purpose , this can essentially be neglected . then , using lemma [ hermite2 ] , the new distribution of @xmath2 is given by @xmath240 } - { \\varepsilon}\\left(\\frac{r}{p}\\right)^{\\frac{3}{2 } } h_3^{[p ] } ) + f(\\delta)\\end{aligned}\\ ] ] where @xmath241 } +    \\left(\\frac{r}{p}\\right)^{\\frac{4}{2 } } h_4^{[p]})$ ] , which tends to zero when @xmath187 tends to zero .",
    "now , by picking @xmath242 , we have @xmath243 hence , by taking @xmath187 arbitrarily small , the distribution of @xmath2 is arbitrarily close to the gaussian distribution with variance @xmath244 .",
    "we now want to evaluate how these hermite perturbations perform , given that we want to maximize , i.e. , @xmath245 we wonder if , by moving away from gaussian distributions , the gain achieved for the term @xmath246 is higher than the loss suffered from the other terms .",
    "using theorem [ hermite2 ] , lemma [ appro ] and lemma [ corr ] , we are able to precisely measure this and we get @xmath247 } ) ) \\\\ & = \\frac{1}{2 } \\log 2 \\pi e ( v+r h^2 ) - \\frac{{\\varepsilon}^2}{2 }   \\left(\\frac{r h^2}{v+rh^2}\\right)^{3 } + o({\\varepsilon}^2 ) + o(\\delta)\\end{aligned}\\ ] ] @xmath248 and because of @xmath249 therefore , collecting all terms , we find that for @xmath250 and @xmath251 defined in and , expression reduces to @xmath252 where @xmath253 is equal to ( and is the mu - rate obtained with gaussian inputs ) . hence , if for some distribution of @xmath15 and some @xmath24 , we have that @xmath254 when @xmath255 and @xmath231 is optimal for @xmath136 , we can take @xmath74 and @xmath187 small enough in order to make strictly larger than @xmath253 .",
    "we have shown how , if verified , inequality leads to counter - examples of the gaussian optimality , but with similar expansions , we would also get counter - examples if the following inequality holds for any power @xmath119 instead of 3 , as long as @xmath128 .",
    "let us summarize what we obtained : let @xmath231 be optimal for @xmath136 , which means that holds if there is only one maxima ( not at the boarder ) .",
    "then , non - gaussian codes along hermite s strictly outperforms gaussian codes , if , for some @xmath128 , holds .",
    "if the maxima is unique , this becomes @xmath256 where @xmath257 so we want the jensen gap of @xmath258 for the power @xmath119 to be small enough compared to the jensen gap of @xmath259 .",
    "we now give an example of a fading distribution for which the above conditions can be verified .",
    "let @xmath15 be binary , taking values 1 and 10 with probability half and let @xmath260 .",
    "let @xmath261 , then for any values of @xmath244 , the maximizer of is at @xmath262 , cf .",
    "figure [ maxval ] , which corresponds in this case to the unique value of @xmath231 for which is satisfied .     for @xmath261 , @xmath260 , @xmath263 and @xmath15 binary @xmath264 .",
    "maxima at @xmath262.,width=287,height=249 ]    hence if @xmath244 is larger than this value of @xmath231 , there is a corresponding fading bc for which the best gaussian code splits the power on @xmath22 and @xmath229 with @xmath262 to achieve the best mu - rate with @xmath261 . to fit the counter - examples with the choice of hermite perturbations made previously ,",
    "we pick @xmath242 . finally , for these values of @xmath136 and @xmath231 , can be verified for @xmath265 , cf .",
    "figure [ ineq ] , and the corresponding hermite code ( along @xmath266 ) strictly outperforms any gaussian codes .    , for @xmath261 , @xmath260 , @xmath267 and @xmath15 binary @xmath264 , positive at @xmath262.,width=287,height=249 ]",
    "note that we can consider other non - gaussian encoders , such as when @xmath22 and @xmath229 are independent with @xmath22 gaussian and @xmath229 non - gaussian along hermite s .",
    "then , we get the following condition .",
    "if for @xmath128 and @xmath231 optimal for @xmath136 , we have @xmath268 , \\label{weak}\\end{aligned}\\ ] ] then gaussian encoders are not optimal .",
    "notice that previous inequality is stronger than the one in for fixed values of the parameters .",
    "yet , it can still be verified for valid values of the parameters and there are also codes with @xmath22 gaussian and @xmath229 non - gaussian that outperform gaussian codes for some degraded fading bcs .",
    "+ _ proof of theorem [ calc ] : _ + let @xmath269 and let @xmath37 and @xmath38 be respectively distributed as @xmath270)$ ] and @xmath271)$ ] , where @xmath272 .",
    "we have @xmath273 where @xmath274 are independent gaussian 0-mean and @xmath14-variance random variables .",
    "hence , we need to evaluate the contribution of each divergence appearing in previous expression , in order to know if the perturbations are improving on the gaussian distributions .",
    "let us first analyze @xmath275 .",
    "the density of @xmath276 is given by @xmath277 ) \\star g_{a^2 p}(1-{\\varepsilon}[h_k-\\delta h_{4k } ] ) \\star g_1 ,   \\end{aligned}\\ ] ] which , from theorem [ hermite2 ] , is equal to @xmath278 \\\\   & -\\left [ \\left(\\frac{a^2p}{p+a^2 p + 1 } \\right)^{\\frac{k}{2 } } h_k -\\delta \\left(\\frac{a^2p}{p+a^2 p + 1 } \\right)^{2k } h_{4k } \\right ]   \\\\ & - { \\varepsilon}l \\ } ) \\end{aligned}\\ ] ] where @xmath279 \\star g_{a^2 p } [ h_k-\\delta h_{4k } ]   \\star g_1}{g_{p+a^2p+1}}.\\ ] ] note that each direction in each line of the bracket @xmath280 above , including @xmath72 , satisfy and .",
    "using lemma [ new ] , we have @xmath281 \\star g_{a^2 p+1 } [ \\left(\\frac{a^2 p}{a^2 p+1 } \\right)^{\\frac{k}{2}}h_k- \\left(\\frac{a^2 p}{a^2 p+1 } \\right)^{2k } \\delta h_{4k } ]   } { g_{p+a^2p+1 } } \\notag \\\\ & = c_1 h_{2k}^{[p+a^2 p + 1 ] } + c_2 h_{5k}^{[p+a^2 p + 1 ] } + c_3 h_{8k}^{[p+a^2 p + 1 ] } , \\label{bug }    \\ ] ] where @xmath282 are constants .",
    "therefore , the density of @xmath276 is a gaussian @xmath283 perturbed along the direction @xmath116 in the order @xmath74 and several @xmath284 with @xmath285 in the order @xmath203 ( and other directions but that have a @xmath187 order ) .",
    "so we can use lemma [ corr ] and write @xmath286 using lemma [ appro ] , we have @xmath287 \\\\   & -\\left [ \\left(\\frac{a^2p}{p+a^2 p + 1 } \\right)^{\\frac{k}{2 } } h_k -\\delta \\left(\\frac{a^2p}{p+a^2 p + 1 } \\right)^{2k } h_{4k } \\right ] \\|^2\\\\ & =   \\frac{{\\varepsilon}^2}{2 } ( 1 - a^k)^2 \\left(\\frac{p}{p+a^2 p + 1 } \\right)^k + \\frac{{\\varepsilon}^2}{2 } o(\\delta).\\end{aligned}\\ ] ] hence @xmath288 similarly , we get @xmath289 and @xmath290 + \\frac{{\\varepsilon}^2}{2 } o(\\delta ) .\\end{aligned}\\ ] ] finally , we have @xmath291 and @xmath292 + { \\varepsilon}^2 o(\\delta).\\end{aligned}\\ ] ] hence , if for some @xmath293 we have @xmath294 we can improve on the iid gaussian distributions @xmath65 by using the respective hermite perturbations .",
    "now , we could have started with @xmath37 and @xmath38 distributed as @xmath295 and @xmath296 , where @xmath142 is defined in . with similar expansions , we would then get that we can improve on the gaussian distributions if for some some @xmath297 and @xmath272 we have @xmath298 ( b_k^2+c_k^2 ) \\\\    & - 4   \\left(\\frac { a p }",
    "{ p+a^2p+1}\\right)^k     b_k c_k > 0.\\end{aligned}\\ ] ] but the quadratic function @xmath299 with @xmath225 , can be made positive if and only if @xmath300 , and is made so by taking @xmath301 .",
    "hence , the initial choice we made about @xmath37 and @xmath38 is optimal .",
    "moreover , note that for this distribution of @xmath37 and @xmath38 , we could have actually chosen @xmath302 as well . because , even if lemma [ corr ] tells us that we must use correction terms , these correction terms will cancel out when we consider the sum - rate , since @xmath303 and since the correction is in @xmath74 .",
    "there is however another problem when using @xmath302 , which is that @xmath304 has a larger second moment than @xmath14 .",
    "however , if we use a scheme of block length 2 , we can compensate this excess on the first channel use with the second channel use , and because of the symmetry , we can achieve the desired rate .",
    "but this is allowed only with synchronization .",
    "we could also have used perturbations that are mixtures of hermite s , such as @xmath305 .",
    "we would then get mixtures of previous equations as our condition .",
    "but in the current problem this will not be helpful .",
    "finally , perturbing iid gaussian inputs in a independent but non i.d .",
    "way , i.e. , to perturb different components in different hermite directions , can not improve on our scheme , from previous arguments .",
    "the only option which is not investigated here ( but in a work in progress ) , is to perturb iid gaussian inputs in a non independent manner . finally , if we work with @xmath175 , the proof sees the following modification . in",
    ", we now have a term in @xmath114 .",
    "however , even if this term is in the order @xmath203 , we can no longer neglect it , since from lemma [ corr ] , a @xmath306 term in the direction comes out as a @xmath307 term in the entropy .",
    "hence , we do not get the above condition for @xmath175 , but the one obtained by replacing @xmath308 with @xmath309 , and the condition for positivity can never be fulfilled .",
    "+ _ proof of proposition [ f2 ] : _ + from theorem [ calc ] , we know that when treating interference as noise and when @xmath310 , it is better to use encoders drawn from the 2 dimensional distributions @xmath311 and @xmath312 , where @xmath313 , @xmath314 , @xmath315 and @xmath316 , as opposed to using gaussian distributions . but perturbations in @xmath114 are changing the second moment of the input distribution . hence , this scheme is mimicing a time - sharing in our local setting .",
    "moreover , a direct computation also allows to show that , constraining each user to use gaussian inputs of arbitrarily block length @xmath55 , with arbitrary covariances having a trace bounded by @xmath317 , the optimal covariances are @xmath318 if @xmath319 , and otherwise , are given by a time - sharing scheme ( cf .",
    "definition [ ts ] for the definition of a gaussian time - sharing scheme and covariance matrices ) .",
    "+ _ proof of proposition [ b2 ] : _ + note that when using blind time - sharing , no matter what the delay in the asynchronization of each user is , the users are interfering in @xmath320 channel uses and have each a non - intefering channel in @xmath320 channel uses ( the rest of the @xmath320 channel uses are not used by any users ) . hence ,",
    "if the receiver have the knowledge of the asynchronization delay , the following sum - rate can be achieved : @xmath321 . and",
    "if the delay is unknown to the receivers , the previous sum - rate can surely not be improved on .",
    "+ _ disproof of conjecture [ sl ] : _ + this proof uses similar steps as previous proofs . using lemma",
    ", we express @xmath49 as @xmath322 we then pick @xmath323 and assume that @xmath119 is even for now .",
    "we then have @xmath324 and @xmath325 similarly , @xmath326 and @xmath327 finally , @xmath328 and @xmath329 therefore , is given by @xmath330 and if @xmath331 for some @xmath119 even and greater than 4 , we have a counter example to the strong conjecture .",
    "note that , using the same trick as in previous proofs , that is , perturbing along @xmath142 instead of @xmath116 , we get that if holds for any @xmath128 , we have a counter example to the strong conjecture .",
    "defining @xmath332 , is equivalent to @xmath333 as shown in figure [ slplot ] , this can indeed happen .",
    "an interesting observation is that the range where holds is broader when @xmath334 is larger , i.e. , when snr is smaller . indeed , when @xmath335 , which corresponds to dropping the additive noise @xmath336 , we do not get a counter - example to the conjecture .",
    "but in the presence of gaussian noise , the conjecture does not hold for some distributions of @xmath337 .",
    "the conjecture had been numerically checked with binary inputs at low snr , and in this regime , it could not be disproved . with the hint described above",
    ", we checked numerically the conjecture with binary inputs at high snr , and there we found counter - examples .",
    "we have developed a technique to analyze codes drawn from non - gaussian ensembles using the hermite polynomials .",
    "if the performance of non - gaussian inputs is usually hard to analyze , we showed how with this tool , it reduces to the analysis of analytic power series .",
    "this allowed us to show that gaussian inputs are in general not optimal for degraded fading gaussian bc , although they might still be optimal for many fading distributions .",
    "for the ic problem , we found that in the asynchronous setting and when treating interference as noise , using non - gaussian code ensembles ( @xmath166 perturbations ) can strictly improve on using gaussian ones , when the interference coefficient is above a given threshold , which significantly improves on the existing threshold ( cf .",
    "we have also recovered the threshold of the moderate regime by using @xmath114 perturbations in the synch setting , showing that this global threshold is reflected in our local setting .",
    "we also met mysteriously in our local setting the other global threshold found in @xcite , below which treating interference as noise with iid gaussian inputs is optimal .",
    "it is worth noting that this two global thresholds ( moderate regime and noisy interference ) are recovered with our tool from a common analytic function .",
    "we hope to understand this better with a work in progress .",
    "the hermite technique provides not only counter - examples to the optimality of gaussian inputs but it also gives insight on the competitive situations in gaussian network problems .",
    "for example , in the fading bc problem , the hermite technique gives a condition on what kind of fading distributions and degradedness ( values of @xmath24 ) non - gaussian inputs must be used .",
    "it also points out that the perturbation in @xmath166 are most effective when carried in an opposite manner for the two users , so as to make the distribution of @xmath2 close to gaussian .",
    "finally , in a different context , local results could be `` lifted '' to corresponding global results in @xcite .",
    "there , the localization is made with respect to the channels and not the input distribution , yet , it would be interesting to compare the local with the global behavior for the current problem too . the fact that we have observed some global results locally , as mentioned previously ,",
    "gives hope for possible local to global extensions .",
    "a work in progress aims to use our tool beyond the local setting , in particular , by analyzing all sub - gaussian distributions .",
    "moreover , there are interesting connections between the results developed in this paper and the properties of the ornstein - uhlenbeck process .",
    "indeed , some of these properties have already been used in @xcite to solve the long standing entropy monotonicity conjecture , and we are currently investigating these relations from closer .",
    "the authors would like to thank tie liu and shlomo shamai for pointing out problems relevant to the application of the proposed tool , daniel stroock for discussions on hermite polynomials , and emre telatar for stimulating discussions and helpful comments .    1 e. abbe and l. zheng , `` linear universal decoding : a local to global geometric approach '' , _ submitted ieee trans .",
    "inform . theory _",
    "annapureddy and v.v .",
    "veeravalli , `` gaussian interference network : sum capacity in the low interference regime and new outer bounds on the capacity region '' , _ submitted ieee trans .",
    "inform . theory _",
    ", 2008 .",
    "m. costa , `` on the gaussian interference channel , '' ieee trans .",
    "theory , vol .",
    "it-31 , pp .",
    "607 - 615 , sept .",
    "1985 . t. m. cover , `` comments on broadcast channel '' , _ ieee trans . on inform .",
    "44 , n. 6 , pp .",
    "2524 - 2530 , oct . 1998",
    ". t. m. cover and j. a. thomas , `` elements of information theory '' , john wiley & sons , new york , ny , 1991 .",
    "stam , `` some inequalities satisfied by the quantities of information of fisher and shannon '' , _ information and control _ , 2 : 101 - 112 , 1959 .",
    "d. w. stroock , `` a concise introduction to the theory of integration '' , third edition , birkhuser , 1999 ."
  ],
  "abstract_text": [
    "<S> this paper studies network information theory problems where the external noise is gaussian distributed . </S>",
    "<S> in particular , the gaussian broadcast channel with coherent fading and the gaussian interference channel are investigated . </S>",
    "<S> it is shown that in these problems , non - gaussian code ensembles can achieve higher rates than the gaussian ones . </S>",
    "<S> it is also shown that the strong shamai - laroia conjecture on the gaussian isi channel does not hold . in order to analyze non - gaussian code ensembles over gaussian networks , </S>",
    "<S> a geometrical tool using the hermite polynomials is proposed . </S>",
    "<S> this tool provides a coordinate system to analyze a class of non - gaussian input distributions that are invariant over gaussian networks . </S>"
  ]
}