{
  "article_text": [
    "this paper proposes a method of finding a sequence of improving models for a given fixed data set that we call continuous learning .",
    "the method is based on iterative exploration of a space of models that have a specific limited number of parameters , which correspond to non - linear polynomial features of an input space .",
    "the feature set is evolving with each iteration , so the most important features are selected from the current feature set then the reduced feature set is expanded to include higher degree polynomials while the dimension of the expanded feature space is limited or fixed .",
    "resulting features are computed recursively from iteration to iteration with different parameters of the recursions found for each execution of the iteration algorithm .",
    "the paper is organized as follows :    to search a model space we need to compare different models and solutions .",
    "we use bayesian framework that provides a natural way for model comparison @xcite .",
    "continuous learning consists of a sequence of iteration cycles .",
    "each iteration cycle has a number of steps . in first of these steps ,",
    "we use bootstrap method to obtain a set of sampled solutions @xcite and parameter - feature duality as a method for exploring the feature space .",
    "we construct a model of a solution distribution and use principal component analysis to select a subspace of the most important solutions and reduce dimensions of the parameter space .",
    "then we use non - linear expansion of the feature space by adding tensor - features that are products of the principal features selected in the previous step . that concludes a definition of one iteration cycle .",
    "each iteration is recursively redefining features that become non - linear functions on the original feature space .",
    "we analyzed a stationary solution of the iteration cycle and found that , in a limit of the infinite number of iterations , features form a feature algebra .",
    "different solutions of feature algebra define non - linear feature representations .    for the purpose of this work",
    ", we will consider a prediction problem setup .",
    "the goal is to find a probability of a class label @xmath2 for a given input @xmath3 : @xmath4 .",
    "the model class family is defined by a conditional probability function @xmath5 , where @xmath6 is a parameter vector . for a set of independent training data samples @xmath7 ,",
    "the probability of labels for given inputs is defined by the bayesian integrals    @xmath8    @xmath9    @xmath10    where @xmath11 is a prior probability of parameters @xmath6 that guarantees existence and convergence of the bayesian integrals in equations [ eq : bayesianintegral1 ] and [ eq : bayesianintegral2 ] and it is normalized as follows :    @xmath12    then the bayesian integral in the equation [ eq : bayesianintegral2 ] is equal to a probability of labels @xmath13 for the given input vectors @xmath14 of the training set .",
    "the prior distribution @xmath15 itself depends on some parameters @xmath16 ( hyper - parameters or regularization parameters ) .",
    "then the bayesian integrals in the equations [ eq : bayesianintegral1 ] and [ eq : bayesianintegral2 ] also depend on regularization parameters @xmath16",
    ". optimal values of the regularization parameters could be found by maximizing the probability of the training data given by the bayesian integral in equation [ eq : bayesianintegral2 ] .",
    "this is possible because due to normalization of the prior probability in the equation [ eq : priorprobnormalization ] , the bayesian integrals include the contribution of the normalization factor that depends only on regularization parameters @xmath16 .",
    "the regularization solution found by maximizing the probability of training data is equivalent to the solution for regularization parameters found by cross - validation .    for the purpose of this paper ,",
    "it is sufficient to estimate values of the bayesian integrals in the equations [ eq : bayesianintegral1 ] and [ eq : bayesianintegral2 ] using maximum likelihood approximation ( mla ) by finding a solution @xmath17 that maximizes log - likelihood of the training set that includes the prior probability of parameters @xmath6 :    @xmath18    then the bayesian integral can be approximated by the maximum likelihood as follows    @xmath19",
    "the training data are always a limited - size set or a selection from a limited - size set . for that reason it contains a sampling noise .",
    "the sampling noise affects solutions .",
    "this is easy to see by sub - sampling the training data : for each sampled training set the mla solution is different in the equation [ eq : mlasolution ] as well as the value of the bayesian integral @xmath20 in the equation [ eq : mlaintegral ] .",
    "our goal is to find a solution that is the least dependent on the sampling noise and better represents an actual statistics of a source .    to achieve",
    "that , we can sample the training data to create a set of the training sets @xmath21 and then find an mla solution @xmath22 for each sampled training set @xmath23 .",
    "@xmath24    now we have a set of solutions @xmath25 which is a noisy representation of a source statistics .",
    "the probability of each solution is given by the value of the bayesian integral on the solution and is equal to    @xmath26    the solution distribution in the equation [ eq : solutionprob ] is based on sampling of the original training data set .",
    "it is a variant of a bootstrap technique @xcite .",
    "this type of methods is actively used in different forms to improve training and to find a robust solution , that is less dependent on a sampling noise in the original training set .",
    "for example , see the dropout method that randomly omits hidden features during training @xcite or the method of adding artificial noise or a corruption to a training data @xcite .    instead of trying to find a single best solution we use the bootstrap method here to obtain a distribution of solutions .",
    "let s consider the set of solutions in the equation [ eq : mlasolutions ] as samples from unknown distribution of solutions , where each solution @xmath22 has a weight @xmath27 and the probability of the solution is given by the equation [ eq : solutionprob ] .",
    "then we can model this distribution of solutions by proposing a probability function @xmath28 with parameters @xmath29 , which we can find by maximizing by @xmath29 the following log - likelihood    @xmath30    up until now we did not specify the model class distribution @xmath31 .",
    "for the following consideration , we will use logistic regression for model class distribution with binary label @xmath32 as follows    @xmath33    where @xmath34 is a product of the parameter vector @xmath35 with the feature vector @xmath36 which includes a bias feature 1 .",
    "the use of the logistic regression here is not a limitation on the possible models .",
    "it is selected only for certainty and to avoid an unnecessary complication of the consideration . as we will see the following approach is applicable to any model class distribution that is a function of a scalar product of a feature vector and a parameter vector .",
    "also it could be used for a model class distribution that is a function of multiple products of a feature vector and parameter vectors .    using the equation [ eq : modellogit ] we will find a set of solutions defined in the equation [ eq : mlasolutions ] for each corresponding training set @xmath37 .    to model the distribution of solutions we will start by considering gaussian model for the distribution of solutions @xmath38",
    ".    then the model is defined by mean    @xmath39    and covariance matrix    @xmath40 @xmath41    we will use principal component analysis ( pca ) to separate important solutions from noise .",
    "that leads to the following representation of the parameter vector @xmath6 :    @xmath42    where @xmath43 and @xmath44 are selected eigenvectors of the covariance matrix @xmath45 indexed by @xmath46 .",
    "the coordinates @xmath47 span over the principal - component subspace that is defined by selected eigenvectors .",
    "the selected eigenvectors correspond to a high - variance subspace , where eigenvalues of the covariance matrix @xmath45 are larger than a certain threshold .",
    "the value of the threshold for selecting the principal components is a hyper - parameter that controls the dimension of the principal component space , which in practice is constrained by the available memory .",
    "the important property of the model class probability distribution is a parameter - feature duality : the parameters @xmath6 for the model class distribution @xmath48 are used only in a product form    @xmath49    where @xmath36 are the original features .    by considering solutions that are limited to the principal component space we can find that the product is given by the following equation    @xmath50    we will have exactly the same product form here as in the equation [ eq : origproduct ] when we will define new features @xmath51 via original features @xmath52",
    "as follows    @xmath53    so the parameter - feature product will look like this    @xmath54    where now @xmath55 and @xmath47 are new parameters for the model class distribution with re - defined super - features @xmath56 from the equation [ eq : redefinedfeatures ] .",
    "the result of this step is that using pca and redefining features we reduced the original parameter space to a new smaller space .",
    "let s now extend the parameter - feature space by adding products of the super - features    @xmath57    by extending the feature space in the equation [ eq : extendingfeatures ] , we increased the dimension of the parameter space by adding new parameters @xmath58 and creating new features as non - linear ( quadratic ) functions of the previous features .",
    "now we can repeat the iteration cycle , which consists of the steps in the table [ tab : iteration ] .",
    ".iteration cycle [ cols= \" < , < \" , ]     it is important to emphasize that    at each iteration we have a model that is defined on a new feature space and has a limited defined number of dimensions in its parameter space .    at each iteration",
    "the feature space is a non - linear map of the original feature space .",
    "each iteration makes new super - features to be higher degree polynomials of the original basic features .",
    "after @xmath0 iterations new features are @xmath1-degree polynomials of the original features .",
    "the expansions of the feature set by adding products of features were used in recently proposed sum - product networks @xcite and neural tensor networks @xcite .",
    "to simplify notations , let s allow the feature indices @xmath59 to include value @xmath60 .",
    "then the equation [ eq : extendingfeatures ] will look like this    @xmath61    the iterations will converge when the product of super - features @xmath62 in the equation [ eq : extendingfeaturesnewalpha ] could be expressed only as a linear combination of the super - features    @xmath63    where for @xmath64 the super - feature @xmath65 is the bias super - feature @xmath66 .",
    "the equation [ eq : falgebra ] defines a feature algebra with structure constants @xmath67 .",
    "the feature algebra has following important properties :    1 .",
    "it must be associative : + @xmath68 + that property leads to major equations for structure constants : + @xmath69 2 .",
    "the super - feature space with the feature algebra is a complete linear vector space : due to the algebra , any function @xmath70 on the super - feature space representable by power series is equal to a linear combination of the super - features with computable coefficients @xmath71 : + @xmath72    the feature algebra defined by the equation [ eq : falgebra ] is not limited to polynomial functions , it could be any function set that satisfies the algebra equation [ eq : falgebra ] with structure constants that are a solution of the equation [ eq : structureconstants ] .",
    "simple examples of algebras that are defined by equations [ eq : falgebra ] and [ eq : structureconstants ] are complex numbers and quaternions .",
    "less trivial examples of such algebras are operator algebras that were successfully used in statistical physics of phase transitions and quantum field theory .",
    "we proposed an iterative procedure for generating non - linear features ( super - features ) that are high - degree polynomials on the original feature space after a finite number of iterations .      by selecting a small set of principal components ,",
    "the dimensionality of a feature space is limited at each iteration while resulting super - features are highly non - linear ( as polynomials of exponentially high with number of iterations degree ) .",
    "that contrasts with an approach when high - degree polynomials are used as the original features - which requires to find a solution for an exponentially high - dimensional model ."
  ],
  "abstract_text": [
    "<S> in this paper we consider a problem of searching a space of predictive models for a given training data set . </S>",
    "<S> we propose an iterative procedure for deriving a sequence of improving models and a corresponding sequence of sets of non - linear features on the original input space . after a finite number of iterations @xmath0 , </S>",
    "<S> the non - linear features become @xmath1-degree polynomials on the original space . </S>",
    "<S> we show that in a limit of an infinite number of iterations derived non - linear features must form an associative algebra : a product of two features is equal to a linear combination of features from the same feature space for any given input point . </S>",
    "<S> because each iteration consists of solving a series of convex problems that contain all previous solutions , the likelihood of the models in the sequence is increasing with each iteration while the dimension of the model parameter space is set to a limited controlled value . </S>"
  ]
}