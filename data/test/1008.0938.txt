{
  "article_text": [
    "zipf s law is one of the most common power laws found in nature and society @xcite .",
    "although it was early observed in the distribution of money income @xcite and city sizes @xcite , it was popularized by the linguist george kingsley zipf , who observed that it accounts for the frequency of words within written texts @xcite .",
    "specifically , if we rank all the occurrences of words in a text from the most common to the least , zipf s law states that the probability @xmath0 that in a random trial we find the @xmath1-th most common word ( @xmath2 ) falls off as @xmath3 where @xmath4 with @xmath5 .",
    "the ubiquity of this scaling behavior suggested several mechanisms to account for the emergence of this distribution , among many others , see @xcite .",
    "in which eq .",
    "( [ symmetryeq ] ) holds is depicted . in ( b ) we summarize the evolution rules of our communicative system .",
    "suppose that symmetry between coder and decoder -i.e .",
    ", eq . ( [ symmetryeq])- holds for the step @xmath6 ( above ) . at each step ( below ) a new element is added to the set @xmath7 and eq .",
    "( [ symmetryeq ] ) holds again for this new configuration .",
    "furthermore , the new configuration is constrained by the @xmath8 , which introduces a path dependency in the evolutionary process .",
    ", width=321 ]    within the context of human language , g. k. zipf early conjectured that this scaling law is the outcome of a tension between two _ forces _ acting in a communication system @xcite .",
    "following zipf s proposal , speakers and hearers need to simultaneously minimize their efforts , under what he called _ vocabulary balance _ , a particular case of the so - called _ principle of least effort_. this triggers a tension between the two communicative agents , while trying to simultaneously minimize their efforts .",
    "the speaker s economy would favour a reduction of the size of the vocabulary to a single word whereas the hearer s economy would lead to an increase of the size of a vocabulary to a point where there would be a different _ word _ for each _",
    "meaning_. the resulting vocabulary would emerge out of this unification - diversification conflict @xcite .",
    "although both numerical and theoretical studies have explored this idea @xcite , no truly analytic proof of unicity has been provided under realistic , information - theoretic constraints .",
    "we can view the proposals made in @xcite as _ static _ for they consider a fixed size of the code .    a recent approach -which goes beyond the communicative framework- defined the key complexity properties of a system to display a statistics of events following zipf s law : an open , unbounded number of accessible states and a linear loss of entropy due to generic internal constraints @xcite .",
    "the linear loss of entropy grasps the intuitive idea that the studied systems are in an _ intermediate state _ between order and disorder -or that a possible informative tension is balanced , as we shall see- and the unbounded number of accessible states reflects their open nature .",
    "it was shown that , under a very general parametrization , and imposing properties of scale - invariance to the solution , zipf s law was the only possible outcome .",
    "now we adapt and enrich the general framework proposed in @xcite to the communicative context .",
    "as we shall see , zipf s hypothesis can be interpreted in such a way that the system can be studied within the framework proposed in @xcite . moreover ,",
    "the parameters that were arbitrary in the general mathematical framework mentioned above can now be naturally interpreted in the communicative framework as the key pieces of the mathematical statement of zipf s hypothesis .    beyond the mathematical formalization of the communicative conflict described by zipf , we need another ingredient , pointed out -in a different context- in @xcite , namely the active role played by the evolutionary path followed by the code . as it occurs with other systems growing out of equilibrium , such as scale - free networks @xcite",
    ", we will consider the evolution of the communicative exchange under system s growth . here",
    "the evolutionary component is variationally introduced by minimizing the divergence between code configurations belonging to successive time steps .",
    "this minimal change follows the so - called _ minimum discrimination information principle _ ( henceforth @xmath8 ) , a general variational principle considered analogous to the maximum entropy principle @xcite , from which statistical mechanics can be properly formalized @xcite .",
    "the @xmath8 states that , under changes in the constraints of the system , the most expected probability distribution is the one minimizing the kullback - leibler divergence ( also referred as to _ kullback - leibler entropy _ or _ relative entropy _ ) from the original one @xcite .",
    "such a variational principle constrains the changes of the internal configurations of an statistical ensemble when the external conditions change in the same way that internal configurations of an statistical ensemble change when we introduce moment constraints in a jaynesian formalism . in our context , this information theoretic functional assumes the role of a lagrangian whose minimization along the process defines the possible ensemble configurations one can observe at a certain point of an evolutionary path . using the @xmath8 and the framework provided in @xcite , we provide a proof of unicity for the emergence of zipf s law in evolving codes .",
    "we stress that no arbitrary assumptions are made on the nature of solutions .",
    "the remainder of the paper is structured as follows : in section ii we rigorously define the communicative tension intuitively defined by zipf and explicitly characterize the evolutionary process in terms of the mathematical statement of such a tension . in section iii",
    "we apply the @xmath8 as the guiding , variational principle which accounts for the possible evolutionary paths of the code . finally , we demonstrate that the consequences of the application of both the communicative tension and the @xmath8 account for the emergence of zipf s law as the unique possible solution of the evolving code . in section",
    "iv we discuss the implications of our results .",
    "in this section we mathematically define @xmath9 the communicative tension described by zipf and @xmath10 the evolution or growth of a given code subject to such a tension .",
    "we furthermore define the range of application of our formalism .",
    "as we shall see in section iii , the proposal made in this section defines a framework whose key piece to work with is eq .",
    "( [ unboundednessxs ] ) .",
    "the first task is to properly define the communicative tension between the coder and the decoder and how this tension is solved .",
    "following the standard nomenclature used in studies of the evolution of communicating , autonomous agents @xcite , in our system there are two agents : the coder agent , @xmath11 , encoding information from a set of external events , @xmath12 , and the decoder or external observer , which infers the behavior of @xmath12 through the code provided by the coder agent @xmath11 . in this way",
    ", @xmath13 is the set of external events acting as the input alphabet , and @xmath14 is the set of signals or output alphabet . the coder module @xmath11 -fig .",
    "( [ zipf1]a)- is fully described by a matrix @xmath15 , where @xmath16 is a random variable taking values on the set @xmath12 following the probability measure @xmath17 ; being @xmath18 the probability to have symbol @xmath19 as the input in a given computation .",
    "complementarily , @xmath20 is a random variable taking values on @xmath21 and following the probability distribution @xmath22 which , for a given @xmath23 , reads : @xmath24 i.e. , the probability to obtain @xmath25 as the output of a codification .",
    "we assume that @xmath26 for the decoder agent inferring the input set from the output set with least effort , the best scenario is a one - to - one mapping between @xmath7 and @xmath27 . in this case",
    ", @xmath11 generates an unambiguous code , and no supplementary amount of information to successfully reconstruct @xmath28 is required .",
    "however , from the coding device perspective , this coding has a high cost . in order to characterize this conflict ,",
    "let us properly formalize the above intuitive statement : the decoder agent wants to reconstruct @xmath16 through the intermediation of the coding performed by @xmath11 .",
    "therefore , the amount of _ bits _ needed by the decoder of @xmath20 to unambiguously reconstruct @xmath16 is @xmath29 which is the _ joint shannon entropy _ or , simply , _",
    "joint entropy _ of the two random variables @xmath30 .. ] from the codification process , the decoder receives @xmath31 bits , and thus , the remaining uncertainty it must face will be @xmath32 where @xmath33 ( i.e , the _ entropy _ of the random variable @xmath20 ) and @xmath34 the _ conditional entropy _ of the random variable @xmath16 conditioned to the random variable @xmath20 .",
    "the tension between the coder and the decoder is solved by imposing a symmetric balance between its associated _ efforts _ -see fig .",
    "( [ zipf1]a)- , i.e. : the coder sends as many bits as the additional bits the observer needs to perfectly reconstruct @xmath16 : @xmath35 the above _ ansatz _ is the mathematical formulation of the symmetric balance between the efforts of the coder and the decoder",
    ". we will refer to this equation as the _ symmetry condition _ and , as pointed out in @xcite , it mathematically describes how the communicative tension is solved by using a cooperative strategy between the coder and the decoder agents .",
    "it is worth noting that different equations sharing the same spirit were formerly proposed , within the framework of the so - called _ code - length _",
    "game @xcite . from eq . ( [ symmetryeq ] )",
    ", we can state that : @xmath36 and knowing the classical inequalities @xmath37 we reach a general relation between the informative richness of the input variable @xmath38 and the informative richness of the messages sent by the coder , constrained by eq .",
    "( [ symmetryeq ] ) : @xmath39 the first relation becomes equality only in the case of @xmath11 performing a deterministic codification process . the second relation becomes equality when the coding device performs completely random associations .",
    "it is clear that eqs .",
    "( [ symmetryeq ] ) and ( [ generalx(s ) ] ) alone can not explain the emergence of zipf s law since one could tune the parameters of , say , an exponential distribution to reach the desired relation between entropies .",
    "therefore we need to introduce another ingredient to obtain zipf s law as the unique possible solution of our problem .",
    "the unicity in the solution is provided by the evolution , which is now explicitly introduced -see fig ( [ zipf1]b ) .",
    "let us suppose that our communicative success grows over time , thereby increasing the number of input symbols that @xmath40 can encode .",
    "formally , this implies that the cardinality of the set @xmath7 defined above increases .",
    "we introduce this feature by defining a sequence of @xmath12 s @xmath41 satisfying an inclusive ordering , i.e. , @xmath42 which is introduced , without any loss of generality , assuming that @xmath43 at time step @xmath6 , @xmath40 will be able to process the @xmath6 symbols of @xmath44.the elements @xmath45 are members of some infinite , countable set @xmath46 , i.e. , @xmath47 .",
    "@xmath46 can be understood , using a thermodynamical metaphor , as a _ reservoir of information_. following this characterization , we say that for every set @xmath48 there is a random variable @xmath49 , taking values in @xmath48 following the ordered probability distribution @xmath50 . furthermore , we assume that exists a unique @xmath51 such that @xmath52 , @xmath53 this means that the entropy of the input set is unbounded when its size increases , which implies that the potential input set @xmath46 acts as an _ infinite _ reservoir of information .",
    "the behavior of the output set at the stage @xmath6 is described by a random variable @xmath54 , which follows the ordered probability distribution @xmath55 , as defined in eq .",
    "( [ nua ] ) , taking values on @xmath56 .",
    "we observe that @xmath57 , defining a sequence @xmath58 also ordered by inclusion . at every time step ,",
    "the consequences of the symmetry condition -see eq . ( [ symmetryeq])- depicted in eq .",
    "( [ generalx(s ) ] ) are satisfied , which implies that the sequence @xmath59 also satisfies the convergence ansatz made over the sequence of normalized entropies of the input -see eq . ( [ unboundedness ] ) .",
    "the only difference is the value of the limit , @xmath60 .",
    "the value of @xmath60 can be bounded by using eqs ( [ generalx(s ) ] ) and ( [ unboundedness ] ) , thereby obtaining : @xmath61 therefore , in this case , by virtue of eqs .",
    "( [ generalx(s ) ] ) , ( [ unboundedness ] ) and ( [ 1/2mu ] ) , the convergence condition for the normalized entropies of the sequence of random variables @xmath62 reads : exists a unique @xmath63 such that @xmath64 : @xmath65 the above equation depicts two crucial facts in the forthcoming derivations : if the potential informative richness of the input set is unbounded , so is the informative richness of the output set , under the constraints imposed by the symmetry condition -see eq . ( [ symmetryeq ] ) .",
    "the @xmath8 is presented in this section as the variational principle guiding the evolution of the code .",
    "as we shall see at the end of this section , the consequences of its application result in a proof of unicity for the emergence of zipf s law in evolving codes .",
    "the question is thus how the probability distribution @xmath55 evolves along the growth process . under the @xmath8",
    "we face a variational problem which is stated as follows : during the growth process , the most likely code at step @xmath66 is the one minimizing the _ distance _ with respect to the code at step @xmath6 , consistently with the @xmath8 .",
    "furthermore , the evolution of the code must satisfy , along all the evolutionary steps , the symmetry condition depicted by eq .",
    "( [ symmetryeq ] ) .",
    "the crucial contribution of the @xmath8 is that it naturally introduces the footprints of the path dependence imposed by evolution . following the thermodynamical metaphor , this variational principle acts , in our context , as a principle on energy minimization acting over the transitions of successive codes . putting it formally ,",
    "let @xmath67 be the _ kullback - leibler _ divergence of the distribution @xmath68 with respect the distribution @xmath55 @xcite .",
    "therefore , the @xmath8 is achieved by minimizing the following functional @xcite : @xmath69 we observe that this functional has a role equivalent to the one attributed to the lagrangian function in a given continuous , differentiable system ; therefore , the _ trajectories _ minimizing it will govern the evolution of the system . furthermore , the symmetry condition on coding / decoding -eq .",
    "( [ symmetryeq])- imposes that the solutions must lie in the region defined by eq .",
    "( [ unboundednessxs ] ) .",
    "the minimum of @xmath70 is found when @xmath68 satisfies : @xmath71 being @xmath72 the lagrange multiplier , which is a positive , unique constant for all elements of the probability distribution @xmath68 .",
    "we observe that , for @xmath73 , @xmath74 , but , in this case , @xmath75 , in contradiction to the assumption provided by eq .",
    "( [ unboundednessxs ] ) , according to which informative richness grows during the evolutionary process .",
    "( @xmath76 ) obtained by constraining the growth process with i)the consequences of the symmetry of coding / decoding -see eq . ( [ symmetryeq])- provided by eq .",
    "( [ unboundednessxs ] ) and ii ) the application of the @xmath8 at every step of the growth process .",
    "different convergence values are studied : a ) @xmath77 , b ) @xmath78 and c ) @xmath79 .",
    "the dashed lines are the best fit interpolations , which give estimated exponents @xmath80 and @xmath81 , respectively ( all with correlation coefficients @xmath82 ) .",
    ", width=283 ]    now we want to find the asymptotic behavior of @xmath83 under the above justified conditions ( [ unboundednessxs ] ) and ( [ elambdadistrib ] ) .",
    "the key feature we derive from the path dependency in the evolution imposed by the @xmath8 is that the following quotient @xmath84 does not depend on @xmath6 .",
    "therefore , along the evolutionary process , as soon as @xmath85 @xmath86 remains invariant .      the asymptotic behavior of quotient @xmath87 and , thus , the tail of @xmath55 , is strongly constrained by the entropy restriction provided by eq .",
    "( [ unboundednessxs ] ) @xcite . as shall see",
    ", the key of the forthcoming derivations will be the convergence properties of the normalized entropies of a given random variable @xmath88 having @xmath6 possible states whose ( ordered ) probabilities follow a power - law distribution function , namely @xmath89 .",
    "the explicit form of these entropies is : @xmath90 consistently , @xmath91 is the normalization constant .",
    "the first observation is that it can be shown that the convergence properties of the riemann @xmath92-function on @xmath93 @xcite @xmath94 strongly constrain the convergence properties of a given probability distribution @xcite .",
    "indeed , we find that , if @xmath95 such that : @xmath96 then @xmath97 , which contradicts the assumptions of the problem , depicted by eq .",
    "( [ unboundednessxs ] ) . indeed ,",
    "primarily , one can observe that the above statement implies that @xmath55 is _ dominated _ by a power - law having exponent @xmath98 , i.e. that @xmath55 decays faster than @xmath99 , defined as : @xmath100 where @xmath101 is the normalization constant .",
    "now , we write the explicit form of the entropy of @xmath102 -to be written as @xmath103- when @xmath104 by multiplying the expression derived in eq .",
    "( [ h(y ) ] ) by @xmath105 : @xmath106 we observe that all the elements of the above equation are finite constants , since @xmath107 thus , having @xmath99 as defined above , @xmath108 therefore , during the growth process , due to the constraint imposed by eq .",
    "( [ unboundednessxs ] ) , @xmath109 with @xmath110 arbitrarily small , provided that @xmath6 can increase unboundedly . otherwise , its normalized entropy -see eq .",
    "( [ h(y)])- will have , as an asymptotic value @xmath111 in contradiction to the assumption that @xmath112 as depicted in eq .",
    "( [ unboundednessxs ] ) .    furthermore , we observe that , if @xmath113 such that @xmath114 then @xmath115 again in contradiction to eq .",
    "( [ unboundednessxs ] ) , except in the extreme , pathological case where @xmath116 , when the coding process is completely noisy . to see how we reach this latter point we observe that statement ( [ dominated ] )",
    "implies that @xmath55 is _ not dominated _ by a power - law probability distribution @xmath99 having exponent @xmath117 , namely : @xmath118 where @xmath119 is the normalization constant .",
    "putting explicitly the expression of the normalized entropy -see eq . ( [ h(y)])- for the random variable @xmath120 , one obtains : @xmath121 which is the desired result .",
    "accordingly , since from eq .",
    "( [ unboundednessxs ] ) @xmath60 is generally different from @xmath122 , @xmath123    thus , combining eq .",
    "( [ > 1+d ] ) and ( [ < 1-d ] ) , we have shown that the asymptotic solution is bounded by the following chain of inequalities :",
    "@xmath124 the crucial step is that it can be shown that , if @xmath125 , we can set @xmath126 ( the mathematical technicalities of this result can be found in @xcite . )",
    "this implies , in turn , that , for @xmath127 : @xmath128 and , from the definition of @xmath87 provided in eq .",
    "( [ f(m , m ) ] ) , we conclude that @xmath129 thereby leading us to zipf s law as the unique asymptotic solution .    in fig .",
    "( [ zipf2 ] ) we numerically explored the behavior of the rank probability distribution of signals belonging to a growing code under the assumption of symmetry in coding / decoding provided by eqs .",
    "( [ symmetryeq ] ) and ( [ unboundednessxs ] ) , and the @xmath8 whose consequences in the evolution of @xmath55 are depicted in eq .",
    "( [ elambdadistrib ] ) .",
    "the outcome perfectly fits with the mathematical derivations , showing very well - defined power - laws with exponents close to @xmath122 although the convergence values @xmath60 diverge from @xmath130 to @xmath131 .",
    "this numerical validation shows that the predicted asymptotic effects -i.e . , the convergence of @xmath55 to zipf s law- are perfectly appreciated even in finite simulations where @xmath132 signals are at work .",
    "we end this section with a remark on the boundary conditions needed for the emergence of zipf s law . in the section [ evolution ]",
    ", we imposed that the potential information richness of the source must be unbounded . such a condition",
    "is mathematically stated by ( [ unboundedness ] ) .",
    "we observe that , more than an assumption , equation ( [ unboundedness ] ) is a boundary condition under which a growing code can ( assymptotically ) exhibit zipf s law ) depicts a linear relation between @xmath133 and @xmath105 ; i.e. : @xmath134 .",
    "there are strong reasons to believe that one could generalize this statement by saying that the only condition needed is that , in spite that @xmath135 if @xmath133 is a monotonous , growing and unbounded function on @xmath6 , then zipf s law would emerge using similar arguments to the ones used in this paper .",
    "the lack of a rigorous demonstration for this latter point forces us to restrict our arguments to the region of application of eq .",
    "( [ unboundedness ] ) . ] . in this way , since @xmath136 has a linear relation with @xmath133 , the divergence of the latter implies the divergence of the former . and",
    "it is a required condition , since the entropy of a system exhibiting a power law with an exponent equal to @xmath122 diverges with @xmath6 .",
    "otherwise , exponents are higher , or other probability distributions can emerge .",
    "the results provided in our study define a general rationale for the emergence of zipf s law in the abundance of signals of evolving communication systems . the variational approach taken here as a formal picture of the least effort hypothesis has two ingredients .",
    "first , starting from zipf s conjecture , we reach a static symmetry equation to solve the communicative tension between coder and decoder .",
    "this is consistent with previous work , but reveals itself insufficient to derive zipf s law as the unique solution , for it is easy to check that _",
    "static _ equations of the kind of eq .",
    "( [ symmetryeq ] ) and ( [ generalx(s ) ] ) have infinite arbitrary solutions , even in the asymptotic regime , due to the possible parametrizations of the solutions .",
    "secondly -and crucially- we consider that the code evolves through time , and that , consistently , there is a path dependence in its evolution , mathematically stated by imposing a variational principle , the @xmath8 , between successive states of the code .",
    "it is only by imposing evolution ( and thus , path dependence ) that we reach zipf s law as the only asymptotic solution .",
    "therefore , the origin of the power - law with exponent @xmath137 derives from three complementary , very general conditions :    * the unbounded informative potential of the code , * the loss of information resulting from the symmetry condition , depicted in eq .",
    "( [ symmetryeq ] ) , and * evolution , and its associated path dependency , variationally imposed by the application of the @xmath8 over successive states of the evolution of the system .",
    "there is another , very interesting point , intimately tied to a code exhibiting zipf s law and , more specially , the consequences of the _ symmetry condition _ , the mathematical ansatz which abstractly encodes the zipf s hypothesis of vocabulary balance : the presence of an inevitable ambiguity in the code .",
    "it is a common observation that natural languages are ambiguous , namely , that linguistic utterances or parts of linguistic utterances can be assigned more than one interpretation .",
    "if the principle of least effort is at work , and thus there is a cooperative strategy between the coder and the decoder , then the presence of a certain amount of ambiguity is expected , provided that the speaker tends to assign more than one meaning to certain signals . therefore , ambiguity is a byproduct of efficient communication rather than a fingerprint of poor communicative design .",
    "the presented framework is general , and rigorously demonstrates that zipf s law is a natural outcome of a broad class of communication systems evolving under coding / decoding tensions . in other words , zipf s law emerges in a system where the coder and decoder _ coevolve _ under a general problem of energy minimization .",
    "the range of application to real - world phenomena , however , must be contrasted with the validity of data , for it has been pointed out that many supposed power - law behaviors show deviations when the statistical analysis is performed accurately @xcite .",
    "it should be noted , however , that a deviation of the predicted behavior need not be necessarily attributed to a failure of the framework .",
    "one should take into account that other constraints , such as general memory limitations , can play a role in shaping the final distribution .",
    "we thank the members of the complex systems lab for useful discussions and an anonymous reviewer for his / her constructive comments .",
    "this work has been supported by nwo research project dependency in universal grammar , the spanish mcin _ theoretical linguistics _",
    "2009sgr1079 ( jf ) , the james s. mcdonnell foundation ( bcm ) and by santa fe institute ( rs ) ."
  ],
  "abstract_text": [
    "<S> zipf s law seems to be ubiquitous in human languages and appears to be a universal property of complex communicating systems . following the early proposal made by zipf concerning the presence of a tension between the efforts of speaker and hearer in a communication system , we introduce evolution by means of a variational approach to the problem based on kullback s minimum discrimination of information principle . </S>",
    "<S> therefore , using a formalism fully embedded in the framework of information theory , we demonstrate that zipf s law is the only expected outcome of an evolving , communicative system under a rigorous definition of the communicative tension described by zipf .     </S>"
  ]
}