{
  "article_text": [
    "in recent years , considerable progress has been made in analyzing data for inferring the topology of a space from which the data is sampled .",
    "often this process involves building a complex on top of the data points , and then analyzing the complex using various mathematical and computational tools developed in computational topology .",
    "there are two main issues that need attention to make this approach viable in practice .",
    "the first one stems from the requirement of choosing appropriate parameters to build the complexes so that the provable guarantees align with the computations .",
    "the other one arises from the unmanageable ` size ' of the complex  a problem compounded by the fact that the input can be large and usual complexes such as vietoris - rips built on top of it can be huge in size .    in this paper , we address both of the above two issues with a technique for data sparsification .",
    "the data points are assumed to be sampled from a smooth manifold of known dimension sitting in some euclidean space .",
    "we sparsify the data so that the resulting set is locally uniform and is still good for homology inference .",
    "observe that , with a sample whose density varies with respect to a local feature size ( such as the @xmath5 proposed for surface reconstruction  @xcite ) , no global parameter for building an appropriate complex can be found .",
    "the figure in the next paragraph illustrates this difficulty .    for the non - uniformly sampled curve , there is no single radius that can be chosen to construct , for example , rips or ech complexes . to connect points in the sparsely sampled part on right",
    ", the radius needs to be bigger than the feature size at the small neck in the middle .",
    "if chosen , this radius destroys the neck in the middle thus creating spurious topology .",
    "our solution to this problem is a sparsification strategy so that the sample becomes locally uniform  @xcite while guaranteeing that no topological information is lost .",
    "the sparsification is carried out without requiring any extra parameter and the resulting local uniformity eventually helps constructing the appropriate complex on top of the sparsified set without requiring any user supplied parameter .",
    "the sparsification also addresses the problem of ` size ' because it produces a sub - sample of the original input .",
    "the technique of subsampling has been suggested in some of the recent works .",
    "the well - known witness complex builds on the idea of subsampling the input data by restricting the delaunay centers on the data points  @xcite .",
    "unfortunately , guarantees about topological inference can not be achieved with witness complexes unless some non - trivial modifications are made and parameters are tuned .",
    "sparsified rips complexes proposed by sheehy  @xcite also uses subsampling to summarize the topological information contained in a rips filtration ( a nested sequence ) .",
    "the graph induced complex proposed in  @xcite alleviates the ` size ' problem even further by replacing the rips complexes with a more sparsified complex . both approaches , however , only approximate the true persistence diagram and hence to infer homology exactly require a user - supplied parameter to find the ` sweet spot ' in the filtration range . furthermore",
    ", none of these sparsifications is designed to work with a non - uniform input that is adaptive to a _",
    "local _ as opposed to a _",
    "global _ feature size .",
    "our algorithm first identifies a set of points that supposedly approximates only a subset of the medial axis .",
    "it is known that the medial axis of a manifold embedded in @xmath0 can be approximated with the voronoi diagrams of the @xmath1 input sample points  @xcite which requires @xmath6 voronoi vertices in the worst - case .",
    "in contrast , we approximate the medial axis only with a _",
    "lean set _ of @xmath2 points ( which can be brought down to @xmath4 with some more processing as shown in section  [ appendix : sec : linearsize ] ) .",
    "the distance to this lean set which we call the _ lean feature size _ is shown to be sandwiched between the local feature size @xmath5 and the weak local feature size @xmath7 .",
    "sparsifying the input with respect to this lean feature size allows the data to be decimated at least to the level of @xmath5 , but at the same time keeps it dense enough with respect to the weak local feature size , which eventually leads to topological fidelity .",
    "this roughly means that the data is sparsified adaptively as much as possible without sacrificing the topological information ( see experimental results in figure  [ experiment ] ) .",
    "the sparsified points are connected in a rips - like complex using the lean feature size computed for each sample point . following the approach in  @xcite , the guarantee for topological fidelity is obtained by interleaving the union of a set of balls with the offsets of the manifold . to account for the adaptivity of the sample density ,",
    "these offsets are scaled appropriately by the lean feature size and the approach in  @xcite is adapted to this framework . to the best of our knowledge ,",
    "this is the first sparsification strategy that handles adaptive input samples , produces an adaptive as well as a locally uniform sparsified sample , and infers homology without requiring a threshold parameter .    [",
    "cols=\"^ \" , ]         the more general noise model which allows outliers would also be worthwhile to investigate .",
    "one may explore the ` distance to measure ' technique proposed in  @xcite for this case .",
    "but , it is not clear how to adapt the entire development in this paper to this setting .",
    "one possibility is to eliminate all outliers first to make the noise only hausdorff , and then apply the technique for hausdorff noise as alluded in the previous paragraph .",
    "this will certainly require more parameters to be supplied by the user .",
    "* compacts : * the case for compact sets is perhaps more challenging .",
    "the normal spaces are not well defined everywhere for such spaces .",
    "thus , we need to devise a different strategy to compute the lean set . the theory of compacts developed in the context of topology inference in  @xcite may be useful here .",
    "computing the lean sets efficiently in high dimensions for compact spaces remain a formidable open problem .",
    "this work was partially supported by the nsf grants ccf-1064416 , ccf-1116258 , ccf 1318595 , and ccf 1319406 .",
    "99    p. k. agarwal . range searching .",
    "chapter 36 , _",
    "handbook discr .",
    "_ , j. e. goodman , j. orourke ( eds . ) , chapman & hall / crc , boca raton , florida , 2004 .",
    "n. amenta and m. bern .",
    "surface reconstruction by voronoi filtering .",
    "_ discrete comput .",
    "* 22 * ( 1999 ) , 481504 .",
    "n. amenta , s. choi , and r. k. kolluri .",
    "the power crust , union of balls , and the medial axis transform .",
    "theory appl . _",
    "* 19 * ( 2001 ) 127153 .",
    "boissonnat and a. ghosh .",
    "manifold reconstruction using tangential delaunay complexes .",
    "tech report inria-00440337 , version 2 , ( 2011 ) , http://hal.inria.fr/inria-00440337 .",
    "k. borsuk . on the imbedding of systems of compacta in simplicial complexes .",
    "_ fund . math _ * 35 * , ( 1948 ) 217 - 234 .",
    "f. chazal , d. cohen - steiner , and a. lieutier . a sampling theory for compact sets in euclidean space .",
    "* 41 * ( 2009 ) , 461479 .",
    "f. chazal , d. cohen - steiner , and q. mrigot .",
    "geometric inference for measures based on distance functions . _ found .",
    "_ , springer verlag ( germany ) , * 11 * ( 2011 ) , pp.733 - 751 .",
    "f. chazal and a. lieutier .",
    "topology guaranteeing manifold reconstruction using distance function to noisy data .",
    "( 2006 ) , 112118 .",
    "f. chazal and a. lieutier . the @xmath8-medial axis . _",
    "models _ * 67 * ( 2005 ) , 304331 .    f. chazal and a. lieutier . stability and computation of topological invariants of solids in @xmath9 .",
    "_ discrete comput . geom._*37 * ( 2007 ) , 601607 .",
    "f. chazal and s. oudot . towards persistence - based reconstruction in euclidean spaces .",
    "( 2008 ) , 232241 .",
    "cheng , j. jin and m - k .",
    "lau . a fast and simple surface reconstruction algorithm .",
    "( 2012 ) , 6978 .",
    "t. k. dey , f. fan , and y. wang .",
    "graph induced complex on point data .",
    "29th annu .",
    "( 2013 ) , 107116 .",
    "t. k. dey .",
    "curve and surface reconstruction : algorithms with mathematical analysis .",
    "cambridge university press , new york , 2007 .",
    "s. funke and e. a. ramos .",
    "smooth - surface reconstruction in near - linear time .",
    "13th annu .",
    "acm - siam sympos .",
    "discrete algorithms _ ( 2002 ) , 781790 .    k. grove .",
    "critical point theory for distance functions .",
    "symposia in pure mathematics _ * 54 * , american mathematical society , providence , ri , 1993 .",
    "j. giesen and m. john .",
    "the flow complex : a data structure for geometric modeling .",
    "14th annu .",
    "acm - siam sympos",
    ". discrete algorithms _ ( 2003 ) , 285294 .    a. lieutier .",
    "any open bounded subset of @xmath9 has the same homotopy type as its medial axis . _",
    "j. comput .",
    "aided design _ * 36 * ( 2004 ) , 10291046 .    james  r. munkres .",
    "elements of algebraic topology . addison  wesley publishing company , menlo park , 1984 .    d. sheehy",
    ". linear - size approximations to the vietoris - rips filtration .",
    "( 2012 ) , 239247 .",
    "v. de silva and g. carlsson .",
    "topological estimation using witness complexes .",
    "sympos . point based graphics _ ( 2004 ) , 157166 .",
    "[ [ proving - that - pq - is - beta - good - for - propositionmed - approx . ] ] proving that @xmath10 is @xmath11-good for proposition  [ med - approx ] .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we know that @xmath12 which implies that @xmath13 .",
    "consider the triangle @xmath14 . by triangle inequality , @xmath15 .",
    "the angle @xmath16 is at most @xmath17 the last inequality follows from that @xmath18 for @xmath19 . in our case , choose @xmath20 .",
    "since @xmath21 , we have that @xmath22 now assume without loss of generality that @xmath23 .",
    "then , @xmath24 recall that @xmath25 . considering the triangle @xmath26 , we have @xmath27 where the last inequality follows from a similar argument used for eqn .",
    "( [ eqn : smallangle1 ] ) .",
    "we know that , @xmath28 , @xmath29 and @xmath30 . combining these with eqn .",
    "( [ eqn : smallangle1 ] ) , ( [ eqn : smallangle2 ] ) and the assumption that @xmath31 , we have that @xmath32 similar bound holds for @xmath33 .",
    "it follows that the pair @xmath10 satisfies the first condition of being @xmath11-good , as long as @xmath34 .",
    "this is guaranteed by requiring @xmath35 ( as specified in the proposition ) .",
    "next , we argue that @xmath10 also satisfies the second condition of being @xmath11-good .",
    "to do so , let @xmath36 be half of the angle spanned by @xmath37 and @xmath38 .",
    "note that by the definition of @xmath39-medial axis @xmath40 , we have that @xmath41 .",
    "see the right figure for an illustration .",
    "first , observe that the ball @xmath42 with @xmath43 does not intersect @xmath44 , since this ball is contained inside the medial ball @xmath45 .",
    "the midpoint @xmath46 of @xmath47 is at most @xmath48 distance away from @xmath49 because both @xmath50 and @xmath51 are at most @xmath52 away from @xmath53 and @xmath54 ( assuming w.o.l.g @xmath55 ) .",
    "this means that the ball @xmath56 centering at the midpoint of @xmath47 and with radius @xmath57 is contained in the ball @xmath58 and thus does not have any point of @xmath44 and hence @xmath59 inside .    on the other hand , note that @xmath60 thus",
    ", the second condition for @xmath61 being a good pair is satisfied as long as @xmath62 consider the function @xmath63 , its derivative @xmath64 is greater than @xmath65 for @xmath66 $ ] .",
    "indeed , @xmath67 hence @xmath68 is an increasing function , and @xmath69 since @xmath41 .",
    "in other words , the second condition for @xmath10 being a good pair is satisfied as long as @xmath70 . to further simplify it , note that using @xmath71 , one can show that @xmath72 .",
    "combining this with @xmath73 , we then have @xmath74 hence as @xmath75 , the ball @xmath76 is contained in @xmath77 and thus contains no point in @xmath59 .",
    "therefore , the pair @xmath10 is @xmath11-good and its midpoint is in @xmath78 .",
    "[ [ proof - of - theoremthmsparsification . ] ] proof of theorem  [ thm : sparsification ] .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    let @xmath79 be any point in @xmath44 to which @xmath50 is the nearest sample point in @xmath59 .",
    "then , @xmath80 where @xmath81 . if @xmath50 is retained in @xmath82 , @xmath83 for sufficiently small @xmath84 , where @xmath85 is the constant from proposition [ lb - to - med ] .",
    "now consider the case when @xmath50 is deleted while processing another point , say @xmath86 . by the decimation procedure in lines 59 , @xmath87 and @xmath51 will remain in @xmath82 since we process points in non - decreasing order of their distances to @xmath88 . using proposition [ lb - to - med ] , we then have : @xmath89 the last inequality holds when @xmath90 is sufficiently small ( in which case the estimation error @xmath91 in the normal space is also small ) . therefore , @xmath92 now applying remark  [ lnfs - remark ] , @xmath82 is also @xmath93-dense because @xmath94 for @xmath95 . the fact that @xmath82 is @xmath96-sparse w.r.t .",
    "@xmath97 follows easily from the decimation procedure .",
    "here , we provide the justification for the claimed bound of @xmath98 on the tangent space estimation(and thus the normal space ) of the hidden manifold @xmath44 at a sample point @xmath99 . for completion , we restate the procedure described in section  [ sec : computation ] for estimating the tangent space @xmath100 . set @xmath101 for the calculations to follow .",
    "let @xmath102 denote the intrinsic dimension of the manifold @xmath44 , which we assume is known a - priori .",
    "let @xmath103 be the nearest neighbor of @xmath50 in @xmath104 .",
    "suppose we have already obtained points @xmath105 with @xmath106 .",
    "let @xmath107 denote the affine hull of the points in @xmath108 .",
    "next , we choose @xmath109 that is closest to @xmath50 among all points forming an angle within the range @xmath110 $ ] with @xmath107 .",
    "we add @xmath111 to the set and obtain @xmath112 .",
    "this process is repeated until @xmath113 , at which point we have obtained @xmath114 points @xmath115 .",
    "we use @xmath116 to approximate the tangent space @xmath100 .",
    "we now show that the simplex @xmath108 is `` fat '' .",
    "in particular , we will leverage a result ( corollary 2.6 ) of @xcite to bound the angle between the true tangent space @xmath100 and approximate tangent space @xmath107 .    more specifically , we first modify the simplex @xmath108 to another one @xmath117 as follows .",
    "let @xmath118 denote the longest length of any edge incident to @xmath50 in @xmath108 .",
    "later we will prove that @xmath119 .",
    "now , we extend each edge @xmath120 along the same line segment but to @xmath121 such that @xmath122 . the resulting simplex spanned by @xmath123",
    "is denoted by @xmath117 . by construction , @xmath124 .",
    "hence , we only need to bound the angle @xmath125 .",
    "corollary 2.6 of @xcite states that @xmath126 , where @xmath127 and @xmath128 are the longest and shortest edge length of @xmath117 respectively ; while @xmath129 stands for the volume of the simplex @xmath117 . to use this result",
    ", we bound the terms @xmath130 , and @xmath129 .",
    "see the figure on right for an illustration .",
    "first , we bound the angle between any two @xmath131 and @xmath121 , for @xmath132 $ ] .",
    "assume w.o.l.g .",
    "that @xmath133 . by construction",
    ", @xmath134 forms an angle @xmath135 such that @xmath136 $ ] with @xmath137 .",
    "it follows that @xmath138 , that is , @xmath139 $ ] .",
    "therefore , the edge length @xmath140 satisfies @xmath141.\\ ] ] therefore the longest edge length @xmath127 in simplex @xmath117 is at most @xmath142 , while the smallest edge length @xmath128 in simplex @xmath117 is at least @xmath143 .    next , we bound the volume @xmath129 of @xmath117 , which we do inductively .",
    "we claim that @xmath144 .",
    "this claim holds when @xmath145 in which case @xmath146 .",
    "assume it holds for @xmath147 .",
    "then , we have that @xmath148 , where @xmath149 is the height of the simplex @xmath117 using @xmath150 as the base facet . on the other hand , by construction @xmath151 , which gives @xmath152",
    "it follows that @xmath153 , which then proves the claim inductively .",
    "now we derive an upper bound on @xmath58 .",
    "inductively , assume that for @xmath154 , @xmath155 for @xmath156 and sufficiently small @xmath90 , it is true because the nearest point @xmath157 to @xmath50 satisfies @xmath158 and also @xmath159  ( this follows easily from the @xmath90-dense sampling condition , see e.g. corollary 3.1 and lemma 3.4  @xcite ) for induction consider the time when we choose @xmath160 . consider the projection @xmath161 of @xmath162 onto @xmath100 and the @xmath163-dimensional affine subspace @xmath164 of @xmath100 containing this projection . by our inductive hypothesis , @xmath165 .",
    "let @xmath166 be the subspace of @xmath100 orthogonal to @xmath167 and let @xmath168 be such that @xmath169 .",
    "the closest point @xmath170 of @xmath79 to @xmath44 has @xmath171 .",
    "therefore , we can assume that @xmath172 when @xmath173 is sufficiently small .",
    "there is a sample point @xmath174 with @xmath175 .",
    "this means that the angle @xmath176 is at most @xmath177 when @xmath90 is sufficiently small .",
    "it follows that @xmath178 .",
    "one can make @xmath179 arbitrarily small by choosing @xmath90 sufficiently small .",
    "therefore , if @xmath101 and @xmath90 is small enough , we have @xmath180 $ ] .",
    "since @xmath160 is chosen with the smallest distance from @xmath50 satisfying the above angle condition , we have , for small enough @xmath90 , @xmath181 since @xmath58 can not be larger than the maximum between older @xmath58 from stage @xmath147 and @xmath182 , one has @xmath183 . combining all these with corollary 2.6 of @xcite",
    ", we obtain that @xmath184 as claimed .    evaluating @xmath185 we obtain @xmath186 for all @xmath187 $ ] where the big - o notation hides constants depending exponentially on the intrinsic dimension @xmath102 and @xmath188 . in other words ,",
    "the angle @xmath91 between the approximate tangent space and the true tangent space ( thus between the approximate normal space and the true normal space ) at any sample point is bounded by @xmath98 , where the big - o notations hides constant depending on the angle @xmath11 and intrinsic dimension @xmath102 of the manifold @xmath44 ."
  ],
  "abstract_text": [
    "<S> in topology inference from data , current approaches face two major problems . </S>",
    "<S> one concerns the selection of a correct parameter to build an appropriate complex on top of the data points ; the other involves with the typical ` large ' size of this complex . </S>",
    "<S> we address these two issues in the context of inferring homology from sample points of a smooth manifold of known dimension sitting in an euclidean space @xmath0 . </S>",
    "<S> we show that , for a sample size of @xmath1 points , we can identify a set of @xmath2 points ( as opposed to @xmath3 voronoi vertices ) approximating a subset of the medial axis that suffices to compute a distance sandwiched between the well known _ local feature size _ and the local _ weak feature size _ </S>",
    "<S> ( in fact , the approximating set can be further reduced in size to @xmath4 ) . </S>",
    "<S> this distance , called the _ </S>",
    "<S> lean feature size _ , helps pruning the input set at least to the level of local feature size while making the data locally uniform . </S>",
    "<S> the local uniformity in turn helps in building a complex for homology inference on top of the sparsified data without requiring any user - supplied distance threshold . unlike most topology inference results , ours does not require that the input is dense relative to a _ </S>",
    "<S> global _ feature such as _ reach _ or _ weak feature size _ ; instead it can be adaptive with respect to the local feature size . </S>",
    "<S> we present some empirical evidence in support of our theoretical claims . </S>"
  ]
}