{
  "article_text": [
    "consider the classical regression problem of estimating the mean function @xmath0 with @xmath1 and @xmath2 .",
    "when @xmath3 is large , it is often worthwhile to reduce the dimensionality of @xmath4 for a better graphical exploration of the data , more parsimonious modeling , and more efficient prediction .",
    "the need for dimension reduction methods has increased in past decades to deal with high dimensional data that arise frequently in modern scientific research .",
    "a number of approaches for dimension reduction exist in the literature . of interest in regression",
    "is the concept of sufficient dimension reduction defined by @xcite as follows .",
    "a reduction @xmath5 is sufficient if it satisfies one of the following three statements : ( i. ) @xmath6 , ( ii . ) @xmath7 , or ( iii . )",
    "the symbol @xmath9 denotes statistical independence , and @xmath10 denotes @xmath11 and @xmath12 having identical distributions .",
    "typically , the sufficient reduction @xmath13 has a dimension strictly less than @xmath3 , so that a more parsimonious analysis is possible by replacing the predictor @xmath4 with @xmath13 in the regression of @xmath14 on @xmath4 .",
    "many methods for sufficient dimension reduction exist .",
    "they can be roughly grouped into three classes .",
    "moment - based methods include sliced inverse regression ( sir ; * ? ? ?",
    "* ) , inverse regression estimation ( ire ; * ? ? ?",
    "* ) , contour regression @xcite , and directional regression ( dr ; * ? ? ?",
    "likelihood - based methods include principal fitted components ( pfc ; * ? ? ?",
    "* ) and likelihood acquired directions ( lad ; * ? ? ?",
    "kernel - based methods include mave @xcite , penalized mave @xcite and other variants .",
    "the majority of sufficient dimension reduction methods assume that @xmath4 is random and @xmath14 is fixed ; these are known as inverse regression methods .",
    "nearly all moment - based methods are based on the first few moments of @xmath15 and likelihood - based methods assume a distribution of @xmath15 .",
    "arguably , inverse regression methods are best suited to deal with high dimensionality of the predictors .",
    "however , forward regression methods such as mave @xcite , which assume @xmath14 is random and @xmath4 is fixed , have been developed with a great deal of success .",
    "mave is based on the estimation of the gradient of the conditional expectation @xmath0 by way of a local regression .",
    "it does not impose strong assumptions on the distribution of @xmath4 , and is developed essentially for continuous @xmath14 to yield the so - called central mean subspace @xcite .    in this paper ,",
    "our focus is on regressions with exponential family response @xmath14 , including binomial , poisson , geometric , negative binomial , gaussian , exponential , gamma , inverse gamma , and log - normal distributions .",
    "our goal is to obtain a sufficient dimension reduction @xmath13 so that @xmath6 without much assumption on @xmath4 .",
    "we proceed using a local regression method .",
    "local regression methods have been well - established in the literature .",
    "a well - known example is the local likelihood estimation of @xcite .",
    "@xcite and @xcite provide introductions to local regression and local likelihood .",
    "these methods carry out inference at a given @xmath16 using a locally weighted log - likelihood .",
    "the contribution of observed @xmath17 may be weighted through a kernel density function whose bandwidth controls the degree of localization .",
    "local likelihood methods allow the fit to vary locally at each @xmath16 of interest , and are also capable of estimating the relationship between @xmath14 and @xmath4 without full specification of a parametric form for the regression function .",
    "@xcite developed a local likelihood regression in generalized linear models .",
    "their work focuses essentially on a single - index reduction .",
    "ours expands to multiple - index to obtain the sufficient dimension reduction of dimension @xmath18 which is to be estimated . in mave",
    ", the reduction matrix of dimension @xmath18 in @xmath19 was estimated one column at the time , while fixing the other columns in iterations between two quadratic optimizations steps .",
    "we instead consider optimization over the natural parameter space of the reduction matrix , which is either a stiefel or grassmann manifold , depending whether remaining parameters are held fixed .",
    "we also explore three prediction methods to be used with the sufficient reduction without requiring a parametric model .",
    "the remainder of the article is structured as follows .",
    "section  [ sec : llad ] presents the made methodology , including the model and estimation procedure .",
    "section  [ sec : dim ] discusses inference methods for the dimension @xmath18 of the reduction subspace .",
    "section  [ sec : sim ] we presents the three prediction methods and provides a simulation study comparing their performance .",
    "simulations are also carried out to demonstrate effectiveness to estimate the reduction . some applications",
    "to datasets are given in section  [ sec : appl ] and section  [ sec : discuss ] ends with concluding remarks .",
    "suppose @xmath1 is a response , @xmath4 is a @xmath3-dimensional predictor , and the distribution of @xmath20 is given by an exponential family distribution of the form @xmath21/a(\\phi ) \\right\\}. \\label{eqn : exp - fam}\\ ] ] for a particular dataset , a specific form of distribution would be assumed , yielding the functions @xmath22 and @xmath23 .",
    "the canonical parameter @xmath24 is related to the mean function @xmath0 through a link function @xmath25 so that @xmath26 . the variance function @xmath27 where @xmath28 is referred to as dispersion parameter .",
    "it is often assumed that @xmath29 which amounts to the generalized linear model @xcite .",
    "the specific distribution of @xmath20 determines the choice of the link function @xmath30 which is assumed to be the canonical link for the remainder of the paper .",
    "the canonical parameter @xmath24 holds the main information that connects @xmath14 to @xmath4 .",
    "let @xmath31 represent independent samples from the distribution of @xmath32 so that @xmath33 has the distribution .",
    "we will assume that @xmath24 is a continuous and smooth function so that it admits at any point @xmath4 the first order linear expansion @xmath34^t(x_i - x ) \\label{eqn : theta - approx}\\ ] ] for any @xmath35 in the neighborhood of @xmath4 .",
    "let @xmath36 and @xmath37 .",
    "the term @xmath38 retains the core information that connects @xmath39 to @xmath35 locally at @xmath4 . as @xmath4 varies in its sample space",
    ", @xmath38 describes a @xmath40-dimensional subspace @xmath41 in @xmath19 with @xmath42 .",
    "let @xmath11 be an orthonormal basis of @xmath41 so that @xmath43 for some @xmath44 .",
    "it follows that @xmath45 , thus the distribution of @xmath20 is approximately the same as that of @xmath46 .",
    "consequently , @xmath47 can be used in lieu of @xmath4 in the regression of @xmath14 on @xmath4 .",
    "the subspace @xmath41 , called a sufficient dimension reduction subspace @xcite , is not unique , and may not be minimal in its dimension . when the dimension of @xmath41 is @xmath3 , no reduction is achieved .",
    "of all the sufficient dimension reduction subspaces , let @xmath48 be the subspace of minimal @xmath18 with @xmath49 with basis matrix @xmath50 ( which is one of many possible bases ) .",
    "then @xmath50 is a semi - orthogonal matrix whose columns span @xmath48 .",
    "consequently , we have @xmath51 and @xmath52 can be replaced by @xmath53 in the regression of @xmath14 on @xmath4 . for single - parameter distributions such a binomial or a poisson , @xmath48 is a central subspace .",
    "however , for a two - parameter distribution such as gaussian , @xmath48 is a central mean subspace .",
    "we will proceed under the assumption of a one - parameter exponential family so that the dispersion parameter @xmath28 is known",
    ". this will be sufficient to develop made under gaussian , poisson , and binomial families , which have been the main focus of this work .",
    "we will also discuss in section  [ sec : twopar ] on extensions needed to estimate an unknown @xmath28 , to facilitate the use of other useful family types .",
    "the ultimate goal is to determine the reduction subspace @xmath48 . a typical way is to determine @xmath50 so that locally at each point @xmath35 , @xmath39 is the closest to @xmath54 for all @xmath55 .",
    "consider for example @xmath14 from a normal distribution where the link function @xmath56 is the identity function .",
    "the closeness of @xmath14 to @xmath57 can be evaluated with a square loss function , and consequently , the parameter @xmath50 can be estimated as @xmath58 ^ 2|b^tx\\}\\ ] ] where @xmath59 is a stiefel manifold , the set of all @xmath18-dimensional orthonormal matrices in @xmath60 .",
    "for a discrete @xmath14 from a bernoulli distribution for example , a square loss may not be meaningful .",
    "an absolute loss function can be used to that @xmath50 is estimated as @xmath61 clearly , for each distribution of the exponential family , the appropriate loss function should be considered . however , there is a more general loss function that could be considered across all the exponential family distributions : the local deviance function based on local likelihood function .",
    "a local deviance has been used for example in @xcite for diagnostics purposes .",
    "regression based on the local log - likelihood evaluated at a given @xmath52 can be written as @xmath62.\\end{aligned}\\ ] ] the weights @xmath63 represent the contribution of each observation toward @xmath64 .",
    "note that the function @xmath65 can vary with @xmath66 in our formulation ; for example , as shown in appendix  [ sec : exp - fams ] , binomial observations with @xmath67 trials will have @xmath68 .",
    "however , we assume that @xmath69 does not depend on @xmath4 .",
    "a deviance of the local likelihood for @xmath70 at @xmath71 can be expressed as @xmath72.\\end{aligned}\\ ] ] the term @xmath73 is the maximum local likelihood achievable for an individual observation .",
    "the local deviance @xmath74 is a measure of the closeness of @xmath70 to @xmath75 .",
    "clearly @xmath76 , @xmath77 if @xmath78 , and @xmath74 gets larger when @xmath79 gets far from @xmath70 .",
    "consider minimizing the average deviance @xmath80 with respect to @xmath81 for @xmath82 and @xmath83 such that @xmath84 .",
    "this is equivalent to maximizing @xmath85 which is the full local log - likelihood evaluated at each of the sample points , where @xmath86 and @xmath87 . while each sample point @xmath71 has its own regression coefficients @xmath88 and @xmath89 , they all share a common dimension reduction kernel matrix @xmath50 .",
    "we provide detailed expressions of the made objective function @xmath90 for several commonly used exponential family outcomes in appendix  [ sec : exp - fams ] .",
    "the kernel weights are computed as @xmath91 with @xmath92 , where @xmath93 denotes one of the usual multidimensional kernel density functions and the bandwidth @xmath94 is a @xmath95 symmetric and positive definite matrix .",
    "for example , the multivariate gaussian kernel is @xmath96 .",
    "the choice of the kernel density @xmath97 and the bandwidth @xmath98 are next discussed in section  [ sec : estimate - coefs ] .",
    "when the reduction matrix @xmath50 is known , or an estimator is available , the kernel weights can be refined and written as @xmath99 these weights now depend the @xmath18-dimensional data and @xmath94 is a @xmath100 symmetric positive definite matrix .",
    "note that may be naturally extended to the scenario of a training set @xmath101 and a test set @xmath102 which are not necessarily the same .",
    "the test set may contain observations without an observed @xmath14 that we wish to predict , or whose @xmath14 value has been held out for the purpose of cross - validation . in this case , the made objective function becomes @xmath103 where @xmath104 and @xmath105 .",
    "we now proceed using as the objective function , but note that computations can readily be changed to use .",
    "the parameters of interest are @xmath106 , and @xmath83 .",
    "we start by assuming that the dimension @xmath18 is known . for any orthogonal matrix @xmath107 , @xmath108 , which implies that @xmath109 and @xmath50 are not uniquely determined but obtained up to an orthogonal transformation .",
    "furthermore , refined weights based on the gaussian kernel @xmath110 with @xmath111 depend on @xmath50 only through @xmath112 . in this",
    "setting , the made problem is invariant to orthogonal transformation of @xmath50 in the sense that @xmath113 the parameter space of @xmath50 is the set of @xmath18-dimensional subspaces in @xmath19 known as the grassmann manifold of dimension @xmath114 .",
    "however , the estimation method we adopt does not estimate all the parameters jointly , but works iteratively . for fixed values of @xmath88 and @xmath89 , @xmath82 ,",
    "the parameter space of @xmath50 is the set of @xmath18-dimensional orthonormal matrices in @xmath19 , also known as stiefel manifold of dimension @xmath115 .",
    "the dimension of steifel and grassmann manifolds is discussed in @xcite . in the following , we present an iterative method to maximize for a given dimension @xmath18 , and later discuss selection of @xmath18 .    to estimate the parameters @xmath116 , we start by fixing @xmath50 in . we see that maximizing @xmath90 over @xmath117 is equivalent to maximizing each @xmath118 separately .",
    "there is no closed - form solution of the estimator , except in certain special cases such as gaussian outcomes . instead",
    ", we proceed with a multivariate newton - raphson iterative approach . for a particular @xmath119 ,",
    "let @xmath120 , @xmath121 , and @xmath122 so that @xmath123.\\end{aligned}\\ ] ] let @xmath124 , @xmath125 and @xmath126 with entries @xmath127 / a_i(\\phi)$ ] for @xmath128 .",
    "the first derivative at @xmath4 is then @xmath129 the function @xmath130 has an @xmath131 jacobian @xmath132 to formulate newton - raphson iterations , suppose @xmath133 is a given iterate and @xmath134 will be the next iterate . to solve for @xmath135 approximately , set the first order taylor expansion of , @xmath136 to zero and solve to obtain @xmath137 this suggests the update of @xmath138 as @xmath139 .",
    "these steps are iterated until the @xmath25th iteration where @xmath140 for some small prescribed @xmath141 .    to estimate @xmath50",
    ", we suppose that @xmath117 , @xmath142 , are fixed and known . omitting the terms of the objective function that are free of @xmath50 , estimation of @xmath50",
    "is carried out by maximizing @xmath143 over the set of @xmath18 dimensional semi - orthogonal matrices in @xmath19 . in the present work , @xmath50",
    "is estimated in its natural parameter space , a stiefel manifold , which naturally honors the orthonormality constraint .",
    "we implemented a conjugate gradient method from @xcite to optimize @xmath144 on the stiefel manifold in the statistical software r @xcite .",
    "a short background on the algorithm is provided in appendix  [ sec : manifold - opt ] .",
    "use of the optimization method requires programming the objective function and its gradient .",
    "the gradient of @xmath144 is a @xmath145 matrix with @xmath146th entry @xmath147 for @xmath148 and @xmath149 .",
    "here , @xmath150 represents the mean function for the exponential family and @xmath151 represents the @xmath146th element of @xmath50 .",
    "the optimization on the stiefel manifold converges when @xmath152 for a user - specified @xmath153 , where @xmath154 and @xmath155 with @xmath156 .",
    "joint estimation of all parameters necessitates cycling through the newton - raphson iterations for @xmath88 and @xmath89 , and the stiefel manifold optimization for @xmath50 .",
    "the procedure is presented as algorithm  [ alg : made ] .",
    "the full algorithm converges if the estimate of the @xmath50 matrix ( which is common to all observations ) converges ; this occurs when @xmath157 for some user - specified @xmath141 . here",
    ", @xmath158 is the frobenius norm and @xmath159 is the iterate obtained on the @xmath160th iteration of the algorithm .    1 .",
    "provide an initial @xmath50 and weights @xmath161 .",
    "2 .   do until convergence : 1 .",
    "fix @xmath50 and estimate @xmath88 and @xmath89 for @xmath82 using newton - raphson .",
    "2 .   fix @xmath162 and @xmath109 and the weights @xmath163 for @xmath164 , and estimate @xmath50 using the stiefel manifold optimization .",
    "3 .   update the weights @xmath165 if refined weights are desired .",
    "a good starting value for @xmath50 helps for a fast convergence .",
    "practically , any estimator that can be quickly computed can be considered .",
    "for example , when dealing with a continuous response , the outer product of gradients method proposed by @xcite is one attractive option .",
    "another option is to use the matrix of first @xmath18 eigenvectors of the fitted covariance matrix @xmath166 @xcite , where @xmath167 is the centered @xmath168 data - matrix of the predictors , and @xmath169 is a @xmath170 matrix with @xmath66th row @xmath171 .    the choice of kernel density @xmath172 and bandwidth @xmath94 are important in nonparametric regressions , and there is abundant literature on this subject .",
    "see for example @xcite for continuous - type outcomes and @xcite for exponential family outcomes , as well as the book by @xcite .",
    "optimal bandwidth based on asymptotic mean squared errors is often considered in this literature .",
    "@xcite instead considered the data - driven cross - validation approach for use with mave .",
    "cross - validation selects the bandwidth to minimize an out - of - sample prediction error . in our implementations , we have used the multivariate bandwidth @xmath173 where @xmath174 for some @xmath175 , is the usual optimal bandwidth in the sense of mean integrated squared errors @xcite .",
    "the dimension @xmath18 of the reduction kernel matrix @xmath50 is to be estimated .",
    "three estimation methods are discussed herein .",
    "the first is a sequential permutation test , the second is a bootstrap method , and the third is a cross - validation .",
    "the permutation tests has been used by @xcite to estimate the dimension of sufficient dimension reduction .",
    "their setup was different from ours , but the concept is otherwise identical .",
    "the interest is in testing the hypotheses @xmath176 we propose to test sequentially for @xmath177 until the first time @xmath178 is not rejected .",
    "the value @xmath179 is then taken to be the estimated dimension .",
    "consider testing ( [ eq : test ] ) with @xmath180 . under the null hypothesis , we have @xmath181 while @xmath182 under @xmath183 for any @xmath184 .",
    "the parameter @xmath185 resides in a stiefel manifold so that @xmath186 .",
    "thus , testing ( [ eq : test ] ) is equivalent to testing @xmath187 against @xmath188 .",
    "now consider testing ( [ eq : test ] ) with @xmath189 .",
    "let @xmath190 and @xmath191 be the reduction kernel matrices under @xmath178 and @xmath183 , respectively .",
    "we can then write @xmath192 $ ] where @xmath185 such that @xmath193 and @xmath186",
    ". the canonical parameter can be written as @xmath194 under @xmath178 while @xmath195 under @xmath183 , at any @xmath4 where @xmath196 and @xmath197 . again , testing @xmath198 against @xmath199 is equivalent to testing @xmath200 against @xmath201 .",
    "the sample value of @xmath109 and its distribution are needed to carry out the test . however , for a given data set with @xmath202 observations , there are @xmath202 local parameters @xmath203 to estimate . under @xmath178",
    ", we expect each of these @xmath202 estimates @xmath204 close to zero .",
    "let @xmath205 be a summary statistic of these @xmath202 estimates using the unperturbed @xmath14 values .",
    "for example , @xmath206 can be the sample mean or other summary statistics .    to obtain a sampling distribution of these estimates under @xmath178 ,",
    "generate a large number @xmath207 of permutations @xmath208 of the @xmath14-observations , which yields @xmath209 for @xmath210 and @xmath211 .",
    "denote @xmath212 as the summary obtained from @xmath213 for @xmath211 .",
    "we would expect @xmath214 to be ` close ' to @xmath215 if @xmath178 is true , and far from @xmath215 otherwise .",
    "the fraction @xmath216 is an approximate p - value to test .",
    "the procedure is summarized as follows .    * starting with @xmath180 , do until @xmath217 is not rejected 1 .",
    "estimate @xmath218 2 .",
    "generate @xmath219 and compute @xmath220 , @xmath211 3 .   if @xmath178 if rejected then @xmath221 .",
    "practically , for testing with @xmath180 , all the parameters are estimated directly using the estimation method in section  [ sec : estimate - coefs ] . however , for @xmath189 , we first obtained @xmath222 , an estimate of @xmath223 under @xmath178 .",
    "then @xmath224 , and @xmath225 are estimated with @xmath223 replaced by @xmath222 .",
    "the sample mean is used in our implementation for the summary @xmath206 .",
    "we propose bootstrapping a statistic similar to the likelihood ratio test ( lrt ) statistic to test the dimension .",
    "consider again testing sequentially .",
    "let @xmath222 be the made estimate under @xmath226 , which yields the fitted local coefficients @xmath227 for @xmath82 , and hence the fitted regressions @xmath228 for @xmath82",
    ". similarly , denote @xmath229 as the made estimate under @xmath230 , which yields fitted local coefficients @xmath231 and fitted regressions @xmath232 for @xmath82 .",
    "the fitted regressions from made may be used to evaluate the likelihood based on , which is @xmath233 a quantity analogous to the lrt statistic may be computed as @xmath234.\\end{aligned}\\ ] ] we do not know the distribution of @xmath235 under the null hypothesis .",
    "to fully specify the test procedure , a parametric bootstrap procedure can approximate the null distribution from the data .    * starting with @xmath180 , do until @xmath217 is not rejected 1 .",
    "obtain @xmath235 , an estimate of @xmath236 using the original sample .",
    "2 .   draw a bootstrap sample @xmath237 from the null likelihood @xmath238 .",
    "3 .   estimate @xmath239 and @xmath240 under @xmath178 and @xmath241 respectively using made with data @xmath242 .",
    "4 .   compute @xmath243 $ ] .",
    "repeat steps 24 for @xmath244 , where @xmath245 is the desired number of bootstrap iterations . 6 .   if @xmath178 if rejected then @xmath246 .",
    "an approximate p - value can now be computed as @xmath247      in the context of local likelihood approach , @xcite considered a cross - validation in conjunction with a prediction method based on nadaraya - watson kernel .",
    "we provide a similar approach with an alternative prediction method .",
    "the true dimension @xmath18 of @xmath50 can be estimated to yield the best predictive model on out - of - sample observations .",
    "we propose a @xmath97-fold cross - validation to estimate the mean squared prediction error @xcite .",
    "suppose @xmath248 contains all indexes in the dataset .",
    "let us partition @xmath249 randomly into @xmath97 subsets @xmath250 of approximately equal sizes , and let @xmath251 be the subset of @xmath249 where @xmath252 is held out .",
    "denote @xmath253 the predicted value for observation @xmath254 , where @xmath252 is a test set for evaluation and @xmath251 is a training set .",
    "the parameters are estimated for a fixed dimension @xmath18 using @xmath251 .",
    "taking @xmath255 as a predetermined loss function , we estimate the dimension @xmath18 as @xmath256 the prediction values @xmath253 are computed according to one of the three approaches outlined in section  [ sec : pred ] .",
    "for example , the loss @xmath255 may be a squared loss for continuous responses , or an absolute loss for a bernoulli outcome .",
    "minimum average variance estimation , or mave was proposed by @xcite .",
    "it is an adaptive estimation method using a local estimation to determine a dimension reduction of @xmath4 in the regression of @xmath257 .",
    "it assumes that a model of @xmath257 is of the form @xmath258 , where @xmath259 is an unknown smooth link function , and @xmath223 is a @xmath145 semi - orthogonal matrix so that @xmath260 . there is no extraneous distributional assumption , however , in that formulation , @xmath257 has the same distribution as @xmath261 . the direction @xmath223 was then determined as the solution of @xmath262\\}\\ ] ] using the approximation that @xmath263 at any @xmath71 , let @xmath264 , and @xmath265 , and let the weights @xmath266 as in expression  ( [ eq : weight ] ) . the local parameters in @xmath267 and @xmath268 , and the matrix @xmath50 are estimated essentially as @xmath269    now let write the local deviance version in the case of gaussian outcome @xmath14 where the variance @xmath270 is assumed fixed and known",
    "the local parameters @xmath271 and @xmath272 , and the parameter @xmath50 are estimated as @xmath273 ^ 2 } { 2\\sigma^{2}}\\right\\ } w_{i}(b^tx_j ) \\\\ % & = & \\argmin_{\\alphabf , \\gammabf , b } \\sum_{j=1}^n \\sum_{i=1}^n \\left\\ { y_i - ( \\alpha_j + \\gamma_j^t b^t ( x_i - x_j ) ) \\right\\}^2 w_{i}(b^tx_j ) .\\end{aligned}\\ ] ] clearly made with gaussian outcomes is equivalent to mave , thus made effectively subsumes mave . in the gaussian case of made",
    ", the normality assumption does not add any limitation in the formulation nor in the estimation .",
    "it is noteworthy that in the setting of mave , @xcite an iterative least squares method is employed to estimate @xmath274 and @xmath275 .",
    "furthermore , a quadratic programming method was used .",
    "the orthogonality constraint of @xmath50 was dealt with by estimating individual columns of @xmath50 sequentially and orthonormalizing these column - vectors to obtain @xmath229 . in our case",
    ", we broke away from the procedure of @xcite by carrying out the estimation of @xmath50 on its natural space , which is a stiefel manifold .",
    "discuss the case where both @xmath276    for non - gaussian exponential family types , the dispersion parameter @xmath28 may not simply cancel out of the made objective function .",
    "when @xmath28 is not known , it must be estimated within made ; this can be accomplished by adding a step to algorithm  [ alg : made ] , which is given as algorithm  [ alg : made - dispersion ] .    here , the objective function is free of the dispersion parameter @xmath277 , and an unknown @xmath270 can be estimated outside of made .",
    "for example , given estimates @xmath278 computed from made , we may consider the likelihood of observations @xmath279 and accordingly use the maximum likelihood estimator @xmath280 .",
    "for non - gaussian exponential family types , the dispersion parameter @xmath28 may not simply cancel out of the made objective function .",
    "when @xmath28 is not known , it must be estimated within made ; this can be accomplished by adding a step to algorithm  [ alg : made ] , which is given as algorithm  [ alg : made - dispersion ] .",
    "we will proceed using refined weights for the remainder of the paper , and will make use only of algorithm  [ alg : made ] .",
    "we will proceed using refined weights for the remainder of the paper , and will make use only of algorithm  [ alg : made ] .    1 .",
    "provide an initial @xmath50 and weights @xmath161 .",
    "2 .   do until convergence : 1 .",
    "fix @xmath281 and estimate @xmath88 and @xmath89 for @xmath82 using newton - raphson .",
    "fix @xmath28 , @xmath162 and @xmath109 and the weights @xmath163 for @xmath164 , and estimate @xmath50 using the stiefel manifold optimization .",
    "3 .   fix @xmath282 and estimate @xmath28 by maximizing the resulting objective function .",
    "update the weights @xmath283 if refined weights are desired .",
    "we study the performance of made in estimating the reduction subspace under several settings for the distribution of response @xmath14 .",
    "an empirical consistency of the estimate @xmath229 is evaluated together with a contrast to mave of @xcite and pfc of @xcite .",
    "we report the results for binomial , gaussian , and poisson distributions . under each setup ,",
    "a dataset was generated with a specified matrix @xmath83 representing the subspace @xmath48 .",
    "the made , mave , and pfc methods are then used to obtain @xmath284 , the estimator of @xmath285 , where the dimension of @xmath229 was not estimated but set to the true @xmath18 . to compare @xmath285 to @xmath284 , we used the distance @xmath286 suggested in @xcite . for a given sample size @xmath202",
    ", the procedure was repeated one hundred times .",
    "following is the description of the data generation under the three aforementioned setups . in all cases ,",
    "@xmath287 and @xmath202 was varied from 25 to 400 .    1 .   _ binomial : _ we first generated the response vector @xmath288 as @xmath202 independent @xmath289 .",
    "then the predictors were generated as @xmath290 , @xmath291 and the elements of @xmath292 are independently sampled from a standard normal distribution .",
    "_ gaussian : _ the response was obtained as @xmath293 ^ 2 ) ) , 0.3 ^ 2 \\big)$ ] with @xmath294 .",
    "the predictors were generated as @xmath295 , @xmath296 , @xmath297 , @xmath298 , @xmath299 , @xmath300 .",
    "poisson : _ the response was generated as @xmath301 , and the six predictors were obtained from @xmath302 .",
    "figures  [ fig : na]a - c show distances @xmath303 for varying values of @xmath202 .",
    "overall , it appears that the made estimator converges to the true @xmath50 as the sample size increases . for the binomial case ,",
    "pfc dominated made , and mave showed the worse performance . in the gaussian and poisson cases , made outperformed mave , and both dominated pfc .",
    "it is possible that the poor performance of pfc in the latter two cases is due to the data generation scheme , which is based on forward regression .",
    "suppose we wish to estimate @xmath0 for a new observation @xmath304 .",
    "let @xmath229 denote the estimate of @xmath50 based on @xmath202 independent observations .",
    "we provide three different prediction methods that do not rely on the exact specification of the regression function to predict the response corresponding to a new observation @xmath305 .",
    "let @xmath306 denote the set of kernel weights obtained as @xmath307 .",
    "the first prediction method yields the predicted response as @xmath308 this prediction method is essentially a nadaraya - watson estimator which is typical for nonparametric methods , and was used in @xcite in the context of cross - validation .",
    "it can be used with any dimension reduction method that could provide an estimate for @xmath50 .",
    "we will refer to this prediction method as the nw method .",
    "the second prediction method is relative to local likelihood regression .",
    "we continue to assume that the @xmath202 independent samples are used to obtain @xmath229 , an estimator of the reduction matrix @xmath50 . for the new observation @xmath305 , @xmath309 and @xmath310 may be are obtained as @xmath311 recall in the original taylor expansion that @xmath312 ; as is often done in local likelihood literature @xcite we may predict @xmath313 using only the intercept as @xmath314 .",
    "we will denote this as local likelihood prediction i , or @xmath315 .",
    "we also consider local likelihood prediction ii , computed as @xmath316 which incorporates the estimate for the slope as well .",
    "we will now compare these three prediction methods in a simulation study .",
    "the datasets were generated as in section  [ sec : sim_na ] except for the following details .    1 .",
    "_ binomial _ : the response observations were obtained from @xmath317 .",
    "gaussian _ : the observations were generated as @xmath318 .",
    "_ poisson _ : the elements of @xmath4 were obtained from the @xmath319 , and the response was generated as @xmath320)})$ ] .",
    "for each step of the simulation , a training set of @xmath202 observations was generated for estimation of the parameters , and an additional test set of @xmath321 observations was generated to predict the response .    [ cols=\"^,^,^ \" , ]",
    "we have introduced made as an extension to mave @xcite for sufficient dimension reduction using local likelihood regression .",
    "while mave assumes a model consisting of a mean function with an additive error , made applies to exponential family outcomes such as gaussian , poisson , binomial , etc .",
    "initial simulations and data analyses have yielded encouraging results , but many issues can be raised and should be investigated in future work .",
    "the present work does not offer analytical proof of the consistency of @xmath322 , @xmath323 , and @xmath229 .",
    "a study of consistency and other statistical properties using the geometry of a stiefel or grassmann manifold would be of interest for made .",
    "the performance of made depends on the bandwidth @xmath324 .",
    "we have hand - picked @xmath324 to obtain results for the applications shown in this paper .",
    "we have also considered using a grid of possible @xmath324 values and selecting by cross - validation .",
    "this process becomes excessively computational because of the iterative estimation procedure , whose performance and reliability vary with the choice of @xmath324 . we note that specific outcome types may admit faster and more efficient estimation procedures by foregoing the general exponential family framework ; for example , closed - form solutions for @xmath322 and @xmath323 can be obtained in the gaussian case .    we have devised and demonstrated permutation and bootstrap test procedures to determine the dimension of the reduction .",
    "we have also considered cross - validation , although its simulations were not reported .",
    "all three procedures are data - driven and computationally intensive .",
    "a possible alternative is to use an information theoretic criteria , such as akaike s information criteria or the bayesian information criteria .",
    "there is a limited literature on information criteria in local regression @xcite , and we are not aware of an approach suitable for a parameter constrained to be a subspace or basis matrix",
    ". it may therefore be of interest to study information criteria which apply to parameters on manifolds .",
    "so far , results and applications of the methodology were initiated for relatively smaller dimension of the predictors .",
    "scalability of the methodology to large numbers of observations and dimensions should be investigated and properly evaluated .",
    "group - wise reduction may help reduce the computing time along with a faster algorithm .",
    "the made objective function is formulated for outcomes from an exponential family .",
    "the function @xmath325 and canonical link @xmath30 are provided below for some selected exponential family distributions . to identify the canonical link between the mean parameter @xmath326 and the regression function @xmath327 , take binomial as an example .",
    "the expression @xmath328 $ ] is multiplied by the observation @xmath329 in the exponential term , therefore @xmath330",
    "\\equiv \\text{logit}(\\mu)$ ] is the canonical link function .    *",
    "@xmath331 with known @xmath259 @xmath332 * @xmath333 @xmath334 * @xmath335 @xmath336 * @xmath337 with known @xmath338 @xmath339 * @xmath340 with known @xmath270 @xmath341 * @xmath342 @xmath343 * @xmath344 with known @xmath338 @xmath345 * @xmath346 with known @xmath338 @xmath347",
    "the collection of semi - orthogonal @xmath145 matrices forms what is known as the stiefel manifold . working directly on",
    "the manifold acknowledges the constraints of the problem in a natural way .",
    "optimization algorithms on manifolds require that the manifold is endowed with a differentiable structure so that fundamental operations , such as computation of a gradient or stepping from a previous iterate to the next iterate , are meaningful .    in a seminal paper on stiefel and grassmann manifold optimization of real - valued functions ,",
    "@xcite propose newton - type and conjugate gradient algorithms .",
    "the algorithms rely on geodesics , tangent spaces , and other manifold constructs which are developed in that paper .",
    "we briefly summarize the conjugate gradient algorithm for stiefel optimization used in the made algorithm for estimation of @xmath50 , with some additional detail .    1 .   given @xmath223 such that @xmath348 , compute @xmath349 and set @xmath350 .",
    "2 .   for @xmath351 1 .   set @xmath352 .",
    "2 .   calculate the norm of the gradient on the tangent space to @xmath353 , equal to @xmath354 .",
    "if this norm is less than the tolerance , stop .",
    "3 .   calculate the qr decomposition of @xmath355 .",
    "4 .   minimize @xmath356 over t , where @xmath357 , with @xmath358 and @xmath359 obtained by using the matrix exponential : @xmath360 if @xmath361 , shrink the search window .",
    "5 .   set @xmath362 .",
    "compute @xmath363 .",
    "parallel transport tangent vector @xmath364 to the point @xmath365 : @xmath366 8 .",
    "use the conjugate gradient method to compute the new search direction , @xmath367 , where @xmath368 with @xmath369 .",
    "we use @xmath370 , although we may also use @xmath371 .",
    "reset @xmath372 if @xmath373 mod @xmath374 .",
    "note that the gradient for a function @xmath375 with respect to the canonical metric on the stiefel manifold is defined as @xmath376 , where @xmath377 is the @xmath145 matrix of partial derivatives of @xmath375 with respect to the elements of @xmath50 , i.e. , @xmath378 .",
    "for the made objective function , the expression for @xmath377 is given in section  [ sec : estimate - coefs ] .",
    "we refer interested readers to @xcite and also @xcite for more information about matrix manifolds and optimization algorithms on matrix manifolds ."
  ],
  "abstract_text": [
    "<S> sufficient dimension reduction reduces the dimensionality of data while preserving relevant regression information . in this article </S>",
    "<S> , we develop minimum average deviance estimation ( made ) methodology for sufficient dimension reduction . </S>",
    "<S> it extends the minimum average variance estimation ( mave ) approach of xia et al . </S>",
    "<S> ( 2002 ) from continuous responses to exponential family distributions to include binomial and poisson responses . </S>",
    "<S> local likelihood regression is used to learn the form of the regression function from the data . </S>",
    "<S> the main parameter of interest is a dimension reduction subspace which projects the covariates to a lower dimension while preserving their relationship with the outcome . to estimate this parameter within its natural space </S>",
    "<S> , we consider an iterative algorithm where one step utilizes a stiefel manifold optimizer . </S>",
    "<S> we empirically evaluate the performance of three prediction methods , two that are intrinsic to local likelihood estimation and one that is based on the nadaraya - watson estimator . </S>",
    "<S> initial results show that , as expected , made can outperform mave when there is a departure from the assumption of additive errors . </S>"
  ]
}