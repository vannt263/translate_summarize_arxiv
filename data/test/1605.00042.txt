{
  "article_text": [
    "we aim to estimate a sparse low - rank matrix @xmath1 from its noisy observation @xmath2 , i.e. , @xmath3 where @xmath4 represents additive white gaussian noise ( awgn ) matrix .",
    "the estimation of sparse low - rank matrices has been studied @xcite and used for various applications such as covariance matrix estimation @xcite , subspace clustering @xcite , biclustering @xcite , sparse reduced rank regression @xcite , graph denoising and link prediction @xcite , image classification @xcite and hyperspectral unmixing @xcite .    in order to estimate the sparse low - rank matrix @xmath5 , it has been proposed @xcite to solve the following optimization problem @xmath6 where @xmath7 is the nuclear norm , @xmath8 is the entry - wise @xmath0 norm and @xmath9 are the regularization parameters .",
    "the nuclear norm induces sparsity of the singular values of the matrix @xmath5 , while the entry - wise @xmath0 norm induces sparsity of the elements of @xmath5 .",
    "the nuclear norm and the @xmath0 norm are convex relaxations of the non - convex rank and sparsity constraints , respectively .",
    "the nuclear norm can be considered as the @xmath0 norm applied to the singular values of the matrix .",
    "it is known that the @xmath0 norm underestimates non - zero signal values , when used as a sparsity - inducing regularizer . as a result , the sparse low - rank ( slr ) problem in can be considered , in general , to be over - relaxed @xcite .",
    "further , it is known that the performance of nuclear norm for sparse regularization of the singular values is sub - optimal @xcite .    in order to estimate the non - zero signal values more accurately ,",
    "non - convex regularization has been favored over convex regularization @xcite .",
    "furthermore , it has been shown that non - convex penalty functions can induce sparsity of the singular values more effectively than the nuclear norm @xcite .",
    "indeed , it was shown that nonconvex regularizers are better able to estimate simultaneously sparse and low - rank matrices in the context of spectral unmixing for hyperspectral images @xcite .",
    "the use of non - convex regularizers ( penalty functions ) , however , generally leads to non - convex optimization problems .",
    "the non - convex optimization problems suffer from numerous issues ( sub - optimal local minima , sensitivity to changes in the input data and the regularization parameters , non - convergence , etc . ) .    in this paper",
    ", we avoid the issues of non - convexity by using parameterized penalty functions , which aid in ensuring the strict convexity of the proposed objective function .",
    "we propose to solve the following improved sparse low - rank ( islr ) formulation @xmath10 where @xmath11 and @xmath12 is a parameterized non - convex penalty function ( see sec .  [ subsection::penalty functions ] ) .",
    "note that , if @xmath13 , then the islr formulation reduces to the generalized nuclear norm minimization problem @xcite .",
    "further , if @xmath14 and @xmath15 , then the islr problem reduces to the singular value thresholding ( svt ) problem .",
    "the contributions of this paper are two - fold .",
    "first , we show how to set the parameters @xmath16 and @xmath17 to ensure that the function @xmath18 in is strictly convex .",
    "second , we provide an admm based algorithm to solve , which utilizes single variable - splitting compared to two variable - splitting as in @xcite .",
    "we guarantee the convergence of admm , provided the scalar augmented lagrangian parameter @xmath19 , satisfies @xmath20 .",
    "the parameterized non - convex penalty functions used in this paper have designated non - convexity , which enables the overall objective function @xmath18 in to be strictly convex .",
    "in particular , if the parameters @xmath16 and @xmath17 exceed their critical value , then the function @xmath18 in is non - convex",
    ". a similar framework of convex objective functions with non - convex regularization was studied for several signal processing applications ( see for eg . ,",
    "@xcite , @xcite , @xcite , @xcite and the references therein ) .",
    "it was reported that non - convex regularization outperformed convex regularization methods for these applications .",
    "the sparse low - rank ( slr ) formulation in is different from the low - rank + sparse decomposition @xcite , also known as the robust principal component analysis ( rpca ) .",
    "both the slr and the rpca formulations utilize the nuclear norm and the @xmath0 norm as sparsity - inducing regularizers @xcite .",
    "the rpca formulation aims to estimate the matrix , which is the sum of a low - rank and a sparse matrix .",
    "note that , in the case of rpca , the matrix to be estimated is itself neither sparse or low - rank @xcite .",
    "in contrast , the slr problem , and the one proposed in this paper , considers the case wherein the matrix to be estimated is simultaneously sparse and low - rank ( similar to @xcite ) .    several well - studied convex optimization algorithms , such as admm @xcite , ista / fista @xcite , and proximal gradient methods @xcite can be applied to solve convex objective functions of the type .",
    "the slr objective function , has been solved using generalized forward - backward @xcite , incremental proximal descent @xcite ( introduced in @xcite ) , majorization - minimization @xcite , and the inexact augmented lagrangian multiplier ( ialm ) method @xcite .",
    "the ialm method can also be used to solve the slr problem , although with a different data - fidelity term @xcite .",
    "we denote vectors and matrices by lower and upper case letters respectively . for a matrix @xmath21",
    ", we use the following entry - wise norms , @xmath22 further , we use the nuclear norm ( also called the ` schatten-1 ' norm ) defined as @xmath23 where @xmath24 represent the singular values of the matrix @xmath2 and @xmath25 .",
    "we propose to use non - convex penalty functions @xmath26 parameterized by the parameter @xmath27 .",
    "the value of @xmath28 provides the degree of non - convexity of the penalty functions .",
    "below we define such non - convex penalty functions and list their properties .",
    "[ theorem::assumption 1 ] the non - convex penalty function @xmath29 satisfies the following    1 .",
    "@xmath30 is continuous on @xmath31 , twice differentiable on @xmath32 and symmetric , i.e. , @xmath33 2 .",
    "@xmath34 3 .",
    "@xmath35 4 .",
    "@xmath36 5 .",
    "@xmath37     in for three values of @xmath28 .",
    "( b ) the twice continuously differentiable concave function @xmath38 in for the corresponding values of @xmath28 . ]",
    "an example of a non - convex penalty function satisfying assumption 1 is the rational penalty function @xcite defined as @xmath39 the @xmath0 norm is recovered as a special case of the non - convex rational penalty function ( i.e. , if @xmath40 , then @xmath41 ) .",
    "figure  [ fig::penalty functions](a ) shows the rational penalty function for different values of @xmath28 .",
    "other examples of penalty functions satisfying assumption [ theorem::assumption 1 ] are the logarithmic penalty @xcite , arctangent penalty @xcite and the laplace penalty @xcite .    the proximity operator of @xmath30 @xcite , @xmath42 , is defined as @xmath43 the proximity operator associated with the function @xmath26 , satisfying assumption 1 , is continuous with @xmath44 if @xmath45 .",
    "the proximity operators associated with the arctangent and the logarithmic penalty are provided in @xcite .",
    "note that for @xmath46 , the proximity operator is the soft - threshold function @xcite .",
    "the proximity operator associated with the @xmath0 norm is the well - known soft - threshold function @xcite .",
    "note that the soft - threshold function underestimates non - zero values .",
    "in contrast , the proximity operators , associated with the non - convex penalty functions satisfying assumption 1 , approach the identity function asymptotically @xcite .",
    "thus , the proximity operators used in this paper estimate the non - zero values more accurately than the @xmath0 norm .",
    "in this section we derive a condition to ensure that the function @xmath18 in is strictly convex .",
    "in particular , we show that the objective function @xmath18 is strictly convex if @xmath16 and @xmath17 lie inside a designated region . to this end , we note the following lemmas .",
    "@xcite [ lemma 1 ] let @xmath47 be a non - convex penalty function satisfying assumption [ theorem::assumption 1 ] .",
    "the function @xmath48 defined as @xmath49 is twice continuously differentiable , concave and @xmath50    the twice continuously differentiable function @xmath38 is shown in fig .",
    "[ fig::penalty functions](b ) , for three values of @xmath28 .",
    "@xcite [ lemma 2 ] let @xmath29 be a non - convex penalty function satisfying assumption 1 and @xmath51 be the function as defined in lemma 1 .",
    "the function @xmath52 defined as @xmath53 where @xmath11 and @xmath54 , is strictly convex if @xmath55    note that the proof of lemma [ lemma 2 ] in @xcite considers the case if @xmath56 , however the generalization for @xmath54 follows directly .    [ lemma 3 ] let @xmath29 be a non - convex penalty function satisfying assumption 1 and @xmath51 be the function as defined in lemma 1 .",
    "the function @xmath57 defined as @xmath58where @xmath59 , is strictly convex if @xmath60    the frobenius norm of a matrix @xmath61 can be viewed as the @xmath62 norm of a vector @xmath63 , where @xmath64 contains the entries of the matrix @xmath21 .",
    "similarly , we stack the entries of the matrix @xmath1 into a vector @xmath65 and re - write the function @xmath66 as @xmath67 in order to ensure the strict convexity of @xmath66 , we seek to ensure that the hessian of @xmath66 be positive definite ( i.e. , @xmath68 ) . to this end , the hessian of @xmath66 is given by @xmath69 where @xmath70 represents a diagonal matrix .",
    "note that the identity matrix in is of size @xmath71 . to ensure that @xmath72 is positive definite , we seek to ensure @xmath73 thus , using lemma 1 and",
    ", the hessian of @xmath66 , i.e. , @xmath72 , is positive definite if @xmath74 .",
    "the following theorem provides the critical values of the parameters @xmath16 and @xmath17 to ensure that the function @xmath18 in is strictly convex .",
    "[ theorem::convexity condition ] let @xmath47 be a parameterized non - convex penalty function satisfying assumption [ theorem::assumption 1 ] .",
    "the function @xmath75 defined as @xmath76 is strictly convex if @xmath77    let @xmath78 $ ] .",
    "consider the function @xmath79 defined as @xmath80 where @xmath51 is defined in lemma 1 .",
    "using the functions @xmath81 and @xmath66 , as defined in and respectively , the function @xmath82 can be written as @xmath83 where @xmath84 , and @xmath85 .",
    "due to lemma [ lemma 2 ] and lemma [ lemma 3 ] , the function @xmath82 is strictly convex ( being a sum of two strictly convex functions ) if @xmath16 and @xmath17 satisfy the following inequalities , @xmath86 as a result , the function @xmath82 is strictly convex if @xmath16 and @xmath17 satisfy the inequality in . recall that @xmath87 from , due to which the function @xmath18 in can be written as @xmath88\\nonumber \\\\      & \\qquad + \\lambda_1\\sum_{i=1}^{m}\\sum_{j=1}^{n } \\bigl [ s({\\mathbf{x}}_{i , j};a_1 ) + |{\\mathbf{x}}_{i , j}| \\bigr ]",
    "\\\\      = & \\dfrac{1}{2}\\|{\\mathbf{y - x}}\\|_f^2 + \\lambda_0 \\sum_{i=1}^{m } s(\\sigma_i({\\mathbf{x}});a_0 ) + \\|{\\mathbf{x}}\\| _ * \\nonumber \\\\      & \\qquad + \\lambda_1\\sum_{i=1}^{m}\\sum_{j=1}^{n } s({\\mathbf{x}}_{i , j};a_1 ) + \\|{\\mathbf{x}}\\|_1 \\\\      = & g({\\mathbf{x } } ) + \\|{\\mathbf{x}}\\| _ * + \\|{\\mathbf{x}}\\|_1 .",
    "\\end{aligned}\\ ] ] thus , the function @xmath18 is strictly convex ( being a sum of a strictly convex function and convex functions ) if @xmath16 and @xmath17 satisfy the inequality .",
    "is convex when @xmath16 and @xmath17 satisfy the inequality .",
    "( b ) the function @xmath82 is non - convex otherwise ( multiple local minima can be seen in the contour plot ) . ]",
    "the convexity condition provided by theorem 1 is illustrated in fig .",
    "[ fig::convexity condition ] .",
    "the matrix @xmath5 is constructed by tiling 10 copies of the matrix @xmath89 , @xmath90 , \\quad x_i \\in \\mathbb{r},\\end{aligned}\\ ] ] and randomly setting 70% of its entries zero .",
    "thus , the matrix @xmath5 is of rank 2 .",
    "we use @xmath91 and set @xmath92 .",
    "we set the value of @xmath93 , and @xmath94 , as per , to ensure that the function @xmath82 in is strictly convex . as seen in fig .",
    "[ fig::convexity condition](a ) , the function @xmath82 is strictly convex . however , on increasing the value of @xmath17 to @xmath95 , the function @xmath82 is non - convex , as seen in fig .",
    "[ fig::convexity condition](b ) .     in is strictly convex for all values of @xmath16 and @xmath17 inside the triangular region . ]",
    "the inequality given by theorem 1 , constitutes a convexity triangle for the function @xmath18 in . for all values of @xmath16 and @xmath17 inside the triangular region of fig .",
    "[ fig::convexity region ] the function @xmath18 is strictly convex .",
    "however , the function @xmath18 in is non - convex for values of @xmath16 and @xmath17 outside the triangular region .",
    "we use the alternating direction method of multipliers ( admm ) @xcite in conjunction with variable splitting to derive an algorithm for the solution to the islr problem .",
    "the convergence of admm to the global minimum is guaranteed when the objective function is a sum of two convex functions @xcite .",
    "the convergence of admm for a non - convex optimization problem to a stationary point is guaranteed under certain mild assumptions @xcite .",
    "the following theorem derives an algorithm for solving the islr problem and guarantees it convergence .",
    "in particular , the theorem provides a condition on the value of the scalar augmented lagrangian parameter @xmath19 , to ensure that the sub - problems of admm are strictly convex .",
    "@xmath21 , @xmath96 , @xmath97 , @xmath19 , @xmath98 @xmath99 , @xmath100 @xmath101 @xmath102 \\gets \\mbox{svd } \\left({\\mathbf{x } } - { \\mathbf{d } } \\right)$ ] @xmath103 @xmath104 @xmath105    [ theorem 2 ] let @xmath29 be a non - convex penalty function satisfying assumption 1 .",
    "let @xmath16 and @xmath17 satisfy @xmath106for @xmath107 .",
    "further , let @xmath19 be the scalar augmented lagrangian parameter .",
    "if @xmath20 , then the iterative algorithm in table  [ table::algorithm ] converges to the global minimum of the function @xmath18 in .    without loss of generality , we set @xmath108 .",
    "we re - write the islr objective function using variable splitting @xcite as @xmath109 the minimization of the islr objective function in is separable in @xmath5 and @xmath110 . applying admm to , yields the following iterative procedure , where @xmath19 is the scalar augmented lagrangian parameter and @xmath111 is update variable .",
    "[ eq::sub - problems ] @xmath112    combining the quadratic terms and ignoring the constant terms , the sub - problem can be written as @xmath113 since @xmath114 , as per the assumption , the sub - problem is strictly convex and its solution can be obtained using the proximal operator associated with @xmath30 , i.e. , @xmath115 the sub - problem is the generalized nuclear norm minimization problem , whose solution in closed form is provided by theorem 2 in @xcite .",
    "note that the solution is guaranteed to be the global minimum if @xmath116 .",
    "hence , using the inequality , the sub - problem is guaranteed to be strictly convex if @xmath20 . as a result , using @xmath117 as the singular value decomposition ( svd ) of the matrix @xmath118 , we get @xmath119 combining and , we obtain the iterative algorithm  [ table::algorithm ] , which converges to the global minimum of the islr objective function in .",
    "we illustrate the proposed islr method for estimating simultaneously sparse and low - rank matrices via the following examples .",
    "we first describe setting the parameters for the proposed method and then showcase the examples .       in at every iteration of the islr algorithm for the speech signal denoising problem . ]",
    "the proposed islr algorithm  [ table::algorithm ] requires the specification of two regularization parameters @xmath120 and @xmath121 , two penalty parameters @xmath16 and @xmath17 and the scalar augmented lagrangian parameter @xmath19 .",
    "we set the regularization parameters @xmath96 as @xmath122 where @xmath123 is the standard deviation of awgn ( or an estimate ) and @xmath124 are chosen so as to maximize the signal - to - noise ratio ( snr ) for the slr and the islr methods .",
    "the values of @xmath16 and @xmath17 are set as @xmath125 respectively .",
    "the value of @xmath126 affects the sparsity of the singular values , and the value of @xmath127 affects the sparsity of the elements of the matrix to be estimated .",
    "thus , if the sparsity of the singular values is favored over the sparsity of the elements of the matrix to be estimated , the point @xmath128 may be set in the lower - right region of the convexity triangle shown in fig",
    ".  [ fig::convexity region ] .",
    "alternatively , if the sparsity of the elements is preferred over the sparsity of the singular values of the matrix to be estimated , the point @xmath128 may be set in the upper - left region of convexity triangle in fig .",
    "[ fig::convexity region ] .",
    "we set the value of @xmath19 as @xmath129 . as per theorem 2",
    ", the islr algorithm listed in table 1 is guaranteed to converge to the global minimum for @xmath20 . however , depending on the value of @xmath19 , the convergence may be slow . as such for this example , and the one that follows , we run the islr algorithm till",
    "a certain tolerance level is reached , i.e. , we run the algorithm till @xmath130 where @xmath98 is a user - defined tolerance level , usually set to @xmath131 .",
    "the value of the objective function for 20 iterations of the islr algorithm , with different values of @xmath19 , is shown in fig .",
    "[ fig::cost function history ] .",
    "[ fig::loglog plot ] shows the log - log plot of the stopping criteria for the islr objective function and the slr objective function .",
    "note that for the examples that follow , we use the arctangent penalty @xcite as the nonconvex penalty for the proposed islr method .",
    "we generate a synthetic matrix @xmath132 of rank @xmath133 using two random matrices @xmath134 and @xmath135 such that @xmath136 where the entries of @xmath137 and @xmath138 are chosen from an i.i.d standard normal distribution . to measure the performance of the proposed islr method and the slr method",
    ", we use the normalized root square error ( rse ) defined as @xmath139 where @xmath140 represents the estimated matrix and @xmath141 represents the desired clean matrix .",
    "we consider two types of simulations : rse as a function of the rank ( @xmath133 ) of the synthetically generated matrix and rse as a function of the sparsity level of the input matrix .",
    "for the first simulation , we fix the sparsity level at 60% ( i.e. , approximately 40% of the entries of the clean input matrix @xmath142 are zero ) while varying the rank @xmath133 of the input matrix .",
    "we add white gaussian noise ( @xmath143 ) to @xmath142 to generate a noisy input matrix @xmath21 .",
    "we generate 15 matrices for each value of the rank @xmath133 where @xmath144 in increments of @xmath145 and denoise them using the proposed islr method and the slr method . for the second simulation ,",
    "wherein we consider the rse as a function of the level of sparsity of the input matrix , we synthetically generate 15 matrices @xmath142 of fixed rank @xmath146 but with varying levels of sparsity ( from 10% to 90% ) .",
    "again , we add white gaussian noise ( @xmath143 ) to generate the noisy matrix @xmath21 .",
    "figure  [ fig::average rse as a function](a ) shows the average rse values as a function of the rank @xmath133 of the input matrix .",
    "figure  [ fig::average rse as a function](b ) shows the average rse values as a function of the level of sparsity of the input matrix .",
    "the proposed islr method consistently obtains lower rse values than the slr method . as expected , for both the methods , the proposed islr method and the slr method , rse values are lower for matrices that are relatively more sparse .",
    "on the other hand , as seen in fig .",
    "[ fig::average rse as a function](a ) , we observe that for matrices that are not necessarily low - rank but have decaying singular values , lower values of rse are obtained for both the methods .",
    "note that for both the simulations , we do a grid search over a range of values for @xmath147 and @xmath148 to obtain the values of @xmath120 and @xmath121 respectively , which yield the lowest rse values ( recall that @xmath149 , for @xmath150 ) .",
    "furthermore , in both the simulations , we fix the value of @xmath151 at @xmath152 for setting the values of @xmath16 and @xmath17 .",
    "we consider the problem of denoising a speech signal with awgn .",
    "we apply the sparse low - rank matrix estimation methods to the spectrogram of the noisy speech signal , and invert the estimated spectrogram to the time domain to obtain the denoised speech signal . specifically ,",
    "if @xmath153 is the input speech signal , then the denoised estimate @xmath154 is obtained using islr as @xmath155 where @xmath156 and @xmath157 represent the short - time fourier transform ( stft ) and its inverse , respectively . for this example",
    ", we set @xmath156 to be an over - complete stft , implemented with perfect reconstruction , i.e. , @xmath158 . the stft is implemented with 50% overlap between the windows , for a window size of 64 samples .",
    "the dft length is set to 512 samples .",
    "we add awgn @xmath159 to realize the noisy speech signal .",
    "we compare the proposed islr method with the slr method and the nonconvex sparse low - rank matrix estimation method @xcite which uses the weighted nuclear norm @xcite and the weighted @xmath0 norm @xcite .",
    "the spectrogram of the clean speech signal is shown in fig .",
    "[ fig::speech signal denoising](a ) .",
    "it can be seen that the spectrogram consists of repeated ridges .",
    "the spectrogram of the noisy signal is shown in fig .",
    "[ fig::speech signal denoising](b ) .",
    "figure  [ fig::speech signal denoising](c ) shows the denoised spectrogram obtained using the slr method and fig .",
    "[ fig::speech signal denoising](d ) shows the denoised spectrogram using the islr method .",
    "[ fig::speech signal denoising](e ) shows the spectrogram estimated using the nonconvex method with weighted @xmath0 norm and the weighted nuclear norm ( see algorithm 1 in @xcite ) .",
    "the islr estimated spectrogram has a higher snr than the slr estimated spectrogram and contains fewer artifacts .",
    "the nonconvex method obtains a slightly higher snr than the proposed islr method and also contains relatively fewer artifacts than the slr method .",
    "the proposed islr method obtains snr values comparable to the state - of - the - art nonconvex method , while being able to guarantee convergence to the unique global minimum .     with fixed @xmath121 for the islr and the slr methods .",
    "]    figure  [ fig::snr vs lam ] shows the snr as a function of the regularization parameter @xmath121 , when @xmath120 is fixed , for the slr and the islr methods .",
    "note that the standard deviation @xmath123 of the noise level is also fixed at @xmath159 .",
    "the improvement in the snr value , when using the islr method , is the same when the value of @xmath120 is also varied , in addition to the value of @xmath121 .",
    "the snr values are obtained by averaging over 15 realizations for each @xmath160 pair .      protein interactions in the `",
    "escherichia coli ' bacteria , scored by strength in @xmath161 $ ] , were studied in @xcite .",
    "the data can be represented as a weighted graph , which is sparse and low - rank by nature @xcite .",
    "the rationale behind the low - rank property of the weighted graph , is that the interactions between two sets of proteins are governed by a small set of factors @xcite .",
    "figure  [ fig::protein example](a ) shows the protein interaction data as a weighted adjacency matrix .",
    "the adjacency matrix is obtained after retaining 440 proteins of the entire set of 4394 proteins .",
    "we corrupt 10% of the entries of the clean adjacency matrix , selected uniformly at random , with uniform noise in the interval @xmath162 $ ] .",
    "the noisy adjacency matrix is shown in fig .",
    "[ fig::protein example](b ) , with @xmath163 .",
    "we set the parameters @xmath96 and @xmath164 as in the previous example .",
    "figure  [ fig::protein example](c ) and fig .",
    "[ fig::protein example](d ) show the denoised adjacency matrices obtained using the islr and the slr methods , respectively . as in the case of previous example",
    ", the islr method offers a better rse , and tends to correctly estimate the sparsity - pattern of the true matrix .    .",
    "]    in order to assess the relative performance of the proposed islr method in comparison to the slr method , we realize 15 noisy adjacency matrices and denoise them . for each value of @xmath123",
    ", we choose the parameters @xmath96 , for both the methods , that yields the lowest rse .",
    "shown in fig .",
    "[ fig::rse as a func of sigma ] are the average rse values as a function of @xmath123 .",
    "it can be seen that the islr method consistently offers a lower rse .",
    "we consider the problem of estimating a sparse low - rank matrix from its noisy observation .",
    "we generalize the convex formulation proposed for estimation of sparse low - rank matrix estimation @xcite , by utilizing non - convex sparsity - inducing regularizers .",
    "the non - convex penalty functions proposed are known to estimate the non - zero signal values more accurately .",
    "we show how to set the parameters of the non - convex penalty functions , so as to preserve the convexity of the overall problem ( sum of data - fidelity and the rssegularization terms ) .",
    "the critical value of the non - convex penalty parameters define a convexity triangle ; for all values of the nonconvex penalty parameters within this triangular region , the cost function is guaranteed to be strictly convex .",
    "we derive an efficient algorithm using admm with a single variable - splitting which solves the proposed convex objective function consisting of non - convex regularizers .",
    "we guarantee the convergence of admm to the global minimum of the objective function , provided the scalar augmented lagrangian parameter @xmath19 is chosen such that @xmath20 .",
    "we illustrate several examples to demonstrate the effectiveness of the proposed formulation for estimation of sparse low - rank matrices .",
    "the proposed method utilizes separable penalty functions ( nonconvex ) to induce sparsity stronger than separable convex penalty functions .",
    "a possible future direction involves the use of non - separable penalty functions , possibly nonconvex , that are designed so as to ensure the strict convexity of the objective function @xcite .",
    "the authors thank the anonymous reviewers for their detailed suggestions and corrections .",
    "this work was supported by the onr under grant n00014 - 15 - 1 - 2314 and the nsf under grant ccf-1525398 .",
    "10                                                                    a.  lanza , s.  morigi , and f.  sgallari . .",
    "in j .- f . aujol , m.  nikolova , and n.  papadakis , editors , _ scale sp .",
    "methods comput .",
    "_ , volume 9087 of _ lecture notes in computer science _ , pages 666677 .",
    "springer , 2015 ."
  ],
  "abstract_text": [
    "<S> we address the problem of estimating a sparse low - rank matrix from its noisy observation . </S>",
    "<S> we propose an objective function consisting of a data - fidelity term and two parameterized non - convex penalty functions . </S>",
    "<S> further , we show how to set the parameters of the non - convex penalty functions , in order to ensure that the objective function is strictly convex . the proposed objective function better estimates sparse low - rank matrices than a convex method which utilizes the sum of the nuclear norm and the @xmath0 norm . </S>",
    "<S> we derive an algorithm ( as an instance of admm ) to solve the proposed problem , and guarantee its convergence provided the scalar augmented lagrangian parameter is set appropriately . </S>",
    "<S> we demonstrate the proposed method for denoising an audio signal and an adjacency matrix representing protein interactions in the ` escherichia coli ' bacteria .    </S>",
    "<S> low - rank matrix , sparse matrix , speech denoising , nonconvex regularization , convex optimization </S>"
  ]
}