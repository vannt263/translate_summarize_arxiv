{
  "article_text": [
    "we consider two related , but distinct tasks : domain adaptation ( da ) @xcite and multitask learning ( mtl ) @xcite . both involve learning related hypotheses on multiple data sets . in da , we learn multiple classifiers for solving the _ same problem _ over data from _ different distributions . _ in mtl , we learn multiple classifiers for solving _ different problems _ over data from the _ same distribution_. seen from a bayesian perspective , a natural solution is a hierarchical model , with hypotheses as leaves @xcite .",
    "however , when there are more than two hypotheses to be learned ( i.e. , more than two domains or more than two tasks ) , an immediate question is : are all hypotheses equally related ? if not , what is their relationship ?",
    "we address these issues by proposing two hierarchical models with _ latent _ hierarchies , one for da and one for mtl ( the models are nearly identical ) .",
    "we treat the hierarchy nonparametrically , employing kingman s coalescent @xcite .",
    "we derive an em algorithm that makes use of recently developed efficient inference algorithms for the coalescent @xcite . on several da and mtl problems ,",
    "we show the efficacy of our model .    our models for da and mtl share a common structure based on an unknown hierarchy .",
    "the key difference between the da model and the mtl model is in what information is shared across the hierarchy . for simplicity , we consider the case of linear classifiers ( logistic regression and linear regression ) .",
    "this can be extended to non - linear classifiers by moving to gaussian processes @xcite . in domain adaption ,",
    "a useful model is to assume that there is a single classifier that `` does well '' on all domains @xcite . in the context of hierarchical bayesian modeling , we interpret this as saying that the weight vector associated with the linear classifier is generated according to the hierarchical structure . on the other hand , in mtl , one does _ not _ expect the same weight vector to do well for all problems .",
    "instead , a common assumption is that features co - vary in similar ways between tasks @xcite . in a hierarchical bayesian model",
    ", we interpret this as saying that the covariance structure associated with the linear classifiers is generated according to the hierarchical structure . in brief : for da , we share weights ; for mtl , we share covariance .",
    "yu et al . @xcite have presented a linear multitask model for domain adaptation . in the linear multitask model , a shared mean and covariance",
    "is generated by a normal - inverse - wishart prior , and then the weight vector for each task is generated by a gaussian conditioned on this shared mean and variance .",
    "the key idea in the linear multitask model @xcite is to model feature covariance ; this is also the intuition behind the informative priors model @xcite , carried out in a more bayesian framework .",
    "( the linear multitask model is almost identical to the _ conjoint analysis _",
    "model @xcite ) .",
    "xue et al .",
    "@xcite present a dirichlet process mixture model formulation , where domains are clustered into groups and share a single classifier across groups .",
    "this helps to prevent `` negative transfer '' ( the effect of `` unrelated '' tasks negatively affecting performance on other tasks ) .",
    "xue et al.s model is effectively a _ task - clustering _ model ,",
    "in which some tasks share common structure ( those in the same cluster ) , but are otherwise independent from other tasks ( those in other clusters ) .",
    "this work was later improved on by dunson , xue and carin @xcite in the formulation of the matrix stick breaking process : a more flexible approach to bayesian multitask learning that allows for more sharing .",
    "this is also a large body of work on non - bayesian approaches to multitask learning and domain adaptation .",
    "bickel et al .",
    "@xcite offer an extension of the logistic regression model that simultaneously learns a good classifier and a classifier to provide instance weights for out of sample data .",
    "this approach is only applicable when no labeled `` target '' data is available , but much unlabeled target data is .",
    "blitzer , mcdonald and pereira @xcite present another approach to this `` unsupervised '' setting of domain adaptation that makes use of prior knowledge of features that are expected to behave similarly across domains .",
    "both of these approaches are developed only in the two - domain setting .",
    "dredze and crammer @xcite describe an online approach for dealing with the many - domains problem , sharing information across domains via confidence - weighted classifiers .",
    "-coalescent . ]    our model for da and mtl makes use of a latent hierarchical structure .",
    "being bayesian , we wish to attach a prior distribution to this hierarchy . a convenient choice of prior is kingman s coalescent @xcite .",
    "our description and notation is borrowed directly from @xcite .",
    "kingman s coalescent originated in the study of population genetics for a set of haploid organisms ( organisms which have only a single parent ) .",
    "the coalescent is a nonparametric model over a countable set of organisms .",
    "it is most easily understood in terms of its finite dimensional marginal distributions over @xmath0 individuals , in which case it is called an @xmath0-coalescent .",
    "we then take the limit @xmath1 . in our case",
    ", the @xmath0 individuals will correspond to @xmath0 classifiers ( tasks ) .",
    "the @xmath0-coalescent considers a population of @xmath0 organisms at time @xmath2 ( see figure  [ fig : coalescent ] for an example with @xmath3 ) .",
    "we follow the ancestry of these individuals backward in time , where each organism has exactly one parent at time @xmath4 .",
    "the @xmath0-coalescent is a continuous - time , partition - valued markov process which starts with @xmath0 singleton clusters at time @xmath2 and evolves _ backward _ , coalescing lineages until there is only one left .",
    "we denote by @xmath5 the _ time _ at which the @xmath6th coalescent event occurs ( note @xmath7 ) , and @xmath8 the time between events ( note @xmath9 ) . under the @xmath0-coalescent ,",
    "each pair of lineages merges independently with exponential rate @xmath10 ; so @xmath11 . with probability one , a random draw from the @xmath0-coalescent is a binary tree with a single root at @xmath12 and @xmath0 individuals at time @xmath2 .",
    "we denote by @xmath13 the tree structure and by @xmath14 the collection of @xmath15 .",
    "leaves are denote by @xmath16 and internal nodes by @xmath17 , where @xmath6 indexes a coalescent event ( see figure  [ fig : coalescent ] ) .",
    "the marginal distribution over tree topologies is uniform and independent of @xmath18 ; and the model is infinitely exchangeable .",
    "we consider the limit as @xmath1 , called _ the coalescent .",
    "_    once the tree structure is obtained , one can define an additional markov process to evolve over the tree .",
    "one common , and easy to understand , choice is a brownian diffusion process . in brownian diffusion in @xmath19 dimensions , we assume an underlying diffusion covariance of @xmath20 positive semi - definite . the root is a @xmath19-dimensional vector drawn @xmath21 .",
    "each @xmath22 is drawn @xmath23 , where @xmath24 is the parent of @xmath6 in the tree .",
    "@xmath25s are drawn conditioned on their parent .",
    "the coalescent is a very popular model in population genetics ( it corresponds to a limiting case of the wright - fisher model ) , but has been plagued with the lack of efficient inference algorithm .",
    "( most inference occurs by metropolis - hastings sampling over tree structures . )",
    "recently , teh et al .",
    "@xcite proposed a collection of efficient bottom - up agglomerative inference algorithms for the coalescent .",
    "the one we make use is called and proceeds in a greedy manner , merging nodes that want to coalesce most quickly . in the case of",
    ", the exponential rate is fixed as @xmath10 .",
    "belief propagation is used to marginalize out internal nodes @xmath17 .",
    "if we associate with each node in the tree a _ mean _ @xmath26 and _ variance _ @xmath27 message , we can compute messages as eq  , where @xmath6 is the current node and @xmath28 and @xmath29 are its children .",
    "@xmath30\\inv \\label{eq : bp}\\\\ \\vec y_{i } & = \\left [ \\vec y_{li } ( \\vec v_{{li}}+(t_{li}-t_i)\\vec \\la)\\inv +   \\vec y_{ri } ( \\vec v_{{ri}}+(t_{ri}-t_i)\\vec \\la)\\inv\\right]\\inv \\vec v_{i } \\nonumber\\end{aligned}\\ ] ]    importantly , this model is applicable when the @xmath25s are not _ known _ entirely , but are represented by gaussians .",
    "this can be done efficient since , given a hierarchical structure , inference is simply message passing in a gaussian random field .",
    "( we will need this property in order to perform expectation - maximization . )",
    "in this section , we present a model for domain adaptation ( da ) and a model for multitask learning ( mtl ) , plus some minor variants .",
    "( the variants are evaluated in section  [ sec : experiments ] . ) as mentioned previously , the _ structure _ of the two models is the same : they differ in what information is shared .    to fix notation , suppose that we wish to learn @xmath31 different hypotheses ( @xmath31 domains in da or @xmath31 tasks in mtl ) .",
    "we suppose that we have training data for each hypothesis , with @xmath32 labeled examples examples for hypothesis @xmath33 .",
    "( notational confusion warning : in reference to the coalescent , the @xmath31 hypotheses will be the leaves of the coalescent tree , so this is more akin to a @xmath31-coalescent . )",
    "the inputs are drawn from @xmath34 and outputs from @xmath35 , where @xmath36 for regression tasks or @xmath37 for classification tasks .",
    "we assume a distribution @xmath38 over @xmath34 for each hypothesis ( in mtl , we assume identical distributions @xmath39 ) .",
    "our data thus has the form @xmath40 \\ } : k \\in [ k ] \\}$ ] , where @xmath41 = \\ { 1 , \\dots , i \\}$ ] , @xmath42 is the @xmath43th input for task @xmath33 and @xmath44 is the corresponding label .",
    "each @xmath45 iid .",
    "we will be using linear or logistic regression , parameterized by hypothesis - specific weight vectors @xmath46 , where predictions are made on the basis of @xmath47 .",
    "one important design choice in both our models is whether we explicitly model the input @xmath48 . in the cases where we do _ not _ , our model is a conditional model of the form @xmath49 . in the cases where we _ do _ , our model is a joint model that factorizes as @xmath50 . in this case",
    ", the same tree structure is used to model both the conditional likelihood of @xmath26 given @xmath48 _ and _ the data itself . in effect , this gives more data on which to learn the tree structure , at the cost that it might not be directly related to the prediction problem .",
    "we refer to this choice in the future as `` model the data . ''",
    "we propose the following model for domain adaptation .",
    "the basic idea is to generate a tree structure according to a @xmath31-coalescent and then propagate weight vectors along this tree .",
    "the root of the tree corresponds to the `` global '' weight vector and the leaves correspond to the task - specific weight vectors .",
    "we assume the weight vectors evolve according to brownian diffusion .",
    "our generative story is :    1 .",
    "choose a global _ mean _ and _ covariance _ @xmath51 .",
    "the normal - inverse - wishart distribution with prior mean @xmath52 , prior covariance @xmath53 and @xmath54 degrees of freedom . ]",
    "2 .   choose a tree structure @xmath55 over @xmath31 leaves .",
    "3 .   for each non - root node",
    "@xmath6 in @xmath13 ( top - down ) : 1 .",
    "choose @xmath56 , where @xmath57 is the parent of @xmath6 in @xmath13 .",
    "4 .   for each domain @xmath58 $ ] : 1 .   denote by @xmath59 where @xmath6 is the leaf in @xmath13 corresponding to domain @xmath33 .",
    "2 .   for each example",
    "@xmath60 $ ] : 1 .",
    "choose input @xmath45 .",
    "2 .   choose output @xmath44 by : + regression : : :    @xmath61 classification : : :    @xmath62    here , @xmath63 and @xmath64 are hyperparameters that we assume are known ( we use held - out data to set them ) .",
    "we consider the following variants of this model : is @xmath65 is assumed diagonal or full ?",
    "do we explicitly model the data ?",
    "we call these :    : :    diagonal @xmath65 , do not model the data . : :    diagonal @xmath65 , do model the data . : :    full @xmath65 , do not model the data . : :    full @xmath65 , do model the data",
    ".    in the case where the input data is modeled explicitly ( i.e. , and ) , we assume a base parameter vector over @xmath66 generated at the root ( in step ( 1 ) ) , propogated down the tree ( in step ( 3 ) ) and used to generate the inputs @xmath67 ( in step ( 4.b.i ) ) . in the case that the input is modeled , we _ always _ assume diagonal covariance on the input .",
    "for continuous data , we use a gaussian mutation kernel , as in step 4.a . for discrete data , we use a multinomial equilibrium distribution @xmath68 and transition rate matrix @xmath69 where @xmath70 is a vector of @xmath31 ones , while the transition probability matrix for entry @xmath71 in a time interval of length @xmath72 is @xmath73 .      in the multitask learning case",
    ", we no longer wish to share the weight vectors , but rather wish to share their _ covariance _ structure .",
    "this model is slightly more difficult to specify because brownian motion no longer makes sense over a covariance structure ( for instance , it will not maintain positive semi - definiteness ) .",
    "our solution to this problem is to _ decompose _ the covariance structure into correlations and standard deviations .",
    "we assume a constant , global _",
    "correlation _ matrix and only allow the standard deviations to evolve over the tree .",
    "( the idea of decomposing the covariance comes from @xcite , section 19.2 . )",
    "we model the _ log _ standard deviations using brownian diffusion .",
    "in particular , our model assumes that each node in the tree is associated with a diagonal log standard deviation matrix @xmath74 .",
    "the weight vector for task @xmath33 is then drawn gaussian with zero mean and covariance given by @xmath75 , where @xmath76 are the shared correlations ( with diagonal elements equal to @xmath10 ) .",
    "our prior on @xmath77 is : @xmath78 here , @xmath79 is the @xmath6th principle submatrix of @xmath77 .",
    "this is the marginal distribution of @xmath77 when @xmath80 has an inverse - wishart prior with identity prior covariance and @xmath81 degrees of freedom , which leads to uniform marginals for each pairwise correlation .",
    "given this setup , our multitask learning model has the following generative story :    1 .",
    "choose @xmath77 by eq   and deviation covariance @xmath82 .",
    "2 .   choose a tree structure @xmath55 over @xmath31 leaves .",
    "3 .   for each non - root node",
    "@xmath6 in @xmath13 ( top - down ) : 1 .",
    "choose @xmath83 , where @xmath57 is the parent of @xmath6 in @xmath13 .",
    "4 .   for each task",
    "@xmath58 $ ] : 1 .",
    "choose @xmath84 by ( @xmath6 is the leaf associated with task @xmath33 ) : @xmath85 2 .",
    "for each example @xmath60 $ ] : 1 .",
    "choose input @xmath86 .",
    "2 .   choose output @xmath44 by : + regression : : :    @xmath61 classification : : :    @xmath62    the steps that _ differ _ from the the domain adaptation model are marked with an arrow ( @xmath87 ) .      for both the da and mtl models , we perform inference using an expectation - maximization algorithm . the _ latent variables _ in both algorithms are the variables associated with the leaves of the trees ( in da : the weight vectors ; in mtl : the log standard deviations ) .",
    "the _ parameters _ are everything else : the tree structure @xmath13 and times @xmath14 , the brownian covariance @xmath65 and all other prior parameters .",
    "we begin with the domain adaptation model . for simplicity",
    ", we consider the case where the input data is _ not _ modeled . in the e - step ,",
    "we compute expectations over the leaves ( classifiers ) . in the m - step ,",
    "we optimize the tree structure and the other hyperparameters .",
    "[ [ e - step ] ] e - step : + + + + + + +    the e - step can be performed exactly in the case of regression ( the expectations of the classifiers are simply gaussian ) . in the case of classification , we approximate the expectations by gaussians ( via the laplace approximation ) . in particular , for each domain @xmath33 , we compute :    @xmath88    in eq  , @xmath89 is the prior on @xmath90 given by its parent in the tree ; the likelihood term is the data likelihood ( logistic for classification , or gaussian for regression ) .",
    "we solve the optimization problem by conjugate gradient .",
    "@xmath84 is the mean of the gaussian representing the expectation of the @xmath33th weight vector .",
    "the covariance of the estimate is @xmath91 , with @xmath92 diagonal . for regression , @xmath93 ; for classification , @xmath94 has entries @xmath95 , where @xmath96 .",
    "[ [ m - step ] ] m - step : + + + + + + +    here , we optimize @xmath97 by integrating out @xmath98s associated with internal nodes ( using belief propagation ) . this can be done efficiently using the algorithm @xcite .",
    "optimize @xmath99 as the mode of an inverse - wishart with @xmath100 degrees of freedom and mean @xmath101 :    @xmath102    here , @xmath103 and @xmath104 are the left and right children respectively of node @xmath6 in @xmath13 .",
    "@xmath105 is the variance of node @xmath6 ( obtained by eq   for leaves or via belief propagation for internal nodes ) .",
    "the sum in eq   ranges over all non - leaf nodes in @xmath13 .",
    "we initialize em by computing @xmath84 for each task according to a maximum a posteriori estimate with zero mean and @xmath106 variance .",
    "this initialization effectively assumes no shared structure .",
    "constructing an exact em algorithm for the multitask learning model is significantly more complex .",
    "the complexity arises from the convolution of the normal ( over @xmath90 ) with the log - normal ( over @xmath107 ) .",
    "this makes the computation of exact expectations ( over @xmath107 ) intractable .",
    "we therefore use the popular `` hard em '' approximation , in which we estimate the expectation of the latent variables ( @xmath107 ) with a point mass centered at their mode .",
    "( experiments in the domain adaptation model show that the hard em approximation to @xmath90 does not affect results . )",
    "the only additional complication is that of optimizing @xmath77 ( the overall correlations ) and each @xmath108 ( the per - node standard deviations ) .",
    "@xmath77 can be handled exactly as @xmath65 in the domain adaptation case : see eq  , but constrained to have ones along the diagonal . the case for @xmath108 is slightly more involved .",
    "we first maximize @xmath90 as before , and then also maximize @xmath107 .",
    "the log posterior and its derivative have the forms below , where @xmath109 is a constant independent of @xmath110 and @xmath111 : @xmath112 \\\\ & \\quad    - \\frac 1 2 \\tr \\left[\\mat w ( e^{-\\mat s } \\mat r\\inv e^{-\\mat s } ) \\mat w\\right ]     + c\\\\ \\grad_{\\mat s } \\log p(\\mat s ) & = - \\mat i     - ( \\mat s - \\mat p ) \\vec \\la\\inv     + \\mat w ( e^{-\\mat s } \\mat r\\inv e^{-\\mat s } ) \\mat w\\end{aligned}\\ ] ] here @xmath113 is the ( diagonal ) matrix at the _ parent _ of the current node in the hierarchy .",
    "we optimize @xmath110 by gradient descent with step size @xmath114 until convergence of @xmath110 to @xmath115 .",
    "[ cols=\"<,<,^,^,>,<,>,<\",options=\"header \" , ]     the results for all data sets and all methods are shown in table  [ tab : scores ] . here , we also compare all five settings of the coalescent model ( full covariance and diagonal covariance , with and without the data , and then the tree derived just by clustering the data ) . here , we can see that the more complex coalescent - based models tend to outperform the other approaches .",
    "our first experiment is on sentiment analysis data gathered from amazon @xcite .",
    "the task is to predict whether a review is positive or negative based on the text of the review .",
    "there are eight domains in this task : apparel ( a ) , books ( b ) , dvd ( d ) , electronics ( e ) , kitchen ( k ) , music ( m ) , video ( v ) and other ( o ) .",
    "if we cluster these tasks on the basis of the _ data _ , we obtain the tree shown in figure  [ fig : sentres - coal ] .            in our first experiment",
    ", we treat every domain equally and vary the amount of data used to learn a model . in figure",
    "[ fig : sentres - alltasks ] , we show the results of the coalescent - based model ( with full covariance but without data : ) , baselines , and comparison methods .",
    "as we can see , the coalescent - based approach dominates , even with very many data points ( @xmath116 per domain ) . in table  [ tab : scores ] , we see that moving from full to diagonal covariance does not hurt significantly . adding the data hurts performance significantly , and brings the performance down to the level of , the model that uses the data - based tree . in comparison",
    "to previously published results on this problem @xcite , our results are not quite as good .",
    "however , prior results depend on a large amount of prior knowledge in terms of `` pivot features , '' which our model does not require , and also begin with a different feature representation .        in figure",
    "[ fig : sentres2 ] , we show the trees after ten iterations of em .",
    "we can see a difference between these trees and the tree built just on the data ( cf .",
    ", figure  [ fig : sentres - coal ] ) .",
    "for instance , the data tree thinks that `` music '' is more like `` appliances '' than it is like `` dvds , '' something that does not happen in the em tree .        in the next experiments , we select one task as the `` target '' .",
    "we use @xmath116 examples from all the `` source '' tasks and vary the amount of labeled target data .",
    "we perform an evaluation on four targets , the same as those used previously @xcite : books , dvd , electronics and kitchen .",
    "these results are shown in figure  [ fig : sentres3 ] . here , we again see that the coalescent - based approach outperforms the baselines .",
    "however , for many of these per - target results , the feda baseline is the consistent - best alternative .",
    "one somewhat surprising result is that adding more and more target data does not appear to help significantly for this problem .",
    "the second domain adaptation task we attempt is landmine detection @xcite . to conserve space , we only present overall results and results for one subtask : the last one . to uncrowd the figure",
    ", we also limit the baseline models to a subset of approaches ; recall that the full results are shown in table  [ tab : scores ] .",
    "these are shown in figure  [ fig : landmine ] .",
    "note that the performance measure here is auc : there are very few positives in this data ( around @xmath117 ) . here , we see that on the target - based evaluation , the coalescent - based approach dominates . for small amounts of data it performs equivalently to , but",
    "the gap increases for more data .",
    "our final evaluation is on data drawn from 20-newsgroups . here",
    ", we construct @xmath118 binary classification problems , each of which is its own task .",
    "we use an identical setup to previous work @xcite . as before , we present overall results and then results for two subtasks .",
    "the subtasks we choose are `` baseball versus politics '' and `` ibm hardware versus forsale ''  these were chosen as an example of good and bad transfer from previous studies @xcite . here , we have cut out the baseline because it does not make sense in a pure mtl setting . to uncrowd the figure",
    ", we also limit the baseline models to a subset of approaches ; recall that the full results are shown in table  [ tab : scores ] .",
    "the results are in figure  [ fig:20ng ] . here",
    ", we see that the coalescent - based model overall outperforms the baselines , and further maintains an advantage for baseball - versus - politics , for which we expect a reasonable amount of transfer .",
    "one significant difference between these results and the da results is that on the per - target results , in the da case , our model continued to outperform .",
    "however , in the mtl case , with enough labeled target data , the independent classifiers quickly catch up . in comparison to prior results on this problem @xcite , our rate of improvement is roughtly comparable .",
    "one additional question that arises in work related to a large number of domains ( or tasks ) is whether the addition of unrelated domains can damage a model . in this section ,",
    "we explore the effect of the addition of unrelated domains on our learning algorithms .",
    "we simulate this on the sentiment data by adding a task obtained by scrambling the features of one of the true tasks , where we vary the percentage scrambled .",
    "the results are shown in figure  [ fig : sentbogus ] . here , we see that there is a slighly trend toward degradation in performance for the original tasks as the amount of noise in the new task increases .",
    "this is true for all of the learning algorithms ; unfortunately , this includes our own .",
    "one would hope that the model could learn to not share information with this irrelevant task , but apparently the prior toward short trees is too strong to overcome the noiser .",
    "addressing this remains open .",
    "we have presented two models : one for domain adaptation ( da ) and one for multitask learning ( mtl ) .",
    "inference in our models is based on expectation maximization .",
    "we observe significant performance improvements on three very different data sets from our models .",
    "the only distinction between the models is what aspects are shared .",
    "we believe this is a reasonable way to divide up the da / mtl landscape .",
    "two interesting special cases fall out of our model .",
    "first , if we set @xmath119 and construct a tree where every node branches directly from the root , our model is precisely the linear multitask model proposed by yu et al .",
    "second , we consider the fact that a special case of the coalescent can describe the same distribution as a _ dirichlet process _",
    "through this view , we can see that dirichlet - process based multitask model of xue et al .",
    "@xcite is achieved as a special case .",
    "there are several ideas in the literature for both da and mtl that are not reflected in our model .",
    "an easy example is the idea that it should be difficult to build a classifier for separating source from target data in a da context @xcite .",
    "similar ideas have been exploited in discriminative models for domain adaptation @xcite .",
    "however , these models are most successful when there is _ no _ labeled target data : a case we have not considered .",
    "it is an open question to address this in our framework ."
  ],
  "abstract_text": [
    "<S> we learn multiple hypotheses for related tasks under a latent hierarchical relationship between tasks . </S>",
    "<S> we exploit the intuition that for _ domain adaptation _ , we wish to share classifier structure , but for _ multitask learning _ , we wish to share covariance structure . </S>",
    "<S> our hierarchical model is seen to subsume several previously proposed multitask learning models and performs well on three distinct real - world data sets . </S>"
  ]
}