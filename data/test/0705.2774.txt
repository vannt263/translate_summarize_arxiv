{
  "article_text": [
    "linear regression is one of the most common statistical techniques used in astronomical data analysis . in general ,",
    "linear regression in astronomy is characterized by intrinsic scatter about the regression line , and measurement errors in both the independent and dependent variables .",
    "the source of intrinsic scatter is variations in the physical properties of astronomical sources that are not completely captured by the variables included in the regression .",
    "it is important to correctly account for both measurement error and intrinsic scatter , as both aspects can have a non - negligible effect on the regression results . in particular , ignoring the intrinsic scatter and weighting the data points solely by the measurement errors can result in the higher - precision measurements being given disproportionate influence on the regression results .",
    "furthermore , when the independent variable is measured with error , the ordinary least squares ( ols ) estimate of the regression slope is biased toward zero ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "when there are multiple independent variables , measurement error can have an even stronger and more unpredictable effect @xcite .",
    "in addition , the existence of non - detections , referred to as ` censored data ' , in the data set will result in additional complications ( e.g. , * ? ? ? * ) .",
    "therefore , when performing regression , it is essential to correctly account for the measurement errors and intrinsic scatter in order to ensure that the data analysis , and thus the scientific conclusions based on it , are trustworthy .",
    "many methods have been proposed for performing linear regression when intrinsic scatter is present and both variables are measured with error .",
    "these include methods that correct the observed moments of the data ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) , minimize an ` effective ' @xmath1 statistic ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) , assume a probability distribution for the true independent variable values ( so - called  structural equation models ",
    ", e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) ; bayesian approaches to these models have also been developed ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "in addition , methods have been proposed to account for measurement error in censored regression ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "the most commonly used methods in astronomy are the bces estimator @xcite and the ` fitexy ' estimator @xcite .",
    "both methods have their advantages and disadvantages , some of which have been pointed out by @xcite .",
    "however , neither method is applicable when the data contain non - detections .    in this work",
    "i describe a bayesian method for handling measurement errors in astronomical data analysis .",
    "my approach starts by computing the likelihood function of the complete data , i.e. , the likelihood function of both the unobserved true values of the data and the measured values of the data .",
    "the measured data likelihood is then found by integrating the likelihood function for the complete data over the unobserved true values ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "this approach is known as ` structural equation modelling ' of measurement error problems , and has been studied from both a frequentist approach ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) and a bayesian approach ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) . in this work ,",
    "i extend the statistical model of @xcite to allow for measurement errors of different magnitudes ( i.e. , ` heteroscedastic ' errors ) , non - detections , and selection effects , so long as the selection function can be modelled mathematically .",
    "our method models the distribution of independent variables as a weighted sum of gaussians .",
    "the mixture of gaussians model allows flexibility when estimating the distribution of the true values of the independent variable , thus increasing its robustness against model mispecification ( e.g. , * ? ? ?",
    "the basic idea is that one can use a suitably large enough number of gaussians to accurately approximate the true distribution of independent variables , even though in general the individual gaussians have no physical meaning .",
    "the paper is organized as follows . in ",
    "[ s - notation ] we summarize some notation , and in ",
    "[ s - measerr ] i review the effects of measurement error on the estimates for the regression slope and correlation coefficient . in   [ s - statmod ]",
    "i describe the statistical model and derive the likelihood functions , and in ",
    "[ s - dcissues ] i describe how to incorporate knowledge of the selection effects and account for non - detections . in ",
    "[ s - prior ] i describe the prior distribution for this model , and in   [ s - markov ] i describe a gibbs sampler for sampling from the posterior distributions . in   [ s - sims ] i use simulation to illustrate the effectiveness of this structural model and compare with the ols , bces(@xmath2 ) , and fitexy estimators . finally , in   [ s - data ]",
    "i illustrate the method using astronomical data by performing a regression of the x - ray photon index , @xmath3 , on the eddington ratio using a sample of 39 @xmath4 radio - quiet quasars",
    ". sections [ s - statmod ] , [ s - dcissues ] , and [ s - compmeth ] are somewhat technical , and the reader who is uninterested in the mathematical and computational details may skip to them .",
    "i will use the common statistical notation that an estimate of a quantity is denoted by placing a ` hat ' above it ; e.g. , @xmath5 is an estimate of the true value of the parameter @xmath6 . in general",
    ", greek letters will denote the true value of a quantity , while roman letters will denote the contaminated measured value .",
    "i will frequently refer to the ` bias ' of an estimator .",
    "the bias of an estimator is @xmath7 , where @xmath8 is the expectation value of the estimator @xmath5 , and @xmath9 is the true value of @xmath6 .",
    "an unbiased estimator is one such that @xmath10 .",
    "i will denote a normal density with mean @xmath11 and variance @xmath12 as @xmath13 , and i will denote as @xmath14 a multivariate normal density with @xmath15-element mean vector @xmath11 and @xmath16 covariance matrix @xmath17 . if i want to explicitly identify the argument of the gaussian function , i will use the notation @xmath18 , which should be understood to be a gaussian with mean @xmath11 and variance @xmath12 as a function of @xmath19 . following @xcite ,",
    "i denote the scaled inverse-@xmath1 density as inv-@xmath20 , where @xmath21 is the degrees of freedom and @xmath22 is the scale parameter , and we denote the inverse - wishart as inv-@xmath23 , where @xmath21 is the degrees of freedom and @xmath24 is the scale matrix .",
    "the inverse - wishart distribution can be thought of as a multivariate generalization of the scaled inverse-@xmath1 distribution .",
    "i will often use the common statistical notation where `` @xmath25 '' means `` is drawn from '' or `` is distributed as '' .",
    "for example , @xmath26 states that @xmath19 is drawn from a normal density with mean @xmath11 and variance @xmath12 .",
    "it is well known that measurement error can attenuate the estimate of the regression slope and correlation coefficient ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ) . for completeness ,",
    "i give a brief review of the effect of measurement error on correlation and regression analysis for the case of one independent variable .",
    "denote the independent variable as @xmath27 and the dependent variable as @xmath28 ; @xmath27 and @xmath28 are also referred to as the ` covariate ' and the ` response ' , respectively .",
    "i assume that @xmath27 is a random vector of @xmath29 data points drawn from some probability distribution .",
    "the dependent variable , @xmath28 , depends on @xmath27 according to the usual additive model : @xmath30 here , @xmath31 is a random variable representing the intrinsic scatter in @xmath32 about the regression relationship , and @xmath33 are the regression coefficients .",
    "the mean of @xmath34 is assumed to be zero , and the variance of @xmath34 is assumed to be constant and is denoted as @xmath12 .",
    "we do not observe the actual values of @xmath35 , but instead observe values @xmath36 which are measured with error .",
    "the measured values are assumed to be related to the actual values as @xmath37 where @xmath38 and @xmath39 are the random measurement errors on @xmath40 and @xmath41 , respectively . in general , the errors are normally distributed with known variances @xmath42 and @xmath43 , and covariance @xmath44 . for simplicity ,",
    "throughout the rest of this section i assume that @xmath45 and @xmath46 are the same for each data point .    when the data are measured without error , the least - squares estimate of the regression slope , @xmath47 , and the estimated correlation coefficient , @xmath48 , are @xmath49 here",
    ", @xmath50 is the sample covariance between @xmath27 and @xmath28 , and @xmath51 is the sample variance of @xmath27 .",
    "when the data are measured with error , the least - squares estimate of the regression slope , @xmath52 , and the estimated correlation coefficient , @xmath53 , become @xmath54 from these equations it is apparent that the estimated slope and correlation are biased when the data are measured with error .",
    "it is informative to assess the effect of measurement error in terms of the ratios @xmath55 , as these quantities can be calculated from the data . the fractional bias in the estimated slope and correlation",
    "may then be expressed as @xmath56 from equations ( [ eq - frac_bbias ] ) and ( [ eq - frac_rbias ] ) it is apparent that measurement errors have the following effects .",
    "first , covariate measurement error reduces the magnitude of the observed correlation between the independent variable and the response , as well as biasing the estimate of the slope towards zero .",
    "second , measurement error in the response also reduces the magnitude of the observed correlation between the variables .",
    "third , if the measurement errors are correlated the effects depend on the sign of this correlation .",
    "if the measurement error correlation has the same sign as the intrinsic correlation between @xmath27 and @xmath28 , then the measurement errors cause a spurious increase in the observed correlation ; otherwise the measurement errors cause a spurious decrease in the observed correlation .",
    "the magnitude of these effects depend on how large the measurement errors are compared to the observed variance in @xmath19 and @xmath57 .    in figure [ f - bias ] i plot the fractional bias in the correlation coefficient , @xmath58 , as a function of @xmath59 and @xmath60 when",
    "the errors are uncorrelated .",
    "as can be seen , measurement error can have a significant effect on the estimation of the linear correlation coefficient .",
    "for example , when @xmath61 and @xmath62 , the estimated correlation is @xmath63% lower than the true correlation .",
    "therefore , interpretation of correlation coefficients and regression slopes must be approached with caution when the data have been contaminated by measurement error . to ensure accurate results",
    ", it is necessary to employ statistical methods that correct for the measurement errors .",
    "i assume that the independent variable , @xmath27 , is drawn from a probability distribution @xmath64 , where @xmath65 denotes the parameters for this distribution .",
    "the dependent variable is then drawn from the conditional distribution of @xmath28 given @xmath27 , denoted as @xmath66 ; @xmath6 denotes the parameters for this distribution .",
    "the joint distribution of @xmath27 and @xmath28 is then @xmath67 . in this work",
    "i assume the normal linear regression model given by equation ( [ eq - adderr ] ) , and thus @xmath66 is a normal density with mean @xmath68 and variance @xmath12 , and @xmath69 .",
    "since the data are a randomly observed sample , we can derive the likelihood function for the measured data .",
    "the likelihood function of the measured data , @xmath70 , is obtained by integrating the complete data likelihood over the missing data , @xmath27 and @xmath28 ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ) : @xmath71 here , @xmath72 is the complete data likelihood function . because of the hierarchical structure inherent in the measurement error model",
    ", it is helpful to decompose the complete data likelihood into conditional probability densities : @xmath73 the density @xmath74 describes the joint distribution of the measured values @xmath19 and @xmath57 at a given @xmath27 and @xmath28 , and depends on the assumed distribution of the measurement errors , @xmath75 and @xmath76 . in this work i assume gaussian measurement error , and thus @xmath77 is a multivariate normal density with mean @xmath78 and covariance matrix @xmath79 , where @xmath80 and @xmath81 .",
    "the statistical model may then be conveniently expressed hierarchically as @xmath82,\\sigma_i ) \\label{eq - hierxy }    \\end{aligned}\\ ] ] note that if @xmath40 is measured without error , then @xmath83 is a dirac delta function , and @xmath84 .",
    "an equivalent result holds if @xmath41 is measured without error .",
    "equation ( [ eq - obslik2 ] ) may be used to obtain the observed data likelihood function for any assumed distribution of @xmath27 . in this work ,",
    "i model @xmath64 as a mixture of @xmath85 gaussians , @xmath86 where @xmath87 .",
    "note that , @xmath88 may be interpreted as the probability of drawing a data point from the @xmath89 gaussian .",
    "i will use the convenient notation @xmath90 and @xmath91 ; note that @xmath92 .",
    "it is useful to model @xmath64 using this form because it is flexible enough to adapt to a wide variety of distributions , but is also conjugate for the regression relationship ( eq.[[eq - adderr ] ] ) and the measurement error distribution , thus simplifying the mathematics .    assuming the gaussian mixture model for @xmath64 , the measured data likelihood for the @xmath93 data point can be directly calculated using equation ( [ eq - obslik2 ] ) . denoting the measured data as @xmath94 ,",
    "the measured data likelihood function for the @xmath93 data point is then a mixture of bivariate normal distributions with weights @xmath95 , means @xmath96 , and covariance matrices @xmath97 .",
    "because the data points are statistically independent , the full measured data likelihood is then the product of the likelihood functions for the individual data points : @xmath98 here , @xmath99 denotes the transpose of @xmath100 .",
    "equation ( [ eq - bobslik ] ) may be maximized to compute the maximum - likelihood estimate ( mle ) . when @xmath101 , the expectation - maximization ( em ) algorithm @xcite is probably the most efficient tool for calculating the mle .",
    "@xcite describe an em algorithm when @xmath102 is assumed to be a mixture of normals and the measurement error distribution is multivariate @xmath103 , and their results can be extended to the statistical model described in this work .",
    "it is informative to decompose the measured data likelihood , @xmath104 , as this representation is useful when the data contain non - detections ( cf . ,   [ s - nondet ] ) .",
    "the marginal distribution of @xmath40 is @xmath105 and the conditional distribution of @xmath41 given @xmath40 is @xmath106 ^ 2}{var(y_i|x_i , k ) }        \\right\\ } \\label{eq - cdensyx } \\\\",
    "\\gamma_k & = & \\frac{\\pi_k n(x_i|\\mu_k , \\tau^2_k + \\sigma^2_{x , i } ) }        { \\sum_{j=1}^k \\pi_j n(x_i|\\mu_j , \\tau^2_j + \\sigma^2_{x , i } ) }        \\label{eq - cdenskx } \\\\",
    "e(y_i|x_i , k ) & = &   \\alpha + \\left ( \\frac{\\beta \\tau^2_k + \\sigma_{xy , i } }      { \\tau^2_k + \\sigma^2_{x , i } } \\right ) x_i + \\left ( \\frac{\\beta \\sigma^2_{x , i } - \\sigma_{xy , i } }      { \\tau_k^2 + \\sigma^2_{x , i } }",
    "\\right ) \\mu_k \\label{eq - cmeanyx } \\\\",
    "var(y_i|x_i , k ) & = & \\beta^2 \\tau^2_k + \\sigma^2 + \\sigma^2_{y , i } -       \\frac{(\\beta \\tau^2_k - \\sigma_{xy , i})^2}{\\tau^2_k + \\sigma^2_{x , i } } \\label{eq - cvary}.    \\end{aligned}\\ ] ] here , @xmath107 can be interpreted as the probability that the @xmath93 data point was drawn from the @xmath89 gaussian given @xmath40 , @xmath108 gives the expectation value of @xmath41 at @xmath40 , given that the data point was drawn from the @xmath89 gaussian , and @xmath109 gives the variance in @xmath41 at @xmath40 , given that the data point was drawn from the @xmath89 gaussian .",
    "it is informative to investigate the case where the distribution of @xmath27 is assumed to be uniform , @xmath110 . interpreting @xmath102 as a ` prior ' on @xmath27 ,",
    "one may be tempted to consider assuming @xmath110 as a more objective alternative to the normal distribution .",
    "a uniform distribution for @xmath27 may be obtained as the limit @xmath111 , and thus the likelihood function for @xmath110 can be calculated from equation ( [ eq - cdensyx ] ) by taking @xmath111 and @xmath112 . when the measurement errors are uncorrelated , the likelihood for uniform @xmath102 is @xmath113 the argument of the exponential is the fitexy goodness of fit statistic , @xmath114 , as modified by @xcite to account for intrinsic scatter ; this fact has also been recognized by @xcite . despite this connection ,",
    "minimizing @xmath114 is not the same as maximizing the conditional likelihood of @xmath57 given @xmath19 , as both @xmath115 and @xmath12 appear in the normalization of the likelihood function as well .    for a given value of @xmath12 , minimizing @xmath114 can be interpreted as minimizing a weighted sum of squared errors , where the weights are given by the variances in @xmath41 at a given @xmath40 , and one assumes a uniform distribution for @xmath27 .",
    "unfortunately , this is only valid for a fixed value of @xmath12 .",
    "moreover , little is known about the statistical properties of the fitexy estimator , such as its bias and variance , although bootstrapping ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ) may be used to estimate them .",
    "furthermore , it is ambiguous how to calculate the fitexy estimates when there is an intrinsic scatter term .",
    "the fitexy goodness - of - fit statistic , @xmath114 , can not be simultaneously minimized with respect to @xmath116 and @xmath12 , as @xmath114 is a strictly decreasing function of @xmath12 . as such",
    ", it is unclear how to proceed in the optimization beyond an _ ad hoc _ approach .",
    "many authors have followed the approach adopted by @xcite and increase @xmath12 until @xmath117 , or assume @xmath118 if @xmath119 .    despite the fact that minimizing @xmath120 is not the same as maximizing equation ( [ eq - uniflik ] ) , one may still be tempted to calculate a mle based on equation ( [ eq - uniflik ] ) .",
    "however , it can be shown that if one assumes @xmath110 , and if all of the @xmath19 and @xmath57 have the same respective measurement error variances , @xmath121 and @xmath122 , the mle estimates for @xmath123 and @xmath115 are just the ordinary least squares estimates @xcite .",
    "while this is not necessarily true when the magnitudes of the measurement errors vary between data points , one might expect that the mle will behave similarly to the ols estimate .",
    "i confirm this fact using simulation in   [ s - sims1 ] .",
    "unfortunately , this implies that the mle for @xmath110 inherits the bias in the ols estimate , and thus nothing is gained .",
    "furthermore , as argued by @xcite , one can easily be convinced that assuming @xmath110 is incorrect by examining a histogram of @xmath19 .      the formalism developed in   [ s - normreg ] can easily be generalized to multiple independent variables . in this case equation ( [ eq - adderr ] ) becomes @xmath124 where @xmath115 is now a @xmath15-element vector and @xmath125 is a @xmath15-element vector containing the values of the independent variables for the @xmath93 data point .",
    "similar to before , we assume that the distribution of @xmath125 can be approximated using a mixture of @xmath85 multivariate normal densities with @xmath15-element mean vectors @xmath126 , @xmath16 covariance matrices @xmath127 , and weights @xmath128 . the measured value of @xmath125 is the @xmath15-element vector @xmath129 , and the gaussian measurement errors on @xmath130 have @xmath131 covariance matrix @xmath79 . the statistical model is then @xmath132 , \\sigma_i ) .",
    "\\label{eq - mhierxy } \\\\    \\end{aligned}\\ ] ]    denoting @xmath133 , the measured data likelihood is @xmath134 here , @xmath135 is the @xmath136-element mean vector of @xmath137 for gaussian @xmath138 , @xmath139 is the @xmath140 covariance matrix of @xmath137 for gaussian",
    "@xmath138 , @xmath43 is the variance in the measurement error on @xmath41 , @xmath44 is the @xmath15-element vector of covariances between the measurement errors on @xmath41 and @xmath129 , and @xmath141 is the @xmath16 covariance matrix of the measurement errors on @xmath129 .",
    "similar to the case for one independent variable , the measured data likelihood can be decomposed as @xmath142 , where @xmath143 and @xmath144 ^ 2}{var(y_i|{\\bf x}_i , k ) }         \\right\\ } \\label{eq - mcdensyx } \\\\",
    "\\gamma_k & = &   \\frac{\\pi_k n({\\bf x}_i|\\mu_k , t_k + \\sigma_{x , i } ) }        { \\sum_{j=1}^k \\pi_j n({\\bf x}_i|\\mu_j , t_j + \\sigma_{x , i } ) }        \\label{eq - mcdenskx } \\\\      e(y_i|{\\bf x}_i ,",
    "k ) & = & \\alpha + \\beta^t \\mu_k + ( \\beta^t t_k + \\sigma^t_{xy , i } )      ( t_k + \\sigma_{x , i})^{-1 } ( { \\bf x}_i - \\mu_k ) \\label{eq - mxihati } \\\\      var(y_i|{\\bf x}_i ,",
    "k ) & = & \\beta^t t_k \\beta + \\sigma^2 + \\sigma^2_{y , i } -       ( \\beta^t t_k + \\sigma^t_{xy , i } ) ( t_k + \\sigma_{x , i})^{-1 } ( t_k \\beta + \\sigma_{xy , i } )      \\label{eq - mcvary}.    \\end{aligned}\\ ] ]",
    "there are several issues common in the collection of astronomical data that violate the simple assumptions made in   [ s - statmod ] .",
    "astronomical data collection consists almost entirely of passive observations , and thus selection effects are a common concern .",
    "instrumental detection limits often result in the placement of upper or lower limits on quantities , and astronomical surveys are frequently flux - limited . in this section",
    "i modify the likelihood functions described in ",
    "[ s - statmod ] to include the effects of data collection .",
    "general methods for dealing with missing data are described in @xcite and @xcite , and i apply the methodology described in these references to the measurement error model developed here .",
    "although in this work i focus on linear regression , many of these results can be applied to more general statistical models , such as estimating luminosity functions .",
    "suppose that one collects a sample of @xmath29 sources out of a possible @xmath145 sources .",
    "one is interested in understanding how the observable properties of these sources are related , but is concerned about the effects of the selection procedure on the data analysis .",
    "for example , one may perform a survey that probes some area of the sky .",
    "there are @xmath145 sources located within this solid angle , where @xmath145 is unknown .",
    "because of the survey s selection method , the sample only includes @xmath29 sources . in this case",
    "the astronomer is interested in how measurement error and the survey s selection method affect statistical inference .",
    "i investigate selection effects within the framework of our statistical model by introducing an indicator variable , @xmath146 , which denotes whether a source is included in the sample . if the @xmath147 source is included in the sample , then @xmath148 , otherwise @xmath149 .",
    "in addition , i assume that the selection function only depends on the measured values , @xmath19 and @xmath57 . under this assumption ,",
    "the selection function of the sample is the probability of including a source with a given @xmath19 and @xmath57 , @xmath150 .",
    "this is commonly the case in astronomy , where sources are collected based on their measured properties . for example , one may select sources for a sample based on their measured properties as reported in the literature .",
    "in addition , if one performs a flux - limited survey then a source will only be considered detected if its measured flux falls above some set flux limit . if a sample is from a survey with a simple flux limit , then @xmath151 if the measured source flux @xmath41 is above the flux limit , and @xmath152 if the measured source flux is below the flux limit .",
    "since the selection function depends on the measured flux value , and not the true flux value , sources with true flux values above the flux limit can be missed by the survey , and sources with true flux below the limit can be detected by the survey .",
    "this effect is well - known in astronomy and is commonly referred to as malmquist bias ( e.g. , * ? ? ?",
    "including the variable @xmath146 , the complete data likelihood can be written as @xmath153 equation ( [ eq - complik ] ) is valid for any number of independent variables , and thus @xmath40 and @xmath125 may be either scalar or vector . integrating equation ( [ eq - complik ] ) over the missing data ,",
    "the observed data likelihood is @xmath154 here , @xmath155 is the binomial coefficient , @xmath156 denotes the set of @xmath29 included sources , @xmath157 and @xmath158 denote the values of @xmath19 and @xmath57 for the included sources , and @xmath159 denotes the set of @xmath160 missing sources .",
    "in addition , i have omitted terms that do not depend on @xmath161 , or @xmath145 .",
    "note that @xmath145 is unknown and is thus also a parameter of the statistical model .",
    "the binomial coefficient is necessary because it gives the number of possible ways to select a sample of @xmath29 sources from a set of @xmath145 sources .",
    "it is apparent from equation ( [ eq - mislik ] ) that statistical inference on the regression parameters is unaffected if the selection function is independent of @xmath57 and @xmath19 .",
    "( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ) . in this case the selection function may be ignored .",
    "it is commonly the case that a sample is selected based only on the measured independent variables .",
    "for example , suppose one performs a survey in which all sources with measured optical flux greater than some threshold are included .",
    "then , these optically selected sources are used to fit a regression in order to understand how the x - ray luminosity of these objects depends on their optical luminosity and redshift . in this case",
    ", the probability of including a source only depends on the measured values of the optical luminosity and redshift , and is thus independent of the x - ray luminosity .    when the sample selection function is independent of @xmath57 , given @xmath19 , then @xmath162 . because we are primarily interested in the regression parameters , @xmath6",
    ", i model the distributions of @xmath27 for the included and missing sources",
    "seperately , with the parameters for the distribution of included sources denoted as @xmath163 .",
    "in addition , i assume that the measurement errors between @xmath57 and @xmath19 are statistically independent . then the @xmath160 integrals over @xmath57 and @xmath28 for the missing sources in equation ( [ eq - mislik ] ) are equal to unity , and we can write the observed data likelihood as @xmath164 where @xmath165 is the distribution of those @xmath27 included in one s sample . here",
    "i have omitted terms depending on @xmath145 because one is primarily interested in inference on the regression parameters , @xmath6 .",
    "equation ( [ eq - indlik ] ) is identical to equation ( [ eq - obslik2 ] ) , with the exception that @xmath64 now only models the distribution of those @xmath27 that have been included in one s sample , and i have now assumed that the measurement errors on @xmath57 and @xmath19 are independent . in particular , for the gaussian mixture models described in ",
    "[ s - normreg ] and   [ s - multreg ] , the observed data likelihood is given by equations ( [ eq - bobslik ] ) and ( [ eq - mobslik ] ) , where @xmath95 , @xmath11 , and @xmath166 ( or @xmath167 ) should be understood as referring to the parameters for the distribution of the observed @xmath27 . as is evident from the similarity between equations ( [ eq - indlik ] ) and ( [ eq - obslik2 ] ) ,",
    "_ if the sample is selected based on the measured independent variables , and if the measurement errors on the dependent and independent variables are statistically independent , then inference on the regression parameters , @xmath6 , is unaffected by selection effects_.      if the method in which a sample is selected depends on the measured dependent variable , @xmath57 , or if the measurement error in @xmath19 and @xmath57 are correlated , the observed data likelihood becomes more complicated . as an example",
    ", one might encounter this situation if one uses an x - ray selected sample to investigate the dependence of x - ray luminosity on optical luminosity and redshift . in this case ,",
    "the selection function of the sample depends on both the x - ray luminosity and redshift , and is thus no longer independent of the dependent variable .",
    "such data sets are said to be ` truncated ' .    if the selection function depends on @xmath57 , or if the measurement errors on @xmath57 and @xmath19 are not independent , one can not simply ignore the terms depending on @xmath145 , since the @xmath168 integrals in equation ( [ eq - mislik ] ) depend on @xmath6 .",
    "however , we can eliminate the dependence of equation ( [ eq - mislik ] ) on the unknown @xmath145 by applying a bayesian approach .",
    "the posterior distribution of @xmath169 and @xmath145 is related to the observed data likelihood function as @xmath170 , where @xmath171 is the prior distribution of @xmath172 .",
    "if we assume a uniform prior on @xmath173 and @xmath174 , then one can show ( e.g. , * ? ? ?",
    "* ) that the posterior distribution of @xmath6 and @xmath65 is @xmath175^{-n }       \\prod_{i=1}^n p(x_i , y_i|\\theta , \\psi ) .      \\label{eq - trunclik}\\ ] ] here , @xmath176 is given by equation ( [ eq - obslik2 ] ) , and @xmath177 is the probability of including a source in one s sample , given the model parameters , @xmath6 and @xmath65 : @xmath178 i have left off the subscripts for the data points in equation ( [ eq - detprob ] ) because the integrals are the same for each @xmath179 .",
    "if one assumes the gaussian mixture model of sections [ s - normreg ] and [ s - multreg ] , then @xmath176 is given by equations ( [ eq - bobslik ] ) or ( [ eq - mobslik ] ) . the posterior mode can then be used as an estimate of @xmath6 and @xmath65 , which is found by maximizing equation ( [ eq - trunclik ] ) .",
    "in addition to issues related to the sample selection method , it is common in astronomical data to have non - detections .",
    "such non - detections are referred to as ` censored ' data , and the standard procedure is to place an upper and/or lower limit on the censored data point .",
    "methods of data analysis for censored data have been reviewed and proposed in the astronomical literature , ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) , and @xcite describe censored regression when the variables are measured without error .",
    "see @xcite for a review of censored data in astronomy .    to facilitate the inclusion of censored data ,",
    "i introduce an additional indicator variable , @xmath180 , indicating whether a data point is censored or not on the dependent variable .",
    "if @xmath41 is detected , then @xmath181 , else if @xmath41 is censored then @xmath182 .",
    "it is commonly the case that a source is considered ` detected ' if its measured flux falls above some multiple of the background noise level , say @xmath183 .",
    "then , in this case , the probability of detecting the source given the measured source flux @xmath41 is @xmath184 if @xmath185 , and @xmath186 if @xmath187 .",
    "since source detection depends on the measured flux , some sources with intrinsic flux @xmath28 above the flux limit will have a measured flux @xmath57 that falls below the flux limit .",
    "similarly , some sources with intrinsic flux below the flux limit will have a measured flux above the flux limit .",
    "i assume that a sample is selected based on the independent variables , i.e. , @xmath162 .",
    "it is difficult to imagine obtaining a censored sample if the sample is selected based on its dependent variable , as some of the values of @xmath57 are censored and thus unknown .",
    "therefore , i only investigate the effects of censoring on @xmath57 when the probability that a source is included in the sample is independent of @xmath57 , given @xmath19 .",
    "in addition , i do not address the issue of censoring on the independent variable .",
    "although such methods can be developed , it is probably simpler to just omit such data as inference on the regression parameters is unaffected when a sample is selected based only on the independent variables ( cf . ,   [ s - seffects_x ] ) .    the observed data likelihood for an @xmath19-selected sample is given by equation ( [ eq - indlik ] ) .",
    "we can modify this likelihood to account for censored @xmath57 by including the indicator variable @xmath180 and again integrating over the missing data : @xmath188 here , the first product is over the set of data points with detections , @xmath189 , and the second product is over the set of data points with non - detections , @xmath190 .",
    "the conditional distribution @xmath191 and the marginal distribution @xmath192 for the gaussian mixture model are both given in ",
    "[ s - normreg ] and   [ s - multreg ] .",
    "if the data points are measured without error and one assumes the normal regression model , @xmath193 , then equation [ eq - censlik ] becomes the censored data likelihood function described in @xcite .",
    "a mle for censored regression with measurement errors is then obtained by maximizing equation ( [ eq - censlik ] ) .",
    "in this section i describe a bayesian method for computing estimates of the regression parameters , @xmath6 , and their uncertainties .",
    "the bayesian approach calculates the posterior probability distribution of the model parameters , given the observed data , and therefore is accurate for both small and large sample sizes .",
    "the posterior distribution follows from baye s formula as @xmath194 , where @xmath195 is the prior distribution of the parameters .",
    "i describe some markov chain methods for drawing random variables from the posterior , which can then be used to estimate quantities such as standard errors and confidence intervals on @xmath6 and @xmath65 .",
    "@xcite is a good reference on bayesian methods , and @xcite gives a review of bayesian methods intended for astronomers .",
    "further details of markov chain simulation , including methods for making the simulations more efficient , can be found in @xcite .      in order to ensure a proper posterior for the gaussian mixture model ,",
    "it is necessary to invoke a proper prior density on the mixture parameters @xcite .",
    "i adopt a uniform prior on the regression parameters @xmath196 , and take @xmath197 .",
    "the dirichlet density is a multivariate extension of the beta density , and the @xmath198 prior adopted in this work is equivalent to a uniform prior on @xmath95 , under the constraint @xmath199 .",
    "the prior on @xmath11 and @xmath166 ( or @xmath167 ) adopted in this work is very similar to that advocated by @xcite and @xcite .",
    "i adopt a normal prior on the individual @xmath200 with mean @xmath201 and variance @xmath202 ( or covariance matrix @xmath203 ) .",
    "this reflects our prior belief that the distribution of @xmath27 is more likely to be fairly unimodal , and thus that we expect it to be more likely that the individual gaussians will be close together than far apart . if there is only one covariate",
    ", then i adopt a scaled inverse-@xmath1 prior on the individual @xmath204 with scale parameter @xmath205 and one degree of freedom , otherwise if there are @xmath206 covariates i adopt an inverse - wishart prior on the individual @xmath207 with scale matrix @xmath208 and @xmath15 degrees of freedom .",
    "this reflects our prior expectation that the variances for the individual gaussian components should be similar , but the low number of degrees of freedom accomodates a large range of scales .",
    "both the gaussian means and variances are assumed to be independent in their prior distribution , and the ` hyper - parameters ' @xmath209 ( or @xmath203 ) , and @xmath205 ( or @xmath208 ) are left unspecified . by leaving the parameters for the prior distribution unspecified , they becomes additional parameters in the statistical model , and therefore are able to adapt to the data .    since the hyper - parameters are left as free parameters they also require a prior density .",
    "i assume a uniform prior on @xmath201 and @xmath205 ( or @xmath208 ) .",
    "if there is one covariate , then i assume a scaled inverse-@xmath1 prior for @xmath202 with scale parameter @xmath205 and one degree of freedom , otherwise if there are multiple covariate we assume a inverse - wishart prior for @xmath203 with scale matrix @xmath208 and @xmath15 degrees of freedom .",
    "the prior on @xmath202 ( or @xmath203 ) reflects the prior expectation that the dispersion of the gaussian components about their mean @xmath201 should be on the order of the typical dispersion of each individual gaussian .",
    "the prior density for one covariate is then @xmath210 and is summarized hierarchically as @xmath211 the prior density for multiple covariates is just the multivariate extension of equations ( [ eq - coefprior])([eq - wsqrprior ] ) .",
    "the posterior distribution summarizes our knowledge about the parameters in the statistical model , given the observed data and the priors .",
    "direct computation of the posterior distribution is too computationally intensive for the model described in this work .",
    "however , we can obtain any number of random draws from the posterior using markov chain monte carlo ( mcmc ) methods . in mcmc methods ,",
    "we simulate a markov chain that performs a random walk through the parameter space , saving the locations of the walk at each iteration .",
    "eventually , the markov chain converges to the posterior distribution , and the saved parameter values can be treated as a random draw from the posterior .",
    "the random draws can then be used to estimate posterior medians , standard errors , of plot histogram estimates of the posterior .      the easiest method for sampling from the posterior",
    "is to construct a gibbs sampler .",
    "the basic idea behind the gibbs sampler is to construct a markov chain , where new values of the model parameters and missing data are simulated at each iteration , conditional on the values of the observed data and the current values of the model parameters and the missing data . within the context of the measurement error model considered in this work , the gibbs sampler undergoes four different stages .",
    "the first stage of the gibbs sampler simulates values of the missing data , given the measured data and current parameter values , a process known as data augmentation . in this work",
    "the missing data are @xmath212 and any non - detections .",
    "in addition , i introduce an additional latent variable , @xmath213 , which gives the class membership for the @xmath93 data point .",
    "the vector @xmath213 has @xmath85 elements , where @xmath214 if the @xmath93 data point comes from the @xmath89 gaussian , and @xmath215 if @xmath216 .",
    "i will use @xmath217 to refer to the set of @xmath29 vectors @xmath218 . noting that @xmath88 gives the probability of drawing a data point from the @xmath89 gaussian , the mixture model for @xmath27",
    "may then be expressed hierarchically as @xmath219 here , @xmath220 is a multinomial distribution with @xmath221 trials , where @xmath222 is the probability of success for the @xmath89 class on any particular trial .",
    "the vector @xmath213 is also considered to be missing data , and is introduced to simplify construction of the gibbs sampler .",
    "the new values of the missing data simulated in the data augmentation step are then used to simulate new values of the regression and gaussian mixture parameters .",
    "the second stage of the gibbs sampler simulates values of the regression parameters , @xmath6 , given the current values of @xmath223 and @xmath28 .",
    "the third stage simulates values of the mixture parameters , @xmath65 , given the current values of @xmath27 and @xmath28 .",
    "the fourth stage uses the new values of @xmath6 and @xmath65 to update the parameters of the prior density .",
    "the values of the parameters are saved , and the process is repeated , creating a markov chain . after a large number of iterations , the markov chain converges , and",
    "the saved values of @xmath6 and @xmath65 from the latter part of the algorithm may then be treated as a random draw from the posterior distribution , @xmath224 .",
    "methods for simulating random variables from the distributions used for this gibbs sampler are described in various works ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "a gibbs sampler for the gaussian mixture model is    1 .",
    "[ i - gstart ] start with initial guesses for @xmath225 and the prior parameters",
    "[ i - ycens ] if there are any non - detections , then draw @xmath41 for the censored data points from @xmath226 .",
    "this may be done by first drawing @xmath41 from @xmath227 : @xmath228 one then draws a random variable @xmath229 , uniformly - distributed on @xmath230 $ ] .",
    "if @xmath231 then the value of @xmath41 is kept , otherwise one draws a new value of @xmath41 and @xmath229 until @xmath232 .",
    "[ i - xi ] draw values of @xmath27 from @xmath233 .",
    "the distribution @xmath233 can be derived from equations ( [ eq - hierxi])([eq - hierxy ] ) or ( [ eq - mhierxi])([eq - mhierxy ] ) and the properties of the multivariate normal distribution : 1 .   [ i - xi1 ] if there is only one independent variable then @xmath125 is updated as : @xmath234           \\label{eq - gxihatk } \\\\",
    "\\hat{\\xi}_{xy , i } & = & x_i + \\frac{\\sigma_{xy , i}}{\\sigma^2_{y , i } } ( \\eta_i - y_i )         \\label{eq - gxixy } \\\\",
    "\\sigma^2_{\\hat{\\xi},i } & = & \\sum_{k=1}^{k } g_{ik } \\sigma^2_{\\hat{\\xi},ik }          \\label{eq - gxihvark } \\\\",
    "\\sigma^2_{\\hat{\\xi},ik } & = & \\left [ \\frac{1}{\\sigma^2_{x , i}(1 - \\rho^2_{xy , i } ) } +              \\frac{\\beta^2}{\\sigma^2 } + \\frac{1}{\\tau_k^2 } \\right]^{-1}. \\label{eq - gxihvar }      \\end{aligned}\\ ] ] here , @xmath235 is the correlation between the measurement errors on @xmath40 and @xmath41 .",
    "note that @xmath125 is updated using only information from the @xmath89 gaussian , since @xmath236 only for @xmath237 and @xmath215 otherwise .",
    "[ i - xi2 ] if there are multiple independent variables , i have found it easier and computationally faster to update the values of @xmath125 using a scalar gibbs sampler . in this case , the @xmath15 elements of @xmath125 are updated individually .",
    "i denote @xmath238 to be the value of the @xmath239 independent variable for the @xmath93 data point , and @xmath240 to be the measured value of @xmath238 .",
    "in addition , i denote @xmath241 to be the @xmath242-element vector obtained by removing @xmath238 from @xmath125 , i.e. , @xmath243 .",
    "similarly , @xmath244 denotes the @xmath242-element vector of regression coefficients obtained after removing @xmath245 from @xmath115 .",
    "then , @xmath238 is updated as @xmath246^{-1}.           \\label{eq - gmxihvark }        \\end{aligned}\\ ] ] here , @xmath247 is a @xmath136-element vector obtained by subtracting @xmath248 from @xmath249 , with the exception of the @xmath239 element of @xmath125 ; instead , the @xmath250 element of @xmath251 is just @xmath240 . the @xmath15-element vector @xmath252 is obtained in an equivalent manner .",
    "the @xmath140 matrix @xmath79 is the covariance matrix of the measurement errors on @xmath253 .",
    "the term @xmath254 denotes the @xmath250 element of the vector @xmath255 , and likewise for @xmath256 .",
    "the terms @xmath257 and @xmath258 denote the @xmath250 and @xmath239 elements of the diagonals of @xmath259 and @xmath260 , respectively .",
    "this step is repeated until all @xmath15 independent variables have been updated for each data point .",
    "+ if any of the @xmath125 are measured without error , then one simply sets @xmath261 for those data points . 4 .",
    "[ i - eta ] draw values of @xmath28 from @xmath262 . similar to @xmath27 , the distribution @xmath262 can be derived from equations ( [ eq - hierxi])([eq - hierxy ] ) or ( [ eq - mhierxi])([eq - mhierxy ] ) and the properties of the multivariate normal distribution . 1 .",
    "[ i - eta1 ] if there is only one covariate then @xmath28 is updated as @xmath263 \\label{eq - gmetahat } \\\\",
    "\\sigma^2_{\\hat{\\eta},i } & = &   \\left [ \\frac{1}{\\sigma^2_{y , i}(1 - \\rho^2_{xy , i } ) } +           \\frac{1}{\\sigma^2 } \\right]^{-1}. \\label{eq - gmetahvar }        \\end{aligned}\\ ] ] 2 .",
    "[ i - eta2 ] if there are multiple covariates then @xmath28 is updated as @xmath264^{-1 } \\label{eq - gmetahvar2 } \\\\      { \\bf z}^*_i & = & ( y_i , { \\bf x}_i - \\xi_i ) .",
    "\\label{eq - zstar }        \\end{aligned}\\ ] ] here , @xmath265 is the first element of the vector @xmath266 , @xmath247 is a @xmath136-element vector whose first element is @xmath41 and remaining elements are @xmath267 , and @xmath268 is the first diagonal element of @xmath259 .",
    "+ if any of the @xmath28 are measured without error , then one sets @xmath269 for those data points . 5 .   [ i - glabel ]",
    "draw new values of the gaussian labels , @xmath217 .",
    "the conditional distribution of @xmath213 is multinomial with number of trials @xmath270 and group probabilities @xmath271 : @xmath272 note that if there is only one covariate then @xmath273 and @xmath274 .",
    "[ i - coef ] draw @xmath275 from @xmath276 . given @xmath277 and @xmath12 , the distribution of @xmath123 and @xmath115 is obtained by ordinary regression : @xmath278 here , @xmath279 is a @xmath280 matrix , where the first column is a column of ones , the second column contains the @xmath29 values of @xmath125 for the first independent variable , the third column contains the @xmath29 values of @xmath125 for the second independent variable , etc . 7 .",
    "[ i - sigsqr ] draw a new value of @xmath12 from @xmath281 .",
    "the distribution @xmath281 is derived by noting that given @xmath282 and @xmath125 , @xmath32 is normally distributed with mean @xmath283 and variance @xmath12 .",
    "re - expressing this distribution in terms of @xmath12 instead of @xmath28 , and taking the product of the distributions for each data point , it follows that @xmath12 has a scaled inverse-@xmath1 distribution : @xmath284 8 .",
    "[ i - pi ] draw new values of the group proportions , @xmath95 .",
    "given @xmath217 , @xmath95 follows a dirichlet distribution : @xmath285 note that @xmath286 is the number of data points that belong to the @xmath89 gaussian .",
    "[ i - mu ] draw a new value of @xmath200 from @xmath287 . if there is only one independent variable , then @xmath288 and @xmath289 .",
    "the new value of @xmath200 is simulated as @xmath290 10 .",
    "[ i - tausqr ] draw a new value of @xmath204 or @xmath207 .",
    "the distribution of @xmath291 or @xmath292 is derived in a manner similar to @xmath293 , and noting that the prior is conjugate for this likelihood .",
    "the distribution of @xmath294 is a scaled inverse-@xmath1 distribution , and the distribution of @xmath292 is an inverse - wishart distribution : 1 .",
    "[ i - tsqr ] if there is only one independent variable then draw @xmath295 .",
    "\\label{eq - tsqr }        \\end{aligned}\\ ] ] 2 .",
    "[ i - tcovar ] if there are multiple independent variables then draw @xmath296 11 .",
    "[ i - mu0 ] draw a new value for @xmath297 .",
    "noting that conditional on @xmath201 and @xmath203 , @xmath298 are independently distributed as @xmath299 , it is straight - forward to show that @xmath300",
    "if there is only one covariate then @xmath301 and @xmath289 . 12 .",
    "[ i - usqr0 ] draw a new value for @xmath202 or @xmath203 , given @xmath302 and @xmath205 ( or @xmath208 ) .",
    "similar to the case for @xmath303 or @xmath207 , the conditional distribution of @xmath202 or @xmath203 is scaled inverse-@xmath1 or inverse - wishart",
    "[ i - usqr ] if there is only one covariate then @xmath304 .",
    "\\label{eq - usqrhat }        \\end{aligned}\\ ] ] 2 .",
    "[ i - umat ] if there are multiple covariates then @xmath305 13 .",
    "[ i - wsqr0 ] finally , draw a new value of @xmath306 or @xmath307 : 1 .",
    "[ i - wsqr ] if there is only one covariate then @xmath306 is drawn from a gamma distribution .",
    "this can be derived by noting that @xmath308 has the form of a gamma distribution as a function of @xmath205 . the new value of @xmath205 is then simulated as @xmath309",
    ".      \\label{eq - b }        \\end{aligned}\\ ] ] 2 .",
    "[ i - wmat ] if there are multiple covariates then @xmath307 is drawn from a wishart distribution .",
    "this can be derived by noting that @xmath310 has the form of a wishart distribution as a function of @xmath208 . the new value of @xmath208 is then simulated as @xmath311    after completing steps [ i - ycens][i - wsqr0 ] above , an iteration of the gibbs sampler is complete .",
    "one then uses the new simulated values of @xmath312 and the prior parameters , and repeats steps [ i - ycens][i - wsqr0 ] .",
    "the algorithm is repeated until convergence , and the values of @xmath6 and @xmath65 at each iteration are saved . upon reaching convergence , one discards the values of @xmath6 and @xmath65 from the beginning of the simulation , and the remaining values of @xmath313 and @xmath166 ( or @xmath167 ) may be treated as a random draw from the posterior distribution , @xmath224 .",
    "one can then use these values to calculate estimates of the parameters , and their corresponding variances and confidence intervals .",
    "the posterior distribution of the parameters can also be estimated from these values of @xmath6 and @xmath65 using histogram techniques .",
    "techniques for monitering convergence of the markov chains can be found in @xcite .",
    "the output from the gibbs sampler may be used to perform bayesian inference on other quantities of interest . in particular , the pearson linear correlation coefficient , @xmath314 , is often used in assessing the strength of a relationship between the @xmath19 and @xmath57 . a random draw from the posterior distribution for the correlation between @xmath28 and @xmath315 , denoted as @xmath316 ,",
    "can be calculated from equation ( [ eq - rho ] ) for each draw from the gibbs sampler . for the gaussian mixture model ,",
    "the variance @xmath317 and covariance matrix @xmath318 are @xmath319 and @xmath320 is the @xmath239 diagonal element of @xmath321 .",
    "the simplification for one covariate is self - evident .",
    "if there is considerable posterior probability near @xmath322 or @xmath323 , then the gibbs sampler can get ` stuck ' .",
    "for example , if @xmath323 , then step [ i - xi1 ] of the gibbs sampler will draw values of @xmath324 . then , step [ i - mu ] will produce a new value of @xmath200 that is almost identical to the previous iteration , step [ i - tsqr ] will produce a new value of @xmath323 , and so on . the gibbs sampler will eventually get ` unstuck ' , but this can take a long time and result in very slow convergence . in particular , it is very easy for the gibbs sampler to get stuck if the measurement errors are large relative to @xmath12 or @xmath204 , or if the number of data points is small . in this situation",
    "i have found it useful to use the metropolis - hastings algorithm instead .",
    "if the selection function is not independent of @xmath57 , given the independent variables ( cf .",
    "eq.[[eq - trunclik ] ] ) , or if the selection function depends on @xmath19 and the measurement errors are correlated , then posterior simulation based on the gibbs sampler is more complicated . in addition , if the measurement errors are large compared to the intrinsic dispersion in the data , or if the sample size is small , then the gibbs sampler can become stuck and extremely inefficient . in both of these cases",
    "one can use the metropolis - hastings algorithm @xcite to sample from the posterior distribution , as the metropolis - hasting algorithm can avoid constructing markov chains for @xmath27 and @xmath28 . for a description of the metropolis - hastings algorithm",
    ", we refer the reader to @xcite or @xcite .",
    "in this section i perform simulations to illustrate the effectiveness of the gaussian structural model for estimating the regression parameters , even in the presence of severe measurement error and censoring .",
    "in addition , i compare the ols , bces(@xmath2 ) , and fitexy estimators with a maximum - likelihood estimator based on the gaussian mixture model with @xmath112 gaussian .",
    "the first simulation i performed is for a simple regression with one independent variable .",
    "i generated @xmath325 data sets by first drawing @xmath29 values of the independent variable , @xmath27 , from a distribution of the form @xmath326 the distribution of @xmath27 is shown in figure [ f - xidist ] , along with the best - fitting one and two gaussian approximations . in this case",
    "the two gaussian mixture is nearly indistinguishable from the actual distribution of @xmath27 , and thus should provide an excellent approximation to @xmath102 .",
    "the values for @xmath27 had a mean of @xmath327 and a dispersion of @xmath328 .",
    "i varied the number of data points in the simulated data sets as @xmath329 and @xmath330 .",
    "i then simulated values of @xmath28 according to equation ( [ eq - adderr ] ) , with @xmath331 and @xmath332 .",
    "the intrinsic scatter , @xmath34 , had a normal distribution with mean zero and standard deviation @xmath333 , and the correlation between @xmath28 and @xmath27 was @xmath334 .",
    "the joint distribution of @xmath27 and @xmath28 for one simulated data set with @xmath335 is shown in figure [ f - simdist1 ] .    measured values for @xmath27 and @xmath28 were simulated according to equations ( [ eq - xerr ] ) and ( [ eq - yerr ] ) .",
    "the measurement errors had a zero mean normal distribution of varying dispersion and were independent for @xmath19 and @xmath57 .",
    "the variances in the measurement errors , @xmath336 and @xmath43 , were different for each data point and drawn from a scaled inverse-@xmath1 distribution .",
    "the degrees of freedom for the inverse-@xmath1 distribution was @xmath337 , and the scale parameters are denoted as @xmath103 and @xmath338 for the @xmath19 and @xmath57 measurement error variances , respectively .",
    "the scale parameters dictate the typical size of the measurements errors , and were varied as @xmath339 and @xmath340 .",
    "these values corresponded to values of @xmath341 and @xmath342 respectively .",
    "i simulated @xmath343 data sets for each grid point of @xmath344 and @xmath29 , giving a total of @xmath325 simulated data sets .",
    "the joint distributions of @xmath19 and @xmath57 for varying values of @xmath345 and @xmath346 are also shown in figure [ f - simdist1 ] .",
    "these values of @xmath19 and @xmath57 are the ` measured ' values of the simulated data set shown in the plot of @xmath28 as a function of @xmath27 .    for each simulated data set",
    ", i calculated the maximum - likelihood estimate , found by maximizing equation ( [ eq - bobslik ] ) . for simplicity ,",
    "i only use @xmath112 gaussian .",
    "i also calculated the ols , bces(@xmath2 ) , and fitexy estimates for comparison .",
    "i calculated a ols estimate of @xmath12 by subtracting the average @xmath122 from the variance in the regression residuals .",
    "if the ols estimate of @xmath12 was negative , i set @xmath347 . following @xcite",
    ", i estimate @xmath12 for a bces(@xmath2)-type estimator as @xmath348 , where @xmath349 is the average measurement error variance in @xmath57 , and @xmath350 is the bces(@xmath2 ) estimate of the slope . if @xmath351 is negative , i set @xmath352 . following @xcite ,",
    "i compute a fitexy estimate of @xmath353 by increasing @xmath12 until @xmath117 , or assume @xmath118 if @xmath119 . the sampling distributions of the slope and intrinsic scatter estimators for @xmath354",
    "are shown in figures [ f - sampdist1 ] and [ f - sampdist2 ] as a function of @xmath355 , and the results of the simulations are summarized in table [ t - univest ] .    the bias of the ols estimate is apparent , becoming more severe as the measurement errors in the independent variable increase .",
    "in addition , the variance in the ols slope estimate decreases as the measurement errors in @xmath27 increase , giving one the false impression that one s estimate of the slope is more precise when the measurement errors are large .",
    "this has the effect of concentrating the ols estimate of @xmath115 around @xmath356 , thus effectively erasing any evidence of a relationship between the two variables . when the measurement errors are large , the ols estimate of the intrinsic scatter , @xmath357 , is occasionally zero .",
    "the bces(@xmath2 ) estimator performs better than the ols and fitexy estimators , being approximately unbiased when the measurement errors are @xmath358 .",
    "however , the bces estimate of the slope , @xmath359 , suffers some bias when the measurement errors are large and/or the sample size is small .",
    "in addition , the variance in @xmath360 is larger than the mle , and @xmath360 becomes considerably unstable when the measurement errors on @xmath27 are large",
    ". this instability results because the denominator in the equation for @xmath350 is @xmath361 .",
    "if @xmath362 , then the denominator is @xmath363 , and @xmath364 can become very large .",
    "similar to the ols and fitexy estimates , the estimate of the intrinsic variance for the bces - type estimator is often zero when the measurement errors are large , suggesting the false conclusion that there is no intrinsic scatter about the regression line .",
    "the fitexy estimator performed poorly in the simulations , being both biased and highly variable .",
    "the bias of the fitexy estimator is such that @xmath365 tends to overestimate @xmath115 , the severity of which tends to increase as @xmath60 decreases .",
    "this upward bias in @xmath365 has been noted by @xcite , who also performed simulations comparing @xmath365 with @xmath360 .",
    "they note that when one minimizes @xmath114 alternatively with respect to @xmath115 and @xmath12 , and iterates until convergence , then the bias in @xmath365 can be improved .",
    "i have tested this and also find that the bias in @xmath365 is reduced , but at the cost of a considerable increase in variance in @xmath365 .",
    "in general , our simulations imply that the variance of the fitexy estimator is comparable to that of the bces(@xmath2 ) estimator if one does not iterate the minimization of @xmath114 , and the variance of @xmath365 is larger if one does iterate . however , since @xmath360 is approximately unbiased when @xmath59 is not too large , @xmath360 should be preferred over @xmath365 . in addition , when the measurement errors are large the fitexy estimate of @xmath353 is commonly @xmath366 , similar to the bces - type estimate of the intrinsic dispersion .",
    "the maximum - likelihood estimator based on the gaussian structural model performs better than the ols , bces , and fitexy estimators , and gives fairly consistent estimates even in the presence of severe measurement error and low sample size .",
    "the mle is approximately unbiased , in spite of the fact that the mle incorrectly assumes that the independent variables are normally distributed .",
    "the variance in the mle of the slope , @xmath367 , is smaller than that of @xmath360 and @xmath365 , particularly when @xmath59 is large .",
    "in contrast to the ols estimate of the slope , the dispersion in @xmath367 increases as the measurement errors increases , reflecting the additional uncertainty in @xmath367 caused by the measurement errors . finally , in contrast to the other estimators , the mle of the intrinsic variance is always positive , and the probability of obtaining @xmath368 is negligible for these simulations .",
    "i argued in ",
    "[ s - normreg ] that assuming a uniform distribution on @xmath27 does not lead to better estimates than the usual ols case .",
    "i also used these simulations to estimate the sampling density of the mle assuming @xmath110 .",
    "the results were nearly indistinguishable from the ols estimator , supporting our conjecture that assuming @xmath110 does not offer an improvement over ols .",
    "while it is informative to compare the sampling distribution of our proposed maximum - likelihood estimator with those of the ols , bces(@xmath2 ) , and fitexy estimators , i do not derive the uncertainties in the regression parameters from the sampling distribution of the mle .",
    "as described in   [ s - markov ] , we derive the uncertainties in the regression parameters by simulating draws from the posterior distribution , @xmath224 .",
    "this allows a straight - forward method of interpreting the parameter uncertainties that does not rely on large - sample approximations , as the posterior distribution is the probability distribution of the parameters , given the observed data .",
    "the posterior distributions of @xmath369 and @xmath353 for a simulated data set with @xmath370 and @xmath371 is shown in figure [ f - post1d ] . when estimating these posteriors , i used @xmath372 gaussians in the mixture model .",
    "as can be seen from figure [ f - post1d ] , the true values of @xmath369 and @xmath353 are contained within the regions of non - negligible posterior probability .",
    "i have estimated posteriors for other simulated data sets , varying the number of data points and the degree of measurement error .",
    "as one would expect , the uncertainties in the regression parameters , represented by the widths of the posterior distributions , increase as the size of the measurement errors increase and the sample size decreases .",
    "a common frequentist approach is to compute the covariance matrix of the mle by inverting the estimated fisher information matrix , evaluated at the mle .",
    "then , under certain regularity conditions , the mle of the parameters is asympotically normally distributed with mean equal to the true value of the parameters and covariance matrix equal to the inverse of the fisher information matrix .",
    "furthermore , under these regularity conditions the posterior distribution and sampling distribution of the mle are asymptotically the same .",
    "figure [ f - fbcompare ] compares the posterior distribution of the slope for a simulated data set with that inferred from the mle .",
    "the posterior and mle was calculated assuming @xmath112 gaussian . as can be seen ,",
    "the posterior distribution for @xmath115 is considerably different from the approximation based on the mle of @xmath115 , and thus the two have not converged for this sample . in particular , the posterior is more skewed and heavy - tailed , placing more probability on values of @xmath373 than does the distribution approximated by the mle .",
    "therefore , uncertainties in the mle should be interpreted with caution if using the asymptotic approximation to the sampling distribution of the mle .      to assess the effectiveness of the gaussian structural model in dealing with censored data sets with measurement error",
    ", i introduced non - detections into the simulations .",
    "the simulations were performed in an identical manner as that described in ",
    "[ s - sims1 ] , but now i only consider sources to be ` detected ' if @xmath374 . for those sources that were ` censored ' ( @xmath375 ) , i placed an upper limit on them of @xmath376 .",
    "i focus on the results for a simulated data set with @xmath377 data points and measurement errors similar to the intrinsic dispersion in the data , @xmath371 and @xmath378 .",
    "the detection threshold of @xmath374 resulted in a detection fraction of @xmath379 .",
    "this simulation represents a rather extreme case of large measurement errors and low detection fraction , and provides an interesting test of the method . in figure [ f - cens_reg ]",
    "i show the distribution of @xmath27 and @xmath28 , as well as the distribution of their measured values , for one of the simulated data sets .",
    "for this particular data set , there were 29 detections and 71 non - detections .",
    "as can be seen , the significant censoring and large measurement errors have effectively erased any visual evidence for a relationship between the two variables .",
    "i estimated the posterior distribution of the regression parameters for this data set using the gibbs sampler ( cf ,   [ s - gibbs ] ) with @xmath372 gaussians .",
    "the posterior median of the regression line , as well as the @xmath380 pointwise confidence intervals of the posterior probability . while the difference between confidence intervals and credibility intervals is not purely semantical",
    ", i do not find the difference to be significant within the context of my work , so i use the more familiar term ` confidence interval ' . ] on the regression line are shown in figure [ f - cens_reg ] .",
    "the posterior distributions for @xmath369 and @xmath353 are shown in figure [ f - censpost ] . as can be seen ,",
    "the true value of the parameters is contained within the @xmath381 probability regions , although the uncertainty is large .",
    "for this particular data set , we can put limits on the value of the correlation coefficient as @xmath382 and the slope as @xmath383 . for comparison , the usual maximum - likelihood estimate that ignores the measurement error ( e.g. , * ? ? ?",
    "* ) concludes @xmath384 .",
    "this estimate is biased and differs from the true value of @xmath115 at a level of @xmath385 .",
    "the posterior constraints on the regression parameters are broad , reflecting our considerable uncertainty in the slope , but they are sufficient for finding a positive correlation between the two variables , @xmath27 and @xmath28 . therefore , despite the high level of censoring and measurement error in this data set , we would still be able to conclude that @xmath28 increases as @xmath27 increases .",
    "to further illustrate the effectiveness of the method , i apply it to a data set drawn from my work on investigating the x - ray properties of radio - quiet quasars ( rqqs ) .",
    "recent work has suggested a correlation between quasar x - ray spectral slope , @xmath387 and quasar eddington ratio , @xmath386 ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) . in this section",
    "i apply the regression method to a sample of 39 @xmath4 rqqs and confirm the @xmath3@xmath386 correlation . because the purpose of this section is to illustrate the use of this regression method on real astronomical data , i defer a more in - depth analysis to a future paper .",
    "estimation of the eddington luminosity , @xmath388 , requires an estimate of the black hole mass , @xmath389 .",
    "black hole virial masses may be estimated as @xmath390 , where @xmath391 is the broad line region size , and @xmath392 is the velocity dispersion of the gas emitting the broad emission lines",
    ". a correlation has been found between the luminosity of a source and the size of it s broad line region ( the @xmath391@xmath393 relationship , e.g. , * ? ? ?",
    "one can then exploit this relationship , and use the broad line @xmath394 as an estimate for @xmath392 , obtaining virial mass estimates @xmath395 ( e.g. , * ? ? ?",
    "* ) , where the exponent is @xmath396 ( e.g. , * ? ? ?",
    "unfortunately , the uncertainty on the broad line estimates of @xmath389 can be considerable , having a standard deviation of @xmath397 dex ( e.g. , * ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) . for ease of comparison with previous work",
    ", i estimate @xmath389 using only the h@xmath115 emission line .",
    "the logarithm of the virial mass estimates were calculated using the h@xmath115 luminosity and @xmath394 according to the relationship given by @xcite .",
    "my sample consists of a subset of the sample of @xcite .",
    "these sources have measurements of the x - ray photon index , @xmath398 , obtained from _",
    "chandra _ observations , and measurements of the optical / uv luminosity at @xmath399 , denoted as @xmath400 , obtained from sdss spectra .",
    "the h@xmath115 profile was modeled as a sum of gaussians and extracted from the sdss spectra according to the procedure described in @xcite .",
    "i estimated the h@xmath115 @xmath394 and luminosity from the line profile fits .",
    "i estimate the bolometric luminosity , @xmath401 , from the luminosity at @xmath399 , assuming a constant bolometric correction @xmath402 @xcite .",
    "the standard deviation in this bolometric correction reported by @xcite is 3.1 , implying an uncertainty in @xmath403 of @xmath404 dex .",
    "combining this with the @xmath405 dex uncertainty on @xmath406 , the total ` measurement error ' on @xmath407 becomes @xmath408 dex .",
    "the distribution of @xmath3 as a function of @xmath409 is shown in figure [ f - gamx_eddrat ] .",
    "as can be seen , the measurement errors on both @xmath3 and @xmath409 are large and make a considerable contribution to the observed scatter in both variables , where @xmath410 and @xmath411 .",
    "therefore , we expect the measurement errors to have a significant effect on the correlation and regression analysis .",
    "i performed the regression assuming the linear form @xmath412 , and modelleling the intrinsic distribution of @xmath413 using @xmath372 gaussians .",
    "draws from the posterior were obtained using the gibbs sampler .",
    "the marginal posterior distributions for @xmath414 , and the correlation between @xmath3 and @xmath413 , @xmath314 , are shown in figure [ f - posthb ] , and the posterior median and @xmath381 ( @xmath415 ) pointwise intervals on the regression line are shown in figure [ f - gamx_eddrat ] .",
    "the posterior median estimate of the parameters are @xmath416 for the constant , @xmath417 for the slope , @xmath418 for the intrinsic scatter about the regression line , @xmath419 for the mean of @xmath409 , and @xmath420 dex for the dispersion in @xmath413 . here , i have used a robust estimate of the posterior standard deviation as an ` error bar ' on the parameters .",
    "these results imply that the observed scatter in @xmath409 is dominated by measurement error , @xmath421 , as expected from the large value of @xmath59 .    for comparison ,",
    "the bces(@xmath2 ) estimate of the slope is @xmath422 , the fitexy estimate is @xmath423 , and the ols estimate is @xmath424 ; the standard error on @xmath365 was estimated using bootstrapping .",
    "figure [ f - gamx_eddrat ] also compares the ols , bces , and fitexy best - fit lines with the posterior median estimate .",
    "the @xmath381 confidence region on the slope implied by the posterior draws is @xmath425 , whereas the approximate @xmath381 confidence region implied by the bces , fitexy , and ols standard errors are @xmath426 , @xmath427 , and @xmath428 , respectively . the ols and fitexy estimates and the bayesian approach give ` statistically significant ' evidence for a correlation between @xmath409 and @xmath3",
    "; however the bces estimate is too variable to rule out the null hypothesis of no correlation . as noted before , the large measurement errors on @xmath429 bias the ols estimate of @xmath115 toward shallower values and the fitexy estimate of @xmath115 toward steeper values .",
    "because of this bias , confidence regions based on @xmath47 and @xmath365 are not valid because they are not centered on the true value of @xmath115 , and thus do not contain the true value with the stated probability ( e.g. , @xmath381 ) . on the other hand ,",
    "confidence regions based on the bces estimate are likely to be approximately valid ; however , in this example the large measurement errors have caused @xmath360 to be too variable to give meaningful constraints on the regression slope .",
    "the bces - type estimate of the intrinsic dispersion was @xmath430 and the ols estimate of the intrinsic dispersion was @xmath431 , where both were calculated in the same manner as in   [ s - sims1 ] .",
    "the fitexy estimate of the intrinsic dispersion was @xmath366 , as @xmath119 .",
    "the bces - type estimate of @xmath353 is similar to the bayesian posterior median estimate , while @xmath432 overestimates the scatter compared to the bayesian estimate by @xmath433 .",
    "in contrast , the fitexy estimator does not find any evidence for intrinsic scatter in the regression , which is inconsistent with the posterior distribution of @xmath353 .    from the posterior distribution",
    ", we can constrain the correlation between @xmath3 and @xmath409 to be @xmath434 with @xmath435 probability , confirming the positive correlation between @xmath3 and eddington ratio seen previously . the posterior median estimate of the correlation",
    "is @xmath436 , compared with an estimate of @xmath437 if one naively calculates the correlation directly from the measured data .",
    "the large measurement errors significantly attenuate the observed correlation , making the observed correlation between @xmath3 and @xmath413 appear weaker than if one does not correct for the measurement errors .",
    "in this work i have derived a likelihood function for handling measurement errors in linear regression of astronomical data .",
    "our probability model assumes that the measurement errors are gaussian with zero mean and known variance , that the intrinsic scatter in the dependent variable about the regression line is gaussian , and that the intrinsic distribution of the independent variables can be well approximated as a mixture of gaussians .",
    "i extend this model to enable the inclusion of non - detections , and describe how to incorporate the data selection process .",
    "a gibbs sampler is described to enable simulating random draws from the posterior distribution .",
    "i illustrated the effectiveness of structural gaussian mixture model using simulation .",
    "for the specific simulations performed , a maximum - likelihood estimator based on the gaussian structural model performed better than the ols , bces(@xmath2 ) , and fitexy estimators , especially when the measurement errors were large .",
    "in addition , our method also performed well when the measurement errors were large and the detection fraction was small , with the posterior distributions giving reasonable bounds on the regression parameters .",
    "these results were in spite of the fact that the intrinsic distribution of the independent variable was not a sum of gaussians for the simulations , suggesting that approximating the distribution of the independent variable as a mixture of gaussians does not lead to a significant bias in the results .",
    "finally , i concluded by using the method to fit the radio - quiet quasar x - ray photon index as a function of @xmath409 , using a sample of 39 @xmath4 sources .",
    "the posterior distribution for this data set constrained the slope to be @xmath438 and the linear correlation coefficient to be @xmath439 , confirming the correlation between x - ray spectral slope and eddington ratio seen by other authors .",
    "although i have focused on linear regression in this work , the approach that i have taken is quite general and can be applied to other applications . in particular , equations ( [ eq - obslik2 ] ) , ( [ eq - trunclik ] ) , and ( [ eq - censlik ] )",
    "are derived under general conditions and are not limited to regression . in this work ,",
    "i assume forms for the respective probability densities that are appropriate for linear regression ; however , these equation provide a framework for constructing more general probability models of one s data , as in , for example , nonlinear fitting * ? ? ? * or estimation of luminosity functions .",
    "this work was supported in part by nsf grant ast-0307384 .",
    "the author would like to thank the referee for comments that contributed to the improvement of this paper , and for providing some of the references to the statistics literature .",
    "the author would also like to thank jill bechtold , eric feigelson , and aneta siemiginowska for looking over and offering helpful comments on an early version of this paper .",
    "aitken , m. , & rocci , r. , 2002 , statistics and computing , 12 , 163 akritas , m.  g. , & bershady , m.  a.  1996 , , 470 , 706 akritas , m.  g. , & siebert , j.  1996 , , 278 , 919 barker , d.  r. , & diana , l.  m.  1974 , am .  j.  phys . , 42 , 224 carroll , r.  j. , roeder , k. , & wasserman , l. ,  1999 , biometrics , 55 , 44 carroll , r.  j. , ruppert , d. , & stefanski , l.  a. ,  1995 , measurement error in nonlinear models ( london : chapman & hall ) chib , s. , & greenberg , e.  1995 , amer .",
    "stat . , 49 , 327 clutton - brock , m.  1967 , technometrics , 9 , 261 davison , a.  c. , & hinkley , d.  v.  1997 , bootstrap methods and their application ( cambridge : cambridge university press ) dellaportas , p. , stephens , d.  a. ,  1995 , biometrics , 51 , 1085 dempster , a. , laird , n. , & rubin , d.  1977 , j.  r. statist .",
    "b. , 39 , 1 efron , b.  1979 , ann .",
    ", 7 , 1 elvis , m. , et al .",
    "1994 , , 95 , 1 feigelson , e.  d. , & nelson , p.  i.  1985 , , 293 , 192 feigelson , e.  d. ,  1992 , in statistical challenges in modern astronomy , ed .",
    "e. feigelson & g. babu , ( new york : springer - verlag ) fox , j.  1997 , applied regression analysis , linear models , and related methods ( thousand oaks : sage publications , inc . ) freedman , l.  s. , fainberg , v. , kipnis , v. , midthune , d. , & carrol , r.  j. ,  2004 , biometrics , 60 , 172 fuller , w.  a.  1987 , measurement error models ( new york : john wiley & sons ) gelman , a. , carlin , j.  b. , stern , h.  s. , & rubin , d.  b.  2004 , bayesian data analysis ( 2nd ed . ; boca raton : chapman & hall / crc ) gull , s.  f.  1989 , in maximum entropy and bayesian methods , ed .",
    "j. skilling , ( dordrecht : kluwer academic publishers ) 511 hastings , w.  k.  1970 , biometrika , 57 , 97 huang , x. , stefanski , l.  a. , & davidian , m.  2006 , biometrika , 93 , 53 isobe , t. , feigelson , e.  d. , & nelson , p.  i.  1986 , , 306 , 490 isobe , t. , feigelson , e.  d. , akritas , m.  g. , & babu , g.  j.  1990 , , 364 , 104 kaspi , s. , maoz , d. , netzer , h. , peterson , b.  m. , vestergaard , m. , & jannuzi , b.  t.  2005 , , 629 , 61 kelly , b.  c. , bechtold , j. , siemiginowska , a. , aldcroft , t. , & sobolewska , m.  2007 , , 657 , 116 kelly , b.  c. , & bechtold , j.  2007 , , 168 , 1 landy , s.  d. , & szalay , a.  s.  1992 , , 391 , 494 little , r.  j.  a. , & rubin , d.  b.  2002 , statistical analysis with missing data ( 2nd ed .",
    "; hoboken : john wiley & sons ) loredo , t.  j. ,  1992 , in statistical challenges in modern astronomy , ed .",
    "e. feigelson & g. babu , ( new york : springer - verlag ) marshall , h.  l. ,  1992 , in statistical challenges in modern astronomy , ed .",
    "e. feigelson & g. babu , ( new york : springer - verlag ) mclure , r.  j. , & jarvis , m.  j.  2002 , , 337 , 109 metropolis , n. , & ulam , s.  1949 , j.  amer .",
    "assoc . , 44 , 335 metropolis , n. , rosenbluth , a.  w. , rosenbluth , m.  n. , teller , a.  h. , & teller , e.  1953 , j.  chem .",
    "phys . , 21 , 1087 mller , p. , & roeder , k. ,  1997 , biometrika , 84 , 523 piconcelli , e. , jimenez - bailn , e. , guainazzi , m. , schartel , n. , rodrguez - pascual , p.  m. , & santos - lle , m.  2005 , , 432 , 15 porquet , d. , reeves , j.  n. , obrien , p. , & brinkmann , w.  2004 , , 422 , 85 press , w.  h. , teukolsky , s.  a. , vetterling , w.  t. , & flannery , b.  p.  1992",
    ", numerical recipes ( second ed . ; cambridge : cambridge unv . press ) richardson , s. , & leblond , l. ,  1997 , statistics in medicine , 16 , 203 richardson , s. , leblond , l. , jaussent , i. , & green , p.  j. ,  2002 , j.  r. statist .",
    "a , 165 , 549 ripley , b.  d.  1987 , stochastic simulation ( new york : john wiley & sons ) roeder , k. , & wasserman , l.  1997 , j.  amer .",
    "assoc . , 92 , 894 roy , s. , banerjee , t. ,  2006 , ann . instit .",
    "statist . math . , 58 , 153 schafer , d.  w. ,  1987 , 74 , 385 schafer , d.  w. ,  2001 , biometrics , 57 , 53 scheines , r. , hoijtink , h. , boomsma , a. ,  1999 , psychometrika , 64 , 37 schmitt , j.  h.  m.  m.  1985 , , 293 , 178 shemmer , o. , brandt , w.  n. , netzer , h. , maiolino , r. , & kaspi , s.  2006 , , 646 , l29 stapleton , d.  c. & young , d.  j.  1984 , econometrica , 52 , 737 tremaine , s. , et al .",
    "2002 , , 574 , 740 vestergaard , m. , & peterson , b.  m.  2006 , , 641 , 689 wandel , a. , peterson , b.  m. , & malkan , m.  a.  1999 , , 526 , 579 weiner , b.  j. , et al .   2006 , , 653 , 1049 weiss , a.  a. ,  1993 , j.  econometrics . , 56 , 169 .",
    "zellner , a.  1971 , an introduction to bayesian inference in econometrics ( new york : john wiley & sons )     and @xmath27 ( left ) , and the measured values of @xmath57 and @xmath19 ( right ) , from a simulated censored data set of @xmath354 data points , @xmath378 , and @xmath371 ( cf . , ",
    "[ s - sims2 ] ) . in the plot of @xmath28 and @xmath27 ,",
    "the solid squares denote the values of @xmath27 and @xmath28 for the detected data points , and the hollow squares denote the values of @xmath27 and @xmath28 for the undetected data points .",
    "the solid line in both plots is the true regression line . in the plot of @xmath57 and @xmath19 ,",
    "the squares denote the measured values of @xmath19 and @xmath57 for the detected data points , and the arrows denote the ` upper limits ' on @xmath57 for the undetected data points . the fictitious data point with error bars",
    "illustrates the median values of the error bars .",
    "the dashed - dotted line shows the best fit regression line , as calculated from the posterior median of @xmath123 and @xmath115 , and the filled region defines the approximate @xmath381 @xmath440 pointwise confidence intervals on the regression line .",
    "the true values of the regression line are contained within the @xmath381 confidence intervals.[f - cens_reg],title=\"fig : \" ]   and @xmath27 ( left ) , and the measured values of @xmath57 and @xmath19 ( right ) , from a simulated censored data set of @xmath354 data points , @xmath378 , and @xmath371 ( cf . ,   [ s - sims2 ] ) . in the plot of @xmath28 and @xmath27 ,",
    "the solid squares denote the values of @xmath27 and @xmath28 for the detected data points , and the hollow squares denote the values of @xmath27 and @xmath28 for the undetected data points .",
    "the solid line in both plots is the true regression line . in the plot of @xmath57 and @xmath19 ,",
    "the squares denote the measured values of @xmath19 and @xmath57 for the detected data points , and the arrows denote the ` upper limits ' on @xmath57 for the undetected data points . the fictitious data point with error bars",
    "illustrates the median values of the error bars .",
    "the dashed - dotted line shows the best fit regression line , as calculated from the posterior median of @xmath123 and @xmath115 , and the filled region defines the approximate @xmath381 @xmath440 pointwise confidence intervals on the regression line .",
    "the true values of the regression line are contained within the @xmath381 confidence intervals.[f - cens_reg],title=\"fig : \" ]      , as a function of @xmath409 for 39 @xmath0 radio - quiet quasars . in both plots",
    "the thick solid line shows the posterior median estimate ( pme ) of the regression line . in the left plot ,",
    "the filled region denotes the @xmath381 @xmath440 pointwise confidence intervals on the regression line . in the right plot",
    ", the thin solid line shows the ols estimate , the dashed line shows the fitexy estimate , and the dot - dashed line shows the bces(@xmath2 ) estimate ; the error bars have been omitted for clarity .",
    "a significant positive trend is implied by the data.[f - gamx_eddrat],title=\"fig : \" ] , as a function of @xmath409 for 39 @xmath0 radio - quiet quasars . in both plots",
    "the thick solid line shows the posterior median estimate ( pme ) of the regression line . in the left plot ,",
    "the filled region denotes the @xmath381 @xmath440 pointwise confidence intervals on the regression line . in the right plot",
    ", the thin solid line shows the ols estimate , the dashed line shows the fitexy estimate , and the dot - dashed line shows the bces(@xmath2 ) estimate ; the error bars have been omitted for clarity .",
    "a significant positive trend is implied by the data.[f - gamx_eddrat],title=\"fig : \" ]    cccccccccc 0.5 & 25 & @xmath441 & @xmath442 & @xmath443 & @xmath444 & @xmath445 & @xmath446 & @xmath447 & @xmath448 + & 50 & @xmath449 & @xmath450 & @xmath451 & @xmath452 & @xmath453 & @xmath454 & @xmath455 & @xmath456 + & 100 & @xmath457 & @xmath458 & @xmath459 & @xmath460 & @xmath461 & @xmath462 & @xmath463 & @xmath464 + 1.0 & 25 & @xmath465 & @xmath466 & @xmath467 & @xmath468 & @xmath469 & @xmath470 & @xmath471 & @xmath472 + & 50 & @xmath473 & @xmath474 & @xmath475 & @xmath476 & @xmath477 & @xmath478 & @xmath479 & @xmath480 + & 100 & @xmath481 & @xmath482 & @xmath483 & @xmath484 & @xmath485 & @xmath486 & @xmath487 & @xmath488 + 2.0 & 25 & @xmath489 & @xmath490 & @xmath491 & @xmath492 & @xmath493 & @xmath494 & @xmath495 & @xmath496 + & 50 & @xmath497 & @xmath498 & @xmath499 & @xmath500 & @xmath501 & @xmath502 & @xmath503 & @xmath504 + & 100 & @xmath505 & @xmath506 & @xmath507 & @xmath508 & @xmath509 & @xmath510 & @xmath511 & @xmath512 +"
  ],
  "abstract_text": [
    "<S> i describe a bayesian method to account for measurement errors in linear regression of astronomical data . </S>",
    "<S> the method allows for heteroscedastic and possibly correlated measurement errors , and intrinsic scatter in the regression relationship . </S>",
    "<S> the method is based on deriving a likelihood function for the measured data , and i focus on the case when the intrinsic distribution of the independent variables can be approximated using a mixture of gaussians . </S>",
    "<S> i generalize the method to incorporate multiple independent variables , non - detections , and selection effects ( e.g. , malmquist bias ) . </S>",
    "<S> a gibbs sampler is described for simulating random draws from the probability distribution of the parameters , given the observed data . </S>",
    "<S> i use simulation to compare the method with other common estimators . </S>",
    "<S> the simulations illustrate that the gaussian mixture model outperforms other common estimators and can effectively give constraints on the regression parameters , even when the measurement errors dominate the observed scatter , source detection fraction is low , or the intrinsic distribution of the independent variables is not a mixture of gaussians . </S>",
    "<S> i conclude by using this method to fit the x - ray spectral slope as a function of eddington ratio using a sample of 39 @xmath0 radio - quiet quasars . </S>",
    "<S> i confirm the correlation seen by other authors between the radio - quiet quasar x - ray spectral slope and the eddington ratio , where the x - ray spectral slope softens as the eddington ratio increases . </S>",
    "<S> idl routines are made available for performing the regression . </S>"
  ]
}