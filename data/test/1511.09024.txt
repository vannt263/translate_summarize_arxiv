{
  "article_text": [
    "cloud radio access networks ( c - rans ) have drawn considerable attention for their potential to sustain the explosive traffic demand in wireless communications .",
    "unlike traditional cellular networks , a c - ran splits the low - cost and light - weighted remote radio heads ( rrhs ) from the baseband processing units ( bbus ) , and merges the latter into a data center .",
    "the rrhs and bbus are connected by a low - latency , high - bandwidth fiber network .",
    "the special architecture of c - ran allows full - scale rrh coordination , which enables flexible interference management , dynamic resource allocation , and collaborative radio technology .",
    "this consequently leads to significant capacity enhancement .",
    "the full - scale coordination , however , also introduces a severe complexity issue .",
    "the state - of - the - art c - ran technology is able to support thousands of rrhs @xcite .",
    "full - scale rrh coordination over such a large network involves prohibitively high computational complexity .",
    "for example , the linear minimum mean square error ( mmse ) detector requires cubic complexity in the network size ( in terms of the number of rrhs ) , or equivalently a quadratic complexity normalized by the number of rrhs@xcite .",
    "this implies that the detection complexity quickly becomes unaffordable as the network size grows . as such ,",
    "a main challenge of c - ran is to design _ scalable _ coordination algorithms , where _ scalable _ means : 1 ) the performance is near the optimum performance of full - scale rrh coordination , 2 ) the normalized computational complexity per rrh does not grow with the network size , or equivalently , the total computational complexity grows linearly with the network size .    in a c - ran ,",
    "users and rrhs are scattered over a large area . due to the attenuation effect in the propagation of electromagnetic waves , an rrh usually receives strong signals from only a small number of nearby users .",
    "intuitively , ignoring the signals from far - away users in general does not cause much performance loss . as shown in our previous work @xcite , with a distance - threshold - based channel sparsification approach",
    ", a vast majority of signals over the transmission links can be ignored if we can tolerate a small degradation in the signal - to - noise - plus - interference ratio ( sinr ) . as such , each rrh only needs to serve its nearby users whose distances to the rrh are below a certain threshold . based on the sparsified channel matrix",
    ", @xcite proposes an algorithm that greatly reduces the computational complexity of mmse detection from @xmath0 to @xmath1 , where @xmath2 is the total number of rrhs and @xmath3 $ ] is a constant depending on the computing implementations .",
    "however , the algorithm is not perfectly scalable yet , in the sense that the complexity grows faster than linear with the network size .    in this paper",
    ", we are interested in designing a perfectly scalable algorithm for joint signal detection in the uplink of c - ran . with channel sparsification @xcite",
    ", a c - ran system can be represented by a bipartite random geometric graph , as shown in fig .",
    "[ fig : sfg ] . here ,",
    "rrhs and users are treated as vertices / nodes , and an edge connects an rrh and a user if the distance between them does not exceed the threshold .",
    "then , signal detection in a c - ran is converted to a statistical inference problem over a bipartite random geometric graph , where an inference problem is to estimate the signals from unobserved nodes ( i.e. , user nodes ) , conditional on signals from observed nodes ( i.e. , rrh nodes ) .",
    "message passing ( a.k.a .",
    "belief propagation ) is well known as a low - complexity iterative solution to inference over random sparse graphs @xcite . in a message - passing algorithm",
    ", messages are exchanged between nodes with edge connection .",
    "thus , the complexity of message passing is proportional to the number of edges in the network . in a c",
    "- ran with channel sparsification , the number of messages per rrh is proportional to the number of nearby users in its neighborhood , which does not scale with the network size .",
    "thus , the total complexity per iteration of message passing in c - rans is linear in the network size .",
    "unfortunately , the convergence of message passing over a bipartite random geometric graph is not guaranteed .",
    "the reason is that the random geometric graph is locally dense and always contains loops .",
    "it is well - known that message passing is not guaranteed to converge when the graph is loopy @xcite . indeed , numerical simulations indicate a non - trivial probability that the message - passing algorithm for c - rans does not converge .      in this paper",
    ", we propose a novel algorithm , referred to as randomized gaussian message passing ( rgmp ) , to address the convergence issue of message passing over a bipartite random geometric graph .",
    "unlike conventional message passing with synchronous message updating , the rgmp algorithm updates messages asynchronously with a random sequential order .",
    "to the best of our knowledge , this is the first work to introduce randomness to the update schedule of message passing algorithms .",
    "intuitively , when messages are exchanged among nodes of a loopy graph , errors may accumulate along loops , which eventually leads to the divergence of the algorithm @xcite .",
    "updating messages sequentially in a random order effectively weakens the effect of loops and thus improves the convergence .",
    "the randomness of the message update schedule also simplifies the analysis , and allows to derive a necessary and sufficient condition for the expected convergence of the rgmp algorithm .",
    "we show by both analysis and numerical results that the rgmp algorithm converges with a much higher probability than conventional message passing .",
    "indeed , we have never observed a single divergence in our simulations when the network size is moderately large ( i.e. , when the network has more than five rrhs ) .",
    "moreover , our numerical results indicate that the number of iterations of rgmp does not increase with the network size .",
    "this implies that the total computational complexity is linear with the network size , and thus the algorithm is perfectly scalable .      in the previous work @xcite",
    ", we proposed a dynamic clustering algorithm to reduce the computational complexity of the mmse detector .",
    "the complexity of the algorithm is reduced from cubic to no more than quadratic in the number of rrhs . in @xcite ,",
    "shi _ et al .",
    "_ presented a two - stage approach to solve large - scale convex optimization problems for dense wireless cooperative networks , such as c - rans .",
    "matrix stuffing and alternating direction method of multipliers ( admm ) were used to speed up the computation .",
    "particularly , it was shown in @xcite that the expected output of randomly permuted admm converges to the unique solution of the optimal linear detector .",
    "however , we will see that the admm algorithm converges much more slowly than the proposed rgmp algorithm when applied to large networks like c - rans .",
    "in addition , there are other iterative solvers of the linear mmse detector , such as the preconditioned conjugate gradient ( pcg ) method @xcite and the generalized approximate message passing ( gamp ) algorithm @xcite .",
    "we show by numerical simulation that the number of iterations needed for convergence in these algorithms are roughly linear in the network size .",
    "this translates to quadratic computational complexity in total .",
    "therefore , all these algorithms are not scalable .",
    "the rest of the paper is organized as follows . in section",
    "ii , we describe the system model . in section iii , we introduce a gaussian message - passing algorithm with channel sparsification for signal detection in c - rans with linear complexity per iteration , and then discuss the convergence issue . in section iv",
    ", we propose the rgmp algorithm to address the convergence issue of gaussian message passing . in section",
    "v , the convergence condition of the rgmp algorithm are analysed . in section",
    "vi , simulation results are demonstrated to compare the rgmp algorithm with other existing algorithms .",
    "conclusions and future works are discussed in section vii .",
    "in this paper we consider the uplink transmission of a c - ran with @xmath2 single - antenna rrhs and @xmath4 single - antenna users .",
    "suppose that both the rrhs and the users are randomly located over an area .",
    "let @xmath5 be the signal transmitted by user @xmath6 , and @xmath7 be the received signal at rrh @xmath8 .",
    "denote @xmath9^t$ ] and @xmath10^t$ ] .",
    "then , the received signal vector @xmath11 at the rrhs is @xmath12 where @xmath13 denotes the channel matrix , with the @xmath14-th entry @xmath15 being the channel coefficient between the @xmath6-th user and the @xmath8-th rrh ; @xmath16 is the transmission power allocated to each user ; and @xmath17 is a noise vector received by the rrhs .",
    "the transmitted signals are assumed to have zero mean and unit variance , i.e. , @xmath18=\\mathbf{0}$ ] and @xmath19=\\mathbf{i}$ ] .",
    "we further assume @xmath20 , where @xmath21 is the i.i.d .",
    "rayleigh fading coefficient with zero mean and unit variance , @xmath22 is the distance between the @xmath8-th rrh and the @xmath6-th user , and @xmath23 is the path loss exponent . here , @xmath24 is the path loss from the @xmath6-th user to the @xmath8-th rrh .    in this paper",
    ", we employ linear mmse detection to estimate the transmitte signal vector @xmath25 , with the decision statistics given by @xmath26 in the above , the inversion of the @xmath27 matrix @xmath28 requires computational complexity of @xmath0 .",
    "this complexity is prohibitively high for a large - scale c - ran with hundreds and thousands of rrhs , thus posing a serious scalability problem . in",
    "what follows , we endeavour to develop a scalable algorithm to detect @xmath25 with complexity @xmath29 under the assumption that @xmath4 grows at the same rate as @xmath2 . in other words ,",
    "the average computational complexity per rrh ( or per unit network size ) does not scale with @xmath2 .",
    "in this section , we first describe the channel sparsification approach introduced by the authors in @xcite to model a c - ran as a bipartite random geometric graph .",
    "then , we apply the gaussian message - passing algorithm over bipartite random geometric graphs for signal detection .",
    "we borrow the channel sparsification approach in our recent work @xcite to sparsify the channel matrix , as described below .",
    "the entries of @xmath30 are discarded based on the distances between rrhs and users .",
    "specifically , the @xmath31-th entry in the resulting sparisified channel matrix @xmath32 is given by @xmath33 where @xmath34 is a distance threshold .",
    "given the sparsified channel matrix @xmath32 , the received signal can be represented as @xmath35 where @xmath36 the mmse estimator of @xmath25 is approximated by @xmath37 with @xmath38 + n_0 $ ] for arbitrary rrh @xmath8 .",
    "as proven in @xcite , the channel matrix can be sparsified without considerably compromising the sinr .",
    "the reason is that as the rrhs and users are uniformly distributed over a large area , an rrh can only receive reasonably strong signals from a small number of nearby users , and vice versa .",
    "therefore , the majority of the elements of @xmath30 are relatively small in magnitude , and ignoring them in signal detection leads to marginal loss in the overall system performance . indeed , according to @xcite , when @xmath2 scales in the same order as @xmath4 , the distance threshold @xmath34 does not increase with the network size to achieve a certain sinr performance .",
    "thus , in this paper , we assume that @xmath34 is a predetermined constant regardless of the network size .",
    "this implies that the average number of users connecting to an rrh does not scale with the network size .",
    "channel sparsification simplifies the signal detection in a c - ran to an inference problem over a bipartite random geometric graph ( see fig .",
    "[ fig : sfg ] ) . in the bipartite random geometric graph , rrhs and users in a c - ran",
    "are referred to rrh nodes and user nodes respectively , and edge connections exist only between rrh nodes and user nodes . more specifically , an rrh node is connected to a user node only if the distance between them falls below the threshold @xmath34 , and the weight over such an edge is the channel coefficient from the corresponding user to the corresponding rrh .",
    "suppose that the entries in @xmath25 follow an independent complex gaussian distribution . does not follow a gaussian distribution , the message - passing algorithm presented in this paper gives an approximation of the linear mmse estimation @xcite .",
    "] then , @xmath39 and @xmath25 are jointly gaussian , and therefore the mmse detector in ( [ eqn : x ] ) is also the maximum _ a posteriori _ probability ( map ) detector that maximizes the _ a posteriori _ probability @xmath40 .",
    "that is , @xmath41 the probability density function @xmath40 can be factorized as @xmath42 recall that we sparsify the channel matrix by using the channel sparsification approach given in @xcite . based on ( [ eqn : app_x ] )",
    ", the factorization of @xmath40 is approximated as @xmath43 where @xmath44 contains all @xmath45 with @xmath46 and @xmath47 is the set of user indices with @xmath48 .",
    "we now transfer the bipartite random geometric graph to a factor graph with the factorization in ( [ eqn : pdf ] ) . as illustrated in fig .",
    "[ fig : sfg_2 ] , a factor graph is also a bipartite graph comprising two types of nodes , namely , variable nodes ( denoted by circles ) and check nodes ( denoted by squares ) , together with edges connecting these two types of nodes .",
    "the relation between the factorization ( [ eqn : pdf ] ) and its associated factor graph is as follows .",
    "a check node @xmath49 is connected to a variable node @xmath5 by an edge when there is an edge connecting the @xmath8-th rrh node and the @xmath6-th user node in the corresponding random geometric graph ( i.e. , @xmath48 ) , or equivalently , when the function @xmath49 takes @xmath5 as input .",
    "@xmath32 , @xmath39 @xmath50 for all @xmath6 initial @xmath51 , @xmath52 for all @xmath53 set @xmath54 for all @xmath55 such that @xmath56 , compute @xmath57 @xmath58 @xmath59 @xmath60 compute @xmath61 @xmath62    we are now ready to introduce the gaussian message - passing algorithm for signal detection .",
    "the algorithm will be implemented in the centralized data center .",
    "the messages , namely , the marginals of @xmath63 and @xmath64 , are exchanged along the edges . in this paper , both @xmath63 and @xmath64 are gaussian distributed , and therefore the messages are gaussian probability density functions and can be completely characterised by mean and variance .",
    "denote by @xmath65 and @xmath66 the mean and variance sent from check node @xmath49 to variable node @xmath5 at iteration @xmath67 , respectively , and denote by @xmath68 and @xmath69 the mean and variance sent from variable node @xmath5 to check node @xmath49 at iteration @xmath67 , respectively .",
    "the detailed steps of message passing are presented in algorithm 1 .",
    "we refer to this algorithm as gaussian message passing ( gmp ) , as all the messages involved are gaussian marginals .",
    "note that each rrh only serves users located in a circle with a constant radius @xmath34 .",
    "thus , the average number of messages to be exchanged and computed at each node does not scale with the network size . therefore , the complexity per iteration of the gmp algorithm is linear in the number of rrhs and users .    in spite of its linear complexity per iteration ,",
    "the gmp algorithm is not guaranteed to converge on the factor graphs induced by c - rans .",
    "it is known that the gaussian message - passing algorithm always converges to the optimal solution on a tree - type factor graph @xcite .",
    "it is also known that , if a factor graph is random and sparse enough , the corresponding message - passing algorithm converges asymptotically as the network size grows to infinity @xcite",
    ". however , the factor graph for a bipartite random geometric graph induced from a c - ran is locally dense and far from being a tree .",
    "this is due to the fact that every rrh needs to simultaneously serve multiple nearby users .",
    "for example , @xmath70 in fig .",
    "[ fig : sfg ] form a loop of length 4 . indeed ,",
    "we observe in simulations that the gmp algorithm diverges in c - ran with a non - trivial probability .",
    "even worse , the probability of divergence grows with the network size , as illustrated later in fig .",
    "[ fig : prob_con ] .",
    "we focus on improving the convergence performance of gmp in the rest of the paper .",
    "the gmp algorithm for a c - ran with channel sparsification can be simply extended to the case without channel sparsification by setting the distance threshold to infinity .",
    "however , this leads to an increase of the computational complexity per iteration .",
    "we see that in each iteration of algorithm 1 , messages need to be updated on every edge of the factor graph . from channel randomness , the entries of @xmath30 are non - zero with probability one .",
    "thus , in the factor graph without channel sparsification , every rrh check node @xmath71 is connected to all user variable nodes @xmath72 .",
    "this implies that the total number of edges in the factor graph is @xmath73 , implying that the complexity of the gmp algorithm is @xmath74 per iteration , which is unaffordable for a large - scale c - ran .",
    "@xmath32 , @xmath39 @xmath50 for all @xmath6 initialize @xmath51 , @xmath52 for all @xmath53 . set @xmath54 .",
    "pick a permutation @xmath75 of @xmath76 uniformly at random . for @xmath77 , and @xmath78 , compute @xmath79 @xmath80 @xmath81 @xmath82 compute @xmath83 @xmath84      in this section , we propose the rgmp algorithm to address the convergence issue of gmp .",
    "the main novelty of the rgmp algorithm is on the scheduling strategy for message updating .",
    "the conventional gmp algorithm employs synchronous message passing , i.e. , messages are updated in parallel . as aforementioned ,",
    "synchronous message passing does not work well in c - rans due to local loops in the factor graph . to address this issue",
    ", we propose the rgmp algorithm with random asynchronous scheduling , i.e. , messages are updated sequentially in a randomly permuted order .    the rgmp algorithm is described as follows .",
    "define @xmath85 as @xmath86 at each iteration , we draw a permutation @xmath75 of @xmath87 uniformly from @xmath85 , and update the messages at the user variable node side in the order of @xmath75 .",
    "for example , at the @xmath67-th iteration , when @xmath88 and @xmath89 , we first update all the messages on the edges connecting the user variable node @xmath90 .",
    "then , the messages on the edges connecting the user variable node @xmath91 are updated .",
    "finally , messages related to variable node @xmath92 are updated .",
    "the rgmp algorithm is given in algorithm 2 .",
    "in this subsection , we use a toy example to illustrate the difference between our proposed rgmp algorithm and synchronous / asynchronous gmp . consider the following randomly generated channel matrix @xmath93,\\ ] ] and",
    "let the transmit snr ( i.e. , @xmath94 ) be @xmath95db .",
    "the corresponding received signal @xmath39 is @xmath96^t.\\ ] ] for fairness of comparison , we do not conduct channel sparsification in this example .",
    "that is , the distance threshold is set to infinity .",
    "[ fig : err_mp ] plots the relative error versus the number of iterations for the rgmp algorithm and the gmp algorithm with different message update strategies , i.e. , synchronous update and asynchronous update with different fixed orders , @xmath97 and @xmath98 .",
    "the relative error is defined as @xmath99 , where @xmath100 is the estimation of the transmitted signal after the @xmath67-th iteration .",
    "we see that the synchronous gmp algorithm and the asynchronous one with order @xmath98 diverge , but the asynchronous gmp with order @xmath97 and the proposed rgmp algorithm converge .    the examples in fig .",
    "[ fig : err_mpc ] and [ fig : err_mpd ] show that convergence of asynchronous gmp heavily depends on the update order .",
    "unfortunately , there is no systematic way to derive a fixed update order that guarantees convergence . in general , finding such an update order is difficult , especially in large networks .",
    "this issue is avoided in the proposed rgmp algorithm by randomizing the update schedule instead of fixing one .",
    "indeed , the randomization significantly weakens the loopy effect of the graph , and thus convergence is almost ensured in rgmp .    in fig .",
    "[ fig : prob_con ] , we plot the empirical probability of convergence against the network size , where users and rrhs are uniformly located in a circular network area with user density @xmath101 and rrh density @xmath102 .",
    "the distance threshold @xmath34 is @xmath103 m . for each simulated point in fig .",
    "[ fig : prob_con ] , both gmp and rgmp are run for over @xmath104 times that are randomized over both rrh / user location and channel fading . for gmp ,",
    "the convergence probability decreases when the network size becomes large .",
    "in contrast , no divergence has been observed for the rgmp algorithm throughout our simulations .",
    "in this section , we establish a necessary and sufficient condition for the expected convergence of the proposed rgmp algorithm . for self - containedness , we start with existing results on the analysis of the convergence condition for conventional gmp .",
    "the factor graph of a c - ran contains loops with high probability .",
    "the convergence of gmp on a loopy factor graph has been previously studied in @xcite , with the main result summarized below .    from algorithm 1",
    ", we see that the evolution of the variances @xmath105 is independent of the means @xmath106 , @xmath107 and the received signal @xmath39 . substituting ( [ eqn : v2 ] ) into ( [ eqn : v1 ] )",
    ", we obtain @xmath108 denote ( [ eqn : evo_v ] ) in a vector form as @xmath109 where @xmath110 is the evolution function determined by ( [ eqn : evo_v ] ) , and @xmath111 is a vector consisting of @xmath112 for all @xmath8 and @xmath6 with @xmath113 .",
    "note that @xmath110 is a standard function , the definition of which is given below .",
    "a function @xmath114 is standard if for all @xmath115 the following properties are satisfied .    *",
    "_ positivity : _ @xmath116 . * _ monotonicity : _ if @xmath117 , then @xmath118 . * _ scalability : _ for all @xmath119 , @xmath120 .    from theorem 5.1 of @xcite , the sequence of @xmath111 always converges to a unique fixed point if the initial point @xmath121 and the evolution function is standard .",
    "we now consider the convergence of means .",
    "a vector of means , @xmath122 , is constructed with its @xmath123-th entry being @xmath124 the recursion of the means is given by ( [ eqn : m1 ] ) and ( [ eqn : m2 ] ) . as the variances always converge , the evolution of the means can be written as follows : @xmath125 where @xmath126 is an @xmath127 vector with its @xmath123-th entry being @xmath128 and @xmath129 is an @xmath130 matrix with the @xmath131-th entry being @xmath132 with @xmath133 and @xmath134 . then , a necessary and sufficient condition for the convergence of ( [ eqn : evo_m ] ) is given in theorem 5.2 , @xcite .",
    "that is , in algorithm 1 , the sequence of @xmath122 converges to a unique fixed point if and only if the spectral radius @xmath135 .      in this subsection",
    ", we first show that the message variances always converge in the rgmp algorithm .",
    "then , we focus on the convergence condition of the means in rgmp .",
    "recall that the evolution function of the variances in ( [ eqn : evo_v ] ) is a standard function .",
    "as proven in @xcite , if the evolution function of a synchronous algorithm is standard , then the corresponding asynchronous algorithm converges .",
    "based on that , we obtain the following theorem .    in the rgmp algorithm , the sequence of @xmath112 always converges to the same unique fixed point as in algorithm 1 if the initial point @xmath121 .    with theorem 1",
    ", it suffices to focus on the convergence condition of the means in the rgmp algorithm .",
    "we first derive the evolution function of the means .",
    "denote the permutation at the @xmath67-th iteration as @xmath136 . combining ( [ eqn : rm1 ] ) and ( [ eqn : rm2 ] )",
    ", we obtain the evolution of means @xmath122 as @xmath137 where @xmath138 is an @xmath139 subvector of @xmath122 with the @xmath8-th entry being @xmath140 and @xmath141 is an @xmath139 subvector of @xmath142 with the @xmath8-th entry being @xmath143 @xmath144 is the @xmath27 evolution matrix from user @xmath145 to user @xmath146 with the @xmath147-th entry being @xmath148 @xmath144 is the @xmath149-th submatrix of @xmath150 . more specifically , @xmath151        \\mathbf{\\omega}_{2,1 } & \\mathbf{0 } & \\mathbf{\\omega}_{2,3 } & \\cdots & \\mathbf{\\omega}_{2,k}\\\\[0.6em ]      \\vdots     & \\ddots & \\ddots & \\ddots&\\vdots\\\\[0.6em ]         \\mathbf{\\omega}_{k-1,1 } & \\cdots & \\mathbf{\\omega}_{k-1,k-2 } & \\mathbf{0}&\\mathbf{\\omega}_{k-1,k}\\\\[0.6em ]         \\mathbf{\\omega}_{k,1 } & \\mathbf{\\omega}_{k,2}&\\cdots&\\mathbf{\\omega}_{k-1,k}&\\mathbf{0 }       \\end{bmatrix}.\\ ] ]    we can further rewrite the equation ( [ eqn : random_mp ] ) as @xmath152 where @xmath153_{i ,",
    "j}\\in \\mathbb{c}^{nk\\times nk}$ ] with its @xmath149-th submatrix being @xmath154 and @xmath155_{i , j}\\in \\mathbb{c}^{nk\\times nk}$ ] with its @xmath149-th submatrix being @xmath156 based on the definition of @xmath157 , the determinant of @xmath157 is always @xmath158 or @xmath159 .",
    "it implies that @xmath157 is nonsingular .",
    "then , multiplying both sides of ( [ eqn : evo_m_r ] ) by @xmath160 , we obtain @xmath161 consequently , we obtain the following condition for the convergence of the rgmp algorithm .    for a given sequence of permutations @xmath162",
    ", the rgmp algorithm converges to the fixed point @xmath163 if and only if @xmath164 , where @xmath136 is the permutation at the @xmath67-th iteration .    for an arbitrary permutation @xmath75 , the fixed point of @xmath165",
    "is given by @xmath166 substituting @xmath167 into ( [ eqn : update_3 ] ) , we obtain @xmath168 . clearly , @xmath169 is independent of the choice of permutation @xmath75 .",
    "define @xmath170 . then , @xmath171 , for any iteration number @xmath67 . by recursion",
    ", we obtain @xmath172 .",
    "therefore , @xmath173 provided @xmath174 as @xmath175 .",
    "this concludes the proof .",
    "proposition 2 discusses the convergence for a given sequence of permutations @xmath162 . to quantify the average performance of rgmp over random permutations , we consider expected convergence in the following , where the expectation is taken over all possible permutations .",
    "let the expected output be @xmath176,\\ ] ] where @xmath177 is the set of the permutations after iteration @xmath67 .",
    "we are now ready to present a necessary and sufficient condition for the convergence of @xmath178 .",
    "the expected output @xmath179 $ ] converges to the unique point @xmath163 if and only if the spectral radius @xmath180 , where @xmath181=\\frac{1}{k!}\\sum_{\\sigma \\in \\sigma}(\\mathbf l_{\\sigma}^{-1}\\mathbf r_{\\sigma } ) .",
    "\\label{eqn : condition } \\ ] ]    denote @xmath182 $ ] .",
    "based on the definition of @xmath183 , we obtain @xmath184\\\\ & = & & \\mathrm{e}_{\\sigma_t}\\left[\\mathrm{e}_{\\xi_{t-1}}\\left[\\mathbf l_{\\sigma_t}^{-1}\\mathbf r_{\\sigma_t}\\mathbf{m}^{(t)}+\\mathbf{l}_{\\sigma_t}^{-1}\\mathbf{z}\\right]\\right]\\\\ & = & & \\mathbf{\\lambda}\\bm \\phi^{(t)}+\\mathbf{a}\\mathbf{z}. \\end{aligned}\\ ] ]    from theorem 5.3 in @xcite , the sequence of @xmath178 converges to the fixed point @xmath185 if and only if the spectral radius @xmath180 .",
    "then , it suffices to show that @xmath186 .",
    "note that @xmath187 .",
    "substituting @xmath188 into ( [ eqn : condition ] ) , we obtain @xmath189\\\\ & = & & \\mathrm{e}_{\\sigma}[\\mathbf l_{\\sigma}^{-1}(\\mathbf{\\omega}-\\mathbf{i})+\\mathbf{i}]\\\\ & = & & \\mathbf{a}(\\mathbf{\\omega}-\\mathbf{i})+\\mathbf{i}. \\end{aligned}\\label{eqn : proofa}\\ ] ] recall that @xmath180 , and thus @xmath190 is nonsingular . together with @xmath191 from ( [ eqn : proofa ] ) , we see that both @xmath192 and @xmath193 are nonsingular .",
    "hence , @xmath194 which concludes the proof .    from theorem 1 ,",
    "both gmp and rgmp have guaranteed convergence for variances .",
    "however , they have different conditions to ensure the convergence of means : gmp requires @xmath195 while rgmp requires @xmath196 .",
    "since both @xmath150 and @xmath197 highly depend on the network geometry , it is difficult to theoretically compare these two convergence conditions . to shed light on the difference , we plot the cumulative distribution function ( cdf ) of the spectral radius of @xmath150 and @xmath197 in fig .  [ fig : cdf_rho ] .",
    "we assume that users and rrhs are randomly located in a circular network area with radius @xmath198 .",
    "the user density is @xmath199 and the rrh density is @xmath200 .",
    "the path loss exponent is @xmath201 , and the average transmit snr at the user side equals to @xmath95db .",
    "that is @xmath202db .",
    "we see that @xmath203 is more likely to take small values than @xmath204 .",
    "this implies that rgmp converges with a higher probability than gmp .",
    "indeed , we have run over @xmath205 times for each setting , and @xmath196 for all the cases .      in this subsection",
    ", we present some special cases to show that the convergence condition in theorem 3 is less stringent than that of the synchronous gmp .",
    "when @xmath150 is a hermitian matrix , the condition of expected convergence in theorem 2 can be further simplified as given in the following corollary .",
    "the expected output @xmath179 $ ] converges to the unique point @xmath163 if @xmath150 is a hermitian matrix , and all the eigenvalues of @xmath150 is no greater than @xmath158 .    if @xmath150 is hermitian and all the eigenvalues of @xmath150 is no greater than @xmath158 , the matrix @xmath193 is a positive - semidefinite matrix and can be rewritten as @xmath206 .",
    "as proven by lemma 2 in @xcite , all eigenvalues of @xmath207 , or equivalently @xmath208 , lie in @xmath209 .",
    "substituting @xmath188 into ( [ eqn : condition ] ) , we obtain @xmath210 as all the eigenvalues of @xmath208 lie in @xmath209 , @xmath196 holds all the time .    obviously , the condition that all the eigenvalues of @xmath150 is no greater than one is much weaker than @xmath135 .",
    "it implies that in special cases where @xmath150 is hermitian , @xmath196 is a much less stringent condition than @xmath135 , and thus our rgmp algorithm converges with a higher probability than gaussian message passing .",
    "we now consider the c - ran with two users .",
    "we show that if the gmp algorithm converges , the expected output @xmath211 of rgmp always converges , as proven in the following corollary .",
    "when the number of users @xmath212 , the expected output @xmath211 converges to the unique point if and only if the spectral radius @xmath213 .    from theorem 2 , it suffices to show that @xmath214 .",
    "note that there are only two different permutations when @xmath212 .",
    "that is , @xmath215 . for simplicity of notation ,",
    "let @xmath216 and @xmath217 .",
    "then , @xmath218 .",
    "noting @xmath219 for all @xmath220 , we obtain @xmath221 by using taylor series expansion . similarly , @xmath222 .",
    "then @xmath223    denote by @xmath224 the eigenvalues of @xmath150 .",
    "then , the corresponding eigenvalues of @xmath225 are @xmath226 .",
    "then , the spectral radius of @xmath150 and @xmath225 are @xmath227 and @xmath228 respectively .",
    "thus , @xmath213 always holds if @xmath195 .",
    "therefore , the condition for expected convergence of the rgmp algorithm is less stringent than that of the synchronous message passing .",
    "in this section , we compare the performance of rgmp with other existing algorithms . unless specified otherwise , we assume that both users and rrhs are uniformly at random located in a circular network area with user density @xmath199 and rrh density @xmath200 .",
    "the path loss exponent is @xmath201 , and the average transmit snr at the user side equals to @xmath229db .",
    "that is @xmath230db .",
    "moreover , the stopping criteria is @xmath231 , where @xmath232 is the relative error after the @xmath67-th iteration .",
    "in particular , @xmath233 , with @xmath100 being the estimated transmitted signal after @xmath67 iteration .      in this subsection",
    ", we compare the converge speed of rgmp with other algorithms including admm @xcite , gamp @xcite , and pcg @xcite . for fairness ,",
    "the channel sparsification approach with distance threshold @xmath234 m is adopted in all algorithms . in this way ,",
    "all the algorithms have a linear per - iteration computational complexity with the network size .",
    "thus , we only focus on the convergence speed of these algorithms .    in fig .",
    "[ fig : err_admm ] , the relative error @xmath232 is plotted against the number of iterations for @xmath235 and @xmath236 .",
    "we see that both rgmp and pcg converge very fast . for example",
    ", the relative error of rgmp reduces to @xmath237 within @xmath238 iterations .",
    "however , the performance of the admm algorithm is unsatisfactory . around @xmath239 iterations",
    "are needed for the admm algorithm to reduce the relative error to @xmath240 .",
    "in fact , from simulation results not presented here , admm requires over @xmath241 iterations on average to reduce the relative error to @xmath237 for the network configuration in fig .",
    "[ fig : err_admm ] . therefore , even though the computational complexity per iteration of admm is linear in the number of rrhs and the expected convergence is guaranteed @xcite , it is still impractical to adopt the admm algorithm in c - ran due to the extremely slow convergence .    in fig .",
    "[ fig : rate_size ] , we plot the convergence speed of the rgmp algorithm against the network size , where the convergence speed is measured by the critical number of iterations to achieve @xmath242 . due to the extremely slow convergence speed of admm as shown in fig .",
    "[ fig : err_admm ] , we ignore admm and only plot the convergence speed of pcg and gamp for comparison .",
    "we observe that the number of iterations needed by both pcg and gamp grow roughly linearly with the network size .",
    "in contrast , the convergence speed of rgmp is constant with the network size .",
    "note that the computational complexity per iteration of pcg / gamp / rgmp is linear in the network size .",
    "thus , the total computational complexity of the proposed rgmp algorithm is linear in the network size , while that of pcg and gamp grows quadratically with the network size .      in this subsection",
    ", we compare the performance of the rgmp algorithm with a disjoint clustering algorithm .",
    "the disjoint clustering algorithm divides the whole network into disjoint square clusters with area @xmath243 , and mmse detection is done independently in each disjoint cluster .",
    "channel sparsification is also applied in the disjoint clustering algorithm . in fig .",
    "[ fig : disjoint ] , we plot the mean squared error ( mse ) against the distance threshold , where mse refers to @xmath244 $ ] .",
    "the network area is 200km@xmath245 .",
    "thus , the numbers of rrhs and users are @xmath246 and @xmath247 , respectively .",
    "we see that the gap between the rgmp algorithm and the disjoint clustering algorithm is very large . for example , when the distance threshold @xmath34 is @xmath248 m , the mse of the rgmp algorithm is less than @xmath249 , which is only half of the mse of the disjoint clustering algorithm with cluster area @xmath250km@xmath245 .",
    "in this paper , we proposed the rgmp algorithm for scalable uplink signal detection in c - rans . with channel sparsification , signal detection in a c - ran",
    "was converted to an inference problem over a bipartite random geometric graph .",
    "a random message - update schedule was employed to address the convergence issue of gmp over a bipartite random geometric graph .",
    "we analysed the convergence condition of the proposed rgmp algorithm and showed that the convergence condition of rgmp is much less stringent than that of gmp .",
    "moreover , numerical results demonstrated that rgmp exhibits much faster convergence than existing algorithms , such as pcg , gamp , and admm .",
    "the work in this paper sheds light on the design of message - passing algorithms on general loopy graphs , which has been a challenging topic in the field for years .",
    "future work can be done in a number of interesting directions , including the extension to the downlink processing of c - rans and the design of signal detectors with limited fronthaul capacity .",
    "y.  shi , j.  zhang , b.  odonoghue , and k.  b.  letaief , `` large - scale convex optimization for dense wireless cooperative networks , '' _ ieee trans .",
    "signal process .",
    "18 , pp . 4729 - 4743 , sept",
    ". 2015 .",
    "r.  barrett , m.  berry , t.  chan , j.  demmel , j.  donato , j.  dongarra , v.  eijkhout , r.  pozo , c.  romine , and h.  van  der  vorst .",
    "_ templates for the solution of linear systems : building blocks for iterative methods .",
    "_ philadelphia , pa : siam , 1994        t.  j.  richardson , m.  a.  shokrollahi , and r.  l.  urbanke , `` the capacity of low - density parity - check codes under message - passing decoding , '' _ ieee trans .",
    "inf . theory _ ,",
    "599 - 618 , february 2001 ."
  ],
  "abstract_text": [
    "<S> cloud radio access network ( c - ran ) is a promising architecture for unprecedented capacity enhancement in next - generation wireless networks thanks to the centralization and virtualization of base station processing . </S>",
    "<S> however , centralized signal processing in c - rans involves high computational complexity that quickly becomes unaffordable when the network grows to a huge size . </S>",
    "<S> this paper endeavours to design a _ scalable _ uplink signal detection algorithm , in the sense that the complexity per unit network area remains constant when the network size grows . to this end </S>",
    "<S> , we formulate the signal detection in c - ran as an inference problem over a bipartite random geometric graph . by passing messages among neighboring nodes , message passing ( a.k.a . </S>",
    "<S> belief propagation ) provides an efficient way to solve the inference problem over a sparse graph . </S>",
    "<S> however , the traditional message - passing algorithm is not guaranteed to converge , because the corresponding bipartite random geometric graph is locally dense and contains many short loops . as a major contribution of this paper , </S>",
    "<S> we propose a randomized gaussian message passing ( rgmp ) algorithm to improve the convergence . instead of exchanging messages simultaneously or in a fixed order , we propose to exchange messages asynchronously in a random order . </S>",
    "<S> numerical results show that the proposed rgmp algorithm has significantly better convergence performance than conventional message passing . </S>",
    "<S> the randomness of the message update schedule also simplifies the analysis , and allows the derivation of the convergence conditions for the rgmp algorithm .    </S>",
    "<S> cloud - ran ; signal processing ; message passing ; belief propagation </S>"
  ]
}