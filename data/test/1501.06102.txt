{
  "article_text": [
    "in recent years , there has been rapid growth in research on the human brain , as well as brains of other animals .",
    "the improvement and wide applicability of non - invasive techniques , such as fmri and new high - resolution electron microscopy , are fueling new research on brain physiology , and brain function . in the past ,",
    "advanced research on brain physiology was constrained to domain experts from the medical fields , neuroscience and other researchers with access to limited datasets .",
    "the advent of `` big data '' ecosystems , coupled with new brain research initiatives , large datasets and techniques are becoming more widely available .",
    "this is opening a new set of problems to a wider audience of professionals from computer science , mathematics and engineering .",
    "this paper outlines a framework for developing and deploying image - based analytics on high - resolution electron microscopy of animal brains .",
    "our focus is design and development of a `` full - stack '' prototype for deploying analytics in a hadoop map / reduce ecosystem . by `` full - stack '' ,",
    "we include interfaces and services that implement a pipeline from data download to distributed analytic processing and results output .",
    "this task was challenging in a short time - frame , so much more work is required to produce a mature system that can be used broadly .",
    "this work was completed as part of a graduate course at columbia university , called `` big data analytics '' .",
    "we plan to continue this research in the follow - on course , `` advanced big data analytics '' .",
    "big data analytics is a broad area of research . by its nature",
    ", it encompasses research on large frameworks for processing massive quantities of data .",
    "often the data is disparate and multi - modal .",
    "finally , the goal is state - of - the - art application of cutting - edge analytics in a large framework . in recent years",
    ", there has been tremendous growth in development of breakthrough advanced mathematical and computer science methods for tackling big data .    in computer vision",
    ", areas such as deep learning are advancing the state - of - the - art in significant ways .",
    "also , for years , research and development of probabilistic graphical models , belief propagation and graph analytics have pushed state - of - the - art forward . in particular , in areas of face detection and recognition , probabilistic graphical models have been known to produce state - of - the - art accuracy , when coupled with robust feature extraction , and careful training .",
    "currently , several centers are researching and developing new technologies that address brain understanding problems .",
    "new insight is being gained by collection massive quantities of high resolution imagery , and advanced analytic processing in large frameworks .",
    "for this project , we were fortunate to leverage expertise from the open connectome project .",
    "see @xcite , @xcite and @xcite for further details on this project .    there is a proliferation of big data frameworks , distributed processing and storage architectures , as well as stream processing environments .",
    "prior to undertaking analytic research and development , we surveyed several publicly - available distributed processing frameworks .",
    "we wanted to make early decisions on which programming languages and libraries to focus on .",
    "below we provide a list of some of the frameworks that were considered for this project .",
    "we do not provide details on each framework here .",
    "here is a list of publicly available distributed processing frameworks that were considered for this final project :    1 .",
    "hadoop map / reduce 2 .",
    "apache spark 3 .",
    "apache ( twitter ) storm 4 .   yahoo !",
    "ibm infosphere streams 6 .",
    "mit csail streamit 7 .",
    "zeroc ice 8 .",
    "fastflow ( with zeromq ) 9 .",
    "muesli 10 .",
    "rayplatform 12 . caf ( c++ actor framework ) 13 .",
    "flowvr 14 .",
    "openmp & mpi 15 .",
    "intel tbb 16 .",
    "apache ( facebook ) thrift",
    "http://cloud.ganita.org/images/beddiagram.png [ ]    developing all the components from figure [ sys_diag ] in a short time frame is very challenging .",
    "we divided research into the following categories :    1 .",
    "distributed download service 2 .",
    "image handling service 3 .   on - demand web service 4 .",
    "feature extraction service 5 .",
    "feature matching service 6 .",
    "graph reconstruction    one or more components have been developed in each of the six categories above . at this time , a complete pipeline is not fully constructed .",
    "we stood - up a cluster of commodity computers for the purpose of this course .",
    "we continue to use this cluster for development , testing and giving demos of our services .",
    "our system is called cloud ganita , and the following section provides more detail . following that",
    ", we describe data used for this project and then outline various components developed for the course project : brain edge detection .",
    "all installations , maintenance and system administration were performed on a small cluster , stood - up for this project .",
    "the table below shows the role of individual nodes in the cloud ganita cluster .",
    ".1 in     nn = namenode , rm = resourcemanager , dn = datanode , nm = nodemanager , jhs = jobhistoryserver , zk = zookeeper , hm = hbase master , hbm = hbase backup master , rs = hbase regionserver , qp = zk quorumpeer , hrs = hbase restserver    figure [ cloud - ganita1 ] shows a web interface created for this project . from the web interface , it is possible to view namenode , resource manager and job history on the hadoop cluster .",
    "it is currently password protected .",
    "log in - username : * ganita * and password : * cloud*.    .1 in    http://cloud.ganita.org [ ]      all of the data used for this project was made available by the open connectome project ( ocp ) .",
    "we focused on high - resolution electron microscopy collected by bobby kasthuri ( bu & harvard ) .",
    "it is known as the kasthuri11 dataset .",
    "here is a description of this data :    * ocp data is available via a restful interface * 3x3x30 cubic nanometer spatial resolution * 660 gb of images * up to 1,000,000,000,000 pixels * focused on 10752 x 13312 x 1850 resolution * @xmath0 265 billion pixels * info @xmath1 http://openconnecto.me/ocp/ca/kasthuri11/info/      the distributed download service has a ui and allows a user to carve out hdf5 voxels for download and indexing into hdfs .",
    "the ui is built using html5 , javascript and php",
    ". the php commands partition the em blocks into a manageable number of slices and load a configuration file into hdfs .",
    "then , a php command launches a hadoop map / reduce job that feeds lines from the configuration file to a ruby script .",
    "the ruby script uses curl on each configuration line to start the download from any one of the datanodes in our cluster .",
    "the image handling service allows conversion of hdf5 image data into any of the following formats :    * uncompressed imagery ( pnm / ppm / pgm ) * png compressed images * jpeg compressed images * ogg / theora compressed video * mp4/h.264 compressed video    we used ffmpeg along with the plug - in codecs : libogg , libtheora , libx264 , and the standard codecs .",
    "the on - demand web service is the user interface for automatically downloading voxels from ocp .",
    "the feature extraction service includes two main components .",
    "the first is a newly built c / c++ library for extracting gradient features from em data .",
    "the second is integration of opencv into the cloud environment and application of standard opencv feature extractors to the converted imagery .",
    "opencv is the prevailing open package for deploying and running computer vision algorithms on image or video .",
    "the feature matching service is still in development .",
    "it will include feature matching from opencv , along with newly created matchers in a c / c++ library .",
    "we may also continue to explore feature extraction and matching in matlab .",
    "it enables quick development of new advanced algorithms .",
    "also , ocp has built a new powerful api ( called cajal3d ) that will work directly with the ocp restful interface .    for graph reconstruction , we have begun development of a new graph processing engine in c / c++ .",
    "this engine will be tailored to the em brain data .",
    "plans are to integrate this engine with other existing graphical capabilities such as system g , libdai , etc .",
    "several tools and algorithms have been researched and developed for the purpose of processing electron microscopy . only recently ,",
    "have massive volumes of high - res em scans of brains been available .",
    "typically , in the medical imaging fields , focus has been on lower resolution 2d imagery .",
    "recently , the open connectome project has built a big data framework ( using sun grid engine ) for deploying computer vision algorithms in a distributed manner .",
    "also , created by ocp is a matlab api for rapidly developing and testing new algorithms in this framework .",
    "we found that advanced and robust computer vision algorithms are developed in matlab , c / c++ , or in cuda .",
    "we have made an early decision to focus on three main languages ( or development environments ) for creating new algorithms . with our eye on future research to include development of deep learning algorithms , probabilistic graphical models and dynamical systems approaches , we will focus on a few different deploy - and - test environments .",
    "1 .   matlab @xmath1 ocp sun grid engine 2 .",
    "c / c++ @xmath1 hadoop streaming & infosphere streams 3 .",
    "cuda @xmath1 gpu & caffe    for this paper , we focus on research and development of c / c++ algorithms and deployment of some of these algorithms in a hadoop map / reduce framework .",
    "here is a list of the main algorithms developed for this effort :    1 .",
    "hadoop streaming hdf5 download service 2 .   image handling service ( data conversion ) 3 .",
    "feature building ( c++ ) 4 .   graph analytic starter engine ( c++ ) 5 .",
    "web interface for project and basic service ( html5/js )    also , we started testing of basic computer vision feature extractors from opencv on the em data .",
    "however , since these algorithms are designed for 2-dimensional data ( images or motion frames ) , we have started development of a new c / c++ library for integration of 3-dimensional volumetric processing of em data . initially , we focused on research and development of 3d gradient feature extractors .",
    "we developed a 3d sobel operator , along with modified versions for tuning on high - res em volumes .",
    "we developed a standard 3d sobel operator for application on em voxels .",
    "we used the following kernel : @xmath2    @xmath3    @xmath4    below are two images : the first shows an original em slice , and the second shows the output of the 3d sobel operator .",
    "we also added the norm as a parameter and tested a modified sobel operator with @xmath5 replaced by @xmath6 for various @xmath7 values .",
    "finally , we added the ability to binarize the output of the 3d sobel operator using mean and standard deviation - based thresholds .",
    "notice clear remnants of closed curves , many with circular eccentricity .",
    "we are working on an algorithm to integrate around points on the closed curves and output a single stable center point .",
    "our main goal is to detect and extract feature patches for registering regions of the connectome .",
    "we develop and energy function that can be optimized that outputs many of these center points . from these",
    ", a sparse graph can be constructed , and then further dense modeling of em blocks and connectome regions .",
    "we will extract a feature vector based on cell regions , neurons , axons and synapses .",
    "we first build a graphical model based on statistically stable regions learned from experiments on large volumes of em data .",
    "the ganita feature extraction module consists of two main c++ classes : ganita3d and ganitaimage .",
    "ganitaimage handles reading , writing of images , videos and interfacing with external image and video libraries .",
    "ganita3d implements tailored 3d feature extractors such as the 3d sobel filter .      we have developed a basic graph processing engine in c++ .",
    "our goal is to use this engine to further analyze em data and integrate with existing probabilistic graphical frameworks .",
    "currently , our plans are to integrate with libdai which is a free and open source c++ library for discrete approximate inference in graphical models .",
    "to store a graph , several data structures are created .",
    "the goal is to avoid allocating @xmath8 locations in memory , since this may be very large .",
    "we create an array that stores the index of each vertex , and an array that stores the offset into the edge array for each vertex .",
    "then all edges are packed into a single array of longs in numeric ( vertex order , adjacency vertex order ) .",
    "total number of memory allocations equals : @xmath9    with each node and edge , a property can be stored . for image processing",
    ", the property may contain features from a region of the em block data .",
    "as an example , the graph library can compute the dot product between two nodes ( as the sum of common adjacent vertices ) .",
    "this can be computed combinatorically efficiently by scanning through the adjacent vertices in unison , while incrementing .",
    "note , a standard method for computing the dot product would be to store the edges in an adjacency matrix and compute the dot product of two binary vectors .",
    "we do not wish to allocate this much memory .",
    "the ganita graph software is composed of the following c++ classes : @xmath10",
    "a new project was created in github that contains some of the source code used for this project .",
    "the project name under github is * ganitabrain*. the software is structured as shown in figure [ software - fig ] .",
    "[ software - fig ] = [ thick , anchor = west , rounded corners , font= , inner sep=2.5pt ] = [ draw = blue , fill = blue!10 ] = [ selected , fill = blue!30 ]    child node [ selected ] build child node at ( 0,-.25 ) [ selected ] doc child node [ selected ] at ( 0,-.5 ) src child node build.sh child node [ selected ] at ( 0,-.2 ) feat child node build.sh child node c++ child node thirdparty child node at ( .4,-3.2 ) [ selected ] child node [ selected ] graph child node build.sh child node c++ child node at ( .4,-4.5 ) [ selected ] child node [ selected ] prep child node build.sh child node c child node cpp child node java child node python child node ruby child node script child node thirdparty child node at ( .4,-8.6 ) [ selected ] child node [ selected ] service child node [ selected ] at ( 0,-.2 ) em - to - video child node dashboard - g child node at ( 0,-.2 ) php ;    the software in prep is used to prepare the em data for analytic processing .",
    "this includes distributed download from the open connectome project and conversion to various formats .",
    "the directory feat contains the gradient feature extractor , including a 3-dimensional modified sobel operator . also , included in the same class is a binarization routine .",
    "the root directory contains three subdirectories : build , doc and src . to build the software , a user must first install all dependencies .",
    "there is a actually a long list of dependencies , but the most important ones are : libcurl , h5dump , hdf5 libraries and executables , ffmpeg , opencv .",
    "the c++ software in feat and graph can be built without installing most dependencies . to build executables and jar files ,",
    "a user should change directory to the root directory and enter the command : @xmath11    .1 in the following figure shows the download of ganitabrain from github and the successful build process .",
    "several experiments were run using the software developed for this effort .",
    "below is a screenshot from cloud ganita jobs that perform a distributed download , ingest and indexing of hdf5 imagery .",
    ".1 in    below are screenshots of cloud ganita namenode and datanodes .",
    "following that is a screenshot of the hdfs directory showing em data stored in 3 different formats : hdf5 , sequencefile and uncompressed pgm text files .",
    ".1 in        at this time , we have various analytics running in hadoop / hdfs .",
    "we have not yet collected ground - truthed data to test results from our query - by - example framework .",
    "this paper focuses on research to identify components for the big data framework , and development to deploy and integrate many of these components .",
    "future research will focus on establishing a pipeline for processing data in an on - demand fashion and comparing results to ground - truth .",
    "an interesting task is identifying synapses in the kasthuri11 dataset .",
    "for future directions , this author plans to focus on application of 3d feature extraction , labeling and training of classification using deep learning framework ( i.e. caffe ) , and application of probabilistic graphical models .",
    "focus will be on environments well suited c / c++ algorithm development .",
    "further utilization of matlab will take place .",
    "the author acknowledges assistance from several members of the open connectome project , including will gray roncal , jacob vogelstein and mark chevillet ( johns hopkins university ) .",
    "the author did this research while a student in columbia university s big data analytics course .",
    "the author thanks teacher , ching - yung lin , for several insightful comments that will guide future research .",
    "yamamoto , m. and kaneko , k. , parallel image database processing with mapreduce and performance evaluation in pseudo distributed mode , _ international journal of electronic commerce studies _ , * 3:2 * ( 2012 ) ."
  ],
  "abstract_text": [
    "<S> this paper outlines research and development of a new hadoop - based architecture for distributed processing and analysis of electron microscopy of brains . </S>",
    "<S> we show development of a new c++ library for implementation of 3d image analysis techniques , and deployment in a distributed map / reduce framework . </S>",
    "<S> we demonstrate our new framework on a subset of the kasthuri11 dataset from the open connectome project .    </S>",
    "<S> brain ; electron microscopy ; connectome ; hadoop ; map / reduce ; service ; </S>"
  ]
}