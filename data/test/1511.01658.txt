{
  "article_text": [
    "ordinary differential equation models are widely used in engineering , physics and life sciences .",
    "as the parameters of the underlying processes are often unknown , they have to be determined from experimental data  @xcite .",
    "this requires efficient and robust parameter optimisation methods .",
    "unfortunately , for a wide range of models these efficiency and robustness requirements are not met by available methods ( see discussion  @xcite ) . in particular",
    "parameter estimation and optimisation problems with nonlinear equality constraints , e.g. , steady state constraints , are intricate .    in the following",
    "we will consider ode models @xmath0 with state variables @xmath1 and parameters @xmath2 .",
    "the vector field @xmath3 is assumed to be lipschitz continuous to ensure existence and uniqueness of the solution .",
    "the parameter dependent steady states @xmath4 of such systems are constrained by @xmath5 this defines a nonlinear manifold .",
    "as many systems operate in steady state and/or undergo fast equilibration , constrained optimisation problems @xmath6 with objective function @xmath7 , are widely studied .",
    "such constrained optimisation problems arise , e.g. , in systems biology when model parameters are estimated from steady state data and in engineering when the steady state of a process is designed .",
    "a variety of methods has been developed for optimisation under nonlinear equality constraints  .",
    "in particular local and manifold optimisation are used in practice  @xcite .",
    "standard local optimisers have to move on the nonlinear manifold defined by the steady state constraint ( figure  [ fig : concept figure ] ) .",
    "these methods become quickly inefficient and/or have convergence problems  @xcite .",
    "improvements are achieved by reformulating the constrained to an unconstrained optimisation problem using lagrangian multipliers .",
    "however , this increases the problem size .",
    "algorithms for manifold optimisation generally circumvent an increase of the problem size and moves on the manifold .",
    "instead retraction operators are employed  @xcite , projecting a point onto the manifold .",
    "retraction operators which can be efficiently evaluated are unfortunately only available for a few classes of manifolds . for vector fields",
    "@xmath8 which are non - linear , there are generally no analytical functions enabling the projections , limiting the application of established algorithms for optimisation on manifolds .    in the following , we will propose a simulation - based framework for numerical optimisation with steady state constraints .",
    "using ideas from continuous analogues of optimisers  @xcite and systems theory  @xcite , we will introduce an ode system exploiting the local geometry of the steady state manifold to move on the manifold .",
    "the manifold will be stabilised using a continuous retraction , available because we are interested in stable steady states of the model  .",
    "the simulation of the resulting ode system with sophisticated adaptive step - size solvers yields an improved optimisation performance .",
    "we provide a proof for exponential stability of a locally optimal point for a locally strictly convex objective function @xmath9 and a locally exponentially stable system .",
    "to evaluate the developed optimisation method , two application examples from the field of systems biology are studied .     to a local minimum @xmath10 on the steady state manifold ( * ",
    "* ) is depicted . ]",
    "to develop a tailored method for solving optimisation problems with steady state constraints , we will exploit the first order geometry of the manifold of steady states . to this end",
    "we consider the sensitivities of the states @xmath11 with respect to the parameters @xmath12 , @xmath13 whose dynamics are governed by the forward sensitivity equation @xmath14 in matrix form , we obtain @xmath15 with @xmath16 .    in a steady state @xmath4 corresponding to some parameter @xmath12 , the forward sensitivity equation   simplifies to @xmath17 evaluated at @xmath18 .",
    "if the considered steady state @xmath4 is locally exponentially stable , the jacobian @xmath19 is negative definite and invertible .",
    "accordingly , @xmath20 furthermore , this implies that there is a local parametrization of the steady state , @xmath4 such that @xmath18 is an isolated root of .",
    "the sensitivity of the steady state with respect to the parameters , @xmath21 , can be used in the taylor series of @xmath4 , @xmath22 with @xmath23 . the perturbation direction and the step size",
    "are denoted by @xmath24 and @xmath25 , respectively .",
    "the taylor expansion   reveals how the steady state changes locally with the parameters . by reformulating   and letting @xmath26",
    ", we obtain a dynamical system which evolves on @xmath4 , @xmath27 given an update direction @xmath24 and a length @xmath25 ,   provides the steady state @xmath28 up to the accuracy of the used ode solver .",
    "hence ,   enables moves on the steady state manifold , similar to results in  @xcite .",
    "a recalculation of the steady state for changed parameter values or a retraction operator are not longer required .",
    "in this section , we exploit the insight into the local geometry of the steady state manifold to derive an ode model converging to a locally optimal solution of the constrained optimisation problem  .      in local optimisation",
    ", the parameters @xmath12 are updated using the gradient of the objective function .",
    "the method of gradient descent , for instance , exploits an update @xmath29 with step length @xmath25 .",
    "the continuous analogue of gradient decent methods is @xmath30  @xcite .",
    "this ode system can be coupled with the dynamical system evolving on the steady state manifold  , using @xmath31 .",
    "we obtain the ode system @xmath32 with the steady state sensitivity @xmath21 .",
    "initialisation of   in a point @xmath33 fulfilling   yields a trajectory evolving on the steady state manifold , along which the objective function decreases .",
    "the formulation   bears two disadvantages : ( i )  an appropriate initial point @xmath33 has to be determined by solving  ; and ( ii )  numerical errors can result in a divergence from the steady state manifold . to address these problems , we introduce the term @xmath34 which locally retracts the state of the system to the manifold by exploiting the stability properties of the steady state .",
    "this yields the system @xmath35 for this modified system we do not require that the initial point @xmath33 fulfils the steady state conditions  , hence , the jacobian @xmath36 might not be invertible . to address this",
    ", we define @xmath37 in which @xmath38 denotes the moore ",
    "penrose pseudoinverse of @xmath36 . on the steady state manifold ,",
    "the jacobian is invertible and we recover the standard steady state sensitivity . for large retraction factor @xmath39 ,",
    "the state @xmath40 is retracted quickly to the steady state manifold @xmath18 .      in the following ,",
    "we assess the local stability and convergence of   to a local optimum @xmath10 under four conditions .",
    "[ assumption : functional relationship ] there exists an locally isolated root @xmath4 of the equation @xmath41 for all @xmath42 .",
    "this root is a steady state of the system  .",
    "[ assumption : exponential stability ] the steady state @xmath4 is locally exponentially stable uniformly in @xmath12 , with @xmath43 .",
    "accordingly , @xmath44 such that @xmath45 , @xmath46 and @xmath47 .",
    "this implies that @xmath48 such that @xmath49  @xcite .",
    "[ assumption : identifiability ] there exists a neighbourhood around @xmath50 , @xmath43 , in which the objective function evaluated on the steady state manifold @xmath4 , @xmath51 , is locally strictly convex in @xmath12 .",
    "this implies that the parameters @xmath12 are locally structurally identifiable and that solutions to @xmath52 converge to the optimal point @xmath50 for initial points @xmath53 .",
    "[ assumption : smoothness ] the functions @xmath54 , @xmath55 and @xmath4 and their partial derivatives up to order 2 are bounded for @xmath56 and @xmath57 .    using these assumptions we find :    [ theorem : local convergence ] let assumptions  [ assumption : functional relationship ] - [ assumption : smoothness ] be satisfied .",
    "then there exists a @xmath58 such that for all @xmath59 a local minimum @xmath60 of the optimisation problem   is a locally exponentially stable steady state of the system  .    to prove theorem  [ theorem : local convergence ] we use perturbation theory @xcite .",
    "we define @xmath61 and shift the optimum to the origin using the linear state transformation , @xmath62 this yields the singular perturbed system @xmath63 with @xmath64 , @xmath8 and @xmath65 evaluated at @xmath66 .",
    "the system   captures the dynamics of the deviances from the optimal parameter , @xmath67 , and the deviances from the steady state for the optimal parameter , @xmath68 .",
    "furthermore , it possesses the following properties :    * @xmath69 and @xmath70 as the objective function gradient vanishes , @xmath71 and as the steady state condition is fulfilled , @xmath72 .",
    "both follows from optimality of @xmath50 and assumption  [ assumption : functional relationship ] . *",
    "the equation @xmath73 has the isolated root @xmath74 ( assumption  [ assumption : functional relationship ] ) with @xmath75 . *",
    "the functions @xmath76 , @xmath77 , and @xmath78 and their partial derivatives up to order  2 are bounded for @xmath79 ( assumption  [ assumption : smoothness ] ) .",
    "* the origin of the reduced systems @xmath80 obtained for @xmath81 is locally exponentially stable , with @xmath82 from ( ii ) .",
    "this follows from strict local convexity of the objective at @xmath50 ( assumption  [ assumption : identifiability ] ) .",
    "* the boundary - layer system is derived from   using the transformation @xmath83 , yielding @xmath84 with @xmath64 , @xmath8 and @xmath65 evaluated at @xmath85 and @xmath21 evaluated at @xmath86 .",
    "after rescaling of the simulation time , @xmath87 , and setting @xmath81 , we obtain the boundary - layer system @xmath88 the origin of this boundary - layer system is locally exponentially stable , uniformly in @xmath67 as the steady state @xmath4 of   is exponentially stable uniformly in @xmath89 ( assumption  [ assumption : exponential stability ] ) .",
    "the properties ( i)-(v ) are the prerequisites of  ( * ? ? ?",
    "* theorem 9.3 ) , establishing the existence of @xmath90 such that for all @xmath91 systems of type   are locally exponentially stable . as stability properties",
    "are conserved under the performed transformations , we obtain for @xmath61 the theorem  [ theorem : local convergence ] .",
    "theorem   establishes local exponential stability of local optima @xmath10 for appropriate choice of @xmath92 , assuming a convex objective function and a well - behaved system . from this",
    "follows also local convergence .",
    "stability and convergence are not affected by the approximation of the steady state sensitivity via @xmath65 .      in practice often steady state data for different inputs @xmath93 are available , yielding constraints @xmath94 for @xmath95 .",
    "the simulation - based method and the stability proof presented in sections  [ sec : method ] and  [ sec : stability ] can be generalised to this case .",
    "we simply concatenate constraints and state vectors , @xmath96 with @xmath97 , and apply the method to the concatenated system . due to the block structure",
    "some calculations , e.g. , matrix inversion , can be simplified .",
    "+ in the following , we illustrate the behaviour of the simulation - based optimisation method .",
    "furthermore , the performance of the proposed approach will be compared to standard constrained and unconstrained optimisation methods . for this purpose ,",
    "we consider two simulation examples for which the ground truth is known .",
    "in this section , we study parameter estimation for conversion reactions from steady state data .",
    "conversion reactions are among the most common motifs in biological systems , therefore particular interesting , and provide a simple test case .",
    "we consider the conversion reaction @xmath98 with parameters @xmath99 .",
    "it is well - known that the concentration of the biochemical species @xmath100 is governed by @xmath101 with @xmath102 denoting the initial concentration of @xmath100 and @xmath103 denoting the sum of the initial concentrations of @xmath100 and @xmath104 .",
    "the steady state of model   is @xmath105      to illustrate the properties of the simulation - based method , we consider a simple parameter estimation problem .",
    "we assume that for @xmath106 , the steady state @xmath107 is observed .",
    "furthermore , for the parameters @xmath108 , @xmath109 , prior knowledge suggests the value @xmath110 and @xmath111 .",
    "accordingly , we estimate the parameters using the weighted least squares optimisation problem @xmath112      to formulate the ode model solving  , the gradient of the objective function with respect to the parameters @xmath113 and the local sensitivities @xmath114 are derived .",
    "these components are substituted into  , yielding the system @xmath115 with initial conditions @xmath116 , @xmath117 and @xmath118 .",
    "it can be verified that the objective function @xmath119 is locally strictly convex in @xmath12  the parameters are locally identifiable  and that the model   is asymptotically stable .",
    "accordingly , system   converges to a local optimum of the constrained optimisation problem   ( theorem  [ theorem : local convergence ] ) . as stopping criterion for the simulation - based optimisation we use @xmath120 with @xmath121",
    "additionally , a maximal number of function evaluations is implemented .      to illustrate the simulation - based optimisation method we simulate system   using the matlab ode solver ` ode15s ` with default settings .",
    "exemplary trajectories are depicted in figure  [ fig : cr euler ] .",
    "we find that for retraction factors @xmath122 , the states @xmath123 converge to the optimal solution .",
    "as retraction renders the steady state manifold   attractive , also for initial conditions @xmath124 which do not fulfil the steady state condition , fast convergence to the steady state manifold can be achieved using @xmath125 . for large retraction ( @xmath125 ) , the dynamic consists of two phases : ( phase  1 ) the state @xmath11 converges quickly to the parameter - dependent steady state  ; and ( phase  2 ) the state @xmath123 moves along the steady state manifold to a local optimum .",
    "to challenge the proposed simulation - based optimisation method , we consider in the following a larger and more realistic problem : fitting of dose response data for ngf - induced erk activation .",
    "ngf is known to induce a strong pain sensitisation during inflammation via the activation of erk . as",
    "this is clinically relevant , a simple model for ngf - induced erk activation has been developed in  @xcite , @xmath126 in which @xmath127 denotes the activity of the ngf receptor trka , @xmath128 denotes the activity of erk , and @xmath129 denotes the concentration of ngf .",
    "the parameters @xmath130 are logarithms of effective rate parameters .",
    "the steady state of model   is @xmath131 for details on the model , we refer to the original publication  @xcite .      the parameters @xmath12 are unknown and have to be estimated from experimental data . in practice ,",
    "the activity of the ngf receptor trka ( @xmath127 ) can not be assessed and merely data for the erk activity level ( @xmath128 ) are available .",
    "these data can be time resolved , but mostly dose response curves for the steady state are recorded .    in the following ,",
    "we assume that for ten different ngf concentrations ( @xmath93 , @xmath132 ) the stationary erk activity ( @xmath133 , @xmath132 ) has been measured .",
    "therefore , we evaluate the steady state   for these different inputs @xmath134 . using a standard least squares objective function , we obtain the constrained optimisation problem @xmath135 & \\hspace*{30 mm } \\quad i = 1,\\ldots,10",
    ". \\end{aligned } \\label{eq : ne opt prob}\\ ] ] this constrained optimisation problem has @xmath136 optimisation variables .",
    "as an analytical expression for the steady state   is available the constrained optimisation problem   can be reduced to an unconstrained optimisation problem by substituting the expression for the parameter dependent steady state   into the objective function @xmath137 , yielding @xmath138 this unconstrained optimisation problem has @xmath139 optimisation variables , and the size is therefore independent of the number of ngf concentrations .",
    "as before , a dynamical system converging to the locally optimal points of the constrained optimisation problem   can be formulated . to this end",
    "the objective function gradient , @xmath140 , and the local sensitivities , @xmath141 , are derived .",
    "this can be done manually , but for high - dimensional problems symbolic math toolboxes should be used .",
    "the results are inserted in   to yield a problem specific system .      in the following ,",
    "we compare our simulation - based optimisation method with two state - of - the - art implementations of standard methods :    * constrained optimisation   using the algorithm ` fmincon ` for constrained nonlinear optimisation ; and * unconstrained optimisation   using the algorithm ` fminunc ` for unconstrained nonlinear optimisation .",
    "constrained optimisation using ` fmincon ` ( or a similar algorithm ) is the nive approach .",
    "this approach does neither require an analytical formula for the steady state nor does it exploit the structure of the problem .",
    "unconstrained optimisation   on the other hand requires an analytical solution of the steady state . using this analytical solution",
    "the problem is simplified significantly .",
    "accordingly , we expect that unconstrained optimisation will provide a very good performance , its applicability is however limited to simple models for which an analytical expression for the steady state is known . for the evaluation , `",
    "fmincon ` is supplied with the objective function value and the values of the constraint , as well as the respective analytical derivatives . for ` fminunc ` only the objective function and its gradient are implemented . both algorithms use default settings .",
    "the ode system obtained for the simulation - based method is simulated using the matlab solver ` ode15s ` with default settings .",
    "artificial data for the comparison are generated by calculating the analytical steady state   for inputs @xmath142 and adding normally distributed noise with zero mean and variance of 0.01 .",
    "we used constrained optimisation , unconstrained optimisation as well as the proposed simulation - based method to estimate the parameters . for all methods",
    "the initial parameter estimates were sampled from a uniform distribution , @xmath143 .",
    "in addition to initial parameters , constrained optimisation and simulation - based methods require initial estimates for the steady states .",
    "they are also sampled from a uniform distribution , @xmath144 for @xmath145 and @xmath132 . to assess the convergence properties and the computation time",
    ", all aforementioned methods were initialised with 100 sampled starting point .",
    "the final objective function values as well as the respective computation time is depicted in figure  [ fig : ne comp obj ] .",
    "we find  as expected  that the unconstrained optimisation method is on average computationally most efficient and for almost all runs a same final objective function value is achieved ( which is probably the global optimum ) .",
    "the simulation - based optimisation methods with retraction factor @xmath146 and @xmath147 possess similar convergences properties ( figure  [ fig : ne comp obj]a ) , the computation time is however roughly 10-times higher .",
    "the standard constrained optimisation method implemented by ` fmincon ` possesses a significantly worse convergence rate of roughly 28% .",
    "the computation time for ` fmincon ` varies over several orders of maginitude and the average is comparable to the computation time for the simulation - based optimisation methods ( figure  [ fig : ne comp obj]b ) .",
    "however , the mean computation time per converged start is roughly 7 to 8 times higher for ` fmincon ` in comparison to the simulation based method with retraction factor @xmath147 ( ` fmincon ` : 7.56 seconds per converged start ; simulation based with @xmath147 : 0.98 seconds per converged start ) . in general it is difficult to ensure that the objective function is locally strictly convex ",
    "a prerequisite for theorem  [ theorem : local convergence ]  prior to the optimisation . also for this example",
    ", this was not possible , and indeed it is also not the case .",
    "the optimisation results reveal that the parameters are practically non - identifiable and that there is a continuum of parameters which yields the same optimal objective function value . as the simulation - based methods worked fine despite the lack of identifiability , it is well suited for practical applications and we expect that theorem  [ theorem : local convergence ] can be extended .     and @xmath147 are depicted . ]    in summary , for this estimation problem , the simulation - based method outperforms standard constrained optimisation methods and achieves a performance only sightly worse than unconstrained optimisation methods .",
    "furthermore , simulations - based methods seem to be robust with respect to non - identifiability . while we use artificial data to evaluate our method , similar experiment data are available ( see  @xcite ) .",
    "this renders also this application example important for practice .",
    "optimisation problems with steady state constraints are often challenging .",
    "if an analytical expression for the parameter and input dependent steady state , @xmath148 is not available , the vector of optimisation variables contains the unknown parameters as well as the corresponding steady states .",
    "accordingly , optimisers have to evolve on the non - linear manifold defining the set of steady states . in this manuscript",
    ", we propose a continuous analogue of a gradient descent algorithm for manifold optimisation .",
    "this simulation - based method exploits the local geometry of the steady state manifold for optimisation .",
    "the local asymptotic stability of the steady state is used to render the steady state manifold attractive .",
    "we establish a result for local convergence using perturbation theory .",
    "the proposed simulation - based optimisation method is evaluated using two models for biological processes .",
    "we find that the simulation - based optimisation method possesses improved convergence properties in comparison to standard constrained optimisation methods implemented in the matlab routine ` fmincon ` .",
    "furthermore , the simulation - based optimisation method yields convergence properties almost comparable to those of unconstrained optimisation methods exploiting an analytical expression for the steady state .",
    "the proposed simulation - based optimisation method is however applicable to a broader class of problems as an analytical expression for the steady state is not necessary .",
    "an open question is how the proposed method behaves in the presence of practical and structural non - identifiability .",
    "preliminary results suggest that the method always provides a point on the non - identifiable subspace but this has to be studied in more detail . as in traditional optimisation , also using alternative update schemes , e.g. newton - type instead of gradient descent - based parameter updates , might yield even more convincing convergence results . as shown in the proof ,",
    "the retraction factor @xmath92 has to be chosen large to ensure convergence .",
    "howerver , a large @xmath92 will render the simulation - based system stiff .",
    "an intelligent choice of @xmath92 is therefore necessary .",
    "furthermore , extension towards a combination of steady state and time series data might be interesting .",
    "the simulations - based approach can in principle be generalised to other model classes , including partial differential equations .",
    "a.  raue , m.  schilling , j.  bachmann , a.  matteson , m.  schelke , d.  kaschek , s.  hug , c.  kreutz , b.  d. harms , f.  j. theis , u.  klingmller , and j.  timmer , `` lessons learned from quantitative dynamical modeling in systems biology , '' _ plos one _ , vol .  8 , no .  9 , e74335 , sept .",
    "2013 .",
    "drr and c.  ebenbauer , `` a smooth vector field for saddle point problems , '' in _ proceedings of the 50th conference on decision and control ( cdc 2011 ) _ , orlando , florida , usa , dec .",
    "2011 , pp . 46544660 .",
    " , `` global analysis of continuous analogues of the levenberg - marquardt and newton - raphson methods for solving nonlinear equations , '' _ ann .",
    "_ , vol .",
    "part b , pp . 189203 , 1985 .",
    "s.  hug , a.  raue , j.  hasenauer , j.  bachmann , u.  klingmller , j.  timmer , and f.  j. theis , `` high - dimensional bayesian parameter estimation : case study for a model of jak2/stat5 signaling , '' _ mathematical biosciences _ ,",
    "246 , no .  2 ,",
    "293304 , nov .",
    "j.  hasenauer , c.  hasenauer , t.  hucho , and f.  j. theis , `` ode constrained mixture modelling : a method for unraveling subpopulation structures and dynamics , '' _ plos comput .",
    "_ , vol .  10 , no .  7 , p. e1003686 , july 2014"
  ],
  "abstract_text": [
    "<S> ordinary differential equations ( odes ) are widely used to model biological , ( bio-)chemical and technical processes . </S>",
    "<S> the parameters of these odes are often estimated from experimental data using ode - constrained optimisation . </S>",
    "<S> this article proposes a simple simulation - based approach for solving optimisation problems with steady state constraints relying on an ode . </S>",
    "<S> this simulation - based optimisation method is tailored to the problem structure and exploits the local geometry of the steady state manifold and its stability properties . </S>",
    "<S> a parameterisation of the steady state manifold is not required . </S>",
    "<S> we prove local convergence of the method for locally strictly convex objective functions . </S>",
    "<S> efficiency and reliability of the proposed method are demonstrated in two examples . </S>",
    "<S> the proposed method demonstrated better convergence properties than existing general purpose methods and a significantly higher number of converged starts per time . </S>"
  ]
}