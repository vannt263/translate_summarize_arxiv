{
  "article_text": [
    "traditional practice in astronomy is to take images of the sky , detect or enumerate sources visible in those images , and create catalogs .",
    "these catalogs are then used to perform fundamental astronomical measurements , for example reconstructing the three - dimensional structure of the galaxy or the two - point correlation function of galaxies .",
    "indeed , the process of catalog construction is so `` baked in '' to our ideas about what astronomy is , we sometimes forget that the catalog is _ not _ the fundamental data product of astronomy ; catalogs are produced from imaging ; their production involves many decisions and ideas that go beyond the information provided to the telescope by the incident intensity field .",
    "in addition , catalogs are not usually the final goal of any imaging project or survey .",
    "typically , they are produced in order to facilitate the scientific study of populations of objects ( e.g. the initial mass function of a population of stars ) , or to provide a sky - search capability to the community who might be interested in only a small subset of objects .",
    "standard tools for generating catalogs from astronomical imaging include sextractor  @xcite , daophot @xcite , dolphot @xcite , and sdss photo @xcite .",
    "telescopes do nt make catalogs @xcite , they measure the intensity field .",
    "viewed through the lens of probabilistic inference , the goals of astronomy are to take the information in the telescope - generated records of the intensity field and use this information to obtain quantities of astronomical interest with as little loss as possible .",
    "insertion of a catalog - generation step in the inference pipeline between the raw imaging and the final astrophysical analyses is potentially lossy .",
    "the hard decisions of catalog making destroy information , at least in principle .",
    "probability theory suggests that it may be less lossy to pass forward not a catalog but a probabilisitic description of all the catalogs that could be consistent with the imaging  a posterior probability distribution in the ( enormously large ) space of possible catalogs .",
    "essentially , the creation of a catalog is an attempt to answer the question , `` given the image we have obtained , what objects are present in the field and what are their properties ? '' .",
    "this article represents an attempt at implementing this ambitous goal in the specific situation where the only objects in the field are stars or other point sources .    beyond these philosophical concerns ,",
    "there are practical issues ; standard methods for constructing catalogs can have difficulty in some challenging situations .",
    "for example , when multiple sources overlap partially or completely , it can be difficult to determine how many sources are present , and how much flux belongs to each source . in principle , the uncertainty about the existence and properties of the objects can be significant and should be propagated into any inferences about the stellar population . a bayesian approach that obtains the posterior distribution over catalog space ( rather than a single catalog estimate ) has the potential to overcome these problems by deblending objects when it is possible , and clearly indicating the uncertainty remaining when it is not possible .    in practice ,",
    "bayesian inferences are often implemented using markov chain monte carlo ( mcmc ) methods @xcite to sample from the posterior distribution . sampling a posterior probability distribution for catalogs",
    "is a challenging numerical task for a number of reasons .",
    "firstly , the number @xmath0 of objects in the image ( and that should therefore be listed in the catalog ) is itself unknown .",
    "secondly , if @xmath0 is large , then the parameter space of positions and properties ( flux , size , etc ) of the objects is also large .",
    "this can cause markov chain monte carlo ( mcmc ) algorithms difficulties  they may take a long time to converge to the target posterior distribution over the space of catalogs .",
    "thirdly , this problem is subject to the so - called label - switching problem that is commonly encountered in mixture modeling ( e.g. * ? ? ?",
    "given any proposed catalog , another catalog that is equally plausible is the catalog obtained by shuffling the entries of the first catalog .",
    "this leads to a posterior distribution with @xmath1 identical peaks in parameter space .",
    "this can lead to difficulties with certain ( otherwise very effective ) mcmc algorithms such as the affine - invariant stretch move @xcite .",
    "bayesian object detection ( as this problem is sometimes called ) has been implemented both inside and outside of astronomy ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "however , the @xcite approach makes the assumption of a known number of objects @xmath0 .",
    "this assumption is required for the multinest sampler @xcite to be applicable . using the results from the known @xmath0 run , it is possible ( under certain circumstances ) to reconstruct what the results would have been if an unknown-@xmath0 model had been used .",
    "however , this will not work well in situations where there is significant confusion ( i.e. two or more sources overlap ) .",
    "what is really required is a variable dimension model , where @xmath2 is an unknown quantity to be inferred from the data ( e.g. * ? ?",
    "the computational implementation of these models will require tools such as reversible jump markov chain monte carlo @xcite .",
    "other statistical methods have also been used to model crowded fields ( e.g. maximum likelihood , * ? ? ?",
    "* ) . however , maximum likelihood is not completely appropriate in flexible models because it may lead to overfitting . in this situation overfitting",
    "would result in more stars being added to the model to explain small positive fluctuations in the image which are actually due to noise .",
    "various other techniques have also been proposed in the literature .    in this paper",
    ", we develop a bayesian object detection model with the following features : i ) the number @xmath0 of objects in the image is an unknown parameter to be inferred from the data , ii ) the objects that we expect to find are point sources such as stars , and iii ) the point - spread function is unknown ( but a parametric model is used ) and must be inferred from the data ( but a single bright star may not be available to help with estimating it ) .",
    "the paper is structured as follows . in section  [ sec : bayes ] we give a brief overview of bayesian inference . in section  [ sec : model ] we discuss the model assumptions we make in our method . in section  [ sec : mcmc ] we briefly discuss our mcmc implementation",
    ". section  [ sec : simulated_data ] describes the tests we carried out on simulated data , and a comparison with sextractor  results is presented in section  [ sec : sex ] .",
    "we conclude in section  [ sec : conclusion ] .",
    "to quantitatively model uncertainties and transform noise in observed data into uncertainties in parameters of interest , bayesian inference is the appropriate framework @xcite .",
    "suppose there exist unknown parameters ( denoted collectively by @xmath3 ) and we expect to obtain some data @xmath4 .",
    "our prior state of knowledge about the parameters is modelled by a prior probability distribution : @xmath5 note that this is a very concise notation @xcite and should be read as , `` the probability distribution for @xmath3 '' .",
    "we also model how the parameters give rise to the data , via a generative model .",
    "this is also known as a _ sampling distribution _ : @xmath6 despite the singular , the sampling distribution is actually a family of probability distributions over the space of possible data sets , one probability distribution for each possible value of @xmath3 .",
    "note that the choice of the sampling distribution is also an assumption about prior knowledge : it models prior information about the fact that the data @xmath4 is connected to the parameters @xmath3 in some way @xcite . without this prior knowledge , learning is impossible : there has to be some relationship between the parameters and the data , otherwise it would be impossible to learn about parameters by obtaining data .",
    "when specific data @xmath7 are taken into account , our state of knowledge about @xmath3 gets updated from the prior distribution to the posterior distribution via bayes rule : @xmath8 the term @xmath9 is the _ likelihood function _ , which is the probability of obtaining the actual data set @xmath7 as a function of the parameters . in the case",
    "that the sampling distribution is a probability density function , the likelihood is the probability density function evaluated at the observed data .",
    "this usually causes no problems , although one should be aware of the borel - kolmogorov paradox @xcite .",
    "as suggested by the above notation , the likelihood function is obtained from the sampling distribution with the actual data substituted in and is therefore a function of the parameters only .    to proceed with the model for inferring catalogs from image data",
    ", we must specify a definite hypothesis space and choices for the prior distribution and the sampling distribution .",
    "these choices are presented and discussed in section  [ sec : model ] .",
    "the hypothesis space is the set of possible catalogs , or the set of possible answers to the question , `` what objects are present in the field and what are their properties ? ''",
    "we shall assume that there are an unknown number of stars @xmath0 in the field .",
    "each star has an unknown position @xmath10 in the plane of the sky , and an unknown flux @xmath11 .",
    "we also describe the distribution of fluxes ( commonly known as the _ luminosity function _ ) of the stars by some parameters denoted collectively by @xmath12 . in summary ,",
    "the unknown parameters are : @xmath13 we note that models similar to this have been implemented for general image modeling and deconvolution ( e.g. * ? ? ? * ) , however in this case it is more justified as we are actually searching for point fluxes .",
    "the prior probability distribution for the unknown parameters can be factorized using the product rule of probability theory . with a variety of independence assumptions ,",
    "the prior can be factorized as : @xmath14 here , we have assumed that the luminosity function does not depend on position . finally , the fluxes of the stars come independently from a common distribution .",
    "if we knew the luminosity function of the stars , then the location and flux of a particular star would not tell us anything about the location and flux of another star . really , this is just a way of implementing exchangeability of the stars , and is often called a _ hierarchical model_.    for simplicity , we assume a uniform prior probability distribution for the position of each star .",
    "the use of independent priors for the positions creates a strong preference for catalogs where the stars are uniformly scattered across the image .",
    "thus , this model is appropriate for small images where the density of stars is approximately uniform across the image .",
    "in other scenarios , such as images of stellar clusters , it is possible to parameterize the spatial distribution of the stars in a similar way to how we have parameterized the luminosity function , i.e. as a hierarchical model .",
    "the sampling distribution is a probabilistic model for the process that generates the data ; it describes the probability distribution we would use to predict the data if we happened to know the true catalog . in our case , the data will be an @xmath15 array of pixel intensities @xmath16 : @xmath17 where the central position of each pixel is : @xmath18 the image is assumed to be a noisy version of the true underlying intensity field .",
    "thus , we need a prescription for simulating an image @xmath19 from a catalog @xmath3 : @xmath13 if we knew the true catalog , we could compute the `` mock image '' we would expect to see in the absence of noise . this mock image ( defined at every point on the sky )",
    "is given by : @xmath20 where @xmath21 is the pixel - convolved point spread function ( psf ) .",
    "the use of a pixel - convolved psf is computationally advantageous because the psf need only be evaluated at the center of each pixel , rather than integrated over each pixel .    throughout this paper",
    "we will assume the pixel - convolved psf is a weighted mixture of two concentric circular gaussians ( a similar mixture model that uses a gaussian core and flexible wings is also used by daophot , @xcite ) with widths @xmath22 and @xmath23 : @xmath24 + \\frac{1-w}{2\\pi s_2 ^ 2}\\exp \\left[-\\frac{1}{2s_2 ^ 2}\\left(x^2 + y^2\\right ) \\right].\\end{aligned}\\ ] ] the pixellated observed image is assumed to be generated from the mock image ( evaluated at the center of each pixel ) plus noise : @xmath25 where the errors @xmath26 are independent and normally distributed .",
    "the variance of the normal distribution for each pixel is determined by the brightness of the sky and the brightness of the mock image in that pixel .",
    "this can be modelled by assuming the following distribution : @xmath27 where @xmath28 is a constant noise level and @xmath29 is an unknown = coefficient that allows for the possibility that the noise level is higher in brighter regions of the image .",
    "this dependence of the noise variances on the model intensity arises as a result of the poissonian nature of photon counts , but allows for the fact that a `` sky '' background may have already been subtracted from the image in the reduction process .",
    "this parameterisation has been used by @xcite and is an alternative to the common practice of producing a `` variance map '' from the image data that is then assumed to be known .",
    "the prior distribution for the number of stars @xmath0 is assigned to be uniform between 0 and some maximum number @xmath30 .",
    "the extent of the image is assumed to be from @xmath31 to @xmath32 and from @xmath33 to @xmath34 in arbitrary units , and the positions of the stars are assigned independent uniform priors : @xmath35 the stars are allowed to be slightly outside of the observed image because the psf can scatter light from these stars into the image .    for the purposes of this paper , we model the luminosity function as a broken power - law distribution , which has four free parameters : @xmath36 where @xmath37 is a lower flux limit",
    ", @xmath38 is a break - point , @xmath39 is the slope of the distribution between @xmath37 and @xmath38 , and @xmath40 is the slope of the distribution above @xmath38 . for mathematical details on the broken power - law model ,",
    "see appendix  [ power_law ] . while the broken power - law is likely to be unrealistic in many cases , it is a reasonably flexible distribution and this is sufficient for demonstrating the properties of our method .",
    "the prior distribution on @xmath37 , @xmath38 , @xmath39 , and @xmath40 is assigned to be : @xmath41 these priors express vague prior knowledge about @xmath39 and @xmath40 in addition to vague prior knowledge about @xmath37 and @xmath38 apart from the fact that the flux units are not extreme and that @xmath38 should be no more than an order of magnitude greater than @xmath37 .    this simply - parameterized model for the luminosity function can be criticized on the basis that information from bright stars can be used to infer the parameters of the luminosity function which then still apply at lower flux levels . in principle , this can be resolved by using a more flexible distribution ( e.g. * ? ? ?",
    "* ) where each star s measured brightness affects the inference of the luminosity function locally but not globally .",
    "the priors for the psf parameters and the noise parameters were assigned to be : @xmath42 where @xmath43 is the width of a pixel .",
    "these priors describe vague prior knowledge about the overall scale of the psf except that the wider component is less than 10 times as wide as the narrow component , as well as the knowledge that the noise variance is not extreme relative to the fluxes of the stars .",
    "the mcmc sampling was implemented using the diffusive nested sampling @xcite method ( hereafter dns ) .",
    "dns is a variant of the nested sampling @xcite algorithm that uses metropolis - hastings updates , and is very generally applicable .",
    "the main difference between dns and the standard metropolis - hastings algorithm is that the target distribution is modified . rather than simply exploring the posterior distribution over catalog space",
    ", dns constructs an alternative target distribution which is a mixture of the prior distribution with more constrained versions of the prior distribution .",
    "the modified target distribution assists the sampling in several ways .",
    "firstly , the target distribution shrinks at a constant rate with time during the initial phase of the exploration .",
    "this is similar to the popular `` simulated annealing '' method @xcite but with an optimal annealing schedule .",
    "secondly , communication with the prior is maintained : once a catalog is found that fits the data , the catalog can `` disintegrate '' back to the prior distribution and re - fit , allowing different peaks in the parameter space to be explored ( if they exist ) .",
    "this all happens naturally within the context of a valid mcmc sampler .",
    "the mcmc may also be run using the standard metropolis algorithm targeting the posterior distribution .",
    "in order to test our approach , we applied the method to two illustrative simulated images generated from the above model ( figure  [ fig : simulated_data ] ) .",
    "the purpose of this experiment was to test the computational feasibility of the model , as well as to compare the inferences from the model with those from more standard techniques .",
    "the true parameter values for the two simulated data sets are listed in table  [ tab : truth ] .",
    "the broken power - law parameter values were chosen so that roughly half of the stars fluxes were below and above the break - point respectively .",
    "figure  [ fig : powerlaw ] in appendix  [ power_law ] also shows the true flux distribution used for the simulated images .",
    "each of the images is @xmath44 pixels in extent and covers a range from @xmath45 to @xmath46 in arbitrary units for both the @xmath4 and @xmath47 axes .",
    "the first image contains 100 stars ( including stars just outside of the image ; there are 63 stars whose central positions lie within the image ) and the second image contains @xmath481000 stars ( 699 of which are positioned within the image ) .    .true parameter values used to generate the simulated data .",
    "@xmath0 is the number of stars , @xmath37 and @xmath38 are the lower limit and break point of the flux distribution respectively , and @xmath39 and @xmath40 are the slopes of the flux distribution . @xmath28 and @xmath29 describe the noise properties , and @xmath22 , @xmath23 , and @xmath49 are the psf parameters .",
    "the only difference between the two images is that test case 2 contains more stars than test case 1 .",
    "[ tab : truth ] [ cols=\"^,^,^\",options=\"header \" , ]        brewer , b.  j. , lewis , g.  f. , belokurov , v. , irwin , m.  j. , bridges , t.  j. , evans , n.  w.  2011.modelling of the complex cassowary / slugs gravitational lenses .",
    "monthly notices of the royal astronomical society 412 , 2521 - 2529"
  ],
  "abstract_text": [
    "<S> we present and implement a probabilistic ( bayesian ) method for producing catalogs from images of stellar fields . </S>",
    "<S> the method is capable of inferring the number of sources @xmath0 in the image and can also handle the challenges introduced by noise , overlapping sources , and an unknown point spread function ( psf ) . </S>",
    "<S> the luminosity function of the stars can also be inferred even when the precise luminosity of each star is uncertain , via the use of a hierarchical bayesian model . </S>",
    "<S> the computational feasibility of the method is demonstrated on two simulated images with different numbers of stars . </S>",
    "<S> we find that our method successfully recovers the input parameter values along with principled uncertainties even when the field is crowded . </S>",
    "<S> we also compare our results with those obtained from the sextractor  software . </S>",
    "<S> while the two approaches largely agree about the fluxes of the bright stars , the bayesian approach provides more accurate inferences about the faint stars and the number of stars , particularly in the crowded case . </S>"
  ]
}