{
  "article_text": [
    "hierarchical semi - markov models such as hhmm @xcite and hscrf @xcite are deep generalisations of the hmm @xcite and the linear - chain crf @xcite , respectively .",
    "these models are suitable for data that follows nested markovian processes , in that a state in a sub - markov chain is also a markov chain at the child level .",
    "thus , in theory , we can model arbitrary depth of semantics for sequential data .",
    "the models are essentially members of the probabilistic context - free grammar family with bounded depth .",
    "however , the main drawback of these formulations is that the inference complexity , as inherited from the inside - outside algorithm of the context - free grammars , is cubic in sequence length . as a result",
    ", this technique is only appropriate for short data sequences , e.g. in nlp we often need to limit the sentence length to , says , @xmath0 .",
    "there exists a linearisation technique proposed in @xcite , in that the hhmm is represented as a dynamic bayesian network ( dbn ) . by collapsing all states within each time slice of the dbn , we are able achieve linear complexity in sequence length , but exponential complexity in depth .",
    "thus , this technique can not handle deep architectures .    in this contribution",
    ", we introduce an approximation technique using gibbs samplers that have a potential of achieving sub - cubic time complexity in sequence length and linear time in model depth .",
    "the idea is that , although the models are complex , the nested property allows only one state transition at a time across all levels .",
    "secondly , if all the state transitions are known , then the model can be collapsed into a markov tree , which is efficient to evaluate .",
    "thus the trick is to sample only the markov transition at each time step and integrating over the state variables .",
    "this trick is known as rao - blackwellisation , which has previously been applied for dbns @xcite .",
    "thus , we call this method rao - blackwellisation gibbs sampling ( rbgs ) . of course , as a mcmc method , the price we have to pay is some degradation in inference quality .",
    "recall that in the linear - chain crfs @xcite , we are given a sequence of observations @xmath1 and a corresponding sequence of state variables @xmath2 .",
    "the model distribution is then defined as @xmath3 where @xmath4 are potential functions that capture the association between @xmath5 and @xmath6 as well as the _ transition _ between state @xmath7 to state @xmath8 , and @xmath9 is the normalisation constant .    thus , given the observation @xmath5 , the model admits the markovian property in that @xmath10 , where @xmath11 is a shorthand for @xmath12 .",
    "this is clearly a simplified assumption but it allows fast inference in @xmath13 time , and more importantly , it has been widely proved useful in practice . on the other hand , in some applications where",
    "the state transitions are not strictly markovian , i.e. the states tend to be persistent for an extended time . a better way is to assume only the transition between _ parent - states _",
    "@xmath14 , whose elements are not necessarily markovian .",
    "this is the idea behind the semi - markov model , which has been introduced in @xcite in the context of crfs .",
    "the inference complexity of the semi - markov models is generally @xmath15 since we have to account for all possible segment lengths .",
    "the hierarchical semi - markov conditional random field ( hscrf ) is the generalisation of the semi - markov model in the way that the parent - state is also an element of the grandparent - state at the higher level . in effect",
    ", we have a _ fractal _ sequential architecture , in that there are multiple levels of detail , and if we examine one level , it looks exactly like a markov chain , but each state in the chain is a sub - markov chain at the lower level",
    ". this may capture some real - world phenomena , for example , in nlp we have multiple levels such as character , unigram , word , phrase , clause , sentence , paragraph , section , chapter and book .",
    "the price we pay for these expressiveness is the increase in inference complexity to @xmath16 .",
    "one of the most important properties that we will exploit in this contribution is the _ nestedness _ , in that a parent can only transits to a new parent if its child chain has terminated .",
    "conversely , when a child chain is still active , the parent state must stay the same .",
    "for example , in text when a noun - phrase is said to transit to a verb - phrase , the subsequence of words within the noun - phase must terminate , and at the same time , the noun - phrase and the verb - phrase must belong to the same clause .",
    "the parent - child relations in the hscrf can be described using a state hierarchical topology .",
    "figure  [ fig : topo - model ] depicts a three - level topology , where the top , middle and bottom levels have two , four , and three states respectively .",
    "each child has multiple parents and each parent may share the same subset of children .",
    "note that , this is already a generalisation over the topology proposed in the original hhmm @xcite , where each child has exactly one parent .    [",
    "cols=\"^ \" , ]",
    "deep neural architectures ( dna ) such as deep belief networks @xcite and deep boltzmann machines @xcite have recently re - emerged as a powerful modelling framework which can potentially discover high - level semantics of the data .",
    "the hscrf shares some similarity with the dna in the way that they both use stacking of simpler building blocks .",
    "the purpose is to capture _ long - range _ dependencies or _ higher - order _ correlations which are not directly evident in the raw data .",
    "the building blocks in the hscrf are the chain - like conditional random fields , while they are the restricted boltzmann machines in the dna .",
    "these building blocks are different , and as a result , the hscrf is inherently sequential and localised in state representation , while the dna was initially defined for non - sequential data and distributed representation . in general , the distributed representation is richer as it carries more bits of information given a number of hidden units .",
    "the drawback is that probabilistic inference of rbm and its stacking is intractable , and thus approximation techniques such as mcmc and mean - field are often used .",
    "inference in the hscrf , on the other hand , is polynomial . for approximation ,",
    "the mcmc technique proposed for the hscrf in this paper exploits the efficiency in localised state representation so that the rao - blackwellisation can be used .",
    "perhaps the biggest difference between the hscrf and dna is the modelling purpose .",
    "more specifically , the hscrf is mainly designed for discriminative mapping between the sequential input and the nested states , usually in a fully supervised fashion .",
    "the states often have specific meaning ( e.g. noun - phrase , verb - phrase in sentence modelling ) . on the other hand , dna is for discovering hidden features , whose meanings are often unknown in advance .",
    "thus , it is generative and unsupervised in nature . finally , despite this initial difference",
    ", the hscrf can be readily modified to become a generative and unsupervised version , such as the one described in ( * ? ? ?",
    "training in hscrfs can be done simultaneously across all levels , while for the dna it is usually carried out in a layer - wise fashion .",
    "the drawback of the layer - wise training is that errors made by the lower layers often propagate to the higher .",
    "consequently , an extra global fine tuning step is often employed to correct them .",
    "there have been extensions of the deep networks to sequential patterns such as temporal restricted boltzmann machines ( trbm ) @xcite and some other variants .",
    "the trbm is built by stacking rbms both in depth and time .",
    "another way to build deep sequential model is to feed the top layer of the deep networks into the chain - like crf , as in @xcite .",
    "we have introduced a novel technique known as rao - blackwellised gibbs sampling ( rbgs ) for approximate inference in the hierarchical semi - markov conditional random fields .",
    "the goal is to avoid both the cubic - time complexity in the standard inside - outside algorithms and the exponential states in the dbn representation of the hscrfs .",
    "we provide some simulation - based evaluation of the quality of the rgbs with respect to run time and sequence length .",
    "this work , however , is still at an early stage and there are promising directions to follow .",
    "first , there are techniques to speed up the mixing rate of the mcmc sampler .",
    "second , the rbgs can be equipped with the contrastive divergence @xcite for stochastic gradient learning . and finally , the ideas need to be tested on real , large - scale applications with arbitrary length and depth .",
    "h.  h. bui , d.  q. phung , and s.  venkatesh .",
    "hierarchical hidden markov models with general state hierarchy . in d.  l. mcguinness and g.  ferguson , editors ,",
    "_ proceedings of the 19th national conference on artificial intelligence ( aaai ) _ , pages 324329 , san jose , ca , jul 2004 .",
    "j.  lafferty , a.  mccallum , and f.  pereira .",
    "conditional random fields : probabilistic models for segmenting and labeling sequence data . in _ proceedings of the international conference on machine learning ( icml ) _ , pages 282289 , 2001 .",
    "sunita sarawagi and william  w. cohen .",
    "semi - markov conditional random fields for information extraction . in bottou",
    "l saul  lk , weiss  y , editor , _ advances in neural information processing systems 17 _ , pages 11851192 . mit press , cambridge , massachusetts , 2004 ."
  ],
  "abstract_text": [
    "<S> deep architecture such as hierarchical semi - markov models is an important class of models for nested sequential data . </S>",
    "<S> current exact inference schemes either cost cubic time in sequence length , or exponential time in model depth . </S>",
    "<S> these costs are prohibitive for large - scale problems with arbitrary length and depth . in this contribution </S>",
    "<S> , we propose a new approximation technique that may have the potential to achieve sub - cubic time complexity in length and linear time depth , at the cost of some loss of quality . </S>",
    "<S> the idea is based on two well - known methods : gibbs sampling and rao - blackwellisation . </S>",
    "<S> we provide some simulation - based evaluation of the quality of the rgbs with respect to run time and sequence length . </S>"
  ]
}