{
  "article_text": [
    "phenix @xcite is a large , widely spread collaboration where many organizations from many countries participate ( 12 nations , 57 institutions , about 400 collaborators , about 1/4 pb is planned to be produced in the current year ) .",
    "the collaboration is in third year of data taking .",
    "distributed data ( experimental and simulated ) , distributed computing facilities and distributed users is our reality .    before discussing technical decisions we have to emphasize a range of common things and features in grid computing for hep which are important for further discussion .    first of all we paid attention that many systems for physics analysis are already in developing or prototyping stage @xcite .",
    "we recognized that using the globus tools @xcite is now standard de facto for distributed systems in hep .",
    "the last thing was emphasized many times on chep2003 @xcite .    in more detail",
    "we could see the following common components ( on all projects ) :    * using globus security infrastructure ( gsi ) ; * using the replica / file catalog for files distributed around the globe , however different projects use different cataloging engines : globus replica catalog @xcite , other types of file catalogs ; * using the gridftp over wan in conjunction with other data moving protocols @xcite ; * job submission with globus tools ; * using the concept of virtual organization .",
    "that is not surprising because every collaboration ( phenix is not exclusion ) needs several common tools :    * data moving over wan ; * job submission from distributed users to distributed computing clusters ; * monitoring tools .    on other hand",
    "the character of using globus middleware is different in different projects .",
    "if someone tries to see in deeper details - a lot of differences between collaborations could be discovered .",
    "those differences are rooted in the style of physics measurements which in turn depends on the details of the physics experiment , style of collaboration work , existing computing resources , prehistory of computing infrastructure and many other details . in other words",
    "there is no evidence that concrete large and sophisticated system built on top of grid middleware might be used in a different collaboration without reasonable adaptation or redesigning .",
    "that means we have to discuss briefly phenix computing environment for physics analysis .",
    "phenix has several computing facilities for physics analysis :    * rhic computing facility - rcf @xcite - main computing facility for phenix ; * cc - j @xcite ; * midrange cluster at sunysb @xcite ; * also there is a range of other computing facilities which is used for phenix physics analysis at several member institutes of phenix .",
    "it is assumed phenix will have more computing facilities in future .    by taking a look at phenix member list",
    "it is clear that data distribution inside collaboration is not trivial .",
    "even limited task to know the files are ( in which site and location ) is not possible without file / replica catalog ( or file catalog ) .",
    "in general file catalog is required to keep locations of all data files in the collaboration .",
    "there was a range of various decisions @xcite concerning file catalog .",
    "architecturally we tested two level _ file cataloging engine _ : distributed phenix file catalog @xcite is located at bnl and at remote universities and local sunysb file catalog based on magda @xcite .",
    "all instances of the catalog are interconnected by special replication process .",
    "technical description for central phenix file catalog was available in a different presentation on chep-2003 @xcite .",
    "at the same time it was recognized that remote universities need some cataloging facilities for internal use .",
    "an adapted version of magda @xcite was implemented and used at sunysb as local ( at sunysb ) cataloging facility @xcite . with time , it became clear that it is suitable and important to keep the information about many files ( physics data and other type of files : scripts , papers , etc . ) .",
    "part of this information ( information about files with physics data ) is propagated to the central phenix file catalog .",
    "there are several tools on top of adapted magda at sunysb which are most interesting for end users .",
    "first of all there are web pages @xcite with detailed information where and which files are available .",
    "another tool is the set of scripts to link locally available data files to the local directory .    as described before",
    ", sunysb magda catalog has replicated subset of central phenix file catalog .",
    "periodically cronjob starts the scripts to transfer the information about part of the files from central phenix file catalog to magda catalog and back , part of information from magda catalog is to be copied to central phenix file catalog .    in this way it is possible to keep detailed information about files which are interesting for sunysb .      in this context",
    "we mean that commands below create the soft link for locally available files .",
    "also information about linked files can be seen at the web pages @xcite if the panel _ used files _ will be clicked .    * ` ma_linklocalfile ` _ lfn _ - to link locally available file _ lfn _ otherwise special completion code will be returned ; * ` ma_linkfilelist ` _ list _ - to link locally available files from the _ list _ ; * ` ma_linkfilesubstr ` _ substring _ - to link all files which names are containing the _ substring _ ; * ` ma_showlinkedfiles ` - to display all files linked by current user ; * ` ma_releasefile ` _ lfn _ - to release the file _ lfn _ ; * ` ma_releaseherefiles ` - to release all files in current directory ; * * ` ma_releaseallfiles ` * - to release all files earlier linked by current user .    when data file names are released the following steps are performed for every file :    * the appropriate soft link is deleted ; * the appropriate record is deleted from the magda database ; that means this record will not appear anymore in output of the command ` ma_showlinkedfiles ` and in statistics delivered on the web pages .",
    "the information to the catalog magda is coming from special spider scripts which are running on required _ sites _ ( in our case there are 3 sites where spiders are running ) . on most sites a spider",
    "is started once a day or even once a week if the information is not changed often .",
    "file moving over wan is done at sunysb through use of adapted magda and through an alternative way - by a script . with magda user",
    "could go to the web site @xcite and describe required _ site _ , _ host _ , _ location _",
    "( actually the exact directory path ) and _ collection _ ( collection of files ) .",
    "after that it is possible to describe the _ task _ for data transfer ( with web pages on @xcite ) .",
    "real data moving is possible after activating the _ task _ by using the web pages @xcite . at night cronjob",
    "will perform all the activated _",
    "tasks_.    for the user convenience the script * gcopy * was developed to copy the files over wan .",
    "the script uses cluster descriptions which is discussed in chapter _ job submission _ of this paper .",
    "the usage of the script :    * gcopy * _ fromsite_:_fromlocation _ _ tosite_:[_tolocation _ ] ` \\ `    [ _ substring _ ]    the parameters _ fromsite _ and _ tosite _ are names of globus gateways .",
    "_ fromlocation _ and _ tolocation _ are exact directory paths .",
    "the parameter _ substring _ with wild - cards may be used to select file names in directory _ fromlocation _ to be copied .",
    "technically the file transfer is performed with gridftp protocol ( command * globus - url - copy * ) .",
    "the feature _ third party transfer _ is used the feature because the host where the file transfers are started is not the part of any computing cluster .",
    "default number of threads for data transfer is 5 . due to a range of network routers ( 6 or so ) between sunysb and bnl and due to other causes we see a significant difference in network connectivity speed during a day ( a factor of 2 ) .",
    "the best throughput we saw was about 7 mbytes / sec .",
    "this maximum throughput could be reached with different number of transfer threads and different size of network window at different time .",
    "taking into account those facts we conclude that it is difficult to predict what time the data transfer between bnl and sunysb will take .",
    "this is true for relatively large portion of data ( 0.2 tb and more ) .",
    "finally , it is much better to be sure that your data are available locally on the computing cluster where you plan to submit the jobs before job submission .",
    "in a distributed environment , it is often effective to use several computing clusters in different sites to get enough computing power or to load available clusters more evenly .",
    "we have to emphasize that nobody in the collaboration needs computing power as it is .",
    "physicists have a need to use * _ qualified _ * computing power .",
    "that means such a computing cluster where all phenix software is already installed and ready to be used . in further discussion",
    "we will assume the following :    * all required application software has been installed ; * required physics data are already locally available or if you plan to do a simulation you have enough disk space for the output simulated data ; * all globus tools have been deployed ; * users have already the certificates to use globus secure infrastructure ( gsi ) as well .    as already mentioned , the use of globus infrastructure for job submission is common place now .",
    "at the same time till last autumn ( 2002 ) we had some difficulties with globus toolkit ( gt ) ( especially with data transfer ) .",
    "it was decided to create a light weight testbed with minimum functionality of gt , with minimum efforts , and with minimum time for implementation which could be tested in real environment where conditions are close to production reality . to do that",
    "a simple set of scripts was developed .",
    "our script set ( set of wrappers on top of gt 2.2.3 ) for job submission and job output retrieval is deployed at client side .",
    "about 30 scripts were developed with total number of lines about 2000 .",
    "several of them are most significant for users .    *",
    "* gparam * - script to describe the configuration : number of computing clusters , names of globus gateways , other information ; in addition the script $ home/.gsunyrc ( same meaning as * gparam * ) is used to keep local values for current account ; * * gproxw * - to create globus proxy for a week ; * * gping * - to test availability of all clusters described in * gparam * ; * * gping-*_m _ to test availability of a desired cluster ( here _ m _ is suffix to denote a cluster : * s * - for cluster at sunysb @xcite , * p * - for phenix at bnl @xcite , * unm * for cluster at university of new mexico @xcite ) ; * * grun-*_m _ to perform one command ( script ) on a desired cluster ( see remark to * gping-*_m _ ) ; * * gsub-*_m _ _ job - script _ - to submit the _ job - script _ to a desired computing cluster ( see remark to * gping-*_m _ ) ; * * gjobs-*_m _ [ _ parameter _ ] - to get the output of command * qstat * on a desired cluster adn _ parameter _ is parameter to * qstat * ( see remark to * gping-*_m _ ) ; * * gsub * _ job - script _ - to submit the _ job - script _ to less loaded computing cluster ; * * gsub - data * _ job - script _ _ file _ - to submit the _ job - script _ to the cluster where file _ file _ is located ; if the file _ file _ has replica on all clusters - to submit to less loaded cluster .",
    "first of all the file location will be tested through local ( on site ) file catalog .",
    "* * gget * _ job - id _ - to get output from accomplished job _ job - id _ , if parameter is missing then last submitted job will by taken into account . * * gstat * _ job - id _ - to get status of the job _ job - id _ , if parameter is missing then last submitted job will be taken into account .",
    "if submitted job generates some files they will be left on the cluster where the job was performed .",
    "the files could be copied to a different location by data moving facilities described in previous section of the paper .",
    "the meaning of * _ less loaded cluster _",
    "* is important .",
    "we need to know on which cluster the expected execution time for the job is minimum .",
    "unfortunately this task in an unstable environment has no simple and precise solution . the estimate becomes worse if the job runs long time ( many hours for instance ) .",
    "all estimates might be done only on some level of probability .",
    "that was the reason why we took for a prototype a simple algorithm to determine * _ less loaded cluster_*. in principle that choice reflects our hope that situation with job queues will not be changed fast .",
    "values from queuing system ( the answer from the command * qstat * ) are used in algorithm .",
    "the estimate algorithm uses also a priori information about the average relative power of a node in the cluster . in our configuration",
    "we use two clusters : average computing power for the node at suny was determined as 1 , average computing power at bnl was determined as 2 ( the machine at bnl was twice faster ) .",
    "another parameter that is used the maximum number of jobs which may be in run stage at suny and at bnl .",
    "all those parameters are assigned in the cluster description script * param*.    just before starting the job the scripts * gsub , gsub - data * will gather the information about real status of queues on every cluster described in the configuration ( it is done with globus command _ globus - job - run _ which gets the answer from * qstat * ) . after that the following value for each cluster is calculated :    l = [ ( number of jobs in run stage ) + ( number of jobs in wait stage ) - ( maximum jobs in run stage ) ] / ( relative computing power )    the cluster with a minimum value of l is considered as * _ less loaded cluster_*. of course it is only an estimate of the reality .",
    "some peculiarities of a dispatching policies in local job manager ( lsf , pbs , other ) could make the above estimate wrong .",
    "however in most simple cases it gives the right direction .",
    "more sophisticated algorithms might be discussed separately .",
    "the current client part is really simple .",
    "we assume the client side ( usually desktop or laptop ) has a stable ip address and can be seen in the internet .",
    "this feature is mandatory to use all client globus stuff ( we used gt-2.2.3 and gt-2.2.4 on client side ) .    *",
    "deploy the globus toolkit .",
    "* copy all scripts from @xcite .",
    "after that please become root and do + .... tar zxvf gsuny.tar.gz cd gsuny ./setup .... + all the scripts will be at the directory /usr / local / gsuny/. to make them available please add this directory to the $ path environment variable .    now the client is ready to use almost all the commands ( excluding * gsub - data * which requires an additional package @xcite ) .",
    "first command has to be * gproxw * , it creates the globus proxy for a week .",
    "you could start * gping * after that .",
    "the command * gping * will show existing configuration ( number of clusters , globus gateways , other parameters ) .",
    "now you could submit the jobs to described clusters .",
    "if you submit many jobs ( several tens or hundred ) it may happen that they will run on different clusters .",
    "the job submission scripts use special logs to remember where the jobs were submitted and which globus commands in multicluster environment were performed .      in order to keep the trace of user commands in globus context and job submission several log files",
    "have been created and used :    * ` $ user/.globus / clusters / commands ` - file contains list of performed globus commands in format _ date / time command parameters _ ; * ` $ user/.globus / clusters / jobs ` - file contains job i d ( with name of globus gateway ) in format _ date / time jobid _ * ` $ user/.globus / clusters / datatranslogs ` - directory contains several files to keep trace of data moving over wan .",
    "almost all of mentioned scripts add some records to the above log files .",
    "the logs are very valuable for many reasons : debugging , statistics , etc .",
    "they are also important because the set of clusters which is used by a user may be different .",
    "the technical adding of a new computing cluster consists of several simple steps for every / only client computer :    * to change the parameters you can edit the script /usr / local / gsuny / gparam ( for system wide parameters ) or to edit the script $ user/.gsunyrc ( for current account ) ; * to talk to cluster authority to add your account on new cluster to the file /etc / grid - security / grid - mapfile on new cluster globus gateway . * to prepare three scripts and put them into the directory /usr / local / gsuny/ * * script to ping the new cluster ( please see * gping - p * as an example ) ; * * script to submit the job ( please see * gsub - p * as an example ) ; * * script to get the new cluster load level ( please see * gchk - p * as an example ) .    after that you can use the cluster under your own account from the desktop where you changed the scripts .",
    "if the cluster has to be from the configuration , you have to edit the script /usr",
    "/ local / gsuny / gparam accordingly : it is possible to delete the description of the cluster and change the number of clusters .",
    "if you have to change the cluster ( to use new cluster instead old one ) , you have to edit script /usr / local / gsuny / gparam ( or $ user/.gsunyrc ) as well .",
    "an execution time for different scripts is shown in table [ submission ] .",
    "here we have to emphasize that this time is required only to submit the job to the standard job queue .",
    "we use lsf at bnl and open pbs at sunysb as local job managers .",
    ".measurement results [ cols=\"<,>,^\",options=\"header \" , ]     [ submission ]    by looking at the table it is easy to realize that job submission takes time .",
    "it is not surprising because we used globus command _ globus - job - run _ to get current load of the cluster and we did not use globus index information service ( giis ) @xcite . also we use the parameter _",
    "-stage _ for the command _ globus - job - submit _ to copy a job script from local desktop to remote cluster ( it takes time as well ) .",
    "somebody may think that there is no reason to submit the job with expected execution time 1 minute because the overheads for job submission might be more than execution time . on the other hand if you do not know exactly the situation on clusters your short 1 minute job may stay in input queue many hours due to high load or some problem on the computing cluster .    to decrease the delays we plan to use monitoring and discovery service ( mds ) @xcite in nearest time .",
    "the first working prototype includes central phenix ( bnl ) , sunysb , vanderbilt university ( only replication subsystem for phenix file catalog was deployed ) , university of new mexico @xcite ( only simple job submission scripts were deployed ) .",
    "all components are working and results look promising :    * the distributed information about location of significant collaboration physics data is delivered in uniform and consistent manner ; * job submission from remote universities could be directed to less loaded cluster where required data are .",
    "it helps to use computing resources more effectively in two different scenarios . * * in first scenario it is needed to collect all available distributed computing power to do an analysis . in this situation",
    "you need to distribute the data around computing clusters before starting the job chain .",
    "described tools will help to load all available clusters evenly .",
    "* * in second scenario there are already distributed over several clusters data ( experimental and simulated ) . in this case",
    "the using of described tools will help to minimize data moving over wan .",
    "apparently it was done the first step in implementing the flexible distributed computing infrastructure .",
    "some additional work is required on robust interfaces in between cataloging engine , job submission tools , job accounting tools , data moving , and trace information about the jobs .",
    "* deployment of the grid infrastructure in collaboration scale may not be a business for one person .",
    "* grid architecture has to be supported at central computing facilities . *",
    "better understanding of our needs in grid comes with real deployment of the components .",
    "authors have to mention the people who gave us valuable information and spent discussion time : rich baker , predrag buncic , gabriele carcassi , wensheng deng , jerome lauret , pablo saiz , timothy l. thomas , torre wenaus , dantong yu .",
    "special thanks for our colleagues nuggehalli n. ajitanand , michael issah , wolf gerrit holzmann who asked many questions and helped to formulate things more clearly ."
  ],
  "abstract_text": [
    "<S> every year the phenix collaboration deals with increasing volume of data ( now about 1/4 pb / year ) . </S>",
    "<S> apparently the more data the more questions how to process all the data in most efficient way . in recent past </S>",
    "<S> many developments in hep computing were dedicated to the production environment . </S>",
    "<S> now we need more tools to help to obtain physics results from the analysis of distributed simulated and experimental data . </S>",
    "<S> developments in grid architectures gave many examples how distributed computing facilities can be organized to meet physics analysis needs . </S>",
    "<S> we feel that our main task in this area is to try to use already developed systems or system components in phenix environment .    </S>",
    "<S> we are concentrating here on the followed problems : file / replica catalog which keep names of our files , data moving over wan , job submission in multicluster environment .    </S>",
    "<S> phenix is a running experiment and this fact narrowed our ability to test new software on the collaboration computer facilities . </S>",
    "<S> we are experimenting with system prototypes at state university of new york at stony brook ( sunysb ) where we run midrange computing cluster for physics analysis @xcite . </S>",
    "<S> the talk is dedicated to discuss some experience with grid software and achieved results . </S>"
  ]
}