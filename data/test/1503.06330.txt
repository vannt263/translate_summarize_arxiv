{
  "article_text": [
    "model predictive control ( mpc ) is a consolidated control technique that can efficiently handle constraints on the process to be controlled .",
    "nevertheless , its application is not yet widespread in many domains where real - time computational constraints and requirements of certified solutions are of major concern , such as aerospace or automotive applications .",
    "there is a growing interest in both industry and academia for exploring parallel solutions to mpc problems  ( @xcite ) , especially in light of the emerging many - core architectures , aiming to improve the computational efficiency of solving the underlying optimization problem .",
    "* contribution*. in this paper , we explore the use of parallelization techniques to efficiently solve a typical mpc problem for a linear discrete - time system , with a substantial computational speedup compared to nonparallel implementations .",
    "our proposed algorithm combines the use of alternating direction method of multipliers ( admms  @xcite , @xcite ) to handle the coupling constraints that arise from the dynamics of the system and inexact solvers ( i.e. , solvers that guarantee feasibility and optimality only _ asymptotically _ with the number of iterations ) , such as the nesterov s dual fast gradient ( dfg ) method  @xcite .",
    "in particular , the first step of the proposed algorithm is to split the original mpc problem over the length @xmath0 of the prediction horizon into @xmath1 independent subproblems ( _ time - splitting _",
    "@xcite ) solved by @xmath1 parallel _ workers _ periodically exchanging information at predetermined synchronization points . then",
    ", the second step is to solve these subproblems in parallel using an inexact solver and guarantee , at the same time , that the solution of the original mpc problem is _ recursively feasible _ and the system is _ closed - loop stable_. the combination of parallelization and inexact solvers can result in infeasibility and closed - loop instability .",
    "we rely on an algorithm based on constraint tightening to overcome these issues . loosely speaking ,",
    "constraint - tightening algorithms solve an alternative problem in which the constraints have been tightened by a certain amount to compensate for the accuracy loss ( and possible related infeasibility ) introduced by the solver .",
    "we rely on an _ adaptive _ tightening strategy to select an appropriate amount of tightening for our algorithm .",
    "every time new measurements are available from the plant , our algorithm chooses the amount of tightening required for each subproblem in order to compensate for the error introduced by the time - splitting combined with the inexact solver .    *",
    "related work*. the _ time - splitting _ technique has been proposed in  @xcite .",
    "in contrast to  @xcite , we combine admm with inexact solvers and focus on the requirements for recursive feasibility and closed - loop stability of the original problem .",
    "other constraint - tightening schemes have been proposed in the literature ( outside the parallel framework ) .",
    "for example , the authors in  @xcite propose an algorithm in which the amount of tightening is chosen offline to guarantee suboptimality and feasibility of the solution for all the initial states of the mpc problem .",
    "solutions based on adaptive constraint tightening have been recently proposed in @xcite , where the tightening parameter is chosen adaptively .",
    "compared to  @xcite , our tightening update rule allows for a nonuniform amount of tightening ( the tightening varies along the prediction horizon ) .",
    "furthermore , thanks to the modular structure of our approach , the optimizer solves simpler problems of fixed dimension , which is independent from @xmath0 . as a consequence ,",
    "an increase of @xmath0 does not affect the conditioning of the problem and the convergence of the solver .",
    "hence , our approach leads to a performance improvement even when forcing full serialization of the parallel operations ( i.e. , _ serialized _ mode  @xcite ) .    _ * outline*_. in the following , section  [ sec : problem_formulation ] presents the initial problem formulation .",
    "section  [ sec : subproblem_reformulation ] introduces the auxiliary subproblems and our proposed solver .",
    "section  [ sec : tightening_improvements ] describes our strategy to select the tightening of each subproblem to handle the parallelization error .",
    "section  [ sec : closed_loop_certification ] proposes an online update strategy of the tightening parameters that guarantees recursive feasibility , suboptimality , and closed - loop stability .",
    "section  [ sec : evaluation ] presents numerical results using an academic example .",
    "finally , section  [ sec : conclusion ] concludes the paper . _ * notation*_. for @xmath2 , @xmath3 is the euclidean norm and @xmath4_+$ ] is the projection onto non - negative orthant @xmath5 . given a matrix @xmath6 , @xmath7_i$ ] denotes the @xmath8-th row of a and @xmath7_{i , j}$ ] the entry @xmath9 of a. furthermore , @xmath10 is the vector of ones in @xmath11 and @xmath12 the identity matrix in @xmath13 . in addition ,",
    "@xmath14 and @xmath15 denote the largest and the smallest ( modulus ) eigenvalues of the matrix a , respectively .",
    "@xmath16 denotes that @xmath17 is positive definite .",
    "consider the discrete - time linear system described below : @xmath18 where @xmath19 denotes the state of the system and @xmath20 denotes the control input .",
    "the sets @xmath21 and @xmath22 are simple proper convex sets ( i.e. , convex sets that contain the origin in their interior ) .",
    "our goal is to steer @xmath23 to the origin and satisfy the plant constraints .",
    "we use mpc to achieve these objectives . in this respect , consider the following finite - time optimal control problem :    [ eq : initial_problem ] @xmath24    where @xmath25 and @xmath26 are more compact notations for @xmath23 and @xmath27 , respectively . for @xmath28 (",
    "@xmath0 denotes the prediction horizon ) , the states and the control inputs are constrained in the polyhedral set described by  , where @xmath29 , @xmath30 , @xmath31 .",
    "note that can include constraints on the state only , i.e. , @xmath32 , and/or constraints on the control inputs only , i.e. , @xmath33 . in  , @xmath34 and @xmath35 .",
    "our problem formulation considers also a terminal cost @xmath36 associated with a terminal polyhedral set @xmath37 .    through the remaining of the paper ,",
    "we assume :    [ ass : stabilizability ] the pair @xmath38 is stabilizable .",
    "[ ass : terminalset ] suppose assumption  [ ass : stabilizability ] holds .",
    "given the gain @xmath39 obtained by the infinite - horizon linear quadratic regulator ( ih - lqr)characterized by the matrices @xmath6 , @xmath40 , @xmath41 , and @xmath42the following holds : @xmath43 in addition , the terminal penalty @xmath44 in the stage cost   is defined by the solution of the algebraic riccati equation associated with the ih - lqr .",
    "in general , the mpc controller solves the optimization problem   every time new measurements are available from the plant and returns an optimal sequence of states and control inputs that minimizes the cost  .",
    "let the optimal sequence be defined as follows : @xmath45 only the first element of @xmath46 is implemented in closed - loop , i.e. , the control law obtained using the mpc controller is given by : @xmath47 and the closed - loop system is described by @xmath48      we aim to solve problem   in parallel .",
    "hence , we exploit a similar approach as the one proposed in  @xcite .",
    "specifically , as in  @xcite , problem   is decomposed along the length of the prediction horizon @xmath0 into @xmath1 independent subproblems to be solved by @xmath1 _ parallel workers _ @xmath49 ( @xmath50 ) .",
    "each @xmath49 is allowed to communicate with its neighbours @xmath51 and @xmath52 at predefined synchronization points .",
    "the decomposition is possible thanks to the introduction of @xmath0 auxiliary variables @xmath53 used to break the dynamic coupling that arises from  .",
    "these @xmath54 can be seen as the global variables of the algorithm . in particular ,",
    "each @xmath54 stores the local predicted state @xmath55 of each subproblem and exchanges this stored information to guarantee consensus between neighboring subproblems , i.e. , to ensure that the predicted state of the @xmath56-th subproblem , namely @xmath57 , is equal to the current state of the @xmath58-st subproblem , namely @xmath59 . specifically , by introducing the _ consensus constraints _ @xmath60 , defining @xmath61^t$ ] , @xmath62 $ ] , @xmath63 $ ] , @xmath64 problem   becomes :    [ eq : consensus_problem ] @xmath65    where , defining @xmath66^t$ ] :    * @xmath67 , where @xmath68 . *",
    "@xmath69 , @xmath70 , . *",
    "@xmath71 , where @xmath72 .    furthermore , @xmath73 and @xmath74 vary for each subproblem as follows :    * @xmath75 $ ] and @xmath76 , @xmath77 .",
    "* @xmath78 $ ] and @xmath79 .",
    "note that we introduced a quadratic penalty in the cost of the form @xmath80 , according to the admm strategy  @xcite .",
    "this penalty has no impact on the cost of the original problem  , if the consensus constraints are satisfied .    in the following ,",
    "we introduce the subproblems that derive from problem  .",
    "let @xmath81 ( @xmath28 ) and @xmath82 ( @xmath83 ) be the lagrange multipliers associated with the equality constraints   and  , respectively .",
    "then , let the augmented lagrangian with respect to the multipliers @xmath81 and @xmath84 be defined as follows : @xmath85.\\end{aligned}\\ ] ] hence , we obtain the following @xmath1 independent subproblems , called _ original _ subproblems , associated with the @xmath1 workers @xmath49 ( @xmath50 ) : @xmath86      figure  [ fig : steps ] summarizes the main steps that lead to a suboptimal solution of the aforementioned problem and introduces the keywords used in the remaining of the paper .",
    "consider the mpc problem   and refer to this problem as the _ original _ mpc problem .",
    "the first step ( _ parallelization _ , detailed in section  [ subsec : parallelization ] ) is to rewrite the original problem in @xmath1 independent subproblems ( the _ original _ subproblems ) .",
    "the aim is to use a dual fast gradient ( dfg ) method to solve these subproblems in order to certify  in terms of suboptimality and recursive primal feasibility , along with closed - loop stability  the mpc solution",
    ". the use of the inexact solver will eventually cause a violation of the consensus constraints  - introduced to define the subproblems  .",
    "hence , the second step ( _ relaxation equality constraints _ detailed in section  [ subsec : equality_relaxation ] ) is introduced to relax the consensus constraints by a quantity indicates that @xmath87 varies along the prediction horizon .",
    "this also holds for the later - defined @xmath88 , @xmath89 , and @xmath90 . ]",
    "@xmath87 , preventing the occurrence of consensus - constraint violations .",
    "we refer to these subproblems as the _ equality relaxed _ ( er ) subproblems .",
    "the set of inequality constraints of the er subproblems includes the inequality constraints of the original subproblems   and the inequality constraints due to the relaxation of the consensus constraints  - .",
    "the third step ( _ definition tightening parameters _ detailed in sections  [ subsec : up_lagrange_multiplier ] ,  [ sec : tightening_improvements ] , and  [ sec : closed_loop_certification ] ) is required to address the following remaining issues .",
    "first , the solution of each er subproblem computed by the dual fast gradient method might violate the inequality constraints , due to the termination of the solver after a finite number of iterations .",
    "hence , the constraints of the er subproblems must be tightened by a quantity @xmath88 proportional to the desired level of suboptimality @xmath90 chosen by the algorithm .",
    "second , due to the relaxation of the consensus constraints , the _ consolidated _ prediction , i.e. , the predicted evolution of the state computed ( a posteriori ) using the control sequence obtained by the independent subproblems , might deviate from the predicted _ local _ solution computed by the independent subproblems and , eventually , violate the inequality constraints of the original problem . hence , an additional tightening ( dependent on @xmath87 ) must be introduced on the subset of inequality constraints of the er subproblems that corresponds to the original inequality constraints .",
    "the proposed algorithm addresses the aforementioned issues by exploiting the _ inequality tightened _ ( it ) subproblems .",
    "the it subproblems differ from the er subproblems in the definition of the feasibility region , which is tightened by a quantity @xmath91 that depends on both @xmath88 and @xmath87 .",
    "the last step ( _ set solver accuracy _ ) selects a suboptimality level @xmath90 for each subproblem , that guarantees a primal feasible and suboptimal solution for the original mpc problem within a fixed number of iterations .",
    "in the following , we introduce the er subproblems and the proposed algorithm to solve them using @xmath1 parallel workers . furthermore , we introduce an initial formulation of the it subproblems and derive conditions on the choice of the relaxation and tightening parameters to guarantee primal feasible and suboptimal solutions for of each subproblem .",
    "our goal is to obtain a solution for problem   by solving the independent subproblems   in parallel using inexact solvers , such as the nesterov s dfg  @xcite . in order to use our proposed solver ( introduced in section  [ sec : alg1 ] ) , which relies on first - order methods ,",
    "we introduce a reformulation of problem   to take into account that the constraints   and   can not be satisfied at the equality due to the iterative nature of the proposed solver and its asymptotic convergence properties .",
    "in particular , introducing the relaxation parameters @xmath92 , for each subproblem ( @xmath28 ) , the former equality constraints  - are replaced by the following inequality constraints :      or , in a more compact notation : @xmath94 where @xmath95 and @xmath96 .    in the remaining of the paper ,",
    "we consider the following _ equality relaxed _",
    "( er ) subproblems : @xmath97 hence , let @xmath98^t\\in \\mathbb r^{p_{\\xi_t}}_+$ ] be the lagrange multiplier associated with the new set of inequality constraints defined by  , where @xmath99 , @xmath100 @xmath101 , @xmath102 , and @xmath103 are the multipliers associated with the constraints  ,  ,  ,  , and  , respectively .",
    "then , to handle the complicated constraints  , define , for each subproblem , the dual function @xmath104 as follows : @xmath105 where @xmath106 .",
    "we refer to as the _ inner subproblem_. hence , we aim to solve the following dual subproblems ( _ outer subproblem _ ) in parallel to obtain a solution for problem  : @xmath107    [ rem : modularity ] the size of the er subproblems remains unaffected if @xmath0 increases .",
    "intuitively , this _",
    "modularity _ is an additional feature that can be exploited to preserve some favorable numerical properties of the problem ( e.g. , conditioning , lipschitz constant , etc . )",
    "even when the algorithm is running in serialized mode .",
    "this section introduces algorithm  [ alg : alg1 ] that we use to solve the mpc problem   exploiting @xmath1 parallel workers @xmath49 .",
    "note that , at this stage , we can not yet ensure that the computed solution is feasible and suboptimal for problem  .",
    "algorithm  [ alg : alg1 ] relies on nesterov s dfg in which the inner problem is solved in an admm fashion , as explained below .",
    "specifically ,  at each iteration of the algorithm_{1:n}=x_0 $ ] and @xmath108 to initialize algorithm  [ alg : alg1 ] . ]",
    ", @xmath49 computes a minimizer @xmath109 for @xmath110 ( steps 1 - 4 ) , i.e. , the algorithm returns a solution for each inner subproblem  . in particular , our algorithm , in compliance with the admm strategy , first minimizes @xmath111 with respect to @xmath112 in parallel for each subproblem ( step 1 ) .",
    "then , using the information received by @xmath52 , i.e. , the updated value of @xmath113 ( synchronization step 2 ) , our algorithm computes  in parallel for each subproblem  the value of the global variable @xmath54 according to the following rule ( step 3 ) : @xmath114 note that this strategy allows to handle the coupling introduced by the 2-norm in the cost function of  .",
    "then , ( synchronization step 4 ) @xmath49 receives ( sends ) the updated value of @xmath115 ( @xmath116 ) from @xmath52 ( to @xmath117 ) , respectively .",
    "finally , the worker  @xmath49 computes the new values of the multipliers @xmath118 ( steps 5 - 7 ) .",
    "we compute offline ( for each subproblem ) the lipschitz constant @xmath119 associated with @xmath120 to perform the multipliers update :    * @xmath121 . * @xmath122 , ( @xmath70 ) .",
    "* @xmath123 .",
    "[ sec : alg1 ]     of algorithm  [ alg : alg1 ] . ]",
    "note that our update rule is different from the one proposed in  @xcite , where admm is used in combination with nesterov s fast gradient methods . at each iteration",
    ", the algorithm proposed in  @xcite , first computes the exact minimizer @xmath112 and then updates @xmath124 and @xmath84 . our algorithm does not wait until the dfg returns a minimizer @xmath112 to update the multipliers @xmath124 and @xmath84 , but starts updating their values along with the dfg iterations , encouraging the information exchange between neighboring subproblems .",
    "this algorithm is also different from the one proposed in  @xcite . in particular , the workers exchange the necessary pieces of information before the update of the lagrange multipliers and none of the dual variables is exchanged between the neighboring workers .",
    "furthermore , the information exchange between neighboring workers is unidirectional , i.e. , @xmath52 sends the updated information to @xmath49 , but @xmath49 does not send any updated information to @xmath52 .    using an argument similar to the one of theorem 1 in  @xcite , we can compute the primal feasibility violation and the level of suboptimality of the solution of each er subproblem returned by algorithm  [ alg : alg1 ] .",
    "[ thm : thm1 ] ( @xcite ) let @xmath125 be strongly convex , the sequences ( @xmath126 , @xmath127 , @xmath128 ) be generated by algorithm  [ alg : alg1 ] , and @xmath129 .",
    "then , an estimate on the primal feasibility violation for the er subproblem   is given by the following : @xmath130_+\\|\\le \\frac{8r_{t}{\\max\\{l_{\\mu_t}\\}}}{(k+1)^2}=:\\eta_t,\\ ] ] where @xmath131 .",
    "moreover , an estimate on primal suboptimality is given by the following : @xmath132    algorithm  [ alg : alg1 ] terminates after a fixed number of iterations that depends on @xmath90 and @xmath133  @xcite : @xmath134      in order to guarantee the primal feasibility of each subproblem using algorithm  [ alg : alg1 ] , we introduce @xmath1 auxiliary subproblems , namely the _ inequality tightened _ ( it ) subproblems , which differ from the er subproblems   in the definition of the feasible region .",
    "in particular , each it subproblem can be defined as follows : @xmath135 where @xmath136 is the tightening parameter , which depends on the suboptimality level  @xmath90 that the proposed algorithm can reach within @xmath137 iterations  . according to  @xcite , solving   using algorithm  [ alg : alg1 ] ensures , with a proper choice of @xmath88 , that the solution of   is primal feasible and suboptimal for subproblem  .    to define an @xmath88 similar to the one introduced in  @xcite",
    ", we must compute an upper bound for the optimal lagrange multiplier , namely @xmath138 , associated with the it subproblem  .",
    "we use an argument similar to the one of lemma 1 in  @xcite .",
    "in particular , we compute the aforementioned upper bound for @xmath138 according to the following lemma .    [",
    "lem : lemma_up_bound ] assume that there exists a slater vector @xmath139 such that @xmath140 .",
    "then , there exists @xmath141 , @xmath142 , @xmath143 , such that the upper bound for @xmath138 is given by @xmath144_j\\}}},\\ ] ] where @xmath145 [ 2\\rho(\\epsilon_{z_t}-\\epsilon_t)\\operatorname{\\mathbf{1}}_{n}^t ] [ 2\\rho(\\epsilon_{z_{t+1}}-\\epsilon_t)\\operatorname{\\mathbf{1}}_{n}^t]\\big]^t\\in \\mathbb r^{p_t+2n}$ ] , and @xmath146 is the dual function for the original subproblem   evaluated at @xmath147",
    ".    see appendix  [ app : lemma1 ] .",
    "see appendix i in  @xcite .",
    "lemma  [ lem : lemma_up_bound ] does not only provide an upper bound for @xmath148 , but it also provides guidelines to select the values of @xmath149 and @xmath150 as a function of @xmath151 , which only depends on the primal variable @xmath152 . an alternative way to determine the relaxation parameters is to include @xmath149 and @xmath150 in the set of decision variables and penalize them in the cost function as it is usually done to handle soft constraints .",
    "this will , however , increase the number of decision variables in the problem formulation and it will have an impact on the original cost .",
    ", the blue color refers to subproblem handled by worker  @xmath153 , etc . ) . ]",
    "the previous section showed how to choose the tightening parameter @xmath88 of each it subproblem to ensure that the @xmath154-th _ local _ solution , i.e. , the solution computed by the @xmath154-th it subproblem  , is primal feasible for the @xmath154-th er subproblem .",
    "this section provides guidelines to improve the choice of the tightening parameter of each it subproblem   in order to guarantee the primal feasibility of the _ consolidated _  solution , i.e. , the predictions obtained , starting from the initial state @xmath155 , using the control sequence @xmath156 where the elements of @xmath157 are computed by the independent it subproblems  .",
    "in particular , when a new measurement is available from the plant , the subsystems   compute in parallel @xmath158 @xmath159,@xmath160 and @xmath161 , respectively . according to the results of the previous section ,",
    "the pair @xmath162 is primal feasible for the @xmath154-th subproblem  , thanks to the introduction of the it subproblems .",
    "nevertheless , due to the relaxation introduced on the equality constraints  - , there is a bounded mismatch between @xmath57 and @xmath59 ( @xmath28 ) .",
    "hence , starting from the initial state @xmath155 , when the control sequence @xmath163 is applied to compute the consolidated state prediction   @xmath164 the feasibility of @xmath165 is no longer guaranteed .",
    "note , however , that @xmath166 , i.e. , @xmath157 is feasible . hence , no additional tightening is needed on the input constraints .    in the following",
    ", section  [ subsec : upbound ] defines an upper bound on the maximal feasibility violation of @xmath165 .",
    "this feasibility violation is a consequence of the local relaxations of the equality constraints .",
    "then , section  [ subsec : improve_tightening ] introduces sufficient conditions to ensure the primal feasibility of the _ consolidated _  prediction and provides guidelines for the choice of the tightening parameters for each it subproblem .",
    "let @xmath168 and @xmath169 be defined by   and  , respectively .",
    "moreover , from   and  , the following holds : @xmath170 our goal is to characterize how far the _ consolidated _  predicted state is from the _ local _  predicted state .",
    "[ lem : upper_bound_global_prediction ] let the @xmath154-step - ahead _ consolidated _  prediction @xmath171 be defined by   and assume that   holds .",
    "then , there exists @xmath172 , such that the mismatch between @xmath171 and the state of the @xmath154-th subproblem @xmath173 is bounded , as follows : @xmath174",
    ". see appendix  ii in  @xcite .",
    "[ rem : after_lemma_2_a ] according to lemma  [ lem : upper_bound_global_prediction ] a possible choice of @xmath175 is the following : @xmath176      according to lemma  [ lem : upper_bound_global_prediction ] , @xmath171 differs from @xmath173 by a quantity bounded by @xmath175 .",
    "thus , @xmath171 might violate the constraints of the @xmath154-th subproblem   by as much as @xmath175 , in the worst - case scenario .",
    "in particular , we must ensure that @xmath177 . using the computed upper",
    "bound  , the following holds : @xmath178 where @xmath179 indicates the absolute value of @xmath180 . recall that these mismatches are caused by the use of inexact solvers and that @xmath175 depends on @xmath87 . in the following , we provide guidelines to improve the choice of @xmath88 for each subproblem .",
    "furthermore , we provide a modified upper bound for the optimal lagrange multiplier associated with the tightened subproblems  , which considers the additional tightening introduced by @xmath175 .",
    "[ lem : improve_epsilon_t ] consider the following it subproblems : @xmath181 for @xmath182 , where @xmath89@xmath183@xmath184  @xmath185^t$ ] .",
    "consider the assumptions of lemma  [ lem : lemma_up_bound ] and the existence of @xmath175 for all @xmath83 according to lemma  [ lem : upper_bound_global_prediction ] .",
    "then , for each subproblem , there exist @xmath141 , @xmath186 such that the upper bound for the optimal lagrange multiplier associated with the it subproblems   is described by @xmath187_j\\}}},~{\\ensuremath{t\\!=\\!0,\\ldots , n}},\\ ] ] @xmath188 [ 2\\rho ( \\epsilon_{z_{t}}-\\epsilon_t)\\mathbf{1}_n^t ] [ 2\\rho(\\epsilon_{z_{t+1}}-\\epsilon_t)\\mathbf{1}_n^t]\\big]^t\\in \\mathbb r^{p_t+2n}$ ] .",
    "see appendix  [ app : lemma3 ] .",
    "see appendix  iii in  @xcite .",
    "[ rem : after_lemma_3 ] the choice of @xmath88 ( @xmath182 ) is not unique and depends on the choice of @xmath87 ( @xmath83 ) .",
    "for example , given @xmath175 in  , a possible choice of @xmath87 ( @xmath83 ) is : @xmath189_{j , i}|\\big\\}}}}\\!\\bigg\\}. \\label{eq : choice_epsilon_z}\\end{aligned}\\ ] ] consequently , the tightening parameters are given by : @xmath190 for @xmath182 .",
    "this choice implies that first we select the relaxation parameters and then we _ adapt _ the tightening parameters on the original inequality constraints based on the choice of @xmath87 for all @xmath83 .",
    "an alternative is to fix @xmath88 for the inequality constraints and consequently compute @xmath87 . in general",
    ", the choice of the parameters strongly depends on the system - state matrix @xmath6 in  .",
    "[ rem : offline_computation_parameters ] in the context of this work , algorithm  [ alg : alg2 ] , described in the next section , adapts the above derived parameters at each problem instance .",
    "if we consider a fixed tightening scheme , such as the one proposed by  @xcite , @xmath191 and @xmath87 can be computed offline ( for all the initial states in the region of attraction ) .    in the following ,",
    "we show that by using @xmath192@xmath193 is the control sequence obtained by solving the it subproblems   and @xmath194 is the corresponding consolidated prediction  the inequality constraints of the original mpc problem   are satisfied .",
    "consequently , the predicted final state is in the terminal set of the original problem .",
    "if the desired level of suboptimality of algorithm  [ alg : alg1 ] is chosen as : @xmath195 then , according to theorem  [ thm : thm1 ] , there exists @xmath196^t$ ] such that @xmath197_+\\| \\leq\\eta_t<\\epsilon_t$ ] . using similar arguments as in  @xcite",
    ", the following holds for @xmath182 : @xmath198_+<\\epsilon_t\\operatorname{\\mathbf{1}}_{p_t+4n}.\\end{aligned}\\ ] ] hence , for all @xmath199 , the following holds @xmath200_j\\bigg]_+\\leq \\epsilon_t.\\end{aligned}\\ ] ] consequently , exploiting the upper bound  , for all @xmath199 , we have : @xmath201_j\\leq \\epsilon_t\\end{aligned}\\ ] ] which leads to @xmath202 , where @xmath203 is the @xmath154-step - ahead _ consolidated _  prediction computed using the solution to the it subproblem   with tighening parameter @xmath89 .    in summary",
    ", this section showed that there exists a choice of the relaxation and tightening parameters that guarantee a feasible consolidated prediction with respect to the original problem  .",
    "in the following , we derive bounds for @xmath204 , i.e. , the cost obtained using @xmath192 , with respect to the optimal cost @xmath205 of the original problem .",
    "[ th : theorem2 ] assuming that there exist @xmath88 ( @xmath182 ) and @xmath87 ( @xmath83 ) selected according to lemma  [ lem : improve_epsilon_t ] , then the following holds : @xmath206 where @xmath207_{j , i}|\\big\\}}}\\alpha_t$ ] .",
    "see appendix  [ app : proof_theorem_2 ] .",
    "see appendix iv in  @xcite .",
    "theorem  [ th : theorem2 ] established the level of suboptimality of the consolidated prediction with respect to the original problem . in particular , the sequence @xmath208 is suboptimal for the original problem and satisfies the original inequality constraints ( including those associated with @xmath209 ) .",
    "recall that for the update of @xmath210 , our algorithm requires a strictly feasible vector @xmath152 for  .",
    "hence , every time new measurements are available from the plant , our algorithm must provide a strictly feasible solution ( not necessarily optimal ) for the first @xmath211 inequality constraints of each er subproblem .",
    "the following lemma provides guidelines to compute @xmath152 .",
    "[ lem : update_y ] let @xmath212 be defined as @xmath213=[(x_{0}^t~\\bar u_{0,\\gamma_0}^t)\\ldots(\\bar x_{n-1,\\gamma_{n-1}}^t~\\bar u_{n-1,\\gamma_{n-1}}^t)~(\\bar x_{n,\\gamma_n}^t))]^t$ ] .",
    "then , a feasible @xmath214 at the next problem instance , is given by : @xmath215}}((a+bk_f)\\bar x_{n,\\gamma_n})^t]^t \\end{aligned}\\ ] ]    see appendix  [ app : proof_lemma_update_y ] . see appendix v in  @xcite .",
    "we want to show that the cost decreases at each problem instance . using a similar argument as in  @xcite , under assumption  [ ass : terminalset ] on @xmath216 and ensuring that @xmath217 ( thanks to a proper choice of the tightening parameters , as the previous section showed )",
    ", we can show that : @xmath218 where @xmath219 is the region of attraction .",
    "hence , from and , the following holds : @xmath220\\!\\!-\\!\\!{\\ensuremath{\\mathbf v}}_{0}(y_{0,\\gamma_0})\\end{aligned}\\ ] ] where @xmath221 , using @xmath222 to represent the updated values of these parameters according to @xmath223 .",
    "the inequality above shows that the total cost decreases at each problem instance if @xmath216 is defined according to assumption  [ ass : terminalset ] and if the @xmath0-step - ahead _ consolidated _  prediction lies in the terminal set .",
    "asymptotic stability of our controller follows if @xmath224 .",
    "hence , we can modify the update of @xmath88 and @xmath87 to ensure that   is satisfied .",
    "[ rem : update_z ] a possible choice of @xmath87 @xmath225 to fulfill   is the following : @xmath226 @xmath227_{j , i}|\\big\\}}}\\bigg)}\\bigg]^{-1}.\\notag\\end{aligned}\\ ] ] consequently , @xmath88 can be selected according to   to preserve the definition of the upper bound on the optimal lagrange multipliers given in lemma  [ lem : improve_epsilon_t ] .",
    "algorithm  [ alg : alg2 ] summarizes the main steps needed to obtain a stabilizing control law when the original mpc problem is solved in parallel using inexact solvers . in particular , note that , if the measured state is in @xmath216 , from assumption  [ ass : terminalset ] , the state and the control constraints are automatically satisfied without solving the mpc problem in parallel .",
    "steps 17 - 23 are the only nonparallel ones of the algorithm ( algorithm  [ alg : alg1 ] is instead fully parallelizable ) .",
    "the main reason is in the adaptive nature of the algorithm ( see also remark  [ rem : offline_computation_parameters ] ) .",
    "algorithm  [ alg : alg2 ] adapts @xmath88 and @xmath87 every time new measurements are available from the plant . a fully parallel algorithm  [ alg : alg2 ] is possible using a fixed tightening strategy , in which @xmath88 and @xmath87 can be computed offline .",
    "we evaluated algorithm  [ alg : alg2 ] using the lti system described in  @xcite .",
    "the system ( sampled at @xmath228 s ) is described by : @xmath229 where @xmath230 , @xmath231 , and the quadruple @xmath232 is given by : @xmath233 the weighting matrices @xmath41 , @xmath42 , and @xmath234 in the cost   and the ih - lqr gain @xmath235 are selected according to  @xcite .",
    "we implemented our design in matlab ( to tune the controller and test the initial design ) and in c ( to run a performance analysis ) .",
    "in particular , in matlab , we used the parallel computing toolbox  to assign the computation of algorithm  [ alg : alg1 ] to 8 parallel workers , given a prediction horizon @xmath236 .",
    "furthermore , we relied on the mpt3 toolbox  @xcite to compute @xmath216 and the optimal solution of problem  .",
    "finally , we compared our design to  @xcite .",
    "we considered the following scenario .",
    "the initial state of the system is @xmath237^t$ ] .",
    "the total number of complicated constraints   for the original problem is 90 .",
    "we used   and   to initialize @xmath87 and @xmath191 , respectively . to update them",
    ", we relied on   and  .",
    "the selected @xmath155 caused @xmath238 and @xmath239 to saturate ( 12 active constraints ) . in this scenario ,",
    "the state enters @xmath216 in 3 steps .",
    "figure  [ fig : upp_bound_alpha ] shows the mismatch between the _ local _  prediction and the _ consolidated _  prediction for one problem instance . as figure  [ fig : upp_bound_alpha ] depicts , the mismatch ( for both states ) is below the predicted upper bound @xmath175 for all the @xmath1 subproblems .",
    "table  [ tab : iterations ] compares the proposed technique to  @xcite .",
    "the table reports the upper bound @xmath240 on the number of iterations needed to achieve a suboptimal solution for problem   and the level of suboptimality @xmath241 .",
    "the table lists only the first four subproblems , which are the most significant due to the presence of active constraints in these subproblems .",
    "the method in  @xcite and our new proposed parallel algorithm produce a comparable behavior , thanks to an appropriate selection of the tightening parameters .",
    "the parallel algorithm , however , is able to achieve similar results to those in  @xcite using a smaller number of iterations .",
    "the larger values of @xmath240 for  @xcite are probably caused by the value of the lipschitz constant and by the problem conditioning , which affect the convergence requiring a higher accuracy for the solver .",
    "in particular , in our proposed framework , the dfg is applied to simpler problems characterized , in the worst case scenario , by a lipschitz constant @xmath242 and by a condition number @xmath243 . in  @xcite",
    ", the dfg solves a larger problem characterized by a lipschitz constant @xmath244 and by a condition number @xmath245 .",
    "hence , the modularity of our approach has positive implications on important properties for the convergence of the solver .",
    "table  [ tab : iterations ] lists the time required by the optimizer to return a suboptimal solution for problem  . to measure the performance , we implemented both algorithms in c on a linux - based os .",
    "we noticed that given the small size of the problem , running algorithm  [ alg : alg1 ] in parallel did not result in significant speedups compared to our algorithm running in serialized mode  @xcite .",
    "nevertheless , in both cases , we registered a speedup ( 230x ) compared to  @xcite .",
    "the modularity of the proposed algorithm is beneficial even for problems of small size , such as the one considered in this section for the comparison with  @xcite .",
    "we expect the benefits to be even more pronounced when considering problems of larger size .",
    ".performance analysis of algorithm  [ alg : alg1 ] and  @xcite .",
    "results show the median of 11 experiments .",
    "[ cols=\"^ , < , < , < , < , < \" , ]     [ tab : iterations ]",
    "we proposed an algorithm tailored to mpc that guarantees recursive feasibility and closed - loop stability , when the solution of the mpc problem is computed using inexact solvers in a parallel framework .",
    "in particular , our algorithm combines admm and dfg methods and relies on an adaptive constraint - tightening strategy to certify the mpc law .",
    "our numerical analysis shows performance improvements compared to state - of - the - art nonparallel techniques  @xcite .",
    "furthermore , our study shows that , for small - size problems , even if the solver is implemented in a serialized mode , there is substantial performance improvement with respect to the state of the art .",
    "we expect further benefits from the parallelization when the size of the problem increases .",
    "a scalability analysis of the proposed algorithm on many - core architectures is part of our ongoing work .",
    "this section contains the proof of lemma  [ lem : lemma_up_bound ] presented in section  [ subsec : up_lagrange_multiplier ] .",
    "assume that there exists a slater vector @xmath139 such that @xmath140 .",
    "then , there exists @xmath246 , @xmath142 , @xmath143 , such that the upper bound on @xmath138 is given by @xmath247_j\\}}},\\ ] ] where @xmath248 and @xmath146 is the dual function for the original subproblem   evaluated at @xmath147 .",
    "the following inequality holds : @xmath249 where the last inequality takes into account that @xmath250 .",
    "define @xmath251 and @xmath252 . consequently , using the definition of @xmath253 and @xmath254 , the following holds : @xmath255^t\\|.\\ ] ] furthermore , recalling that @xmath256 , @xmath257 , and @xmath258 , the following holds : @xmath259^t\\|\\!\\leq\\![\\lambda_{t,\\epsilon_t}^{*t}~w_{t,\\epsilon_t}^{*t}~v_{t+1,\\epsilon_t}^{*t}]^t\\!\\!\\operatorname{\\mathbf{1}}_{p_t+2n}.\\ ] ] hence , if we compute an upper bound for the vector @xmath260^t$ ] , we obtain an upper bound for @xmath148 .",
    "thus , from the inequality  , it follows that : @xmath261 notice that choosing @xmath262 , @xmath263 , i.e. , according to the assumptions of the lemma , the elements of @xmath264 are all greater than zero .",
    "thus , using   and  , it follows : @xmath265_j\\}\\|\\mu_{t,\\epsilon_t}^*\\| \\le\\notag \\\\ \\label{eq : lemma_1_proof } & \\le \\begin{bmatrix } \\lambda_{t,\\epsilon_t}^ { * } &   w_{t,\\epsilon_t}^ { * } & v_{t+1,\\epsilon_t}^{*}\\end{bmatrix } \\gamma_t.\\end{aligned}\\ ] ] consequently , the upper bound on the optimal lagrange multiplier is given by : @xmath266_j\\}}.\\end{aligned}\\ ] ]",
    "this section contains the proof of lemma  [ lem : upper_bound_global_prediction ] presented in section  [ subsec : upbound ] .",
    "let the @xmath154-step - ahead _ consolidated _  prediction @xmath171 be defined by   and assume that   holds .",
    "then , there exists @xmath172 , such that the mismatch between @xmath171 and the state of @xmath154-th subproblem @xmath173 is bounded , as follows : @xmath267    in the following , we omit the dependence from @xmath88 to simplify the notation .",
    "the proof is constructive . for @xmath268 , @xmath269 . for @xmath270 , @xmath271 , which is the 1-step - ahead state computed by the local subproblem 0 , i.e. , the subproblem associated to worker @xmath272 .",
    "hence , the mismatch between @xmath273 and @xmath274 is simply given by @xmath275 for @xmath276 , the following holds : @xmath277 @xmath278 which proves the lemma .",
    "this section contains the proof of lemma  [ lem : improve_epsilon_t ] presented in section  [ sec : tightening_improvements ] .    consider the following it subproblems : @xmath279 for @xmath182 , where @xmath89@xmath183@xmath184  @xmath185^t$ ] .",
    "given the assumptions of lemma  [ lem : lemma_up_bound ] and the existence of @xmath175 for all @xmath83 according to lemma  [ lem : upper_bound_global_prediction ] .",
    "then , for each subproblem , there exist @xmath141 , @xmath186 such that the upper bound on the optimal lagrange multiplier associated with the it subproblems   is described by @xmath187_j\\}}},~{\\ensuremath{t\\!=\\!0,\\ldots , n}},\\ ] ] @xmath280    this lemma follows from lemma  [ lem : lemma_up_bound ] applied to the subproblems  . from inequality   formulated for subproblem  ,",
    "the following must hold @xmath281 hence , in order to satisfy the inequality above , we can select the relaxation parameters @xmath87 and the tightening parameters @xmath88 according to the assumption of the lemma for @xmath182 , i.e. , the following must hold : @xmath282_{j , i}|\\big\\}}}\\alpha_t+\\epsilon_t\\\\ \\text{\\it ( ii)~ } & { \\ensuremath{\\max\\limits_{j=1,\\ldots , p_t}\\!\\!\\!\\big\\{\\sum_{i=1}^n|[c_{t}]_{j , i}|\\big\\}}}\\alpha_t+\\epsilon_t > 0\\\\ \\text{\\it ( iii)~}&\\epsilon_{z_t},\\epsilon_{z_{t+1}}\\geq \\epsilon_t\\geq 0,\\end{aligned}\\ ] ] where @xmath152 is a strictly feasible solution for the original @xmath154-th subproblem . hence , there exists @xmath210 such that the upper bound on the optimal lagrange multiplier @xmath283 is defined as follows : @xmath284\\}}}.\\end{aligned}\\ ] ]",
    "this section contains the proof of theorem  [ th : theorem2 ] presented in section  [ sec : closed_loop_certification ] .    assuming that there exist @xmath88",
    "( @xmath182 ) and @xmath87 ( @xmath83 ) selected according to lemma  [ lem : improve_epsilon_t ] , then the following holds : @xmath285 where @xmath207_{j , i}|\\big\\}}}\\alpha_t$ ] .    due to the tightening of the original inequality constraints , @xmath286 , given that , as proved in section  [ subsec : improve_tightening ] ,",
    "the feasible region of the tightened subproblems is inside the one of the original subproblems .",
    "recall that the consolidated prediction satisfies the equality constraints   by construction .",
    "hence , the following holds : @xmath287\\gamma_t\\right\\rangle\\big]\\\\ \\leq & \\mathcal v^ * + 2\\sum\\limits_{t=0}^n\\mathcal r_t\\sqrt{p_t}(\\epsilon_t+\\!\\!\\!{\\ensuremath{\\max\\limits_{j=1,\\ldots , p_t}\\!\\!\\!\\big\\{\\sum_{i=1}^n|[c_{t}]_{j , i}|\\big\\}}}\\alpha_t).\\end{aligned}\\ ] ] where @xmath288\\gamma_t$ ] selects the first @xmath211 components of the vector @xmath89 .",
    "this section contains the proof of lemma  [ lem : update_y ] presented in section  [ sec : closed_loop_certification ] .",
    "let @xmath212 be defined as @xmath213=[(x_{0}^t~\\bar u_{0,\\gamma_0}^t)\\ldots(\\bar x_{n-1,\\gamma_{n-1}}^t~\\bar u_{n-1,\\gamma_{n-1}}^t)~(\\bar x_{n,\\gamma_n}^t))]^t$ ] .",
    "then , @xmath214 at the next problem instance , is given by : @xmath289}}((a+bk_f)\\bar x_{n,\\gamma_n})^t]^t \\end{aligned}\\ ] ]    we can use a similar argument as the one of lemma 4.2 in  @xcite , recalling that @xmath290 for @xmath28 , @xmath291 for @xmath292 , @xmath293 for @xmath28 , @xmath294 for @xmath292 , and @xmath295 according to assumption  [ ass : terminalset ] .",
    "99 s.  d.  cairano , m.  brand , and s.  a.   bortoff , `` projection - free parallel quadratic programming for linear model predictive control '' .",
    "_ international journal of control _ , vol .",
    "86 , no . 8 , pp . 13671385 , 2013 .",
    "m.  k \" ogel and r.  findeisen , `` parallel solution of model predictive control using the alternating direction multiplier method '' , proc . of the _ ifac conference on nmpc",
    "1 , pp . 369374 , 2012 .",
    "s.  boyd et al .",
    "`` distributed optimization and statistical learning via the alternating direction method of multipliers '' . in  _ foundations and trends@xmath296 in machine learning _ , vol .",
    "1 , pp . 1122 , 2011 .",
    "m.  k \" ogel and r.  findeisen , `` fast predictive control of linear , time - invariant systems using an algorithm based on the fast gradient method and augmented lagrange multipliers '' , proc . of the _ ieee cca _ , pp .  780785 , 2011 .",
    "m. rubagotti , p. patrinos and a. bemporad , `` stabilizing linear model predictive control under inexact numerical optimization '' , in _ ieee trans . on automatic control _ ,",
    "59 , no . 6 , pp . 16601666 , 2014 .",
    "i.  necoara , l.  ferranti , and t.  keviczky , `` an adaptive tightening approach to linear model predictive control based on approximation algorithms for optimization '' , _ optimal control applications and methods _",
    ", doi : 10.1002/oca.2121 , http://dx.doi.org/10.1002/oca.2121 , to appear in 2015 .",
    "99 s.  d.  cairano , m.  brand , and s.  a.   bortoff , `` projection - free parallel quadratic programming for linear model predictive control '' . _ international journal of control _ , vol .",
    "86 , no . 8 , pp . 13671385 , 2013 .",
    "m.  k \" ogel and r.  findeisen , `` parallel solution of model predictive control using the alternating direction multiplier method '' , proc . of the _ ifac conference on nmpc",
    "1 , pp . 369374 , 2012 .",
    "s.  boyd et al .",
    "`` distributed optimization and statistical learning via the alternating direction method of multipliers '' . in  _ foundations and trends@xmath296 in machine learning _",
    "1 , pp . 1122 , 2011 .      m.  k \" ogel and r.  findeisen , `` fast predictive control of linear , time - invariant systems using an algorithm based on the fast gradient method and augmented lagrange multipliers '' , proc . of the _ ieee cca _ , pp .  780785 , 2011 .",
    "m. rubagotti , p. patrinos and a. bemporad , `` stabilizing linear model predictive control under inexact numerical optimization '' , in _ ieee trans . on automatic control _ , vol .",
    "59 , no . 6 , pp . 16601666 , 2014 .",
    "i.  necoara , l.  ferranti , and t.  keviczky , `` an adaptive tightening approach to linear model predictive control based on approximation algorithms for optimization '' , in _ optimal control applications and methods _",
    ", doi : 10.1002/oca.2121 , 2015 ."
  ],
  "abstract_text": [
    "<S> we propose a parallel adaptive constraint - tightening approach to solve a linear model predictive control problem for discrete - time systems , based on inexact numerical optimization algorithms and operator splitting methods . </S>",
    "<S> the underlying algorithm first splits the original problem in as many independent subproblems as the length of the prediction horizon . </S>",
    "<S> then , our algorithm computes a solution for these subproblems in parallel by exploiting auxiliary tightened subproblems in order to certify the control law in terms of suboptimality and recursive feasibility , along with closed - loop stability of the controlled system . compared to prior approaches based on constraint tightening , our algorithm computes the tightening parameter for each subproblem to handle the propagation of errors introduced by the parallelization of the original problem . </S>",
    "<S> our simulations show the computational benefits of the parallelization with positive impacts on performance and numerical conditioning when compared with a recent nonparallel adaptive tightening scheme . </S>"
  ]
}