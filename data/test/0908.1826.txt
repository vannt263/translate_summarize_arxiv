{
  "article_text": [
    "parse signal recovery problem is the reconstruction of such signals with characteristic of `` sparsity '' from a set of nonadaptive linear measurements .",
    "it has great potential of application on various engineering fields such as coding and information theory , signal processing , machine learning and others ( see papers in website @xcite and reference herein ) .",
    "sparse signals contains much less information than their ambient dimension suggests .",
    "most of entries in its vector representation are zero ( or negligible ) .",
    "so it is possible to reconstruct original signal approximately or even accurately using only a small number of linear measurements .",
    "the measurements are of the form @xmath0 where @xmath1 is some @xmath2 measurement matrix , @xmath3 and @xmath4 is original signal . clearly the process of sparse signal recovery could be formulated as solving undetermined linear algebraic equation @xmath5 , where @xmath6 is measurement data .",
    "it is well - known that undetermined equation has infinite solutions in general .",
    "but if we focus on `` sparse '' solution only , situation will be different .",
    "although the operation for finding the most sparse solution of undetermined linear equation is np - hard commonly@xcite , theoretical work in compressed sensing has shown that for certain kinds of measurement matrices , it is possible when the number of measurements m is nearly linear in the sparsity of original signal @xcite@xcite .",
    "the two major algorithmic approaches to sparse signal recovery are based on @xmath7-minimization and on greedy methods ( matching pursuit ) .",
    "finding the most sparse solution of undetermined linear equation is a @xmath8 optimization problem : @xmath9 it could be solved by @xmath7 relaxation for some measurement matrices @xmath1@xcite . that is , solving ( [ label1 ] ) is equivalent to solving the following @xmath7 optimization problem @xmath10 where @xmath11 for @xmath12 .",
    "recently , more stronger sufficient condition called restricted isometry property ( rip ) on measurement matrix @xmath1 to guarantee the equivalence of ( [ label1 ] ) and ( [ label2 ] ) was also proposed @xcite .",
    "it was widely accepted that @xmath7-minimization ( [ label2 ] ) was normal path to complete sparse signal recovery .",
    "( [ label2 ] ) is essentially a linear programming problem and technique of convex optimization could be utilized to solve it effectively @xcite .",
    "the @xmath7-minimization method provides uniform guarantees for sparse recovery .",
    "once the measurement matrix @xmath1 satisfies the restricted isometry condition , this method works correctly for all sparse signals @xmath4 .",
    "the method is also stable , so it works for non - sparse signals such as those which are compressible , as well as noisy signals .",
    "however , the method is based on linear programming , and there is no strongly polynomial time algorithm in linear programming @xcite .",
    "but its efficiency was questionable and most popular software package of convex programming , such as cvx@xcite , is hard to be used in practical application for its low rate of convergence , especially when dimension of target signal is large .",
    "on the other hand , greedy algorithms are quite fast , both theoretically and experimentally .",
    "it runs by iterating in general .",
    "typically , on each iteration , the inner products of residue vector @xmath13 with the columns of measurement matrix @xmath1 is computed and a least squares problem is solved to obtain the estimation of original signal on this iteration .",
    "it is hoped that the convergence of iterating could be ensured and the estimator could tend to original signal in fewer steps @xcite .",
    "the typical case of greedy algorithms for sparse recovery includes orthogonal matching pursuit ( omp ) @xcite , regularized orthogonal matching pursuit ( romp ) @xcite and compressive sampling matching pursuit ( cosamp ) @xcite .",
    "it was shown that omp recovered the sparse signal with high probability and had great speed , but it would fail for some sparse signals and matrices @xcite .",
    "the development of romp provides a greedy algorithm with uniform guarantees for sparse recovery , just as that provided by @xmath7-minimization method .",
    "furthermore , cosamp improves upon these results and provides rigorous runtime guarantees . however , there are one disadvantage for these two algorithms .",
    "firstly , sparsity level must be presented as prior parameter for algorithms .",
    "but it is unknown in most practical scenario and must be guessed in advance .",
    "once the estimated sparsity level has large difference with actual one , the error of algorithm will increase evidently ( although this error could be analyzed theoretically @xcite ) .",
    "the problem becomes more severe in noisy environment .",
    "romp and cosamp ca nt adapt their running process to noise condition when noise is presented . actually , noise is inevitable in engineering problem and target signal of our recovery algorithms is always buried in it .",
    "there exist few `` pure sparse '' signal in real world .",
    "because the dimension of target signal is unknown , it is always given with some margin to avoid possible missing .",
    "hence it is hard to extract target signal without including certain amount of noise .",
    "this not only has influence on accuracy of algorithm , but also reduce the speed of convergence for algorithms .",
    "in fact , some calculation is carried through to estimate noise , however , which is useless at all .",
    "a new greedy algorithm for sparse recovery is presented in this paper .",
    "compared with romp and cosamp , our new algorithm need no any prior information on sparsity level of target signal .",
    "furthermore , it is a kind of `` adaptive '' algorithm which can inspect the existence of noise and adjust the halting condition automatically based on detailed state of noise .",
    "besides that , it has uniformly guarantee and good efficiency , just as romp or cosamp .",
    "hence our new algorithm is a better choice when signal with unknown sparsity level is to be extracted ( such as compressible signal ) under noisy background .",
    "this paper is organized as follows : in section 2 we introduce our new algorithm .",
    "section 3 describes some consequences of the restricted isometry property that pervade our analysis . the convergence of theorem is also established for sparse signals in sections 3 .",
    "practical consideration for algorithm implementation is provided in section 4 . empirical performance and some numerical experiment",
    "is described in section 5 .",
    "finally , section 7 presents overall conclusion .",
    "the most difficult and important part of signal reconstruction is to identify the locations of the components in the target signal .",
    "the common approach adopted by most greedy algorithms is `` local approximating '' , that is , computing the inner products between measurement vector @xmath6 and columns of measurement matrix @xmath1 .",
    "we will obtain observation vector @xmath14 and use @xmath15 as `` local approximation '' ( or `` proxy '' ) of target signal @xmath4 .",
    "note that @xmath1 is a dictionary and @xmath16 is sparse , so @xmath6 has a sparse representation with respect to the dictionary .",
    "it is reasonable that only a few entries of @xmath15 are remarkable , which imply the locations of the components of @xmath4 , and most of its entries are comparatively small .",
    "of course , the precondition for argument above is that @xmath1 must satisfy some condition such as rip . intuitively , given sparsity level @xmath17 of @xmath4 , every @xmath17 columns form approximately an orthonormal system",
    ". therefore , every @xmath17 coordinates of the observation vector u look like correlations of the measurement vector @xmath6 with the orthonormal basis and therefore are close in the euclidean norm to the corresponding coefficients of @xmath4 .",
    "popular greedy algorithms , including omp , romp and cosamp , pay much attention to observation vector @xmath15 and build their estimator of location of components in @xmath4 based on @xmath15 .",
    "omp uses one biggest coordinate of @xmath15 .",
    "it is argued that using only the biggest could nt provides uniformly guarantee .",
    "so romp makes use of the @xmath17 biggest coordinates of @xmath15 , rather than just biggest one , and take a further step of regularization to improve the performance of algorithm .",
    "it should be noted that sparsity level @xmath17 is always unknown .",
    "cosamp employs more coordinates of @xmath15 , the @xmath18 biggest , to avoid the possible leakage of component in @xmath4 .",
    "but @xmath17 must be guessed to be input in romp or cosamp as important parameter .",
    "if guessed @xmath17 is smaller than its true value , correct result ca nt be found ; on the other hand , if guessed @xmath17 is set to a very large value ( maybe much larger than true value ) to ensure that all of entries of @xmath4 will enter the view of algorithms , certain amount of noise will presented in our calculation inevitably .",
    "how can we make a good guess for sparsity level @xmath17 without any knowledge on its true value ?",
    "our new approach , named adaptive orthogonal matching pursuit ( amop ) , chooses appropriate number of biggest entries of observation vector @xmath15 by studying the fine feature of @xmath15 . at each step , @xmath15 is computed by @xmath19 where @xmath13 is the residue vector of last step .",
    "unlike other algorithms , amop determines the estimation for @xmath17 by analyzing the trend of entries in @xmath15 arranged by descend order of their amplitude .",
    "that is , relative amplitude difference of adjacent elements in above queue is calculated and a threshold is set .",
    "once the relative amplitude difference between @xmath20th and @xmath21th element in ordered queue of entries in @xmath15 exceed threshold , @xmath20 will be chosen as estimation of @xmath17 .",
    "detailed description of amop is proposed as follows :    [ label8 ]    * * input : * measurement matrix @xmath22 , measurement vector @xmath23 , threshold @xmath24 , @xmath25 and @xmath26 .",
    "* let @xmath27 , @xmath28 . *",
    "calculate @xmath19 and @xmath29 . *",
    "arrange @xmath30 by descend order to obtain @xmath31},|u|_{[2]},\\cdots,|u|_{[n]}),\\label{label3}\\ ] ] * determine index @xmath20 as follows , let @xmath32 , @xmath33}-|u|_{[k]}}{|u|_{[k]}}>t*\\beta\\right.\\right\\},\\ ] ] * if @xmath34 and @xmath35 , set @xmath36 ; else @xmath37 , goto step ( 4 ) ; * update the set of indices by @xmath38,\\cdots,[k]\\}$ ] .",
    "* solve least square problem @xmath39 where @xmath40 is a submatrix of @xmath1 composed of its columns with index in @xmath41 .",
    "* calculate the residue vector of @xmath42 . *",
    "if @xmath43 , output @xmath44 and @xmath41 , stop ; else go to step ( 2 ) .    as input ,",
    "the aomp algorithm requires two adjustable parameter @xmath24 and @xmath25 besides matrix @xmath1 and measurement vector @xmath6 .",
    "but it does nt need sparsity level of target signal @xmath4 anymore , unlike romp and cosamp .",
    "it can be extracted incidentally along with running of algorithm .",
    "furthermore , the number of components selected in the step ( 4 ) is determined by algorithm itself automatically .",
    "it is easy to understand that this number is critical for performance of algorithm .",
    "any manual setting will introduce extra error when mismatch between prescribed value and actual situation of data exists .",
    "so it is very necessary to let greedy algorithm of sparse recovery be adaptive , just as aomp .",
    "step ( 5 ) should be noted that it give amop algorithm more flexibility and stability .",
    "if threshold @xmath24 is set too large so that too much coordinates was selected in one iteration , algorithm is prone to degrade or crash .",
    "for this we build a upper bound in step ( 5 ) to prevent the crazy growing of number of chosen components .",
    "if this bound is exceed , threshold @xmath24 will be adjusted to smaller value to increase the possibility of components in @xmath30 in step ( 3 ) to satisfy the condition in step ( 4 ) .",
    "the importance of step ( 5 ) is also illustrated in following section on analysis of convergence .",
    "there are two kinds of iterative invariant of greedy algorithm for sparse recovery deduced in the convergence analysis for romp and cosamp respectively . as to cosamp , assume the sparsity level @xmath45 is preliminary and @xmath46 where @xmath47 is s - sparse approximation for @xmath4 and @xmath48 is additive noise , the following assertion could be proved @xcite .",
    "@xmath49 where @xmath50 is result of pruning step in cosamp .",
    "hence it is forced to be s - sparse .",
    "so this kind of iterative invariant is not suitable for analysis of amop because the sparsity level of intermediate result at each step in amop is nt fixed .",
    "however , the iterative invariant in romp simply concerns with the percentage of the coordinates in the newly selected set belong to the support of target signal @xmath4 .",
    "it is argued that the ratio above is nt lower than 50% with the help of regularization step @xcite .",
    "we will follow the idea in @xcite to derive our result on convergence of amop .      by induction on the iteration of aomp , we study the gain in each iteration . losing no generality ,",
    "suppose sparsity level of target signal @xmath4 be @xmath51 , and @xmath20 coordinates is selected eventually in this iteration .",
    "then its percentage of energy of first @xmath20 components in queue ( [ label3 ] ) is @xmath52}^2}{\\sum_{n=1}^k|y|_{[n]}^2+\\sum_{n = k+1}^k|y|_{[n]}^2}\\ ] ] for the descend order of queue ( [ label3 ] ) , we have @xmath53}^2}{\\sum_{n=1}^k|y|_{[n]}^2+(k - k)|y|_{[k]}^2}\\nonumber\\\\ & \\geq&\\frac{k|y|_{[k]}^2}{\\sum_{n=0}^{k-1}(1-t)^{-2n}|y|_{[k]}^2+(k - k)(1-t)^2|y|_{[k]}^2}\\nonumber\\\\ & = & \\frac{k}{\\sum_{n=0}^{k-1}(1-t)^{-2n}+(k - k)(1-t)^2}\\nonumber\\\\ & = & \\frac{k}{\\frac{1-(1-t)^{-2k}}{1-(1-t)^{-2}}+(k - k)(1-t)^2}\\label{label4}\\end{aligned}\\ ] ] it is easily seen that [ label4 ] achieves its minimum at @xmath54 , that is @xmath55 here @xmath54 means only the largest coordinates was chosen , which is just the choice of omp . so omp is a special case of amop . according to @xcite",
    "lemma 3.6 , a large portion of energy of unidentified part of target signal would be locked by queue ( [ label3 ] ) and certain amount of energy would be reserved by `` regularization step '' in romp by @xcite lemma 3.8 . in amop ,",
    "the `` regularization step '' is replaced by choosing @xmath20 largest coordinates , so more energy would be identified in amop than in omp because more than one coordinates would be chosen in amop generally .",
    "the ability of locking uncovered energy of target signal for amop and romp is compared in fig.[fig1 ]    it is shown that ability of amop is superior to that of romp when sparsity of target signal is small .",
    "although the advantage becomes vague when sparsity grows , amop still has relatively good performance considering more coordinates would be chosen and percentage of actual identified energy would be larger than ( [ label5 ] ) .      in @xcite and @xcite",
    ", the correctness of support of solution for oga algorithm under different noise scenario were analyzed .",
    "mutual coherence of matrix @xmath1 , overcomplete dictionary system , was the key parameter for performance of greedy algorithm in discussion therein .",
    "here we will give analogous results for amop under noisy condition based on ric ( restricted isometry constant)@xcite of @xmath1 .",
    "in practice the noise is unavoidable and it is always assumed that ideal noiseless signal @xmath56 has sparse representation @xmath57 , where the support of @xmath58 is very small .",
    "what we can observe is noisy version @xmath59 where @xmath60 .",
    "suppose @xmath58 be solution of @xmath61 @xmath62 be final result of amop , and @xmath63 we argue that @xmath64 under certain conditions on value distribution of target signal .",
    "that is , the correctness of support of solution for amop can be guaranteed even in noisy environment .",
    "if target signal @xmath65 satisfy @xmath66 here matrix @xmath1 is supposed to has @xmath26-order restricted isometry constant @xmath67 , @xmath26 is twice of sparsity level of target signal @xmath58 .",
    "then @xmath64 holds throughout the iteration process of amop unless all the coordinates in @xmath68 were chosen .",
    "we proceed by induction .",
    "@xmath64 holds at beginning of step 1 in amop initially for @xmath69 .",
    "assume it is true at beginning of step 1 in given iteration , we prove it is still true at beginning of step 1 in next iteration",
    ". consider case of @xmath70 , we have @xmath71 it is trivial that @xmath72 because @xmath73 is the solution of least square optimization on step 7 in amop , @xmath74 multiply @xmath75 on two side of [ label7 ] , @xmath76 we have @xmath77 where @xmath78 is the identity matrix .    for @xmath79 , the norm of @xmath80 is estimated and bounded .",
    "firstly , because @xmath81 is the projection matrix of orthogonal complement of subspace spanned by @xmath82 , its 2-norm is 1 .",
    "so @xmath83 secondly , @xmath84 because @xmath85 and @xmath86 , according to @xcite , proposition 3.2 , we have @xmath87 and @xmath88 on the other hand , according to definition of ric , we obtain @xmath89 hence @xmath90 thirdly , by analogous deduction , if @xmath91 , @xmath92 summarize the results above , we have for @xmath91 , @xmath93    lower bound for @xmath94 is considered similarly . for @xmath95 , @xmath96 hence @xmath97 if target signal @xmath98 satisfy ( [ label11 ] ) , we have @xmath99 that is to say , @xmath100 will hold throughout the iteration process of amop unless all the coordinates in @xmath68 were chosen .",
    "it is argued that value distribution of target signal , power of noise and ric of matrix @xmath1 are all critical to performance of greedy algorithm .",
    "the proposition above gives a general condition for correctness of support of solutions for a large class of greedy algorithms ( not just amop ) which use inner product between residue @xmath13 and dictionary vectors ( columns of @xmath1 ) to obtain information of support of target signal .",
    "it seems that condition ( [ label11 ] ) is too restricted .",
    "but it is easy to see from ( [ label11 ] ) that `` dynamic scope '' of target signal ( that is , the norm difference between the elements with maximal and minimal norm ) depends on ric @xmath67 of matrix @xmath1 and noise power @xmath25 .",
    "consider the requirement on ric in romp , which is @xmath101 with @xmath45 is sparsity level of target signal according to @xcite , theorem 3.1 , we write ( [ label11 ] ) as @xmath102 with maximum is normalized to 1 . it is depicted in fig.[fig2 ] for noise level is 0.1(snr is 20db ) .",
    "when sparsity level is small , target signals with considerable dynamic scope are guaranteed to have good performance in greedy algorithms .",
    "the restriction on dynamic scope of target signal becomes tighter gently when sparsity level increases .",
    "the actual number of chosen coordinates in iteration step of amop in practical scenario is smaller than sparsity level in general .",
    "so amop could choose correct coordinates in most cases .",
    "this assertion will be illustrated further in numerical experiments .",
    "for efficient implementation of amop , it is important to design a appropriate computational scheme with low complexity and good numerical stability to calculate the solution of least square problem in step 7 in amop . it should be noticed that amop is incremental , that is , in each iteration some new coordinates were selected and none of previously chosen coordinates was excluded , unlike cosamp .",
    "so it is possible to construct a recursive algorithm to solve the least square problem .",
    "assume on the first iteration several coordinates were chosen and denote the set of corresponding columns of matrix @xmath1 as @xmath103 .",
    "then observation vector @xmath6 would be linearly approximating with vectors in @xmath103 and coefficients @xmath104are computed by solving the following equation @xmath105 ordinary solver with good numerical performance such as qr decomposition or singular value decomposition could be used to compute @xmath104 . from geometrical point of view , calculation of @xmath104 is equivalent to project @xmath6 onto subspace spanned by @xmath103 .",
    "we wrote it intuitively as @xmath106 on the next iteration , a set @xmath107 of columns of matrix @xmath1 with respect to newly chosen coordinates would be added to least square regression .",
    "the projection became @xmath108 it is well - known that orthogonalization could simplify the calculation for projection onto subspace spanned by two mutually orthogonal subspace could be regarded as sum of projections on each one .",
    "so @xmath107 was written as @xmath109 where @xmath110 was projection of @xmath107 onto @xmath103 and @xmath111 is orthogonal to @xmath103 .",
    "hence @xmath112 this could be accomplished with gram - schmidt orthogonalization procedure . without loss of generality ,",
    "suppose @xmath113 then @xmath114 using following procedure @xmath115 where @xmath116 denotes the inner product of vectors in euclidean space .",
    "we can obtain @xmath117 the projection in ( [ label12 ] ) could be written as @xmath118 the numerical stability of gram - schmidt orthogonalization procedure could be improved further @xcite . instead of computing the vector @xmath119 as ( [ label13 ] ) , it is computed as @xmath120 this approach gives the same result as the original formula in exact arithmetic , but it introduces smaller errors in finite - precision arithmetic .",
    "there are one points worth mentioning .",
    "although other orthogonalization algorithms such as householder transformations or givens rotations are more stable than the stabilized gram - schmidt process , they produce all the orthogonalized vectors only at the end . on the contrary , the gram - schmidt procedure produces the @xmath121th orthogonalized vector after the @xmath121th iteration and this makes it the only choice for iterative algorithm like amop .",
    "amop was designed to be a practical method for sparse signal recovery .",
    "the main barrier for algorithm efficiency is least square problem in step 7 of amop .",
    "using recursive orthogonalization procedure above could mitigate computational burden of amop dramatically .",
    "furthermore , the orthogonalization technique has the additional advantage that they only interact with the matrix @xmath1 through its action on vectors .",
    "in fact , it only concern with the inner products and additions of columns of matrix @xmath1 .",
    "it follows that the algorithm performs better when this sampling matrix has a fast matrix - vector multiply , such as on parallel computational platforms . on the other hand ,",
    "less memory consumption is another advantage of recursive orthogonalization based least square .",
    "in fact , direct method such as svd and qr have storage cost @xmath122 , where @xmath20 is the number of chosen coordinates in each iteration and @xmath123 is row number of matrix @xmath1 .",
    "it is too huge for large scale problems .",
    "but for amop , only one vector need be put in memory in recursive orthogonalization calculation and storage cost is @xmath124 .",
    "it is more suitable for implemented with vlsi circuit .",
    "we estimate the time complexity of main steps in amop as follows :    * * step 2 : * in this step , the inner products of residue vector and columns of matrix @xmath1 is computed as proxy for support of target signal .",
    "its cost is bounded by matrix - vector multiply @xmath125 , which is @xmath126 with standard multiply operation or @xmath127 for fast matrix - vector multiply . *",
    "* step 3 : * according to standard textbook on algorithms @xcite , the expected time for selecting largest @xmath45 entries in vector with dimension @xmath128 is @xmath129 .",
    "using efficient schemes such as quicksort or heapsort , a fully sorting of vector could be completed with expected time cost @xmath130 and largest @xmath45 entries could be selected directly , which is faster n some situation . * * step 4 & 5 : * certain amount of support of target signal would be identified in these two steps .",
    "although sometimes the threshold needs to be adjusted according to step 5 and several cycles of operations may be necessary , the total cost is still @xmath131 . *",
    "* step 7 : * the main advantage of amop is recursive orthogonalization based implementation of least square problem .",
    "inner products of vectors are involved in orthogonalization and occupy much of computational resource which can be implemented efficiently by matrix - vector multiply .",
    "the cost is @xmath132 with standard multiply operation or @xmath127 for fast matrix - vector multiply .",
    "table 1 summarizes the discussion above in standard multiply operation and fast matrix - vector multiply with cost @xmath133 , respectively .",
    ".time complexity of amop [ cols=\"^,^,^ \" , ]     storage cost for amop is also considered for showing its practicability . aside from the storage required by the sampling matrix ,",
    "amop algorithm constructs only one vector of length n as the signal proxy .",
    "the sample vectors have length m , so they require @xmath124 storage .",
    "the signal approximations require at most @xmath134 storage .",
    "similarly , the index sets that appear require only @xmath134 storage .",
    "the total storage is at worst @xmath135 .",
    "in this section some numerical experiments were conducted to illustrate the performance of signal recovery of aomp .",
    "there are three factors to be considered in numerical testing of amop : construction of @xmath1 matrix , value distribution of target signal and snr condition which will be examined in our experiments .",
    "the property of measurement matrix @xmath1 is critical to performance of any greedy algorithm for sparse recovery . as indicated in section on theoretical analysis ,",
    "its ric has direct influence on probability of recovery of algorithms . here",
    ", several kinds of matrix @xmath1 were built and utilized to test the performance of amop , including well - known gaussian random matrix , bernoulli random matrix and random fourier matrix .",
    "the target signal was set to be flat and no noise was added in , then 500 independent trials were performed .",
    "figure [ fig3]-[fig5 ] depicts the percentage ( from the 500 trials ) of sparse flat signals that were reconstructed exactly when @xmath136 gaussian random matrix was chosen as measurement matrix @xmath1 .",
    "this plot was generated with @xmath137 for various levels of sparsity @xmath51 .",
    "the horizontal axis represents the number of measurements @xmath123 , and the vertical axis represents the exact recovery percentage .    as comparison ,",
    "recovery percentage of algorithm cosamp is also given under the same setting .",
    "standard cosamp needs sparsity of target signal as its important prior knowledge and it is widely regarded as one of main drawbacks of cosamp . in our experiment ,",
    "sparsity of target signal was given to cosamp as input parameter to guarantee the power of cosamp to be exploited fully .",
    "it should be noted that even the sparsity of target signal is known beforehand ( which is impossible in practice ) , recovery percentage of cosamp is lower than that of amop .",
    "especially when sparsity was relatively large , performance of cosamp degenerated very rapidly .",
    "on the contrary , the behavior of amop was very stable . according to well - known theoretical result of compressed sensing , for gaussian random measurements matrix @xmath1 , if row number @xmath123 , column number @xmath128 and sparsity @xmath51 satisfies @xmath138 where @xmath139 is a constant independent of @xmath51 , then the probability of recovery failure is exponentially small @xcite .",
    "our experiment result indicates that for amop , the value of constant @xmath139 is about @xmath140 for gaussian measurement matrix .",
    "figure [ fig6]-[fig8 ] depicts corresponding result for bernoulli random measurement matrix , which is analogous to gaussian case .",
    "it had been proved that condition ( [ label20 ] is also sufficient for overwhelming probability of successful recovery for binary bernoulli measurement matrix @xcite .",
    "it is observed that the constant @xmath139 for amop in bernoulli case is probably the same as that in gaussian case .",
    "figure [ fig9]-[fig11 ] depicts corresponding result for fourier random measurement matrix .",
    "somewhat surprisingly , it is similar to that of gaussian and bernoulli case . to our knowledge",
    ", the best known bounds on size of measurements in fourier case is given by @xcite @xmath141 which is conjectured to be the same as [ label20 ] @xcite but there exists no strict theoretical proof until now .",
    "our experiment result verified this conjecture in some extent indirectly .",
    "pure flat signal is rarely seen in practical engineering application .",
    "so it is necessary to investigate the performance of sparse recovery algorithms on non - flat target signal .",
    "there are two cases to be studied .",
    "one is piecewise flat signal which is common in various fields of imaging , such as optical , microwave and magnetic resonance .",
    "the result is depicted in figure [ fig12 ] to [ fig14 ] . here the measurement matrix is fixed to gaussian random matrix .",
    "recovery percentage of cosamp in our experiment is nt ideal .",
    "especially when the number of non - zero elements of target signal is large , the successful recovery probability of cosamp becomes more and more ignorable .",
    "the capability of cosamp is doubtful in this setting . on the contrary ,",
    "the behavior of amop is very robust when value distribution of target signal is changed .",
    "furthermore , the constant @xmath139 in [ label20 ] is approximately equal to that in flat signal setting . although it is predicted theoretically that constant @xmath139 only depends on the construction of measurement matrix @xmath1 , not rely on nature of target signal , it is common in practice that the detailed value distribution of target signal certainly has influence on performance of recovery algorithms .",
    "our experimental result indicates that the actual behavior of amop coincides with theoretical conclusion perfectly .",
    "this argument is confirmed by result for the other case of non - flat target signal . here",
    "target signal is chosen as compressible signal which is frequently presented in orthogonal representation of signal , such as discrete cosine transform and wavelet transform , and data compression .",
    "two kinds of compressible signal are analyzed in our experiment , one is exponential signal , @xmath142 the other is polynomial signal @xmath143 for brevity , only result for exponential signal is depicted in [ fig15 ] and corresponding curve for cosamp is omitted .",
    "we also performed the same experiment for polynomial compressible signals and found the results very similar to those in figure [ fig14 ] .",
    "random noise was added in target signal to test the performance of sparse recovery algorithms in noisy environment .",
    "measurement matrix is fixed to fourier random matrix and target signal is set to flat .",
    "the sparsity of target signal is fixed to 20 and size of measurements @xmath123 is fixed to 200 .",
    "the relative error of amop and cosamp in gaussian white noise with various level is depicted in figure [ fig16 ]    the capacity of amop in noisy environment is satisfactory .",
    "relative recovery error could be controlled within @xmath144 when snr is about 10db .",
    "even when snr is as low as 5db , relative error of amop still could be governed within @xmath145 .",
    "though the error curve rise very acutely in very low snr region , it is shown that amop works normally in most noisy environment .",
    "on the other hand , the relative error of cosamp keeps on a high level when noise is presented .",
    "when snr was increased , it did nt exhibit the obvious trend of decreasing . with the language of statistics , cosamp is nt consistent estimator .",
    "figure [ fig17 ] illustrates the advantage of amop in noisy environment from other viewpoint . here",
    "snr is fixed to 10db , the error with various size of measurements is plotted .",
    "it is observed that if number of measurements @xmath17 is small , the performance of amop is heavily abnormal .",
    "in fact , relative error of amop is even higher than cosamp when @xmath17 is lower than 100 .",
    "but it falls abruptly when @xmath123 increases while that of cosamp remains in narrow range .",
    "when @xmath123 is larger than 100 , the relative error of amop tends to be stable .",
    "it is well controlled with in @xmath144 , which is similar to figure [ fig16 ] .",
    "we would build a measurement matrix @xmath1 with spatial - temporal basis vectors , which is key component in theory and computation of stap ( space - time adaptive processing ) , to investigate the potential of sparse recovery algorithms to be applied in field of modern radar and communication engineering . here",
    "@xmath1 is set to @xmath146,\\ ] ] and @xmath147^t,\\nonumber\\end{aligned}\\ ] ] where @xmath148 and @xmath149 denotes doppler and spatial frequency , respectively .",
    "unlike fourier matrix , the construction of spatial - temporal matrix @xmath1 is more complex .",
    "the phase of elements in @xmath1 composed of two parts , one is contributed by doppler frequency and the other by spatial frequency .",
    "it lead to following consequence : different @xmath148 and @xmath149 could be combined to form the same ( or approximately equal ) phase . in other words ,",
    "strong correlation exists in different column vectors in @xmath1 , which corresponding to various points far away with each other on spatial - temporal plane .",
    "so restricted isometric constant of @xmath1 is conjectured to be relatively large .",
    "it is a challenge for sparse recovery algorithm to be feasible when measurement matrix @xmath1 is chosen as spatial - temporal matrix .    generally speaking",
    ", the support ( `` position '' of non - zero elements ) of target signal is much important than its detailed value .",
    "it is indeed true in practical engineering discipline .",
    "for example , in radar stap processing , sparse recovery is utilized to estimate the energy distribution of clutter and interference on spatial - temporal plane from sample data directly .",
    "because echo of clutter and interference is much stronger than radar target , the detailed amplitude and phase of clutter and interference is nt crucial .",
    "as long as the accurate support ( `` position '' ) of clutter and interference on spatial - temporal plane is found out , we can design the efficient filter to suppress clutter and interference effectively .",
    "hence the most important feature of sparse recovery algorithm applied to stap processing is detecting the support of target signal with great precision .",
    "the experiment was performed to test the ability of amop and cosamp to detect signal support .",
    "measurement matrix @xmath1 was set to @xmath150 spatial - temporal matrix as [ label21 ] .",
    "target signal is noise - free flat signal and its sparsity is fixed to 20 .",
    "figure [ fig18 ] depicts the average result from 100 trials .",
    "it is clear that amop is much superior to cosamp when applied to stap processing because it could detect a majority of target support with no error while cosamp could only find very few .",
    "but even so , the accuracy of algorithms for support , whether amop or cosamp , can hardly satisfy the requirement of practical stap processors . due to ultra - low signal clutter ratio ( generally lower than -50db ) ,",
    "missing four or five frequency points on spatial - temporal plane would lead to very high false alarm rate and the performance of radar would degrade heavily .",
    "so we should detect as much target support as possible to minimize false alarm rate .",
    "if tiny error is allowed , the behavior of sparse recovery algorithms becomes better , as depicted in figure [ fig19 ]    it is easily seen that if the `` position '' detected having difference 1 with true `` position '' of target signal is allowed to be counted , the average size of detected target support for amop increases by about 2 and that of cosamp is still negligible .",
    "it accounted for that some support of target signal missed by amop was nt really missed .",
    "that is to say , their neighborhood , which corresponding to the points adjacent to them on spatial - temporal plane , were discovered instead .",
    "this is a good news for high performance filter to suppress clutter could still be designed with amop and carefully chosen notch , without losing much resolution and snr .",
    "figure [ fig20 ] depicted the case of error 2 .",
    "the behavior of amop continued to be made better and approach the best .",
    "it seems that it make little sense to enlarge error tolerance further .",
    "in this paper , a novel greedy algorithm for sparse recovery , called amop , was given and examined .",
    "its performance was studied by theoretical analysis of simulation experiment .",
    "the motivation of this algorithm is two obvious drawbacks in popular methods , such as cosamp : need of sparsity of target signal as prior knowledge , and weak ability of working in noisy environment . with well - designed algorithmic steps",
    ", amop can extract the information on sparsity of target signal adaptively and sense the nature of target signal automatically .",
    "it can recover the detail value of target signal with very high precision with little prior knowledge .",
    "its validity is illustrated by strict deduction .",
    "fine stability of performance under random noise is another advantage of amop .",
    "furthermore , its robustness for various setting of target signal , flat or compressible , and construction of measurement matrix , such as spatial - temporal matrix , were also shown by thorough numerical experiment .",
    "it is argued that amop is a excellent greedy algorithm for sparse recovery and has great potential of widely utilization on signal processing .    1 compressed sensing webpage : http://www.dsp.ece.rice.edu/cs .",
    "m.r . garey and d.s .",
    "johnson , computers and intractability , a guide to the theory of np - completeness , w. h. freeman and company , new york , 1979 e. candes .",
    "compressive sampling . in proc .",
    "international congress of mathematics , volume 3 , pages 1433 - 1452 , madrid , spain , 2006 .",
    "e. candes and m. wakin , an introduction to compressive sampling .",
    "ieee signal processing magazine , 25(2 ) , pp .",
    "21 - 30 , march 2008 .",
    "d. l. donoho and p. b. stark .",
    "uncertainty principles and signal recovery .",
    "siam j. appl .",
    ", 49(3):906 - 931 , june 1989 .",
    "candes and t. tao .",
    "decoding by linear programming , ieee trans .",
    "theory 51 , 4203 - 4215 , 2006 . s. boyd , and l. vandenberghe , convex optimization , cambridge university press , 2004 d. needell , r. vershynin , uniform uncertainty principle and signal recovery via regularized orthogonal matching pursuit , foundations of computational mathematics , doi : 10.1007/s10208 - 008 - 9031 - 3 cvx official website : http://www.stanford.edu/  boyd / cvx/ d. needell , j.a .",
    "tropp , r. vershynin , greedy signal recovery review , arxiv , 0812:2202 , dec , 2008 j. a. tropp and a. c. gilbert .",
    "signal recovery from random measurements via orthogonal matching pursuit .",
    "ieee trans .",
    "theory , 53(12 ) : 4655 - 4666 , 2007 .",
    "d. needell and j. a. tropp .",
    "cosamp : iterative signal recovery from incomplete and inaccurate samples .",
    "acm technical report 2008 - 01 , california institute of technology , pasadena , july 2008 .",
    "h. rauhut . on the impossibility of uniform sparse reconstruction using greedy methods . to appear , sampl .",
    "theory signal image process . , 2008 .",
    "d. l. donoho , m. elad , and v. n. temlyakov , `` stable recovery of sparse overcomplete representations in the presence of noise , '' ieee trans .",
    "theory , vol .",
    "6 - 18 , jan . 2006 .",
    "tseng , `` further results on stable recovery of sparse overcomplete representations '' , ieee transactions on information theory , vol .",
    "2 , february 2009 .",
    "bau iii , david ; trefethen , lloyd n. , numerical linear algebra , philadelphia : society for industrial and applied mathematics , 1997 . t. cormen , c. lesierson , l. rivest , and c. stein .",
    "introduction to algorithms .",
    "mit press , cambridge , ma , 2nd edition , 2001 .",
    "e. candes and t. tao , near optimal signal recovery from random projections : universal encoding strategies ?",
    "ieee trans . on information theory , 52(12 ) , pp .",
    "5406 - 5425 , december 2006 .",
    "rudelson , m. , vershynin , r. , sparse reconstruction by convex relaxation : fourier and gaussian measurements .",
    "preprint , 2006 .",
    "hao zhang , gang li , huadong meng , a class of novel stap algorithms using sparse recovery technique , arxiv:0904.1313 , apr , 2009"
  ],
  "abstract_text": [
    "<S> greedy algorithm are in widespread use for sparse recovery because of its efficiency . </S>",
    "<S> but some evident flaws exists in most popular greedy algorithms , such as cosamp , which includes unreasonable demands on prior knowledge of target signal and excessive sensitivity to random noise . a new greedy algorithm called amop </S>",
    "<S> is proposed in this paper to overcome these obstacles . </S>",
    "<S> unlike cosamp , amop can extract necessary information of target signal from sample data adaptively and operate normally with little prior knowledge . </S>",
    "<S> the recovery error of amop is well controlled when random noise is presented and fades away along with increase of snr . moreover , amop has good robustness on detailed setting of target signal and less dependence on structure of measurement matrix . </S>",
    "<S> the validity of amop is verified by theoretical derivation . </S>",
    "<S> extensive simulation experiment is performed to illustrate the advantages of amop over cosamp in many respects . </S>",
    "<S> amop is a good candidate of practical greedy algorithm in various applications of compressed sensing .    </S>",
    "<S> compressed sensing , greedy algorithm , sparse recovery , noisy environment . </S>"
  ]
}