{
  "article_text": [
    "let @xmath3}$ ] denote the entropy of a  discrete variable @xmath4 on a  probability space @xmath5 , where @xmath6 is the expectation with respect to @xmath7 , @xmath8 is the binary logarithm , and the variable @xmath9 takes the value @xmath10 for @xmath11 .",
    "we have the mutual information @xmath12 for finite entropies on the right hand side . besides , we have the conditional entropy @xmath13 and the conditional mutual information @xmath14 .",
    "these definitions are generalized to arbitrary random variables , e.g. , in @xcite .",
    "let @xmath15 be a  stationary process on @xmath5 , where @xmath16 .",
    "for its distribution @xmath17 we denote the mutual information between blocks of length @xmath0 as @xmath18 the limiting value of mutual information , called excess entropy , is defined as @xmath19 these quantities are natural measures of dependence for discrete - valued processes @xcite .",
    "we are interested in constructing diverse examples of stationary measures for which @xmath20 where @xmath2 , because certain measures of this kind may be useful for modeling natural language , cf . , @xcite .    mentioning related results ,",
    "let us first consider gaussian processes",
    ". for theses processes the conditional mutual information equals @xmath21 , where function @xmath22 is the partial autocorrelation , cf .",
    "regardless of the alphabet , the mutual information between blocks may be reconstructed from conditional mutual information as @xmath23 thus the asymptotics ( [ limen ] ) holds if and only if @xmath24 . as a  result ,",
    "the construction of processes that satisfy condition ( [ limen ] ) is easy because the sole constraint on partial correlation reads @xmath25 @xcite .",
    "however , a  classical result @xcite says that excess entropy of nonsingular gaussian autoregressive moving average ( arma ) processes is finite , cf . ,",
    "@xcite , ( * ? ? ?",
    "* theorem 9.4.1 ) , ( * ? ? ?",
    "* section 5.5 ) .",
    "some examples of stationary processes for which excess entropy is infinite are also known for discrete - valued processes .",
    "the trivial example for a  countably infinite alphabet is a  process such that @xmath26 does not depend on @xmath27 and @xmath28 .",
    "then we have @xmath29 for any @xmath30 .",
    "the aforementioned construction is impossible for processes over a  finite alphabet . considering those processes , we mention first that asymptotics @xmath31 holds for any bayesian mixture of a  @xmath32-parameter model with a  prior concentrated on a  subset of parameters with bounded fisher information ( * ? ? ?",
    "* theorem 8.3 ) .",
    "similar asymptotics @xmath33 holds for a  binary process constructed by gramss @xcite , cf . , @xcite . the distribution of that process is formed by the frequencies of 0 s and 1 s in the rabbit sequence . as for processes with infinite excess entropy that are mixing ,",
    "bradley @xcite constructed a  binary process which satisfies two conditions , cf .",
    ", @xcite : ( i ) the process is @xmath34-mixing and ( ii ) the restricted measure @xmath35 is singular with respect to the product measure @xmath36 for any @xmath30 ( * ? ? ?",
    "* lemma 3 ) .",
    "the first property implies that the process is mixing in the ordinary ergodic theoretic sense ( * ? ? ?",
    "* volume 1 , chapters 3 and 5 ) .",
    "the second property implies that the excess entropy is infinite .",
    "a  few other examples concern hidden markov chains . by the data processing inequality ,",
    "excess entropy is finite for hidden markov chains with a  finite number of hidden states @xcite . on the other hand ,",
    "if the distribution of ergodic components of a  stationary process has infinite entropy then the process has infinite excess entropy ( * ? ? ?",
    "* theorem 5 ) .",
    "such a  situation may arise for hidden markov chains with a  countably infinite number of hidden states .",
    "( consider for instance a  mixture of periodic processes where the probability of a  period is a  sufficiently slowly decreasing function of the cycle length @xcite . )",
    "a  less trivial example , constructed in @xcite , is a  stationary _ ergodic _ hidden markov chain with infinite excess entropy , a  finite number of output symbols , and a  countably infinite alphabet of hidden states .    in this paper",
    "we will consider another class of processes that are nonergodic , ergodic , or mixing and satisfy condition ( [ limen ] ) .",
    "the construction of these processes is motivated linguistically .",
    "let us first sketch this motivation . in our previous work @xcite",
    ", we have shown that proportionality ( [ limen ] ) implies a  power law which resembles zipf s law for the distribution of words .",
    "namely , product @xmath37 is upper bounded by the expected vocabulary size of an admissibly minimal grammar for the text of length @xmath0 .",
    "it was empirically observed that the latter quantity approximates the number of distinct words for texts in natural language @xcite .",
    "our bound for mutual information and the vocabulary size holds if the alphabet @xmath38 is finite and the process s distribution has finite energy property ( * ? ? ?",
    "* theorem 3 ) .",
    "there is also another linguistically motivated bound for @xmath39 .",
    "that one is a  lower bound .",
    "namely , asymptotics @xmath40 follows from a  hypothesis that texts describe an infinite random object in a  highly repetitive way so that @xmath1 independent facts about the object can be inferred on average from the text of length @xmath0 ( * ? ? ?",
    "* theorem 2 ) .",
    "the goal of this paper is to prove the stronger asymptotics ( [ limen ] ) for processes that were discussed in @xcite and to define a  new model of texts that describe a  random object .",
    "so far , we have considered objects that do not change in time .",
    "this leads to models of texts being nonergodic measures . here",
    ", we will admit objects that evolve slowly .",
    "that leads to models of texts which are mixing measures and still satisfy proportionality ( [ limen ] ) . in this way",
    ", linguistic inspiration contributes to better understanding of yet another problem in information theory .",
    "let us introduce our basic example . throughout this paper",
    ", @xmath15 denotes a  stationary process on @xmath5 with @xmath16 and @xmath41 , where @xmath42 is the set of positive integers . in the series of papers @xcite",
    "we have examined some properties of the following process @xmath15 , called the ( original ) santa fe process in @xcite .",
    "namely , the variables @xmath26 consist of pairs @xmath43 where processes @xmath44 and @xmath45 are independent and distributed as follows .",
    "first , variables @xmath46 are binary and equidistributed , @xmath47 second , variables @xmath48 obey the power law @xmath49 where @xmath2 and @xmath50 is the zeta function .",
    "let us recall that @xmath17 and @xmath51 .",
    "the first new result of this paper is :    [ theoenudp ] the block mutual information @xmath39 for the original santa fe process @xmath15 given by formula ( [ exudp ] ) obeys @xmath52^\\beta }      .",
    "\\end{aligned}\\ ] ]    the calculation of the limit is facilitated by a  decomposition of mutual information between blocks @xmath53 and @xmath54 into a  series of triple information among blocks @xmath53 and @xmath54 and variables @xmath46 .",
    "this decomposition is a  particular property of the santa fe process and some similar measures .",
    "the uncommon construction of process ( [ exudp ] ) can be interpreted in this way .",
    "imagine that the santa fe process is a  sequence of statements which describe a  random object @xmath45 consistently .",
    "each statement @xmath55 reveals both the address @xmath32 of a  random bit of @xmath45 and its value @xmath56 .",
    "observe that the description is repetitive and consistent : if two statements @xmath55 and @xmath57 describe bits of the same address ( @xmath58 ) then they always assert the same bit value ( @xmath59 ) .",
    "it follows hence that variables @xmath46 can be predicted from realization @xmath15 in a  shift - invariant way and therefore the santa fe process is ( strongly ) nonergodic , cf . ,",
    "@xcite , ( * ? ? ?",
    "* definition 1 ) .",
    "now let us introduce an example of a mixing process which satisfies ( [ limen ] ) .",
    "for this goal , we will replace individual variables @xmath46 in the santa fe process with markov chains @xmath60 .",
    "these markov chains will be obtained by iterating a  binary symmetric channel .",
    "subsequently , the following process @xmath15 will be called the generalized santa fe process .",
    "let us put @xmath61 where processes @xmath44 and @xmath60 , where @xmath62 , are independent and distributed as follows .",
    "first , variables @xmath48 are distributed according to formula ( [ zetak ] ) , as before .",
    "second , each process @xmath60 is a  markov chain with marginal distribution @xmath63 and cross - over probabilities @xmath64    a  linguistic interpretation of this process is as follows .",
    "facts that are mentioned in texts repeatedly fall roughly under two types , as mentioned in the discussion of definition 1 in @xcite : ( i ) facts about objects that do not change in time ( like mathematical or physical constants ) , and ( ii ) facts about objects that evolve with a  varied speed ( like culture , language , or geography ) .",
    "the random object @xmath65 described by the original santa fe process does not evolve , or rather , no bit @xmath66 is ever forgotten once revealed . on the other hand , the object @xmath67 described by the generalized santa fe process is a  function of an instant @xmath27 and the probability that the @xmath32-th bit flips at a  given instant equals @xmath68 .",
    "for vanishing cross - over probabilities , the generalized santa fe process collapses to the original process .",
    "as we will establish later in this paper , the generalized santa fe process is mixing for cross - over probabilities different to @xmath69 or @xmath70 .",
    "[ theomixing ] the generalized santa fe process @xmath71 given by formula ( [ exmixing ] ) is mixing for @xmath72 .",
    "the proof consists in noticing that infinite direct products of mixing processes are mixing .",
    "this is an easy generalization of the well known fact for finite products ( * ? ? ?",
    "* chapter 10.1 ) .",
    "we will also demonstrate this fact , which generalizes proposition [ theoenudp ] :    [ theoenmixing ] the block mutual information @xmath39 for the generalized santa fe process @xmath15 given by formula ( [ exmixing ] ) obeys @xmath73^\\beta }      .",
    "\\end{aligned}\\ ] ] the lower limits in particular cases are as follows :    1 .",
    "if @xmath74 then @xmath75^\\beta }        ,      \\end{aligned}\\ ] ] where @xmath76 and @xmath77 is the entropy of binary distribution @xmath78 , @xmath79 2 .   if @xmath80 then @xmath39 obeys ( [ limenudp ] ) .",
    "now let us introduce a  similar ergodic process over a  finite alphabet .",
    "for this goal we use a  transformation of processes over an infinite alphabet into processes over a  finite alphabet that preserves stationarity and ( non)ergodicity and does not distort entropy too much , as we have shown in @xcite .",
    "we call this transformation stationary ( variable - length ) coding .",
    "( the same or a  similar construction has been considered in @xcite . ) it is a  composition of two operations .",
    "first , let a  function @xmath81 , called a  coding function , map symbols from alphabet @xmath38 into strings over another alphabet @xmath82 .",
    "we define its extension to double infinite sequences @xmath83 as @xmath84 where @xmath85 and the bold - face dot separates the @xmath69-th and the first symbol . then for a  stationary process @xmath15 on @xmath5 , where variables @xmath26 take values in space @xmath86",
    ", we introduce process @xmath87 where variables @xmath88 take values in space @xmath89 , as long as the right hand side is a  double infinite sequence almost surely .",
    "the second operation is as follows .",
    "transformation ( [ defy ] ) does not preserve stationarity in general but process @xmath90 is asymptotically mean stationary ( ams ) under mild conditions ( * ? ? ?",
    "* proposition 2.3 ) , which are satisfied in the setting considered further .",
    "then for the distribution @xmath91 and the shift operation @xmath92 there exists a  stationary measure @xmath93 called the stationary mean of @xmath94 @xcite .",
    "it is convenient to suppose that probability space @xmath5 is rich enough to support a  process @xmath95 with the distribution @xmath96 whereas process @xmath90 need not be stationary , process @xmath97 is stationary and will be called the stationary ( variable - length ) coding of @xmath15 .",
    "processes @xmath15 , @xmath90 , and @xmath97 have isomorphic shift - invariant algebras for some nice coding functions , called synchronizable injections ( * ? ? ?",
    "* proposition 3.3 ) .",
    "for example , for the infinite alphabet @xmath41 , let us assume the ternary alphabet @xmath98 and the coding function @xmath99 where @xmath100 is the binary representation of a  natural number @xmath32 stripped of the leading digit @xmath70",
    ". coding function ( [ conjcode ] ) is an instance of a  synchronizable injection .",
    "hence we have the following fact :    [ theoencoded ] let @xmath97 be the stationary coding obtained from applying the coding function ( [ conjcode ] ) to the generalized santa fe process ( [ exmixing ] ) .",
    "process @xmath101 is nonergodic if @xmath102 and ergodic if @xmath72 .",
    "notice , however , that the stationary coding of a  mixing process is not mixing for a  synchronizable coding function in general .",
    "for example , if we take the generalized santa fe process and the coding function @xmath103 , which is also a  synchronizable injection , the stationary coding @xmath97 is not mixing because of periodic oscillations in the realizations of the process @xmath90 .",
    "such regular periods do not arise for the generalized santa fe process and the coding function ( [ conjcode ] ) since variables @xmath104 , where @xmath105 is the length of string @xmath106 , differ from constants and are independent and identically distributed .",
    "thus , we conjecture that the resulted process @xmath95 is mixing for @xmath72 .",
    "now let us consider block mutual information for the stationary coding of the generalized santa fe process .",
    "let us recall that @xmath107 and @xmath108 .",
    "as the last new result , we will show this fact :    [ theoenmixingcoded ] let @xmath97 be the stationary coding obtained from applying the coding function ( [ conjcode ] ) to the generalized santa fe process ( [ exmixing ] ) .",
    "define the expansion rate @xmath109 .",
    "the block mutual information @xmath110 for process @xmath97 satisfies @xmath111^\\beta }      .",
    "\\end{aligned}\\ ] ] the lower limits in particular cases are as follows :    1 .",
    "if @xmath74 then @xmath112^\\beta }        .      \\end{aligned}\\ ] ] where @xmath113 is defined in ( [ abeta ] ) .",
    "2 .   if @xmath80 then @xmath114^\\beta }      .    \\end{aligned}\\ ] ]    proposition [ theoenmixingcoded ] follows from proposition [ theoenmixing ] by the conditional data processing inequality and chernoff bounds . this proposition strengthens inequality @xmath115 which follows for @xmath102 by ( * ? ? ?",
    "* proposition 1.4 ) and ( * ? ? ?",
    "* theorem 2 ) .",
    "the further organization of this paper is as follows .",
    "the rate of mutual information for the original and generalized santa fe processes is discussed in section [ secexcess ] .",
    "the rate of mutual information for the stationary coding is established in section [ secencoding ] .",
    "subsequently , the mixing property for the generalized santa fe process is shown in appendix [ secmixing ] . as an auxiliary result",
    ", we demonstrate that infinite direct products of mixing processes are also mixing .",
    "in this section we evaluate the rate of block mutual information for the santa fe process and its mixing counterpart . the main tool is conditional mutual information for stochastic processes as discussed , e.g. , in @xcite .    here",
    "are some facts about conditional information that will be used , cf .",
    ", @xcite :    1 .",
    "continuity @xmath116 , 2 .",
    "chain rule @xmath117 , and 3 .",
    "equality @xmath118 for @xmath4 and @xmath119 conditionally independent given @xmath120 .",
    "two simple corollaries of the chain rule will be used as well :    1 .",
    "@xmath121 for @xmath122 , where we define triple information @xmath123 2 .",
    "@xmath124 for @xmath4 and @xmath119 independent and conditionally independent given @xmath120 .",
    "the second identity follows from @xmath125 where both @xmath126 and @xmath118 .",
    "now we can evaluate block mutual information @xmath39 for the santa fe processes .",
    "the case of the original santa fe process is simpler and will be considered separately to guide the reader through the more complicated proof for the generalized process .",
    "proposition [ theoenudp ] notice that variables @xmath46 , @xmath62 , are independent and conditionally independent given any finite block @xmath127 .",
    "hence @xmath128 also @xmath53 and @xmath54 are conditionally independent given @xmath65 .",
    "hence @xmath129 .",
    "both results yield @xmath130    \\nonumber    \\\\    & =    \\sum_{k=1}^\\infty i(x_{1:n};x_{n+1:2n};z_k )    \\label{entriple }    .\\end{aligned}\\ ] ]    computing simple expressions @xmath131^n )    , \\end{aligned}\\ ] ] we obtain the triple mutual information @xmath132^n)^2\\end{aligned}\\ ] ] and the block mutual information @xmath133 where @xmath134 .",
    "the right - hand side of ( [ enudp ] ) equals up to an additive constant @xmath135 to the integral @xmath136 where we use substitution @xmath137 and functions @xmath138^{\\beta+1 } }    .\\end{aligned}\\ ] ]    we have the limit @xmath139 with the upper bound @xmath140 moreover , function @xmath141 is integrable on @xmath142 .",
    "hence @xmath143 follows by the dominated convergence theorem .",
    "it remains to compute @xmath144 .",
    "putting @xmath145 yields @xmath146\\,t^{-\\beta-1 } dt    \\\\    &    = ( 2 - 2^\\beta)\\beta^{-1}\\gamma(1-\\beta )    , \\end{aligned}\\ ] ] where integral @xmath147 can be integrated by parts for the considered @xmath148 .",
    "next , we prove the more general statement , partly using the preceding proof .",
    "proposition [ theoenmixing ] observe that processes @xmath149 , where @xmath62 , are independent and conditionally independent given any finite block @xmath127 .",
    "also @xmath53 and @xmath54 are conditionally independent given @xmath150",
    ". thus we obtain @xmath151 by replacing @xmath46 with @xmath152 in derivation ( [ entriple ] ) from the previous proof .    by the assumed markov property ,",
    "process @xmath153 is independent from @xmath53 given @xmath154 .",
    "this yields @xmath155 the expressions on the right - hand side can be analyzed as @xmath156 because @xmath154 is independent from @xmath26 given @xmath157 and @xmath158 .",
    "moreover , @xmath159    to evaluate the conditional entropies , put @xmath160 and @xmath161 .",
    "notice that by the markovity of @xmath60 we have @xmath162 similarly , since @xmath163 , we obtain @xmath164    thus we may reconstruct @xmath165 }    ,    \\\\",
    "i{\\left ( x_{1:n};(z_{ik})_{1\\le i\\le n } \\right ) }    = & \\sum_{m=1}^{n-1 } ( n - m ) a_{mk } b_k^2(1-b_k)^{m-1 }    + { \\left [ 1-(1-b_k)^{n } \\right ] }    , \\end{aligned}\\ ] ] and @xmath166}^2    .\\end{aligned}\\ ] ]    for a  fixed @xmath167 , we see that @xmath168 is minimized for @xmath169 .",
    "this case arises when @xmath170 and @xmath60 are iid .",
    "a  direct evaluation yields then @xmath171 , @xmath172 , @xmath173 , and @xmath174 . in this way we have proved that @xmath175}^2    .\\end{aligned}\\ ] ] on the other hand ,",
    "@xmath176 is maximized for @xmath177 .",
    "this holds if @xmath102 or @xmath178 . for @xmath102 ,",
    "the process @xmath71 collapses to ( [ exudp ] ) .    by equality ( [ bkequality ] ) , we obtain @xmath179}^2     ,      { \\left [ 1-(1-b_k)^{n } \\right]}^2    \\right]}\\end{aligned}\\ ] ] if @xmath180 for @xmath181 . to bound coefficients",
    "@xmath182 , observe @xmath183 hence @xmath184 for @xmath181 if @xmath185 .",
    "thus we obtain @xmath186}^2      ,          \\sum_{k\\in\\mathbb{n } } { \\left [ 1-(1-b_k)^{n } \\right]}^2     \\right ] }    .\\end{aligned}\\ ] ] the most tedious part of the proof is completed .",
    "the limiting behavior of the upper bound in ( [ enboundsgeneral ] ) has been analyzed in the proof of proposition [ theoenudp ] , and by that reasoning ( [ limenmixing ] ) holds .",
    "now we will consider the limit of the lower bound in ( [ enboundsgeneral ] ) . as in the previous proof",
    ", we will approximate the respective sum with an integral .",
    "recall that @xmath187 with @xmath188 .",
    "let us define @xmath167 for real @xmath32 in the same way .    1 .   for @xmath189",
    ": notice that @xmath190 implies @xmath191 .",
    "thus @xmath192 is greater than @xmath193 where we use substitution ( [ u ] ) and functions ( [ fnu ] ) .",
    "this yields ( [ limenmixingpower ] ) by the dominated convergence theorem .",
    "2 .   for @xmath194 : let @xmath195 be the largest number @xmath32 such that @xmath196 or put @xmath197 if there is no such number .",
    "then @xmath192 is greater than @xmath198 where @xmath199 .",
    "we have @xmath200 if @xmath201 . on the other hand ,",
    "if @xmath202 then we use @xmath203 and @xmath204 to infer @xmath205 and hence @xmath206 . thus the dominated convergence theorem in both cases yields @xmath207 taking @xmath208 gives ( [ limenudp ] )",
    "in this section we study the rate of mutual information for the stationary coding of the generalized santa fe process .",
    "let @xmath105 be the length of string @xmath106 and let @xmath15 denote the generalized santa fe process . for the coding function ( [ conjcode ] ) , regardless of the value of @xmath68 , the expansion rate @xmath209 is almost surely constant and equals the expansion rate @xmath109 .",
    "hence the stationary coding @xmath95 can be constructed as detailed below .",
    "this construction was formally introduced in ( * ? ? ?",
    "* section 6 ) and justified by ( * ? ? ?",
    "* proposition 2.3 ) .",
    "suppose that probability space @xmath210 is sufficiently rich to support some previously unmentioned random variable @xmath211 , called a  random shift , and a  nonstationary process @xmath212 where @xmath213 .",
    "we assume that @xmath214 and @xmath215 are conditionally independent given @xmath216 and their distribution is @xmath217 process @xmath97 with the desired distribution @xmath107 , where @xmath218 for @xmath219 , can be obtained as @xmath220 where @xmath92 is the shift operation .",
    "[ theobarxx ] denote blocks @xmath221 with @xmath216 removed as @xmath222 . for the santa fe",
    "processes variables @xmath222 and @xmath223 have the same distribution .",
    "notice that @xmath224 does not depend on @xmath225 and @xmath226 is independent of @xmath223 .",
    "hence @xmath227    in the following we write @xmath228 and @xmath229 .",
    "variables @xmath230 are independent and identically distributed . for these variables",
    "we define indices @xmath231 where @xmath232 . for the given distribution of @xmath230",
    ", we have @xmath233 for sufficiently small @xmath234 . by the jensen inequality",
    "@xmath235 is a  growing function of @xmath234 and @xmath236 is a  decreasing function of @xmath234 .",
    "jensen inequality implies also @xmath237 .",
    "we have @xmath238    consider function @xmath239 .",
    "for @xmath240 , it is a  growing function of @xmath234 .",
    "consider next such a  @xmath241 that @xmath242 .",
    "for @xmath243 , we obtain @xmath244 this yields @xmath245 on the other hand , for @xmath232 , we have @xmath246 hence @xmath247    define events @xmath248 subsequently , we will use the chernoff bounds :    for @xmath232 and @xmath249 , @xmath250    because variables @xmath230 are independent and identically distributed , using markov inequality we observe @xmath251 analogously we obtain the claims for @xmath252 and @xmath253 .    next , for an event @xmath254 , we introduce conditional entropy @xmath255 and mutual information @xmath256 which are respectively the entropy of variable @xmath4 and mutual information between variables @xmath4 and @xmath119 taken with respect to probability measure @xmath257 .    for the generalized santa fe process , let @xmath258 and @xmath259 .",
    "then for sufficiently large @xmath0 , @xmath260    we have @xmath261 write @xmath262 . then for sufficiently large @xmath214 , @xmath263 let @xmath264 and @xmath265 . then , for sufficiently large @xmath0 , @xmath266",
    "analogously we obtain the claim for @xmath252 .",
    "now , define events @xmath267    for @xmath268 we have @xmath269 whereas for @xmath270 we have @xmath271    the claims follow by equality ( [ defbary ] ) and conditional data processing inequality @xmath272 which holds if equalities @xmath273 and @xmath274 are satisfied on @xmath275 .",
    "there is an additional fact that we shall use .",
    "let @xmath276 be the indicator function of event @xmath275 .",
    "observe that @xmath277 where @xmath278 by the information diagram @xcite .",
    "proposition [ theoenmixingcoded ] observe that @xmath279 because @xmath280 is conditionally independent from @xmath281 given @xmath282 and @xmath283 is conditionally independent from @xmath252 given @xmath284 . now , assume that @xmath0 is sufficiently large so that bounds ( [ psplushsplus ] ) and ( [ ptplushtplus ] ) hold true . for brevity , define events @xmath285 then inequalities ( [ hxsplus ] ) , ( [ hxtplus ] ) , ( [ psplus ] ) , ( [ ptplus ] ) , ( [ psplushsplus ] ) , and ( [ ptplushtplus ] ) yield @xmath286 moreover , assume that @xmath268 . then applying subsequently ( [ pcic ] ) , ( [ barxcbaryc ] ) , ( [ barxx ] ) , ( [ pcic ] ) , and ( [ pcixc ] ) we obtain @xmath287}- 1      \\nonumber\\\\      & \\ge       p{\\left ( \\bar b \\right)}p{\\left ( c^+_n \\right ) }      i{\\left ( x_{-n+1:0};x_{1:n}\\middle|c^+_n \\right ) }      -h(x_0 ) - 1      \\nonumber\\\\      &      \\ge      p{\\left ( \\bar b \\right ) }      { \\left [         e_\\mu(n)-1        -p{\\left ( { c^+_n}^c \\right)}i{\\left ( x_{-n+1:0};x_{1:n}\\middle|{c^+_n}^c \\right ) }       \\right]}-h(x_0 ) - 1          \\nonumber\\\\      &      \\ge      p{\\left ( \\bar b \\right ) }      e_\\mu(n )      -p{\\left ( { c^+_n}^c \\right)}i{\\left ( x_{-n+1:-1};x_{1:n}\\middle|{c^+_n}^c \\right ) }      -2h(x_0 ) - 2          \\nonumber\\\\      \\label{enuemu }      &      \\ge      p{\\left ( \\bar b \\right)}e_\\mu(n )      -{\\left [ \\frac{2n}{2^{(n-1)t\\epsilon}}+2 \\right]}h{\\left ( x_0 \\right ) }      -\\frac{2n}{2^{(n-1)s\\epsilon } } - 2      .",
    "\\end{aligned}\\ ] ]    next , define events @xmath288 by ( [ psminus ] ) and ( [ ptminus ] ) we have @xmath289 assume that @xmath270 .",
    "then applying subsequently ( [ barxx ] ) , ( [ pcic ] ) , ( [ barycbarxc ] ) , ( [ pcic ] ) , and ( [ pciyc ] ) we obtain @xmath290    from bounds ( [ enuemu ] ) and ( [ emuenu ] ) we obtain @xmath291^\\beta }      \\limsup_{n\\rightarrow\\infty } \\frac{e_{\\bar\\nu}(n)}{n^\\beta }      & \\ge      \\limsup_{m\\rightarrow\\infty } \\frac{e_{\\bar\\nu}(m)}{m^\\beta }      \\ge      \\frac{p(\\bar l_0\\le l)}{[l^+_t+\\epsilon]^\\beta }      \\limsup_{n\\rightarrow\\infty } \\frac{e_{\\bar\\nu}(n)}{n^\\beta }      ,      \\\\",
    "\\frac{1}{[l^-_t-\\epsilon]^\\beta }      \\liminf_{n\\rightarrow\\infty } \\frac{e_{\\bar\\nu}(n)}{n^\\beta }      & \\ge      \\liminf_{m\\rightarrow\\infty } \\frac{e_{\\bar\\nu}(m)}{m^\\beta }      \\ge      \\frac{p(\\bar l_0\\le l)}{[l^+_t+\\epsilon]^\\beta }      \\liminf_{n\\rightarrow\\infty } \\frac{e_{\\bar\\nu}(n)}{n^\\beta }      .",
    "\\end{aligned}\\ ] ] if we consider @xmath292 , @xmath293 , and @xmath294 then the requested claims will follow by equation ( [ liml ] ) and proposition [ theoenmixing ] .",
    "in this appendix we will discuss mixing properties of the generalized santa fe process .",
    "the setting makes use of the @xmath295 space of complex valued functions . then , for a  measure space @xmath296 let @xmath297 and denote the inner product @xmath298 and the norm @xmath299 for @xmath300 .",
    "let also @xmath301 be an invertible transformation that preserves the measure , @xmath302 .",
    "the dynamical system @xmath303 is called _ mixing _ when @xmath304 by the way , we know that any mixing dynamical system is ergodic ( * ? ? ?",
    "* chapter 1.6 ) .",
    "the following proposition generalizes theorem 2 from ( * ? ? ?",
    "* chapter 10.1 ) .",
    "whereas the original claim deals with finite direct products of dynamical systems , we will extend it here to infinite products . to the best of our knowledge",
    "this generalization has not been discussed in the literature so far .",
    "the proof is similar to the finite case , except for using a  different orthonormal basis of the product space .",
    "[ theostrongmixing ] let @xmath305 , where @xmath306 , be dynamical systems with probability measures @xmath307 .",
    "consider the direct product @xmath303 , where @xmath308 , @xmath309 , @xmath310 , and @xmath311 for @xmath312 , @xmath313 .",
    "if @xmath305 are mixing then @xmath303 is also mixing .",
    "let @xmath314 be orthonormal bases of spaces @xmath315 with @xmath316 and @xmath317 .",
    "then the set @xmath318 with multi - indices @xmath319 is an orthonormal basis of the space @xmath320 , cf .",
    "( orthogonality of set ( [ basis ] ) is obvious whereas its completeness follows from the completeness of the analogical orthonormal sets for finite products and the @xmath295-bounded martingale convergence . )",
    "let @xmath321 .",
    "we have @xmath322 and @xmath323 by schwarz inequality if @xmath324 and @xmath324 have the same length @xmath32 . otherwise , @xmath325 .",
    "hence @xmath326 holds by the hypothesis .",
    "any other functions @xmath327 can be represented as series @xmath328 and @xmath329 , where @xmath330 . assume without loss of generality that @xmath331 .",
    "we will show that for every @xmath249 , inequality @xmath332 holds for sufficiently large @xmath0 .",
    "let @xmath333 and @xmath334 be finite subsets of multi - indices such that @xmath335 for certain @xmath336 and @xmath337 where @xmath338 and @xmath339 . for sufficiently large @xmath0 , we have @xmath340 . then @xmath341 which completes the proof",
    ".    now let us apply this result to the generalized santa fe process .",
    "a  stochastic process @xmath71 on @xmath5 , where @xmath16 , is called mixing if @xmath342 is mixing for @xmath343 and @xmath344 .",
    "proposition [ theomixing ] introduce an auxiliary process @xmath345 , where @xmath346 .",
    "process @xmath345 is a direct product of processes @xmath347 , @xmath348 , @xmath349 , ... , which are all mixing for @xmath72 .",
    "hence @xmath345 is mixing by proposition [ theostrongmixing ] .",
    "( in our application , we take @xmath350 , @xmath351 , and @xmath352 for @xmath353 .",
    "the transformations are @xmath354 , @xmath355 , and @xmath356 for @xmath353 . ) having established the mixing property for @xmath345 , we notice that @xmath357 for a  measurable function @xmath358 .",
    "hence @xmath71 is mixing by theorem 3 from ( * ? ? ? * chapter 10. 1 ) .",
    "first , i  would like to thank richard bradley for letting me know about his paper and nicholas travers for suggesting a  few other references .",
    "second , special thanks are due to peter grnwald for inviting me to centrum wiskunde & informatica , where the proof of proposition [ theoenudp ] was drafted .",
    "third , i  appreciate comments of jan mielniczuk and the referees , regarding the paper composition ."
  ],
  "abstract_text": [
    "<S> we construct mixing processes over an infinite alphabet and ergodic processes over a  finite alphabet for which shannon mutual information between adjacent blocks of length @xmath0 grows as @xmath1 , where @xmath2 . </S>",
    "<S> the processes are a  modification of nonergodic santa fe processes , which were introduced in the context of natural language modeling . </S>",
    "<S> the rates of mutual information for the latter processes are alike and also established in this paper . as an auxiliary result </S>",
    "<S> , it is shown that infinite direct products of mixing processes are also mixing . </S>",
    "<S> + * key words * : direct products , ergodic processes , mixing , mutual information , variable length coding + * msc 2010 : * 37a25 , 94a17 + * running head * : processes with rapidly growing information </S>"
  ]
}