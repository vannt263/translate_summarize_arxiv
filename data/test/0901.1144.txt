{
  "article_text": [
    "parameter inference from limited and noisy data in complex nonlinear models is a common and necessary step among many disciplines in modern science and engineering . a prominent framework to extract nonlinear relations from data",
    "is given by artificial neural networks ( ann s ) .",
    "these formal constructs are flexible enough to learn extremely complicated maps . however , the potential power of ann s is usually limited in practice because the network size must be bounded in order to avoid poor generalization ( i.e. out of sample ) performance .",
    "authors like @xcite and @xcite have given strong arguments that favor a bayesian perspective , in which the so called _ overtraining _ problem is alleviated .",
    "this bayesian approach has proven its effectiveness in a number of applications @xcite . however , bayesian inference based on the use of the full posterior density ( i.e. not limited to a small subset of the posterior modes ) usually demands very intensive computation @xcite . several techniques oriented to improve efficiency have been proposed .",
    "variational bayes @xcite , a method that has been mainly applied to particular inference procedures like hidden markov models and graphical models , is a promising tool where the posterior is approximated by simple distributions .",
    "approaches based on genetic programming also seem valuable @xcite , but their usefulness has not yet been established for large scale systems . here is introduced a new paradigm from which a proper bayesian estimation for large complex models can be done in the basis of a gibbs sampling for the given model s weights .",
    "this makes the procedure of relatively low computation cost and this cost increases slowly as the inference model s dimension grows .",
    "moreover , from the proposed method approximate closed expressions for the posterior marginals can be derived .",
    "as far as the author of the present letter knows , this is the first approach that admits the construction of analytic expressions for the posterior density marginals .",
    "this is useful in a number of ways .",
    "it permits the definition of efficient maximum likelihood and incremental learning methods .",
    "maximum likelihood can be used to drastically reduce the computation cost for the trained inference models , making them suitable for chip implementation , for instance .",
    "incremental learning on the other hand , gives a way to manage data that dynamically arrives to the inference system , enlarging the horizons for the applications of bayesian techniques .",
    "the proposed method admits an arbitrary close approximation to the posterior , with a computational effort that is controlled through a small set of meaningful parameters .",
    "also , the formalism is directly connected with equilibrium statistical mechanics .",
    "the approach is general , but in this contribution it s validity is tested on three layered ann s of increasing size .",
    "a classification benchmark problem and two regression problems consisting of real time series with well documented difficulty and experimental interest are considered .",
    "a discussion of the approach in the larger context of monte carlo methods for sampling is also presented .",
    "the proposed method is based on a recently introduced algorithm for density estimation in stochastic search processes , namely the stationary fokker  planck sampling ( sfp ) strategy @xcite .",
    "this algorithm learns the stationary density of a general stochastic search in a potential with high dimension @xmath0 , using only one  dimensional linear operators .",
    "essentialy , sfp consists on projecting the multi ",
    "dimensional fokker ",
    "planck equation associated to the stochastic search into a one  dimensional equation for the stationary conditional cumulative distributions , @xmath1 .",
    "the starting point is the following stochastic search defined over @xmath2 , @xmath3 where @xmath4 is an additive noise with zero mean .",
    "the model given by eq.([langevin ] ) can be interpreted as an overdamped nonlinear dynamical system composed by @xmath5 interacting particles .",
    "the temporal evolution of the probability density of such a system in the presence of an additive gaussian white noise , is described by a linear differential equation , the fokker ",
    "planck equation @xcite , @xmath6 + d \\sum_{n=1}^n \\sum_{m=1}^n \\frac{\\partial^2 p(x ) } { \\partial x_n \\partial x_m } , \\end{aligned}\\ ] ] where @xmath7 is a constant , called diffusion constant , that is proportional to the noise strength . the direct use of eq . ( [ fp ] ) for optimization or deviate generation purposes would imply the calculation of high dimensional integrals .",
    "it results numerically much less demanding to perform the following one dimensional projection of eq .",
    "( [ fp ] ) . under very general conditions ( e. g.",
    ", the absence of infinite cost values ) , the equation ( [ fp ] ) has a stationary solution over a search space with reflecting boundaries @xcite .",
    "the stationary conditional probability density satisfy the one dimensional fokker ",
    "planck equation @xmath8 from eq .",
    "( [ sfp ] ) follows a linear second order differential equation for the cumulative distribution @xmath1 , @xmath9 random deviates can be drawn from the density @xmath10 by the fact that @xmath11 is a uniformly distributed random variable in the interval @xmath12 $ ] .",
    "viewed as a function of the random variable @xmath13 , @xmath14 can be approximated through a linear combination of functions from a complete set that satisfy the boundary conditions in the interval of interest , @xmath15 choosing for instance , a basis in which @xmath16 , the @xmath17 coefficients are uniquely defined by the evaluation of eq .",
    "( [ sfpm ] ) in @xmath18 interior points . in this way",
    ", the approximation of @xmath11 is performed by solving a set of @xmath17 linear algebraic equations , involving @xmath18 evaluations of the derivative of @xmath19 .",
    "the sfp sampling is based on the iteration of the following steps :    * 1 ) * fix the variables @xmath20 and approximate @xmath14 by the use of formulas ( [ sfpm ] ) and ( [ set ] ) .    *",
    "2 ) * by the use of @xmath21 construct a lookup table in order to generate a deviate @xmath22 drawn from the stationary distribution @xmath23 .",
    "* 3 ) * update @xmath24 and repeat the procedure for a new variable @xmath25 .    the fundamental parameters of sfp sampling , @xmath17 and @xmath7 , have a clear meaning , which is very helpful for their selection .",
    "the diffusion constant `` smooth '' the density .",
    "this is evident by taking the limit @xmath26 in eq .",
    "( [ sfpm ] ) , which imply a uniform density in the domain .",
    "the number of base functions @xmath17 , on the other hand , defines the algorithm s capability to `` learn '' more or less complicated density structures .",
    "therefore , for a given @xmath7 , the number @xmath17 should be at least large enough to assure that the estimation algorithm will generate valid distributions @xmath14 .",
    "a valid distribution should be a monotone increasing continuos function that satisfy the boundary conditions .",
    "the parameter @xmath17 ultimately determines the computational cost of the procedure , because at each iteration a system of size @xmath27 of linear algebraic equations must be solved @xmath5 times .",
    "therefore , the user is able to control the computational cost through the interplay of the two basic parameters : for a larger @xmath7 a smoother density should be estimated , so a lesser @xmath17 can be used",
    ".    it should be noticed that sfp is a generalization of gibbs sampling .",
    "therefore , the deviates generated by the iterative procedure are in the long run sampled from the full joint equilibrium density @xmath28 where @xmath29 is a normalization factor .",
    "additionaly , a convergent representation for @xmath30 is obtained after taking the average of the coefficients @xmath31 s in the expansion ( [ set ] ) over the iterations @xcite , @xmath32 under general conditions , the gibbs sampler converges at a geometric rate @xcite and there is some evidence that this fast convergence is shared by sfp @xcite . in the next section the properties of the sfp sampler",
    "are studied in the wider context of monte carlo methods .",
    "thereafter the general toolkit for bayesian inference based on sfp is developed and tested .",
    "in order to sample a given distribution @xmath33 , the potential function that enters into sfp should be given by @xmath34 using @xmath35 .",
    "the correct normalization is directly obtained by construction without explicitly calculating it nor including it in the definition of @xmath19 .",
    "the advantages of sfp sampling with respect to previous monte carlo methods are first illustrated through an important class of probability densities , namely the separable probability densities of the form @xmath36 in this case",
    "sfp converges in a single iteration .",
    "this result follows from the fact that the dynamics ( [ langevin ] ) associated to the random search decouples into @xmath5 independent one dimensional stochastic differential equations , which makes the gibbs sampling stage ( steps 2 and 3 ) of sfp unnecessary .",
    "because @xmath7 is fixed , sfp requiers a single parameter , @xmath17 . but",
    "@xmath17 simply refers to the number of base functions used in expansion ( [ set ] ) , so in sfp there is no need to adjust a step size parameter .",
    "step size parameters are difficult to tune because their correct selection depends on the actual variance of the sampled distribution , which in general is unknown .",
    "the parameter @xmath17 on the other hand , is selected by objective criteria that are independent from any knowledge about the sampled density .",
    "specifically , @xmath17 should be large enough such that the observed marginals are valid probability densities .",
    "consider the next example , a one dimensional mixture density given by @xmath37 where @xmath38 are normal densities @xmath39 .",
    "it has been pointed out in the literature that mixture densities with well separated modes are difficult to sample by monte carlo methods @xcite .",
    "consider a case with three modes , which result from the mixture of normal densities @xmath40 .",
    "this mixture density has mean and standard deviation @xmath41 and @xmath42 respectively .",
    "the figure [ density1 ] shows the resulting estimation of the density after a single sfp iteration with @xmath43 .    0.7 cm     in all the numerical experiments discussed in this letter , a fourier basis is used in formula ( [ set ] ) to approximate the distributions produced by sfp sampling , @xmath44 in the estimation of the mixture density ,",
    "the parameter @xmath17 has been chosen by growing it s size by @xmath45 units per experiment , starting at @xmath46 .",
    "after each experiment the resulting density is visually inspected and it s variance is calculated by direct integration from the formula eq .",
    "( [ set ] ) .",
    "it s selected the minimum value of @xmath17 which produce a valid density with positive variance .",
    "each sfp iteration takes around @xmath47 seconds in the equipment used ( details of which are given in section [ experiments ] ) .",
    "points from this density are easly drawn by the construction of a lookup table for the cumulative distribution eq .",
    "( [ set ] ) to generate each deviate as in step 2 of sfp .",
    "figure [ sample1 ] shows a sample of @xmath48 points whose generation took a fraction of a second . the auto ",
    "correlation function for the sample is also plotted .",
    "no significant correlations appear between the points in the sample .",
    "the average and standard deviation estimated from the sample are @xmath49 and @xmath50 respectively .",
    "0.7 cm   0.3 cm     the same mixture density sampling problem is studied by the metropolis and hybrid markov ",
    "chain montecarlo ( hmcmc ) algorithms .",
    "the implementations provided in the `` software for flexible bayesian modeling and markov chain sampling '' developed by radford neal ( downloadable at http://www.cs.toronto.edu/@xmath51radford/ ) are used .",
    "the parameters for the metropolis and hmcmc have been carefully tuned in order to have acceptable rejections rates , which are @xmath52 for metropolis and @xmath53 for hmcmc .",
    "samples of @xmath54 points provided by both methods are presented in fig .",
    "the estimated average from both samples is around the value of @xmath55 .",
    "0.7 cm   0.3 cm     these results show one of the main drawbacks in many monte carlo strategies , namely the possibility to reach a state of apparent equilibrium which in fact is unrepresentative of the whole density .",
    "the mixture density example illustrates what might be one of the most promising features of sfp : it s capacity to jump large regions of low probability , reducing the danger of getting trapped into local high probability regions .    for multidimensional non ",
    "separable distributions the gibbs sampling stage of sfp is essential .",
    "consider the following example , provided in the tutorial of the `` software for flexible bayesian modeling and markov chain sampling '' , @xmath56 which gives a three  dimensional ring density .",
    "the table [ table - ring ] lists the results of the estimated expectations for each one of the variables from @xmath57 points generated by the samplers without rejecting initial points . by symmetry ,",
    "the correct expected values are equal to zero .",
    "for sfp @xmath58 in the search space defined by the cube @xmath59 ^ 3 $ ] .",
    "the parameters for hmcmc and metropolis are the same as discussed in the software s tutorial .",
    ".[table - ring ] estimations of the mean values by sfp , hmcmc and metropolis algorithms in the three  dimensional ring distribution problem . by the symmetries of the sampled distribution , the true mean values are equal to zero for all the three variables .",
    "sfp and hmcmc display similar results . [ cols=\"^,^,^,^,^,^,^,^,^,^,^\",options=\"header \" , ]     the number of loss function evaluations of a sfp sampling grows linearly with the potential s dimension @xcite .",
    "therefore , table [ table - lasersize ] indicates that sfp is capable to perform a correct estimation of the posterior density with a total number of loss function evaluations that grows linearly with system s size .",
    "the evaluations of the potential function gives the major contribution to the computational cost . in fig .",
    "[ fig - lasertime ] is presented the total computation time of each of the runs of table [ table - lasersize ] .",
    ", title=\"fig : \" ]    the computation time grows slowly with the system s dimension , which is consistent with the linear behavior predicted by sfp theory .",
    "this experimental result is important because it shows the value of sfp sampling for large scale systems . in the context of global optimization",
    ", it appears that sfp should be further adapted to alleviate at some extent the curse of dimensionality suffered by any stochastic optimization method in order to be competitive with the current best algorithms @xcite .",
    "however , table [ table - lasersize ] and fig .",
    "[ fig - lasertime ] suggest that for density estimation purposes , the correct estimation of large dimensional densities via sfp sampling is a polynomial time computational procedure , with a total number of loss function evaluations that behave linearly . these properties may be valuable in other applications besides bayesian inference , like for instance monte ",
    "carlo simulations of large physical systems .",
    "the framework for bayesian learning based on sfp sampling introduced in this letter is directly connected to equilibrium statistical mechanics . in particular , using a dimensionless boltzmann constant @xmath60 , it turns out that an entropy @xmath61 can be introduced , @xmath62 therefore obtaining the thermodynamic relation @xmath63 the exploitation of the link of sfp bayesian learning with equilibrium statistical mechanics appears to be promising taking into account that sfp provides analytic expressions for the marginals , from which mean  field approximations to the quantities of interest may be derived .",
    "an additional interesting research question is the study of more general forms of uncertainty affecting the stochastic search in the weight space . in this regard",
    ", it should be noticed that the sfp formalism is in principle not limited to the estimation of the stationary density of a diffusion on a potential under white gaussian additive noise . through the use of generalizations to the fokker ",
    "planck equation based on the expansion of the master equation , like for instance the van kampen expansion @xcite , several other stochastic search processes may be considered .",
    "if the posterior densities resulting from such generalized processes are more adequate in some situations seems to be an appealing question to further study .",
    "a techical issue in which there may be room for the improvement of the sfp approach regards the selection of the most appropriate basis function family for the approximation of the conditionals . in this letter",
    "it has been used the fourier basis only because of its simplicity , but in principle any other basis can be used .    other relevant research line that follows from the results presented so far consists on the application of sfp learning to large and complex systems .",
    "if the observed polynomial behavior of computation time holds in general , it would be valuable to apply the sfp technique on very large inference models , taking advantage of the intrinsic parallel nature of the sfp sampling algorithm .",
    "this work was partially supported by the national council of science and technology of mexico and by the uanl ",
    "paicyt program .",
    "badii , r. , brun , e. , finardi , m. , flepp , l. , holzner , r. , parisi , j. , reyl , c. , & simonet , j. ( 1994 ) .",
    "progress in the analysis of experimental chaos through periodic orbits .",
    "66 _ , 13891415 .",
    "weigend , a. s. & gershenfeld , n. a. ( eds . ) ( 1994 ) .",
    "_ time series prediction : forecasting the future and understanding the past .",
    "santa fe institute studies in the sciences of complexity xv_. addison  wesley ."
  ],
  "abstract_text": [
    "<S> a novel formalism for bayesian learning in the context of complex inference models is proposed . </S>",
    "<S> the method is based on the use of the stationary fokker  planck ( sfp ) approach to sample from the posterior density . </S>",
    "<S> stationary fokker  </S>",
    "<S> planck sampling generalizes the gibbs sampler algorithm for arbitrary and unknown conditional densities . by the sfp procedure approximate analytical expressions for the conditionals and marginals of the posterior </S>",
    "<S> can be constructed . at each stage of sfp , </S>",
    "<S> the approximate conditionals are used to define a gibbs sampling process , which is convergent to the full joint posterior . by the analytical marginals </S>",
    "<S> efficient learning methods in the context of artificial neural networks are outlined . off  </S>",
    "<S> line and incremental bayesian inference and maximum likelihood estimation from the posterior is performed in classification and regression examples . </S>",
    "<S> a comparison of sfp with other monte carlo strategies in the general problem of sampling from arbitrary densities is also presented . </S>",
    "<S> it is shown that sfp is able to jump large low  probabilty regions without the need of a careful tuning of any step size parameter . </S>",
    "<S> in fact , the sfp method requires only a small set of meaningful parameters which can be selected following clear , problem  independent guidelines . </S>",
    "<S> the computation cost of sfp , measured in terms of loss function evaluations , grows linearly with the given model s dimension . </S>"
  ]
}