{
  "article_text": [
    "there are many open problems of optimization in systems and control theory , often non - smooth , non - convex and np - hard or of unknown complexity . because of the success obtained with linear matrix inequalities ( lmis ) since the mid - nineties , a general tendency in the system and control literature is to formulate many of these problems with bilinear matrix inequalities ( bmis ) or lmis plus a non - convex rank constraint .",
    "once these formulations obtained , the usual techniques for trying to solve these problems are iterative linear matrix inequalities algorithms ( ilmis ) .",
    "such algorithms solve successively different lmi subproblems of the original bmi formulation .",
    "however , these algorithms loose the guaranteed polynomial time complexity of lmi solvers but more importantly most do not have guaranteed convergence to locally optimal solutions ( i.e. global convergence when starting from any feasible initial solution ) .",
    "such ilmis , heuristics without global convergence , are then likely to be outperformed by other methods like general purpose optimization solvers .",
    "this is the fact that we aim at putting forward with the current paper : much research work sticks to a unique approach , like bmis / lmis , and ignores alternative formulations which can however be much more appropriate .    the performance can be compared in three directions : 1 ) objective value , 2 ) computational time and 3 ) user time .",
    "more details will be given in the paper but we already outline here some key elements .",
    "1 ) the methods that will be proposed are often found in the literature to behave well on non - smooth functions and some are proved globally convergent on smooth functions .",
    "this is not the case for many ilmis .",
    "2 ) on non - smooth and non - convex problems there are no worst - case complexity bounds neither for the proposed methods nor for ilmis .",
    "thus , on this front , only experience will tell .",
    "3 ) the time required by the user to implement the method is clearly smaller with general purpose optimization solvers , which are intended to minimize any user - defined function @xmath0 . formulating a problem under that form",
    "may be done in a matter of minutes or hours . but developing a theoretically consistent ilmi algorithm , along with a working implementation , can easily require several weeks .",
    "this is by the way admissible in fundamental research but clearly unacceptable in the industry .      to illustrate the point of the paper",
    ", we will consider a problem for which an ilmi approach was recently proposed @xcite .",
    "this problem is the design of a reduced - order positive filter for linear systems .",
    "positive systems , found in many areas ( see @xcite ) , are dynamic systems with state variables and outputs positive at all times .",
    "the filter to be designed has to estimate an unmeasured output ( @xmath1 ) of a system ( @xmath2 ) from the measurements ( @xmath3 ) , with minimum objective value chosen as the @xmath4 norm of the filtering error transfer function ( @xmath5 ) between the exogenous disturbance signal ( @xmath6 ) and the error ( @xmath7 ) : the difference between the output ( @xmath1 ) and its estimate ( @xmath8 ) .",
    "details on the problem motivations and challenges are given in the introduction of @xcite , here only the elements necessary to reproduce the contribution are recalled .",
    "consider the following asymptotically stable discrete - time system : @xmath9 the notations are classical and identical to those of @xcite , where all terms are more formally detailed .",
    "this system is positive if and only if the matrices @xmath10 have only positive entries @xcite . note also that in this paper we only require the positivity of the filter to be designed and not of the system @xmath2 , but it can be assumed because otherwise it is not necessary to design a positive filter .    the aim of the filter is to compute an estimation @xmath11 of the unmeasured signal @xmath12 in @xmath2 from the measured signal @xmath13 .",
    "more specifically , we want to build the following filter : @xmath14 where @xmath15 are the filtering parameters to be determined , matrices having only positive entries to ensure the positiveness of @xmath11 @xcite .",
    "the difficulty of designing this filter stems from that particular requirement , which prevents the application of methods normally used for this kind of problem ( see @xcite and ref . therein ) .    defining @xmath16^t$ ] and @xmath17",
    ", we get from ( [ sys ] ) and ( [ filt ] ) the description of the filtering error system : @xmath18 where @xmath19 the transfer function of the filtering error system @xmath20 is given by : @xmath21 the problem considered is defined hereunder .    _",
    "reduced - order minimal @xmath4 positive filtering problem : _",
    "@xmath22 this problem is a generalization of the one in @xcite .",
    "the difference is that we do not only seek a filter respecting a given level @xmath23 but we rather wish to minimize this level .",
    "this objective is more meaningful and also more practical for the user who does not need to choose an arbitrary level @xmath24 beforehand .",
    "note that considering the expression of @xmath25 , since @xmath2 is stable we have that the filtering error system @xmath20 is stable iff the filter @xmath26 is stable .",
    "the problem above can be expressed as either of the two following optimization problems of minimizing an objective function @xmath0 : + _ the unconstrained problem : _ @xmath27 _ the constrained problem : _ @xmath28 where @xmath29 is the number of variables in @xmath30 , the vector containing the entries of @xmath31 .",
    "these problems can actually be dealt with many general - purpose optimization solvers . amongst these we outline some direct search ( ds ) methods , using the unconstrained formulation : the mesh adaptive direct search ( mads ) @xcite , the multidirectional search ( mds ) @xcite , the alternative directions @xcite , the nelder - mead algorithm ( nm ) @xcite , the particle swarm optimizer ( pso ) @xcite ( see @xcite for the latest survey on ds methods ) .",
    "we also mention gradient - based methods , which standard and widely known principles are implemented e.g. in the following two functions of matlab : ` fminunc ` for the unconstrained formulation and ` fmincon ` for the constrained formulation , both methods estimating numerical gradients when the expression of the gradient of @xmath32 is not given ( see the matlab help @xcite and references therein for detailed descriptions ) .",
    "all these methods can be used for the problem discussed here .    note",
    "that when the expression of an ( estimate ) of the gradient is known , more evolved techniques can be developed ( like @xcite ) to ( try to ) reduce the computational time , especially for problems with larger number of variables .",
    "for complex problems like the one considered , the gradient information is useful to accelerate the search but without guarantee to find better optima than methods which do not use this information ( see benchmarks in @xcite ) .      for illustration purposes we put forward one method in particular , the nelder - mead algorithm ( nm ) .",
    "this method is probably the most popular algorithm for unconstrained minimization of problems of moderate dimension ( e.g. @xmath33 50 to 100 variables ) .",
    "the central question is whether locally optimal solutions can be reached , which is not guaranteed for complex problems with most ( if not all ) iterative linear matrix inequality algorithms ( ilmis ) including the one proposed in @xcite , as motivated in the introduction .",
    "note that for further illustration , we will also use several other methods than nm in the next section .",
    "the nm algorithm does not appear often in ( recent ) systems and control literature .",
    "this algorithm was first proposed almost fifty years ago in @xcite and belongs to the class of ds methods .",
    "these methods , characterized by the fact that they do not build a model of the objective function , belong to the broader class of derivative - free optimization methods which do not use any gradient or hessian information .",
    "the reader is referred to the last survey on ds methods ( * ? ? ?",
    "1.4 ) for a broader description .",
    "the basic ideas behind nm are briefly described in the next paragraph , detailed descriptions can be found in @xcite or @xcite .",
    "the first step is the generation of an initial simplex of @xmath34 solutions around and including the provided initial solution .",
    "the objective function is evaluated at each of these @xmath34 solutions and sorted from the best to the worst .",
    "then nm chooses iteratively between several possibilities ( or steps ) to change the shape of the simplex ( eventually displacing it ) , trying to find better solutions .",
    "for example the basic step is that the worst solution is reflected on the other side of the simplex , in order to create a ` downhill ' effect .",
    "two easily available implementations of nm are the ` fminsearch ` function in the optimization toolbox of matlab @xcite or the ` nmsmax ` function of @xcite .",
    "it must be noted however that these implementations may fail to converge to locally optimal solutions , starting from a feasible initial solution .",
    "indeed the original nm can fail because of the deterioration of the simplex geometry or lack of sufficient decrease .",
    "however the convergence of the method can be guaranteed on smooth functions by taking care of these situations , which is done for example in @xcite where an additional step ensuring the convergence is proposed .",
    "actually there is a third easily available implementation , the grid restrained nm algorithm @xcite .",
    "this algorithm enjoys , like @xcite , guaranteed convergence on smooth functions and will also be used in our tests .    an earlier method to improve the convergence of nm proposed in @xcite consists of an involved restarting strategy .",
    "here we suggest a method with easier implementation : restarting the algorithm until the latest obtained objective value is not better than the previous one to a given accuracy .",
    "this does not formally guarantee the convergence to local solutions on smooth objective functions but renders it much more likely .",
    "indeed , restarting nm regenerates its simplex and this allows in practice to properly span the space @xmath35 around the last solution .",
    "anyway , the hypothesis for convergence analysis in @xcite require that the function be smooth . and",
    "as pointed out in @xcite , the @xmath4 norm objective function is non - smooth and this may cause the algorithm -or any of the techniques cited above- to stop at suboptimal solutions .",
    "this is the reason why we will use not only @xcite but also try the idea of restarting nm in the results section .",
    "we summarize that the point is not to guarantee the convergence to locally optimal solutions for all optimizations , which require smooth objective functions , but instead to propose an easily implementable technique that performs well and in practice often reaches locally optimal solutions even for non - smooth objectives functions .",
    "nelder - mead with local restart(s ) can be more formally defined as :    @xmath36@xmath37 ; @xmath38 ; ( @xmath39 ) @xmath40 ; @xmath41@xmath42 ; @xmath43 ; * return * @xmath44    where @xmath44 is the solution after optimization @xmath45 , @xmath46 the initial solution , @xmath47 contains the stopping criteria and tolerances of the nelder - mead implementation , @xmath48 is the absolute value , @xmath49 is the stopping accuracy required for @xmath50 , the relative improvement between the last and the current objective value .",
    "we already mention another kind of useful restarts , also suggested e.g. in @xcite , the global restarts ( or multi - starts ) .",
    "these consist simply of running other optimizations from other different initial solutions @xmath46 .",
    "the aim of these global restarts allows to better span the search space which is useful to try to : 1 ) get around the non - smoothnesses that could have caused other optimizations to stall , for non - smooth objective functions 2 ) to find better local optima ( nearby ) , for multimodal objective functions 3 ) get away from suboptimal solutions against the infinite penalization barrier , which is an implicit constraint . by obtaining an increasing number of final solutions having the best objective value found so far ( to a given accuracy ) , we get increasing probabilities that these solutions are locally optimal and in a lesser measure globally optimal .",
    "this illustrating algorithm may compete with other methods , e.g. with @xcite using benchmarks of static output feedback optimization in @xcite .",
    "it can often find the same objective values or even better ones , possibly in shorter computational times although this is more an exception rather than the rule , the main drawback being the lack of explicit handling of non - smoothnesses as proposed in @xcite .",
    "the main advantage is the great flexibility to handle any objective function @xmath51 , only some of which can be written under other frameworks like lmis .",
    "moreover , even without using gradient information , it can often lead to locally optimal solutions . also , unlike usual quasi - newton methods , nm has the ability to explore neighboring valleys with better local optima and likewise this exploring feature may allow nm to overcome non - smoothnesses .    the flexibility is what makes it a candidate of choice for dealing with designs requiring particular structures or properties , such as ensuring the positivity of the solution like desired in this paper , while exploring a non - smooth objective function .",
    "more precisely , nm will be particularly efficient for optimization objectives as an alternative to ilmis , as motivated before and described further as follows .",
    "ilmis are often efficient for feasibility problems such as finding a stabilizing controller , possibly also ensuring a given performance level , even though they may fail to find a solution even if there exists one ( see @xcite ) .",
    "however when used for minimization problems , like minimizing a norm of a performance channel , ilmis have in general no guarantee of convergence to locally optimal solutions ( as discussed in @xcite and noted in @xcite ) .",
    "for instance in @xcite this stems essentially from the fact that the objective minimization would require a combinatorial optimization problem involving the objective ( @xmath24 ) and a parameter ( @xmath52 ) . considering this important drawback",
    ", other methods should be used instead of ilmis for objective minimizations .",
    "ilmis can be useful when obtaining feasible ( e.g. stable ) initial solutions is not trivial , which is not the case here ( the system is already stable ) .",
    "and then more suited methods should be used for optimizing the objective , like ds methods .    indeed using these methods",
    "can often lead to locally optimal solutions , unlike the algorithm of @xcite that produces suboptimal solutions under an a - priori defined level @xmath24 .",
    "although the non - smoothness prevents guarantees of local optimality for direct search methods , it is conjectured at places in the literature that this does not happen often ( see ref . in @xcite ) .",
    "how ` often ' depends on the density of non - smoothnesses in the objective function and in particular at ` partial ' optimal solutions , optimal in some but not all directions .",
    "this is actually a matter of debate , on which the reader is advised to consult e.g. @xcite and the references therein . and",
    "as noted before , global restarts can be used to try to reduce the likeliness of getting stuck at a suboptimal solution .",
    "many optimization problems in systems and control can be quickly implemented to be solved with ds methods .",
    "indeed the objective function is easily built and evaluated with adequate methods , for example the functions from the control system toolbox of matlab .",
    "so there is no need to modify or add cumbersome lmis , which are also especially large with the technique of @xcite ( see the matrix inequality ( 16 ) there ) .",
    "likewise the unhandy implementation of an ilmi is avoided , moreover often involving parameter(s ) for which no automatic a priori choice can be made . in short , the method can be implemented in a brief time even by non - expert users .",
    "also ds methods are useful for simulation - based optimization ( * ? ? ?",
    "1.21 ) , where more intricate objective functions that could hardly be obtained from analytical expressions are computed using simulation results ( illustration given in next section ) .",
    "an important remark can be made regarding the worst - case computational complexity bounds .",
    "in @xcite we can read that mds and nm are not competitive with more sophisticated methods such as ( quasi-)newton methods when applied to smooth problems .",
    "indeed it seems clear that gradient expressions and moreover hessian information should accelerate the search .",
    "however a recent result @xcite proves that directional ds methods share the worst - case complexity bound of steepest descent for the unconstrained minimization of a smooth function .",
    "this gives further motivation to consider ds methods .",
    "let us also mention that the formulation of the problem in @xcite requires many matrix variables @xmath53 , @xmath54 and three scalars @xmath55 -with @xmath56 entering non - affinely the matrix inequalities , thus the need of an ilmi- whereas here we only need the original variables @xmath15 which put together lead to the same size as only @xmath57 in @xcite .",
    "using lmis leads to this typical key problem of inflation of size and number of variables for large systems ( @xmath2 and @xmath26 ) . instead , with general purpose solvers , only the original problem s variables ( @xmath26 ) are used .",
    "lmi problems are solved efficiently with interior point methods converging to the optimum in worst - case polynomial time .",
    "this is not guaranteed with the method proposed here , but also not with ilmis , since the iterative scheme does not preserve the polynomial time complexity . in practice nm",
    "deals well with problems with limited number of variables ( e.g. @xmath33 10 - 20 ) but its performances decrease notably for larger number of variables ( e.g. @xmath58 50 - 100 ) . as noted before",
    ", one should then use the advantage that ds methods can handle problems formulations with the least number of variables .",
    "the positive system considered is given by @xcite : @xmath59 & 0.0510\\\\          0.1021 & 0.1250          \\end{bmatrix}w_k\\ ] ] @xmath60 the objective is to estimate @xmath61x_k = x_k^1 $ ] . for the problem considered ,",
    "any positive and stable filter can be used as initial solution . since like in @xcite we want to design a first order filter , we can choose the scalar @xmath62 in @xmath63 and the other variables in @xmath64 .",
    "we then simply use ` rand ` of matlab to generate the random initial solutions ( entries chosen uniformly in @xmath65 $ ] ) . performing three optimizations ,",
    "each from a different random initial solution , gives us the best following solution :    @xmath66 , \\hat{c } = 0.15218,\\ \\hat{d } = \\left[0.15435\\ 0.10931\\right]\\ ] ] instead of the solution in @xcite : @xmath67 , \\hat{c } = 0.14130,\\ \\hat{d } = \\left[0.17889\\ 0.34404\\right]\\ ] ] let it be clear that we do not need the solution of @xcite as initial solution , indeed here we simply use independent random feasible solutions and therefore the proposed technique is self - standing . perhaps",
    "for some particular problems ilmis would be necessary to find initial solutions , but this is not the case here .",
    "we nevertheless give as further information what happens when using the solution of @xcite as initial solution , at the end of this section .",
    "note also that since linear systems admit an infinity of equivalent state - space representations , we can restrict the search space to one representation ( e.g. by fixing here @xmath68 ) . on the one hand , reducing the number of variables",
    "reduces the computational time . on the other hand",
    ", this removes the possibility for ds methods to explore different state - space coordinates , therefore reducing their ability to find better local optima and getting around non - smoothnesses or away from the border of ( implicit ) constraints .",
    "two of the three solutions found have a @xmath4 performance level around @xmath69 , significantly better than the level @xmath70 reached in @xcite .",
    "we present in fig .",
    "[ fig1 ] the same simulation as in @xcite of the actual state @xmath71 and its estimations , using the following initial condition of the error system : @xmath72^t$ ] and exogenous disturbance input : @xmath73^t$ ] .",
    ", estimations and corresponding errors @xmath74.,height=340 ]    the gain of performance level can be seen , with a maximum absolute error about three times smaller with the restarted nm . a third simulation response",
    "is also provided , obtained by minimizing an objective specific to this simulation : the sum of the absolute values of the errors @xmath74 .",
    "the obtained filter is given by : @xmath75 , \\hat{c } = 0.3,\\ \\hat{d } = \\left[0.000046\\ 0.0607\\right]$ ] .",
    "this gives an illustration of simulation - based optimization .    to give further illustration of the performance of the proposed method",
    ", we run 100 optimizations each starting from a different random initial solution .",
    "we then give the minimum and the average of all 100 obtained objectives values .",
    "also we give the percentage of objectives values that were smaller than 0.1415 and of those smaller than 0.0448 . as last indication",
    ", we give the average of the 100 computational times required in seconds .",
    "we perform these tests with the proposed technique as well as with other optimizations techniques , each using the same 100 random initial points .",
    "the results are given in table [ result100 ] .",
    "[ result100 ]    .comparison of objective values and computational times [ cols=\"<,<,<,^,^,^\",options=\"header \" , ]     the methods are named after their matlab implementation file : 1 ) ` fminsearch ` , the matlab optimization toolbox nm implementation 2 ) the same with local restart(s ) , as described in the previous section 3 ) ` nmsmax ` , n. higham nm implementation @xcite 4 ) the same with local restart(s ) 5 ) ` mdsmax ` n. higham mds implementation @xcite @xcite 6 ) the grid restrained nm algorithm ` gridnm ` @xcite , proved convergent on smooth functions 7 ) ` fminunc ` also using the unconstrained formulation ( using @xmath76 instead of @xmath77 for constraint penalization ) 8) ` fmincon ` , the only method using the constrained formulation of the problem .",
    "+ all accuracies required : ` tolf ' , ` tolx ' , ` tolcon ' ( see matlab help @xcite for descriptions ) , @xmath49 and the accuracies in ` nmsmax , mdsmax , gridnm ` have been set to @xmath78 as well as the tolerance of the objective @xmath79 ( ` norm(g , inf , tol ) ` ) evaluation .",
    "also the numbers of iterations and functions evaluations were not limited ( e.g. ` maxiter ' and ` maxfunevals ' set to inf ) .    except for 7 ) , at least 96% of the trials give a better performance level than 0.1415 from @xcite .",
    "this is a first illustration of how general purpose optimization solvers might compete with ilmis .",
    "observe then the results 2 ) , 4 ) and 6 ) , obtained with improved nm .",
    "as can be seen these have high success rate at reaching objectives values very close to the best one found .",
    "4 ) is especially good since it was 100 % successful . considering this , even though @xmath51 is non - smooth here",
    ", the solutions found with objective value @xmath80 are very likely locally optimal and probably globally as well .",
    "this illustrates the efficiency of the method put forward .",
    "the average computational times are under the minute , therefore the methods are not only convenient to encode but also reasonably fast to run .",
    "note that the computational times could be shortened by using gradient expressions like in @xcite or , as suggested in @xcite , by interrupting the bisection algorithm computing the @xmath4 norm once it is sure that the solution being evaluated is better or worse than the other solutions . according to one of the authors in @xcite the computational time required there was around a few seconds , which is comparable to those required here .",
    "we also run the different alternatives starting from the solution in @xcite , which gives the following improved objectives values : 1 ) 0.1395 2 ) 0.0565 3 ) 0.0455 4 ) 0.0447 5 ) 0.0466 6 ) 0.0449 7 ) 0.1391 8) 0.0454 .",
    "we see that two methods were unable to improve the solution of @xcite to be close to the objective value 0.0447 , which gives an indication that this solution is badly located .",
    "indeed its very small @xmath81 matrix , ` blocked ' near the positivity constraint , almost cancels the effect of the dynamical part of the filter : the solution in @xcite can be approximated by a static filter ( the @xmath82 matrix ) with almost no impact ( 0.003@xmath83 ) on the objective in ` norm ` instead of the default @xmath84 . on this matter",
    "we also mention that an important element for the performance of ilmis is to try to stay away from the borders of the bmi feasible set ( using e.g. ( * ? ? ?",
    "4.3 ) ) , otherwise the algorithm may stall early at partial optimal solutions .",
    "considering the heuristical nature of ilmis , which can be efficient for feasibility problems but have in general no guarantee of convergence to locally optimal solution for minimization objectives nor a bound on the worst - case complexity , many other methods can be used as competitors of such techniques .",
    "the problem of @xcite , designing a reduced - order positive filter to estimate the output of a positive system under a given maximum @xmath4 error level @xmath24 , is dealt with in that paper using an ilmi and indeed it can be read there that the convergence of the algorithm is not guaranteed .",
    "this motivates us to propose techniques where the @xmath4 error level is minimized .",
    "we then consider a less restrictive approach than ( i)lmis and reformulate the problem so as to be solvable by general purpose optimization solvers .",
    "in particular , we put forward the nm algorithm improved with local restarts that can behave well even with non - smooth objective functions .",
    "this approach has apparently only advantages compared to that of @xcite .",
    "it is easy to encode and use even by non - expert users .",
    "it is more flexible , i.e. straightforward to modify for example to 1 ) handle continuous - time systems 2 ) change the objective into @xmath85 minimization or multi - objectives 3 ) take into account complex requirements such as a structure of the solution .",
    "the inflation of size of the system -but not necessarily of the filter- will have a smaller impact on the computational time than techniques using lmis , since the only variables are those of the filter to be designed ( no additional variables needed , like lyapunov matrices ) .",
    "also the technique often leads to locally optimal solutions , where ` often ' depends on the objective to be minimized , whereas this should be seldom the case with most ilmis ( see @xcite and references therein , in particular @xcite ) .",
    "finally since it is generally fast for moderate size problems , it can be run multiple times from several feasible initial solutions . and",
    "so by getting several times the same best solution one gets an increasing probability that this solution is indeed locally optimal or even globally optimal .    in the end",
    "our claim is that most open optimization problems in systems and control should not be handled with conservative lmi formulations , except for providing an initial suboptimal solution .",
    "indeed conservative lmi formulations or ilmis to deal with bmi problems typically do not have guaranteed convergence to local optima . to get locally optimal solutions one will need to use other approaches , the best two methods being probably @xcite , which use gradient expressions and take non - smoothnesses into account .",
    "nevertheless the method proposed can be a competitor to these techniques by finding the same or better objectives .",
    "also the set of problems that can be solved by ds methods is broader than those dealt with the methods in @xcite , thanks to the great flexibility offered by the general objective formulation @xmath51 and without need of gradient expression .",
    "the authors thanks ping li for comments on the manuscript and rpd brmen for providing the grid restrained nm implementation @xcite .",
    "this research is supported by the belgian network dysco ( dynamical systems , control , and optimization ) funded by the interuniversity attraction poles programme of the belgian state , science policy office .",
    "e.  simon , p.  rodriguez - ayerbe , c.   stoica , d.  dumur , and v.  wertz .",
    "lmis - based coordinate descent method for solving bmis in control design . in _",
    "18th ifac world congress _ , august 2011 ."
  ],
  "abstract_text": [
    "<S> the motivation of this work is to illustrate the efficiency of some often overlooked alternatives to deal with optimization problems in systems and control . in particular </S>",
    "<S> , we will consider a problem for which an iterative linear matrix inequality algorithm ( ilmi ) has been proposed recently . </S>",
    "<S> as it often happens , this algorithm does not have guaranteed global convergence and therefore many methods may perform better . </S>",
    "<S> we will put forward how some general purpose optimization solvers are more suited than the ilmi . </S>",
    "<S> this is illustrated with the considered problem and example , but the general observations remain valid for many similar situations in the literature .    </S>",
    "<S> optimization ; linear systems ; positive filtering </S>"
  ]
}