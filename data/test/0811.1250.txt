{
  "article_text": [
    "classification is a basic task in machine learning . a training data set @xmath0 consists of @xmath1 feature vectors ( samples ) @xmath2 , and @xmath1 class labels , @xmath3 , @xmath4 to @xmath1 .",
    "here @xmath5 and @xmath6 is the number of classes .",
    "the task is to predict the class labels .",
    "this study focuses on multi - class classification ( @xmath7 ) .",
    "many classification algorithms are based on * * boosting**@xcite , which is regarded one of most significant breakthroughs in machine learning .",
    "mart@xcite ( * * m**ultiple * * a**dditive * * r**egression * * t**rees ) is a successful boosting algorithm , especially for large - scale applications in industry practice .",
    "for example , the regression - based ranking method developed in yahoo!@xcite used an underlying learning algorithm based on mart . _",
    "mcrank_@xcite , the classification - based ranking method , also used mart as the underlying learning procedure .",
    "this study proposes * abc*-boost ( * * a**daptive * * b**ase * * c**lass boost ) for multi - class classification .",
    "we present * abc*-mart , a concrete implementation of abc - boost .",
    "abc - boost is based on the following two key ideas :    1 .   for multi - class classification ,",
    "popular loss functions for @xmath6 classes usually assume a constraint@xcite such that only the values for @xmath8 classes are needed .",
    "we can choose a * base class * and derive algorithms only for @xmath8 classes . 2 .   at each boosting step , although the base class is not explicitly trained , it will implicitly benefit from the training on @xmath8 classes , due to the constraint .",
    "thus , we * adaptively * choose the base class which has the `` worst '' performance .",
    "the idea of assuming a constraint on the loss function and using a base class may not be at all surprising . for binary ( @xmath9 ) classification ,",
    "a `` sum - to - zero '' constraint on the loss function is automatically considered so that we only need to train the algorithm for one ( instead of @xmath9 ) class . for multi - class ( @xmath10 ) classification ,",
    "the sum - to - zero constraint on the loss function is also ubiquitously adopted@xcite . in particular ,",
    "the multi - class _ logitboost_@xcite algorithm was derived by explicitly averaging over @xmath8 base classes .",
    "the loss function adopted in our abc - mart is the same as in mart@xcite and _ logitboost_@xcite .",
    "all three algorithms assume the `` sum - to - zero '' constraint . however , we obtain different first and second derivatives of the loss function , from mart@xcite and _ logitboost_@xcite .",
    "see section [ sec_abc ] for details .    in terms of implementation ,",
    "our proposed abc - mart differs from the original mart algorithm only in a few lines of code . since mart is known to be a successful algorithm , much of our work is devoted to the empirical comparisons of abc - mart with mart .",
    "our experiment results on publicly available data sets will demonstrate that abc - mart could considerably improves mart .",
    "also , abc - mart reduces both the training and testing time by @xmath11 , which may be quite beneficial when @xmath6 is small .",
    "we notice that data sets in industry applications are often quite large ( e.g. , several million samples@xcite ) .",
    "publicly available data sets ( e.g. , uci repository ) , however , are mostly small . in our study , the _ covertype _ data set from the uci repository is reasonably large with 581,012 observations . + we first review the original mart algorithm and functional gradient boosting@xcite .",
    "the mart algorithm is the marriage of the functional gradient boosting and regression trees . given the training data set @xmath12 and a loss function @xmath13 , @xcite adopted a `` greedy stagewise '' approach to build an additive function @xmath14 , which is a sum of @xmath15 terms , @xmath16 such that , at each stage @xmath17 , @xmath18 to @xmath15 , @xmath19 here the function @xmath20 is the `` base learner '' or `` weak learner . '' in general , ( [ eqn_f_m_argmin ] )",
    "is still a difficult optimization problem .",
    "@xcite approximately conducted steepest descent in the function space , by solving a least square problem @xmath21 ^ 2,\\end{aligned}\\ ] ] where @xmath22_{f(\\mathbf(x ) ) = f^{(m-1)}(\\mathbf(x))}\\end{aligned}\\ ] ] is the negative gradient ( the steepest descent direction ) in the @xmath1-dimensional data space at @xmath23 . for obtaining another coefficient @xmath24 ,",
    "a line search is performed : @xmath25    a generic `` gradient boosting '' algorithm is described in alg .",
    "[ alg_gb ] , for any differentiable loss function @xmath13 . for multi - class classification , @xcite proposed mart , which implemented line 5 in alg .",
    "[ alg_gb ] by regression trees and line 7 by a one - step newton update within each terminal node of the trees .    1 : @xmath26 + 2 : for @xmath27 to @xmath15 do + 3 : for @xmath28 to @xmath8 do + 4 : @xmath29_{f(\\mathbf(x ) )",
    "= f^{(m-1)}(\\mathbf(x)))}$ ] , @xmath4 to @xmath1 .",
    "+ 5 : @xmath30 ^ 2 $ ] + 6 : @xmath31 + 7 : @xmath32 + 8 : end + 9 : end      for multi - class classification , mart adopted the following _ negative multinomial log - likelihood loss _ , which is also the loss function in _",
    "logitboost_@xcite : @xmath33 where @xmath34 if @xmath35 and @xmath36 otherwise .",
    "apparently , @xmath37 for any @xmath38 . here @xmath39 is the probability that the @xmath38th observation belongs to class @xmath40 : @xmath41    mart adopted the following logistic probability model@xcite @xmath42 or equivalently@xcite , @xmath43    apparently , the model ( [ eqn_logistic_f ] ) implies @xmath44 , the sum - to - zero constraint .",
    "in fact , since @xmath45 , the model only has @xmath8 degrees of freedom .",
    "some constraint on @xmath46 is necessary in order to obtain a unique solution . for binary ( @xmath9 )",
    "classification , the sum - to - zero constraint is automatically enforced .",
    "[ alg_mart ] describes the mart algorithm for multi - class classification using _ negative multinomial log - likelihood loss _",
    "( [ eqn_loss ] ) and multi - class logistic model ( [ eqn_logistic_p ] ) .    0 : @xmath34 , if @xmath47 , and @xmath48 otherwise .",
    "+ 1 : @xmath49 , @xmath50 to @xmath8 , @xmath4 to @xmath1 + 2 : for @xmath27 to @xmath15 do + 3 : for @xmath28 to @xmath8 do + 4 : @xmath51 + 5 : @xmath52-terminal node regression tree from @xmath53 + 6 : @xmath54 + 7 : @xmath55 + 8 : end + 9 : end    .",
    "[ alg_mart ]    mart follows the generic paradigm of functional gradient boosting alg .",
    "[ alg_gb ] . at each stage @xmath17",
    ", mart solves the mean square problem ( line 5 in alg .",
    "[ alg_gb ] ) by regression trees .",
    "mart builds @xmath6 regression trees at each boosting step . + we elaborate in more detail on serval key components of mart .",
    "mart performs gradient descent in the function space , using the gradient evaluated at the function values . for the @xmath38th data point , using the _ negative multinomial log - likelihood loss _ ( [ eqn_loss ] ) , i.e. , @xmath56 and the probability model ( [ eqn_logistic_p ] ) , @xcite showed @xmath57 this explains the term @xmath58 in line 5 of alg .",
    "[ alg_mart ] .",
    "while only the first derivatives were used for building the structure of the trees , mart used the second derivatives to update the values of the terminal nodes by a one - step newton procedure .",
    "@xcite showed @xmath59 which explains line 6 in alg .",
    "[ alg_mart ] .",
    "+ ( [ eqn_mart_d1 ] ) and ( [ eqn_mart_d2 ] ) were also derived in the _ logitboost_@xcite . however , in this paper we actually obtain different first and second derivatives .      for each @xmath38",
    ", there are @xmath6 function values , @xmath46 , @xmath50 to @xmath8 , and consequently @xmath6 gradients .",
    "mart builds @xmath6 regression tress at each boosting step .",
    "apparently , the constraint @xmath44 will not hold .",
    "note that one can actually re - center the @xmath46 at the end of every boosting step so that the sum - to - zero constraint is satisfied after training .",
    "that is , one can insert a line @xmath60 after line 8 in alg .",
    "[ alg_mart ] to make @xmath44 . however , we observe that this re - centering step makes no difference in our experiments .",
    "we believe the sum - to - zero constraint should be enforced before the training instead of after the training , at every boosting step .",
    "the case @xmath9 is an exception . because the two pseudo responses , @xmath61 and @xmath62 are identical with signs flipped ( i.e. , @xmath63 automatically holds ) , there is no need to build @xmath9 tress .",
    "in fact , @xcite presented the binary classification algorithm separately ( * ? ? ?",
    "5 ) , which is the same as ( * ? ? ?",
    "6 ) by letting @xmath9 , although the presentations were somewhat different .",
    "line 6 of alg .",
    "[ alg_mart ] contains a factor @xmath64 . for binary classification , it is clear that the factor @xmath65 comes from the mathematical derivation . for @xmath10",
    ", we believe this factor in a way approximates the constraint @xmath44 . because mart builds @xmath6 trees while there are only @xmath8 degrees of freedom , a factor @xmath64 may help reduce the influence .",
    "practitioners like mart partly because this great algorithm has only a few parameters , which are not very sensitive as long as they fall in some `` reasonable '' range .",
    "it is often fairly easy to identify the ( close to ) optimal parameters with limited tuning .",
    "this is a huge advantage , especially for large data sets .",
    "the number of terminal nodes , @xmath66 , determines the capacity of the base learner .",
    "mart suggested @xmath68 often might be a good choice .",
    "the shrinkage parameter , @xmath67 , should be large enough to make a sufficient progress at each boosting step and small enough to avoid over - fitting .",
    "also , a very small @xmath67 may require a large number of boosting steps , which may be a practical concern for real - world applications if the training and/or testing is time - consuming .",
    "@xcite suggested @xmath69 .    the number of boosting steps , @xmath15 , in a sense is largely determined by the computing time one can afford .",
    "a commonly - regarded merit of boosting is that over - fitting can be largely avoided for reasonable @xmath66 and @xmath67 and hence one might simply let @xmath15 be as large as possible",
    ". however , for small data sets , the training loss ( [ eqn_loss ] ) may reach the machine accuracy before @xmath15 can be too large .",
    "we re - iterate the two key components in developing abc - boost ( adaptive base class boost ) :    1 .   using the popular constraint in the loss function",
    ", we can choose a * base class * and derive the boosting algorithm for only @xmath8 classes .",
    "2 .   at each boosting step , we can * adaptively * choose the base class which has the `` worst '' performance .",
    "abc - mart is a concrete implementation of abc - boost by using the _ negative multinomial log - likelihood loss _ ( [ eqn_loss ] ) , the multi - class logistic model ( [ eqn_logistic_p ] ) , and the paradigm of functional gradient boosting .",
    "apparently , there are other possible implementations of abc - boost .",
    "for example , one can implement an `` abc - logitboost . ''",
    "this study focuses on abc - mart .",
    "since mart has been proved to be successful in large - scale industry applications , demonstrating that abc - mart may considerably improve mart will be appealing .",
    "we first derive some basic formulas needed for developing abc - mart . without loss of generality , we assume class 0 is the base class .",
    "lemma [ lem_derivative_p ] provides the first and second derivatives of the class probabilities @xmath39 under the multi - class logistic model ( [ eqn_logistic_p ] ) and the sum - to - zero constraint @xmath44@xcite .",
    "[ lem_derivative_p ] @xmath70 * proof : * note that @xmath71 .",
    "hence @xmath72 the other derivatives can be obtained similarly .",
    "@xmath73    lemma [ lem_derivative_p ] helps derive the derivatives of the loss function , presented in lemma [ lem_derivative_l ] .",
    "[ lem_derivative_l ] @xmath74 * proof : * @xmath75 it first derivative is @xmath76    and the second derivative is @xmath77 +    note the the first and second derivatives we derives differ from ( [ eqn_mart_d1 ] ) and ( [ eqn_mart_d2 ] ) , which are the derivatives used in _ logitboost_@xcite and mart@xcite .      alg .",
    "[ alg_abc - mart ] provides the pseudo code for abc - mart . compared with mart ( alg . [ alg_mart ] )",
    ", we use different gradients to build the trees and different second derivatives to update the values of terminal nodes .",
    "in addition , there is a procedure ( line 10 ) for selecting the base class , denoted by @xmath78 , which has the `` worst '' ( i.e. , largest ) training loss ( [ eqn_loss ] ) . at each boosting step , we only need to build @xmath8 trees because the constraint @xmath79 is enforced .    0 : @xmath34 , if @xmath47 , @xmath48 otherwise .",
    "choose a random base @xmath78 + 1 : @xmath49 ,   @xmath80 ,  @xmath50 to @xmath8 ,  @xmath4 to @xmath1 + 2 : for @xmath27 to @xmath15 do + 3 : for @xmath28 to @xmath8 , @xmath81 , do + 4 : @xmath52-terminal node regression tree from @xmath82 +   + 5 : @xmath83 +   + 6 : @xmath55 + 7 : end + 8 : @xmath84 ,   @xmath4 to @xmath1 .",
    "+ 9 : @xmath85 ,  @xmath50 to @xmath8 ,  @xmath4 to @xmath1 + 10 : @xmath86 ,  where    @xmath87 .",
    "+ 11 : end      note that the factor @xmath64 does not appear in abc - mart ( alg . [ alg_abc - mart ] ) .",
    "interestingly , when @xmath9 , abc - mart recovers mart .",
    "for example , consider @xmath88 , @xmath89 , @xmath90 , then @xmath91 in other words , the first ( second ) derivative is twice ( four times ) of the first ( second ) derivative in mart .",
    "using the one - step newton update ( line 6 in alg .",
    "[ alg_abc - mart ] ) , the factor @xmath65 ( which appeared in mart ) is recovered .",
    "note that scaling the first derivatives does not affect the tree structures .      in a sense",
    ", mart did consider the averaging affect from all @xmath8 base classes .",
    "for the first derivatives , the following equality holds , @xmath92 because @xmath93 in other words , the gradient used in mart is the averaged gradient of abc - mart .",
    "next , we can show that , for the second derivatives , @xmath94 with equality holding only when @xmath95 , because @xmath96    thus , even the second derivative used in mart may be approximately viewed as the averaged second derivatives in abc - mart . it appears the factor @xmath64 in mart may be reasonably replaced by @xmath97 ( both equal @xmath65 when @xmath9 ) .",
    "this , of course , will not make a real difference because the constant ( either @xmath64 or @xmath97 ) can be absorbed into the shrinkage factor @xmath67 .",
    "the goal of the evaluation study is to compare abc - mart ( alg . [ alg_abc - mart ] ) with mart ( alg . [ alg_mart ] ) for multi - class classification .",
    "the experiments were conducted on one fairly large data set ( _ covertype _ ) plus five small data sets ( _ letter , pendigits , zipcode , optdigits _ , and _ isolet _ ) ; see table [ tab_data ] .",
    "l r r r r + data set & @xmath6 & # training samples & # test samples & # features + covertype & 7 & 290506 & 290506 & 54 + letter & 26 & 16000 & 4000 & 16 + pendigits & 10 & 7494 & 3498 & 16 + zipcode & 10 & 7291 & 2007 & 256 + optdigits & 10 & 3823 & 1797 & 64 + isolet & 26 & 6218 & 1559 & 617 +    [ tab_data ]    in general , a comprehensive and fair comparison of two classification algorithms is a non - trivial task . in our case , however , the comparison task appears quite easy because abc - mart and mart differ only in a few lines of code and their underlying base learners can be completely identical .    ideally , we hope that abc - mart will improve mart , for every set of reasonable parameters , @xmath66 and @xmath67 . for the five small data sets , we experiment with every combination of number of terminal nodes @xmath66 and shrinkage @xmath67 , where @xmath66 and @xmath67 are chosen from @xmath98    for the five small data sets , we let the number of boosting steps @xmath99 .",
    "however , the experiments usually terminated well before @xmath99 because the machine accuracy was reached .    for the _ covertype _ data set , since it is fairly large , we only experimented with @xmath100 and @xmath101 and we limited @xmath15 to be 16000 , 11500 , 6000 , for @xmath102 , respectively .      the test misclassification error is a direct measure of performance .",
    "mart and abc - mart output @xmath6 class probabilities @xmath39 , @xmath50 to @xmath8 , for each observation @xmath38 . to obtain the class labels",
    ", we adopt the commonly used rule @xmath103    we define @xmath104 , the `` relative improvement of test mis - classification errors , '' as @xmath105    since we experimented with a series of parameters , @xmath66 , @xmath67 , and @xmath15 , we report the overall `` best '' ( i.e. , smallest ) mis - classification errors in table [ tab_summary ] . later , we will also report the more detailed mis - classification errors for every combination of @xmath66 and @xmath67 , in sections [ sec_covertype ] to [ sec_isolet ] .",
    "r r r l + data set & mart & abc - mart & @xmath104 ( % ) & @xmath106-value + covertype & 11133 & 10203 & 8.4 & @xmath107 + letter & 135 & 111 & 17.8 & 0.060 + pendigits & 123 & 104 & 15.5 & 0.100 + zipcode & 111 & 98 & 11.7 & 0.178 + optdigits & 56 & 41 & 26.8 & 0.061 + isolet & 84 & 69 & 17.9 & 0.107 +    [ tab_summary ]    table [ tab_summary ] also provides the `` @xmath106-value '' of the one - sided @xmath108-test .",
    "the idea is to model the test error rate ( i.e. , the test mis - classification errors divided by the number of test samples ) as a binomial probability and then conduct the @xmath108-test using the normal approximation of the difference of two binomial probabilities .",
    "we can see that for the _ covertype _ data set , the improvement of abc - mart over mart is highly significant ( the @xmath106-value is nearly zero ) under this test . for the five small data sets , the @xmath106-values are also reasonably small .",
    "we shall mention that this @xmath108-test is very stringent when the error rate is small .",
    "in fact , we do not often see papers which calculated the @xmath106-values when comparing different classification algorithms .",
    "+ next , we present the detailed experiment results on the six data sets .      table [ tab_covertype ] summarizes the test mis - classification errors along with the relative improvements ( @xmath104 ) , for every combination of @xmath109 and @xmath110 . for each @xmath66 and @xmath67 , the smallest test mis - classification errors , separately for abc - mart and mart , are the lowest points in the curves in figure [ fig_covertypetest ] .",
    "[ tab_covertype ]    to report the experiments in a more informative manner , figure [ fig_covertypetrain ] , figure [ fig_covertypetest ] , and figure [ fig_covertypeimprove ] , respectively , present the training loss , the test mis - classification errors , and the relative improvements , for the complete history of @xmath15 boosting steps ( iterations ) .",
    "figure [ fig_covertypetrain ] indicates that abc - mart reduces the training loss ( [ eqn_loss ] ) considerably and consistently faster than mart .",
    "figure [ fig_covertypetest ] demonstrates that abc - mart exhibits considerably and consistently smaller test mis - classification errors than mart .",
    "figure [ fig_covertypeimprove ] illustrates that the relative improvements of abc - mart over mart , in terms of the mis - classification errors , may be considerably larger than the numbers reported in table [ tab_covertype ] if we stop the training earlier",
    ". this phenomenon may be quite beneficial for real - world large - scale applications when either the training or test time is part of the performance measure .",
    "for example , real - time applications ( e.g. , search engines ) may not be able to afford to use models with a very large number of boosting steps .",
    "the next five subsections are respectively devoted to presenting the experiment results of five small data sets .",
    "the results exhibit similar characteristics across data sets .",
    "table [ tab_letter ] summarizes the test mis - classification errors along with the relative improvements for every combination of @xmath111 and @xmath112 .",
    "[ tab_letter ]    figure [ fig_lettertrain ] again indicates that abc - mart reduces the training loss ( [ eqn_loss ] ) considerably and consistently faster than mart .",
    "since this is a small data set , the training loss could approach zero even when the number of boosting steps is not too large .",
    "figure [ fig_lettertest ] again demonstrates that abc - mart exhibits considerably and consistently smaller test mis - classification errors than mart .",
    "[ tab_pendigits ]      [ tab_zipcode ]      .",
    "[ tab_optdigits ]      [ tab_isolet ]",
    "retrospectively , the ideas behind abc - boost and abc - mart appear simple and intuitive .",
    "our experiments in section [ sec_evaluations ] have demonstrated the effectiveness of abc - mart and its considerable improvements over mart .",
    "the two key components of abc - boost are : ( 1 ) developing boosting algorithms by assuming one * base class * ; ( 2 ) * adaptively * choosing the base class so that , at each boosting step , the `` worst '' class will be selected .",
    "we believe both components contribute critically to the good performance of abc - mart .",
    "note that assuming the sum - to - zero constraint on the loss function and the base class is a ubiquitously adopted strategy@xcite .",
    "our contribution in this part is the different set of derivatives of the loss function ( [ eqn_loss ] ) , compared with the classical work@xcite .",
    "one may ask two questions . 1 : can we use the mart derivatives ( [ eqn_mart_d1 ] ) and ( [ eqn_mart_d2 ] ) and adaptively choose the base ?",
    "2 : can we use abc - mart derivatives and a fixed base ? neither will achieve a good performance .    to demonstrate this point",
    ", we consider three alternative boosting algorithms and present their training and test results using the _ pendigits _ data set ( @xmath113 ) .    1 .",
    "_ mart derivatives + adaptively choosing the worst base_. it is the same as abc - mart except it uses the derivatives of mart . in figure",
    "[ fig_pendigits_comp ] , we label the corresponding curves by `` mb .",
    "_ abc - mart derivatives + a fixed base class chosen according to mart training loss_. in the experiment with mart , we find `` class 1 '' exhibits the overall largest training loss .",
    "thus , we fix `` class 1 '' as the base and re - train using the derivatives of abc - mart . in figure",
    "[ fig_pendigits_comp ] , we label the corresponding curves by `` b1 .",
    "abc - mart derivatives + a fixed base class chosen according to mart test mis - classification errors_. in the experiment with mart , we find `` class 7 '' exhibits the overall largest error .",
    "we fix `` class 7 '' as the base and re - train using the derivatives of abc - mart . in figure",
    "[ fig_pendigits_comp ] , we label the corresponding curves by `` b7 . ''",
    "figure [ fig_pendigits_comp ] demonstrate that none of the alternative boosting algorithms could outperform abc - mart .",
    "we present the general idea of * abc - boost * and its concrete implementation named * abc - mart * , for multi - class classification ( with @xmath7 classes ) . two key components of abc - boost include : ( 1 ) by enforcing the ( commonly used ) constraint on the loss function , we can derive boosting algorithms for @xmath8 class using a * base class * ; ( 2 ) we * adaptively * choose the current `` worst '' class as the base class , at each boosting step .",
    "both components are critical .",
    "our experiments on a fairly large data set and five small data sets demonstrate that abc - mart could considerably improves the original mart algorithm , which has already been highly successful in large - scale industry applications .",
    "the author thanks tong zhang for providing valuable comments to this manuscript . in late june 2008 ,",
    "the author visited stanford university and google in mountain view . during that visit , the author had the opportunity of presenting to professor friedman and professor hastie the algorithm and some experiment results of abc - mart ; and the author highly appreciates their comments and encouragement .",
    "also , the author would like to express his gratitude to phil long ( google ) and cun - hui zhang for the helpful discussions of this work , and to rich caruana who suggested the author to test the algorithm on the _ covertype _ data set .",
    "zhaohui zheng , keke chen , gordon sun , and hongyuan zha .",
    "a regression framework for learning ranking functions using relative relevance judgments . in _",
    "pages 287294 , amsterdam , the netherlands , 2007 ."
  ],
  "abstract_text": [
    "<S> we develop the concept of * abc*-boost ( * * a**daptive * * b**ase * * c**lass boost ) for multi - class classification and present * abc*-mart , a concrete implementation of * abc*-boost . </S>",
    "<S> the original mart ( * * m**ultiple * * a**dditive * * r**egression * * t**rees ) algorithm has been very successful in large - scale applications . for binary classification , </S>",
    "<S> abc - mart recovers mart . for multi - class classification , </S>",
    "<S> abc - mart considerably improves mart , as evaluated on several public data sets . </S>"
  ]
}