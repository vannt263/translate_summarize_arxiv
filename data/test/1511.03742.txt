{
  "article_text": [
    "the generic matrix - matrix multiplication ( gemm ) is given by the equation : @xmath0 where @xmath1 , @xmath2 and @xmath3 are matrices , and @xmath4 and @xmath5 are scalars .",
    "gemm is arguably the most popular computational kernel of the 20th century .",
    "the apparent simplicity of gemm has haunted generation after generation of researchers who have evaluated its performance on generation after generation of computer systems , while uncovering layer after layer of its hidden complexity .",
    "for example , discovering the beneficial effects of cache blocking on gemm performance  @xcite has fuelled research on locality optimizations in compilers for many years .    yet ,",
    "surprisingly , no common methodology for evaluating gemm performance has been established over the many decades of using this kernel for comparing architectures , compilers and ninja - class programmers .",
    "consequently , the reader of a report presenting gemm results is often left wondering :    * was the kernel specialized , for example , to @xmath6 ?",
    "( in other words , @xmath7 and @xmath8 . ) * which of the data types were used : single precision ( sgemm ) , double precision ( dgemm ) , complex single precision ( cgemm ) , or complex double precision ( zgemm ) ?",
    "* which data layouts were used : normal ( n ) or transposed ( t ) ? typically results in better locality , because @xmath9 is read row - wise . ]",
    "if transposed , did the execution time include the overhead for transposition ? * which data shapes were used : square or rectangular ? if rectangular , did the execution time depend on the ratio between the dimensions ? * which data sizes were used : small or large ? * on a system with caches , did ` large ' result in cache thrashing ; did ` small ' result in good locality ( no thrashing ) ? * on a heterogeneous system equipped with a discrete accelerator , did the execution time include the overhead for copying the data to the accelerator and back , or only the kernel execution time ? * did the evaluation include power or energy measurements ?",
    "* if a diesel generator was used to get the system running , how many megaflops per gallon were they getting ?",
    "* more seriously , have we achieved significant improvements in energy efficiency of floating - point operations over the last decade ? * how much human effort and ingenuity was involved in writing the kernel or in implementing the compiler that generated the kernel ? * can we compare the generators , for example , based on polyhedral compilation  @xcite and functional expression rewriting  @xcite in a fair way ( including code quality , code generation time and robustness ) ?",
    "* can we evaluate the generators against ninja - class programmers  @xcite or vendor libraries ? *",
    "have we used all the tricks up our sleeves to get the fastest gemm implementation for our hardware and problem at hand ? * can we _",
    "adapt _ our gemm implementations to work well across a range of architectures , data types , data sizes , etc . ?",
    "given that we are discussing gemm , a simple kernel intended to give us insights for solving more complex ` real - world ' problems , it is essential to start getting some of the answers right to facilitate our learning and knowledge sharing .",
    "we introduce gemmbench , a framework and methodology for evaluating performance of gemm implementations .",
    "gemmbench is implemented on top of collective knowledge ( ck ) , a lightweight framework for reproducible and collaborative r&d in computer systems .",
    "our initial implementation supports hand - written opencl kernels operating on matrices consisting of single- and double - precision floating - point values , and producing single or multiple output elements per work - item ( via thread coarsening and vectorization ) .    over time",
    ", we plan to involve the community to add further hand - written and generated kernels ( e.g. from @xcite ) , and , importantly , to collectively study the gemm performance across multiple platforms , data sizes and data types .",
    "the gemmbench framework reads from a json file the metadata describing a kernel .",
    "the json file specifies the data type ( s or d ) , the layout of the matrices ( n or t ) , the thread - coarsening configuration ( di for the number of rows and dj for the number of columns in a block computed by a single work - item ) , and so on .",
    "for example , the sgemm kernel that assumes that @xmath1 is non - transposed and @xmath2 is transposed and outputs a single element per work - item :    .... kernel void gemm (      global float const * restrict a ,      global float const * restrict b ,      global float        * restrict c ,      float alpha , float beta , uint n ) {      const uint j = get_global_id(0 ) ;      const uint",
    "i = get_global_id(1 ) ;        float abij = 0.0f ;      for ( uint k = 0 ; k <",
    "n ; k + = 1 )      {          abij + = a[i*n + k ] * b[j*n + k ] ;      }      c[i*n + j ] = alpha * abij + beta * c[i*n + j ] ; } ....    is described by the following metadata :    .... {      \" name \"    : \" sgemm_nt_1x1 \" ,      \" file \"    : \" sgemm_nt_1x1.cl \" ,      \" type \"    : \" s \" ,      \" transa \" : \" n \" ,      \" transb \" : \" t \" ,      \" dj \"      : 1 ,      \" di \"      : 1 } ....    see further examples in the dataset entries of the gemmbench repository .",
    "install collective knowledge ( ck ) _ e.g. _ :",
    ".... $ export ck_repos=~/ck $ mkdir $ ck_repos $ export ck_root=$ck_repos / ck $ git clone https://github.com/ctuning/ck.git $ ck_root $ export path=$ck_root / bin:$path $ ck status your version is up - to - date : v1.6.12 ....    for using ck in python scripts ( _ e.g. _  with ipython notebook ) :    .... $ cd $ ck_root & & sudo python setup.py install $ python -c \" import ck.kernel as ck ;               print ck.version({})['version_str ' ] \" 1.6.12 ....      install gemmbench along with other ck repositories it depends upon : ck - autotuning , ck - env , _ etc . _",
    ".... $ ck pull repo : gemmbench \\",
    "--url = https://github.com / dividiti / gemmbench ....      register an ( already installed ) opencl driver _",
    "e.g. _ :    .... $ ck find soft : lib.opencl * ~/ck / ck - env / soft / lib.opencl.linux ... $ ck setup soft : lib.opencl.linux ....    enter the opencl driver version ( a string to identify it later ) . enter the path to ` libopencl.so ` without ` lib ` _ e.g. _ ` /usr ` for ` libopencl.so ` in ` /usr",
    "/ lib ` .",
    "take a look at one of the available scripts for disabling frequency and voltage scaling ( dvfs ) and fixing the frequencies :    .... $ ck find platform.init : * ~/ck / ck - autotuning / platform.init / generic - android ~/ck / ck - autotuning / platform.init / chromebook - ubuntu ~/ck / ck - autotuning / platform.init / generic - odroid ~/ck / ck - autotuning / platform.init / generic - linux ....    customize the scripts called from ck - set - performance ( you may leave them blank initially ) and add them to the system path _",
    ":    .... $ cp ~/ck / ck - autotuning / platform.init / generic - linux \\",
    "~/ck / ck - autotuning / platform.init / my - linux - platform ... $ export path=$path:\\    ~/ck / ck - autotuning / platform.init / my - linux - platform ....    ( ensure the scripts are executable . )      to compile gemmbench :    .... $ ck compile program : gemmbench - cl - launcher-1.0 ....    ( the cjson and xopenme libraries should be installed automatically the first time you compile gemmbench . )      to run gemmbench with the default parameters :    .... $ ck run program : gemmbench - cl - launcher-1.0 ....    select the default command ( press `` enter '' ) , one of the four currently supported `` flavours '' ( sgemm nn , sgemm nt , dgemm nn , dgemm nt ) , and finally one of the kernel variants .    to override the default parameters , use _",
    "e.g. _    .... $ ck run program : gemmbench - cl - launcher-1.0 \\",
    "--extra_run_cmd =\"- p 1 -d 1 -n 512 -lws 4,16 \" ....    to run on platform 1 , device 1 , with the matrix order of 512 and the local work size of ( 4,16 ) .",
    ".... $ ck find script : sgemm * ~/ck / gemmbench / script / sgemm_nt $ cd ~/ck / gemmbench / script / sgemm_nt $ ./_clean_program_pipeline.sh $ ./_setup_program_pipeline.sh ... pipeline is ready !",
    "$ ./explore - f - n.sh $ ./explore - n - lws.sh ....      to replay our experiments , obtain our experimental data for this paper :    .... $ ck pull repo : gemmbench - adapt16 \\",
    "--url = https://github.com / dividiti / gemmbench - adapt16 $ ck find experiment : sgemm_nt * ~/ck / gemmbench - adapt16/experiment / sgemm_nt - explore - f - n ~/ck / gemmbench - adapt16/experiment / sgemm_nt - explore - n - lws ....    start the ck web server :    .... $ ck start web ....",
    "open http://localhost:3344 in a web browser .",
    "select gemmbench - adapt16 in the `` repository '' dropdown menu .",
    "open sgemm_nt - explore - f - n or sgemm_nt - explore - n - lws .",
    "select gemmbench - view in the `` select experiment view '' menu under a qr code .",
    "the table shows one experiment ( with a number of statistical repetitions ) per row .",
    "click on a `` copy to clipboard '' button in the rightmost column to obtain a command to replay the corresponding experiment .",
    "explains the meaning of most other columns .",
    "] for example , to replay the intermittently failing experiment mentioned in section  [ sec : validating ] , run :    .... $ ck replay \\",
    "experiment:8bcbe025bd8803c2 --point = ca4a5dbe25613c7d ....",
    "we demonstrate using the gemmbench framework for evaluating 3 sgemm nt opencl kernels on a hardkernel odroid  xu3 board ( table  [ odroid ] ) .",
    "the odroid xu3 board has 4 integrated power consumption sensors :    * for the lpddr3 ram ; * for the cpu cluster 0 comprised of 4 arm cortex - a7 ( `` little '' ) cores ; * for the cpu cluster 1 comprised of 4 arm cortex - a15 ( `` big '' ) cores ; * for both the gpu cluster 0 and the gpu cluster 1 comprised respectively of 4 and 2 arm mali - t628 cores .",
    "we reused the _ pipeline _ functionality of the underlying collective knowledge framework to conduct experiments under controlled conditions .",
    "we ran the sgemm kernels on the gpu cluster 0 ( opencl device 0 ) ; we disabled dynamic voltage and frequency scaling ( dvfs ) and set the frequency to the maximum of 600 mhz .",
    "likewise , we set the cpu governors to the performance mode and the cpu frequencies to the maximum .      the sgemm nt kernels are contained in 3 separate files :    * ` sgemm_nt_1x1.cl ` : a nave version shown in section  [ sec : implementation ] which computes a single element of matrix @xmath3 per work - item ; * ` sgemm_nt_4x1.cl ` : a vectorised version which computes a vector of four adjacent elements of matrix @xmath3 per work - item ; * ` sgemm_nt_4x1_barrier.cl ` : a similarly vectorised version which synchronises work - items in a work - group with a barrier to improve cache utilisation  @xcite .    [ cols=\"<,<\",options=\"header \" , ]     for the ` sgemm_nt_1x1.cl ` program and the matrix order of 64 , no results matched .",
    "in fact , the maximum absolute differences suggest a possible bug in either the opencl or the reference implementation .",
    "( neither has been around for long or code - reviewed . )    for the ` sgemm_nt_4x1.cl `",
    "program and the matrix order of 64 , the results are mixed : they twice matched and twice did not .",
    "the failures may be difficult to debug , since they are intermittent .",
    "luckily , replaying an experimental point under the collective knowledge framework is a matter of running a single command for the command to replay this very experiment . ] which would help investigate the failures and quickly test potential solutions .    for the ` sgemm_nt_4x1_barrier.cl ` program ,",
    "the results show repeatable failures for the orders of 64 , 96 , 192 .",
    "this may give us a clue to what goes wrong here .",
    "returning to choosing the value of @xmath10 , we note that these results would pass under @xmath11 .",
    "it is likely , however , that even this @xmath10 would need to be changed had the input values been drawn from a different distribution _",
    "e.g. _  the uniform distribution over the range @xmath12 .    rather than making an arbitrary choice of @xmath10 for an arbitrary choice of the random distribution",
    ", we could look into defining representative datasets .",
    "ideally , they would come from real - world problems along with precision requirements .",
    "we have presented gemmbench , a framework and methodology for systematically evaluating performance of matrix multiplication implementations .",
    "our initial implementation supports hand - written opencl kernels , producing single or multiple output elements per work - item ( via thread coarsening and vectorization ) .",
    "our goal is to involve the community to extend gemmbench to evaluate performance of compiler - generated opencl kernels , non - opencl implementations , library implementations and so on , across many target platforms . to this end ,",
    "the underlying collective knowledge framework provides unique opportunities for the community to gradually gather and share valuable knowledge for optimizing performance of matrix multiplication and other programs , as well as of compilers and processors .",
    "we will build upon other strengths of the collective knowledge framework including support for multiple operating systems ( windows , linux , android and macos ) , compilers ( llvm , gcc , icc , msvc , _ etc . _ ) and interfaces to packages for data mining and predictive analytics .    where do we start ?",
    "first , we encourage the interested reader help us investigate the failures reported in section  [ sec : validating ] .",
    "eric s.  raymond s proposition that `` given enough eyeballs , all bugs are shallow '' should apply well in this case .",
    "indeed , some of the failures may have nothing to do with numerical ( in)stability . programming errors ( such as tacit assumptions ) may be detected with static and dynamic analysis tools such as gpuverify  @xcite and oclgrind  @xcite . in the final version of this article",
    ", we will acknowledge those who help us explain the existing failures and perhaps find new ones , and of course fix them .",
    "second , we welcome contributions in the form of experimental data in the collective knowledge format . the contributors should acknowledge their compliance with the terms of use of their systems , specifically that they do not breach confidentiality .. as far as we are aware , the standard hardkernel bsp is distributed without such limitations . ]",
    "third , we welcome contributions in the form of improvements for the core gemmbench code _ e.g. _  support for rectangular matrices and complex floating - point numbers .",
    "fourth , we welcome contributions in the form of opencl kernels or other implementations .",
    "the initial set of kernels optimised for the arm mali - t600 architecture is intentionally small .",
    "we would like to see contributed kernels optimised for different architectures .",
    "the contributors should acknowledge their copyright in the source code and specify the licensing terms .",
    "fifth , we welcome contributions in the form of representative datasets or their descriptions .",
    "we envision gemmbench to inspire community - driven development of other representative workloads for use in performance evaluation and optimisation of computer systems .",
    "we thank grigori fursin , cto of dividiti and chief scientist of the ctuning foundation , for designing and implementing the collective knowledge framework , on top of which we implemented gemmbench .",
    "we look forward to the public discussion of this draft and hope to get useful feedback and contributions from the community , which will be acknowledged in the final version of this article .    1    u.  beaugnon , a.  kravets , s.  van haastregt , r.  baghdadi , d.  tweed , j.  absar , and a.  lokhmotov .",
    ": a vehicle for optimized basic linear algebra . in",
    "_ proceedings of the 2014 sigplan / sigbed conference on languages , compilers and tools for embedded systems _ , lctes 14 , pages 115124 , new york , ny , usa , 2014 .",
    "a.  betts , n.  chong , a.  donaldson , s.  qadeer , and p.  thomson .",
    "gpuverify : a verifier for gpu kernels . in _ proceedings of the acm international conference on object oriented programming systems languages and applications _ , oopsla 12 , pages 113132 , new york , ny , usa , 2012 .",
    "g.  fursin , a.  memon , c.  guillon , and a.  lokhmotov . : towards performance- and cost - aware software engineering as a natural science .",
    "g.  fursin , r.  miceli , a.  lokhmotov , m.  gerndt , m.  baboulin , d.  malony , allen , z.  chamski , d.  novillo , and d.  d. vento . : towards practical and collaborative auto - tuning . , 22(4):309329 , july 2014 .",
    "k.  goto and r.  a. v.  d. geijn .",
    "anatomy of high - performance matrix multiplication .",
    ", 34(3):12:112:25 , may 2008 .",
    "j.  gronqvist and a.  lokhmotov . optimizing opencl kernels for the arm mali - t600 gpus . in _",
    "gpu pro 5 : advanced rendering techniques_. 2014 .",
    "m.  d. lam , e.  e. rothberg , and m.  e. wolf . the cache performance and optimizations of blocked algorithms . in _ proceedings of the fourth international conference on architectural support for programming languages and operating systems _ , asplos iv , pages 6374 , new york , ny , usa , 1991 .",
    "j.  price and s.  mcintosh - smith .",
    "oclgrind : an extensible opencl device simulator . in _ proceedings of the 3rd international workshop on opencl _ , 2015 .",
    "m.  steuwer , c.  fensch , s.  lindley , and c.  dubach .",
    "generating performance portable code using rewrite rules : from high - level functional expressions to high - performance opencl code . in _ proceedings of the 20th acm sigplan international conference on functional programming _ ,",
    "icfp 2015 , pages 205217 , new york , ny , usa , 2015 .",
    "sharing gemmbench experimental results is easy !",
    "optionally , provide a license file and update further details in .cm / info.json and .cm / meta.json in your experimental entries .",
    "( if not provided , the standard ck license and copyright details will apply . )        please do not forget to send us a link to your new repository !",
    "we will maintain a list of such repositories for the community to access , similarly to this paper s repository ( section  [ sec : reproduce ] ) ."
  ],
  "abstract_text": [
    "<S> the generic matrix - matrix multiplication ( gemm ) is arguably the most popular computational kernel of the 20th century . yet , </S>",
    "<S> surprisingly , no common methodology for evaluating gemm performance has been established over the many decades of using gemm for comparing architectures , compilers and ninja - class programmers .    </S>",
    "<S> we introduce gemmbench , a framework and methodology for evaluating performance of gemm implementations . </S>",
    "<S> gemmbench is implemented on top of collective knowledge ( ck ) , a lightweight framework for reproducible and collaborative r&d in computer systems . </S>",
    "<S> using ck allows the r&d community to crowdsource hand - written and compiler - generated gemm implementations and to study their performance across multiple platforms , data sizes and data types . </S>",
    "<S> our initial implementation supports hand - written opencl kernels operating on matrices consisting of single- and double - precision floating - point values , and producing single or multiple output elements per work - item ( via thread coarsening and vectorization ) . </S>"
  ]
}