{
  "article_text": [
    "we consider the following optimization problem @xmath4},\\end{aligned}\\ ] ] where @xmath5 is a non - linear function , @xmath6 denotes the response variable , @xmath7 denotes the predictor ( or covariate ) , and the expectation is over the joint distribution of @xmath8 .",
    "the above minimization is called a generalized linear problem in its canonical representation , and it is commonly encountered in the statistical learning .",
    "celebrated examples include binary classification with smooth surrogate losses @xcite , and generalized linear models ( glms ) such as poisson regression , logistic regression , ordinary least squares , multinomial regression and many applications involving graphical models @xcite .",
    "these methods play a crucial role in numerous machine learning and statistics problems , and provide a miscellaneous framework for many regression and classification tasks .",
    "the exact minimization of the stochastic optimization problem , requires the knowledge of the underlying distribution of the variables @xmath9 . in practice , however , the joint distribution is not available . therefore , after observing @xmath0 independent data points @xmath10 , the standard approach is to minimize the empirical risk approximation given as @xmath11 in the case of glms , the empirical risk minimization given in",
    "is called the maximum likelihood estimation , whereas in the case of binary classification , it is generally referred to as surrogate loss minimization . due to non - linear structure of the optimization task given in , for both problems",
    ", the minimization of the empirical risk requires iterative methods .",
    "regardless of the problem formulation , the most commonly used optimization method is the newton - raphson method , which may be viewed as a reweighted least squares algorithm @xcite .",
    "this method uses a second - order approximation to benefit from the curvature of the log - likelihood and achieves locally quadratic convergence .",
    "a drawback of this approach is its excessive per - iteration cost of @xmath12 . to remedy this ,",
    "hessian - free krylov sub - space based methods such as conjugate gradient and minimal residual are used , but the resulting direction is imprecise @xcite . on the other hand ,",
    "first - order approximation yields the gradient descent algorithm , which attains a linear convergence rate with @xmath13 per - iteration cost .",
    "although its convergence rate is slow compared to that of the second - order methods , its modest per - iteration cost makes it practical for large - scale problems . in the regime @xmath14",
    ", another popular optimization technique is the class of quasi - newton methods @xcite , which can attain a per - iteration cost of @xmath13 , and the convergence rate is locally super - linear ; a well - known member of this class of methods is the bfgs algorithm @xcite .",
    "there are recent studies that exploit the special structure of glms @xcite , and achieve near - quadratic convergence with a per - iteration cost of @xmath15 , and an additional cost of covariance estimation .    in this paper , we take an alternative approach for minimizing , based on an identity that is well - known in some areas of statistics , but appears to have received relatively little attention for its computational implications in large - scale problems .",
    "let @xmath16 denote the true minimizer of the population risk given in , and let @xmath17 denote the corresponding ordinary least squares ( ols ) coefficients defined as @xmath18}^{-1}{\\mathbb{e}\\left[xy\\right]}$ ] . then , under certain random predictor ( design ) models , @xmath19 for logistic regression with gaussian design ( which is equivalent to fisher s discriminant analysis ) ,",
    "was noted by fisher in the 1930s @xcite ; a more general formulation for models with gaussian design is given in @xcite .",
    "the relationship suggests that if the constant of proportionality is known , then @xmath16 can be estimated by computing the ols estimator , which may be substantially simpler than minimizing the empirical risk .",
    "in fact , in some applications like binary classification , it may not be necessary to find the constant of proportionality in .",
    "our work in this paper builds on this idea .",
    "our contributions can be summarized as follows .    1 .",
    "we show that @xmath16 is approximately proportional to @xmath17 in the random design setting , regardless of the covariate ( predictor ) distribution .",
    "that is , we prove @xmath20 for some @xmath21 which depends on the non - linearity @xmath22 .",
    "our generalization uses zero - bias transformations @xcite .",
    "we also show that the above relation still holds under certain types of regularization .",
    "we design a computationally efficient estimator for @xmath16 by first estimating the ols coefficients , and then estimating the proportionality constant @xmath23 via line search .",
    "we refer to the resulting estimator as the scaled least squares ( sls ) estimator and denote it by @xmath24 . after estimating the ols coefficients",
    ", the second step of our algorithm involves finding a root of a real valued function ; this can be accomplished using iterative methods with up to a cubic convergence rate and only @xmath25 per - iteration cost .",
    "this is cheaper than the classical batch methods mentioned above by at least a factor of @xmath3 .",
    "3 .   for random design with sub - gaussian predictors ,",
    "we show that @xmath26 this bound characterizes the performance of the proposed estimator in terms of data dimensions , and justifies the use of the algorithm in the regime @xmath2 .",
    "we demonstrate how to transform a binary classification problem with smooth surrogate loss into a generalized linear problem , and how our methods can be applied to obtain a computationally efficient optimization scheme .",
    "we further discuss the canonicalization of the square loss , which may be of independent interest to non - convex optimization community .",
    "we propose a scalable algorithm for converting one generalized linear problem to another by exploiting the proportionality relation .",
    "the proposed algorithm requires only @xmath27 per each iteration , with no additional cost .",
    "we study the statistical and computational performance of @xmath24 , and compare it to that of the empirical risk minimizer ( using several well - known implementations ) , on a variety of large - scale datasets .",
    "the rest of the paper is organized as follows : section [ sec::related - work ] surveys the related work and section [ sec::preliminaries ] introduces the required background and the notation . in section [ sec::equivalence ] , we provide the intuition behind the relationship , which are based on exact calculations for the gaussian design setting . in section [ sec::algorithm ] ,",
    "we propose our algorithm and discuss its computational properties .",
    "theoretical results are given in section [ sec::theory ] . in section [ sec::conversion ]",
    ", we propose an algorithm to convert one glm type to another .",
    "we discuss how a binary classification problem can be cast as a generalized linear problem in section [ sec::binary - classification ] , and in section [ sec::canonicalization ] we propose a method to canonicalize the square loss .",
    "section [ sec::experiments ] provides a thorough comparison between the proposed algorithm and other existing methods .",
    "finally , we conclude with a brief discussion in section [ sec::discussion ] .",
    "as mentioned in section [ sec::intro ] , the relationship is well - known in several forms in statistics .",
    "brillinger @xcite derived for models with gaussian predictors using stein s lemma .",
    "li & duan @xcite studied model misspecification problems in statistics and derived when the predictor distribution has linear conditional means ( this is a slight generalization of gaussian predictors ) . the relation has led to various techniques for dimension reduction @xcite , and more recently , it has been studied by @xcite in the context of compressed sensing .",
    "it has been shown that the standard lasso estimator may be very effective when used in models where the relationship between the expected response and the signal is nonlinear , and the predictors ( i.e. the design or sensing matrix ) are gaussian .",
    "a common theme for all of this previous work is that it focuses solely on settings where holds exactly and the predictors are gaussian ( or , in the case of @xcite , very nearly gaussian ) .",
    "two key novelties of the present paper are ( i ) our focus on the computational benefits following from for large scale problems with @xmath2 ; and ( ii ) our rigorous finite sample analysis of models with non - gaussian predictors , where is shown to be approximately valid . to the best of our knowledge ,",
    "the present paper and its earlier version @xcite are the first to consider the relation in the context of optimization .",
    "we assume a random design setting , where the observed data consists of @xmath0 random iid pairs @xmath28 , @xmath29 , @xmath30 , @xmath31 ; @xmath32 is the response variable and @xmath33 is the vector of predictors or covariates .",
    "we focus on problems where the minimization is desirable , but we do not need to assume that @xmath10 are actually drawn from a particular distribution or the corresponding statistical model ( i.e. we allow for model misspecification ) .",
    "@xmath34}.\\ ] ]    while we make no assumptions on @xmath35 beyond smoothness , note that when the optimization problem is glm , and @xmath35 is the cumulant generating function for @xmath36 , then the problem reduces to the standard glm with canonical link and regression parameters @xmath16 @xcite .",
    "examples of glms in this form include logistic regression with @xmath37 , poisson regression with @xmath38 , and linear regression ( least squares ) with @xmath39 .",
    "our objective is to find a computationally efficient estimator for @xmath16 .",
    "the alternative estimator for @xmath16 proposed in this paper is related to the ols coefficient vector , which is defined by @xmath40^{-1}{\\mathbb{e}\\left[x_iy_i\\right]}$ ] ; the corresponding ols estimator is @xmath41 , where @xmath42 is the @xmath43 design matrix and @xmath44 .    additionally , throughout the text we let @xmath45\\!=\\ ! \\{1,2, ...",
    ",m \\}$ ] , for positive integers @xmath46 , and we denote the size of a set @xmath47 by @xmath48 .",
    "the @xmath46-th derivative of a function @xmath49 is denoted by @xmath50 . for a vector @xmath51 and a @xmath52 matrix @xmath53 ,",
    "we let @xmath54 and @xmath55 denote the @xmath56-vector and -operator norms , respectively . if @xmath57 $ ] , let @xmath58 denote the @xmath59 matrix obtained from @xmath53 by extracting the rows that are indexed by @xmath47 . for a symmetric matrix @xmath60 , @xmath61 and",
    "@xmath62 denote the maximum and minimum eigenvalues , respectively , and @xmath63 denotes the condition number of @xmath64 with respect to @xmath65-norm .",
    "we denote by @xmath66 the @xmath67-variate normal distribution , and all expectations are over all randomness inside the brackets .",
    "finally , we use @xmath68 and @xmath69 interchangeably , whichever is convenient ( where @xmath70 refers to the big o notation ) .",
    "to motivate our methodology , we assume in this section that the covariates are multivariate normal , as in @xcite . these distributional assumptions will be relaxed in section [ sec::theory ] .",
    "[ prop::equivalence ] assume that the covariates are multivariate normal with mean 0 and covariance matrix @xmath71 , i.e. @xmath72 . then @xmath16 can be written as @xmath73 where @xmath21 is the fixed point of the mapping @xmath74}^{-1}.    \\end{aligned}\\ ] ]    the optimal point in the optimization problem , has to satisfy the following normal equations , @xmath75 } = { \\mathbb{e}\\left[x_i \\dcgf(\\<x_i,\\beta\\>)\\right]}.    \\end{aligned}\\ ] ] now , denote by @xmath76 the multivariate normal density with mean 0 and covariance matrix @xmath71 .",
    "we recall the well - known property of gaussian density @xmath77 . using this and integration by parts on the right hand side of the above equation",
    ", we obtain @xmath78 } = & \\int x \\dcgf(\\<x,\\beta\\ > ) \\phi(x\\mid\\sig ) \\ \\d      x , \\\\      = & \\sig \\beta \\",
    "\\underbrace{\\mathbb{e}\\big[\\ddcgf(\\<x_i,\\beta\\>)\\big]}_{\\in\\   \\reals},\\nonumber\\end{aligned}\\ ] ] which is basically the stein s lemma . combining this with the normal equations and multiplying both side with @xmath79 , we obtain the desired result .",
    "proposition [ prop::equivalence ] and its proof provide the main intuition behind our proposed method .",
    "observe that in our derivation , we only worked with the right hand side of the normal equations which does not depend on the response variable @xmath80 . therefore , the equivalence will hold regardless of the joint distribution of @xmath10 .",
    "this is the main difference from the proof of @xcite where @xmath80 is assumed to follow a single index model . in section [ sec::theory ] , where we extend the method to non - gaussian predictors , the identity is generalized via the zero - bias transformations @xcite .",
    "a version of proposition [ prop::equivalence ] incorporating regularization  an important tool for datasets where @xmath1 is large relative to @xmath0 or the predictors are highly collinear  is also possible , as outlined briefly in this section .",
    "we focus on @xmath81-regularization ( ridge regression ) in this section ; some connections with lasso ( @xmath82-regularization ) are discussed in section [ sec::theory ] and corollary [ cor::lasso ] .    for @xmath83 ,",
    "define the @xmath84-regularized empirical risk minimizer , @xmath85 } +    \\frac{\\lambda}{2 } { \\left\\|\\beta\\right\\|_2}^2\\end{aligned}\\ ] ] and the corresponding @xmath81-regularized ols coefficients @xmath86 } + \\lambda \\i \\right)^{-1}{\\mathbb{e}\\left[x_iy_i\\right]}$ ] ( so @xmath87 and @xmath88 ) .",
    "the same argument as above implies that @xmath89 this suggests that the ordinary ridge regression for the linear model can be used to estimate the @xmath81-regularized empirical risk minimizer @xmath90 .",
    "further pursuing these ideas for problems where regularization is a critical issue may be an interesting area for future research .",
    "data @xmath91 for a sub - sampling based ols estimator , let @xmath92 $ ] be a random subset and take @xmath93 .",
    "use newton s root - finding method : initialize @xmath94 ; repeat until convergence : @xmath95 .",
    ": @xmath96 .    motivated by the results in the previous section ,",
    "we design a computationally efficient algorithm that approximates the stochastic optimization problem that is as simple as solving the least squares problem ; it is described in algorithm [ alg::1 ] .",
    "the algorithm has two basic steps .",
    "first , we estimate the ols coefficients , and then in the second step we estimate the proportionality constant via a simple root - finding algorithm",
    ".    there are numerous fast optimization methods to solve the least squares problem , and even a superficial review of these could go beyond the page limits of this paper .",
    "we emphasize that this step ( finding the ols estimator ) does not have to be iterative and it is the main computational cost of the proposed algorithm .",
    "we suggest using a sub - sampling based estimator for @xmath17 , where we only use a subset of the observations to estimate the covariance matrix .",
    "let @xmath92 $ ] be a random sub - sample and denote by @xmath97 the sub - matrix formed by the rows of @xmath98 in @xmath47 .",
    "then the sub - sampled ols estimator is given as @xmath99 .",
    "properties of sub - sampling and sketching based estimators have been well - studied @xcite . for sub - gaussian covariates , it suffices to use a sub - sample size of @xmath100 @xcite .",
    "hence , this step requires a single time computational cost of @xmath101 . for other approaches ,",
    "we refer reader to @xcite and the references therein .     logistic regression with iid standard gaussian design .",
    "the left plot shows the computational cost ( time ) for finding the mle and sls as @xmath0 grows and @xmath102 .",
    "the right plot depicts the accuracy of the estimators . in the regime where the mle is expensive to compute ,",
    "the sls is found much more rapidly and has the same accuracy . `",
    "r ` s built - in functions are used to find the mle .",
    ", title=\"fig : \" ]   logistic regression with iid standard gaussian design .",
    "the left plot shows the computational cost ( time ) for finding the mle and sls as @xmath0 grows and @xmath102 .",
    "the right plot depicts the accuracy of the estimators . in the regime where the mle is expensive to compute ,",
    "the sls is found much more rapidly and has the same accuracy . `",
    "r ` s built - in functions are used to find the mle .",
    ", title=\"fig : \" ]    the second step of algorithm [ alg::1 ] involves solving a simple root - finding problem . as with the first step of the algorithm ,",
    "there are numerous methods available for completing this task .",
    "newton s root - finding method with quadratic convergence or halley s method with cubic convergence may be appropriate choices .",
    "we highlight that this step costs only @xmath27 per - iteration and that we can attain up to a cubic rate of convergence .",
    "the resulting per - iteration cost is cheaper than other commonly used batch algorithms by at least a factor of @xmath103  indeed , the cost of computing the gradient is @xmath15 . for simplicity",
    ", we use newton s root - finding method .",
    "correct initialization of the scaling constant @xmath94 depends on the optimization problem . for example , in the case of glm problems , assuming that the glm is a good approximation to the true conditional distribution , by the law of total variance and basic properties of glms , we have @xmath104 }     + { \\mathrm{var}\\left({\\mathbb{e}\\left[y_i\\mid x_i\\right]}\\right)}\\approx c_\\cgf^{-1 }    + \\text{var}\\big(\\dcgf({{\\langle x_i,\\beta \\rangle}})\\big).\\end{aligned}\\ ] ] it follows that the initialization @xmath105 is reasonable as long as @xmath106 } $ ] is not much smaller than @xmath107 .",
    "our experiments show that sls is very robust to initialization .    in figure",
    "[ fig::mle - vs - our ] , we compare the performance of our sls estimator to that of the mle in a glm optimization problem , when both are used to analyze synthetic data generated from a logistic regression model under general gaussian design with randomly generated covariance matrix .",
    "the left plot shows the computational cost of obtaining both estimators as @xmath0 increases for fixed @xmath1 .",
    "the right plot shows the accuracy of the estimators . in the regime @xmath2  where the mle is hard to compute",
    " the mle and the sls achieve the same accuracy , yet sls has significantly smaller computation time .",
    "we refer the reader to section [ sec::theory ] for theoretical results characterizing the finite sample behavior of the sls .",
    "in this section , we use the zero - bias transformations @xcite to generalize the equivalence relation given in the previous section to the settings where the covariates are non - gaussian .    [ def::zb ]",
    "let @xmath108 be a random variable with mean 0 and variance @xmath109 .",
    "then , there exists a random variable @xmath110 that satisfies @xmath111}= \\sigma^2\\mathbb{e}[f^{(1)}(z^ * ) ] ,    $ ]  for all differentiable functions @xmath112 .",
    "the distribution of @xmath110 is said to be the @xmath108-zero - bias distribution .",
    "the existence of @xmath110 in definition [ def::zb ] is a consequence of riesz representation theorem @xcite .",
    "the normal distribution is the unique distribution whose zero - bias transformation is itself ( i.e. the normal distribution is a fixed point of the operation mapping the distribution of @xmath108 to that of @xmath110  which is basically stein s lemma ) .    to provide some intuition behind the usefulness of the zero - bias transformation , we refer back to the proof of proposition [ prop::equivalence ] . for simplicity ,",
    "assume that the covariate vector @xmath113 has iid entries with mean 0 , and variance 1 .",
    "then the zero - bias transformation applied to the @xmath114-th normal equation in yields @xmath115 } = { \\mathbb{e}\\left[x_{ij}\\dcgf\\big(x_{ij}\\beta_j + \\sigma_{k \\neq j}x_{ik } \\beta_k\\big)\\right ] }    } _",
    "\\text{$j$-th normal equation }    =    \\underbrace {      \\beta_j{\\mathbb{e}\\left[\\ddcgf\\left(x_{ij}^*\\beta_j + \\sigma_{k \\neq j}x_{ik } \\beta_{ik } \\right)\\right ] }    } _ \\text{zero - bias transformation}.\\end{aligned}\\ ] ] the distribution of @xmath116 is the @xmath117-zero - bias distribution and is entirely determined by the distribution of @xmath117 ; general properties of @xmath116 can be found , for example , in @xcite . if @xmath118 is well spread , it turns out that taken together , with @xmath119 , the far right - hand side in behaves similar to the right side of , with @xmath120 ; that is , the behavior is similar to the gaussian case , where the proportionality relationship given in proposition [ prop::equivalence ] holds .",
    "this argument leads to an approximate proportionality relationship for problems with non - gaussian predictors , which , when carried out rigorously , yields the following result .",
    "[ thm::population - bound ] suppose that the whitened covariates @xmath121 are independent with mean 0 , covariance @xmath122 , and have sub - gaussian norm bounded by @xmath123 .",
    "furthermore , @xmath124 s have constant first and second conditional moments , i.e. , @xmath125 $ ] and @xmath126 , @xmath127 $ ] and @xmath128 $ ] are constant .",
    "let @xmath129 and assume @xmath16 is @xmath130-well - spread in the sense that @xmath131 for some @xmath132 $ ] , and the function @xmath133 is lipschitz continuous with constant @xmath65 .",
    "then , for @xmath134}$ ] , and @xmath135 denoting the condition number of @xmath136 , we have @xmath137    theorem [ thm::population - bound ] is proved in the appendix .",
    "it implies that the population parameters @xmath17 and @xmath16 are approximately equivalent up to a scaling factor , with an error bound of @xmath138 . the assumption that @xmath16 is well - spread can be relaxed with minor modifications .",
    "for example , if we have a sparse coefficient vector , where @xmath139 is the support set of @xmath16 , then theorem [ thm::population - bound ] holds with @xmath1 replaced by the size of the support set .",
    "the assumptions on the conditional moments are the relaxed versions of assumptions that are commonly encountered in dimension reduction techniques .",
    "for example , sliced inverse regression methods assume that the first conditional moment @xmath140}$ ] is linear in @xmath141 for all @xmath118 @xcite , which is satisfied by elliptically distributed random vectors .",
    "an important case that is not covered by these methods is the independent coordinate case , i.e. , when the whitened covariates have independent , but not necessarily identical entries .",
    "it is straightforward to observe that this case satisfies the assumptions of theorem [ thm::population - bound ] .",
    "we refer reader to @xcite , for a good review of dimension reduction techniques and their corresponding assumptions .",
    "we also highlight that our moment assumptions can be relaxed further , at the expense of introducing some additional complexity into the results",
    ".    an interesting consequence of theorem [ thm::population - bound ] and the remarks following the theorem is that whenever an entry of @xmath16 is zero , the corresponding entry of @xmath17 has to be small , and conversely . for @xmath83 , define the lasso coefficients @xmath142 } + \\lambda { \\left\\|\\beta\\right\\|_1}.\\end{aligned}\\ ] ]    [ cor::lasso ] for any @xmath143 , if @xmath144 } = 0 $ ] and @xmath145 } = \\i$ ] , we have @xmath146 further , if @xmath147 and @xmath16 also satisfy that @xmath148 , @xmath149 , then we have @xmath150    so far in this section , we have only discussed properties of the population parameters , such as @xmath16 and @xmath17 . in the remainder of this section , we turn our attention to results for the estimators that are the main focus of this paper ; these results ultimately build on our earlier results , i.e. theorem [ thm::population - bound ] .    in order to precisely describe the performance of @xmath24 ,",
    "we first need bounds on the ols estimator .",
    "the ols estimator has been studied extensively in the literature ; however , for our purposes , we find it convenient to derive a new bound on its accuracy .",
    "while we have not seen this exact bound elsewhere , it is very similar to theorem 5 of @xcite .",
    "[ prop::ols - rate ] assume that @xmath144 } = 0 $ ] , @xmath145 } = \\sig$ ] , and that @xmath151 and @xmath80 are sub - gaussian with norms @xmath123 and @xmath152 , respectively . for @xmath153 denoting the smallest eigenvalue of @xmath71 , and @xmath154 , @xmath155 with probability at least @xmath156 , where @xmath157 depends only on @xmath152 and @xmath123 .",
    "proposition [ prop::ols - rate ] is proved in the supplementary material .",
    "our main result on the performance of @xmath24 is given next .",
    "[ thm::main - bound ] let the assumptions of theorem [ thm::population - bound ] and proposition [ prop::ols - rate ] hold with @xmath158 = \\tmu \\sqrt{p}$ ] .",
    "further assume that the function @xmath159}$ ] satisfies @xmath160 for some @xmath161 and @xmath162 such that the derivative of @xmath112 in the interval @xmath163 $ ] does not change sign , i.e. , its absolute value is lower bounded by @xmath164 .",
    "then , for @xmath0 and @xmath48 sufficiently large , with probability at least @xmath165 , we have @xmath166 where the constants @xmath167 and @xmath168 are defined by @xmath169 and @xmath170 is a constant depending on @xmath123 and @xmath152 .",
    "note that the convergence rate of the upper bound in depends on the sum of the two terms , both of which are functions of the data dimensions @xmath0 and @xmath1 .",
    "the first term on the right in comes from theorem [ thm::population - bound ] , which bounds the discrepancy between @xmath171 and @xmath16 .",
    "this term is small when @xmath1 is large , and it does not depend on the number of observations @xmath0 .",
    "the second term in the upper bound comes from estimating @xmath17 and @xmath172 .",
    "this term is increasing in @xmath1 , which reflects the fact that estimating @xmath16 is more challenging when @xmath1 is large .",
    "as expected , this term is decreasing in @xmath0 and @xmath48 , i.e. larger sample size yields better estimates .",
    "when the full ols solution is used ( @xmath173 ) , the second term becomes @xmath174 , which suggests that @xmath175 should be at least of order @xmath1 for good performance .",
    "also , note that there is a theoretical threshold for the sub - sampling size @xmath48 , namely @xmath176 , beyond which further sub - sampling provides no improvement .",
    "this suggests that the sub - sampling size should be smaller than @xmath176 .",
    "in this section , we describe an efficient algorithm to transform a generalized linear model to another .",
    "it is often the case that a practitioner would like to change the loss function ( equivalently the model ) he / she uses based on its performance .",
    "when the dataset is large , training a new model from the scratch is computationally inefficient and will be time consuming . in the following , we will use the proportionality relation to transition between different loss functions .",
    "assume that a practitioner fitted a glm using the loss function ( or cumulant generating function ) @xmath177 , but he / she would like to train a new model using the loss function @xmath178 . instead of maximizing the log - likelihood based on @xmath178 , one can exploit the proportionality relation and obtain the coefficients for the new glm problem .",
    "data @xmath91 , and @xmath179    use newton s root - finding method : initialize @xmath180 ; repeat until convergence : @xmath181 .",
    ": @xmath182 .",
    "denote by @xmath183 and @xmath184 the glm coefficients corresponding to the loss functions @xmath177 and @xmath178 , respectively .",
    "we have @xmath185 that is , both coefficients are proportional to the ols coefficients which does not depend on the loss function .",
    "therefore , these coefficients @xmath183 and @xmath184 are also proportional to each other and we can write @xmath186 where the proportionality constant between two glm types turns out to be the ratio between @xmath187 and @xmath188 , i.e. @xmath189 . using the definition of @xmath190 , we write @xmath191},\\\\    & = \\c_{\\cgf_1}\\rho\\ { \\mathbb{e}\\left[\\cgf_2^{(2)}({{\\langle x , \\betaglm_1 \\rangle } } \\rho)\\right]}.\\end{aligned}\\ ] ] dividing the both sides by @xmath192 and using the equality @xmath193}$ ] , we obtain @xmath194 } =     \\rho\\ { \\mathbb{e}\\left[\\cgf_2^{(2)}({{\\langle x , \\betaglm_1 \\rangle } } \\rho)\\right]}.\\end{aligned}\\ ] ] the above equation only involves @xmath183 as the coefficients ( which is already assumed to be known or fitted by the practitioner ) . therefore ,",
    "if we solve it for the ratio @xmath195 , we can estimate @xmath184 by simply using the proportionality relation given in .",
    "the procedure described above is summarized as algorithm [ alg::2 ] .",
    "we emphasize that this procedure does not require the computation of the ols estimator which was the main cost of sls .",
    "the procedure only requires a per - iteration cost of @xmath27 . in other words ,",
    "conversion from one glm type to another is much simpler than obtaining the glm coefficients from the scratch .",
    "in this section , we assume that for @xmath196 $ ] , the response is binary @xmath197 . the binary classification problem can be described by the following minimization of an empirical risk @xmath198 where @xmath199 and @xmath67 are referred to as the loss and the link functions , respectively",
    ". there are various loss functions that are used in practice .",
    "examples include log - loss , boosting loss , square loss etc ( see table [ tab::loss ] ) .",
    "as before , we constrain our analysis on the canonical links .",
    "the concept of canonical links for binary classification is introduced by @xcite , and it is quite similar to the generalized linear problems .    [ cols=\"<,<,^,^\",options=\"header \" , ]",
    "[ lem::vector - hoeff ] let @xmath200 be independent centered sub - exponential random vectors with @xmath201 .",
    "then we have @xmath202 whenever @xmath203 for an absolute constant @xmath94 .",
    "for a vector @xmath204 , we have @xmath205 since the dual of @xmath84 norm is itself .",
    "therefore , we write @xmath206 now , let @xmath207 be an @xmath208-net over @xmath209 , and observe that @xmath210 with @xmath211 .",
    "hence , we may write @xmath212 for any @xmath213 , we have @xmath214 .",
    "then , by the bernstein - type inequality for sub - exponential random variables @xcite , we have @xmath215 for an absolute constant @xmath94 .",
    "therefore , the probability on the left hand side of can be bounded by @xmath216 whenever @xmath217 .",
    "choosing @xmath218 and for an absolute constant @xmath219 and letting @xmath220 we conclude the proof .      for @xmath225 ,",
    "let @xmath226 be i.i.d . centered sub - gaussian random vectors with norm bounded by @xmath123 and @xmath227 } = \\tmu \\sqrt{p}$ ] . given a function @xmath49 that is uniformly bounded by @xmath228 , and lipschitz continuous with @xmath65 , @xmath229 }   \\right| } > c(b+\\kappa/\\tmu)\\sqrt{\\frac { p}{n/\\log(n ) } }   \\right ) } \\leq 2{\\exp\\left\\{-p\\right\\ } } ,    \\end{aligned}\\ ] ] whenever @xmath230 for @xmath231 .",
    "above , @xmath94 is an absolute constant .    let @xmath232 } = \\mu = \\tmu \\sqrt{p}$ ] and for @xmath233 , @xmath234 and @xmath235 define the bounding functions @xmath236 let @xmath237 be a net over @xmath221 in the sense that for any @xmath238 , @xmath239 such that @xmath240 .",
    "we fix @xmath241 and write @xmath242 , @xmath243 ,        hence , we can write that @xmath246 , @xmath247 such that @xmath248 } -\\e/2       & \\leq       \\frac{1}{n}\\sum_{i=1}^n g(\\<x_i,\\beta_1 \\ > ) - { \\mathbb{e}\\left[g(\\ < x,\\beta_1\\>)\\right]},\\\\      & \\leq   \\frac{1}{n}\\sum_{i=1}^n u_{\\beta_2}(x_i ) - { \\mathbb{e}\\left[u_{\\beta_2}(x)\\right ] } + \\e/2      .    \\end{aligned}\\ ] ]    the above inequalities translate to the following conclusion : whenever the following event happens , @xmath249 }   \\right | > \\e \\right\\ } ,    \\end{aligned}\\ ] ] at least one of the following events happens @xmath250 } > \\e/2 \\right\\ }      \\ \\ \\text{or}\\ \\",
    "\\left\\ { \\frac{1}{n}\\sum_{i=1}^n l_{\\beta_2}(x_i )         - { \\mathbb{e}\\left[l_{\\beta_2}(x)\\right ] }   < -\\e/2 \\right\\}.    \\end{aligned}\\ ] ]    therefore , using the union bound on the above events , we may obtain @xmath251 }   \\right | > \\e   \\right ) } \\\\      & \\leq      { \\mathbb{p}\\left (   \\max_{\\beta\\in\\n_{\\delta_*}}\\frac{1}{n}\\sum_{i=1}^n u_{\\beta}(x_i )         - { \\mathbb{e}\\left[u_{\\beta}(x)\\right ] } > \\e/2   \\right ) }      \\nonumber \\\\      & \\",
    "\\ + \\nonumber      { \\mathbb{p}\\left ( \\max_{\\beta\\in\\n_{\\delta _ * } } \\frac{1}{n}\\sum_{i=1}^n l_{\\beta}(x_i )         - { \\mathbb{e}\\left[l_{\\beta}(x)\\right ] }   < - \\e/2   \\right ) }      .",
    "\\end{aligned}\\ ] ]    note that the right hand side of the above inequality has two terms both of which are of the same form . for simplicity",
    ", we bound only the first one .",
    "the bound for the second one follows from the exact same steps .",
    "the relation between sub - gaussian and sub - exponential norms @xcite allows us to write @xmath252 where the second step follows from the triangle inequality .",
    "hence , we conclude that @xmath253}$ ] is a centered sub - gaussian random variable with norm upper bounded by @xmath254 .    for @xmath255",
    ", we notice that the random variable @xmath256 is also sub - gaussian with norm @xmath257 and consequently , the centered random variable @xmath258}$ ] has the sub - gaussian norm upper bounded by @xmath259 .",
    "then , by the hoeffding - type inequality for the sub - gaussian random variables , we obtain @xmath260 }        > \\e/2 \\right ) }   \\leq &      { \\exp\\left\\{-c n\\frac{\\e^2}{(b+\\kappa/\\tmu)^2}\\right\\ } }    \\end{aligned}\\ ] ] for an absolute constant @xmath261 .    by the same argument above",
    ", one can obtain the same result for the function @xmath262 .",
    "using hoeffding bounds in along with the union bound over the net , we immediately obtain @xmath263 }   \\right | > \\e   \\right ) }       \\leq      2{\\left|\\n_{\\delta_*}\\right| }      { \\exp\\left\\{-c n\\frac{\\e^2}{(b+\\kappa/\\tmu)^2}\\right\\ } }    \\end{aligned}\\ ] ] for some absolute constant @xmath94 .    using a standard covering argument over the net @xmath264",
    "as given in lemma [ lem::sphere ] , we have @xmath265 combining this with the previous bound , and choosing @xmath266 we get @xmath267 whenever @xmath230 for @xmath231 .      the set @xmath270 can be contained in a @xmath1-dimensional cube of size @xmath272 .",
    "consider a grid over this cube with mesh width @xmath273 .",
    "then @xmath270 can be covered with at most @xmath274 many cubes of edge length @xmath273 .",
    "if ones takes the projection of the centers of such cubes onto @xmath270 and considers the circumscribed balls of radius @xmath208 , we may conclude that @xmath270 can be covered with at most @xmath275 many balls of radius @xmath208 .",
    "[ lem::versh - sg ] let @xmath276 be isotropic random vectors with sub - gaussian norm upper bounded by @xmath123 .",
    "then for every @xmath277 , with probability at least @xmath278 , the empirical covariance @xmath279 satisfies , @xmath280 where @xmath281 are constants depending only on @xmath123 .",
    "[ lem::versh - heavy ] let @xmath288 be random vectors with mean 0 and covariance @xmath71 supported on a centered euclidean ball of radius @xmath289 , i.e. , @xmath290 .",
    "for @xmath291 and @xmath261 an absolute constant , with probability at least @xmath292 , the empirical covariance matrix satisfies @xmath293 for @xmath294 ."
  ],
  "abstract_text": [
    "<S> in stochastic optimization , the population risk is generally approximated by the empirical risk . however , in the large - scale setting , minimization of the empirical risk may be computationally restrictive . in this paper , we design an efficient algorithm to approximate the population risk minimizer in generalized linear problems such as binary classification with surrogate losses and generalized linear regression models . </S>",
    "<S> we focus on large - scale problems , where the iterative minimization of the empirical risk is computationally intractable , i.e. , the number of observations @xmath0 is much larger than the dimension of the parameter @xmath1 , i.e. @xmath2 . we show that under random sub - gaussian design , the true minimizer of the population risk is approximately proportional to the corresponding ordinary least squares ( ols ) estimator . using this relation </S>",
    "<S> , we design an algorithm that achieves the same accuracy as the empirical risk minimizer through iterations that attain up to a cubic convergence rate , and that are cheaper than any batch optimization algorithm by at least a factor of @xmath3 . </S>",
    "<S> we provide theoretical guarantees for our algorithm , and analyze the convergence behavior in terms of data dimensions . </S>",
    "<S> finally , we demonstrate the performance of our algorithm on well - known classification and regression problems , through extensive numerical studies on large - scale datasets , and show that it achieves the highest performance compared to several other widely used and specialized optimization algorithms . </S>"
  ]
}