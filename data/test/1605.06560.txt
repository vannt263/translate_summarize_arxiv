{
  "article_text": [
    "deep neural networks ( dnns ) have been receiving ubiquitous success in wide applications , ranging from computer vision @xcite , to speech recognition @xcite , natural language processing @xcite , and domain adaptation @xcite .",
    "as the sizes of data mount up , people usually have to increase the number of parameters in dnns so as to absorb the vast volume of supervision .",
    "high performance computing techniques are investigated to speed up dnn training , concerning optimization algorithms , parallel synchronisations on clusters w / o gpus , and stochastic binarization / ternarization , etc @xcite .    on the other hand",
    "the memory and energy consumption is usually , if not always , constrained in industrial applications @xcite . for instance , for commercial search engines ( e.g. , google and baidu ) and recommendation systems ( e.g. , netflix and youtube ) , the ratio between the increased model size and the improved performance should be considered given limited online resources .",
    "compressing the model size becomes more important for applications on mobile and embedded devices @xcite .",
    "having dnns running on mobile apps owns many great features such as better privacy , less network bandwidth and real time processing .",
    "however , the energy consumption of battery - constrained mobile devices is usually dominated by memory access , which would be greatly saved if a dnn model can fit in on - chip storage rather than dram storage ( c.f .",
    "@xcite for details ) .    a recent trend of studies are thus motivated to focus on compressing the size of dnns while mostly keeping their predictive performance @xcite . with different intuitions ,",
    "there are mainly two types of dnn compression methods , which could be used in conjunction for better parameter savings .",
    "the first type tries to revise the training target into more informative supervision using _",
    "dark knowledge_. in specific , hinton _",
    "_ @xcite suggested to train a large network ahead , and distill a much smaller model on a combination of the original labels and the soft - output by the large net .",
    "the second type observes the redundancy existence in network weights @xcite , and exploits techniques to constrain or reduce the number of free - parameters in dnns during learning .",
    "this paper focuses on the latter type .    to constrain the network redundancy , efforts @xcite formulated an original weight matrix into either low - rank or fast - food decompositions .",
    "moreover @xcite proposed a simple - yet - effective pruning - retraining iteration during training , followed by quantization and fine - tuning .",
    "@xcite proposed hashednets to efficiently implement parameter sharing prior to learning , and showed notable compression with much less loss of accuracy than low - rank decomposition .",
    "more precisely , prior to training , a hash function is used to randomly group ( virtual ) weights into a small number of buckets , so that all weights mapped into one hash bucket directly share a same value .",
    "hashednets was further deliberated in frequency domain for compressing convolutional neural networks in @xcite .    in applications , we observe hashednets compresses model sizes greatly at marginal loss of accuracy for some situations , whereas also significantly loses accuracy for others . after revisiting its mechanism , we conjecture this instability comes from at least three factors .",
    "first , hashing and training are disjoint in a two - phase manner , i.e. , once inappropriate collisions exist , there may be no much optimization room left for training .",
    "second , _ one single hash function _ is used to fetch a single value in the compression space , whose collision risk is larger than multiple hashes @xcite .",
    "third , parameter sharing within a buckets implicitly uses _ identity mapping _ from the hashed value to the virtual entry .",
    "this paper proposes an approach to relieve this instability , still in a two - phase style for preserving efficiency .",
    "specifically , we use _ multiple hash functions _ @xcite to map per virtual entry into multiple values in compression space .",
    "then an additional network plays in a _ mapping function _ role from these hashed values to the virtual entry before hashing , which can be also regarded as `` reconstructing '' the virtual entry from its multiple hashed values",
    ". plugged into and jointly trained within the original network , the reconstruction network is of a comparably ignorable size , i.e. , at low memory cost .",
    "this functional hashing structure includes hashednets as a degenerated special case , and facilitates less value collisions and better value reconstruction .",
    "shortly denoted as funhashnn , our approach could be further extended with dual space hashing and multi - hops .",
    "since it imposes no restriction on other network design choices ( e.g. dropout and weight sparsification ) , funhashnn can be considered as a standard tool for dnn compression .",
    "experiments on several datasets demonstrate promisingly larger reduction of model sizes and/or less loss on prediction accuracy , compared with hashednets .",
    "[ [ notations . ] ] notations .",
    "+ + + + + + + + + +    throughout this paper we express scalars in regular ( @xmath0 or @xmath1 ) , vectors in bold ( @xmath2 ) , and matrices in capital bold ( @xmath3 ) .",
    "furthermore , we use @xmath4 to represent the @xmath5-th dimension of vector @xmath2 , and use @xmath6 to represent the @xmath7-th entry of matrix @xmath3 . occasionally , @xmath8_i$ ]",
    "is also used to represent the @xmath5-th dimension of vector @xmath2 for specification clarity .",
    "notation @xmath9 $ ] stands for the expectation operator .",
    "[ [ feed - forward - neural - networks . ] ] feed forward neural networks .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we define the forward propagation of the @xmath10-th layer as @xmath11.\\end{aligned}\\ ] ] for each @xmath10-th layer , @xmath12 is the output dimensionality , @xmath13 is the bias vector , and @xmath14 is the ( _ virtual _ ) weight matrix in the @xmath10-th layer .",
    "vectors @xmath15 denote the units before and after the activation function @xmath16 .",
    "typical choices of @xmath16 include rectified linear unit ( relu ) @xcite , sigmoid and tanh @xcite .",
    "[ [ feature - hashing ] ] feature hashing + + + + + + + + + + + + + + +    has been studied as a dimension reduction method for reducing model storage size without maintaining the mapping matrices like random projection @xcite .",
    "briefly , it maps an input vector @xmath17 to a much smaller feature space via @xmath18 with @xmath19 . following the definition in @xcite ,",
    "the mapping @xmath20 is a composite of two approximate uniform hash functions @xmath21 and @xmath22 .",
    "the @xmath23-th element of @xmath24 is defined as : @xmath25_j = \\sum_{i : h(i)=j } \\xi(i)x_i.\\end{aligned}\\ ] ] as shown in @xcite , a key property is its inner product preservation , which we quote and restate below .",
    "* _ lemma [ inner product preservation of original feature hashing ] _ * with the hash defined by eq .",
    "( [ eq : originalfeaturehash ] ) , the hash kernel is unbiased , i.e. , @xmath26={\\mathbf}{x}^{\\top}{\\mathbf}{y}$ ] .",
    "moreover , the variance is @xmath27 , and thus @xmath28 if @xmath29 .    [ [ hashednets - in- . ] ] hashednets in @xcite .",
    "+ + + + + + + + + + + + + + + + + + + + + +    as illustrated in figure  [ fig : model](a ) , hashednets randomly maps network weights into a smaller number of groups prior to learning , and the weights in a same group share a same value thereafter .",
    "a naive implementation could be trivially achieved by maintaining a secondary matrix that records the group assignment , at the expense of additional memory cost however .",
    "hashednets instead adopts a hash function that requires no storage cost with the model .",
    "assume there is a finite memory budge @xmath30 per layer to represent @xmath14 , with @xmath31 .",
    "we only need to store a weight vector @xmath32 , and assign @xmath33 an element in @xmath34 indexed by a hash function @xmath35 , namely @xmath36 where hash function @xmath35 outputs an integer within @xmath37 $ ] .",
    "another independent hash function @xmath38 outputs a sign factor , aiming to reduce the bias due to hash collisions @xcite .",
    "the resulting matrix @xmath14 is _ virtual _ , since @xmath12 could be increased without increasing the _ actual _ number of parameters in @xmath34 once the compression space size @xmath30 is determined and fixed .",
    "substituting eq .",
    "( [ eq : hashednets ] ) into eq .  ( [ eq : ffnn ] ) , we have @xmath39 . during training , @xmath34 is updated by back propagating the gradient via @xmath40 ( and the virtual @xmath14 ) . besides , the activation function @xmath16 in eq .",
    "( [ eq : ffnn ] ) was kept as relu in @xcite to further relieve the hash collision effect through a sparse feature space . in both @xcite and this paper , the open source _",
    "_ xxhash _ _ is adopted as an approximately uniform hash implementation with low cost .",
    "for clarity , we will focus on a single layer throughout and drop the super - script @xmath10 .",
    "still , vector @xmath41 denotes parameters in the compression space .",
    "the key difference between funhashnn and hashednets @xcite lies in ( i ) how to employ hash functions , and ( ii ) how to map from @xmath42 to @xmath43 :    * instead of adopting one pair of hash function @xmath44 in eq .",
    "( [ eq : hashednets ] ) , we use a set of multiple pairs of independent random hash functions .",
    "let s say there are @xmath45 pairs of mappings @xmath46 , each @xmath47 outputs an integer within @xmath48 $ ] , and each @xmath49 selects a sign factor . *",
    "( [ eq : hashednets ] ) of hashednets employs an identity mapping between one element in @xmath43 and one hashed value , i.e. , @xmath50 . in contrast , we use a multivariate function @xmath51 to describe the mapping from multiple hashed values @xmath52 to @xmath53 . specifically , @xmath54;\\ { \\boldsymbol}{\\alpha}\\right ) .",
    "\\end{aligned}\\ ] ] therein , @xmath55 is referred to as the parameters in @xmath51 .",
    "note that the input @xmath56 is order sensitive from @xmath57 to @xmath45 .",
    "we choose @xmath51 to be a multi - layer feed forward neural network , and other multivariate functions may be considered as alternatives .    as a whole , figure  [ fig : model](b ) illustrates our funhashnn structure , which can be easily plugged in any matrices of dnns .",
    "note that @xmath55 in the reconstruction network @xmath51 is of a much smaller size compared to @xmath42 .",
    "for instance , a setting with @xmath58 and a 1-layer @xmath59 of @xmath60 performs already well enough in experiments .",
    "in other words , eq .",
    "( [ eq : funhashnn ] ) just uses an ignorable amount of additional memory to describe a functional @xmath42-to-@xmath43 mapping , whose properties will be further explained in the sequel .",
    "the parameters in need of updating include @xmath42 in the compression and @xmath55 in @xmath51 .",
    "training funhashnn is equivalent to training a standard neural network , except that we need to forward / backward - propagate values related to @xmath42 through @xmath51 and the virtual matrix @xmath43 .",
    "[ [ forward - propagation . ] ] forward propagation .",
    "+ + + + + + + + + + + + + + + + + + + +    substituting eq .",
    "( [ eq : funhashnn ] ) into eq .",
    "( [ eq : ffnn ] ) , we still omit the super - script @xmath10 and get @xmath61;\\ { \\boldsymbol}{\\alpha}\\right).\\end{aligned}\\ ] ]    [ [ backward - propagation . ] ] backward propagation .",
    "+ + + + + + + + + + + + + + + + + + + + +    denote @xmath62 as the final loss function , e.g. , cross entropy or squared loss , and suppose @xmath63 is already available by back - propagation from layers above .",
    "the derivatives of @xmath62 with respect to @xmath42 and @xmath55 are computed by @xmath64 where , since we choose @xmath51 as a multilayer neural network , derivatives @xmath65 and @xmath66 can be calculated through the small network @xmath51 in a standard back - propagation manner .    [",
    "[ complexity . ] ] complexity .",
    "+ + + + + + + + + + +    concerning time and memory cost , funhashnn roughly has the same complexity as hashednets , since the small network @xmath51 is quite light - weighted .",
    "one key variable factor is the way to implement multiple hash functions .",
    "on one hand , if they are calculated online , then funhashnn requires little additional time if tackling them in parallel . on the other , if they are pre - computed and stored in dicts to avoid hashing time cost , the multiple hash functions of funhashnn demand more storage space . in application",
    ", we suggest to pre - compute hashes during offline training for speedup , and to compute hashes in parallel during online prediction for saving memory under limited budget .      in this part",
    ", we try to depict the properties of our funhashnn from several aspects to help understanding it , especially in comparison with hashednets @xcite .",
    "[ [ value - collision . ] ] value collision .",
    "+ + + + + + + + + + + + + + + +    it should be noted , both hashednets and funhashnn conduct hashing prior to training , i.e. , in a two - phase manner .",
    "consequently , it would be unsatisfactory if hashing collisions happen among important values .",
    "for instance in natural language processing tasks , one may observe wired results if there are many hashing collisions among embeddings ( which form a matrix ) of frequent words , especially when they are not related at all . in the literature",
    ", multiple hash functions are known to perform better than one single function @xcite . in intuition , when we have multiple hash functions , the items colliding in one function are hashed differently by other hash functions .",
    "[ [ value - reconstruction . ] ] value reconstruction .",
    "+ + + + + + + + + + + + + + + + + + + + +    in both hashednets and funhashnn , the hashing trick can be viewed as a reconstruction of the original parameter @xmath43 from @xmath67 . in this sense , the approach with a lower reconstruction error is preferred , whereas here we could imagine @xmath43 is already structured and filled by values with least redundancy . ]",
    ". then we have at least the following two observations :    * * the maximum number of possible distinct values * output by hashing intuitively explains the modelling capability @xcite . for hashednets , considering the sign hashing function @xmath68 , we have at most @xmath69 possible distinct values of eq .",
    "( [ eq : hashednets ] ) to represent elements in @xmath43 .",
    "in contrast , since there are multiple ordered hashed inputs , funhashnn has at most @xmath70 possible distinct values of eq .",
    "( [ eq : funhashnn ] ) .",
    "note that the memory size @xmath71 is the same for both .",
    "* * the reconstruction error * may be difficult to analyzed directly , since the hashing mechanism is trained jointly within the whole network .",
    "however , we observe @xmath72;\\ { \\boldsymbol}{\\alpha}\\right)$ ] degenerates to @xmath73 if we assign zeros to all entries in @xmath55 unrelated to the 1st input dimension .",
    "since @xmath73 depends only on one single pair of hash functions , it is conceptually equivalent to hashednets . consequently , including hashednets as a special case , funhashnn with freely adjustable @xmath55 is able to reach a lower reconstruction error to fit the final accuracy better .",
    "[ [ feature - hashing . ] ] feature hashing .",
    "+ + + + + + + + + + + + + + + +    in line with previous work @xcite , we compare hashednets and funhashnn in terms of feature hashing . for specification clarity",
    ", we drop the sign hashing functions @xmath68 below for both methods , the analysis with which is straightforward by replacing @xmath71 hereafter with @xmath69 .    * for hashednets ,",
    "one first defines a hash mapping function @xmath74 , whose @xmath75-th element is @xmath76_k \\triangleq \\sum_{j : h(i , j)=k } a_j , \\quad { \\mathrm}{for } \\quad k=1,\\ldots , k .",
    "\\end{aligned}\\ ] ] thus @xmath77 by hashednets can be computed as the inner product ( details c.f . section  4.3 in @xcite ) @xmath78 * for funhashnn , we first define a hash mapping function @xmath79 .",
    "different from a @xmath71-dim output in eq .",
    "( [ eq : featurehash_hashnet1 ] ) , it is of a much larger size @xmath80 , with @xmath81-th element as @xmath82_{\\sum_{u=1}^u k_uk^{(u-1 ) } }   \\triangleq \\sum_{\\substack{j : h_1(i , j)=k_1 \\\\ \\ \\ h_2(i , j)=k_2 \\\\ \\ \\ \\ldots \\\\ \\ \\ h_u(i , j)=k_u } } a_j , \\quad { \\mathrm}{for}\\quad \\forall u , \\",
    "k_u=1,\\ldots , k .",
    "\\end{aligned}\\ ] ] second , we define vector @xmath83 still of length @xmath80 , whose @xmath81-th entry is @xmath84_{\\sum_{u=1}^u k_uk^{(u-1 ) } }   \\triangleq g\\left(w_{k_1},w_{k_2},\\ldots , w_{k_u } ; \\ {",
    "\\boldsymbol}{\\alpha}\\right ) , \\quad { \\mathrm}{for}\\quad \\forall u , \\",
    "k_u=1,\\ldots , k .",
    "\\end{aligned}\\ ] ] thus @xmath77 by funhashnn can be computed as the following inner product @xmath85 the difference between eq .",
    "( [ eq : featurehash_hashnet2 ] ) and eq .",
    "( [ eq : featurehash_funhashnn3 ] ) further explains the above discussion about `` the maximum number of possible distinct values '' .",
    "[ [ hashing - on - dual - space . ] ] hashing on dual space .",
    "+ + + + + + + + + + + + + + + + + + + + + +    if considering a linear model @xmath86 , one can not only deliver analysis like bayesian or hashing on input feature space of @xmath2 , but also do similarly on the _ dual space _ of @xmath87 @xcite .",
    "we now revisit the `` reconstruction '' network @xmath88 in eq .",
    "( [ eq : funhashnn ] ) , where vector @xmath89 concatenates the hashed values @xmath56 for @xmath90 . what we did in eq .",
    "( [ eq : funhashnn ] ) is in fact hashing @xmath7 through @xmath42 to get the input feature of @xmath51 . in analogy",
    ", we can also hash @xmath7 to fetch parameters of @xmath51 , namely we have a new `` reconstruction '' network in the following form : @xmath91_u = \\xi_u(i , j)w_{h_u(i , j)}\\quad { \\mathrm}{and }       \\quad [ { \\boldsymbol}{\\alpha}_{ij}]_r = \\xi'_r(i , j ) w'_{h'_r(i , j)},\\end{aligned}\\ ] ] where @xmath92 are additional multiple pairs of hash functions applied on @xmath55 , and @xmath93 is an additional vector in the compression space of @xmath55 .",
    "the size of @xmath94 remains the same as previous . using this trick , the maximum number of possible distinct values of @xmath43 further increases exponentially , so that funhashnn has more potential ability to fit the prediction well .",
    "we denote funhashnn with dual space hashing shortly as funhashnn - d , and illustrate its structure in figure  [ fig : model](c ) .",
    "[ [ multi - hops . ] ] multi - hops .",
    "+ + + + + + + + + + +    we conjecture that funhashnn could be used in a multi - hops structure , by imagining @xmath42 in the compression space plays a _",
    "virtual _ role similar to @xmath43 .",
    "specifically , we can build another level of hash functions @xmath95 and compression space @xmath96 .",
    "thereafter , each entry in @xmath42 is hashed into multiple values in @xmath96 via @xmath95 .",
    "then another reconstruction network @xmath97 is used to learn the mapping from the hashed values in @xmath96 to the corresponding entry in @xmath42 .",
    "this procedure can be implemented recursively .",
    "if there are in total @xmath98-hops , what we need to save in fact just includes a ( possibly much more smaller ) vector @xmath99 at the final hop , a series of @xmath98 small reconstruction networks @xmath100 , and a series of hashing functions .",
    "in contrast , the multi - hops version of hashednets is equivalent to just adjusting the compression ratio , or say the size @xmath71 .",
    "recent studies have confirmed the redundancy existence in the parameters of deep neural networks .",
    "@xcite decomposed a matrix in a fully - connected layers as the product of two low - rank matrices , so that the number of parameters decreases linearly as the latent dimensionality decreases .",
    "more structured decompositions fastfood @xcite and deep fried @xcite were proposed not only to reduce the number of parameters , but also to speed up matrix multiplications .",
    "more recently , han _ et al . _",
    "@xcite proposed to iterate pruning - retraining during training dnns , and used quantization and fine - tuning as a post - processing step .",
    "huffman coding and hardware implementation were also considered . in order to mostly keep accuracy , the authors suggested multiple rounds of pruning - retraining .",
    "that is , for little accuracy loss , we have to prune slowly enough and thus suffer from increased training time .",
    "again , the most related work to ours is hashednets @xcite , which was then extended in @xcite to random hashing in frequency domain for compressing convolutional neural networks .",
    "either hashednets or funhashnn could be combined in conjunction with other techniques for better compression .",
    "extensive studies have been made on constructing and analyzing multiple hash functions , which have shown better performances over one single hash function @xcite .",
    "one multi - hashing algorithm , @xmath101-random scheme @xcite , uses only one hash table but @xmath101 hash functions , pretty similar to our settings .",
    "one choice alternative to @xmath101-random is the @xmath101-left algorithm proposed in @xcite , used for improving ip lookups .",
    "hashing algorithms for natural language processing are also studied in @xcite .",
    "papers @xcite investigated feature hashing ( a.k.a . the hashing trick ) , providing useful bounds and feasible results .",
    "we conduct extensive experiments to evaluate funhashnn on dnn compression . codes for fully reproducibility will be open source soon after necessary polishment .",
    "[ [ datasets . ] ] datasets .",
    "+ + + + + + + + +    three benchmark datasets @xcite are considered here , including ( 1 ) the original ` mnist ` hand - written digit dataset , ( 2 ) dataset ` bg - img ` as a variant to ` mnist ` , and ( 3 ) binary image classification dataset ` convex ` .",
    "for all datasets , we use prespecified training and testing splits .",
    "in particular , the original ` mnist ` dataset has # train=60,000 and # test=10,000 , while the remaining both have # train=12,000 and # test=50,000 .",
    "moreover , collected from a commercial search engine , a large scale dataset with billions of samples is used to learn dnns for pairwise semantic ranking .",
    "we randomly split out 20% samples from the training data to form the validation set .    [",
    "[ methods - and - settings . ] ] methods and settings .",
    "+ + + + + + + + + + + + + + + + + + + + +    in @xcite , the authors compared hashednets against several dnn compression approaches , and showed hashednets performs consistently the best , including the low - rank decomposition @xcite .",
    "under the same settings , we compare _ funhashnn _ with _ _",
    "/ project / hashednets / index.html ] and a standard neural network without compression . all activation functions are chosen as relu .",
    "the settings of funhashnn are tested in two scenarios .",
    "first , we will fix to use funhashnn in figure  [ fig : model](b ) without extensions , and then compare the effects of compression by funhashnn and hashednets .",
    "second , we compare different configurations of funhashnn itself , including the number @xmath45 of seeds , the layer of reconstruction network @xmath51 , and extension with the dual space hashing .",
    "hidden layers within @xmath51 keep using tanh as activation functions .",
    "results by the multi - hops extension of funhashnn will be included in another ongoing paper for systematic comparisons .      to test robustness , we vary the compression ratio with ( 1 ) a fixed virtual network size ( i.e. , the size of @xmath14 in each layer ) , and then with ( 2 ) a fixed memory size ( i.e. , the size of @xmath34 in each layer ) . three - layer ( 1 hidden layer ) and five - layer ( 3 hidden layers ) networks are investigated . in experiments , we vary the compression ratio geometrically within @xmath102 . for funhashnn , this comparison sticked to use 4 hash functions , 3-layer @xmath51 , and without dual space hashing .    [",
    "[ with - virtual - network - size - fixed . ] ] with virtual network size fixed .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the hidden layer for 3-layer nets initializes at 1000 units , and for 5-layer nets starts at 100 units per layer .",
    "as the compression ratio ranges from 1 to 1/64 with a fixed virtual network size , the memory decreases and it becomes increasingly difficult to preserve the classification accuracy .",
    "the testing errors are shown in figure  [ fig : res_varycomp_fixnet ] , where standard neural networks with equivalent parameter sizes are included in comparison .",
    "funhashnn shows robustly effective compression against the compression ratios , and persistently produces better prediction accuracy than hashednets .",
    "it should be noted , even when the compression ratio equals to one , funhashnn with the reconstruction network structure is still not equivalent to hashednets and performs better .",
    "+            [ [ with - memory - storage - fixed . ] ] with memory storage fixed .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + +    we change to vary the compression ratio from 1 to 1/64 with a fixed memory storage size , i.e. , the size of the virtual network increases while the number of free parameters remains unchanged . in this sense , we d better call it expansion instead of compression .",
    "both 3-layer and 5-layer nets initialize at 50 units per hidden layer .",
    "the testing errors in this scenario are shown in figure  [ fig : res_varycomp_fixmem ] . at all compression ( expansion )",
    "ratios on each dataset , funhashnn performs better than or at least comparably well compared to hashednets .             +              on 3-layer nets with compression ratio @xmath103 , we vary the configuration dimensions of funhashnn , including the number of hash functions ( @xmath45 ) , the structure of layers of the reconstruction network @xmath51 , and whether dual space hashing is turned on . since it is impossible to enumerate all probable choices , @xmath45 is restricted to vary in @xmath104 .",
    "the structure of @xmath51 is chosen from @xmath105 layers , with @xmath106 , @xmath107 , @xmath108 layerwise widths , respectively .",
    "we denote u@xmath109-g@xmath110 as @xmath109 hash functions and @xmath110 layers of @xmath51 , and a suffix -d indicates the dual space hashing .",
    "table  [ tab : res_varyconfig ] shows the performances of funhashnn with different configurations on ` mnist ` .",
    "the observations are summarized below .",
    "first , the series from index ( 0 ) to ( 1.x ) fixes a 3-layer @xmath51 and varies the number of hash functions .",
    "as listed , more hash functions do not ensure a better accuracy , and instead u4-g3 performs the best , perhaps because too many hash functions potentially brings too many partial collisions .",
    "second , the series from ( 0 ) to ( 2.x ) fixes the number of hash functions and varies the layer number in @xmath51 , where three layers performs the best mainly due to its strongest representability .",
    "third , indices ( 3.x ) show further improved accuracies using dual space hashing .",
    "captypetable    [ cols=\"<,<,^\",options=\"header \" , ]      + captypefigure      finally , we evaluate the performance of funhashnn on semantic learning - to - rank dnns .",
    "the data is collected from logs of a commercial search engine , with per clicked query - url being a positive sample and per non - clicked being a negative sample .",
    "there are totally around 45b samples .",
    "we adopt a deep convolutional structured semantic model similar to @xcite , which is of a siamese structure to describe the semantic similarity between a query and a url title .",
    "the network is trained to optimize the cross entropy for each pair of positive and negative samples per query .",
    "the performance is evaluated by correct - to - wrong pairwise ranking ratio on testing set . in figure",
    "[ fig : res_hash_ltr ] , we plot the performance by a baseline network as training proceeds , compared to funhashnn and hashnet both with 1/4 compression ratio . with @xmath58 hash functions",
    ", funhashnn performs better than hashednets throughout the training epochs , and even comparable to the full network baseline which requires 4 times of memory storage .",
    "the deterioration of hashednets probably comes from many inappropriate collisions on word embeddings , especially for words of high frequencies .",
    "this paper presents a novel approach funhashnn for neural network compression . briefly , after adopting multiple low - cost hash functions to fetch values in compression space , funhashnn employs a small reconstruction network to recover each entry in an matrix of the original network .",
    "the reconstruction network is plugged into the whole network and learned jointly .",
    "the recently proposed hashednets @xcite is shown as a degenerated special case of funhashnn .",
    "extensions of funhashnn with dual space hashing and multi - hops are also discussed . on several datasets",
    ", funhashnn demonstrates promisingly high compression ratios with little loss on prediction accuracy .    as future work",
    ", we plan to further systematically analyze the properties and bounds of funhashnn and its extensions .",
    "more industrial applications are also expected , especially on mobile devices .",
    "this paper focuses on the fully - connected layer in dnns , and the compression performance on other structures ( such as convolutional layers ) is also planned to be studied . as a simple and effective approach , funhashnn is expected to be a standard tool for dnn compression .",
    "g.  hinton , l.  deng , d.  yu , g.  e. dahl , a.  rahman mohamed , n.  jaitly , a.  senior , v.  vanhoucke , p.  nguyen , t.  n. sainath , and b.  kingsbury .",
    "deep neural networks for acoustic modeling in speech recognition : the shared views of four research groups .",
    ", 29(6):8297 , 2012 ."
  ],
  "abstract_text": [
    "<S> as the complexity of deep neural networks ( dnns ) trend to grow to absorb the increasing sizes of data , memory and energy consumption has been receiving more and more attentions for industrial applications , especially on mobile devices . </S>",
    "<S> this paper presents a novel structure based on functional hashing to compress dnns , namely funhashnn . for each entry in a deep net , </S>",
    "<S> funhashnn uses multiple low - cost hash functions to fetch values in the compression space , and then employs a small reconstruction network to recover that entry . </S>",
    "<S> the reconstruction network is plugged into the whole network and trained jointly . </S>",
    "<S> funhashnn includes the recently proposed hashednets @xcite as a degenerated case , and benefits from larger value capacity and less reconstruction loss . </S>",
    "<S> we further discuss extensions with dual space hashing and multi - hops . on several benchmark datasets , funhashnn demonstrates high compression ratios with little loss on prediction accuracy . </S>"
  ]
}