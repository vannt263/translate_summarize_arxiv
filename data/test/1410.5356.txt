{
  "article_text": [
    "entropy is very important in thermodynamics and statistical mechanics . in fact",
    ", it is the key concept , upon which the equilibrium statistical mechanics is formulated @xcite , and from which all the other thermodynamical quantities can be derived .",
    "also , the increasing of entropy shows the time - evolutionary direction of a thermodynamical system .",
    "the concept of entropy can even be applied to non - thermodynamical systems such as information theory , in which entropy is a measure of the uncertainty in a random variable @xcite , and to non - ordinary thermodynamical systems such as self - gravitating systems , in which the dominating microscopic interaction between particles is long - ranged @xcite .",
    "the latter is relevant to our studies , in which we formulated a framework of equilibrium statistical mechanics for self - gravitating systems @xcite . in these works",
    ", we demonstrated that the boltzmann entropy @xmath0 = -\\int f(\\mathbf{v } ) \\ln f(\\mathbf{v } ) { { \\rm d}}^3 \\mathbf{v}\\ ] ] is also valid for self - gravitating systems , in which @xmath1 is the system s probability density function ( pdf , hereafter ) .",
    "hence , the pdf is necessary for computing the systems entropy , but often , instead of giving an analytic form of pdf , we have to deal with a thermodynamical system that is in data - form .",
    "for instance , @xcite performed numerical simulations to study satellite galaxy disruption in a potential resembling that of the milky way . in their work",
    ", the coarse - grained boltzmann entropy is used as a measure of the phase - mixing to indicate how mixing of disrupted satellites can be quantified .",
    "the analytic pdf is unavailable , and hence they derived the coarse - grained pdf from the simulation data by histogram , that is , by taking a partition in the 6d phase - space and counting how many particles fall in each 6d cell .",
    "so , with analytic pdf unavailable , it is indispensable to compute the system s entropy from the data - based samples .",
    "this seemingly easy computation , however , is plagued with some unexpected troubles . as we have seen in equation  ( [ eq : bs ] ) ,",
    "the evaluation of entropy is actually related to the estimation of probability density , which is one of the most important techniques in exploratory data analysis ( eda ) .",
    "usually , @xmath2 , the estimation of the underlying unknown pdf @xmath1 of a statistical sample , is derived by data binning , i.e. histogram @xcite , just as @xcite . however , in the real practice , we find the following interesting phenomenon",
    ". see figure  [ fig : sdv105 ] , the resulting entropy @xmath3 depends on the bin width @xmath4 , monotonically increasing with the bin width , and thus it is troublesome to select an appropriate bin width of histogram for computing the entropy .",
    "prior to our study , there have been many investigations on how to make the optimal probability density estimation of statistical samples @xcite .",
    "histogram is the mostly used method , in which the bin width is its characteristic parameter .",
    "a larger bin width produces over - smooth estimation and the pdf looks smoother and featureless , while a smaller bin width produces under - smooth estimation so that the resulting pdf has large fluctuations and spurious bumps .",
    "hence , the selection of an optimal bin width is indeed an important task in eda , and such an effort can be dated back to @xcite , but the famous selector for optimal bin width is given by @xcite .",
    "usually , the optimal bin width is obtained by minimizing the asymptotic mean integrated squared error ( amise ) , which is a tradeoff between the squared bias and variance .",
    "contrary to the conventional treatment , however , @xcite proposed a straightforward data - based method of determining the optimal number of bins in a uniform bin - width histogram using bayesian probability theory .",
    "another commonly used density estimation method is kernel estimation method , which was first introduced by @xcite and @xcite and is considered to be much superior to the histogram method @xcite . with kernel density estimation , we arrive at the similar results as with histogram that the entropy increases as a monotonic function of the kernel bandwidth @xmath5 .    .",
    "the underlying distribution is the univariate standard normal distribution , from which the data set is randomly drawn , with the sample size @xmath6 .",
    "@xmath4 is the bin width of the one - dimensional histogram , and @xmath3 is evaluated by equation  ( [ eq : bs ] ) , but with @xmath7 instead of @xmath8 .",
    "@xmath9 , computed by kernel estimation , exhibits the similar behaviour with respect to the bandwidth @xmath5 .",
    "@xmath10 , shown with the dashed line , is directly computed by using the analytical univariate standard normal pdf of equation  ( [ eq:1dgauss ] ) . ]     versus @xmath4 is caused by coarse - graining of the pdf .",
    "the step solid line and the dotted line indicate the fine - grained and coarse - grained pdf , respectively . the entropy evaluated with coarse - grained pdf is larger than that with fine - grained pdf .",
    "see section  [ sec : origin ] for detail .",
    "this explanation is also applicable to @xmath9 versus @xmath5 . ]    besides the two main methods , there are also other methods of density estimation , such as average shifted histogram , orthogonal series estimators @xcite .",
    "we just focus on histogram and kernel methods in this work .",
    "investigations on histogram and kernel estimation have indeed provided us with useful criteria for optimal bin width of histogram or bandwidth of kernel function @xcite , yet all these criteria need to take into account the functional form of the true pdf , which is usually unavailable .",
    "so it is a difficulty on how to select an optimal bin width or bandwidth for computing the entropy .",
    "fortunately , by large amounts of numerical experiments , we find that the first derivatives of @xmath11 with respect to @xmath4 for histogram estimation , or of @xmath9 with respect to @xmath5 for kernel estimation , correspond to the optimal bin width and bandwidth in all the cases we considered in this work .    in this paper , we describe our numerical experiments and demonstrate this finding in detail .",
    "although we do not provide a robust mathematical proof of this finding , we suggest that the first derivative of entropy be regarded as an alternative selector other than the usual amise to pick out an optimal bin width of histogram and bandwidth of kernel estimation .",
    "the paper is organized as follows . in section  [",
    "sec : method ] , we introduce our methods and describe the working procedure . in section  [ sec : result ] , we present the results .",
    "finally , we give our conclusions and discussions in section  [ sec : summy ] .",
    "we demonstrate the origin of the monotonicity of @xmath11 verse @xmath4 . see figure  [ fig : dfcg ] , we design a simple one - dimensional normalized experimental pdf , as : @xmath12 with which the entropy evaluated by equation  ( [ eq : bs ] ) is @xmath13 .",
    "next , the averaged , or coarse - grained , pdf within the interval @xmath14 can be easily derived and normalized as : @xmath15 so , the entropy evaluated with this coarse - grained pdf @xmath16 is , @xmath17 , and we can immediately see that @xmath18 . hence , the monotonicity of @xmath11 verse @xmath4 is caused by the coarse - graining of pdf .",
    "the same explanation is also applicable to the monotonicity of @xmath9 versus @xmath5 .    beyond the monotonicity of @xmath11",
    ", however , there should be another important feature hiding in the @xmath11 curve . by scrutinizing @xmath11 of figure  [ fig : sdv105 ] ,",
    "we speculate that the first derivative of @xmath11 with respect to @xmath4 , might take its local minimum around the cross point of @xmath10 and @xmath11 .",
    "if this is the case , then we can use this property to construct a selector of optimal bin width of histogram , so that we can compute the entropy to the best extent .",
    "we will verify this speculation below .",
    "[ cols=\"^,^,^,^,^,^,^,^,^,^ \" , ]      prior to our results , there are some well - known estimators for the optimal bin width of histogram or bandwidth of kernel estimation .",
    "the @xcite optimal bin width is one of them , @xmath19^{1/3 } n^{-1/3},\\ ] ] in which @xmath20 is the first derivative of @xmath21 , and the functional @xmath22 $ ] is defined for the roughness of @xmath23 as @xmath24    another one is the optimal bandwidth for kernel estimation @xcite : @xmath25^{1/5 } n^{-1/5}.\\ ] ] note that @xmath26 scales with the sample size as @xmath27 , while @xmath28 scales as @xmath29 .",
    "the kernel estimation is believed to be superior to the histogram method , and indeed , we notice that @xmath30 as well as @xmath31 are much smoother than their counterparts of histogram estimation .",
    "so we just concentrate on the kernel estimation below .",
    "figure  [ fig : hnp ] shows the relations of @xmath32 with the sample size @xmath33 .",
    "we see that in all the three cases , differences between @xmath32 and @xmath26 are significant , and @xmath32 well scales as @xmath29 , in contrast to @xmath34 .",
    "it is interesting to note that this @xmath29-scaling is similar to that of scott s formula of equation  ( [ eq : scott ] ) for optimal bin width of histogram estimation .",
    "entropies evaluated at @xmath32 and @xmath26 of the three cases are shown in figure  [ fig : snp ] .",
    "it can be seen that @xmath35 is much closer to entropy s true value @xmath10 than @xmath36 , but the difference between the two vanishes asymptotically with increasing sample size .",
    "nonetheless , as far as the statistical computation of entropy is concerned , @xmath26 , selected by minimizing amise , is less than the optimal bandwidth @xmath32 .     and",
    "sample size @xmath33 for the case of kernel estimation .",
    "the three panels correspond to the three experimental pdfs of equations  ( [ eq:1dgauss ] ) , ( [ eq:1dpower ] ) and ( [ eq:3dgauss ] ) .",
    "the bandwidth @xmath26 that is derived by minimizing amise is also shown for comparison . ]     and sample size @xmath33 for the case of kernel estimation .",
    "the three panels correspond to the three experimental pdfs of equations  ( [ eq:1dgauss ] ) , ( [ eq:1dpower ] ) and ( [ eq:3dgauss ] ) . entropies of @xmath26 are also shown for comparison . ]      from previous results , we have seen that the first derivative of @xmath11 or @xmath9 can be used as a selector for the optimal bin width or bandwidth .",
    "we just concentrate on the kernel method here , and present some analytical results . with the estimated pdf @xmath37 , the first derivative of the entropy is @xmath38 in which we use the normalization @xmath39 , and the fact that the partial derivative w.r.t .",
    "@xmath5 can be taken out from the integral .",
    "the second derivative of @xmath9 is @xmath40 { { \\rm d}}v \\nonumber \\\\ & = &   -\\int \\big(\\frac{\\partial^2 \\hat{f}_h}{\\partial h^2 } \\ln\\hat{f}_h   - \\hat{f}_h \\frac{\\partial^2 \\ln\\hat{f}_h}{\\partial h^2 } + \\frac{\\partial^2\\hat{f}_h}{\\partial h^2}\\big){{\\rm d}}v   \\nonumber \\\\ & = & -\\int \\frac{\\partial^2 \\hat{f}_h}{\\partial h^2 } \\ln\\hat{f}_h { { \\rm d}}v   + \\int \\hat{f}_h \\frac{\\partial^2 \\ln\\hat{f}_h}{\\partial h^2 } { { \\rm d}}v.\\end{aligned}\\ ] ] in deriving the last line of the above relation , we again use the normalization @xmath39 .",
    "hence , the minimum of the first derivative of @xmath9 should be picked out by @xmath41 , and from the above equation , we have @xmath42 we can make use of this relation to pick out the optimal bandwidth @xmath32",
    ".    note that both @xmath28 of equation  ( [ eq : scott ] ) and @xmath26 of equation  ( [ eq : hopt ] ) depend on the unknown true pdf @xmath21 , which is usually difficult or even impossible to acquire . on the contrary ,",
    "the selector of optimal bandwidth based on the first derivative of entropy is purely data - based , and hence , our approach is much superior to the existing methods .",
    "however , we do not provide a proof on the existence of the minimum of the first derivative of entropy , and do not explain why such a minimum can help pick out the optimal bandwidth , either .",
    "these issues are surely of great importance and deserve further investigations .",
    "entropy is a very important concept , and in statistical mechanics , it is computed with the probability distribution function of the system under consideration . in our statistical - mechanical investigations of self - gravitating systems , however , we have to do statistical computation of entropy directly from statistical samples , without knowing the underlying analytic pdf . thus , the evaluation of entropy is actually related to the estimation of the pdf from statistical samples in data - form .",
    "usually , there are two approaches to estimate a pdf from a statistical sample .",
    "the first density estimation method is histogram .",
    "another way is kernel estimation , which is considered to be superior to the histogram .",
    "we use both the methods to evaluate the entropy , and we find that the entropy thus computed depends on the bin width of the histogram , or bandwidth of kernel method . concretely , the entropy is a monotonic increasing function of the bin width or bandwidth .",
    "we attribute this monotonicity to the pdf coarse - graining .",
    "thus , it is a difficulty on how to select an optimal bin width / bandwidth for computing the entropy .",
    "fortunately , we notice that there may exist a minimum of the first derivatives of entropy for both histogram and kernel estimation , and this minimum may correspond to the optimal bin width and bandwidth .",
    "we perform a large amount of numerical experiments to verify this finding .",
    "first , we select three analytical pdfs , one - dimensional standard normal , one - dimensional power - law , and three - dimensional standard normal distribution , and with monte carlo technique , we draw @xmath33 random data from these analytical pdfs , respectively . with these statistical samples ,",
    "we construct the estimator of the true pdfs of both histogram and kernel estimation .",
    "secondly , we compute the entropy with the estimated pdfs , so in this way , we derive the curves of @xmath11 or @xmath9 , in which @xmath4 or @xmath5 are the bin width of histogram or bandwidth of kernel , respectively .",
    "meanwhile , the entropy can be exactly evaluated with these analytical pdfs .",
    "these exact results can be used to calibrate our empirical results , and to help select the optimal bin width or bandwidth .",
    "we do the same experiments for the three pdfs with the sample size ranging from @xmath43 to @xmath44 . in all cases ,",
    "whatever using histogram or kernel estimation , we find that :    * the first derivative of entropy indeed has a minimum around the cross - point of the entropy curve @xmath11 or @xmath9 and the theoretical straight line @xmath10 . *",
    "the minimum of the derivative goes to zero with increasing sample size . *",
    "@xmath45 or @xmath46 , at which the derivative takes its minimum , asymptotically approaches the abscissa of the above - mentioned cross - point with increasing sample size , such that the entropy at @xmath47 or @xmath46 asymptotically approaches the theoretical value @xmath10 .",
    "* @xmath32 scales with the sample size as @xmath29 , in contrast to @xmath26 , the bandwidth selected by minimizing amise , whose scaling is @xmath48 .",
    "* the entropy evaluated at @xmath32 is much closer to the true value @xmath10 than at @xmath26 , but the difference between the two vanishes asymptotically with increasing sample size .",
    "hence , we see that the minimum of the first derivative of @xmath11 or @xmath9 can be used as a selector for the optimal bin - width or bandwidth of density estimation , and @xmath26 is less the optimal bandwidth than @xmath32 for computing entropy .    note that both scott s optimal bin width @xmath28 and @xmath26 depend on the unknown underlying pdf of the system , which is usually difficult or even impossible to acquire . on the contrary ,",
    "the estimator of optimal bandwidth selected from the minimum of the first derivative of entropy is purely data - based , and hence , our method is clearly superior to the existing methods .",
    "we emphasize that our results are by no means restricted to one - dimensional , but can also be extended to multivariate cases .",
    "finally , we acknowledge that we do not provide a robust mathematical proof of the existence of the minimum of the first derivative of entropy , nor theoretically explain why such a minimum can help pick out the optimal bandwidth .",
    "these issues are surely of great importance and deserve further investigations .",
    "we leave these issues with those specialists who are interested in them .",
    "we thank the referee very much for many constructive suggestions and comments , especially for the reminding of the techniques for estimating information - theoretic quantities developed by @xcite and @xcite .",
    "this work is supported by the national basic research program of china ( no : 2010cb832805 ) and by the national science foundation of china ( no . 11273013 ) , and also supported by the open project program of state key laboratory of theoretical physics , institute of theoretical physics , chinese academy of sciences , china ( no .",
    "y4kf121cj1 ) ."
  ],
  "abstract_text": [
    "<S> in this work , we investigate the statistical computation of the boltzmann entropy of statistical samples . for this purpose , </S>",
    "<S> we use both histogram and kernel function to estimate the probability density function of statistical samples . </S>",
    "<S> we find that , due to coarse - graining , the entropy is a monotonic increasing function of the bin width for histogram or bandwidth for kernel estimation , which seems to be difficult to select an optimal bin width / bandwidth for computing the entropy . </S>",
    "<S> fortunately , we notice that there exists a minimum of the first derivative of entropy for both histogram and kernel estimation , and this minimum point of the first derivative asymptotically points to the optimal bin width or bandwidth . </S>",
    "<S> we have verified these findings by large amounts of numerical experiments . </S>",
    "<S> hence , we suggest that the minimum of the first derivative of entropy be used as a selector for the optimal bin width or bandwidth of density estimation </S>",
    "<S> . moreover , the optimal bandwidth selected by the minimum of the first derivative of entropy is purely data - based , independent of the unknown underlying probability density distribution , which is obviously superior to the existing estimators . </S>",
    "<S> our results are not restricted to one - dimensional , but can also be extended to multivariate cases . </S>",
    "<S> it should be emphasized , however , that we do not provide a robust mathematical proof of these findings , and we leave these issues with those who are interested in them .    </S>",
    "<S> [ firstpage ]    methods : data analysis  methods : numerical  methods : statistical  cosmology : theory  large - scale structure of universe . </S>"
  ]
}