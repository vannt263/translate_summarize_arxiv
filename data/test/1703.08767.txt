{
  "article_text": [
    "the _ polynomial eigenvalue problem _ consists of computing the eigenvalues , and often eigenvectors , of an @xmath0 matrix polynomial of degree @xmath1 : @xmath2 an eigenvalue of @xmath3 is any scalar @xmath4 such that @xmath5 .",
    "any nonzero vector @xmath6 is an eigenvector corresponding to @xmath7 .",
    "the _ algebraic multiplicity _ of @xmath7 is its multiplicity as a root of @xmath8 , and the _ geometric multiplicity _ of @xmath7 is the dimension of @xmath9 .    throughout this paper",
    "we assume that the matrix polynomial is _ regular _ , that is , @xmath8 is not the constant zero polynomial , and therefore the set of all eigenvalues is a subset of the extended complex plane with cardinality @xmath10 .",
    "infinite eigenvalues of   can occur if the leading coefficient matrix is singular and are defined as the zero eigenvalues of the reversal polynomial , @xmath11    computing an eigenpair @xmath12 is useful for a large range of applications  @xcite . of extreme importance",
    "are special cases of the polynomial eigenvalue problem , such as finding the roots of a scalar polynomial ( @xmath13 ) and solving the linear eigenvalue problem ( @xmath14 ) .",
    "what s more , the established techniques for solving these special case problems motivate two current approaches for solving the polynomial eigenvalue problem : linearization and root finding methods .",
    "the linearization of a matrix polynomial results in an equivalent linear eigenvalue problem which is often solved using qz iteration .",
    "algorithms which adopt this approach have computational complexity @xmath15 and include the popular matlab functions quadeig  @xcite and polyeig  @xcite .",
    "more recently , it was shown that exploiting the inherent structure in the companion linearization results in a @xmath16 algorithm  @xcite .",
    "however , often the original matrix polynomial comes with structure worth exploiting and , in general , the companion linearization does not preserve this structure .",
    "furthermore , the conditioning of the larger linear problem can be worse than the original problem  @xcite .    to our knowledge ,",
    "kublanovskaya was the first to use root - finding methods to solve the polynomial eigenvalue problem  @xcite when she suggested the use of the qr decomposition with column pivoting and newton s method to compute an eigenvalue of the matrix polynomial .",
    "improvements to this method were given , and quadratic convergence was shown , by jain , singhal , and huseyin  @xcite .",
    "more recently , a cubic convergent algorithm using the ehrlich - aberth method to compute the eigenvalues of a matrix polynomial was presented  @xcite .",
    "these root - finding methods are rather inefficient , though , as they compute one eigenvalue at a time , each requiring several @xmath17 factorizations .",
    "however , certain structures in the original problem can be exploited , thus increasing the efficiency of these methods .",
    "in addition to preserving the structure , root - finding methods have the advantage of preserving the size and conditioning of the original problem    root - finding methods exhibit a high level of accuracy , thus making them useful in the context of iterative refinement of computed eigenvalues and eigenvectors .",
    "they have been shown to be cost efficient for solving large degree polynomial eigenvalue problems  @xcite , and are the driving force behind what is perhaps the fastest and most accurate algorithm for solving the nonsymmetric tridiagonal eigenvalue problem  @xcite .",
    "furthermore , both laguerre s method and the ehrlich - aberth method have been used as an accurate and efficient method for solving the quadratic tridiagonal eigenvalue problem  @xcite .    in this paper , we propose a root - finding algorithm which uses laguerre iteration to solve the polynomial eigenvalue problem .",
    "our method is motivated by the previous work of bini and noferini  @xcite , gary  @xcite , and parlett  @xcite . in ",
    "[ sec : lmpep ] we present a method for computing the laguerre iterate of an approximate eigenvalue .",
    "we provide robust methods for computing the corresponding right and left eigenvectors , backward error , and condition estimates .",
    "both hessenberg and tridiagonal structures are considered , and it is shown that hyman s method can be used to obtain significant computational savings . in  [ subsec : ie ]",
    "we develop a method based on the numerical range for computing initial estimates to the eigenvalues of a matrix polynomial . under suitable conditions",
    "these initial estimates are no bigger in absolute value than the upper pellet bounds .",
    "finally , an a priori check for both zero and infinite eigenvalues is implemented and comparisons are made to the approach developed in  @xcite .    in  [ sec : stability ] we discuss the stability of our method .",
    "specifically , we show that our method is robust against overflow and that under suitable conditions we can guarantee the backward stability of the eigenvalues computed . in  [ sec : ne ] numerical experiments are provided to verify the accuracy and cost analysis of our method .",
    "additionally , comparisons are made to the methods in  @xcite and  @xcite to verify the effectiveness of our method for computing the roots of a polynomial and solving the tridiagonal polynomial eigenvalue problem , respectively .",
    "laguerre s method has a rich history originating with the work of edmond laguerre  @xcite .",
    "laguerre s method has incredible virtues including guaranteed global convergence when all roots are real  @xcite , and when these are simple zeros this method is known to exhibit local cubic convergence . in practice , the complex iterations seem as powerful as the real one s  @xcite .",
    "both numerical recipes ( zroots ) and the nag 77 library ( c02aff ) employ a modified laguerre method to compute the roots of a scalar polynomial . in 1964 - 65",
    ", laguerre s method was applied to the linear eigenvalue problem , both in the monic  @xcite and non - monic  @xcite cases .",
    "now we apply laguerre s method to the polynomial eigenvalue problem .",
    "since @xmath3 is assumed to be regular , the polynomial @xmath18 has at most @xmath19 roots , where @xmath20 and @xmath21 is the number of infinite eigenvalues .",
    "given an approximation @xmath7 to one of the roots of @xmath22 , laguerre s method uses @xmath22 , @xmath23 , and @xmath24 to obtain a better approximation .",
    "following the development in  @xcite , we define the following : @xmath25 where @xmath26 are the roots of @xmath22 , and @xmath27 then the next approximation is given by @xmath28 where the sign of the square root is chosen to maximize the magnitude of the denominator .",
    "we call @xmath29 the _ laguerre iterate _ of @xmath7 .",
    "once the roots @xmath30 have been found , we deflate the problem by subtracting @xmath31 from equations   and  , respectively .",
    "the undesirable numerical properties of the determinant are well - known , and it is for these reasons that we do not work with the polynomial @xmath22 directly .",
    "rather , an effective method for computing equations   can be derived from jacobi s formula : @xmath32 where @xmath33 and @xmath34 . the first formula in",
    "can be found in  @xcite .",
    "the second formula follows from the first by using the derivative product rule and noting that @xmath35 .",
    "note that only the diagonal entries of @xmath36 are needed in  , which is significantly less expensive than computing the matrix product .",
    "in general , the method we propose begins with initial estimates to the eigenvalues of the matrix polynomial .",
    "then , proceeding one at a time , the laguerre iteration of each eigenvalue approximation is computed , which requires solving the matrix equations in  .",
    "each eigenvalue is updated until at least one of the stopping criteria are met ( see  [ sec : esc ] ) .",
    "locally , if the root is simple , convergence is cubic ; otherwise , it is linear .",
    "furthermore , in practice , the total number of iterations needed to compute all eigenvalues is proportional to the product @xmath10 ; therefore our method has computational complexity @xmath37 .    in  [ sec : hesstri ] , we show that significant computational savings can be obtained from hyman s method for both hessenberg and tridiagonal matrix polynomials .",
    "in addition , the general method can easily be specialized for scalar polynomials , and we are left with a method that has computational complexity @xmath38 .",
    "denote by @xmath4 an approximate eigenvalue , and define @xmath39 where @xmath40 is a permutation matrix such that @xmath41 .",
    "if @xmath42 , where @xmath43 is some predetermined tolerance , then we say that the approximate eigenvalue has converged .",
    "this constitutes our first stopping criterion . in ",
    "[ sec : bs ] we define @xmath43 and show that the first stopping criterion guarantees that the backward error in the approximate eigenpair is very small .    given that @xmath7 has converged , we compute the corresponding right and left eigenvectors by @xmath44 where @xmath45 @xmath46 and @xmath47 is the @xmath48 standard basis vector .",
    "this approach works well when @xmath42 , and while this is sufficient to guarantee that the approximate eigenvalue has converged , it is not necessary .",
    "indeed , there exist upper triangular matrices that are `` nearly '' rank deficient , yet none of the main diagonal entries are extremely small  @xcite .",
    "for this reason , we introduce a second stopping criterion based on an upper bound estimation of the backward error in the eigenvalue approximation .",
    "if an approximate eigenvector has not been computed , then it follows from  @xcite[lemma 3 ] that for any nonzero vector @xmath49 the backward error in the eigenvalue approximation is bounded above by @xmath50 where @xmath51 .",
    "suppose @xmath7 is an approximate eigenvalue such that the upper bound on its backward error , and therefore its backward error , is less than double precision _ unit roundoff _",
    ": @xmath52 ; then we say that @xmath7 has converged .",
    "this constitutes our second stopping criterion . in practice",
    "we take the min of   over three nonzero vectors @xmath49 .",
    "if none of the diagonal entries in the matrix @xmath53 are less than @xmath43 , then   is not suitable for computing the corresponding eigenvectors .",
    "rather , we compute the singular vectors corresponding to the smallest singular value of @xmath3 . with the qr factorization with column pivoting in  , we apply inverse iteration to @xmath54 to compute the right and left singular vectors , respectively .",
    "our experience indicates that using   to form initial estimates for the inverse iterations results in quick convergence to excellent eigenvector approximations .    now , suppose that @xmath7 is an approximate eigenvalue which satisfies latexmath:[\\[\\label{eq : ssc }    @xmath29 is the laguerre iterate defined in  .",
    "then no significant change to the current eigenvalue approximation is made , and we say that @xmath7 has converged .",
    "this constitutes our third stopping criterion . in this case , or in the case where some predefined maximum number of iterations has been reached , we can not make a strong statement about the approximations backward error . at this point ,",
    "our best option is to proceed by computing the singular vectors corresponding to the smallest singular value of @xmath3 using the inverse iteration described in  .    in summary , given an approximate eigenvalue , we compute the qr factorization with column pivoting in  .",
    "if any of the three stopping criteria are met , or the maximum number of iterations is reached , then we cease to update the eigenvalue approximation and compute corresponding right and left eigenvectors .",
    "otherwise , we use the qr factorization to update the eigenvalue approximation by solving   and computing the laguerre iterate .",
    "several remarks are in order .",
    "first , the norm of the matrix coefficients are only computed once , and in practice , we replace the matrix 2-norm with the frobenius norm .",
    "second , the definition we ve given for @xmath56 in   results in a relative normwise measurement of the backward error ( see  [ sec : bs ] ) .",
    "finally , we note that the addition of computing the eigenvectors for each approximate eigenvalue has not changed the computational complexity of our method , which is @xmath37 .",
    "once the approximate eigenvalue has converged and the corresponding right and left eigenvectors have been computed , we report each eigenvalue s condition number .",
    "it follows from  @xcite[theorem 5 ] , that the normwise condition number of a nonzero finite simple eigenvalue is given by @xmath57 for simple zero and infinite eigenvalues , we report @xmath58 as the condition number , where @xmath59 and @xmath60 are right and left eigenvectors corresponding to the zero eigenvalues of the matrices @xmath61 and @xmath62 , respectively .",
    "a root - finding method s performance is greatly influenced by its initial estimates . in  @xcite , it is suggested to use the newton polygon of a polynomial formed from the norm of the coefficient matrices to obtain initial estimates to the eigenvalues of the matrix polynomial @xmath3 . in this section , we review this newton polygon approach , since we will use it to form initial estimates in the scalar case .",
    "however , for matrix polynomials we propose a new method , motivated by the numerical range of the matrix polynomial , for computing the initial estimates .",
    "the newton polygon approach works by placing initial estimates on circles of suitable radii .",
    "we quantify what constitutes suitable radii from the pellet bounds for matrix polynomials .",
    "[ thm : pellet ] let @xmath3 be an @xmath0 matrix polynomial of degree @xmath63 , where @xmath64 . for each @xmath65 such that @xmath66 is nonsingular , consider the equation @xmath67 where @xmath68 is any induced matrix norm .",
    "if @xmath69 there exists one real positive solution @xmath70 , and @xmath3 has no eigenvalues of moduli less than @xmath70 .    if @xmath71 there are either no real positive solutions or two real positive solutions @xmath72 .",
    "in the latter case , @xmath3 has no eigenvalues in the annulus @xmath73 .    if @xmath74 , then there exists one real positive solution @xmath53 , and @xmath3 has no eigenvalues of moduli greater than @xmath53 .",
    "a proof of theorem  [ thm : pellet ] can be found in  @xcite .",
    "moreover , it was noted in  @xcite that the bounds in theorem  [ thm : pellet ] can be sharpened if   is replaced by @xmath75    let @xmath76 be values of @xmath77 such that @xmath66 is nonsingular and there exists real positive solution(s ) @xmath78 to  .",
    "then @xmath79 for @xmath80 , and there are @xmath81 eigenvalues of @xmath3 in the closure of the annulus @xmath82 .",
    "if for @xmath69 or @xmath74 the matrix @xmath66 is singular , then @xmath83 or @xmath84 , respectively .",
    "computing the value of @xmath85 and @xmath86 is expensive , since it requires solving several matrix and polynomial equations .",
    "however , a cheap algorithm for approximating @xmath85 and @xmath86 was proposed in  @xcite .    for the scalar case ( @xmath13 ) there is an alternative to computing the values of @xmath85 and @xmath86 .",
    "consider the polynomial @xmath87 , where @xmath88 .",
    "newton polygon _ associated with this polynomial is the upper convex hull of the discrete set @xmath89 .",
    "let @xmath90 denote the abscissas of the vertices of the newton polygon , and define the radii @xmath91 for @xmath80 .",
    "then @xmath92 initial estimates to the roots of @xmath93 are placed on circles centered at @xmath94 with radius @xmath95 . in  (",
    "* theorem 1.2 ) they show that these estimates lie within the pellet bounds for the polynomial @xmath93 , and in  @xcite they establish the efficiency of these initial estimates for solving the roots of a polynomial .    in",
    "@xcite they generalize this approach for a specific class of matrix polynomials , and in  @xcite to general matrix polynomials . in practice , the idea is simple .",
    "let @xmath96 then , @xmath81 initial estimates to the eigenvalues of @xmath3 are placed on circles centered at zero with radius @xmath95 , for @xmath80 , where both @xmath97 and @xmath95 are defined as in   with reference to the newton polygon associated with the polynomial @xmath93 .",
    "the _ numerical range _ of a matrix polynomial is the set @xmath98 which clearly contains the set of all eigenvalues . under suitable conditions ,",
    "see theorem  [ thm : sampie ] , the roots of the quadratic form @xmath99 , where @xmath100 is of unit length , are no bigger in absolute value than the upper pellet bound , see theorem  [ thm : pellet ] . in practice , we make use of the columns of @xmath101_{j=1}^{n}$ ] , already obtained from the qr factorization of the constant and leading coefficient matrices , see  [ sec : zeroinf ] .",
    "initial estimates to the finite eigenvalues are computed as the roots of @xmath102 for @xmath103 .    if @xmath104 , then @xmath105 coincides with the classical numerical range ( field of values ) of the matrix @xmath106 , which has wonderful properties including convexity and connectedness . in general , however , the numerical range of a matrix polynomial need not have these properties and is bounded if and only if the field of values of the leading coefficient matrix does not contain the origin . for a detailed introduction to the numerical range of a matrix polynomial and",
    "its geometric properties see  @xcite .",
    "it is highly nontrivial to give a complete description of the set @xmath105 . despite this , we have experienced great success using elements from the numerical range as initial estimates for the eigenvalues we wish to compute .",
    "this seems to be a consequence of the habitual nature of elements from the numerical range to adhere to the geometric structure of the spectrum . to exemplify this statement ,",
    "consider the hyperbolic matrix polynomial @xmath3 , which by definition has a numerical range that satisfies @xmath107 .",
    "then , it is clearly advantageous to use initial estimates from the numerical range over elements on a circle in the complex plane .",
    "even more revealing , the numerical range of a hyperbolic matrix polynomial is split into @xmath1 `` spectral regions '' each containing a root of @xmath99 .",
    "each spectral region is an interval ( possibly degenerate ) on the real line that contains @xmath108 eigenvalues of @xmath3  @xcite . in general , singling out a part of @xmath105 containing precisely @xmath77 roots of @xmath99 for any unit vector @xmath100 and separated from the rest of @xmath105 by a circle establishes the existence of a spectral divisor of order @xmath77 whose spectrum lies in that region  @xcite[  26.4 ] . for simplicity",
    ", we also reference this region as a spectral region .    in what follows ,",
    "we provide three example problems from the nlevp package  @xcite to illustrate the potential competitive advantage to be had from using the numerical range .",
    "the first two examples are of hyperbolic matrix polynomials , but the third is not . in each case , it is clear that the roots of the quadratic form are adhering to some spectral region in the plane . each example contains a plot of the initial estimates using both the numerical range and newton polygon , as well as the approximated eigenvalues .    [ ex : spring ]        [ ex : cd_player ]        the earlier examples highlight the advantage the numerical range has to offer , especially when the eigenvalues are real .",
    "this advantage leads to cutting the computation time in half when solving the spring problem , and by a quarter when solving the cd player problem . in the following example , the eigenvalues are complex , but the advantage of the numerical range is still evident .",
    "note how the elements from the numerical range clearly identify the @xmath109 spectral regions in the complex plane .",
    "[ ex : butterfly ]        not only do the elements of the numerical range adhere to the spectrum better than points on a circle in the complex plane , they are often , in practice , within the pellet bounds from theorem  [ thm : pellet ] .",
    "we can make the following precise statement .",
    "[ thm : sampie ] let @xmath3 be a self - adjoint matrix polynomial .",
    "then for any @xmath110 , @xmath111 is no bigger than the upper pellet bound .",
    "let @xmath100 be a vector with unit length .",
    "the upper pellet bound on the roots of the polynomial @xmath99 is the unique real positive solution to the equation @xmath112 for any self - adjoint matrix @xmath106 , it is well - know that @xmath113 . therefore , @xmath114 let @xmath53 denote the upper bound on the roots of @xmath99 and @xmath115 denote the upper bound on the eigenvalues of @xmath3 .",
    "then , by theorem  [ thm : pellet ] , @xmath116 and the result follows .",
    "laguerre s method experiences local cubic convergence if the root is simple ; otherwise , convergence is linear . in practice , it is most common to have multiple zero and infinite eigenvalues .",
    "therefore to avoid poor performance when dealing with multiple roots , we employ an a priori identification of zero and infinite eigenvalues . during this identification process",
    ", we assume that the zero and infinite eigenvalues are semi - simple and thus our problem turns into a familiar one : to determine the rank of the matrices @xmath61 and @xmath62 .    in order to determine the rank of a matrix @xmath106",
    ", we perform a qr factorization with column pivoting .",
    "let @xmath117 , where @xmath118 @xmath119 is @xmath120 , @xmath121 is @xmath122 , @xmath123 , and @xmath40 is a permutation matrix such that the diagonal entries in @xmath53 occur in non - increasing order .",
    "our aim is to determine an index @xmath124 such that @xmath119 is well - conditioned and @xmath125 is negligible . if @xmath126 , then the matrix @xmath106 is rank deficient and the dimension of its null space is @xmath127 .",
    "we compute a basis for the right and left nullspace by @xmath128 where @xmath129 @xmath130 ,  @xmath131 , and @xmath132 is the @xmath133 standard basis vector for @xmath134 .",
    "the above process is performed on both matrices @xmath61 and @xmath62 , thereby computing the geometric multiplicity of the zero and infinite eigenvalues , respectively , and their corresponding right and left eigenvectors .",
    "once this is done , the columns of the matrix @xmath135 are then used to compute initial estimates to the remaining finite eigenvalues via the roots of the quadratic form @xmath102 for @xmath103 .",
    "we compute the roots of each polynomial using laguerre s method , specialized for the scalar polynomial , which was outlined previously .",
    "note that the computation of the qr factorization along with solving the @xmath108 polynomial equations has a computational complexity of @xmath136 and is therefore in accordance with the computational complexity of our method .",
    "we are motivated to consider the case where the coefficients of the matrix polynomial are in hessenberg or tridiagonal form .",
    "the hessenberg case is of both theoretical and practical importance . in light of the original development of hyman s method",
    ", we will consider this method for upper hessenberg matrix polynomials and note the tridiagonal matrix polynomial as a special case .",
    "what s more , every matrix polynomial can be reduced to hessenberg form  @xcite . while no numerically stable algorithm currently exists to perform this reduction , there exist applications where this hessenberg structure arises naturally ; for example , the bilby problem in  @xcite . with regards to the tridiagonal case ,",
    "previous developments have focused on the linear and quadratic polynomial eigenvalue problem  @xcite , whereas our development is applicable to any degree polynomial eigenvalue problem .",
    "hyman s method , a method for evaluating the characteristic polynomial and its derivatives at a point , is attributed to a conference presentation given by m.a .",
    "hyman of the naval ordnance laboratory in 1957  @xcite .",
    "the backward stability of this method has been shown  @xcite , and this method has been used to evaluate the characteristic polynomial of a matrix  @xcite and matrix pencil  @xcite . here",
    "we generalize these approaches in order to apply hyman s method to the matrix polynomial .",
    "we denote an upper hessenberg matrix polynomial as follows @xmath137 where @xmath138 is a scalar polynomial of degree at most @xmath1 .",
    "note that in the tridiagonal case @xmath139 for @xmath140 .",
    "the insightful observation that hyman made was that @xmath3 has the same determinant as @xmath141 provided that @xmath142 if we let @xmath18 , then @xmath143 where @xmath144 . given a fixed scalar @xmath7 , all unknown values in   can be computed in @xmath145 time for hessenberg @xmath3 and in @xmath146 time for tridiagonal @xmath3 .",
    "the values of @xmath147 are then used to solve the following equation @xmath148 then the values of @xmath147 and their derivatives are used to compute @xmath149 @xmath150    once @xmath151 , @xmath152 , and @xmath149 have been computed , an efficient computation of the laguerre correction term can be obtained from the following @xmath153 note that we have carefully avoided the potentially hazardous product in computing @xmath154 and its derivatives by replacing it with @xmath155 and @xmath156 where @xmath157 .",
    "several remarks are in order .",
    "first , if any subdiagonal of @xmath3 is zero , then solving  - will require division by zero .",
    "fortunately , we can replace any zero subdiagonal with double precision unit roundoff @xmath158 and maintain the backward stability of hyman s method  @xcite .",
    "second hyman s method significantly reduces the cost of each iteration and the resulting cost of our method is @xmath159 for hessenberg matrix polynomials and @xmath160 for tridiagonal matrix polynomials .",
    "let @xmath4 be an approximate eigenvalue and define @xmath161 this factorization can be done in @xmath145 time for hessenberg @xmath3 and in @xmath146 time for tridiagonal @xmath3 .",
    "let @xmath162 denote the index that minimizes @xmath163 .",
    "if @xmath164 , where @xmath43 is some predetermined tolerance , then we say that the approximate eigenvalue has converged .",
    "this constitutes our first stopping criterion .",
    "given that @xmath7 has converged , we compute the corresponding right and left eigenvectors using @xmath165 where @xmath166 @xmath167 ,  @xmath168 ,  @xmath169 , and @xmath170 .",
    "if there exists no index @xmath162 such that @xmath164 , then we compute an upper bound for the backward error of the eigenvalue approximation via  .",
    "if the backward error of @xmath7 is less than @xmath158 , then we say that @xmath7 has converged .",
    "this constitutes our second stopping criterion .",
    "we then apply inverse iteration to @xmath171 to compute the right and left singular vectors , respectively . using   to form initial estimates for the inverse iteration results in quick convergence to excellent eigenvector approximations .    as was done in ",
    "[ sec : esc ] , we also check if the approximate eigenvalue @xmath7 satisfies  . in this case , no significant change to the current eigenvalue approximation is made , and we say that @xmath7 has converged .",
    "this constitutes our third stopping criterion .    in summary ,",
    "given an approximate eigenvalue , we compute the qr factorization in  . if any of the three stopping criteria are met , or the maximum number of iterations allowed is reached , then we cease to update the eigenvalue approximation and compute corresponding right and left eigenvectors .",
    "otherwise , we use hyman s method to compute the laguerre iterate . once the approximate eigenvalue has converged and",
    "the corresponding right and left eigenvectors are computed , we report each eigenvalue s condition number  .      just as was done with the general matrix polynomial , initial estimates consist of computing the geometric multiplicity of the zero and infinite eigenvalues , a basis for the corresponding eigenspace , and initial estimates to the remaining finite eigenvalues via the numerical range . for the hessenberg case",
    ", there is no difference whatsoever , since we can accomplish all of the above while adhering to the cost of the method .",
    "however , for the tridiagonal case we must make several changes in order to align with the method s cost .",
    "when computing the geometric multiplicity of the zero and infinite eigenvalues , we must settle for only a qr factorization of the coefficient matrices @xmath61 and @xmath62 , since the column pivoting has the potential to destroy the tridiagonal structure and make this method too expensive .",
    "therefore , we can not expect that the diagonal entries of the upper triangular @xmath53 appear in descending order .",
    "it is for this reason that we identify the pivots of @xmath53 one row at a time . by keeping track of the location of the previous pivot and utilizing the structure of @xmath53",
    ", we can identify whether or not each row has a pivot , and the location of said pivot , in @xmath145 time .",
    "then , the dimension of the corresponding eigenspace is @xmath172 , where @xmath77 is the number of rows with a pivot .",
    "if @xmath173 , then we use the location of each non - pivot column to compute a basis for eigenspace in @xmath145 time .",
    "the qr factorization of the tridiagonal matrices @xmath61 and @xmath62 is computed using plane rotations and therefore each column vector of @xmath135 can be computed in @xmath146 time . furthermore , each quadratic form @xmath99 can be computed in @xmath174 time and the roots of each scalar polynomial can be computed in @xmath38 time .",
    "it follows that the initial estimates of the tridiagonal matrix polynomial can be found in @xmath160 time .",
    "the stability of any numerical method is of the utmost importance . in this section ,",
    "we provide a detailed account of why our method is robust against the potentially harmful overflow in the evaluation of the matrix polynomial and its derivatives .",
    "furthermore , we identify the predetermined tolerance used in the stopping criteria (  [ sec : esc ] ) and show that if either the first or second stopping criterion holds then we can guarantee the backward stability of our eigenvalue approximation .      both the computation of the laguerre iterate as well as the corresponding eigenvector approximation is driven by a qr factorization of @xmath3 , with column pivoting for the general matrix polynomial , where @xmath7 is the current eigenvalue approximation .",
    "the evaluation of @xmath3 can be done efficiently using horner s method , but for large degree matrix polynomials this computation is prone to overflow .",
    "it is for this reason that when @xmath175 we opt to work with the reversal polynomial  , with @xmath176 .",
    "one may argue that now our computation is prone to underflow , but this is not harmful ; as @xmath177 , @xmath178 , which is aligned with our definition of the infinite eigenvalues of @xmath3 being the zero eigenvalues of @xmath179 .",
    "now , the general laguerre correction term in   becomes : @xmath180 where @xmath181 , and @xmath182 . by using   when @xmath183 and   when @xmath175",
    ", we have a method for computing the laguerre iterate of a matrix polynomial which is robust against overflow .",
    "this is similar to the approach in  @xcite for evaluating polynomials , but to our knowledge , we are the first to apply this to matrix polynomials .    for hessenberg matrix polynomials ( tridiagonal case included ) , we apply hyman s method to the reversal polynomial in order to obtain the values of @xmath184 and @xmath185 , where @xmath186 .",
    "the laguerre correction term in   then becomes : @xmath187 by using   when @xmath183 and   when @xmath175 , we have a method for computing the laguerre iteration of an upper hessenberg matrix polynomial ( tridiagonal case included ) which is both efficient and robust against overflow .    for nonzero eigenvalues @xmath188 , where @xmath176 .",
    "therefore , if @xmath175 then we switch @xmath3 with @xmath179 in both  , for the general matrix polynomial , and  , for the hessenberg matrix polynomial ( which includes the tridiagonal case ) .",
    "the discussion on computing corresponding right and left eigenvectors in  [ sec : esc ] , for the general matrix polynomial , and  [ sec : hessesc ] , for the hessenberg matrix polynomial , carries over naturally . with the exception",
    "that the upper bound on the backward error in the eigenvalue approximation from   becomes : @xmath189 where @xmath190 , and @xmath49 is nonzero . in addition , the normwise condition number from   becomes : @xmath191      let @xmath4 be an approximate eigenvalue , @xmath59 a corresponding right eigenvector , and @xmath60 a corresponding left eigenvector . following the development in  @xcite , we define the normwise backward error of the right eigenpair by @xmath192x=0,\\,\\left\\vert\\delta a_{i}\\right\\vert_{2}\\leq\\epsilon\\left\\vert a_{i } \\right\\vert_{2},\\,i=0,1,\\ldots , d\\}\\ ] ] where @xmath193 .",
    "this definition of the normwise backward error is concerned with a relative measurement of perturbation in the coefficients of the matrix polynomial .",
    "the normwise backward error for the left eigenpair @xmath194 is similarly defined .",
    "the first stopping criterion outlined in sections  [ sec : esc ] and  [ sec : hessesc ] is concerned with the smallest diagonal entry of @xmath53 being less than @xmath43 .",
    "we define @xmath195 if @xmath183 and @xmath196 otherwise , where @xmath51 , @xmath190 , @xmath176 , and @xmath158 is double precision unit roundoff .",
    "[ thm : berr1 ] if the first stopping criterion holds , then the approximate right eigenpair has a backward error bounded above by @xmath197 .",
    "from definition   and  @xcite[theorem 1 ] it follows that we may compute the normwise backward error for the right eigenpair @xmath12 by @xmath198 without loss of generality we assume that @xmath183 for the remainder of the proof .",
    "denote by @xmath199 the qr factorization , with column pivoting for general matrix polynomials , of @xmath3 .",
    "denote by @xmath59 the corresponding eigenvector , for the general matrix polynomial see   and for the hessenberg matrix polynomial ( including tridiagonal case ) see  .",
    "then the computed right eigenvector satisfies @xmath200 where @xmath201 .",
    "it follows from  @xcite[corollary 2.7.9 ] that @xmath202 , where @xmath203 denotes the frobenius norm .",
    "therefore , @xmath204 recall , in practice , that we replace the matrix 2-norm in the definition of @xmath56 with the frobenius norm and note that @xmath205 .",
    "thus , the result follows from dividing both sides of the above equation by @xmath206 to give @xmath207    if an approximate eigenvector has not been computed , then an appropriate measure of the backward error is given by @xmath208 again , without loss of generality , we assume that @xmath183 .",
    "if @xmath7 is an approximate eigenvalue for which the second stopping criterion holds , then there exists a nonzero vector @xmath49 such that @xmath209 it follows that the backward error in the approximate eigenvalue   is bounded above by @xmath158 .",
    "the corresponding right eigenvector is computed as an approximate right singular vector of @xmath3 corresponding to the smallest singular value @xmath7 and therefore minimizes the backward error in the right eigenpair  .",
    "note that the results in the section hold naturally for left eigenpairs . additionally , the result in theorem  [ thm : berr1 ] is a worst case scenario and typically you can ignore the factor of @xmath108 . finally , even though we can only guarantee the backward stability of our eigenvalue approximation",
    "if the first or second stopping criterion hold , in practice it is highly unlikely to experience anything but backward stability .",
    "we have implemented the algorithm for solving the polynomial eigenvalue problem via laguerre s method in the software package lmpep .",
    "this package contains our implementation in fortran 90 and can be freely downloaded from github by visiting https://github.com/nick314159/lmpep .    in this section ,",
    "we provide numerical experiments to verify the computational complexity , stability , and accuracy of our methods .",
    "all tests were performed on a computer running centos 7 with an intel core i5 processor , where the code was compiled with the gnu fortran ( gcc ) 4.8.5 20150623 ( red hat 4.8.5 - 11 ) compiler .",
    "we first verify the asymptotic complexity of the method . in ",
    "[ sec : lmpep ] it was shown that the computational complexity of the method for general matrix polynomials is @xmath37 , and therefore for scalar polynomials , the expected computational complexity is @xmath38 .",
    "in addition , in  [ sec : hesstri ] it was shown that the computational complexity of the method for hessenberg matrix polynomials is @xmath159 and for tridiagonal matrix polynomials is @xmath210 .",
    "four tests were executed :    * for the general matrix polynomial , we verify the quadratic complexity in @xmath1 by fixing @xmath211 and computing the eigenvalues of random matrix polynomials of degree @xmath212 .",
    "we also verify the quartic complexity in @xmath108 by fixing @xmath213 and computing the eigenvalues of random matrix polynomials of size @xmath214 .",
    "* for the scalar polynomial , we verify the quadratic complexity by computing the roots of random polynomials of degree @xmath215 .",
    "we compare the timings with the polzeros from  @xcite and amvw from  @xcite .",
    "* for the hessenberg matrix polynomial , we verify the quadratic complexity in @xmath1 by fixing @xmath211 and computing the eigenvalues of random hessenberg matrix polynomials of degree @xmath212 .",
    "we also verify the cubic complexity in @xmath108 by fixing @xmath213 and computing eigenvalues of random hessenberg matrix polynomials of size @xmath214 .",
    "* for the tridiagonal matrix polynomial , we verify the quadratic complexity in @xmath1 by fixing @xmath211 and computing the eigenvalues of random tridiagonal matrix polynomials of degree @xmath212 .",
    "we also verify the quadratic complexity in @xmath108 by fixing @xmath213 and computing eigenvalues of random tridiagonal matrix polynomials of size @xmath214 .",
    "we then verify the stability of our method . in ",
    "[ sec : stability ] it was shown that our method is robust against overflow and that if either the first or second stopping criterion are met then the approximate eigenpair has a tiny backward error . this along with a well - conditioned problem implies that our method is highly accurate .",
    "four tests were executed :    * for the scalar polynomial , we verify the accuracy of our method by computing the roots of random polynomials of degree @xmath216 .",
    "we compare the forward error with polzeros from  @xcite and amvw from  @xcite .",
    "* for the tridiagonal matrix polynomial , we verify the accuracy of our method by computing the eigenvalues of selected problems from both  @xcite and  @xcite .",
    "the forward error in our method is compared to the forward error in each respective method .",
    "* for the general matrix polynomial , we verify the stability of our method by solving select problems from the nlevp package  @xcite and comparing the backward error in our approximation to the backward error in quadeig .",
    "* for the general matrix polynomial , we verify the accuracy of our method by comparing the forward error in our approximations to those from quadeig for select problems from the nlevp package  @xcite .",
    "leveraging the inherent strengths of laguerre s method and the numerical range , we have proposed a versatile , stable , and efficient method for solving the polynomial eigenvalue problem ; supported by numerical experiments .",
    "furthermore , we have demonstrated the effectiveness of our initial estimates  (  [ subsec : ie ] ) , as well as the robustness  (  [ sec : rao ] ) and backward stability  (  [ sec : bs ] ) of our method .    to our knowledge , we are the first to utilize laguerre s method with such generality as to cover such a large range of polynomial eigenvalue problems .",
    "our method is also alone in its use of the numerical range for initial estimates . in section  [ subsec : ie ]",
    "we argue that these initial estimates adhere naturally to the geometry of the spectrum and we show that under suitable conditions they are no bigger in absolute value than the upper pellet bound  ( theorem  [ thm : pellet ] ) .",
    "implemented in the fortran package lmpep , numerical results attest to our method s computational complexity of @xmath38 in the scalar case , @xmath160 in the tridiagonal case , @xmath159 in the hessenberg case , and @xmath37 in the general case .",
    "moreover , numerical results verify the backward stability of our method and exhibit its unprecedented level of accuracy .",
    "we are eagerly awaiting the formal release of the complete code in  @xcite , so that we can make additional comparisons to our method , especially for solving large degree polynomial eigenvalue problems .",
    "it would be remiss not to mention some open questions and areas worth exploration . in theorem  [ thm : sampie ] , we show that roots of the quadratic form , under a vector of unit length , are no bigger in absolute value than the upper pellet bound .",
    "we conjecture that they are also no smaller than the lower pellet bound , but at this time are unable to produce a proof .",
    "we also conjecture that there are easily constructible vectors @xmath59 that such that the corresponding quadratic forms @xmath99 are distinct and their roots are within some minimal distance of the eigenvalues of @xmath3 .",
    "however , we know of no such construction at the time of this writing .    in summary , we have proposed a new method for solving the polynomial eigenvalue problem that is strong in its virtues , capable of high degrees of accuracy , relatively unconstrained in its domain of operability , and promising in its possibility for future advancements .",
    "the authors wish to acknowledge conversations with david watkins and dario bini which helped construct the ideas in this paper , and we wish to thank zdenek strakos and an anonymous referee whose comments helped improve this paper ."
  ],
  "abstract_text": [
    "<S> the polynomial eigenvalue problem arises in many applications and has received a great deal of attention over the last decade . </S>",
    "<S> the use of root - finding methods to solve the polynomial eigenvalue problem dates back to the work of kublanovskaya ( 1969 , 1970 ) and has received a resurgence due to the work of bini and noferini ( 2013 ) . in this paper , we present a method which uses laguerre iteration for computing the eigenvalues of a matrix polynomial . an effective method based on the numerical range is presented for computing initial estimates to the eigenvalues of a matrix polynomial . </S>",
    "<S> a detailed explanation of the stopping criteria is given , and it is shown that under suitable conditions we can guarantee the backward stability of the eigenvalues computed by our method . then , robust methods are provided for computing both the right and left eigenvectors and the condition number of each eigenpair . </S>",
    "<S> applications for hessenberg and tridiagonal matrix polynomials are given and we show that both structures benefit from substantial computational savings . finally , we present several numerical experiments to verify the accuracy of our method and its competitiveness for solving the roots of a polynomial and the tridiagonal eigenvalue problem .    </S>",
    "<S> matrix polynomial , polynomial eigenvalue problem , root - finding algorithm , laguerre s method    15a22 , 15a18 , 47j10 , 65f15 </S>"
  ]
}