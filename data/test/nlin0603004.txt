{
  "article_text": [
    "the physical phenomena in the real world are usually attributed to certain causalities @xcite . with different causalities ,",
    "the corresponding phenomena , often captured in the time series of measurement , might diverge significantly as illustrated in fig .",
    "[ somedata ] : they could be irregular fluctuations as shown in panel ( a ) , or pseudo - periodic data in panel ( b ) , or the mixture , i.e. , irregular fluctuations with periodic trends as indicated in panel ( c ) .        to understand the underlying mechanisms responsible for generating those different time series in fig .",
    "[ somedata ] , one needs to reply some elementary questions first : are the data linear or nonlinear , stochastic or deterministic , pseudo - periodic or chaotic ? to explore the possible answers , the method of monte carlo hypothesis test @xcite , or surrogate data test equivalently @xcite , is often applied .",
    "this method has become a useful tool to validate the results of dynamical analysis , and thus help understand the causal processes underlying the experimental data .",
    "for example , if through the method one finds that irregular fluctuations are not random variables , then it immediately implies that there exists some kind of dynamical ( deterministic ) structure .",
    "therefore a consequential conclusion is that , it is possible to build deterministic models  ( or model systems ) from the time series .",
    "the focus of this section is to introduce the framework of monte carlo hypothesis test .",
    "moreover , we will discuss some important concepts associated with the components that form the framework , which will be frequently applied in our later analysis .",
    "null hypothesis tests use statistical measures of the underlying system to determine the probability that a proposed hypothesis is true ( or false ) @xcite .",
    "the common procedures include @xcite :    1 .",
    "formulate the null hypothesis of interest , the alternative hypothesis and the potential risks associated with a decision .",
    "2 .   choose a test statistic .",
    "3 .   compute the frequency distribution of the test statistic under the null .",
    "4 .   with the guide of the frequency distribution , choose certain criterion to determine whether to reject the hypothesis or not .",
    "it is easy to see that the framework of hypothesis testing consists of the null hypothesis , the test statistic , the frequency distribution of the test statistic , and the discriminating criterion that determines whether to reject the null hypothesis or not . in order to obtain the frequency distribution of the test statistic ,",
    "traditionally one would need to carefully choose the test statistic such that it follows a well known distribution .",
    "but in practice , on one side it might be difficult to find the refined statistics for tests in many situations ; on the other , with modern computers the computer - intensive methods become feasible and popular @xcite .",
    "hence , the concept of monte carlo hypothesis test naturally appeared @xcite .",
    "the basic idea is to produce a number of different realizations under the null through monte carlo simulation . in practice",
    "these realizations are usually generated from the original experimental data , but not really ever observed , therefore they will also be called the surrogate data in the later . from the ensemble of the surrogates , one could calculate the empirical distribution and the confidence interval of the test statistic . in this sense ,",
    "the frequency distribution will essentially depend on the surrogate generation algorithm ( and the chosen statistic of course ) .",
    "therefore one could also say that the surrogate algorithm is one of the elements that form a null hypothesis test , as we do in the later .      for the convenience of our later discussion",
    ", we need to introduce some terminologies . following the notation in @xcite ,",
    "let @xmath0 be the set of all the possible processes for the problem under consideration .",
    "also let @xmath1 be the formulated null hypothesis and @xmath2 the set of processes that are consistent with the null @xmath1 .",
    "if @xmath2 consists of only one element , then the null hypothesis @xmath1 is called _ simple _ , otherwise it is called _ composite_.    given a composite null hypothesis @xmath1 and a process @xmath3 consistent with @xmath4 , let us denote the chosen test statistic by @xmath5 , and the corresponding probability distribution function ( pdf ) under the null hypothesis by @xmath6 .",
    "if for any two processes @xmath7 and @xmath8 ( @xmath9 ) in the set @xmath2 , one has that @xmath10 , then the statistic @xmath5 is said _ pivotal _ ; otherwise it is _ non - pivotal_.    a remarkable advantage of test statistics which are pivotal , as can be seen from the definition , is that one will always obtain the same statistic distribution @xmath11 , which is independent of the process @xmath3 chosen from the set @xmath2 . therefore adopting a pivotal test statistic",
    "might significantly reduce the difficulty in devising the algorithm to produce surrogates of the null @xcite .",
    "however , if the test statistic is non - pivotal , then there is no guarantee that @xmath12 holds for arbitrary processes @xmath7 and @xmath8 in @xmath2 .",
    "suppose that the time series @xmath13 under test is generated from a process @xmath14 , then in order to avoid possible false rejection of the null hypothesis , , but @xmath15 . with @xmath16 as the reference distribution",
    ", one would falsely reject the hypothesis .",
    "] it is usually required , as the sufficient condition , that the process @xmath17 producing the surrogate data satisfies @xmath18 .",
    "@xcite . here",
    "@xmath19 denotes the process estimated from the surrogate and @xmath20 the process estimated from the original data .",
    "strictly , the condition that the estimates coincide rather than the processes themselves helps to prevent to possibility over over - constrained surrogates .",
    "such over - constrainedness generates surrogates that agree to closely with the true data , and the discriminating power of the hypothesis test is reduced @xcite .",
    "] surrogates generated from such processes are called _ constrained _ realizations ; otherwise it is said _ non - constrained_. obviously , given a set of constrained - realization surrogates @xmath21 , any adopted test statistic @xmath5 will appear as if it were pivotal for the processes @xmath22 .    in the following sections , we will introduce various applications of the monte carlo hypothesis test method .",
    "one of the popular applications is to detect nonlinearity in a time series , as described in @xcite .",
    "other applications include the detection of aperiodicity @xcite and the correlation between irregular fluctuations with long term trends @xcite . as an important , and possibly the most attractive component , the well - tailored surrogate generation algorithm associated with the hypothesis deserves to receive great attention .",
    "in fact , because of its importance , the monte carlo hypothesis test method is often called surrogate data test , or surrogate data method @xcite in the literature . in this review",
    "we use these two terms interchangeably .",
    "there are already some excellent introductory works covering the topic of the surrogate data method ( for example @xcite ) , therefore we will not provide excessive detail in this paper . instead , we will dedicate our effort to introducing some of more recent progress .",
    "the readers are referred to the broad literature , much of which is cited here , for further detail .",
    "the three reviews @xcite are particularly recommended .",
    "a rational step before the application of nonlinear time series methods is to identify the presence of nonlinearity . for this purpose",
    ", one could employ the direct detection strategy , that is , in order to detect nonlinearity , one adopts some characteristic nonlinear statistics , such as the correlation dimension , the lyapunov exponent , the continuity , and so on @xcite , as the discriminating measures in the belief that these statistics reveal the essential behaviors of nonlinear systems .",
    "this strategy , however , may encounter a few disadvantages in practice . on one hand , in certain situations",
    "the characteristic nonlinear statistics do not play the role well as the unequivocal identifiers of the underlying systems @xcite .",
    "take the correlation dimension @xcite as an example , it was shown that some linear stochastic processes with simple power law spectra would also have finite non - integer values as many nonlinear systems do @xcite , thus one would fail to distinguish between linearity and nonlinearity by simply examining the values of the correlation dimension . on the other hand ,",
    "given only a limited amount of the realizations of the underlying systems , it is often difficult to evaluate the reliability of the test results based on the direct detection strategy .",
    "the situation becomes even worsen with the presence of noise , which , often is the case , will reduce the discriminating power of the characteristic statistics .",
    "thus one is forced back to the aforementioned scenario , i.e. , the adopted characteristic statistic fails to unequivocally identify the underlying system . as an example , let us consider the ( largest ) lyapunov exponent .",
    "theoretically its value shall be zero for a periodic orbit .",
    "however , if the periodic orbit is perturbed by noise components , the value of the lyapunov exponent might slightly increase and become positive , which is often deemed as the sign of chaos and therefore possibly engenders misleading conclusion .    an alternative strategy for nonlinearity detection",
    "is the surrogate data methods , as an application of the monte carlo hypothesis test .",
    "the test procedures go as follows , one first proposes a null hypothesis which usually assumes that the time series is initially generated by a linear stochastic process . with the null hypothesis ,",
    "one produces an ensemble of surrogate data based on the original time series .",
    "then one chooses a proper test statistic in the sense that , if the original data is consistent with the null hypothesis , the statistic of the original data shall follow the same distribution as those of the surrogates , otherwise it shall appear atypical to the distribution . after calculating the test statistic ,",
    "one inspects whether the statistic value of the original data appears typical to the distribution of the surrogates according to certain discriminating criterion .",
    "if the answer is no , one rejects the null with certain confidence level ( depending on the chosen discriminating criterion , as will be discussed in the later ) , which implies that the data in test is very likely to be nonlinear .    in this section",
    ", we will first review the the hierarchical surrogate data tests for nonlinearity detection proposed by theiler et al .",
    ". we will also introduce other surrogate data methods @xcite , which essentially follow the same hierarchical framework but differentiate in the way of surrogate generation , as will be explained in the following .      here",
    "we confine our discussion to stationary irregular time series . with the data , as the first step of analysis we want to detect the potential nonlinearity in it .",
    "of course it is also possible that the irregular data is produced from a linear stochastic system , or a linear deterministic system contaminated by noise . since the stationary data generated by a linear deterministic system appear either periodic or constant , even with noise components it is trivial to distinguish between linear stochastic and deterministic cases if the noise level is not extremely high .",
    "therefore in the later we will only consider the scenario that the stationary irregular time series is generated from either a linear stochastic process or a nonlinear stochastic or deterministic process .",
    "following the aforementioned procedures , we will introduce one by one the basic elements that form the framework of null hypothesis tests .",
    "the basic assumption is that the time series under test is from a linear stochastic noise process , either i.i.d .",
    "( independent and identically distributed ) or correlated ( in the general form of an auto - regressive moving average @xmath23 process ) . but note that , it is possible to introduce nonlinearity into the original linear data during the measurement step by letting the original data pass through a nonlinear filter @xcite . with this consideration",
    ", one could formulate the following hierarchical composite hypotheses ( or those equivalently stated in @xcite ) , as shown in fig .",
    "[ hierarchicalnh ] :        * * null hypothesis 0 ( nh0 ) * : the data in test are i.i.d",
    ". noise with unknown mean and variance .",
    "* * null hypothesis 1 ( nh1 ) * : the data in test are produced from a linear stochastic process in the form of an @xmath23 model with unknown parameters , which is essentially a linear filter of the i.i.d noise . * * null hypothesis 2 ( nh2 ) * : the data in test are obtained by applying a static and monotonic nonlinear filter to the time series originally generated by an @xmath23 process .      in principle one shall choose the statistics in a way that measures of the original data and the surrogates shall appear consistent if the null hypothesis is true ; otherwise they will reveal the discrepancy .",
    "for this purpose , one of the popular choices in the literature is the correlation dimension , since it was shown that the correlation dimension is a pivotal statistic for the hierarchical null hypothesis tests for nonlinearity detection @xcite .",
    "of course , there are also many other proper candidates . and in general",
    ", the discriminating powers of the test statistics may vary from case to case @xcite .      broadly speaking",
    ", there are two strategies to generate surrogate data .",
    "one is to first build a parametric model based on the null hypothesis ( rather than the original data ) , and then use the model to produce the surrogates .",
    "the other one is to seek a nonparametric model to produce surrogates consistent with the null hypothesis , which is especially useful for the test of a composite null hypothesis and will thus be the focus in our later discussions .",
    "of course , parametric algorithms can also be constructed for composite hypotheses as long as one could find suitable pivotal test statistics . in such situations ,",
    "the hypothesis which one can test is that , the data are consistent with a particular parametric model ( possibly fitted to the data ) , or any other model with the same frequency distribution of the test statistic .",
    "this parametric approach can be particularly useful if one is interested in providing a behavior ( or dynamics ) based test of the suitability of a particular model . however , such parametric algorithms are often non - constrained and we will not consider them in this review .",
    "the interested readers are referred to @xcite .",
    "now let us begin introducing the nonparametric ( and constrained ) surrogate generation algorithms that correspond to the above hierarchical null hypotheses :    * * algorithm 0 * : to produce i.i.d .",
    "surrogates , one only needs to randomly shuffle the original data . * * algorithm 1 * : to produce linear stochastic surrogates , one first applies the fourier transform to the original data to obtain the corresponding moduli and phases .",
    "then one keeps the moduli but replace the phases by random numbers uniformly drawn from the interval @xmath24 $ ] .",
    "finally one applies the inverse fourier transform to the coefficients with preserved moduli but randomized phases .",
    "thus obtained data are the desired surrogates . * * algorithm 2 * : to produce surrogates consistent with _ nh2 _ , one first needs to invert the static and monotonic nonlinear filter to obtain the original linear stochastic data , then applies _ algorithm 1 _ to generate interim surrogate data of the linear data , and finally introduce the nonlinear filter back into the interim surrogates to obtain the final surrogates .",
    "the surrogates produced from the above algorithms will preserve the amplitude distribution of the original data as we expect .",
    "however there also exist a few defects in practice .",
    "one problem is that , for surrogates generated by _",
    "algorithm 1 _ , their power spectra will often deviate from that of the original data . as a remedy ,",
    "schreiber and schmitz @xcite suggested to repeat the surrogate generation procedures until the difference between the power spectra reaches certain stopping criterion .",
    "another problem is that , to apply the discrete fourier transform , the data has to be assumed periodic",
    ". therefore the wraparound artifact @xcite will be introduced .",
    "a possible remedy , as suggested in @xcite , is to conduct limited phase randomization .",
    "another remedy is to avoid adopting the fourier transform , as will be discussed later .",
    "since the exact knowledge of the statistic distribution is often not available , one will resort to certain discriminating criterion to help make the decision and determine the corresponding confidence level ( if to reject ) .",
    "the popular discriminating criteria in the literature include two classes : parametric and nonparametric .",
    "the parametric criterion assumes that the statistic follows a gaussian distribution , and the distribution parameters , i.e. the mean and the variance , would be estimated from the finite samples .",
    "one can determine whether to reject the null by examining whether the statistic of the original time series follows the statistic distribution of the surrogates , while the corresponding confidence level of inference can be calculated from the estimated statistic distribution ; the nonparametric criterion @xcite examines the ranks of the statistic values of the original time series and its surrogates .",
    "supposes that the statistic of the original time series is @xmath25 and the surrogate values are @xmath26 given @xmath27 surrogate realizations . then if the statistic of both the original time series and the surrogates follows the same distribution , the probability is @xmath28 for @xmath25 to be the smallest or largest among all of the values @xmath29 .",
    "thus if @xmath27 is large , when one finds that @xmath25 is smaller or larger than all of the values in @xmath30 , it is quite possible that @xmath25 instead follows a different distribution from that of @xmath26 .",
    "hence the criterion rejects the null hypothesis whenever the original statistic @xmath25 is the smallest or largest among @xmath29 , the false rejection rate is considered as @xmath31 for one - sided tests and @xmath32 for two - sided ones .",
    "the basic idea of the temporal shift algorithm @xcite goes as follows : for two independent time series @xmath33 and @xmath34 , if they are produced from a same linear stochastic process , then the additions @xmath35 for arbitrary real scalar coefficients @xmath36 and @xmath37 will also follow the same linear process , although possibly with different initial conditions . however , if @xmath38 and @xmath39 are from a nonlinear ( stochastic or deterministic ) process , in general adding them together will increase the complexity .",
    "thus their additions @xmath40 may behave different from @xmath38 and @xmath39 . and by adopting a proper test statistic , one may detect this difference .    in practice , if only given a single time series @xmath41 , then in order to produce surrogates , one could extract two subsets from the original data , for example , @xmath42 and @xmath43 , where parameter @xmath44 is the temporal shift - or more precisely , index shift - between @xmath45 and @xmath46 , and is often required to decorrelate @xmath45 and @xmath46 . the surrogates @xmath47",
    "are produced according to the formula @xmath48 , by either varying the temporal shift @xmath44 , or randomizing the coefficients @xmath36 and @xmath37 , or the combinations . in principle , there is no requirement for the coefficients @xmath36 and @xmath37 .",
    "but note that , if the ratio @xmath49 or @xmath50 , then roughly the surrogates @xmath51 or @xmath45 . therefore whether the original data is consistent the null hypothesis or",
    "not , the produced surrogates will look very close to it .",
    "consequently , even if the null hypothesis does not actually hold , the test statistic may fail to detect the tiny difference between the surrogates and the original data .",
    "thus spurious results will appear in these situations . to avoid this problem",
    ", we suggest that the ratio @xmath52 takes moderate values . for detail , see @xcite .    from the above discussions , it is easy to find that the temporal shift algorithm actually utilizes the fact that the superposition principle is applicable to linear processes rather than nonlinear ones . and",
    "with this fact , one could avoid applying the fourier transform to the original data and thus circumvent the consequential wraparound artifact .",
    "one may also note that , in contrast to the standard algorithms ( i.e. , _ algorithm 0 - 2 _ and the iterative version of _ algorithm 2 _ ) , the surrogates produced by the temporal shift algorithm do not exactly preserve the amplitude distribution of the original data .",
    "however , since the algorithm ensures that the surrogates are generated from the same process under the null hypothesis , it is still a constrained - realization surrogate generation algorithm . in our viewpoint ,",
    "the elimination of the restriction to preserving the amplitude distribution is actually an advantage of the new algorithm , which makes the algorithm more flexible and efficient to produce surrogates .",
    "simulated annealing @xcite is a stochastic approach that mimics the physical process to solve the combinatorial optimization problems .",
    "physically , the annealing process starts from a high temperature that melts the solids , then one gradually decreases the temperature .",
    "if the variation amplitude of the temperature is small enough , after a sufficiently long time all of the particles will reach the ground state so that the system energy is minimal .",
    "the simulated annealing bears an analogy to the physical process . as the initial condition ,",
    "the control parameter ( analogy to the temperature ) adopts a proper value .",
    "then one needs to carefully tune the parameter according to certain cooling schedule . at each parameter value , by devising an appropriate neighborhood generation ( or state updating ) mechanism and acceptance criterion , the transitions between the accepted states prove to form a homogeneous markov chain . and the global optimal state(s ) , which minimize(s ) or maximize(s ) the cost function ( analog to the system energy ) , will be achieved as the control parameter tends to zero .",
    "for detail , see , for example , @xcite .",
    "as we have mentioned previously , surrogates produce by _ algorithm 2 _ preserve the amplitude distribution of the original data but differentiate in the power spectra .",
    "one remedy for this problem is to iterate the surrogate generation procedures for a number of times until certain criterion is satisfied .",
    "however , usually there is no guarantee that the chosen criterion is the best , therefore it is possible that the iterative algorithm only engenders sub - optimal solutions ( i.e. , local minima ) or even worse .",
    "for this reason , applying the simulated annealing method for surrogate generation , in contrast , will often achieve better performance , as pointed out by schreiber @xcite .",
    "however , in some situations , this approach can be time consuming and provide limited benefit @xcite .    because the linear autocorrelations are directly related to the power spectrum @xcite , in configuration of the simulated annealing it is natural to choose , as the cost function , the norm of the difference between the linear autocorrelations of the original and the simulated data ( see eq.(2 ) of @xcite ) .",
    "the ( accepted ) simulated data is updated by exchanging the pairs of the former one , while the expected surrogate is the final simulated data when the stopping criterion is reached .",
    "hence it is easy to arrive the conclusion that thus generated surrogates are also constrained realizations in the sense that they preserve the amplitude distribution of the original data .    in practice , one inherent advantage to apply the simulated annealing method is that , after adequate cooling for the control parameter , the obtained solution could reach a local minimum sufficiently close to the global one .",
    "another advantage is that it does not need to invert the nonlinear filter for the test of _ nh2_. of course , there is also one obvious disadvantage , that is , depending on the size of the problem and the configuration of the algorithm , the computational time might substantially increase as often the case .",
    "in the previous section we have described the tests that can be applied to arbitrary time series data .",
    "we now confine our discussion to pseudo - periodic data . by _ pseudo - periodic data _ we mean those time series that exhibit strong periodic trends manifesting as clear spikes in the frequency domain @xcite ( see figure [ somedata](a ) and ( c ) ) .",
    "the underlying systems of pseudo - periodic data can be periodic orbits contaminated with observational or dynamical data , or oscillatory chaotic systems ( for example the rssler system ) . in this sense",
    ", one could also apply the surrogate data method to detect chaos in a pseudo - periodic time series , which will be the focus of this section .      here",
    "let us first specify the null hypothesis , which assumes that there are no temporal correlations at all between the spike - and - wave patterns ( i.e. , the individual cycle patterns ) of the pseudo - periodic time series @xcite .",
    "obviously , any purely periodic time series is consistent with the hypothesis .",
    "however , if there exists perturbations to the periodic orbits , then it requires that those inter - cycle perturbations are also uncorrelated at all ( for example , the i.i.d noise ) , which is a stronger constraint than that of the null hypothesis to be introduced in the next subsection .    since there are no temporal correlations between individual cycle patterns , similar to the idea of _ block bootstrap _ @xcite to decompose and shuffle individual blocks , the natural way to produce pseudo - periodic surrogates is to first extract the individual cycles from the pseudo - periodic time series and then randomly shuffle these cycles .",
    "for this reason , this method is often called cycle shuffled algorithm .",
    "note that , although not explicitly specified in the null hypothesis , in order to let the algorithm produce constrained realizations , it requires that intra - cycle dynamics of the individual cycles distributes periodically .",
    "theoretically the cycle shuffled algorithm is very simple , but there is a practical problem in implementation , which essentially lies in the difficulty in extracting the individual cycles from the test data . given a pseudo - periodic time series , shuffling the split cycles will often lead to the spurious discontinuity . to eliminate this phenomenon , one could vertically shift the individual cycles , but it often turns out to make the data become non - stationary and thus generate artificial long term correlation ( see , for example , the illustrations in @xcite ) . in the following",
    "we will introduce another algorithm that produces surrogates from a different viewpoint and avoids the above problems .",
    "moreover , if one is interested primarily in the variation between cycles , it may be better to study that directly @xcite .      here",
    "the null hypothesis under test is that the pseudo - periodic time series is produced from a periodic orbit perturbed by noise components that are identically distributed and uncorrelated for large enough temporal shifts @xcite .",
    "this null hypothesis is slightly more general than that in the previous subsection in the sense that , it does not require that there are no temporal correlations between the individual cycles .",
    "note that , adding together two subsets of the same periodic time series will lead to a new periodic data , while for chaotic time series applying the same transformation will usually increase the complexity . with this property , one could adopt the temporal shift algorithm as well to produce the surrogates under the hypothesis , i.e. , given a @xmath27-point pseudo - periodic time series @xmath41 , one could generate the surrogates @xmath47 according to the formula @xmath48 with proper coefficient ratios @xmath52",
    ". however , there is still an important difference , that is , for pseudo - periodic time series usually the temporal shift algorithm is not constrained .",
    "this is because in this situation the surrogates only preserve the periodicity but not necessarily the cycle pattern ( see fig .",
    "[ periodts ] for an illustration ) . therefore the surrogates ,",
    "although also periodic , may not come from the same underlying system as that of the original data . with this consideration , one shall choose a pivotal test statistic with robust performance against noise in calculation .",
    "an example of such choices is the correlation dimension evaluated by the gaussian kernel algorithm ( gka ) , as described in @xcite .     from the rssler system ;",
    "( b ) the reconstructed attractor in two dimensional embedding space @xmath53 vs. @xmath54 ; ( c ) addition time series @xmath55 ; ( d ) the reconstructed attractor in two dimensional embedding space @xmath56 vs. @xmath57 . from the reconstructed attractor @xcite in panel ( d ) , one can find that the addition time series is also period 6 , however its cycle pattern differs from that of the original time series.,width=432 ]      the attractor trajectory surrogate ( ats ) algorithm produces surrogates by inferring the underlying systems from a local model , and contaminating a trajectory on the attractor with dynamical noise @xcite . in this way",
    ", the surrogates preserve the gross scale dynamics of the original data but destroy the fine scale one .",
    "examples of the ats algorithm can be found in @xcite .",
    "here we only introduce the pseudo - periodic surrogate ( pps ) algorithm @xcite which is designed for detection of the null hypothesis that assumes the time series from a periodic orbit with uncorrelated dynamical noise .    given a scalar time series @xmath58 , the procures for surrogate generation go as follows @xcite :    1 .",
    "choose proper embedding dimension @xmath59 and time delay @xmath44 for time delay embedding reconstruction @xcite . by reconstruction based on the original data @xmath40 , one obtains a set of delay vectors @xmath60 with delay vector @xmath61^t$ ] and the embedding window @xmath62 .",
    "2 .   randomly choose a delay vector @xmath63 for initialization .",
    "3 .   let index @xmath64 start from @xmath65 .",
    "4 .   let @xmath66 be the current delay vector in operation .",
    "search in @xmath67 the neighbors of @xmath66 and randomly pick out one as the successor of @xmath66 , which is denoted by @xmath68 .",
    "take @xmath68 as the current operation vector .",
    "repeat the procedure in step 4 until index @xmath64 reaches the specified length , say , @xmath69 .",
    "6 .   the surrogate data @xmath70 , where @xmath71 denotes the first element in vector @xmath72 .    it was shown @xcite that the surrogates @xmath73 produced through the above procedures share the same vector field as the original data @xmath40 but are contaminated with dynamical noise .",
    "however , the produced surrogates may not strictly preserve the gross scale dynamics of the original data as we observed in practice . in this sense , the surrogates are not constrained realizations , therefore in tests one needs to choose a pivotal statistic ( e.g. , the correlation dimension as aforementioned ) .",
    "a new application of the surrogate data method is to detect correlations between irregular fluctuations possibly with a long term trend @xcite .",
    "the corresponding null hypothesis is that irregular fluctuations are independently distributed , which differentiates _",
    "nh0 _ of section 2 in that it does not require the identical distribution of the fluctuations .",
    "similar to the idea of the attractor trajectory surrogate ( ats ) algorithm , the surrogate generation algorithm devised in @xcite also aims to preserve the global behavior ( e.g. , the trend ) but destroy the local one . in the following",
    "let us explain in more detail",
    ".    given a scalar data @xmath74 , let the index set be @xmath75 such that @xmath76 , then the concrete steps for implementation of the idea include :    1 .",
    "perturb the original index set @xmath77 with gaussian random numbers @xmath78 so as to obtain a real number set @xmath79 .",
    "sort @xmath80 in the ascendant order to produce a new data set @xmath81 .",
    "re - ordering the index set @xmath77 will lead to the disturbed index set @xmath82 , which satisfies @xmath83 .",
    "the surrogate data @xmath84 is obtained by letting @xmath85 .    by choosing a proper amplitude @xmath86 ,",
    "typically the irregular fluctuations will only slightly move from the positions in the original data , therefore the generation mechanism is called small - shuffle surrogate ( sss ) algorithm .",
    "but note that , although the surrogates @xmath47 preserve the amplitude distribution of the original data @xmath87 , usually the surrogates are not constrained realizations .",
    "this is because the irregular perturbations are possibly not identically distributed , and locally shuffling the irregular perturbations may not exactly preserve the global dynamics .",
    "thus , one needs to carefully choose the test statistics , which may be the linear autocorrelation function or the average mutual information as suggested in @xcite .",
    "in the above sections we have reviewed the concept of surrogate data tests , the primary components that form the framework of this method , and some important properties of these components . we have also reviewed the applications of the surrogate data method , with the emphasis on some recently developed surrogate generation algorithms .    in all of the applications , since the specified null hypotheses are composite , it is required that the surrogate generation algorithms are nonparametric and work for any process consistent with the hypothesis .",
    "consequently , the broader range of the underlying processes a composite null hypothesis may cover , the more difficult it is to design the corresponding nonparametric surrogate algorithm .",
    "this fact limits the applications of the surrogate data method to detect many other interesting properties .",
    "another challenge is the design of a proper test statistic .",
    "the problem comes not only from the requirement of pivotal - ness when the surrogates are not constrained realizations , but also from the expectation that one obtains the exact confidence level to reject a null hypothesis .",
    "although simple in handle , the two discriminating criteria described in section 2 actually can not lead to inferences with exact confidence levels ( see the discussion in @xcite ) . the solution to this problem",
    "requires that one seeks the full knowledge of the distribution of the test statistic , which is often infeasible , even only at the asymptotic level , for many nonlinear statistics .",
    "this research was supported by hong kong university grants council competitive earmarked research grant  ( cerg ) no .",
    "polyu 5235/03e ."
  ],
  "abstract_text": [
    "<S> the surrogate data method is widely applied as a data dependent technique to test observed time series against a barrage of hypotheses . however , often the hypotheses one is able to address are not those of greatest interest , particularly for system known to be nonlinear . in the review we focus on techniques which overcome this shortcoming . </S>",
    "<S> we summarize a number of recently developed surrogate data methods . </S>",
    "<S> while our review of surrogate methods is not exhaustive , we do focus on methods which may be applied to experimental , and potentially nonlinear , data . in each case </S>",
    "<S> , the hypothesis being tested is one of the interests to the experimental scientist . </S>"
  ]
}