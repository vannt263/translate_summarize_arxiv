{
  "article_text": [
    "many online algorithms base their model update on the margin of each example .",
    "passive online algorithms , such as rosenblatt s perceptron @xcite and crammer et al s online passive - aggressive algorithms @xcite , update the algorithm s model only if the value of the margin falls below a defined threshold .",
    "these algorithms fully evaluate the margin for each example , even if the model is not to be updated !",
    "the running time of these algorithms is linear either in the number of features , or in the dimensionality of the input space .",
    "contemporary models may have thousands of features making running time daunting .",
    "the budgeted learning community addresses this problem by putting a budget on the number of features a classifier can evaluate while learning and while making predictions .",
    "our work stems from the theoretical framework suggested by ben david and dichterman @xcite , and is closely related to recent work by cesa - bianchi et al .",
    "@xcite as well as reyzin @xcite .",
    "we differ by the fact that we do not impose a hard budget constraint on the number of features , but rather look at the probability of making decision errors .",
    "decision error are errors that occur when the algorithm stops the feature evaluation process , predicts its outcome , and is wrong .",
    "this work extends on previous work by pelossof et al .",
    "@xcite .",
    "we propose a new method for early stopping the computation of feature evaluations for uninformative examples by connecting the perceptron algorithm to sequential statistical tests @xcite ( figure [ fig : attentive - perceptron ] . )",
    "this connection results in a general method that makes margin based learning algorithms attentive , which means that they have the ability to quickly filter uninformative examples .",
    "the margin of each example is computed as a weighted sum of feature evaluations .",
    "informative examples are misclassified examples , which force the perceptron to preform a model update , whereas uninformative examples are correctly classified and therefore ignored by the perceptron .",
    "we break up the feature evaluation for every example in the stream .",
    "the breakup of every example allows the attentive perceptron to make a decision after the evaluation of each feature about whether the feature evaluation should continue or be stopped .",
    "this decision making process allows us to stop the evaluation of features early on examples with a large partial margin after having evaluated only a few features .",
    "for example , examples with a large partial margin are unlikely to have a negative full margin .",
    "therefore , rejecting these examples early achieves large savings in computation .",
    "we define the mathematical setup to derive the stopping conditions for margin evaluation .",
    "let @xmath0 be weakly dependent random variables .",
    "let a partial sum be defined by @xmath1 and the remainder sum by @xmath2 .",
    "the expectation of a sum is denoted by @xmath3 and its standard deviation by @xmath4 .",
    "the perceptron compares the margin ( a sum ) to a threshold , and updates its model if the margin of the example is negative .",
    "we formulate the equivalent sequential decision making process , and drive constant stopping thresholds @xmath5 .",
    "these thresholds will essentially tell us when it s highly unlikely for the margin to end below the desired importance threshold @xmath6 .",
    "the stopping thresholds are derived by requiring that the joint distribution of stopping ( and predicting @xmath7 ) while the actual full sum satisfies @xmath8 is less than a required error rate @xmath9 @xmath10 we bound the probability of making a decision error @xmath11 equation [ eqn : rw_reflection ] is derived by applying the reflection principle , and equation [ eqn : flat_reflection_standerdized ] is its standardization .",
    "since we assume that @xmath0 are weakly independent , the sum @xmath12 is approximately normally distributed by the central limit theorem . by standardizing @xmath13",
    "we upper bound the probability of making a decision error with the inverse normal cumulative distribution function @xmath14 .",
    "therefore , requiring that the probability of making a decision error be less than @xmath9 we get the following equality from equation [ eqn : flat_reflection_standerdized ] @xmath15 the quantities @xmath16 and @xmath17 can be approximated using the empirical data .    finally , by solving for the stopping threshold @xmath5 we get from equation [ eqn : boundary_inequality ] @xmath18 therefore , examples with partial margin calculations @xmath19 that hit this boundary should be filtered and with probability at least @xmath20 determined that their full margin satisfies @xmath7 .",
    "in summary , we presented a simple test to speed up the perceptron algorithm by quickly filtering unimportant examples without fully evaluating their features .",
    "this results in an algorithm which typically focuses on examples by the decision boundary - the attentive perceptron ."
  ],
  "abstract_text": [
    "<S> we propose a focus of attention mechanism to speed up the perceptron algorithm . </S>",
    "<S> focus of attention speeds up the perceptron algorithm by lowering the number of features evaluated throughout training and prediction . whereas the traditional perceptron evaluates all the features of each example , the attentive perceptron evaluates less features for easy to classify examples , thereby achieving significant speedups and small losses in prediction accuracy . </S>",
    "<S> focus of attention allows the attentive perceptron to stop the evaluation of features at any interim point and filter the example . </S>",
    "<S> this creates an attentive filter which concentrates computation at examples that are hard to classify , and quickly filters examples that are easy to classify . </S>"
  ]
}