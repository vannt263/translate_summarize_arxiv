{
  "article_text": [
    "controversies over the apparent dichotomy between microscopic reversibility and macroscopic irreversibility are as old as statistical mechanics itself and continue to the present day , as exemplified by the popular ref . @xcite and the ensuing vivid debate @xcite . broadly speaking",
    ", the issue can be tackled `` bottom up '' or `` top down '' .",
    "the bottom up approach , which has been pursued by the majority of researchers , involves specifying ( or at least imposing constraints on ) some microscopic hamiltonian and subsequently studying the evolution of those degrees of freedom that are deemed `` macroscopic '' , `` accessible '' or otherwise `` relevant '' to the problem at hand .",
    "this line of research has of late enjoyed cross - fertilization with topical areas such as nanoscale thermodynamics @xcite , quantum many - body physics @xcite and quantum information @xcite , leading to some powerful new results .",
    "they confirm that the eventual thermalization of a quantum system is a universal phenomenon which holds true for virtually all hamiltonians and sensible choices for the relevant degrees of freedom @xcite .",
    "the rather generic assumptions that are needed amount to ( i ) excluding the special case of isolated systems with highly regular , completely integrable dynamics ; and ( ii ) introducing some form of coarse graining , such as limiting the resolution of realistic preparation and measurement devices @xcite or tracing out the degrees of freedom of a bath @xcite .",
    "coarse graining entails that information about the microstate is siphoned off from the retained to the discarded degrees of freedom .",
    "this leakage becomes irreversible whenever the dynamics of the latter is sufficiently fast and irregular , leading to an effective memory loss on time scales much shorter than those pertaining to the evolution of the relevant degrees of freedom @xcite .",
    "in contrast , the lesser - known top down approach , pioneered by jaynes for classical statistical mechanics @xcite and subsequently generalized @xcite , refrains from considering any specifics of the underlying microscopic dynamics and instead derives macroscopic irreversibility from the very basic requirement  essential to the scientific method  that macroscopic experiments be reproducible .",
    "the central argument is very simple : an experiment is reproducible if its initial preparation uniquely determines its final outcome ; i.e. , if merely on the basis of their initial values one can predict with certainty the final values of the relevant degrees of freedom . since a prediction can not possibly contain more information than the data on which it is based , the final values of the relevant degrees of freedom can not carry more information than do their initial values .",
    "so in the course of a reproducible experiment the amount of missing information about the system s microstate , and hence the entropy , can only increase , q.e.d .",
    "there are other top down approaches which are similar in spirit , yet which rather than from `` reproducibility '' start from different primitives like `` adiabatic accessibility '' @xcite .    reversing the top down logic",
    ", violations of the second law may well occur ; but such violations are never reproducible , and with increasing system size , become exceedingly unlikely . experiments that purport to violate the second law in a reproducible fashion must presuppose the preparation of some special ( say , highly correlated ) initial state , or else some peculiar prior history of the system ( such as in the classic example of spin echoes @xcite ) .",
    "the apparent systematic violation of the second law then stems from the fact that the experimenter actually controls degrees of freedom other than the supposedly relevant ones , either directly in the present or through specific interventions in the past .    despite their seemingly different outlooks the bottom up and top down approaches",
    "both revolve around the pivotal issue of memory loss .",
    "they either show ( bottom up ) or simply postulate ( top down ) that in realistic experiments the relevant degrees remote history has no influence on their future evolution , and thus can be safely disregarded .",
    "this intimate connection between irreversibility and memory loss is captured succinctly in landauer s principle @xcite , which has spawned another highly interesting line of research @xcite .    in the present paper",
    "i wish to add yet another , and rather practical , perspective on the issue of thermalization . when a novel quantum system is fabricated and investigated in the laboratory for the first time , its precise dynamics and possibly even its constants of the motion are not known in advance .",
    "( of course , there is generally some theoretical expectation ; but whether this will be confirmed or refuted by actual measurements is not _ a priori _ clear . ) a particular experiment might then be aimed at assessing whether or not a certain process leads to thermalization ; and if so , which set of thermodynamic variables characterizes the final equilibrium state .",
    "operationally , one might do this by assembling multiple samples , each consisting of identically prepared copies of the system .",
    "each sample is prepared in a different initial state and subjected to the process in question .",
    "if thermalization does occur , subsequent quantum - state tomography @xcite on all samples will reveal that , modulo random fluctuations , their respective final states are distributed on some low - dimensional submanifold of state space .",
    "this submanifold is composed of states of the gibbs form @xmath0 , with the observables @xmath1 being the constants of the motion .",
    "their expectation values or the associated lagrange parameters , respectively , then constitute the appropriate set of thermodynamic variables . this approach to assessing thermalization",
    "is based on output data only and does not require tight control over the initial states of the various samples .    yet in a real - world setting",
    ", the system in question might be difficult to manufacture , and the above idealized procedure difficult to execute . specifically , it might only be possible to prepare a small number of samples , which in turn are small in size . as a consequence , there will be just a few data points in state space , which moreover have non - negligible error bars . reconstructing the gibbs manifold and hence the constants of the motion on the basis of such imperfect measurement data",
    "then becomes a nontrivial statistical inference task .",
    "in purely statistical terms , this is a situation where noisy data in some high - dimensional space ( the tomographic images in state space ) are presumed to be explained by a small number of latent variables ( the expectation values of the constants of the motion ) , effectively reducing the dimensionality of the data . in such a generic setting ,",
    "the task is to infer the optimal dimension and orientation of the lower - dimensional latent space .",
    "problems of this type can be tackled with a variety of statistical techniques such as factor analysis or principal component analysis @xcite . in the present paper",
    ", i shall build on these generic techniques to develop a statistical framework tailored to the relevant task of assessing whether or not thermalization has occurred , and if so , inferring the most plausible set of constants of the motion .",
    "whenever the above statistical analysis suggests that thermalization has indeed occurred and there is one constant of the motion only , this single constant of the motion is by default the hamiltonian .",
    "the same statistical framework can then be used to estimate that hamiltonian .",
    "this estimation procedure is based on studying thermal properties rather than time evolution ; and it uses only output rather than input - output data .",
    "therefore , it is very different in its approach from the usual quantum - process tomography @xcite and hamiltonian tomography @xcite . as the second key result of the present paper",
    ", i shall lay out this `` thermal '' estimation procedure for the hamiltonian and illustrate its use in a simple example .",
    "the remainder of the paper is organized as follows . in sec .",
    "[ theory ] , i will present the general statistical framework for assessing thermalization along with the key approximations made . in sec .",
    "[ hamest ] , i will turn to the rather common case where the hamiltonian is the sole constant of the motion , and explain how in this case one can infer the most plausible hamiltonian from the data . in sec .",
    "[ qubit ] , i will put the general framework to use in the simple example of qubits , both to assess their thermalization and to estimate the pertinent hamiltonian . finally , in sec .",
    "[ conclusions ] , i will conclude with a brief discussion .",
    "let @xmath2 denote the number of distinct samples and @xmath3 , the size of the @xmath4-th sample .",
    "after the samples have undergone the process in question they are all subjected to quantum - state tomography , which may or may not be informationally complete .",
    "let @xmath5 denote the set of observables whose totals are ascertained in a tomographic experiment ( by performing measurements on each member of the sample and adding up the results , or via global measurements on the entire sample ) , and @xmath6 the associated sample means gleaned from the @xmath4-th sample .",
    "finally , let the quantum state @xmath7 denote a possible prior bias as to the samples final state @xcite ; in case of complete prior ignorance , this is simply taken to be the totally mixed state .",
    "the hypothesis to be tested is whether or not the totality of experimental data @xmath8 can be explained by the expectation values of some smaller set of observables @xmath1 , the presumed constants of the motion .",
    "associated with these presumed constants of the motion and with the measured observables are subspaces @xmath9 and @xmath10 of the space of observables ( with @xmath11 being the unit operator ) , termed respectively the `` theoretical '' and `` experimental '' level of description @xcite . for the former to have any explanatory value , it must be @xmath12 .",
    "the plausibility of the theoretical hypothesis is encoded in the posterior probability of the level of description @xmath13 , given the data @xmath14 and prior bias @xmath7 . by bayes rule @xcite",
    ", this probability of interest is given by @xmath15 whenever the prior @xmath16 is sufficiently non - committal , the right hand side will be dominated by the likelihood function .",
    "as the various runs of the experiment are independent , the latter can be factorized : @xmath17 with the data @xmath18 pertaining to the @xmath4-th sample . and finally , according to the theoretical hypothesis , each individual factor can be marginalized : @xmath19 where the integration ranges not over the complete state space @xmath20 but over the gibbs manifold @xmath21 associated with the theoretical level of description @xmath13 and reference state @xmath7 @xcite . this gibbs manifold is composed of states of the generalized gibbs form @xmath22      , \\label{gibbs}\\ ] ] which minimize the relative entropy with respect to @xmath7 under given constraints for the expectation values of @xmath1 @xcite .    for reasonably large sample sizes and a near optimal measurement setup the first factor ( likelihood ) in the integrand of eq .",
    "( [ marginalize ] ) can be approximated with the help of the quantum stein lemma @xcite , @xmath23      .",
    "\\label{entropic_likelihood}\\ ] ] ( else the quantum stein lemma provides only a lower bound . ) here @xmath24 has the generalized gibbs form ( [ gibbs ] ) with @xmath1 replaced by @xmath5 and reference state @xmath25 rather than @xmath7 , and with the lagrange parameters @xmath26 adjusted such that @xmath27 for all @xmath28 . for both conceptual and practical reasons i shall model the second factor ( prior ) in the integrand as an entropic distribution , too , @xmath29       , \\label{entropic_ansatz}\\ ] ] with a factor of proportionality that does not depend on @xmath25 @xcite",
    "this ansatz contains an unknown hyperparameter @xmath30 , whose most likely value will be estimated later via the evidence procedure .",
    "i assume that the theoretical level of description is a proper subspace of the experimental level , @xmath31 , so that @xmath32 .",
    "the gibbs manifold @xmath21 , which contains the theoretical models @xmath25 and the reference state @xmath7 , is then a proper submanifold of @xmath33 which contains the tomographic images @xmath34 . each tomographic image",
    "@xmath35 has a unique projection @xmath36 on the submanifold @xmath21 , where @xmath37 is the coarse graining operation that maps arbitrary states to gibbs states on @xmath21 , thereby preserving the expectation values of the relevant observables @xmath1 . also on the submanifold @xmath21 , between the projection @xmath38 and the reference state @xmath7 , lies the interpolated state @xcite @xmath39 \\label{interpolation}\\ ] ] with @xmath40 ; its lagrange parameters are the weighted average of those of @xmath38 and @xmath7 , with respective weights @xmath41 and @xmath42 .",
    "finally , for both the tomographic images @xmath34 and their projections @xmath43 one defines respective center - of - mass states @xmath44      \\ , \\       \\tilde{\\pi}:\\propto",
    "\\exp\\left[\\sum_{i=1}^r w_i \\ln\\pi_i\\right]\\ ] ] with @xmath45 , which lie on @xmath33 and @xmath21 , respectively , and which are obtained by taking the weighted average over all samples of the respective lagrange parameters",
    ".    for nearby states on the manifold @xmath33 the relative entropy is approximately quadratic in their coordinate differentials , @xmath46 here @xmath47 denotes the correlation matrix @xmath48 with @xmath49 and canonical correlation function @xmath50 the correlation matrix varies little between @xmath51 and @xmath52 , and so to lowest order , can be evaluated in either of the two states or in any other state @xmath53 in their vicinity . in the following",
    "i shall assume that the tomographic images @xmath34 , their projections @xmath43 , as well as their respective centers of mass @xmath54 and @xmath55 all lie inside a region in which the above quadratic ( `` gaussian '' ) approximation is warranted , with the correlation matrix evaluated in the center of mass @xmath54 , @xmath56 .",
    "this presupposes that for all samples the presumed constants of the motion take values within a sufficiently narrow range .",
    "moreover , i shall assume that the sample sizes @xmath57 are sufficiently large compared to @xmath42 so that the interpolated states @xmath58 , too , lie inside this region . and",
    "finally , i assume that the sample sizes are also large enough in absolute terms to render the likelihood function ( [ entropic_likelihood ] ) largely concentrated inside the gaussian region .",
    "the reference state @xmath7 , on the other hand , need not necessarily be inside the gaussian region ( fig .",
    "[ gauss ] ) .    .",
    "black dots indicate the tomographic images @xmath34 associated with data garnered from different samples , and the small black circle their center of mass @xmath54 .",
    "the straight lines are the reduced gibbs manifolds @xmath21 ( solid line ) and @xmath59 ( dashed line ) , respectively .",
    "gray dots or circles denote states on either of these reduced gibbs manifolds . in particular ,",
    "the gray dots are obtained by applying the coarse graining @xmath37 or @xmath60 , respectively , to the tomographic images .",
    "( for simplicity , not all coarse grainings are shown . )",
    "the state @xmath61 is the interpolation ( [ interpolation ] ) between the coarse grained image @xmath38 and the reference state @xmath7 .",
    "all states inside the big circle are assumed to be sufficiently close to each other to warrant the gaussian approximation for their relative entropies ; the only state that might lie outside this gaussian region is the reference state @xmath7 .",
    "the gray concentric circles around one of the tomographic images indicate an exemplary likelihood function ( [ entropic_likelihood ] ) .",
    "it has a width of order @xmath62 , which is assumed to lie inside the gaussian region .",
    ", width=321 ]    the confinement of all pertinent states ( with the exception of the reference state ) to a gaussian region entails a number of simplifications : relative entropies become approximately symmetric , @xmath63 ; for the interpolated states @xmath58 , it is @xmath64 up to corrections of order @xmath65 that account for the possible non - gaussianity of @xmath66 ; the centers of mass @xmath54 and @xmath55 coincide approximately with the ordinary mixtures @xmath67 respectively ; and the coarse graining map is approximately linear , so @xmath68 .    using these approximations , as well as",
    "the ( exact ) law of pythagoras @xcite @xmath69 for all @xmath70 and the ( exact ) mixing rules @xmath71 and @xmath72 one obtains the log - likelihood @xmath73      - \\frac{p\\lambda}{2 }       \\nonumber \\\\      & &      - \\sum_{i=1}^r x_i n_i [ s(\\pi_i\\|\\bar{\\pi } ) + s(\\bar{\\pi}\\|\\sigma ) ]      + \\frac{p}{2 } \\sum_{i=1}^r \\ln ( x_i n_i ) \\label{loglike}\\end{aligned}\\ ] ] with @xmath74 and @xmath75 , modulo a small correction term that accounts for the possible non - gaussianity of @xmath76 and varies only weakly with @xmath42 , and modulo additive constants that do not depend on @xmath42 , @xmath7 or @xmath13 . since @xmath77 , the terms in the last row of eq .",
    "( [ loglike ] ) do not scale with sample size ( at fixed @xmath42 ) and so become negligible in the regime @xmath78 .",
    "the log - likelihood then approaches ( again modulo additive constants that do not depend on @xmath7 or @xmath13 ) the asymptotic result @xmath79      - \\frac{p\\lambda}{2 }       .",
    "\\label{asymptotic}\\ ] ] this asymptotic log - likelihood is the central quantity which i will use for my subsequent analysis .    strictly speaking , one has yet to check that it is consistent to assume that @xmath42 stays constant when taking the limit @xmath80 ; i.e. , that the most likely value of @xmath42 does not itself scale with sample size .",
    "in order to determine this most likely value , i follow the prescription of the evidence procedure @xcite .",
    "i consider the log - likelihood ( [ loglike ] ) and seek its maximum as a function of @xmath42 . setting its derivative with respect to @xmath42 equal to zero yields the extremum condition @xmath81 - \\frac{p}{2n_i }      \\right\\ }      =      0      .\\ ] ] ( this maximum likelihood condition generalizes an earlier result for experiments on a single sample @xcite . ) in the asymptotic limit @xmath80 ( at fixed relative entropies ) , the maximum likelihood estimates for the @xmath82 must scale as the inverse sample size ; and so indeed , @xmath83 must _ not _ scale with sample size .",
    "this conclusion about @xmath42 is robust as long as @xmath84 in the relevant regime @xmath78 the left hand side of this condition is approximately @xmath85 , so one has good accuracy whenever the number of samples is sufficiently large , @xmath86 .",
    "the asymptotic log - likelihood ( [ asymptotic ] ) is the difference of two sums , reflecting a trade - off that is typical for model selection @xcite .",
    "the first sum gets bigger as the theoretical level of description becomes more detailed and yields a better fit with the data ; in fact , it is maximal for the largest possible level of description , @xmath87 .",
    "the sum which is subtracted from this , on the other hand , being proportional to the gibbs manifold dimension , penalizes excessive detail ; it embodies `` occam s razor '' .",
    "therefore , finding the most plausible level of description and hence the constants of the motion always involves a trade - off between goodness of fit and simplicity .    in case",
    "the reference state @xmath7 is not given _ a priori _ but is itself a variable to be inferred , one must consider the asymptotic log - likelihood ( [ asymptotic ] ) also as a function of @xmath7 .",
    "the log - likelihood attains its maximum for any @xmath88 ; then the relative entropy @xmath89 vanishes .",
    "using such a maximum likelihood estimate for @xmath7 , and assuming further that the dimension @xmath90 of the gibbs manifold is fixed from the outset , the remaining optimization of ( the orientation of ) @xmath13 reduces to maximizing the weighted average of the relative entropies @xmath91 . in the gaussian regime , this is tantamount to the optimization task known in statistics as `` principal component analysis '' @xcite .",
    "now i turn to the general case in which there is an arbitrary given reference state , and where both the dimension and the orientation of the explanatory level of description are to be inferred .",
    "suppose there are two rival proposals for the level of description , @xmath13 and @xmath92 , where the latter is more detailed than the former ( and both are contained in the experimental level of description ) , @xmath93 .",
    "the associated gibbs manifolds @xmath21 and @xmath94 have respective manifold dimensions @xmath90 and @xmath95 .",
    "as discussed earlier , the choice between the two proposals will involve a trade - off between goodness of fit ( favoring @xmath92 ) and simplicity ( favoring @xmath13 ) .",
    "using the fact that within the gaussian region the relative entropy of two coarse grained states is approximately invariant under a change of reference state @xmath96 , @xmath97 ( and likewise for @xmath92 ) , the difference of the asymptotic log - likelihoods can be written as @xmath98      -      \\frac{s\\lambda}{2 }       .",
    "\\nonumber \\\\ \\label{criterion}\\end{aligned}\\ ] ] if this difference is positive , the more detailed level of description @xmath92 is called for ; if it is negative , one better stick to the simpler model @xmath13 .",
    "this criterion extends an earlier result obtained in ref .",
    "@xcite for experiments on a single sample .    finding the optimal level of description , and hence the most plausible set of constants of the motion , can now proceed in two ways : either directly , by maximizing the asymptotic log - likelihood ( [ asymptotic ] ) as a function of @xmath13 ; or indirectly ( and usually more feasible in practice ) , by formulating various hypotheses about the level of description and then comparing them by means of the difference criterion ( [ criterion ] ) .",
    "if the optimal @xmath13 is spanned by only one or very few observables ( aside from the unit operator ) , this indicates that thermalization has indeed occurred .",
    "the reconstruction of the appropriate level of description precedes the reconstruction of the quantum state of any individual system .",
    "the former requires data from the totality of all samples .",
    "once the reconstruction of the level of description has succeeded , one may take this level as a given and turn to reconstructing the gibbs state of an individual system , based on data from the pertinent sample only , by means of well - known state estimation techniques @xcite .",
    "whenever the above statistical analysis reveals or it is posited from the outset that there is only one constant of the motion , this is by default the hamiltonian .",
    "the gibbs manifold is then made up of canonical states @xmath99 , with hamiltonian @xmath100 and inverse temperature @xmath101 .",
    "( for a non - uniform reference state there is an additional term @xmath102 in the exponent . ) strictly speaking , in case the @xmath5 are not informationally complete , @xmath100 is not the full hamiltonian but the _ effective _ hamiltonian pertaining to the measured degrees of freedom . since the latter usually coincide with the slow degrees of freedom , @xmath100 is then an effective low - energy hamiltonian .",
    "if the hamiltonian is not known in advance , it must be estimated from the data . in this section",
    ", i shall lay out the appropriate estimation procedure .",
    "let the hamiltonian be parametrized by some set of parameters @xmath103 .",
    "then so are the coarse grained states @xmath104      , \\ ] ] with arbitrary reference state @xmath7 , where the partition function @xmath105      \\}\\ ] ] ensures state normalization , and the inverse temperature @xmath106 is adjusted such that @xmath107 .",
    "their weighted average @xmath108 , equally parametrized by @xmath109 , has ( in the gaussian approximation ) the same canonical form , with inverse temperature @xmath110 and internal energy @xmath111 .",
    "the asymptotic log - likelihood ( [ asymptotic ] ) thus becomes a function of @xmath109 .",
    "it attains its maximum when @xmath112    to evaluate the left hand side of this extremization condition , i use eq .",
    "( [ shiftreference ] ) and the gaussian approximation to write @xmath113 where @xmath114 with @xmath115 , and @xmath116 the latter two functions have the respective derivatives @xmath117 with @xmath118 and @xmath119 with covariance @xmath120 the right hand side of the extremization condition is given by @xmath121 altogether , this yields the condition @xmath122    one particularly simple ansatz for the hamiltonian is the linear form @xmath123 modulo some additive constant .",
    "for the implementation of this ansatz it will be convenient to adopt a number of index conventions in the style of general relativity : identical upper and lower indices are to be summed over ; the correlation matrix @xmath47 ( eq .",
    "( [ correlation ] ) ) and its inverse @xmath124 lower or raise indices , respectively , akin to a metric tensor @xcite ; and the scalar product is defined as @xmath125 .",
    "furthermore , i define the covariance matrix @xmath126 with @xmath127 , its `` expectation value '' @xmath128 as well as @xmath129 and @xmath130 . with these conventions and definitions the asymptotic log - likelihood ( [ asymptotic ] ) for the level of description @xmath131 reads @xmath132      -      ( \\lambda/2 )      .",
    "\\label{xi_likelihood}\\ ] ]    if one is still uncertain as to whether the process in question has actually led to thermalization , yet can already exclude the existence of other constants of the motion besides the hamiltonian , one must compare the log - likelihood of @xmath133 for all values of @xmath109 with the log - likelihood of @xmath134 , i.e. , the hypothesis that the data do not warrant any dimensional reduction at all .",
    "the latter log - likelihood is given by @xmath135 where @xmath136 .",
    "the process may be considered `` thermalizing '' with hamiltonian @xmath137 iff @xmath138 , and hence @xmath139      +      \\delta f(\\xi)\\cdot \\delta f(\\xi )      \\ll      ( \\lambda / n )      [ \\dim \\pi^\\sigma_{\\cal f}({\\cal s } ) - 1 ]      .",
    "\\label{thermal_condition}\\ ] ]    the most likely value of @xmath109 is determined by the maximum likelihood condition ( [ mle_condition ] ) , which for the linear ansatz ( [ linear_ansatz ] ) simplifies to @xmath140 with matrix @xmath141 . in order to estimate @xmath142 , i consider @xmath143 where as before @xmath144 . in the typical case of a uniform reference state",
    "@xmath7 the latter two terms cancel so that @xmath145 the right hand side in turn may be approximated to lowest order by @xmath146",
    "in the following , i shall illustrate the general framework in the simple example of qubits , which is tractable analytically . in this example",
    "the @xmath5 are the pauli operators , and the parameter vector @xmath147 may be viewed as ( parallel to ) an effective magnetic field . in the typical case of a uniform reference state @xmath7 the expectation values of @xmath148 in the states @xmath149 and @xmath108 are related linearly : @xmath150 as a result , the maximum likelihood condition ( [ mle_condition2 ] ) becomes @xmath151 this condition no longer depends on @xmath142 , and moreover , is invariant under rescaling of @xmath109 . without loss of generality ,",
    "therefore , @xmath109 may be taken to be normalized , @xmath152 .    for qubits",
    "the covariance matrix @xmath153 is a @xmath154 matrix . to simplify matters",
    ", i assume that it singles out one dominant direction @xmath155 , and is isotropic in the remaining two directions : @xmath156 where the projector @xmath157 projects orthogonally ( with respect to the scalar product used here ) onto the subspace complementary to @xmath155 , and @xmath158 with @xmath159 are the respective eigenvalues .",
    "the unit vectors @xmath155 and @xmath160 ( the unit vector pointing in the direction of @xmath161 ) then constitute the two preferred directions in the problem .",
    "symmetry dictates that the solution of the maximum likelihood condition ( [ mle_qubits ] ) must lie in the subspace spanned by these two preferred directions , @xmath162 .",
    "in fact , if @xmath155 is aligned with @xmath160 , the solution is @xmath163 . in case @xmath155 and @xmath160",
    "are not aligned , the solution will generally not coincide with either of the two .    in order to quantify how @xmath109 interpolates between @xmath155 and @xmath160 in case the two are not aligned ,",
    "i define a further unit vector @xmath164 , the normalized projection of @xmath160 onto the subspace complementary to @xmath155 .",
    "to lowest ( first ) order perturbation theory in @xmath165 , i.e. , for small misalignments , the maximum likelihood condition ( [ mle_qubits ] ) has the solution @xmath166^{-1 }      \\eta\\cdot\\hat{f }      \\ , \\",
    "\\gamma\\cdot\\xi \\approx 1 - o((\\eta\\cdot\\hat{f})^2 )      .",
    "\\label{interpol}\\ ] ] this result illustrates nicely how the maximum likelihood algorithm interpolates between alignment with the center of mass ( @xmath167 ) and alignment with the covariance pattern ( @xmath168 ) . for a perfectly isotropic covariance pattern ( @xmath169 ) the parameter vector is aligned with @xmath160 .",
    "the more pronounced the anisotropy of the covariance pattern ( @xmath170 ) and the smaller the lever of the center of mass ( @xmath171 small ) , the more @xmath109 tends to be aligned with @xmath155 .    inserting the maximum likelihood solution into the formula ( [ xi_likelihood ] ) for the log - likelihood yields , to lowest order in perturbation theory , @xmath172^{-1 }      ( \\eta\\cdot\\hat{f})^2      \\right\\ }      -\\frac{\\lambda}{2 }       .",
    "\\label{xi_mle}\\end{aligned}\\ ] ] the maximum likelihood solution satisfies the thermalization condition ( [ thermal_condition ] ) if and only if @xmath173      , \\label{thermal}\\ ] ] where @xmath174 is the tilting angle between @xmath155 and @xmath160 , @xmath175 .    as a simple numerical example , i consider data gleaned from multiple qubit samples of identical size @xmath176 , and hence @xmath177 .",
    "i assume that the distribution of tomographic images has a width which in the dominant direction is of comparable magnitude as the distance of the center of mass from the origin ; more specifically , that both are about @xmath178 of the radius of the bloch sphere , @xmath179 . in the other directions , the standard deviation of the tomographic images",
    "is assumed to be smaller by a factor @xmath180 , @xmath181 . the dominant direction @xmath155 and the orientation @xmath160 of the center of mass are not aligned ;",
    "rather , they are tilted against each other by an angle @xmath182 .",
    "this raises doubts about thermalization , as the canonical curves of a qubit are straight lines through the origin of the bloch sphere .",
    "may the qubits nevertheless be considered thermalized ?",
    "in fact they may , as the standard deviation and the tilting angle still satisfy both thermalization conditions in eq .",
    "( [ thermal ] ) .",
    "their most plausible hamiltonian , parametrized as in the ansatz ( [ linear_ansatz ] ) , contains an effective magnetic field @xmath109 which ( modulo rescaling ) is given by eq .",
    "( [ interpol ] ) , and which in this example approximately bisects the angle between @xmath155 and @xmath160 .    in the above example the preferred axis @xmath109 of the hamiltonian",
    "is inferred from the data , rather than given or conjectured from the outset .",
    "this distinguishes this example from other inference tasks where one weighs the hypothesis of some _ a priori _ fixed axis against the hypothesis that no such preferred axis exists , for instance when comparing ising and heisenberg models for an anisotropic ferromagnet on the basis of a single sample @xcite .",
    "in this paper i focused not on the theoretical question whether or not some system with a given hamiltonian _ ought to _ thermalize , but on the practical question whether or not experimental data indicate that a system with hitherto unknown dynamics _ has _ actually thermalized .",
    "this issue never really arises for systems that are macroscopic . outside the macroscopic realm , however , and with data pertaining to small samples composed of , say , a few hundred system copies only , it becomes a nontrivial statistical inference task .",
    "i have laid out the appropriate statistical framework for assessing thermalization under such adverse conditions .    in case",
    "the data do support the hypothesis of thermalization , and provided there is no evidence for additional constants of the motion , i have shown how the data can be used to estimate the system s unknown hamiltonian .",
    "hamiltonian estimation is increasingly important in quantum technology , as it is needed to assess and certify the proper functioning of quantum devices .",
    "since my estimation scheme is based on studying thermal properties rather than time evolution and thus requires output data only , it may constitute a viable alternative to conventional time - based approaches especially in situations where initial states or time are difficult to control .    aside from its practical relevance , the framework presented here is also of interest conceptually .",
    "one example is a better understanding of the iterative dynamics of thermalization .",
    "whenever a physical system exhibits a hierarchy of time scales , thermalization typically occurs in stages , on successively longer time scales .",
    "for instance , a dense plasma , initially in the kinetic regime far from equilibrium , might quickly equilibrate locally and thus enter the hydrodynamic regime , but only much later reach global equilibrium @xcite .",
    "associated with these various stages are successively smaller levels of description ; in this particular example , first the boltzmann level of description ( all single - particle observables ) , then the hydrodynamic level of description ( local particle , energy and momentum density ) , and finally the equilibrium level of description ( total energy and particle number ) .",
    "thermalization is thus accompanied by a sequence of level contractions .",
    "the framework developed here provides the quantitative criterion as to when exactly these level contractions are warranted .",
    "i see four routes for further research .",
    "first , it will be important to test the mathematical framework developed here on real or simulated experimental data . in principle , any experiment that probes only tiny samples of matter such as an array of atoms or the debris from a single high energy collision @xcite will lend itself to such an analysis . processing the data will likely require the use of suitable numerical techniques .",
    "second , in the present paper i made a number of idealizing assumptions .",
    "for instance , i assumed that the only source of experimental error is projection noise due to the finiteness of the samples , whereas there is no error stemming from inaccuracies of the measurement devices .",
    "moreover , i took the tomographic measurement setup to be near optimal in the sense of the quantum stein lemma . in case",
    "the observables @xmath5 do not commute , this may involve global measurements which are difficult to implement in practice . in future work",
    "i plan to investigate how the mathematical framework must be adapted when these assumptions are relaxed .",
    "third , on a more conceptual level , i consider it worthwhile to generalize the mathematical framework in the following way .",
    "while the approach laid out in the present paper aims to infer the most plausible level of description in a single step , a different approach might split this into two distinct inference tasks : first estimating the optimal _ dimension _ of the level of description ; and then , given the dimension , its optimal _",
    "orientation_. in this alternative approach the first step involves an additional occam factor , and so in principle , might lead to other conclusions than the present approach",
    ". it will be interesting to understand under which circumstances such divergent conclusions may arise , and why .    finally , also on the conceptual level , the pivotal log - likelihood function @xmath183 which features in the statistical analysis depends on a number of scaling parameters : the total number @xmath2 of samples , their sizes @xmath57 , the gibbs manifold dimension @xmath90 , and  when calibrating against @xmath184  the number of different measurement setups .",
    "i propose to investigate in more detail how the log - likelihood scales with each of these parameters , and whether any general conclusions can be drawn from this about the typicality of thermalization in different scaling regimes .      70ifxundefined [ 1 ] ifx#1 ifnum [ 1 ] # 1firstoftwo secondoftwo ifx [ 1 ] # 1firstoftwo secondoftwo `` `` # 1''''@noop [ 0]secondoftwosanitize@url [ 0 ] ",
    "+ 12$12  & 12#1212_12%12@startlink[1]@endlink[0]@bib@innerbibempty link:\\doibase 10.1063/1.881363 [ * * , ( ) ] link:\\doibase 10.1063/1.2808690 [ * * ,   ( ) ] link:\\doibase 10.1080/09500340408231829 [ * * ,   ( ) ] link:\\doibase 10.1209/epl / i2004 - 10101 - 2 [ * * ,   ( ) ] link:\\doibase 10.1007/s10955 - 005 - 8015 - 9 [ * * ,   ( ) ] link:\\doibase 10.1103/physrevlett.104.090602 [ * * , ( ) ] link:\\doibase 10.1103/physrevlett.98.050405 [ *",
    "* ,   ( ) ] link:\\doibase 10.1103/physrevlett.103.100403 [ * * ,   ( ) ] link:\\doibase 10.1103/physrevlett.105.260402 [ * * , ( ) ] link:\\doibase 10.1103/physrevlett.106.040401 [ * * ,   ( ) ] link:\\doibase 10.1103/revmodphys.83.863 [ * * ,   ( ) ] @noop `` , ''   ( ) , link:\\doibase 10.1103/physrevlett.96.050403 [ * * ,   ( ) ] link:\\doibase 10.1038/nphys444 [ * * ,   ( ) ] link:\\doibase 10.1038/nphys1100 [ * * ,   ( ) ] link:\\doibase 10.1007/s00220 - 010 - 1003 - 1 [ * * ,   ( ) ] link:\\doibase 10.1103/physrevlett.101.190403 [ * * ,   ( ) ] link:\\doibase 10.1103/physreve.79.061103 [ * * ,   ( ) ] link:\\doibase 10.1088/1367 - 2630/13/5/053009 [ * * ,   ( ) ] @noop `` , ''   ( ) ,   link:\\doibase 10.1016/0370 - 1573(95)00077 - 1 [ * * ,   ( ) ] link:\\doibase 10.1119/1.1971557 [ * * , ( ) ] link:\\doibase 10.1016/j.physleta.2010.07.024 [ * * ,   ( ) ] link:\\doibase 10.1016/s0370 - 1573(98)00082 - 9 [ * * ,   ( ) ] link:\\doibase 10.1103/physrev.80.580 [ * * , ( ) ] link:\\doibase 10.1147/rd.53.0183 [ * * , ( ) ] link:\\doibase 10.1007/bf02084158 [ * * ,   ( ) ] link:\\doibase 10.1103/physrevlett.100.080403 [ * * , ( ) ] link:\\doibase 10.1103/physrevlett.102.250602 [ * * , ( ) ] ,   link:\\doibase    10.1103/physreve.83.030102 [ * * ,   ( ) ] link:\\doibase 10.1038/nature10123 [ * * ,   ( ) ]  and ,  eds . , link:\\doibase 10.1007/b98673 [ _ _ ] ,  , vol .",
    "( ,  ) in  @noop _ _ ,  ( ,  )  pp .",
    "link:\\doibase 10.1162/089976699300016674 [ * * ,   ( ) ] in  @noop _ _ ,  ( ,  )  pp .   in  \\doibase 10.1049/cp:19991160 [ _ _ ]  ( )",
    "link:\\doibase 10.1111/1467 - 9868.00196 [ * * ,   ( ) ] link:\\doibase 10.1162/089976699300016728 [ * * ,   ( ) ] link:\\doibase 10.1126/science.290.5500.2323 [ * * ,   ( ) ] `` , '' in  link:\\doibase 10.4018/978 - 1 - 60566 - 766 - 9 [ _ _ ]  ( , )  pp .",
    "link:\\doibase 10.2277/0521635039 [ _ _ ]  ( ,  ) link:\\doibase 10.1103/physrevlett.78.390 [ * * ,   ( ) ] link:\\doibase 10.1080/09500349708231894 [ * * ,   ( ) ] link:\\doibase 10.1103/physreva.63.054104 [ * * ,   ( ) ] link:\\doibase 10.1103/physrevlett.86.4195 [ * * ,   ( ) ] link:\\doibase 10.1063/1.1518554 [ * * , ( ) ] link:\\doibase    10.1103/physrevlett.90.193601 [ * * ,   ( ) ] link:\\doibase 10.1103/physreva.72.022106 [ * * ,   ( ) ] link:\\doibase 10.1103/physreva.78.032118 [ * * , ( ) ]",
    "link:\\doibase 10.1103/physreva.77.032322 [ * * , ( ) ] link:\\doibase 10.1103/physreva.77.042320 [ * * ,   ( ) ] link:\\doibase 10.1103/physreva.80.022333 [ * * , ( ) ] link:\\doibase 10.1134/s1054660x10090434 [ * * , ( ) ] in  link:\\doibase 10.1109/isccsp.2010.5463437 [ _ _ ]  ( )  pp .",
    "link:\\doibase    10.1103/physreva.84.012107 [ * * ,   ( ) ] link:\\doibase 10.1103/physreva.82.012104 [ * * , ( ) ] link:\\doibase 10.1103/physreva.84.012101 [ * * , ( ) ] link:\\doibase 10.1103/physreva.64.014305 [ * * ,   ( ) ] link:\\doibase 10.1016/0034 - 4877(88)90009 - 2 [ * * ,   ( ) ] link:\\doibase 10.1103/physreva.76.042120 [ * * , ( ) ] link:\\doibase 10.1007/bf02100287 [ * * , ( ) ] link:\\doibase 10.1109/18.887855 [ * * ,   ( ) ] link:\\doibase 10.1103/revmodphys.74.197 [ * * , ( ) ] link:\\doibase 10.1007/s00220 - 010 - 1005-z [ * * ,   ( ) ] link:\\doibase 10.1162/neco.1992.4.3.415 [ * * ,   ( ) ] link:\\doibase 10.1007/978 - 3 - 540 - 74636 - 2 [ _ _ ]  ( ,  ) link:\\doibase 10.1016/0167 - 2789(93)90241-r",
    "[ * * ,   ( ) ] link:\\doibase 10.1016/0370 - 1573(86)90005 - 0 [ * * ,   ( ) ] link:\\doibase 10.1016/0003 - 4916(87)90085 - 6 [ * * ,   ( ) ] link:\\doibase 10.1140/epjc / s10052 - 008 - 0671-x [ * * ,   ( ) ]"
  ],
  "abstract_text": [
    "<S> i consider the generic situation where a finite number of identical test systems in varying ( possibly unknown ) initial states are subjected independently to the same unknown process . </S>",
    "<S> i show how one can infer from the output data alone whether or not the process in question induces thermalization , and if so , which constants of the motion characterize the final equilibrium states . in case thermalization </S>",
    "<S> does occur and there is no evidence for constants of the motion other than energy , i further show how the same output data can be used to estimate the test systems effective hamiltonian . for both inference tasks </S>",
    "<S> i devise a statistical framework inspired by the generic techniques of factor and principal component analysis . </S>",
    "<S> i illustrate its use in the simple example of qubits . </S>"
  ]
}