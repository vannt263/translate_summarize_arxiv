{
  "article_text": [
    "source coding ( dsc ) refers to the compression of the outputs of two or more physically separated sources that do not communicate with each other ( hence the term distributed coding ) .",
    "these sources send their compressed outputs to a central point ( e.g. , the base station ) for joint decoding . driven by a host of emerging applications ( e.g. , sensor networks and wireless video )",
    ", dsc has recently become a very active research area - more than 30 years after slepian and wolf laid its theoretical foundation @xcite .",
    "wyner first realized the close connection of dsc to channel coding and suggested the use of linear channel codes as a constructive approach for slepian - wolf coding in his 1974 paper @xcite .",
    "wyner s scheme was only recently used in @xcite for practical slepian - wolf code designs based on conventional channel codes like block and trellis codes .",
    "if the correlation between the two sources can be modelled by a binary channel , wyner s syndrome concept can be extended to all binary linear codes ; and state - of - the - art near - capacity channel codes such as turbo @xcite and ldpc codes @xcite @xcite can be employed to approach the slepian - wolf limit .",
    "slepian - wolf rate region is the rate region in which there exist encoding and decoding schemes such that reliable transmission can take place , which was given in @xcite as @xmath0 , @xmath1 , and @xmath2 , where @xmath3 and @xmath4 are the source encoding rates of the two sources x and y , and @xmath5 and @xmath6 the conditional entropies of the sources , and @xmath7 the joint entropy of the two sources .",
    "the most important points in the slepian - wolf rate region are the asymmetric point ( in which one source , say x , is encoded at the rate that equals its unconditional entropy ( h(x ) ) and the other source , say y , is encoded at the rate equal to its conditional entropy ( @xmath6 ) ) , and the symmetric point ( in which both sources are encoded at the same rate @xmath8 ) .",
    "constructive approaches ( e.g. , @xcite ) have been proposed for the symmetric point of the boundary of the slepian - wolf rate region , for the asymmetric point of the boundary of the slepian - wolf rate region ( example @xcite@xcite@xcite @xcite ) and also for any point between on the boundary ( for example @xcite@xcite ) .",
    "most of the cases in the literature deal with correlation model being a binary symmetric channel , though some work on arbitrary correlation has also been done ( for example @xcite ) .",
    "after the practical scheme of @xcite , there was a lot of work in the direction of the practical code construction due to its application in sensor networks @xcite .",
    "some of the works relating to this field are @xcite@xcite@xcite@xcite@xcite@xcite@xcite@xcite . in @xcite ,",
    "practical construction using syndrome approach was given for arbitrarily correlated sources .",
    "there is also another scheme not using the syndrome approach , but rather encoding in the same way as the channel coding , by multiplying at the encoder by the generator matrix as is given in @xcite ( the idea of using such approach is also there in @xcite ) .",
    "they consider in the paper source correlation model as bsc with error probability p. to encode , a non - uniform systematic ldpc code with generator matrix of size n @xmath9 n(1+h(p ) ) is used ( where h(p ) is the binary entropy of p ) , and at the encoder of the first source , say @xmath10 , 1/a fraction of the information bits , and ( 1 - 1/a ) fraction of the parity bits are sent . at the encoder of the other source ,",
    "say @xmath11 , the remaining ( 1 - 1/a ) fraction of information bits and the remaining 1/a fraction of the parity bits are sent .",
    "decoding is performed as follows : at decoder of @xmath10 , we have 1/a fraction of information bits of x with infinite likelihood , ( 1 - 1/a ) fraction of parity bits with infinite likelihood , ( 1 - 1/a ) fraction of information bits with ln((1-p)/p ) likelihood , from which we decode @xmath10 , and similarly we can decode @xmath11 .",
    "note that the remaining parity bits from @xmath11 are not used in the decoding of @xmath10 , they are considered as being punctured .",
    "they also suggest to use unequal error protection for ldpc since the information bits are more important .",
    "more important bits have higher degrees while the rest have lower degree .",
    "hence , they design systematic ldpc code with distinct variable node degree distributions for information and parity bits .",
    "our scheme will extend upon this basic idea of doing source coding in the same way as traditional channel coding scheme .",
    "there has also been some work in the direction of joint source channel decoding of correlated sources over noisy channels ( for example @xcite , @xcite , @xcite , @xcite , and @xcite ) but none of the work till now covers the problem in that generality using ldpc codes .",
    "it has been proven in @xcite that ldpc codes can approach the slepian wolf bound for general binary sources , and hence the choice of ldpc codes @xcite@xcite in this work .",
    "most of the cases in the literature deal with correlation model being a binary symmetric channel , though some work on arbitrary correlation has also been done ( for example @xcite ) and also for correlation having markov structure ( for example @xcite @xcite ) . to the best of our knowledge ,",
    "no work for arbitrary point in the slepian - wolf rate region is there in the literature although there are attempts for asymmetric point in the rate region ( for example @xcite @xcite ) .",
    "the paper is organized as follows : we will start with giving the scheme of joint source channel coding for the source correlation model being being arbitrary memoryless with arbitrary link capacities at arbitrary point in the slepian - wolf rate region in section [ bscmodel ] .",
    "then , we give some simulation results for this scheme in section [ simuarb ] . following this",
    ", we give a scheme for doing distributed source coding at arbitrary rates in the slepian - wolf rate achievable region when the source correlation has memory in section [ marcor ] and some simulations for the scheme are given in section [ simmar ] .",
    "finally , we give some concluding remarks in section [ conc ] .",
    "consider first that we encode @xmath10 as follows : @xmath10 is fed into systematic ldpc code of rate @xmath3 .",
    "the encoder sends the corresponding parity bits and a 1/a fraction of the information bits .",
    "these parity bits involve the bits needed for source as well as channel .",
    "let @xmath12 be the parity bits needed for source and @xmath13 the parity bits needed for the channel for source @xmath10 .",
    "the compression rate of source @xmath10 is @xmath14 , and the corresponding ldpc code has rate @xmath15 where k is the number of bits of @xmath10 and of @xmath11 at the input .",
    "the same procedure with a few modifications is applied at the source @xmath11 . for this source ,",
    "we use a systematic ldpc code of rate @xmath4 .",
    "the encoder sends the related parity bits and the remaining 1 - 1/a fraction of the information bits .",
    "similarly , assume that the parity bits are @xmath16 for the source coding and @xmath17 for the channel coding at the source @xmath11 , we have the compression rate and the ldpc code rate equal to @xmath18 and @xmath19 respectively .",
    "the procedure as we discussed above involves two ldpc codes .",
    "however , we are interested in designing a single channel code for both sources .",
    "so , we will make a single ldpc matrix having the information bits , and all parity bits corresponding to @xmath12 , @xmath16 , @xmath13 , and @xmath17 .",
    "@xmath12 + @xmath16 = k(@xmath20 + @xmath21 -1 )    add @xmath20 and @xmath21 given above to get the required    @xmath22 $ ]    @xmath23 add @xmath13 and @xmath17 to get the required result .",
    "let the rate of the ldpc code needed is @xmath24 . with this parity check matrix",
    ", we will generate required parity bits for each source . now , as we can see , the modified algorithm is to encode both the sources with a single ldpc code , and at encoder of @xmath10 , send the first fraction 1/a of the information bits , and the first 1/b fraction of the parity bits , and at encoder of @xmath11 , send the remaining 1 - 1/a information bits and last 1/c fraction of the parity bits ( figure [ ena1 ] ) . also keep in mind that to satisfy the slepian - wolf compression bounds , @xmath25 and @xmath26 ( the source compression rates of the two sources ) have to be in the slepian - wolf rate region . at the decoder of the each source",
    ", we use the information bits coming from both the sources , and the parity bits from that source , and the remaining bits are considered as punctured . the parameters a , b , c , r , @xmath27 , @xmath28 ( the channel rates of the two channels ) , @xmath25 and @xmath26 are related as in . @xmath29 and @xmath30 are the capacities of the channel from the source @xmath10 to @xmath11 and from source @xmath11 to source @xmath10 respectively .",
    "@xmath31 \\hfill \\\\",
    "r_{x_2 }   = r_{c_2 } [ 1- \\frac{1 } { a } + \\frac{{\\frac{1 } { r } - 1 } } { c } ] \\hfill \\\\ \\end{gathered}\\ ] ]    as we can see that punctured bits can be further decreased by taking the total parity bits as the maximum number of parity bits being used and which equals maximum of @xmath12 + @xmath13 and @xmath16 + @xmath17 . hence we perform the second step according to .",
    "the encoding and decoding functions are explained in detail below :    @xmath33encoding : @xmath34 the scheme is explained in the figure [ ena1 ] .     and ( b ) source @xmath11,title=\"fig:\",width=340 ] +    at the encoder of @xmath10",
    ", we send the first 1/a fraction of information bits and first 1/b fraction of the parity bits . at the encoder of source @xmath11",
    ", we send the remaining 1 - 1/a fraction of information bits , and last 1/c fraction of parity bits .",
    "the ldpc matrix made is of rate r .",
    "we have four variables here a , b , c , r for given @xmath20 , @xmath21 , @xmath35 and @xmath28 for which we use the set of equations to solve them explicitly .",
    "it can also be seen that @xmath36    ( b ) , and ( b ) special case of this model as c=1 , title=\"fig:\",width=340 ] +    ( a ) , and ( b ) special case of this model as b=1 , title=\"fig:\",width=340 ] +    consider channels of figure [ ena1](b ) . in figure  [ ena2](a ) , we have an equivalent model with @xmath37=n-(1 - 1/c)(n - k ) , where k = nr , and the parity bits are @xmath16 and @xmath17 .",
    "according to @xcite , the performance of decoder do not change after puncturing if r / c do not change .",
    "it is easy to see using that the model of source @xmath11 is similar to test model in figure  [ ena2](b ) when @xmath38 .",
    "we will show that the decoder performance of the model of figure  [ ena1](b ) and figure  [ ena2](b ) are the same in the next few lemmas .",
    "similarly , we will show that the decoder performance of the model of figure  [ ena1](a ) and figure  [ ena21](b ) ( its equivalent model when b=1 ) which means that decoder performance do not change with the choices of a , b , c .",
    "the ratio of rate and capacity are same for figure [ ena2](a ) ( or figure [ ena1](b ) ) and figure [ ena2](b ) are the same    as @xmath39 after some manipulations , we get @xmath40 using equations  .",
    "the ratio of rate and capacity are same for figure [ ena1](a ) and figure [ ena21](b ) are the same .    as @xmath41 after some manipulations , we get @xmath42 using equations  .",
    "it is also easy to see that the bits sent satisfy equation , the total bits that are sent through encoder @xmath10 are@xmath43 and the total bits that are sent through encoder @xmath11",
    "are@xmath44    hence",
    "all the equations in equation makes good sense.as we can see that punctured bits can be further decreased by taking the total parity bits as the maximum number of parity bits being used and which equals maximum of @xmath12 + @xmath13 and @xmath16 + @xmath17 . hence we perform the second step according to equation .    we still need to find @xmath29 and @xmath30 for solving the above equations .",
    "consider the forward model as in figure [ fwm ] .",
    "+    the capacity of the forward model is given by @xmath45    where @xmath46 if the probability that @xmath47 is @xmath48 , then the backward channel is given as in figure [ bwm ]     +    where @xmath49 and @xmath50 .",
    "hence , the capacity of the backward channel is given by @xmath51    where @xmath52    it is clear that i have given separate parameters for source and channel encoding so that they can be separately decided , and then use equations , and then equations to get all the relevant parameters of the code and design .    according to @xcite , for any rates @xmath3 and @xmath4 that 0 < @xmath3 < @xmath4",
    "< 1 , there exists an ensemble of ldpc codes with the following property : the ensemble can be punctured from rate @xmath3 to @xmath4 resulting in asymptotically good codes for all rates @xmath53 . hence , we design punctured codes according to @xcite for rate min(@xmath3,@xmath4 ) and that will work for both and this is the rate we get after the step 2 using the equations .",
    "@xmath33decoding : @xmath34 the decoder needs to determine @xmath10 from information bits of @xmath10 ( partly that were sent from encoder of @xmath10 and partly sent from encoder of @xmath11 ) , and the parity bits @xmath12 and @xmath13 .",
    "likelihood of these parity bits are decided by the channel noise alone , and similar for the information bits coming from the encoder of @xmath10 , while the information bits from the encoder of @xmath11 will have the effect of the bsc in the path from @xmath10 to @xmath11 also .",
    "similar procedure will decode the source @xmath11 .",
    "@xmath33remark : @xmath34 the code construction(irregular ldpc codes ) and decoding has to be done in the way suggested in @xcite which gives a scheme using density evolution with erasures and errors which is the model of our scheme .",
    "so , this model gives the density evolution analysis for this case , and we can choose the degree distribution and decoding parameters according to this density evolution analysis .",
    "as a first example , take @xmath54 = .05 , @xmath55 and @xmath48 = .4 in figure [ fwm ] .",
    "this gives a joint entropy of two sources as 1.5 .",
    "solving @xmath29 and @xmath30 gives @xmath29 = 0.4998 , @xmath30 = 0.4839 .",
    "also take the source rates @xmath25 = .8 , @xmath56 = .7 , and the channel rates as @xmath27 = .9 and @xmath28 = .94 .",
    "putting all these in equations and , we get the parameters a , b , c and r as 1.58 , 1.4754 , 1 and .7259 respectively .",
    "these parameters should ideally work for this scheme .",
    "to simulate , we use @xmath54 = .05 and @xmath48 = .4 in figure [ fwm ] , but we keep @xmath57 as a variable , changing which we vary the joint entropy of the sources . also , choose the both the channels as bsc with error probabilities .0129 and .0069 respectively so that both the channels rates are the same as the capacity .",
    "the simulation result is shown in figure [ simu1 ] .",
    "+     +    as a second example , consider the model for @xmath54 = .09 , @xmath58 and @xmath48 = .4 .",
    "this gives a joint entropy of two sources as 1.5 .",
    "solving @xmath29 and @xmath30 gives @xmath29 = 0.4839 , @xmath30 = 0.4703 .",
    "also take the source rates @xmath25 = .8 , @xmath56 = .7 , and the channel rates as @xmath27 = .85 and @xmath28 = .9 .",
    "putting all these in equations and , we get the parameters a , b , c and r as 1.5392 , 1.4665 , 1 and .7005 respectively .",
    "to simulate , we use @xmath54 = .09 and @xmath48 = .4 in figure [ fwm ] , but we keep @xmath57 as a variable , changing which we vary the joint entropy of the sources . also , choose the both the channels as bsc with error probabilities .0215 and .0129 respectively so that both the channels rates are the same as the capacity .",
    "the simulation result is shown in figure [ simu2 ] .",
    "the correlation between the sources has a memory defined by a markov model .",
    "the two source sequences be @xmath10 and @xmath11 as before , with @xmath11 = @xmath10 + n , where n is generated by a markov model @xmath59 , and addition is modulo two addition .",
    "the model is characterized by a set of states @xmath60 , @xmath61 , the matrix of transition probabilities among states [ a=(@xmath62 ) , with @xmath63 the probability of transition from state @xmath64 to @xmath60 ] , and the list giving the bit probability to associate with each state [ b=(@xmath66 ) , with @xmath67 the probability of getting output v in state @xmath60 ] .",
    "@xmath68 is the initial distribution of each state .",
    "this model has been taken for the source compression in @xcite but this solves the asymmetric case of slepian - wolf only .",
    "similar model is also used in @xcite for the channel with memory .",
    "some work along the lines of the soure correlation having markov structure for the asymmetric case of slepian - wolf problem has also been done in @xcite . for the distributed source coding ,",
    "the results of the previous section still hold with @xmath69 , @xmath70 and different values of @xmath29 and @xmath30 .",
    "hence , define ( @xmath71)-sw distributed code as the code which returns value of a , b , c and r as the parameters of the code construction as in previous section taking values of @xmath71 by the two step approach in equations and @xmath72 \\hfill \\\\",
    "r_b    =   [ 1- \\frac{1 } { a } + \\frac{{\\frac{1 } { r } - 1 } } { c } ] \\hfill \\\\ \\end{gathered}\\ ] ]    @xmath73    let us describe the encoding and decoding process in detail     +    encoding :    we have two sources @xmath10 and @xmath11 each of k bits .",
    "the scheme has been shown in figure [ encmem ] .",
    "send a portion k*alpha of the bits equally spaced directly to the receiver . from the remaining k(1-alpha ) bits , encode them using ( ( @xmath25 -alpha)/(1-alpha ) , ( @xmath56 -alpha)/(1-alpha ) , @xmath29 , @xmath30)-sw distributed code defined earlier where @xmath25 , @xmath56 , @xmath29 and @xmath30 are respectively the desired source rates for the two sources and the forward and backward capacities of the channel between the two sources .",
    "+    decoding :    the decoding approach has been shown in figure [ decmem ] .",
    "the steps of message parsings in the scheme are :    1 .   message parsing from the bit nodes to the markov model : the systematic bits of the two sources and the k*alpha bits that were sent without any encoding are used to get the likelihoods of the bits of n , as n is the sum of the two sources , and hence we can easily get their likelihoods from the likelihoods of the bits of the two sources .",
    "the likelihoods of the nodes of the received vector corresponding to the systematic bits of the two sources be @xmath74 and @xmath75 respectively with n denoting the @xmath76 bit with n varying from 1 to k and the ordering is according to the ordering in the input source vector of the two sources , then the likelihoods of the nodes of n @xmath77 are given by @xmath78 2 .",
    "message parsing from markov model to the bit nodes : after receiving the noisy version of n , and knowing the memory structure of n , we can use trellis decoding to update the likelihoods of n and send the likelihoods towards the bit nodes . consider the trellis for a finite - state binary markov model . the starting and ending states associated with a particular edge @xmath79",
    "are represented by @xmath80 and @xmath81 , respectively , and the bit corresponding to @xmath79 is denoted by @xmath82 .",
    "the trellis has two parallel branches between states ( one associated with the bit @xmath83 and the other with the bit @xmath84 ) .",
    "each one of the branches in the trellis will have an associated a priori probability @xmath85 , which is obtained from the parameters of the markov model .",
    "the resulting equations that implement the belief propagation algorithm over the markov model are given by@xmath86 } a_e g_k^{\\epsilon ( e ) }   \\hfill \\\\",
    "\\beta _ k ( s ) = \\sum\\limits_{e : s^s",
    "( e ) = s } { \\beta _ { k + 1 } [ s^e ( e ) ] } a_e g_{k + 1}^{\\epsilon ( e ) }   \\hfill \\\\",
    "t_n^x   = \\eta _ n \\sum\\limits_{e:\\epsilon ( e ) = x } { \\alpha _ { n - 1 } [ s^s ( e)]\\beta _ n [ s^e ( e ) ] } a_e g_n^x   \\hfill \\\\",
    "\\end{gathered}\\ ] ] where @xmath87 is the normalizing factor so that @xmath88 .",
    "now , this message @xmath89 foes back to the systematic portion of the bit nodes . using",
    "@xmath89 we update the less reliable of the systematic bit nodes of the two sources by the xor operation .",
    "the systematic bits of two sources are updated by messages @xmath90 and @xmath91 respectively .",
    "let us consider message update of @xmath90 , @xmath91 will be similar .",
    "the update equations for @xmath90 are given by @xmath92 since one of @xmath93 or @xmath94 will be known perfectly ( by the encoding scheme ) , this just means to update the other if that is not known perfectly .",
    "message parsing from the bit nodes to the check nodes : this message propagation is from the k(1-alpha ) systematic bits and the parity bits to the check nodes using their respective likelihoods , likelihoods of the systematic nodes given by @xmath90 and @xmath91 and those of parity bits the same as the initial likelihoods in the first iteration and those updated from the previous iteration from check nodes to bit nodes later .",
    "this message parsing is the same as the standard belief propagation scheme 4 .",
    "message parsing from the check nodes to the bit nodes : using the belief propagation , we send the updated likelihoods towards the bit nodes .",
    "hence , the overall scheme can be summarized as :    1 .",
    "message parsing from bit nodes to markov model and back to the bit nodes .",
    "message parsing from bit nodes to check nodes .",
    "3 .   if the parity equations are not satisfied , and iterations are less then maximum - iterations(say 100 ) go to step 1 after message parsing from the check nodes to the bit nodes .",
    "take the source correlation model as a markov source with two states @xmath95 with probability of 0 as one , and @xmath96 with probability of 0 as zero , and the transition probability from @xmath95 to @xmath96 and from @xmath96 and @xmath95 are the same and equal to p , the parameter which decides the entropy rate in the plot . also ,",
    "take the first source to be i.i.d , making @xmath29 = @xmath30 = 1-h(p ) .",
    "take k=6250 , alpha=.2 , and @xmath25 = @xmath56 = 0.75 .",
    "hence , we send 6250*.2=1250 bits of each source without encoding and for the remaining bits we use the equations and with @xmath97 = .6875 , and @xmath29 = @xmath30 = .5 , we get a=2 , b=1 , c=1 , r=.8421 . the simulation with varying value of p is shown in figure [ simu7 ] .",
    "this work deals with the duality between source and channel coding .",
    "we take ideas developed for channel coding and transform them appropriately to construct joint source channel coding techniques .",
    "distributed source coding scheme is a special case of the above work with the capacity of the channels to be equal to 1 .",
    "we illustrated how to do joint distributed source channel coding at arbitrary source rates in the slepian - wolf rate region with arbitrary memoryless source correlation and arbitrary channel .",
    "we also illustrated how to do distributed source coding at arbitrary point in the slepian - wolf rate region when the source correlation model has memory . in all the simulations , we considered regular ( 3,x ) ldpc codes . taking irregular codes instead may perform better due to irregularity in the coding scheme .",
    "further work can be done to extend the distributed source coding for sources with markov correlation to joint source channel coding .",
    "joint source channel coding for sources with memory has been studied in @xcite@xcite .",
    "some work on the asymmetric case for this problem has also been done using turbo codes in @xcite .",
    "but the problem using ldpc codes and doing for general source rates are still open problems .",
    "s. s. pradhan and k. ramchandran , _ distributed source coding using syndromes ( discus ) : design and construction , _ proc .",
    "ieee data compression conference , pp .",
    "158 - 167 , march 1999 .",
    "j. li and r. hu , _ slepian - wolf cooperation : a practical and efficient compress - and - forward relay scheme _ proceeding of 43rd annual allerton conference on communication , control and computing ( allerton ) , st .",
    "louis , mo , nov . 2005 .",
    "d. schonberg , k. ramachandran , s.s .",
    "pradhan , _ distributed code constructions for the entire slepian - wolf rate region for arbitrarily correlated sources _",
    "proceedings of the data compression conference , 2004 .",
    "s. s. pradhan and k. ramchandran , _ distributed source coding : symmetric rates and applications to sensor networks , _ proc .",
    "ieee data compression conference , pp .",
    "363 - 372 , march 2000 .",
    "a. d. liveris , z. xiong , and c. n. georghiades , _ a distributed source coding technique for correlated image using turbo codes , _ ieee comm .",
    "letters , vol .",
    "379 - 381 , sept . 2002 .",
    "j. garcia - frias and y. zhao , _ compression of correlated binary sources using turbo codes , _ ieee comm .",
    "letters , vol . 5 , pp . 417 - 419 , oct . 2002 .",
    "d. schonberg , k. ramchandran , and s. s. pradhan , _ ldpc codes can approach the slepian - wolf bound for general binary sources , _ proc . of fortieth annual allerton conference , urbana - champaign , il , oct .",
    "a. d. liveris , z. xiong , and c. n. georghiades , _ compression of binary sources with side information at the decoder using ldpc codes , _ ieee comm .",
    "letters , vol .",
    "440 - 442 , oct . 2002 .",
    "mina sartipi and faramarz fekri , _ distributed source coding in wireless sensor networks using ldpc coding : the entire slepian - wolf rate region _ ieee communications society wcnc 2005 .",
    "h. pishro - nik and f. fekri , _ results on punctured low - density parity check codes and improved iterative decoding techniques _ submitted to ieee transactions on information theory",
    ". m. mitzenmacher,_a note on low density parity check codes for erasures and errors _ , src technical note 1998 - 017 , december 1998 .",
    "frias and w. zhong , _ ldpc codes for compression of multi - terminal sources with hidden markov correlation _ , ieee communication letters , vol . 7 , no .",
    "3 , mar 2003 .",
    "frias,_decoding of low - density parity - check codes over finite - state binary markov channels _ , ieee transactions on communications , vol .",
    "52 , no . 11 ,",
    "giuseppe caire , shlomo shamai , and sergio verdu,_noiseless data compression with low - density parity - check codes _ , advances in network information theory p. gupta , g. kramer and a. j. van wijngaarden , eds . , dimacs series in discrete mathematics and theoretical computer science , vol .",
    "263 - 284 , american mathematical society , 2004 a. d. liveris , z. xiong and c. n. georghiades , _ joint source - channel coding of binary sources with side information at the decoder using ira codes _ , multimedia signal processing , 2002 ieee workshop on 9 - 11 dec .",
    "2002 page(s):53 - 56 r. hu , r. viswanathan and jing li , _ a new coding scheme for the noisy - channel slepian - wolf problem : separate design and joint decoding _ , globecom 04 .",
    "ieee volume 1 , 29 nov.-3 dec .",
    "2004 page(s):51 - 55 vol.1 j. garcia - frias and y. zhao , _ near - shannon / slepian - wolf performance for unknown correlated sources over awgn channels _ , ieee transactions on communications , volume 53 , issue 4 , april 2005 page(s):555 - 559      r. g. gallager , _ low - density parity - check codes _ , mit press , 1963",
    ". t. j. richardson and r. l. urbanke , _ the capacity of low - density parity - check codes under message - passing decoding _ , ieee trans .",
    "information theory 47 ( feb .",
    "2001 ) , 599 - 618 .",
    "a. wyner , _ recent results in the shannon theory , _ ieee trans .",
    "theory , vol .",
    "20 , pp . 2 - 10 ,",
    "january 1974 .",
    "c. berrou and a. glavieux , _ near optimum error correcting coding and decoding : turbo - codes , _ ieee trans .",
    "communications , vol .",
    "44 , pp . 1261 - 1271 , october 1996 .",
    "g. caire , s. shamai and s. verdu , _ almost - noiseless joint source - channel coding - decoding of sources with memory _ , 5th international itg conference on source and channel coding ( scc ) , jan 14 - 16 , 2004 j. garcia - frias and j. d. villasenor,_joint turbo decoding and estimation of hidden markov sources _ , ieee journal on selected areas in communications , vol .",
    "2001    j. d. ser , p. m. crespo and o. galdos , _ asymmetric joint source - channel coding for correlated sources with blind hmm estimation at the receiver _ , eurasip journal on wireless communications and networking 2005:4 , 483 - 492"
  ],
  "abstract_text": [
    "<S> in this paper , we give a distributed joint source channel coding scheme for arbitrary correlated sources for arbitrary point in the slepian - wolf rate region , and arbitrary link capacities using ldpc codes . </S>",
    "<S> we consider the slepian - wolf setting of two sources and one destination , with one of the sources derived from the other source by some correlation model known at the decoder . distributed encoding and separate decoding is used for the two sources . </S>",
    "<S> we also give a distributed source coding scheme when the source correlation has memory to achieve any point in the slepian - wolf rate achievable region . in this setting , we perform separate encoding but joint decoding .    </S>",
    "<S> distributed source coding , joint source - channel coding , ldpc codes , slepian - wolf . </S>"
  ]
}