{
  "article_text": [
    "suppose we are trying to learn a difficult classification problem : for example determining whether the given image contains a human face , or whether the mri image shows a malignant tumor , etc .",
    "we may first try to train a simple model such as a small neural network .",
    "if that fails , we may move on to other , potentially more complex , methods of classification such as support vector machines with different kernels , techniques to apply certain transformations to the data first , etc .",
    "conventional statistical learning theory attempts to bound the number of samples needed to learn to a specified level of accuracy for each of the above models ( e.g.  neural networks , support vector machines ) .",
    "specifically , it is enough to bound the vc - dimension of the learning model to determine the number of samples to use  @xcite .",
    "however , if we allow ourselves to change the model , then the vc - dimension of the overall learning algorithm is not finite , and much of statistical learning theory does not directly apply .    accepting that much of the time the complexity of the model can not be a priori bounded , structural risk minimization  @xcite explicitly considers a hierarchy of increasingly complex models .",
    "an alternative approach , and one we follow in this paper , is simply to consider a single learning model that includes all possible classification methods .",
    "we consider the unrestricted learning model consisting of all computable classifiers .",
    "since the vc - dimension is clearly infinite , there are no uniform bounds ( independent of the distribution and the target concept ) on the number of samples needed to learn accurately  @xcite .",
    "yet we still want to guarantee a desired level of accuracy . rather than deciding on the number of samples a priori",
    ", it is natural to allow the learning algorithm to decide when it has seen sufficiently many labeled samples based on the training samples seen up to now and their labels . since the above learning model includes any practical classification scheme , we term it universal ( pac- ) learning .",
    "we first show that there is a computable learning algorithm in our universal setting .",
    "then , in order to obtain bounds on the number of training samples that would be needed , we consider measuring sample complexity of the learning algorithm as a function of the unknown correct labeling function ( i.e.  target concept ) . although the correct labeling is unknown , this sample complexity measure could be used to compare learning algorithms speculatively : ",
    "if the target labeling were such and such , learning algorithm @xmath0 requires fewer samples than learning algorithm @xmath1 \" . by asking what is the largest sample size needed assuming the target labeling function is in a certain class",
    ", we could compare the sample complexity of the universal learner to a learner over the restricted class ( e.g.  with finite vc - dimension ) .",
    "however , we prove that it is impossible to bound the sample complexity of any _ computable _ universal learning algorithm , even as a function of the target concept .",
    "depending on the distribution , any such bound will be exceeded with arbitrarily high probability .",
    "the impossibility of a distribution - independent bound is entirely due to the computability requirement .",
    "indeed we show there is an uncomputable learning procedure for which we bound the number of samples queried as a function of the unknown target concept , independently of the distribution .",
    "our results imply that computable learning algorithms in the universal setting must  waste samples \" in the sense of requiring more samples than is necessary for statistical reasons alone .",
    "there is comparatively little work in statistical learning theory on learning arbitrary computable classifiers compared to the volume of research on learning in more restricted settings . computational learning theory ( aka pac - learning )",
    "requires learning algorithms to be efficient in the sense of running in polynomial time of certain parameters  @xcite .",
    "that work generally restricts learning to very limited concept / hypothesis spaces such as perceptrons , dnf expressions , limited - weight neural networks , etc .",
    "the purely statistical learning theory paradigm ignores issues of computability  @xcite .",
    "work on learning arbitrary computable functions is mostly in the  learning in the limit \" paradigm  @xcite , in which the goal of learning is to eventually converge to the perfectly correct hypothesis as opposed to approximating it with an approximately correct hypothesis .",
    "the idea of allowing the learner to ask for a varying number of training samples based on the ones previously seen was studied before in statistical learning theory  @xcite .",
    "linial et al  @xcite called this model  dynamic sampling \" and showed that dynamic sampling allows learning with a hypothesis space of infinite vc - dimension if all hypotheses can be enumerated .",
    "this is essentially theorem  [ thm : uncomputablealgorithm ] of our paper .",
    "however , the hypothesis space of all computable functions can not be enumerated by any algorithm , and thus these results do not directly imply the existence of a learning algorithm in our setting .    our proof technique for establishing positive results ( theorem  [ thm : learningalgorithmexists ] ) is parallel evaluation of all hypotheses , and is based on levin s universal search  @xcite . in learning theory , levin s",
    "universal search was previously used by goldreich and ron  @xcite to evaluate all learning algorithms in parallel and obtain an algorithm with asymptotically optimal computation time .",
    "the main negative result of this paper is showing the absence of distribution independent bounds on sample complexity for computable universal learning algorithms ( theorem  [ thm : nobound ] ) .",
    "recently ryabko  @xcite considered learning arbitrary computable classifiers , albeit in a setting where the number of samples for the learning algorithm is externally chosen .",
    "he demonstrated a computational difficulty in determining the number of samples needed : it grows faster than any computable function of the length of the target concept . in contrast",
    ", we prove that distribution - independent bounds do not exist altogether for computable learning algorithms in our setting .",
    "the _ sample space _ @xmath2 is the universe of possible points over which learning occurs . here",
    "we will largely suppose the sample space @xmath2 is the set of all finite binary strings @xmath3 .",
    "concept space _ @xmath4 and _ hypothesis space _",
    "@xmath5 are sets of boolean - valued functions over @xmath2 , which are said to _ label",
    "_ points @xmath6 as @xmath7 .",
    "the concept space @xmath4 is the set of all possible labeling functions that our learning algorithm may be asked to learn from . in each learning scenario",
    ", there is some unknown _ target concept _ @xmath8 that represents the desired way of labeling points .",
    "there is also an unknown _ sample distribution _ @xmath9 over @xmath2 .",
    "the learning algorithm chooses a _",
    "@xmath10 based on iid samples drawn from @xmath9 and labeled according to the target concept @xmath11 . since we can not hope to distinguish between a hypothesis that is always correct and one that is correct most of the time , we adopt the  probably approximately correct \"  @xcite goal of producing with high probability ( @xmath12 ) a hypothesis @xmath13 such that the probability over @xmath14 that @xmath15 is small ( @xmath16 ) .    here we will mostly consider the concept space @xmath4 to be the set of all total recursive functions @xmath17 .",
    "we say that this is a universal learning setting because @xmath4 includes any practical classification scheme .",
    "we will mostly consider the hypothesis space to be the set of all partial recursive functions @xmath18 , where @xmath19 indicates failure to halt . from pac learning it is known that sometimes it helps to use different concept and hypothesis classes , if one desires the learning algorithm to be efficient  @xcite . in a related way , allowing our algorithm to output a partial recursive function that may not halt on all inputs seems to permit learning ( e.g.  theorem  [ thm : learningalgorithmexists ] ) .",
    "abusing notation , @xmath8 or @xmath10 will refer to either the function or to a representation of that function as a program .",
    "similarly @xmath4 and @xmath5 will refer to the sets of functions or to the sets of representations of the corresponding functions .",
    "we assume all programs are written in some fixed alphabet and are interpreted by some fixed universal turing machine . if @xmath13 is a partial recursive function and @xmath20 then by convention @xmath21 for any partial recursive function @xmath22 ( even if @xmath23 also ) .",
    "we can now define what we mean by a learning algorithm :    algorithm @xmath0 is a _ learning algorithm _ over sample space @xmath2 , concept space @xmath4 , and hypothesis space @xmath5 if :    * ( syntactic requirements ) @xmath0 takes two inputs @xmath24 and @xmath25 , queries an oracle for pairs in @xmath26 , and if @xmath0 halts it outputs a hypothesis @xmath10 . *",
    "( semantic requirements ) for any @xmath27 , for any concept @xmath8 , and distribution @xmath9 over @xmath2 , if the oracle returns pairs @xmath28 for @xmath29 drawn iid from @xmath9 , then @xmath0 always halts , and with probability at least @xmath12 outputs a hypothesis @xmath13 such that @xmath30 < { \\varepsilon}$ ]",
    ".    the always halting requirement seems a nice property of the learning algorithm and indeed the learning algorithm we develop ( theorem  [ thm : learningalgorithmexists ] ) will halt for any concept and sequence of samples . however , relaxing this requirement to allow a non - zero probability that the learning algorithm queries the oracle for infinitely many samples does not change our negative results ( theorem  [ thm : nobound ] ) , as long as a finite number of oracle calls implies halting .",
    "the fundamental notion in statistical learning theory is that of sample complexity . since the vc - dimension of our hypothesis space is infinite , there is no _ uniform bound _",
    "@xmath31 on the number of samples needed to learn to the @xmath27 level of accuracy .",
    "we will consider the question of whether for a given learning algorithm there is a _ distribution - independent bound _",
    "@xmath32 on the number of samples queried from the oracle where @xmath8 is the target hypothesis . in other words",
    "the bound is allowed to depend on the target concept @xmath11 but not on the sample distribution @xmath9 .",
    "such a bound may be satisfied with certainty , or satisfied with high probability over the learning samples .",
    "we first show that there is a computable learning algorithm in our setting .",
    "[ thm : learningalgorithmexists ] there is a learning algorithm over sample space @xmath2 of all finite binary strings , hypothesis space @xmath5 of all partial recursive functions , and concept space @xmath4 of all total recursive functions .    in order to prove this theorem we need the following lemma .",
    "results equivalent to this lemma can be found in  @xcite .",
    "[ lem : m ] let @xmath2 be any sample space and @xmath9 be any distribution over @xmath2 . fix any function @xmath33 .",
    "suppose hypothesis space @xmath5 is countable , and let @xmath34 be some ordering of @xmath5 .",
    "for any @xmath27 , let @xmath35 .",
    "suppose @xmath36 is an infinite sequence of iid samples drawn from @xmath9 .",
    "then the probability that there exists @xmath37 such that @xmath38 > { \\varepsilon}$ ] , but @xmath39 agrees with @xmath11 on @xmath40 , is less than @xmath41 .",
    "the probability that a particular @xmath39 with error probability @xmath38 > { \\varepsilon}$ ] gets @xmath42 i.i.d .",
    "instances drawn from @xmath9 correct is less than @xmath43 . by the union bound ,",
    "the probability that _ any _ @xmath39 with error probability greater than @xmath16 gets @xmath42 instances correct is less than @xmath44 .",
    "* proof of theorem  [ thm : learningalgorithmexists ] : * let @xmath34 be a recursive enumeration of @xmath5 ( for example in lexicographic order ) . for the given @xmath27 , let @xmath42 be defined as in lemma  [ lem : m ] .",
    "the learning algorithm computes infinitely many threads @xmath45 running in parallel .",
    "this can be done by a standard dovetailing technique .",
    "( for example use the following schedule : for @xmath46 to infinity , for @xmath47 to k , perform step @xmath48 of thread @xmath49 . )",
    "thread @xmath49 sequentially checks whether @xmath50 , @xmath51 , @xmath52 , @xmath53 , exiting if a check fails .",
    "if all @xmath42 checks pass , thread @xmath49 terminates and outputs @xmath39 .",
    "the learning algorithm queries the oracle as necessary for new learning samples and their labeling .",
    "the overall algorithm terminates as soon as some thread outputs an @xmath39 , and outputs this hypothesis . by lemma  [ lem : m ] , with probability at least @xmath12 , this @xmath39 has error probability less than @xmath16 .",
    "further , since @xmath54 , the learning algorithm will always terminate .",
    "note that it seems necessary to expand the hypothesis space to include all partial recursive functions because the concept space of total recursive functions does not have a recursive enumeration ( it is uncomputable whether a given program is total recursive or not ) .",
    "we will see in theorem  [ thm : nobound ] that there is no bound @xmath55 on the number of samples queried by any computable learning algorithm in our setting .",
    "let us obtain some intuition for why that is true for the above learning algorithm .",
    "then we will contrast this to the case of an uncomputable learning algorithm .",
    "in essence , we can make the above learning algorithm query for more samples than is necessary for statistical reasons alone .",
    "intuitively , suppose that an @xmath56 coming early in the ordering is always correct but takes a very long time to compute .",
    "the learning algorithm can not wait for this @xmath56 to finish , because it does not know that any particular @xmath39 will ever halt . at some point it has to start testing @xmath39 s that come later in the ordering and that have larger @xmath42 s .",
    "testing these requires more learning samples than @xmath57 .",
    "if we can know which @xmath39 s are safe to skip over since they do nt halt , and for which @xmath39 s we should wait , then the above problem is solved .",
    "indeed , the following theorem shows that there is no statistical reason why a distribution - independent bound @xmath55 is impossible .",
    "the theorem presents a well defined method of learning ( albeit an uncomputable one ) for which there exists such a bound , and this bound is satisfied with certainty .",
    "below , the halting oracle gives @xmath7 answers to questions of the form @xmath58 where @xmath59 such that a @xmath60 answer indicates that @xmath61 halts and a @xmath62 answer indicates it does not ; the answers are clearly uncomputable .",
    "[ thm : uncomputablealgorithm ] if a learning algorithm is allowed to query the halting oracle , then there is a learning algorithm over sample space @xmath2 of all finite binary strings , hypothesis space @xmath5 of all partial recursive functions , and concept space @xmath4 of all total recursive functions , and a function @xmath63 , such that for any approximation parameters @xmath27 , any target concept @xmath8 , and any distribution @xmath9 over @xmath2 , the learning algorithm uses at most @xmath55 training samples .    rather than dovetailing",
    "as is done for the computable learning algorithm ( theorem  [ thm : learningalgorithmexists ] ) , we can sequentially test every @xmath39 on samples @xmath64 , @xmath52 , @xmath65 because we can determine whether @xmath39 halts on a given input .",
    "since @xmath66 for some @xmath67 , the hypothesis @xmath39 we output will always satisfy @xmath68 , and therefore we will require at most @xmath69 samples .",
    "we now show that for any _ computable _ learning algorithm , and any possible sample bound @xmath55 , there is a target concept @xmath11 and a sample distribution such that this sample bound is violated with high probability .",
    "the probability of violation can be made arbitrarily close to @xmath70 ( which approaches @xmath60 as @xmath71 ) .",
    "in fact this theorem is stronger : it shows that given a learning algorithm , without varying the target concept , but just by varying the distribution it is possible to make the algorithm ask for arbitrarily many learning samples with high probability .",
    "[ thm : nobound ] for any learning algorithm over sample space @xmath2 of all finite binary strings , hypothesis space @xmath5 of all partial recursive functions , and concept space @xmath4 of all total recursive functions , there is a target concept @xmath8 , such that for any approximation parameters @xmath27 , for any @xmath72 , and for any sample bound @xmath73 there is a distribution @xmath9 over @xmath2 , such that the learning algorithm uses more than @xmath74 training samples with probability at least @xmath75 .",
    "the key difference between a computable and an uncomputable learning algorithm , is that a concept can simulate a computable one . by simulating the learning algorithm , a concept can choose to behave in way that is bad for the learning algorithm s sample complexity .    to prove the above theorem",
    ", we will first need the following lemma .",
    "the lemma essentially shows a situation such that any learning algorithm according to our definition must query for more than @xmath74 learning samples with high probability when the target concept is chosen adversarily .",
    "the lemma is true even without requiring the learning algorithm to be computable .",
    "note that the lemma does not directly imply the theorem above , even in its weaker form , because in order to increase the number of learning samples that are likely queried by the learning algorithm , we have to change the target concept . since @xmath55 is a function of @xmath11 ,",
    "there is no guarantee that the bound does nt become larger as well .",
    "[ lem : probabilisticmethod ] let @xmath2 be a set of @xmath76 points , and let @xmath4 be the set of all labelings of @xmath2 .",
    "let @xmath9 be a uniform distribution over @xmath2 .",
    "suppose @xmath0 is a learning algorithm over sample space @xmath2 , concept and hypothesis space @xmath4 .",
    "for any accuracy parameters @xmath27 and any @xmath77 , there is a concept @xmath8 such that when the oracle draws from @xmath9 labeled according to @xmath11 the probability that @xmath0 samples more than @xmath74 points is at least @xmath78 .",
    "we use the probabilistic method to find a particularly bad concept @xmath79 .",
    "suppose we do not start with a fixed target concept @xmath11 , but draw it uniformly from @xmath4 .",
    "in other words , @xmath11 is determined by values @xmath80 drawn uniformly from @xmath81 . given some @xmath82 , @xmath83 , and @xmath84 , the value of @xmath85 is a fair coin flip .",
    "thus if on @xmath82 labeled by @xmath83 , @xmath0 outputs a hypothesis without asking for more samples , then the hypothesis is incorrect on @xmath29 with probability @xmath86 . if we now let @xmath29 vary , the probability that the hypothesis is incorrect on @xmath29 is at least @xmath87 since there are at least @xmath88 points not in @xmath82 .",
    "now suppose for any @xmath11 the probability that @xmath0 samples more than @xmath74 points is at most @xmath75 .",
    "then the unconditional probability that the hypothesis output by @xmath0 is incorrect on a random sample point is at least @xmath89 .",
    "this implies that there is a concept @xmath90 such that the probability that the hypothesis output by @xmath0 is incorrect on a random sample point is at least @xmath89 .    since @xmath0 is a learning algorithm , when we use @xmath79 to label the training points , and use accuracy parameters @xmath27 , the probability that the hypothesis produced by @xmath0 has error probability greater than @xmath16 is at most @xmath41 .",
    "if we make the worst case assumption that whenever the error probability of the hypothesis is larger than @xmath16 it is exactly @xmath60 , and otherwise the error probability is exactly @xmath16 , then the probability that the hypothesis output by @xmath0 is incorrect on a random sample point is at most @xmath91 .",
    "thus @xmath92 , implying that @xmath93 .",
    "now in order to prove theorem  [ thm : nobound ] , we essentially show that there is some fixed concept @xmath79 that behaves as the bad @xmath11 s in arbitrary instances of lemma  [ lem : probabilisticmethod ] .",
    "* proof of theorem  [ thm : nobound ] : * consider the following program @xmath94 .",
    "first it interprets the given string @xmath95 as a tuple @xmath96 for @xmath24 , @xmath25 and @xmath97 using some fixed one - to - one encoding of such tuples as binary strings . if @xmath29 can not be decoded appropriately , or if @xmath98 then @xmath99 returns @xmath62",
    "otherwise , for these @xmath100 , let @xmath101 be the set of @xmath76 strings which are interpreted as @xmath102 , @xmath52 , @xmath103 , and let @xmath104 be a uniform distribution over @xmath105 and @xmath62 elsewhere .",
    "let @xmath106 be the set of all possible labelings of @xmath105 .",
    "for each labeling @xmath107 , program @xmath99 computes the probability @xmath108 that @xmath0 given accuracy parameters @xmath27 , queries for more than @xmath74 sample points if points are drawn from @xmath104 labeled according to @xmath109 .",
    "for each @xmath109 , this requires simulating @xmath0 for at most @xmath110 different sequences of sample points .",
    "let @xmath111 , breaking ties in some fixed way .",
    "finally @xmath99 outputs @xmath112 .",
    "observe that @xmath99 is total recursive since @xmath0 spends a finite time on any finite sequence of sample points .",
    "( this is a weaker condition than the always halting requirement of our definition of a learning algorithm . )",
    "thus @xmath99 is some @xmath90 .",
    "further , for any @xmath100 , on all points @xmath113 for @xmath114 , @xmath99 finds the same @xmath115 , and thus on these points @xmath79 acts like this @xmath115 . by lemma  [ lem : probabilisticmethod ] , if @xmath77 then this @xmath115 has the property that @xmath116 .",
    "therefore , if @xmath0 is given accuracy parameters @xmath27 , the target concept is @xmath79 , and the distribution @xmath9 is uniform over @xmath117 for some @xmath118 such that @xmath77 , then the probability that @xmath0 requests more than @xmath74 samples is at least @xmath78 . since we can choose @xmath9 such that @xmath76 is large enough , we obtain the desired result .",
    "we have shown that learning arbitrary computable classifiers is possible in the statistical learning paradigm .",
    "however for any computable learning algorithm , the number of samples required to learn to a desired level of accuracy may become arbitrarily large depending on the sample distribution .",
    "this is in contrast to uncomputable learning methods in the same universal setting whose sample complexity can be bounded independently of the distribution .",
    "our results mean that there is a big price in terms of sample complexity to be paid for the combination of universality and computability of the learner . specifically , by tweaking the distribution we can make a computable universal learner arbitrarily worse than a restricted learning algorithm on a finite vc - dimensional hypothesis space , or even an uncomputable universal learner .",
    "while we have presented a single computable learning algorithm in our universal setting , one would like to develop a measure that would allow different learning algorithms to be compared to each other in terms of sample complexity .",
    "we have seen that sample complexity @xmath55 is not such a measure ; is there a viable alternative ?    finally , we have ignored computation time in our analysis . as such ,",
    "our learning algorithm is not likely to have practical significance .",
    "integrating running time into the theory presented would be a critical extension ."
  ],
  "abstract_text": [
    "<S> statistical learning theory chiefly studies restricted hypothesis classes , particularly those with finite vapnik - chervonenkis ( vc ) dimension . </S>",
    "<S> the fundamental quantity of interest is the sample complexity : the number of samples required to learn to a specified level of accuracy . </S>",
    "<S> here we consider learning over the set of all computable labeling functions . </S>",
    "<S> since the vc - dimension is infinite and a priori ( uniform ) bounds on the number of samples are impossible , we let the learning algorithm decide when it has seen sufficient samples to have learned . we first show that learning in this setting is indeed possible , and develop a learning algorithm . </S>",
    "<S> we then show , however , that bounding sample complexity independently of the distribution is impossible . </S>",
    "<S> notably , this impossibility is entirely due to the requirement that the learning algorithm be computable , and not due to the statistical nature of the problem . </S>"
  ]
}