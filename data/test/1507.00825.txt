{
  "article_text": [
    "in recent years , _ zero - shot learning _ ( zsl ) @xcite has been an active research topic in machine learning , computer vision , and natural language processing .",
    "many practical applications can be formulated as a zsl task : drug discovery @xcite , bilingual lexicon extraction @xcite , and image labeling @xcite , to name a few .",
    "cross - lingual information retrieval @xcite can also be viewed as a zsl task .",
    "zsl can be regarded as a type of ( multi - class ) classification problem , in the sense that the classifier is given a set of known example - class label pairs ( training set ) , with the goal to predict the unknown labels of new examples ( test set ) .",
    "however , zsl differs from the standard classification in that the labels for the test examples are not present in the training set . in standard settings ,",
    "the classifier chooses , for each test example , a label among those observed in the training set , but this is not the case in zsl .",
    "moreover , the number of class labels can be huge in zsl ; indeed , in bilingual lexicon extraction , labels correspond to possible translation words , which can range over entire vocabulary of the target language .",
    "obviously , such a task would be intractable without further assumptions .",
    "labels are thus assumed to be embedded in a metric space ( _ label space _ ) , and their distance ( or similarity ) can be measured in this space .",
    "such a label space can be built with the help of background knowledge or external resources ; in image labeling tasks , for example , labels correspond to annotation keywords , which can be readily represented as vectors in a euclidean space , either by using corpus statistics in a standard way , or by using the more recent techniques for learning word representations , such as the continuous bag - of - words or skip - gram models @xcite .    after a label space",
    "is established , one natural approach would be to use a regression technique on the training set to obtain a mapping function from the example space to the label space .",
    "this function could then be used for mapping unlabeled examples into the label space , where nearest neighbor search is carried out to find the label closest to the mapped example .",
    "finally , this label would be output as the prediction for the example .    to find the mapping function , some researchers use the standard linear ridge regression @xcite , whereas others use neural networks @xcite .    in the machine learning community ,",
    "meanwhile , the _ hubness phenomenon _",
    "@xcite is attracting attention as a new type of the `` curse of dimensionality . ''",
    "this phenomenon is concerned with nearest neighbor methods in high - dimensional space , and states that a small number of objects in the dataset , or _",
    "hubs _ , may occur as the nearest neighbor of many objects .",
    "the emergence of these hubs will diminish the utility of nearest neighbor search , because the list of nearest neighbors often contain the same hub objects regardless of the query object for which the list is computed .      in this paper",
    ", we show the interaction between the regression step in zsl and the subsequent nearest neighbor step has a non - negligible effect on the prediction accuracy .    in zsl",
    ", examples and labels are represented as vectors in high - dimensional space , of which the dimensionality is typically a few hundred . as demonstrated by dinu and baroni @xcite",
    "( see also sect .",
    "[ sec : experiment ] ) , when zsl is formulated as a problem of ridge regression from examples to labels , `` hub '' labels emerge , which are simultaneously the nearest neighbors of many mapped examples .",
    "this has the consequence of incurring bias in the prediction , as these labels are output as the predicted labels for these examples .",
    "the presence of hubs are not necessarily disadvantageous in standard classification settings ; there may be `` good '' hubs as well as `` bad '' hubs @xcite .",
    "however , in typical zsl tasks in which the label set is fine - grained and huge , hubs are nearly always harmful to the prediction accuracy .    therefore , the objective of this study is to investigate ways to suppress hubs , and to improve the zsl accuracy .",
    "our contributions are as follows .    1 .",
    "we analyze the mechanism behind the emergence of hubs in zsl , both with ridge regression and ordinary least squares .",
    "it is established that hubness occurs in zsl not only because of high - dimensional space , but also because ridge regression has conventionally been used in zsl in a way that _ promotes _ hubness . to be precise ,",
    "the distributions of the mapped examples and the labels are different such that hubs are likely to emerge .",
    "2 .   drawing on the above analysis , we propose using ridge regression to map labels into the space of examples .",
    "this approach is contrary to that followed in existing work on zsl , in which examples are mapped into label space .",
    "our proposal is therefore to reverse the mapping direction . + as shown in sect .",
    "[ sec : experiment ] , our proposed approach outperformed the existing approach in an empirical evaluation using both synthetic and real data .",
    "3 .   in terms of contributions to the research on hubness , this paper is the first to provide in - depth analysis of the situation in which the query and data follow different distributions , and to show that the variance of data matters to hubness .",
    "in particular , in sect .  [ sec : hubness ] , we provide a proposition in which the degree of bias present in the data , which causes hub formation , is expressed as a function of the data variance . in sect  [ sec : regression - hubness ] , this proposition serves as the main tool for analyzing hubness in zsl .",
    "let @xmath0 be a set of examples , and @xmath1 be a set of class labels . in zsl , not only examples but also labels are assumed to be vectors .",
    "for this reason , examples are sometimes referred to as _ source objects _ , and labels as _ target objects_. in the subsequent sections of this paper , we mostly follow this terminology when referring to the members of @xmath0 and @xmath1 .",
    "let @xmath2 and @xmath3 .",
    "these spaces , @xmath4 and @xmath5 , are called _ source space _ and _ target space _ , respectively . although @xmath0 can be the entire space @xmath4",
    ", @xmath1 is usually a finite set of points in @xmath5 , even though its size may be enormous in some problems .",
    "let @xmath6 be the training examples ( training source objects ) , and @xmath7 be their labels ( training target objects ) ; i.e. , the class label of example @xmath8 is @xmath9 , for each @xmath10 . in a standard classification setting ,",
    "the labels in the training set are equal to the entire set of labels ; i.e. , @xmath11 .",
    "in contrast , this assumption is not made in zsl , and @xmath12 is a strict subset of @xmath1 .",
    "moreover , it is assumed that the true class labels of test examples do not belong to @xmath12 ; i.e. , they belong to @xmath13 .    in such a situation , it is difficult to find a function @xmath14 that maps @xmath15 directly to a label in @xmath1 . therefore , a popular ( and also natural ) approach is to learn a projection @xmath16 , which can be done with a regression technique . with a projection function @xmath17 at hand ,",
    "the label of a new source object @xmath18 is predicted to be the one closest to the mapped point @xmath19 in the target space .",
    "the prediction function @xmath14 is thus given by @xmath20 after a source object @xmath21 is projected to @xmath19 , the task is reduced to that of nearest neighbor search in the target space .",
    "the utility of nearest neighbor search would be significantly reduced if the same objects were to appear consistently as the search result , irrespective of the query .",
    "radovanovi et  al .",
    "@xcite showed that such objects , termed _ hubs _ , indeed occur in high - dimensional space .",
    "although this phenomenon may seem counter - intuitive , hubness is observed in a variety of real datasets and distance / similarity measures used in combination @xcite .",
    "the aim of this study is to analyze the hubness phenomenon in zsl , which involves nearest neighbor search in high - dimensional space as the last step .",
    "however , as a tool for analyzing zsl , the existing theory on hubness @xcite is inadequate , as it was mainly developed for comparing the emergence of hubness in spaces of different dimensionalities .    in the analysis of zsl in sect .",
    "[ sec : two - eggs ] , we aim to compare two distributions in the same space , but which differ in terms of _ variance_. to this end , we first present a proposition below , which is similar in spirit to the main theorem of radovanovi et  al .",
    "* theorem  1 ) , but which distinguishes the query and data distributions , and also expresses the expected difference between the squared distances from queries to database objects in terms of their variance .",
    "the proposition is concerned with nearest neighbor search , in which @xmath21 is a query , and @xmath22 and @xmath23 are two objects in a dataset . in the context of zsl as formulated in sect .",
    "[ sec : zsl ] , @xmath21 represents the image of a source object in the target space ( through the learned regression function @xmath17 ) , and @xmath22 and @xmath23 are target objects ( labels ) lying at different distances from the origin .",
    "we are interested in which of @xmath22 and @xmath23 are more likely to be closer to @xmath21 , when @xmath21 is sampled from a distribution @xmath24 with zero mean .",
    "let @xmath25 $ ] and @xmath26 $ ] denote the expectation and variance , respectively , and let @xmath27 be a multivariate normal distribution with mean @xmath28 and covariance matrix @xmath29 .",
    "[ prop : radovanovic ] let @xmath30^{\\text{\\upshape t}}$ ] be a @xmath31-dimensional random vector , with components @xmath32 ( @xmath33 ) sampled i.i.d . from a normal distribution with zero mean and variance @xmath34 ; i.e. , @xmath35 , where @xmath36 .",
    "further let @xmath37}$ ] be the standard deviation of the squared norm @xmath38 .",
    "consider two fixed samples @xmath22 and @xmath23 of random vector @xmath39 , such that the squared norms of @xmath22 and @xmath23 are @xmath40 apart .",
    "in other words , @xmath41 let @xmath21 be a point sampled from a distribution @xmath24 with zero mean .",
    "then , the expected difference @xmath42 between the squared distances from @xmath22 and @xmath23 to @xmath21 , i.e. , @xmath43      -      \\operatorname{e}_{\\mathcal{x } } \\left [ \\| \\mathbf{x } - \\mathbf{y}_1 \\|^2 \\right ]      \\label{eq : delta - definition}\\ ] ] is given by @xmath44    for @xmath45 , the distance between a point @xmath21 and @xmath9 is given by @xmath46 and its expected value is @xmath47      & = \\operatorname{e}_{\\mathcal{x } } \\left [ \\| \\mathbf{x } \\|^2 \\right ] + \\| \\mathbf{y}_i \\|^2 - 2 \\operatorname{e}_{\\mathcal{x } } \\left [ \\mathbf{x } \\right]^{\\text{t } } \\mathbf{y}_i          = \\operatorname{e}_{\\mathcal{x } } \\left [ \\| \\mathbf{x } \\|^2 \\right ] + \\| \\mathbf{y}_i \\|^2 ,    \\end{aligned}\\ ] ] since @xmath48 = 0 $ ] by assumption . substituting this equality in yields @xmath49 + \\| \\mathbf{y}_2 \\|^2             \\right )            } ^ { \\operatorname{e}_{\\mathcal{x } } \\left [ \\| \\mathbf{x } - \\mathbf{y}_2 \\|^2 \\right ] }           -            \\overbrace {           \\left (           \\operatorname{e}_{\\mathcal{x } } [ \\| \\mathbf{x } \\|^2 ] + \\| \\mathbf{y}_1 \\|^2           \\right )           } ^ { \\operatorname{e}_{\\mathcal{x } } \\left [ \\| \\mathbf{x } - \\mathbf{y}_1 \\|^2 \\right ] }           =           \\| \\mathbf{y}_2 \\|^2 - \\| \\mathbf{y}_1 \\|^2           =           \\gamma \\sigma .",
    "\\label{eq : diff - variance }    \\end{aligned}\\ ] ]    now , it is well known that if a @xmath31-dimensional random vector @xmath50 follows the multivariate standard normal distribution @xmath51 , then its squared norm @xmath52 follows the chi - squared distribution with @xmath31 degrees of freedom , and its variance is @xmath53 . since @xmath54 ,",
    "the variance @xmath55 of the squared norm @xmath56 is @xmath57      = \\operatorname{var}_{\\mathcal{z } } \\left [ s^2 \\| \\mathbf{z } \\|^2 \\right ]      = s^4 \\operatorname{var}_{\\mathcal{z } } \\left [ \\| \\mathbf{z } \\|^2 \\right ]      = 2d s^4 .",
    "\\label{eq : norm - variance}\\ ] ] from and , we obtain @xmath58 .",
    "note that in proposition  [ prop : radovanovic ] , the standard deviation @xmath59 is used as a yardstick of measurement to allow for comparison of `` similarly '' located object pairs across different distributions ; two object pairs in different distributions are regarded as similar if objects in each pair are @xmath60 apart as measured by the @xmath59 for the respective distributions , but has an equal factor @xmath61 .",
    "this technique is due to radovanovi et  al .",
    "@xcite .",
    "now , @xmath42 represents the expected difference between the squared distances from @xmath21 to @xmath62 and @xmath63 .",
    "equation   shows that @xmath42 increases with @xmath61 , the factor quantifying the amount of difference @xmath64 .",
    "this suggests that a query object sampled from @xmath24 is more likely to be closer to object @xmath22 than to @xmath23 , if @xmath65 ; i.e. , @xmath22 is closer to the origin than @xmath23 is . because this holds for any pair of objects @xmath22 and @xmath23 in the dataset",
    ", we can conclude that the objects closest to the origin in the dataset tend to be hubs .",
    "equation   also states the relationship between @xmath42 and the component variance @xmath34 of distribution @xmath66 , by which the following is implied : for a fixed query distribution @xmath24 , if we have two distributions for @xmath39 , @xmath67 and @xmath68 with @xmath69 , it is preferable to choose @xmath70 , i.e. , the distribution with a smaller @xmath34 , when attempting to reduce hubness .",
    "indeed , assuming the independence of @xmath24 and @xmath66 , we can show that the influence of @xmath42 relative to the expected squared distance from @xmath21 to @xmath39 ( which is also subject to whether @xmath71 or @xmath72 ) , is weaker for @xmath70 than for @xmath72 , i.e. , @xmath73    }    <    \\frac {      \\delta(\\gamma , d , s_2 )    } {      \\operatorname{e}_{\\mathcal{x}\\mathcal{y}_2}[\\| \\mathbf{x } - \\mathbf{y}\\|^2 ]    } , \\ ] ] where we wrote @xmath42 explicitly as a function of @xmath61 , @xmath31 , and @xmath74 .",
    "in this section , we analyze the emergence of hubs in the nearest neighbor step of zsl . through the analysis",
    ", it is shown that hubs are promoted by the use of ridge regression in the existing formulation of zsl , i.e. , mapping source objects ( examples ) into the target ( label ) space .    as a solution",
    ", we propose using ridge regression in a direction opposite to that in existing work .",
    "that is , we project target objects in the space of source objects , and carry out nearest neighbor search in the source space .",
    "our argument for this approach consists of three steps .    1 .",
    "we first show in sect .",
    "[ sec : shrinkage ] that , with ridge regression ( and ordinary least squares as well ) , mapped observation data tend to lie closer to the origin than the target responses do . because the existing work formulates zsl as a regression problem that projects source objects into the target space , this means that the norm of the projected source objects tends to be smaller than that of target objects . 2 .   by combining the above result with the discussion of sect .",
    "[ sec : hubness ] , we then argue that placing source objects closer to the origin is not ideal from the perspective of reducing hubness . on the contrary ,",
    "placing target objects closer to the origin , as attained with the proposed approach , is more desirable ( sect .",
    "[ sec : two - eggs ] ) .",
    "3 .   in sect .",
    "[ sec : nn - balls ] , we present a simple additional argument against placing source objects closer to the origin ; if the data is unimodal , such a configuration increases the possibility of another target object falling closer to the source object .",
    "this argument diverges from the discussion on hubness , but again justifies the proposed approach .",
    "we first prove that ridge regression tends to map observation data closer to the origin of the space .",
    "this tendency may be easily observed in ridge regression , for which the penalty term shrinks the estimated coefficients towards zero .",
    "however , the above tendency is also inherent in ordinary least squares .",
    "let @xmath75 and @xmath76 respectively denote the frobenius norm and the 2-norm of matrices .    [",
    "prop : shrinkage ] let @xmath77 be the solution for ridge regression with an observation matrix @xmath78 and a response matrix @xmath79 ; i.e. , @xmath80 where @xmath81 is a hyperparameter . then , we have @xmath82 .",
    "it is well known that @xmath83 .",
    "thus we have @xmath84 let @xmath59 be the largest singular value of @xmath85 .",
    "it can be shown that @xmath86 substituting this inequality in establishes the proposition .",
    "recall that if the data is centered , the matrix 2-norm can be interpreted as an indicator of the variance of data along its principal axis .",
    "proposition  [ prop : shrinkage ] thus indicates that the variance along the principal axis of the mapped observations @xmath87 tends to be smaller than that of responses @xmath88 .",
    "furthermore , this tendency even persists in the ordinary least squares with no penalty term ( i.e. , @xmath89 ) , since @xmath90 still holds in this case ; note that @xmath91 is an orthogonal projection and its 2-norm is @xmath92 , but the inequality in holds regardless .",
    "this tendency therefore can not be completely eliminated by simply decreasing the ridge parameter @xmath93 towards zero .    in existing work on zsl , @xmath85 represents the ( training ) source objects @xmath94 \\in { \\mathbb{r}}^{c\\times n}$ ] , to be mapped into the space of target objects ( by projection matrix @xmath95 ) ; and @xmath88 is the matrix of labels for the training objects , i.e. , @xmath96\\in { \\mathbb{r}}^{d\\times n}$ ] .",
    "although proposition  [ prop : shrinkage ] is thus only concerned with the training set , it suggests that the source objects at the time of testing , which are not in @xmath97 , are also likely to be mapped closer to the origin of the target space than many of the target objects in @xmath98 .",
    "we learned in sect .",
    "[ sec : shrinkage ] that ridge regression ( and ordinary least squares ) shrink the mapped observation data towards the origin of the space , relative to the response .",
    "thus , in existing work on zsl in which source objects @xmath0 are projected to the space of target objects @xmath1 , the norm of the mapped source objects is likely to be smaller than that of the target objects .",
    "the proposed approach , which was described in the beginning of sect .",
    "[ sec : regression - hubness ] , follows the opposite direction : target objects @xmath1 are projected to the space of source objects @xmath0 .",
    "thus , in this case , the norm of the mapped target objects is expected to be smaller than that of the source objects .",
    "the question now is which of these configurations is preferable for the subsequent nearest neighbor step , and we provide an answer under the following assumptions : ( i ) the source space and the target space are of equal dimensions ; ( ii ) the source and target objects are isotropically normally distributed and independent ; and ( iii ) the projected data is also isotropically normally distributed , except that the variance has shrunk .",
    "let @xmath99 and @xmath100 be two multivariate normal distributions , with @xmath69 .",
    "we compare two configurations of source object @xmath21 and target objects @xmath39 : ( a ) the one in which @xmath101 and @xmath102 , and ( b ) the one in which @xmath103 and @xmath104 on the other hand ; here , the primes in ( b ) were added to distinguish variables in two configurations .",
    "these two configurations are intended to model situations in ( a ) existing work and ( b ) our proposal . in configuration ( a ) , @xmath21 is shorter in expectation than @xmath39 , and therefore this approximates the situation that arises from existing work .",
    "configuration ( b ) represents the opposite situation , and corresponds to our proposal in which @xmath39 is the projected vector and thus is shorter in expectation than @xmath21 .",
    "now , we aim to verify whether the two configurations differ in terms of the likeliness of hubs emerging , using proposition  [ prop : radovanovic ] .",
    "first , we scale the entire space of configuration ( b ) by @xmath105 , or equivalently , we consider transformation of the variables by @xmath106 and @xmath107 .",
    "note that because the two variables are scaled equally , this change of variables preserves the nearest neighbor relations among the samples .",
    "see fig .",
    "[ fig : illustration - hub ] for an illustration of the relationship among @xmath21 , @xmath39 , @xmath108 , @xmath109 , @xmath110 , and @xmath111 .     in two - dimensional space .",
    "the left and the right panels depict configurations  ( a ) and ( b ) , respectively , with the center panel showing both configuration ( a ) and the scaled version of configuration ( b ) in the same space .",
    "a circle represents a distribution , with its radius indicating the standard deviation .",
    "the radius of the circles for @xmath21 ( on the left panel ) and @xmath109 ( right panel ) is @xmath112 , whereas that of the circles for @xmath39 ( left panel ) and @xmath108 ( right panel ) is @xmath113 , with @xmath114 .",
    "circles @xmath110 and @xmath111 are the scaled versions of @xmath108 and @xmath109 such that the standard deviation ( radius ) of @xmath110 is equal to @xmath21 , which makes the standard deviation of @xmath115 equal to @xmath116 . ]",
    "let @xmath117 and @xmath118 be the components of @xmath108 and @xmath109 , respectively , and let @xmath119 and @xmath120 be those for @xmath110 and @xmath111 .",
    "then we have @xmath121 &                         = \\operatorname{var}\\left [ \\frac{s_1}{s_2 } x_i ' \\right ]                         = \\left(\\frac{s_1}{s_2}\\right)^2 \\operatorname{var } [ x'_i ]                         = s_1 ^ 2 ,    \\\\",
    "\\operatorname{var}[y''_i ] &                         = \\operatorname{var}\\left [ \\frac{s_1}{s_2}y'_i \\right ]                         = \\left(\\frac{s_1}{s_2}\\right)^2 \\operatorname{var } [ y'_i ]                         = \\frac{s_1 ^ 4}{s_2 ^ 2 } .\\end{aligned}\\ ] ] thus , @xmath110 follows @xmath122 , and @xmath111 follows @xmath123 . since both @xmath21 in configuration ( a ) and @xmath110 above follow the same distribution , it now becomes possible to compare the properties of @xmath39 and @xmath111 in light of the discussion at the end of sect .",
    "[ sec : hubness ] : in order to reduce hubness , the distribution with a smaller variance is preferred to the one with a larger variance , for a fixed distribution of source @xmath21 ( or equivalently , @xmath110 ) .",
    "it follows that @xmath111 is preferable to @xmath39 , because the former has a smaller variance .",
    "as mentioned above , the nearest neighbor relation between the scaled variables , @xmath111 against @xmath110 ( or equivalently @xmath21 ) , is identical to @xmath109 against @xmath108 in configuration  ( b ) .",
    "therefore , we conclude that configuration  ( b ) is preferable to configuration  ( a ) , in the sense that the former is more likely to suppress hubs .    finally , recall that the preferred configuration  ( b ) models the situation of our proposed approach , which is to map target objects in the space of source objects .      by assuming a unimodal data distribution of which the probability density function ( pdf ) @xmath124 is decreasing in @xmath125 , we are able to present the following proposition which also advocates placing the source objects outside the target objects , and not the other way around .    proposition  [ prop : nn - balls ]",
    "is concerned with the placement of a source object @xmath21 at a fixed distance @xmath126 from its target object @xmath39 , for which we have two alternatives @xmath127 and @xmath128 , located at different distances from the origin of the space .",
    "[ prop : nn - balls ] consider a finite set @xmath1 of objects ( i.e. , points ) in a euclidean space , sampled i.i.d . from a distribution",
    "whose pdf @xmath124 is a decreasing function of @xmath125 .",
    "let @xmath129 be an object in the set , and let @xmath130 .",
    "further let @xmath127 and @xmath128 be two objects at a distance @xmath126 apart from @xmath39 .",
    "if @xmath131 , then the probability that @xmath39 is the closest object in @xmath1 to @xmath128 is greater than that of @xmath127 .    for @xmath45 , if another object in @xmath1 appears within distance @xmath126 of @xmath8 , then @xmath39 is not the nearest neighbor of @xmath8 .",
    "thus , we aim to prove that this probability for @xmath128 is smaller than that for @xmath127 .",
    "since objects in @xmath1 are sampled i.i.d , it suffices to prove @xmath132 where @xmath133 ( @xmath45 ) denote the balls centered at @xmath8 with radius @xmath126 .",
    "however , obviously holds because the balls @xmath134 and @xmath135 have the same radii , @xmath124 is a decreasing function of @xmath125 , and @xmath136 .",
    "see figure  [ fig : illustration ] for an illustration with a bivariate standard normal distribution in two - dimensional space .    . here",
    ", it is assumed that @xmath137 and @xmath138 .",
    "the intensity of the background shading represents the values of the pdf of a bivariate standard normal distribution , from which @xmath39 and other objects ( not depicted in the figure ) in set @xmath1 are sampled .",
    "the probability mass inside the circle centered at @xmath127 is greater than that centered at @xmath128 , as the intensity of the shading inside the two circles shows . ]    in the context of existing work on zsl , which uses ridge regression to map source objects in the space of target objects , @xmath21 can be regarded as a mapped source object , and @xmath39 as its target object .",
    "proposition  [ prop : nn - balls ] implies that if we want to make a source object @xmath21 the nearest neighbor of a target object @xmath39 , it should rather be placed farther than @xmath39 from the origin , but this idea is not present in the objective function   for ridge regression ; the first term of the objective allocates the same amount of penalty for @xmath127 and @xmath128 , as they are equally distant from the target @xmath39 . on the contrary",
    ", the ridge regression actually _ promotes _ placement of the mapped source object @xmath21 closer to the origin , as stated in proposition  [ prop : shrinkage ] .      drawing on the analysis presented in sections  [ sec : shrinkage][sec : nn - balls ] , we propose performing regression that maps",
    "_ target _ objects in the space of _ source _ objects , and carry out nearest neighbor search in the source space .",
    "this opposes the approach followed in existing work on regression - based zsl @xcite , which maps source objects into the space of target objects .    in the proposed approach , matrix @xmath88 in proposition  [ prop : shrinkage ] represents the source objects @xmath97 , and @xmath85 represents the target objects @xmath98 .",
    "therefore , @xmath90 means @xmath139 , i.e. , the mapped target objects tend to be placed closer than the corresponding source objects to the origin .    admittedly ,",
    "the above argument for our proposal relies on strong assumptions on data distributions ( such as normality ) , which do not apply to real data .",
    "however , the effectiveness of our proposal is verified empirically in sect .",
    "[ sec : experiment ] by using real data .",
    "the first use of ridge regression in zsl can be found in the work of palatucci et  al .",
    "ridge regression has since been one of the standard approaches to zsl , especially for natural language processing tasks : phrase generation @xcite and bilingual lexicon extraction @xcite .",
    "more recently , neural networks have been used for learning non - linear mapping @xcite .",
    "all of the regression - based methods listed above , including those based on neural networks , map source objects into the target space .",
    "zsl can also be formulated as a problem of _ canonical correlation analysis _ ( cca ) .",
    "hardoon et .",
    "@xcite used cca and kernelized cca for image labeling .",
    "lazaridou et .",
    "@xcite compared ridge regression , cca , singular value decomposition , and neural networks in image labeling . in our experiments ( sect .",
    "[ sec : experiment ] ) , we use cca as one of the baseline methods for comparison .    dinu and baroni @xcite reported the hubness phenomenon in zsl .",
    "they proposed two reweighting techniques to reduce hubness in zsl , which are applicable to cosine similarity .",
    "tomaev et  al .",
    "@xcite proposed hubness - based instance weighting schemes for cca .",
    "these schemes were applied to classification problems in which multiple instances ( vectors ) in the target space have the same class label",
    ". this setting is different from the one assumed in this paper ( see sect .  [",
    "sec : zsl ] ) , i.e. , we assume that a class label is represented by a single target vector .. ]    _ structured output learning _",
    "@xcite addresses a problem setting similar to zsl , except that the target objects typically have complex structure , and thus the cost of embedding objects in a vector space is prohibitive .",
    "_ kernel dependency estimation _",
    "@xcite is an approach that uses kernel pca and regression to avoid this issue . in this context ,",
    "nearest neighbor search in the target space reduces to the _ pre - image _ problem @xcite in the implicit space induced by kernels .",
    "we evaluated the proposed approach with both synthetic and real datasets . in particular , it was applied to two real zsl tasks : bilingual lexicon extraction and image labeling .",
    "the main objective of the following experiments is to verify whether our proposed approach is capable of suppressing hub formation and outperforming the existing approach , as claimed in sect .",
    "[ sec : regression - hubness ] .",
    "the following methods were compared .",
    "* @xmath140 : linear ridge regression mapping source objects @xmath0 into the space of target objects @xmath1 .",
    "this is how ridge regression was used in the existing work on zsl @xcite .",
    "* @xmath141 : linear ridge regression mapping target objects @xmath1 into the source space .",
    "this is the proposed approach ( sect .",
    "[ sec : proposed - method ] ) . * cca : canonical correlation analysis ( cca ) for zsl @xcite .",
    "we used the code available from http://www.davidroihardoon.com/professional/code.html .",
    "we calibrated the hyperparameters , i.e. , the regularization parameter in ridge regression and the dimensionality of common feature space in cca , by cross validation on the training set .",
    "after ridge regression or cca is applied , both x and y ( or their images ) are located in the same space , wherein we find the closest target object for a given source object as measured by the euclidean distance .",
    "in addition to the euclidean distance , we also tested the _ non - iterative contextual dissimilarity measure _",
    "( nicdm ) @xcite in combination with @xmath140 and cca .",
    "nicdm adjusts the euclidean distance to make the neighborhood relations more symmetrical , and is known to effectively reduce hubness in non - zsl context @xcite .",
    "all data were centered before application of regression and cca , as usual with these methods .",
    "the compared methods were evaluated in two respects : ( i ) the correctness of their prediction , and ( ii ) the degree of hubness in nearest neighbor search .",
    "[ [ measures - of - prediction - correctness . ] ] measures of prediction correctness .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in all our experiments , zsl was formulated as a ranking task ; given a source object , all the target objects were ranked by their likelihood for the source object . as the main evaluation criterion , we used the mean average precision ( map ) @xcite , which is one of the standard performance metrics for ranking methods . note that the synthetic and the image labeling experiments are the single - label problems for which map is equal to the mean reciprocal rank @xcite .",
    "we also report the top-@xmath142 accuracy ) _ macro - averaged _ over classes , to allow direct comparison with published results .",
    "note also that @xmath143 with a larger @xmath142 would not be an informative metric for the image labeling task , which only has 10 test labels . ]",
    "( @xmath143 ) for @xmath144 and @xmath145 , which is the percentage of source objects for which the correct target objects are present in their @xmath142 nearest neighbors .    [",
    "[ measure - of - hubness . ] ] measure of hubness .",
    "+ + + + + + + + + + + + + + + + + + +    to measure the degree of hubness , we used the _ skewness _ of the ( empirical ) @xmath146 distribution , following the approach in the literature @xcite .",
    "the @xmath146 distribution is the distribution of the number @xmath147 of times each target object @xmath148 is found in the top @xmath142 of the ranking for source objects , and its skewness is defined as follows : @xmath149 \\right)^3 / \\ell } { \\operatorname{var}\\left [   n_k \\right]^\\frac{3}{2}}\\ ] ] where @xmath150 is the total number of test objects in @xmath1 , @xmath147 is the number of times the @xmath148th target object is in the top-@xmath142 closest target objects of the source objects .",
    "a large @xmath146 skewness value indicates the existence of target objects that frequently appear in the @xmath142-nearest neighbor lists of source objects ; i.e. , the emergence of hubs .",
    "we tested our method on the following zsl tasks .      to simulate a zsl task ,",
    "we need to generate object pairs across two spaces in a way that the configuration of objects is to some extent preserved across the spaces , but is not exactly identical . to this end",
    ", we first generated 3000-dimensional ( column ) vectors @xmath151 for @xmath152 , whose coordinates were generated from an i.i.d .",
    "univariate standard normal distribution .",
    "vectors @xmath153 were treated as _ latent _ variables , in the sense that they were not directly observable , but only their images @xmath8 and @xmath9 in two different features spaces were .",
    "these images were obtained via different random projections , i.e. , @xmath154 and @xmath155 , where @xmath156 are random matrices whose elements were sampled from the uniform distribution over @xmath157 $ ] . because random projections preserve the length and the angle of vectors in the original space with high probability @xcite , the configuration of the projected objects is expected to be similar ( but different ) across the two spaces . finally , we randomly divided object pairs @xmath158 into the training set ( 8000 pairs ) and the test set ( remaining 2000 pairs ) .",
    "our first real zsl task is bilingual lexicon extraction @xcite , formulated as a ranking task : given a word in the source language , the goal is to rank its gold translation ( the one listed in an existing bilingual lexicon as the translation of the source word ) higher than other non - translation candidate words .    in this experiment",
    ", we evaluated the performance in the tasks of finding the english translations of words in the following source languages : czech ( cs ) , german ( de ) , french ( fr ) , russian ( ru ) , japanese ( ja ) , and hindi ( hi ) .",
    "thus , in our setting , each of these six languages was used as @xmath0 alternately , whereas english was the target language @xmath1 throughout . and other languages as @xmath1 .",
    "the results are not presented here due to lack of space , but the same trend was observed . ]",
    "following related work @xcite , we trained a cbow model @xcite on the pre - processed wikipedia corpus distributed by the polyglot project ( see @xcite for corpus statistics ) , using the word2vec tool .",
    "the window size parameter of word2vec was set to 10 , with the dimensionality of feature vectors set to 500 .    to learn the projection function and measure the accuracy in the test set",
    ", we used the bilingual dictionaries of cs  et  al .",
    "@xcite as the gold translation pairs .",
    "these gold pairs were randomly split into the training set ( 80% of the whole pairs ) and the test set ( 20% ) .",
    "we repeated experiments on four different random splits , for which we report the average performance .",
    "the second real task is image labeling , i.e. , the task of finding a suitable word label for a given image .",
    "thus , source objects @xmath0 are the images and target objects @xmath1 are the word labels .",
    "we used the animal with attributes ( awa ) dataset , which consists of 30,475 images of 50 animal classes .",
    "for image representation , we used the decaf features @xcite , which are the 4096-dimensional vectors constructed with convolutional neural networks ( cnns ) .",
    "decaf is also available from the awa website . to save computational cost",
    ", we used random projection to reduce the dimensionality of decaf features to 500 .",
    "as with the bilingual lexicon extraction experiment , label features ( word representations ) were constructed with word2vec , but this time they were trained on the english version of wikipedia ( as of march 4 , 2015 ) to cover all awa labels .",
    "except for the corpus , we used the same word2vec parameters as with bilingual lexicon extraction .",
    "we respected the standard zero - shot setup on awa provided with the dataset ; i.e. , the training set contained 40 labels , and test set contained the other 10 labels .",
    "table  [ tab : result ] shows the experimental results .",
    "the trends are fairly clear : the proposed approach , @xmath141 , outperformed other methods in both map and @xmath143 , over all tasks .",
    "@xmath140 and cca combined with nicdm performed better than those with euclidean distances , although they still lagged behind the proposed method @xmath141 even with nicdm .",
    "the @xmath159 skewness achieved by @xmath141 was lower ( i.e. , better ) than that of compared methods , meaning that it effectively suppressed the emergence of hub labels .",
    "in contrast , @xmath140 produced a high skewness which was in line with its poor prediction accuracy .",
    "these results support the expectation we expressed in the discussion in sect .",
    "[ sec : regression - hubness ] .",
    "the results presented in the tables show that the degree of hubness ( @xmath146 ) for all tested methods inversely correlates with the correctness of the output rankings , which strongly suggests that hubness is one major factor affecting the prediction accuracy .    for the awa image dataset , akata et .",
    "* the fourth row ( cnn ) and second column ( @xmath160 ) of table  2 ) reported a 39.7% @xmath161 score , using image representations trained with cnns , and 100-dimensional word representations trained with word2vec .",
    "for comparison , our proposed approach , @xmath141 , was evaluated in a similar setting : we used the decaf features ( which were also trained with cnns ) without random projection as the image representation , and 100-dimensional word2vec word vectors . in this setup , @xmath141 achieved a 40.0% @xmath161 score .",
    "although the experimental setups are not exactly identical and thus the results are not directly comparable , this suggests that even linear ridge regression can potentially perform as well as more recent methods , such as akata et  al.s , simply by exchanging the observation and response variables .",
    "this paper has presented our formulation of zsl as a regression problem of finding a mapping from the target space to the source space , which opposes the way in which regression has been applied to zsl to date . assuming a simple model in which data follows a multivariate normal distribution",
    ", we provided an explanation as to why the proposed direction is preferable , in terms of the emergence of hubs in the subsequent nearest neighbor search step .",
    "the experimental results showed that the proposed approach outperforms the existing regression - based and cca - based approaches to zsl .",
    "future research topics include : ( i ) extending the analysis of sect .",
    "[ sec : regression - hubness ] to cover multi - modal data distributions , or other similarity / distance measures such as cosine ; ( ii ) investigating the influence of mapping directions in other regression - based zsl methods , including neural networks ; and ( iii ) investigating the emergence of hubs in cca ."
  ],
  "abstract_text": [
    "<S> this paper discusses the effect of hubness in zero - shot learning , when ridge regression is used to find a mapping between the example space to the label space . </S>",
    "<S> contrary to the existing approach , which attempts to find a mapping from the example space to the label space , we show that mapping labels into the example space is desirable to suppress the emergence of hubs in the subsequent nearest neighbor search step . assuming a simple data model , we prove that the proposed approach indeed reduces hubness . </S>",
    "<S> this was verified empirically on the tasks of bilingual lexicon extraction and image labeling : hubness was reduced with both of these tasks and the accuracy was improved accordingly . </S>"
  ]
}