{
  "article_text": [
    "recent flourish of studies on compressed sensing inspires researchers to apply the idea for applications in wireless communications .",
    "fletcher et al .",
    "@xcite showed that a detection problem for on - off random access channels is equivalent a sparse signal recovery problem discussed in compressed sensing .",
    "kaneko et al .",
    "@xcite used a compressed sensing technique to develop an identification protocol for a passive rfid system .",
    "for such an application in wireless communications , we need powerful sparse signal recovery algorithms that are suitable for hardware implementation in order to achieve high speed signal processing and energy efficiency .    _ binary ( or one - bit ) compressed sensing _ firstly proposed by boufounos and baraniuk @xcite is a variant of the original compressed sensing . in the scenario of binary compressed",
    "sensing , a linear observation vector that is quantized to a binary value , typically in the binary alphabet @xmath1 , is given to a receiver .",
    "the linear observation vector is the product of a sensing matrix and a hidden sparse signal vector .",
    "a receiver then tries to reconstruct the original sparse signal .",
    "this process is called a _ sparse signal recovery _ process .",
    "it is known that power consumption of an ad converter is closely related to the number of its quantization levels and the sampling frequency , namely , the power consumption of an ad converter increases as the sampling frequency increases .",
    "when we pursue to develop an extremely low power consumption device for battery - powered sensors or to develop a digital signal processing device operating at very high sampling frequency , binary quantization by a comparator is a reasonable choice .",
    "binary compressed sensing is well suited to such a situation .",
    "moreover , since the input to the receiver is restricted to binary values , no gain control is required in the case of binary compressed sensing .",
    "this fact further simplifies the hardware needed in the receiver .    the optimal sparse signal recovery for binary compressed sensing can be attained by solving a certain integer programming problem .",
    "however , the ip problem is , in general , computationally hard to solve and the approach can handle only small problems . boufounos and baraniuk @xcite studied a relaxation method that replaces @xmath2-norm with @xmath0-norm and introduced a convex relaxation for integer constraints .",
    "although nonlinearity induced by binary quantization prohibits direct applications of known sparse recovery algorithms for the original compressed sensing , several sparse recovery algorithms for binary compressed sensing has been developed @xcite",
    "@xcite @xcite based on the known iterative methods for the original compressed sensing .",
    "boufounos @xcite proposed a greedy algorithm called matched sign pursuit ( msp ) that is a counter part of orthogonal matching pursuit ( omp ) .",
    "the paper @xcite presents binary iterative hard thresholding ( biht ) algorithm by reforming iterative hard thresholding ( iht ) algorithm @xcite .",
    "although the known sparse recovery algorithms exhibit reasonable sparse recovery performance , it may not be suitable for applications in high speed wireless communications .",
    "this is because most of algorithms require a number of iterations to achieve reasonable sparse recovery results .",
    "most of known algorithms also require calculations involving matrix - vector products with @xmath3-computations for each iteration where @xmath4 is the length of the sparse signal vector .",
    "our approach for sparse signal recovery is to employ _ feedforward neural networks _ as building blocks of sparse signal recovery schemes .",
    "we expect that an appropriately designed and trained neural networks greatly reduce required computing resources and are well suited to hardware implementation . in this paper",
    ", we will propose the majority voting neural networks composed of several independently trained neural networks , which are feedforward 3-layer neural networks employing the sigmoid function as an activation function .",
    "as far as we know , there is no previous studies on sparse signal recovery based on neural networks .",
    "our focus is thus not only on the practical aspect of the neural sparse signal recovery but also on studies on the fundamental behavior of the neural networks for sparse signal recovery .",
    "recently , _ deep neural networks _ @xcite have been actively studied because they provide surprisingly excellent performance in the areas of image / speech recognition and natural language processing @xcite .",
    "such powerful neural networks can be used in wireless communication as well .",
    "this work can be seen as a first attempt to this direction .",
    "the main problem for binary compressed sensing is to reconstruct an unknown sparse signal vector @xmath5 from the observation signal vector @xmath6 under the condition that these signals satisfy the relationship : @xmath7 the sign function @xmath8 is defined by @xmath9 the matrix @xmath10 is called a _ sensing matrix_. we assume that the length of the observation signal vector @xmath11 is smaller than the length of the sparse signal vector @xmath12 , i.e. , @xmath13 .",
    "this problem setup is similar to that of the original compressed sensing .",
    "the notable difference between them is that the observation signal @xmath11 is binarized in a sensing process of binary compressed sensing .",
    "a receiver obtains the observation signal @xmath11 and then it tries to recover the corresponding hidden signal @xmath12 .",
    "we here make two assumptions for the signal @xmath12 and the sensing matrix @xmath14 .",
    "the first assumption is sparsity of the hidden signal @xmath12 .",
    "the original binary signal @xmath15 contains only @xmath16 non - zero elements , where @xmath16 is a positive integer much smaller than @xmath4 , i.e. , hamming weight of @xmath12 should be @xmath16 .",
    "we call the set of binary vectors with hamming weight @xmath16 is @xmath16-_sparse signals_. the second assumption is that the receiver completely knows the sensing matrix @xmath14",
    ".      when we need an extremely high speed signal processing or an energy - efficient sparse signal processing method for battery powered sensor , it would be reasonable to develop a sparse signal recovery algorithm suitable for the situation . in the sparse signal recovery method based on neural networks to be described in this section",
    "requires only several matrix - vector products to obtain an output signal , which is an estimate signal of the sparse vector @xmath12 .",
    "thus , the proposed method needs smaller computational costs than those required by conventional iterative methods",
    ".     comes from the left and is fed to the input layer .",
    "a sparse estimation vector comes out from the output layer .",
    "the sigmoid function is used as an activation function.,width=264 ]    our sparse recovery method is based on a 3-layer feedforward neural network illustrated in fig.[fig : nnmodel ] .",
    "this architecture is fairly common one ; it consists of the input , hidden and output layers .",
    "adjacent layers are connected by weighted edges and each layer includes neural units that can keep real values . as an _ activation function _ ,",
    "we employed the sigmoid function to determine the values of the hidden and output layers . in our problem setting , the observation signal @xmath11 is fed into the input layer from the left in fig.[fig : nnmodel ] .",
    "the signal propagates from left to right and the output signal @xmath17 eventually comes out from the output layer . the network should be trained so that the output signal @xmath17 is an accurate estimation of the original sparse signal @xmath12 .",
    "the precise description of the network in fig.[fig : nnmodel ] is given by the following equations : @xmath18 the function @xmath19 is the sigmoid function defined by @xmath20 in this paper , we will follow a simple convention that @xmath21 represents @xmath22 where @xmath23 .",
    "the round function @xmath24 gives the nearest integer from @xmath25 .",
    "the equation ( [ hyde ] ) defines the signal transformation from the input layer to the hidden layer .",
    "an affine transformation is firstly applied to the input signal @xmath26 and then the sigmoid function is applied .",
    "the weight matrix @xmath27 and the bias vector @xmath28 defines the affine transformation .",
    "the resulting signal @xmath29 is kept in the units in the hidden layer , which are called _ hidden units_.",
    "the parameter @xmath30 thus means the number of hidden units . from the hidden layer to the output layer , the equation ( [ output ] ) governs the second signal transformation . the second transformation to yield the signal @xmath17 consists of the affine transformation , based on the weight matrix @xmath31 and the bias vector @xmath32 , and the nonlinear mapping based on the sigmoid function .",
    "the vector @xmath33 emitted from the output layer is finally rounded to a nearest integer vector because we assumed that non - zero elements in the original sparse signal @xmath12 takes the value one .",
    "since the range of the sigmoid function lies in the open interval @xmath34 , an element in the estimate vector @xmath35 should take the value zero or one .",
    "the network in fig.[fig : nnmodel ] can be seen as a parametrized estimator @xmath36 where @xmath37 is the set of the trainable parameters @xmath38 .",
    "it is expected that the trainable parameter @xmath37 should be adjusted in the training phase so as to minimize the error probability @xmath39 $ ] .",
    "however , it may be computationally intractable to minimize the error probability directly .",
    "instated of direct minimization of the error probability itself , we will minimize a loss function including a cross entropy - like loss function and an @xmath0-regularization term . in this subsection",
    ", the details of the training process is described .    in the training phase of the network ,",
    "the parameter @xmath37 should be updated in order to minimize the values of the given loss function .",
    "let @xmath40 be the set of the training data used in the training phase .",
    "the signals @xmath41 and @xmath42 relate as @xmath43 . in the training process , we use randomly generated training samples ; the sparse vectors @xmath44 are generated uniformly at random from the set of @xmath16-sparse vectors .",
    "the sample @xmath41 is fed into the network and the corresponding sample @xmath42 is used as the _",
    "supervisory signal_.    we here employ _",
    "stochastic gradient descent ( sgd ) algorithms _ to minimize the loss function described later .",
    "it is empirically known that sgd and its variations behave very well for non - convex objective functions that are computationally hard to minimize .",
    "this is the reason why sgd and related algorithms are widely used for training deep neural networks . in order to use sgd , we need to partition the training set into minibatches .",
    "a minibatch is a subset of the training data and the use of minibatches introduces stochastic disturbance in training processes .",
    "such stochastic disturbance helps a search point in an sgd process to escape from a stationary point of the non - convex objective function to be minimized .",
    "we divide the training data into a number of minibatches as follows : @xmath45 in this case , every minibatch contains @xmath46-pair of samples .",
    "we denote @xmath16-th @xmath47 , minibatch as @xmath48 .",
    "the choice of the loss function , i.e. , the objective function to be minimized in training processes , is crucial to achieve appropriate recovery performance .",
    "we introduce a loss function designed for sparse signal recovery .",
    "the loss function of @xmath16-th minibatch is defined by @xmath49 the vector @xmath50 is given by @xmath51 i.e , the output of our neural network corresponding to the input @xmath41 .",
    "the first term of @xmath52 measures closeness between @xmath53 and @xmath54 .",
    "this measure is closely related to _",
    "cross entropy _ that are often used in supervised classification problems . in the case of a classification problem",
    ", @xmath42 is a one - hot vector that can be interpreted as a probability vector . in our case , since @xmath54 contains @xmath16-ones , thus the first term is not the same as the cross entropy .",
    "it has been empirically observed that this term plays an important role for sparse signal recovery .",
    "for example , from several numerical experiments indicated that the @xmath55-distance between @xmath53 and @xmath54 is not suitable for our purpose .",
    "the second term of equation ( [ loss_k ] ) is the @xmath0-_regularization term _ for promoting sparsity of the output @xmath17 .",
    "the regularization parameter @xmath56 adjusts effectiveness of regularization .",
    "some experiments showed that the @xmath0-regularization term is indispensable for obtaining sparse output vector .",
    "the training process of our network can be summarized as follows .",
    "we first generate the training data @xmath57 . in the @xmath16-th update iteration of the parameter @xmath37 , the minibatch @xmath48 is fed to the adam optimizer @xcite based on the loss function ( [ loss_k ] ) .",
    "the adam optimizer is a variant of sgd that provides fast convergence in many cases and it is widely used in learning process for deep neural networks .",
    "an iteration corresponding to a process for a minibatch is called a _ learning step_. a training process finishes when all the minibatches are processed .",
    "as the primal performance measure of sparse signal reconstruction , we adopt the _ recovery rate _ which is the probability of the event @xmath58 under the assumption where @xmath12 is chosen uniformly at random from the set of @xmath16-sparse binary vectors . in this section",
    ", we evaluate the sparse signal recovery performance of our feedforward neural networks in fig .",
    "[ fig : nnmodel ] .",
    "we used tensorflow @xcite to implement and to train our neural networks .",
    "tensorflow is a framework designed for distributed data flow based numerical calculations that is especially well suited for training of deep neural networks .",
    "tensorflow supports automatic back propagation for computing the gradient vectors required for the parameter updates and it also provides gpu - computing that can significantly accelerate training processes .",
    "it is rather straightforward to implement our neural networks and the training process descried in the previous section by using tensorflow .",
    "the details of parameters used throughout the paper are following .",
    "the length of the original sparse signal @xmath12 is set to @xmath59 .",
    "the sensing matrix @xmath10 is generated at random , i.e. , each element of @xmath14 is independently generated according to gaussian distribution with mean @xmath60 and variance @xmath61 .",
    "the sensing matrix is generated just before an experiment and fixed during an experiment . at the beginning of a training process , the weight matrices @xmath62 and @xmath63 at the hidden and output layers are initialized based on pseudo random numbers , namely each element of these matrices follows gaussian distribution with mean @xmath60 and variance @xmath64 .",
    "the bias vectors @xmath65 and @xmath66 at the hidden and output layers are initialized to the zero vector .",
    "the number of hidden units is set to @xmath67 and the minibatch size is set to @xmath68 .",
    "the initial value of the learning coefficient of adam optimizer is @xmath69 and the coefficient of regularization term is set to @xmath70 .      as an example of sparse signal reconstruction via our neural network , we show a @xmath71-sparse vector @xmath12 and the corresponding output of the trained neural network @xmath72 in fig.[fig : nn_output ] .",
    "the parameter @xmath73 is obtained by a training process with @xmath74 learning steps , i.e. , minibatches . in this experiment",
    ", we set the length of the observation signal to @xmath75 .",
    "-sparse vector .",
    "( top : the original sparse signal @xmath12 , bottom : the output @xmath72 from the trained neural network .",
    "@xmath76 ) , width=302 ]    from fig.[fig : nn_output ] , we can observe that the output @xmath17 shows fairly good match with the original signal @xmath12 .",
    "for example , the support of components in @xmath17 with the value larger than @xmath77 exactly coincides with the support of @xmath12 .",
    "some of the components have the value pretty close to @xmath61 as well .",
    "it is also seen that some of components with small values , e.g. , at indices around 110 and 145 , incur false positive which means that the corresponding components in the original @xmath12 are zero .",
    "this is the reason why we introduced the round function at the final stage of our neural network in ( [ round ] ) .",
    "it is expected that the round function eliminates the effect of the components with small values that may produce false positive elements in the final estimation @xmath35 .",
    "fig.[fig : rr_steps ] indicates a relationship between the recovery rate and the learning steps .",
    "the parameters are @xmath78 . from fig.[fig :",
    "rr_steps ] , we can see that the recovery rate increases as the number of learning steps increases although the progress contains fluctuations .",
    "the recovery rate appears to be saturated around @xmath79 steps . in following experiments ,",
    "the number of learning steps is thus set to @xmath74 based on this observation .    ,",
    "width=317 ]      we introduce _ integer programming ( ip)-based sparse signal recovery _ as a performance benchmark in the subsequent subsections because it provides the optimal recovery rate . the ip formulation shown here",
    "is based on the linear programming formulation in @xcite .",
    "although ip - based sparse signal recovery requires huge computer resources , it is applicable to moderate size problems if we employ a recent advanced ip solver .",
    "we used ibm cplex optimizer for solving the ip problem shown below .",
    "the problem needed to solve is to find a feasible binary vector @xmath80 satisfying the following conditions : @xmath81,\\quad \\sum_{j = 1}^n a_{i , j } z_j > 0,\\quad \\mbox{if } u_i = + 1 , \\\\ & & \\forall i \\in [ 1,m],\\quad \\sum_{j = 1}^n a_{i , j } z_j \\le 0,\\quad \\mbox{if } u_i = -1 , \\end{aligned}\\ ] ] where @xmath82 is the @xmath83 element of the sensing matrix @xmath14 and @xmath84 is the @xmath85-th element of the observation signal @xmath11 .",
    "if a feasible solution @xmath86 satisfying all the above conditions exists , it becomes an estimate @xmath87 .",
    "it is clear that these conditions are consistent with our setting of binary compressed sensing .",
    "fig.[fig : rr_normal ] presents the recovery rates of the proposed scheme under the condition where @xmath88 .",
    "the neural network shown in fig.[fig : nnmodel ] was used .    ) as benchmarks , recovery rates of ip - based scheme is also included ( denoted by * ip * ) . ]    in fig.[fig : rr_normal ] , it is seen that the recovery rate tend to increase as @xmath89 increases for all @xmath16 .",
    "the recovery rate is beyond @xmath90 at @xmath75 when the original signal is @xmath91-sparse",
    ". it can be also observed that the recovery rate strongly depends on the sparseness parameter @xmath16 .",
    "for example , the recovery rate of @xmath92 comes around @xmath93 at @xmath75 .",
    "the ip - based sparse recovery provides the recovery rate @xmath94 at @xmath95 when the original signal is @xmath71-sparse vector . on the other hand ,",
    "our neural network yields the recovery rate more than @xmath90 at @xmath96 when the original signal is @xmath71-sparse .",
    "although computation costs of the neural network in the recovery phase are much smaller than those required for ip - based sparse recovery , the performance gap appears rather huge and the neural - based reconstruction should be further improved .",
    "in the previous section , we saw that neural - based sparse signal recovery is successful under some parameter setting but there are still much room for improvement in terms of the recovery performance . in this section",
    ", we will propose a promising variant of the feedforward neural networks which is called _ majority voting neural networks_. the majority voting neural network consists of several independently trained neural networks .",
    "the outputs from these neural network are combined by soft majority voting nodes and the final estimation vector is obtained by rounding the output from the soft majority voting nodes . combining a several neural networks to obtain improved performance is not a novel idea , e.g. , @xcite , but it will be shown that the idea is very effective for our purpose .      from statistics of reconstruction errors occurred in our computer experiments , we observed that many reconstruction error events ( i.e. , @xmath97 ) occur due to only one symbol mismatch .",
    "in addition to this observation , we also found that independently trained neural networks tend to make symbol errors at distict positions .",
    "these observations inspire us to use majority voting to combine several outputs from independently trained neural networks .",
    "figure [ fig : mvnn ] presents the architecture of the majority voting neural networks . in this case , the majority voting neural network consists of @xmath98 component feedforward neural networks defined by @xmath99 where @xmath100 . the output of the component neural networks are aggregated by the soft majority logic nodes and it yields the estimation vector : @xmath101 where @xmath102 is the the threshold function defined by @xmath103 in the following experiments , we set @xmath104 . each component network was trained independently .",
    "this means that the training sets were independently generated for each component network .",
    "independently trained feedforward neural networks .",
    ", width=317 ]      the simple architecture of the majority voting neural networks is advantageous for both software and hardware implementations . in a case of software implementation , computation time required for matrix - vector products",
    "are dominant in a recovery process . for computing @xmath105",
    ", we need approximately @xmath106 basic arithmetic operations such as additions and multiplications .",
    "since there are @xmath98-components , approximately @xmath107 basic arithmetic operations are required for computing the output .",
    "this number appears competitive to known iterative methods @xcite because , in most iterative algorithms , @xmath3-basic operations are required for each iteration to compute @xmath108 where @xmath10 and @xmath109 . for a case of hardware implementation ,",
    "_ parallelism in the architecture _ of the majority voting neural networks possibly enables us to create high speed sparse recovery circuits on fpga or asic .",
    "note that implementation of neural networks with fpga is recently becoming a hot research topic @xcite .",
    "fig.[fig : rr_and_m_k6 ] presents comparisons of the recovery rates of the majority voting neural networks .",
    "the length of the sparse signal is set to @xmath59 and the sparseness parameter is set to @xmath110 .    ) , width=317 ]    from fig.[fig : rr_and_m_k6 ] , we can observe significant improvement in recovery performance compared with the performance of the single neural network . a single feedforward neural network discussed in the previous section",
    "provides recovery rate around @xmath111 at @xmath112 . on the other hand , the majority voting neural networks with 3 component nets achieves @xmath113 at @xmath112 .",
    "the majority nets with 5 components shows further improvement to @xmath94 at @xmath114 .",
    "this result implies that the soft majority voting process introduced in this section is effective to improve the reconstruction performance .",
    "another implication obtained from this result is that independently trained nets tend to have different estimation error patterns .",
    "this property explains the improvement in recovery rate observed in this experiment .",
    "we can see that there is still gap between the curves of the ip - based sparse recovery and the majority voting nets with @xmath115 .",
    "the gap might be considered as the price we need to pay for obtaining reduction in required computing resources .",
    "fig.[fig : rr_and_m_k8 ] also shows the recovery rates when @xmath116 .",
    "the length of the original signal is @xmath59 . in fig.[fig : rr_and_m_k8 ] , we can see the same tendency that has been observed in the previous experimental result in fig.[fig : rr_and_m_k6 ] .",
    "the performance of sparse recovery tends to increase as the number of component networks grows . at @xmath112 ,",
    "the recovery rate is improved from @xmath117 ( with the single network ) to @xmath113 ( with @xmath118 component networks ) .    ,",
    "width=317 ]    table [ comptime ] presents statistics on computation time required for sparse recovery of @xmath119 instances for @xmath120 .",
    "it can be seen that sparse recovery algorithms based on neural networks runs order of several magnitude faster than the ip sparse recovery method .",
    "of course , computation time depends on the computing environment and implementation but the result can be seen as an evidence that supports our claim that the proposed network structure is advantageous to reduce required computing resources .",
    ".computation time ( in second ) for sparse recovery of @xmath119 instances ( @xmath121 ) .",
    "the parameter @xmath98 represents the number of component networks [ cols=\">,>,>,>,>\",options=\"header \" , ]     the processor is intel core i7 - 3770k cpu(3.50ghz , 8-cores ) and the memory size is 7.5 gbytes .",
    "in this paper , we proposed sparse signal recovery schemes based on neural networks for binary compressed sensing .",
    "our empirical study shows a choice of the loss function used for training neural networks is of prime importance to achieve excellent reconstruction performance .",
    "we found a loss function suitable for this purpose , which includes a cross entropy like term and an @xmath0 regularized term .",
    "the majority voting neural network proposed in this paper is composed from several independently trained feedforward neural networks . from the experimental results",
    ", we observed that the majority voting neural network achieves excellent recovery performance , which is approaching the optimal ip - based performance as the number of component nets grows .",
    "the simple architecture of the majority voting neural network would be beneficial for both software and hardware implementation .",
    "it can be expected that high speed sparse signal recovery circuits based on the neural networks produce novel applications in wireless communications such as multiuser detection in multiple access channels .",
    "the present study was supported by grant - in - aid for scientific research ( b ) ( grant number 16h02878 ) from jsps . we used the optimization problem solver cplex optimizer and the distributed numerical computation framework tensorflow in this work .",
    "we gratefully acknowledge ibm academic initiative and google .    99 a. k. fletcher , s. rangan and v. k. goyal , `` on - off random access channels : a compressed sensing framework , '' arxiv:0903.1022 , 2009 .",
    "m. kaneko , w. hu , k. hayashi and h. sakai , `` compressed sensing - based tag identification protocol for a passive rfid system , '' ieee commun .",
    "lett . , vol .",
    "11 , pp . 20232026 , 2014 .",
    "p. boufounos and r. baraniuk , `` 1-bit compressive sensing , '' 42nd annual conference on information sciences and systems ( ciss ) , pp . 1621 , 2008 . y. plan and r. vershynin , `` one - bit compressed sensing by linear programming , '' communications on pure and applied mathematics 66.8 , pp .",
    "12751297 , 2013 .",
    "p. boufounos , `` greedy sparse signal reconstruction from sign measurements '' asilomar conf . on signals systems and comput . , pp . 13051309 , 2009 .",
    "l. jacques , j. laska , p. boufounos , and r. baraniuk , `` robust 1-bit compressive sensing via binary stable embeddings of sparse vectors , '' ieee transactions on information theory 59.4 , 20822102 , 2013 .",
    "j. laska , z. wen , w. yin and r. baraniuk , `` trust but verify : fast and accurate signal recovery from 1-bit compressive measurements , '' ieee trans . signal process .",
    "11 , pp . 52895301 , 2011 . t. blumensath and m. davies , `` iterative hard thresholding for compressive sensing , '' appl .",
    "3 , pp . 265274 , 2009 . l. deng , g. hinton and b. kingsbury , `` new types of deep neural network learning for speech recognition and related applications : an overview , '' international conference on acoustics , speech , and signal processing ( icassp ) , pp .",
    "85998603 , 2013 .",
    "k. simonyan and a. zisserman , `` very deep convolutional networks for large - scale image recognition , '' arxiv:1409.1556 , 2014 .",
    "d. p. kingma and j. l. ba , `` adam : a method for stochastic optimization , '' arxiv:1412.6980 , 2014 .",
    "`` tensorflow : large - scale machine learning on heterogeneous systems , '' http://tensorflow.org/ 2015 . software available from tensorflow.org .",
    "g. orchard , j. g. vogelstein and r. etienne - cummings , `` fast neuromimetic object recognition using fpga outperforms gpu implementation , '' ieee trans .",
    "neural networks learn .",
    "24 , no . 8 , pp . 12391252 , 2013 . s. b. cho and j. h. kim , `` combining multiple neural networks by fuzzy integral for robust classigication , '' ieee trans .",
    "systems man and cybernetics , vol .",
    "2 , pp . 380384 , 1995 ."
  ],
  "abstract_text": [
    "<S> in this paper , we propose majority voting neural networks for sparse signal recovery in binary compressed sensing . </S>",
    "<S> the majority voting neural network is composed of several independently trained feedforward neural networks employing the sigmoid function as an activation function . </S>",
    "<S> our empirical study shows that a choice of a loss function used in training processes for the network is of prime importance . </S>",
    "<S> we found a loss function suitable for sparse signal recovery , which includes a cross entropy - like term and an @xmath0 regularized term . from the experimental results </S>",
    "<S> , we observed that the majority voting neural network achieves excellent recovery performance , which is approaching the optimal performance as the number of component nets grows . </S>",
    "<S> the simple architecture of the majority voting neural networks would be beneficial for both software and hardware implementations . </S>"
  ]
}