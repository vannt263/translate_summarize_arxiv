{
  "article_text": [
    "noise modeling has an important role in the bayesian inference setup to achieve better robustness and accuracy .",
    "typically noise is considered to be additive and dense ( possibly even white ) in nature . in this paper",
    "we investigate the effect of sparse noise modeling in a standard bayesian inference tool called the _ relevance vector machine _",
    "( rvm ) @xcite .",
    "the rvm is a bayesian sparse kernel technique for applications in regression and classification @xcite .",
    "interest in the rvm can be attributed to the cause that it shares many characteristics of the popular support vector machine whilst providing bayesian advantages @xcite , mainly providing posteriors for the object of interest .",
    "generally the rvm is a fully bayesian technique that aims to learn all the relevant system parameters iteratively to infer the object of interest . in a linear model setup used for regression , rvm introduces sparsity through a weight vector where the weights are essential to form linear combinations of relevant kernels to predict the object of interest ; the weight vector is a set of system parameters and its sparsity leads to reduction of model complexity for regression .",
    "naturally , the rvm has been further used for sparse representation techniques as well as developing bayesian compressive sensing methods @xcite .    for a bayesian linear model ,",
    "the standard rvm uses a multivariate isotropic gaussian prior to model the additive dense noise . here",
    "isotropic means that the associated covariance matrix is proportional to the identity matrix .",
    "such a dense noise model has inherent limitations to accommodate instances of outliers @xcite , impulse bursts @xcite or missing ( lost ) data @xcite .",
    "we hypothesize that a sparse and dense noise model can accommodate for the statistics of a variety of noise types , without causing degradation in performance for any noise type compared to the standard case of using only a dense noise model . in this paper , we develop rvm for such a combined ( joint ) sparse and dense noise scenario .",
    "we consider the following linear system model @xmath0 where @xmath1 is the measurements , @xmath2 is a sparse vector ( for example weights in regression or sparse signal to estimate in compressed sensing ) , @xmath3 is a known system matrix ( for example , regressors or sampling system ) .",
    "further , @xmath4 is sparse noise and @xmath5 is dense noise . using @xmath6-norm notation to represent the number of non - zeros in a vector",
    ", we assume that @xmath7 and @xmath8 are small and unknown .",
    "the random vectors @xmath9 , @xmath10 and @xmath11 are independent .",
    "the model is used in face recognition @xcite , image denoising @xcite and compressed sensing @xcite .",
    "we develop a rvm for the model , by treating @xmath12 as a combined noise . by learning parameters of @xmath9 and @xmath12",
    ", we estimate @xmath9 without the need of estimating @xmath10 .",
    "we also consider the scenario where the signal @xmath9 and noise @xmath10 are block sparse . by using techniques similar to the ones in @xcite",
    "we generalize the methods to signals with unknown block structure .",
    "the main technical contribution is to derive update equations that are used iteratively for estimation of parameters in the new rvm .",
    "we refer to the new rvm as the rvm for combined sparse and dense noise ( sd - rvm ) . by an approximate analysis ,",
    "the sd - rvm algorithm is shown to be equivalent to the minimization of a non - symmetric sparsity inducing cost function . finally , the performance of sd - rvm is evaluated numerically using examples from compressed sensing , block sparse signal recovery , house price prediction and image denoising . throughout the paper ,",
    "we take an approach of comparing sd - rvm vis - a - vis the existing robust bayesian rvm ( rb - rvm ) @xcite ( described in the next section ) .      to establish relevance of our work",
    "we briefly describe prior work in this section .",
    "almost all prior works @xcite translate the linear setup to the equivalent setup",
    "@xmath13    \\left [ \\begin{array}{c }          \\mathbf{x } \\\\",
    "\\mathbf{e }         \\end{array }   \\right ]    + \\mathbf{n},\\end{aligned}\\ ] ] where @xmath14 is the @xmath15 identity matrix , @xmath16 $ ] acts as the effective system matrix and @xmath17^{\\top}$ ] acts as the parameter vector to be estimated .",
    "the rb - rvm of @xcite uses the standard rvm approach for directly .",
    "hence rb - rvm learns model parameters for all three signals @xmath18^{\\top}$ ] , @xmath19^{\\top}$ ] and @xmath11 , and thus estimates both @xmath9 and @xmath10 jointly .",
    "rb - rvm assumes gaussian priors @xmath20 where the precisions ( inverse variances ) @xmath21 , @xmath22 and @xmath23 are unknown .",
    "the precisions are given gamma priors @xmath24 where @xmath25 @xcite .",
    "typical practice is to maximize @xmath26 to infer the precisions , where we used boldface symbols to denote the vectors @xmath27^\\top , \\\\ &",
    "\\boldsymbol{\\nu } = [ \\nu_1,\\nu_2 , \\dots , \\nu_m]^\\top .\\end{aligned}\\ ] ] instead we take the alternative ( full bayesian ) approach of maximizing @xmath28 and assume that precisions have non - informative prior by taking the limit @xmath29 .",
    "for the distributions considered here , maximizing the conditional distribution @xmath26 becomes equivalent to maximizing the joint distribution @xmath28 , in the limit of non - informative priors . in calculations , however , the parameters @xmath30 are often given small values to avoid numerical instabilities . to estimate @xmath17^{\\top}$ ] , rb - rvm fixes the precisions and sets @xmath31^{\\top } = \\beta \\boldsymbol{\\sigma}_{rb } [ \\mathbf{a}\\,\\ \\mathbf{i}_{m}]^\\top \\mathbf{y } , \\label{xupdate}\\\\ & \\boldsymbol{\\sigma}_{rb } = \\left ( \\left(\\begin{array}{cc } \\boldsymbol{\\gamma } & \\mathbf{0}\\\\ \\mathbf{0 } & \\mathbf{n } \\end{array } \\right )   + \\beta   [ \\mathbf{a}\\,\\ \\mathbf{i}_{m}]^{\\top }   [ \\mathbf{a}\\,\\ \\mathbf{i}_{m } ] \\right)^{-1 } , \\nonumber\\end{aligned}\\ ] ] where @xmath32 and @xmath33 .",
    "the rb - rvm iteratively updates the precisions by maximizing @xmath28 , resulting in the update equations @xmath34_{ii}}{\\hat{x}_i^2 } , \\,\\,\\ ,",
    "\\nu_i^{new } = \\frac{1 - \\nu_i [ \\boldsymbol{\\sigma}_{rb}]_{n+i , n+i}}{\\hat{e}_i^2 } , \\nonumber\\\\%\\label{rvmalphaupdate}\\\\ % & \\beta^{new } = \\frac{\\sum_{i=1}^n \\alpha_i \\sigma_{ii } } { ||\\mathbf{y } - [ \\mathbf{a}\\,\\ \\mathbf{i}_{m } ] \\left [ \\mathbf{\\hat{x}}^{\\top } \\,\\ , \\mathbf{\\hat{e}}^{\\top } \\right]^{\\top } ||_2 ^ 2 }   & \\beta^{new } = \\frac{\\sum_{i=1}^n \\gamma_i [ \\boldsymbol{\\sigma}_{rb}]_{ii } + \\sum_{j=1}^m \\nu_j [ \\boldsymbol{\\sigma}_{rb}]_{n+i , n+i } } { ||\\mathbf{y } - \\mathbf{a}\\mathbf{\\hat{x } } - \\mathbf{\\hat{e } } ||_2 ^ 2 }   , \\label{rvmbetaupdate}\\end{aligned}\\ ] ] where @xmath35_{ii}$ ] denotes the @xmath36 component of the matrix @xmath37 .",
    "the update equations and are found by applying the standard rvm to .",
    "derivations can be found in e.g. @xcite .",
    "iterating until convergence gives the final estimates @xmath38 and @xmath39 . in the iterations , some precisions become large , making their respective components in @xmath38 and @xmath39 close to zero .",
    "this makes the final estimate of @xmath38 and @xmath39 sparse .",
    "rvm has high similarity with sparse bayesian learning ( sbl ) @xcite .",
    "sparse bayesian learning has been used for structured sparse signals , for example block sparse signals @xcite , where the problem of unknown signal block structure was treated using overlapping blocks . the model extension of rb - rvm shown in for handling block",
    "sparse noise with unknown block structure is straight - forward to derive .",
    "however , in our formulation , as we are not estimating the noise explicitly , the use of block sparse noise with unknown block structure is non - trivial .",
    "further , non bayesian ( even not statistical ) methods have been used for sparse estimation problems @xcite .",
    "for example , the @xmath40-norm minimization method justice pursuit ( jp ) @xcite uses the optimization technique of the standard basis pursuit denoising method @xcite , as follows @xmath41^{\\top}||_1   \\label{jp } \\text { s.t . }",
    "|| \\mathbf{y } - \\mathbf{ax } - \\mathbf{e } ||_2 \\leq \\epsilon , \\end{aligned}\\ ] ] where @xmath42 is a model parameter set by the user . for unknown noise power , it is impossible to know @xmath43 a - priori .",
    "we mention that a fully bayesian setup like the rvm does not require parameters set by a user .",
    "for , we propose to use a combined model for the two additive noises , as follows @xmath44 where @xmath45",
    ". we also use @xmath46 to denote the vector @xmath47^\\top$ ] .",
    "that means the two noises are treated as a single combined noise where each noise component has its own precision .",
    "the rationale is that we do not need to seperate the two noises .",
    "although our model promotes sparsity in the noise we empirically find that it is able to model sparse and non - sparse noise .",
    "using the noise model and that @xmath48 , we find the maximum a posteriori ( map ) estimate@xmath49 where as before @xmath50 . the precisions are updated as @xmath51_{jj } } { [ \\mathbf{y } - \\mathbf{a\\hat{x}}]_j^2 } \\label{betaupdate } , \\end{aligned}\\ ] ] where @xmath52_{ii}$ ] .",
    "the derivations of and are given in the next section .",
    "to update the precisions we maximize the distribution @xmath53 ( obtained by marginalizing over @xmath9 ) , with respect to @xmath54 and @xmath55 , where we use the prior @xmath56 and @xmath57 is as in .",
    "the log - likelihood of the parameters is @xmath58    we maximize @xmath59 w.r.t .",
    "@xmath54 by setting the derivative to zero . to simplify the derivative we use that @xmath60 and the determinant lemma @xcite @xmath61 using and we find that @xmath59 is maximized w.r.t",
    "@xmath54 when @xmath62 instead of solving for @xmath54 ( which would require solving a non - linear coupled equation since @xmath63 and @xmath64 depend on @xmath54 ) we approximate the equation as @xmath65 we solve for @xmath66 rather than for @xmath54 since it in practice often results in a better convergence @xcite .",
    "the update equation then becomes @xmath67 setting @xmath68 we obtain .    for the noise precisions we use that @xmath69 = [ \\mathbf{y - a\\hat{x}}]_j^2 .\\end{aligned}\\ ]",
    "] we find that @xmath59 is maximized w.r.t .",
    "@xmath55 when @xmath70_j^2 + \\frac{c}{\\beta_j } - d = 0 , \\end{aligned}\\ ] ] where @xmath71 denotes the @xmath72th row vector of @xmath73 . rewriting the equation as @xmath74_j^2 + 2d ) \\beta_j^{new } = 0 , \\end{aligned}\\ ] ] using that @xmath75_{jj}$ ] , we find that @xmath76_{jj } + 2c}{[\\mathbf{y - a\\hat{x}}]_j^2 + 2d } .",
    "\\end{aligned}\\ ] ] setting @xmath77 we obtain .",
    "the derivations of and are given in appendix  [ xhat_identity_derivation ] .",
    "several approximations are made in the derivation of the iterative update equations .",
    "it is interesting to see how the approximations affect the sparsity of the solution . in this subsection",
    ", we show that the approximations make the sd - rvm equivalent to minimizing a non - symmetric sparsity promoting cost function .    to motivate that the standard rvm is sparsity promoting",
    ", one can use that the marginal distribution of @xmath78 is a student - t distribution . for a fixed @xmath23 ( and @xmath79 ) ,",
    "the standard rvm is therefore an iterative method for solving ( details can be found in @xcite ) @xmath80 the log - sum cost function can be used as a sparsity promoting cost function , making it plausible that the rvm promotes sparsity .    for the sd - rvm ,",
    "the precisions are updated by maximizing the marginal distribution @xmath81 .",
    "the problem is equivalent to maximizing @xmath59 in .",
    "we show approximations for relevant parts of the right hand side of @xmath59 as follows @xmath82_{jj } ( \\beta_j - \\beta_j^{old}),\\label{eq : approx_1}\\end{aligned}\\ ] ] where the approximation is up to first order in @xmath83 and @xmath46 .",
    "we rewrite the problem in variables @xmath9 and @xmath84 using that @xcite @xmath85 where now @xmath86 as in . under the approximation and the reformulation , maximization of @xmath87",
    "becomes equivalent to @xmath88 \\\\%\\label{equivalent}\\\\ & + \\sum_{j=1}^m \\left[(e_j^2 + [ \\mathbf{a}\\boldsymbol{\\sigma}^{old } \\mathbf{a}^\\top]_{jj } + 2d)\\beta_j + ( 1 + 2c)\\log(\\beta_j ) \\right ]   .",
    "\\nonumber\\\\ & \\text{such that   } \\mathbf{ax + \\tilde{e } = y } \\nonumber\\end{aligned}\\ ] ] by minimizing the objective with respect to @xmath54 and @xmath55 , the problem reduces to @xmath89_{jj } + 2d ) , \\nonumber\\\\ & \\text{such that   } \\mathbf{ax + \\tilde{e } = y } \\nonumber\\end{aligned}\\ ] ] where we have ignored additive constants . because of the approximations , the constants @xmath90 and @xmath91_{jj}$ ] make the cost function non - symmetric in the components of @xmath9 and @xmath84 .",
    "the sd - rvm is thus equivalent to minimizing a non - symmetric sparsity promoting cost function . in a similar way it can be shown that the standard rvm and rb - rvm are also equivalent to non - symmetric cost functions under appropriate approximations . a two - dimensional example using @xmath92^{\\top}$ ] is shown in fig .  [ logball ] .",
    "in this section we take a non - rigorous approach for quantifying the computational complexity of sd - rvm .",
    "the complexity is computed per iteration , since the number of iterations depends on the stopping criterion used , and with the assumption of a naive implementation .",
    "each iteration of sd - rvm requires @xmath93 flops to compute the matrix @xmath63 using gauss - jordan elimination @xcite .",
    "updating the precisions requires @xmath94 flops since the residual @xmath95 needs to be computed .",
    "hence the computational complexity of sd - rvm is @xmath96 a natural interest is the complexity of rb - rvm . again with the assumption of a naive implementation , each iteration of rb - rvm requires the inversion of a @xmath97 matrix to compute @xmath37 .",
    "updating the precisions requires @xmath94 flops and hence the computational complexity of rb - rvm is @xmath98 in section [ cs_problem ] we provide numerical evaluations to quantify algorithm run time requirements that confirm that sd - rvm is typically faster than rb - rvm .",
    "to describe a block sparse signal @xmath99 with known block structure we partition @xmath100 = \\{1,2,\\dots , n\\}$ ] into blocks as @xmath101 = i_1 \\cup i_2 \\cup \\dots \\cup i_p , \\end{aligned}\\ ] ] where @xmath102 and @xmath103 for @xmath104 .",
    "the signal is block sparse when only a few blocks of the signal are non - zero .",
    "the component - wise sd - rvm generalizes to this scenario by requiring that the precisions are equal in each block , i.e. we choose the prior distribution for the components of block @xmath105 to be @xmath106 where @xmath107 denotes the vector consisting of the components of @xmath9 with indices in @xmath105 .",
    "similarly we can partition the components of the sparse noise @xmath108 into blocks as @xmath109 = j_1 \\cup j_2 \\cup \\dots \\cup j_q , \\end{aligned}\\ ] ] where @xmath110 , @xmath111 for @xmath104 and the block @xmath112 of @xmath10 is given the prior distribution @xmath113 as before , the precisions are given gamma distributions as priors , where now @xmath114 using this model , we derive the update equations of precisions as below @xmath115_{j_j } ) + 2c}{||(\\mathbf{y - a\\hat{x}})_{j_j}||_2 ^ 2 + 2d } , \\label{blockbetaupdate}\\end{aligned}\\ ] ] where @xmath116 denotes the @xmath117 submatrix of @xmath63 formed by elements appropriately indexed by @xmath105 . by setting @xmath118 and @xmath119 we obtain the update equations for component - wise sparse signal and noise . we see that when @xmath120 and @xmath121 $ ] , then reduces to the update equations of the standard rvm since @xmath122    the derivation of the update equations and is found in appendix  [ sec : derivation_sdrvm_knownblocks ] .",
    "in some situations the signal can have an unknown block structure , i.e. the signal is block sparse , but the dimensions and positions of the blocks are unknown .",
    "this scenario can be handled by treating the signal as a superposition of block sparse signals @xcite ( see illustration in figure  [ blocks ] ) .",
    "this approach also describes the scenario when @xmath10 is component wise sparse and @xmath11 is dense ( e.g. gaussian ) .",
    "the precision of each component is then a combination of the precisions of the blocks to which the component belongs .",
    "let @xmath54 be the precision of the component @xmath78 and @xmath123 be the precision of block @xmath124 .",
    "we model the signal as @xmath125 we model the noise in a similar way with precisions @xmath55 for component @xmath72 and precisions @xmath126 for the block with support @xmath127 . to promote sparsity ,",
    "the precisions of the underlying blocks are given gamma distributions as priors @xmath128 in each iteration we update the underlying precisions @xmath123 .",
    "the componentwise precisions are then updated using . with this model ,",
    "the update equations for the precisions become @xmath129 where @xmath130 is the diagonal matrix with @xmath131_{ii } = \\gamma_i$ ] if @xmath132 and @xmath131_{ii } = 0 $ ] otherwise .",
    "we denote the corresponding matrix for @xmath133 by @xmath134 .",
    "the componentwise precisions are updated using and similar for @xmath55 .",
    "( 0,0 ) rectangle ( 1,6 ) ; ( 0,1 )  ( 1,1 ) ; ( 0,2 )  ( 1,2 ) ; ( 0,3 )  ( 1,3 ) ; ( 0,4 )  ( 1,4 ) ; ( 0,5 )  ( 1,5 ) ; at ( 0.5,-0.75 ) @xmath135 ; at ( 1.5,3 ) @xmath136 ; ( 2,4 )  ( 2,0 )  ( 3,0 )  ( 3,4 ) ; ( 2,6 ) rectangle ( 3,4 ) ; ( 2,5 )  ( 3,5 ) ; at ( 2.5,-0.75 ) @xmath137 ; at ( 3.5,3 ) @xmath138 ; ( 4,6 ) rectangle ( 5,0 ) ; ( 4,4 ) rectangle ( 5,2 ) ; ( 4,3 )  ( 5,3 ) ; at ( 4.5,-0.75 ) @xmath139 ; at ( 5.5,3 ) @xmath138 ; ( 6,6 ) rectangle ( 7,0 ) ; ( 6,2 ) rectangle ( 7,0 ) ; at ( 6.5,-0.75 ) @xmath140 ; ( 6,1 )  ( 7,1 ) ; ( 0,0 - 7.5 ) rectangle ( 1,6 - 7.5 ) ; ( 0,1 - 7.5 ) ",
    "( 1,1 - 7.5 ) ; ( 0,2 - 7.5 )  ( 1,2 - 7.5 ) ; ( 0,3 - 7.5 ) ",
    "( 1,3 - 7.5 ) ; ( 0,4 - 7.5 ) ",
    "( 1,4 - 7.5 ) ; ( 0,5 - 7.5 ) ",
    "( 1,5 - 7.5 ) ; at ( 0.5,-0.75 - 7.5 ) @xmath135 ; at ( 1.5,3 - 7.5 ) @xmath136 ; ( 2,4 - 7.5 )  ( 2,0 - 7.5 ) ",
    "( 3,0 - 7.5 ) ",
    "( 3,4 - 7.5 ) ; ( 2,6 - 7.5 ) rectangle ( 3,4 - 7.5 ) ; ( 2,5 - 7.5 ) ",
    "( 3,5 - 7.5 ) ; at ( 2.5,-0.75 - 7.5 ) @xmath141 ; at ( 3.5,3 - 7.5 ) @xmath138 ; ( 4,5 - 7.5 )  ( 4,6 - 7.5 ) ",
    "( 5,6 - 7.5 ) ",
    "( 5,4 - 7.5 ) ; ( 4,5 - 7.5 ) rectangle ( 5,3 - 7.5 ) ; ( 4,3 - 7.5 )  ( 4,0 - 7.5 ) ",
    "( 5,0 - 7.5 ) ",
    "( 5,3 - 7.5 ) ; at ( 4.5,-0.75 - 7.5 ) @xmath142 ; ( 4,4 - 7.5 ) ",
    "( 5,4 - 7.5 ) ; at ( 5.5,3 - 7.5 ) @xmath138 ; ( 6,4 - 7.5 ) ",
    "( 6,6 - 7.5 ) ",
    "( 7,6 - 7.5 )  ( 7,4 - 7.5 ) ; ( 6,4 - 7.5 ) rectangle ( 7,2 - 7.5 ) ; ( 6,2 - 7.5 )  ( 6,0 - 7.5 ) ",
    "( 7,0 - 7.5 )  ( 7,2 - 7.5 ) ; at ( 6.5,-0.75 - 7.5 ) @xmath143 ; ( 6,3 - 7.5 )  ( 7,3 - 7.5 ) ; at ( 7.5,3 - 7.5 ) @xmath138 ; ( 8,3 - 7.5 ) ",
    "( 8,6 - 7.5 ) ",
    "( 9,6 - 7.5 ) ",
    "( 9,3 - 7.5 ) ; ( 8,3 - 7.5 ) rectangle ( 9,1 - 7.5 ) ; ( 8,1 - 7.5 ) ",
    "( 8,0 - 7.5 ) ",
    "( 9,0 - 7.5 ) ",
    "( 9,1 - 7.5 ) ; at ( 8.5,-0.75 - 7.5 ) @xmath144 ; ( 8,2 - 7.5 ) ",
    "( 9,2 - 7.5 ) ; at ( 9.5,3 - 7.5 ) @xmath138 ; ( 10,2 - 7.5 ) ",
    "( 10,6 - 7.5 ) ",
    "( 11,6 - 7.5 ) ",
    "( 11,2 - 7.5 ) ; ( 10,2 - 7.5 ) rectangle ( 11,0 - 7.5 ) ; at ( 10.5,-0.75 - 7.5 ) @xmath145 ; ( 10,1 - 7.5 ) ",
    "( 11,1 - 7.5 ) ;    we see that when the underlying blocks are disjoint , then @xmath146 for all @xmath132 and @xmath147 for all @xmath148 .",
    "the update equations then reduce to the update equations for the block sparse model with known block structure .      in the model where @xmath9 and @xmath10 are componentwise sparse and @xmath11 is dense , then @xmath149 , \\end{aligned}\\ ] ] where @xmath150 and @xmath151 . in this scenario",
    "the support set of the sparse and dense noise is overlapping , so the update equations for the precisions become @xmath152_{jj } + 2c}{\\beta_j [ \\mathbf{y - a\\hat{x } } ] _ j^2 + 2d } , \\ , \\ , j = 1,2,\\dots , m , \\\\ & \\tilde{\\beta}_{m+1}^{new } = \\frac{\\sum_{j=1}^m \\beta_j - \\frac{1}{\\tilde{\\beta}_{m+1 } } \\sum_{j=1}^m \\beta_j^2 [ \\mathbf{a}^\\top \\boldsymbol{\\sigma } \\mathbf{a}]_{jj } + 2c } { \\sum_{j=1}^m \\beta_j^2 [ \\mathbf{y - a\\hat{x } } ] _ j^2 + 2d},\\\\ & \\beta_j = ( \\tilde{\\beta}_j^{-1 } + \\beta_{m+1}^{-1})^{-1 } .",
    "\\end{aligned}\\ ] ] we will use these update equations in the simulations where the signal is component - wise sparse and the noise is a sum of ( component - wise ) sparse and dense noise .",
    "it turns out that this method is slightly better than the sd - rvm in section  [ subsec : sd - rvm ] .",
    "in this section we evaluate the performance of the sd - rvm using several scenarios  for simulated and real signals . for simulated signals , we considered the sparse and block sparse recovery problem in compressed sensing .",
    "then for real signals , we considered prediction of house prices using the boston housing dataset @xcite and denoising of images contaminated by _ salt and pepper _ noise . in the simulations we used the cvx toolbox @xcite to implement jp .       for outlier free measurements .",
    "]     for @xmath153 outliers contaminated measurements . ]",
    "the recovery problem in compressed sensing consists of estimating a sparse vector @xmath99 in from @xmath154 linear measurements , where @xmath155 . to evaluate the performance of the algorithms , we generated measurement matrices @xmath3 by drawing their components from a @xmath156 distribution and scaling their column vectors to unit norm .",
    "we selected the positions of the active components of @xmath9 and @xmath10 uniformly at random and draw their values from @xmath156 . in the simulation",
    "we draw the additive noise @xmath11 from @xmath157 .",
    "we compared jp , the standard rvm , rb - rvm and sd - rvm .",
    "for jp we assumed @xmath158 to be known and set @xmath159 as proposed in @xcite .    in the simulations we varied the measurement rate @xmath160 ( ratio of the number of measurements and the signal dimension ) for measurements without outliers and with @xmath153 outliers .",
    "we chose @xmath161 and fixed the signal - to - dense - noise - ratio ( sdnr ) @xmath162",
    "/ e[||\\mathbf{n}||_2 ^ 2 ] = ||\\mathbf{x}||_0 / ( m \\sigma_n^2 ) , \\end{aligned}\\ ] ] to @xmath163 db . by generating @xmath164 measurement matrices and @xmath164 vectors @xmath9 and",
    "@xmath10 for each matrix we numerically evaluated the normalized mean square error ( nmse )",
    "@xmath165 / e[||\\mathbf{x}||_2 ^ 2 ] .\\end{aligned}\\ ] ] the results are shown in figure [ cs1 ] and figure [ cs2 ] .",
    "we found that sd - rvm outperformed the other methods .",
    "the improvement of sd - rvm over rb - rvm was @xmath166 to @xmath167 db for @xmath168 , with and without outliers . compared to jp ,",
    "the improvement of sd - rvm was @xmath169 to @xmath170 without outlier noise and @xmath166 to @xmath171 db with outlier noise when @xmath168 .",
    "the poor performance of rvm is due to sensitivity to the regularization parameters .",
    "the performance of rvm improves greatly when the regularization are optimally tuned , however , the optimal values varies with snr and measurement dimensions .",
    "the experiments show that the performance of sd - rvm does not degrade in the absence of sparse noise .    for each realization of the problem we measured the runtime ( cpu time ) of each algorithm .",
    "the histogram of the runtimes is shown in figure  [ fig : cputimes ] .",
    "we found that the runtimes of the rvm algotithms ( the standard rvm , rb - rvm and sd - rvm ) were shorter than the runtime of jp and the runtimes of jp were spread over a larger range .",
    "of the rvm algorithms , sd - rvm had the highest concentration of low runtime ( @xmath172 seconds ) , while the runtimes of the standard rvm and rb - rvm was more concentrated around @xmath173 seconds .",
    "the histogram in figure  [ fig : cputimes ] has been truncated to only show percentage for the visible values .       for signals with known block structure and @xmath153 outliers in measurements .",
    "]     for signals with unknown block structure and @xmath153 outliers in measurements . ]",
    "the recovery problem in compressed sensing can be generalized to block sparse signals and noise @xcite . for block sparse signals ,",
    "the signal components are partitioned into blocks of which only a few blocks are non - zero .",
    "sparse bayesian learning ( sbl ) extended to the block sparse signal case is often referred to as block sbl ( bsbl ) @xcite .",
    "the problem of unknown block structure can be solved by overparametrizing the blocks @xcite . in bsbl @xcite ,",
    "the signal is modelled as @xmath174 } \\mathbf{a}_i \\mathbf{x}_i,\\end{aligned}\\ ] ] i.e. the measured signal is modelled as a sum of signals where each signal represents a block of the original signal .",
    "the resulting problem can then be solved using bsbl for known block structure . when the minimum block size @xmath175 is known , the summation can be restricted to subsets of size @xmath176 @xcite .",
    "the sd - rvm can be extended to the block sparse case using the methods developed in section  [ sd - rvm_blocks ] and section  [ unknownblocks ] .",
    "justice pursuit can be extended to the block sparse case in a similar way as bsbl by setting @xmath177 where the sum runs over all blocks ( non - overlapping or overlapping ) and as before we assume the noise variance to be known and set @xmath178 . for unknown block structure",
    "we also compared with component sparse methods rvm , rb - rvm and sd - rvm .    to numerically evaluate the performance of the block sparse algorithms we varied the measurement rate @xmath160 for measurements with @xmath153 sparse noise .",
    "we set the signal dimension to @xmath161 and fixed the sdnr to @xmath163 db .",
    "we divided the signal @xmath9 into @xmath163 blocks of equal size of which @xmath169 blocks were non - zero .",
    "the sparse noise consisted of blocks with @xmath179 components in each block . in the sparse noise , @xmath153 of the blocks were active .",
    "for known block structure , the blocks were choosen uniformly at random from a set of predefined and non - overlapping blocks while for unknown block structure , the first component of each block was choosen uniformly at random , making it possible for the blocks to overlap .",
    "the active components of the signal and the sparse noise were drawn from @xmath156 . by generating @xmath180 measurement matrices @xmath73 and @xmath180 signals @xmath9 and sparse noises @xmath10 for each matrix we numerically evaluated the nmse .",
    "for known block structure we found that block sd - rvm outperformed the other methods .",
    "the nmse of the block sparse sd - rvm was lower than the nmse of block jp by more than @xmath181 db for @xmath182 and from @xmath183 to @xmath184 db lower for @xmath185 .",
    "the results are presented in figure  [ blockcs1 ] .    for unknown block structure we found that for @xmath186 , sd - rvm for unknown block structure gave best performance while for @xmath185 , the usual component sparse sd - rvm gave the best perfomance .",
    "the nmse of jp for unknown block structure was about @xmath179",
    "db larger than the nmse of block sd - rvm for @xmath187 , while for @xmath188 block jp gave a better nmse than block sd - rvm .",
    "as expected , rvm and bsbl gave poor performance since they are not developed to handle measurements with sparse noise .",
    "the results are shown in figure  [ blockcs2 ] .",
    "one real world problem is the prediction of house prices . to test the algorithms on real data",
    ", we used the boston housing dataset @xcite .",
    "the dataset consists of @xmath189 house prices in suburbs of boston and @xmath190 parameters ( air quality , accessibility , pupil - to - teacher ratio , etc . ) for each house .",
    "the problem is to predict the median house price for part of the dataset ( test data ) using the complement dataset ( training data ) to learn regression parameters .",
    "we model the house prices as @xmath191 where @xmath192 is the price of house @xmath193 , @xmath194 contains the parameters of house @xmath193 , @xmath195 is the regression vector , @xmath196 is ( gaussian ) noise and @xmath197 is a ( possible ) outlier .",
    "very expensive or inexpensive houses can treated as outliers .",
    "the goal is to estimate the median house price for the test set .",
    "we find the median by estimating the regression parameters and setting @xmath198 where @xmath199 contains the parameters of the houses in the test set .",
    "it is believed that only a few parameters are important to the average customer , @xmath9 can therefore be modelled as a sparse vector .",
    "we used a fraction @xmath200 of the dataset as training data and the rest as test set . by choosing the training set uniformly at random we evaluated the mean absolute error of the predicted median and mean cputime ( in seconds ) over @xmath201 realizations .",
    "we found that sd - rvm gave @xmath202 to @xmath153 lower mean error than that of rb - rvm and the mean error of rb - rvm and sd - rvm was about @xmath203 lower than the error of the rvm ( see table  [ tab : boston ] ) .",
    "the cputime of sd - rvm was @xmath204 to @xmath205 of the cputime of rb - rvm .",
    ".prediction of median houseprice using the boston housing dataset .",
    "mean error and mean cputime ( in seconds ) for different fractions , @xmath200 , of the dataset used as training set .",
    "[ cols= \" < , < , < , < , < , < , < \" , ]     [ tab : res_image ]",
    "in this paper we introduced the combined sparse and dense noise revelance vector machine ( sd - rvm ) which is robust to sparse and dense additive noise .",
    "sd - rvm was shown to be equivalent to the minimization of a non - symmetric sparsity promoting cost function . through simulations ,",
    "sd - rvm was shown to empirically perform better than the standard rvm and the robust rb - rvm .",
    "here we derive the update equations for @xmath64 , @xmath54 and @xmath55 for the sd - rvm in sections [ subsec : sd - rvm ] , [ sd - rvm_blocks ] and [ unknownblocks ] . for fixed precisions @xmath83 and @xmath46 , the maximum a posteriori ( map ) estimate of @xmath9",
    "becomes @xmath206 where @xmath207 .",
    "the form of the map estimate is the same for all models considered in this paper .              to update the precisions we maximize the marginal distribution @xmath218 with respect to @xmath83 and @xmath46 , where @xmath219 and @xmath220 is as in and .",
    "the log - likelihood of the parameters is @xmath221 using we get that @xmath59 is maximized when @xmath222 where @xmath223 is the submatrix of @xmath63 consisting of the columns and rows in @xmath105 .",
    "further , using we get that @xmath224 thus , is fulfilled when @xmath225 as before , instead of solving for @xmath54 we rewrite the equation as @xmath226 solving for @xmath66 gives us the update equation . to find the update equation for @xmath55 we use that @xmath227 = ||\\mathbf{y}_{j_j}||_2 ^ 2 \\\\ & - 2\\mathbf{y}_{j_j}^\\top \\mathbf{a}_{j_j , : } \\boldsymbol{\\sigma } \\mathbf{a}^\\top \\mathbf{b } \\mathbf{y }   + \\mathbf{y}^\\top \\mathbf{b}\\mathbf{a } \\boldsymbol{\\sigma }",
    "\\mathbf{a}_{j_j,:}^\\top \\mathbf{a}_{j_j , : }   \\boldsymbol{\\sigma } \\mathbf{a}^\\top \\mathbf{b } \\mathbf{y } \\\\ & = ||(\\mathbf{y - a\\hat{x}})_{j_j}||_2 ^ 2 , \\end{aligned}\\ ] ] where @xmath228 consists of the row vectors of @xmath73 which row number belongs to @xmath112 .",
    "we get that @xmath229 rewriting the equation as @xmath230 and using that @xmath231_{j_j})$ ] gives us the update equation .",
    "we search to maximize with respect to the underlying variables @xmath123 and @xmath126 . using that @xmath233 , @xmath234 , when @xmath132 and zero otherwise , and we find that @xmath59 is maximized when @xmath235 by rewriting as @xmath236 solving for @xmath237 gives us the update equation . for the noise precisions , we similarly find that @xmath238 by rewriting the expression as @xmath239 we find the update equation .",
    "we see that the form of update equations depends on how the equations are rewritten .",
    "the form used here has the advantage of reducing to when the underlying blocks are disjoint .",
    "j.  laska , m.  davenport and r.  baraniuk , _ exact signal recovery from sparsely corrupted measurements through the pursuit of justice _ , proceedings of the 43rd asimolar conference on signals , systems and computers , piscataway , nj , usa , 2009 , pp .",
    "1556 - 1560 , ieee press .",
    "y.  jin and b.d .",
    "algorithms for robust linear regression by exploiting the connection to sparse signal recovery _ , ieee international conference on acoustics speech and signal processing ( icassp ) , 2010 , pp.3830 - 3833 , 14 - 19 march 2010 .",
    "m.  vehkapera , y.  kabashima and s.  chatterjee , _ statistical mechanics approach to sparse noise denoising _ ,",
    "proceedings of the 21st european signal processing conference ( eusipco ) , 2013 , pp . 1 - 5 , 9 - 13 september 2013 .",
    "a.  cherian , s.  sra and n.  papanikolopoulos , _ denoising sparse noise via online dictionary learning _ ,",
    "ieee international conference on acoustics , speech and signal processing ( icassp ) , 2011 , pp .",
    "2060 - 2063 , 22 - 27 may 2011 .",
    "r.  giri and b.d .",
    "rao , _ block sparse excitation based all - pole modeling of speech _ , ieee international conference on acoustics , speech and signal processing ( icassp ) , 2014 , pp . 3754 - 3758 , 4 - 9 may 2014 .",
    "d.  giacobello , m.g .",
    "christensen , m.n .",
    "murthi , s.h .",
    "jensen and m.  moonen , _ sparse linear prediction and its applications to speech processing _ , ieee transactions on audio , speech and language processing , vol.20 , no.5 , pp.1644 - 1657 , july 2012 .",
    "carrillo , k.e .",
    "barner and t.c .",
    "aysal , _ robust sampling and reconstruction methods for sparse signals in the presence of impulsive noise _ , ieee journal of selected topics in signal processing , vol.4 , no.2 , pp .",
    "392 - 408 , april 2010 .",
    "z.  zhang and b.d .",
    "rao , _ extension of sbl algorithms for the recovery of block sparse signals with intra - block correlation _",
    ", ieee transactions on signal processing , vol .",
    "61 , no . 8 , pp .",
    "2009 - 2015 , april 2013 .",
    "z.  zhang and b.d .",
    "rao , _ sparse signal recovery with temporally correlated source vectors using sparse bayesian learning _",
    ", ieee journal of selected topics in signal processing , vol.5 , no.5 , pp.912,926 , sept . 2011 ."
  ],
  "abstract_text": [
    "<S> using a bayesian approach , we consider the problem of recovering sparse signals under additive sparse and dense noise . typically , sparse noise models outliers , impulse bursts or data loss . to handle sparse noise </S>",
    "<S> , existing methods simultaneously estimate the sparse signal of interest and the sparse noise of no interest . for estimating the sparse signal , without the need of estimating the sparse noise </S>",
    "<S> , we construct a robust relevance vector machine ( rvm ) . in the rvm , sparse noise and ever present dense noise </S>",
    "<S> are treated through a combined noise model . </S>",
    "<S> the precision of combined noise is modeled by a diagonal matrix . </S>",
    "<S> we show that the new rvm update equations correspond to a non - symmetric sparsity inducing cost function . </S>",
    "<S> further , the combined modeling is found to be computationally more efficient . </S>",
    "<S> we also extend the method to block - sparse signals and noise with known and unknown block structures . through simulations , </S>",
    "<S> we show the performance and computation efficiency of the new rvm in several applications : recovery of sparse and block sparse signals , housing price prediction and image denoising . </S>"
  ]
}