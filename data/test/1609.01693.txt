{
  "article_text": [
    "we propose an eulerian approach towards motion representation learning .",
    "the main difference between lagrangian and eulerian motion is that lagrangian motion ( optical flow ) focuses on individual points and analyzes their change in location over time .",
    "therefore , lagrangian motion performs tracking of points over time and for this it requires a unique matching method between point or patches . on the other hand ,",
    "eulerian motion considers a set of locations in the image and analyzes the changes at these locations over time .",
    "thus , eulerian motion does not estimate where a given point moves to , instead , it measures flux properties .",
    "figure  [ fig : eulerian ] depicts this difference between eulerian and lagrangian motion . as a specific instance of the eulerian model , we consider phase - based motion .",
    "the phase variations over time of the coefficients of the complex - steerable pyramid are indicatives of motion @xcite and form the basis for learning motion representations .",
    "the gain of an eulerian motion approach is that it avoids the need for hand - crafted optical flow constructions .",
    "phase is an innate property of the image , it does not need to be estimated from explicit patch correspondences .",
    "we propose a general - purpose phase - based motion description learning setup that can be used in any task relying on motion . here",
    "we explore four use cases : ( i ) action recognition , ( ii ) motion prediction in static images , ( iii ) motion transfer in static images and , ( iv ) motion transfer in video .",
    "note that phase - based motion representations are readily applicably to other motion - related task as well , including : human gait analysis , object tracking , action localization , etc .",
    "eulerian motion modeling has shown remarkable results for motion magnification @xcite where a phase - based approach significantly improves the quality @xcite and broadens its application  @xcite .",
    "a phase - based video interpolation is proposed in @xcite and a phase - based optical flow estimation is proposed in @xcite . inspired by the these work , we advocate the use of the eulerian model as exemplified by phase for learning motion representations .",
    "optical flow - based motion features have been extensively employed for action recognition in works such as @xcite .",
    "these works , use hand crafted features extracted from the optical flow .",
    "instead , we propose to input phase - based motion measurements to a cnn to reap the benefits of deep feature representation learning methods .",
    "a natural extension of going beyond a single frame in a deep net is by using 3@xmath0 space - time convolutions  @xcite .",
    "3@xmath0 convolutions learn appearance and motion jointly . while elegant , it makes it difficult to add the wealth of information that is available for appearance - only datasets through pre - training . in our method",
    ", we keep the benefit of pre - training by separating the appearance and the phase - based motion streams .",
    "using pre - trained networks is possible in the two - stream network approaches proposed in @xcite .",
    "this combines a multi - frame optical flow network stream with an appearance stream and obtains competitive results in practice .",
    "the appearance stream can employ a pretrained network .",
    "similarly , we also consider the combination of appearance and motion in a two - stream fashion , but with innate phase information rather than using a hand - crafted optical flow .",
    "the temporal frame ordering is exploited in @xcite , where the parameters of a ranking machine are used for video description . while in @xcite",
    "recurrent neural networks are proposed for improving action recognition . in this paper",
    "we also model the temporal aspect , although we add the benefit of a two - stream approach by separating appearance and phase variation over time .      in @xcite , optical flow motion",
    "is learned from videos and predicted in static images in a structured regression formulation . in @xcite",
    "the authors propose predicting optical flow in a cnn from input static images .",
    "where these works predict optical flow , we propose to predict the motion through phase changes , which does not depend on pixel tracking .    predicting the future rgb frame from the current rgb frame",
    "is proposed in @xcite in the context of action prediction .",
    "similar to this work , we also start from an input appearance and obtain an output appearance image , however in our case the learning part learns the mapping from input phase information to future phase .      animating a static image by transferring the motion from an input video is related to the notion of artistic style transfer @xcite .",
    "the style transfer aims at changing an input image or video such that the artistic style matches the one of a provided target image . here ,",
    "instead , we consider the motion transfer  given an input image , transfer the phase - based motion from the video to the image .",
    "additionally , we also consider video - to - video transfer where the style of performing a certain action is transferred from a target video to the input video . in @xcite",
    "the authors allow the users to change the video by adding plausible object manipulations in the video .",
    "similar to this work , we also want to change the video motion after the recording is done , by adjusting the style of the action being performed .",
    "the local phase and amplitude of an image are measured by complex oriented filters of the form : @xmath1 , where @xmath2 is the filter orientation and @xmath3 the filter scale @xcite , @xmath4 where @xmath5 is the local phase at scale @xmath3 and orientation @xmath2 , and @xmath6 the amplitude , @xmath7 is the image brightness / input channel , and @xmath8 the convolution operator , and @xmath9 are image coordinates .",
    "the filters have multiple scales and orientations , forming a complex steerable pyramid @xcite which captures various levels of image resolution .",
    "there is a direct relation between motion and the change measured in phase over time .",
    "the fourier shift theorem makes the connection between the variation in phase of the subbands over time and the global image motion . rather than estimating global motion , using a steerable pyramid we can decompose the image into localized subbands and thus , recover the local motion in the phase variations over time .",
    "from the above decomposition only the phase , not the amplitude , corresponds to motion . in @xcite",
    "the authors show that the temporal gradient of phase computed from a spatially bandpassed video over time , directly relates to the motion field . therefore , here , we focus on local phase at multiple scales and orientations to represent motion .",
    "we propose using phase to learn motion representations for solving general motion - related tasks in a deep net .",
    "we add phase as an additional motion input channel to a standard appearance ( rgb ) convolutional deep neural network .",
    "figure  [ fig : idea ] shows our proposed general - purpose phase - based pipeline",
    ". the input video frame is decomposed using the complex steerable pyramid into amplitude and phase .",
    "both phase and amplitude have multiple corresponding orientations and scales .",
    "since the phase is an indicative of motion , we ignore the amplitude and we use the input phase for the motion representation learning .",
    "we treat the orientations as input channels while the scales represent different streams of the network , similar to @xcite who use this setup for a different image pyramid .",
    "we explore phase - based motion representation learning in four practical use cases .",
    "while a thorough in - depth experimental investigation is out of scope , we detail the setup of motion representation learning for each use case .    [ cols=\"^,^ \" , ]          separating appearance and motion in two - streams is effective for action recognition @xcite . for the appearance stream",
    "we follow @xcite and use the input rgb frame , which offers the advantage of pre - training features on static images .",
    "however , where @xcite uses hand - crafted optical flow features , we propose to use eulerian motion for the second stream with oriented phase over multiple scales , as depicted in figure  [ fig : action ] .    for evaluating action recognition , a comparison of our two - stream phase - based motion with the two - stream optical - flow approach of @xcite on the two datasets used in their paper  hmdb51 and ucf101 is needed .",
    "we expect benefits from a phase - based motion representation because it does not depend on a specific hand - crafted optical flow implementation and does not rely on pixel tracking .",
    "the benefit of eulerian motion for motion prediction is that the prediction locations are fixed over time .",
    "this contrasts sharply with lagrangian motion , as pixels tracked by optical flow may be lost as they move in or out of the frame , or move to the same spatial location .",
    "such lost pixels make it hard to recover long - term relations beyond just the next frame .",
    "the fixed prediction locations of a eulerian motion representation do not suffer from this and offer long - term relation predictions of several frames .",
    "we propose to learn from a given input rgb the output future rgb , by recovering from the rgb the phase scales and orientations , then predicting the multi - scale future phase - orientations and transforming them back into future rgb frames as in figure  [ fig : phase_prediction].(a ) . for long - term motion prediction",
    "we propose an rnn ( recurrent neural network ) version of this phase - based frame prediction , as depicted in figure  [ fig : phase_prediction].(b ) .",
    "thus , predicting motion @xmath10 timesteps away from the input .    for evaluating motion prediction we use the same datasets as in @xcite  hmdb51 and ucf101 , where the authors aim at predicting optical - flow based motion in single images .",
    "to evaluate the difference between the predicted motion and the actual video motion , we use pixel accuracy , as in our method we recover the appearance of the future frame . for comparison with @xcite which reports epe ( end point errors ) , we use their chosen optical flow estimation algorithm to recover optical flow from our predicted rgb .",
    "similar to @xcite , where the style of a given target painting is transferred to another image , we propose to transfer the short motion of a given video sequence to an input static image . in @xcite",
    "a combination of two losses is optimized : content loss which ensures that the objects present in the newly generated image remain recognizable and correspond to the ones in the input image , and a style loss which imposes that the artistic style of the new image is similar to the one of the provided target painting . for motion transfer",
    "we have an additional requirement , namely that parts of the image that are similar  e.g. horses , people , should move similar . for this",
    "we use two pretrained network streams , an rgb stream and a phase stream and consider certain convolutional layers along these streams for estimation rgb / phase responses .",
    "therefore , we first estimate an element - wise correlation between the responses at a given convolutional network layer of the input rgb values of the static image and the target video frame : @xmath11 where @xmath12 is the number of channels in the layer @xmath13 , @xmath14 and @xmath15 the responses at layer @xmath13 for the input image and video frame , respectively .",
    "following @xcite , we subsequently define our motion - style loss by weighting the feature maps in the gram matrix computation by the appearance correlation .",
    "the motion transfer is obtained by enforcing that the phase of objects over time in the input image , should be similar to the phase over time of the same objects present in the target video .",
    "the motion - style loss optimization is performed per phase - scale .",
    "@xmath16 & = \\sum_k^{m_l } \\mathcal{k}^l_k f^l_\\sigma[ik ] f^l_\\sigma[kj],\\text { } i , j\\in \\{1, .. n_l\\},\\\\      a^l_\\sigma[ij ] & = \\sum_k^{m_l } \\mathcal{k}^l_k p^l_\\sigma[ik ]",
    "p^l_\\sigma[kj],\\text { } i , j\\in \\{1, .. n_l\\},\\\\      \\mathcal{l}_l & = \\sum_\\sigma \\frac{1}{n_l^2 m_l^2}\\sum_{i , j}^{n_l } \\mathcal{k}^l_j ( g^l_\\sigma[ij ] - a^l_\\sigma[ij])^2,\\end{aligned}\\ ] ] where @xmath17 is the number of elements in one channel of layer @xmath13 , @xmath3 indicates the phase - scale , and @xmath18 is the weighted gram matrix of the phase - image to be generated , while @xmath19 is the weighted gram matrix of the current video frame and , @xmath20 and @xmath21 are the responses of the phase - image to be generated and the phase - image of the input video frame , respectively .",
    "because we want the find similar looking objects by using the element - wise correlations , we expect that the higher convolutional levels of the network will perform better .",
    "we additionally also add the content loss term of @xcite to avoid large distortions of the image appearance .",
    "due to the input being a static image , only short video motions can be transferred in this case .    for evaluating motion transfer",
    ", we perform a two - step evaluation . in the first step , we select an existing video frame and transfer the video motion to the selected frame and compare the transferred motion with the actual video motion . for this",
    "we use videos from hmdb51 and ucf101 .",
    "the second evaluation is transferring the motion to actual static images . for this",
    "we select images from the static willow dataset @xcite and transfer the motion of corresponding videos from the hmdb51 and ucf101 datasets containing the same objects .",
    "for this we provide the static images animated with the transferred video motion .",
    "+      we use as a starting point the work of @xcite , where artistic style is transferred to video .",
    "however in our case , the motion of one given video is transferred to another input video .",
    "the gain in so doing , is that we can transfer the style of performing a certain action .",
    "for example an amateur performing the moonwalk can be lifted to the expert level by transferring the motion of michael jackson himself .",
    "the idea of transferring motion in videos is similar to the idea of transferring motion in static images , with the additional constraint that the motion must be temporally coherent . for this ,",
    "similar to @xcite , we add a temporal loss term to the motion transfer loss discussed in section  [ ssec : transfer_im ] .    for performing motion transfer between videos , we use a set of target videos : the walk of charlie chaplin , the moonwalk of michael jackson , and the walk of a runway model .",
    "we transfer these walking styles to a set of input videos of people walking , and provide the results as a qualitative form of evaluation .      here .",
    "] , we show a very simple proof of concept for phase - based motion transfer . we animate a static image by transfering the motion of another semantic related video . correctly aligning the moving entities between the video frames and",
    "the static image is essential for this task . for this proof of concept",
    "the alignment was not very good and no learning was used whatsoever .",
    "misalignment errors show up as artifacts in the results and we expect that adding ( deep ) learning will improve results .",
    "we propose an eulerian phase - based approach to motion representation learning .",
    "we argue for the intrinsic stability offered by the phase - based motion description .",
    "a phase - based approach does not require pixel tracking and directly encodes flux .",
    "phase is an innate property of an image and does not rely on hand - crafted optical - flow algorithms .",
    "we explore a set of motion learning tasks in an eulerian setting : ( a ) action recognition , ( b ) motion prediction in static images , ( c ) motion transfer from a video to a static image and ( d ) motion transfer in videos . for each one of these tasks",
    "we propose a phase - based approach and provide a small proof of concept .",
    "we do not offer in - depth experimental results but instead make a case for a brave new motion representation with phase ."
  ],
  "abstract_text": [
    "<S> this work advocates eulerian motion representation learning over the current standard lagrangian optical flow model . </S>",
    "<S> eulerian motion is well captured by using phase , as obtained by decomposing the image through a complex - steerable pyramid . </S>",
    "<S> we discuss the gain of eulerian motion in a set of practical use cases : ( i ) action recognition , ( ii ) motion prediction in static images , ( iii ) motion transfer in static images and , ( iv ) motion transfer in video . </S>",
    "<S> for each task we motivate the phase - based direction and provide a possible approach . </S>"
  ]
}