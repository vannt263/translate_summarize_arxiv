{
  "article_text": [
    "question answering ( qa ) aims to automatically understand natural language questions and to respond with actual answers .",
    "the state - of - the - art qa systems usually work relatively well for factoid , list and definition questions , but they might not necessarily work well for real world questions , where more comprehensive answers are required . frequently asked questions ( faq ) based qa is an economical and practical solution for general qa @xcite . instead of answering questions from scratch",
    ", faq - based qa tries to search the faq archives and check if a similar question was previously asked .",
    "if a similar question is found , the corresponding answer is returned to the user .",
    "the faq archives are usually created by experts , so the returned answers are usually of higher - quality .",
    "the core of faq - based qa is to calculate semantic similarities between questions .",
    "this is a very challenging task , because two questions , which share the same meaning , may be quite different at the word or syntactic level .",
    "for example ,  how do i add a vehicle to this policy ? \" and  what should i do to extend this policy for my new car ?",
    "\" have few words in common , but they share the same answer . in the past two decades , many efforts have been made to tackle this lexical gap problem .",
    "one type of methods tried to bridge the lexical gap by utilizing semantic lexicons , like wordnet @xcite .",
    "another method treated this task as a statistical machine translation problem , and employed a parallel question set to learn word - to - word or phrase - to - phrase translation probabilities @xcite . both of these methods have drawbacks .",
    "the first method is hard to adapt to many other languages , because the semantic lexicon is unavailable . for the second method",
    ", a large parallel question set is required to learn the translation probabilities , which is usually hard or expensive to acquire . to overcome these drawbacks , we utilize distributed word representations to calculate the similarity between words , which can be easily trained by only using amount of monolingual data .    in this paper",
    ", we propose a novel word - alignment - based method to solve the faq - based qa tasks .",
    "the characteristics of our method include : ( 1 ) a neural network model for calculating question similarity with word alignment features . for an input question and a candidate question , the similarities of each word pairs ( between the two questions )",
    "are calculated first , and then the best word alignment for the two questions is computed .",
    "we extract a vector of dense features from the word alignment , then import the feature vector into a neural network and calculate the question similarity in the network s output layer .",
    "( 2 ) a bootstrap - based feature extraction method .",
    "the faq archives usually contain less than a few hundred questions , and in order to avoid overfitting , we are unable to use too many sparse features .",
    "therefore , we come up with this method to extract a small set of effective sparse features according to our system s ranking results .",
    "( 3 ) a learning - to - rank algorithm for training .",
    "the faq - based qa task is essentially a ranking task , our model not only needs to calculate a proper similarity for each question pair , but also needs to rank the most relevant one on top of the other candidates .",
    "so we propose a learning - to - rank method to train parameters more suitable for ranking .",
    "experimental results , conducted on faq archives from three languages , demonstrate that our method is very effective . we also evaluate our method on the answer sentence selection task .",
    "experimental results on the standard trec data set show that our method outperforms all previous state - of - the - art systems .",
    "the task of faq - based qa is that given a _ query _ question , rank all the _ candidate _",
    "questions according to the similarities between two questions .",
    "we define the similarity between a query @xmath0 and a candidate @xmath1 as @xmath2 , where @xmath3 is a feature vector extracted from ( @xmath0 , @xmath1 ) pair , and @xmath4 is a linear or non - linear function . in this work ,",
    "we represent @xmath4 as a neural network model , where the input is the feature vector @xmath3 , and the output layer contains only one neuron which predicts the similarity of the two questions .",
    "we choose the _ sigmoid _ activation function for the output layer , so that the similarity is constrained in the range [ 0 , 1 ] . in order to execute this model",
    ", we still have two remaining questions : ( 1 ) how to represent the feature vector @xmath3 ? ( 2 ) how to train the model for ranking ?      for the first question , we propose to extract features from the word alignment between two questions .",
    "let s denote the query question as @xmath5 and the candidate question as @xmath6 , where @xmath7 and @xmath8 represent words in questions .",
    "first , we calculate the word similarity between @xmath7 and @xmath8 according to the cosine distance of their distributed representations @xmath9 and @xmath10 @xmath11 then , we obtain the similarity matrix for the question pair by calculating similarities of all word pairs , e.g. figure [ fig : wordalign](a ) . finally , we compute the best word alignment for the question pair based on the similarity matrix , e.g. figure [ fig : wordalign](b ) shows the word alignment computed based on figure [ fig : wordalign](a ) .",
    "we define some dense features based on the word alignment .",
    "let s denote the alignment position for each query word @xmath7 as @xmath12 , and the corresponding alignment score as @xmath13 .",
    "for example , for word @xmath14 in figure [ fig : wordalign ] , @xmath15 and @xmath16 . we denote the unaligned word as @xmath17 .",
    "we also take into account the importance of each word by employing its inverse document frequency ( idf ) score , and denote it as @xmath18 .",
    "we define the following dense features :    * * similarity * : @xmath19 .",
    "this feature measures the question similarity based on the aligned words . *",
    "* dispersion * : @xmath20 .",
    "this feature prefers the candidate where contiguous query words are aligned to contiguous words in the candidate . * * penalty * : @xmath21 .",
    "this feature penalizes candidates based on the unaligned query words . *",
    "* 5 important words * : @xmath22 .",
    "this feature type contains 5 features .",
    "each feature shows the alignment score of the i - th important words , where we evaluate the importance of a word by its idf score .",
    "* * reverse * : extract above features by swapping roles of query and candidate questions .",
    "we also define some spare lexical features .",
    "considering the fact that our faq archives contain only less than a few hundred questions , we can not extract too manny sparse features , otherwise our model will overfit the training set .",
    "we design a bootstrap - based feature extraction method to extract a small set of effective lexical features according to the model s ranking results .",
    "the input to our method contains a seed model , a faq archive and a set of sparse feature templates . for each query question ,",
    "the workflow includes :    * * step 1 * : rank all the candidates with the seed model .",
    "if the rank-1 candidate is relevant , return without doing anything . *",
    "* step 2 * : find the first relevant candidate @xmath23 from the ranking list , and collect all the irrelevant candidates \\{@xmath24 } above @xmath23 . * * step 3 * : collect sparse features @xmath25 from @xmath23 , and sparse features @xmath26 from \\{@xmath24}. then , only keep the sparse features occurred in @xmath25 but not occurred in @xmath26 .",
    "we use this method to extract a group of sparse features , then add these features to the model and retrain the model .",
    "this procedure can iterate many times until getting a stable performance .",
    "our feature templates contain : aligned query words , aligned candidate words and aligned query - candidate word pairs . in our experiments ,",
    "the performance can converge within 10 iterations , and the final model usually contains less than 1,500 sparse features .",
    "the traditional method for training the similarity model is to cast the task as a binary classification problem @xcite .",
    "all the possible question pairs are collected from the training set , and if the question pair is relevant , assign a label ",
    "+ 1 \" , otherwise assign a label  -1 \" .",
    "then the model is trained to optimize the classification accuracy",
    ". however , the faq - based qa task is essentially a ranking problem .",
    "the similarity model not only needs to calculate a proper similarity for each question pair , but also needs to rank the most relevant candidate on top of the others .",
    "therefore , we propose a novel learning - to - rank algorithm to explicitly optimize the ranking ( top-1 ) accuracy .",
    "we define the loss function for each query @xmath27 and all its irrelevant candidates \\{@xmath28 } as : @xmath29 where @xmath30 is a margin , and @xmath31 is the first relevant candidate in the ranking list .",
    "@xmath32 aims to decrease the similarities for the irrelevant candidates ranked above @xmath31 or below @xmath31 but with a margin less than @xmath30 , and @xmath33 aims to improve the similarity of @xmath31 .",
    "we utilize the back propagation algorithm @xcite to minimize the loss function over the training set , and employ the adagrad strategy @xcite . in our experiments",
    ", we set @xmath30 as 0.03 and the learning rate as 0.1 .",
    "we conducted experiments on faq archives from three languages ( english , spanish and japanese ) .",
    "these faq archives are collected from customer service or online q&a webpage of three companies .",
    "the number of question - answer pairs for each archive is 987 ( english ) , 687 ( spanish ) and 2384 ( japanese ) .",
    "each question may have more than one relevant questions within the archives .",
    "we split each archive into train , dev and test sets .",
    "the number of questions for each set is 790/99/98 , 549/69/69 and 1684/350/350 .",
    "to train distributed word representations and calculate idf scores , we employed the english gigaword ( ldc2011t07 ) , the spanish gigaword ( ldc2011t12 ) and an in - house japanese corpus ( about 2 billion tokens ) .",
    "these corpus were preprocessed by our in - house tokenizer , and the word2vec toolkit was used for training the distributed word representations .",
    "first , we conducted a group of experiments by incrementally adding features .",
    "we used the learning - to - rank algorithm to train models , but did nt use hidden layer for these models .",
    "figure [ fig : curves](a ) shows the top-1 accuracies on the dev sets .",
    "we found that the  dispersion \" and  penalty \" features are effective for english .",
    "the  five important words \" features are helpful for both spanish and japanese .",
    "reverse \" feature is very useful for english and japanese .",
    "the  sparse \" features bring around 5% improvements for all languages .",
    "second , we verified the effectiveness of our learning - to - rank algorithm .",
    "we built two systems : the first one takes only the dense features , and the second one takes both dense and sparse features .",
    "the two systems were trained with both the traditional classification method and our learning - to - rank method .",
    "experimental results on dev sets are given in figure [ fig : curves](b ) .",
    "we see that the learning - to - rank method works consistently better than the classification method .",
    "third , we illustrated the influence of the neural network structure by changing the number of hidden neurons .",
    "we found the model acquired the best performance when using 300 hidden neurons .",
    "figure [ fig : curves](c ) shows the performance on the dev sets of two systems .",
    "the first system has no hidden layer , and the second one employs 300 hidden neurons",
    ". we can find that using the hidden layer is really helpful for the final performance .      in this section , we evaluated our systems on the test sets .",
    "we tested three systems : ( 1 )  dense \" takes the dense features , ( 2 )  sparse \" takes both dense and sparse features , and ( 3 )  sparsehidden \" adds 300 hidden neurons to the second system .",
    "we also designed three baseline systems : ( 1 )  bagofword \" calculates question similarity by counting how many query words also occur in the candidate ; ( 2 )  idf - vsm \" represents each question with vector space model ( vsm ) ( each dimension is the idf score of the corresponding word ) , and calculates the cosine distance as the question similarity ; ( 3 )  similarity \" only uses our  similarity \" feature .",
    "table [ tab : testresult ] gives the top-1 accuracies .",
    "the  bagofword `` and ' ' idf - vsm \" systems only counted the exactly matched words , so they did nt work well .",
    "the  similarity \" system got some improvements by matching words with distributed word representations .",
    "the performance of the baseline systems also showed the difficulty of this task . by adding dense and sparse features extracted from word alignment , our systems significantly outperformed the baseline systems .      to compare with other state - of - the - art systems",
    ", we further evaluated our system on the answer sentence selection task with the standard trec data set @xcite .",
    "the task is to rank candidate answers for each question , which is very similar to our faq - based qa task .",
    "we used the same experimental setup as , and evaluated the result with mean average precision ( map ) and mean reciprocal rank ( mrr ) .",
    "table [ tab : comparison ] shows the performance from our system and the state - of - the - art systems .",
    "we observe our system get a significant improvement than the other systems .",
    "therefore , our method is quite effective for this kind of ranking tasks .",
    ".evaluation on the test set [ cols=\"<,^,^,^\",options=\"header \" , ]",
    "in this paper , we propose a question similarity model to extract features from word alignment between two questions .",
    "we also come up with a bootstrap - based feature extraction method to extract a small set of effective lexical features . by training the model with our learning - to - rank algorithm ,",
    "the model works very well for both the faq - based qa task and the answer sentence selection task .",
    "adam berger and john lafferty .",
    "information retrieval as statistical translation . in _ proceedings of the 22nd annual international acm sigir conference on research and development in information retrieval _ , pages 222229 .",
    "delphine bernhard and iryna gurevych .",
    "2009 . combining lexical semantic resources with question & answer archives for translation - based answer finding . in _ proceedings of the joint conference of the 47th annual meeting of the acl and the 4th international joint conference on natural language processing of the afnlp : volume 2-volume 2 _ , pages 728736 .",
    "association for computational linguistics .",
    "robin  d burke , kristian  j hammond , vladimir kulyukin , steven  l lytinen , noriko tomuro , and scott schoenberg .",
    "question answering from frequently asked question files : experiences with the faq finder system .",
    ", 18(2):57 .",
    "michael heilman and noah  a smith .",
    "tree edit models for recognizing textual entailments , paraphrases , and answers to questions . in _",
    "human language technologies : the 2010 annual conference of the north american chapter of the association for computational linguistics _ , pages 10111019 .",
    "association for computational linguistics .",
    "jiwoon jeon , w  bruce croft , and joon  ho lee .",
    "finding similar questions in large question and answer archives . in _ proceedings of the 14th acm international conference on information and knowledge management _ , pages 8490 .",
    "jung - tae lee , sang - bum kim , young - in song , and hae - chang rim .",
    "2008 . bridging lexical gaps between queries and questions on large online q&a collections with compact translation models . in _ proceedings of the conference on empirical methods in natural language processing _ , pages 410418 .",
    "association for computational linguistics .",
    "xiaobing xue , jiwoon jeon , and w  bruce croft .",
    "retrieval models for question and answer archives . in _ proceedings of the 31st annual international acm sigir conference on research and development in information retrieval _ , pages 475482 .",
    "acm .",
    "guangyou zhou , li  cai , jun zhao , and kang liu . 2011 .",
    "phrase - based translation model for question retrieval in community question answer archives . in _ proceedings of the 49th annual meeting of the association for computational linguistics : human language technologies - volume 1 _ , pages 653662 .",
    "association for computational linguistics ."
  ],
  "abstract_text": [
    "<S> in this paper , we propose a novel word - alignment - based method to solve the faq - based question answering task . </S>",
    "<S> first , we employ a neural network model to calculate question similarity , where the word alignment between two questions is used for extracting features . </S>",
    "<S> second , we design a bootstrap - based feature extraction method to extract a small set of effective lexical features . </S>",
    "<S> third , we propose a learning - to - rank algorithm to train parameters more suitable for the ranking tasks . </S>",
    "<S> experimental results , conducted on three languages ( english , spanish and japanese ) , demonstrate that the question similarity model is more effective than baseline systems , the sparse features bring 5% improvements on top-1 accuracy , and the learning - to - rank algorithm works significantly better than the traditional method . </S>",
    "<S> we further evaluate our method on the answer sentence selection task . </S>",
    "<S> our method outperforms all the previous systems on the standard trec data set . </S>"
  ]
}