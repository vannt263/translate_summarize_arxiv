{
  "article_text": [
    "message passing algorithms have important applications in various contexts ranging from random constraint satisfaction problems  @xcite , supervised learning  @xcite to information theory  @xcite . in recent years",
    ", the idea of message passing was introduced into the study of the inverse ising problem  @xcite .",
    "given observed magnetizations ( mean activities ) and pairwise correlations @xmath0 , the inverse ising problem aims at finding the underlying parameters @xmath1 , local fields and coupling constants to describe the statistics of the experimental data which can be collected either in real experiments ( e.g. , microarray measurements in gene expression experiments or multi - electrode recordings in a neuronal population ) or in monte carlo simulations .",
    "the active research of inverse ising problem is mainly motivated by the observation of correlated activity in the retinal network  @xcite , the cortical network  @xcite and other biological networks  @xcite .",
    "encouragingly , the pairwise ising model , as a least structured model , was shown to be capable of capturing most of the correlation structure of the network activity  @xcite .",
    "based on the ising model , computationally efficient inverse algorithms were proposed to analyze multi - electrode recordings in the salamander retina  @xcite and to identify correlation between amino acid positions in interacting proteins  @xcite .",
    "the pairwise ising model only requires @xmath2 parameters to describe the original distribution and is thus attractive for dimensional reduction in modeling vast amounts of biological data ( for a general statistical physics analysis of the pairwise ising model on the inverse ising problem , see , e.g. , ref .",
    "@xcite ) .    for large system",
    ", the inverse ising problem is known to be a hard computational problem  @xcite .",
    "various approximate schemes were proposed to tackle this problem .",
    "as one of these approximations , message passing strategy looks promising and the susceptibility propagation ( susprop ) algorithm has been derived to infer couplings of sherrington - kirkpatrick ( sk ) model  @xcite .",
    "susprop solves a closed set of equations by iteration , passing messages along the directed edges of the factor graph representation  @xcite of the problem .",
    "the new message is computed only based on the incoming messages .",
    "this locally updating feature makes susprop fully distributed and amenable to parallelization .",
    "correlation information of any two variables provided by susprop can be used not only to decimation procedure in solving some hard constraint satisfaction problems  @xcite but also to the network reconstruction  @xcite .",
    "dependence of the performance of susprop on the quality of the originally observed data has been studied in ref .",
    "at high temperature , the quality of reconstruction is constrained by the implementation precision of the algorithm and the random noise embedded in the supplied data .",
    "statistical errors presented in the monte carlo noisy data could also have detrimental effects on the reconstruction performance .",
    "_ in a recent work  @xcite studied the dynamical behavior of susprop on inferring couplings from synthetic data of sk model .",
    "they found that , at the low temperature ( @xmath3 ) , the algorithm does nt converge typically with diverging inferred couplings , however , by introducing a stopping criterion , the threshold could be pushed to lower value . a transition from reconstructible to non - reconstructible phase for susprop",
    "was also observed in their numerical simulations .",
    "high absolute magnetization was claimed to have negative effects on the performance of the algorithm .",
    "all aforementioned investigations were restricted to the sk model .",
    "mean field schemes based on inversion of correlation matrix have been recently tested on hopfield networks  @xcite .",
    "regarding these mean field schemes , the simple ones are naive mean field ( nmf ) method and independent pair ( ind ) approximation , and the more advanced ones inversion of thouless - anderson - palmer ( tap ) as well as sessak - monasson ( sm ) approximation ( for details , see refs .",
    "@xcite , a brief description is also given in appendix  [ app_mfs ] ) .",
    "it was shown that all mean field schemes fail to extract interactions within a desired accuracy in the retrieval phase .",
    "et al . _",
    "@xcite applied recently belief propagation plus an auxiliary updating external field to infer couplings of the sparse hopfield network when the system settles in the retrieval phase .",
    "they showed that inference error with sampling from single basin of the stored pattern is much similar while error with sampling from multiple basins is drastically reduced .    in the present work we will examine the reconstruction performance of susprop on both the fully connected and sparse hopfield networks and discuss the limitation of this message passing algorithm .",
    "improvements over other existing mean field methods are reported and a threshold behavior relative to susprop is observed in our simulations on single instances . the rest of this paper is organized as follows .",
    "the definition of the hopfield network is given in sec .",
    "[ sec_hopf ] followed by the detailed demonstration of susprop in sec .",
    "[ sec_mpa ] . in sec .",
    "[ sec_result ] , we report improvements achieved by susprop and discuss its limitation and threshold behavior .",
    "conclusions and future perspectives are devoted to sec .",
    "[ sec_con ] .",
    "the hopfield model was proposed to mimic the memory and recall functions of real neuronal networks  @xcite .",
    "it yields rich statistical physics properties and is moreover a simple model to assess the efficiency of various inverse algorithms  @xcite , which may have some implications for neuroscience .",
    "the equilibrium properties of the hopfield network are governed by the following hamiltonian : @xmath4 where @xmath5 describes the state of each neuron in the network .",
    "@xmath6 indicates the spiking of neuron @xmath7 and @xmath8 the silence of the neuron .",
    "coupling @xmath9 is constructed according to the hebb s rule : @xmath10 where @xmath11 taking @xmath12 with equal probability are @xmath13 stored random patterns .",
    "the ratio of the number of stored patterns to the network size @xmath14 is termed the memory load of the network , i.e. , @xmath15 . in the fully connected network",
    ", each neuron is connected to all the other neurons and no self - interactions are assumed .",
    "the mean field behavior of the fully connected hopfield model has been thoroughly studied in refs .",
    "@xcite . when @xmath16 , paramagnetic , spin glass and metastable ferromagnetic retrieval phases appear in order as the temperature decreases .",
    "the retrieval phase becomes stable at low temperatures if @xmath17 .",
    "replica symmetry breaking occurs for the retrieval phase only at very low temperatures .",
    "however , the replica symmetry solution for the spin glass phase are unstable in the entire spin glass phase  @xcite . for the sparse network ,",
    "the coupling or interaction is constructed as @xmath18 where @xmath19 is the mean degree of each neuron . in the thermodynamic limit",
    ", @xmath13 scales as @xmath20 where @xmath21 is the memory load .",
    "no self - interactions are also assumed and the connectivity @xmath22 follows the distribution : @xmath23 mean field properties of the sparse hopfield network have been discussed within replica symmetric approximation in refs .",
    "three phases ( paramagnetic , retrieval and spin glass phases ) have also been observed in this sparsely connected hopfield network with arbitrary finite @xmath19 . for large @xmath19 ( e.g. , @xmath24 )",
    ", the phase diagram resembles closely that of extremely diluted case  @xcite where the transition line between paramagnetic and retrieval phase is @xmath25 for @xmath26 and that between paramagnetic and spin glass phase @xmath27 for @xmath28 .",
    "the spin glass / retrieval transition occurs at @xmath29 .",
    "we simulate the hopfield network using glauber dynamics plus simulated annealing techniques to collect enough experimental data ( totally @xmath30 samplings ) : @xmath31 where @xmath32 denotes the average over the collected data ( simulation details are given in appendix  [ app_sim ] ) , then we estimate the couplings @xmath33 between neurons based on these measured magnetizations and two - point connected correlations , such that the resulting ising distribution @xmath34 $ ] is able to provide an accurate description of the statistics of the experimental data . note",
    "that @xmath35 is the inferred value and has been scaled by the inverse temperature @xmath36 .",
    "the external field @xmath37 is zero for all neurons in the current hopfield model . in other cases",
    "@xcite , the external field can be used to represent the preferred direction of @xmath5 .",
    "neurons in the hopfield network usually interact with each other to yield collective behavior at the network level .",
    "@xmath38 measure the tendency for each pair of neurons to spike cooperatively . together with the information about mean firing rates",
    "@xmath39 , they can be used as inputs to susprop for the network reconstruction .",
    "susprop passes messages along the directed edges of the network by iterative updating .",
    "to iterate susprop , two kinds of messages are needed .",
    "we first define the cavity magnetization @xmath40 as the message propagating from neuron @xmath7 to neuron @xmath41 , then define the other kind of message , namely the cavity susceptibility @xmath42 where @xmath43 is termed cavity field of neuron @xmath7 in the absence of neuron @xmath41 and @xmath44 is the local perturbation .",
    "the susprop then reads as follows :    [ susp ]",
    "@xmath45+(1-\\epsilon)j_{ij}^{{\\rm old}}\\\\ \\widetilde{c_{ij}}&=\\frac{c_{ij}-(1-m_{i}^{2})g_{i\\rightarrow j , j}}{g_{j\\rightarrow i , j}}+m_{i}m_{j}\\end{aligned}\\ ] ]    where @xmath46 denotes neighbors of neuron @xmath7 except @xmath41 , @xmath47 is the kronecker delta function and @xmath48 serves as a damping factor .",
    "the damping factor allows the new @xmath9 to memorize a given fraction ( @xmath49 ) of @xmath9 computed at the last step ( denoted as @xmath50 ) , which helps susprop converge to a fixed point if the temperature is not very low although the convergence is slowed down . in our current simulations",
    ", we employ very small @xmath48 of order from @xmath51 to @xmath52 .",
    "detailed derivation of susprop is given in appendix  [ app_susp ] .",
    "for other discussions of this algorithm , we refer the reader to previous works  @xcite .",
    "the susprop algorithm is able to estimate the correlation between any two variables even if they are not directly linked in the network  @xcite and this information could be used further to update couplings . furthermore , the memory term @xmath48 ensures the update of @xmath9 towards its true value step by step when the temperature is not very low . for the fully connected network",
    ", susprop has the complexity of @xmath53 .    to assess the reconstruction performance of susprop , we define intuitively the inference error as @xmath54^{1/2}\\ ] ] where @xmath35 is the inferred value of the coupling and @xmath55 the original coupling constructed according to the hebb s rule eq .",
    "( [ hebb ] ) or eq .",
    "( [ j_spar ] ) . to run susprop , we initially set all @xmath9 to be zero , and randomly initialize for every edge of the network the message @xmath56 $ ] and @xmath57 if @xmath58 and @xmath59 otherwise .",
    "then susprop is iterated according to eq .",
    "( [ susp ] ) until the inferred couplings converge or the preset maximal number of iterations @xmath60 is saturated . in our simulations , we adopt convergence criterion @xmath61 , i.e. , convergence of susprop is identified once all updated couplings have converged within precision @xmath62 .    .",
    "lines are guides to the eye .",
    "each point is an average over five random samples and error bars are also shown .",
    ", scaledwidth=70.0% ]    .",
    "lines are guides to the eye .",
    "each point represents an average over five random samples and error bars are also shown .",
    "( a ) comparison of reconstruction performances of susprop and other mean field schemes for @xmath63 .",
    "( b ) inference error versus temperature for susprop with different memory loads .",
    ", title=\"fig:\",width=321 ] .5 cm .",
    "lines are guides to the eye .",
    "each point represents an average over five random samples and error bars are also shown .",
    "( a ) comparison of reconstruction performances of susprop and other mean field schemes for @xmath63 .",
    "( b ) inference error versus temperature for susprop with different memory loads .",
    ", title=\"fig:\",width=321 ] .2 cm    .",
    "lines are guides to the eye . each point is an average over five random samples and error bars are also shown .",
    "efficiencies of susprop and other mean field schemes are compared , so are the reconstruction performances of susprop with and without prior knowledge on the sparseness of the network . , scaledwidth=70.0% ]     for susprop .",
    "different network sizes are considered with the same memory load @xmath64 .",
    "twenty random samples are simulated and a good reconstruction is identified when @xmath65 .",
    ", scaledwidth=70.0% ]",
    "to avoid the expensive computational cost , we assess the reconstruction performance of susprop only on small size networks .",
    "the phase diagram of the hopfield model has been derived for the fully connected case  @xcite and the finite connectivity case  @xcite . in the finite size system ,",
    "we distinguish different phases by two order parameters ; one is the overlap between the network configuration and the @xmath66th pattern @xmath67 and the other is the mean - squared magnetization @xmath68 . to implement susprop , @xmath60 is set to be @xmath69 and we need to adopt an appropriate damping factor to prevent the absolute updated @xmath70 from being larger than one .",
    "comparison of reconstruction performances of susprop and other mean field schemes is shown in fig .",
    "[ error_fva ] .",
    "susprop turns out to be the most efficient , reducing the inference error by a significant amount .",
    "moreover , it seems to be less sensitive to the memory load compared with other mean field methods .",
    "[ error_fvt ] reports the inference error as a function of temperature for various reconstruction algorithms .",
    "susprop operates fairly accurately and extends the reconstructible region well into a much lower temperature down to @xmath71 .",
    "however , when the system settles in low temperatures ( @xmath72 ) , susprop first suffers highly magnetized data and its performance gets worse with non - convergence , which may be remedied by adopting much smaller @xmath48 and larger @xmath60 or by introducing a stopping criterion  @xcite .",
    "when the temperature approaches lower values ( e.g. , below @xmath71 in fig .",
    "[ error_fvt ] ( a ) ) , the collected data will then become frozen , i.e. , at least two of its magnetizations equal one in absolute value , as a result , susprop yields diverging couplings and fails to reconstruct the network . actually , in the high absolute magnetization case , both @xmath73 and @xmath74 ( and @xmath75 ) are very close to one on some edges @xmath76 .",
    "we then rewrite @xmath77 as @xmath78 , and similarly @xmath79 where @xmath80 is a sign function ; @xmath81 and @xmath82 are small positive values compared to one .",
    "then one can readily recast @xmath70 according to eq .",
    "( [ susp ] ) as @xmath83 for @xmath84 and @xmath85 for @xmath86 .",
    "if @xmath87 and neither of @xmath81 and @xmath82 vanishes , the estimated @xmath9 remains finite and susprop does work .",
    "in other cases ( e.g. , @xmath88 and @xmath89 ) , @xmath9 suffers divergence and susprop is unable to infer the couplings .",
    "in this situation , at least two of the supplied magnetizations are equal to one in absolute value , e.g. , @xmath90 and @xmath91 , under the update rule eq .",
    "( [ susp ] ) , @xmath92 and @xmath93 .",
    "these highly polarized messages will then spread out in the network , which yields an infinite value for @xmath9 as explained above .",
    "a simple physical interpretation is that , in this case , @xmath94 , and this implies that some neurons in the network tend to behave independently of other neurons and thus susprop could nt get all information about correlations of the network , which leads to the failure of reconstruction . in the current hopfield model , the frozen type data does appear for small enough @xmath21 and @xmath95 where the system gets trapped by one of stable or metastable memory states  @xcite .",
    "increasing @xmath21 but maintaining low @xmath95 , many spurious minima show up and monte carlo sampling becomes very difficult  @xcite , which produces fairly noisy collected data . on the other hand , the current susprop hasnt taken into account the complex structure of the phase space ,",
    "therefore it remains a non - trivial issue for susprop to deal with this more involved case .",
    "the reconstruction error against temperature is also shown with respect to different memory loads for susprop in fig .",
    "[ error_fvt ] ( b ) . in the high temperature region ( @xmath96 ) , the smaller the memory load is , the more precisely susprop reconstructs the hopfield network . as temperature decreases further , susprop becomes less precise for all memory loads while still maintaining a relatively small error .",
    "susprop is applied to reconstruct the fully connected hopfield network , whereas , it shows a surprisingly good performance . if the network is sparse and locally treelike , susprop is believed to be fast and able to give a precise estimation . to test its efficiency on reconstructing the sparse network , we compare performances of susprop with those of other mean field schemes in fig .",
    "[ error_svt ] .",
    "it is clearly shown that susprop performs exceedingly well particularly in the low temperature region ( down to @xmath97 ) and exhibits less sample - to - sample fluctuations .",
    "if we have a prior knowledge of the sparseness of the network , i.e. , we know a priori the connectivity pattern of the network , the inference error could be reduced substantially . in this case",
    ", we only infer the strength of interaction between neurons which are really connected .",
    "in fact , when the system is presented at the high temperature , one can reconstruct the network using an appropriate cutoff since the estimated couplings between unconnected neurons are very small compared to those between really connected neurons .    in fig .",
    "[ threshold ] , we report the fraction of good reconstruction versus temperature for different network sizes at fixed memory load @xmath64 .",
    "a good reconstruction is identified when @xmath98 and susprop converges within @xmath60 .",
    "as increasing temperature , a threshold behavior is observed and the transition becomes sharper with growing network size .",
    "given large enough @xmath14 , the probability that susprop gives good reconstruction tends to be zero when the temperature is below the critical value , and at a high enough temperature , susprop succeeds in reconstructing the network in all instances .",
    "the critical temperature is estimated to be about @xmath99 . for a more precise estimation , more samples and larger network size",
    "are required .",
    "the threshold behavior of susprop on network reconstruction has also been observed in sk model  @xcite .",
    "susprop solves the inverse ising problem by iteratively updating messages along the directed edges of the network , and is shown in the present work to outperform all other mean - field - type schemes and extend the reconstructible region into lower temperatures .",
    "we also study the fraction of good reconstruction as a function of temperature and a threshold behavior is observed .",
    "the transition from good reconstruction to poor one becomes sharp as increasing network size and the critical temperature is estimated to be about @xmath99 .",
    "susprop is also amazingly efficient for reconstructing sparse hopfield network and its performance can be further improved by introducing a prior knowledge about the sparseness of the network .",
    "the sparse case is more relevant in modeling real biological data than its dense counterpart .",
    "we hope our analysis of susprop on the hopfield network reconstruction can be extended to the more biologically relevant cases .    at high temperatures , the performance of susprop",
    "is believed to be limited by the quality of the supplied data  @xcite .",
    "once the system is presented at the low temperature , the efficiency of susprop is also determined by the nature of the input data . in the presence of highly magnetized data , the reconstruction performance of susprop",
    "gets deteriorated with a high inference error .",
    "furthermore , the frozen type data makes updated couplings diverge and any value of damping factor can not overcome this hurdle , reminiscent of the fact that the frozen phase in random constraint satisfaction problems is most difficult for any known algorithm  @xcite .",
    "for large enough @xmath21 but low enough @xmath95 , the system enters the spin glass phase where glauber dynamics is easily trapped by one of the spurious minima correlated or uncorrelated with the stored patterns .",
    "susprop fails to extract couplings precisely in this region since it does not take into account the complex structure of the phase space .",
    "in fact , at finite temperatures , the support of cavity field distributions becomes real - valued and a sampling procedure is required . on the other hand , other values of parisi parameter ( smaller than the optimal value associated with the ground states ) also carry physical information and can be used to describe the metastable states which trap glauber dynamics  @xcite .",
    "however , searching for an optimal parisi parameter ( also known as replica symmetry breaking parameter ) is also a time consuming task for the network reconstruction  @xcite .",
    "rather , provided that glauber dynamics gets stuck in some metastable state for a very long time , how much information can be extracted from this state about the couplings of the network remains an important issue for future study .",
    "helpful discussions with erik aurell , haijun zhou and pan zhang are acknowledged .",
    "the present work was in part supported by the national science foundation of china ( grant numbers 10774150 and 10834014 ) and the china 973-program ( grant number 2007cb935903 ) .",
    "the naive mean field theory gives @xmath100 where @xmath37 is the external field and @xmath101 the magnetization .",
    "using the fluctuation - response relation , @xmath102\\ ] ] one obtains the nmf prediction of couplings , @xmath103 where @xmath104 .      in this approximate scheme ,",
    "each pair of neurons are independent of other neurons of the system , i.e. , their joint probability @xmath105 $ ] where @xmath106 is the local field neuron @xmath107 feels when neuron @xmath108 is removed from the system",
    ". then the ind prediction is given by @xmath109\\ ] ] where @xmath110 .",
    "the sm prediction of couplings is derived based on a perturbative expansion in the correlations  @xcite and it can be formulated as @xmath111      the usual tap equation reads @xmath112  @xcite . differentiating the field @xmath37 with respect to the magnetization @xmath113 ,",
    "one readily obtains the tap prediction equation , @xmath114",
    "the rule for glauber dynamics can be generally expressed as @xmath115 where @xmath116 is the energy change due to such a flip .",
    "for the current hopfield model , the dynamics rule is recast into @xmath117\\ ] ] where @xmath36 is the inverse temperature and @xmath118 is the local field acting on @xmath5 .    in our numerical simulations",
    ", we update the state of each neuron according to eq .",
    "[ gdrule ] in a randomly asynchronous manner .",
    "we define a glauber dynamics step as @xmath14 proposed flips . introducing simulated annealing strategy , we set the initial temperature to be @xmath59 and the cooling rate @xmath119 . at each intermediate temperature , we run @xmath30 glauber dynamics steps . when the temperature is decreased to the desired one",
    ", we run another @xmath120 steps to calculate magnetizations and correlations .",
    "we sample the state of the network every @xmath121 steps . for high temperatures ( @xmath122 )",
    ", we run totally @xmath123 steps , among which the first @xmath124 steps are run for the system to reach the equilibrium state and the other @xmath124 steps for calculating magnetizations and correlations .",
    "to derive eq .",
    "[ susp ] , we first derive the susceptibility propagation equations for general @xmath125-body interaction problem ( @xmath126 for the hopfield model ) . using the factor graph representation  @xcite ,",
    "we denote @xmath127 as the function node representing the constraint imposed on a subset of spins @xmath128 ( @xmath129 denotes neighbors of the function node @xmath127 ) , and @xmath7 as variable node representing the spin on the factor graph .",
    "the belief propagation is then formulated as  @xcite    [ bpeq ] @xmath130\\\\ & = \\frac{m_{i}-\\tanh j_{a}\\prod_{j\\in\\partial a\\backslash i}m_{j\\rightarrow a } } { 1-m_{i}\\tanh j_{a}\\prod_{j\\in\\partial a\\backslash i}m_{j\\rightarrow a } } \\end{split}\\\\ \\tanh u_{b\\rightarrow i}&=\\tanh j_{b}\\prod_{j\\in\\partial b\\backslash i}m_{j\\rightarrow b }    \\end{aligned}\\ ] ]    where @xmath131 is the cavity field ( correspondingly @xmath132 is the cavity magnetization ) acting on spin @xmath5 in the absence of @xmath127 ; @xmath133 the cavity bias when @xmath7 is involved in @xmath134 only , and @xmath135 as well as @xmath136 have been rescaled by @xmath36 .",
    "we define cavity susceptibility @xmath137 . from the belief propagation equations ,",
    "one readily gets the update rule for @xmath138 : @xmath139g_{j\\rightarrow b , k}(1-m_{j\\rightarrow b}^{2 } ) \\end{split}\\ ] ] using the identity  @xcite : @xmath140 where @xmath141 and @xmath142 denotes the average under the joint probability distribution @xmath143 which can be computed from the belief propagation equations , one can re - express the correlations @xmath144 through the fluctuation - response relation : @xmath145\\\\ & = ( 1-m_{i}^{2})\\left[g_{i\\rightarrow a , j}+\\sum_{n\\in\\partial a\\backslash i}\\frac{\\partial u_{a\\rightarrow i}}{\\partial h_{n\\rightarrow a}}\\frac{\\partial h_{n\\rightarrow a}}{\\partial h_{j}}\\right]\\\\ & = ( 1-m_{i}^{2})\\left[g_{i\\rightarrow a , j}+\\sum_{n\\in\\partial a\\backslash i}\\frac{\\partial u_{a\\rightarrow i}}{\\partial h_{n\\rightarrow a}}g_{n\\rightarrow a , j}\\right ] \\end{split}\\ ] ] for the simple case , the hopfield network , @xmath146 , and the constraint @xmath127 involves only two neurons , say @xmath7 and @xmath41 . to obtain the update rule for @xmath9 ,",
    "we compute @xmath147 directly by assuming @xmath148 , finally we get @xmath149 where @xmath77 can be evaluated from eqs .",
    "[ identity ] and  [ correlation ] , i.e. , @xmath150 from eqs .",
    "[ bpeq ] and  [ cavity_sus ] , the update rules for @xmath40 and @xmath151 are finally obtained as follows ,"
  ],
  "abstract_text": [
    "<S> the hopfield network is reconstructed as an inverse ising problem by passing messages . </S>",
    "<S> the applied susceptibility propagation algorithm is shown to improve significantly on other mean - field - type methods and extends well into the low temperature region . however , this iterative algorithm is limited by the nature of the supplied data . </S>",
    "<S> its performance deteriorates as the data becomes highly magnetized , and this method finally fails in the presence of the frozen type data where at least two of its magnetizations are equal to one in absolute value . on the other hand , </S>",
    "<S> a threshold behavior is observed for the susceptibility propagation algorithm and the transition from good reconstruction to poor one becomes sharper as the network size increases . </S>"
  ]
}