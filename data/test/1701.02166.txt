{
  "article_text": [
    "object registration is an important task in computer vision that determines the position and the orientation of an object in camera - centered coordinates @xcite .",
    "an object of interest that was detected beforehand in a coarse 2d bounding box is fed into a registration system that can superimpose the desired translation and rotation of the object onto the raw camera image . by utilizing such a system",
    ", one can propose promising solutions for various problems related to scene understanding , augmented reality , control and navigation of robotics , _",
    "etc_. recent developments on visual depth sensors and their increasing ubiquity have allowed researchers to make use of the information acquired from these devices to facilitate challenging registration scenarios . + iterative closest point ( icp ) algorithm @xcite , point - to - model based methods @xcite and point - to - point techniques @xcite demonstrate good registration results .",
    "however , the performance of these approaches is severely degraded in cases of heavy occlusion and clutter , and similar looking distractors . in order to address these challenges ,",
    "several learning based methods formulate occlusion aware features @xcite , derive patch - based ( local ) descriptors @xcite or encode the contextual information of the objects with simple depth pixels @xcite and integrate them into random forests .",
    "most particularly , iterative random forest algorithms such as latent - class hough forest ( lchf ) @xcite and iterative multi - output random forest ( imorf ) @xcite obtain state - of - the - art accuracy on pose estimation .",
    "on the other hand , these methods rely on scale - invariant features , while the exploitation of rich discriminative information inherently embedded into the scale - variability is one important point been overlooked .",
    "+        [ fig1 ]    unlike the aforementioned learning - based methods , the ones presented by novatnack _",
    "@xcite utilize the detailed information of the scale variation in order to register range images in a coarse - to - fine fashion .",
    "although promising , they extract and match conventional salient 3d key points .",
    "however , real depth sensors have several imperfections such as missing depth values , noisy measurements , foreground fattening , _ etc_. as a result , salient feature points used in @xcite tend to be located on these deficient parts of the depth images , and hence , they are rather unstable @xcite . in such a scenario ,",
    "3d reconstruction methods that provide more reliable shape information can be utilized @xcite .",
    "implicit b - splines ( ibs ) @xcite are techniques that can provide shape descriptors through their zero - sets and reconstruct surfaces .",
    "they are based on locally controlled functions that when combined with their control points produce a very rich part - based object representation .",
    "+ our architecture is originated from these observations .",
    "we integrate the coarse - to - fine registration approach presented in @xcite into the random forest framework @xcite using histogram of control points ( hocp ) features that we adapt from recently introduced ibss @xcite .",
    "we train our forest only from positive samples and learn the detailed information of the scale - variability during training .",
    "we normalize every training point cloud into a unit cube and then generate a set of scale - space images , each of which is separated by a constant factor .",
    "the parts extracted from the images in this set are represented with the scale - variant hocp features . during inference , the parts centered on the pixels that belong to the background and foreground clutters are removed iteratively using the most confident hypotheses and the test image is updated . since this removal process decreases the standard deviation of the test point cloud , subsequent normalization applied to the updated test image increases the relative scale of the object ( foreground pixels ) in the unit cube .",
    "more discriminative control point descriptors are computed at higher scales and this ensures the refinement of the object pose . in our prior work @xcite , we have evaluated the registration performance of the proposed architecture by only using fixed - size parts .",
    "we extend the work engineering an automatic variable size part extraction framework in such a way that we can further exploit the discriminative information provided by the hocp features .",
    "this framework first roughly aligns the object of interest by extracting coarsest parts , the ones occupying the largest area in image pixels , and then iteratively refines its alignment based on finer ( smaller ) parts that are represented with more discriminative control point descriptors . note that we employ a compositional approach , _",
    "i.e. _ , we concurrently detect the object in the target region and estimate its pose by aligning the parts in order to increase robustness across clutter .",
    "figure [ fig1 ] depicts a sample result of our architecture . to summarize , our main contributions are as follows :    * to the best of our knowledge this is the first time an implicit object representation , implicit b - spline , is adapted into a `` scale - variant '' part descriptor and is associated with the random forests .",
    "* we introduce a novel iterative algorithm for the hough forests : it finds out an initial hypothesis and improves its confidence iteratively by extracting more discriminative `` scale - variant '' descriptors due to the elimination of the background / foreground clutter .",
    "* we engineer an automatic variable size part extraction framework for the random forests : it first roughly aligns the object of interest by extracting coarsest parts and iteratively improves its confidence based on finer ( smaller ) ones .",
    "the rest of the paper is organized as follows : in sect .",
    "[ related work ] , we present a review on the object registration .",
    "section [ approach ] demonstrates the computation procedure of the hocp features as a scale - variant part representation , their combination with the iterative hough forest ( ihf ) , and the registration process .",
    "experimental results are provided in sect .",
    "[ experiments ] and finally , the paper is concluded in sect .",
    "[ conclusion ] with several remarks , and discussions .",
    "a large number of studies have been proposed for the object registration , ranging from the point - wise correspondence based methods to the learning based approaches .",
    "iterative closest point ( icp ) algorithm , originally presented in @xcite , requires a good initialization in order not to be trapped in a local minimum during fine tuning .",
    "this issue is addressed in @xcite providing globally optimal registration by the integration of a global branch - and - bound ( bnb ) optimization into the local icp .",
    "the point - wise correspondence problem is converted into a point - to - model registration in @xcite .",
    "the object model is represented with implicit polynomials ( ip ) and the distance between the test point set and the object model is minimized via the levenberg - marquardt algorithm ( lma ) .",
    "@xcite propose a 6 dof pose estimation technique utilizing 3d ips on ultrasound images . in the off - line phase ,",
    "object model is represented with 3d ips and by utilizing its gradient flow , 2d ultrasound image is registered in the on - line process . in @xcite ,",
    "a coarse - to - fine fast ip - driven registration method is presented .",
    "a rough pose estimation is quickly acquired with a coarse ip model ( low degree curve fitting ) and finer models refine the parameters of this rough estimation ( high degree curve fitting ) .",
    "hinterstoisser et al .",
    "@xcite extract holistic templates from 3d models of the objects and match to the scene at test time .",
    "these studies have demonstrated good registration results on the target point sets that are occlusion - free and/or are subjected to the artificial gaussian noises and outliers .",
    "+ unlike the abovementioned methods , more realistic registration scenarios have been addressed by the point - to - point techniques that build point - pair features for sparse representations of the test and the model point sets @xcite .",
    "_ align two noisy point clouds of real scenes by finding correct point - to - point correspondences between the point feature histograms ( pfh ) and feed this alignment to an icp algorithm for fine tuning @xcite .",
    "the cluttered and partially occluded objects poses are hypothesized by accumulating the votes of the matching features in @xcite .",
    "_ @xcite propose point - pair features for both rgb and depth channels that are conducted in a voting scheme to hypothesize the rotation and translation parameters of the objects in the cluttered scenes .",
    "the features proposed in @xcite make use of the visibility context of the scene to tackle the registration . despite achieving good registration results ,",
    "these techniques underperform when the scenes are under heavy occlusion and clutter , and the objects geometry are indistinguishable from background .",
    "+ learning - based methods have good generalization across severe occlusion and clutter @xcite .",
    "the method presented in @xcite formulates the recognition problem globally and derives occlusion aware features .",
    "a set of principal curvature ratios are computed for all pixels in depth images to extract the edgelets . in @xcite ,",
    "the contextual information of the objects is encoded with simple depth and rgb pixels .",
    "this technique improves the confidence of a pose hypothesis using a ransac - like algorithm .",
    "@xcite back project the parts inside the initially found coarse bounding box to the image and pass down the forest again .",
    "the parts with the lowest contributions are penalized in such a way that finer registration is produced .",
    "+ the state - of - the - art accuracy on registration is acquired by the iterative random forest algorithms .",
    "the part - based strategy , latent - class hough forest @xcite , refines the initially hypothesized object pose by iteratively updating the object class distributions in the leaf nodes during testing .",
    "iterative multi output random forest ( imorf ) @xcite jointly predicts the head pose , the facial expression and the landmark positions .",
    "the relations between these tasks are modelled so that their performances are iteratively improved with the extraction of more informative features .",
    "the ideas , iterative pose refinement during testing and iterative extraction of more discriminative features , form a basis for our iterative hough forest ( ihf ) architecture : during training , we encode discriminative shape information of the hocp features into the forest . despite that the skeleton of our training procedure is similar to the methodology in @xcite , our forest learns the discriminative shape information that will be iteratively exploited at test time . in the course of inference , unlike @xcite , we update the test image itself and the hypotheses confidence by a noise removal process that allows us to extract more informative features from the test images . whilst these approaches @xcite rely on the scale - invariant features to improve the confidence of a pose hypothesis , novatnack _ et al . _ @xcite introduce a framework that registers the range images in a coarse - to - fine fashion by utilizing the detailed information provided by the scale variation .",
    "the shape descriptors with the coarsest scale are matched initially and a rough alignment is achieved since fewer features are extracted in coarser scales .",
    "the descriptor matching at higher scales produces improved predictions of the pose .",
    "inspired by @xcite , we design a `` scale - variant '' implicit volumetric part description , `` histogram of control points ( hocp ) '' , and associate it with the random forest framework . + selecting the part size is important since larger parts tend to match the disadvantages of a holistic template while smaller ones are prone to noise @xcite . in heavily occluded and cluttered scenes ,",
    "relatively smaller parts perform well whilst the larger ones are more convenient in occlusion / clutter - free scenarios .",
    "discriminative information encoded into small sized parts might not be fully exploited by larger parts , most particularly when the object representation is scale - variant .",
    "hence , this application - specific / task - dependent part size selection degrades the generalization and it is one of the remaining challenges that should be addressed , apart from occlusion , clutter and/or similar looking distractors",
    ". beyond object pose estimation @xcite , there are several part - based solutions proposed for different tasks such as human pose recognition @xcite , 3d face analysis @xcite , or hand pose estimation @xcite to name a few .",
    "they experience different part sizes and select the one that performs best , however , none of these solutions investigate how extracting variable size parts can be utilized in a single framework . in this study , we investigate the effect of this _ size variation _ and show that the simultaneous utilization of the parts of varying size can improve 6d object pose estimation , especially in heavily occluded and cluttered depth maps , supplying a rich source of discriminative information .",
    "in this section we detail our registration approach by firstly describing the computation procedure of the hocp features as a scale - variant part representation .",
    "we then present how to encode the discriminative information of these scale - variant features into the forest .",
    "finally , we demonstrate how to exploit the learnt shape information in a coarse - to - fine fashion to refine the pose hypotheses .      given a positive depth image",
    "we initially normalize it into a unit cube and then new point clouds at different scales are sampled as follows : @xmath0 with @xmath1 where @xmath2 $ ] is the world coordinate vector of the original foreground point cloud , @xmath3 is the mean of @xmath4 , @xmath5 $ ] is the normalized foreground pixels , @xmath6 is the number of the scales , @xmath7 is the scale factor and @xmath8 is the scale . the constant @xmath9 takes real numbers to generate the point clouds at different scales , starting from @xmath10 that corresponds to the initial normalization . a training image and its samples at different scales",
    "are shown in fig .",
    "[ fig2 ] ( a ) .",
    "+ once we generate a set of scale - space images , we represent these point clouds first globally with the control points of implicit b - splines ( ibs ) .",
    "ibs is defined through the combination of b - spline tensor products :        @xmath11 where @xmath12 are the coefficients defining a control lattice of size @xmath13 and @xmath14 , @xmath15 , @xmath16 are the spline basis functions .",
    "this definition can be reformulated as the following inner product : @xmath17 where the coefficient vector @xmath18 includes the control values @xmath12 and the basis vector @xmath19 depends on the given data point since it sorts the spline basis function products @xmath20 .",
    "the basis vectors in eq .",
    "[ eq4 ] are computed for the whole point cloud and the coefficient vector @xmath18 is calculated based on the 3l algorithm @xcite .",
    "rouhani et al .",
    "@xcite construct the spline basis functions @xmath14 , @xmath15 , @xmath16 through the following blending functions : @xmath21 and reformulate eq .",
    "[ eq3 ] in order to determine the control point vector @xmath18 of the point clouds that are normalized into the unit cube @xmath22 ^ 3 $ ] : @xmath23 where @xmath24 thus , the unit cube is split into an @xmath25 voxel grid where @xmath26 is the ibs resolution .",
    "each control point in @xmath18 is defined with an index - weight pair : the index number indicates the vertex of this grid at which the related control point is located .",
    "the weight informs the descriptor significance about the control of the geometry to be represented .",
    "the scale - space images in fig .",
    "[ fig2 ] ( a ) are globally represented in fig .",
    "[ fig2 ] ( b ) with the control point descriptors .",
    "we use all control points to represent the structures , but one can sort these descriptors based on their weights and utilize the ones higher than a threshold . in fig .",
    "[ fig2 ] ( b ) , the point clouds are lastly reconstructed by using the control points to show the increase in their discriminative power as the scale gets higher .",
    "note that ibs resolution @xmath26 determines the complexity of the representation ( level of detail ) in a unit cube , whilst the scale indicates the relative size of the object with respect to the unit cube dimensions . in our architecture , despite sampling point clouds at different scales @xmath27 , ( _ e.g. _ @xmath28 ) , we fix the complexity of the representation , ( _ e.g. _ @xmath29 ) .",
    "+ ibs is the combination of the locally controlled functions and allows one to propose effective part - based solutions for object registration .",
    "we benefit from such a property and partition the globally represented scale - space depth maps into parts .",
    "we express the part size @xmath30 in image pixels which also depicts the ratio between the sizes of the extracted part and the bounding box of the global point cloud . in our prior work @xcite ,",
    "we have extracted and represented the parts that have the same size , that is , the parts growing around every individual pixel at each scale occupy the same area in image pixels .",
    "we now extend the work extracting the parts that are different in size . a 3d bounding box defined in metric coordinates",
    "is traversed in the unit cube of each scale - space image and the parts are extracted around non - zero pixels .",
    "the total number of the data points in this 3d bounding box varies for the point clouds at different scales , and consequently , the size of the extracted parts differs .",
    "figure [ fig2 ] ( c ) shows an example of variable size part extraction process in which the red parts are grown around the same data point .",
    "note how the part size decreases when the scale of the normalized object point cloud gets higher , since less number of data points are deployed in the 3d bounding box .",
    "each part has its own implicit volumetric representation , formed by the closest control points to the part center , the ones lying inside the 3d bounding box along depth direction .",
    "such a part description characterizes the locality in a cascaded fashion , growing regions with different characteristics around a point .",
    "we encode this information into histograms in spherical coordinates .",
    "each of the part centers is coincided with the center of a sphere .",
    "the control points of the part are described by the log of the radius @xmath31 , the cosine of the inclination @xmath32 and the azimuth @xmath33 .",
    "then , the sphere is divided into the bins and the relation between the bin numbers @xmath34 and the histogram coordinates @xmath35 is given as follows @xcite : @xmath36 where @xmath37 and @xmath38 are the radii of the nested spheres with the minimum and the maximum volumes , @xmath39 are the cartesian coordinates of each descriptor with radius @xmath40 .",
    "@xmath38 equals to the distance between the patch center and the farthest descriptor of the related patch .",
    "the numbers of the control points in each bin are counted and stored in a @xmath41 dimensional feature vector @xmath42 .",
    "figure [ fig2 ] ( d ) illustrates the hocp representations of the parts extracted in fig . [ fig2 ] ( c ) .",
    "note that the control points computed at higher scales capture more detailed part geometry .",
    "the proposed ihf is the combination of randomized binary decision trees .",
    "it is trained only on foreground synthetically rendered depth images of the object of interest .",
    "we generate a set of scale - space images from each training point cloud and sample a set of parts @xmath43 as explained in subsection [ varpart ] and annotate those as follows : @xmath44 where @xmath45 is the part center in pixels , @xmath46 is the 3d offset between the centers of the part and the object , @xmath47 is the rotation parameters of the point cloud from which the part @xmath48 is extracted and @xmath49 is the depth map of the part .",
    "+ each tree is constructed by using a subset @xmath50 of the annotated training parts @xmath51 .",
    "we randomly select a template patch @xmath52 from @xmath50 and assign it to the root node .",
    "we measure the similarity between @xmath52 and each patch @xmath53 in @xmath50 as follows :    * * depth check : * the depth values of the descriptors @xmath54 and @xmath55 that represent the parts @xmath53 and @xmath52 are checked , and the spatially inconsistent ones in @xmath54 are removed as in @xcite , generating @xmath56 that includes the spatially consistent descriptors of the patch @xmath53 . * * similarity measure : * using @xmath56 , the feature vector @xmath57 is generated and the @xmath58 norm between this vector and @xmath59 is measured : @xmath60 * * similarity score comparison : * each patch is passed either to the left or the right child nodes according to the split function that compares the score of the similarity measure @xmath61 and a randomly chosen threshold @xmath62 .",
    "the main reason why we apply a depth check to the patches is to remove the structural perturbations , due to occlusion and clutter @xcite .",
    "these perturbations most likely occur on patches extracted along depth discontinuities such as the contours of the objects .",
    "they force a test patch to diverge ( occluded / cluttered ) from its positive correspondence by changing its representation , @xmath38 of the sphere , and the histogram coordinates consequently . + a group of candidate split functions are produced at each node by using a set of randomly assigned patches @xmath63 and thresholds @xmath64 .",
    "the one that best optimize the offset and pose regression entropy @xcite is selected as the split function .",
    "each tree is grown by repeating this process recursively until the forest termination criteria are satisfied . when the termination conditions are met , the leaf nodes are formed and they store votes for both the object center @xmath65 and the object rotation @xmath66 .",
    "+ depending on the part extraction approach , all parts in @xmath67 ( see eq .",
    "[ eq18 ] ) can either be of the same size or of the variable size . from now on",
    ", we will refer to the forests trained on variable size parts as the _ ihf - variable size _ , and to the ones learnt by using fixed size parts as the _ ihf - fixed size_.      once we encode the discriminative information of the scale - variant hocp features into the forest , we next demonstrate 6d pose estimation of objects considering that the learnt forest is ihf - variable size . + the proposed architecture registers objects in two steps : the _ initial registration _ and the _ iterative pose refinement_. the _ initial registration _",
    "roughly aligns the test object and this alignment is further improved by the _ iterative pose refinement_. + consider an object that was detected by a coarse bounding box , @xmath68 , as shown in the leftmost image of fig . [ fig3 ] ( a ) . at an iteration instant @xmath69 ,",
    "the following quantities are defined :    * @xmath70 : the history of the object position predictions .",
    "* @xmath71 : the history of the object rotation estimations .",
    "* @xmath72 : the history of the inputs ( noise removals ) applied to the test image .",
    "* @xmath73 : the history of the set of the feature vectors where @xmath74 .",
    "* @xmath75 : the object scale ( the scale of the foreground pixels ) in the unit cube at iteration @xmath69 ( see eq .",
    "[ eq2 ] ) .",
    "* @xmath76 : the size of the parts extracted at iteration @xmath69 .",
    "we formulate the _ initial registration _ as follows : @xmath77        [ fig3 ]    we find the best parameters that maximize the joint posterior density of the initial object position @xmath78 and the initial object rotation @xmath79 .",
    "the initial registration process is illustrated in fig .",
    "[ fig3 ] ( a ) .",
    "the test image is firstly normalized into a unit cube . unlike training , this is a `` single '' scale normalization that corresponds to @xmath10 ( see eq .",
    "[ eq1 ] ) .",
    "the patches extracted from the globally represented point cloud are described with the hocp features and are passed down all the trees . at this stage , we extract the coarsest patches from the test image , _",
    "i.e. _ , the ones occupying the largest area in image pixels .",
    "we determine the effect that all patches have on the object pose by accumulating the votes stored in the leaf nodes as in @xcite and approximate the initial registration given in eq .",
    "once the initial hypothesis @xmath80 is obtained , the set of pixels that belong to the background / foreground clutter @xmath81 are removed from @xmath68 according to the following criterion : @xmath82 with @xmath83 where @xmath84 and @xmath85 are the depth maps of the hypothesis @xmath86 at iteration @xmath69 , and of the @xmath68 , @xmath87 and @xmath88 are the scaling coefficients .",
    "the efficacy of @xmath89 is illustrated in fig .",
    "[ fig3 ] . in the rightmost image of fig .",
    "[ fig3 ] ( a ) , the test image and the initial hypothesis are superimposed .",
    "this hypothesis is exploited and the test image is updated by @xmath90 as in eq .",
    "the updated image is shown in fig .",
    "[ fig3 ] ( b ) and assigned as input for the @xmath91 iteration .",
    "it is normalized and represented globally .",
    "the object `` scale '' ( @xmath92 ) in the unit cube is relatively increased ( compare with the initial registration ) and more discriminative control point descriptors @xmath18 are computed .",
    "this is mainly because of the fact that the standard deviation of the input image decreased since we removed foreground / background clutter . as a follow up step , we traverse the 3d bounding box in the unit cube during part extraction , while the increase in the normalized object scale gives rise to extract patches whose size are smaller ( finer ) than the ones extracted during the initial registration .",
    "the resulted hypothesis of the @xmath91 iteration is shown on the right .",
    "the extraction of finer parts represented with more discriminative control point descriptors along with the noise removal process result in more accurate and confident hypothesis .",
    "this pose refinement process is iteratively performed until the maximum iteration is reached ( see fig .",
    "[ fig3 ] ( c ) ) : @xmath93 we approximate the registration hypothesis at each iteration by using the stored information in the leaf nodes as we do in the initial registration . if we would demonstrate the 6d object pose estimation considering that the learnt forest is the ihf - fixed size , the only difference in the formulation would be the part extraction viewpoint . instead of traversing 3d bounding box in the unit cube",
    ", we would extract the parts with a predefined size in pixels , and at an iteration instant @xmath69 , @xmath76 would remain the same as @xmath94 . in the next section , we will compare the registration performances of the forests that are separately trained on fixed and variable size parts .",
    "there are several publicly available datasets @xcite , @xcite to test the performances of the object registration methods . for each object type in these datasets , a set of rgb - d test images",
    "are provided with ground truth object pose parameters .",
    "we have analysed these images and have found that `` coffee cup '' , `` camera '' and `` shampoo '' ( included in the icvl dataset @xcite ) are some of the best demonstrable objects to test and to compare our registration architecture with the state - of - the - art methods since they are located in highly occluded and cluttered scenes .",
    "we further process the test images of these objects to generate a new test dataset according to the following criteria :        * since the hocp features are scale - variant , the depth values of the training and the test images should be close to each other up to a certain degree . in this study , we train the forests at a single depth value , @xmath95 mm , and test with the images at the range of @xmath96 $ ] mm . *",
    "the test object instances located at the range of @xmath96 $ ] mm are assumed as detected by coarse bounding boxes ( see fig .",
    "[ fig4 ] ) .",
    "the image regions included in these bounding boxes are cropped and the new test dataset is generated .",
    "the generated dataset includes @xmath97 `` coffee cup '' , @xmath98 `` camera '' and @xmath99 `` shampoo '' images each of which is at the range of @xmath100 $ ] mm since we train the forests used in all experiments with the positive samples at @xmath101 mm depth .",
    "the maximum depth is @xmath102 and the number of the maximum samples at each leaf node is @xmath103 for each tree .",
    "every forest is the ensemble of @xmath104 trees with these termination criteria . +     and feature dimensions @xmath105 .",
    "( c ) shows the effect of the iteration number . for the corresponding f1 scores",
    "see table [ table_params].,height=528 ]    our experiments are two folds : _",
    "parameter optimization _ and _ comparative study_. the architecture parameters have an important impact upon the registration and include the size of the parts extracted during the initial registration @xmath94 , the ibs resolution @xmath26 , the hocp feature dimension @xmath105 ( the number of the bins or quantization parameter ) and the iteration number . once the best parameters are acquired , we compare the performance of our architecture with the state - of - the - art methods in the comparative study . +",
    "both experiments use the metric proposed in @xcite to determine whether a registration hypothesis is correct .",
    "this metric outputs a score @xmath106 that calculates the distance between the ground truth and estimated poses of the test object .",
    "the registration hypothesis that ensures the following inequality is considered as correct : @xmath107 where @xmath108 is the diameter of the 3d model of the test object and @xmath109 is a constant that determines the coarseness of an hypothesis that is assigned as correct .",
    "we set @xmath109 to @xmath110 when we refine the parameters of the proposed architecture and the effect of various @xmath109 values is separately examined in the comparative study .",
    ".f1 scores of the initial registrations determined for different part sizes ( g ) , ibs resolution ( n ) & feature dimension ( d ) and number of iteration [ cols=\"^,^,^,^ \" , ]     [ table_interclass_aver ]    the pr curves of the coffee cup dataset are depicted in fig .",
    "[ fig7 ] for several @xmath111 values and their corresponding f1 scores are presented in table [ table_interclass_cup ] . a short analysis on the images of fig .",
    "[ fig7 ] reveals that the increase in @xmath111 value generates higher f1 scores for each approach .",
    "according to table [ table_interclass_cup ] , the lchf trained on the color gradient + surface normal features underperforms the lchf trained merely on the surface normals .",
    "the main reason of this underperformance is the distortion along the object borders arising from the occlusion and the clutter , that is , the distortion of the color gradient information in the test process .",
    "the initial registration of the ihf - fixed size performs better than any versions of the lchf .",
    "this shows that the hocp features represent the parts better than the surface normals and color gradients providing robustness across occlusion , clutter and missing depth pixels .",
    "when this initial registration ( see the @xmath112 column of table [ table_interclass_cup ] ) acquired from the ihf - fixed size is iteratively refined , more accurate registrations are resulted ( see the @xmath113 column of table [ table_interclass_cup ] ) . because , the _ iterative pose refinement _ module of the ihf - fixed size reduces the amount of the noise in the test depth maps removing foreground / background clutter .",
    "this removal process also enables ihf - fixed size to compute more discriminative control points for better shape representation . the ihf - variable size with initial registration + iterative pose refinement outperforms other approaches .",
    "the main reason of this high performance is the utilization of the parts that are different in size .",
    "the cascaded representation of the locality increases the robustness across clutter , occlusion , missing depth pixels and/or similar looking distractors .",
    "the object of interest is first roughly aligned by extracting the coarsest parts .",
    "it is highly possible that these initially extracted parts include the portions belonging to the background / foreground clutter since they are the coarsest and are close to a holistic template .",
    "despite the fact that we apply a depth check in order to get rid of those noise during testing , it is highly naive . on the other hand ,",
    "the proposed framework can get rid of those structural perturbations by growing smaller regions as the iteration progresses . apart from that",
    ", the control point descriptors computed at later iterations allows the complete framework to represent the shapes in a more discriminative manner .",
    "+ regarding the camera dataset , we show its pr curves in fig .",
    "[ fig8 ] for several @xmath111 values and the corresponding f1 scores in table [ table_interclass_camera ] .",
    "the approaches under comparison perform worse on this dataset with respect to the coffee cup . unlike the results obtained from the coffee cup dataset",
    ", we see the positive impact of the color gradients when they are utilized along with the surface normals at most of the @xmath111 values ( compare the @xmath91 and the @xmath114 columns of table [ table_interclass_camera ] ) .",
    "ihf - fixed size register objects more accurate than any versions of the lchf thanks to the utilization of the discriminative information embedded into the scale - variant hocp features and the iterative refinement of the test depth maps .",
    "ihf - variable size significantly outperforms other approaches demonstrating the importance of the simultaneous utilization of variable size parts .",
    "the hocp representations of the cascaded regions grown around the same data points allow the algorithm to be aware of occlusion , clutter and missing depth values .",
    "more confident registrations are hypothesized as the iteration progresses based on more discriminative representations of the smaller parts .",
    "the registration performance of the proposed architecture is shown in figure [ fig8 ] for the shampoo object and the corresponding f1 scores are demonstrated in table [ table_interclass_shampoo ] for varying @xmath111 values . in cases of registering at lower @xmath111 values ,",
    "our approach shows better performance than the lchfs , however , when we accept relatively rough estimations as correct , _",
    "i.e. _ , higher @xmath111s , our approach underperforms .",
    "+ since we address the registration problem rather than individual object detection or pose estimation , we integrate the effect of the different error ratios into our comparisons .",
    "we average the f1 scores that are computed at each @xmath111 in the range of [ 0.05 - 0.15 ] and report in table [ table_interclass_aver ] .",
    "figure [ fig9 ] illustrates several accurate registrations hypothesized by the proposed framework on the camera and the coffee cup objects .",
    "we further evaluate the performance of the globally optimized icp algorithm proposed in @xcite on our test dataset .",
    "we use the software kindly provided by the authors and set the default parameters .",
    "while accurate registration results are obtained on the clean dataset , it diverges in the case of highly occluded and cluttered point clouds ( see fig .",
    "[ fig10 ] ) .",
    "in this study , we have proposed a novel architecture , _ iterative hough forest with histogram of control points _ , addressing 6d object registration rather than individually estimating either the object s location in a 2d/3d bounding box , or the object s orientation ( roll , pitch , yaw ) .",
    "any off - the - shelf detector can accurately provide a coarse 2d or 3d bounding box for the object of interest .",
    "various object orientation predictors are also available , however , they depend on clearly segmented target objects .",
    "our architecture fundamentally targets to eliminate the shortcomings of these individual detectors and orientation predictors estimating occluded and cluttered objects 6d pose given a candidate 2d bounding box .",
    "our ihf is learnt using parts extracted only from the positive samples .",
    "these parts are represented with scale - variant hocp features , which we derive from recently introduced implicit b - splines ( ibs ) .",
    "+ at test time , we apply two different strategies regarding the parts used to train the forest : the first strategy we apply roughly aligns the object and iteratively refines this alignment based on more discriminative control point descriptors that are computed due to the elimination of background / foreground clutter .",
    "the part size is fixed and is empirically predefined . on the other hand , the predefined part size might not be generalizable enough across different objects , degrading the registration performance of the proposed study on one object while working well on another one .",
    "besides , discriminative information encoded into small sized parts might not be fully exploited by larger parts , most particularly when the object representation is scale - variant .",
    "inspiring by these observations , we use variable size parts in the second strategy .",
    "an automatic variable size part extraction framework iteratively refines the object s initial pose that is roughly aligned due to the extraction of coarsest parts , the ones occupying the largest area in image pixels .",
    "the iterative refinement is accomplished based on finer ( smaller ) parts that are represented with more discriminative control point descriptors by using our _ iterative hough forest_. the experimental results report that our approach show better registration performance than the state - of - the - art methods .",
    "caner sahin is funded by the turkish ministry of national education .",
    "b.  zheng , r.  ishikawa , j.  takamatsu , t.  oishi , and k.  ikeuchi , `` a coarse - to - fine ip - driven registration for pose estimation from single ultrasound image '' , _ computer vision and image understanding _ , 2013 ,",
    "vol.117 , no.12 , ( pp .",
    "1647 - 1658 ) .            c. r.  cabrera , r. l.  sastre , and t.  tuytelaars ,",
    "`` all together now : simultaneous object detection and continuous pose estimation using a hough forest with probabilistic locally enhanced voting '' , _ proceedings bmvc _ , 2014 , ( pp . 1 - 12 ) .",
    "s.  hinterstoisser , v.  lepetit , s.  ilic , s.  holzer , g.  bradski , k.  konolige , and n.  navab , `` model based training , detection and pose estimation of texture - less 3d objects in heavily cluttered scenes '' , _ accv _ , 2012 ."
  ],
  "abstract_text": [
    "<S> state - of - the - art techniques for 6d object pose recovery depend on occlusion - free point clouds to accurately register objects in 3d space . to deal with this shortcoming </S>",
    "<S> , we introduce a novel architecture called _ iterative hough forest with histogram of control points _ that is capable of estimating the 6d pose of occluded and cluttered objects given a candidate 2d bounding box . </S>",
    "<S> our _ iterative hough forest ( ihf ) _ is learnt using parts extracted only from the positive samples . </S>",
    "<S> these parts are represented with _ </S>",
    "<S> histogram of control points ( hocp ) _ , a `` scale - variant '' implicit volumetric description , which we derive from recently introduced implicit b - splines ( ibs ) . the rich discriminative information provided by the scale - variant hocp features is leveraged during inference . </S>",
    "<S> an automatic variable size part extraction framework iteratively refines the object s initial pose that is roughly aligned due to the extraction of coarsest parts , the ones occupying the largest area in image pixels . </S>",
    "<S> the iterative refinement is accomplished based on finer ( smaller ) parts that are represented with more discriminative control point descriptors by using our _ iterative hough forest_. experiments conducted on a publicly available dataset report that our approach show better registration performance than the state - of - the - art methods . </S>",
    "<S> +    object registration , 6 dof pose estimation , scale - variant hocp features , one class training , random forest , iterative refinement . </S>"
  ]
}