{
  "article_text": [
    "in this paper , we study online learning problems within a drifting - games framework , with the aim of developing a general methodology for designing learning algorithms based on a minimax analysis .    to solve an online learning problem , it is natural to consider game - theoretically optimal algorithms which find the best solution even in worst - case scenarios .",
    "this is possible for some special cases ( @xcite ) but difficult in general . on the other hand , many other efficient algorithms with optimal regret rate ( but not exactly minimax optimal ) have been proposed for different learning settings ( such as the exponential weights algorithm @xcite , and follow the perturbed leader @xcite ) . however , it is not always clear how to come up with these algorithms .",
    "recent work by rakhlin et al .",
    "@xcite built a bridge between these two classes of methods by showing that many existing algorithms can indeed be derived from a minimax analysis followed by a series of relaxations .    in this paper , we provide a parallel way to design learning algorithms by first converting online learning problems into variants of drifting games , and then applying a minimax analysis and relaxations .",
    "_ drifting games _ @xcite ( reviewed in section [ sec : drifting ] ) generalize freund s `` majority - vote game '' @xcite and subsume some well - studied boosting and online learning settings .",
    "a nearly minimax optimal algorithm is proposed in @xcite .",
    "it turns out the connections between drifting games and online learning go far beyond what has been discussed previously . to show that , we consider variants of drifting games that capture different popular online learning problems .",
    "we then generalize the minimax analysis in @xcite based on one key idea : _ relax a 0 - 1 loss function by a convex surrogate_. although this idea has been applied widely elsewhere in machine learning , we use it here in a new way to obtain a very general methodology for designing and analyzing online learning algorithms . using this general idea ,",
    "we not only recover existing algorithms , but also design new ones with special useful properties .",
    "a somewhat surprising result is that our new algorithms are totally _ parameter - free _ , which is usually not the case for algorithms derived from a minimax analysis .",
    "moreover , a generalized notion of regret ( @xmath0-regret , defined in section [ sec : hedge ] ) that measures how good the algorithm is compared to all but the top @xmath0 fraction of candidates arises naturally in our drifting - games framework .",
    "below we summarize our results for a range of learning settings .",
    "* hedge settings : * ( section [ sec : hedge ] ) the hedge problem @xcite investigates how to cleverly bet across a set of actions .",
    "we show an algorithmic equivalence between this problem and a simple drifting game ( dgv1 ) .",
    "we then show how to relax the original minimax analysis step by step to reach a general recipe for designing hedge algorithms ( algorithm [ alg : hedge_recipe ] ) .",
    "three examples of appropriate convex surrogates of the 0 - 1 loss function are then discussed , leading to the well - known exponential weights algorithm and two other new ones , one of which ( normalhedge.dt in section [ sec:3algs ] ) bears some similarities with the normalhedge algorithm @xcite and enjoys a similar @xmath0-regret bound _ simultaneously _ for all @xmath0 and horizons .",
    "however , our regret bounds do not depend on the number of actions , and thus can be applied even when there are infinitely many actions .",
    "our analysis is also arguably simpler and more intuitive than the one in @xcite and easy to be generalized to more general settings .",
    "moreover , our algorithm is more computationally efficient since it does not require a numerical searching step as in normalhedge .",
    "finally , we also derive high probability bounds for the randomized hedge setting as a simple side product of our framework _ without _ using any concentration results .",
    "* multi - armed bandit problems : * ( section [ sec : gen ] ) the multi - armed bandit problem @xcite is a classic example for learning with incomplete information where the learner can only obtain feedback for the actions taken . to capture this problem , we study a quite different drifting game ( dgv2 ) where randomness and variance constraints are taken into account . again",
    "the minimax analysis is generalized and the exp3 algorithm @xcite is recovered .",
    "our results could be seen as a preliminary step to answer the open question @xcite on exact minimax optimal algorithms for the multi - armed bandit problem .",
    "* online convex optimization : * ( section [ sec : gen ] ) based the theory of convex optimization , online convex optimization @xcite has been the foundation of modern online learning theory . the corresponding drifting game formulation is a continuous space variant ( dgv3 ) .",
    "fortunately , it turns out that all results from the hedge setting are ready to be used here , recovering the continuous exp algorithm @xcite and also generalizing our new algorithms to this general setting .",
    "besides the usual regret bounds , we also generalize the @xmath0-regret , which , as far as we know , is the first time it has been explicitly studied .",
    "again , we emphasize that our new algorithms are adaptive in @xmath0 and the horizon .",
    "* boosting : * ( section [ sec : gen ] ) realizing that every hedge algorithm can be converted into a boosting algorithm ( @xcite ) , we propose a new boosting algorithm ( nh-boost.dt ) by converting normalhedge.dt .",
    "the adaptivity of normalhedge.dt is then translated into training error and margin distribution bounds that previous analysis in @xcite using nonadaptive algorithms does not show . moreover",
    ", our new boosting algorithm ignores a great many examples on each round , which is an appealing property useful to speeding up the weak learning algorithm .",
    "this is confirmed by our experiments",
    ".    * related work * : our analysis makes use of potential functions .",
    "similar concepts have widely appeared in the literature @xcite , but unlike our work , they are not related to any minimax analysis and might be hard to interpret .",
    "the existence of parameter free hedge algorithms for unknown number of actions was shown in @xcite , but no concrete algorithms were given there . boosting algorithms that ignore some examples on each round were studied in @xcite , where a heuristic was used to ignore examples with small weights and no theoretical guarantee is provided .",
    "we consider a simplified version of drifting games similar to the one described in @xcite ( also called chip games ) .",
    "this game proceeds through @xmath1 rounds , and is played between a player and an adversary who controls @xmath2 chips on the real line .",
    "the positions of these chips at the end of round @xmath3 are denoted by @xmath4 , with each coordinate @xmath5 corresponding to the position of chip @xmath6 .",
    "initially , all chips are at position @xmath7 so that @xmath8 . on every round @xmath9 :",
    "the player first chooses a distribution @xmath10 over the chips , then the adversary decides the movements of the chips @xmath11 so that the new positions are updated as @xmath12 . here ,",
    "each @xmath13 has to be picked from a prespecified set @xmath14 , and more importantly , satisfy the constraint @xmath15 for some fixed constant @xmath16 .    at the end of the game ,",
    "each chip is associated with a nonnegative loss defined by @xmath17 for some nonincreasing function @xmath18 mapping from the final position of the chip to @xmath19 .",
    "the goal of the player is to minimize the chips average loss @xmath20 after @xmath1 rounds .",
    "so intuitively , the player aims to `` push '' the chips to the right by assigning appropriate weights on them so that the adversary has to move them to the right by @xmath16 in a weighted average sense on each round .",
    "this game captures many learning problems .",
    "for instance , binary classification via boosting can be translated into a drifting game by treating each training example as a chip ( see @xcite for details ) .",
    "we regard a player s strategy @xmath21 as a function mapping from the history of the adversary s decisions to a distribution that the player is going to play with , that is , @xmath22 where @xmath23 stands for @xmath24 . the player s worst case loss using this algorithm is then denoted by @xmath25 .",
    "the minimax optimal loss of the game is computed by the following expression : @xmath26 where @xmath27 is the @xmath2 dimensional simplex and @xmath28 is assumed to be compact .",
    "a strategy @xmath29 that realizes the minimum in @xmath30 is called a minimax optimal strategy . a nearly optimal strategy and its analysis",
    "is originally given in @xcite , and a derivation by directly tackling the above minimax expression can be found in @xcite . specifically , a sequence of potential functions of a chip s position is defined recursively as follows : @xmath31 let @xmath32 be the weight that realizes the minimum in the definition of @xmath33 , that is , @xmath34 . then the player s strategy is to set @xmath35 .",
    "the key property of this strategy is that it assures that the sum of the potentials over all the chips never increases , connecting the player s final loss with the potential at time @xmath7 as follows : @xmath36 it has been shown in @xcite that this upper bound on the loss is optimal in a very strong sense .",
    "moreover , in some cases the potential functions have nice closed forms and thus the algorithm can be efficiently implemented .",
    "for example , in the boosting setting , @xmath37 is simply @xmath38 , and one can verify @xmath39 and @xmath40 . with the loss function",
    "@xmath41 being @xmath42 , these can be further simplified and eventually give exactly the boost - by - majority algorithm @xcite .",
    "the connection between drifting games and some specific settings of online learning has been noticed before ( @xcite ) .",
    "we aim to find deeper connections or even an equivalence between variants of drifting games and more general settings of online learning , and provide insights on designing learning algorithms through a minimax analysis .",
    "we start with a simple yet classic hedge setting .      in the hedge setting @xcite , a player tries to earn as much as possible ( or lose as little as possible ) by cleverly spreading a fixed amount of money to bet on a set of actions on each day .",
    "formally , the game proceeds for @xmath1 rounds , and on each round @xmath9 : the player chooses a distribution @xmath10 over @xmath2 actions , then the adversary decides the actions losses @xmath43 ( i.e. action @xmath6 incurs loss @xmath44 $ ] ) which are revealed to the player .",
    "the player suffers a weighted average loss @xmath45 at the end of this round .",
    "the goal of the player is to minimize his `` regret '' , which is usually defined as the difference between his total loss and the loss of the best action .",
    "here , we consider an even more general notion of regret studied in @xcite , which we call _ @xmath0-regret_. suppose the actions are ordered according to their total losses after @xmath1 rounds ( i.e. @xmath46 ) from smallest to largest , and let @xmath47 be the index of the action that is the @xmath48-th element in the sorted list ( @xmath49 ) .",
    "now , @xmath0-regret is defined as @xmath50 in other words , @xmath0-regret measures the difference between the player s loss and the loss of the @xmath48-th best action ( recovering the usual regret with @xmath51 ) , and sublinear @xmath0-regret implies that the player s loss is almost as good as all but the top @xmath0 fraction of actions .",
    "similarly , @xmath52 denotes the worst case @xmath0-regret for a specific algorithm @xmath53 . for convenience , when @xmath54 or @xmath55 , we define @xmath0-regret to be @xmath56 or @xmath57 respectively .",
    "next we discuss how hedge is highly related to drifting games . consider a variant of drifting games where @xmath58 , \\beta=0 $ ] and @xmath59 for some constant @xmath60 .",
    "additionally , we impose an extra restriction on the adversary : @xmath61 for all @xmath6 and @xmath62 . in other words ,",
    "the difference between any two chips movements is at most @xmath63 .",
    "we denote this specific variant of drifting games by dgv1 ( summarized in appendix [ apd : dgv ] ) and a corresponding algorithm by @xmath64 to emphasize the dependence on @xmath60 .",
    "the reductions in algorithm [ alg : hedge2drift ] and [ alg : drift2hedge ] and theorem [ thm : equivalence ] show that dgv1 and the hedge problem are algorithmically equivalent ( note that both conversions are valid ) .",
    "the proof is straightforward and deferred to appendix [ apd : equiv ] . by theorem [ thm :",
    "equivalence ] , it is clear that the minimax optimal algorithm for one setting is also minimax optimal for the other under these conversions .",
    "[ thm : equivalence ] dgv1 and the hedge problem are algorithmically equivalent in the following sense : + ( 1 ) algorithm [ alg : hedge2drift ] produces a dgv1 algorithm @xmath64 satisfying @xmath65 where @xmath66 is such that @xmath67 .",
    "( 2 ) algorithm [ alg : drift2hedge ] produces a hedge algorithm @xmath53 with @xmath68 for any @xmath60 such that @xmath69 .      from now on we only focus on the direction of converting a drifting game algorithm into a hedge algorithm . in order to derive a minimax hedge algorithm , theorem [ thm : equivalence ] tells us it suffices to derive minimax dgv1 algorithms .",
    "exact minimax analysis is usually difficult , and appropriate relaxations seem to be necessary . to make use of the existing analysis for standard drifting games ,",
    "the first obvious relaxation is to drop the additional restriction in dgv1 , that is , @xmath70 for all @xmath6 and @xmath62 .",
    "doing this will lead to the exact setting discussed in @xcite where a near optimal strategy is proposed using the recipe in eq . .",
    "it turns out that this relaxation is reasonable and does not give too much more power to the adversary . to see this , first recall that results from @xcite , written in our notation , state that @xmath71 which , by hoeffding s inequality , is upper bounded by @xmath72 .",
    "second , statement ( 2 ) in theorem [ thm : equivalence ] clearly remains valid if the input of algorithm [ alg : drift2hedge ] is a drifting game algorithm for this relaxed version of dgv1 .",
    "therefore , by setting @xmath73 and solving for @xmath60 , we have @xmath74 , which is the known optimal regret rate for the hedge problem , showing that we lose little due to this relaxation .",
    "however , the algorithm proposed in @xcite is not computationally efficient since the potential functions @xmath75 do not have closed forms . to get around this , we would want the minimax expression in eq . to be easily solved , just like the case when @xmath76 .",
    "it turns out that convexity would allow us to treat @xmath77 $ ] almost as @xmath76 .",
    "specifically , if each @xmath75 is a convex function of @xmath78 , then due to the fact that the maximum of a convex function is always realized at the boundary of a compact region , we have @xmath79 } { \\left(}\\phi_t(s+z ) + wz{\\right)}= \\min_{w \\in { \\mathbb{r}}_+}\\max_{z\\in \\{-1,1\\ } } { \\left(}\\phi_t(s+z ) + wz{\\right)}= \\frac{\\phi_t(s-1 ) + \\phi_t(s+1)}{2 } , \\ ] ] with @xmath80 realizing the minimum . since the 0 - 1",
    "loss function @xmath41 is not convex , this motivates us to find a convex surrogate of @xmath41 .",
    "fortunately , relaxing the equality constraints in eq .",
    "does not affect the key property of eq . as we will show in the proof of theorem [ thm : hedge_recipe ] .",
    "`` compiling out '' the input of algorithm [ alg : drift2hedge ] , we thus have our general recipe ( algorithm [ alg : hedge_recipe ] ) for designing hedge algorithms with the following regret guarantee .",
    "set : @xmath8 .",
    "+    [ thm : hedge_recipe ] for algorithm [ alg : hedge_recipe ] , if @xmath60 and @xmath0 are such that @xmath81 and @xmath82 for all @xmath83 , then @xmath68 .",
    "_ proof . _",
    "it suffices to show that eq .",
    "holds so that the theorem follows by a direct application of statement ( 2 ) of theorem [ thm : equivalence ] .",
    "let @xmath84 .",
    "then @xmath85 since @xmath35 and @xmath86 . on the other hand , by eq . , we have @xmath87 }   { \\left(}\\phi_t(s_{t-1,i}+z ) + wz{\\right)}= \\frac{1}{2}{\\left(}\\phi_t(s_{t-1,i}-1)+\\phi_t(s_{t-1,i}+1){\\right)}$ ] , which is at most @xmath33 by algorithm [ alg : hedge_recipe ] .",
    "this shows @xmath88 and eq . follows .",
    "theorem 2 tells us that if solving @xmath81 for @xmath60 gives @xmath89 for some value @xmath90 , then the regret of algorithm [ alg : hedge_recipe ] is less than any value that is greater than @xmath90 , meaning the regret is at most @xmath90 .",
    "now we are ready to recover existing algorithms and develop new ones by choosing an appropriate potential @xmath91 as algorithm [ alg : hedge_recipe ] suggests .",
    "we will discuss three different algorithms below , and summarize these examples in table [ tab : algs ] ( see appendix [ apd : lemmas ] ) .",
    "[ [ exponential - weights - exp - algorithm . ] ] exponential weights ( exp ) algorithm .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    exponential loss is an obvious choice for @xmath91 as it has been widely used as the convex surrogate of the 0 - 1 loss function in the literature .",
    "it turns out that this will lead to the well - known exponential weights algorithm @xcite .",
    "specifically , we pick @xmath91 to be @xmath92 which exactly upper bounds @xmath93 . to compute @xmath75 for @xmath94 , we simply let @xmath95 hold with equality . indeed ,",
    "direct computations show that all @xmath75 share a similar form : @xmath96 therefore , according to algorithm [ alg : hedge_recipe ] , the player s strategy is to set @xmath97 which is exactly the same as exp ( note that @xmath60 becomes irrelevant after normalization ) . to derive regret bounds",
    ", it suffices to require @xmath98 which is equivalent to @xmath99 by theorem [ thm : hedge_recipe ] and hoeffding s lemma ( see ( * ? ? ?",
    "* lemma a.1 ) ) , we thus know @xmath100 where the last step is by optimally tuning @xmath101 to be @xmath102 .",
    "note that this algorithm is _ not adaptive _ in the sense that it requires knowledge of @xmath1 and @xmath0 to set the parameter @xmath101 .",
    "we have thus recovered the well - known exp algorithm and given a new analysis using the drifting - games framework . more importantly , as in @xcite , this derivation may shed light on why this algorithm works and where it comes from , namely , a minimax analysis followed by a series of relaxations , starting from a reasonable surrogate of the 0 - 1 loss function .    [ [ norm - algorithm . ] ] 2-norm algorithm .",
    "+ + + + + + + + + + + + + + + + +    we next move on to another simple convex surrogate : @xmath103_-^2 \\geq { { \\bm 1}}\\{s \\leq -1/\\sqrt{a}\\ } , $ ] where @xmath104 is some positive constant and @xmath105_- = \\min\\{0 , s\\}$ ] represents a truncating operation .",
    "the following lemma shows that @xmath75 can also be simply described .",
    "[ lem:2-norm ] if @xmath106 , then @xmath107_-^2 + t- t{\\right)}$ ] satisfies @xmath95 .",
    "thus , algorithm 3 can again be applied .",
    "the resulting algorithm is extremely concise : @xmath108_-^2 - [ s_{t-1,i}+1]_-^2.\\ ] ] we call this the `` 2-norm '' algorithm since it resembles the @xmath109-norm algorithm in the literature when @xmath110 ( see @xcite ) .",
    "the difference is that the @xmath109-norm algorithm sets the weights proportional to the derivative of potentials , instead of the difference of them as we are doing here .",
    "a somewhat surprising property of this algorithm is that it is totally adaptive and parameter - free ( since @xmath104 disappears under normalization ) , a property that we usually do not expect to obtain from a minimax analysis .",
    "direct application of theorem [ thm : hedge_recipe ] ( @xmath111 ) shows that its regret achieves the optimal dependence on the horizon @xmath1 .",
    "[ cor:2-norm ] algorithm [ alg : hedge_recipe ] with potential @xmath75 defined in lemma [ lem:2-norm ] produces a hedge algorithm @xmath53 such that @xmath112 simultaneously for all @xmath1 and @xmath0 .",
    "[ [ normalhedge.dt . ] ] normalhedge.dt .",
    "+ + + + + + + + + + + + + + +    the regret for the 2-norm algorithm does not have the optimal dependence on @xmath0 . an obvious follow - up question would be whether it is possible to derive an adaptive algorithm that achieves the optimal rate @xmath113 simultaneously for all @xmath1 and @xmath0 using our framework .",
    "an even deeper question is : instead of choosing convex surrogates in a seemingly arbitrary way , is there a more natural way to find the _",
    "choice of @xmath91 ?",
    "to answer these questions , we recall that the reason why the 2-norm algorithm can get rid of the dependence on @xmath0 is that @xmath0 appears merely in the multiplicative constant @xmath104 that does not play a role after normalization .",
    "this motivates us to let @xmath91 in the form of @xmath114 for some @xmath115 . on the other hand , from theorem [ thm : hedge_recipe ]",
    ", we also want @xmath114 to upper bound the 0 - 1 loss function @xmath116 for some constant @xmath117 .",
    "taken together , this is telling us that the right choice of @xmath115 should be of the form @xmath118 .",
    "of course we still need to refine it to satisfy the monotonicity and other properties .",
    "we define @xmath91 formally and more generally as : @xmath119_-^2}{dt}{\\right)}- 1{\\right)}\\geq { { \\bm 1}}\\left\\ { s \\leq -\\sqrt{dt \\ln { \\left(}\\tfrac{1}{a } + 1{\\right ) } } \\right\\},\\ ] ] where @xmath104 and @xmath117 are some positive constants .",
    "this time it is more involved to figure out what other @xmath75 should be .",
    "the following lemma addresses this issue ( proof deferred to appendix [ apd : lemmas ] ) .",
    "[ lem : anh ] if @xmath120 and @xmath121_-^2}{dt}{\\right)}- b_t{\\right)}$ ] ( define @xmath122 ) , then we have @xmath95 for all @xmath123 and @xmath124 .",
    "moreover , eq . still holds .    note that even if @xmath125 is not valid in general , lemma [ lem : anh ] states that eq . still holds . thus algorithm [ alg : hedge_recipe ]",
    "can indeed still be applied , leading to our new algorithm : @xmath126_-^2}{dt}{\\right)}-   \\exp{\\left(}\\tfrac{[s_{t-1,i}+1]_-^2}{dt}{\\right)}.\\ ] ] here , @xmath117 seems to be an extra parameter , but in fact , simply setting @xmath127 is good enough :    [ cor : anh ] algorithm [ alg : hedge_recipe ] with potential @xmath75 defined in lemma [ lem : anh ] and @xmath127 produces a hedge algorithm @xmath53 such that the following holds simultaneously for all @xmath1 and @xmath0 : @xmath128    we have thus proposed a parameter - free adaptive algorithm with optimal regret rate ( ignoring the @xmath129 term ) using our drifting - games framework . in fact , our algorithm bears a striking similarity to normalhedge @xcite , the first algorithm that has this kind of adaptivity .",
    "we thus name our algorithm normalhedge.dt .",
    "we include normalhedge in table [ tab : algs ] for comparison .",
    "one can see that the main differences are : 1 ) on each round normalhedge performs a numerical search to find out the right parameter used in the exponents ; 2 ) normalhedge uses the derivative of potentials as weights . compared to normalhedge , the regret bound for normalhedge.dt has no explicit dependence on @xmath2 , but has a slightly worse dependence on @xmath1 ( indeed @xmath129 is almost negligible ) .",
    "we emphasize other advantages of our algorithm over normalhedge : 1 ) normalhedge.dt is more computationally efficient especially when @xmath2 is very large , since it does not need a numerical search for each round ; 2 ) our analysis is arguably simpler and more intuitive than the one in @xcite ; 3 ) as we will discuss in section [ sec : gen ] , normalhedge.dt can be easily extended to deal with the more general online convex optimization problem where the number of actions is infinitely large , while it is not clear how to do that for normalhedge by generalizing the analysis in @xcite .",
    "indeed , the extra dependence on the number of actions @xmath2 for the regret of normalhedge makes this generalization even seem impossible .",
    "finally , we will later see that normalhedge.dt outperforms normalhedge in experiments . despite the differences , it is worth noting that both algorithms assign zero weight to some actions on each round , an appealing property when @xmath2 is huge .",
    "we will discuss more on this in section [ sec : gen ] .",
    "we now consider a common variant of hedge : on each round , instead of choosing a distribution @xmath10 , the player has to randomly pick a single action @xmath130 , while the adversary decides the losses @xmath43 at the same time ( without seeing @xmath130 ) .",
    "for now we only focus on the player s regret to the best action : @xmath131 notice that the regret is now a random variable , and we are interested in a bound that holds with high probability .",
    "using azuma s inequality , standard analysis ( see for instance ( * ? ? ?",
    "* lemma 4.1 ) ) shows that the player can simply draw @xmath130 according to @xmath132 , the output of a standard hedge algorithm , and suffers regret at most @xmath133 with probability @xmath134 .",
    "below we recover similar results as a simple side product of our drifting - games analysis _ without _ resorting to concentration results , such as azuma s inequality .    for this , we only need to modify algorithm [ alg : hedge_recipe ] by setting @xmath135 .",
    "the restriction @xmath136 is then relaxed to hold in expectation .",
    "moreover , it is clear that eq .",
    "also still holds in expectation . on the other hand , by definition and",
    "the union bound , one can show that @xmath137 = \\sum_i { { \\text { pr}}}\\left [ s_{t , i } \\leq -r \\right ] \\geq   { { \\text { pr}}}\\left [ { { \\bf r}}_t(i_{1:t } , { { \\bm \\ell}}_{1:t } ) \\geq r \\right]$ ] . so setting @xmath138 shows that the regret is smaller than @xmath60 with probability @xmath134 .",
    "therefore , for example , if exp is used , then the regret would be at most @xmath139 with probability @xmath134 , giving basically the same bound as the standard analysis .",
    "one draw back is that exp would need @xmath140 as a parameter .",
    "however , this can again be addressed by normalhedge.dt for the exact same reason that normalhedge.dt is independent of @xmath0 .",
    "we have thus derived high probability bounds without using any concentration inequalities .",
    "* multi - armed bandit ( mab ) problem : * the only difference between hedge ( randomized version ) and the non - stochastic mab problem @xcite is that on each round , after picking @xmath130 , the player only sees the loss for this single action @xmath141 instead of the whole vector @xmath43 .",
    "the goal is still to compete with the best action . a common technique used in the bandit setting is to build an unbiased estimator @xmath142 for the losses , which in this case could be @xmath143 .",
    "then algorithms such as exp can be used by replacing @xmath43 with @xmath142 , leading to the exp3 algorithm @xcite with regret @xmath144 .",
    "one might expect that algorithm [ alg : hedge_recipe ] would also work well by replacing @xmath43 with @xmath142 .",
    "however , doing so breaks an important property of the movements @xmath13 : boundedness . indeed , eq .",
    "no longer makes sense if @xmath145 could be infinitely large , even if in expectation it is still in @xmath146 $ ] ( note that @xmath13 is now a random variable ) .",
    "it turns out that we can address this issue by imposing a variance constraint on @xmath13 .",
    "formally , we consider a variant of drifting games where on each round , the adversary picks a random movement @xmath13 for each chip such that : @xmath147 \\leq 1 , { \\mathbb{e}}_t[z_{t , i}^2 ] \\leq 1/p_{t , i}$ ] and @xmath148 \\geq 0 $ ] .",
    "we call this variant dgv2 and summarize it in appendix [ apd : dgv ] . the standard minimax analysis and the derivation of potential functions need to be modified in a certain way for dgv2 , as stated in theorem [ thm : dgv2 ] ( appendix [ apd : bandit ] ) . using the analysis for dgv2 , we propose a general recipe for designing mab algorithms in a similar way as for hedge and also recover exp3 ( see algorithm [ alg : bandit_recipe ] and theorem [ thm : bandit_recipe ] in appendix [ apd : bandit ] )",
    ". unfortunately so far we do not know other appropriate potentials due to some technical difficulties .",
    "we conjecture , however , that there is a potential function that could recover the poly - inf algorithm @xcite or give its variants that achieve the optimal regret @xmath149 .    *",
    "online convex optimization : * we next consider a general online convex optimization setting @xcite .",
    "let @xmath150 be a compact convex set , and @xmath151 be a set of convex functions with range @xmath152 $ ] on @xmath153 . on each round @xmath3 ,",
    "the learner chooses a point @xmath154 , and the adversary chooses a loss function @xmath155 ( knowing @xmath156 ) .",
    "the learner then suffers loss @xmath157 .",
    "the regret after @xmath1 rounds is @xmath158 .",
    "there are two general approaches to oco : one builds on convex optimization theory @xcite , and the other generalizes exp to a continuous space @xcite .",
    "we will see how the drifting - games framework can recover the latter method and also leads to new ones .    to do so",
    ", we introduce a continuous variant of drifting games ( dgv3 , see appendix [ apd : dgv ] ) .",
    "there are now infinitely many chips , one for each point in @xmath153 . on round @xmath3 , the player needs to choose a distribution over the chips , that is , a probability density function @xmath159 on @xmath153 .",
    "then the adversary decides the movements for each chip , that is , a function @xmath160 with range @xmath146 $ ] on @xmath153 ( not necessarily convex or continuous ) , subject to a constraint @xmath161 \\geq 0 $ ] . at the end , each point @xmath162 is associated with a loss @xmath163 , and the player aims to minimize the total loss @xmath164 .",
    "oco can be converted into dgv3 by setting @xmath165 and predicting @xmath166 \\in s$ ] .",
    "the constraint @xmath161 \\geq 0 $ ] holds by the convexity of @xmath167 .",
    "moreover , it turns out that the minimax analysis and potentials for dgv1 can readily be used here , and the notion of @xmath0-regret , now generalized to the oco setting , measures the difference of the player s loss and the loss of a best fixed point in a subset of @xmath153 that excludes the top @xmath0 fraction of points . with different potentials ,",
    "we obtain versions of each of the three algorithms of section [ sec : hedge ] generalized to this setting , with the same @xmath0-regret bounds as before .",
    "again , two of these methods are adaptive and parameter - free . to derive bounds for the usual regret , at first glance it seems that we have to set @xmath0 to be close to zero , leading to a meaningless bound .",
    "nevertheless , this is addressed by theorem [ thm : oco_recipe ] using similar techniques in @xcite , giving the usual @xmath168 regret bound .",
    "all details can be found in appendix [ apd : oco ] .",
    "* applications to boosting : * there is a deep and well - known connection between hedge and boosting @xcite . in principle , every hedge algorithm can be converted into a boosting algorithm ; for instance , this is how adaboost was derived from exp . in the same way , normalhedge.dt can be converted into a new boosting algorithm that we call nh-boost.dt .",
    "see appendix [ apd : anb ] for details and further background on boosting .",
    "the main idea is to treat each training example as an `` action '' , and to rely on the hedge algorithm to compute distributions over these examples which are used to train the weak hypotheses .",
    "typically , it is assumed that each of these has `` edge '' @xmath169 , meaning its accuracy on the training distribution is at least @xmath170 .",
    "the final hypothesis is a simple majority vote of the weak hypotheses . to understand the prediction accuracy of a boosting algorithm",
    ", we often study the training error rate and also the distribution of margins , a well - established measure of confidence ( see appendix [ apd : anb ] for formal definitions ) .",
    "thanks to the adaptivity of normalhedge.dt , we can derive bounds on both the training error and the distribution of margins after any number of rounds :    [ thm : anb ] after @xmath1 rounds , the training error of nh-boost.dt is of order @xmath171 , and the fraction of training examples with margin at most @xmath172 is of order @xmath173 .",
    "thus , the training error decreases at roughly the same rate as adaboost .",
    "in addition , this theorem implies that the fraction of examples with margin smaller than @xmath174 eventually goes to zero as @xmath1 gets large , which means nh-boost.dt converges to the optimal margin @xmath174 ; this is known not to be true for adaboost ( see @xcite ) .",
    "also , like adaboost , nh-boost.dt is an adaptive boosting algorithm that does not require @xmath169 or @xmath1 as a parameter .",
    "however , unlike adaboost , nh-boost.dt has the striking property that it completely ignores many examples on each round ( by assigning zero weight ) , which is very helpful for the weak learning algorithm in terms of computational efficiency . to test this , we conducted experiments to compare the efficiency of adaboost , `` nh - boost '' ( an analogous boosting algorithm derived from normalhedge ) and nh-boost.dt .",
    "all details are in appendix [ apd : experiments ] . here",
    "we only briefly summarize the results .",
    "while the three algorithms have similar performance in terms of training and test error , nh-boost.dt is always the fastest one in terms of running time for the same number of rounds .",
    "moreover , the average faction of examples with zero weight is significantly higher for nh-boost.dt than for nh - boost ( see table [ tab : results ] ) . on one hand",
    ", this explains why nh-boost.dt is faster ( besides the reason that it does not require a numerical step ) . on the other hand",
    ", this also implies that nh-boost.dt tends to achieve larger margins , since zero weight is assigned to examples with large margin .",
    "this is also confirmed by our experiments .",
    "* acknowledgements . * support for this research was provided by nsf grant # 1016029 .",
    "the authors thank yoav freund for helpful discussions and the anonymous reviewers for their comments .",
    "we study three different variants of drifting games throughout the paper , which corresponds to the hedge setting , the multi - armed bandit problem and online convex optimization respectively .",
    "the protocols of these variants are summarized below .",
    "* dgv1 *    given : a loss function @xmath59 .",
    "+ for @xmath9 :    1 .",
    "the player chooses a distribution @xmath10 over @xmath2 chips .",
    "the adversary decides the movement of each chip @xmath175 $ ] subject to @xmath176 and @xmath177 for all @xmath6 and @xmath62 .",
    "the player suffers loss @xmath178 .",
    "* dgv2 *    given : a loss function @xmath59 .",
    "+ for @xmath9 :    1 .",
    "the player chooses a distribution @xmath10 over @xmath2 chips .",
    "the adversary randomly decides the movement of each chip @xmath179 subject to @xmath180 \\leq 1 , { \\mathbb{e}}_t[z_{t , i}^2 ] \\leq 1/p_{t , i}$ ] and @xmath148 \\geq 0 $ ] .",
    "the player suffers loss @xmath178 .",
    "* dgv3 *    given : a compact convex set @xmath153 , a loss function @xmath59 .",
    "+ for @xmath9 :    1 .",
    "the player chooses a density function @xmath159 on @xmath153 .",
    "the adversary decides a function @xmath181 $ ] subject to @xmath161 \\geq 0 $ ] .",
    "the player suffers loss @xmath182 .",
    "we first show that both conversions are valid . in algorithm",
    "[ alg : hedge2drift ] , it is clear that @xmath183 . also , @xmath184 is guaranteed due to the extra restriction of dgv1 . for algorithm [ alg : drift2hedge ]",
    ", @xmath13 lies in @xmath77 $ ] since @xmath185 $ ] , and direct computation shows @xmath186 and @xmath187 for all @xmath6 and @xmath62 .",
    "\\(1 ) for any choices of @xmath188 , we have @xmath189 where the inequality holds since @xmath190 is required to be nonnegative and @xmath18 is a nonincreasing function . by algorithm [ alg : hedge2drift ] , @xmath191 is equal to @xmath192 , leading to @xmath193 since @xmath67 , we must have @xmath194 except for the best @xmath6 actions , which means @xmath195 .",
    "this holds for any choices of @xmath11 , so @xmath65 .",
    "\\(2 ) by algorithm [ alg : drift2hedge ] and the condition @xmath196 , we have @xmath197 which means there are at most @xmath198 actions satisfying @xmath199 , and thus @xmath200 . since this holds for any choices of @xmath43 , we have @xmath201 .",
    ".different algorithms derived from algorithm [ alg : hedge_recipe ] , and comparisons with normalhedge [ cols=\"^,^,^,^,^\",options=\"header \" , ]"
  ],
  "abstract_text": [
    "<S> we provide a general mechanism to design online learning algorithms based on a minimax analysis within a drifting - games framework . </S>",
    "<S> different online learning settings ( hedge , multi - armed bandit problems and online convex optimization ) are studied by converting into various kinds of drifting games . the original minimax analysis for drifting games </S>",
    "<S> is then used and generalized by applying a series of relaxations , starting from choosing a convex surrogate of the 0 - 1 loss function . with different choices of surrogates , </S>",
    "<S> we not only recover existing algorithms , but also propose new algorithms that are totally parameter - free and enjoy other useful properties . </S>",
    "<S> moreover , our drifting - games framework naturally allows us to study high probability bounds without resorting to any concentration results , and also a generalized notion of regret that measures how good the algorithm is compared to all but the top small fraction of candidates . </S>",
    "<S> finally , we translate our new hedge algorithm into a new adaptive boosting algorithm that is computationally faster as shown in experiments , since it ignores a large number of examples on each round . </S>"
  ]
}