{
  "article_text": [
    "users confront with the `` information overload '' dilemma and it is increasingly difficult for them to choose the preferred items over others because of the growing large item set , e.g. , hundreds of millions products at amazon.com and tens of thousands videos at netflix.com  @xcite . recommender systems ( rss ) assist users in tackling this problem and help them make choices by ranking the items based on their past behavior history .",
    "item recommendation predicts a personalized ranking over a set of items for individual user and hence alleviates the dilemma .",
    "the rating - based ( or point - wise ) methods predict ratings that a user will give to items and then rank the items according to their predicted ratings .",
    "many methods are proposed and matrix factorization based models are most popular due to their scalability , simplicity , and flexibility  @xcite .",
    "this paradigm concentrates on explicit feedback and it faces the difficulties in collecting them .",
    "meanwhile , the ranking - based ( pair - wise ) methods are presented with seen items and then rank the seen above the unseen .",
    "bayesian personalized ranking ( bpr - mf ) and collaborative item selection are typical representatives  @xcite .",
    "this paradigm takes advantage of widely available implicit feedback but it usually ignores a kind of important information : item reviews",
    ".    * related works .",
    "* item reviews justify the preferences of users and help alleviate the cold - start problem ; they are a diverse and complementary data source for recommendation beyond the user - item co - rating information .",
    "the cmf method  @xcite can be adapted to factorize the user / item - word matrix constructed from the item reviews .",
    "the ctr  @xcite and hft  @xcite models integrate explicit ratings with item content / reviews to build better rating predictors ; they employ topic modeling to learn hidden topic factors which explain the variations of users preferences .",
    "the ctrank model  @xcite also adopts topic modeling to exploit item meta - data like article title and abstract via bag - of - words representation for one - class collaborative filtering  @xcite , while the cdr  @xcite and cke  @xcite models adopt stacked denoising autoencoders .",
    "nevertheless , integrating item reviews into the ranking - based methods presents both opportunities and challenges for traditional bayesian personalized ranking .",
    "there are few works on leveraging item reviews to improve personalized ranking .    in this paper",
    "we propose two novel and simple models to incorporate item reviews into bpr - mf . like hft",
    ", they integrate item reviews and unlike hft they generate a ranked list of items for individual ranking . like ctrank , they focus on personalized ranking and unlike ctrank they are based on matrix factorization and using word embeddings to extract features . like bpr - mf , they rank preferred items over others and unlike bpr - mf they leverage the information from item reviews . in each of the two models , we make use of text features extracted from item reviews using word embeddings . and on top of text features we uncover the review dimensions that explain the variation in users feedback .",
    "these review factors represent a prior preference of a user .",
    "one model treats the review factor space independent of the latent factor space ; another connects implicit feedback and item reviews through the shared item space .",
    "the contributions of this work are summarized as follows .",
    "we propose two novel models to integrate item reviews into matrix factorization based bayesian personalized ranking ( section [ paper : different ] and [ paper : shared ] ) .",
    "they generate a ranked list of items for individual user by leveraging the information from item reviews .    \\2 . for exploiting item reviews ,",
    "we build the proposed models on the top of text features extracted from them .",
    "we demonstrate a simple and effective way of extracting features from item reviews by averagely composing word embeddings ( section [ paper : features ] ) .",
    "we empirically evaluate the proposed models on multiple real - world datasets which contains over millions of feedback in total .",
    "the experimental results show the benefit of leveraging item reviews on personalized ranking prediction .",
    "we also conduct analyses to understand the proposed models including the training efficiency and the impact of the number of latent factors .",
    "before proposing our models , we briefly review the personalized ranking task and then describe the problem statement . to this end",
    ", we first introduce the notations used throughout the paper .",
    "suppose there are @xmath0 users @xmath1 and @xmath2 items @xmath3 .",
    "we reserve @xmath4 for indexing users and @xmath5 for indexing items .",
    "let @xmath6 denote the user - item binary implicit feedback matrix , where @xmath7 is the preference of user @xmath8 on item @xmath9 , and we mark a zero if it is unknown .",
    "define @xmath10 as the set of items on which user @xmath8 has an action : @xmath11 .",
    "rating - based methods  @xcite and ranking - based methods  @xcite are mainly to learn the latent user factors @xmath12 \\in \\mathbb{r}^{f \\times m}$ ] and latent item factors @xmath13 \\in \\mathbb{r}^{f \\times n}$ ] from partially observed feedback @xmath14 .",
    "item @xmath9 may have text information , e.g. , review @xmath15 commented by user @xmath8 .",
    "we aggregate all reviews of a particular item as a ` doc ' @xmath16 .",
    "approaches like ctr and hft  @xcite integrate item content / reviews with explicit ratings for rating prediction using topic modeling .",
    "another approach is to learn word embeddings and then compose them into document level as the item text features ; we adopt this way of extracting text features @xmath17 from @xmath18 ( see section  [ paper : features ] ) .",
    "our work focuses on the item recommendation or personalized ranking task where a ranked list of items is generated for each individual user .",
    "the goal is to accurately rank the unobserved items which contain both truly negative items ( e.g. , the user dislikes the netflix movies or is not interesting in buying amazon products ) and missing ones ( e.g. , the user wants to see a movie or buy a product in the future when she knows it ) .    instead of accurately predicting unseen ratings by learning a model from training samples @xmath19 where @xmath20 , personalized ranking optimizes for correctly ranking item pairs by learning a model from training tuples @xmath21 .",
    "the meaning of item pairs of a user @xmath22 is that she prefers the former than the latter , i.e. , the model tries to reconstruct parts of a total order @xmath23 for each user @xmath8 . from the history feedback @xmath14",
    "we can infer that the observed items @xmath9 are ranked higher than the unobserved ones @xmath24 ; and for both observed items",
    "@xmath25 or both unobserved items @xmath26 we can infer nothing .",
    "random ( negative ) sampling is adopted since the number of such pairs is huge .",
    "see the original bpr paper  @xcite for more details .",
    "* problem 1 .",
    "* personalized ranking with item reviews .",
    "* input : * 1 ) a binary implicit feedback matrix @xmath14 , 2 ) an item reviews corpus @xmath27 , and 3 ) a user @xmath8 in the user set @xmath28 .",
    "* output : * a ranked list @xmath23 over the unobserved items @xmath29 . in problem 1 , to generate the ranked list , we have item reviews to exploit besides implicit feedback .",
    "in this section , we propose two models as a solution to problem 1 which leverage item reviews into bayesian personalized ranking . one model treats the review factor space independent of the latent factor space ( section [ paper : different ] ) .",
    "another model connects implicit feedback and item reviews through the shared item space ( section [ paper : shared ] ) . in each of the two proposed models , we make use of text features extracted from item reviews via word embeddings ( section [ paper : features ] ) . on top of text features we uncover",
    "the review dimensions that explain the variation in users feedback and these review factors represent a prior preference of a user .",
    "both models are based on basic matrix factorization ( section [ paper : basicmf ] ) and learned under the bayesian personalized ranking framework ( section [ paper : learning ] ) .",
    "the basic matrix factorization ( basic mf ) is mainly to find the latent user - specific feature matrix @xmath30^m_1 $ ] and item - specific feature matrix @xmath31^n_1 $ ] to approximate the partially observed feedback matrix @xmath14 in the regularized least - squares ( or ridge regression ) sense by solving the following problem .",
    "@xmath32 where @xmath33 is the regularization parameter to avoid over - fitting .",
    "the predicted scores @xmath34 can be modeled by various forms which embody the flexibility of matrix factorization .",
    "a basic form is @xmath35 , where @xmath36 , @xmath37 and @xmath38 are biases  @xcite .      in this section ,",
    "we propose our first model _ tbpr - diff _ to integrate item reviews with implicit feedback .",
    "analogical to the basic mf which factorizes the ratings into user- and item- _ latent _ factors , we can factorize the reviews into user- and item- _ text _ factors ( see the illustration in figure  [ fig : predictors]up ) .",
    "the tbpr - diff model sharpens this idea and teases apart the rating dimensions into latent factors and text factors : @xmath39 where the term @xmath40 is newly introduced to capture the text interaction between user @xmath8 and item @xmath9 . to exploit item reviews , text features",
    "@xmath17 are firstly extracted from item reviews via word embeddings ( hence they are known and fixed ) .",
    "the shared embedding kernel @xmath41 linearly transforms original text features @xmath42 from high - dimensional space ( e.g. , 200 ) into a lower text rating space ( e.g. , 15 ) , and then it @xmath43 interacts with text factors of user @xmath44 .",
    "a text bias vector @xmath45 is also introduced to model users overall preferences towards the item reviews .",
    "the details of text features extracted from item reviews using word embeddings are described later ( see section  [ paper : features ] ) .",
    "since the text factors of user @xmath46 and of item @xmath43 are _ independent _ of latent factors @xmath47 and @xmath48 , there is no deep interactions between the information sources of observed feedback and item reviews , and hence they can not benefit from each other . also additional parameters increase the model complexity .",
    "based on these observations , we propose another model to alleviate the above challenges .      in this section ,",
    "we propose our second model _",
    "tbpr - shared _ to integrate item reviews with implicit feedback more compactly . for an item @xmath9 ,",
    "its latent factors @xmath48 learned from feedback can be considered as characteristics that it processes ; meanwhile , these characteristics are probably discussed in its reviews and hence exhibit in its text factors @xmath49 ( see the illustration in figure  [ fig : predictors]down ) . for user",
    "@xmath8 , if we let @xmath48 and @xmath50 be in the same space then it leads to deep interactions between text factors of user @xmath8 and the latent factor of item @xmath9 . the tbpr - shared model sharpens this idea and enables the deep interactions between text factors and latent factors as well as reduces complexity of the model : @xmath51    on the right hand , the last four terms are the same with the tbpr - diff model .",
    "different from the tbpr - diff model , the shared item factors @xmath48 now have two - fold meanings : one is item latent factors that represent items characteristics ; another is to interact with item text factors that capture items semantics from item reviews . also different from the tbpr - diff model",
    ", the preferences of a user now have a prior term which shows the ` text influence of her rated items ' captured by the text factors of corresponding items . in summary , on top of text",
    "features the tbpr - shared model uncovers the review dimensions that explain the variation in users feedback and these factors represent a prior preference of user .",
    "* remarks * * i. * the vbpr model  @xcite proposed an analogical formulation with eq  ( [ eq : diff ] ) .",
    "it exploits visual features extracted from item images and we leverage item features extracted from item reviews .",
    "the svd++ and nsvd  @xcite models proposed similar formulas with eq  ( [ eq : shared ] ) .",
    "they learn an implicit feature matrix to capture implicit feedback and we learn a text correlation matrix to capture text factors ; note that they did nt exploit item reviews and hence they had no the text bias term . * ii .",
    "* there can be an adjustable weight on the term of text ( i.e. , @xmath40 in eq  ( [ eq : diff ] ) and @xmath52 in eq  ( [ eq : shared ] ) ) to balance the influence from feedback and from reveiws , but here we just let feedback and reviews be equally important .",
    "before we delve into the learning algorithm , the preference predictors of tbpr - diff and of tbpr - shared models are shown in figure  [ fig : predictors ] .",
    "revisit problem 1 , we need to generate a ranked list of items for individual user .",
    "bayesian personalized ranking  @xcite is a generic pair - wise optimization framework that learns from the training item pairs using gradient descent .",
    "denote the model parameters as @xmath53 and let @xmath54 ( for simplicity we omit model parameters , and the notation @xmath55 is the same with @xmath7 ) represent an arbitrary real - valued mapping under the model parameters . then the optimization criterion for personalized",
    "ranking bpr - opt is    [ eq : bpr - opt ] ( ) _ ( u , i , j ) d_s - ^2 ,    where @xmath56 , and the sigmoid function is defined as @xmath57 .",
    "the meaning behind bpr - opt requires ranking items accurately as well as using a simple model .    under the generic bpr - opt framework",
    ", we derive the learning process for our proposed models tbpr - diff and tbpr - shared by embodying @xmath58 with @xmath59 and @xmath60 , respectively .",
    "the bpr - opt defined in eq  ( [ eq : bpr - opt ] ) is differentiable and hence gradient ascent methods can be used to maximize it . for stochastic gradient ascent ,",
    "a triple @xmath22 is randomly sampled from training sets @xmath61 and then update the model parameters by :    [ eq : update - sgd ] + ( ( -x_uij ) - ) .",
    "the same gradients for user latent factors and bias terms of both models are : @xmath62    parameter gradients of the model tbpr - diff are : @xmath63    parameter gradients of the model tbpr - shared are : @xmath64    @xmath65    * complexity of models and learning . * the complexity of model tbpr - diff is @xmath66 while the complexity of model tbpr - shared is @xmath67 .",
    "we can see that the latter model reduces the complexity by @xmath68 , i.e. , the parameters @xmath69^m_1 $ ] . for updating each training sample @xmath70 ,",
    "the complexity of learning tbpr - diff is linear in the number of dimensions ( @xmath71 ) while the complexity of learning tbpr - shared is also linear provided that the scale of rated items of users is amortizing constant , i.e. , @xmath72 , which holds in real - world datasets because of sparsity ( see table  [ table : datasets ] ) .",
    "recall that when generating the ranked list of items for individual user , we have item reviews to exploit besides implicit feedback . to exploit item reviews ,",
    "we extract text features from them , i.e. , there is a feature vector for each item .",
    "our proposed two models are both built on the top of text features ( @xmath73_{i=1}^{n}$ ] ) and hence they are important for improving personalized ranking . in this section ,",
    "we give one simple way to extract text features from reviews of item  word embedding .",
    "the sgns model  @xcite is an architecture for learning continuous representations of words from large corpus ; these representations , or word embeddings , can capture the syntactic and semantic relationships of words .",
    "we first run the google word2vec code on amazon reviews corpus ( see table  [ table : datasets ] ) using the default setting ( particularly , dimensionality @xmath74 ) to learn a vector @xmath75 for each word @xmath76 .",
    "and then we directly sum up all of the embeddings in an item s reviews ( excluding stop words ) and get a composition vector as the text feature for this item : @xmath77 to get @xmath42 , we can also use complex methods ( e.g. , tensor networks to compose word embeddings or learning the doc representation directly ) ; they are left for future work .",
    "lrrrrrr|c datasets & # users & # items & # feedback & # words & # colduser & # colditem & density ( % ) + girls & 778 & 3,963 & 5,474 & 302 m & 572 & 3,946 & 0.177 + boys & 981 & 4,114 & 6,388 & 302 m & 787 & 4,080 & 0.158 + baby & 1,238 & 4,592 & 8,401 & 302 m & 959 & 4,482 & 0.147 + men & 21,793 & 55,647 & 157,329 & 302 m & 15,821 & 52,031 & 0.013 + women & 62,928 & 157,656 & 504,847 & 302 m & 41,409 & 143,444 & 0.005 + phones&58,741 & 77,979 & 420,847 & 210 m & 43,429 & 67,706 & 0.009 +",
    "we evaluate our two models on multiple amazon.com datasets in terms of ranking performance ( section  [ paper : data - metric ] ) .",
    "they integrate item reviews into bayesian personalized ranking optimization criterion and we want to know the benefit from them .",
    "so we compare with bpr - mf  @xcite which ignores them and also with the most popular ( pop ) baseline that does nt show personalized ranking ( section  [ paper : compare ] ) .",
    "we report the results in different settings ( section  [ paper : results ] ) and analyse the proposed methods ( section  [ paper : analysis ] )",
    ".      * datasets .",
    "* we evaluate our models on six amazon datasets  @xcite  http://jmcauley.ucsd.edu / data / amazon/. they consist of five from clothing and shoes category , and one from cell phones and accessories .",
    "we use the review history as implicit feedback and aggregate all users reviews to an item as a doc for this item .",
    "we draw the samples from original datasets such that every user has rated at least five items ( i.e. , @xmath78 ) and the statistics of final evaluation datasets are show in table  [ table : datasets ] . from the table",
    "we can see that : 1 ) the observed feedback is very sparse , typically less than 0.01% ; 2 ) the average feedback events for users are typical about ten , i.e. , @xmath79 holds ; 3 ) more than half of the users and of the items are cold and have feedback less than seven . note that the cold - users/-items are those that have less than seven feedback events , and the feedback density = @xmath80 .",
    "we split each of the whole datasets into three parts : training , validation , and test . in detail , for each user @xmath81 , we randomly sample two items from her history feedback for test set @xmath82 , two for validation set @xmath83 , and the rest for training set @xmath84 ; and hence @xmath85 .",
    "this is the reason that we discard users who rated items less than five to ensure that there is at least one training sample for her .",
    "* evaluation protocol .",
    "* for item recommendation or personalized ranking , we need to generate a ranked list over the unobserved items .",
    "therefore for the hold - out test item @xmath86 of individual user @xmath8 , the evaluation calculates how accurately the model rank @xmath9 over other unobserved items @xmath87 .",
    "the widely used measure area under the roc curve ( auc ) sharpens the ranking correctness intuition :    @xmath88    where @xmath89 and the @xmath90 is an indicator function .",
    "a higher auc score indicates a better recommendation performance .",
    "the validation set @xmath91 is used to tune hyperparameters and we report the corresponding results on the test set @xmath92 .",
    "we compare our proposed models tbpr - diff ( see eq  ( [ eq : diff ] ) ) and tbpr - shared ( see eq  ( [ eq : shared ] ) ) with the most popular * ( pop ) * and * bpr - mf *  @xcite baselines .",
    "the difference of models lies in their preference predictors .    * reproducibility*. we use the released code in  @xcite to implement the comparing methods and our proposed models .",
    "the hyperparameters are tuned on the validation set . referring to the default setting , for the brp - mf model , the norm - penalty @xmath93 , and learning rate @xmath94 . as with our proposed models tbpr - diff and tbpr - shared , the norm - penalty @xmath95 for latent factors and @xmath96 for text factors , and learning rate @xmath97 . for simplicity",
    ", the number of latent factors equals to the number of text factors ; the default values for them are both fifteen ( i.e. , @xmath98 ) .",
    "since the raw datasets , comparing code , and parameter setting are given publicly , we confidently believe our experiments are easily reproduced .",
    "cccccc|cc datasets&setting & pop & bpr - mf & tbpr - diff&tbpr - shared & improv1 & improv2 + girls & all & 0.1699 & 0.5658 & 0.5919 & * 0.5939 * & 4.966 & 7.097 + boys & all & 0.2499 & 0.5493 & 0.5808 & * 0.5852 * & 6.535 & 11.99 + baby & all & 0.3451 & 0.5663 & 0.5932 & * 0.6021 * & 6.321 & 16.18 + & all & 0.5486 & 0.6536 & 0.6639 & * 0.6731 * & 2.983 & 18.57 + men & cold & 0.4725 & 0.5983 & 0.6114 & * 0.6225 * & 4.044 & 19.23 + & all & 0.5894 & 0.6735 & 0.6797 & * 0.6842 * & 1.588 & 12.72 + women & cold & 0.4904 & 0.6026 & 0.6110 & * 0.6152 * & 2.090 & 11.22 + & all & 0.7310 & 0.7779 & 0.7799 & * 0.7809 * & 0.386 & 6.396 + phones & cold & 0.5539 & 0.6415 & 0.6464 & * 0.6467 * & 0.811 & 5.936 +      the auc performance results on eight amazon.com datasets are shown in table  [ table : results ] where the last but one column is @xmath99 , and the last column is @xmath100 . for each dataset",
    "there are three evaluation settings : the _ all items or all _ setting evaluates the models on the full test set @xmath101 ; the _ cold start or cold _ setting evaluates the models on a subset @xmath102 such that the number of training samples for each item within @xmath103 is no greater than three ( i.e. , @xmath104 or @xmath105 ) ; the _ warm _ setting evaluates the models on the difference set of all and cold .",
    "revisit the table  [ table : datasets ] we can see that : 1 ) almost all of the items are cold - item for datasets girls , boys , and baby ; and hence the results of cold setting are almost the same with all and the results of warm setting is not available to get a statistical reliable results ; and 2 ) for other three datasets , the percent of cold - items is also more than 86% which requires the model to address the inherent cold start nature of the recommendation problem .",
    "there are several observations from the evaluation results .",
    "_ under the all setting",
    "_ , tbpr - shared is the top performer , tbpr - diff is the second , with bpr - mf coming in third and pop the weakest .",
    "these results firstly show that leveraging item reviews besides the feedback can improve the personalized ranking ; and also show that the personalization methods are distinctly better than the user - independent pop method .",
    "for example , tbpr - shared averagely obtains relative 4.83% performance improvement compared with bpr - mf on the first three smaller datasets in terms of auc metric , and 2.74% in total six datasets .",
    "this two figures show , to some extent , that transferring the knowledge from auxiliary data source ( here item reviews ) helps most when the target data source ( here rating feedback ) is not so rich .",
    "_ under the cold setting",
    "_ , tbpr - shared is the top performer , tbpr - diff is the second , with bpr - mf coming in third and pop is also the weakest .",
    "these results firstly show that leveraging item reviews besides the feedback can improve the personalized ranking even in the cold start setting ; and also show that the personalization methods are distinctly better than the user - independent pop method since the cold items are not popular .",
    "in detail , tbpr - shared averagely obtains relative 2.31% performance improvement compared with bpr - mf in terms of auc metric .",
    "furthermore , tbpr - shared compared with bpr - mf , the relative improvement in the _ cold start setting _ is about 1.6 times than that in the _ all setting _ which implies that integrating item reviews more benefits when observed feedback is sparser . as with the results on the phones dataset",
    ", revisiting table  [ table : datasets ] we can see that the ratio of cold items over all item is 86.8% which is far less than those on other two datasets ( @xmath106 ) . and in this case adding auxiliary information does nt help much .",
    "we also evaluate on _ the warm setting _",
    "( not shown in table  [ table : results ] ) , and all of the personalized , complex methods are worse than the user - independent , simple method pop .",
    "warm items are more likely to be popular and show less personalized characteristics .",
    "it reminds us the commonplace that recommendation plays an important role in long - tailed items .      after demonstrating the benefits of leveraging item reviews",
    ", we analyse the proposed models from two points ; one is the impact of number of latent factors , and one is the training efficiency and convergence analysis .",
    "more depth investigation like the impact of embedding dimensionality and of corpus source to train the embeddings , is left to future work .",
    "* impact of the number of latent factors . * the two proposed models tbpr - shared and tbpr - diff have two important hyperparameters ; one is the number of latent factors @xmath107 and one is the number of text factors @xmath108 . for simplicity , we let the two values equal .",
    "we vary the number of latent factors @xmath109 to observe the performance results of different methods .",
    "the test auc scores are shown in figure  [ fig : factors ] .",
    "on the girls and boys datasets , both of the personalized models are to perform better as the number of factors increases ; on the other datasets , the performance improves as the number of factors increases to around fifteen ; then it does nt go up and may even downgrade .",
    "we set the default value as 15 .",
    "also the plots visually show the benefits of integrating item reviews ( tbpr - shared vs. bpr - mf ) and of generating a personalized ranking item list for individual user ( tbpr - shared and bpr - mf vs. pop ) .",
    "* training efficiency and convergence analysis . * the complexity of learning is approximately linear in the number of parameters of our proposed models .",
    "figure  [ fig : iterations ] shows the auc scores of the tbpr - shared model on validation sets with increasing training iterations . in summary , our models take 3 - 4 times more iterations to converge than bpr - mf . on three smaller datasets ( girls , boys , and baby )",
    ", the first five iterations are enough to get a better score than pop ; and on the other larger datasets ( men , women , and phones ) , it takes longer .    as a reference",
    ", the bpr - mf model usually converges in 50 iterations . as another reference ,",
    "all of our experiments are completed in about one week using one server that has 65gib memory and 12 cores with frequency 3599mhz .",
    "we proposed two models to integrate item reviews into bayesian personalized ranking based on matrix factorization for cold start recommendation . in each of the two models , we make use of text features extracted from item reviews via word embeddings . on top of text features we uncover the review dimensions that explain the variation in users feedback .",
    "these review factors represent a prior preference of a user and show the ` text influence of her rated items ' .",
    "empirical results on multiple real - world datasets demonstrated the improved ranking performance under the all and cold start setting .",
    "and the shared space model is slightly better than the different space one which shows the benefits of considering the interactions between latent factors and text factors .",
    "training efficiency is analyzed .",
    "since we investigate the benefits of leveraging item reviews , we only compare our models with bpr - mf ( and pop ) ; and to know the effectiveness , comparing with more baselines is needed .",
    "the construction strategy of positive / negative samples is also worth further investigating because it deeply affects the modeling design , the learning results , and the evaluation performance ."
  ],
  "abstract_text": [
    "<S> item recommendation task predicts a personalized ranking over a set of items for individual user . </S>",
    "<S> one paradigm is the rating - based methods that concentrate on explicit feedbacks and hence face the difficulties in collecting them . </S>",
    "<S> meanwhile , the ranking - based methods are presented with rated items and then rank the rated above the unrated . </S>",
    "<S> this paradigm uses widely available implicit feedback but it usually ignores some important information : item reviews . </S>",
    "<S> item reviews not only justify the preferences of users , but also help alleviate the cold - start problem that fails the collaborative filtering . in this paper , we propose two novel and simple models to integrate item reviews into matrix factorization based bayesian personalized ranking ( bpr - mf ) . in each model , we make use of text features extracted from item reviews via word embeddings . on top of text features we uncover </S>",
    "<S> the review dimensions that explain the variation in users feedback and these review factors represent a prior preference of a user . </S>",
    "<S> experiments on real - world data sets show the benefits of leveraging item reviews on ranking prediction . </S>",
    "<S> we also conduct analyses to understand the proposed models . </S>"
  ]
}