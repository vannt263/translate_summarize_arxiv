{
  "article_text": [
    "in both spoken and written language , word occurrences are not random but vary greatly from document to document .",
    "indeed , the field of information retrieval ( ir ) relies on the degree of departure from randomness as a discriminative indicator .",
    "ir systems are typically based on unigram statistics ( often referred to as a `` bag - of - words '' model ) , coupled with sophisticated term weighting schemes and similarity measures  @xcite . in an attempt to mathematically realise the intuition that an occurrence of a certain word may increase the chance that the same word is observed later ,",
    "several probabilistic models of word occurrence have been proposed .",
    "much of this work has evolved around the use of ( a mixture of ) the poisson distribution  @xcite .",
    "recently , church and gale have demonstrated that a continuous mixture of poisson distributions can produce accurate estimates of variable word rate  @xcite .",
    "lowe has introduced a beta - binomial mixture model which was applied to topic tracking and detection  @xcite .",
    "although a constant word rate is an unlikely premise , it is nevertheless adopted in many areas including @xmath0-gram language modelling . in order to address the problem of variable word rate ,",
    "several adaptive language modelling approaches have been proposed with a moderate degree of success .",
    "typically , some notion of `` topic '' is inferred from the text according to the `` bag - of - words '' model .",
    "information from different language model statistics ( , a general model and/or models specific to each topic ) are then combined using methods such as mixture modelling  @xcite or maximum entropy  @xcite .",
    "the _ dynamic cache model _",
    "@xcite is a related approach , based on an observation that recently appearing words are more likely to re - appear than those predicted by a static @xmath0-gram model .",
    "it blends cached unigram statistics for recent words with the baseline @xmath0-grams using an interpolation scheme .",
    "theoretically , it should not be necessary to rely on an _ ad hoc _ device such as a cache in order to model variable word occurrences .",
    "all the parameters of a language model may be completely determined according to probabilistic model of word rate , such as a poisson mixture .    in this paper",
    ", we outline the theoretical background for modelling the variable word rate , and illustrate a key observation that word rates are not static using spoken data transcripts",
    ". the constant word rate assumption is then eliminated , and we introduce a variable word rate @xmath0-gram language model . an approach to estimating relative frequencies using prior information of word occurrences is presented .",
    "it is integrated with standard @xmath0-gram modelling that naturally involves discounting and smoothing schemes for practical use . using the darpa / nist 4e north american broadcast news task ,",
    "the approach demonstrates the reduction of perplexity up to 10% .",
    "in this section , we illustrate how the assumption of a constant word rate fails to capture the statistics of word occurrence in spoken ( or written ) documents .",
    "we show that the word rate is variable and may be modelled using a poisson distribution or a continuous mixture of poissons .",
    "the poisson distribution is one of the most commonly observed distributions in both natural and social environments .",
    "it is fundamental to the queueing theory : under certain conditions , the number of occurrences of a certain event during a given period , or in a specified region of space , follows a poisson distribution ( a _ poisson process _  @xcite ) .    by assuming randomness in a poisson process",
    ", word rate is no longer uniform .",
    "firstly , we provide a loose definition of a document as a unit of spoken ( or written ) data of a certain length that contains some topic(s ) , or content(s ) .",
    "we consider a model in which a word occurs at random in a fixed length document . for a set of documents",
    "we assume that each document produces this word independently and that the underlying process is the poisson with a single parameter @xmath1 .",
    "formally , a poisson distribution is a discrete distribution ( of a random variable @xmath2 ) which is defined for @xmath3 such that @xmath4}(x )      = { \\cal p}(x = x;\\ \\lambda )      = \\frac{e^{- \\lambda } \\lambda^x}{x ! } \\label{eq : poisson}\\end{aligned}\\ ] ] whose expectation and variance are given by @xmath5 = \\lambda$ ] and @xmath6 = \\lambda$ ] , respectively  @xcite .      a less constrained model of variable word rate is offered by a multiple of poissons , rather than a single poisson .",
    "suppose the parameter @xmath7 of the pdf  ( [ eq : poisson ] ) is distributed according to some function @xmath8 , then we define a continuous mixture of poisson distributions by @xmath9}(x ) \\phi(\\lambda ) d\\lambda \\ .",
    "\\label{eq : mixture}\\end{aligned}\\ ] ] in particular , if @xmath8 is a gamma distribution , , @xmath10 for @xmath11 and @xmath12 , then the integral  ( [ eq : mixture ] ) is reduced to a discrete distribution for @xmath3 such that @xmath13}(x ) \\hspace{-0.1 in } & = & \\hspace{-0.1 in }      { \\cal nb}(x = x;\\ \\alpha , \\beta ) \\nonumber \\\\    & = & \\hspace{-0.1 in }      \\left ( \\begin{array}{c } \\alpha + x - 1 \\\\ x \\end{array } \\right )      \\frac{\\beta^x}{(1 + \\beta)^{\\alpha + x } } \\ .",
    "\\label{eq : nb}\\end{aligned}\\ ] ] this @xmath14}(x)$ ] is a negative binomial distribution be @xmath15 in  ( [ eq : mixture ] ) .",
    "this integration is straightforward using the definition of the gamma function , @xmath16 , and the recursion , @xmath17 .",
    "the resultant pdf  ( [ eq : nb ] ) has a slightly unconventional form in comparison to that in most of standard textbooks ( ,  @xcite ) , but is identical by setting a new parameter @xmath18 with @xmath19 . ] and its expectation and variance are respectively given by @xmath5 = \\alpha\\beta$ ] and @xmath6 = \\alpha\\beta ( \\beta + 1)$ ] .",
    "the histograms in figure  [ fig : uhist ] show the number of word ( unigram ) occurrences in spoken news broadcast , taken from transcripts of the 4e broadcast news acoustic training data ( 199697 ) .",
    "these transcripts were separated into documents according to section markers and those with less than 100 words were removed , resulting 2583 documents containing slightly less than 1.3 million words in total . in the following ,",
    "the number of word occurrences were normalised to 1000-word length documents .    and  appeared approximately the same number of times across all the transcripts .",
    "using a constant word rate assumption , they would have been assigned a probability of around 0.0086 .",
    "however their occurrence rates varied from document to document ; about 11% and 33% of all documents did not contain  and  ( respectively ) , while 1% and 3% contained these words more than 30 times .",
    "this seems to indicate that occurrences of  is less dependent on the content of the document .",
    "a negative binomial distribution was used to model the variable word rate in each case ( the solid line in figure  [ fig : uhist ] ) .",
    "the negative binomial seems to model word occurrence rate relatively well for most vocabulary items , regardless of frequency .",
    "figure  [ fig : uhist ] illustrates this for one of the most frequent words  ( probability of 0.023 according to the constant word rate assumption ) and the less frequently occurring  ( less than 0.00029 ) .",
    "in particular ,  appeared only in 93 out of 2583 documents , but 28 of them contained more than 10 instances , suggesting strong correlation with document content .    .",
    "they are fitted by negative binomial distributions ( solid lines).,width=307 ]    we also collected statistics of bigrams appearing in the broadcast news transcripts .",
    "figure  [ fig : bhist ] show histograms and their negative binomial fits for bigrams  and .",
    "although very sparse ( , they appeared in 127 and 6 documents , respectively ) , this suggests that variable bigram rate can also be modelled using a continuous mixture of poissons .",
    "taking word occurrence rate into account changes a probabilistic language model from a situation akin to playing a lottery , to something closer to betting on a horse race : the odds for a certain word improve if it has come up in the past . in this section",
    ", we eliminate the constant word rate assumption and present a variable word rate @xmath0-gram language model .",
    "let @xmath20 denote a relative frequency after we observe @xmath21 occurrences of word @xmath22 .",
    "it is calculated by @xmath23 the function is defined for @xmath24 , where @xmath25 is a fixed document length ( , @xmath25 is normalised to 1000 in figures  [ fig : uhist ] and  [ fig : bhist ] ) .",
    "@xmath26 is the occurrence rate for word @xmath22 in an @xmath25-length document ( , poisson , negative binomial ) , satisfying @xmath27 in particular , @xmath28 which corresponds to the case with no prior information of word occurrence .",
    "for the conventional approach with the constant word rate assumption , this @xmath29 is not modified regardless of any word occurrences .",
    "further , function  ( [ eq : rf_var ] ) satisfies our intuition ; the value of @xmath20 increases monotonically as the number of observation @xmath21 accumulates ( easy to verify ) , and it reaches a unity ( ` 1 ' ) when @xmath30 .    ) .",
    "the right figure demonstrates relative frequencies after a certain number of word occurrences .",
    "circles ( ` ' ) correspond to relative frequencies under the constant word rate assumption ( 0.023 for  and 0.00029 for ) .,width=307 ]    the characteristics of function  ( [ eq : rf_var ] ) are illustrated in figure  [ fig : rfrequency ] .",
    "the right hand figure shows relative frequencies for  and  after a certain number of previous observations of the word .",
    "it indicates that the first few instances of the frequent word ( ) do not modify its relative frequency very much , but have a substantial effect on the relative frequency of the less common word ( ) . as the number of observations increases , the former is caught up by the latter .    finally , in order to convert this relative frequency model to any type of probabilistic model for language , normalisation is required .",
    "this is achieved by dividing @xmath20 by @xmath31 , where @xmath32 implies a set of vocabulary .",
    "variable relative frequencies for bigrams can also be calculated in a similar fashion .      for any practical application ,",
    "smoothing of the probability estimates is essential to avoid zero probabilities for events that were not observed in the training data .",
    "let @xmath33 denote a bigram entry ( a word @xmath34 followed by @xmath22 ) in the model .",
    "further , @xmath35 implies a relative frequency after we observe @xmath36 occurrences of the bigram .",
    "a bigram probability @xmath37 may be smoothed with a unigram probability @xmath38 . using the interpolation method  @xcite : @xmath39 where @xmath40 implies a `` discounted '' relative frequency ( described later ) and @xmath41 is a non - zero probability estimate ( , the probability that a bigram entry @xmath33 exists in the model ) .",
    "alternatively , the back - off smoothing  @xcite may be applied : @xmath42 in  ( [ eq : backoff ] ) , @xmath43 is a back - off factor and is calculated by @xmath44 a unigram probability @xmath38 can be obtained similarly by smoothing with some constant value .    finally , a number of standard discounting methods exist for constant word rate models ( see , ,  @xcite ) .",
    "analogous discounting functions for variable word rate models may be @xmath45 for the absolute discounting , and @xmath46 for the good - turing discounting . discounting factors ( @xmath47 and @xmath48 )",
    "may be obtained using zero prior information case  , @xmath49 s of all bigrams in the model  and the rest should be referred to , ,  @xcite or  @xcite .      as noted in section  [ sc : word ] , we extracted 2583 documents from the transcripts of the broadcast news acoustic training data , each with a minimum of 100 words .",
    "a vocabulary of 19885 words was selected and 390000 bigrams were counted . in these experiments ,",
    "the absolute discounting scheme  ( [ eq : absolute ] ) was applied , followed by interpolation smoothing  ( [ eq : interpolation ] ) . figure  [ fig : perplex ] shows perplexities for the reference ( key ) transcription of the 1997 4e evaluation data , containing three hours of speech and approximately 32000 words . using conventional modelling with a constant word rate assumption , unigram and bigram perplexities",
    "were 936.5 and 237.9 , respectively .",
    "for the variable word rate models , the poisson distribution was adopted because of simplicity in calculation .",
    "the number of word occurrences were normalised to @xmath25-word length document with @xmath25 being between 200 and 50000 , and the model parameters were modified ` on - line ' during the perplexity calculation .",
    "for each occurrence of a word ( bigram ) in the evaluation data , a histogram of the past @xmath25 words ( bigrams ) was collected and their relative frequencies were modified according to the poisson estimates ( appropriate normalisation applied ) , then discounted and smoothed .",
    "as figure  [ fig : perplex ] indicates , the variable word rate models were able to reduced perplexities from the constant word rate models .",
    "a unigram perplexity of 843.4 ( 10% reduction ) was achieved when @xmath50 , and a bigram perplexity of 219.0 ( 8% reduction ) when @xmath51 .",
    "the difference was predictable because bigrams were orders of magnitude more sparse than unigrams .",
    "in this paper , we have presented a variable word/@xmath0-gram rate language model , based upon an approach to estimating relative frequencies using prior information of word occurrences .",
    "poisson and negative binomial models were used to approximate word occurrences in documents of fixed length . using the broadcast news task ,",
    "the approach demonstrated a reduction of perplexity up to 10% , indicating potential although the technique is still premature .",
    "because of the data sparsity problem , it is not clear if the approach can be applied to language model components of current state - of - the - art speech recognition systems that typically use 3/4-grams .",
    "however , we believe this technique does have application to problems in the area of information extraction . in particular , we are planning to apply these methods to the named entity annotation task , along with further theoretical development .",
    "k.  sprck jones , s.  walker , and s.  e. robertson . a probabilistic model of information retrieval : development and status .",
    "technical report tr446 , university of cambridge , computer laboratory , 1998 .",
    "available from .",
    "stephen  a. lowe .",
    "the beta - binomial mixture model for word frequencies in documents with applications to information retrieval . in _ proceedings of eurospeech-99",
    "_ , volume  6 , pages 24432446 , budapest , september 1999 .",
    "f.  jelinek and r.  l. mercer .",
    "interpolated estimation of markov source parameters from sparse data . in _ proceedings of the workshop : pattern recognition in practice",
    "_ , pages 381397 , amsterdam , may 1980 ."
  ],
  "abstract_text": [
    "<S> the rate of occurrence of words is not uniform but varies from document to document . despite this observation , parameters for conventional @xmath0-gram language models </S>",
    "<S> are usually derived using the assumption of a constant word rate . in this paper </S>",
    "<S> we investigate the use of variable word rate assumption , modelled by a poisson distribution or a continuous mixture of poissons . </S>",
    "<S> we present an approach to estimating the relative frequencies of words or @xmath0-grams taking prior information of their occurrences into account . discounting and smoothing schemes </S>",
    "<S> are also considered . using the broadcast news task </S>",
    "<S> , the approach demonstrates a reduction of perplexity up to 10% .    </S>",
    "<S> ( 0,0 ) ( 0,0 ) </S>"
  ]
}