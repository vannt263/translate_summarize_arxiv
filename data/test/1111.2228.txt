{
  "article_text": [
    "in recent years , mapreduce has emerged as a computational paradigm for processing large - scale data sets in a series of rounds executed on conglomerates of commodity servers @xcite , and has been widely adopted by a number of large web companies ( e.g. , google , yahoo ! , amazon ) and in several other applications ( e.g. , gpu and multicore processing ) .",
    "( see @xcite and references therein . )",
    "informally , a mapreduce computation transforms an input set of key - value pairs into an output set of key - value pairs in a number of _ rounds _ , where in each round each pair is first individually transformed into a ( possibly empty ) set of new pairs ( _ map step _ ) and then all values associated with the same key are processed , separately for each key , by an instance of the same reduce function ( simply called _ reducer _ in the rest of the paper ) thus producing the next new set of key - value pairs ( _ reduce step _ ) .",
    "in fact , as already noticed in @xcite , a reduce step can clearly embed the subsequent map step so that a mapreduce computation can be simply seen as a sequence of rounds of ( augmented ) reduce steps .",
    "the mapreduce paradigm has a functional flavor , in that it merely requires that the algorithm designer decomposes the computation into rounds and , within each round , into independent tasks through the use of keys .",
    "this enables parallelism without forcing an algorithm to cater for the explicit allocation of processing resources .",
    "nevertheless , the paradigm implicitly posits the existence of an underlying unstructured and possibly heterogeneous parallel infrastructure , where the computation is eventually run . while mostly ignoring the details of such an underlying infrastructure , existing formalizations of the mapreduce paradigm constrain the computations to abide with some local and aggregate memory limitations .    in this paper , we look at both modeling and algorithmic issues related to the mapreduce paradigm .",
    "we first provide a formal specification of the model , aimed at overcoming some limitations of the previous modeling efforts , and then derive interesting tradeoffs between memory constraints and round complexity for the fundamental problem of matrix multiplication and some of its applications .",
    "the mapreduce paradigm has been introduced in @xcite without a fully - specified formal computational model for algorithm design and analysis .",
    "triggered by the popularity quickly gained by the paradigm , a number of subsequent works have dealt more rigorously with modeling and algorithmic issues  @xcite .    in @xcite , a mapreduce algorithm specifies a sequence of rounds as described in the previous section .",
    "somewhat arbitrarily , the authors impose that in each round the memory needed by any reducer to store and transform its input pairs has size @xmath0 , and that the aggregate memory used by all reducers has size @xmath1 , where @xmath2 denotes the input size and @xmath3 is a fixed constant in @xmath4 .",
    "the cost of local computation , that is , the work performed by the individual reducers , is not explicitly accounted for , but it is required to be polynomial in @xmath2 .",
    "the authors also postulate , again somewhat arbitrarily , that the underlying parallel infrastructure consists of @xmath5 processing elements with @xmath5 local memory each , and hint at a possible way of supporting the computational model on such infrastructure , where the reduce instances are scheduled among the available machines so to distribute the aggregate memory in a balanced fashion .",
    "it has to be remarked that such a distribution may hide non negligible costs for very fine - grained computations ( due to the need of allocating multiple reducer with different memory requirements to a fixed number of machines ) when , in fact , the algorithmic techniques of @xcite do not fully explore the larger power of the mapreduce model with respect to a model with fixed parallelism . in @xcite the same model of @xcite is adopted but when evaluating an algorithm the authors also consider the total work and introduce the notion of work - efficiency typical of the literature on parallel algorithms .    an alternative computational model for mapreduce is proposed in @xcite , featuring two parameters which describe bandwidth and latency characteristics of the underlying communication infrastructure , and an additional parameter that limits the amount of i / o performed by each reducer .",
    "also , a bsp - like cost function is provided which combines the internal work of the reducers with the communication costs incurred by the shuffling of the data needed at each round . unlike the model of @xcite , no limits are posed to the aggregate memory size .",
    "this implies that in principle there is no limit to the allowable parallelism while , however , the bandwidth / latency parameters must somewhat reflect the topology and , ultimately , the number of processing elements .",
    "thus , the model mixes the functional flavor of mapreduce with the more descriptive nature of bandwidth - latency models such as bsp @xcite .",
    "a model which tries to merge the spirit of mapreduce with the features of data - streaming is the mud model of @xcite , where the reducers receive their input key - value pairs as a stream to be processed in one pass using small working memory , namely polylogarithmic in the input size .",
    "a similar model has been adopted in @xcite .",
    "mapreduce algorithms for a variety of problems have been developed on the aforementioned mapreduce variants including , among others , primitives such as prefix sums , sorting , random indexing @xcite , and graph problems such as triangle counting @xcite minimum spanning tree , @xmath6-@xmath7 connectivity , @xcite , maximal and approximate maximum matching , edge cover , minimum cut @xcite , and max cover @xcite .",
    "moreover simulations of the pram and bsp in mapreduce have been presented in @xcite . in particular , it is shown that a @xmath8-step erew pram algorithm can be simulated by an @xmath9-round mapreduce algorithm , where each reducer uses constant - size memory and the aggregate memory is proportional to the amount of shared memory required by the pram algorithm @xcite .",
    "the simulation of crew or crcw pram algorithms incurs a further @xmath10 slowdown , where @xmath11 denotes the local memory size available for each reducer and @xmath12 the aggregate memory size @xcite .",
    "all of the aforementioned algorithmic efforts have been aimed at achieving the minimum number of rounds , possibly constant , provided that enough local memory for the reducer ( typically , sublinear yet polynomial in the input size ) and enough aggregate memory is available .",
    "however , so far , to the best of our knowledge , there has been no attempt to fully explore the tradeoffs that can be exhibited for specific computational problems between the local and aggregate memory sizes , on one side , and the number of rounds , on the other , under reasonable constraints of the amount of total work performed by the algorithm .",
    "our results contribute to filling this gap .",
    "matrix multiplication is a building block for many problems , including matching  @xcite , matrix inversion  @xcite , all - pairs shortest path  @xcite , graph contraction  @xcite , cycle detection  @xcite , and parsing context free languages  @xcite .",
    "parallel algorithms for matrix multiplication of dense matrices have been widely studied : among others , we remind @xcite which provide upper and lower bounds exposing a tradeoff between communication complexity and processor memory . for sparse matrices ,",
    "interesting results are given in @xcite for some network topologies like hypercubes , in @xcite for pram , and in @xcite for a bsp - like model .",
    "in particular , techniques in @xcite are used in the following sections for deriving efficient mapreduce algorithms . in the sequential settings ,",
    "some interesting works providing upper and lower bounds are @xcite for dense matrix multiplication , and @xcite for sparse matrix multiplication .",
    "the contribution of this paper is twofold , since it targets both modeling and algorithmic issues .",
    "we first formally specify a computational model for mapreduce which captures the functional flavor of the paradigm by allowing a flexible use of parallelism .",
    "more specifically , our model generalizes the one proposed in @xcite by letting the local and aggregate memory sizes be two independent parameters , @xmath11 and @xmath12 , respectively .",
    "moreover our model makes no assumption on the underlying execution infrastructure , for instance it does not impose a bound on the number of available machines , thus fully decoupling the degree of parallelism exposed by a computation from the one of the machine where the computation will be eventually executed .",
    "this decoupling greatly simplifies algorithm design , which has been one of the original objectives of the mapreduce paradigm .",
    "( in section  [ sec : preliminary ] , we quantify the cost of implementing a round of our model on a system with fixed parallelism . )",
    "our algorithmic contributions concern the study of attainable tradeoffs in mapreduce for several variants of the fundamental primitive of matrix multiplication .",
    "in particular , building on the well - established three - dimensional algorithmic strategy for matrix multiplication @xcite , we develop upper and lower bounds for dense - dense matrix multiplication and provide similar bounds for deterministic and/or randomized algorithms for sparse - sparse and sparse - dense matrix multiplication .",
    "the algorithms are parametric in the local and aggregate memory constraints and achieve optimal or quasi - optimal round complexity in the entire range of variability of such parameters .",
    "finally , building on the matrix multiplication results , we derive similar space - round tradeoffs for matrix inversion and matching , which are important by - products of matrix multiplication .",
    "the rest of the paper is structured as follows . in section  [ sec : preliminary ] we introduce our computational model for mapreduce and describe important algorithmic primitives ( sorting and prefix sums ) that we use in our algorithms .",
    "section  [ sec : intromatrix ] deals with matrix multiplication in our model , presenting theoretical bounds to the complexity of algorithms to solve this problem .",
    "we apply these results in section  [ sec : applications ] to derive algorithms for matrix inversion and for matching in graphs .",
    "our model is defined in terms of two integral parameters @xmath12 and @xmath11 , whose meaning will be explained below , and is named .",
    "algorithms specified in this model will be referred to as _ mr - algorithms_. an mr - algorithm specifies a sequence of _ rounds _ : the @xmath13-th round , with @xmath14 transforms a multiset @xmath15 of key - value pairs into two multisets @xmath16 and @xmath17 of key - value pairs , where @xmath16 is the input of the next round ( empty , if @xmath13 is the last round ) , and @xmath17 is a ( possibly empty ) subset of the final output .",
    "the input of the algorithm is represented by @xmath18 while the output is represented by @xmath19 , with @xmath20 denoting the union of multisets .",
    "the universes of keys and values may vary at each round , and we let @xmath21 denote the universe of keys of @xmath15 . the computation performed by round @xmath13",
    "is defined by a _",
    "reducer _ function @xmath22 which is applied independently to each multiset @xmath23 consisting of all entries in @xmath15 with key @xmath24 .",
    "let @xmath2 be the input size .",
    "the two parameters @xmath12 and @xmath11 specify the memory requirements that each round of an mr - algorithm must satisfy . in particular ,",
    "let @xmath25 denote the space needed to compute @xmath26 on a ram with @xmath27-bit words , including the space taken by the input ( i.e. , latexmath:[$m_{r , k } \\geq    output , which contributes either to @xmath17 ( i.e. , the final output ) or to @xmath16 .",
    "the model imposes that @xmath29 , for every @xmath14 and @xmath24 , that @xmath30 , for every @xmath14 , and that @xmath31 . the complexity of an mr - algorithm is the number of rounds that it executes in the worst case , and it is expressed as a function of the input size @xmath2 and of parameters @xmath11 and @xmath12 . the dependency on the parameters @xmath11 and @xmath12 allows for a finer analysis of the cost of an mr - algorithm .",
    "as in @xcite , we require that each reducer function runs in time polynomial in @xmath2 .",
    "in fact , it can be easily seen that the model defined in @xcite is equivalent to the  model with @xmath32 and @xmath33 , for some fixed constant @xmath34 , except that we eliminate the additional restrictions that the number of rounds of an algorithm be polylogarithmic in @xmath2 and that the number of physical machines on which algorithms are executed are @xmath35 , which in our opinion should not be posed at the model level .",
    "compared to the model in @xcite , our  model introduces the parameter @xmath12 which limits the size of the aggregate memory required at each round , whereas in @xcite this size is virtually unbounded .",
    "moreover , the complexity analysis in  focuses on the tradeoffs between @xmath11 and @xmath12 , on one side , and the number of rounds on the other side , while in @xcite a more complex cost function is defined which accounts for the overall message complexity of each round , the time complexity of each reducer computation , and the latency and bandwidth characteristics of the executing platform .",
    "sorting and prefix sum primitives are used in the algorithms presented in this paper .",
    "the input to both primitives consists of a set of @xmath2 key - value pairs @xmath36 with @xmath37 and @xmath38 , where @xmath39 denotes a suitable set . for sorting ,",
    "a total order is defined over @xmath39 and the output is a set of @xmath2 key - value pairs @xmath40 , where the @xmath41 s form a permutation of the @xmath42 s and @xmath43 for each @xmath44 . for prefix sums , a binary associative operation @xmath45 is defined over @xmath39 and the output consists of a collection of @xmath2 pairs @xmath40 where @xmath46 , for @xmath47 .",
    "by straightforwardly adapting the results in @xcite to our model we have :    [ prefixsorting ] the sorting and prefix sum primitives for inputs of size @xmath2 can be implemented in  with round complexity @xmath48 for @xmath49 .",
    "we remark that the each reducer in the implementation of the sorting and prefix primitives makes use of @xmath50 memory words .",
    "hence , the same round complexity can be achieved in a more restrictive scenario with fixed parallelism . in fact",
    ", our  model can be simulated on a platform with @xmath51 processing elements , each with internal memory of size @xmath50 , at the additional cost of one prefix computation per round .",
    "therefore , @xmath52 can be regarded as an upper bound on the relative power of our model with respect to one with fixed parallelism .",
    "goodrich  @xcite claims that the round complexities stated in theorem  [ prefixsorting ] are optimal for any @xmath53 as a consequence of the lower bound for computing the or of @xmath2 bits on the bsp model  @xcite .",
    "it can be shown that the optimality carries through to our model where the output of a reducer is not bounded by @xmath11 .",
    "let @xmath54 and @xmath55 be two @xmath56 matrices and let @xmath57 .",
    "we use @xmath58 and @xmath59 , with @xmath60 , to denote the entries of @xmath61 and @xmath62 , respectively . in this section",
    "we present upper and lower bounds for computing the product @xmath62 in .",
    "the algorithms we present envision the matrices as conceptually divided into submatrices of size @xmath63 , and we denote these matrices with @xmath64 , @xmath65 and @xmath66 , respectively , for @xmath67 . clearly , @xmath68 .",
    "all our algorithms exploit the following partition of the @xmath69 products between submatrices ( e.g. , @xmath70 ) into @xmath71 _ groups _ : group @xmath72 , with @xmath73 , consists of products @xmath74 , for every @xmath67 and for @xmath75 .",
    "observe that each submatrix of @xmath54 and @xmath55 occurs exactly once in each group @xmath72 .",
    "we focus our attention on matrices whose entries belong to a semiring @xmath76 such that for any @xmath77 we have @xmath78 , where @xmath79 is the identity for @xmath45 . in this",
    "setting , efficient matrix multiplication techniques such as strassen s can not be employed .",
    "moreover , we assume that the inner products of any row of @xmath54 and of any column of @xmath55 with overlapping nonzero entries never cancel to zero , which is a reasonable assumption when computing over natural numbers or over real numbers with a finite numerical precision .    in our algorithms , any input matrix @xmath80 ( @xmath81 ) is provided as a set of key - value pairs @xmath82 for all elements @xmath83 .",
    "key @xmath84 represents a progressive index , e.g. , the number of nonzero entries preceding @xmath85 in the row - major scan of @xmath80 .",
    "we call a @xmath86 matrix _ dense _ if the number of its nonzero entries is @xmath87 , and we call it _ sparse _ otherwise . we suppose that @xmath12 is sufficiently large to contain the input and output matrices .",
    "in what follows , we present different algorithms tailored for the multiplication of dense - dense ( section  [ sec : ddmult ] ) , sparse - sparse ( section  [ sec : ssmult ] ) , and sparse - dense matrices ( section  [ sec : sdmult ] ) .",
    "we also derive lower bounds which demonstrate that our algorithms are either optimal or close to optimal ( section  [ sec : lb ] ) , and an algorithm for estimating the number of nonzero entries in the product of two sparse matrices ( section  [ sec : evaluation ] ) .      in this section",
    "we provide a simple , deterministic algorithm for multiplying two dense matrices , which will be proved optimal in subsection  [ sec : lb ] .",
    "the algorithm is a straightforward adaptation of the well - established three - dimensional algorithmic strategy for matrix multiplication of @xcite , however we describe a few details of its implementation in  since the strategy is also at the base of algorithms for sparse matrices .",
    "we may assume that @xmath88 , since otherwise matrix multiplication can be executed by a trivial sequential algorithm .",
    "we consider matrices @xmath54 and @xmath55 as decomposed into @xmath89 submatrices and subdivide the products between submatrices into groups as described above .    in each round",
    ", the algorithm computes all products within @xmath90 consecutive groups , namely , at round @xmath91 , all multiplications in @xmath72 are computed , with @xmath92 .",
    "the idea is that in a round all submatrices of @xmath54 and @xmath55 can be replicated @xmath93 times and paired in such a way that each reducer performs a distinct multiplication in @xmath94 .",
    "then , each reducer sums the newly computed product to a partial sum which accumulates all of the products contributing to the same submatrix of @xmath62 belonging to groups with the same index modulo @xmath93 dealt with in previous rounds . at the end of the @xmath95-th round",
    ", all submatrix products have been computed .",
    "the final matrix @xmath62 is then obtained by adding together the @xmath93 partial sums contributing to each entry of @xmath62 through a prefix computation .",
    "we have the following result .",
    "[ th : upddmult ] the above -algorithm multiplies two @xmath96 dense matrices in @xmath97 rounds .",
    "the algorithm clearly complies with the memory constraints of  since each reducer multiplies two @xmath98 submatrices and the degree of replication is such that the algorithm never exceeds the aggregate memory bound of @xmath12 . also , the @xmath69 products are computed in @xmath99 rounds , while the final prefix computation requires @xmath100 rounds    we remark that the multiplication of two @xmath96 dense matrices can be performed in a constant number of rounds whenever @xmath101 , for constant @xmath102 , and @xmath103 .",
    "consider two @xmath86 sparse matrices @xmath54 and @xmath55 and denote with @xmath104 the maximum number of nonzero entries in any of the two matrices , and with @xmath105 the number of nonzero entries in the product @xmath106 .",
    "below , we present two deterministic mr - algorithms ( d1 and d2 ) and a randomized one ( r1 ) , each of which turns out to be more efficient than the others for suitable ranges of parameters .",
    "we consider only the case @xmath107 , since otherwise matrix multiplication can be executed by a trivial one - round mr - algorithm using only one reducer .",
    "we also assume that the value @xmath108 is provided in input .",
    "( if this were not the case , such a value could be computed with a simple prefix computation in @xmath52 rounds , which does not affect the asymptotic complexity of our algorithms . )",
    "however , we do not assume that @xmath105 is known in advance since , unlike @xmath108 , this value can not be easily computed .",
    "in fact , the only source of randomization in algorithm r1 stems from the need to estimate @xmath105 .",
    "this algorithm is based on the following strategy adapted from @xcite . for @xmath109 ,",
    "let @xmath42 ( resp . ,",
    "@xmath41 ) be the number of nonzero entries in the @xmath110th column of @xmath54 ( resp .",
    ", @xmath110th row of @xmath55 ) , and let @xmath111 be the set containing all nonzero entries in the @xmath110th column of @xmath54 and in the @xmath110th row of @xmath55 .",
    "it is easily seen that all of the @xmath112 products between entries in @xmath111 ( one from @xmath54 and one from @xmath55 ) must be computed .",
    "the algorithm performs a sequence of _ phases _ as follows .",
    "suppose that at the beginning of phase  @xmath7 , with @xmath113 , all products between entries in @xmath111 , for each @xmath114 and for a suitable value @xmath13 ( initially , @xmath115 ) , have been computed and added to the appropriate entries of @xmath62 . through a prefix computation ,",
    "phase  @xmath7 computes the largest @xmath116 such that @xmath117 .",
    "then , all products between entries in @xmath118 , for every @xmath119 , are computed using one reducer with constant memory for each such product .",
    "the products are then added to the appropriate entries of @xmath62 using again a prefix computation .",
    "algorithm d1 multiplies two sparse @xmath96 matrices with at most @xmath108 nonzero entries each in @xmath120 rounds , on an .",
    "the correctness is trivial and the memory constraints imposed by the model are satisfied since in each phase at most @xmath12 elementary products are performed .",
    "the theorem follows by observing that the maximum number of elementary products is @xmath121 and that two consecutive phases compute at least @xmath12 elementary products in @xmath122 rounds .",
    "the algorithm exploits the same three - dimensional algorithmic strategy used in the dense - dense case and consists of a sequence of phases . in phase @xmath7 , @xmath113 ,",
    "all @xmath63-size products within @xmath116 consecutive groups are performed in parallel , where @xmath116 is a phase - specific value . observe that the computation of all products within a group @xmath72 requires space @xmath123 $ ] , since each submatrix of @xmath54 and @xmath55 occurs only once in @xmath72 and each submatrix product contributes to a distinct submatrix of @xmath62 .",
    "however , the value @xmath124 can be determined in @xmath125 space and @xmath52 rounds by `` simulating '' the execution of the products in @xmath72 ( without producing the output values ) and adding up the numbers of nonzero entries contributed by each product to the output matrix .",
    "the value @xmath116 is determined as follows .",
    "suppose that , at the beginning of phase @xmath7 , groups @xmath72 have been processed , for each @xmath126 and for a suitable value @xmath13 ( initially , @xmath115 ) .",
    "the algorithm replicates the input matrices @xmath127 times .",
    "subsequently , through sorting and prefix computations the algorithm computes @xmath128 for each @xmath129 and determines the largest @xmath130 such that @xmath131 .",
    "then , the actual products in @xmath132 , for each @xmath133 are executed and accumulated ( again using a prefix computation ) in the output matrix @xmath62 .",
    "we have the following theorem .",
    "algorithm @xmath134 multiplies two sparse @xmath96 matrices with at most @xmath108 nonzero entries each in @xmath135 rounds on an , where @xmath105 denotes the maximum number of nonzero entries in the output matrix .",
    "the correctness of the algorithm is trivial .",
    "phase @xmath7 requires a constant number of sorting and prefix computations to determine @xmath116 and to add the partial contributions to the output matrix @xmath62 .",
    "since each value @xmath124 is @xmath136 and the groups are @xmath71 , clearly , @xmath137 , and the theorem follows .",
    "we remark that the value @xmath105 appearing in the stated round complexity needs not be explicitly provided in input to the algorithm .",
    "we also observe that with respect to algorithm d1 , algorithm d2 features a better exploitation of the local memories available to the individual reducers , which compute @xmath89-size products rather than working at the granularity of the single entries .",
    "by suitably combining algorithms d1 and d2 , we can get the following result .",
    "[ d12round ] there is a deterministic algorithm which multiplies two sparse @xmath96 matrices with at most @xmath108 nonzero entries each in @xmath138 rounds on an , where @xmath105 denotes the maximum number of nonzero entries in the output matrix .",
    "algorithm d2 requires @xmath122 rounds in each phase @xmath7 for computing the number @xmath116 of groups to be processed .",
    "however , if @xmath105 were known , we could avoid the computation of @xmath116 and resort to the fixed-@xmath93 strategy adopted in the dense - dense case , by processing @xmath139 consecutive groups per round .",
    "this would yield an overall @xmath140 round complexity , where the @xmath141 additive term accounts for the complexity of summing up , at the end , the @xmath93 contributions to each entry of @xmath62 .",
    "however , @xmath105 may not be known a priori . in this case , using the strategy described in section  [ sec : evaluation ] we can compute a value @xmath142 which is a 1/2-approximation to @xmath105 with probability at least @xmath143 .",
    "( we say that @xmath142 @xmath144-approximates @xmath105 if @xmath145 . ) hence , in the algorithm we can plug in @xmath146 as an upper bound to @xmath105 . by using the result of theorem  [ otilde ] with @xmath147 and @xmath148",
    ", we have :    [ r1round ] let @xmath149 .",
    "algorithm r1 multiplies two sparse @xmath96 matrices with at most @xmath108 nonzero entries in @xmath150 rounds on an , with probability at least @xmath143 .    by comparing the rounds complexities stated in corollary  [ d12round ] and theorem  [ r1round ] ,",
    "it is easily seen that the randomized algorithm r1 outperforms the deterministic strategies when @xmath151 , for any constant @xmath3 , @xmath152 , and @xmath153 . for a concrete example , r1 exhibits better performance when @xmath154 , @xmath155 , and @xmath11 is polylogarithmic in @xmath12 .",
    "moreover , both the deterministic and randomized strategies can achieve a constant round complexity for suitable values of the memory parameters .",
    "observe that a @xmath156-approximation to @xmath105 derives from the following simple argument .",
    "let @xmath42 and @xmath41 be the number of nonzero entries in the @xmath110th column of @xmath54 and in the @xmath110th row of @xmath55 respectively , for each @xmath157 . then , @xmath158 .",
    "evaluating the sum requires @xmath159 sorting and prefix computations , hence a @xmath156-approximation of @xmath105 can be computed in @xmath160 rounds .",
    "however , such an approximation is too weak for our purposes and we show below how to achieve a tighter approximation by adapting a strategy born in the realm of streaming algorithms .",
    "let @xmath161 and @xmath162 be two arbitrary values .",
    "an @xmath144-approximation to @xmath105 can be derived by adapting the algorithm of  @xcite for counting distinct elements in a stream @xmath163 , whose entries are in the domain @xmath164=\\{0,\\ldots , n-1\\}$ ] .",
    "the algorithm of  @xcite makes use of a very compact data structure , customarily called _ sketch _ in the literature , which consists of @xmath165 lists , @xmath166 .",
    "for @xmath167 , @xmath168 contains the @xmath169 distinct smallest values of the set @xmath170 , where @xmath171\\rightarrow [ n^3]$ ] is a hash function picked from a pairwise independent family .",
    "it is shown in @xcite that the median of the values @xmath172 , where @xmath173 denotes the @xmath7th smallest value in @xmath168 , is an @xmath144-approximation to the number of distinct elements in the stream , with probability at least @xmath174 . in order to compute an @xmath144-approximation of @xmath105 for a product @xmath175 of @xmath176 matrices , we can modify the algorithm as follows .",
    "consider the stream of values in @xmath164 $ ] where each element of the stream corresponds to a distinct product @xmath177 and consists of the value @xmath178 .",
    "clearly , the number of distinct elements in this stream is exactly @xmath105 .",
    "( a similar approach has been used in @xcite in the realm of sparse boolean matrix products . )",
    "we now show how to implement this idea on an .",
    "the mr - algorithm is based on the crucial observation that if the stream of values defined above is partitioned into segments , the sketch for the entire stream can be obtained by combining the sketches computed for the individual segments . specifically , two sketches are combined by merging each pair of lists with the same index and selecting the @xmath7 smallest values in the merged list .",
    "the -algorithm consists of a number of phases , where each phase , except for the last one , produces set of @xmath179 sketches , while the last phase combines the last batch of @xmath179 sketches into the final sketch , and outputs the approximation to @xmath105 .",
    "we refer to the partition of the matrices into @xmath180 submatrices and group the products of submatrices as done before . in phase",
    "@xmath7 , with @xmath181 , the algorithm processes the products in @xmath182 consecutive groups , assigning each pair of submatrices in one of the @xmath93 groups to a distinct reducer .",
    "a reducer receiving @xmath183 and @xmath184 , each with at least a nonzero entry , either computes a sketch for the stream segment of the nonzero products between entries of @xmath183 and @xmath184 , if the total number of nonzero entries of @xmath183 and @xmath184 exceeds the size of the sketch , namely @xmath185 words , or otherwise leaves the two submatrices untouched ( observe that in neither case the actual product of the two submatrices is computed ) . in this latter case , we refer to the pair of ( very sparse ) submatrices as a _",
    "pseudosketch_. at this point , the sketches produced by the previous phase ( if @xmath186 ) , together with the sketches and pseudosketches produced in the current phase are randomly assigned to @xmath179 reducers .",
    "each of these reducers can now produce a single sketch from its assigned pseudosketches ( if any ) and merge it with all other sketches that were assigned to it . in the last phase ( @xmath187 ) the @xmath179 sketches are combined into the final one through a prefix computation , and the approximation to @xmath105",
    "is computed .",
    "[ otilde ] let @xmath188 and let @xmath189 and @xmath190 be arbitrary values .",
    "then , with probability at least @xmath191 , the above algorithm computes an @xmath144-approximation to @xmath105 in @xmath192 rounds , on an    the correctness of the algorithm follows from the results of @xcite and the above discussion .",
    "recall that the value computed by the algorithm is an @xmath144-approximation to @xmath105 with probability @xmath174 . as for the rounds complexity",
    "we observe that each phase , except for the last one , requires a constant number of rounds , while the last one involves a prefix computation thus requiring @xmath122 rounds .",
    "we only have to make sure that in each phase the memory constraints are satisfied ( with high probability ) .",
    "note also that a sketch of size @xmath193 is generated either in the presence of a pair of submatrices @xmath183 , @xmath184 containing at least @xmath194 entries , or within one of the @xmath179 reducers . by the choice of @xmath93 , it is easy to see that in any case , the overall memory occupied by the sketches is @xmath195 . as for the constraint on local memories , a simple modification of the standard balls - into - bins argument @xcite and the union bound suffices to show that with probability @xmath174 , in every phase when sketches and pseudosketches are assigned to @xmath179 reducers , each reducer receives in @xmath196 words .",
    "the theorem follows .",
    "( more details will be provided in the full version of the paper . )",
    "let @xmath54 be a sparse @xmath56 matrix with at most @xmath108 nonzero entries and let @xmath55 be a dense @xmath56 matrix ( the symmetric case , where @xmath54 is dense and @xmath55 sparse , is equivalent ) .",
    "the algorithm for dense - dense matrix multiplication does not exploit the sparsity of @xmath54 and requires @xmath197 rounds .",
    "also , if we simply plug @xmath198 in the complexities of the three algorithms for the sparse - sparse case ( where @xmath108 represented the maximum number of nonzero entries of @xmath54 or @xmath55 ) we do not achieve a better round complexity .",
    "however , a careful analysis of algorithm d1 in the sparse - dense case reveals that its round complexity is @xmath199 .",
    "therefore , by interleaving algorithm d1 and the dense - dense algorithm we have the following corollary .",
    "the multiplication on  of a sparse @xmath56 matrix with at most @xmath108 nonzero entries and of a dense @xmath56 matrix requires a number of rounds which is the minimum between @xmath200 and @xmath201 .",
    "observe that the above sparse - dense strategy outperforms all previous algorithms for instance when @xmath202 .",
    "in this section we provide lower bounds for dense - dense and sparse - sparse matrix multiplication .",
    "we restrict our attention to algorithms which perform all nonzero elementary products , that is , on _ conventional _ matrix multiplication @xcite .",
    "although this assumption limits the class of algorithms , ruling out strassen - like techniques , an elaboration of a result in  @xcite shows that computing all nonzero elementary products is necessary when entries of the input matrices are from the semirings @xmath203 and @xmath204 .",
    "semiring , where @xmath205 is the identity of the @xmath206 operation , is usually adopted while computing the shortest path matrix of a graph given its connection matrix .",
    "] indeed , we have the following lemma which provides a lower bound on the number of products required by an algorithm multiplying any two matrices of size @xmath56 , containing @xmath207 and @xmath208 nonzero entries and where zero entries have fixed positions ( a similar lemma holds for @xmath209 ) . as a consequence of the lemma , an algorithm that multiplies any two arbitrary matrices in the semiring",
    "@xmath203 must perform all nonzero products .",
    "consider an algorithm @xmath210 which multiples two @xmath56 matrices @xmath54 and @xmath55 with @xmath207 and @xmath208 nonzero entries , respectively , from the semiring @xmath211 and where the positions of zero entries are fixed .",
    "then , algorithm @xmath210 must perform all the nonzero elementary products .",
    "@xcite shows that each @xmath59 can be computed only by summing all terms @xmath212 , with @xmath213 , if the algorithm uses only semiring operations .",
    "the proof relies on the analysis of the output for some suitable input matrices , and makes some assumptions that force the algorithm to compute even zero products .",
    "however , the result still holds if we allow all the zero products to be ignored , but some adjustments are required . in particular , the input matrices used in @xcite do not work in our scenario because may contain less than @xmath207 and @xmath208 nonzero entries , however it is easy to find inputs with the same properties working in our case .",
    "more details will be provided in the full version .",
    "the following theorem exhibits a tradeoff in the lower bound between the amount of local and aggregate memory and the round complexity of an algorithm performing conventional matrix multiplication .",
    "the proof is similar to the one proposed in  @xcite for lower bounding the communication complexity of dense - dense matrix multiplication in a bsp - like model : however , differences arise since we focus on round complexity and our model does not assume the outdegree of a reducer to be bounded . in the proof of the theorem we use the following lemma which was proved using the red - blue pebbling game in  @xcite and then restated in  @xcite as follows .",
    "[ lem : nummult ] consider the conventional matrix multiplication @xmath214 , where @xmath54 and @xmath55 are two arbitrary matrices .",
    "a processor that uses @xmath215 elements of @xmath54 , @xmath216 elements of @xmath55 , and contributes to @xmath217 elements of @xmath62 can compute at most @xmath218 multiplication terms .",
    "[ th : lbddmult ] consider an -algorithm @xmath210 for multiplying two @xmath56 matrices containing at most @xmath207 and @xmath208 nonzero entries , using conventional matrix multiplications .",
    "let @xmath219 and @xmath105 denote the number of nonzero elementary products and the number of nonzero entries in the output matrix , respectively .",
    "then , the round complexity of @xmath210 is @xmath220    let @xmath210 be an @xmath221-round -algorithm computing @xmath214 .",
    "we prove that @xmath222 .",
    "consider the @xmath13-th round , with @xmath223 , and let @xmath224 be an arbitrary key in @xmath21 and @xmath225 .",
    "we denote with @xmath226 the space taken by the output of @xmath26 which contributes either to @xmath17 or to @xmath16 , and with @xmath25 the space needed to compute @xmath26 including the input and working space but excluding the output . clearly , @xmath227 , @xmath228 , and @xmath229 .",
    "suppose @xmath230 . by lemma  [ lem : nummult ]",
    ", the reducer @xmath22 with input @xmath231 can compute at most @xmath232 elementary products since @xmath233 and @xmath234 , where @xmath215 and @xmath216 denote the entries of @xmath54 and @xmath55 used in @xmath26 and @xmath217 the entries of @xmath62 for which contributions are computed by @xmath26 .",
    "then , the number of terms computed in the @xmath13-th round is at most @xmath235 since @xmath236 and the summation is maximized when @xmath237 for each @xmath238 .",
    "suppose now that @xmath239 .",
    "partition the keys in @xmath21 into @xmath240 sets @xmath241 such that @xmath242 for each @xmath243 ( the lower bound may be not satisfied for @xmath244 ) . clearly , @xmath245 . by lemma  [ lem : nummult ] ,",
    "the number of elementary products computed by all the reducers @xmath26 with keys in a set @xmath246 is at most @xmath247 . since @xmath248 for each non negative assignment of the @xmath249 variables and since @xmath250",
    ", it follows that at most @xmath251 elementary products can be computed using keys in @xmath246 , where @xmath252 .",
    "therefore , the number of elementary products computed in the @xmath13-th round is at most @xmath253 since @xmath254 and the sum is maximized when @xmath255 for each @xmath243 .",
    "therefore , in each round @xmath256 nonzero elementary products can be computed , and then @xmath257 . the second term of the lower bound follows since there is at least one entry of @xmath62 given by the sum of @xmath258 nonzero elementary products .",
    "we now specialize the above lower bound for algorithms for generic dense - dense and sparse - sparse matrix multiplication .",
    "an -algorithm for multiplying any two dense @xmath86 matrices , using conventional matrix multiplication , requires @xmath259 rounds . on the other hand ,",
    "an -algorithm for multiplying any two sparse matrices with at most @xmath108 nonzero entries requires @xmath260 rounds .    in the dense - dense case",
    "the lower bound follows by the above theorem  [ th : lbddmult ] since we have @xmath261 and @xmath262 when @xmath263 . in the sparse - sparse case ,",
    "we set @xmath264 and we observe that there exist assignments of the input matrices for which @xmath265 , and others where @xmath266    the deterministic algorithms for matrix multiplication provided in this section perform conventional matrix multiplication , and hence the above corollary applies .",
    "thus , the algorithm for dense - dense matrix multiplication described in section  [ sec : ddmult ] is optimal for any value of the parameters . on the other hand , the deterministic algorithm d2 for sparse - sparse matrix multiplication given in section  [ sec : dssmult ] is optimal as soon as @xmath267 , @xmath268 and @xmath11 is polynomial in @xmath12 .",
    "our matrix multiplications results can be used to derive efficient algorithms for inverting a square matrix and for solving several variants of the matching problem in a graph .",
    "the algorithms in this section make use of division and exponentiation . to avoid the intricacies of dealing with limited precision , we assume each memory word is able to store any value that occurs in the computation .",
    "a similar assumption is made in the presentation of algorithms for the same problems on other parallel models ( see e.g. @xcite ) .      in this section",
    "we study the problem of inverting a lower triangular matrix @xmath54 of size @xmath269 .",
    "we adopt the simple recursive algorithm which leverages on the easy formula for inverting a @xmath270 lower triangular matrix  ( * ? ? ?",
    "@xmath271^{-1 } = \\left [ \\begin{array}{cc } a^{-1 } & 0 \\\\ -c^{-1}ba^{-1 }   & c^{-1 } \\end{array } \\right].\\ ] ] for @xmath272 and @xmath273 , let @xmath274 be the @xmath275 submatrix resulting from the splitting of @xmath54 into submatrices of size @xmath276 . since equation   holds even when @xmath277 are matrices , we have that @xmath278 can be expressed as in equation   in figure  [ fig : prodmatrinv ] .",
    "note that @xmath279 .",
    "@xmath280 , 0\\leq i\\leq 2^k-1.\\ ] ]    the -algorithm for computing the inverse of @xmath54 works in @xmath281 phases .",
    "let @xmath282 for @xmath283 .",
    "in the first part of phase @xmath79 , the inverses of all the lower triangular submatrices @xmath284 , with @xmath285 , are computed in parallel .",
    "since each submatrix has size @xmath98 , each inverse can be computed sequentially within a single reducer . in the second part of phase @xmath79 , each product @xmath286 for @xmath287 ,",
    "is computed within a reducer .    in phase @xmath13 , with @xmath288 , each term @xmath289 for @xmath290 , is computed in parallel by performing two matrix multiplications using @xmath291 aggregate memory and local size @xmath11 .",
    "therefore , at the end of phase @xmath292 we have all the components of @xmath293 , i.e. , of @xmath294 .",
    "[ thm : trianmatrinv ] the above algorithm computes the inverse of a nonsingular lower triangular @xmath269 matrix @xmath54 in @xmath295 rounds on an .    the correctness of the algorithm follows from the correctness of   which in turns easily follows from the correctness of the formula to invert a lower triangular @xmath296 matrix . from the above discussion",
    "it easy to see that the memory requirements are all satisfied .",
    "we now analyze the round complexity of the algorithm . at phase @xmath13",
    "we have to compute @xmath297 products between matrices of size @xmath298 each product is computed in parallel by using @xmath299 aggregate memory and thus each phase @xmath13 requires @xmath300 rounds by using the algorithm described in section  [ sec : ddmult ] .",
    "the cost of the lower triangular matrix inversion algorithm is then @xmath301 which gives the bound stated in the theorem .",
    "if @xmath302 is @xmath303 and @xmath101 for some constant @xmath3 , the complexity reduces to @xmath27 rounds , which is a logarithmic factor better than what could be obtained by simulating the pram algorithm .",
    "it is also possible to compute @xmath294 using the closed formula derived by unrolling a blocked forward substitution . in general",
    ", the closed formula contains an exponential number of terms .",
    "there are nonetheless special cases of matrices for which a large number of terms in the sum are zero and only a polynomial number of terms is left .",
    "this is , for instance , the case for triangular band matrices .",
    "( note that the inverse of a triangular band matrix is triangular but not necessarily a triangular band matrix . )",
    "if the width of the band is @xmath304 , then we have a polynomial number of terms in the formula . in this case",
    "we can do matrix inversion in constant rounds for sufficiently large values of @xmath11 and @xmath12 .",
    "a complete discussion of this method will be presented in the full version of the paper .",
    "building on the inversion algorithm for triangular matrices presented in the previous subsection , and on the dense - dense matrix multiplication algorithm , in this section we develop an -algorithm to invert a general @xmath269 matrix @xmath54 .",
    "let the trace @xmath305 of @xmath54 be defined as @xmath306 , where @xmath307 denotes the entry of @xmath54 on the @xmath110-th row and @xmath110-th column .",
    "the algorithm is based on the following known strategy ( see e.g. , ( * ? ? ?",
    "8.8 ) ) .    1 .",
    "compute the powers @xmath308.[step : inv1 ] 2 .",
    "compute the traces @xmath309 , for @xmath310.[step : inv2 ] 3 .",
    "compute the coefficients @xmath311 of the characteristic polynomial of @xmath54 by solving a lower triangular system of @xmath156 linear equations involving the traces @xmath312 ( the system is shown below).[step : inv3 ] 4 .",
    "compute @xmath313[step : inv4 ]    we now provide more details on the mr implementation of above strategy .",
    "the algorithm requires @xmath314 , which ensures that enough aggregate memory is available to store all the @xmath156 powers of @xmath54 . in step  [ step : inv1 ]",
    ", the algorithm computes naively the powers in the form @xmath315 , @xmath316 , by performing a sequence of @xmath317 matrix multiplications using the algorithm in section  [ sec : ddmult ] .",
    "then , each one of the remaining powers is computed using @xmath318 aggregate memory and by performing a sequence of at most @xmath319 multiplications of the matrices @xmath315 obtained earlier . in step  [",
    "step : inv2 ] , the @xmath156 traces @xmath312 are computed in parallel using a prefix like computation , while the coefficients @xmath311 of the characteristic polynomial are computed in step  [ step : inv3 ] by solving the following lower triangular system : @xmath320 \\left [ \\begin{array}{c }    c_{n-1 } \\\\",
    "c_{n-1 } \\\\",
    "c_{n-3 } \\\\    \\vdots",
    "\\\\    c_0 \\end{array } \\right ] = - \\left [ \\begin{array}{c }    s_1 \\\\    s_2 \\\\    s_3 \\\\",
    "\\vdots \\\\    s_n \\end{array } \\right].\\ ] ] if we denote with @xmath321 the matrix on the left hand side , with @xmath62 the vector of unknowns , and with @xmath39 the vector of the traces on the right hand side , we have @xmath322 . in order to compute the coefficients in @xmath62 the algorithm inverts the @xmath269 lower triangular matrix @xmath321 as described in section  [ sec : lowertrianinv ] , and computes the product between @xmath323 and @xmath39 , to obtain @xmath62 . finally , step  [ step : inv4 ] requires a prefix like computation .",
    "we have the following theorem .",
    "[ thm : genmatrinv ] the above algorithm computes the inverse of any nonsingular @xmath269 matrix @xmath54 in @xmath324 rounds on , with @xmath314 .    for the correctness of the algorithm see ( * ? ? ?",
    "it is easy to check that the memory requirements of the  model are satisfied .",
    "we focus here on analyzing the round complexity .",
    "computing the powers in the form @xmath315 , @xmath316 requires @xmath325 rounds , since the algorithm performs a sequence of @xmath317 products .",
    "the remaining powers are computed in @xmath326 rounds since each power is computed by performing at most @xmath317 product using @xmath327 aggregate memory . the prefix like computation for finding the @xmath156 traces @xmath312 requires @xmath52 rounds , while the linear system takes @xmath328 rounds .",
    "the final step takes @xmath52 rounds using a prefix like computation .",
    "the round complexity in the statement follows .",
    "if @xmath302 is @xmath329 and @xmath101 for some constant @xmath3 , the complexity reduces to @xmath27 rounds , which is a quadratic logarithmic factor better than what could be obtained by simulating the pram algorithm .",
    "the above algorithm for computing the inverse of any nonegative matrix requires @xmath314 . in this section",
    "we provide an -algorithm providing a strong approximation of @xmath294 assuming @xmath53 .",
    "a matrix @xmath55 is a",
    "_ strong approximation _ of the inverse of an @xmath269 matrix @xmath54 if @xmath330 , for some constant @xmath331 .",
    "the norm @xmath332 of a matrix @xmath54 is defined as @xmath333 where @xmath334 denotes the euclidean norm of a vector .",
    "the condition number @xmath335 of a matrix @xmath54 is defined as @xmath336 .",
    "an iterative method to compute a strong approximation of the inverse of a @xmath269 matrix @xmath54 is proposed in  ( * ? ? ?",
    "the method works as follows .",
    "let @xmath337 be a @xmath269 matrix satisfying the condition @xmath338 for some @xmath339 and where @xmath340 is the @xmath269 identity matrix .",
    "for a @xmath269 matrix @xmath62 let @xmath341 .",
    "we define @xmath342 , for @xmath343 .",
    "we have @xmath344 by setting @xmath345 where @xmath346 , we have @xmath347  @xcite . then ,",
    "if @xmath348 for some constant @xmath349 , @xmath350 provides a strong approximation when @xmath351 . from the above discussion , it is easy to derive an efficient -algorithm to compute a strong approximation of the inverse of a matrix using the algorithm for dense matrix multiplication in section  [ sec : ddmult ] .",
    "the above algorithm provides a strong approximation of the inverse of any nonegative @xmath352 matrix @xmath54 in @xmath353 rounds on an  when @xmath348 for some constant @xmath349 .",
    "the correctness of the algorithm derives from  @xcite .",
    "once again we only focus on the round complexity of the algorithm .",
    "computing @xmath354 requires a a constant number of prefix like computations , and hence takes @xmath52 rounds . to compute @xmath350 , @xmath343 from @xmath355 ,",
    "we need the value @xmath356 which involves a multiplication between two @xmath269 matrices and a subtraction between two matrices .",
    "hence , each phase requires @xmath357 rounds .",
    "since the algorithm terminates when @xmath358 , the theorem follows .",
    "a strategy for computing , with probability at least 1/2 , a perfect matching of a general graph using matrix inversion is presented in  @xcite .",
    "the strategy is the following :    1 .",
    "let the input of the algorithm be the adjacency matrix @xmath54 of a graph @xmath359 with @xmath156 vertices and @xmath224 edges .",
    "[ step : match1 ] 2 .",
    "let @xmath55 be the matrix obtained from @xmath54 by substituting the entries @xmath360 corresponding to edges in the graph with the integers @xmath361 and @xmath362 respectively , for @xmath363 , where @xmath364 is an integer chosen independently and uniformly at random from @xmath365 $ ] .",
    "we denote the entry on the @xmath110th row and @xmath366th column of @xmath55 as @xmath367.[step : match2 ] 3 .",
    "compute the determinant @xmath368 of @xmath55 and the greatest integer @xmath369 such that @xmath370 divides @xmath368.[step : match3 ] 4 .",
    "compute @xmath371 , the adjugate matrix of @xmath55 , and denote the entry on the @xmath110th row and @xmath366th column as @xmath372.[step : match4 ] 5 .   for each edge",
    "@xmath373 , compute @xmath374 if @xmath375 is odd , then add the edge @xmath376 to the matching.[step : match5 ]    an -algorithm for perfect matching easily follows by the above strategy .",
    "we now provide more details on the mr implementation which assumes @xmath314 .    in step  [ step : match2 ] ,",
    "@xmath55 is obtained as follows .",
    "the algorithm partitions @xmath54 into square @xmath98 submatrices @xmath377 , @xmath378 , and then assigns each pair of submatrices @xmath379 to a different reducer .",
    "this assignment ensures that each pair of entries @xmath380 of @xmath54 is sent to the same reducer .",
    "consider now the reducer receiving the pair of submatrices @xmath381 and consider the set of pairs @xmath380 of @xmath54 such that @xmath360 , where @xmath382 , @xmath383 , and @xmath384 .",
    "for each of these pairs the reducer chooses a @xmath364 independently and uniformly at random from @xmath365 $ ] , and sets @xmath367 to @xmath361 and @xmath385 to @xmath362 .",
    "for all the other entries @xmath386 , the reducer sets @xmath387 .",
    "let @xmath388 , @xmath389 be the coefficients of the characteristic polynomial of @xmath55 , which can be computed as described in section  [ sec : genmatrinv ] .",
    "steps  [ step : match3 ] and  [ step : match4 ] can be easily implemented since the determinant of @xmath55 is @xmath390 and @xmath391    finally , in step  [ step : match5 ] , matrices @xmath55 and @xmath371 are partitioned in square submatrices of size @xmath98 , and corresponding submatrices assigned to the same reducer , which computes the values @xmath375 for the entries in its submatrices and outputs the edges belonging to the matching .",
    "the above algorithm computes , with probability at least 1/2 , a perfect matching of the vertices of a graph @xmath392 , in @xmath324 rounds on , where @xmath314 .",
    "the correctness of the algorithm follows from the correctness of  @xcite and it is easy to see that the memory requirements of the  model are satisfied .",
    "we focus here on the round complexity . from the above description , it is easy to see that the computation of @xmath55 and the @xmath364 s in step  [ step : match2 ] only takes one round .",
    "steps  [ step : match3 ] and  [ step : match4 ] require the computation of the coefficients of the characteristic polynomial of @xmath55 , and so takes a number of rounds equal to the algorithm for matrix inversion described in section  [ sec : genmatrinv ] , i.e. , @xmath393 .",
    "step  [ step : match5 ] takes one round . since the round complexity is dominated by the number of rounds needed to compute the coefficients of the characteristic polynomial of @xmath55 , the theorem follows .",
    "we note that matching is as easy as matrix inversion in the  model",
    ". the above result can be extend to minimum weight perfect matching , to maximum matching , and to other variants of matching in the same way as in  ( * ? ? ?",
    "* sect .  5 ) .",
    "in this paper , we provided a formal computational model for the mapreduce paradigm which is parametric in the local and aggregate memory sizes and retains the functional flavor originally intended for the paradigm , since it does not require algorithms to explicitly specify a processor allocation for the reduce instances .",
    "performance in the model is represented by the round complexity , which is consistent with the idea that when processing large data sets the dominant cost is the reshuffling of the data .",
    "the two memory parameters featured by the model allow the algorithm designer to explore a wide spectrum of tradeoffs between round complexity and memory availability . in the paper , we covered interesting such tradeoffs for the fundamental problem of matrix multiplication and some of its applications .",
    "the study of similar tradeoffs for other important applications ( e.g. , graph problems ) constitutes an interesting open problem .",
    "the work of pietracaprina , pucci and silvestri was supported , in part , by miur of italy under project algodeep , and by the university of padova under the strategic project stpd08ja32 and project cpda099949/09 .",
    "the work of riondato and upfal was supported , in part , by nsf award iis-0905553 and by the university of padova through the visiting scientist 2010/2011 grant ."
  ],
  "abstract_text": [
    "<S> this work explores fundamental modeling and algorithmic issues arising in the well - established mapreduce framework . </S>",
    "<S> first , we formally specify a computational model for mapreduce which captures the functional flavor of the paradigm by allowing for a flexible use of parallelism . </S>",
    "<S> indeed , the model diverges from a traditional processor - centric view by featuring parameters which embody only global and local memory constraints , thus favoring a more data - centric view . </S>",
    "<S> second , we apply the model to the fundamental computation task of matrix multiplication presenting upper and lower bounds for both dense and sparse matrix multiplication , which highlight interesting tradeoffs between space and round complexity . finally , building on the matrix multiplication results , we derive further space - round tradeoffs on matrix inversion and matching .    algorithms for distributed computing ; algorithms for high performance computing ; parallel algorithms ; parallel complexity theory . </S>"
  ]
}