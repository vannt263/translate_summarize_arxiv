{
  "article_text": [
    "in the literature of adaptive filtering algorithms @xcite , numerous algorithms with different trade - offs between performance and complexity have been reported in the last decades .",
    "a designer can choose from the simple and low - complexity least - mean squares ( lms ) algorithms to the fast converging though complex recursive least squares ( rls ) techniques @xcite .",
    "a great deal of research has been devoted to developing cost - effective adaptive filters with an attractive trade - off between performance and complexity and automatic tuning of key parameters @xcite .",
    "combination schemes @xcite-@xcite are recent and effective design approaches , where several filters are mixed in order to obtain an overall quality improvement .",
    "the individual filters are set up so as to optimize different desirable properties : fast tracking or low error in steady - state , for example .",
    "the combined filter is able to keep the advantages of all the individual filters , achieving superior performance despite a higher computational cost than the single filter approach .",
    "the computational complexity required by combination schemes is due to the use of two or more adaptive filters in parallel and can become unacceptably high with large filters @xcite-@xcite .",
    "reduced - rank adaptive filters @xcite-@xcite are cost - effective techniques when dealing with problems involving large filters and reduced training . a number of reduced - rank adaptive filtering methods have been proposed in the last several years @xcite-@xcite . among them are eigen - decomposition - based techniques @xcite , the multistage wiener filter ( mswf ) @xcite , the auxiliary vector filtering ( avf ) algorithm @xcite , the interpolated reduced - rank filters @xcite , the reduced - rank filters based on joint and iterative optimization ( jio ) @xcite and joint iterative interpolation , decimation , and filtering ( jidf ) @xcite .",
    "key problems with previously reported reduced - rank adaptive filters are the tuning of the step sizes and/or the forgetting factors , and the model - order selection @xcite-@xcite .    in this paper , we propose schemes for joint model - order and step - size adaptation of reduced - rank adaptive filters based on the jidf technique @xcite which address these drawbacks of the jidf scheme .",
    "the proposed schemes employ reduced - rank adaptive filters in parallel operating with different orders and step sizes , which are exploited by convex combination strategies .",
    "the main reason for choosing the jidf technique @xcite is that it allows a substantial reduction in the number of coefficients for adaptation and yields the best performance among the techniques reported so far . although the number of coefficients that are actually adapted is smaller , leading to better convergence rates and mean - square error , the complexity of the jidf is comparable to that of the full - rank lms .",
    "other reduced - rank schemes have in general a higher complexity .",
    "we derive least - mean squares ( lms ) reduced - rank filters based on the jidf method and propose strategies to automatically adjust the model - order and step sizes used . without an interpolator , the jidf approach may also address the main drawback of combination schemes , i.e. , the increase in the number of elements for computation , but this possibility will be pursued elsewhere .",
    "this paper is organized as follows .",
    "section ii presents the proposed convex combination schemes of reduced - rank adaptive filters .",
    "section iii is devoted to the derivation of the convex combiners and section iv to the derivation of lms algorithms with the jidf approach .",
    "section v presents and discusses the simulation results and section vi draws the conclusions of this work .",
    "in this section , we detail the proposed convex combination schemes of reduced - rank adaptive filters . the basic idea behind these schemes is to employ a number of parallel sets of transformation matrices and reduced - rank filters that are jointly optimized and exploit them via the setting of different model orders and step sizes .",
    "the different model orders and step sizes are determined _ a priori_. since we are interested in this work in adapting the model order and the step size , then in principle we need at most @xmath0 parallel structures for setting upper and lower values for the model order ( or rank ) and the step sizes . to this end , we can build a tree structure where only two structures are combined at each stage .",
    "a block diagram of the first tree structured scheme , denoted scheme a , is shown in fig .",
    "[ fig1 ] .",
    "let us now mathematically describe the signal processing performed by the proposed scheme a. consider an @xmath1 input data vector @xmath2 $ ] that is processed by reduced - rank schemes in parallel with different parameters , namely the model order and the step size . the output of scheme a is given by @xmath3 & = \\lambda_c[i ] y_a[i ] + ( 1-\\lambda_c[i ] ) y_b[i ] \\\\ & = \\lambda_c[i ] \\bigl(\\lambda_a[i ] y_1[i ] + ( 1-\\lambda_a[i ] ) y_2[i]\\bigr ) \\\\",
    "& \\quad + ( 1-\\lambda_c[i ] ) \\bigl(\\lambda_b[i ] y_3[i ] + ( 1-\\lambda_b[i ] ) y_4[i]\\bigr)\\\\ & = \\lambda_c[i ] \\bigl(\\lambda_a[i ] \\bar{\\boldsymbol w}_1^h[i ] { \\boldsymbol s}_{d_1}^h[i ] { \\boldsymbol r}[i ] \\\\ & \\quad + ( 1-\\lambda_a[i ] ) \\bar{\\boldsymbol w}_2^h[i ] { \\boldsymbol s}_{d_2}^h[i ] { \\boldsymbol r}[i]\\bigr ) \\\\ & \\quad + ( 1-\\lambda_c[i ] ) \\bigl(\\lambda_b[i ] \\bar{\\boldsymbol w}_3^h[i ] { \\boldsymbol s}_{d_3}^h[i ] { \\boldsymbol r}[i ] \\\\ & \\quad + ( 1-\\lambda_b[i ] ) \\bar{\\boldsymbol w}_4^h[i ] { \\boldsymbol s}_{d_4}^h[i ] { \\boldsymbol r}[i]\\bigr ) \\\\ & = { \\boldsymbol w}_{\\rm eq}^{a,~h}[i ] { \\boldsymbol r}[i ] , \\label{output } \\end{split}\\ ] ] where the equivalent filter @xmath4 $ ] is given by @xmath5 & = \\lambda_c[i ] \\lambda_a[i ] { \\boldsymbol s}_{d_1}[i ] \\bar{\\boldsymbol w}_1[i ] \\\\ & \\quad + \\lambda_c[i ] ( 1-\\lambda_a[i ] ) { \\boldsymbol s}_{d_2}[i ] \\bar{\\boldsymbol w}_2[i ] \\\\ & \\quad + ( 1-\\lambda_c[i ] ) \\lambda_b[i ] { \\boldsymbol s}_{d_3}[i ] \\bar{\\boldsymbol w}_3[i ] \\\\ & \\quad + ( 1-\\lambda_c[i ] ) ( 1-\\lambda_b[i]){\\boldsymbol s}_{d_4}[i ] \\bar{\\boldsymbol w}_4[i ] \\end{split}\\ ] ] the main strategy is to set the constituent reduced - rank filters with an estimate of the lowest and highest ranks @xmath6 and @xmath7 , respectively , and the smallest and largest step sizes @xmath8 and @xmath9 , respectively .",
    "therefore , the proposed convex combination would be able to exploit the differences in rank and step size of the constituent reduced - rank filters and keep their advantages .",
    "now let us consider a second scheme , denoted scheme b , with the convex combination of only two structures and shown in fig .",
    "the output of scheme b is given by @xmath3 & = \\lambda_c[i ] y_1[i ] + ( 1-\\lambda_c[i ] ) y_2[i ] ) \\\\   & = \\lambda_c[i ] \\bar{\\boldsymbol w}_1^h[i ] { \\boldsymbol s}_{d_1}^h[i ] { \\boldsymbol r}[i ] + ( 1-\\lambda_c[i ] ) \\bar{\\boldsymbol w}_2^h[i ] { \\boldsymbol s}_{d_2}^h[i ] { \\boldsymbol r}[i ] \\\\ & = { \\boldsymbol w}_{\\rm eq}^{b,~ h}[i ] { \\boldsymbol r}[i ] , \\label{output2 } \\end{split}\\ ] ] where the equivalent filter @xmath10 $ ] is given by @xmath11 & = \\lambda_c[i ] { \\boldsymbol s}_{d_1}[i ] \\bar{\\boldsymbol w}_1[i ] + ( 1-\\lambda_c[i ] ) { \\boldsymbol s}_{d_2}[i ] \\bar{\\boldsymbol w}_2[i ] \\end{split}\\ ] ] the strategy for the scheme b is to set one of the constituent reduced - rank filters with an estimate of the lowest rank @xmath12 together with the largest step size @xmath9 , whereas the other uses the highest rank @xmath7 and the smallest step size @xmath8 , respectively .",
    "therefore , the proposed convex combination would be able to exploit fast adaptation ( @xmath6 and @xmath9 ) with low misadjustment ( @xmath7 and @xmath8 ) .",
    "we detail the jidf scheme @xcite used as the constituent of the proposed schemes .",
    "let us now review the jidf and its main parameters for the @xmath13th branch , where @xmath14 and @xmath15 .",
    "a block diagram of the jidf scheme is shown in fig . [ fig3 ] , where an interpolator @xmath16 $ ] with @xmath17 coefficients , a decimation unit and a reduced - rank filter @xmath18 $ ] with @xmath19 coefficients that are time - varying are employed .",
    "the @xmath1 input vector @xmath2 $ ] is filtered by the @xmath16 $ ] and yields the interpolated vector @xmath20 $ ] with @xmath21 samples expressed by @xmath22 = { \\boldsymbol v}^{h}_j[i ] { \\boldsymbol r}[i],\\ ] ] where the @xmath23 toeplitz convolution matrix @xmath24 $ ] is given by @xmath25 =   \\left[\\begin{array}{c c c c c c c c   } v_{j,0}^{[i ] }   & 0 & \\ldots & 0 &   \\\\ \\vdots & v_{j,0}^{[i ] } & \\ldots & 0 &   \\\\",
    "v_{j,{i}_{j}-1}^{[i ] } & \\vdots & \\ldots & 0 &   \\\\ 0 & v_{j,{i}_{j}-1}^{[i ] }   & \\ldots & 0    \\\\ 0   &    0 & \\ddots & 0   \\\\ \\vdots &    \\vdots & \\ddots & \\vdots   \\\\ 0 & 0 &   \\ldots & v_{j,0}^{[i ] }",
    "\\\\   \\end{array}\\right ] .",
    "\\nonumber\\ ] ]    in order to facilitate the description of the scheme , let us introduce an alternative way of expressing the vector @xmath20 $ ] , that will be useful in the following through the equivalence : @xmath22={\\boldsymbol v}^{h}_j[i]{\\boldsymbol r}[i ] = \\boldsymbol { \\re}_{o_{j}}[i]{\\boldsymbol v}^{*}_j[i],\\ ] ] where the @xmath26 matrix @xmath27 $ ] with the samples of @xmath2 $ ] has a hankel structure described by @xmath28 = \\left[\\begin{array}{c c c c c } r_{0}^{[i ] } & r_{1}^{[i ] }    & \\ldots & r_{{i}_j-1}^{[i ] }   \\\\ r_{1}^{[i ] }   & r_{2}^{[i ] }    & \\ldots & r_{{i}_j}^{[i ] }   \\\\ \\vdots & \\vdots   & \\ddots & \\vdots \\\\ r_{m-1}^{[i ] }   & r_{m}^{[i ] }   & \\ldots & r_{m + { i}_j-2}^{[i ] }   \\\\",
    "\\end{array}\\right].\\ ] ] the dimensionality reduction is performed by a decimation unit with @xmath29 decimation matrices @xmath30 $ ] that project @xmath20 $ ] onto @xmath31 vectors @xmath32 $ ] with @xmath33 , where @xmath34 is the rank and @xmath35 is the decimation factor . the @xmath31 vector @xmath32 $ ] for branch @xmath36 is expressed by @xmath37 = { \\boldsymbol d}_{b_{j}}[i ] { { \\boldsymbol r}_{{\\rm i},j}[i ] } = { \\boldsymbol d}_{b_{j}}[i ] \\boldsymbol { \\re}_{o_{j}}[i]{\\boldsymbol v}^{*}_j[i],\\ ] ] where the vector @xmath32 $ ] for branch @xmath38 is used in the instantaneous minimization of the squared norm of the error for branch @xmath38 @xmath39 = d[i ] - \\bar{\\boldsymbol w}^{h}_j[i]\\bar{\\boldsymbol r}_{b_{j}}[i ] .",
    "\\nonumber\\ ] ] the decimation pattern @xmath30 $ ] is selected according to : @xmath40 = { \\boldsymbol d}_{b_{s , j}}[i ] ~~ \\textrm{when } ~~ b_{s , j } = \\arg \\min_{1\\leq b_j \\leq b } |e_{b_{j}}[i]|^{2},\\ ] ] where @xmath41 is the number of decimation branches , which is a parameter to be set by the designer .",
    "we denote @xmath42\\leftarrow\\bar{\\boldsymbol r}_{b_{\\text{opt},j } }   [ i]$ ] . after the decimation unit , which carries out dimensionality reduction , the jidf scheme employs a reduced - rank fir filter @xmath43 $ ] with @xmath19 elements to yield the output of the scheme .",
    "a key strategy for the joint and iterative optimization that follows is to express the output of the jidf structure @xmath44 = \\bar{\\boldsymbol w}^{h}_j[i]\\bar{\\boldsymbol r}_{j}[i]$ ] as a function of @xmath16 $ ] , the decimation matrix @xmath45 $ ] and @xmath43 $ ] as follows : @xmath46 & = \\bar{\\boldsymbol w}^{h}_j[i ] { \\boldsymbol s}_{d_j}^h[i]{\\boldsymbol r}[i ] = \\bar{\\boldsymbol w}^{h}_j[i ] { \\boldsymbol d}_{b_{j}}[i]\\boldsymbol{\\re}_{o_{j}}[i ] { \\boldsymbol v}^{*}_j[i ] = { \\boldsymbol v}^{h}_j[i]{\\boldsymbol u}_j[i ] \\end{split},\\ ] ] where @xmath47= \\boldsymbol{\\re}_{o_j}^t[i]{\\boldsymbol d}^t_{b_j}[i ] \\bar{\\boldsymbol w}^{*}_j[i]$ ] is an @xmath48 vector .",
    "the expression in ( 10 ) indicates that the dimensionality reduction carried out by the proposed scheme depends on finding appropriate @xmath16 $ ] , @xmath45 $ ] for constructing @xmath49 $ ] . in the next section",
    ", we will develop adaptive algorithms for adjusting the coefficients of @xmath16 $ ] and @xmath43 $ ] for determining the best @xmath45 $ ] iteratively .",
    "in this section , we develop adaptive lms algorithms for the proposed convex combination scheme .",
    "the key feature of the proposed algorithms is the joint and iterative optimization of the filters , the decimation unit and the convex combiners .",
    "the algorithms can be derived by minimizing the cost function @xmath50 , d_{b_j}[i],\\bar{\\boldsymbol w}_j[i ] , \\lambda_u[i ] ) = e[| d[i ] - y_c[i]|^2 , \\label{cost } \\end{split}\\ ] ] where @xmath16 $ ] , @xmath51 $ ] and @xmath52 $ ] are the interpolators , the decimators and the reduced - rank filters of the @xmath13th constituent filtering scheme , and @xmath53 $ ] where @xmath54 are the generic combiners .",
    "the mixing parameters @xmath53 $ ] in the tree structure of the schemes depicted in figs .",
    "[ fig1 ] and [ fig2 ] are updated via auxiliary variables @xmath55 $ ] ( @xmath55=a[i ] , b[i ] , c[i]$ ] ) and a sigmoid function , as in @xmath53 = \\frac{1}{1+e^{-u[i]}}$ ] .",
    "we derive next the expressions for scheme a. the expressions for scheme b can be derived in a similar way . substituting the output of scheme a into the cost function , minimizing the cost function with respect to @xmath51 $ ] and computing the instantaneous gradients of the cost function with respect to @xmath16 $ ] , @xmath43 $ ] , we get the following jidf recursions for @xmath56 @xmath57|^2,\\ ] ] where the error signal used in the decimation unit is @xmath58 = d[i ] - \\bar{\\boldsymbol w}_j^h[i ] { \\boldsymbol d}_{b_j}[i ] { \\boldsymbol \\re}_{o_{j}}[i ] { \\boldsymbol v}_j^*[i]$ ] . after the selection of the best branch ,",
    "the error signal becomes @xmath59 \\leftarrow   e_{b_{{\\rm opt},j}}[i]$ ] .",
    "the recursions for @xmath16 $ ] and @xmath43 $ ] are @xmath60 = { \\boldsymbol v}_j[i ] + \\eta_j e_j^*[i ] { \\boldsymbol u}_j[i],\\ ] ] @xmath61 = { \\boldsymbol w}_j[i ] + \\mu_j e_j^*[i ] { \\boldsymbol r}_j[i],\\ ] ] where the @xmath62 vector @xmath47 = { \\boldsymbol \\re}_{o_j}^t[i ] { \\boldsymbol d}_{b_{{\\rm opt } , j}}^t[i ] \\bar{\\boldsymbol w}_j[i]$ ] is the regressor for the update recursion of the interpolator @xmath16 $ ] and the @xmath63 vector @xmath64 { \\boldsymbol \\re}_{o_j}[i ] { \\boldsymbol v}_j^*[i]$ ] is the regressor for the update equation of @xmath52 $ ] and the error signal is @xmath59 = d[i ] - \\bar{\\boldsymbol w}_j^h[i ] { \\boldsymbol d}_{b_{{\\rm opt } , j}}[i ] { \\boldsymbol \\re}_{o_j}[i ] { \\boldsymbol v}_j^*[i]$ ] .",
    "now , we need to derive the recursions for the convex combiners @xmath53 $ ] for @xmath65 .",
    "computing the gradient of the cost function in ( [ cost ] ) with respect to @xmath55 $ ] for @xmath66 we obtain the following recursion @xmath67 & = a[i ] - \\mu_a \\frac{\\partial { \\mathcal c}({\\boldsymbol v}_j[i ] , d_{b_j}[i],\\bar{\\boldsymbol w}_j[i ] , \\lambda_u[i])}{\\partial a[i ] } \\\\ & = a[i ] - \\mu_a \\frac{\\partial { \\mathcal c}({\\boldsymbol v}_j[i ] , d_{b_j}[i],\\bar{\\boldsymbol w}_j[i ] , \\lambda_u[i])}{\\partial \\lambda_a[i ] } \\frac{\\partial \\lambda_a[i]}{\\partial a[i ] } \\\\ & = a[i ] + \\mu_a ( y_1[i ] - y_2[i])^ * \\lambda_a[i ] ( 1- \\lambda_a[i ] ) e_a[i ] , \\end{split}\\ ] ] where the error signal for this combiner is @xmath68 = d[i ] - y_a[i]$ ] and the combiner is @xmath69 = \\frac{1}{1-e^{-a[i]}}$ ] . following this approach ,",
    "we can obtain the combiner for @xmath70 : @xmath71 & = b[i ] + \\mu_b   ( y_3[i ] - y_4[i])^ * \\lambda_b[i ] ( 1- \\lambda_b[i ] ) e_b[i ] , \\end{split}\\ ] ] where the error signal for this combiner is @xmath72 = d[i ] - y_b[i]$ ] and the combiner is @xmath73 = \\frac{1}{1-e^{-b[i]}}$ ] .",
    "the recursion for the last combiner in the tree structure is @xmath74 & = c[i ] + \\mu_c ( y_a[i ] - y_b[i])^ * \\lambda_c[i ] ( 1- \\lambda_c[i ] ) e_c[i ] , \\end{split}\\ ] ] where the error signal for this combiner is @xmath75 = d[i ] - y_c[i]$ ] and the combiner is @xmath76 = \\frac{1}{1-e^{-c[i]}}$ ] .",
    "the complexity of the existing algorithms is @xmath77 additions and @xmath78 multiplications for the full - rank lms algorithm , and @xmath79 additions and @xmath80 multiplications for the full - rank lms algorithms with convex combination ( full - rank - clms ) . for the jidf scheme",
    "@xmath81 additions and @xmath82 multiplications are required , where @xmath83 and @xmath84 are the lengths of the interpolator and the reduced - rank filter .",
    "the complexity of the proposed schemes a and b with the jidf is @xmath85 $ ] additions and @xmath86 $ ] multiplications .",
    "we can reduce the complexity of the jidf by setting small values for @xmath83 , @xmath84 and @xmath41 . however , the key advantage is a substantial reduction in the number of coefficients that need to be estimated , from @xmath21 to @xmath87 , where @xmath88 .",
    "this allows a much faster adaptation rate and smaller excess mean - square error , compared to full - rank schemes .",
    "we assess the performance of the proposed and existing schemes for interference suppression in cdma systems .",
    "we compare the full - rank scheme with the convex combination scheme of @xcite , the jidf @xcite , the jidf with the proposed schemes a and b , and the optimal linear minimum mean - squared error ( mmse ) filter that is computed with the knowledge of the channels , the signature sequences of all users , and the noise variance at the receiver .",
    "all techniques are equipped with lms algorithms .",
    "consider the downlink of a synchronous ds - cdma system with @xmath89 users , @xmath90 chips per symbol and @xmath91 paths .",
    "assuming that the channel is constant during each symbol interval , the received signal after filtering by a chip - pulse matched filter and sampled at the chip rate yields the @xmath92 received vector @xmath93 = \\sum_{k=1}^{k}a_{k}b_{k}[i]{\\bf c}_{k } { \\bf h}_{k}[i ]   + \\boldsymbol{\\eta}_{k}[i ] + { \\bf n}[i],\\ ] ] where @xmath94 , @xmath95 = [ n_{1}[i ] ~\\ldots~n_{m}[i]]^{t}$ ] is the complex gaussian noise vector with @xmath96{\\bf n}^{h}[i ] ] = \\sigma^{2}{\\bf i}$ ] , where @xmath97 and @xmath98 denotes transpose and hermitian transpose , respectively .",
    "the operator @xmath99 $ ] stands for ensemble average , @xmath100 \\in \\{\\pm1 j\\pm1\\}/\\sqrt{2}$ ] is the symbol for user @xmath101 with @xmath102 , @xmath103 $ ] represents the isi , the amplitude of user @xmath101 is @xmath104 , the channel vector is @xmath105 = [ h_{0}[i]~ \\ldots~ h_{l_{p}-1}[i]]^{t}$ ] and the @xmath106 convolution matrix @xmath107 contains one - chip shifted versions of the signature sequence for user @xmath101 given by @xmath108^{t}$ ] .",
    "the linear receiver observes @xmath109 samples per symbol and employs one of the analyzed and proposed schemes , which provides an estimate of the desired symbol as given by @xmath110 =   \\textrm{sgn}\\big(\\re\\big [ { y}_{c}[i]\\big]\\big ) +   j \\textrm{sgn}\\big(\\im\\big [ { y}_{c}[i]\\big]\\big)$ ] , where @xmath111 and @xmath112 select the real and imaginary parts , respectively , @xmath113 is the signum function , and we consider user @xmath114 as the desired one . in our simulations , we use @xmath115 and @xmath116 , the channels are time - varying with complex gains computed with clarke s model @xcite , and have @xmath117 paths with profile @xmath118 , @xmath119 and @xmath120 db with spacing between paths randomly distributed between @xmath118 and @xmath121 chips .    in the first experiment",
    ", we assess the bit error ratio ( ber ) performance against the received symbols .",
    "packets of @xmath122 qpsk symbols are transmitted and the curves are averaged over @xmath123 runs .",
    "the results depicted in fig .",
    "[ fig4 ] show that the proposed schemes a and b with the jidf scheme obtain significantly better performance than the jidf without any combination .",
    "the use of the proposed schemes is able to jointly adjust the best model order and exploit the different step sizes for adaptation . in terms of computational complexity , the scheme b is more attractive as it has a performance very close to that of scheme a , but employs only @xmath121 jidf constituent structures as opposed to scheme a , which uses a combination of @xmath0 filters .    # 1#20.95    [ fig4 ]    in the second experiment , we assess the ber performance against the signal - to - noise - ratio ( snr ) defined as @xmath124 .",
    "packets of @xmath122 qpsk symbols are again transmitted and the curves are averaged over @xmath123 runs .",
    "the results depicted in fig .",
    "[ fig5 ] show that the proposed schemes a and b with the jidf scheme can obtain significantly better performance than the jidf without any combination for different values of snr . from the results , we conclude that it might be more attractive to use scheme b as it achieves a performance very close to scheme a with half the complexity .",
    "# 1#20.95    [ fig5 ]",
    "we proposed convex combination schemes for joint model - order and step - size adaptation of reduced - rank adaptive filters based on the jidf method .",
    "the proposed schemes employ reduced - rank adaptive filters in parallel operating with different orders and step sizes , which are exploited by convex combination strategies .",
    "we investigated the performance of the proposed schemes in an interference suppression application for cdma systems .",
    "simulations showed that the proposed schemes significantly improve the performance of the existing reduced - rank adaptive filters based on the jidf method .",
    "s. haykin,_adaptive filter theory _ ,",
    "4th ed.,prentice- hall , 2002 .",
    "j. arenas - garcia , a.r .",
    "figueiras - vidal ,  adaptive combination of normalised filters for robust system identification \" , electronics letters vol .",
    "15 , july 2005 , pp .",
    "874 - 875 .",
    "silva , v.h .",
    "nascimento ,  improving the tracking capability of adaptive filters via convex combination \" , _ ieee trans . on sig .",
    "56 , no . 7 , july 2008 , pp .",
    "3137 - 3149 .",
    "p. strobach ,  low - rank adaptive filters \" , _ ieee trans . on sig .",
    "1996 , pp .",
    "2932 - 2947 . m. l. honig and j. s. goldstein ,  adaptive reduced - rank interference suppression based on the multistage wiener filter , \" _ ieee trans . on communications _",
    "6 , june 2002 .",
    "d. a. pados , g. n. karystinos ,  an iterative algorithm for the computation of the mvdr filter , \" _ ieee trans . on sig .",
    "2 , feb . 2001 .",
    "r. c. de lamare and r. sampaio - neto ,  adaptive reduced - rank mmse filtering with interpolated fir filters and adaptive interpolators \" , _ ieee sig .",
    "proc . letters _ , vol .",
    "12 , no . 3 , 2005 , pp .",
    "177 - 180 .",
    "r. c. de lamare and r. sampaio - neto ,  adaptive interference suppression for ds - cdma systems based on interpolated fir filters with adaptive interpolators in multipath channels \" , _ ieee trans .",
    "vehicular technology _",
    "56 , no . 6 , pp . 2457 - 2474 , september 2007 .",
    "r. c. de lamare and r. sampaio - neto ,  reduced - rank adaptive filtering based on joint iterative optimization of adaptive filters , \" _ ieee sig . proc . letters _ , vol .",
    "2007 , pp .",
    "980 - 983 .",
    "r. c. de lamare and r. sampaio - neto ,  adaptive reduced - rank processing based on joint and iterative interpolation , decimation , and filtering \" , _ ieee trans .",
    "57 , no . 7 , july 2009 , pp .",
    "2503 - 2514 .",
    "m. yukawa , r. c de lamare and r. sampaio - neto ,  efficient acoustic echo cancellation with reduced - rank adaptive filtering based on selective decimation and adaptive interpolation \" , _ ieee transactions on audio , speech , and language processing _ , vol .",
    "16 , no . 4 , 2008 , pp .",
    "696 - 710 ."
  ],
  "abstract_text": [
    "<S> in this work we propose schemes for joint model - order and step - size adaptation of reduced - rank adaptive filters . </S>",
    "<S> the proposed schemes employ reduced - rank adaptive filters in parallel operating with different orders and step sizes , which are exploited by convex combination strategies . the reduced - rank adaptive filters used in the proposed schemes are based on a joint and iterative decimation and interpolation ( jidf ) method recently proposed . </S>",
    "<S> the unique feature of the jidf method is that it can substantially reduce the number of coefficients for adaptation , thereby making feasible the use of multiple reduced - rank filters in parallel . </S>",
    "<S> we investigate the performance of the proposed schemes in an interference suppression application for cdma systems . </S>",
    "<S> simulation results show that the proposed schemes can significantly improve the performance of the existing reduced - rank adaptive filters based on the jidf method .    </S>",
    "<S> adaptive filters , convex combinations , model - order selection , reduced - rank adaptive filters . </S>"
  ]
}