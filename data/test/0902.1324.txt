{
  "article_text": [
    "several problems in engineering and in particular signal and image processing necessitate to estimate binary vectors corrupted by some noise and can be simply addressed using the least squares principle under binarity consraints .",
    "the resulting problem is a minimization of a quadratic form over @xmath0 , a problem which is known to be @xmath1-hard in general .",
    "one of the main approaches to relax this problem into a convex one is the semi - definite programming relaxation which has been extensively used in classification , pattern recognition and communication systems .",
    "some of the main achievements in the study of the sdp relaxation were obtained by goemans and williamson @xcite and @xcite . however , solving a semidefinite program in practice relies on interior point methods which although enjoying nice theoretical convergence properties are limited to problems of size up to 500 @xmath2 . on the other hand ,",
    "very pratically efficient bundle methods are available for the eigenvalue relaxation of the same binary quadratic optimization problems .",
    "we refer the reader to @xcite for a discussion of the pratical superiority of bundle methods for solving certain semi - definite programs such as the ones appearing in the present paper . despite this empirical fact in favor of the eigenvalue relaxation , one of the main reasons most users prefer",
    "the sdp relaxation is that good primal binary solutions can be recovered using goemans and williamson s randomized algorithm .",
    "the main motivation of the present paper is to show how a solution of the sdp can be recovered from a solution of the eigenvalue relaxation . as a by product , a new geometric interpretation of the randomized algorithm is proposed .",
    "penalized binary least squares estimation problems are problems of the form @xmath3 where the vector @xmath4 is the observed data , the matrix @xmath5 represents the `` filter '' , the vector @xmath6 is the signal , or parameter vector , that has to be estimated , and the term @xmath7 is a penalization term that can often be interpreted as an _ a priori _ information in terms of bayesian statistics .",
    "this problem belongs to the larger class of minimization of quadratic forms over binary vectors which is known to be @xmath1-hard .",
    "much work has been devoted to constructing semi definite programming ( sdp ) based relaxations for general quadratic binary problems .",
    "semi - definite programs are linear optimization problems over symmetric matrices with real coefficients and with the additional convex constraint of positive semidefiniteness ; see for instance @xcite or @xcite for excellent introductions to convex programming and in particular sdp .",
    "sdp methods have already played an important role in various topics inside signal processing problems and we refer to @xcite for a nice survey on possible applications .",
    "a common feature of essentially all the existing relaxations is that they can be obtained using lagrange duality which is a general methodology for obtaining lower bounds to hard minimization problems , as overviewed in @xcite and @xcite .",
    "the goal of the paper is to survey what is known about another lagrangian duality based relaxation , namely the eigenvalue relaxation , for this problem .",
    "this relaxation was first proposed by delorme and poljak @xcite for the max - cut problem .",
    "see also the work of poljak , rendl and wolkowics @xcite for more details .",
    "the main advantage of the eigenvalue relaxation over the sdp relaxation is that the eigenvalue relaxation can be solved much faster than the sdp relaxation , as reported for instance in @xcite , @xcite , @xcite and @xcite .",
    "this remarkable computational tractability of the eigenvalue relaxation is the main motivation for writing this detailed survey .",
    "the content of the paper is as follows .",
    "the second section is devoted to a rapid presentation of the relaxation and its relationship with lagrangian duality .",
    "we also recall a simple and well known certificate for exactness of the relaxation , i.e. the fact that a globally optimal binary solution is obtained .",
    "the third section details the relationships between the semi - definite relaxation and the eigenvalue relaxation .",
    "the main result of this section is the following : a solution of the sdp relaxation can be recovered from the solution of the eigenvalue relaxation .",
    "the case of inexact solutions to the eigenvalue relaxation is also studied .    the forth section deals with the problem of recovering binary primal solutions from the dual scheme .",
    "we first give sufficient conditions under which strong duality holds and the eigenvectors of norm @xmath8 associated to the maximum eigenvalue at optimality are binary solutions .",
    "next , in the case where strong duality does not hold , we show that goemans and williamson s randomized algorithm has a very natural meaning when viewed in terms of the optimal eigenspace associated to the maximal eigenvalue in the eigenvalue relaxation .    in the last section , we propose simulation experiments in the case of binary image denoising and cdma multiuser detection problems .",
    "the first of these problems has been previously approached by stochastic methods based on markov chains like simulated annealing and metropolis hastings schemes ; see for instance @xcite and the more recent work of gibbs @xcite .",
    "the approach discussed here was presented in @xcite . recently",
    "a lot more problems have been addressed using the sdp relaxation in @xcite .",
    "the results obtained so far are quite encouraging and the approach performs well on very dirty images .",
    "we prove hat strong duality holds for the immage denoising problem , thus recovering back the polynomial solvability result of greig , porteous and seheult as a special case and by a very different path . passing to the second problem ,",
    "our monte carlo experiments show that the average computational effort for solving the eigenvalue relaxation as a function of the number of users grows slowlier than for the sdp relaxation with the standard implementations available with scilab .",
    "* notations*. in the sequel we will use the following notations . the inner product on @xmath9 is denoted by @xmath10 , the set of real symmetric matrices of order @xmath11",
    "are denoted by @xmath12 .",
    "the partial order @xmath13 denotes the loewner ordering , i.e. for @xmath14 and @xmath15 in @xmath12 , @xmath16 means that @xmath17 is positive semidefinite . for a set @xmath18 in @xmath9",
    ", @xmath19 denotes the convex hull of @xmath18 and @xmath20 denotes its closure . for a matrix @xmath14 in @xmath12",
    ", @xmath21 denotes its diagonal vector and for @xmath22 in @xmath9 , @xmath23 denotes the diagonal matrix whose diagonal vector is @xmath22 .",
    "if an equation number @xmath24 corresponds to an optimization problem , then @xmath25 will denote the optimum value for this problem .",
    "we first introduce the eigenvalue relaxation and at the same time , we propose a quick refresher on lagrangian duality , collecting all the results that will play an essential role in the sequel .",
    "the proofs of almost all the results presented here can be found in @xcite .",
    "the binary least - squares estimation problem is in fact equivalent to the homogenized problem @xmath26 x   \\text { s.t . }",
    "x\\in \\{-1,1\\}^{n+1}. \\hspace{2 cm } { \\rm ( bls ) } \\nonumber\\ ] ] indeed , if we add the constraint @xmath27 in ( bls ) , we obtain exactly the binary least squares problem .",
    "now , if @xmath28 is a solution of ( bls ) , then @xmath29 is again a solution of of ( bls ) , thus adding the constraint @xmath27 is in fact redundant , which proves the claimed equivalence .",
    "set @xmath30 . \\nonumber\\ ] ]    notice further that the constraint @xmath31 is equivalent to @xmath32 for all @xmath33 .",
    "thus , to problem ( bls ) , we can associate the lagrangian function @xmath34    now we can add to the problem the implicit spherical constraint @xmath35 which is redundant with the binary constraints . then , optimizing over this sphere , we obtain the lagrangian dual function , i.e. @xmath36 which , using raleigh - ritz variational formulation of the largest eigenvalue of symmetric matrices , can be written @xmath37    finally , the dual problem , i.e. the eigenvalue relaxation , is given by @xmath38        it is important to notice first that the dual function @xmath39 is convex , since it is the maximum over a family parametrized by @xmath40 of linear functions in the variable @xmath41 .",
    "the main classical property of the lagrangian dual is weak duality , i.e. @xmath42 where @xmath43 denotes the optimal value .",
    "this property explains in part why lagrange duality is used : it provides a bound on the primal optimal value .",
    "when equality holds in the weak duality property , we say that strong duality holds .",
    "sometimes , like in the case of the max - cut problem , the bound can be proved to be proportional to the optimal original value .",
    "more precisely , goemans and williamson proved that the optimum value of the eigenvalue relaxation ( in fact the equivalent sdp formulation ; see the original paper and section [ eqsdp ] below ) is greater than or equal to the optimal original value ( this is just weak duality ) , which itself is always greater than or equal to .876 times the eigenvalue relaxation s optimal value . a quite similar but less tight bound , proved by nesterov applies directly to the present problem .",
    "we will recall this bound in section [ gwn ] below .",
    "it is well known that there exists an optimal dual solution .",
    "this was proved by poljak and wolkowicz in @xcite .",
    "the proof given here is more direct .",
    "the dual function admits a minimizer .",
    "* proof*. let @xmath44 .",
    "make the change of variable @xmath45 , i.e. define @xmath46 we now have the property that @xmath47 .",
    "we prove that @xmath48 is coercive .",
    "take any sequence @xmath49 with @xmath50 as @xmath51 .",
    "we can assume that @xmath52 for some @xmath53 because otherwise , the fact that @xmath50 implies that there must exists a sequence @xmath54 with @xmath55 and the fact that @xmath47 gives a contradiction .",
    "now , the gershgorin circle around the diagonal element @xmath56 has a constant radius , say @xmath57 and its center goes to @xmath58 .",
    "since @xmath59 , this implies that @xmath60 . thus @xmath48 is coercive and since it is continuous , it admits a minimizer that we will denote by @xmath61 .",
    "now , for all @xmath62 , @xmath45 , we have @xmath63 but , on the other hand , @xmath64 .",
    "therefore , @xmath65 and the proof is complete . @xmath66      the subdifferential @xmath67 of the eigenvalue relaxation has been much studied .",
    "recall that for any convex function @xmath68 , the subdifferential @xmath69 is defined by @xmath70 the analysis of @xmath67 is based on the following general theorem .",
    "[ eigsubd]@xcite let @xmath71 be an affine operator defined by @xmath72 for some linear operator @xmath73 and some matrix @xmath74 .",
    "then , we have @xmath75 with @xmath76 where @xmath77 is the adjoint of @xmath78 , @xmath79 denotes the multiplicity of @xmath80 at @xmath81 and @xmath82 is a matrix whose columns form any orthonormal basis of the eigenspace of @xmath83 associated to @xmath80 .",
    "now , if we set @xmath84 , we get @xmath85 , @xmath86 and @xmath87 . for @xmath88 ,",
    "let @xmath89 be defined by @xmath90 using the previous theorem , we obtain    [ subdtheta ] the subdifferential @xmath67 of the dual function @xmath91 is given by @xmath92    following oustry @xcite , the formula for @xmath93 in theorem [ eigsubd ] is proved by showing that the maximum eigenvalue function @xmath94 on @xmath12 is nothing but the support function @xmath95 of @xmath96 , defined by @xmath97 with the scalar product defined by @xmath98 . by definition ,",
    "the face @xmath99 of @xmath96 exposed by @xmath83 is the set of maximizers in ( [ support ] ) , i.e. @xmath100 knowing that the subdifferential of a support function of a set is exactly the exposed face of this set , we finally get @xmath101 the formula follows after some linear algebra .",
    "there is a different path to the subdifferential s formula , which is perhaps more _ a propos _ in the context of duality : it is proved in ( * ? ? ?",
    "* chapter xii ) that @xmath102 where @xmath103 denotes the closure of the convex hull .",
    "this fact is in fact true for general continuous constrained problems in the case where the underlying space is compact ( for example ) ] and the associated technical condition is called the _ filling property_. the following proposition provides a useful sufficient condition for proving that the relaxation is exact , i.e. strong duality applies .",
    "[ multone ] let @xmath104 be a minimizer of the dual eigenvalue relaxation . then , if @xmath105 has multiplicity one , then @xmath106 and any eigenvector @xmath107 of @xmath108 whose squared norm is @xmath8 is a binary solution of ( bls ) .",
    "the proof is a direct consequence of ( * ? ? ?",
    "* theorem xii.2.3.4 . ) .",
    "we provide a specialized proof here because it is short and instructive .",
    "* proof*. since the multiplicity of @xmath105 is one , the subdifferential of @xmath109 at @xmath104 is a single vector .",
    "thus , @xmath91 is differentiable at @xmath104 and its gradient is simply @xmath110 for any @xmath28 in @xmath111 such that @xmath112 .",
    "since , @xmath104 minimizes @xmath91 , we must have @xmath113 .",
    "this implies that @xmath114 for all @xmath33 .",
    "thus , using weak duality @xmath115 which proves that @xmath28 solves the original problem ( bls ) .",
    "@xmath66    we now have a nice criterion for deciding whether our relaxation was exact and if so , we also know how to recover a binary solution from an optimal eigenvector .",
    "this approach works for any quadratic binary problem and is extensively used for approximating combinatorial problems .",
    "however , the question remains on what to do when the relaxation is not exact , i.e. when the multiplicity at the optimum is greater than one .",
    "the next two sections will help answer this crucial question .",
    "the purpose of the next two sections is to describe how to recover primal binary solutions from the eigenvector solutions of the dual eigenvalue problem .",
    "it was first shown that good binary solution can be generated at random using the sdp solution by goemans and williamson @xcite in the case of the max - cut problem in graph theory .",
    "their results were then extended by nesterov to the case of indefinite quadratic binary programming @xcite .",
    "those results allowed to conclude that both eigenvalue and sdp relaxations are in a certain precise sense very efficient . however , both relaxations are not equivalent from the computational point of view .",
    "recall that one of the main motivations for using the eigenvalue relaxation is its manageable practical complexity which is often favorable compared to the one of solving the sdp relaxation .",
    "but what is not clear is how to generate good ( primal ) binary solutions in average with the eigenvalue relaxation only ?",
    "the first natural approach to this question is of course to try and recover an optimal sdp solution from the eigenvalue relaxation .",
    "thus , we devote this section to this problem . it can be solved as follows : an appropriate convex combination of rank one matrices obtained from a set of optimal eigenvectors is shown to be a solution we are looking for .",
    "our approach simplifies the presentation of @xcite .",
    "the adaptation of the randomized algorithm of goemans and williamson and the associated bound established by nesterov will be discussed in the next section .      in order to obtain the semi - definite programming ( sdp ) relaxation of the the homogenized problem ( bls ) ,",
    "we begin with the following equivalence relating our problem to a problem on symmetric matrices .",
    "we have ] @xmath116 this last problem is itself equivalent to @xmath117 this problem being nonconvex , we drop the rank constraint and obtain the following sdp ( convex ) relaxation @xmath118 whose value is obviously greater than or equal to @xmath119 .",
    "an important result of pataki ( * ? ? ?",
    "* theorem 2.1 ) gives a bound on the rank of solutions to semi - definite programs . in the case of our semi - definite relaxation , this theorem implies that the rank @xmath120 of an optimal matrix @xmath121 satisfies @xmath122 .",
    "it follows from the subdifferential s formula given in corollary [ subdtheta ] that at any minimizer @xmath104 , we have @xmath123 suppose we have in hand a matrix @xmath124 such that @xmath125 it appears that a good guess for a candidate solution @xmath121 to the sdp relaxation in the general case is @xmath126 we just need to check the details to see how it works .",
    "this result was initially proved in @xcite but the proof given here is more direct .",
    "@xcite [ equiv ] let @xmath104 be the optimal solution of the eigenvalue relaxation let @xmath82 be a matrix whose columns for an orthonormal basis of the eigenspace associated to @xmath105 and let @xmath127 be as in ( [ optz ] ) .",
    "then the matrix @xmath128 is an optimal solution of the sdp relaxation .",
    "we would like to underline at this point that a more elegant proof of the theorem could be obtained using conic duality but we preferred to keep on with elementary arguments since this is possible in the present context .",
    "* proof*. compute the eigenvalue / eigenvector decomposition @xmath129 , set @xmath130 , @xmath131 , let @xmath57 be the multiplicity of @xmath108 and let @xmath132 denote the columns of @xmath133 . recall that",
    "from the definition of @xmath127 , we have @xmath134 .",
    "then , we get @xmath135 thus , @xmath136 using this fact , we obtain @xmath137 since @xmath134 .",
    "thus , the optimal value of the sdp is greater than or equal to the optimal value of the eigenvalue relaxation . on the other hand",
    ", it is well known that the optimal value of the eigenvalue relaxation is greater than or equal to the one of the sdp relaxation .",
    "we provide a proof here for the sake of completeness .",
    "let @xmath138 be an optimal solution to the sdp relaxation .",
    "now , for all @xmath41 in @xmath139 , we have @xmath140 by using the fact that @xmath141 .",
    "now , compute the eigenvalue / eigenvector decomposition @xmath142 and let @xmath80 be the greatest of these eigenvalues . then , @xmath143 since this is true for all @xmath41 , we obtain that the eigenvalue relaxation majorates the sdp relaxation .",
    "thus , both optimal values are equal and this completes the proof of the proposition . @xmath66      of course",
    ", it can be hard to find a matrix @xmath144 that works .",
    "we will now try to overcome this problem .",
    "we first have to specify how the subgradients are obtained in practice . at each point",
    "@xmath145 , choose an eigenvector @xmath107 of squared norm equal to @xmath8 associated to @xmath146 .",
    "then , using the alternative representation of the subdifferential ( [ subd2 ] ) , a subgradient of @xmath91 at @xmath41 is obtained by setting @xmath147^t$ ] .",
    "assume that we have a set of subgradients @xmath148^t\\in \\partial \\theta(u^j)$ ] for some @xmath149 , @xmath150 and such that @xmath151 for some nonnegative @xmath152 s with @xmath153 .",
    "this can be performed for @xmath154 as small as we want by using a bundle method .",
    "such a method will construct in a finite number of iterations , say @xmath155 , an iterate @xmath156 and a family of @xmath149 s with the desired property , all of them lying in a small neighborhood of @xmath156 .",
    "this is one very nice feature of the bundle mechanism which is extensively described in ( * ? ? ?",
    "* volume ii ) .",
    "moreover , it is a well known fact , called caratheodory s theorem , that only @xmath157 subgradients are sufficient in the expression ( @xmath154opt ) .",
    "set @xmath158 then , we have the following result .    for any @xmath159 , the matrix @xmath160 defined above satisfies @xmath161    * proof*. let @xmath104 be any minimizer of @xmath91 .",
    "then , for each @xmath150 , we have by the definition of the subdifferential @xmath162 but @xmath163 is given by @xmath164 on the other hand , since @xmath165 , @xmath166 thus , we obtain @xmath167 which implies , after multiplying by @xmath152 and summing over @xmath150 @xmath168 using cauchy - schwartz inequality , this gives @xmath169 since the eigenvalue and the sdp relaxation have equal optimal values , we finally obtain @xmath170 which implies the desired result . @xmath66      it is a common idea that the sdp relaxation contains more information than the eigenvalue relaxation .",
    "we hope that the results of this section managed to convince the reader that this is in fact not the case and a good approximate solution can be recovered quite easily using subgradient information at the optimum .",
    "we now are in position to answer our main question of how to recover a satisfactory although sometimes suboptimal primal binary solution . in the first part of this section ,",
    "we show that optimal binary solutions can actually be exactly recovered using the eigenvalue relaxation , i.e. strong duality holds , under some simple conditions .",
    "then , in the case where the problem does not satisfy these necessary conditions for strong duality , we develop a randomized algorithm based on the optimal eigenspace of the maximum eigenvalue dual function and show that this procedure is equivalent to goemans and williamson s randomized algorithm for max - cut .",
    "this provides a new interpretation of goemans and williamson s procedure .",
    "we have the following theorem .",
    "[ strong ] for almost all @xmath14 in the sense of the lebesgue measure , such that @xmath171 is componentwise negative outside the diagonal .",
    "then the eigenvalue relaxation is exact , i.e. strong duality holds .    * proof*. fix @xmath145 .",
    "let @xmath172 be the vector of the first @xmath11 components of @xmath41 .",
    "the fact that @xmath171 is componentwise negative outside the diagonal implies that @xmath173 is componentwise positive .",
    "thus , the perron - frobenius theorem implies that the maximum eigenvalue of @xmath173 has multiplicity one . from this",
    ", we deduce that the maximum eigenvalue of @xmath174 also has multiplicity one .",
    "let @xmath175 be an eigenvalue decomposition of @xmath176 , where we used the subscript @xmath172 in order to remember that whatever the chosen decomposition , it is a nonlinear and non necessarily continuous function of @xmath41 .",
    "moreover , since the maximum eigenvalue has multiplicity one , corollary 4 in @xcite says that it is possible to choose the eigenvector associated to the maximum eigenvalue as a continuously differentiable function of @xmath172 .",
    "we will denote by @xmath177 this eigenvector . using this parametrization , the matrix @xmath178+d(u ) \\nonumber\\ ] ] can be rewritten as @xmath179 \\left [ \\begin{array}{cc } d_{u_1^n } & -v_{u_1^n}^ta^ty \\\\ -y^ta v_{u_1^n } & y^ty+u_{n+1 } \\end{array } \\right ] \\left [ \\begin{array}{cc } v_{u_1^n } & 0 \\\\ 0 & 1 \\end{array } \\right]^t , \\nonumber\\ ] ] where all dimensions can easily be guessed from the previous knowledge on the involved submatrices .",
    "let @xmath180 be the codimension one differentiable submanifold defined by @xmath181 let @xmath182 be the optimal set defined by @xmath183 due to the representation @xmath184 the set @xmath182 is the projection onto the cartesian product @xmath185 of the set @xmath186 where @xmath187 is the upper bound of pataki ( see section [ sdp ] ) on the optimal rank of the sdp relaxation ( here @xmath188 for @xmath11 large ) and where @xmath189 is the set @xmath190 whose intersection with @xmath191 corresponds to the parameter set allowing for zero to belong to the subdifferential of the dual function @xmath91 in the case where @xmath192 .",
    "now , since the constraint @xmath193 is described by @xmath194 equations , @xmath195 by @xmath196 equations , @xmath197 by one equation and @xmath198 , the dimension of @xmath189 is greater than or equal to @xmath199 .",
    "furthermore , notice that since the eigenvalues are continuous fonctions of the entries of @xmath200 , the subset of @xmath201 for which @xmath192 is open in the topology induced by the ambiant space .",
    "therefore its projection set onto the cartesian product @xmath185 is of dimension at least @xmath202 which garantees that the projection onto the @xmath14-space @xmath203 of its intersection with @xmath180 is a set of null lebesgue measure . and",
    "thus , for almost all @xmath14 , such that @xmath171 is componentwise negative outside the diagonal , @xmath204 .    using this result , theorem a about the interlacing property of the eigenvalues for arrow matrices in the appendix implies that the maximum eigenvalue of @xmath205 is greater than the maximum diagonal element of @xmath206 which nothing by @xmath207 and all @xmath11 other eigenvalues are less than @xmath207 .",
    "this implies that for allmost all @xmath14 , the maximum eigenvalue of @xmath205 has multiplicity one at the optimum , which implies that @xmath91 is differentiable at the optimum .",
    "therefore , using proposition [ multone ] we obtain that strong duality holds for allmost all @xmath14 such that @xmath171 is componentwise negative outside the diagonal .",
    "we start this section with some recalls on goemans and williamson s algorithm and nesterov s bound .",
    "the method relies on the cholesky factorization of the optimal solution @xmath121 of the sdp relaxation , @xmath208 from theorem [ equiv ] we see that @xmath209 where @xmath79 is the multiplicity of @xmath105 at the chosen corresponding solution @xmath104 of the eigenvalue relaxation .",
    "this factorization is important , since it allows to write @xmath210 where @xmath211 is the transpose of @xmath212 row vector of @xmath213 .",
    "let @xmath214 be a random variable with uniform distribution on the unit sphere in @xmath215 .",
    "[ sdpgw ] * ( goemans and williamson s algorithm ) *    \\1 .",
    "find the cholesky factorization @xmath216 .",
    "let @xmath217 be a random vector with uniform distribution on the unit sphere of @xmath218 .",
    "the random cut is defined by @xmath219 where the sign function is defined coordinate - wise .",
    "draw @xmath11 samples from @xmath220 , say @xmath221 , ",
    ", @xmath222 and choose the sample giving the best value of the objective function @xmath223 .",
    "the key result is that , in average , the vector @xmath220 gives a good binary solution to the original problem .",
    "since the best sample will have greater cut value than the average with overwhelming probability , the above procedure should work well .",
    "this is made precise by nesterov s theorem .",
    "define @xmath224 and @xmath225 then , we have @xmath226}{f^*-f_*}\\leq \\frac2{\\pi}.\\ ] ]    this result is remarkable despite the fact that the bound @xmath227 is rather large .",
    "an important issue for future research is to study such type of bounds for particular subclasses of problems in hope of improving nesterov s result .",
    "the main drawback of the former presentation is that using the uniform variable @xmath214 is quite hard to motivate from an optimization viewpoint .",
    "let us take a slightly different perspective .",
    "assume that we have a solution @xmath104 of the eigenvalue relaxation .",
    "as before , let @xmath82 be a matrix whose columns form an orthonormal bases of the eigenspace associated to @xmath105 .",
    "moreover , we may require that @xmath228 where @xmath229 is some diagonal matrix with @xmath230 , @xmath231 and @xmath232 . in the case where the multiplicity at the optimum is one",
    ", the optimal eigenbasis reduces to a unique vector and we saw in proposition [ multone ] that multiplying this vector by @xmath233 gives a binary solution .",
    "now let us turn to the case where there are @xmath234 eigenvectors . to each unit norm eigenvector @xmath235 , we associate a subgradient @xmath236^t$ ] .",
    "then , ( [ eigopt ] ) implies that @xmath237    [ opteig ]    ( -1,-1)(2,2 ) ( 0,0)(0,.95 ) ( 0,0)(1.3,-.95 ) ( 0,0)(-1.42,-.77 ) ( 0,1)(1.4,-1 ) ( 1.4,-1)(-1.5,-.8 ) ( 0,1)(-1.5,-.8 ) ( 0,1.5)@xmath238 ( 1.9,-1)@xmath239 ( -1.6,-.3)@xmath240 ( .2,.15)@xmath241    now one natural strategy might be the following : pick the best eigenvector , i.e. the eigenvector @xmath242 whose associated coefficient @xmath243 in expression ( [ convcombzero ] ) is the _ greatest _ and round its coordinates to the nearest binary values .",
    "there is a second strategy : draw random linear combinations of the @xmath244 s giving preference to the components with higher associated coefficient in ( [ convcombzero ] ) .",
    "this can be done by sampling vectors of the type @xmath245 where the @xmath246 s are independent random variables with distribution @xmath247 . for each sample ,",
    "a feasible solution is obtained by rounding off the components to the nearest binary .",
    "we sum up this procedure as follows .",
    "* ( randomized algorithm based on optimal eigenvectors ) * [ eiggw ] 1 . find the matrix @xmath82 whose columns form an orthonormal eigenbasis associated to @xmath248 such that ( [ convcombzero ] ) holds for some @xmath152 s satisfying @xmath231 and @xmath249 .",
    "let @xmath217 be a random vector with distribution @xmath250 .",
    "the random cut is defined by @xmath251    \\3 .",
    "draw @xmath11 samples from @xmath220 , say @xmath252 ,  , @xmath253 and choose the sample giving the best value of the objective function @xmath254 .",
    "the important result is that this second strategy is equivalent to goemans and williamson s randomized procedure .",
    "procedure [ eiggw ] is equivalent to goemans and williamson s algorithm .",
    "* proof*. set @xmath255 .",
    "then theorem [ equiv ] and equation [ convcombzero ] imply that @xmath216 with @xmath256 , thus retrieving the cholesky factorization of @xmath121 .",
    "let @xmath257 .",
    "it is clear that @xmath214 has distribution @xmath258 .",
    "this proves that the cut @xmath220 obtained by procedure [ eiggw ] is exactly the output of goemans and williamson s procedure .",
    "@xmath66    the eigenvalue point of view thus allowed us to provide an alternative and geometric explanation for taking a random cut using a uniformly distributed variable on the sphere in goemans and williamson s methodology .",
    "in this section , we provide some results for the concrete problems of image denoising and show how this relaxation applies to the problem of multiuser detection in cdma systems .",
    "the first set of simulations is devoted to the denoising problem , in which @xmath14 is simply the identity matrix .",
    "this is the problem considered in @xcite , @xcite and @xcite for instance . the original binary image as 26 rows and 62 columns which gives a total number of 1612 variables .    for this problem , the penalization matrix @xmath259 is chosen so as to smooth the image .",
    "this is achieved by requiring neighboring pixels to be similar in the sense that if @xmath53 and @xmath260 are indices of neighbor pixels , then , we would like the least square cost to be penalized by the quantity @xmath261 .",
    "thus , @xmath259 is the matrix associated to the quadratic form @xmath262 where @xmath263 denotes the property of being neighbor indices and the @xmath264 are nonnegative .",
    "the neighborhood of each pixel is usually chosen to be the north , south , east and west pixels .",
    "the following theorem is the main result of this section .",
    "for @xmath265 , the identity matrix and @xmath259 the matrix associated to the quadratic form ( [ reg ] ) , the eigenvalue relaxation is exact .",
    "* proof*. the eigenvalue relaxation of the optimization problem corresponding to this binary least square denoising problem is as before @xmath266 consider now the perturbed optimization problem @xmath267 where @xmath268 is negative outside the diagonal .",
    "since the @xmath264 are nonnegative , the matrix @xmath259 has only nonpositive off diagonal terms and thus , theorem [ strong ] proves that strong duality holds for this problem and there exists a binary eigenvector that achieves optimality .",
    "assume that @xmath268 is chosen so that @xmath269",
    ". then , the optimum value @xmath270 of problem ( denoise ) and the optimum value @xmath271 of problem ( perturbed ) satisfy @xmath272 moreover , by weak duality , we have @xmath273 since strong duality holds for problem ( perturbed ) , denoting by @xmath274 a solution of @xmath275 we have @xmath276 therefore , we obtain @xmath277 which implies @xmath278 now , since @xmath0 is finite , the image @xmath279 of @xmath0 by the function @xmath280 is a finite set .",
    "let @xmath281 denote the closest number to @xmath282 in @xmath279 .",
    "now , choosing @xmath283 , we obtain @xmath284 which proves that the denoising problem is polynomial time solvable by solving problem ( perturbed ) .",
    "@xmath66    this theorem is to be compared with the results of d. m. greig , b. t. porteous and a. h. seheult @xcite which formulates the binary denoising problem as a minimization problem with cost given at the top of page 273 .",
    "the objective to be minimized in @xcite can be rearranged so as to minimize a linear cost with same penalization as the one given by ( [ reg ] ) .",
    "the main contribution of @xcite is to say that this problem can be solved in polynomial time using a network flow algorithm .",
    "notice that our proof works for @xmath285 and any additional linear term added to the penalized objective function to be optimized .",
    "since the eigenvalue relaxation can also be optimized in polynomial time , this confirms that the eigenvalue relaxation performs at least as good as previous approaches on a well known problem . on the other hand",
    ", the eigenvalue relaxation can be a flexible approach in more complicated cases where @xmath14 is not equal to the identify or other quadratic constraints have to be incorporated such as in @xcite .",
    "the experiments reported on below were performed for the case of quite noisy original images .",
    "the noise was taken to be additive , independent identically distributed and gaussian @xmath286 and was applied to the symmetrized image with pixel values in @xmath287 . in order to show the influence of the smoothing parameter @xmath288 , we displayed the percentage of misspecified bits vs values of @xmath288 .",
    "the recovered image is the one with the choice of @xmath288 giving the best percentage of bits recovered .",
    "we found the results very encouraging . indeed ,",
    "even when the observed image is very noisy , we still recover an image which is readable .",
    "this suggested that an appropriate postprocessing might easily allow to recover the original written words , by comparing the letters to a given dictionary .",
    "cross validation can be used to estimate @xmath288 .",
    "we will not discuss this problem here .",
    "instead , it seems reasonable to argue that the choice of @xmath288 can just be made _ a posteriori _ since it consists of tuning the method until a satisfactory solution is obtained .",
    "this reduces the hard combinatorial initial problem to a simpler one parameter knobing procedure .",
    "the displayed experiment and the numerous simulations not presented here confirm that robust intervals for the values of @xmath288 are not very difficult to identify in practice .",
    "this problem was studied by @xcite using the maximum likelihood approach .",
    "as we will see , the resulting optimization problem is of the same form as the binary least squares problem .",
    "the main difference here is that @xmath289 and @xmath290 .",
    "a synchronous k users ds - cdma system is considered with a common single path additive white gaussian noise ( awgn ) channel .",
    "the signature waveform of the @xmath155th user is denoted by @xmath291 , a function taking nonzero values in @xmath292 $ ] and being equal to zero outside this interval , and @xmath293 is the information bit transmitted by user @xmath155 .",
    "the overall received signal is therefore of the form @xmath294 where @xmath295 is the amplitude of the @xmath155th user s signal and @xmath296 is an additive white gaussian white noise with zero mean and variance @xmath297 .",
    "the signal @xmath298 is then filtered using a bank of @xmath299 matched filters .",
    "the output of the @xmath155th matched filter is given by @xmath300 in matrix form , this can be written @xmath301 where @xmath302^t$ ] , @xmath187 is the correlation matrix whose components are given by @xmath303 , @xmath304 and @xmath288 is the vector with components @xmath305 .",
    "since the gaussian vector has a correlation matrix equal to @xmath306 , the ml estimator is obtained by simply solving the following combinatorial optimization problem .",
    "@xmath307      the sdp approach seems to have been first applied for the ds - cdma detection problem in @xcite .",
    "since then numerous contributions have appeared using the sdr and comparing it to other methods as in @xcite and @xcite .",
    "extension to m - ary phase shift keying symbol constellations is proposed in @xcite .",
    "the issue of accelerating the speed of the method is addressed in @xcite . however , as for the former problem , the main drawback of the standard primal semidefinite relaxation is that the size of the problem is greatly increased by using @xmath308 matrices instead of vectors of size @xmath299 . in order to overcome this problem , a better approach using semidefinite programming duality",
    "was recently proposed in @xcite .",
    "the analysis of the previous sections proves that the eigenvalue relaxation is equally applicable to this problem and maybe a good competitor to the sdp relaxation .",
    "the most important point of our analysis is the following : theorem [ strong ] proves that if the correlation matrix @xmath187 is componentwise negative outside the diagonal , then strong duality holds , i.e. the detection problem can be solved exactly in polynomial time .",
    "the construction of efficient signatures is the current subject of an active research activity .",
    "for instance , the theory of frames allows to consider the problem from an interesting viewpoint as developed in @xcite .",
    "our findings suggest in particular that the componentwise negativity of the correlation matrix may be an interesting constraint to look at in future investigations on this problem .",
    "[ negcorr ]    ( -1,-1)(1,1 )    ( 0,0)(0,1 ) ( 0,0)(.866,-.5 ) ( 0,0)(-.866,-.5 ) ( 0,0)1 ( 0,0).5 - 3090 ( 0,1.2)@xmath309 ( 1.2,-.5)@xmath310 ( -1.2,-.5)@xmath311 ( .6,.6)@xmath312    finally , the eigenvalue relaxation can also be useful even for general signatures because of the weak duality property . indeed , several recent publications prove that clever heuristics can perform better than the sdp relaxation",
    ". however , in real situations it is hard to certify that a primal solution provided by such a heuristic is indeed the optimal solution because the original signal is unknown .",
    "comparing the dual optimal value to a primal value given by a heuristic can give a precise idea of the error without prior information on the signal .      in order to verify this point",
    ", we performed monte carlo simulations over 1000 random problems for a number a users varying from 10 to 35 .",
    "these computational experiments are reported in figure [ cdma ] where the number of users is on the x - axis and the average computation time is on the y - axis .",
    "the computations where performed using the scilab software @xcite .",
    "the sdp solver called _ semidef _ interfaces boyd and vandenberghe s sp.c program .",
    "the eigenvalue relaxation was solved using the solver _ optim _ with the `` nd ''",
    "option for possibly nondifferentiable costs as is the case here .",
    "the curves in figure [ cdma ] interpolate the average computation times for messages taken to be sequences of uniform and independant variables taking values in @xmath313 vs. the number of users .",
    "the curve with dashed style is for the results of the sdp relaxation while the curve with plain style is for the eigenvalue relaxation .",
    "our computations suggest that the eigenvalue relaxation has lower complexity growth as the number of users increases exactly as expected .",
    "the reader should be warned that this experiment does not prove that the complexity of the eigenvalue relaxation is lower than the sdp relaxation .",
    "the experiment only shows that when a widely used routine for sdp is used , the eigenvalue relaxation , solved using a general purpose bundle method available through a free a well established software , has a lower complexity growth on this problem .",
    "arrow matrices are matrices @xmath14 of the form @xmath314,\\ ] ] the properties of the eigenvalues of such matrices have been well studied in the past .",
    "some of them are summarized in the following theorem .",
    "* theorem a*. let @xmath14 be an arrow matrix , with @xmath315 .",
    "moreover , assume that all the components of @xmath316 are different from zero .",
    "let @xmath317 be its eigenvalues considered in increasing order .",
    "then , the characteristic polynomial of @xmath14 is given by @xmath318 then , we have @xmath319 and @xmath320 .",
    "moreover , if @xmath321 we have @xmath322 and if @xmath323 , we have @xmath324 .",
    "the properties of the eigenvalues of arrow matrices are part of the folkore , especially in the realm of mathematical physics .",
    "we give a sketch of the proof of this theorm below in order to give the main ideas underlying the results .",
    "* proof of theorem a*. the formula for the characteristic polynomial @xmath325 is easily obtained by reccurence on the dimension .",
    "we have to consider two cases :    * for some @xmath53 , @xmath321 , * @xmath326    in the first case @xmath327 is a root of @xmath328 . in the second case",
    "@xmath329 which is different from zero since we assumed all the @xmath330 s to be different from zero . in this case",
    ", the eigenvalues of @xmath14 are the zeros of the function @xmath331 from this formula , we deduce that there is a root in each interval @xmath332 , @xmath333 , for all @xmath334 and @xmath335 .",
    "the final conclusions are easily derived by combining the results in the two simple cases discussed above .",
    "in this paper , we surveyed the main properties of the eigenvalue relaxation for binary least squares problem .",
    "a full connection with the standard sdp relaxation was presented and we showed how to recover a solution of the semi - definite program from the solution of the eigenvalue minimization problem .",
    "the problem of recovering primal binary solution was also addressed and we gave simple sufficient conditions for strong duality . in the case where these conditions are not satisfied , the randomized procedure adapted from goemans and williamson s allows to recover binary solutions with garanteed relative approximation ratio due to nesterov s bound .",
    "two applications were presented : binary image denoising and detection in multiuser cdma systems . in the case of image denoising",
    ", we show that strong duality holds . for the multiuser detection problem",
    ", our results prove that strong duality holds when the signature covariance matrix has nonpositive off diagonal components .",
    "lemarchal , claude and oustry , franois _ sdp relaxations in combinatorial optimization from a lagrangian viewpoint_. advances in convex analysis and global optimization ( pythagorion , 2000 ) , 119134 , nonconvex optim .",
    "54 , kluwer acad .",
    "publ . , dordrecht , 2001 .",
    "wolkowicz , henry and anjos , miguel f. _ semidefinite programming for discrete optimization and matrix completion problems_. workshop on discrete optimization , do99 ( piscataway , nj ) .",
    "discrete appl .",
    "123 ( 2002 ) , no . 1 - 3 , 513577 .",
    "ben - tal , aharon and nemirovski , arkadi _ lectures on modern convex optimization .",
    "analysis , algorithms , and engineering applications_. mps / siam series on optimization .",
    "society for industrial and applied mathematics ( siam ) , philadelphia , pa ; mathematical programming society ( mps ) , philadelphia , pa , 2001 .",
    "helmberg , christoph and oustry , franois _ bundle methods to minimize the maximum eigenvalue function_. handbook of semidefinite programming , 307337 , internat .",
    "management sci .",
    ", 27 , kluwer acad .",
    "boston , ma , 2000        nesterov , yuri , wolkowicz , henry and ye , yinyu _ semidefinite programming relaxations of nonconvex quadratic optimization_. handbook of semidefinite programming , 361419 , internat .",
    "management sci .",
    ", 27 , kluwer acad .",
    "boston , ma , 2000 .",
    "keuchel , j. ; schnorr , c. ; schellewald , c. ; cremers , d. ; _ binary partitioning , perceptual grouping , and restoration with semidefinite programming _ , pattern analysis and machine intelligence , ieee transactions on * 25 * , ( 2003 ) , no .",
    "11 , 13641379 .",
    "hiriart - urruty , j .- b . ; lemarchal , c. _ convex analysis and minimization algorithms .",
    "ii . advanced theory and bundle methods .",
    "_ grundlehren der mathematischen wissenschaften , 306 .",
    "springer - verlag , berlin , 1993 .",
    "goemans , michel x.and williamson , david p. _ improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming_. j. assoc .",
    "42 ( 1995 ) , no . 6 , 11151145 .",
    "garey , m. r. and johnson , d. s. _ computers and intractability .",
    "a guide to the theory of np - completeness_. a series of books in the mathematical sciences .",
    "w. h. freeman and co. , san francisco , calif . , 1979 .",
    "fumihiro hasegawa , jie luo , krishna r. pattipati , peter willett and david pham , _ speed and accuracy comparison of techniques for multiuser detection in synchronous cdma _ , ieee trans .",
    "52 ( 2004 ) , no .",
    "4 , 540545 .",
    "moussa abdi , hassan el nahas , alexandre jard and eric moulines , _ semidefinite positive relaxation of the maximum likelihood criterion applied to the multiuser detection in a cdma context _",
    ", ieee sig .",
    "letters , 9 , ( 2002 ) , no . 6 , 165167 .",
    "d. m. greig , b. t. porteous , and a. h. seheult , _ exact maximum a posteriori estimation for binary images _ ,",
    "b 51 ( 1989 ) , no .",
    "2 , 271 279 .",
    "j. tropp , i. s. dhillon , r. w. heath , jr . , and t. strohmer _ designing structured tight frames via an alternating projection method _ , ieee trans .",
    "theory , vol .",
    "51 , ( 2005 ) no .",
    "1 , 188209 ."
  ],
  "abstract_text": [
    "<S> the goal of this paper is to survey the properties of the eigenvalue relaxation for least squares binary problems . </S>",
    "<S> this relaxation is a convex program which is obtained as the lagrangian dual of the original problem with an implicit compact constraint and as such , is a convex problem with polynomial time complexity . </S>",
    "<S> moreover , as a main pratical advantage of this relaxation over the standard semi - definite programming approach , several efficient bundle methods are available for this problem allowing to address problems of very large dimension . </S>",
    "<S> the necessary tools from convex analysis are recalled and shown at work for handling the problem of exactness of this relaxation . </S>",
    "<S> two applications are described . </S>",
    "<S> the first one is the problem of binary image reconstruction and the second is the problem of multiuser detection in cdma systems . </S>"
  ]
}