{
  "article_text": [
    "classical work on variable selection dates back to @xcite , who proposed to choose a model that minimizes the kullback- leibler ( kl ) divergence of the fitted model from the true model , leading to the well - known akaike information criterion ( aic ) .",
    "@xcite took a bayesian approach by assuming prior distributions with nonzero probabilities on lower dimensional subspaces .",
    "he proposed what is known as the bic method for model selection .",
    "other types of @xmath0 penalties include @xmath1 @xcite , aicc @xcite , ric @xcite and ebic @xcite , among others .",
    "the @xmath0 regularization has a natural interpretation in the form of best subset selection .",
    "it also exhibits good sampling properties @xcite .",
    "however , in a high - dimensional setting , the combinatorial problem has np - complexity , which is computationally prohibitive . as a result",
    ", numerous attempts have been made to modify the @xmath0 type regularization to alleviate the computational burden .",
    "they include bridge regression @xcite , non - negative garrote @xcite , lasso @xcite , scad @xcite , elastic net @xcite , adaptive lasso or alasso @xcite , dantzig selector @xcite , sica @xcite , mcp @xcite , among others .    to a certain extent",
    ", existing penalties can be classified into one of the following two categories : convex penalty and nonconvex penalty .",
    "convex penalties , such as lasso @xcite , can lead to a sparse solution and are stable as the induced optimization problems are convex .",
    "nonconvex penalties , such as scad @xcite and mcp @xcite , can on the other hand lead to sparser solutions as well as the so - called oracle properties ( the estimator works as if the identities of nonzero regression coefficients were known beforehand ) . however , the non - convexity of the penalty could make the entire optimization problem nonconvex , which in turn could lead to a local minimizer and the solution may not be as stable as the one if instead a convex penalty is used .",
    "therefore , an important issue for nonconvex penalties is a good balance between sparsity and stability .",
    "for example , both scad and mcp have an extra tuning parameter which regulates the concavity of the penalty so that , when it exceeds a threshold , the optimization problem becomes convex .",
    "it is well known that penalty functions have bayesian interpretation .",
    "the classical @xmath2 penalty ( ridge regression ) is equivalent to the bayesian estimator with a normal prior .",
    "the @xmath3-type penalties , such as lasso , alasso etc . , also have bayesian counterparts ; cf .",
    "@xcite , @xcite and @xcite .",
    "@xcite initiated the discussion about the issue of stability in model selection .",
    "he demonstrated that many model selection methods are unstable but can be stabilized by perturbing the data and averaging over many predictors .",
    "@xcite introduced the random forest , providing a way to stabilize the selection process .",
    "@xcite derived theoretical results to analyze the variance reduction effect of bagging in hard decision problems .",
    "@xcite proposed a stable selection , which combines the subsampling with high - dimensional variable selection methods .",
    "the main objective of the paper is to introduce a family of penalty functions for generalized linear models that can achieve a proper balance between sparsity and stability .",
    "because for the generalized linear models , the loss function is often chosen to be the negative log - likelihood , it is conceivable to take into consideration the form of the likelihood in the construction of penalty functions .",
    "the bayesian connection to the penalty construction and to the likelihood function make it natural to introduce penalty functions through suitable prior distributions . to this end",
    ", we introduce the family of negative absolute priors ( nap ) and use it to develop what to be called likelihood adaptively modified penalties ( lamp ) .",
    "we define two types of asymptotic stability that cover a wide range of situations and provide a mathematical framework under which the issue of stability can be studied rigorously .",
    "we show that under suitable regularity conditions , the lamp results in an asymptotically stable estimator .    the rest of the paper is organized as follows .",
    "section 2 introduces the lamp family with motivations from its likelihood and bayesian connections .",
    "specific examples are given for the commonly encountered generalized linear models . in section 3",
    ", we introduce two types of asymptotic stability and study the asymptotic properties of lamp family .",
    "the choice of the tuning parameters and an efficient algorithm are discussed in section 4 . in section 5 ,",
    "we present simulation results and applied the proposed method to two real data sets .",
    "we conclude with a short discussion in section 6 .",
    "all the technical proofs are relegated to the appendix .",
    "to introduce our approach , we will first focus on the generalized linear models @xcite .",
    "it will be clear that the approach also works for other types of regression models , including various nonlinear regression models .",
    "indeed , our simulation results presented in section 5 also include the probit model , which does not fall into the exponential family induced generalized linear models .    throughout the paper",
    ", we shall use @xmath4 and @xmath5 to denote the iid random variables for @xmath6 , where @xmath7 is the response observation of @xmath8 and @xmath9 is the @xmath10-dimensional covariate observation of @xmath11 , @xmath12 @xmath13 with @xmath14 .",
    "let @xmath15 , the @xmath16 matrix of observed covariates .",
    "following @xcite , we assume that the conditional density of @xmath7 given covariates @xmath9 has the following form @xmath17,\\end{aligned}\\ ] ] where @xmath18 is a smooth convex function , @xmath19 or @xmath20 , @xmath21 and @xmath22 is the dispersion parameter .",
    "then up to an affine transformation , the log - likelihood function is given by @xmath23.\\end{aligned}\\ ] ] note that the form of @xmath24 is uniquely determined by @xmath18 and vice versa .",
    "for a given @xmath18 , we propose the induced penalty function @xmath25,\\ ] ] which contains three parameters @xmath26 , @xmath27 and @xmath28 .",
    "the corresponding negative penalized log - likelihood function is @xmath29 because the penalty function defined by ( [ lamp ] ) is likelihood specific , we will call such a penalty likelihood adaptively modified penalty ( lamp ) .",
    "we clearly have @xmath30 and @xmath31 .",
    "furthermore , taking the first and second derivatives , we get @xmath32    the parameters have clear interpretations : @xmath33 is the usual tuning parameter to control the overall penalty level ; @xmath34 is a location parameter , which may be fixed as a constant ; @xmath35 controls the concavity of the penalty in the same way as @xmath36 in scad ( fan and li , 2001 ) and @xmath37 in mcp ( zhang , 2010 ) .    the exponential family assumption given by implies that @xmath38 and @xmath39 .",
    "when @xmath8 is nonnegative , @xmath40 . thus @xmath41 is negative and the corresponding lamp must be concave .    like many other penalty functions",
    ", the family of lamps also has a bayesian interpretation . to see this , we introduce the following prior for any given @xmath18 that defines the exponential family .",
    "let @xmath42 , @xmath43 and @xmath44 @xmath45 be constants .",
    "define a prior density , to be called negative absolute prior ( nap ) , @xmath46 .",
    "\\end{aligned}\\ ] ]    if we choose @xmath47 , @xmath48 and @xmath49 $ ] , then the posterior mode is exactly the minimizer of ( [ eq::lik - pen ] ) the penalty form of lamp .",
    "we will show that such a choice will lead to good asymptotic properties .",
    "the additive form for the penalty function suggests that the prior must be of the product form .",
    "for each @xmath50 , hyperparameter @xmath51 scales @xmath52 while hyperparameter @xmath53 gives the speed of decay . for the @xmath50th parameter @xmath52 ,",
    "the larger the values of @xmath51 and @xmath53 are , the more information the prior has for @xmath52 . translating it into the penalty function ,",
    "it means that values of @xmath51 and @xmath53 represent the level of penalty and they can be adjusted separately for each component .",
    "the form of nap is to similar to that of a conjugate prior , in that the shape of the prior is adapted to that of the likelihood function .",
    "however , nap also differs from the conjugate prior in several aspects : negative when @xmath54 , absolute , from separate dimension , and with a lasso term taken away . unlike conjugate",
    "prior , which assumes additional samples , this can be seen as to take away the absolute value of redundant sample information from each dimension .",
    "it is worth looking into the commonly encountered generalized linear models and examine properties of the corresponding lamps .",
    "+ _ linear regression_. in this case , @xmath55 .",
    "thus , lamp reduces to the elastic net @xcite . @xmath56",
    "_ logistic regression_. here @xmath57 .",
    "consequently , the penalty function @xmath58 ,    \\end{aligned}\\ ] ] where @xmath59",
    ". this penalty will be called sigmoid penalty .",
    "_ poisson regression_. here @xmath60 and we have @xmath61.\\ ] ] this will be called the poisson penalty .    _ gamma regression_. for the gamma regression , we have @xmath62 .",
    "then , the penalty has the following form @xmath63.\\ ] ]    _ inverse gaussian regression_. for the inverse gaussian , we have @xmath64 . the resulting penalty @xmath65.\\ ] ]    _ probit regression_. as we mentioned earlier , lamp approach can also accommodate regression models not in the form of naturally parametrized exponential families . in particular , it is applicable to the probit regression for binary outcomes . in this case ,",
    "@xmath66 , which leads to the following penalty form @xmath67},\\ ] ] where @xmath68 and @xmath69 are respectively distribution and density functions of the standard normal random variable .",
    "[ remark::alpha_1_choice ] for the above examples , the effect of tuning parameters in the penalty function varies across settings .",
    "for example , @xmath34 does not play a role in the poisson regression .",
    "in addition , we have a natural choice for @xmath34 for the penalty functions .",
    "specifically , we can choose @xmath70 for the cases of linear , gamma , and inverse gaussian , and @xmath71 for the cases of logistic and probit .",
    ", title=\"fig : \" ] , title=\"fig : \" ]    as the examples show , the lamp family is fairly rich .",
    "they also differ from the commonly used penalties .",
    "figure [ fig : penaltyfunctions1 ] contains plots of penalty functions ( left panel ) along with their derivatives ( right panel ) that include lasso , scad , mcp and two members of the lamp family ( sigmoid and poisson ) . here",
    "@xmath72 for mcp , @xmath73 for scad , @xmath74 for sigmoid and @xmath75 for poisson penalty , and @xmath76 for sigmoid penalty .",
    "the parameters are chosen to keep the maximum concavity of these penalties the same .",
    "figure [ fig : penaltyfunctions1 ] shows that sigmoid and poisson penalties lie between mcp and scad when the maximum concavity is the same . also , from the graphs of the derivatives , it is easy to identify that the penalties of the lamp family have continuous derivatives ( actually they have continuous derivatives of any order for most common generalized linear models ) as compared with the discontinuous ones for scad and mcp",
    ". it will be shown that this feature can make the optimization problem easier and the estimation more stable .",
    "consider a one - dimensional penalized logistic regression problem , @xmath77 $ ] , where derivative of the penalty function @xmath78 if the true parameter @xmath79 , we need @xmath80 , where @xmath81 is a @xmath82-centered ball with radius @xmath83 uniform in @xmath84 , to get @xmath85 , while for @xmath86 , we need @xmath87 .",
    "thus , to differentiate a nonzero @xmath82 from 0 , we should make the difference between @xmath88 for @xmath89 and @xmath90 as large as possible .",
    "in addition , we need to control the second derivative of the penalty function to make the penalized negative log - likelihood function globally convex .",
    "figure [ fig : concavity ] illustrates graphically how such dual objectives can be met for the logistic regression under the sigmoid penalty and mcp .",
    "the grey area represents the difference between @xmath91 and @xmath90 for the sigmoid penalty .",
    "a subset of the grey area , marked by darker color , represent the corresponding difference under mcp .",
    "the reason that mcp tends to have a smaller difference compared with sigmoid penalty is because the second derivative of the penalty needs to be controlled by that of the negative log - likelihood in order to maintain the global convexity .    ]",
    "recall that the negative log - likelihood considered here is convex , and the maximum likelihood estimation is uniquely defined and stable . by adding a nonconvex penalty , the convexity may be destroyed . to study stability of the penalized mle ,",
    "it is necessary to study the impact of the penalty on the concavity , especially when @xmath84 is large .",
    "for the negative penalized maximum log - likelihood estimation procedure , if nonconvex penalties are used , the solution to the minimization problem may not be unique .",
    "therefore , it is natural to study the behavior of the local maximizers in penalized likelihood estimates when the observations are perturbed .",
    "here we introduce a new concept of asymptotic stability to describe the asymptotic performances of local minimizers in penalized likelihood estimates .",
    "note that even for the negative penalized likelihood estimators with convex penalty where the unique minimizer exists , such asymptotic stability concept is still useful in characterizing the behavior of the global maximizer .",
    "suppose we want to minimize with respect to @xmath92 a criterion function @xmath93 , where @xmath94 and @xmath95 is the @xmath50th observation of @xmath96 .",
    "denote by @xmath97 and @xmath98 the support for @xmath99 and domain for @xmath92 , respectively .",
    "we say that @xmath100 is a local minimizer if there exists a neighborhood within which @xmath101 attains its minimum .",
    "more precisely , the set of the local minimizers is defined as @xmath102 throughout the paper , @xmath103 denotes the @xmath2-norm of a vector or matrix ( vectorized ) while @xmath104 denotes the @xmath3 norm . for any @xmath105 let @xmath106 the be @xmath107-centered ball with radius @xmath108 .",
    "it is clear from the definition that the set of local maximizers is random .",
    "we characterize its asymptotic behavior in terms of whether or not the set converges to a single point as @xmath109 . for a set @xmath110 ,",
    "define its diameter as @xmath111 .",
    "we say that the set of local minimizers of @xmath112 satisfies weak asymptotic stability if @xmath113 @xmath114>\\delta\\big)=0.\\ ] ]    the weak asymptotic stability characterizes the asymptotic behavior of local minimizers when the data are perturbed slightly .",
    "it shows that for large @xmath84 and small perturbation , the local minimizers stay sufficiently close to each other with high probability .",
    "defined below is a stronger version , which guarantees uniqueness of the minimizer .",
    "we say that the set of local minimizers of @xmath112 satisfies strong asymptotic stability if @xmath115=0\\big)=1.\\ ] ]    under the weak asymptotic stability , multiple minimizers , though shrinking to @xmath116 , may exist with high probability for any finite @xmath84 . the strong asymptotic stability , on the other hand , entails that for sufficiently large @xmath84 , the probability of having multiple minimizers must converge to @xmath116 , implying that there must be a unique minimizer with high probability .",
    "@xmath0 penalties , though may have the weak asymptotic property if we adjust its tuning parameter like aic , will never possess the strong asymptotic property , because then the solution of each submodel with the number of parameters constrained will be a local optimizer , and with no probability all solutions will coincide , as @xmath117 .",
    "we now consider the situation in which @xmath93 can be approximated by an i.i.d .",
    "sum with a remainder term , i.e. , @xmath118 this general form includes the form of a negative log - likelihood plus a penalty .",
    "let @xmath119 .",
    "we assume throughout the rest of the paper that @xmath98 is compact with the true parameter value , denoted by @xmath120 , lying in its interior and that @xmath121 is finite .",
    "we need the following regularity conditions .    1 .   for any @xmath122",
    "is convex as a function @xmath92 , and @xmath123 is strictly convex .",
    "2 .   there exists function @xmath124 such that @xmath125 and , for any @xmath126 @xmath127 and @xmath128 , such that @xmath129 @xmath130 4 .",
    "there exists a local minimizer of that is a consistent estimator of @xmath131 5 .",
    "there exists @xmath132 such that @xmath133    when @xmath134 and @xmath135 are both smooth functions , conditions ( c2 ) and ( c3 ) can be guaranteed by assuming that the derivative of @xmath134 is bounded uniformly and that the derivative of @xmath135 tends to 0 uniformly .",
    "condition ( c5 ) is guaranteed by the convexity around the true parameters , uniformly in @xmath84 .",
    "the next two lemmas provide sufficient conditions for the two types of asymptotic stability .",
    "[ lemma::stability : weak ] if conditions ( c1)-(c3 ) are satisfied , then we have weak asymptotic stability .",
    "it is straightforward to verify that for generalized linear models , scad , mcp , sigmoid penalty and poisson penalty all satisfy conditions ( c1)-(c3 ) , leading to the weak asymptotic stability .",
    "[ lemma : stability : strong ] if conditions ( c1)-(c5 ) are satisfied , then strong asymptotic stability holds .      in this subsection",
    ", we study asymptotic properties for the proposed lamp , including parameter estimation consistency , model selection consistency and asymptotic stability .",
    "suppose @xmath136 or @xmath137 is the true value of @xmath138 or @xmath139 , @xmath140 .",
    "let @xmath141 , the number of signals .",
    "without loss of generalization , we assume @xmath142 , where the true value @xmath143 has no zero element , and @xmath144 , which indicates a @xmath145 vector with each element being @xmath116 . denote @xmath146 , @xmath147 , then @xmath148 $ ] is the range of signal level .",
    "for any sequences @xmath149 we say @xmath150 if @xmath151 . here",
    ", we consider the setting of fixed @xmath152 when @xmath153 though some results may be extended to the case of @xmath154 .",
    "we first introduce certain regularity conditions which are needed for establishing asymptotic properties .    1 .",
    "@xmath155 where @xmath156 denotes the @xmath157th derivative of @xmath18 , and @xmath158 of a matrix `` @xmath159 '' denotes its minimum eigenvalue .",
    "2 .   @xmath160 at @xmath161 and @xmath162 ; @xmath163 is increasing at @xmath164 , where @xmath26 is a constant .",
    "3 .   @xmath165 , @xmath166 , and @xmath167 4 .",
    "@xmath168 [ ^t]}>.@xmath169    [ theo : lampasymp ] suppose that conditions ( c6)-(c9 ) are satisfied",
    ". then the penalized maximum likelihood estimator based on the lamp family is consistent and asymptotically normal , and achieves model selection consistency and strong asymptotic stability .",
    "[ lemma : rate ] suppose that ( c6 ) and ( c7 ) are satisfied and @xmath170=x^ul(x)$ ] , where @xmath171 is a constant and @xmath172 is negative and slowly varying at @xmath173 , i.e. , for any @xmath174 , @xmath175=1,$ ] and @xmath176",
    ". then @xmath177 implies ( c8 ) .",
    "it can be verified that the condition on @xmath170 $ ] is satisfied for the logistic , poisson and probit regression models . to achieve model selection consistency , we can choose @xmath178 and @xmath179    we now present the corresponding results for the case of sigmoid penalty . here , @xmath180 where @xmath181 $ ] is a slowly varying function .",
    "the following conditions simplify ( c6)-(c9 ) .    1 .",
    "parameter @xmath183 is a constant ( does not depend on @xmath84 ) and @xmath184 .",
    "3 .   @xmath185(1+\\rho)/\\rho.\\ ] ]    the following proposition shows that the results of theorem [ theo : lampasymp ] carry over to the penalized logistic regression when ( c6)(c8 ) are assumed .    for the penalized maximum likelihood estimator of logistic regression with the sigmoid penalty",
    ", we have parameter estimation consistency , model selection consistency and strong asymptotic stability under conditions ( c6)-(c8 ) .",
    "an important aspect of the penalized likelihood estimation method is the computational efficiency . for the lasso penalty ,",
    "@xcite proposed the path - following lars algorithm . in @xcite ,",
    "the coordinate - wise descent method was proposed .",
    "it optimizes a target function with respect to a single parameter at a time , cycling through all parameters until convergence is reached . for non - convex penalties",
    ", @xcite used the lqa approximation approach . in @xcite , a local linear approximation type method was proposed for maximizing the penalized likelihood for a broad class of penalty functions . in @xcite , the coordinate - wise descent method was implemented for non - convex penalties as well .",
    "@xcite proposed a hybrid approach of newton - raphson and coordinate descent for calculating the approximate path for penalized likelihood estimators with both convex and non - convex penalties .",
    "we apply quadratic approximation and use the coordinate decent algorithm similar to @xcite and @xcite . recall that our objective function is the negative penalized log - likelihood @xmath186 following @xcite , let @xmath187 and define the violation function @xmath188    we see that the objective function achieves its maximum value if and only if @xmath189 for all @xmath190 .",
    "thus we use @xmath191 as the stop condition of our iteration , with @xmath192 being the chosen tolerance threshold .    in each step",
    ", we use quadratic approximation to the log - likelihood function @xmath193 where @xmath194 and @xmath195 depend on the current value of @xmath92 .",
    "the algorithm is summarized as follows .    * algorithm : * set values for @xmath196 .",
    "denote by @xmath197 the @xmath198th column of @xmath199    * standardize @xmath200 * initialize @xmath201 .",
    "calculate @xmath202 and go to s3 if @xmath203 or else go to s5 .",
    "* choose @xmath204 .",
    "calculate @xmath205 @xmath206 @xmath207 if @xmath208 let @xmath209 ; else @xmath210 . *",
    "if @xmath211 else @xmath212 . calculate @xmath213 and go to s3 if @xmath203 , else go to s5 .",
    "* do the transformation of the coefficients @xmath214 due to standardization .    for s2",
    ", the initial solution @xmath215 can be taken as the zero solution or the mle or the estimate calculated using a parameter @xmath216 from the previous steps .    for s4",
    ", we first carry out the iterations for the variables in the current active set until convergence , then check whether additional variables should join the active set .",
    "alternatively , we may speed up the calculation by using  warm start \" .",
    "readers are referred to @xcite and @xcite for details of the strategies to speed up calculation in coordinate descent algorithms .",
    "for example , the logistic regression with sigmoid penalty is , @xmath217 @xmath218 where @xmath219,$ ] for @xmath220 we define @xmath221 , and @xmath222 . in s3",
    ", we have the following two different approximation methods for updating .",
    "1 .   quadratic approximation from irls @xcite .",
    "let @xmath223 @xmath224 @xmath225 @xmath226 and @xmath227 where @xmath228 .",
    "2 .   quadratic approximation using taylor expansion .",
    "let @xmath229 and @xmath230,$ ] where @xmath231 $ ] and @xmath232 is the component - wise product operator .",
    "for an initial estimator @xmath215 , denote by @xmath233 a quadratic approximation of @xmath234 at @xmath215 , i.e. , @xmath235    let @xmath236 be a closed set and the objective function @xmath237 is strictly convex .",
    "in addition , assume the quadratic approximation at @xmath215 satisfies @xmath238 for all @xmath239 .",
    "then the algorithm constrained in @xmath240 ( minimization within @xmath240 ) converges to the true minimum @xmath241 .",
    "in addition , method 1 for the logistic regression satisfies the conditions on quadratic approximation .",
    "[ theo : algorithmconverge ]      an important issue is how to choose tuning parameters in the penalized likelihood estimation . for the lamp family , there are three tuning parameters , i.e. , @xmath33 , @xmath35 and @xmath34 .",
    "our numerical experiences show that the resulting estimator is not sensitive to the choice of @xmath34 . in most cases , we may simply take @xmath70 or @xmath71 , depending on the type of regression ( see remark [ remark::alpha_1_choice ] ) . for @xmath33 and @xmath35",
    ", we recommend using cross - validation or bic , so long as the solutions are stable enough . the @xmath242 package described in @xcite to determine a stable area or perform local diagnosis",
    "is recommended .",
    "there are two approaches to get a stable area : to control the smoothness of the @xmath33-estimate curve and calculate the smallest eigenvalue of the penalized likelihood at each point of the path as stated in theorem 1 . here , we take the second approach in all numerical analysis .",
    "our algorithm differs from @xmath242 in the following two aspects : we use the  viol \" function as the convergence criteria ; we do not use the adaptive - scale .",
    "both algorithms 1 and 2 use the linear approximation ( suppose at @xmath243 ) of the penalty term .",
    "@xmath244 for algorithm 1 , from concavity , we have @xmath245 which naturally falls into the mm - algorithm framework .",
    "to choose the @xmath246 pair , we use the hybrid approach introduced by @xcite , i.e. , combining bic , cross - validation , and convexity diagnostics . for",
    "a path of solutions with a given value of @xmath35 large enough , use aic / bic to select @xmath33 and use the convexity diagnostics to determine the locally convex regions of the solution path .",
    "if the chosen solution lies outside the stable region , we lower @xmath35 to make the penalty more convex .",
    "after this process is iterated multiple times , we can find a value of @xmath35 that produces a good balance between sparsity and convexity .",
    "then we can fix @xmath35 and use bic or cross - validation to choose the best @xmath33 .",
    "simulation studies cover logistic , poisson and probit cases .",
    "the performance of the lamp family is compared with those of lasso , scad and mcp .",
    "particular attention is given to the logistic regression to demonstrate how sparsity and stability are properly balanced .",
    "two classification examples from microarray experiments involving cancer patients are presented .",
    "we simulate from the logistic regression model with @xmath247 , @xmath248 , @xmath249 , @xmath250 , and @xmath251 , where @xmath252 with @xmath253 .",
    "the number of replications is 100 for this and all the subsequent simulations .",
    "table 1 reports true positive ( tp ) , false positve ( fp ) , proportion of correct fit ( cf ) , proportion of over fit ( of ) , proportion of under fit ( uf ) , @xmath254 ( l1 loss ) , and @xmath255 ( l2 loss ) . to compare performances among lasso , scad , mcp and the sigmoid penalty , we use @xmath256 to calculate the lasso solution path , @xmath242 to calculate the scad and mcp . for all penalties ,",
    "we use ebic @xcite to choose the tuning parameter @xmath33 with other parameters fixed . the ebic parameter @xmath257 .    from table [ tab : logisticcv ] , it is clear that the sigmoid penalty outperforms scad and mcp in the sense that at a similar level of tp , the sigmoid penalty results in a smaller fp .",
    "somewhat surprisingly the lasso has a competitive performance , which may be attributed to the use of the ebic selection criterion .    .",
    "simulation results for model selection and estimation under different penalties .",
    "entries are mean values with the standard error in parentheses . here  sig \" represents the sigmoid penalty .",
    "the number inside the parentheses following by the penalty name represents the concavity parameter .",
    "[ tab : logisticcv ] [ cols=\"<,<,<,<,<,<,<,<\",options=\"header \" , ]",
    "penalty based regularization methods have received much attention in recent years .",
    "this paper proposes a family of penalty functions ( lamp ) that is adaptive and specific to the shapes of the log - likelihood functions .",
    "the proposed lamp family is different from the well - known lasso , scad and mcp .",
    "it can be argued that the new approach provides a good balance between sparsity and stability , two important aspects in model selection .",
    "it is shown that the resulting penalized estimation achieves model selection consistency and strong asymptotic stability , in addition to the usual consistency and asymptotic normality .",
    "an important issue is how to choose the three parameters imbedded in a lamp .",
    "the `` location '' parameter @xmath34 can be chosen in an obvious way for the standard generalized linear models , while @xmath33 , which represents the penalty level , can be chosen through standard cv or information criteria . for @xmath35 , which controls the concavity level , it is computationally intensive to use cv .",
    "it is desirable to develop more effective ways to select @xmath35 .",
    "it is also important to study the stability of the solution path .",
    "the lamp approach can be modified to handle grouped variable selection .",
    "it will also be of interest to develop parallel results for semiparametric regression models .",
    "we first state a general result about estimation consistency , model selection consistency and asymptotic normality .",
    "consider the penalized log - likelihood function @xmath258 as defined by .",
    "let @xmath259 be the true value of @xmath260 .",
    "recall @xmath261 is the nonzero part of @xmath262 and @xmath263 . for notational simplicity ,",
    "let    let @xmath264 , @xmath265 be i.i.d . with density @xmath266 that satisfies the following regularity conditions ( see @xcite ) :    1",
    ".   @xmath267=0,$ ]",
    "@xmath268=- \\mathrm{e}_{{\\boldsymbol{\\theta}}}[\\frac{\\partial^2\\log f({\\boldsymbol{z } } , { \\boldsymbol{\\theta}})}{\\partial\\theta_j\\partial\\theta_k}],\\]]where @xmath269 ; 2 .   @xmath270^{\\bigotimes",
    "2}<\\infty;\\ ] ] 3 .",
    "there exist functions @xmath271 , a neighborhood @xmath272 of @xmath120 such that @xmath273 and @xmath274 for all @xmath275 , @xmath190 , @xmath157 and @xmath24 .",
    "note that for the generalized linear models , ( c10)-(c12 ) are satisfied under mild assumptions on covariates .",
    "@xmath276,1\\leq j\\leq q}|p'_{\\lambda , j}(\\beta)| ; \\ ; p_{2,n}(\\beta)\\triangleq\\inf_{q < j\\leq p}p'_{\\lambda , j}(\\beta);\\ ; p_{2,n}\\triangleq p_{2,n}(0)\\ ] ] @xmath277,1\\leq j\\leq q}|p''_{\\lambda , j}(\\beta)|;\\ ] ] @xmath278,1\\leq j\\leq q}p''_{\\lambda , j}(\\beta);\\ ; { \\boldsymbol{\\sigma}}_1= \\mathrm{diag}\\{p''_{\\lambda,1}(|\\beta_{10}|),\\cdots , p''_{\\lambda , q}(|\\beta_{q0}|)\\};\\ ] ] @xmath279 let @xmath280 be the fisher information matrix at @xmath120 partitioned by the intercept , the nonzero and zero parts and @xmath281 for @xmath282 , define @xmath283 let @xmath284 be the @xmath173 norm of a matrix @xmath285 .",
    "suppose that both @xmath286 and @xmath287 are strictly convex , satisfying ( c10)-(c12 ) , and that @xmath288 are continuous at 0 .",
    "assume there exist @xmath289 such that @xmath290 , the k - th derivative of @xmath291 is continuous for @xmath292 $ ] and exists at the limit @xmath293 , @xmath294.\\ ] ] @xmath295 for @xmath296 , @xmath297 then , we have the following results .    1 .",
    "( parameter estimation consistency ) if either of the following two conditions , ( 1.a ) and ( 1.b ) , holds , then there exists a consistent local maximizer @xmath298 * @xmath299 as @xmath153 . *",
    "@xmath300 and @xmath301 2 .",
    "( model selection consistency ) if any of the following three conditions , ( 2.a)-(2.c ) , holds , then any consistent local maximizer is model selection consistent . * as @xmath302 , @xmath303 and for any @xmath304 , @xmath305 .",
    "* there exists @xmath306 such that @xmath307 and @xmath308 ; @xmath309 ; and @xmath310 * for any @xmath311 , @xmath312 3 .",
    "( asymptotic normality ) assume the estimator has the model selection consistency as stated in 2 .",
    "if @xmath313 , we have the asymptotic normality for the nonzero part , @xmath314    [ lemma : oracle ]    lemma [ lemma : oracle ] covers commonly used penalties including lasso , scad , mcp , adaptive lasso , hard thresholding , bridge and the lamp family . note that the conditions are satisfied for scad , and are slightly weaker than those imposed in @xcite .",
    "it is also easy to verify that condition ( 2.a ) is satisfied for scad and mcp , ( 2.b ) for lasso and ( 2.c ) for bridge .",
    "suppose contrary to ( 2.a ) , @xmath315 and @xmath316 then , the resulting estimator @xmath317 does not have the model selection consistency .",
    "for any @xmath318 we have @xmath319 , @xmath320 .",
    "select @xmath321 define function @xmath322    throughout the proof , we use the following conventions : @xmath323 denotes a quantity approaching 0 as @xmath117 and @xmath324 simultaneously ; @xmath325 for approaching 0 as @xmath117 ; @xmath326 for approaching 0 as @xmath324 with @xmath84 fixed , and @xmath327 for a bounded sequence with @xmath328 if a subscript @xmath329 is used , then the convergence ( boundedness ) is in probability .",
    "second , from condition ( c1 ) , the @xmath332 are convex . we will show that for any @xmath333 $ ] , @xmath334 , and @xmath320 , @xmath335 since @xmath336 is a local minimum of @xmath337 , there exists @xmath338 such that @xmath339 where the last inequality follows from the convexity of @xmath340 .",
    "therefore , @xmath341 which , in view of condition ( c3 ) , implies for @xmath342 .",
    "the case of @xmath343 can be proved similarly .",
    "third , @xmath344 from the law of large number .",
    "note that @xmath345\\\\\\nonumber&+&[m_n({\\mathcal{z}}_n,{\\boldsymbol{u}}'_{2,n})-f_2({\\boldsymbol{u}}'_{2,n } ) ] \\\\\\nonumber     & - & [ \\mathrm{e}m_n({\\mathcal{z}}_n,{\\boldsymbol{u}}_{1,n})- m_n({\\mathcal{z}}_n,{\\boldsymbol{u}}_{1,n})]\\\\\\nonumber&-&[m_n({\\mathcal{z}}_n,{\\boldsymbol{u}}_{1,n } ) -f_1({\\boldsymbol{u}}_{1,n})]\\\\\\nonumber     & + & [ f_2({\\boldsymbol{u}}'_{2,n})-f_1({\\boldsymbol{u}}'_{2,n})]+[f_1({\\boldsymbol{u}}'_{2,n})-f_1({\\boldsymbol{u}}_{1,n})]\\\\\\nonumber     & \\geq&o_p(1,n)+o(1,\\epsilon;n)o_p(1,n)= o_p(1,\\epsilon , n).\\nonumber\\end{aligned}\\ ] ] in other words , for any @xmath346 , @xmath347<-\\delta)=0.\\ ] ] similarly , @xmath348<-\\delta)=0.\\ ] ] combine these two together , we have , for any @xmath346 and @xmath349 $ ] , @xmath350<-\\delta)=0.\\ ] ] define @xmath351,\\ ] ] and @xmath352 since @xmath98 is compact , there exists @xmath353 such that @xmath354 exists for any @xmath355 $ ] .",
    "since @xmath356 is strictly convex , @xmath357 as @xmath358 and @xmath359 we thus conclude from ( [ eqn : lemma2delta ] ) that for any @xmath346 @xmath360 by the definition of @xmath354 , for @xmath361 , @xmath362 therefore , for any @xmath363 , @xmath364 since @xmath346 implies @xmath365 and @xmath366 implies @xmath367 , @xmath368 can be arbitrarily small .",
    "thus , for any @xmath369 , @xmath370    from condition ( c2 ) and similar to the proof of lemma [ lemma::stability : weak ] , @xmath371}{{\\boldsymbol{\\theta}}_i\\in{\\boldsymbol{\\theta}},i=1,2}}\\large\\{[r m_n({\\mathcal{z}}_n+{\\mathcal{e}}_n,{\\boldsymbol{\\theta}}_1)+(1-r ) m_n({\\mathcal{z}}_n+{\\mathcal{e}}_n,{\\boldsymbol{\\theta}}_2 ) \\\\     & - & m_n({\\mathcal{z}}_n+{\\mathcal{e}}_n , r{\\boldsymbol{\\theta}}_1+(1-r){\\boldsymbol{\\theta}}_2)]\\\\&- & [ r m_n({\\mathcal{z}}_n,{\\boldsymbol{\\theta}}_1)+(1-r ) m_n({\\mathcal{z}}_n,{\\boldsymbol{\\theta}}_2 ) \\\\",
    "& - & m_n({\\mathcal{z}}_n , r{\\boldsymbol{\\theta}}_1+(1-r){\\boldsymbol{\\theta}}_2)]\\large\\}\\\\     & = & o_p(1,n)o(1,\\epsilon;n).\\end{aligned}\\ ] ] in view of this and condition ( c5 ) , there exists @xmath372 such that if @xmath373 then @xmath374    on the other hand , ( c4 ) and weak asymptotic stability imply @xmath375 @xmath376 thus , @xmath377 denote by @xmath378 and @xmath100 the minimizers of @xmath379 and @xmath93 over @xmath380 , respectively .",
    "we have @xmath381 thus @xmath382 which , along with strict convexity of @xmath379 as in ( [ eqn : lemmastrongstability : strictconvex ] ) , implies @xmath383 combining and , we have @xmath384        since @xmath394 is a slowly - varing function , we have @xmath395 or @xmath396 .",
    "thus , a sufficient condition will be @xmath397 . then taking @xmath398-th power on both slides , we have @xmath399^{1/u}\\ll \\lambda_0 $ ] , which can be guaranteed by the assumption @xmath400 .",
    "part 1 ( parameter estimation consistency ) .",
    "+ it suffices to show that for any @xmath318 there exists @xmath401 such that @xmath402>1-\\epsilon.\\ ] ] this is because , under ( [ eqn : para - consistency ] ) , there exists a local maximizer @xmath403 such that @xmath404 , where @xmath405 .",
    "therefore , consistency holds as @xmath406 .",
    "we now deal with the four terms on right - hand side of . from condition ( 1.a ) ,",
    "the first term is of order @xmath411 and the second term is of order @xmath412 . by the cauchy inequality , @xmath413 since @xmath414",
    "is fixed , it is also of the same or a smaller order compared to the second term . as @xmath415 , @xmath416 in the fourth term vanishes .",
    "thus the fourth term is also controlled by the second term .",
    "regarding the constant involving @xmath417 , the second term contains @xmath418 , while both the first and third terms contain @xmath419 .",
    "consequently , the whole expression is controlled by its second term as long as we choose a sufficiently large @xmath420 , noting that @xmath421 is positive definite .      .",
    "+ first of all , from any of condition ( 2.a ) , ( 2.b ) , and ( 2.c ) , we have for @xmath84 sufficiently large , @xmath422 .",
    "assume we have an @xmath423-consistent local minimizer @xmath424 .",
    "if the model selection consistency does not hold , then there exists a @xmath425 such that @xmath426",
    ". this will result in contradiction if we can show that there exist @xmath427 and a neighborhood of @xmath428 , @xmath429 , within which the sign of @xmath430 does not change . since @xmath431 is a solution , the sign of left derivative should be different from the sign of right derivative at this value .",
    "so the non - zero @xmath431 does not exist .",
    "taking the taylor expansion , we get @xmath432 for @xmath433 we see that the third term on the right - hand side of does not depend on @xmath434 and is only related to the sign of @xmath138 , which remains constant in @xmath435 since @xmath436 .",
    "so if the first two terms are controlled by the third one , we can derive the sparsity using the method above .",
    "the coefficient of the sign function in the third term should be positive to control the direction of the derivative .",
    "that is , @xmath437    under condition ( 2.a ) , the orders of the three terms in are @xmath438 @xmath439 where @xmath440 is the sequence given in condition ( 2.a ) .",
    "condition ( 2.a ) guarantees that the first two terms are controlled by the third one .",
    "likewise , condition ( 2.c ) leads to the same conclusion .      since @xmath443 , we have @xmath444 @xmath445 and , for any @xmath446 , @xmath447 the above results , together with @xmath448 imply that @xmath449\\\\\\nonumber & = & \\tilde{l}({\\boldsymbol{\\theta}}_0)+np_{1,n}^2\\big[-\\frac{1}{2}{\\boldsymbol{v}}{\\mathcal{r}}{\\boldsymbol{v}}^t-\\sum_{j=1}^q\\mbox{sgn}(\\beta_{j,0})\\frac{p'_{\\lambda , j}(|\\beta_{j,0}|)}{p_{1,n } } v_j\\\\\\nonumber & & -\\sum_{j = q+1}^p\\frac{p'_{\\lambda , j}(0)}{p_{1,n}}|v_j|\\big](1+o_p(1)).\\nonumber\\end{aligned}\\ ] ]    as @xmath450 is an maximizer given by , there exists @xmath451 such that for any given @xmath318 there exists a constant @xmath306 such that @xmath452 from @xmath453(1+o_p(1)),\\ ] ] we get @xmath454 thus we have @xmath455 now , we investigate the minimizer of @xmath456 .",
    "let @xmath457 where @xmath458 is a @xmath459 vector .",
    "we see @xmath460 in probability is a necessary condition for sparsity . from conclusions above , @xmath461\\xrightarrow{~p~}0,\\end{aligned}\\ ] ]",
    "where @xmath462 @xmath463 @xmath464 and @xmath465    kkt conditions lead to the following equations .",
    "@xmath466 or @xmath467 where @xmath468 and @xmath469 therefore @xmath470 then @xmath471.\\end{aligned}\\ ] ] from ( [ eqn::conditionsparsity2 ] ) , @xmath472 .",
    "so with @xmath473 a sufficient condition for sparsity is that @xmath474 is controlled by @xmath475 coordinatewisely .",
    "that is , @xmath476 which is similar to the last equation of ( [ eq::proof - lemma-4:kkt ] ) and equivalent to @xmath477 , which is implied by @xmath478 .",
    "furthermore , it can be guaranteed by @xmath479 from , it follows that the first two terms are controlled by the third one .",
    "part 3 ( asymptotic normality and oracle property ) .",
    "+ suppose @xmath480 is the local minimizer , noting that @xmath481 with probability tending to 1 .",
    "using parameter estimation consistency and model selection consistency property , for @xmath482 @xmath483 .",
    "\\nonumber\\end{aligned}\\ ] ] thus @xmath484     \\longrightarrow n\\left[0,\\left (                                                        \\begin{array}{cc }                                                          { \\mathcal{r}}_{00 } & { \\mathcal{r}}_{01 } \\\\                                                          { \\mathcal{r}}_{10 } & { \\mathcal{r}}_{11 } \\\\                                                        \\end{array }                                                      \\right)\\right].\\end{aligned}\\ ] ] if the order of @xmath485 is controlled by @xmath486 , the oracle property holds .",
    "given @xmath311 , from @xmath501 we get @xmath502 from @xmath503 we get @xmath504 . together with the smoothness of the function @xmath18 , @xmath505 from condition ( c8 ) ,",
    "it is obvious that @xmath506 condition ( 2.a ) and conditions for asymptotic normality in lemma [ lemma : oracle ] hold .",
    "lastly , ( c5 ) is implied by ( c9 ) .    in conclusion , with ( c6)@xmath507(c9 ) ,",
    "the penalized maximum likelihood estimator based on the lamp family is consistent and asymptotically normal , and achieves model selection consistency and strong asymptotic stability .",
    "the idea of the proof is adapted from @xcite and @xcite . for convenience , we define the following notations .",
    "@xmath508 where @xmath509 are the iteration times of @xmath510 respectively and @xmath511 . from the concavity of the penalty on the positive part",
    ", we have @xmath512 where @xmath513 is a @xmath514-vector with the @xmath190-th element 1 and all the others 0 ; from conditions of the theorem , we see @xmath515 and @xmath516 in the algorithm , @xmath517 then @xmath518 since @xmath519 decreases as iteration continues and has an lower bound , it converges .    by monotonicity of @xmath520 (",
    "@xmath521 represents the number of iterations ) , all points @xmath522 are in a compact set @xmath523 it is compact because @xmath524 is continuous and coercive .",
    "then there exists a convergent subsequence @xmath525 , @xmath526 @xmath527 such that @xmath528 next let @xmath529 for any @xmath530 , we have : @xmath531 assume @xmath532 .",
    "taking limit @xmath533 on both sides of the equation above , we have @xmath534 thus the subgradient of @xmath535 at @xmath536 is 0 , which is exactly the derivative of @xmath524 with respect to @xmath537 at @xmath538 because it can be easily verified that the smooth approximation keeps the first - order derivative the same .    from the algorithm and the definition of the `` viol '' function ,",
    "@xmath539 taking the derivative of both sides of the equation above , the subgradient of @xmath540 at @xmath536 is 0 , which is exactly the same as the partial derivative of @xmath524 with respect to @xmath541 at @xmath542 from strict convexity , @xmath100 is the unique local minimum of @xmath524 .",
    "akaike , h. ( 1973 ) .",
    "information theory and an extension of the maximum likelihood principle .",
    "_ in : _ b.n.petrov and f.csaki , eds .",
    "_ second international symposium on information theory .",
    "_ budapest : akademiai kiado , pp .",
    "267 - 281 .",
    "gordon , g. j. , jensen , r. v. , hsiao , l .- l . , gullans , s. r. , blumenstock , joshua e. , ramaswamy , sridhar , richards , william g. , sugarbaker , d. j. and bueno , r. ( 2002 ) .",
    "translation of microarray data into clinically relevant cancer diagnostic tests using gene expression ratios in lung cancer and mesothelioma . _ cancer research _ , * 62 * 4963 - 4967 .",
    "munk , a. , bissantz , n. , dumbgen , l. and stratmann , b. ( 2006 ) .",
    "convergence analysis of generalized iteratively reweighted least squares algorithms on convex function spaces .",
    "_ siam journal on optimization _ , * 19 * , 1828 - 1845 .",
    "singh , d. , febbo , p. , ross , k. , jackson , d. , manola , j. , ladd , c. , tamayo , p. , renshaw , a. , damico , a. and richie , j. ( 2002 ) .",
    "gene expression correlates of clinical prostate cancer behavior .",
    "_ cancer cell _ * 1 * , 203 - 209 ."
  ],
  "abstract_text": [
    "<S> a new family of penalty functions , adaptive to likelihood , is introduced for model selection in general regression models . </S>",
    "<S> it arises naturally through assuming certain types of prior distribution on the regression parameters . to study stability properties of the penalized maximum likelihood estimator , </S>",
    "<S> two types of asymptotic stability are defined . </S>",
    "<S> theoretical properties , including the parameter estimation consistency , model selection consistency , and asymptotic stability , are established under suitable regularity conditions . </S>",
    "<S> an efficient coordinate - descent algorithm is proposed . </S>",
    "<S> simulation results and real data analysis show that the proposed method has competitive performance in comparison with existing ones . </S>"
  ]
}