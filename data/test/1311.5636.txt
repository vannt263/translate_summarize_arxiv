{
  "article_text": [
    "feature selection is an important aspect in the implementation of machine learning methods .",
    "the appropriate selection of informative features can reduce generalisation error as well as the storage and processing requirements for large datasets .",
    "in addition , parsimonious models can provide valuable insight into the relations underlying elements of the process under examination .",
    "there is a wealth of literature on the subject of feature selection when the relationship between variables is linear .",
    "unfortunately when the relation is non - linear feature selection becomes substantially more nuanced .",
    "kernel methods excel in modelling non - linear relations .",
    "unsurprisingly , a number of kernel - based feature selection algorithms have been proposed .",
    "early propositions , such as recursive feature elimination(rfe ) [ 1 ] can be computationally prohibitive , while attempts to learn a convex combination of low - rank kernels may fail to encapsulate nonlinearities in the underlying relation .",
    "recent approaches using explicit kernel approximations can capture non - linear relations , but increase the storage and computational requirements .",
    "the successful use of a kernel - based feature selection methods is a matter of balance .",
    "our approach makes extensive use of _ kernel target alignment _ ( kta ) [ 2,3 ] .",
    "work in [ 4 ] provides the foundation of using the alignment of centred kernel matrices as the basis for measuring statistical dependence .",
    "the hilbert - schmidt independence criterion is the basis for further work in [ 5 ] , where greedy optimisation of centred alignment is employed for feature selection .",
    "additionally , [ 5 ] identifies numerous connections with other existing feature selection algorithms which can be considered as instances of the framework .",
    "stability selection [ 6 ] is a general framework for variable selection and structure estimation of high dimensional data .",
    "the core principle of stability selection is to combine subsampling with a sparse variable selection algorithm . by repeated estimation over a number of different subsamples",
    ", the framework keeps track of the number of times each variable was used , thus maintaining an estimate for the importance of each feature .",
    "more importantly , stability selection provides finite sample control for some error rates of false discoveries and hence a principled approach for parameter selection . in this work",
    ", we propose a synthesis of the two aforementioned approaches through a randomised feature selection algorithm based on estimating the statistical dependence between bootstrapped random subspaces of the dataset in rkhs .",
    "the dependence estimation of random subsets of variables is similar to the approach of [ 13 ] , which is extended through bootstrapping and carefully controlled feature set sizes .",
    "this approach is simple to implement and compares favourably with other methods in terms of scalability .",
    "the rest of the paper is structured as follows : _ section 2 _ presents the necessary background on feature selection for kernel - based learning .",
    "_ section 3 _ introduces a basic randomised algorithm for nonlinear feature selection , along with some simple examples , while _ section 4 _ provides some analysis .",
    "extensive experimentation on real and artificial data in _ section 5 _ concludes this paper .",
    "we consider the supervised learning problem of modelling the relationship between a @xmath0 input matrix @xmath1 and a corresponding @xmath2 output matrix @xmath3 .",
    "the simplest instance of such a problem is binary classification where the objective is the learning problem is to learn a function @xmath4 mapping input vectors @xmath5 to the desired outputs @xmath6 . in the binary case",
    "we are presented with a @xmath7 matrix @xmath1 and a vector of outputs @xmath6 , @xmath8 limiting the class of discrimination functions to linear classifiers we wish to find a classifier @xmath9    the linear learning formulation can be generalised to the nonlinear setting through the use of a nonlinear feature map @xmath10 , leading to the kernelized formulation : @xmath11    the key quantities of interest in our approach is the centred kernel target alignment which is defined as : @xmath12    the matrices @xmath13 and @xmath14 correspond to centred kernels on the features @xmath1 and outputs @xmath3 and are computed as :    @xmath15 k \\left [ i-{{11^t}\\over m } \\right]\\ ] ]    where @xmath16 , in the above equation denotes the m - dimensional vector with all entries set equal to one .",
    "the approach we will take will be based on the following well - known observation that links kernel target alignment with the degree to which an input space contains a linear projection that correlates with the target .",
    "let @xmath17 be a probability distribution on the product space @xmath18 , where @xmath19 has a projection @xmath20 into a hilbert space @xmath21 defined by a kernel @xmath22 .",
    "we have that @xmath23 } = & & \\\\ & & \\hspace{-5cm}= \\sup_{{\\mathbf{w } } : \\|{\\mathbf{w}}\\| \\leq 1 } { \\mathbb{e}}_{({\\mathbf{x}},y)\\sim p}[y\\langle{\\mathbf{w}},\\phi({\\mathbf{x}})\\rangle]\\end{aligned}\\ ] ]    * proof : * @xmath24 = & & \\\\ & & \\hspace*{-5cm}= \\sup_{{\\mathbf{w } } : \\|{\\mathbf{w}}\\| \\leq 1 }",
    "\\left\\langle{\\mathbf{w}},{\\mathbb{e}}_{({\\mathbf{x}},y)\\sim p}[\\phi({\\mathbf{x } } ) y]\\right\\rangle \\\\ & & \\hspace*{-5cm}= \\left\\| { \\mathbb{e}}_{({\\mathbf{x}},y)\\sim p}[\\phi({\\mathbf{x } } ) y]\\right\\|\\\\ & & \\hspace*{-5cm}=\\sqrt { \\int \\int dp({\\mathbf{x}},y)dp({\\mathbf{x}}',y ' ) \\langle \\phi({\\mathbf{x}}),\\phi({\\mathbf{x}}')\\rangle   yy'}\\\\ & & \\hspace*{-5cm}= \\sqrt{{\\mathbb{e}}_{({\\mathbf{x}},y ) \\sim p , ( { \\mathbf{x } } ' , y')\\sim p}[y y ' \\kappa({\\mathbf{x}},{\\mathbf{x } } ' ) ] } \\end{aligned}\\ ] ]    the proposition suggests that we can detect useful representations by measuring kernel target alignment . for non - linear functions the difficulty is to identify which combination of features creates a useful representation .",
    "we tackle this problem by sampling subsets @xmath25 of features and assessing whether on average the presence of a particular feature @xmath26 contributes to an increase @xmath27 in the average kernel target alignment . in this way we derive an empirical estimate of a quantity",
    "we will term the contribution .    the _ contribution _",
    "@xmath27 of feature @xmath26 is defined as @xmath28\\right ] -   { \\mathbb{e}}_{s ' \\sim { \\mathcal{s}}_{\\setminus i}}\\left[{\\mathbb{e}}_{({\\mathbf{x}},y ) \\sim p , ( { \\mathbf{x } } ' , y')\\sim p}[y y ' \\kappa_{s'}({\\mathbf{x}},{\\mathbf{x}}')]\\right],\\ ] ] where @xmath29 denotes the ( non - linear ) kernel using features in the set @xmath25 ( in our case this will be a gaussian kernel with equal width ) , @xmath30 the uniform distribution over sets of features of size @xmath31 that include the feature @xmath26 , @xmath32 the uniform distribution over sets of features of size @xmath33 that do not contain the feature @xmath26 , and @xmath34 is the number of features .    note that the two distributions over features @xmath30 and @xmath32 are matched in the sense that for each @xmath25 with non - zero probability in @xmath32 , @xmath35 has equal probability in @xmath30 .",
    "this approach is a straightforward extension of the idea of bahsic [ 5 ] .",
    "we will show that for variables that are independent of the target this contribution will be negative . on the other hand ,",
    "provided there are combinations of variables including the given variable that can generate significant correlations then the contribution of the variable will be positive .",
    "we will define an _ irrelevant _ feature to be one whose value is statistically independent of the label and of the other features .",
    "we would like an assurance that irrelevant features do not increase alignment .",
    "this is guaranteed for the gaussian kernel by the following result .",
    "let @xmath17 be a probability distribution on the product space @xmath18 , where @xmath19 has a projection @xmath36 into a hilbert space @xmath21 defined by the gaussian kernel @xmath29 on a set of features @xmath25 .",
    "suppose a feature @xmath37 is irrelevant .",
    "we have that @xmath38 \\leq { \\mathbb{e}}_{({\\mathbf{x}},y ) \\sim p , ( { \\mathbf{x } } ' , y')\\sim p}[y y ' \\kappa_{s}({\\mathbf{x}},{\\mathbf{x}}')]\\ ] ]    * proof ( sketch ) : * since the feature is independent of the target and the other features , functions of these features are also independent .",
    "hence , @xmath39 & & = { \\mathbb{e}}_{({\\mathbf{x}},y ) \\sim p , ( { \\mathbf{x } } ' , y')\\sim p}[y y ' \\kappa_{s}({\\mathbf{x}},{\\mathbf{x}}')\\exp(- \\gamma ( x_i - x'_i)^2)]\\\\ & & \\hspace*{-2.5cm}= { \\mathbb{e}}_{({\\mathbf{x}},y ) \\sim p , ( { \\mathbf{x } } ' , y')\\sim p}[y y ' \\kappa_{s}({\\mathbf{x}},{\\mathbf{x}}')]{\\mathbb{e}}_{({\\mathbf{x}},y ) \\sim p , ( { \\mathbf{x } } ' , y')\\sim p}[\\exp(- \\gamma ( x_i - x'_i)^2)]\\\\ & & \\hspace*{-2.5cm}= { \\mathbb{e}}_{({\\mathbf{x}},y ) \\sim p , ( { \\mathbf{x } } ' , y')\\sim p}[y y ' \\kappa_{s}({\\mathbf{x}},{\\mathbf{x}}')]\\alpha\\end{aligned}\\ ] ] for @xmath40 \\leq 1 $ ] .    in fact the quantity @xmath41 is typically less than 1 so that adding irrelevant features decreases the alignment . our approach will be to progressively remove sets of features that are deemed to be irrelevant , hence increasing the alignment together with the signal to noise ratio for the relevant features .",
    "figure [ xor ] shows how progressively removing features from a learning problem whose output is the xor function of the first two features both increases the alignment contributions and helps to highlight the two relevant features .",
    "t        we now introduce our definition of a relevant feature .",
    "a feature @xmath26 will be termed @xmath42-_influential _ when its contribution @xmath43 .",
    "so far have only considered expected alignment . in practice we must estimate this expectation from a finite sample .",
    "we will omit this part of the analysis as it is a straigthforward application of u - statistics that ensures that with high probability for a sufficiently large sample from @xmath30 and @xmath32 and of samples from @xmath17 ( whose sizes depend on @xmath42 , @xmath44 , the number @xmath45 of @xmath42-influential variables and the number @xmath46 of iterations ) an empirical estimate of the contribution of an @xmath42-influential variable will with probability at least @xmath47 be greater than 0 for all of the fixed number @xmath46 of iterations of the algorithm .",
    "our final ingredient is a method of removing irrelevant features that we will term culling . at each iteration of the algorithm",
    "the contributions of all of the features are estimated using the required sample size and the contributions are sorted .",
    "we then remove the bottom 25% of the features in this ordering .",
    "our main result assures us that culling will work under the assumption that the irrelevant variables are independent .",
    "fix @xmath48 .",
    "suppose that there are @xmath45 @xmath49-influential variables and all other variables are irrelevant .",
    "fix @xmath50 and number @xmath46 of iterations . given sufficiently many samples",
    "as described above the culling algorithm will with probability at least @xmath47 remove only irrelevant variables will be removed .",
    "* proof ( sketch ) : * each irrelevant variable has expected contribution less than the contributions of the all the influential features .",
    "hence , with high probability at least 30% of these features will have lower contributions than all the influential features .",
    "hence , the bottom 25% will all be irrelevant .",
    "we now define our algorithm for randomised selection ( randsel ) . given a @xmath0 input matrix @xmath1 and corresponding output matrix @xmath3 , randsel proceeds by estimating the individual contribution of features by estimating the alignment of a number of random subsamples that include @xmath51 and @xmath52 randomly selected features .",
    "this leads to an estimate for the expected alignment contribution of including a feature .",
    "the algorithm is parametrized by the number of bootstraps @xmath53 , a bootstrap size@xmath54 and a percentage @xmath55 of features that are dropped after @xmath53 bootstraps .",
    "the algorithm proceeds iteratively until only two features remain .",
    "optionally the algorithm can be further parametrized by permanently including features which were ranked in the top percentile @xmath56 on at least a number @xmath57 occasions .",
    "this option enhances the probability of detecting non - linear dependencies between variables , should they be present .",
    "there are a number of benefits to this approach , aside from the tangible probabilistic guarantees .",
    "randsel scales gracefully . considering the computation of a kernel @xmath58 for samples @xmath59 atomic ,",
    "the number of kernel computations for a single iteration are @xmath60 , which for a sensible choice of @xmath53 can be substantially smaller than the @xmath61 complexity of hsic variants .",
    "for example setting @xmath62 and @xmath63 an iteration would require @xmath64 kernel element computations , and in addition this process is trivial to parallelize .",
    "input data @xmath1 , labels @xmath3 , number of iterations @xmath65 subsample size @xmath66 , number of features @xmath34 , drop percentile proportion @xmath67 , top percentile proportion @xmath68 , number of occasions @xmath57",
    "@xmath69 = random subsample of size @xmath66 over @xmath70 randomly selected variables @xmath71= alignment(@xmath72 ) @xmath73 = random subsample of size @xmath66 over @xmath52 randomly selected variables @xmath74= alignment(@xmath75 ) mean contribution @xmath27 = mean ( @xmath76 ) - mean(@xmath71 ) , where @xmath77 and @xmath78 drop the @xmath67% bottom - contributing features save the @xmath68% top - contributing features fix feature @xmath79 return sequence of estimated contributions and fixed variables",
    "in this section , we present several experiments comparing our feature selection approach to a number of competing methodologies .",
    "we have used three synthetic datasets in order to better illustrate the performance characteristics of these algorithms before proceeding to experiments on real data arising in infectious disease profiling .      in both synthetic and real datasets",
    "we used nested 10-fold cross validation to perform feature selection , and repeated the simulations on three different reshuffles of the dataset to account for variance . for every iteration we estimate the validation error after feature selection before proceeding to test on the held out test - set",
    "the inner cross - validation loop determines the number of features to use in classifying the test - set for optimal accuracy . if two or more models are tied in terms of performance , the more parsimonious model is preferred .",
    "we compare our proposed approach to kernel based algorithms like * rfe * , * fohsic * and * bahsic * , as well as a filtering approach relying on * correlation coefficients * and * stability selection * using the lasso as the underlying sparse selection algorithm . the same range of gaussian kernel bandwidths was explored in all algorithms and the resulting final classifiers employed a regularisation parameter of @xmath80 .",
    "we generated three synthetic datasets in order to carefully illustrate the properties of the different feature selection algorithms .",
    "all three synthetic datasets contain 300 samples with a dimensionality of 100 features .",
    "the linear and non - linear weston datasets were generated according to [ 7 ] , and consist of 5 relevant and 95 noise features .",
    "neither the linear or non - linear weston datasets exhibit a nonlinear interdependence between features .",
    "we produced a simple xor pattern dataset in order to simulate this scenario .",
    "along with the accuracy on the test set and the sparsity we also record the precision and recall of the selection algorithms .",
    "analogously to information retrieval , we define the precision as the number of the relevant features that were selected from the feature selection procedure over the total number of features selected and recall as the number of relevant features selected over the total number of relevant features .",
    ".results on synthetic data .",
    "[ cols= \" < , < , < , < , < , < , < , < , < \" , ]     for the mass spectrometry tasks we used randsel using 5000 bootstraps of size @xmath81 of the dataset , culling the bottom @xmath82 of variables in terms of expected contribution after the end of each iteration .",
    "we used similar parameters for the micro - array dataset but with an increased number of bootstraps of 10000 in order to account for the substantially higher dimensionality of the data .",
    "again , no variables were fixed and the algorithm iterated until only two variables remained . in _ task 1 _ , randomized selection is tied with stability selection in terms of accuracy , however on average the randomised recovered feature set is significantly sparser .",
    "interestingly , simple filtering based on correlation coefficients performs strongly in the mass spectrometry tasks , often beating the hsic variants and in fact gives the highest accuracy for _ task 2_. the only test where all the hsic variants outperformed correlation filtering is the microarray task , which has a substantially increased dimensionality in comparison to the mass spectrometry datasets .",
    "the results indicate that the hsic - based variants ( randsel , fohsic & bahsic ) often recover sparser solutions compared to competing algorithms .",
    "given their mutual reliance on hsic optimisation , the fact that randsel outperforms the other methods in terms of accuracy can be surprising at first glance .",
    "it is instructive at this point to acknowledge that these methods rely on heuristics to solve an np - hard problem .",
    "the synthetic xor dataset already underlines one scenario where randsel outperforms forward greedy selection .",
    "the results on real data , combined with our theoretical guarantees , suggest the possibility of arriving at an improved global solution through randsel s incorporation of stochastic information , in opposition to the strategy of obliviously eliminating the locally optimal variable employed in bahsic .      a final experimental application where we employed randomised selection was in the recent black box learning challenge [ 10][13 ] .",
    "after performing an initial unsupervised feature learning step on the original dataset using sparse filtering [ 11 ] , we performed randomised selection in the resulting representation , creating kernels corresponding to the remaining features after each iteration of the feature selection algorithm . treating each kernel as defining a class of weak learners , we used lpboost to perform multiple kernel learning ( lpboostmkl [ 12 ] ) .",
    "the resulting classifier beat many of our other approaches and was one of the strongest performers in the challenge .",
    "in this paper we propose randsel , a new algorithm for non - linear feature selection based on randomised estimates of hsic .",
    "randsel , stochastically estimates the expected importance of features at each iteration , proceeding to cull features uninformative features at the end of each iteration .",
    "our theoretical analysis gives strong guarantees for the expected performance of this procedure which is further demonstrated by testing on a number of real and artificial datasets .",
    "this , combined with the algorithm s attractive scaling properties make randsel a strong proposition for use in application areas such as quantitative biology , where the volume of data increases at a frantic pace .",
    "goodfellow , i. j. , erhan , d. , carrier , p. l. , courville , a. , mirza , m. , hamner , b. , ... & bengio , y. ( 2013 ) challenges in representation learning : a report on three machine learning contests .",
    "proceedings of the 20th international conference on neural information processing"
  ],
  "abstract_text": [
    "<S> feature selection plays a pivotal role in learning , particularly in areas were parsimonious features can provide insight into the underlying process , such as biology . </S>",
    "<S> recent approaches for non - linear feature selection employing greedy optimisation of centred kernel target alignment(kta ) , while exhibiting strong results in terms of generalisation accuracy and sparsity , can become computationally prohibitive for high - dimensional datasets . </S>",
    "<S> we propose randsel , a randomised feature selection algorithm , with attractive scaling properties . </S>",
    "<S> our theoretical analysis of randsel provides strong probabilistic guarantees for the correct identification of relevant features . </S>",
    "<S> experimental results on real and artificial data , show that the method successfully identifies effective features , performing better than a number of competitive approaches .    ....    </S>",
    "<S> feature selection , kernels .... </S>"
  ]
}