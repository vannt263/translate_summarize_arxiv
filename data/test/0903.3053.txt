{
  "article_text": [
    "in recent years designers of graphics hardware start the new concept for games developers  to accelerate game physics directly on gpus and , thus , to relieve less efficient for such purposes central processor units ( _ cpus _ ) of _ any _ game physics computations .",
    "originally the gpu was the interface unit between the system and monitor and has been optimized only for the system memory - video memory data transferring and for storage of videopages .",
    "the next evolutionary step of gpu was the appearance of graphics accelerators .",
    "they were designed for simple and frequently used video operations ( displacement of large blocks of images from one location to another on the screen , filling images , and so on ) without much usage of cpu . in the early and mid-1990s , cpu - assisted real - time 3d graphics were becoming increasingly common in personal computer and game consoles , which in turn lead to ever - growing customer s needs for hardware - accelerated 3d graphics . over the past five years",
    "the computer graphics technology has evolved from the possibility of performing basic mathematical operations on image pixels to general purpose computing on gpu technology ( _ gpgpu _ ) .",
    "nowadays gpus transformed into powerful programmable parallel processors providing , in particular , double - precision floating point support , which is especially essential for developing scientific applications . as an illustrative example , the peak performance of ati rv 770 graphics processors exceeds 1 tflops for single - precision operations and is about 250 gflops for double - precision operations that is far beyond the performance level of up - to - date high - end multicore cpus .",
    "another important issue concerning gpu programming is availability of special programming toolkits aimed at development of non - graphics applications and allowing a programmer to abstract away from sophisticated graphics application interfaces ( _ apis _ ) .",
    "corresponding tools , ati stream sdk and nvidia cuda ( compute unified device architecture ) , can be freely downloaded from official sites of leading gpu - makers .",
    "no wonder that gpgpu attracts a lot of interest from the scientific community working in such areas as astrophysics ( @xcite-@xcite ) , lattice qft and spin systems ( @xcite-@xcite ) , numerical analysis of pde s @xcite , molecular and fluid dynamics etc .",
    "( see , for instance @xcite as well as @xcite-@xcite for further references ) .",
    "although there exist several reports on the topic , most of them are based on utilization of nvidia s cuda toolkit while ati programming environment seems to be less popular in the literature . with the present publication",
    "we start our numerical simulations on gpu with ati stream sdk to fill up this gap .",
    "one of the main aims of the present study is to implement existing monte - carlo ( _ mc _ ) algorithms , widely employed for simulations in lattice qft and spin models , on gpu and to investigate their effectiveness on such parallel systems @xcite-@xcite . of course",
    ", one should always keep in mind that efficiency of a stochastic process is also strongly effected by autocorrelation times , which are different for different simulation schemes .",
    "that is , even though some particular algorithm is badly parallelized in principle , as non - local algorithms like the wolff or swendsen - wang cluster algorithm , nevertheless it might be quite effective in comparison with some local algorithm , such as metropolis algorithm that suffers from large autocorrelation time in the critical region ( see however ref .",
    "@xcite , where another local algorithm , the so - called worm algorithm , was investigated , that proved to be deprived of this drawback ) .",
    "however , we do not study efficiency of non - local updating schemes on gpu here .",
    "the paper organized as follows .",
    "the second section contains the implementation details for two local algorithms : the metropolis and the heat - bath algorithms . in subsection 2.1 the common program model of gpu - oriented simulation application",
    "is described .",
    "the features of used pseudo - random number generator is presented in subsection 2.2 .",
    "next subsection includes the description and performance results of @xmath3 ising model simulations for metropolis algorithm on gpu while subsection 2.4 contains some details and performance results of @xmath1 gluodynamics simulations for heat - bath algorithm . in section 3",
    "we discuss the obtained performance results and perspectives for prospective investigations .",
    "a short introduction into ati stream technology is shown in the appendix a. general information about ati rv770 hardware is collected in the appendix b. some basic details of ati stream sdk is outlined in the appendix c. finally , in the appendix d we provide information on the hardware and software configuration used in our monte - carlo simulations .",
    "we considered @xmath3 ising model and @xmath1 gluodynamics as a perfect tool for testing various simulation algorithms . in this section",
    "we examine implementation of the metropolis and the heat - bath updating algorithms on gpu for these two particular models , respectively .",
    "ati stream sdk allows to realize the algorithms by two software tools : brook+ and cal ( for general information see appendix c ) .",
    "we started to explore the ati stream technology with the brook+ , but we found out quickly that its usage does not allow to write gpgpu applications in the most efficient way .",
    "brook+ is a very transparent and suitable language for stream programming , but it does not afford to implement some subtle points , needed for optimal programming on gpu .",
    "in particular , we employed compute shader model that provides more flexibility for general purpose gpu computations .",
    "the basic ingredient of the compute shader is a _ thread _ ( while pixel shaders operate with _ pixels _ ) , one invocation of a gpu computing program ( or _ kernel _ ) corresponding to a single element in the domain of execution .",
    "all threads initiated by the kernel are composed in thread groups and , within such a group , can communicate with each other via special hardware unit , local data share , assigned to each simd core . for further details",
    "we refer to @xcite and @xcite . )",
    "the critical requirement for performance is another essential argument in favor of the cal use .",
    "the present research is done with the cal .",
    "both the ising model and @xmath1 gluodynamics simulations are based on the same program model .",
    "as it is pointed in appendix c , ati stream sdk does not support the execution of a kernel from another kernel .",
    "so , each kernel must perform a complete operation .",
    "all kernels are run by the host program .",
    "[ fig:1 ]   +   +  figure 1 :    another feature of the employed program model is that all necessary data for simulations are stored in gpu memory .",
    "gpu carries out intermediate actions and returns the results to host program for final data handling and output .",
    "we avoid any data transfer during run - time between the host program and kernels to speed - up the execution process .",
    "the structure scheme of simulation program is presented in fig . 1 .    first of all we have to allocate gpu memory .",
    "the most convenient structure for this is the so - called _ global buffer _ , memory space containing the arbitrary address locations to which uncashed kernel outputs are written .",
    "ati stream sdk provides only one global buffer per kernel , therefore , all kernels must utilize the same global buffer to store all data .",
    "in addition , the memory allocation may be accompanied by setup of initial values for some internal variables in the global buffer .",
    "this is realized by preparing necessary data arrays on host system memory with subsequent mapping them into global buffer .",
    "the next execution stage is lattice initialization .",
    "two standard initial states may be produced by the lattice initialization kernels : ordered ( _ cold _ ) or disordered ( _ hot _ ) configuration .",
    "lattice initialization may be obviously done by host program before the memory setup kernel , however we carried out full lattice simulation process entirely on gpu .",
    "thermalization and statistics sweeps are almost the same except for the measuring kernels , which are not involved in the thermalization process .",
    "the locality of the examined here algorithms implies , in particular , that the lattice can be divided into odd and even sites . to update odd sites ,",
    "only information from the even site is necessary and vice versa .",
    "hence , the complete mc sweep for all lattice requires the two groups of kernels execution ( to update the odd and even sites , respectively ) . in working passes after each sweep the measuring kernels can be run in order to measure microstate parameters ( microstate energy , correlations , etc ) .",
    "last stage is the execution of postproduction kernels ( such as statistical data handling ) .",
    "afterwards , the export of all data to the host memory must be done .      in our simulations",
    "we employed the 24-bit marseglia pseudo - random number generator ranmar @xcite .",
    "here we used the implementation by james @xcite .",
    "the algorithm is a combination of a lagged fibonacci generator and a simple arithmetic sequence for the prime modulus @xmath4 .",
    "the total period of ranmar is about @xmath5 .",
    "the generator must be initialized by two given 5-digit integers ( _ initial seeds _ ) , each set of which gives rise to an independent sequence of sufficient length for an entire calculation .",
    "the seed variables can have values between @xmath6 and @xmath7 for the first variable and @xmath6 and @xmath8 for the second variable , respectively .",
    "ranmar can create , therefore , 900 million independent subsequences for different initial seeds with each subsequence having a length of approximately @xmath9 random numbers .",
    "the 97 seed values and two offsets must be stored in gpu global memory for every random number generator used in simulations .",
    "so , it is reasonably to use a small number of random number generators , which is smaller than number of lattice sites for big lattices . as it was mentioned above , every gpu memory cell has four 32-bit components , so it is convenient also to choose the total number of generators to be multiplied by 4 .",
    "the generator consists of two parts : 1 ) kernel , which produces the seed numbers on initial seed values ; in fact this kernel is a replica of rmarin subroutine of james version @xcite , and 2 ) subroutine , which directly produces the random numbers .",
    "first part has been executed only for initialization , and the second one has been running as internal subroutine in all kernels that use random numbers ( e.g. initial lattice equilibration , mc updates ) .",
    "it is not necessary to store generated random numbers in gpu memory in this case ; such approach essentially accelerates the run process .    in",
    "both ising model and @xmath1 gluodynamics simulations we used @xmath10 generators , and their number may be chosen in accordance with the lattice size .",
    "the lowest 10 bits of current thread number serve as identifier ( _ i d _ ) of random number generator .",
    "it is possible to change easily the actual number of generators by selecting any number of lowest bits in the thread i d .",
    "this realization allows to start all generators as independent parallel threads . in our implementation ,",
    "ranmar generators produce integer random numbers , which are mapped into the interval ( of floating point values ) @xmath11 afterwards .",
    "@xmath3 ising model , being the simplest and well - studied spin system , is a handy toy - model for gpu programming experiments .",
    "we remind the reader that subject to the character of the updating procedure , all conventional dynamic monte - carlo algorithms fall into two categories : namely , local and non - local algorithms . from the parallel computing perspective ,",
    "former category of mc algorithms are usually the most convenient to deal with . indeed ,",
    "typically the updating scheme in this case requires local information only ( e.g. information on close neighbors for a given spin in the lattice etc . )",
    "; as a consequence , all stream processors work on their own distinct local data domain of the data with little queuing or synchronization overheads . on the contrary , the non - local algorithms are significantly harder to parallelize since they demand much data communications for stream processors , the feature is especially unwelcome for gpu programming . as it was already noted , in the present study we are concerned about local mc update procedures , and the simplest of them is the metropolis algorithm .",
    "implementation of the metropolis updates on parallel systems presents no difficulties since one can always make use of standard data decomposition of the lattice onto processors to achieve good load balance . here",
    "we use two - dimensional ising model to illustrate basic concepts of gpu parallel programming .",
    "the partition function for the ising model is defined as @xmath12 with @xmath13 being a spin variable at @xmath14 site of the spin lattice . here",
    "@xmath15 denote the summation over nearest neighbor spins .",
    "it is quite natural to represent ising spins as 1-bit variables and to store them in registers of the global buffer ; in the case of gpu programming , this gives additional advantage since it allows to reduce memory fetches as well .",
    "the most essential part of the program is the spin lattice partition .",
    "there are several ways to do this .",
    "the simplest one is the checkerboard ( or _ `` red - black '' _ ) partition , where each stream processor has just one spin .",
    "all spin lattice is divided into `` red '' and `` black '' squares , and all `` red '' ( resp .",
    ", `` black '' ) spins are then processed in parallel .",
    "more efficient way is to divide the system into small squares of some size and to assign each stream processor to one such square .",
    "besides , one has also to take into account that the global buffer register is four - component .",
    "here we proposed the scheme that very much resembles the well - known `` direct - dual '' decomposition of a spin lattice ; this is shown in fig .",
    "black sites of the lattice interact with `` dual '' red sites , and are composed into 4 component registers ( in the figure it is indicated by gray painted rectangles ; each rectangle represents memory organization in the global buffer ) .",
    "similarly , `` red '' sites are collected in the corresponding red painted rectangles and thus stored in global buffer registers .",
    "we found that such lattice organization allows to reduce memory usage and better suites for gpu simulations .",
    "[ fig:2 ]   +   +  figure 2 :    simulation process on gpu was performed in the following stages :    1 .",
    "random number seed array preparation ; 2 .",
    "loading of a chosen initial configuration for the spin lattice in the gpu global buffer ; 3 .",
    "equilibration run using the conventional metropolis updating ; 4 .",
    "statistics run ; 5 .",
    "finalization .",
    "these stages are actually common for both models ( see below ) . in the case of ising model simulation , the source code contains eight kernels . the first kernel is designed for generating seed number arrays , which are stored in the gpu global buffer and accessible for all remaining kernels .",
    "we also dedicate a single kernel for the spin lattice initialization ( as usual , it defines hot or cold initial configuration ) . for equilibration",
    "run we use separate kernels to perform `` even '' and `` odd '' sweeps , while statistic runs are executed with the help of four kernels ( two of them are used for sweeps and measurements , and the other two perform reduction operations and store obtained results for chosen lattice observables in the global memory ) . in the present study we consider lattice of medium size @xmath16 . in particular",
    ", we found that it takes about @xmath17 sec for @xmath18 lattice updates during equilibration phase and @xmath19 sec .",
    "for @xmath18 measurements during statistic run phase .      in our @xmath1 gluodynamics simulations we used the hypercubic lattice @xmath20 ( @xmath21 ) with hypertorus geometry ; @xmath22 and @xmath23 are the temporal and the spatial sizes of the lattice , respectively . it should be noted that implemented algorithm makes possible to work not only with spatial symmetric lattices ( @xmath24 ) , but also with totally asymmetric lattices ( @xmath25 ) .",
    "the total number of lattice sites is @xmath26 . as it was mentioned , we divided whole lattice into 2 parts - odd and even sites .",
    "the number of odd sites is @xmath27 , as well as the number of even ones .",
    "the lattice may be initialized by cold or hot configuration .",
    "there are 2 possibilities for hot initialization to fill lattice with the pseudo - random @xmath1 matrices : constant series ( by predefined seeds ) or random series ( by system timer - dependent random seeds ) of pseudo - random numbers . in first case every startup process would reproduce the same results for every execution ( it is convenient for debugging purposes ) .",
    "standard wilson one - plaquette action of the @xmath1 lattice gauge theory was used , @xmath28;\\\\ \\label{umunu } { \\bf u}_{\\mu\\nu}(x)={\\bf u}_\\mu(x){\\bf u}_\\nu(x+a\\hat{\\mu}){\\bf u}^\\dag_\\mu(x+a\\hat{\\nu}){\\bf u}^\\dag_\\nu(x),\\end{aligned}\\ ] ] where @xmath29 is the lattice coupling constant , @xmath30 is the bare coupling , @xmath31 is the link variable located on the link leaving the lattice site @xmath32 in the @xmath33 direction , @xmath34 is the ordered product of the link variables ( here and below we omitted the group indices ) .",
    "the link variables @xmath31 are the @xmath1 matrices and can be decomposed in terms of the unity , @xmath35 , and pauli @xmath36 , matrices in the color space , @xmath37    each global buffer cell comprises four 32-bit numbers ( single precision ) .",
    "thus , it is natural to store all components of link matrices ( @xmath38 , @xmath39 , @xmath40 and @xmath41 ) as one cell . for example ,",
    "if general purpose register is @xmath42 , then the components of this register are @xmath43 , @xmath44 , @xmath45 and @xmath46 .    to update the lattice , heat - bath algorithm with overrelaxation",
    "was used @xcite , @xcite . at every mc iteration",
    "we successively replaced each lattice link matrix with @xmath47 where @xmath48,\\end{aligned}\\ ] ] @xmath49 is a normalization constant such that @xmath50    in order to thermalize the system we passed 1000 mc iterations for every run . for measuring we used 1000 mc configurations ( separated by 5 updates ) .",
    "mc procedure contains four kernels ( see fig .",
    "1 ) :    1 .",
    "kernel for measuring the value @xmath51 in eqn.([mcupdw ] ) for odd sites ; 2 .",
    "kernel for updating the odd site s links ; 3 .",
    "kernel for measuring the value @xmath51 in eqn.([mcupdw ] ) for even sites ; 4 .",
    "kernel for updating the even site s links .",
    "first two kernels are executed sequently for each @xmath33 direction .",
    "after their execution last two kernels are started for each @xmath33 direction .",
    "updating kernels have 8 attempts per start to modify the link matrices .    the run - time parameters , such as numbers of thermalization and working mc sweeps , number of link updates per one mc sweep , etc .",
    ", can be easily set up before execution .    in our program model",
    "each thread operates only one lattice site .",
    "so , the stream for every kernel has @xmath52 elements .",
    "it is necessary to store @xmath52 values of @xmath51 in global buffer during the mc procedure .",
    "odd and even sites used the same region in global buffer for any @xmath33 .",
    "working mc passes contain the measuring kernels , which calculate microstate energy .",
    "all averaging measurements were performed with the double precision .",
    "every thread in measuring kernel produces two 64-bit values of local averages ( [ umunu ] )  for spatial and temporal plaquettes , correspondingly .",
    "these values are stored in global buffer and summed up by so - called _ reduction kernel _ , which decreases the dimensionality of a stream by folding along one axis .",
    "reduction kernel does not use the gpu power in the best way , because at last execution stages less threads are run than it could be .",
    "nevertheless , such kernels must be used to avoid the high memory consumption .",
    "the final double - precision values of temporal and spatial averages are stored in global buffer and transferred after all mc sweeps to the host for postproduction analysis .    0.5 cm    [ cols=\"<,^,^,^,^\",options=\"header \" , ]      +   +     graphics cards and have two stream processors each , the remaining cards have one stream processor only .",
    "every stream processor of comprises 800 thread processors that are grouped into 10 simd cores ( see @xcite and fig .",
    "the lds size for every simd is 16 kb .",
    "thread processors of contain four stream cores and one t - stream core , which are also named alu.[x , y , z , w ] and alu.t , respectively ( see fig .",
    "each pair of stream cores ( i.e. , alu.x , alu.y and alu.z , alu.w ) may be grouped into one 64-bit alu for double - precision operations .",
    "the thread processor may co - issue up to 6 operations ( five arithmetical / logical and one flow control instruction ) .",
    "it also can perform 1.25 machine scalar operation per clock for each of 64 data elements .",
    "there are two main ati stream environments , which are composed into one complete software development kit :    * ati compute abstraction layer ( _ cal _ ) , * brook+ .",
    "ati cal is a device library that provides a forward - compatible interface to ati stream processors .",
    "ati cal lets software developers interact with the stream processors cores at the lowest - level for optimized performance , while maintaining forward compatibility .",
    "ati cal is ideal for performance - sensitive developers because it minimizes software overhead and provides full - control over hardware - specific features that might not be available with higher - level tools .",
    "brook is the high - level stream programming language for using modern graphics hardware for non - graphical computations .",
    "brook+ is amd s modified brook open source data - parallel c++ compiler @xcite .",
    "being built on top of ati cal , brook+ source - to - source meta - compiler translates programs into device - dependent kernels with the virtual instruction set architecture ( _ isa _ ) @xcite , the so - called ati intermediate language ( _ il _ ) .",
    "the generated c++ source includes the cpu code and the stream processor device code , both of which are later linked into the executable .",
    "brook+ is a higher - level language that is easier to use , but does not provide all the functionality that ati cal does .",
    "ati il is a pseudo - assembly language that can be used to describe kernels for stream processors @xcite .",
    "it is designed for efficient generalization of stream processor instructions , so that programs can run on a variety of platforms without having to be rewritten for each platform .",
    "ati cal api comprises one or more stream processors connected to one or more cpus by a high - speed bus .",
    "the cpu runs the stream processor device driver program and controls the stream processors by sending commands using the ati cal api .",
    "stream processor can read from and write to its own local stream processor memory and system memory using pci express bus .",
    "ati cal api exposes the stream processors as simd array of computational processors .",
    "the cal runtime comprises system initialization and query , device , context and memory management , program loading and execution .    in cal all physical memory blocks allocated by the application for the use in stream kernels",
    "are referred to as resources .",
    "these blocks can be allocated as one - dimensional or as two - dimensional arrays of data .",
    "there are several memory organizing structures , which may be used with the cal kernels : input buffers , output buffers , constant buffers , scratch buffers , global buffers .",
    "the input buffer can only be read from , not written to .",
    "the output buffer can only be written to , not read from .",
    "constant buffer is the off - chip memory that contains constants .",
    "scratch buffer is a variable - sized space in off - chip memory that stores some of the general purpose registers ( _ gprs _ ) .",
    "the global buffer lets applications read from , and write to , arbitrary locations of memory .",
    "global buffers use a linear memory layout .",
    "all these buffers support the following data types : signed or unsigned 32-bit integer , 32-bit floating point ( single precision ) , 64-bit floating point ( double precision ) .",
    "a 128-bit address mapped memory space consisting of four 32-bit components is called a register .",
    "it is accepted to mark its components as .x , .y , .z and .w , respectively . for double - precision operations",
    "the pair of 32-bit components are grouped into 64-bit values ( .xy and .zw ) .",
    "ati il is a typeless language - the value in a register has no intrinsic type , it is simply 32 or 64 bits of data @xcite .",
    "each mathematical instruction performs a typed computation  signed or unsigned integer , floating point , double - precision floating point  on one or more untyped operands .    unfortunately , there are some points , which make the programming on gpu low - level sdk complicate . for instance , ati cal does not support recursive execution of kernels as well as the execution of a kernel from the other . also , there are no runtime debugging tools in ati cal . and finally , ati stream sdk still has beta status , which may implies an occurrence of some compilation errors .",
    "another strong point of ati stream sdk is the fact that ati drivers ( ati catalyst ) from version 8.12 contain all necessary libraries for stream computing , so the compiled program could be directly executed on any computer system equipped with the ati radeon cards without the installation of additional software .",
    "hardware configuration :    * _ ising model : _ intel core 2 duo cpu e6550 @ 2.33ghz , 4 gb ram memory ; * _ su(2 ) : _ intel core 2 quad cpu q6600 @ 2.40ghz , 4 gb ram memory",
    ".    graphics card specification :    * _ graphics card : _ amd / ati radeon hd 4850 * _ gpu model : _ rv770pro",
    "* _ number of thread processors : _ 160 ( combined in 10 simd cores ) * _ number of local memory fetch units : _ 40 ( 4 for each simd core ) * _ engine clock : _ 625 mhz * _ memory clock : _",
    "993mhz ( gddr3 type , effective memory clock - 1984mhz ) * _ memory bus : _ 256 bit * _ memory size : _ 512 mb    software configuration :    * _ os : _ windows xp sp3 ( 32-bit ) * _ used driver : _ ati catalyst 9.1 , @xcite * ms visual studio 2008 express edition ( c++ compiler ) , @xcite * ati stream sdk v. 1.3 beta , @xcite",
    "one of the authors ( v.d . )",
    "thanks e .- m .",
    "ilgenfritz for kindly provided fortran program for @xmath1 lattice simulations .",
    "38 e.  b.  ford , `` parallel algorithm for solving kepler s equation on graphics processing units : application to analysis of doppler exoplanet searches '' , new astron .",
    "* 14 * ( 2009 ) 406 , arxiv:0812.2976 [ astro - ph ] .",
    "e.  gaburov , s.  harfst and s.  p.  zwart , `` sapporo : a way to turn your graphics cards into a grape-6 '' , arxiv:0902.4463 [ astro-ph.im ] .",
    "s.  f.  portegies zwart , r.  belleman and p.  geldof , `` high performance direct gravitational n - body simulations on graphics processing unit i : an implementation in cg '' , new astron .",
    "* 12 * ( 2007 ) 641 , arxiv : astro - ph/0702058 .",
    "r.  g.  belleman , j.  bedorf and s.  p.  zwart , `` high performance direct gravitational n - body simulations on graphics processing units  ii : an implementation in cuda '' , new astron .",
    "* 13 * ( 2008 ) 103 , arxiv:0707.0438 [ astro - ph ] .",
    "s.  ord , l.  greenhill , r.  wayth , d.  mitchell , k.  dale , h.  pfister and r.  g.  edgar , `` gpus for data processing in the mwa '' , arxiv:0902.0915 [ astro-ph.im ] .",
    "t.  hamada and t.  iitaka , `` the chamomile scheme : an optimized algorithm for n - body simulations on programmable graphics processing units '' , arxiv : astro - ph/0703100 .",
    "g.  i.  egri , z.  fodor , c.  hoelbling , s.  d.  katz , d.  nogradi and k.  k.  szabo , `` lattice qcd as a video game '' , comput .  phys .  commun .",
    "* 177 * ( 2007 ) 631 , arxiv : hep - lat/0611022 .",
    "k.  ibrahim , f.  bodin , o.  pene , `` fine - grained parallelization of lattice qcd kernel routine on gpus '' , j.parallel distrib .",
    "* 68 * ( 2008 ) 1350 .",
    "k.  i.  ishikawa , `` recent algorithm and machine developments for lattice",
    "qcd , '' arxiv:0811.1661 [ hep - lat ] . v.  anselmi , g.  conti and f.  di renzo , `` gpu computing for 2-d spin systems : cuda vs opengl '' , arxiv:0811.2111 [ hep - lat ] . k.  barros , r.  babich , r.  brower , m.  a.  clark and c.  rebbi , `` blasting through lattice calculations using cuda , '' pos lattice2008 ( 2008 ) 045 , arxiv:0810.5365 [ hep - lat ] .",
    "a.  klckner , t.  warburton , j.  bridge and j.  s.  hesthaven , `` nodal discontinuous galerkin methods on graphics processors '' , arxiv:0901.1024 [ math ] .",
    "* 68 * ( 2008 ) , special issue : + _ general - purpose processing using graphics processing units_. s.  collange , y.  dandass , m.  daumas and d.  defour , `` using graphics processors for parallelizing hash - based data carving '' , arxiv:0901.1307 [ cs ] . m.  andrecut , `` parallel gpu implementation of iterative pca algorithms '' , arxiv:0811.1081 [ q - bio ] . v.  garcia , e.  debreuve and m.  barlaud , `` fast k nearest neighbor search using gpu '' , arxiv:0804.1448 [ cs ] .",
    "w.  t.  shaw and n.  brickman , `` differential equations for monte carlo recycling and a gpu - optimized normal quantile '' , arxiv:0901.0638 [ q-fin.cp ] .",
    "a.  d.  sokal , `` monte carlo mehtods in statistical mechanics : foundations and new algorithms '' , _ given at the troisieme cycle de la physique en suisse romande , lausanne , switzerland , jun 15 - 29 , 1989 _ , ( 1996 ) w.  janke , `` nonlocal monte carlo algorithms for statistical physics applications '' , math . and comp . in simulation * 47 * ( 1998 ) 329 . b.  berg , `` introduction to markov chain monte - carlo simulations and their statistical analysis '' , arxiv : cond - mat/0410490 . n.   v.   prokofev , b.   v.   svistunov , `` worm algorithms for classical statistical models '' , phys",
    "* 87 * ( 2001 ) 160601 .",
    "g.  marsglia and a.  zaman , `` toward a universal random number generator '' , state university report fsu - scri-87 - 50 ( 1987 )",
    ". f.  james , `` a review of pseudorandom number generators '' , comput .",
    "commun .",
    "* 60 * ( 1990 ) 329 .",
    "a.   srinivasan , m.   mascagni , d.   ceperley , `` testing parallel random number generators '' , parallel computing , * 29 * ( 2003 ) 69 .",
    "j.   apostolakis , p.   coddington and e.   marinari ,  new simd algorithms for cluster labeling on parallel computers  ,",
    "c * 4 * ( 1993 ) 749 .",
    "s.   bae , s.  h.  ko , p.  d.   coddington , `` parallel wolff cluster algorithms '' , int .",
    "c , * 6*(1995 ) 197 .",
    "m.  creutz , `` overrelaxation and monte carlo simulation , '' phys .",
    "d * 36 * ( 1987 ) 515 .",
    "e.  m.  ilgenfritz , m.  l.  laursen , g.  schierholz , m.  muller - preussker and h.  schiller , `` first evidence for the existence of instantons in the quantized su(2 ) lattice vacuum '' , nucl .",
    "b * 268 * ( 1986 ) 693 .",
    "ati stream technology , + ati stream computing user guide , + ati stream computing technical overview , + comparison of ati gpus , + amd r700-family instruction set architecture , + amd intermediate language ( il ) reference manual , + ati catalyst display driver , + microsoft visual c++ 2008 express edition , + intel visual fortran compiler , + ati stream sdk , + khronos group , + opencl in mac os x snow leopard , +"
  ],
  "abstract_text": [
    "<S> implementation of basic local monte - carlo algorithms on ati graphics processing units ( @xmath0 ) is investigated . </S>",
    "<S> the ising model and pure @xmath1 gluodynamics simulations are realized with the compute abstraction layer ( @xmath2 ) of ati stream environment using the metropolis and the heat - bath algorithms , respectively . </S>",
    "<S> we present an analysis of both cal programming model and the efficiency of the corresponding simulation algorithms on gpu . </S>",
    "<S> in particular , the significant performance speed - up of these algorithms in comparison with serial execution is observed .    _ </S>",
    "<S> keywords : _ monte carlo simulations , parallel computing , gpgpu </S>"
  ]
}