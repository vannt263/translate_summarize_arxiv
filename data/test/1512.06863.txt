{
  "article_text": [
    "consumer reviews are invaluable as a source of data to help people form opinions on a wide range of products . beyond telling us whether a product is ` good ' or ` bad ' , reviews",
    "tell us about a wide range of _ personal experiences _ ; these include objective descriptions of the products properties , subjective qualitative assessments , as well as unique use- ( or failure- ) cases .",
    "the value and diversity of these opinions raises two questions of interest to us : ( 1 ) how can we help users navigate massive volumes of consumer opinions in order to find those that are _ relevant _ to their decision ? and",
    "( 2 ) how can we address specific _ queries _ that a user wishes to answer in order to evaluate a product ?    to help users answer specific queries , review websites like _ amazon _ offer community - q / a systems that allow users to pose product - specific questions to other consumers .",
    "our goal here is to respond to such queries automatically and on - demand . to achieve this",
    "we make the basic insight that our two goals above naturally complement each other : given a large volume of community - q / a data ( i.e. , questions and answers ) , and a large volume of reviews , we can automatically _ learn _ what makes a review relevant to a query .",
    "we see several reasons why reviews might be a useful source of information to address product - related queries , especially compared to existing work that aims to solve q / a - like tasks by building knowledge bases of facts about the entities in question :    * general question - answering is a challenging open problem .",
    "it is certainly hard to imagine that a query such as `` will this baby seat fit in the overhead compartment of a 747 ? ''",
    "could be answered by building a knowledge - base using current techniques .",
    "however it is more plausible that some review of that product will contain information that is relevant to this query . by casting the problem as one of surfacing relevant opinions ( rather than necessarily generating a conclusive answer ) , we can circumvent this difficulty , allowing us to handle complex and arbitrary queries .",
    "* fundamentally , many of the questions users ask on review websites will be those that _",
    "ca nt _ be answered using knowledge bases derived from product specifications , but rather their questions will be concerned with subjective personal experiences .",
    "reviews are a natural and rich source of data to address such queries . * finally",
    ", the massive volume and range of opinions makes review systems difficult to navigate , especially if a user is interested in some niche aspect of a product .",
    "thus a system that identifies opinions relevant to a specific query is of fundamental value in helping users to navigate such large corpora of reviews .    to make our objectives more concrete",
    ", we aim to formalize the problem in terms of the following goal :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ goal : _ given a query about a particular product , we want to determine how relevant each review of that product is to the query , where ` relevance ' is measured in terms of how helpful the review will be in terms of identifying the correct response .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    the type of system we produce to address this goal is demonstrated in figure [ fig : examp ] .",
    "here we surface opinions that are identified as being ` relevant ' to the query , which can collectively vote ( along with all other opinions , in proportion to their relevance ) to determine the response to the query .",
    "* product : *  braven brv-1 wireless bluetooth speaker , title=\"fig : \" ] + * query : *  `` i want to use this with my ipad air while taking a jacuzzi bath .",
    "will the volume be loud enough over the bath jets ? ''",
    "+   +    m0.78>m0.07 & vote : + `` the sound quality is great , especially for the size , and if you place the speaker on a hard surface it acts as a sound board , and the bass really kicks up . ''",
    "& yes +   + `` if you are looking for a water resistant blue tooth speaker you will be very pleased with this product . '' & yes +   + `` however if you are looking for something to throw a small party this just does nt have the sound output . ''",
    "& no +   + etc .",
    ". +     + * response : * yes    this simple example demonstrates exactly the features that make our problem interesting and difficult : first , the query ( ` is this loud enough ? ' ) is inherently subjective , and depends on personal experience ; it is hard to imagine that any fact - based knowledge repository could provide a satisfactory answer .",
    "secondly , it is certainly a ` long - tail ' query  it would be hard to find relevant opinions among the ( 300 + ) reviews for this product , so a system to automatically retrieve them is valuable .",
    "third , it is linguistically complex  few of the important words in the query appear among the most relevant reviews ( e.g.  ` jacuzzi bath'/`loud enough')this means that existing solutions based on word - level similarity are unlikely to be effective .",
    "this reveals the need to learn a complex definition of ` relevance ' that is capable of accounting for subtle linguistic differences such as synonyms .    finally ,",
    "in the case of figure [ fig : examp ] , our model is able to respond to the query ( in this instance correctly ) with a binary answer .",
    "more importantly though , the opinions surfaced allow the user to determine the answer themselves  in this way",
    "we can extend our model to handle general open - ended queries , where the goal is not to answer the question _ per se _ , but rather to surface relevant opinions that will help the questioner form their own conclusion .",
    "it seems then that to address our goal we ll need a system with two components : ( 1 ) a _ relevance _ function , to determine which reviews contain information relevant to a query , and ( 2 ) a prediction function , allowing relevant reviews to ` vote ' on the correct answer .",
    "however as we stated , our main goal is _ not _ to answer questions directly but rather to surface relevant opinions that will help the user answer the question themselves ; thus it may seem as though this ` voting ' function is not required .",
    "indeed , at _ test _ time , only the relevance function is required  this is exactly the feature that shall allow our model to handle arbitrary , open - ended , and subjective queries .",
    "however the voting function is critical at _ training _ time , so that with a large corpus of already - answered questions , we can simultaneously learn relevance and voting functions such that ` relevant ' reviews are those that vote for the correct answer .",
    "the properties that we want above are captured by a classical machine learning framework known as _",
    "mixtures of experts _ @xcite .",
    "mixtures of experts are traditionally used when one wishes to combine a series of ` weak learners'there the goal is to simultaneously estimate ( a ) how ` expert ' each predictor is with respect to a particular input and ( b ) the parameters of the predictors themselves .",
    "this is an elegant framework as it allows learners to ` focus ' on inputs that they are good at classifying  it does nt matter if they sometimes make incorrect predictions , so long as they correctly classify those instances where they are predicted to be experts .",
    "in our setting , individual reviews or opinions are treated as experts that get to vote on the answer to each query ; naturally some opinions will be unrelated to some queries , so we must also learn how relevant ( i.e. , expert ) each opinion is with respect to each query .",
    "our prediction ( i.e. , voting ) function and relevance function are then learned simultaneously such that ` relevant ' opinions are precisely those that are likely to vote correctly . at test time",
    ", the relevance function can be used directly to surface relevant opinions .",
    "we evaluate our model using a novel corpus of questions and answers from _",
    "we consider both binary questions ( such as the example in figure [ fig : examp ] ) , and open - ended questions , where reviews must vote amongst alternative answers .",
    "quantitatively , we compare our technique to state - of - the - art methods for relevance ranking , and find that our learned definition of relevance is more capable of resolving queries compared to hand - crafted relevance measures .",
    "qualitatively , we evaluate our system by measuring whether human evaluators agree with the notion of ` relevance ' that we learn .",
    "this is especially important for open - ended queries , where it is infeasible to answer questions directly , but rather we want to surface opinions that are helpful to the user .",
    "we summarize our contributions as follows : first , we develop a new method , _ moqa _ , that is able to uncover opinions that are relevant to product - related queries , and to learn this notion of relevance from training data of previously answered questions .",
    "second , we collect a large corpus of 1.4 million answered questions and 13 million reviews on which to train the model .",
    "ours is among the first works to combine community q / a and review data in this way , and certainly the first to do it at the scale considered here .",
    "third , we evaluate our system against state - of - the - art approaches for relevance ranking , where we demonstrate ( a ) the need to learn the notion of ` relevance ' from training data ; ( b ) the need to handle heterogeneity between questions , reviews , and answers ; and ( c ) the value of opinion data to answer product - related queries , as opposed to other data like product specifications .",
    "code and data is available on the first author s webpage .",
    "the most closely related branches of work to ours are ( 1 ) those that aim to mine and summarize opinions and facets from documents ( especially from review corpora ) , and ( 2 ) those that study q / a systems in general . to our knowledge our work is among the first at the interface between these two tasks , i.e. , to use consumer reviews as a means of answering general queries about products , though we build upon ideas from several related areas .",
    "* document summarization . *",
    "perhaps most related to our goal of selecting relevant opinions among large corpora of reviews is the problem of _ multi - document summarization _ @xcite . like ours ,",
    "this task consists of finding relevant or ` salient ' parts of documents @xcite and intelligently combining them .",
    "most related are approaches that apply document summarization techniques to ` evaluative text ' ( i.e. , reviews ) , in order to build an overview of opinions or product features @xcite .",
    "in contrast to our contribution , most of the above work is not ` query - focused , ' e.g.  the goal is to summarize product features or positive vs.  negative opinions , rather than to address specific queries , though we note a few exceptions below .",
    "* relevance ranking . *",
    "a key component of the above line of work is to learn whether a document ( or a phrase within a document ) is relevant to a given query .",
    "` relevance ' can mean many things , from the ` quality ' of the text @xcite , to its lexical salience @xcite , or its diversity compared to already - selected documents @xcite . in query - focused settings",
    ", one needs a query - specific notion of relevance , i.e. ,  to determine whether a document is relevant in the context of a given query .",
    "for this task , simple ( yet effective ) word - level similarity measures have been developed , such as okapi bm25 , a state - of - the - art tf - idf - based relevance ranking measure @xcite .",
    "a natural limitation one must overcome though is that queries and documents may be linguistically heterogeneous , so that word - level measures may fail @xcite .",
    "this can be addressed by making use of grammatical rules and phrase - level approaches ( e.g.  rouge measures @xcite ) , or through probabilistic language models ranging from classical methods @xcite to recent approaches based on deep networks @xcite .",
    "we discuss ranking measures more in section [ sec : measures ] .",
    "* opinion mining .",
    "* studying consumer opinions , especially through rating and review datasets is a broad and varied topic .",
    "review text has been used to augment ` traditional ' recommender systems by finding the aspects or facets that are relevant to people s opinions @xcite and , more related to our goal , to find ` helpful ' reviews @xcite or experts on particular topics @xcite .",
    "there has also been work on generating summaries of product features @xcite , including work using multi - document summarization as mentioned above @xcite .",
    "this work is related in terms of the data used , and the need to learn some notion of ` relevance , ' though the goal is not typically to address general queries as we do here .",
    "we are aware of relatively little work that attempts to combine question - answering with opinion mining , though a few exceptions include @xcite , which answers certain types of queries on _ amazon _ data ( e.g.  `` find 100 books with over 200 5-star ratings '' ) ; or @xcite which learns to distinguish ` facts ' from subjective opinions ; or @xcite , which tries to solve cold - start problems by finding opinion sentences of old products that will be relevant to new ones .",
    "though in none of these cases is the goal to address general queries .",
    "* q / a systems .",
    "* many of the above ideas from multi - document summarization , relevance ranking , and topical expert - finding have been adapted to build state - of - the - art automated q / a systems .",
    "first is ` query - focused ' summarization @xcite , which is similar to our task in that phrases must be selected among documents that match some query , though typically the relevance function is not learned from training data as it is here .",
    "next ( as mentioned above ) is the notion that questions , answers , and documents are heterogeneous , meaning that simple bag - of - words type approaches may be insufficient to compare them @xcite , so that instead one must decompose questions @xcite or model their syntax @xcite . also relevant",
    "is the problem of identifying experts @xcite or high - quality answers @xcite , or otherwise identifying instances where similar questions have already been answered elsewhere @xcite , though these differ from our paradigm in that the goal is to select among answers ( or answerers ) , rather than to address the questions themselves .    naturally also relevant",
    "is the large volume of q / a work from the information retrieval community ( e.g.  trec q / a ) ; however note first that due to the data involved ( in particular , subjective opinions ) our approach is quite different from systems that build knowledge bases ( e.g.  systems like watson @xcite ) , or generally systems whose task is to retrieve a list of objective facts that conclusively answer a query .",
    "rather , our goal is to use q / a data as a means of learning a ` useful ' relevance function , and as such our experiments mainly focus on state - of - the - art relevance ranking techniques .      though related to the above areas , our work is novel in a variety of ways .",
    "our work is among the first at the interface of q / a and opinion mining , and is novel in terms of the combination of data used , and in terms of scale .",
    "in contrast to the above work on summarization and relevance ranking , given a large volume of answered queries and a corpus of weakly relevant documents ( i.e. , reviews of the product being queried ) , our goal is to be as agnostic as possible to the definition of `` what makes an opinion relevant to a query ? , '' and to learn this notion automatically from data .",
    "this also differentiates our work from traditional q / a systems as our goal is not to answer queries directly ( i.e. , to output ` facts ' or factoids ) , but rather to learn a relevance function that will help users effectively navigate multiple subjective viewpoints and personal experiences .",
    "critically , the availability of a large training corpus allows us to learn complex mappings between questions , reviews , and answers , while accounting for the heterogeneity between them .",
    "[ tab : notation ] [ cols=\"<,<\",options=\"header \" , ]      we also want to evaluate whether review text is a better source of data than other sources , such as product descriptions or specifications . to test this we collected description / specification text for each of the products in our catalogue . from here",
    ", we simply interchange reviews with descriptions ( recall that both models operate at the level of sentences ) .",
    "we find that while _ moqa _ with descriptions ( i.e. , _ mdqa _ ) performs well ( on par with the strongest baselines ) , it is still substantially outperformed when we use review text . here _",
    "moqa _ yields a 37.5% reduction in error over _ mdqa _ in table [ tab : openended ] ; similarly in figure [ fig : prec ] , for binary queries _ mdqa _ is on par with the strongest baseline but substantially outperformed by _ moqa _",
    "( again other datasets are similar and not shown for brevity ) .",
    "partly , reviews perform better because we want to answer subjective queries that depend on personal experiences , for which reviews are simply a more appropriate source of data . but other than that , reviews are simply more abundant  we have on the order of 100 times as many reviews as descriptions ( products with active q / a pages tend to be reasonably popular ones ) ; thus it is partly the sheer volume and diversity of reviews available that makes them effective as a means of answering questions .",
    "we discuss these findings in more detail in section [ sec : discussion ] .",
    "finally , we evaluate _",
    "qualitatively through a user study .",
    "although we have shown _ moqa _ to be effective at correctly resolving binary queries , and at maximizing the auc to select a correct answer among alternatives , what remains to be seen is whether the relevance functions that we learned to do so are aligned with what humans consider to be ` relevant . ' evaluating this aspect is especially important because in a live system our approach would presumably not be used to answer queries directly ( which we have shown to be very difficult , and in general still an open problem ) , but rather to surface relevant reviews that will help the user to evaluate the product themselves .    here",
    "we use the relevance functions @xmath0 that we learned in the previous section ( i.e. , from table [ tab : openended ] ) to compare which definition of ` relevance ' is best aligned with real users evaluations  note that the voting function @xmath1 is not required at this stage .",
    "we performed our evaluation using _",
    "amazon s mechanical turk _ , using ` master workers ' to evaluate 100 queries from each of our five largest datasets , as well as one smaller dataset ( _ baby _ ) to assess whether our method still performs well when less data is available for training . workers were presented with a product s title , image , and a randomly selected query ( binary or otherwise ) .",
    "we then presented them the top - ranked result from our method , as well as the top - ranked result using okapi - bm25+/rouge measures ( with tuned parameters , i.e. , ro - l from table [ tab : openended ] ) ; this represents a state - of - the - art ` off - the - shelf ' relevance ranking benchmark , with parameters tuned following best practices ; it is also the most competitive baseline from table [ tab : openended ] .",
    "results were shown to evaluators in a random order without labels , from which they had to select whichever they considered to be the most relevant .",
    "we also asked workers whether they considered a question to be ` subjective ' or not , in order to evaluate whether the subjectivity of the question impacts performance .",
    "a screenshot of our interface is shown in figure [ fig : interface ] .",
    "results of this evaluation are shown in figure [ fig : user_study ] .",
    "on average , _ moqa _ was preferred in 73.1% of instances across the six datasets we considered .",
    "this is a significant improvement ; improvements were similar across datasets ( between 66.2% on sports and outdoors and 77.6% on baby ) , and for both subjective and objective queries ( 62.9% vs.  74.1% ) .",
    "ultimately _ moqa _ consistently outperforms our strongest baseline in terms of subjective performance , though relative performance seems to be about the same for objective and subjective queries , and across datasets .    ) .",
    "[ fig : user_study ] ]      finally , a few examples of the output produced by _ moqa _ are shown in figure [ fig : examps ] . note that none of these examples were available at training time , and only the question ( along with the product being queried ) are provided as input .",
    "these examples demonstrate a few features of _ moqa _ and the data in question : first is the wide variety of products , questions , and opinions that are reflected in the data ; this linguistic variability demonstrates the need for a model that _ learns _ the notion of relevance from data .",
    "second , the questions themselves ( like the example from figure [ fig : examp ] ) are quite different from those that could be answered through knowledge bases ; even those that seem objective ( e.g. `` how long does this stay hot ? '' ) are met with a variety of responses representing different ( and sometimes contradictory ) experiences ; thus reviews are the perfect source of data to capture this variety of views .",
    "third is the heterogeneity between queries and opinions ; words like `` girl '' and `` tall '' are identified as being relevant to `` daughter '' and `` medium , '' demonstrating the need for a flexible model that is capable of learning complicated semantics in general , and synonyms in particular .",
    "also note that while our bilinear model has many thousands of parameters , at test time relevance can be computed extremely efficiently , since in ( eq .",
    "[ eq : approxtransform ] ) we can project all reviews via @xmath2 in advance .",
    "thus computing relevance takes only @xmath3 ( i.e. , the number of projected dimensions plus the number of words in the query and review ) ; in practice this allows us to answer queries in a few milliseconds , even for products with thousands of reviews .",
    "p0.82m0.13 + &   + & +   +   + & + & +",
    "surprisingly , performance for open - ended queries ( table [ tab : openended ] ) appears to be better than performance for binary queries ( table [ tab : yn ] ) , both compared to random classification and to our strongest baseline , against our intuition that the latter task might be more difficult .",
    "there are a few reasons for this : one is simply that the task of differentiating the true answer from a ( randomly selected ) non - answer is ` easier ' than resolving a binary query ; this explains why outperforming a random baseline is easier , but does not explain the higher relative improvement against baselines . for the latter , note that the main difference between our method and the strongest baseline is the use of a bilinear model ;",
    "while a highly flexible model , it has far more parameters than baselines , meaning that a large dataset is required for training .",
    "thus what we are seeing may simply be the benefit of having substantially more data available for training when considering open - ended questions .    also surprising",
    "is that in our user study we obtained roughly equal performance on subjective vs.  objective queries .",
    "partly this may be because subjective queries are simply ` more difficult ' to address , so that there is less separation between methods , though this would require a larger labeled dataset of subjective vs.  objective queries to evaluate quantitatively . in fact , contrary to expectation only around 20% of queries were labeled as being ` subjective ' by workers .",
    "however the full story seems more complicated ",
    "queries such as `` how long does this stay hot ? ''  ( figure [ fig : examps ] ) are certainly labeled as being ` objective ' by human evaluators , though the variety of responses shows a more nuanced situation .",
    "really , a large fraction of seemingly objective queries are met with contradictory answers representing different user experiences , which is exactly the class of questions that our method is designed to address .",
    "we see several potential ways to extend _",
    "moqa_.    first , while we have made extensive use of reviews , there is a wealth of additional information available on review websites that could potentially be used to address queries .",
    "one is rating information , which could improve performance on certain evaluative queries ( though to an extent we already capture this information as our model is expressive enough to learn the polarity of sentiment words ) .",
    "another is user information  the identity of the questioner and the reviewer could be used to learn better relevance models , both in terms of whether their opinions are aligned , or even to identify topical experts , as has been done with previous q / a systems @xcite .    in categories like electronics , a large fraction of queries",
    "are related to compatibility ( e.g.  `` will this product work with x ? '' ) .",
    "addressing compatibility - related queries with user reviews is another promising avenue of future work ",
    "again , the massive number of potential product combinations means that large volumes of user reviews are potentially an ideal source of data to address such questions .",
    "although our system can already address such queries to some extent , ideally a model of compatibility - related queries would make use of additional information , for instance reviews of _ both _ products being queried , or the fact that compatibility relationships tend to be symmetric , or even co - purchasing statistics as in @xcite .",
    "finally , since we are dealing with queries that are often subjective , we would like to handle the possibility that they may have multiple and potentially inconsistent answers .",
    "currently we have selected the top - voted answer to each question as an ` authoritative ' response to be used at training time .",
    "but handling multiple , inconsistent answers could be valuable in several ways , for instance to automatically identify whether a question is subjective or contentious , or otherwise to generate relevance rankings that support a spectrum of subjective viewpoints .",
    "we presented _ moqa _ , a system that automatically responds to product - related queries by surfacing relevant consumer opinions .",
    "we achieved this by observing that a large corpus of previously - answered questions can be used to _ learn _ the notion of relevance , in the sense that ` relevant ' opinions are those for which an accurate predictor can be trained to select the correct answer as a function of the question and the opinion .",
    "we cast this as a mixture - of - experts learning problem , where each opinion corresponds to an ` expert ' that gets to vote on the correct response , in proportion to its relevance .",
    "these relevance and voting functions are learned automatically and evaluated on a large training corpus of questions , answers , and reviews from _",
    "the main findings of our evaluation were as follows : first , reviews proved particularly effective as a source of data for answering product - related queries , outperforming other sources of text like product specifications ; this demonstrates the value of _ personal experiences _ in addressing users queries .",
    "second , we demonstrated the need to handle heterogeneity between various text sources ( i.e. , questions , reviews , and answers ) ; our large corpus of training data allowed us to train a flexible bilinear model that it capable of automatically accounting for linguistic differences between text sources , outperforming hand - crafted word- and phrase - level relevance measures .",
    "finally , we showed that _",
    "moqa _ is quantitatively able to address both binary and open - ended questions , and qualitatively that human evaluators prefer our learned notion of ` relevance ' over hand - crafted relevance measures .",
    "d.  ferrucci , e.  brown , j.  chu - carroll , j.  fan , d.  gondek , a.  a. kalyanpur , a.  lally , j.  w. murdock , e.  nyberg , j.  prager , n.  schlaefer , and c.  welty .",
    "building watson : an overview of the deepqa project . in _ ai magazine _",
    ", 2010 .",
    "r.  gangadharaiah and b.  narayanaswamy .",
    "natural language query refinement for problem resolution from crowd - sourced semi - structured data . in _ international joint conference on natural language processing _ , 2013 ."
  ],
  "abstract_text": [
    "<S> online reviews are often our first port of call when considering products and purchases online . </S>",
    "<S> when evaluating a potential purchase , we may have a specific query in mind , e.g. ` will this baby seat fit in the overhead compartment of a 747 ? ' or ` will i like this album if i liked taylor swift s _ 1989 _ ? ' . to answer such questions </S>",
    "<S> we must either wade through huge volumes of consumer reviews hoping to find one that is relevant , or otherwise pose our question directly to the community via a q / a system .    in this paper </S>",
    "<S> we hope to fuse these two paradigms : given a large volume of previously answered queries about products , we hope to automatically learn whether a review of a product is relevant to a given query . </S>",
    "<S> we formulate this as a machine learning problem using a mixture - of - experts - type framework  here </S>",
    "<S> each review is an ` expert ' that gets to vote on the response to a particular query ; simultaneously we learn a relevance function such that ` relevant ' reviews are those that vote correctly . at test time </S>",
    "<S> this learned relevance function allows us to surface reviews that are relevant to new queries on - demand . </S>",
    "<S> we evaluate our system , _ moqa _ , on a novel corpus of 1.4 million questions ( and answers ) and 13 million reviews . </S>",
    "<S> we show quantitatively that it is effective at addressing both binary and open - ended queries , and qualitatively that it surfaces reviews that human evaluators consider to be relevant . </S>"
  ]
}