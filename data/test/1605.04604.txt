{
  "article_text": [
    "stochastic partial differential equations ( spdes ) , or partial differential equations ( pdes ) with uncertainties , play an important role in many areas of engineering and applied sciences such as turbulence , flows in random media , solid mechanics , filtering , and finance .",
    "numerical simulations of spdes are typically based on the monte carlo ( mc ) or the polynomial chaos ( pc ) methods @xcite . in both cases , the long - time simulation of spdes proves to be quite expensive  @xcite . in this paper , we extend the pc - based method developed for stochastic differential equations in @xcite to the setting of spdes .",
    "the pc method originates from the works @xcite and enables us to expand ( square integrable ) functionals of brownian motion in a basis of hermite polynomials .",
    "applications of such a framework to random flows and turbulence theory are examined in @xcite .",
    "more recently , works in @xcite combined the pc method with the karhunen  loeve ( kl ) expansion @xcite to study structural mechanics problems .",
    "the generalized polynomial chaos ( gpc ) developed in @xcite next extends the hermite pc to a set of non - gaussian random parameters .",
    "the main advantage of the pc method is that it allows us to propagate stochasticity by providing expansions of quantities of interest in terms of appropriate uncertainties while in effect replacing the stochastic equations by systems of deterministic equations .",
    "once these deterministic equations are solved , the statistical properties of the solution can be readily inferred from the coefficients of the expansion , which facilitates uncertainty quantification . in some cases ,",
    "the pc method can propagate uncertainties with a substantially lower cost than mc methods ; especially for low dimensional uncertainties  @xcite ; see also @xcite .",
    "however , in cases of high dimensional random parameters , the efficiency of the pc method is reduced because of the large number of terms that appear in the expansion .",
    "the presence of a temporal random forcing is a serious challenge to the pc method as the number of stochastic variables increases linearly with time , which hinders the capability of the pc method for long - time computations @xcite .",
    "moreover , the optimality of the initial basis typically degrades as time evolves due to the presence of nonlinearities in the equation .",
    "these drawbacks were addressed in @xcite . in our previous work @xcite , we proposed the _ dynamical generalized polynomial chaos _ ( dgpc ) method to solve low dimensional stochastic differential equations ( sdes ) driven by white noise .",
    "the novelty of the dgpc algorithm is that it utilizes a restart procedure and constructs polynomial chaos expansions ( pces ) dynamically in time by using orthogonal polynomials of the projections of the solution at the current steps and the future random forcing  @xcite .",
    "this allows the algorithm to keep reasonably sparse random bases , and to mitigate the aforementioned curse of dimensionality and loss of optimality .",
    "numerical experiments in that reference illustrate that such a restart procedure has the ability to accurately estimate long - time solutions of sdes ; see the relevant computational and theoretical details in @xcite .",
    "although the algorithm performs well for low dimensional sdes , extension to larger systems is challenging and requires serious modifications . in this manuscript",
    ", we extend dgpc to the framework of spdes driven by white noise .",
    "while solutions to spdes are , in general , high dimensional random fields , they may lend themselves in some cases to low dimensional representations @xcite .",
    "armed with this fact , we propose at each restart to use the kl expansion to compress the solution into a representation involving a finite number of random modes , i.e. , a projection onto a lower dimensional manifold . in cases where the modeling equations contain non - forcing random inputs other than brownian forcing , such as a random viscosity",
    ", the kl expansion is applied to the solution and the random parameters together so that the algorithm automatically selects the intrinsic variables , which have the largest influence on the solution .",
    "since kl expansions are known to be optimal in the mean square sense , at each restart point in time , only a few dominating , most energetic , random modes are chosen and incorporated into pce to represent the future solution . however , the combination of the random kl modes and the random forcing variables brings about high dimensionality .",
    "the computational challenges then become : ( i ) computing orthogonal polynomials of arbitrary multivariate distributions ; ( ii ) keeping the number of terms in the expansion as small as possible .",
    "the construction of orthogonal polynomials of evolving multivariate distributions is possible by estimating their statistical moments  @xcite , which is , in general , a computationally intensive procedure . in this work",
    ", we estimate the moments of the solution using its pce through a sampling approach to greatly reduce their computational cost ; see @xcite for a similar sampling methodology .",
    "we also show that the method is robust with respect to re - sampling .",
    "the second challenge ( ii ) is a major problem for all pc - based methods .",
    "the kl decomposition is computationally expensive as it requires solving a large eigenvalue problem . for problems of moderate size ,",
    "we find that the eigenvalue problem is solved efficiently by using a krylov subspace method . for larger problems , in order to mitigate both memory requirements and computational cost of the kl expansion",
    ", we find low - rank approximations to the covariance matrices based on their pc representations and without assembling them .",
    "the algorithm leverages randomized projection methods as introduced in @xcite , and uses appropriate random matrices to obtain fast and accurate solutions of large eigenvalue problems . after selecting the dominating modes in the kl expansion , we make use of the sparse truncation technique proposed in @xcite to further reduce the number of terms in a pce . for long - time computations",
    ", we also develop an adaptive restart scheme , which adapts the time lag between restarts based on the acuity of the nonlinear effects .",
    "the use of compression techniques to exploit intrinsic lower dimensional structures of solutions of spdes is not new and is in fact necessary in many contexts ; see @xcite .",
    "the novelty of our approach is that a lower dimensional representation of the solution is learned online and integrated into a pce to integrate future random forcing and represent future solutions .",
    "this procedure is computationally viable and the combination of the aforementioned ideas allows us to attain a reasonable accuracy in the long - time evolutions of spdes for a reasonable computational cost .",
    "as we are interested in the long - time evolution of spdes , we restrict ourselves here to dynamics with a dissipation mechanism .",
    "equilibrium statistics and asymptotic properties of the solutions are relevant in many applications and have been extensively studied in the literature @xcite .",
    "based on these motivations , we provide numerical experiments for a 1d randomly forced burgers equation and a 2d stochastic navier  stokes ( sns ) system .",
    "all equations are driven by white noise and satisfy periodic ( spatial ) boundary conditions . to demonstrate the efficiency of our algorithm , we present both short- and long - time computations . in some cases , we model the viscosity as a random process .",
    "statistical moments obtained by the algorithm are compared to the standard mc methods for short times to assess the accuracy of the algorithm .",
    "results show promising speed - ups over standard mc methods .",
    "we exhibit convergence behavior in terms of the degree of the expansion , the number of random modes retained in the kl expansion , and the ( adaptive ) restart time . in some cases",
    ", we provide a purely pce - based numerical verification of the convergence of the process to its invariant measure .",
    "the outline of the paper is as follows .",
    "section [ sec : kl_pc ] introduces the basic setting for the pc and kl expansions . in the following section ,",
    "the main algorithm and its implementation details are discussed .",
    "numerical experiments are conducted in section [ sec : numerical ] using several settings for the stochastic burgers equation and navier  stokes system .",
    "some concluding remarks are offered in section [ sec : summary ] .",
    "throughout the manuscript , we consider the following time - dependent stochastic partial differential equation ( spde ) driven by the white noise @xmath0 : @xmath1 , \\\\ & u(x,0,\\omega)= u_0(x,\\omega ) , \\quad x \\in g , \\ , \\omega \\in \\omega , \\end{aligned } \\right.\\end{aligned}\\ ] ] where , for concreteness , @xmath2 is the @xmath3-dimensional torus so that @xmath4 and its derivatives are periodic functions in the variable @xmath5 . the dgpc algorithm may easily be extended to more general boundary conditions and geometries . above , @xmath6 is a possibly non - linear differential operator in the spatial variables .",
    "the solution takes values in @xmath7 . in the numerical simulations ,",
    "the parameters @xmath3 and @xmath8 are set to either @xmath9 or @xmath10 .",
    "given a probability space @xmath11 , where @xmath12 is a sample space equipped with the sigma - algebra @xmath13 and the probability measure @xmath14 , we denote by @xmath15 the hilbert space of square integrable random fields on @xmath2 . for a random field @xmath16 , we define the expectation @xmath17 = \\int_{\\omega } u(x,\\omega ) { \\mathbb{p}}(d\\omega),\\ ] ] and the covariance @xmath18 , \\quad x , y \\in g,\\end{aligned}\\ ] ] where @xmath19 denotes the transpose .",
    "the kl expansion of a time - independent random field @xmath16 with a continuous covariance is as follows : @xmath20 where the eigenvalues @xmath21 are nonnegative , and the eigenfunctions @xmath22 and the mean zero random variables @xmath23 are given by @xmath24 \\ , , \\phi_l \\rangle_{l^2(g)}.\\end{aligned}\\ ] ] here , @xmath25 denotes the spatial inner product on @xmath2 .",
    "the spatial and random modes satisfy bi - orthogonality , i.e. @xmath26 = \\delta_{lk}$ ] and @xmath27 , where @xmath28 denotes the kronecker delta function ; see @xcite .",
    "the major feature of the kl decomposition is that the truncation after a finite number , denoted by @xmath29 hereafter , of terms is optimal in the @xmath30 sense .",
    "how @xmath29 should be chosen obviously depends on the spectrum of the covariance operator .",
    "when the process shows a high degree of correlation , then typically @xmath29 may be chosen relatively small due to the rapid decay of the eigenvalues @xcite .",
    "this makes the kl expansion a useful dimensionality reduction technique in many applications and will play a crucial role in our algorithm to compress the dimensionality of stochastic solutions of .",
    "we assume that the random solution @xmath31 of is a second - order stochastic process for all @xmath32 $ ] , and for simplicity , that the initial condition is deterministic .",
    "the pc method for is constructed as follows .",
    "given a complete countable orthonormal system @xmath33 $ ] , @xmath34 , we project the brownian motion @xmath35 onto @xmath36 $ ] by defining @xmath37 .",
    "then , @xmath38 consists of a countable number of independent and identically distributed standard gaussian random variables , and the convergence @xmath39 ^ 2   \\rightarrow 0 , \\quad k \\rightarrow \\infty,\\end{aligned}\\ ] ] holds for all @xmath40 .",
    "basic examples of orthonormal systems are trigonometric functions and wavelets ; see  @xcite .",
    "the spde solution is a nonlinear functional of brownian motion on the interval @xmath41 $ ] .",
    "therefore , the projection onto @xmath42 enables us to consider the solution depending on a countable number of random variables , i.e. , @xmath43 .",
    "then , the polynomial chaos expansion ( pce ) of the solution @xmath44 at @xmath32 $ ] is given by @xmath45 .",
    "\\end{aligned}\\ ] ] note that the expansion is a sum of orthogonal projections onto orthogonal subspaces spanned by the wick polynomials @xmath46 , where @xmath47 is the @xmath48th degree normalized one - dimensional hermite polynomial and @xmath49 belongs to set of multi - indices with finite number of nonzero components @xmath50 the completeness of the orthonormal polynomials @xmath51 in @xmath30 , i.e. convergence of the expansion , is established in  @xcite and the result is known as the cameron - martin theorem . in the following , we will use the term hermite pce to emphasize that the expansion utilizes gaussian random variables .",
    "all statistical information of the random field is contained in the deterministic coefficients @xmath52 of the expansion .",
    "specifically , the first two moments are given by @xmath53 = u_{\\boldsymbol{0}}(x , t ) , \\qquad { \\mathbb{e}}[u^2 ] = \\sum_{{\\boldsymbol{\\alpha}}\\in \\mathcal{j } } u_{\\alpha}^2(x , t).\\end{aligned}\\ ] ] higher order moments can be obtained using the triple products @xmath54 $ ] or the hermite product formula for @xmath42 ; see @xcite .    in practice , we truncate the expansion using polynomials of maximum degree @xmath55 and @xmath56 number of random variables @xmath57 , which results in the following approximation @xmath58 this simple truncation gives rise to @xmath59 terms in the approximation .",
    "two major problems of the pce in this context are as follows . to maintain a prescribed accuracy",
    "as time evolves , the degree of freedom @xmath56 should grow in accordance with the error behavior of , which brings about high dimensional randomness that needs to be incorporated in the expansion . therefore , the computational cost increases rapidly with dimension , which decreases the efficiency of the pc method .",
    "the second related problem of pc is that initial predetermined bases may lose their optimality and even fail to converge for long - time evolutions  @xcite .      to alleviate the aforementioned bottlenecks of the pc method , namely high dimensionality and long - term integration ,",
    "several approaches have been proposed  @xcite . in the following ,",
    "we briefly summarize the approach called _ dynamical generalized polynomial chaos _ ( dgpc ) , which was developed in @xcite to solve ( reasonably low dimensional ) stochastic differential equations ( sdes ) driven by brownian motion over long times .",
    "the solution @xmath60 of a @xmath3-dimensional sde system can be seen as a nonlinear functional of the initial condition @xmath61 and the countable number of variables @xmath42 .",
    "the approach in @xcite hinges upon two important observations : ( i ) for sufficiently small time lags @xmath62 , the solution @xmath63 , can be efficiently captured by low - order polynomials in @xmath64 ; and ( ii ) the solution @xmath63 depends on brownian motion only on the interval @xmath65 $ ] by the markov property .",
    "these two remarks allowed us to employ evolving pces based on polynomials of the random forcing and projections of the solution onto underlying chaos spaces . in this way , the algorithm can  forget \" about the past and pc representations remain reasonably sparse in the evolving basis while the accuracy is retained for long times .",
    "to be more specific , and given a set of increasing restart times @xmath66 on @xmath67 , @xmath68 and @xmath69 , the algorithm projects the random variable @xmath70 onto a finite chaos space at each @xmath66 .",
    "this projection will be denoted by @xmath71 in the following .",
    "a major challenge in the implementation is to construct orthogonal polynomials of the varying distribution of the random variable @xmath71 .",
    "the pce gives rise to natural formulas to compute statistical moments of the random variable .",
    "the algorithm constructs orthogonal polynomials of @xmath71 based on the knowledge of its moments via a gram  schmidt procedure . to obtain evolution equations for the expansion coefficients",
    ", the algorithm then computes necessary triple products for @xmath71 and performs galerkin projection onto subspaces spanned by orthogonal polynomials in @xmath71 and @xmath42 .",
    "convergence of the algorithm is related to the unique solvability of the moment problem of the measures associated to variables @xmath71 ; see @xcite .",
    "related theoretical aspects and the estimates of computational costs can be found in detail in @xcite .    in @xcite ,",
    "the algorithm is applied to low dimensional nonlinear sde systems and provides accurate long - time simulations with a small cost compared to standard hermite pce . in particular",
    ", we computed invariant measures of sdes in some cases , which standard hermite pce typically fails to achieve .",
    "in the subsequent sections , we propose an extension of the dgpc algorithm to spdes , which requires several key modifications .",
    "in this section , we present the algorithmic details of the dgpc method applied to the general spde .",
    "the main ideas are summarized as follows .",
    "let a decomposition @xmath72 be given .",
    "following our discussion in section [ sec : dgpc ] and using the markov property , the solution @xmath73 , @xmath74 , can be represented in a pc expansion in terms of @xmath75 and @xmath76 , where @xmath77 denotes the gaussian random variables required for brownian forcing on the interval @xmath78 $ ] .",
    "let @xmath79 denote the projection of the solution @xmath80 onto the polynomial chaos space .",
    "we will also use the shorthand notation @xmath71 to denote this projection . to construct a pce in terms of polynomials of @xmath71 , we separate the spatial dependence and randomness via the kl expansion :",
    "@xmath81 let @xmath82 denote the random modes .",
    "since the solution @xmath31 , @xmath83 $ ] , is a functional of the random forcing @xmath76 and modes @xmath84 , the next step pce @xmath85 is given by @xmath86,\\end{aligned}\\ ] ] with the notation @xmath87 , where @xmath88 denotes an orthonormal basis in its arguments .",
    "the expansion dynamically needs a pc basis depending on the random forcing and evolving random kl modes of the solution .",
    "the coefficients @xmath89 satisfy a pde system obtained by galerkin projection of the spde onto the space spanned by @xmath90 .",
    "statistical properties can be retrieved after solving the induced pde system .",
    "here are the computational bottlenecks of this approach : ( i ) the simple truncation yields a large number of terms in the expansion , which leads to long computational times to solve the deterministic evolution equations ; ( ii ) estimating the terms appearing in the kl expansion is a major computational bottleneck , especially in higher spatial dimensions ; and ( iii ) computation of the orthogonal polynomials @xmath91 may also require intensive amount of computation . in the following , we address these issues in turn .      the number of terms in the simple truncation in the hermite pce increases rapidly with respect to @xmath55 and @xmath56 . given sufficient regularity of the solution , the expansion coefficients decay both in the number of gaussian variables @xmath56 and the degree of polynomials @xmath55 .",
    "this observation led the authors @xcite to introduce a sparse truncation of the multi - index set and retain a truncated random basis , which keeps lower ( higher ) order polynomials in @xmath92 with larger ( smaller ) subscripts .",
    "this truncation can be quantified using an estimate for the decay rate of the coefficients ; see @xcite .",
    "following @xcite , we introduce a sparse index @xmath93 and define the corresponding sparse multi - index set @xmath94 basically , the index @xmath95 keeps track of how much degree we want in each variable @xmath92 . using , one can define the corresponding version of the pc expansion which might have drastically reduced number of terms .",
    "this is possible by a suitable choice of the index @xmath95 so that ineffective cross terms in high degree polynomials are eliminated .      at each restart @xmath66",
    ", we employ the kl expansion for the projected random field @xmath71 , and the expansion is truncated after a finite number of @xmath29 terms .",
    "the decomposition yields the eigenvalues @xmath96 and the eigenfunctions @xmath97 for @xmath98 .",
    "therefore , in addition to the gaussian variables @xmath76 , we have the random modes @xmath84 in the pce so that the total number of random variables becomes @xmath99 . to accommodate @xmath84 , we extend the multi - index set to @xmath100 .",
    "this can also be done by the tensorization @xmath101 of the multi - index sets since @xmath76 and @xmath84 are independent . however , since tensorization yields higher number of terms in the pce for most values of @xmath56 and @xmath29 , it is not considered in the following .",
    "assuming that the orthonormal basis @xmath102 is constructed in the previous step , the solution @xmath80 is approximated using the truncated pce @xmath103 where the time dependence of the coefficients @xmath104 is omitted for brevity . to avoid confusion",
    ", we note that both the projection and its truncation will be denoted by @xmath71 .",
    "armed with this approximation , using the orthogonality of random bases , the covariance of @xmath71 is easily estimated by @xmath105 where @xmath106 .    in practice",
    ", we consider a discretization of the spatial domain @xmath2 with an even mesh parameter @xmath107 and grid points @xmath108 , @xmath109 .",
    "denote by @xmath110 the resulting covariance matrix .",
    "in general , we can use a spectral method , e.g. , fourier series in the case of periodic functions , to approximate the coefficients @xmath111 as @xmath112 where @xmath113 are orthogonal global basis functions on @xmath2 and @xmath114 ; see @xcite .",
    "thus , @xmath104 is approximated by a vector @xmath115 on the grid .",
    "therefore , the dimension of the covariance matrix @xmath110 becomes of order @xmath116 .    after computing the covariance matrix ,",
    "the corresponding eigenvalue problem can be solved for the first @xmath29 largest eigenvalues .",
    "the random modes @xmath84 are given as @xmath117 ) \\ , , \\phi_{j , l } \\rangle_{l^2(g ) } = \\frac{1}{\\sqrt{\\lambda_{j , l } } } \\sum_{{\\boldsymbol{\\alpha}}>0 }   \\langle u_{j,{\\boldsymbol{\\alpha } } } , \\phi_{j , l } \\rangle_{l^2 } \\ ,",
    "t_{{\\boldsymbol{\\alpha}}}({\\boldsymbol{\\xi}}_{j-1 } , { \\boldsymbol{\\eta}}_{j-1 } ) , \\quad l=1,\\ldots , d .",
    "\\end{aligned}\\ ] ] this representation yields the random modes @xmath84 as a function of @xmath118 and the previous modes @xmath119 . here , we assume that the integrals @xmath120 can be computed by an accurate quadrature method .    when the solution @xmath4 is more than one - dimensional , several implementations of the kl expansion can be considered .",
    "for instance , we may apply the kl expansion to each component of the solution @xmath4 separately and incorporate the resulting individual random modes into one pce .",
    "although this approach certainly makes the kl step faster , we found that it yielded inaccurate results in dgpc and needed a large number of variables in the pce to represent the solution accurately since cross covariance structures between the components of the solution are lost .",
    "therefore , in the following , we implement the kl expansion directly to the multi - dimensional solution and produce one set of random modes @xmath121 which represents all of its components .",
    "depending on the resolution of the discretization of the domain @xmath2 and the dimension @xmath3 , assembling the covariance matrix and solving the corresponding eigenvalue problem may prove dauntingly expensive .",
    "several methods have been devised to reduce computational costs , such as fast fourier techniques ( e.g. , in the case of stationary covariance kernels ) or sparse matrix approximations together with krylov subspace solvers @xcite .    here , the assembly of the covariance matrix is performed at each restart via the summation formula . in our one dimensional simulations , with @xmath122 , this assembly can be carried out reasonably fast . since the solution of the eigenvalue problem",
    "is required only for the number @xmath123 of eigenvalues and eigenfunctions , krylov subspace methods @xcite perform well .",
    "we also utilize the implicitly restarted arnoldi method to efficiently find the few largest eigenvalues and corresponding eigenfunctions ; @xcite .    in the higher dimensional simulations , when @xmath124 , computing and storing such large covariance matrices become challenging .",
    "covariance matrices are , in general , not sparse and require @xmath125 units of storage in the memory . moreover ,",
    "assembling a large matrix at every restart is computationally very expensive for long - time simulations .",
    "the problem of computing and storing a large covariance matrix resulting from the kl expansion is not new and was addressed before in @xcite . it was noted that although the covariance matrices were dense , they were usually of low - rank ; @xcite . based on this observation , we next introduce an approximation approach , which leverages low - rank structures and avoids assembling large matrices .",
    "a low - rank approximation @xmath126 tries to capture most of the action of the matrix @xmath110 by a product of two low - rank matrices @xmath127 and @xmath128 .",
    "several efficient algorithms , e.g. , the fast multipole method and @xmath129-matrices , depend on low - rank approximations @xcite .",
    "we approximate eigenvalues and eigenvectors of the correlation matrix @xmath110 by using low - rank approximations as follows .    given a low rank approximation @xmath130 of the symmetric covariance matrix @xmath110 , where the matrix @xmath131 is of size @xmath132 with @xmath133 orthonormal columns , the eigenvalue problem of @xmath110 can be approximated efficiently by applying @xmath134 or @xmath135 algorithm to the much smaller matrix @xmath136 . in the dgpc setting , this amounts to computing @xmath137 the explicit assembly of the covariance matrix @xmath110 is avoided by computing only the matrix - vector product @xmath138 .",
    "an approximate eigenvalue decomposition @xmath139 is deduced from the eigenvalue decomposition of the smaller matrix @xmath140 by setting @xmath141 .",
    "the crucial step of the computation is the construction of a low - rank matrix @xmath131 with @xmath142 orthonormal columns that accurately describes @xmath110 .",
    "we tried an approach based on the discrete unitary fourier transform to map the coefficients @xmath143 to the frequency space and retain only the lowest frequencies . although this approach allowed us to obtain reasonable compressions of the covariance matrix and enabled faster computations , the following approach consistently yielded much better results .",
    "following @xcite , we construct the matrix @xmath131 using random projections .",
    "algorithm 4.1 in @xcite draws an @xmath144 gaussian random matrix @xmath145 and forms the matrix @xmath146 . the matrix @xmath131 with @xmath147 orthonormal columns",
    "is then obtained by the @xmath134 factorization @xmath148 .",
    "note again that we do not assemble the matrix @xmath110 .",
    "rather , the matrix - matrix product @xmath149 is computed as in equation @xmath150 .",
    "since we require the largest @xmath29 eigenvalues , the target low - rank becomes @xmath29 . as indicated in @xcite , we use an oversampling parameter @xmath8 by setting @xmath151",
    ". typical values of @xmath8 are @xmath152 or @xmath153 .",
    "since eigenvalues decay rapidly in our applications , we found @xmath154 to be accurate . with overwhelming probability , the spectral error @xmath155 is bounded within a small polynomial perturbation of the theoretical minimum , the @xmath156 singular value of @xmath110 ; for relevant theoretical details , see ( * ? ? ?",
    "* section 10 ) .    in practice",
    ", we found this randomized approach to be highly accurate in our computational simulations .",
    "moreover , since assembly of large covariance matrices are avoided , running times and memory requirements for the kl expansion in @xmath157 are reduced drastically compared to the previously described methods ; see section [ sec : sns ] .      in this subsection",
    ", we consider the case in which the differential operator @xmath6 in contains additional random input parameters , i.e. , @xmath158 .",
    "the random inputs will be denoted by the process @xmath159 of a dimension @xmath160 .",
    "a typical case is that of a random viscosity , e.g. , depending on a set of uniformly distributed random variables .",
    "we assume that the process @xmath161 is independent of time and that the corresponding orthogonal polynomials are available ; for instance in the askey family @xcite .",
    "we first observe that the solution @xmath31 is now a functional of brownian motion @xmath162 and the random process @xmath161 . therefore ,",
    "assuming @xmath30 integrability , it can be written as a pce in terms of the associated orthogonal polynomials of @xmath162 and @xmath161 . at the restart @xmath66",
    ", there are two options to carry out the kl expansion : ( i ) apply the kl expansion to only the solution @xmath71 and keep the basis variables for @xmath161 in addition to @xmath84 in the next pce ; and ( ii ) compress the combined random variable @xmath163 using the kl expansion and denote by @xmath84 the combined random modes which represent both @xmath71 and @xmath161 . the first approach will yield pces which provide functional representations of the solution in terms of @xmath162 and @xmath161 for each time @xmath66 . in the second approach",
    ", the functional dependence of the solution in terms of @xmath161 is lost in the first kl expansion step .",
    "however , the moments of the solution can still be computed through the combined random kl modes . in many uq settings , rather than a functional dependence ,",
    "it is statistical information of the underlying solution , e.g. , moments of the invariant measure in the long - time , that we are after .",
    "moreover , the second approach can be seen as a dimensionality reduction technique , which compresses @xmath71 and",
    "the process @xmath161 together , thereby further reducing the number of terms in pce . when additional random parameters appear in the equation , we found it reasonable to implement the second approach to reduce cost while the first approach may be used as a reference computation to assess accuracy .",
    "it is useful to note that by combining the random fields @xmath71 and @xmath161 , the algorithm automatically chooses the important part of the random process @xmath161 that influences the solution while keeping the moments of the solution accurate ; see section [ sec : numerical ] .      after obtaining the random modes @xmath84 , @xmath164",
    ", we need to construct the following orthonormal basis : @xmath165 notice that since @xmath76 is gaussian and identically distributed for each @xmath166 , the corresponding orthonormal polynomials of @xmath76 are known to be the hermite polynomials for each @xmath166 .",
    "however , the probability distribution of @xmath84 is arbitrary and changes at each restart .",
    "therefore , the computation of orthonormal polynomials is computationally intensive and can be performed using the gram ",
    "schmidt method as follows .",
    "we note that the set can be computed based on the knowledge of moments of the variables @xmath76 and @xmath84 .",
    "following @xcite , we assemble the gram matrix @xmath167 with the entries @xmath168 , \\quad { \\boldsymbol{\\alpha}}_k,{\\boldsymbol{\\alpha}}_l \\in   \\mathcal{j}^r_{k+d , n}.    \\end{aligned}\\ ] ] the matrix @xmath167 is a @xmath169-dimensional , square and symmetric matrix . for theoretical reasons ,",
    "we assume that the moments up to @xmath170 exist and the measure of @xmath171 is non - degenerate .",
    "then , the cholesky factorization is employed to @xmath167 and the polynomials @xmath172 is found by inverting the resulting upper triangular matrix as @xmath173 where @xmath174 are real coefficients .",
    "the kl expansion yields uncorrelated random variables @xmath84 .",
    "if the underlying process is gaussian , it is known that these variables are also independent . however , in general , marginals of @xmath84 are dependent variables .",
    "multi - index operations can still be used to construct the polynomial set with respect to the joint distribution , although the estimation of multivariate moments of @xmath84 becomes necessary because of such a dependency . in this case",
    ", it is known that orthogonal polynomials are not unique and depend on the ordering imposed on the multi - index set ; see @xcite . in all computations , we use the graded lexicographic ordering for multi - indices .    the completeness of the orthogonal polynomials @xmath175 is closely related to the moment problem of the random variables @xmath76 and @xmath84 . in particular , if the moment problem is uniquely solvable , i.e. , the measure is determinate , then the orthogonal polynomials are dense in @xmath30  @xcite . some basic conditions that guarantee determinacy of the measure of a continuous random variable on a finite dimensional space are compact support and exponential integrability .",
    "gaussian measures are determinate and the hermite pce converges by the cameron - martin theorem . however , in general , whether the distribution of @xmath84 is determinate or not is unknown .",
    "this problem is addressed in our previous work in the case of finite dimensional sde systems ; see @xcite .",
    "theoretical results are applied in the setting where the solutions are approximated by compactly supported distributions under appropriate assumptions . in the following ,",
    "we assume that the measures associated to @xmath84 are determinate so that convergence is guaranteed , which is consistent with our numerical simulations ; see section [ sec : numerical ] .    based on the above discussion",
    ", the orthonormal basis requires the computation of the moments .",
    "the exact moments of the gaussian variables @xmath76 are computed by analytical formulas and then stored during the offline stage .",
    "however , the distribution of @xmath84 is varying with @xmath166 .",
    "therefore , the computation of moments should be carried out based on information provided by the pce .",
    "several methods are available to compute moments of probability distributions in the pc context such as , e.g. , quadrature methods , monte carlo sampling , or a pure pce approach .",
    "this procedure is notoriously expensive and ill - posed @xcite .",
    "the pure pce approach computes the moments of @xmath84 by repeatedly multiplying its pce and taking expectation ; see @xcite .",
    "this approach is discussed in detail in our earlier manuscript @xcite and works reasonably well for a low dimensional sde systems .",
    "however , it becomes prohibitively expensive if the dimension of the random variables in the pc basis is even moderate ; see the computational complexity section in @xcite .",
    "therefore , in this work , we consider an alternative approach using monte carlo sampling , which drastically reduces the computational cost for computing moments compared to the pure pce approach .",
    "we assume that independent samples of the initial condition ( therefore , the samples of @xmath176 ) are provided so that the algorithm can be initialized . to construct the set , based on",
    ", we need to compute the first @xmath170 moments of the joint random variable @xmath171 .",
    "moreover , since the triple products will be required for the evolution of the pce coefficients , the first @xmath177 moments need to be computed as well ; see section [ sec : galerkin_proj ]",
    ". using the same ordering of the multi - index set @xmath178 , we require the computation of the following moments : @xmath179 =   { \\mathbb{e}}[{\\boldsymbol{\\xi}}_j^{(\\alpha_1,\\ldots,\\alpha_k ) } ] \\,{\\mathbb{e}}[{\\boldsymbol{\\eta}}_j^{(\\alpha_{k+1},\\ldots,\\alpha_{k+d } ) } ]    , \\quad { \\boldsymbol{\\alpha}}\\in \\mathcal{j}^r_{k+d,3n},\\end{aligned}\\ ] ] where we used the independence of @xmath76 and @xmath84 .",
    "let @xmath180 denote independent samples of the random modes for @xmath181 , where @xmath182 .",
    "then , provided the samples @xmath183 are given , the moments of @xmath184 can be approximated by @xmath185   \\approx \\frac{1}{s } \\sum_{i=1}^s ( { \\boldsymbol{\\eta}}_j(\\omega_i))^{(\\alpha_{k+1},\\ldots,\\alpha_{k+d } ) }   = \\frac{1}{s } \\sum_{i=1}^s   \\prod_{l=1}^d ( \\eta_{j , l}(\\omega_i ) ) ^{\\alpha_{k+l}},\\ ] ] where we used the usual multi - index notation for powers .",
    "therefore , multivariate moments are computed by a combination of the analytical formulas for @xmath76 and a sampling approximation for @xmath84 . note that in applications , we use small values of @xmath55 with a sufficiently large number of samples @xmath186 to guarantee accuracy .    although we discussed computing moments based on samples ,",
    "we have not explained how the samples of @xmath84 are acquired except for @xmath176 .",
    "the distribution of @xmath84 , @xmath187 , is evolving in time .",
    "however , owing to the pce of the each component @xmath188 , we can write @xmath189 this representation gives a natural way to sample from the distribution of @xmath84 by the recursion @xmath190 assuming that we obtained samples of @xmath191 at the previous restart @xmath192 .",
    "independent samples @xmath193 are obtained through sampling a multivariate gaussian distribution .",
    "therefore , on the subinterval @xmath194 $ ] , pce acts like a transport map which maps previously obtained samples of @xmath119 and new samples of @xmath118 to the samples of the new random modes @xmath84 .    note that @xmath84 is a function of the variables @xmath118 and @xmath119 . therefore , the number of samples of @xmath84 should be ideally @xmath195 provided the same of number of samples @xmath186 is used for each @xmath118 and @xmath119 . however , in practice",
    ", this is not feasible in our method as the number of samples grows in time .",
    "instead , the equation keeps only the diagonal terms in the sample space . in the numerical simulations , we use large values of @xmath186 so that the loss of accuracy incurred from discarding some samples would be minimal .",
    "moreover , it is important to notice that entails samples from the joint distribution of @xmath84 so that monte carlo method is used to approximate the expectations while preserving the dependence structure of marginals .",
    "[ rmk : samples ] the method blends monte carlo sampling into a pc approach to exploit the virtues of the both methods , namely , rapid computation of expectations and spectral accuracy provided by the mc and pc methods , respectively . although the method is utilizing samples for the computation of moments , samples are not used in the evolution stage .",
    "the algorithm essentially propagates moments of the measures between successive times , where moments are computed using a sampling technique . to test the robustness of the method with respect to sampling ,",
    "imagine that the algorithm starts with an infinite supply of independent samples of @xmath196 .",
    "we discard the first sample after using it to construct the corresponding orthogonal polynomials at the end of the first time interval and propagate the remaining samples with the pce map to construct ( an infinite supply of ) samples of @xmath197 .",
    "the algorithm iteratively estimates the distribution , hence the moments , of each @xmath84 while samples are discarded at each restart .",
    "we tested this idea by starting with a set of @xmath48 independent samples of @xmath176 and propagated them by pce for a maximum of @xmath48 restarts .",
    "we found that the accuracy of the calculations was not affected by such a re - sampling tool .",
    "this comparison showed the robustness of the algorithm under changes of samples . since in practice ,",
    "such re - sampling increases the computational costs compared to , it is not considered in the numerical experiments presented in the next sections .",
    "we also emphasize that the sampling approach readily returns samples of the approximated solution at the endpoint @xmath198 through its kl expansion without a further sampling procedure .",
    "these samples can also be useful in uncertainty quantification to estimate further statistical properties of the solution such as probabilities on prescribed sets or probability density functions .",
    "once an orthonormal basis is obtained , the algorithm performs a galerkin projection onto the space spanned by the basis , and this requires the computation of triple products . using the representation ,",
    "the required triple products can be written as @xmath199 = \\sum _ { { \\boldsymbol{\\alpha}}_{k ' } \\leq { \\boldsymbol{\\alpha}}_k}\\sum _ { { \\boldsymbol{\\alpha}}_{l ' } \\leq { \\boldsymbol{\\alpha}}_l } \\sum _ { { \\boldsymbol{\\alpha}}_{m ' } \\leq { \\boldsymbol{\\alpha}}_m }   a_{k'k } \\ , a_{l'l } \\ , a_{m'm } \\ , { \\mathbb{e}}[({\\boldsymbol{\\xi}}_j,{\\boldsymbol{\\eta}}_j)^{{\\boldsymbol{\\alpha}}_{k ' } + { \\boldsymbol{\\alpha}}_{l'}+ { \\boldsymbol{\\alpha}}_{m ' } } ] , \\end{aligned}\\ ] ] where all multi - indices belong to the set @xmath100 . thus this formula can be computed by the knowledge of moments of order up to @xmath177 .",
    "depending on the choice of the sparse index @xmath95 , the multi - index @xmath200 might not be an element of @xmath178 .",
    "therefore , once we fix the multi - index set @xmath100 in the offline stage , we also compute and store the indices that are elements of @xmath178 .",
    "finally , we perform galerkin projection of the spde and obtain the following pde system for the coefficients @xmath89 of , @xmath83 $ ] : @xmath201 +   \\sigma(x ) \\ , { \\mathbb{e}}[t_{{\\boldsymbol{\\alpha}}}({\\boldsymbol{\\xi}}_j , { \\boldsymbol{\\eta}}_j ) \\dot{w}(t)].\\end{aligned}\\ ] ] the first expectation in the above line is computed with the aid of the triple products and the second using the representation .",
    "note that the initial conditions @xmath202 can be obtained by noticing that the representation of @xmath71 is nothing but a sum involving linear polynomials in @xmath188 .",
    "it can therefore be rewritten in the basis @xmath203 with the help of galerkin projections .",
    "hence , the only coefficients that survive in at @xmath204 are the mean and the ones which correspond to the first degree polynomials in @xmath84 .",
    "then , the pde system can be solved in time using a time - integration method combined with the aforementioned spectral method .",
    "if the initial condition @xmath61 of is deterministic , we employ the hermite pce on the first interval @xmath205 $ ] , which does not necessitate the computation of the kl decomposition .",
    "so far , the method uses a predetermined restart time @xmath206 . for long - term simulations , an adaptive restart scheme that sets the restarts online depending on the properties of the solution can reduce the computational cost .    we propose to adapt the restart time based on the following two criteria : ( i ) preserve the accuracy of the representation of the random forcing ; and ( ii ) mitigate the effect the nonlinearities in the accuracy of the polynomial expansions . for a prescribed number of dimensions to describe the random forcing",
    ", the algorithm can not take too large steps to preserve accuracy .",
    "also , nonlinearities force the solution to be less accurately described by low - degree polynomials in the initial condition as time increases . in both cases , we wish @xmath206 to be as large as possible for a given accuracy in mind .    to this end , let @xmath207 denote the adaptive time - step starting from time @xmath66 . to ensure an accurate representation of the forcing term , we set a maximum value @xmath208 for @xmath209 for all @xmath166 , i.e. @xmath210 . in practice ,",
    "@xmath208 is based on the error analysis of random forcing by a finite dimensional approximation ; see for instance @xcite . to address nonlinearities ,",
    "we consider the following ratio for the pc coefficients @xmath111 : @xmath211.\\end{aligned}\\ ] ] the condition measures the norm ratio of the nonlinear terms in the variance to the norm of the variance . in applications ,",
    "the ratio is computed at each time integration point .",
    "similar other conditions were used in different settings in @xcite .    consider a threshold value @xmath212 .",
    "we propose the following conditions for adaptive time - steps using @xmath213 $ ] :    a.   @xmath214 , [ item : less3eps ] b.   @xmath215 , [ item : bigger3eps ]    where @xmath216 is a polynomial approximation to @xmath217 , which can be found by fitting a @xmath8-degree polynomial to @xmath217 on the interval @xmath218 $ ] .",
    "this approximation is only required for time values @xmath219 satisfying @xmath220 .",
    "the time - steps @xmath209 , @xmath221 are set adaptively . for short time - steps",
    ", we do not expect dynamics to change drastically between successive intervals .",
    "therefore , condition [ item : less3eps ] verifies whether the ratio is smaller than @xmath222 on the current interval , and then sets the adaptive time - step for the next interval .",
    "when @xmath223 , then the algorithm selects a bigger time - step by finding the root of @xmath224 .",
    "note that the current evolution on the interval @xmath225 $ ] is not prematurely stopped at the end point .",
    "although pces converge at any point inside the interval , errors , however , are known to wildly oscillate inside the interval and become spectrally accurate only at the end point ; see @xcite .",
    "( this is because @xmath226 in is spectrally accurate at @xmath227 and much less so on @xmath67 . )",
    "condition [ item : bigger3eps ] essentially verifies whether the ratio becomes too large ( i.e. @xmath228 ) , and when this happens , forces the evolution to restart from the current initial point @xmath192 .",
    "this control ensures that the algorithm does not take too large steps .",
    "our procedure is summarized in algorithm [ alg : dgpc ] , where , for simplicity , we only present the version which uses a predetermined number of restarts .     + decompose the time domain @xmath41 = [ 0,t_1 ] \\cup \\ldots",
    "\\cup [ t_{n-1},t]$ ] + initialize the degrees of freedom @xmath229 + choose the sparse index @xmath230 + compute the indices used in the triple - product formula + compute moments of @xmath231 apply the kl expansion to @xmath71 and obtain @xmath232 compute the moments @xmath233 $ ] construct orthogonal polynomials @xmath175 compute the associated triple products perform galerkin projection onto @xmath234 set up initial conditions for @xmath235 evolve the pce coefficients of @xmath235",
    "we now present numerical simulations for the burgers equation in one spatial dimension and a navier  stokes system in two spatial dimensions both driven by white noise .",
    "we consider these equations for two reasons .",
    "first , the statistical behavior of solutions of these equations is of importance in statistical mechanics and turbulence theory ; see , e.g. , @xcite .",
    "second , they serve as challenging test beds for the pce methodology .",
    "we illustrate convergence results in terms of the degrees of freedoms @xmath29 , @xmath55 and the time - step @xmath206 , and consider both short - time and long - time evolutions .",
    "the convergence of the method in terms of @xmath56 is treated in detail in @xcite . as a general comment",
    ", we do not recommend using large values of @xmath55 since the computation of orthogonal polynomials is quite ill - posed . in the following , we use polynomials of degree up to @xmath236 .",
    "the algorithm mitigates the ill - posedness by choosing frequent restarts and small degree ; i.e. small @xmath206 and @xmath55 .",
    "the settings we consider here closely follow those addressed in the manuscripts @xcite .",
    "we consider the following one dimensional viscous stochastic burgers equation for @xmath31 : @xmath237 \\times [ 0,t ] ,   \\end{cases}\\end{aligned}\\ ] ] where @xmath226 is a brownian motion in time , @xmath238 is the viscosity ( which will be either deterministic or random ) , the initial condition @xmath61 is deterministic , and the solution itself is periodic in the spatial domain @xmath239 $ ] .    following @xcite , we choose cosine functions as an orthonormal basis @xmath240 , for @xmath241 $ ] . employing the equation for each subinterval @xmath242 $ ] and using galerkin projection , we derive the governing equations for the pc coefficients @xmath89 of @xmath235 : @xmath243,\\ ] ] where @xmath244 . since the initial condition is deterministic , we employ hermite pce in the subinterval @xmath245 $ ] .",
    "the pc coefficients @xmath246 of the nonlinearity @xmath247 are computed by multiplying the corresponding pces with the help of pre - computed triple products .",
    "since the coefficients @xmath89 are periodic in the physical space , we utilize a truncated fourier series : @xmath248,\\ ] ] with the even number of frequencies @xmath249 to be chosen .",
    "further , using the equidistant partition for the spatial domain @xmath239 $ ] and fast fourier transform ( fft ) , we can compute the fourier coefficients @xmath250 with a reasonable computational cost .",
    "this procedure gives rise to an ode system which is then integrated in time using a second order predictor - corrector method combined with an exponential integrator for the stiff term  @xcite .    in the implementation of the burgers equation , the algorithm assembles the covariance matrix at each restart as discussed in section [ sec : klexpansion ] .",
    "a large memory is not required for such a one dimensional spatial problem .",
    "we found that using the random projection technique described in section [ sec : klexpansion ] did not result in significantly shorter total computational times because the computation of the eigenvalue problem is already efficient in this case by means of krylov subspace methods .",
    "[ ex : burgers1 ]    for this numerical simulation , we choose the spatial part of the random forcing as @xmath251 , the initial condition as @xmath252 and set the viscosity @xmath253 . under these sets of parameters",
    ", it has been proved that there exists a unique invariant measure , which is the long - time limit of the dynamics ( * ? ? ?",
    "* theorem 2 ) .",
    "thus , the main aim of the following simulations is to demonstrate the efficiency of the algorithm in the long - time setting .    for the parameters of dgpc",
    ", we take @xmath254 , @xmath255 , and vary the number of the kl modes @xmath29 .",
    "final time is @xmath256 and the interval divided into @xmath257 pieces by taking @xmath258 .",
    "the spatial mesh size @xmath249 is set to be @xmath259 and the number of samples @xmath186 to compute moments is taken as @xmath260 .",
    "finally , sparse indices @xmath95 are chosen as follows :    a.   if @xmath261 , then @xmath262 , and if @xmath263 , we set @xmath264 resulting in @xmath265 terms in the expansion .",
    "b.   if @xmath266 , then @xmath267 , and if @xmath263 , we set @xmath268 resulting in @xmath269 terms in the expansion . c.   if @xmath270 , then @xmath271 , and if @xmath263 , we set @xmath272 resulting in @xmath273 terms in the expansion .",
    "note that the first @xmath56 indices in @xmath95 correspond to degrees of polynomials in @xmath42 and the remaining to @xmath121 .",
    "choosing @xmath267 means using only first degree polynomials is @xmath274 and @xmath275 .",
    "also , setting @xmath268 for @xmath276 eliminates the cross terms involving first degree polynomials of @xmath275 in the second order terms .",
    "note that using a sparse index not only reduces the number of terms in pce but also alleviates the computation of moments .    to compare our algorithm",
    ", we use a second order weak runge - kutta scheme since the exact solution is not available . to make a fair comparison , both algorithms ( monte carlo & dgpc ) use the same time - step @xmath277 and the same mesh size @xmath278",
    ". the number of samples used in mc algorithms are @xmath279 and @xmath260 and the corresponding algorithms will be denoted by mc1 , mc2 and mc3 , respectively .",
    "the exact solution is taken as the result of mc3 and the relative @xmath30 error @xmath280 - { \\mathbb{e}}[u_{mc } ] ||_{2}/|| { \\mathbb{e}}[u_{mc}]||_2 $ ] of the mean is computed .",
    "errors for higher order centered moments are computed similarly .    from figure",
    "[ fig : ex1_1 ] , we observe that all errors grow with time in the initial stages and in particular , the degree of freedom @xmath261 is the least accurate , which is expected as the dynamics change rapidly during initial stages .",
    "increasing the number of kl modes entails more accurate expansion up to some order .",
    "it can be observed that all the error levels stabilize for moderate times while @xmath270 is the most accurate .",
    "this phenomenon is explained by the convergence to a stationary measure such that statistics do not change considerably after some time .",
    "we now increase the final time to @xmath281 .",
    "dgpc algorithms use the following parameters : @xmath282 , @xmath255 , @xmath283 , @xmath284 and @xmath258 .",
    "the corresponding total number of terms for each subinterval becomes @xmath285 , @xmath286 and @xmath287 .",
    "the mesh size is taken as @xmath288 , which offers better spatial resolution .",
    "table [ table : burgdet ] summarizes the relative @xmath30 errors of the dgpc algorithms with different degrees of freedom and the mc methods .",
    "all errors are computed by taking mc3 as the exact solution .",
    "the time ratio column is computed as the total time required by the each algorithm divided by the elapsed time of mc3 with @xmath289 and @xmath290 .",
    "the parameters for mc1 and mc2 algorithms remain the same as above ; the algorithms are executed a few times and the resulting errors are averaged .",
    "we also include the elapsed times for the offline computation in dgpc algorithms . in practice ,",
    "the required data for the offline step can be computed once and stored for further executions of the algorithm to speed - up the running time .",
    ".relative errors for the centered moments by dgpc and mc methods at t=6 .",
    "each time ratio is computed by comparing to mc3 . [ cols=\">,^,^,^,^,^\",options=\"header \" , ]     table [ table : rel_err_vor_rand_vis ] exhibits the relative errors of dgpc for the vorticity using the random matrix approach for the kl expansion . comparing table [ table : rel_err_vor_rand_vis ] and [ table : rel_err_fourier_truncation ]",
    ", we see that relative elapsed times of dgpc with respect to mc3 are further improved .",
    "additional randomness for mc means an extra dimension to sample from , whereas for dgpc , it means an extra variable that needs to be compressed into the modes @xmath121 . since the dynamics crucially depend on the behavior of the viscosity , using few realizations for viscosity sampling in mc is not recommended .",
    "iin this setting , we found that our mc simulations demanded a high cpu time compared to dgpc .",
    "note , however , that viscosity sampling could clearly be performed in parallel in a mc framework  something that is not as easily feasible in the pce setting .",
    "the preceding simulations were concerned with short time evolutions of sns and comparisons of the proposed algorithm with a monte carlo method .",
    "numerical results for reasonably short time computations indicated that our algorithm achieved a similar accuracy compared to mc typically for a smaller computational cost .",
    "we are now interested in long time simulations and convergence to steady states . since there is no random forcing acting upon the temperature equation in , the ( uncoupled ) temperature diffuses to zero quickly .",
    "therefore , we only solve the vorticity equation in the system .",
    "the following numerical experiment considers the vorticity equation with a deterministic viscosity @xmath291 and a spatial forcing as described in example [ ex : sns_short1 ] .",
    "the parameters of the simulation are @xmath292 , @xmath293 , @xmath294 and @xmath295 .",
    "pc expansions with thirty number of terms are employed on each subinterval .",
    "the four - step adams predictor - corrector method is used for the time integration .",
    "figure [ fig : ex7_1 ] shows three different initial conditions for the vorticity . the first layer is supported around @xmath296 while the others are aligned horizontally .",
    "widths of all layers are widened and different sinusoidal perturbations are considered .    in figure [ fig : ex7_2 ] , we show the @xmath30-norm of the successive differences of the first two moments in time .",
    "each column represents one of the initial conditions presented in the corresponding column in figure [ fig : ex7_1 ] .",
    "after a ( very ) long time , the norms of the successive differences drop below @xmath297 , which ( numerically ) indicates that statistical moments no longer change significantly in time . in all cases",
    ", we found that the dynamics converged to the same state , which is shown in figure [ fig : ex7_3 ] .",
    "notice that the invariant measure is a non - gaussian random field and the moments have oscillations in the @xmath5 variable .",
    "we also see that high variance regions correspond to where the mean fields display peaks .",
    "based on these findings for this scenario , we assert that the dynamics converge to an invariant measure which is numerically captured in the long - time by the dgpc algorithm .",
    "+    in long time computations , the mc method usually requires the propagation of many realizations in time , which renders the method hardly affordable in some cases .",
    "however , if the dynamical system possesses a unique ergodic invariant measure , the mc method may be carried out to sample such a measure by considering a single , very long time mc realization , which repeatedly visits the whole state space . while carrying out such a sampling is also computationally expensive , it is likely to compare favorably to our dgpc algorithm in this case .    in general , ergodicity or uniqueness of an invariant measure may not be known or not hold for complicated physical dynamics ( e.g. , invariant measures parametrized by a random parameter as in example [ ex : randomvis ] ) . in such cases , our algorithm offers a viable alternative to the mc method to capture the long - term dynamics by providing statistical information resulting from the expansion coefficients .",
    "we have presented a pc - based algorithm , called dynamical generalized polynomial chaos , to tackle long - time integration and high dimensional randomness in the context of pdes with markovian forcing . to deal with these challenges",
    ", dgpc uses a restart procedure , which constructs pces dynamically based on the polynomials of the projections of the solution , the random forcing and the random parameters found in the equation . the karhunen ",
    "loeve expansion is employed at each restart to find a representation of the solutions which correspond to low - dimensional dynamics of the underlying physical processes .",
    "the relevant modes are then incorporated in a pce for the future evolution . using sparse multi - index sets and frequent restarts , the algorithm provides an efficient way to capture the solutions in a fairly sparse random bases in terms of orthogonal polynomials of dynamically evolving measures .",
    "the main computational bottlenecks of the algorithm are the simulation of the deterministic evolution equation , the kl expansion , and the computation of moments .",
    "the cost of the deterministic evolution is dictated by the complicated nature of the spdes .",
    "the kl expansion is an expensive dimensionality reduction technique .",
    "since the algorithm constructs pces online , kl expansions ( or other dimensionality reductions methods ) are unavoidable at each restart . we found that for large covariance matrices , the kl cost was drastically reduced when the covariance matrix was estimated by a low - rank approximation obtained by random projections .",
    "the estimation of the orthogonal polynomials and corresponding triple products of evolving arbitrary measures is also a costly step as in most pc - based methods .",
    "using a 1d randomly forced burgers equation and a 2d stochastic navier  stokes system , we provided several numerical simulations for both short- and long - time solutions .",
    "we compared the accuracy and computational time of the algorithm to the standard monte carlo method and found that the proposed algorithm achieved similar error levels for a ( generally significant ) lower computational cost in most cases .",
    "the substantial speed - up of dgpc is especially promising when the equation contains additional , time - independent , random contributions , which is one of the main reasons to use pc  based methods in general . to demonstrate the efficiency of the algorithm for long time simulations",
    ", we computed invariant measures for both equations , which is not a trivial task for two dimensional navier  stokes systems .",
    "other methods such as the recent multilevel monte carlo techniques offer improvements over the standard mc methods @xcite .",
    "the restarting step of our algorithm remains expensive computationally , especially for problems in two ( or three ) spatial dimensions .",
    "however , the restart method provides a viable means to keep the number of random variables to reasonable levels . its ability to compute statistical properties of long - time evolutions for fairly complicated equations is quite promising .",
    "the authors would like to thank the reviewers for their critical reading of the manuscript , and for multiple remarks and suggestions that improved the presentation of our method .",
    "this work was partially funded by nsf grant dms-1408867 and onr grant n00014 - 15 - 1 - 2679 .",
    "h.  c. ozen , g.  bal , http://dx.doi.org/10.1137/15m1019167[dynamical polynomial chaos expansions and long time evolution of differential equations with random forcing ] , siam / asa j. uncertain .",
    "quantif . 4  ( 1 ) ( 2016 ) 609635 .",
    "r.  ghanem , j.  red - horse , propagation of probabilistic uncertainty in complex physical systems using a stochastic finite element approach , phys .",
    "d 133  ( 1 - 4 ) ( 1999 ) 137144 , predictability : quantifying uncertainty in models of complex phenomena ( los alamos , nm , 1998 ) .",
    "g.  bal , http://dx.doi.org/10.1007/978-3-319-11259-6_9-1[propagation of stochasticity in heterogeneous media and applications to uncertainty quantification ] , in : r.  ghanem , d.  higdon , h.  owhadi ( eds . ) , handbook of uncertainty quantification , springer international publishing , cham , 2016 , pp .",
    "124 .",
    "g.  potte , d.  lucor , http://dx.doi.org/10.1016/j.jcp.2011.12.038[non intrusive iterative stochastic spectral representation with application to compressible gas dynamics ] , j. comput .",
    "phys . 231  ( 9 ) ( 2012 ) 35873609 .",
    "g.  berkooz , p.  holmes , j.  l. lumley , the proper orthogonal decomposition in the analysis of turbulent flows , in : annual review of fluid mechanics , vol .",
    "25 , annual reviews , palo alto , ca , 1993 , pp . 539575 .",
    "m.  jardak , c .- h .",
    "su , g.  e. karniadakis , http://dx.doi.org/10.1023/a:1015125304044[spectral polynomial chaos solutions of the stochastic advection equation ] , in : proceedings of the fifth international conference on spectral and high order methods ( icosahom-01 ) ( uppsala ) , vol .",
    "17 , 2002 , pp . 319338 .",
    "m.  arnst , r.  ghanem , e.  phipps , j.  red - horse , measure transformation and efficient quadrature in reduced - dimensional stochastic modeling of coupled problems , internat .",
    "methods engrg . 92  ( 12 ) ( 2012 ) 10441080 .",
    "n.  halko , p.  g. martinsson , j.  a. tropp , http://dx.doi.org/10.1137/090771806[finding structure with randomness : probabilistic algorithms for constructing approximate matrix decompositions ] , siam rev . 53  ( 2 ) ( 2011 ) 217288 .",
    "j.  xu , j.  li , sparse wiener chaos approximations of zakai equation for nonlinear filtering , in proceedings of the 21st annual ieee international conference on chinese control and decision conference ( ccdc09 ) ( 2009 , ) pp .",
    "910913 .",
    "j.  s. hesthaven , s.  gottlieb , d.  gottlieb , spectral methods for time - dependent problems , vol .",
    "21 of cambridge monographs on applied and computational mathematics , cambridge university press , cambridge , 2007 .",
    "r.  b. lehoucq , d.  c. sorensen , c.  yang , arpack users guide : solution of large - scale eigenvalue problems with implicitly restarted arnoldi methods , vol",
    ".  6 of software , environments , and tools , society for industrial and applied mathematics ( siam ) , philadelphia , pa , 1998 .",
    "b.  j. debusschere , h.  n. najm , p.  p. pbay , o.  m. knio , r.  g. ghanem , o.  p. le  matre , numerical challenges in the use of polynomial chaos representations for stochastic processes , siam j. sci .",
    "comput . 26  ( 2 ) ( 2004 ) 698719 .",
    "h.  n. najm , uncertainty quantification and polynomial chaos techniques in computational fluid dynamics , in : annual review of fluid mechanics .",
    "41 , vol .",
    "41 of annu .",
    "fluid mech . , annual reviews , palo alto , ca , 2009 , pp ."
  ],
  "abstract_text": [
    "<S> we propose a dynamical generalized polynomial chaos ( dgpc ) method to solve time - dependent stochastic partial differential equations ( spdes ) with white noise forcing . </S>",
    "<S> the long - time simulation of spde solutions by polynomial chaos ( pc ) methods is notoriously difficult as the dimension of the stochastic variables increases linearly with time . exploiting the markovian property of white noise , </S>",
    "<S> dgpc @xcite implements a restart procedure that allows us to expand solutions at future times in terms of orthogonal polynomials of the measure describing the solution at a given time and the future white noise . </S>",
    "<S> the dimension of the representation is kept minimal by application of a karhunen  loeve ( kl ) expansion . using frequent restarts and low degree polynomials on sparse multi - index sets , </S>",
    "<S> the method allows us to perform long time simulations , including the calculation of invariant measures for systems which possess one . </S>",
    "<S> we apply the method to the numerical simulation of stochastic burgers and navier stokes equations with white noise forcing . </S>",
    "<S> our method also allows us to incorporate time - independent random coefficients such as a random viscosity . </S>",
    "<S> we propose several numerical simulations and show that the algorithm compares favorably with standard monte carlo methods .    </S>",
    "<S> keywords : stochastic partial differential equations , uncertainty quantification , polynomial chaos , karhunen - loeve expansion </S>"
  ]
}