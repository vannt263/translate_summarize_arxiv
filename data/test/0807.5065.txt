{
  "article_text": [
    "one of the main goals of the search for periodic isolated sources of gravitational waves ( g.w . ) is to perform all sky surveys , based on `` blind searches '' , where the source parameters are unknown . in this case",
    "hierarchical procedures are applied , based on a sequence of increasing resolution steps . in this paper",
    "we study in details the problem of sensitivity loss due to discretization of parameters and to the needs to limit the computing cost , with hough procedures .",
    "in particular , we propose and study the characteristics of a frequency hough procedure , designed mainly to reduce the discretization problem , and we compare it with the sky hough procedure , which is actually used in the virgo collaboration .",
    "+ the paper is organized as follows : in sect .",
    "2 we present the basic scheme of the rome hierarchical procedure , based on the main idea of coincidences among subsets of data ; in sect .",
    "3 we discuss the limits due to digitization of the sky hough procedure ; in sects . 4 , 5 we present the new frequency hough procedure , discussing details its implementation and its basic characteristics ; in sect .",
    "6 we present the study of amplitude losses due to digitization , and thus efficiencies , for both the procedures .",
    "conclusions and comments are given in sect .",
    "hierarchical procedures , based on hough transform algorithms , are applied by various groups in the g.w . community .",
    "see , for example , references @xcite .",
    "there are various ways of implementing the hierarchical procedure and the hough transform .",
    "the hough transform is a linear transform that is used to recognize the parameters of the analytical description of a curve from the position of some points on it .",
    "it operates on an `` image '' of points , in our case the peakmap in the time - frequency plane . for each peak of this map",
    "we increase a set of bins of a multi - dimensional histogram ( in our case a two - dimensional histogram ) defined on the parameters space , called the hough map . in the old procedure ,",
    "the parameter space was the position of the source , i.e. the celestial sphere , and we fixed the spin down value for each hough map . in the new one ,",
    "the parameter space is the plane @xmath0 , and for each hough map , we fix the position of the source . the mapping ( i.e. which points of the hough map must be increased for a certain point in the peakmap ) can be done in different ways : we use always what we call the `` biunivocal mapping '' , i.e. a mapping in which every point in the hough map derive from a single point of the peakmap at a given time .",
    "it is easy to demonstrate that in this case the mapping is also uniform , i.e. in the case of uniformly distributed random dots in the peakmap , the expected value of the hough map @xmath1 is a constant ( for all parameter value ) .",
    "this value , depending on the number n of the spectra of the peakmap and on the mapping , defines the `` noise '' of the map .",
    "it is binomially distributed with parameters n and @xmath2 .",
    "we will refer here to the rome scheme , presently used in virgo data .",
    "[ fig : schema ] shows the basic scheme of the rome hierarchical procedure .",
    "details on the main aspects of the procedure are given in references @xcite .",
    "after data cleaning ( short time domain disturbances removal ) and `` short ffts data base '' ( sfdb ) creation , peakmaps are computed , using a very refined auto - regressive algorithm to equalize the spectral data by an appropriate follow - up of the noise .",
    "peakmaps are frequency vs time maps , obtained from equalized spectra by selecting all the local maxima above a chosen threshold .",
    "an accurate cleaning of peakmaps , by removing known noise lines and the more persistent lines , is needed and its implementation is critical for the next step analysis . on the cleaned peakmaps ,",
    "methods of peaks detection are applied .",
    "that is , transformation from the input plane to the hough plane , thresholding and first order candidates selection .",
    "candidate parameters are defined by source frequency , celestial coordinates , first spin - down parameter .",
    "the need for coincidences among candidates obtained in different subsets of data ( two in the scheme of fig .",
    "[ fig : schema ] ) has been discussed in references @xcite .",
    "this method is very efficient to reduce the number of spurious candidates at a fixed threshold .",
    "thus , for a given false alarm probability , we can lower the threshold -with respect to the choice of not doing coincidences- gaining in detection efficiency .",
    "the method has a better efficiency when the data sets have similar sensitivities .",
    "after the coincidence , the survived candidates are analyzed coherently with longer ffts on corrected data .",
    "then the spectral filtering is used to take into account the spread of the power in five bands , as explained in reference @xcite .",
    "finally , second order candidates are produced .",
    "as stated before , the sky hough method shows amplitude losses , and thus loss of sensitivity , which are due to digitization of parameters .",
    "this effect shows up mainly for the complexity of the transform together with the need of reducing the computing cost :    * the method is based on a transform between the time - frequency peakmap and the celestial sphere .",
    "it is not simple for the non linearity of the mapping ; * to reduce the computational effort , we need to use `` look - up tables '' which introduce further digitization errors ; * to reduce the computational effort , fast algorithms have been developed , which require the use of a rectangular grid to map the sky . compared to the `` optimal '' ( see later ) grid , the rectangular one has over - resolution in some regions of the sky .",
    "this leads also to a higher number of candidates .",
    "* the use of the celestial map as the space to spot the candidates is very prone to artifacts , see @xcite : some regions are always `` privileged '' , that is they have a higher candidates number with respect to the expectation . the problem arises because each hough map is constructed over the whole sky .    hence , it seemed important the study of alternative procedures .",
    "given the observation that most of the problems are related to the complexity of the transformation , we exploit the possibility of the use of a different but simpler transformation . a part the simplicity of the transformation we obviously need to study a procedure which is less , or equivalently , computationally expensive .",
    "therefore we studied a procedure which has a better , or equivalent , sensitivity , at the same computational cost of the sky hough .",
    "the transformation we propose transforms the * time - observed frequency * plane into the * source frequency - spin down * plane . let",
    "s go into details . if @xmath3 is the frequency ( doppler corrected for a given sky direction ) , @xmath4 the source intrinsic frequency , @xmath5 the first spin - down parameter , @xmath6 the time at the detector and @xmath7 a reference time , we have that @xmath8 a straight line in the hough plane .",
    "we then get the following : @xmath9 each point in the input plane @xmath10 , that is a peak in the doppler shifted peakmap , is transformed into a straight line in the hough @xmath11 plane , with slope @xmath12 .",
    "the slope depends on the choice of the reference time .",
    "if we choose @xmath7 equal to the beginning time of the data we analyze , then the slope is always negative and inversely proportional to the time gap .",
    "+ this is the choice we have done here . in addition , considering the width @xmath13 of the frequency bins in the input plane we notice that each peak is transformed into a stripe among two parallel straight lines    @xmath14    it is a linear transformation .",
    "now the input plane is obtained from the original peakmap by correcting it for the doppler shift due to the earth revolution and rotation , for each point in the sky grid we need to analyze .",
    "thus `` time '' is the time at the detector and `` frequency '' the observed frequency , after the doppler correction .",
    "but , as each sfdb is short enough to not be affected by a time - varying doppler shift , then the doppler effect removal from the original peakmap , obtained from the collection of all the sfdb data , reduces to a very simple `` shifting '' procedure of the peakmap bins . in the analysis scheme , this bins shift is part of the hough procedure . + in the following , we give details on the construction of the map .",
    "the frequency hough map is constructed using the `` direct differential method '' , as is done with the sky hough . with this method , instead of building directly the hough map , one builds a map that , if `` integrated '' ( i.e. summed over bins from left to right ) , gives the hough map .",
    "this is important to minimize the number of floating point operations .",
    "as already explained , for each sky position , the input peakmap is got from the original one by shifting bins to correct for the doppler effect .",
    "the sky is sampled with a non uniform covering grid , which will be later discussed . here",
    "we explain in detail the technique , by giving the sequence of operations :    * for each point in the sky grid , for each coordinate in the input plane @xmath10 and for each spin - down value @xmath15 ,    the map is incremented by 1 in the point @xmath16 and decremented by 1 in the point @xmath17 .    hence ,",
    "for each sky position , a differential map is constructed .",
    "the sum of the bins along the frequency direction is then performed to construct the final integral map .",
    "this two dimensional histogram is the frequency hough map . in the algorithm implementation",
    "we plan to divide the input peakmap into 10 hz bands , thus constructing , for each position in the sky , a different hough map every 10 hz .",
    "+ in case there is the need to exploit higher order one spin down parameters , one ( or more ) loop(s ) has ( have ) to be added to the sequence of operations , to scan the discrete set of values of the new parameter(s ) .",
    "this clearly influences the computing cost , but does not change the basics of the method .",
    "let s first discuss two peculiar aspects of this new method , which are the basis of its appeal .      from the given analysis scheme , it is easy to see that the frequency resolution for the estimation of the source frequency @xmath4 can be enhanced , with respect to the binning frequency @xmath13 , without relevantly affecting the computational effort .",
    "in fact , the use of a resolution @xmath18 with @xmath19 , affects only the size of the hough map .",
    "this has a computational cost only when summing over the bins to construct the integral map from the differential one .",
    "but we notice that the total cost of the construction of the hough map is due to the construction of the differential map , dominated by the number of peaks in the peakmap and to the construction of the integral map , dominated by the number of bins .",
    "the former , in all practical cases , is the one which dominates .",
    "+ the possibility to enhance the frequency resolution results to be , as will be shown in the next sections , a very important peculiarity of the new method .",
    "it which enhances considerably the efficiency , by reducing the digitalization effect .",
    "the same in the sky hough procedure would have a relevant computational cost .",
    "regarding the increasing of the spin down resolution , it would cost for both the procedures : the better the resolution in the spin down estimation the higher is the number of loops of the procedures .      here",
    "we describe how we construct the grid on the sky .",
    "suppose two sources , at the same frequency @xmath4 and same latitude @xmath20 .",
    "their angular delay @xmath21 with respect to the detector rotation produces a time delay @xmath22 .",
    "the two sources will then have the same frequency variation at the detector , which is the classical equation due to the doppler effect , @xmath23 but with time delay @xmath24 .",
    "the observed frequency difference has thus a maximum value which is given by @xmath25 thus the angular resolution is , in radians : @xmath26 where @xmath27 is the number of points in the doppler band for a signal of max frequency @xmath4 : @xmath28 and @xmath29 .",
    "+ we now repeat the same reasoning , supposing the two sources , at the same frequency @xmath4 and same longitude @xmath30 .",
    "the two sources will have the same frequency variation at the detector , now given by @xmath31 , but with an angular delay @xmath32 .",
    "the observed frequency difference has a maximum value which is : @xmath33 we obtain for the angular resolution , in radians : @xmath34 using eqs .",
    "[ gammalong ] and [ gammalat ] we get : @xmath35 @xmath36 using these equations we construct the grid on the sky , which we call the `` optimal '' grid .",
    "the points of the grid are not uniformly distributed . with a simulation",
    ", we have estimated the the number of points in the grid @xmath37 , which is , in the high frequency limit : @xmath38 @xmath39 is an extra resolution factor , which can be greater than 1 , to enhance the efficiency , but even less than 1 , to save computing cost , obviously worsening the efficiency .",
    "fig.[fig : gridsim1 ] shows the optimal sky grid , for @xmath40 ( which corresponds to a source frequency @xmath41 hz ) .",
    "as already said , the grid used in the sky hough method , is not optimal , but rectangular , to use fastest computing algorithms .",
    "the number of points in this rectangular grid is : @xmath42 which is , asymptotically , a factor @xmath43 higher then the number of points of the optimal grid . in fact",
    "this grid has to be over resolved to maintain the same sensitivity of the corresponding optimal grid .",
    "further , we note that this over resolution produces a higher number of candidates from certain sky positions .    ,",
    "x - axis : ecliptical longitude , degrees , from 0 to 400 ; y - axis : ecliptical latitude , degrees , from -100 to 100 ; the number of points in the map is @xmath37=2902.,width=453 ]          the sensitivity of the sky hough procedure is affected by artifacts , i.e. an excess of candidates in some places of the sky map , which are due to local spectral disturbances . the effect ca nt be eliminated because each map is constructed over the whole sky , and hence the threshold for candidate selection has to be the same for the whole sky . using the frequency hough procedure",
    "this effect disappears because each map is constructed for only one position in the sky .",
    "so , because of the adaptivity of the threshold , if a sky region gives an excess of candidates , the threshold is raised and then there is a loss in sensitivity only for that sky region .",
    "we are now ready to enter into details by studying the efficiency of both the methods , by the use of simulations .",
    "figure [ fig : gridsim2 ] is an example of how a frequency hough map looks like , having injected into white noise three signals , at different frequencies and spin - down .      to study the efficiency of the methods , as a function of the frequency over resolution factor",
    ", we have simulated a signal in the absence of noise .",
    "the reason for this is that we were interested in studying only the losses due to the discretization errors .",
    "the parameters chosen for the simulation are similar to actual situations ( detector parameters , source expected parameters ) .",
    "the parameters of the simulation are shown in table [ tab : par ] .",
    "[ fig : freqloss ] shows the amplitude loss versus the frequency over resolution factor @xmath45 .",
    "the loss was calculated as the average value of all the peaks found in the 500 spectra ( it is important to remember that our procedure considers peaks only the maxima above threshold ) .",
    "the result is clear : using @xmath46 the amplitude loss is 3.6 @xmath47 ( the efficiency @xmath48 ) , while with @xmath49 , which is the only practically possible choice of the sky hough , the amplitude loss is 11.6 @xmath47 ( the efficiency @xmath50 ) . from the figure ,",
    "we notice that there is no further gain of increasing the over resolution factor over 10 .",
    "thus , we fixed to 10 the over resolution factor for the frequency hough .",
    "in next simulations , results with @xmath46 are thus for the frequency hough , results with @xmath49 are for the sky hough .",
    "once we have fixed the frequency over resolution factor we wanted to quantify how the increasing of the spin down resolution from the nominal one would affect the sensitivity .",
    "the results are in fig .",
    "[ fig : freqloss1 ] , which shows the loss in amplitude vs the spin down over resolution factor , for both the cases @xmath49 , sky hough , and @xmath46,frequency hough .",
    "it can be noticed that , in the case of the frequency hough , even for the worst analyzed situation , which corresponds to the nominal spin down step @xmath51 the loss is quite small .",
    "is is 3.6 @xmath47 ( the efficiency @xmath48 ) .",
    "the situation is worst for the sky hough , where the loss in amplitude at the nominal spin down step is 11.6 @xmath47 ( the efficiency @xmath50 ) .",
    "the improvement obtained by a better spin down resolution is not so important , as can be seen from the figure .",
    "it seems reasonable , given the observation that increasing the spin down resolution has a computational cost for both the methods , to use the nominal @xmath52 resolution ( x - axis equal to 1 in the figure ) .      to study the loss due to the sky grid resolution",
    ", we have simulated 50 signals , randomly distributed over the sky .",
    "we have then looked for results using the optimal grid , again registering the average value of all the detected peaks . in what follows ,",
    "we suppose to use the optimal grid for both the procedures , sky and frequency hough .",
    "fig.[fig : loss_spinres ] shows the amplitude losses , as a function of the over resolution sky map factor @xmath39 , in the two cases of @xmath46 ( left ) , frequency hough , and @xmath49 ( right ) , sky hough .",
    "the amplitude loss , for @xmath44 , is @xmath53 for the frequency hough , and @xmath54 , for the sky hough .",
    "again , a better efficiency for the new procedure .",
    "we notice that the use of an over resolution for the sky map , would have an impact on the computing cost , with both the procedures .    .",
    "the figures compare the loss when @xmath46 ( left ) , frequency hough , and when @xmath55 ( right ) , sky hough.,title=\"fig:\",width=302 ] .",
    "the figures compare the loss when @xmath46 ( left ) , frequency hough , and when @xmath55 ( right ) , sky hough.,title=\"fig:\",width=302 ]        + we see that the ratio of the amplitude efficiencies is @xmath57 which in power is 1.317 . from this",
    ", we can compute the gain in computing cost for the same sensitivity .",
    "let us firstly recall that the @xmath58 sensitivity in the hierarchical search is proportional to @xmath59 , and the computing cost to @xmath60 .",
    "thus , the `` equivalent fft '' length factor is @xmath61=1.734 and the gain in computing cost is @xmath62=5.2 ( that is , the ratio of computing costs needed to have the same @xmath58 sensitivity ) .      * the * adaptivity * , that is the weight of peaks to consider the noise level and the gain due to the antenna pattern toward a direction , is , with this approach , immediate and very simple , as each hough map is done for a single sky position .",
    "it has been shown , with the sky hough , that the adaptivity of the procedure is a very important task for the analysis ; * this new procedure is appropriate also for all those situations in which the * source position * is known and we should estimate only source frequency and spin down ; * with a proper choice of parameters , it is also possible to detect and hence remove * spurious signals * , with a constant or linearly varying frequency .    on the latter point , we are now working to study the efficiency of this method in terms of rejection of spurious lines in the peakmap . we know that this is a very critical task for the analysis , since the presence of spurious lines highly affects the sensitivity of the search .",
    "we expect this new method to be much more insensitive to the presence of spurious lines , since in the chosen hough plane spurious lines and g.w .",
    "signals should have a very different and well separable behavior .",
    "b. krisnan , a. sintes , m. a. papa , b .",
    "f. schutz , s. frasca , c. palomba , _",
    "phys.rev.d70:082001_ , 2004 .",
    "`` the hough transform search for continuous gravitational waves '' a. sintes , b. krisnan , _",
    "phys.conf.ser.32:206-211_ , 2006 .",
    "`` improved hough search for gravitational wave pulsars ''",
    "p. astone , s. frasca , c. palomba,_cqg 22:s1197-s1210_,2005 `` the short fft database and the peakmap for the hierarchical search of periodic sources '' s. frasca , p. astone , c. palomba,_cqg 22:s1013-s1019 _ , 2005 `` evaluation of sensitivity and computing power for the virgo hierarchical search for periodic sources '' c. palomba , p. astone , s. frasca , _",
    "cqg 22:s1255-s1264_,2005 `` adaptive hough transform for the search of periodic sources '' f. acernese et al ( virgo coll . ) _ cqg 24:s491-s499 _ , 2007 `` coincidence analysis between periodic source candidates in c6 and c7 virgo data '' f. acernese et al ( virgo coll . ) _ proceedings of the eleventh marcel grossmann meeting on general relativity ( berlin , 2006 ) edited by h. kleinert , r.t .",
    "jantzen and r. ruffini , world scientific , singapore _ , 2008 `` first coincidence search among periodic gravitational wave source candidates using virgo data '' p. astone , s. frasca , c. palomba_proceedings of the eleventh marcel grossmann meeting on general relativity ( berlin 2006 ) edited by h. kleinert , r.t . jantzen and r. ruffini , world scientific , singapore _ , 2008 `` incoherent strategies for the network detection of periodic gravitational waves '' c. palomba , s. frasca , _",
    "cqg 21:s1645-s1654 _ , 2004 `` spectral filtering for hierarchical search of periodic sources ''"
  ],
  "abstract_text": [
    "<S> in the hierarchical search for periodic sources of gravitational waves , the candidate selection , in the incoherent step , can be performed with hough transform procedures . in this paper </S>",
    "<S> we analyze the problem of sensitivity loss due to discretization of the parameters space vs computing cost , comparing the properties of the sky hough procedure with those of a new frequency hough , which is based on a transformation from the _ time - observed frequency _ plane to the _ source frequency - spin down _ plane . </S>",
    "<S> results on simulated peakmaps suggest various advantages in favor of the use of the frequency hough . the ones which show up to really make the difference are 1 ) the possibility to enhance the frequency resolution without relevantly affecting the computing cost . </S>",
    "<S> this reduces the digitization effects ; 2 ) the excess of candidates due to local disturbances in some places of the sky map . </S>",
    "<S> they do not affect the new analysis because each map is constructed for only one position in the sky . </S>",
    "<S> + pacs . </S>",
    "<S> numbers : 04.80nn,07.05kf,97.60jd </S>"
  ]
}