{
  "article_text": [
    "the inverse ising problem is intensively studied in statistical physics , computational biology and computer science in the few past years  @xcite .",
    "the biological experiments or numerical simulations usually generate a large amount of experimental data , e.g. , @xmath2 independent samples @xmath5 in which @xmath6 is an @xmath7-dimensional vector with binary components ( @xmath8 ) and @xmath7 is the system size .",
    "the least structured model to match the statistics of the experimental data is the ising model  @xcite : @xmath9\\ ] ] where the partition function @xmath10 depends on the @xmath7-dimensional fields and @xmath11-dimensional couplings .",
    "these fields and couplings are chosen to yield the same first and second moments ( magnetizations and pairwise correlations respectively ) as those obtained from the experimental data .",
    "the inverse temperature @xmath12 has been absorbed into the strength of fields and couplings .",
    "previous studies of the inverse ising problem on hopfield model  @xcite lack a systematic analysis for treating sparse networks .",
    "inference of the sparse network also have important and wide applications in modeling vast amounts of biological data .",
    "actually , the real biological network is not densely connected . to reconstruct the sparse network from the experimental data ,",
    "an additional penalty term is necessary to be added into the cost function , as studied in recovering sparse signals in the context of compressed sensing  @xcite or in ising model selection  @xcite .",
    "this strategy is known as @xmath0-regularization which introduces an @xmath0-norm penalty to the cost function ( e.g. , the log - likelihood of the ising model ) .",
    "the regularization is able to minimize the impact of finite sampling noise , thus avoid the overfitting of data .",
    "the @xmath0-regularization has been studied in the pseudo - likelihood approximation to the network inference problem@xcite and in the setting of sparse continuous perceptron memorization and generalization  @xcite .",
    "this technique has also been thoroughly discussed in real neural data analysis using selective cluster expansion method  @xcite .",
    "the cluster expansion method involves repeated solution of the inverse ising problem and the computation of the cluster entropy included in the expansion ( cluster means a small subset of spins ) . to truncate the expansion ,",
    "clusters with small entropy in absolute value are discarded and the optimal threshold needs to be determined .",
    "additionally , the cluster size should be small to reduce the computational cost while at each step a convex optimization of the cost function ( see eq .",
    "( [ cost ] ) ) for the cluster should be solved .",
    "this may be complicated in some cases .",
    "the pseudo - likelihood maximization  @xcite method relies on the complete knowledge of the sampled configurations , and involves a careful design of the numerical minimization procedure for the pseudo - likelihood ( e.g. , newton descent method , or interior point method ) at a large computational cost ( especially for large sample size ) . in this paper",
    ", we provide an alternative way to reconstruct the sparse network by combining the bethe approximation and the @xmath0-regularization , which is much simpler in practical implementation .",
    "we expect that the @xmath0-regularization will improve the prediction of the bethe approximation . to show the efficiency",
    ", we apply this method to the sparse hopfield network reconstruction .",
    "our contributions in this work are two - fold .",
    "( 1 ) we provide a regularized quadratic approximation to the negative log - likelihood function for the sparse network construction by neglecting higher order correlations , which yields a new inference equation reducing further the inference error .",
    "furthermore , the implementation is much simple by saving the computational time . ( 2 ) another significant contribution is a scaling form for the optimal regularization parameter is found , and this scaling form is useful for choosing the suitable regularization .",
    "most importantly , the method is not limited to the tested model ( sparse hopfield model ) , and is generally applicable to other diluted mean field models and even real data analysis ( e.g. , neural data ) .",
    "the outline of the paper is as follows .",
    "the sparse hopfield network is defined in sec .",
    "[ sec_shopf ] . in sec .",
    "[ sec_method ] , we present the hybrid inference method by using the bethe approximation and @xmath0-regularization",
    ". we test our algorithm on single instances in sec .",
    "[ sec_result ] .",
    "concluding remarks are given in sec .",
    "[ sec_sum ] .",
    "the hopfield network has been proposed in ref .",
    "@xcite as an abstraction of biological memory storage and was found to be able to store an extensive number of random unbiased patterns  @xcite . if the stored patterns are dynamically stable , then the network is able to provide associative memory and its equilibrium behavior is described by the following hamiltonian : @xmath13 where the ising variable @xmath14 indicates the active state of the neuron ( @xmath15 ) or the silent state ( @xmath16 ) . for the sparse network storing @xmath17 random unbiased binary patterns ,",
    "the symmetric coupling is constructed  @xcite as @xmath18 where @xmath19 is the average connectivity of the neuron . @xmath20",
    "independent of the network size @xmath7 .",
    "note that in this case , the number of stored patterns can only be finite . in the thermodynamic limit",
    ", @xmath17 scales as @xmath21 where @xmath22 is the memory load .",
    "no self - interactions are assumed and the connectivity @xmath23 obeys the distribution : @xmath24    mean field properties of the sparse hopfield network have been discussed within replica symmetric approximation in refs .",
    "three phases ( paramagnetic , retrieval and spin glass phases ) have been observed in this sparsely connected hopfield network with arbitrary finite @xmath19 . for large @xmath19 ( e.g. , @xmath25 )",
    ", the phase diagram resembles closely that of extremely diluted ( @xmath26 , such as @xmath27 ) case  @xcite where the transition line between paramagnetic and retrieval phase is @xmath28 for @xmath29 and that between paramagnetic and spin glass phase @xmath30 for @xmath31 .",
    "the spin glass / retrieval transition occurs at @xmath32 .    to sample the state of the original model eq .",
    "( [ hami ] ) , we apply the glauber dynamics rule : @xmath33\\ ] ] where @xmath34 is the local field neuron @xmath35 feels . in practice , we first randomly generate a configuration which is then updated by the local dynamics rule eq .",
    "( [ gdrule ] ) in a randomly asynchronous fashion . in this",
    "setting , we define a glauber dynamics step as @xmath7 proposed flips .",
    "the glauber dynamics is run totally @xmath36 steps , among which the first @xmath37 steps are run for thermal equilibration and the other @xmath38 steps for computing magnetizations and correlations , i.e. , @xmath39 where @xmath40 denotes the average over the collected data .",
    "the state of the network is sampled every @xmath41 steps after thermal equilibration ( doubled sampling frequency yields the similar inference result ) , which produces totally @xmath42 independent samples .",
    "the magnetizations and correlations serve as inputs to our following hybrid inference algorithm .",
    "the bethe approximation assumes that the joint probability ( boltzmann distribution , see eq .",
    "( [ ising ] ) ) of the neuron activity can be written in terms of single - neuron marginal for each single neuron and two - neuron marginal for each pair of adjacent neurons as @xmath43 where @xmath44 runs over all distinct pairs of neurons .",
    "this approximation is exact on tree graphs and asymptotically correct for sparse networks or networks with sufficiently weak interactions  @xcite . under this approximation",
    ", the free energy ( @xmath45 ) can be expressed as a function of connected correlations @xmath46 ( between neighboring neurons ) and magnetizations @xmath47 .",
    "the stationary point of the free energy with respect to the magnetizations yields the following self - consistent equations : @xmath48 where @xmath49 denotes neighbors of @xmath35 , @xmath50 and @xmath51 . using the linear response relation to calculate the connected correlations for any pairs of neurons ,",
    "we obtain the bethe approximation ( ba ) to the inverse ising problem  @xcite : @xmath52   %   \\end{split }    j_{ij}=-\\tanh^{-1}\\biggl[\\frac{1}{2(\\mathbf{c}^{-1})_{ij}}(a_{ij}-b_{ij } )     -m_{i}m_{j}\\biggr],\\ ] ] where @xmath53 is the inverse of the connected correlation matrix , @xmath54 , @xmath55 and @xmath56 .",
    "the couplings have been scaled by the inverse temperature @xmath57 .",
    "note that fields can be predicted using eq .",
    "( [ m ] ) after we get the set of couplings .",
    "hereafter we consider only the reconstruction of the coupling vector .",
    "in fact , the ba solution of the couplings corresponds to the fixed point of the susceptibility propagation  @xcite , yet it avoids the iteration steps in susceptibility propagation and the possible non - convergence of the iterations .",
    "it was also found that the ba yields a good estimate to the underlying couplings of the hopfield network  @xcite . in the following analysis",
    ", we try to improve the prediction of ba with @xmath0-regularization .",
    "the cost function to be minimized in the inverse ising problem can be written as the following rescaled negative log - likelihood function  @xcite : @xmath58\\\\      & = \\ln      z(\\mathbf{h},\\mathbf{j})-\\mathbf{h}^{t}\\mathbf{m}-\\frac{1}{2}{\\rm tr}(\\mathbf{j}\\mathbf{\\tilde{c } } )      \\end{split}\\ ] ] where @xmath59 and @xmath60 .",
    "@xmath61 denotes the transpose of the field vector while @xmath62 denotes the trace of matrix @xmath63 .",
    "the minimization of @xmath64 in the @xmath65-dimensional space of fields and couplings yields the following equations :    [ mj ] @xmath66    where the average is taken with respect to the boltzmann distribution eq .",
    "( [ ising ] ) with the optimal fields and couplings ( corresponding to the minimum of @xmath67 ) .",
    "actually , one can use bethe approximation to compute the connected correlation in the right - hand side of eq .",
    "( [ mj2 ] ) , which leads to the result of eq .",
    "( [ ba ] ) .",
    "to proceed , we expand the cost function around its minimum with respect to the fluctuation of the coupling vector up to the second order as @xmath68 where @xmath69 defines the fluctuation @xmath70 where @xmath71 is the ( near ) optimal coupling vector .",
    "@xmath72 is the gradient of @xmath67 evaluated at @xmath71 , and @xmath73 is the hessian matrix .",
    "the quadratic approximation to the log - likelihood has also been used to develop fast algorithms for estimation of generalized linear models with convex penalties  @xcite .",
    "we have only made explicit the dependence of @xmath67 on the coupling vector .",
    "the first order coefficient vanishes due to eq .",
    "( [ mj ] ) .",
    "note that the hessian matrix is an @xmath74 symmetric matrix whose dimension is much higher than that of the connected correlation matrix . however , to construct the couplings around neuron @xmath35 , we consider only the neuron @xmath35-dependent part , i.e. , we set @xmath75 in the hessian matrix @xmath76 where @xmath77 and @xmath78 run over distinct pairs of neurons .",
    "this simplification reduces the computation cost but still keeps the significant contribution as proved later in our simulations .",
    "finally we obtain @xmath79 where an @xmath0-norm penalty has been added to promote the selection of sparse network structure  @xcite .",
    "@xmath80 is a positive regularization parameter to be optimized to make the inference error ( see eq .",
    "( [ error ] ) ) as low as possible .",
    "the @xmath0-norm penalizes small but non - zero couplings and increasing the value of the regularization parameter @xmath80 makes the inferred network sparser . in the following analysis , we assume @xmath71 is provided by the ba solution ( a good approximation to reconstruct the sparse hopfield network  @xcite , yielding a low inference error ) , then we search for the new solution to minimize the regularized cost function eq .",
    "( [ apprs ] ) , finally we get the new solution as follows , @xmath81^{-1}_{kj}\\ ] ] where @xmath82 for @xmath83 and @xmath84 .",
    "( [ reg ] ) results from @xmath85 which gives @xmath86 , where @xmath87 and @xmath88 .",
    "@xmath89 represents couplings around neuron @xmath35 . to ensure the symmetry of the couplings , we construct @xmath90 where @xmath91 is also given by eq .",
    "( [ reg ] ) in which @xmath35 and @xmath92 are exchanged .",
    "the inverse of @xmath93 or @xmath94 takes the computation time of the order @xmath95 , much smaller than that of the inverse of a susceptibility matrix @xmath96 .",
    "we remark here that minimizing the regularized cost function eq .",
    "( [ apprs ] ) corresponds to finding the optimal deviation @xmath69 which provides a solution to the regularized cost function .",
    "we also assume that for small @xmath80 , the deviation is small as well . without the quadratic approximation in eq .",
    "( [ expand ] ) , no closed form solution exists for the optimal @xmath97 , however , the solution can still be found by using convex optimization techniques",
    ". similar equation to eq .",
    "( [ reg ] ) has been derived in the context of reconstructing a sparse asymmetric , asynchronous ising network  @xcite .",
    "here we derive the inference equation ( eq .",
    "( [ reg ] ) ) for the static reconstruction of a sparse network .",
    "we will show in the next section the efficiency of this hybrid strategy to improve the prediction of the ba without regularization . to evaluate the efficiency",
    ", we define the reconstruction error ( root mean squared ( rms ) error ) as @xmath98^{1/2}\\ ] ] where @xmath99 is the inferred coupling while @xmath100 is the true one constructed according to eq .",
    "( [ j_spar ] ) .",
    "other performance measures for sparse network inference will also be discussed in the following section .",
    "-regularized ba on sparse hopfield networks . the inference error by ba with prior knowledge of the sparseness of the network",
    "is also shown .",
    "network size @xmath101 , the memory load @xmath102 and the mean node degree @xmath103 .",
    "each data point is the average over five random sparse networks .",
    "the regularization parameter has been optimized .",
    "the inset gives the relative inference error defined as @xmath104 versus the inverse temperature .",
    "( b ) the receiver operating characteristic curve for three typical examples ( @xmath105 ) .",
    "each data point corresponds to a value of @xmath80 for @xmath0-regularized ba . the solid symbol gives the result of ba without regularization .",
    "parameters for these three examples are @xmath106 respectively . , title=\"fig:\",width=321].1 cm -regularized ba on sparse hopfield networks .",
    "the inference error by ba with prior knowledge of the sparseness of the network is also shown .",
    "network size @xmath101 , the memory load @xmath102 and the mean node degree @xmath103 .",
    "each data point is the average over five random sparse networks .",
    "the regularization parameter has been optimized .",
    "the inset gives the relative inference error defined as @xmath104 versus the inverse temperature .",
    "( b ) the receiver operating characteristic curve for three typical examples ( @xmath105 ) .",
    "each data point corresponds to a value of @xmath80 for @xmath0-regularized ba . the solid symbol gives the result of ba without regularization .",
    "parameters for these three examples are @xmath106 respectively . , title=\"fig:\",width=321].1 cm     versus the regularization parameter @xmath80 at @xmath105 .",
    "inference results on three random instances are shown .",
    "the inference errors by applying ba without regularization on these three random instances are @xmath107 respectively .",
    "( b ) correct classification rate ( ccr ) versus the regularization parameter @xmath80 at @xmath105 .",
    "the instances are the same as those in ( a ) .",
    "the ccr of ba without regularization are @xmath108 respectively .",
    "( c ) the optimal @xmath109 versus the number of samples @xmath2 ( @xmath105 ) .",
    "each point is the mean value over five random realizations of the sparse hopfield network .",
    "the standard error is nearly zero and not shown .",
    "the linear fit shows that @xmath110 with @xmath111 for rms error and @xmath112 for ccr measure .",
    ", title=\"fig:\",width=321 ] .1 cm   versus the regularization parameter @xmath80 at @xmath105 .",
    "inference results on three random instances are shown .",
    "the inference errors by applying ba without regularization on these three random instances are @xmath107 respectively .",
    "( b ) correct classification rate ( ccr ) versus the regularization parameter @xmath80 at @xmath105 .",
    "the instances are the same as those in ( a ) .",
    "the ccr of ba without regularization are @xmath108 respectively .",
    "( c ) the optimal @xmath109 versus the number of samples @xmath2 ( @xmath105 ) .",
    "each point is the mean value over five random realizations of the sparse hopfield network .",
    "the standard error is nearly zero and not shown .",
    "the linear fit shows that @xmath110 with @xmath111 for rms error and @xmath112 for ccr measure .",
    ", title=\"fig:\",width=321].1 cm   versus the regularization parameter @xmath80 at @xmath105 .",
    "inference results on three random instances are shown .",
    "the inference errors by applying ba without regularization on these three random instances are @xmath107 respectively .",
    "( b ) correct classification rate ( ccr ) versus the regularization parameter @xmath80 at @xmath105 .",
    "the instances are the same as those in ( a ) .",
    "the ccr of ba without regularization are @xmath108 respectively .",
    "( c ) the optimal @xmath109 versus the number of samples @xmath2 ( @xmath105 ) .",
    "each point is the mean value over five random realizations of the sparse hopfield network .",
    "the standard error is nearly zero and not shown .",
    "the linear fit shows that @xmath110 with @xmath111 for rms error and @xmath112 for ccr measure .",
    ", title=\"fig:\",width=321].1 cm    we simulate the sparsely connected hopfield network of size @xmath101 at different temperatures . the average connectivity for each neuron @xmath103 and the memory load @xmath102 . as shown in fig .",
    "[ regba ] ( a ) , the @xmath0-regularization in eq .",
    "( [ reg ] ) does improve the prediction on the sparse network reconstruction .",
    "the improvement is evident in the presence of high quality data ( e.g. , in the high temperature region , see the inset of fig .",
    "[ regba ] ( a ) ) .",
    "however , the relative inference error ( improvement fraction ) shown in the inset of fig .",
    "[ regba ] ( a ) gets smaller as the temperature decreases .",
    "this may be due to insufficient samplings  @xcite of glassy states at the low temperatures .",
    "the glassy phase is typically characterized by a complex energy landscape exhibiting numerous local minima . as a result , the phase space we sample develops higher order ( higher than second order ) correlations whose contributions to the regularized cost function can not be simply neglected , which explains the behavior observed in the inset of fig .",
    "[ regba ] ( a ) . in this case ,",
    "the pseudo - likelihood method or more complex selective cluster expansion can be used at the expense of larger computation times . for comparison",
    ", we also show the inference error of ba with prior knowledge of the network connectivity , i.e. , the sparseness is known in advance with only the true non - zero couplings to be predicted .",
    "the comparison confirms that the @xmath93 matrix obtained from correlations in the data contains useful information about the sparsity of the network , and this information can be extracted by using @xmath0-regularization in eq .",
    "( [ reg ] ) .",
    "an accurate pruning of the network can be achieved by simple thresholding ( setting to zero some couplings whose absolute values are below certain threshold ) based on the improved prediction . the receiver operating characteristic ( roc ) curves are given in fig .",
    "[ regba ] ( b ) for three typical examples of different network size , memory load and connectivity .",
    "the roc curve is obtained by plotting true positive rate ( the number of inferred non - zero couplings with correct sign divided by the total number of true non - zero couplings ) against true negative rate ( the number of inferred zero couplings divided by the total number of true zero couplings ) .",
    "a threshold @xmath113 is used to get the inferred zero couplings .",
    "the roc curve in fig .",
    "[ regba ] ( b ) shows that one can push the inference accuracy towards the upper right corner ( high true positive rate as well as high true negative rate ) by tuning the regularization parameter .",
    "note that ba without regularization reports low true negative rate .",
    "we also explore the effects of the regularization parameter on the reconstruction , which are reported in fig .",
    "[ regpara ] ( a ) . with increasing @xmath80 ,",
    "the inference error first decreases , then reaches a minimal value followed by an increasing trend in the range we plot in fig .",
    "[ regpara ] ( a ) .",
    "this implies that the optimal regularization parameter guides our inference procedure to a sparse network closest to the original one .",
    "the inference quality can also be measured by the fraction of edges @xmath44 where the coupling strength is classified correctly as positive , zeroor negative .",
    "we call this quantity correct classification rate ( ccr ) .",
    "results for three typical examples are reported in fig .",
    "[ regpara ] ( b ) .",
    "with increasing @xmath80 , ccr first increases and then decreases .",
    "the optimal regularization parameter corresponding to the maximum is slightly different from that in fig .",
    "[ regpara ] ( a ) . by using regularized ba ( eq .  ( [ reg ] ) )",
    ", one can achieve a much higher value of ccr , and furthermore the computational cost is not heavy .",
    "interestingly , the optimal value of @xmath80 yielding the lowest inference error ( rms error ) has the order of @xmath114 for fixed network size ( usually @xmath115 ) , which is consistent with that found in refs .",
    "we verify this scaling form by varying @xmath2 and plotting the optimal @xmath80 in fig .",
    "[ regpara ] ( c ) .",
    "the linear fit implies that the scaling exponent @xmath116 .",
    "however , this scaling exponent depends on the performance measure . taking the ccr measure yields a smaller value @xmath4 , as shown in fig .",
    "[ regpara ] ( c ) as well .",
    "we also find that the magnitude of the optimal regularization parameter shows less sensitivity to specific instances and other parameters ( e.g. , the temperature , memory load or network size ) , since the number of samplings @xmath2 dominates the order of the magnitude . the specific optimal value becomes slightly different across different instances of the sparse network in the low temperature region , where its mean value shifts to a bit larger value for rms error measure or a bit smaller value for ccr measure , as the temperature further decreases . the number of samplings @xmath2 determines the order of the magnitude , which helps us find the appropriate strength for the regularization parameter . in the real application ,",
    "the true coupling vector is _ a priori _ unknown . in this case",
    ", the regularization parameter can be chosen to make the difference between the measured moments and those produced by the reconstructed ising model as small as possible .    , the memory load @xmath102 and the mean node degree @xmath103 .",
    "( b ) misclassification rate versus memory load .",
    "network size @xmath101 , temperature @xmath105 and @xmath117 . , title=\"fig:\",width=321].1 cm , the memory load @xmath102 and the mean node degree @xmath103 .",
    "( b ) misclassification rate versus memory load .",
    "network size @xmath101 , temperature @xmath105 and @xmath117 . , title=\"fig:\",width=321].1 cm    finally , we give the comparison of performance measured by misclassification rate in fig .",
    "[ miscr ] . according to the above definition ,",
    "misclassification rate equals to @xmath118 .",
    "low misclassification rate is preferred in the sparse network inference .",
    "[ miscr ] ( a ) shows the performance versus inverse temperature .",
    "the misclassification rate is lowered by a substantial amount using the hybrid strategy . especially in the high temperature region ,",
    "the error approaches zero while ba still yields an error of the order of @xmath119 . as displayed in fig .",
    "[ miscr ] ( b ) , the hybrid strategy is also superior to ba when the memory load is varied , although the misclassification rate grows with the memory load . compared with ba , the @xmath0-regularized ba yields a much slower growth of the error when @xmath22 increases . even at the high memory load @xmath120 ,",
    "the hybrid strategy is able to reconstruct the network with an error @xmath121 while at the same memory load , the error of ba is as large as @xmath122 .",
    "note that as @xmath22 changes , the average connectivity also changes .",
    "[ miscr ] ( b ) illustrates that our simple inference strategy is also robust to different mean node degrees .",
    "we propose an efficient hybrid inference strategy for reconstructing the sparse hopfield network .",
    "this strategy combines bethe approximation and the @xmath0-regularization by expanding the objective function ( negative log - likelihood function ) up to the second order of the coupling fluctuation around its ( near ) optimal value .",
    "the hybrid strategy is simple in implementation without heavy computational cost , yet improves the prediction by zeroing couplings which are actually not present in the network ( see fig .",
    "[ regba ] and fig .",
    "[ miscr ] ) .",
    "we can control the accuracy by tuning the regularization parameters .",
    "the magnitude of the optimal regularization parameters is determined by the number of independent samples @xmath2 as @xmath123 .",
    "the value of the scaling exponent depends on the performance measure .",
    "@xmath116 for root mean squared error measure while @xmath4 for misclassification rate measure . by varying the value of the regularization parameter",
    ", we show that the reconstruction ( rms ) error first decreases and then increases after the lowest error is reached .",
    "similar phenomenon is observed for the change of misclassification rate with the regularization parameter .",
    "we observe this phenomenon in the sparse hopfield network reconstruction , and this behavior may be different in other cases  @xcite .",
    "the efficiency of this strategy is demonstrated for the sparse hopfield model , but this approximated reconstruction method is generally applicable to other diluted mean field models if we can first find a good solution ( yielding low inference error ) to the inverse ising problem without regularization .",
    "helpful discussions with yoshiyuki kabashima are gratefully acknowledged .",
    "this work was supported by the jsps fellowship for foreign researchers ( grant no .",
    "@xmath124 ) ."
  ],
  "abstract_text": [
    "<S> we propose an efficient strategy to infer sparse hopfield network based on magnetizations and pairwise correlations measured through glauber samplings . </S>",
    "<S> this strategy incorporates the @xmath0 regularization into the bethe approximation by a quadratic approximation to the log - likelihood , and is able to further reduce the inference error of the bethe approximation without the regularization . </S>",
    "<S> the optimal regularization parameter is observed to be of the order of @xmath1 where @xmath2 is the number of independent samples . </S>",
    "<S> the value of the scaling exponent depends on the performance measure . </S>",
    "<S> @xmath3 for root mean squared error measure while @xmath4 for misclassification rate measure . </S>",
    "<S> the efficiency of this strategy is demonstrated for the sparse hopfield model , but the method is generally applicable to other diluted mean field models . in particular , it is simple in implementation without heavy computational cost . </S>"
  ]
}