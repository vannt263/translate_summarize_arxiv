{
  "article_text": [
    "in many scientific applications , we need to solve lower triangular problems and upper triangular problems , such as incomplete lu ( ilu ) preconditioners , domain decomposition preconditioners and gauss - seidel smoothers for algebraic multigrid solvers @xcite . the algorithms for these problems are serial in nature and difficult to parallelize on parallel platforms .",
    "gpu is one of these parallel devices , which is powerful in float point calculation and is over 10 times faster than latest cpu .",
    "recently , gpu has been popular in various numerical scientific applications .",
    "it is efficient for vector operations .",
    "researchers have developed linear solvers for gpu devices @xcite .",
    "however , the development of efficient parallel triangular solvers for gpu is still challenging @xcite .",
    "klie et al . (",
    "2011 ) investigated a triangular solver @xcite .",
    "they developed a level schedule method and a speedup of two was obtained .",
    "naumov ( 2011 ) from the nvidia company also developed parallel triangular solvers @xcite .",
    "he developed new parallel triangular solvers based on a graph analysis .",
    "the average speedup was also around two .    in this paper , we introduce our work on speeding triangular solvers",
    ". a new matrix format , hec ( hybrid ell and csr ) , is developed .",
    "a hec matrix includes two matrices , an ell matrix and a csr matrix .",
    "the ell part is in column - major order and is designed the way to increase the effective bandwidth of nvidia gpu . for the csr matrix ,",
    "each row contains at least one non - zero element .",
    "this design of the csr part reduces the complexity of the solution of triangular systems .",
    "in addition , parallel algorithms for solving the triangular systems are developed .",
    "the algorithms are motivated by the level schedule method described in @xcite .",
    "our parallel triangular solvers can be sped up to seven times faster .",
    "based on these modified algorithms , ilu(k ) , ilut and domain decomposition ( restricted additive schwarz ) preconditioners are developed .",
    "numerical experiments are performed .",
    "these experiments show that we can speed linear solvers around ten times faster .",
    "the layout is as follows . in  2 ,",
    "a new matrix format and algorithms for lower triangular problems and upper triangular problems are introduced . in ",
    "3 , parallel triangular solvers are employed to develop ilu preconditioners and domain decomposition preconditioners . in  4 , numerical tests are performed . at the end",
    ", conclusions are presented .",
    "for the most commonly used preconditioner ilu , the following triangular systems need to be solved : @xmath0 where @xmath1 and @xmath2 are a lower triangular matrix and an upper triangular matrix , respectively , @xmath3 is the right - hand side vector , @xmath4 is the unknown to be solved for , and @xmath5 is the intermediate unknown . the lower triangular problem , @xmath6 , is solved first , and then , by solving the upper triangular problem , @xmath7 , we can obtain the result @xmath4 . in this paper",
    ", we always assume that each row of @xmath1 and @xmath2 is sorted in ascending order according to their column indices .",
    "the matrix format we develop is denoted by hec ( hybrid ell and csr ) @xcite .",
    "its basic structure is demonstrated by figure  [ fig1 ] .",
    "an hec matrix contains two submatrices : an ell matrix , which was introduced in ellpack  @xcite , and a csr ( compressed sparse row ) matrix .",
    "the ell matrix has two submatrices , a column - indices matrix and a non - zeros matrix .",
    "the length of each row in these two matrices is the same .",
    "the ell matrix is in column - major order and is aligned when being stored on gpu .",
    "note that the data access pattern of global memory for nvidia tesla gpu is coalesced @xcite so the data access speed for the ell matrix is high .",
    "a disadvantage of the ell format is that it may waste memory if one row has too many non - zeros . in this paper , a csr submatrix",
    "is added to overcome this problem .",
    "a csr matrix contains three arrays , the first one for the offset of each row , the second one for column indices and the last one for non - zeros . for our hec format matrix , we store the regular part of a given triangular matrix @xmath1 in the ell part and the irregular part in the csr part . when we store the lower triangular matrix , each row of the csr matrix has at least one element , which is the diagonal element in the triangular matrix @xmath1 .          in this section ,",
    "we introduce our parallel lower triangular solver to solve @xmath8 the solver we develop is based on the level schedule method @xcite .",
    "the idea is to group unknowns @xmath9 into different levels so that all unknowns within the same level can be computed simultaneously @xcite . for the lower triangular problem , the level of @xmath9 is defined as @xmath10 where @xmath11 is the @xmath12th entry of @xmath1 , @xmath13 is zero initially and @xmath14 is the number of rows .",
    "we define @xmath15 , which is the union of all unknowns that have the same level . here",
    "we assume that each set @xmath16 is sorted in ascending order according to the indices of the unknowns belonging to @xmath16 .",
    "define @xmath17 the number of unknowns in set @xmath16 and @xmath18 the number of levels .",
    "now , a map @xmath19 can be defined as follows : @xmath20 where @xmath21 is the position @xmath9 in the set @xmath22 when @xmath9 belongs to @xmath22 . with the help of the map , @xmath19",
    ", we can reorder the triangular matrix @xmath1 to @xmath23 , where @xmath11 in @xmath1 is transformed to @xmath24 in @xmath23 .",
    "@xmath23 is still a lower triangular matrix . from this map",
    ", we find that if @xmath9 is next to @xmath25 in some set @xmath22 , the @xmath26th and @xmath27th rows of @xmath1 are next to each other in @xmath23 after reordering .",
    "it means that @xmath1 is reordered level by level , which implies that memory access in matrix @xmath23 is less irregular than that in matrix @xmath1 .",
    "therefore , @xmath23 has higher performance compared to @xmath1 when we solve a lower triangular problem .",
    "the whole algorithm is described in two steps , the preprocessing step and the solution step , respectively .",
    "the preprocessing step is described in algorithm [ alg1 ] . in this step",
    ", the level of each unknown is calculated first . according to these levels ,",
    "a map between @xmath1 and @xmath23 can be set up according to equation ( [ equ - map ] ) .",
    "then the matrix @xmath1 is reordered .",
    "we should mention that @xmath1 can be stored in any kind of matrix format .",
    "a general format is csr . at the end , @xmath23 is converted to the hec format and as we discussed above each row of the csr part has at least one element .    .",
    "@xmath28 ;    start = level(i ) ; end = level(i + 1 ) - 1 ; solve the @xmath27th row ;    @xmath29 ;    the second step is to solve the lower triangular problem .",
    "this step is described in algorithm [ alg2 ] , where @xmath30 is the start row position of level @xmath26 .",
    "first , the right - hand side @xmath3 is permutated according to the map @xmath19 we computed .",
    "then the triangular problem is solved level by level and the solution in the same level is simultaneous .",
    "each thread is responsible for one row . at the end",
    ", the final solution is obtained by a permutation .    to solve the upper triangular problem @xmath31",
    ", we introduce the following transferring map :    @xmath32 where @xmath14 is the number of rows in @xmath2 . with this map ,",
    "the upper triangular problem is transferred to a lower triangular problem , and the lower triangular solver is called to solve the problem .",
    "the ilu factorization for a sparse matrix @xmath33 computes a sparse lower triangular matrix @xmath1 and a sparse upper triangular matrix @xmath2 .",
    "if no fill - in is allowed , we obtain the so - called ilu(0 ) preconditioner .",
    "if fill - in is allowed , we obtain the ilu(k ) preconditioner , where @xmath34 is the fill - in level .",
    "another method is ilut(p , tol ) , which drops entries based on the numerical values @xmath35 of the fill - in elements and the maximal number @xmath36 of non - zeros in each row @xcite .",
    "the performance of parallel triangular solvers for ilu(k ) and ilut is dominated by original problems . in this paper , we implement block ilu(k ) and block ilut preconditioners . when the number of blocks is large enough , we will have sufficient parallel performance",
    ". if the matrix is not well partitioned and the number of blocks is too large , the effect of ilu(k ) and ilut will be weakened . when we partition a matrix , the graph library metis @xcite is employed .",
    "as we discussed above , the effect of ilu preconditioners is weakened if the number of blocks is too large .",
    "the domain decomposition preconditioner is implemented , which was developed by cai et al . @xcite .",
    "the domain decomposition preconditioner we implement is the so - called restricted additive schwarz ( ras ) method .",
    "overlap is introduced , and , therefore , this preconditioner is not as sensitive as block ilu preconditioners",
    ". it can lead to good parallel performance and good preconditioning . in this paper",
    ", we treat the original matrix as an undirected graph and this graph is partitioned by metis @xcite .",
    "the subdomain can be extended according to the topology of the graph .",
    "then each smaller problem can be solved by ilu(k ) or ilut . in this paper",
    ", we use ilu(0 ) .",
    "assume that the domain is decomposed into @xmath37 subdomains , and we have @xmath37 smaller problems , @xmath38 , @xmath39 , @xmath40 , and @xmath41 .",
    "we do not solve these problems one by one but we treat them as one bigger problem : @xmath42 each @xmath43 is factorized by the ilu method , where we have @xmath44 then equation ( [ ddm ] ) is solved by our triangular solvers .",
    "in this section , numerical experiments are presented , which are performed on our workstation with intel xeon x5570 cpus and nvidia tesla c2050/c2070 gpus .",
    "the operating system is centos 6.2 x86_64 with cuda toolkit 4.1 and gcc 4.4 .",
    "all cpu codes are compiled with -o2 option .",
    "the data type of a float point number is double .",
    "the linear solver is gmres(20 ) .",
    "bilu , bilut and ras denote the block ilu(0 ) , block ilut and restricted additive schwarz preconditioners , respectively .",
    "the matrix used in this example is from a three - dimensional poisson equation .",
    "the dimension is 1,000,000 and the number of non - zeros is 6,940,000 .",
    "the ilu preconditioners we use in this example are block ilu(0 ) and block ilut(7 , 0.1 ) .",
    "the performance data is collected in table [ tab - ex1 ] .",
    ".performance of the matrix from the poisson equation [ cols= \" < , < , < , < , < , < , < , < \" , ]     for the block ilu(0 ) , we can speed it over 3 times faster .",
    "when the number of blocks increases , our algorithm has better speedup .",
    "the whole solving part is sped around 8 times faster .",
    "bilut is a special preconditioner , since it is obtained according to the values of @xmath1 and @xmath2 ; sometimes , the sparse patterns of @xmath1 and @xmath2 are more irregular than those in bilu . from table",
    "[ tab - ex1 ] , the speedups are a little lower compared to bilu . however , bilut reflects the real data dependence , and its performance is better in general .",
    "this is demonstrated by table [ tab - ex1 ] .",
    "the cpu version bilut always takes less time than the cpu bilu .",
    "but for the gpu versions , their performance is similar .",
    "the bilut is sped around 2 times faster . the whole solving part",
    "is sped around 6 times faster .",
    "the ras preconditioner is always stable .",
    "it also has better speedup than bilu and bilut .",
    "the triangular solver is sped around 6 times faster and the average speedup is around 9 .",
    "matrix atmosmodd is taken from the university of florida sparse matrix collection @xcite and is derived from a computational fluid dynamics ( cfd ) problem .",
    "the dimension of atmosmodd is 1,270,432 and it has 8,814,880 non - zeros .",
    "the ilu preconditioners we use in this example are block ilu(0 ) and block ilut(7 , 0.01 ) .",
    "the performance data is collected in table [ tab - ex2 ] .",
    "llllllll pre & blocks & cpu ( s ) & gpu ( s ) & speedup & pre cpu ( s)&pre gpu ( s ) & speedup +    bilu & 16 & 20.61 & 2.63 & 7.79 & 0.0248 & 0.0072 & 3.45 + bilu & 128 & 23.94 & 2.80 & 8.50 & 0.0244 & 0.0072 & 3.40 + bilu & 512 & 24.13 & 2.72 & 8.82 & 0.0241 & 0.0070 & 3.46 + bilut & 16 & 14.70 & 2.37 & 6.16 & 0.028669 & 0.0114 & 2.51 + bilut & 128 & 16.58 & 2.43 & 6.79 & 0.028380 & 0.0100 & 2.84 + bilut & 512 & 19.91 & 2.64 & 7.50 & 0.027945 & 0.0113 & 2.47 + ras & 256 & 23.45 & 2.66 & 8.75 & 0.0428 & 0.0072 & 5.96 + ras & 2048 & 25.75 & 3.28 & 7.81 & 0.0546 & 0.0083 & 6.59 +    the performance of bilu , bilut and ras is similar to that of the same preconditioners in example 1 .",
    "when bilu is applied , the triangular solvers are sped over 3 times faster and the speedup of the whole solving phase is about 7 . because of the irregular non - zero pattern of bilut , the speedup of bilut is around 2 .",
    "the average speedup of the solving phase is about 6 .",
    "the speedup increases when the number of blocks grows .",
    "ras is as stable as in example 1 .",
    "the triangular solvers are sped around 6 , and the average speedup of the solving phase is around 8 .",
    "a matrix from spe10 is used @xcite .",
    "spe10 is a standard benchmark for the black oil simulator .",
    "the problem is highly heterogenous and it is designed the way so that it is hard to solve .",
    "the grid size for spe10 is 60x220x85 .",
    "the number of unknowns is 2,188,851 and the number of non - zeros is 29,915,573 .",
    "the ilu preconditioners we use in this example are block ilu(0 ) and block ilut(14 , 0.01 ) .",
    "the performance data is collected in table [ tab - ex3 ] .",
    "llllllll pre & blocks & cpu ( s ) & gpu ( s ) & speedup & pre cpu ( s)&pre gpu ( s ) & speedup +    bilu & 16 & 92.80 & 12.78 & 7.25 & 0.0421 & 0.0118 & 3.54 + bilu & 128 & 86.22 & 12.05 & 7.14 & 0.0423 & 0.0119 & 3.56 + bilu & 512 & 92.82 & 12.87 & 7.20 & 0.0424 & 0.0119 & 3.56 + bilut & 16 & 32.00 & 7.17 & 4.46 & 0.0645 & 0.0747 & 0.86 + bilut & 128 & 42.51 & 7.82 & 5.42 & 0.0647 & 0.0747 & 0.86 + bilut & 512 & 47.44 & 8.80 & 5.37 & 0.0645 & 0.0747 & 0.86 + ras & 256 & 106.61 & 14.36 & 7.41 & 0.100 & 0.0198 & 5.07 + ras & 1024 & 110.36 & 16.36 & 6.73 & 0.124 & 0.0242 & 5.11 +    from table [ tab - ex3 ] , we can speed the whole solving phase 6.2 times faster when ilu(0 ) is applied .",
    "the speedup increases if we increase the number of blocks .",
    "the average speedup of bilu is about 3 and the average speedup for the whole solving stage is about 7 . in this example",
    ", bilut is the best , which always takes the least running time .",
    "however , due to its irregular non - zero pattern , we fail to speed the triangular solvers .",
    "the ras preconditioner is always stable , and the average speedup of ras is about 5 , while the average speedup of the solving phase is around 6.5 .",
    "we have developed a new matrix format and its corresponding triangular solvers . based on them , the block ilu(k ) , block ilut and restricted additive schwarz preconditioners",
    "have been implemented .",
    "the block ilu(0 ) is sped over three times faster , the block ilut is sped around 2 times , and the ras preconditioner is sped up to 7 times faster .",
    "the latter preconditioner is very stable and it can be served as a general preconditioner for parallel platform ."
  ],
  "abstract_text": [
    "<S> in this paper , we investigate gpu based parallel triangular solvers systematically . </S>",
    "<S> the parallel triangular solvers are fundamental to incomplete lu factorization family preconditioners and algebraic multigrid solvers . </S>",
    "<S> we develop a new matrix format suitable for gpu devices . </S>",
    "<S> parallel lower triangular solvers and upper triangular solvers are developed for this new data structure . with these solvers , ilu preconditioners and domain decomposition preconditioners </S>",
    "<S> are developed . </S>",
    "<S> numerical results show that we can speed triangular solvers around seven times faster .    </S>",
    "<S> solver , gpu , parallel , linear system </S>"
  ]
}