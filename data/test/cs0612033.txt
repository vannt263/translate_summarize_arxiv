{
  "article_text": [
    "the automatic extraction of acronyms and their meaning from corpora is an important sub - task of text mining",
    ". we will refer to it by the term _ acronym - meaning extraction_.    much work has been done on it . to mention just some : matched an acronym against a text chunk , thus producing different candidate definitions for the acronym .",
    "they alternatively tried heuristic approaches , naive bayes learning , and compression ( i.e. , shortest description length ) to select the best candidate .",
    "added shallow parsing , and matched an acronym against a parsed ( i.e. , structured ) text chunk . used a heuristic algorithm to deterministically find an acronym s meaning",
    "the task can bes seen as a special case of string alignment between a text chunk and an acronym .",
    "for example , the text chunk `` they have many hidden markov models '' can be aligned with the acronym `` hmms '' in different ways , such as `` they have many * h*idden * m*arkov * m*odel*s * '' or `` t*h*ey have * m*any hidden * m*arkov model*s * '' .",
    "alternative alignments have different cost , and ideally the least costly one should give the correct meaning of the acronym .",
    "string alignment admits in general four different edit operations : insertion , deletion , substitution , and preservation of a letter @xcite . in the case of acronym - meaning extraction ,",
    "only deletion and preservation can occur .",
    "string alignment has a worst - case time complexity of @xmath0 , with @xmath1 and @xmath2 being the lengths of the two aligned strings , respectively .",
    "this also holds for the present special case .",
    "the purpose of this report is to show how the alignment - based approach to acronym - meaning extraction can be implemented by means of a 3-tape weighted finite - state machine ( 3-wfsm ) .",
    "the 3-wfsm will read a text chunk on tape 1 and an acronym on tape 2 , and generate all possible alignments on tape 3 , inserting dots to mark which letters are used in the acronym . for the above example this would give `` they have many .hidden",
    ".markov .model.s '' , among others .",
    "the 3-wfsm can be automatically generated from a clear and relatively simple regular expression that defines the task in as much detail as we wish .",
    "both the generation and the application of the 3-wfsm are done by ( more or less ) standard operations @xcite , that are available in finite - state tools such as wfsc @xcite .",
    "no additional algorithms are required at any stage , which reduces the development time to a few hours .",
    "using basic unix utilities , we first extract from a corpus all sentences that contain acronyms in brackets , such as    between a hidden markov model ( hmm ) and a weighted finite - state +    ' '' ''    automaton ( wfsa ) there is a correspondence .",
    "+    then , we split these sentences into pairs consisting of an acronym and the text chunk that precedes it ( starting from the sentence beginning or form the preceding acronym , respectively ) . for the above example",
    ", this is    between a hidden markov model    ' '' ''    hmm + and a weighted finite - state automaton    ' '' ''    wfsa     +    next , we normalize these pairs : capital letters are transformed into small ones , and separators into underscores :    between_a_hidden_markov_model    ' '' ''    hmm + and_a_weighted_finite_state_automaton    ' '' ''    wfsa     +",
    "we start by compiling a 4-wfsm over the real tropical semiring @xmath3 , from the expression @xmath4    where @xmath5 is a _ symbol class _ accepting any symbol except underscore , @xmath6 represents the empty string , @xmath7 a constraint requiring the different @xmath5 on tapes @xmath8 to @xmath9 to be instantiated by the same symbol @xcite , and the @xmath10 s are weights .",
    "we use a superscript @xmath11 to indicate the arity of a @xmath12-wfsm @xcite .    if we apply @xmath13 with tapes 1 and 2 to a normalized text chunk and a corresponding acronym , respectively , and generate ( neutrally - weighted ) alternative analyses from tapes 3 and 4 , we obtain , for example    .... 1,2 > they_have_many_hidden_markov_models       hmms    3,4 > t.hey_have_.many_hidden_.markov_model.s     iaii_iiii_aiii_iiiiii_aiiiii_iiiiia      0 3,4 > t.hey_have_.many_hidden_markov_.model.s     iaii_iiii_aiii_iiiiii_iiiiii_aiiiia      0 3,4 > t.hey_have_many_hidden_.markov_.model.s     iaii_iiii_iiii_iiiiii_aiiiii_aiiiia      0 3,4 > they_.have_.many_hidden_.markov_model.s     iiii_aiii_aiii_iiiiii_aiiiii_iiiiia      0 3,4 > they_.have_.many_hidden_markov_.model.s     iiii_aiii_aiii_iiiiii_iiiiii_aiiiia      0 3,4 > they_.have_many_hidden_.markov_.model.s     iiii_aiii_iiii_iiiiii_aiiiii_aiiiia",
    "0 3,4 > they_have_many_.hidden_.markov_.model.s     iiii_iiii_iiii_aiiiii_aiiiii_aiiiia      0 ....    on tape 3 , letters of the text chunk which are used in the acronym , are preceded by a dot .",
    "tape 4 shows the performed operations : a meaning `` acronym letter '' , i meaning `` ignored letter '' .",
    "all analyses have neutral weight 0 .    by means of xfst",
    "@xcite we generate from the following regular expression a 2-fsm ( i.e. , a non - weighted transducer ) , @xmath14 , that defines the operations more precisely : @xmath15 .o . [ g - > g ||   [ .#. | .o . [ a - > 1 ||   [ .#. | .o . [ a - > 2 ||   [ .#. | .o . [ a - > 3 ||   [ .#. | .o . [ a - > 4 ||   [ .#. | .o . [ a - > 5 ||   [ .#. | .o . [ a - > 6 ||   [ .#. | .o . [ a - > 7 ||   [ .#. | .o . [ a - > 8 ||   [ .#. | .o . [ \\end{verbatim } } \\end{minipage}\\ ] ]    in this expression , all word - initial i are replaced by u ( `` unused word '' ) if no letter of this word is used in the acronym , but if letters of preceding words are used .",
    "then , all i of a used word are replaced by g ( `` gap letter '' ) if they are followed by an a ( `` acronym letter '' ) in the same word .",
    "next , word - initial g are replaced by g ( `` word - initial gap letter '' ) .",
    "finally , all a are replaced by a number 1 to 8 , expressing their position in the word .",
    "positions higher than 8 are marked as 8 .",
    "furthermore , we generate with xfst another 2-fsm , @xmath16 , that deletes all letters of leading unused words , and the adjacent underscores : @xmath17    these two non - weighted 2-fsms are transformed into 2-wfsms , @xmath18 and @xmath19 , with neutral weight , and are joined @xcite with the previously compiled 4-wfsm @xmath13  : @xmath20    in the resulting @xmath21 , we have a modified form of tape 3 on tape 5 ( describing analyses ) , and a modified form of tape 4 on tape 6 ( describing operations ) .",
    "if we apply tape 1 to a normalized text chunk and tape 2 to a corresponding acronym , we obtain from tapes 5 and 6 for example    .... 1,2 > they_have_many_hidden_markov_models      hmms    5,6 > t.hey_have_many_hidden_.markov_.model.s    g2ii_uiii_uiii_uiiiii_1iiiii_1gggg6      0 5,6 > t.hey_have_.many_hidden_markov_.model.s    g2ii_uiii_1iii_uiiiii_uiiiii_1gggg6      0 5,6 > t.hey_have_.many_hidden_.markov_model.s    g2ii_uiii_1iii_uiiiii_1iiiii_ggggg6      0 5,6 >       .have_many_hidden_.markov_.model.s    iiii_1iii_uiii_uiiiii_1iiiii_1gggg6      0 5,6 >       .have_.many_hidden_markov_.model.s    iiii_1iii_1iii_uiiiii_uiiiii_1gggg6      0 5,6 >       .have_.many_hidden_.markov_model.s    iiii_1iii_1iii_uiiiii_1iiiii_ggggg6      0 5,6 >",
    ".hidden_.markov_.model.s    iiii_iiii_iiii_1iiiii_1iiiii_1gggg6      0 ....    finally , we assign costs ( i.e. , weights ) to the different operations by means of a 1-wfsm generated from the expression @xmath22    here we chose the costs by intuition . in an improved approach they could be estimated from data .    to obtain our acronym - meaning extractor",
    ", we join @xmath23 with the previously compiled @xmath21 : @xmath24    if we apply @xmath25 with tapes 1 and 2 to a text chunk and a corresponding acronym , respectively , we obtain from tapes 5 and 6 for example    .... 1,2 > they_have_many_hidden_markov_models      hmms    5,6 > t.hey_have_many_hidden_.markov_.model.s    g2ii_uiii_uiii_uiiiii_1iiiii_1gggg6      17 5,6 > t.hey_have_.many_hidden_markov_.model.s    g2ii_uiii_1iii_uiiiii_uiiiii_1gggg6      17 5,6 >",
    "t.hey_have_.many_hidden_.markov_model.s    g2ii_uiii_1iii_uiiiii_1iiiii_ggggg6      18 5,6 >       .have_many_hidden_.markov_.model.s    iiii_1iii_uiii_uiiiii_1iiiii_1gggg6      11 5,6 >       .have_.many_hidden_markov_.model.s",
    "iiii_1iii_1iii_uiiiii_uiiiii_1gggg6      11 5,6 >       .have_.many_hidden_.markov_model.s    iiii_1iii_1iii_uiiiii_1iiiii_ggggg6      12 5,6 >                 .hidden_.markov_.model.s",
    "iiii_iiii_iiii_1iiiii_1iiiii_1gggg6       7 ....    we select the analysis with the lowest cost by means of a classical single - source best - path algorithm such as dijkstra s algorithm @xcite or bellman - ford s @xcite .",
    "if our weights have been optimally chosen , we should now obtain the correct analysis .    in practice",
    ", input is read on tapes 1 and 2 and output generated from tapes 2 and 5 , as in the following examples .",
    "all other tapes can therefore be removed , leaving us with a 3-wfsm @xmath26 .    ....",
    "1,2 > they_have_many_hidden_markov_models     hmms 1,2 > between_hidden_markov_models            hmms 1,2 >",
    "and_weighted_finite_state_automata      wfsa 1,2 > and_weighted_finite_state_automata      wfa    2,5 > hmms     .hidden_.markov_.model.s 2,5 > hmms     .hidden_.markov_.model.s 2,5 > wfsa     .weighted_.finite_.state_.automata 2,5 > wfa      .weighted_.finite_state_.automata ....",
    "we tested the acronym - meaning extractor on many examples . finding",
    "the best analysis for one acronym took only a few milliseconds ( ms )  : for example , 3.7 ms for `` they have many hidden markov models''-``hmms '' , 9.6 ms for `` they have many hidden markov models they have many hidden markov models''-``hmms '' , and 12.0 ms for `` they have many hidden markov models they have many hidden markov models''-``hmmshmms '' .",
    "karttunen , lauri , tams gal , and andr kempe .",
    "online demo and documentation .",
    "xerox research centre europe , grenoble , france .",
    "/ competencies / content - analysis / fscompiler/.    kempe , andr , christof baeijs , tams gal , franck guingne , and florent nicart .",
    "2003 .  a new weighted finite state compiler . in o.",
    "h. ibarra and z.  dang , editors , _ proc .",
    "volume 2759 of _ lecture notes in computer science _ , pages 108119 , santa barbara , ca , usa .",
    "springer verlag , berlin , germany .",
    "kempe , andr , jean - marc champarnaud , and jason eisner .",
    "2004 . a note on join and auto - intersection of _ n_-ary rational relations . in b.",
    "watson and l.  cleophas , editors , _ proc .",
    "eindhoven fastar days _ ,",
    "number 0440 in tu / e cs tr , pages 6478 , eindhoven , netherlands .",
    "nicart , florent , jean - marc champarnaud , tibor cski , tams gal , and andr kempe .",
    "multi - tape automata with symbol classes . in o.h . ibarra and h",
    "yen , editors , _ proc .",
    "11th int . conf . on implementation and application of automata ( ciaa06 ) _ , volume 4094 of _ lecture notes in computer science _ , pages 126136 ,",
    "taipei , taiwan .",
    "springer verlag .",
    "pirkola , ari , jarmo toivonen , heikki keskustalo , kari visala , and kalervo jrvelin . 2003 .",
    "fuzzy translation of cross - lingual spelling variants . in _ proceedings of the 26th annual international acm sigir _ , pages 345352 , toronto , canada .",
    "pustejovsky , james , jos  casta  no , brent cochran , maciej kotecki , michael morrell , and anna rumshisky .",
    "linguistic knowledge extraction from medline : automatic construction of an acronym database . in _ proc .",
    "10th world congress on health and medical informatics ( medinfo 2001)_.        yeates , stuart , david bainbridge , and ian  h. witten .",
    "2000 . using compression to identify acronyms in text . in _ proc .",
    "data compression conference ( dcc-2000 ) _ , snowbird , utah , usa . ( also published in a longer form as working paper 00/01 , department of computer science , university of waikato , january 2000 ) ."
  ],
  "abstract_text": [
    "<S> the automatic extraction of acronyms and their meaning from corpora is an important sub - task of text mining . </S>",
    "<S> it can be seen as a special case of string alignment , where a text chunk is aligned with an acronym . </S>",
    "<S> alternative alignments have different cost , and ideally the least costly one should give the correct meaning of the acronym .    </S>",
    "<S> we show how this approach can be implemented by means of a 3-tape weighted finite - state machine ( 3-wfsm ) which reads a text chunk on tape 1 and an acronym on tape 2 , and generates all alternative alignments on tape 3 . </S>",
    "<S> the 3-wfsm can be automatically generated from a simple regular expression . </S>",
    "<S> no additional algorithms are required at any stage . </S>",
    "<S> our 3-wfsm has a size of 27 states and 64 transitions , and finds the best analysis of an acronym in a few milliseconds . </S>"
  ]
}