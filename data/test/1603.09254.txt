{
  "article_text": [
    "models with latent variables have been proposed and investigated for explaining , understanding , or classifying observed data .",
    "if a model is a generative model , observed data are modeled to be as if they were generated by latent variables through parameterized probability distributions .",
    "popular criteria for learning generative models include likelihood or posterior probability , which both evaluate the probability of the given observed data or parameters .",
    "another kind of criteria is mutual information .",
    "mutual information has been used to learn non - linear generative models  @xcite in which relationships between observed and latent variables are directly evaluated .",
    "it has also been used to learn linear encoding ( recognition ) models  @xcite .",
    "the relationships between observed and latent variables have greater importance in more complex generative models , e.g. , deep learning models  @xcite . in the pre - training of deep belief networks ( dbns ) , one of the models or techniques of deep learning , posterior samples of latent variables in the lower layer are used as samples of observed variables in the next , higher layer . for successive layer learning to be possible",
    ", latent variables should possess properties that enable such learning .",
    "it is crucial and fundamental for multiple layer learning theory to assess which observed variable properties are preserved , discarded , or modified in latent variables . for this purpose , it is necessary to have good measures that capture the capability of higher layer learning and to know the configurations of models suitable for higher layer learning .",
    "unfortunately , mutual information is not an adequate measure for this purpose .",
    "the maximization of mutual information is known to yield independent latent variables under certain conditions  @xcite , however , if latent variables are independent of each other , successive learning exploiting their correlations becomes impossible .    in this paper",
    ", we propose a novel measure to capture the dissimilarity between latent and observed variables in two - layer models .",
    "we refer to the proposed measure as latent - observed dissimilarity ( lod ) .",
    "the key idea is to define a `` virtual - latent '' probability mass function ( pmf ) over observed variables , using the conditionally expected information of latent variables .",
    "this definition provides us with a new pmf for which we can measure the dissimilarity from the original pmf .",
    "the dissimilarity between these two pmfs can be regarded as the dissimilarity between the latent and observed variables , since the defined pmf reflects the conditionally expected information of latent samples , while the original pmf reflects the self - information of observed samples .",
    "we applied lod to four essential types of two - layer models : 1 ) a single - latent - variable model ( sl ) , 2 ) a multi - latent - variable model whose latent variables are independent of each other ( il ) , 3 ) a multi - latent - variable model whose latent variables are conditionally independent given observed variables ( ci ) , and 4 ) a multi - latent - variable model whose latent variables are independent of each other and conditionally independent given observed variables ( ici ) .",
    "these four types cover the major possible combinations of independence or conditional independence in two - layer models . in our experiments ,",
    "lod clearly reflected the difference between these four model types .",
    "lod was also shown to reflect the latent layer s capability for higher layer learning .",
    "our experiments also revealed that the conditional independence of latent variables given observed variables , particularly for ci models , contributes to the improvement of higher layer learning , improving lod and the mutual information between lower and higher layers .",
    "let @xmath0 denote the probability mass function ( pmf ) of a generative model where @xmath1 denotes observed variables and @xmath2 denotes latent variables . when an observation @xmath1 is received , its self information under a model @xmath3 is given as @xmath4 .",
    "we first define the corresponding expected information for latent variables .",
    "let @xmath5 denote the expected information of @xmath2 given @xmath1 , @xmath6 where @xmath5 may be said to be the expected surprise of the latent layer given @xmath1 , while @xmath7 is the surprise of the observed layer given @xmath1 .",
    "we then define a pmf @xmath8 based on @xmath5 . to measure the distance between some pmf and @xmath5 ,",
    "preprocessing is necessary because the function @xmath5 is not guaranteed to be a pmf .",
    "based on the fact that @xmath5 represents the expected information , we define the following pmf , @xmath9 where @xmath10 .",
    "let @xmath11 denote a data distribution .",
    "that is , we assume @xmath12 for any function @xmath13 . using @xmath8 , we define the dissimilarity between the observed and latent variables for a dataset using kl - divergence , @xmath14      [ [ single - variable - example . ] ] single variable example .",
    "+ + + + + + + + + + + + + + + + + + + + + + + +    we now study the differences between lod and mutual information using single variable examples .",
    "the proposed measure , lod , behaves differently from the mutual information of @xmath1 and @xmath2 . when the joint probability of @xmath1 and @xmath2 is defined by @xmath0 , the mutual information @xmath15 between @xmath1 and @xmath2 is @xmath16 where @xmath17 denotes the kullback - leibler divergence .",
    "a more data - based evaluation is possible if the data distribution @xmath11 is employed @xmath18 we also refer to mi as ( data - based ) mutual information .",
    "consider the difference between lod and mi in the simplest case .",
    "consider a model consisting of a single observed variable and a single latent variable .",
    "let @xmath19 and @xmath20 .",
    "define the probabilities @xmath21 as @xmath22 , respectively . for simplicity",
    ", we assume the mapping from @xmath1 to @xmath2 to be deterministic , so each @xmath23 is either @xmath24 or @xmath25 . from among all possible @xmath23 under this assumption ,",
    "let @xmath26 denote the one that realizes the best lod , and let @xmath27 denote the one that realizes the best mi .",
    "the joint and marginal probabilities of @xmath28 and @xmath29 as well as the transformed probabilities @xmath8 are shown in table [ tb_p1_p2_6 ] .",
    "note that since @xmath30 by assumption , the log likelihood is maximized for both @xmath28 and @xmath29 , as @xmath31 .",
    "the scores of lod and mi are shown in table [ tb_p1_p2_score_6 ] .",
    ".joint and marginal probabilities of @xmath28 and @xmath29 , and transformed probabilities @xmath8 .",
    "top : the best similarity assignment .",
    "bottom : the best mutual information assignment . note that @xmath32 , @xmath33 . [",
    "cols=\"^,^,^,^,^,^,^,^\",options=\"header \" , ]",
    "we proposed latent - observed dissimilarity ( lod ) , a dissimilarity measure between latent and observed variables in generative models , to evaluate the relationships between latent and observed variables .",
    "lod compares the self - information of an observation with the expected information of a latent layer given that observation .",
    "we numerically evaluated four types of two - layer models ( sl , il , ci , and ici ) using log likelihood , mutual information , and lod .",
    "the results suggested an advantage of using lod as a measure for multi - layer learning ; the lod between observed and latent variables had significant correlation with the lod between observed and higher layer latent variables for all four types of models , while mutual information had significant correlation only for ci models .",
    "the results also suggested the conditional independence of latent variables given observed variables facilitates the transmission of a layer s characteristics to the higher layers .",
    "this fact sheds new light on the advantages of conditional independence , of which usually only its computational advantage is emphasized .",
    "this work was supported by mext kakenhi grant number 23240019 .",
    "masashi ohata , toshiharu mukai , and kiyotoshi matsuoka . independent component analysis on the basis of helmholtz machine . in _",
    "4th international symposium on independent component analysis and blind signal separation ( ica2003 ) _ , 2003 ."
  ],
  "abstract_text": [
    "<S> quantitatively assessing relationships between latent variables and observed variables is important for understanding and developing generative models and representation learning . in this paper , we propose latent - observed dissimilarity ( lod ) to evaluate the dissimilarity between the probabilistic characteristics of latent and observed variables . </S>",
    "<S> we also define four essential types of generative models with different independence / conditional independence configurations . </S>",
    "<S> experiments using tractable real - world data show that lod can effectively capture the differences between models and reflect the capability for higher layer learning . </S>",
    "<S> they also show that the conditional independence of latent variables given observed variables contributes to improving the transmission of information and characteristics from lower layers to higher layers . </S>"
  ]
}