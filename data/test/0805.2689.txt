{
  "article_text": [
    "mixture models provide an intuitive statistical representation of datasets structured in groups , clusters or classes @xcite .",
    "a complex dataset is decomposed into the superposition of simpler datasets .",
    "the inverse problem consists in determining the group decomposition and the statistical parameters characterizing each group . for a fixed number of groups the expectation maximization ( em ) algorithm",
    "provides a recursive solution to the inverse problem @xcite .",
    "the estimation of the right number or groups has been , however , a great challenge .",
    "corrections such as the arkaike information criterion ( aic ) @xcite and the bayesian information criterion ( bic ) @xcite have been derived , penalizing model complexity and overfitting . yet , the number of groups estimated from these criteria is in general unsatisfactory .",
    "in contrast , a bayesian approach would not attempt to estimate what is the `` optimal '' number of groups , but instead average over models with a different number of groups @xcite .",
    "the bayesian approach is becoming a popular technique to solve problems in data analysis , model selection and hypothesis testing @xcite .",
    "many of the original ideas come from the early work of jeffreys @xcite , but it is just recently that they are starting to be used widely @xcite",
    ". the application of bayesian approaches to real problems can be , however , quite challenging . in most cases",
    "the solution is explored via monte carlo sampling @xcite or variational methods @xcite .",
    "the application of variational methods to bayesian problems results in the variational bayes ( vb ) algorithm @xcite .",
    "the vb algorithm is a set of self - consistent equations analog to the em algorithm .",
    "they can be solved recursively obtaining an approximate solution to the inverse inference problem .",
    "these methods have been applied , for example , to gaussian mixture models for real value data @xcite , dirichlet mixture models for categorical data @xcite and the problem of finding graph modules @xcite .    here",
    "we further study the use of variational methods in the context of bayesian approaches , focusing on data clustering problems .",
    "i the first two sections we review the bayesian approach . in section [ variational ]",
    "we revisit the connection between the bayesian formulation and statistical mechanics . in section [ models ]",
    "we introduce the generalities of generative models with a hidden structure at the samples side and at both the samples and variables side . in section [ s : priors ] we extend the previous work by jaynes @xcite deriving prior distributions based on symmetry properties .",
    "we report a correction to his result for the model with a location and scale parameter and an extension of his result for the binomial model to the multinomial model . in the following sections we study the problem of two - sides clustering real value data and of clustering data represented by a hypergraph or bipartite graph . depending on our starting statistical model",
    ", we obtain a vb algorithm . because of its bayesian root ,",
    "the vb algorithms have a built in correction for model complexity or bias and , therefore , they do not require the use of additional complexity criteria .",
    "the performance of the vb algorithms is tested in some examples , obtaining satisfactory results whenever there is a significant distinction between the groups .",
    "the _ bayesian approach _ is a systematic methodology to interpret complex datasets and to evaluate model hypothesis .",
    "its main ingredients or steps are : given a dataset @xmath0 , ( i ) introduce a statistical model with model parameters , @xmath1 , ( ii ) write down the likelihood to observe the data given the proposed model and parameters , @xmath2 , ( iii ) determine the prior distribution for the model parameters based on our current knowledge , @xmath3 , and , finally , ( iv ) invert the statistical model of the data given the likelihood and prior distribution to obtain the posterior distribution of the model parameters given the model and data , @xmath4 .",
    "the latter step is based on bayes rule    @xmath5    where    @xmath6    having obtained the distribution of the model parameters , at least formally , we can determine other magnitudes .",
    "for example , the average of a quantity @xmath7 is given by    @xmath8    in practice calculating ( [ z ] ) or ( [ ave ] ) is a formidable task .",
    "a very powerful approximation scheme is the _ variational method _",
    "the main idea of the variational method is to approximate the generally difficult to handle distribution @xmath4 by a distribution @xmath9 of a more tractable form . in the following",
    "we omit the dependency of @xmath10 on @xmath0 and just write @xmath11 .",
    "given @xmath11 we can obtain a bound for @xmath12 using jensen s inequality    @xmath13    the latter equation can be rewritten as @xcite    @xmath14    where @xmath15 ,    @xmath16    is minus the average log likelihood and    @xmath17    is the kullback - leibler divergence of @xmath11 relative to the prior distribution @xmath3 @xcite .",
    "equation ( [ f ] ) resembles the usual free energy in statistical mechanics : @xmath18 , where @xmath19 , @xmath20 and @xmath21 are the internal energy , entropy and temperature of the system , the temperature being expressed in units of the boltzman constant @xmath22 . minus",
    "the average log likelihood plays the role of the internal energy , the kullback - leibler divergence of @xmath11 plays the role of the entropy and temperature equals one .    equation ( [ f ] ) emphasizes the two components determining the best choice of variational distribution @xmath11 : better fit to the data and model bias . how well the data is fitted is quantified by the internal energy @xmath19 ( [ u ] ) . to achieve the best fit , or internal energy ground state , @xmath11 should be concentrated around the regions of the parameter space where @xmath2 is maximum . the best choice in this respect will by the maximum likelihood estimate ( mle )    @xmath23    where    @xmath24    in the opposite extreme ,",
    "when no data is presented to us , the best distribution is that maximizing the kullback - leibler divergence relative to the prior distribution .",
    "this maximum entropy ( me ) solution is the prior distribution itself    @xmath25    in general , the drive to better fit the data is opposed by the tendency to obtain the least unbiased model .",
    "the variational solution is therefore in the middle between the one extreme of biased models fitting the data very well and completely unbiased models giving a bad fit to the data .",
    "it is obtained after minimizing ( [ f ] ) with respect to @xmath11 over a restricted class of functions . this variational solution @xmath11 represents the closest distribution to @xmath4 within the class of functions considered .",
    "in this section we present the generalities of statistical models with a first level population structure .",
    "similar models has been studied in @xcite .",
    "our working hypothesis is that there is a hidden population structure , characterized by the subdivision of the population samples into groups .",
    "we assume that we are given a dataset @xmath0 which , in some way to be determined , reflects the population structure .",
    "the problem consist in inferring this hidden structure and the associated model parameters from the data . to tackle this problem",
    "we introduce a statistical model with a built in population structure as a generative model of the data .",
    "the population structure and the model parameters are then inferred solving the inverse problem .",
    "more precisely    * we consider a population composed of @xmath26 elements divided in @xmath27 groups .",
    "* the samples assignment to groups is generated by a multinomial model with probabilities @xmath28 , @xmath29 . denoting by @xmath30 the group",
    "to which the @xmath31-th sample belongs , we obtain + @xmath32 * given the group assignments @xmath30 , and depending on the dataset , we write down the likelihood @xmath33 to observe the data parametrized by the parameter set @xmath34 . *",
    "putting all this together we obtain the posterior distribution + @xmath35 + where @xmath36 and @xmath37 , @xmath38 and @xmath39 are the prior distributions of @xmath34 , @xmath40 and @xmath27 .",
    "the form of the prior distributions , except for @xmath39 , is the subject of the next section .",
    "the distribution @xmath39 is irrelevant for problems with large datasets .",
    "the difference between the log - likelihood of models with different values of @xmath27 is in general of the order of the dataset size and , as a consequence , the contribution of @xmath41 is negligible .",
    "thus , in the following sections we simply neglect the contribution given by @xmath39 . finally , we specify the likelihood @xmath33 when addressing specific problems .    in some cases we are going to assume that the variables in our dataset are also divided in groups .",
    "here we consider a set of @xmath42 variables divided in @xmath43 groups .",
    "the variables assignment to groups is generated by a multinomial model with probabilities @xmath44 , @xmath45 . denoting by @xmath46 , @xmath47 , the variable group to which variable @xmath48 belongs we",
    "can then write    @xmath49    after adding this variable group structure , the posterior distribution ( [ gm1 ] ) is replaced by    @xmath50    where @xmath51 and @xmath52 is the prior distribution of @xmath53 .",
    "[ priors ]    [ cols=\"<,<,<,<,<\",options=\"header \" , ]     the choice of the prior distribution @xmath3 is probably one of the less obvious topics in bayesian analysis .",
    "currently the predominant choice is the use of conjugate priors .",
    "the form of conjugate priors is indicated by the likelihood , making the prior selection less ambiguous .",
    "for example , the binomial likelihood @xmath54 suggests a beta distribution for @xmath55 .",
    "furthermore , by choosing a beta distribution as a prior , @xmath56 , the posterior distribution remains a beta distribution , but with exponents @xmath57 and @xmath58 . in this sense , the beta distribution is the conjugate prior of the binomial likelihood .",
    "a list of conjugate priors relevant for this work is provided in table [ priors ] .",
    "yet , the fact that the form of conjugate priors is suggested by the likelihood does not demonstrate that they are the correct choice of priors .",
    "moreover , even if we accept their use , it is not clear what is the correct choice for the prior distribution parameters , e.g. @xmath59 and @xmath60 .",
    "different methods have been proposed to determine these parameters . in general",
    "they are based on _ a posteriori _ analyzes , e.g. calculations , making use of the data in some way or another .",
    "such methods violate , however , the concept of prior distribution , defined as the distribution of the model parameters in the absence of the data .",
    "an alternative approach is that by jaynes @xcite . according to jaynes , in the absence of any data",
    ", the priors should be solely determined based on the symmetries and constraints of the problem under consideration . in this work",
    "we make use of jaynes s approach to determine the prior distribution .",
    "below we derive jaynes s priors for the cases relevant for this work .",
    "consider a problem where the data consists of equally distributed random variables @xmath61 , @xmath62 , taking real values . furthermore let us assume that the likelihood has the form    @xmath63    where @xmath64 is a probability density function in the real line and @xmath65 and @xmath66 are a location and scale parameter respectively .",
    "our task consist in determining the prior distribution of @xmath65 and @xmath66 . now , suppose @xmath61 represent positions , which could be measured from difference systems of reference and using different units",
    ". in this context the prior distribution should be the same regardless of our system of reference and units .",
    "more precisely , our system is invariant under the transformations    @xmath67    where @xmath68 represents a translation and @xmath69 a change of scale or units .",
    "the likelihood is invariant under these transformations and so must be the prior distribution . therefore ,    @xmath70    the solution to this functional equation is    @xmath71    this analysis was first reported by jaynes @xcite .",
    "he obtained , however , @xmath72 .",
    "this discrepancy is rooted in the fact that jaynes did not take into account that the location parameter @xmath65 follows the same rules than @xmath73 upon the translation and scale transformations .",
    "he assumed @xmath74 @xcite while the correct transformation is @xmath75 ( [ t1 ] ) .",
    "consider the multinomial model with @xmath27 states    @xmath76    where @xmath77 is the number of times state @xmath78 was observed and @xmath28 is the probability to observe state @xmath78 in one trial , @xmath79 and @xmath80 . here we extend the approach followed by jaynes for the binomial model @xcite .",
    "the probabilities @xmath28 may be different depending on our believe , e.g. all states are equally probable .",
    "different investigators may have different believes , resulting in different choices of @xmath28 .",
    "the main assumption is that the prior distribution should be independent of what is our specific believe and , therefore , should be invariant under a believe transformation .",
    "_ believe transformation : _ let us represent by @xmath81 the state @xmath78 , and let @xmath82 and @xmath83 be the probabilities to observe state @xmath81 in one trial according to believe @xmath84 and @xmath85 , respectively . from bayes",
    "rule it follows that    @xmath86    for @xmath29 .",
    "the latter equation can be rewritten as    @xmath87    for @xmath88 and @xmath89 , where @xmath90 , @xmath91 ,    @xmath92    and    @xmath93    equation ( [ t3 ] ) provides the transformation rules of the probabilities @xmath28 from one system of believe to another .",
    "the invariance under the above transformation lead to the functional equation    @xmath94    to solve this equation we first need to compute the determinant of the transformation jacobian .",
    "the jacobian of the transformation ( [ t3 ] ) has the matrix elements    @xmath95    @xmath96 .",
    "this matrix can be decomposed into the product @xmath97 , where @xmath98 is a diagonal matrix and @xmath99 has two eigenvalues , @xmath100 and a @xmath101-degenerate eigenvalue @xmath102 . putting all together we obtain",
    "@xmath103 ) , with @xmath104 , is given by    @xmath105    note that for @xmath106 , @xmath107 and @xmath108 , we recover the result by jaynes for the binomial model    @xmath109      the prior distributions ( [ p1 ] ) and ( [ p2 ] ) are improper , i.e. their integral over the parameter space is not finite . at first this",
    "may sound an unsuitable property for a prior distribution .",
    "nevertheless , the improper nature of these prior distributions is just indicating that the symmetries in our problem are not sufficient to fully determine them .",
    "data is required to obtain a proper distribution .",
    "the best example for an intuitive understanding of these arguments is the prior distribution of the location parameter . in the absence of any data and under the assumption of translational invariance ,",
    "it is clear that every value in the real line is an equally probable value for the location parameter , resulting in an improper prior .    from the operational point of view",
    ", the posterior distribution may be proper even when the prior is not .",
    "indeed , the integral @xmath110 may be improper , @xmath111 may be proper .",
    "the posterior distribution can be improper when the inference problem has not been correctly formulated or there is not sufficient data to determine the model parameters .    to avoid dealing with improper distributions , we can renormalize improper priors to some limit of a proper distribution .",
    "since conjugate priors facilitate analytical calculations they are a good starting point .",
    "this is illustrated in table ( [ priors ] ) for selected examples .",
    "these are the prior distributions used herein .",
    "in particular , for the multinomial probabilities @xmath40 and @xmath53 we use the renormalized invariant priors    @xmath112    @xmath113    with @xmath114 and @xmath115 .",
    "in this section we specify the form of the variational function @xmath11 . to allow for an analytical solution we neglect correlations between the group assignments and the remaining model parameters .",
    "we denote by @xmath116 the probability that sample @xmath31 belongs to sample group @xmath78 and by @xmath117 the probability that probe @xmath48 belongs to probe group @xmath118 . furthermore , given that @xmath34 , @xmath40 and @xmath53 always appear in different factors in ( [ gm1 ] ) or ( [ gm2 ] ) then their join distribution factorizes .",
    "within the mean - field approximation for the group assignments and the later factorization the variational function can be written as    @xmath119    when dealing with the generative model ( [ gm1 ] ) and    @xmath120    when dealing with the generative model ( [ gm2 ] ) , where @xmath121 denotes a generic probability density function of @xmath73 .",
    "summarizing , in the case studies below , we are going to solve the generative models ( [ gm1 ] ) or ( [ gm2 ] ) , making use of renormalized invariant priors ( table [ priors ] ) and the mf variational function ( [ mf1 ] ) or ( [ mf2 ] ) , respectively .",
    "this approach is based on the assumptions that : the population is divided in groups , the group assignments are generated by a multinomial model , the priors are renormalized invariant distributions , and a mf approximation of the variational solution with respect to the group assignments .",
    "quite often we deal with datasets consisting of a real value measurement @xmath122 over @xmath123 samples and @xmath47 variables , where the samples and variables are not necessarily independent . for simplicity , the particular kind of dependency we focus on is the existence of sample and variable groups .",
    "our problem is to infer the sample and variable groups and the statistical parameters characterizing them .    to address this problem we consider the generative model ( [ gm2 ] ) with a normal likelihood , representing a two - sides gaussian mixture model .",
    "the two - sides gaussian mixture model is a natural extension of the gaussian mixture model @xcite to characterize datasets with a group structure for both the samples and variables .",
    "our contributions in this context are the use of prior distributions derived from symmetry arguments alone and the inclusion of a group structure at the variables side .",
    "the dataset , likelihood and priors associated with our statistical model are defined as follows :    _ data : _ consider @xmath123 samples , @xmath47 variables , and the real value measurements @xmath122 .",
    "_ likelihood : _ we assume that @xmath122 are random variables with a normal distribution , with group dependent mean @xmath124 and group independent variance @xmath66 , resulting in the likelihood    @xmath125    here we are assuming that the main difference between groups is given by the means while the variance is group independent . the latter is a good approximation when the source of noise is given by the measurement itself and it behaves the same independently of the sample and variable group .    _",
    "priors : _ for the prior @xmath126 we generalize the normal distribution prior in table [ priors ] . accounting for more than one location parameter",
    "we obtain    @xmath127    and we work in the limit @xmath128 .    to apply the variational method we consider the mf approximation ( [ mf2 ] ) . substituting the likelihood ( [ preal ] ) , the priors ( [ ppi ] ) , ( [ pkappa ] ) and ( [ pgm ] ) and the mf variational function ( [ mf2 ] ) into ( [ f ] ) , and integrating over @xmath1 ( summing over @xmath30 and @xmath46 and integrating over @xmath129 , @xmath66 , @xmath28 and @xmath44 ) we obtain    @xmath130 \\nonumber\\\\ & -&\\sum_k\\left(\\sum_ip_{ik}+\\tilde{\\gamma}_k-1\\right ) \\langle\\ln\\pi_k\\rangle",
    "\\nonumber\\\\ & -&\\sum_l\\left(\\sum_jq_{jl}+\\tilde{\\epsilon}_l-1\\right ) \\langle\\ln\\kappa_l\\rangle \\nonumber\\\\ & + & \\int d\\mu d\\sigma r(\\mu,\\sigma)\\ln r(\\mu,\\sigma ) \\nonumber\\\\ & + & \\int d\\pi r(\\pi)\\ln r(\\pi ) + \\int d\\kappa r(\\kappa)\\ln r(\\kappa)\\kappa \\nonumber\\\\ & + & \\sum_{ik } p_{ik}\\ln p_{ik } + \\sum_{jl } q_{jl}\\ln q_{jl}\\end{aligned}\\ ] ]    minimizing ( [ freal ] ) with respect to @xmath131 , @xmath117 , @xmath132 , @xmath133 and @xmath134 we obtain ( vb-1 ) :    @xmath135    @xmath136    @xmath137    @xmath138    @xmath139    @xmath140    @xmath141\\end{aligned}\\ ] ]    @xmath142    @xmath143    @xmath144    these are a set of self - consistent equations which can be solved recursively to determine the probabilistic group assignments and the @xmath65 , @xmath66 , @xmath40 and @xmath53 distributions .",
    "they are the same in spirit as those for the em algorithm @xcite .",
    "following @xcite we refer to them as _ variational bayes _ ( vb ) algorithm .",
    "the main difference between the em and vb algorithms is that in the former case we would take the average of the log likelihood over the group assignments but not over the distributions of @xmath65 , @xmath66 , @xmath40 and @xmath53 . by taking the average over @xmath65 and @xmath66 we",
    "obtain the additional @xmath145 term within the parenthesis in equations ( [ preal ] ) and ( [ qreal ] ) . according to ( [ alphakl ] )",
    "@xmath146 is equal to @xmath59 plus the product of the average number of samples in sample group @xmath78 ( @xmath147 ) and the average number of variables in variable group @xmath118 ( @xmath148 ) .",
    "therefore , the @xmath149 term penalizes assignments to small size groups . and",
    "it balances the contribution of @xmath150 , which drives the estimates towards a better fit and consequently groups of minimal size .",
    "the actual implementation of the vb-1 algorithm in the context of real value data proceeds as follows . set sufficiently large values for @xmath27 and @xmath43 , larger than our expectation for the actual values of @xmath27 and @xmath43 . in the following test examples we use @xmath151 .",
    "set the parameters @xmath59 , @xmath152 , @xmath153 , @xmath154 and @xmath155 .",
    "we set @xmath156 , @xmath157 and @xmath158 .",
    "the choice of @xmath152 and @xmath153 is practically irrelevant provided we have chosen a sufficiently small @xmath59 .",
    "set random initial conditions for @xmath116 and @xmath117 .",
    "starting from these random initial conditions iterate equations ( [ preal])-([f_real ] ) until the solution converges up to some predefined accuracy .",
    "we use relative error of @xmath159 smaller than @xmath160 . in practice , compute @xmath161 , @xmath162 , @xmath163 , @xmath164 , @xmath165 , @xmath166 , @xmath167 , @xmath116 , @xmath117 and @xmath159 in that order . to explore different potential local minima use different initial conditions and select the solution with lowest @xmath159 .",
    "since this algorithm penalizes groups with few members it turns out that , for sufficiently large @xmath27 and @xmath43 , some sample and condition groups result empty . if this is not the case @xmath27 and/or @xmath43 should be increased until at least one sample group and one variable group results empty .     between the original @xmath168 and estimated @xmath169 groups assignments ,",
    "relative to its maximum value @xmath170 when @xmath171 .",
    "the original data was made of @xmath172 samples divided in @xmath27 groups and @xmath173 conditions divided in @xmath43 groups .",
    "the values of @xmath122 were extracted from a normal distribution with mean @xmath174 and variance @xmath66 .",
    "the figure shows the mutual information between the original groups and the group assignment , estimated by the vb-1 algorithm , as a function of the variance @xmath66 .",
    "the dashed - dotted , solid and dashed lines corresponds with the worst , average and best case on 100 test examples , respectively . in a ) @xmath175 and in b ) @xmath176 . in both cases",
    "the mutual information is approximately equal to its maximum @xmath170 for values of @xmath66 less than one , the minimum difference between the original means @xmath129.,width=307 ]      to test the performance of the vb-1 algorithm , ( [ preal])-([f_real ] ) , we consider test examples generated by the likelihood ( [ preal ] ) itself . our aim is to test the variational result in the context of a relatively small number of samples and conditions . to quantify the goodness of the group assignment we consider the mutual information between the original @xmath168 ( @xmath177 ) and estimated @xmath169 sample group assignments ,    @xmath178    where    @xmath179    @xmath180    @xmath181    note that @xmath182 takes its maximum value when @xmath171 , denoted by @xmath183 .",
    "off course , the same could be done for the condition group assignments as well .    in our test examples",
    "the original data was made of @xmath172 samples divided in @xmath27 groups and @xmath173 conditions divided in @xmath43 groups .",
    "the values of @xmath122 were extracted from a normal distribution with mean @xmath174 and variance @xmath66 .",
    "we estimate the group assignment using the vb-1 algorithm , sampling one initial condition .",
    "figure [ fig_real ] shows the mutual information between the original and estimated groups as a function of the variance @xmath66 . in a ) @xmath175 and in b ) @xmath176 . in both cases",
    "the mutual information is approximately equal to its maximum @xmath170 for values of @xmath66 less than 1 .",
    "since 1 is the minimum difference between the original means @xmath129 , we conclude that the vb-1 algorithm performs well when there is a significant difference between the distributions associated with different groups . for larger values of @xmath66",
    "the vb-1 algorithm performance starts to decrease .",
    "this is not , however , a deficiency of the algorithm but an unavoidable consequence of the mixing between the distributions coming from different groups .",
    "it is worth noticing that we obtain similar results for the case @xmath184 and @xmath185 , indicating that the method works when there is no group structure on one side , in this case the conditions .",
    "there are several datasets consisting of a certain number of properties and the information of whether or not each sample exhibits each of the properties . for example , the dataset in fig .",
    "[ fig_hg_bg ] describes a population of three animals characterized by two attributes , hair and legs .",
    "the attribute hair can take the value yes ( has hair ) or no ( does not have hair ) while the attribute legs takes the values 2 or 4 ( at least within this dataset ) .",
    "the mathematical treatment of this problem is significantly simplified if the variables are mapped onto boolean variables . to each @xmath20",
    "states variable we associate @xmath20 boolean variables , each representing the occurrence or not of a specific letter of the alphabet .",
    "for example , the attribute hair is associated with hair - yes and hair - no and the attribute legs with legs-2 and legs-4 ( fig . [ fig_hg_bg]b ) .",
    "the outcome of this mapping is represented by the boolean matrix @xmath186 , taking the value 1 if the answer to the boolean variable @xmath48 is yes on sample @xmath31 and 0 otherwise .",
    "depending on our aim , the boolean matrix can be represented either by a hypergraph or a bipartite graph .",
    "when we aim to cluster the samples without attempting to cluster the boolean variables , @xmath186 is better interpreted as the adjacency matrix of a hypergraph .",
    "a hypergraph is an intuitive extension of the concept of graph to allow for connections between more than two elements . in our case , the hypergraph vertices represent samples and hyper - edges , one associated which each boolean variable , represent the set of all samples with the answer yes to the corresponding boolean variable ( fig .",
    "[ fig_hg_bg]c ) . on the other hand ,",
    "when we aim to cluster both the samples and boolean variables then a bipartite graph interpretation is more appropriate , with one class of vertices for the samples and another one for the boolean variables , and an edge connecting sample @xmath31 and variable @xmath48 whenever @xmath187 ( [ fig_hg_bg]d ) .",
    "the differences between these two approaches will become clear below .      in this case",
    "the samples are assumed to be divided in groups while the hypergraph edges are modeled as independent .",
    "here we follow the statistical model introduced in @xcite :    _ data : _ consider a hypergraph with a vertex set representing @xmath26 samples and @xmath42 edges characterizing the relationships among them . the hypergraph is specified by its adjacency matrix @xmath69 , where @xmath187 if element @xmath31 belongs to edge @xmath48 and it is 0 otherwise .",
    "_ likelihood : _ the adjacency matrix elements are generated by a binomial model with sample group and variable dependent probabilities @xmath188 , @xmath29 and @xmath189 , resulting in    @xmath190    _ priors : _ as priors we use the renormalized invariant prior of the binomial model ( table [ priors ] ) . taking into account that we have a binomial model for each pair of sample group and edge , we obtain    @xmath191    with @xmath192 and @xmath193 .",
    "substitute the likelihood ( [ phg ] ) , the priors ( [ ppi ] ) and ( [ p_hg ] ) , and the mf variational function ( [ mf1 ] ) into ( [ f ] ) , and integrating over @xmath1 ( summing over @xmath30 and integrating over @xmath194 and @xmath28 ) we obtain    @xmath195    minimizing ( [ f_hg ] ) with respect to @xmath131 , @xmath196 and @xmath133 we obtain ( vb-2 )    @xmath197 } } { \\sum_s e^ {   \\langle\\ln\\pi_s\\rangle + \\sum_j \\left [ a_{ij}\\langle\\ln\\theta_{sj}\\rangle + ( 1-a_{ij})\\langle\\ln(1-\\theta_{sj})\\rangle \\right ] } } \\ ] ]    @xmath198    @xmath199    @xmath200    @xmath201    @xmath202    these equations represent the vb algorithm for the statistical model on hypergraphs . in this case",
    "we have not been able to disentangle the contributions weighting the fit to the data and the model bias , both being included in the averages @xmath203 and @xmath204 .",
    "the implementation of the vb algorithm for the statistical model on hypergraphs proceeds as follows . set sufficiently large values for @xmath27 , larger than our expectation for the actual values of @xmath27 .",
    "we use @xmath205 in the following test examples . set the parameters @xmath206 , @xmath207 and @xmath154 .",
    "we set the parameters @xmath208 .",
    "set random initial conditions for @xmath116 .",
    "starting from these initial conditions iterate equations ( [ p_hg])-([f_hg_min ] ) until the solution converges up to some predefined accuracy .",
    "we use relative error of @xmath159 smaller than @xmath160 . in practice , compute @xmath209 , @xmath210 , @xmath211 , @xmath204 , @xmath164 , @xmath165 , @xmath116 and @xmath159 in that order . to explore different potential local minima use different initial conditions and select the solution with lowest @xmath159 .",
    "since this algorithm penalizes groups with few members it turns out that , for sufficiently large @xmath27 , some sample and condition groups result empty . if this is not the case then increase @xmath27 until at least one group is empty .",
    "a matlab code implementing this algorithm can be found at http://www.sns.ias.edu/  vazquez / hgc.html .",
    "consider the animal population in fig .",
    "[ fig_zoo]a together with their attributes : habitat , nutrition behavior , etc .",
    "figure [ fig_zoo]b shows the mapping of this dataset onto a hypergraph . the hypergraph vertices represent animals and the edges represent the association between all animals with a given attribute : edge 1 , all non - airborne animals ; edge 2 , all airborne animals , and so on .",
    "the animal population stratification was already addressed in @xcite , finding the solution in fig .",
    "[ fig_zoo]c .",
    "although the starting statistical model is the same , the solution in @xcite was found assuming fixed the number of groups and estimating the group assignment using the em algorithm ( essentially a maximum likelihood estimate ) .",
    "then , in an an attempt to focus in the solution with better consensus , solutions for different number of groups were obtained and the most representative solution was selected .    here",
    "we address the same problem using a bayesian approach and the variational solution .",
    "we start from the same statistical model on hypergraphs but now obtain a solution using the vb-2 algorithm ( [ p_hg])-([f_hg_min ] ) , sampling 10,000 initial conditions as in @xcite . the solution found by the vb-2 algorithm ( fig .",
    "[ fig_zoo]d ) is quite similar to that previously found in @xcite ( fig .",
    "[ fig_zoo]c ) .",
    "the main differences are the splitting of the terrestrial mammals , the exclusion of the platypus and the tortoise from the amphibia - reptiles group and the scorpion from the terrestrial arthropods .",
    "more important , in both cases the main groups represent terrestrial mammals , aquatic mammals , birds , fishes , amphibia - reptiles , terrestrial arthropods and aquatic arthropods .",
    "the vb-2 ( [ p_hg])-([f_hg ] ) algorithm represents , however , a significant improvement over the approach followed in @xcite .",
    "it finds the consensus solution in one run , because it has built in the balance between better fitting and less bias .     between the original @xmath168 and estimated @xmath169 groups assignments ,",
    "relative to its maximum value @xmath170 when @xmath171 .",
    "the original data was made of a graph with @xmath172 vertices divided in @xmath106 groups , with an intra- and inter - community connection probabilities @xmath212 and @xmath213 , respectively .",
    "the figure shows the mutual information , between the original groups and the group assignment estimated by the vb-2 algorithm ( [ p_hg])-([f_hg_min ] ) , as a function of the inter - community connectivity @xmath213 .",
    "the dashed - dotted , solid and dashed lines corresponds with the worst , average and best case on 100 test examples . in a )",
    "we deal with dense communities ( @xmath214 ) and the algorithm performs well ( @xmath215 ) for small values of the inter - community connectivity probability @xmath213 . in b ) we deal with sparse communities ( @xmath216 ) and the algorithm performs well for large values of the inter - community connectivity probability @xmath213.,width=307 ]      the work by newman and leicht @xcite provides a hint on how to apply the hypergraph clustering to the problem of finding modules or communities in a graph or network . a graph is made by a set of vertices and a set of edges , the latter being pairs of connected vertices",
    "the idea of leicht and newman is a `` guilty by association '' principle : vertices between the same module of a graph will tend to have connections to the same other vertices .",
    "this problem can be translated to a hypergraph problem , where the vertices are the graphs vertices , the hyper - edges are the set of nearest neighbors and the boolean variables characterize whether or not a vertex belongs to the a set of nearest neighbors @xcite ( fig .",
    "[ fig_hg_bg]e and f ) .",
    "more precisely , to each vertex we associate a hyper - edge , given by the set of its nearest neighbors .",
    "therefore , there are @xmath217 hyper - edges , one for every vertex in the original graph .",
    "the hypergraph adjacency matrix has the matrix element @xmath187 if vertex @xmath31 belongs to hyper - edge @xmath48 , i.e. if vertex @xmath31 belongs to the nearest - neighbor set of vertex @xmath48 , and @xmath218 otherwise .",
    "if we label the nearest - neighbor sets with the same label as the vertices then the hypergraph adjacency matrix coincides with the adjacency matrix of the original graph .",
    "thus , there is an exact mapping from the statistical model proposed by newman and leicht @xcite to the statistical model on hypergraphs .",
    "having specified this mapping we use the vb-2 algorithm ( [ p_hg])-([f_hg_min ] ) , sampling one initial condition , to find the graph modules in the original graph . to illustrate its performance we consider as a case study a graph composed by two communities , with probabilities @xmath212 and @xmath213 that two vertices within the same or different communities are connected , respectively . as already anticipated by newman and leicht @xcite",
    ", the nearest - neighbor approach can resolve both dense communities with lesser inter - community connections ( @xmath219 ) and sparse communities with more inter - community connections ( @xmath220 ) .",
    "figure [ fig_hg ] shows that the vb-2 algorithm performs quite well in those two regimes .",
    "we can face situations where there are groups of boolean variables as well , requiring the clustering of both samples and boolean variables . in this case",
    "the bipartite graph representation is more appropriate , with a class of vertices representing the samples and a class of vertices representing the boolean variables .",
    "more precisely ,    _ data : _ consider a bipartite graph with two vertex subsets , representing @xmath26 samples and @xmath42 boolean variables .",
    "the graph is specified by its adjacency matrix @xmath69 , where @xmath187 when sample @xmath31 is connected to boolean variable @xmath48 , i.e. if boolean variable @xmath48 is true for sample @xmath31 , and @xmath218 otherwise .",
    "_ likelihood : _ the adjacency matrix elements are generated by a binomial model with sample group and variable group dependent probabilities @xmath194 , @xmath29 and @xmath221 , resulting in    @xmath222    _ priors : _ for @xmath37 we use the renormalized invariant prior of the binomial model .",
    "taking into account that we have one binomial model per each pair of sample and variable group we obtain    @xmath223    with @xmath224 and @xmath225 .",
    "the likelihood ( [ pbg ] ) is quite similar to ( [ phg ] ) , the main difference being that now the statistical properties of the boolean variables appear through their corresponding group assignments @xmath46 .",
    "this increases the model complexity by considering a group structure for the boolean variables and , at the same time , reduces the number of @xmath34 parameters .",
    "furthermore , ( [ pbg ] ) contains ( [ phg ] ) as the particular case where @xmath226 and one group associated to each boolean variable .    substituting the likelihood ( [ pbg ] ) , the priors ( [ p_bg ] ) , ( [ ppi ] ) and ( [ pkappa ] ) , and the mf variational function ( [ mf2 ] ) in ( [ f ] ) , and integrating over @xmath1 ( summing over @xmath30 and @xmath46 and integrating over @xmath194 , @xmath28 and @xmath44 ) we obtain    @xmath227    minimizing ( [ f_bg ] ) with respect to @xmath131 , @xmath117 , @xmath196 , @xmath133 and @xmath134 we obtain ( vb-3 )    @xmath228 } } { \\sum_s e^ { \\langle\\pi_s\\rangle + \\sum_{jl } q_{jl } \\left [ a_{ij}\\langle\\ln\\theta_{sl}\\rangle + ( 1-a_{ij})\\langle\\ln(1-\\theta_{sl})\\rangle \\right ] } } \\ ] ]    @xmath229 } } { \\sum_s e^ { \\langle\\kappa_s\\rangle + \\sum_{ik } p_{ik } \\left [ a_{ij}\\langle\\ln\\theta_{ks}\\rangle + ( 1-a_{ij})\\langle\\ln(1-\\theta_{ks})\\rangle \\right ] } } \\ ] ]    @xmath230    @xmath231    @xmath232    @xmath233    @xmath234    @xmath235    equations ( [ p_bg])-([f_bg_min ] ) represent the vb algorithm for the statistical model on bipartite graphs .",
    "they can be used to found modules or communities in graphs with a bipartite structure , including those representing samples and boolean variables .",
    "the implementation of the vb-2 algorithm ( [ p_bg])-([f_bg_min ] ) for the statistical model on bipartite graphs proceeds as follows .",
    "set sufficiently large values for @xmath27 and @xmath43 , larger than our expectation for the actual values of @xmath27 and @xmath43 .",
    "set the parameters @xmath236 , @xmath237 , @xmath154 and @xmath155 .",
    "we set the parameters @xmath238 .",
    "set random initial conditions for @xmath116 and @xmath117 .",
    "starting from these initial conditions iterate equations ( [ p_bg])-([f_bg_min ] ) until the solution converges up to some predefined accuracy .",
    "we use relative error of @xmath159 smaller than @xmath160 . in practice , compute @xmath209 , @xmath210 , @xmath211 , @xmath204 , @xmath164 , @xmath165 , @xmath166 , @xmath167 , @xmath116 , @xmath117 and @xmath159 in that order . to explore different potential local minima use different initial conditions and select the solution with lowest @xmath159 . since this algorithm penalizes groups with few members it turns out that , for sufficiently large @xmath27 and @xmath43 some sample and/or variable groups result empty .",
    "if this is not the case , increase @xmath27 and/or @xmath43 until at least one sample group and one variable group results empty .",
    "let us go back to the zoo problem ( fig .",
    "[ fig_zoo]a ) .",
    "now we represent this dataset by a bipartitite graph , with one class of vertices representing the animals and the other class the boolean variables ( e.g. fig .",
    "[ fig_hg_bg]a , b and d ) using the vb-3 algorithm ( [ p_bg])-([f_bg_min ] ) , sampling 10,000 initial conditions as in @xcite , we perform a two - sides clustering of the bipartite graph obtaining the animal population stratification in fig .",
    "[ fig_zoo]e and the boolean variables stratification in fig .",
    "[ fig_zoo]f .",
    "the animal clusters are similar to those previously obtained using the statistical model on hypergraphs ( fig .",
    "[ fig_zoo]c and d ) .",
    "the main difference is the more refined subdivision of terrestrial mammals , now split in four groups ( 1 , 2 , 3 and 4 ) .",
    "in addition to the animal population stratification the two - sides clustering provides association groups between the boolean variables ( fig . [",
    "fig_zoo]f ) .",
    "these associations reflect the fact that not all boolean variables are independent , some of them are linked .",
    "for example , group 2 cluster four typical attributes of terrestrial mammals , they have hair , do not put eggs , milk and have four legs . in the same way , group 3 clusters attributes of fishes and group four of birds .",
    "thus , in general , the bipartite graph model and the resulting two - sides clustering provides more information than the hypergraph approach .     between the original @xmath168 and estimated @xmath169 groups assignments , relative to its maximum value @xmath170 when @xmath171 .",
    "the original data was made of a graph with @xmath172 vertices divided in @xmath106 groups , with an intra and inter - community connection probabilities @xmath212 and @xmath213 , respectively .",
    "the figure shows the mutual information , between the original groups and the group assignment estimate by the vem-3 algorithm ( [ p_bg])-([f_bg_min ] ) , as a function of the inter - community connectivity @xmath213 .",
    "the dashed - dotted , solid and dashed lines corresponds with the worst , average and best case on 100 test examples . in a )",
    "we deal with dense communities ( @xmath214 ) and the algorithm performs well ( @xmath215 ) for small values of the inter - community connectivity probability @xmath213 . in b ) we deal with sparse communities ( @xmath216 ) and the algorithm performs well for large values of the inter - community connectivity probability @xmath213.,width=307 ]      the bipartite graph model can be use to find network modules as well . in this case",
    "one class of vertices represents the original graph vertices and the other represents sets of nearest neighbors ( fig .",
    "[ fig_hg_bg]g ) .",
    "the two - sides clustering thus attempts to cluster both the original graph vertices and the sets of nearest neighbors . when the original graph is undirected the problem is symmetric ( e.g. see fig . [ fig_hg_bg]g ) . indeed , if vertex @xmath31 belongs to the nearest - neighbor set of vertex @xmath48 then vertex @xmath48 belongs to the nearest - neighbor set of vertex @xmath31 . as a consequence",
    "the clustering on the original vertices side can not be differentiated from the clustering of nearest - neighbor sets .",
    "intuitively this means that when two vertices belong to the same graph module we can say that their nearest - neighbor sets belong to the same nearest - neighbor set group .    having specified this mapping we use the vb-3 algorithm ( [ p_bg])-([f_bg_min ] ) , sampling one initial condition , to find the graph modules in the original graph . to illustrate its performance we consider once again a graph composed by two communities , with probabilities @xmath212 and @xmath213 that two vertices within the same or different communities are connected , respectively .",
    "figure [ fig_bg ] shows that the vb-3 algorithm can resolve both dense communities with lesser inter - community connections ( @xmath219 ) and sparse communities with more inter - community connections ( @xmath220 ) .    the comparison of fig .",
    "[ fig_bg ] and [ fig_hg ] indicates that the bipartite graph model performs slightly better than the hypergraph model .",
    "for example , focusing on the average performance , for @xmath214 the vb-3 algorithm performs almost perfectly till @xmath239 , while the vb-2 algorithm does till @xmath240 .",
    "this could be , however , specific to the tested set of examples .",
    "further research is required to determine which version performs better depending on the dataset under consideration .",
    "the bayesian approach allows for a systematic solution of data analysis problems .",
    "its starting point is a statistical model of the data under consideration . from there , using bayes rule , we can invert the statistical model to obtain the posterior distribution of the model parameters .",
    "the latter can be use , in principle , to calculate or compute averages or other magnitudes of interest .",
    "one of the main criticisms to the bayesian approach is the apparent ambiguity in selecting the prior distributions .",
    "here we have worked further on jaynes method @xcite , claiming that the prior distributions are given by the most general distribution dictated by the symmetries of the problem under consideration .",
    "one undesired consequence of this method is that when the symmetries are not sufficient constraints we obtain improper prior distributions .",
    "yet , the use of improper priors can be avoided by working with renormalized distributions that are proper , and approach the improper prior in a certain limit . using this approach",
    "we report here a correction to jaynes prior for a likelihood with translation and scale invariance and a generalization of jaynes prior for the binomial model to the multinomial model .",
    "having resolve the issue about the prior distributions , we can proceed to the application of the bayesian approach to resolve a population structured . taking inspiration from mixture models",
    "@xcite , in particular dirichlet mixture models @xcite , we introduce general statistical models with a built in population structure at the sample , and sample and variable , level . the model with a structure at the sample level",
    "aims one - side clustering problems , where the variables are assumed to be independent measurements .",
    "the model with a structure at both sample and variable level aims two - side clustering problems , where there are classes of variables .",
    "these statistical models are then postulated as generative models of some dataset . introducing a mf approximation as variational function",
    ", we then resolve the population structure by solving the inverse problem , i.e. determining the sample and/or variable groups and model parameters from the data .",
    "to illustrate the applicability and systematicity of the variational method , here we study the problem of data clustering , in the context of real value and boolean variables .",
    "the outcome is a variational bayes ( vb ) algorithm , a self - consistent set of equations to determine the group assignments and the model parameters .",
    "the vb algorithm is based on recursive equations similar to those for the em algorithm , but with some intrinsic penalization for model bias . in the case of real value data , and under the assumption of normal distributions ,",
    "the contributions favoring fitting and penalizing model bias are clearly disentangled .",
    "the fitting is quantified , as it is expected for normally distributed variables , by the mean square deviation . the model bias is quantified by the inverse of the square root of the mean cluster sizes .",
    "the tendency to reduce the mean square deviation is thus balanced by a tendency to increase the cluster sizes .    in the case of boolean variables",
    "our analysis is based on a mapping into a hypergraph or bipartite graph .",
    "when we cluster the samples but not the boolean variables the problem is mapped onto a statistical model on hypergraphs @xcite . on the other hand ,",
    "when we perform a two - side clustering , clustering both the samples and the boolean variables , the problem is mapped onto a statistical model on bipartite graphs .",
    "the vb algorithms associated with the statistical model on hypergraphs and bipartite graphs can be used to find modules on a graph .",
    "starting on an idea by newman and leicht @xcite , we show that the problem of graph modules can be mapped onto the problem of finding hypergraph modules or bipartite graph modules , where the hypergraph edges and the augmented bipartite graph vertices represent nearest - neighbor sets in the original graph .",
    "the resulting vb algorithms represent a significant improvement over the maximum likelihood approaches followed in @xcite and @xcite , by including a self - consistent correction for model complexity and bias .",
    "it is worth mentioning that , depending on the starting statistical model , we could arrive to different versions of the vb algorithm . indeed , for the finding graph modules problem we could use both the hypergraph and bipartite graph models .",
    "furthermore , hofman and wiggins @xcite have obtained another version based on a statistical model with different intra and inter - community connection probabilities .",
    "these approaches differ in the definition of what constitutes a group , community or module .",
    "we use the definition by newman and leicht @xcite based on topological similarity , i.e. two vertixes are topologically identical if they are connected to the same other vertices in the graph .",
    "thus , we obtain group of vertices whose patterns of connectivity are similar . on the other hand ,",
    "the definition used by hofman and wiggins @xcite is based on the existence of two edge densities , characterizing the tendency of having an edge between intra- and inter - group pairs of vertices .",
    "depending on the problem and the question we are asking we may adopt one or the other definition , and use the corresponding clustering method ."
  ],
  "abstract_text": [
    "<S> data clustering , including problems such as finding network communities , can be put into a systematic framework by means of a bayesian approach . </S>",
    "<S> the application of bayesian approaches to real problems can be , however , quite challenging . in most cases </S>",
    "<S> the solution is explored via monte carlo sampling or variational methods . </S>",
    "<S> here we work further on the application of variational methods to clustering problems . </S>",
    "<S> we introduce generative models based on a hidden group structure and prior distributions . </S>",
    "<S> we extend previous attends by jaynes , and derive the prior distributions based on symmetry arguments . as a case study </S>",
    "<S> we address the problems of two - sides clustering real value data and clustering data represented by a hypergraph or bipartite graph . from the variational calculations , and depending on the starting statistical model for the data , we derive a variational bayes algorithm , a generalized version of the expectation maximization algorithm with a built in penalization for model complexity or bias . </S>",
    "<S> we demonstrate the good performance of the variational bayes algorithm using test examples . </S>"
  ]
}