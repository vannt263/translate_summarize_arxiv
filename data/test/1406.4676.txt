{
  "article_text": [
    "classification is an important task in many fields including biomedical research , engineering , sociology and many others .",
    "how to construct a classification rule based on a labeled data set is a classical statistical problem . in machine",
    "learning literature , there are several types of learning problems discussed , and depending on how labeled subjects are included into a learning process , they are usually termed as supervised , unsupervised or semi - supervised learning @xcite . recently , due to technical innovation , `` big data '' becomes a buzz phrase in many fields , and we now often encounter with data sets that have huge amount of unlabeled data . hence , how to utilize these unlabeled data efficiently to construct a classification rule becomes an important problem . because to label each unlabeled subject is usually costly and inefficient , a common approach is active learning ( see , for example , * ? ? ?",
    "* ; * ? ? ?",
    ". this type of a leaning process will only inquire the label information for the `` selected '' subjects , which are usually chosen based on the information learned in the previous learning stages , and then include the newly labeled subjects into its training stage .",
    "a learning process will usually go on until a prefixed criterion is reached , such as a prefixed total number of labeled subjects to be used in the training stage .",
    "moreover , because in an active learning process , subjects are dynamically and sequentially selected , labeled and then added to the training set , this process is naturally related to sequential experimental designs in statistics , where a new observation / experiment is conducted at some particular design points selected according to the information obtained using the data gathered up to current stage .",
    "since data are observed adaptively , this type of methods are also related to the stochastic approximation process , which was first discussed in @xcite .",
    "their original procedure is called robbins - monro ( rm ) procedure and can be viewed as a stochastic version of newton - raphson method for nonlinear root - finding problems . following @xcite ,",
    "sequential design methods have been intensively studied , and there are even more papers discussed different modifications of rm procedure and their corresponding convergence rates .",
    "recently , @xcite further modified rm procedure to improve on its efficiency .",
    "this type of procedures is nonparametric in the sense that no parametric model assumption is presumed .",
    "however , rm procedure can also be derived from a parametric form .",
    "for example , using the maximum likelihood estimate ( mle ) of a logistic model , @xcite proposed a logit - mle method for binary data that uses the currently available labeled data to fit a logistic model , and then select the next input with the desired probability based on the fitted logistic model .",
    "because a classification rule construction under active learning framework can be formulated as a problem of estimating the threshold boundary between two groups , which can usually be defined using a probability quantile , it can also be viewed as a stochastic root - finding procedure described above .",
    "moreover , logistic models are commonly used models in binary classification problems , and the properties of sequential estimation for generalized linear model ( glm ) under general adaptive designs are well studied @xcite .",
    "hence , it is natural to construct a binary classification rule , sequentially and adaptively , by putting all these ingredients together . an active learning algorithm developed in @xcite , which combines",
    "the logic - mle of @xcite and d - optimal design is a successful example .",
    "although , the existence and uniqueness of mle can be achieved after quite a few initial observations @xcite , it may still suffer from severe bias , when sample size is small , which usually results in an inefficient learning process .",
    "in modern literature , @xcite developed a bayesian extension of wu s approach , where they used the maximum a posterior ( map ) estimates of the parameters of a logistic model rather than mles .",
    "@xcite suggested a new sequential experimental design for glm , where observations are selected sequentially based on a bayesian d - optimality criterion and bayesian estimates of model parameters .",
    "these methods motivate us to study a novel modification of @xcite .    as in conventional regression analysis",
    ", it is well - known that when the number of dimensionality of the unknown vector of parameters becomes large , the estimated information of it will be very unstable .",
    "because active learning processes usually rely such kind of information , the unstable estimates of parameters will also affect the learning process .",
    "in the real example studied in @xcite , those two variables are selected based on experts opinions .",
    "however , this situation is rare and there are usually more variables considered for a real example .",
    "thus , how to stabilize a learning process in high dimensional case is difficult and important . in this paper",
    ", we focus on the higher dimensional data sets .",
    "a bayesian sequential design is used and the related computational issues are discussed .",
    "in addition , for practical usages , we also study the effects of using different sizes of labeled data sets as an initial training set of an active learning process .",
    "as to the subject selection during a process , the major difference between a sequential design and an active learning process is that with sequential design , an experiment will be conducted at the selected points , while in active learning processes with existent unlabeled data , we can only select points near the theoretical ones from an existent data set . hence , how to select the next point based on the available information plays a key role in an active learning process . @xcite aimed at shortening the distance between the estimated boundary and the true one , such that their subject selection scheme heavily depends on the initial model assumption . in practice ,",
    "the form of true model is usually unknown .",
    "hence , in order to diminish the effect of model assumptions , we adopt a different design point selection scheme .",
    "the advantage of the proposed method will be discussed from both theoretical and practical aspects .",
    "the rest of this paper is organized as follows . in section [",
    "sec : method ] , we first review the active learning algorithm @xcite , and then discuss the proposed algorithm and some modifications . simulation results and numerical studies with real data sets are presented in sections [ sec : sim ] and [ sec : real ] , respectively .",
    "section [ sec : discu ] is a summarization .",
    "technical details are given in appendix .",
    "let @xmath0 be the explanatory vector of subjects and variable @xmath1 or @xmath2 denotes the category a subject belonging to .",
    "suppose that @xmath3 be the probability model of @xmath1 given @xmath4 .",
    "assume further that each variable has a positive relationship with the response ; that is , for larger value of @xmath5 , the higher the probability of @xmath6 .",
    "then @xcite assumed that @xmath7 had a parametric form @xmath8 where @xmath9 , @xmath10 for each @xmath11 , and @xmath12 .",
    "let @xmath13 be a vector of @xmath14 parameters . then following ( [ model-1 ] ) , for a given @xmath15",
    ", @xmath16 is a bernoulli random variable with mean @xmath17 .",
    "model ( [ model-1 ] ) can be re - written as a conventional logistic regression model : @xmath18 where @xmath19 and @xmath20 .",
    "the fisher information matrix of @xmath21 with a set of design points @xmath22 is @xmath23 where @xmath24 is the regression matrix with @xmath11th row , @xmath25 equal to @xmath26 and @xmath27 is a diagonal matrix with @xmath28 $ ] , @xmath29 .",
    "it is clear that this information matrix is non - linear in @xmath21 and depends on the unknown @xmath21 only through @xmath27 .",
    "suppose that @xmath30 are observed labeled data of size @xmath31 .",
    "using this training set , we obtain an using the current estimates of parameters , the classification rule based on the estimate of @xmath32 becomes @xmath33 with an estimated boundary @xmath34 where @xmath35 when there is no extra information , such as @xmath36 available .",
    "( in general , the cutting point for a logistic classification function is 0.5 .",
    "however , when there is a prior information about the event , such as prevalence rate in epidemiology study , the cutting point will usually be adjusted accordingly .",
    "this will be discussed later . )",
    "therefore , the active learning problem under this set up becomes how to recruit a set of training subjects efficiently such that when a learning process is stopped , the final classification function @xmath37 will have good prediction power .      intuitively , in order to have an efficient learning process , we should learn the most uncertain subjects first , because to do it this way may most improve a classifier .",
    "thus , when using a probabilistic learning model in an active learning framework , the most commonly used query for getting new data is the uncertainty sampling @xcite , where an active learner will query the label information of instance whose class membership is least certain . for a binary classification problem",
    ", this simply means to query the instance whose membership probability is closest to @xmath38 @xcite .",
    "thus , in a binary classification case the uncertainty is usually measured by @xmath39 where @xmath40 .",
    "( note that in @xcite , they used only one parameter for measuring the uncertainty and adjusting the cutting point , and said that this parameter can be data dependent .",
    "however , our numerical studies show that for our method , using two different parameters for measuring uncertainty and adjusting cutting point , separately , will usually perform better . regarding this phenomenon , more discussions , based on statistical decision theory viewpoints ,",
    "are given in section [ sec : uneven ] . )",
    "let @xmath41 be the unlabeled data set .",
    "then rank points in @xmath41 in ascending order based on ( [ model-6 ] ) , and an active learning procedure will choose the top ranked point as follows : @xmath42 that is , to choose the one with an estimated probability closest to 0.5 as the next point to be labeled . because in high dimensional cases , there may be a lot of points that have the same or similar @xmath43 , we choose top @xmath44 points as candidates first , where @xmath44 , in our method , is decided by a local d - efficiency method using a locally optimal design discussed in @xcite and @xcite .",
    "( for the details of this method , please refer to their original papers . ) as mentioned in @xcite , to use ( [ model-7 ] ) as the only criterion can not provide good estimates of model parameters , and the method of optimal design can be a good supplement to this disadvantage .",
    "thus , let @xmath45 be the set of candidate points that are screen out using ( [ model-7 ] ) .",
    "we then access these candidates further with some optimal experimental design criterions .",
    "one of the major differences between our method and the one in @xcite is that we use an uncertainty sampling method instead of distance based scheme to select the candidate set .",
    "the effect of uncertainty sampling becomes obvious when the difference between the sample sizes of two groups is large .",
    "this situation happens very often in those problems that aim for detecting a set of rare subjects within a large data set , or when the population sizes of two groups are uneven .",
    "when the true model is exactly linear and the variables for this model are completely known , these two methods are the same . however , in practice , the form of the true model and the variables involved in it are usually unknown .",
    "for instance , the example discussed in @xcite , those two variables used in their model are selected from a large number of variables by experts , and in fact the true model may involve other variables .",
    "when a model is an approximation with some leftover random errors , then the candidate set defined by a euclidean distance - based method will be very different from the one obtained using a uncertainty measure .",
    "this situation can be easily illustrated using figure [ fig : contour ] , where figure [ fig : contour](a ) is the probability contour plot when the true model is linear , and figure [ fig : contour](b ) is a contour plot of the probabilities for the same linear model plus a small nonlinear error term .",
    "that is , when some perturbation exists , the contour lines can no longer be parallel .",
    "thus , to use a perpendicular distance to find a candidate set , as that in @xcite can not be the best choice .",
    "that is the reason why we use an uncertainty sampling scheme to define a candidate set first , then use a ( bayesian ) d - optimal design method to screen out the best subject for parameter estimation .",
    "moreover , when the number of dimensionality becomes larger , the computation of the determinant of a fisher information matrix is difficult ; especially when the size of labeled data is small , the information matrix will be either singular or nearly singular , which provides less information for designs .",
    "thus , we adopt a bayesian d - optimal design instead , which will stabilize the beginning stages of a learning process .",
    "[ [ computation - of - fisher - information - matrix ] ] computation of fisher information matrix + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    it is known that an active learning is a sequential process , each learning stage heavily relies on the information obtained from its predecessors .",
    "naturally , an unstable initial stage will make the process inefficient and even result in a bias classification rule .",
    "hence , in order to have a stable learning process , especially in higher dimensional data cases , we adopt a bayesian d - optimal design instead ( see , for example , * ? ? ?",
    "* ; * ? ? ?",
    "* ) , which is an extension of the original d - optimality by replacing the determinant of fisher information @xmath46 with @xmath47 where @xmath48 denotes the prior distribution for @xmath49 , and the expectation @xmath50 is with respect to this prior distribution . because to compute the integration in ( [ model-8 ] ) is not trivial , especially when the dimension of @xmath49 is high .",
    "this time - consuming step has to be repeated at each stage in an active learning process , hence any simplification of it will be beneficial .",
    "for this purpose , instead of the exact value of @xmath51 , @xcite proposed using an approximation to ( [ model-8 ] ) below : @xmath52 where @xmath53 are weights obtained with a monte - carlo method ( see remark [ re : weight ] ) .",
    "a new subject in the candidate set @xmath54 that maximizes @xmath55 is selected .",
    "[ re : weight ] the weights @xmath53 s are computed using simple monte - carlo method ( see * ? ? ?",
    "we first generate a large number of points , say m , from the prior @xmath48 , and denote them as @xmath56 .",
    "let @xmath57 be large enough to represent the prior distribution .",
    "then for a vector @xmath58 , @xmath59 , and observations @xmath60 taken at @xmath61 , the likelihood is @xmath62^{y_i } \\left[\\frac{1}{1+\\exp(\\tilde{\\mathbf{x}}^{t}_{i}\\boldsymbol{\\beta}_u ) } \\right]^{1-y_i}.\\ ] ] normalizing the likelihood across the samples , we have weights @xmath63 .",
    "hence , at each stage of the experiment , the likelihood for @xmath64 can be rapidly computed .    in @xcite , they used a local d - optimality criterion to access the unlabeled data in their candidate set ( with a prefixed number ) and select a subject that maximizes the determinant of the fisher information matrix for @xmath65 , @xmath66 .",
    "this new subject will then be labeled by experts and included to the learning process .",
    "it is clear that the determinant @xmath46 is numerically unstable when the number of design points is small and the number of dimensionality of @xmath67 is large . when the information matrix is singular or even just nearly singular , it is hard to provide useful information for selecting next design points .",
    "thus , a bayesian d - optimal design is a good alternative .",
    "let @xmath68 be labeled data points at the initial stage .",
    "then the proposed algorithm consists of the following steps :    * compute @xmath69  the posterior estimate of @xmath70 with the currently available labeled data ( when @xmath71 , we will use the prior median instead ) ; * rank the unlabeled data points in @xmath41 based on equation ( [ model-6 ] ) .",
    "if the estimated posterior probabilities for all points are equal to either 0 or 1 , then stop iteration and use current estimated @xmath72 as the final classifier ; otherwise , go to s3 ; * create the candidate set @xmath54 with the top @xmath44 points based on the ranks in s2 , where @xmath44 is determined based on the local d - efficiency ( see * ? ? ?",
    "* ; * ? ? ?",
    "* ) ; * select a new unlabeled point from the set @xmath54 according to following criteria : * * if the design points up to current stage form a nonsingular information matrix of @xmath21 , then choose the next point that maximizes @xmath73 in ( [ model-9 ] ) ; that is , @xmath74 * * if the information matrix is singular , then select the next point from @xmath75 that maximizes @xmath76 based on the cumulated @xmath31 points , @xmath44-augmentation and the candidate point .",
    "@xmath77    we consider the case with @xmath78 , so a dirichlet distribution is a reasonable prior for @xmath79 .",
    "hence , the following priors are used : @xmath80 assume that @xmath81 , @xmath82 and @xmath83 are mutually independent , then the posterior distribution of @xmath84 , based on the labeled data points @xmath30 is @xmath85 where @xmath86 , @xmath87 and @xmath88 .",
    "then , the map is @xmath89    note that a modified bayesian d - optimal design in s3 is only used to determine @xmath90 ( see * ? ? ?",
    "* ) , and the reason to use it is because of its computational efficiency . in s4 , a more precise criterion @xmath91",
    "is used to evaluate the candidates found in the previous step .    at early stages , because only few labeled data points are available , the information matrix may be singular , and this is one of the reasons why s4 ( ii ) is adopted , which is similar to the method used in @xcite .",
    "in addition , at the early stage the estimates probabilities of whole unlabeled data points may be close to @xmath92 or @xmath93 due to the unstable coefficient estimate .",
    "it implies that the corresponding uncertainty measure provides little information .",
    "when this situation happens , we will use the distance - based measurement as in @xcite instead until the coefficient estimate becomes stable .",
    "in this section , the performance of the proposed method is evaluated through simulation , and compare with that of @xcite with two variables @xmath94 .",
    "( for short , we will refer to their method as adsl in the rest of this paper . )",
    "we evaluate the performances of two methods with the same misclassification error formulae used in @xcite , which can be estimated by @xmath95/n $ ] , where @xmath96 is the total number of data points , @xmath97 and @xmath98 are the numbers of the false - positive and false - negative subjects , respectively .",
    "( note that in @xcite , they only have one parameter , @xmath99 , in their paper . that is , they have @xmath100 all the time , and let @xmath101 when event probability , @xmath36 , is not available .",
    "they also suggested that the parameter @xmath99 should be adjusted when there is information about the event probability .",
    "note that since they have only one parameter @xmath99 , to adjust @xmath99 means to adjust both uncertainty measure and cutting threshold . )",
    "we also assess the closeness between the estimated boundaries and the true boundary based on the distance - based measurement used in @xcite , which is defined as follows : let @xmath102 where @xmath103 is a set of points that lie evenly on the true boundary , ranging from -3 to 3 on the coordinate of @xmath104 , and @xmath105 is the distance of @xmath106 to the estimated boundary for @xmath107 . using ( [ model-19 ] ) , a distance - based performance measure is @xmath108 where @xmath57 is the number of simulations , and @xmath109 is the distance defined in ( [ model-19 ] ) for the @xmath110-th simulation .",
    "we first compare the propose method with alsd using a two - dimensional data set with following steps :    * \\(1 ) data generation : we generate simulation data from model ( [ model-1 ] ) with parameters @xmath111 , @xmath112 and @xmath113 .",
    "let @xmath114 , @xmath115 and @xmath116 , where @xmath117 .",
    "we then uniformly generate @xmath118 from each interval @xmath119 $ ] , which are referred to as @xmath104 .",
    "the variable @xmath120 is then calculated according to @xmath121 .",
    "using @xmath122 and @xmath123 , we then generate the response @xmath124 with probability @xmath125 based on the specified logistic model .",
    "* \\(2 ) priors : the priors for @xmath81 , @xmath82 and @xmath126 are described below .",
    "first , consider the prior for @xmath126 .",
    "assume that the mean of @xmath126 to be @xmath38 , we set @xmath127 , which implies @xmath128 . to get a flat prior , we take @xmath129 .",
    "we then consider the priors for @xmath81 and @xmath82 .",
    "based on the lowest and highest value of @xmath130 ( denoted them as @xmath131 and @xmath132 ) and using formula @xmath133 , we choose two extreme points , @xmath134 and @xmath135 .",
    "let @xmath136 and @xmath137 be the suspicious levels for @xmath134 and @xmath135 , respectively . plugging these values into ( [ model-1 ] ) , we have @xmath138 solving the equations above",
    ", we obtain @xmath139 and @xmath140 below : @xmath141 take @xmath142 as the sample variance of @xmath143 , @xmath144 , where @xmath145 .",
    "then we complete the prior specification for all three parameters . *",
    "\\(3 ) for each method ,",
    "we select points @xmath146 sequentially among the total @xmath147 points . based on the current labeled points ,",
    "we estimate the classification function and calculate the misclassification error and the distance by equation ( [ model-19 ] ) for both methods . *",
    "\\(4 ) repeat the process @xmath148 times .",
    "the final results are based on the average of 100 runs .    according to the previous design , an example set of @xmath149 simulated data points",
    "is illustrated in figure [ fig : toy - data ] , where circle and square denote two different groups . from this figure",
    ", we can see that the two labeled points are mixed together , and the range of @xmath104 remains in @xmath150 .",
    "the responses with @xmath6 are observed mostly when two explanatory variables are both large",
    ".     represents points with response @xmath2 and circle symbol @xmath151 denotes points with response @xmath1.,scaledwidth=60.0% ]      based on 100 runs , curves of the misclassification error and distance - based measure are shown in figure [ fig : sim ] ( a ) and ( b ) , respectively .",
    "the misclassification errors of the proposed method are slightly smaller than those of alsd starting from around @xmath152 .",
    "the estimated boundary of the proposed method also moves towards the true boundary faster than that of alsd .",
    "note that the number of candidate set in alsd is @xmath153 , and fixed for all stages . because @xmath154 in our simulation , the number of candidate set for the proposed method is @xmath155 , which vary according to the criterion of the local efficiency method mentioned before and is smaller than @xmath156 .",
    "hence , we only have to access less candidate points and is usually computational more efficient .",
    "@xcite did not discuss the effects of using more than one labeled subject as an initial training set .",
    "however , because active learning algorithms are sequential procedures , the performance of the current stage relies on the information obtained from its predecessors .",
    "hence , how to have a good and stable early performance in stages will play an important role in a successful active learning process . an easy way to have a good start is to have more labeled samples in its initial stage .",
    "thus , to see the effects of different initial training sizes , we generate a data set with @xmath157 data points , and compare the results of the proposed method with @xmath158 equal to @xmath159 to that of alsd with @xmath71 .",
    "figure [ fig : sim - n0 ] shows misclassification curves of both the proposed methods with @xmath160 , respectively , and alsd with @xmath71 . as expected the proposed method with @xmath161 performs better than alsd , and the larger the initial training size @xmath158 , the better performance of the proposed method . in figure",
    "[ fig : sim - n0 ] ( d ) , for example , shows that at around @xmath162 labeled samples , the proposed method can achieve the same classification performance of alsd at @xmath163 . because the computation of active learning is time consuming process at each stage , and this is especially the case in problems with high dimensional data .",
    "thus , a method requires less learning stages can help to save the computational time .",
    "hence , in order to ensure a stable and efficient learning process , it is recommended to start with a small amount of labeled data , if they can be available .",
    "in fact , for some cases , we actually require less total number of the labeled subjects to achieve the same performance of alsd . this situation can be seen from some real data examples and will be discussed later .",
    "for illustration and comparison purposes , we apply both the proposed method and alsd to liver disorders ( bupa ) and wisconsin diagnostic breast cancer ( wdbc ) data sets , which are available at the uci repository of machine learning databases @xcite .",
    "our main interest is correct classification rate , so we use the same misclassification error formulae defined before to evaluate their performances .",
    "the original bupa data set is from the california state , usa , which contains 345 records ( 145 liver patient and 200 non - liver patient records ) with 6 attributes as shown in table [ tab : bupa ] .",
    "the first 5 variables are from blood tests and sensitive to liver disorders , which might be due to excessive alcohol consumption .",
    "all features are positive related to the response in a general sense .",
    "that is , the higher the value of variables , the higher the probability that the corresponding subject is liver disordered",
    ". the performances of the two methods ( the proposed one and alsd ) in terms of misclassification error are illustrated in figure [ fig : bupa - n0 ] .",
    "our method performs similarly to alsd when @xmath71 .",
    "however , in the proposed method , because @xmath164 , we only need to evaluate @xmath165 candidates at each stage which is smaller than the number of candidates ( @xmath166 ) used in alsd .",
    "that is , we only have to access a smaller number of candidates , which will save us a lot of computational time .",
    ".attribute information for bupa data set . [ cols=\"<,<,<\",options=\"header \" , ]     as in the previous section , we also start with different @xmath167 as an initial data set .",
    "the total size of bupa is @xmath168 ( about @xmath169 ) , and we set @xmath170 , and @xmath171 .",
    "figure [ fig : bupa - n0 ] shows that our method performs better than the alsd as @xmath172 gets larger , and the difference of two curves increases as @xmath172 increases .",
    "it is worth to note that even with @xmath173 at around 130 training samples , the proposed method can achieve the same classification performance of alsd at 150 labeled data points .",
    "that is , it saves about 10 labeled samples in total in this case .",
    "similar situations can be found in the cases with other @xmath172 s .",
    "for example , with @xmath174 , the proposed method requires only , in average , 110 labeled subjects to achieve the performance of adsl with 150 labeled samples .",
    "because it is a sequential process , it implies that the proposed method requires less training stages to achieve the same performance level , and therefore is more efficient in terms of training time . in practice",
    ", there will be some cost for experts to label subjects .",
    "thus , to save labeled samples is not only to save learning time , but also the budget of a learning process .",
    "+      the wdbc data set contains @xmath175 breast masses with @xmath176 benign and @xmath177 malignant cases .",
    "ten different features are measured including radius , perimeter , area , compactness , smoothness , concavity , concave points , symmetry , fractal dimension and texture .",
    "all features are numerically modeled such that larger values are typically indicated a higher likelihood of malignancy ( see * ? ? ?",
    "the details can also be found in @xcite , and @xcite .",
    "the mean value , extreme ( largest or `` worst '' ) value and standard error of each feature are computed for each image , which resulted in a total of 30 features of 569 images , and yielded a database of @xmath178 samples .",
    "+    we apply both the proposed method and alsd to wdbc data set , and their misclassification error curves are shown in figure [ fig : wdbc - n0 ] with different @xmath179 and @xmath180 for the proposed method and @xmath71 for alsd .",
    "when the initial training sample size @xmath172 increases , the proposed method outperforms alsd as expected .",
    "it is worth to note that the misclassification errors for processes starting with a small amount of labeled data ( @xmath181 ) are smaller than that of alsd from the very beginning .",
    "it also shows that with a small amount of initial training subjects the proposed method achieves the same classification performance sooner than alsd at a stage with less labeled samples .",
    "for example , in [ fig : wdbc - n0 ] ( d ) , with around 75 to 90 labeled samples , the proposed method achieves a misclassification error that is similar to that of alsd with 150 labeled samples .",
    "hence , even using 45 labeled subjects at the initial stage , we still save about 15 to 30 subjects .",
    "thus , to start with a small amount of labeled samples as an initial training set will actually be more efficient in both cost and computational time .",
    "[ [ synthesized - data-1 ] ] synthesized data + + + + + + + + + + + + + + + +    when either the ratio of two group size or the odds ratio of two groups is extreme , then the classification rule should take this information into consideration .",
    "@xcite suggested using @xmath182 and adjusted @xmath183 based on the probability of a case if there is a prior information available .",
    "( note that in @xcite , they used the same number , denoted as @xmath99 , in both uncertainty measurement and event probability adjustment . ) in this section , we conduct some numerical studies with uneven group sizes .",
    "the results in figure [ fig : uneven ] are based on simulated data with size ratio equals to 1 to 4 .",
    "we first set both the uncertainty probability ( @xmath183 ) and cutting point ( @xmath184 ) of the proposed method equal to 0.8 , and the @xmath185 in alsd .",
    "it can be seen from figure [ fig : uneven ] ( a ) , that the misclassification rate of the proposed method under such a setup is worse than that of alsd with @xmath185 . however , if we set the uncertainty sampling probability equal to @xmath186 and adjust the uneven group sizes with a shift cutting point based on the ratio of two sample sizes ( i.e. @xmath187 ) , then the performance of the proposed method is much improved ( see figure [ fig : uneven ] ( b ) ) , and is better than that of alsd .",
    "( all the learning curves in figure [ fig : uneven ] are based on the average of 100 replications of each method . )",
    "we also conducted simulations for other group size ratios , and the results are all similar and therefore omitted here .",
    "[ [ real - data - examples ] ] real data examples + + + + + + + + + + + + + + + + + +    similar results are obtained , when we apply both methods to bupa and wdbc data sets . figure [ fig : real - adj ] shows results of three different methods : alsd , the propose method with and without sample sizes adjustment .",
    "the last two methods are denoted as `` proposed-1 '' and `` proposed '' , respectively .",
    "the ratio of sample sizes of two groups in bupa data set is 0.58 , which is close to 0.5 .",
    "hence , the effect of uneven group sizes is not that obvious especially when @xmath71 . in wdbc data set ,",
    "the ratio is 0.627 , which is slightly far away from 0.5 .",
    "we can see from figure [ fig : real - adj ] ( c ) that the misclassification curve of the proposed method with adjusted cutting point ( proposed-1 ) becomes the best one when the number of the cumulated labeled subjects is larger than around 40 . in figure",
    "[ fig : real - adj ] ( d ) , it shows that with @xmath174 , the proposed method with an adjusted cutting point ( proposed-1 ) is the most stable one , among three methods from the very beginning .",
    "+   +    the phenomenon of using two different parameters for uncertainty measure and cutting threshold in fact can be explained from a statistical decision theory viewpoint .",
    "the details are discussed in appendix a.    [ [ statistical - decision - theory - viewpoint ] ] statistical decision theory viewpoint + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    let @xmath188 and @xmath36 be the prior probabilities of two groups , and @xmath189 and @xmath190 are the corresponding posterior probabilities given @xmath67 . in conventional statistical decision theory , when the prior probabilities , @xmath188 and @xmath36 is known and there is no other information available , the best decision rule @xmath191 , for any given subject , will be : @xmath192 , if @xmath193 ; @xmath194 , otherwise .",
    "( the decision function @xmath192 denotes that the subject with explanatory variable @xmath195 is assigned to class 0 , and vice versa . ) when a logistic model is assumed , and suppose that the satisfies that , the problem becomes how to estimate the unknown @xmath32 , and the decision rule will be made based on the posterior probabilities given observed @xmath67 ; that is , @xmath192 , if @xmath196 , and @xmath194 , otherwise .",
    "it follows from bayes formulae , this decision rule is equivalent to @xmath197 that is , when a logistic model is used in a classification problem , based on ( [ eq : bayes ] ) the prior probabilities of two groups are already considered .",
    "thus , it suffices to use @xmath198 to measure the uncertainty .",
    "moreover , let @xmath199 and @xmath200 be the misclassification costs of false positive and false negative errors , respectively . if we introduce these costs of misclassification into the decision rule , then bayes decision rule becomes @xmath192 , if @xmath201 ; @xmath194 , otherwise .",
    "because @xmath202 and @xmath203 can be treated as weights of two types of misclassification errors , we can assume that @xmath204 , and the overall misclassification error becomes @xmath205 , where @xmath206 and @xmath207 denote the false positive and false negative probabilities .",
    "this weighted misclassification error can usually be estimated by @xmath208 ( see * ? ? ?",
    "in fact , in @xcite , they also measured the misclassification using this formulae , which is @xmath209 in their notations , where @xmath210 and @xmath211 are numbers of false positive and false negative results .",
    "that is , the same parameter @xmath99 , in their paper , is used to measure the uncertainty and to adjust the weights of two different types of errors as well . from the discussion above ,",
    "it is reasonable to treat the uncertainty measure and weights of misclassification errors , separately .",
    "when prior probabilities are known , we can use them to adjust the cutting point in order to minimize the weighted misclassification errors , but not the uncertainty measure .    in practice , these probabilities are usually unknown , so it motivates us an interesting future study  `` whether can we use the estimated ratio of sample sizes to adjusted the cutting point ? ''",
    "moreover , because active learning processes are conducted sequentially .",
    "it is naturally to ask whether we can apply a stopping rule to a learning process with a pre - fixed performance target .",
    "all these issues are related sequential estimate of the event probability under adaptive sampling and the results will be reported elsewhere .",
    "active learning selects its own training samples in a sequential manner and requires fewer labeled instances from domain experts , and still achieves high classification performance . in this paper",
    ", we focus on a higher dimensional case and propose a new subject selection scheme that combines a bayesian d - optimal design and an uncertainty sampling method .",
    "thus , the proposed method inherits the advantage of methods of stochastic approximation and optimal design as suggested in @xcite .",
    "because of using a bayesian d - optimal design method , the active learning process is more stable in high dimensional cases even when the information matrix is nearly singular , and therefore will be more suitable for modern analysis with large data sets .",
    "in addition , we also demonstrate that with a small amount of labeled subjects are an initial training set , active learning process is more stable and efficient in both training time and the size of the labeled data . for uneven group sizes case ,",
    "we suggest to use separate parameters to control uncertainty sampling and adjust the cutting threshold for better performance . from our numerical studies",
    ", we found that the uncertainty measure and the probability of a event might play different roles in an active learning process ; especially when the sizes of two groups are uneven .",
    "we found that to use uncertainty measure at 0.5 and then adjust the boundary according to the proportion of group sizes as that in classical logistic regression models produces better results in our studies .",
    "these types of methods are suitable for problems with large amount of unlabeled data available , and have great potential for analyzing `` big data '' problems . from practical viewpoints , to include one new subject at a time is not practical . not only because of the computational efficiency , but also the operational complexity .",
    "this is similar to the situation in clinical trials , where sampling in batch as in a group sequential procedure is usually preferred .",
    "moreover , to label an unclassified subject is not only time consuming , there is also some operational costs such as experts charge and so on .",
    "hence , how to conduct an active learning process with a batch of updated subjects , and how to construct a classification rule with a satisfactory performance under a given budget constraint are important problems in both practical and theoretical viewpoints .",
    "25 natexlab#1#1[1]`#1 ` [ 2]#2 [ 1]#1 [ 1]http://dx.doi.org/#1 [ ] [ 1]pmid:#1 [ ] [ 2]#2 , , . .",
    "http://archive.ics.uci.edu/ml . , , .",
    ". . , . , , ,",
    ". . , . , , , ,",
    ". . , . , ,",
    ". . , . , ,",
    ". . , . , ,",
    ". volume . .",
    ", , , . . ,",
    ", , . , in : , pp . .",
    ", , . , in : . , .",
    ". . , . , , , . , in : , . pp . . , ,",
    ", , , . . ,",
    ", , , , . . ,",
    ". , . . , . , , , .",
    ". chapter  .",
    "let @xmath188 and @xmath36 be the prior probabilities of two groups , and @xmath189 and @xmath190 are the corresponding posterior probabilities given @xmath67 . in conventional statistical decision theory , when the prior probabilities , @xmath188 and @xmath36 is known and there is no other information available , the best decision rule @xmath191 , for any given subject , will be : @xmath192 , if @xmath193 ; @xmath194 , otherwise .",
    "( the decision function @xmath192 denotes that the subject with explanatory variable @xmath195 is assigned to class 0 , and vice versa . ) when a logistic model is assumed , and suppose that the satisfies that , the problem becomes how to estimate the unknown @xmath32 , and the decision rule will be made based on the posterior probabilities given observed @xmath67 ; that is , @xmath192 , if @xmath196 , and @xmath194 , otherwise .",
    "it follows from bayes formulae , this decision rule is equivalent to @xmath197 that is , when a logistic model is used in a classification problem , based on ( [ eq : bayes ] ) the prior probabilities of two groups are already considered .",
    "thus , it suffices to use @xmath198 to measure the uncertainty",
    ".    moreover , let @xmath199 and @xmath200 be the misclassification costs of false positive and false negative errors , respectively . if we introduce these costs of misclassification into the decision rule , then bayes decision rule becomes @xmath192 , if @xmath201 ; @xmath194 , otherwise .",
    "because @xmath202 and @xmath203 can be treated as weights of two types of misclassification errors , we can assume that @xmath204 , and the overall misclassification error becomes @xmath205 , where @xmath206 and @xmath207 denote the false positive and false negative probabilities .",
    "this weighted misclassification error can usually be estimated by @xmath208 ( see * ? ? ?",
    "in fact , in @xcite , they also measured the misclassification using this formulae , which is @xmath209 in their notations , where @xmath210 and @xmath211 are numbers of false positive and false negative results .",
    "that is , the same parameter @xmath99 , in their paper , is used to measure the uncertainty and to adjust the weights of two different types of errors as well . from the discussion above ,",
    "it is reasonable to treat the uncertainty measure and weights of misclassification errors , separately .",
    "when prior probabilities are known , we can use them to adjust the cutting point in order to minimize the weighted misclassification errors , but not the uncertainty measure .    in practice , these probabilities are usually unknown , so it motivates us an interesting future study  `` whether can we use the estimated ratio of sample sizes to adjusted the cutting point ? ''",
    "moreover , because active learning processes are conducted sequentially .",
    "it is naturally to ask whether we can apply a stopping rule to a learning process with a pre - fixed performance target .",
    "all these issues are related sequential estimate of the event probability under adaptive sampling and the results will be reported elsewhere .",
    "[ [ section ] ]        according to the importance sampling approach discussed in @xcite , @xmath81 can be written in the form @xmath216 where the prior @xmath48 serves as the importance sampling distribution .",
    "draw @xmath217 @xmath218 from @xmath48 and then the estimator is @xmath219 where @xmath220 and @xmath221 . + applying @xmath222 where @xmath223 is a constant , we obtain @xmath224 .",
    "therefore , @xmath225 thus , @xmath226"
  ],
  "abstract_text": [
    "<S> classification is an important task in many fields including biomedical research and machine learning . </S>",
    "<S> traditionally , a classification rule is constructed based a bunch of labeled data . recently , due to technological innovation and automatic data collection schemes , we easily encounter with data sets containing large amounts of unlabeled samples . because to label each of them </S>",
    "<S> is usually costly and inefficient , how to utilize these unlabeled data in a classifier construction process becomes an important problem . in machine learning literature , </S>",
    "<S> active learning or semi - supervised learning are popular concepts discussed under this situation , where classification algorithms recruit new unlabeled subjects sequentially based on the information learned from previous stages of its learning process , and these new subjects are then labeled and included as new training samples . from a statistical aspect , these methods can be recognized as a hybrid of the sequential design and stochastic approximation procedure . in this paper , we study sequential learning procedures for building efficient and effective classifiers , where only the selected subjects are labeled and included in its learning stage . </S>",
    "<S> the proposed algorithm combines the ideas of bayesian sequential optimal design and uncertainty sampling . </S>",
    "<S> computational issues of the algorithm are discussed . </S>",
    "<S> numerical results using both synthesized data and real examples are reported .    </S>",
    "<S> active learning , uncertainty sampling , sequential experimental design , d - optimal design , bayes rule </S>"
  ]
}