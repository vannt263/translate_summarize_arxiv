{
  "article_text": [
    "there are numerous situations in which additive ( main effects ) models are insufficient for predicting an outcome of interest . in medical diagnosis ,",
    "the co - occurrence of two symptoms may lead a doctor to be confident that a patient has a certain disease whereas the presence of either symptom without the other would provide only a moderate indication of that disease .",
    "this situation corresponds to a positive ( i.e. , synergistic ) interaction between symptom variables . on the other hand ,",
    "suppose both symptoms convey redundant information to the doctor about the patient so that knowing both provides no more information about the disease status than either one on its own .",
    "this situation is again not additive , but this time there is a negative interaction between symptoms .",
    "fitting regression models with interactions is challenging when one has even a moderate number , @xmath0 , of measured variables , since there are @xmath1 interactions of order @xmath2 .",
    "for this paper , we focus on the case of pairwise ( @xmath3 ) interaction models , although the ideas we develop generalize naturally to higher - order interaction models .",
    "we consider a regression model for an outcome variable @xmath4 and predictors @xmath5 , with pairwise interactions between these predictors .",
    "in particular our model has the form @xmath6 where @xmath7 . regardless of whether the predictors are continuous or discrete",
    ", we will refer to the additive part as the `` main effect '' terms and the quadratic part as the `` interaction '' terms .",
    "our goal is to estimate @xmath8 and @xmath9 , where @xmath10 and @xmath11 .",
    "the factor of one half before the interaction summation is a consequence of our notational decision to deal with a symmetric matrix @xmath12 of interactions rather than a vector of length @xmath13 .",
    "we take @xmath11 throughout this paper because it simplifies notation , but everything carries over if we remove this restriction .",
    "indeed , we provide this as an option in the ` hiernet ` ( pronounced `` hair net '' ) package .",
    "we observe a training sample , @xmath14 , and our goal is to select a subset of the @xmath15 main effect and interaction variables that is predictive of the response , and to estimate the values for the nonzero parameters of the model .",
    "it is a well - established practice among statisticians fitting ( [ eq : model ] ) to only allow an interaction into the model if the corresponding main effects are also in the model .",
    "such restrictions are known under various names , including `` heredity , '' `` marginality , '' and being `` hierarchically well - formulated '' [ @xcite ] .",
    "there are two types of restrictions , which we will call _ strong _ and _ weak hierarchy _ : @xmath16 some statisticians argue that models violating strong hierarchy are not sensible . for example , according to @xcite ,    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ `` [ t]here is usually no reason to postulate a special position for the origin , so that the linear terms must be included with the cross - term . '' _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    to see that violating strong hierarchy amounts to `` postulating a special position for the origin , '' consider writing an interaction model as @xmath17 .",
    "first of all , we would only take @xmath18 if we have special reason to believe that the regression surface must go through the origin .",
    "likewise , taking @xmath19 but @xmath20 would only be appropriate if we actually believe that @xmath21 s effect on @xmath4 should only be present specifically when @xmath22 is nonzero . in most situations",
    ", we do not think that the variable @xmath22 that we measured is any more special than @xmath23 . yet",
    "if our model with @xmath22 violates strong hierarchy , then our model with @xmath23 ( for any @xmath24 ) is strongly hierarchical .",
    "this argument suggests that violations to hierarchy occur in special situations whereas hierarchy is the default .",
    "another argument in favor of hierarchy has to do with statistical power . in the words of @xcite :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ `` [ l]arge component main effects are more likely to lead to appreciable interactions than small components .",
    "also , the interactions corresponding to larger main effects may be in some sense of more practical importance . ''",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    in other words , rather than looking at all possible interactions , it may be useful to focus our search on those interactions that have large main effects .",
    "indeed , the method we propose in this paper makes direct use of this principle .    as a final argument for hierarchy",
    ", it is useful to distinguish between two notions of sparsity , which we will call _ parameter sparsity _ and _ practical sparsity_. parameter sparsity is what most statisticians mean by `` sparsity '' : the number of nonzero coefficients in the model .",
    "practical sparsity is what someone actually collecting data cares about : the number of variables one needs to _ measure _ to make predictions at a future time .",
    "the hierarchy restriction favors models that `` reuse '' measured variables whereas a nonhierarchical model does not .",
    "the top left panel of figure  [ fig : olive ] gives a small example where this difference is manifest .",
    "in fact , a simple calculation shows that this difference can be quite substantial : we can have a hierarchical and a nonhierarchical interaction model with the same parameter sparsity but with the nonhierarchical method having a practical sparsity of @xmath25 whereas the hierarchical method s practical sparsity is just @xmath2 .",
    "for the strong hierarchical lasso .",
    "filled nodes correspond to nonzero main effects , and edges correspond to nonzero interactions . ]    while taking these arguments to the extreme leads to the use of strong hierarchy exclusively , we develop the case of weak hierarchy in parallel throughout this paper",
    ". weak hierarchy , as the name suggests , can be thought of as a compromise between strong hierarchy and imposing no such structure and appears as a principle in certain statistical methods such as classification and regression trees [ @xcite ] and multivariate additive regression splines [ @xcite ] .",
    "the lasso [ @xcite ] is a method that performs both model selection and estimation .",
    "it penalizes the squared loss of the data with an @xmath26-norm penalty on the parameter vector .",
    "this penalty has the property of producing estimates of the parameter vector that are sparse ( corresponding to model selection ) .",
    "given a design matrix @xmath27 and response vector @xmath28 , the lasso is the solution to the convex optimization problem , @xmath29 where @xmath30 is the vector of ones .",
    "the penalty parameter , @xmath31 , controls the relative importance of fitting to the training data ( sum - of - squares term ) and of sparsity ( @xmath26 penalty term ) .",
    "a natural extension of the lasso to our interaction model  ( [ eq : model ] ) would be to take @xmath32 $ ] and @xmath33 , where the columns of @xmath34 correspond to elementwise products of the columns of @xmath35 .",
    "we will refer to this method as the _ all - pairs lasso _ since it is simply the lasso applied to a data matrix which includes all pairs of interactions ( as well as all main effects ) .",
    "it is common with the lasso to standardize the predictors so that they are on the same scale . in this paper , we standardize @xmath35 so that its columns have mean 0 and standard deviation  1 ; we then form @xmath36 from these standardized predictors and , finally , center the resulting columns of @xmath36 . by centering @xmath37 and @xmath38",
    ", we may take @xmath39 .",
    "the lasso s @xmath26 penalty is neutral to the pattern of sparsity , allowing any sparsity pattern to emerge .",
    "the notions of strong and weak hierarchy introduced in section  [ sec : strong - weak - hier ] represent situations in which we want to exclude certain sparsity patterns .",
    "there has been a growing literature focusing on methods that produce _ structured sparsity _ [ @xcite , @xcite , @xcite , @xcite , @xcite , @xcite ] .",
    "these methods make use of the _ group lasso _ penalty ( and generalizations thereof ) which , given a predetermined grouping of the parameters , induces entire groups of parameters to be set to zero [ @xcite ] . given a set of groups of variables , @xmath40 , these methods generalize the @xmath26 penalty by @xmath41 where @xmath42 , @xmath43 is @xmath44 projected onto the coordinates in @xmath45 , and @xmath46 is a nonnegative weight .",
    "hierarchical structured sparsity is obtained by choosing @xmath40 to have nested groups .",
    "for example , @xcite consider the penalty @xmath47 likewise , the framework of @xcite if specialized to this paper s focus would lead to a penalty of the form @xmath48 for some @xmath49 and @xmath50 .",
    "in fact , @xcite suggest a penalty for generalized additive models with interactions that reduces to ( [ eq : bach ] ) in the linear model case , with @xmath51 and @xmath52 independent of @xmath53 .      here",
    ", we propose a lasso - like procedure that produces sparse estimates of @xmath54 and @xmath12 while satisfying the strong or weak hierarchy constraint .",
    "in contrast to much of the structured sparsity literature which is based on group lasso penalties , our approach , presented in section  [ sec : our - proposed - method ] , involves adding a set of convex constraints to the lasso .",
    "although we find this form of constraint more naturally interpretable , we show ( remark  [ re3 ] ) that this problem can be equivalently expressed in a form that relates it to penalties from the structured sparsity literature such as  ( [ eq : bach ] ) .",
    "a key advantage of our specific choice of penalty structure is that it admits a simple interpretation of the effect of the hierarchy demand . unlike other hierarchical sparsity methods , which do not pay much attention to the particular choice of norms (",
    "as long as @xmath42 ) , our formulation is carefully tailored to allow it to be related directly back to the lasso , permitting one to understand specifically how hierarchy alters the solution ( section  [ sec : effect - constraint ] ) .",
    "this feature of our estimator gives it a transparency that exposes the effects ( both positive and negative ! ) of the hierarchy constraint .",
    "furthermore , our characterization suggests that the demand for hierarchy is  analogous to the demand for sparsity  a form of `` regularization . ''",
    "we develop an unbiased estimator of the degrees of freedom of our method ( section  [ sec : degrees - freedom ] ) and an interpretable upper bound on this quantity , which also points to hierarchy as regularization . in particular , we show that we do not `` spend '' in degrees of freedom for main effects that are forced into the model by the hierarchy constraint .",
    "another difference from much of the structured sparsity literature , which aims to develop a broad treatment of structured and hierarchical sparsity methods , is that our focus is narrowed to the problem of interaction models .",
    "our restricted scope allows us to address specifically the performance of such a tool to this important problem . in section",
    "[ sec : related - work ] , we review previous work on the problem of hierarchical interaction model fitting and selection .",
    "these methods fall into three categories : multi - step procedures , which are defined by an algorithm [ @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite ] ; bayesian approaches , which specify the hierarchy requirement through a prior [ @xcite ] ; and , most related to this paper s proposal , regularized regression methods , which are defined by an optimization problem [ @xcite , @xcite , @xcite , @xcite , @xcite ] . in section  [ sec : empirical - study ] , we study via simulation the statistical implications of imposing hierarchy on an interactions - based estimator under various scenarios ( in both the lasso and stepwise frameworks ) . in section  [ sec : algorithmics ] we present an efficient algorithm for computing our estimator .",
    "real data examples are used to illustrate a distinction we draw between `` parameter sparsity '' and `` practical sparsity '' and to discuss hierarchy s role in promoting the latter .",
    "in section  [ sec : sparsity - lasso ] , we introduced the _ all - pairs lasso _ , which can be written as @xmath55 where @xmath56 and @xmath57 is the loss function , typically@xmath58 , but may also include a ridge penalty on the coefficients as discussed later or may be substituted for the binomial negative log - likelihood .",
    "the one - half factors in front of terms involving @xmath12 are merely a consequence of the notational choice to represent @xmath12 as a symmetric matrix ( with @xmath11 for @xmath59 ) . in this paper",
    ", we propose a modification of the all - pairs lasso that produces models that are guaranteed to be hierarchical .",
    "as motivation for our proposal , consider building hierarchy into the optimization problem as a constraint , @xmath60 \\\\[-8pt ] \\nonumber & & \\qquad\\mbox{s.t.}\\qquad \\theta= \\theta^t , \\vert\\theta_j \\vert_1\\le|\\beta_j| \\qquad\\mbox{for } j=1,\\ldots , p,\\end{aligned}\\ ] ] where @xmath61 denotes the @xmath53th row ( and column , by symmetry ) of @xmath12 .",
    "notice that if @xmath62 , then @xmath63 and @xmath64 and thus @xmath65 and @xmath66 . while the added constraints enforce strong hierarchy , they are not convex , which makes  ( [ eq : nonconvex ] ) undesirable as a method . in this paper",
    ", we propose a straightforward convex relaxation of ( [ eq : nonconvex ] ) , which we call the _ strong hierarchical lasso _ , @xmath67 \\\\[-8pt ] \\nonumber & & \\qquad\\mbox{s.t . }",
    "\\qquad\\theta= \\theta^t ,   \\left.\\begin{aligned } \\vert\\theta_j\\vert_1&\\le{\\beta^+}_j+ { \\beta^-}_j \\\\ { \\beta^+}_j&\\ge0 , { \\beta^-}_j\\ge0 \\end{aligned } \\right\\}\\mbox { for } j=1,\\ldots , p , \\nonumber\\end{aligned}\\ ] ] where we have replaced the optimization variable @xmath8 by two vectors @xmath68 . after solving the above problem , our fitted model is of the form @xmath69 . while we might informally think of @xmath70 and @xmath71 as positive and negative parts of a vector @xmath72 , that is , that @xmath73 , this is not actually the case since at a solution we can have both @xmath74 and @xmath75",
    "indeed , if we were to add the constraints @xmath76 for @xmath59 to ( [ eq : hiernet ] ) , then these would be positive and negative parts and so @xmath77 , giving us precisely problem ( [ eq : nonconvex ] ) .",
    "this observation establishes that ( [ eq : hiernet ] ) is a convex relaxation of  ( [ eq : nonconvex ] ) .",
    "the hierarchy constraints can be seen as an embedding into our method of david cox s `` principle '' that `` large component main effects are more likely to lead to appreciable interactions than small components . ''",
    "the constraint @xmath78 budgets the total amount of interactions involving variable @xmath79 according to the relative importance of @xmath79 as a main effect .",
    "one additional advantage of the convex relaxation is that the constraint is less restrictive .",
    "if the best fitting model would have @xmath80 large but @xmath81 only moderate , this can be accommodated by making @xmath82 and @xmath83 both large .",
    "[ rem1 ] another possibility for the hierarchy constraint that we have considered is @xmath84 ; however , we have found that this can lead to an overabundance of interactions relative to main effects .",
    "[ rem2 ] it is desirable to include in the loss function @xmath85 an _ elastic net _ term , @xmath86 , to ensure uniqueness of the solution [ @xcite ] .",
    "we think of @xmath87 as a fixed tiny fraction of @xmath88 , such as @xmath89 , rather than as an additional tuning parameter .",
    "such a modification does not complicate the algorithm , but simplifies the study of the estimator . in all numerical examples and in the ` hiernet ` package , we use this elastic net modification .",
    "[ re3 ] we prove in section  2 of the supplementary materials [ @xcite ] that ( [ eq : hiernet ] ) may equivalently be written as @xmath90 this reparameterization of the problem shows its similarities to the group lasso based methods . in place of the more standard penalty @xmath91 of ( [ eq : bach ] ) , we use @xmath92 . in section [ sec : effect - constraint ] , we show that this unusual choice of penalty admits a particularly simple interpretation for the effect of imposing hierarchy .    in section  [ sec : strong - weak - hier ]",
    ", we also introduced the notion of weak hierarchy . by simply removing the symmetry constraint on @xmath12",
    ", we get what we call the _ weak hierarchical lasso _ , @xmath93 \\\\[-8pt ] \\nonumber & & \\qquad\\mbox{s.t.}\\qquad \\left.\\begin{aligned } \\vert\\theta_j \\vert_1&\\le{\\beta^+}_j+{\\beta^-}_j \\\\ { \\beta^+}_j&\\ge0 , { \\beta^-}_j\\ge0 \\\\ \\end{aligned } \\right\\}\\mbox { for } j=1,\\ldots , p.\\end{aligned}\\ ] ] even though at a solution to this problem , @xmath94 is not symmetric , we should think of the interaction coefficient as @xmath95 since this is what multiplies the interaction term @xmath96 when computing @xmath97 .",
    "[ rem4 ] we can build further on the connection between ( [ eq : bach ] ) and ( [ eq : hiernet ] ) discussed in remark  [ re3 ] .",
    "our removal of the symmetry constraint in ( [ eq : weak - hiernet ] ) is analogous to the technique of duplicating columns of the design matrix used in the overlap group lasso [ @xcite ] .",
    "a favorable property that distinguishes our method from previous approaches discussed in section  [ sec : related - work ] is the relative transparency of the role that the hierarchy constraint plays in our estimator .",
    "this aspect is developed in section [ sec : effect - constraint ] .",
    "although our primary focus in this paper is on the gaussian setting of ( [ eq : model ] ) , our proposal extends straightforwardly to other situations , such as the logistic regression setting in which the response is binary . in this case",
    ", we simply have @xmath98 be the appropriate negative log - likelihood , @xmath99 , where @xmath100^{-1}$ ] . in section  3 of the supplementary materials [ @xcite ] ,",
    "we show that solving this problem requires only a minor modification to our primary algorithm .",
    "it should also be noted that our estimator ( and the algorithms developed to compute it ) is designed for both the @xmath101 and @xmath102 setting .    as a preliminary example , consider predicting whether a sample of olive oil comes from southern apulia based on measurements of the concentration of @xmath103 fatty acids [ @xcite ] .",
    "the dataset consists of @xmath104 samples , and we average our results over 100 random equal - sized train - test splits .",
    "we compare three methods : ( a ) a standard lasso with main effects only ( mel ) , ( b ) the all - pairs lasso ( apl ) , and ( c ) the strong hierarchical lasso ( hl ) .",
    "the top left panel of figure  [ fig : olive ] shows an interesting difference between hl and apl .",
    "we see that , on average , at a parameter sparsity level of five , the hl model uses four of the measured variables whereas apl uses six . using the hierarchical model to classify a future olive oil",
    ", we only need to measure four rather than six of the fatty acids .",
    "the top right panel of figure  [ fig : olive ] shows the predictive performance ( versus the practical sparsity ) of the three methods .",
    "it appears that hl enjoys the `` best of both worlds , '' matching the good performance of mel for low practical sparsity levels ( since it tends to pick out the main effects first ) and the good performance of apl at high practical sparsity levels ( since it can incorporate predictive interactions ) .",
    "finally , the bottom panel of the figure provides a visual display of a sequence of hl s solutions ( by varying @xmath88 ) .",
    "nonzero main effects are shown as filled nodes , and edges indicate nonzero interactions .",
    "since all edges are incident to filled nodes , we see that strong hierarchy holds .    in the next section , we present several properties of our estimator that shed light on the effect of adding the convex hierarchy constraint to the lasso . among these properties",
    "is an unbiased estimate of the degrees of freedom of our estimator .",
    "we view this degrees - of - freedom result as valuable primarily for the sake of understanding the effect of hierarchy .",
    "while such an estimate could be used for parameter selection , we prefer cross validation to select @xmath88 since this is more directly tied to the goal of prediction .",
    "a key advantage of formulating an estimator as a solution to a convex problem is that it can be completely characterized by a set of optimality conditions , known as the karush ",
    "tucker ( kkt ) conditions .",
    "these conditions are useful for understanding the effect that the hierarchy constraint in ( [ eq : hiernet ] ) and ( [ eq : weak - hiernet ] ) has on our solutions . in this section",
    ", we will study the simplest case , taking @xmath98 to be the quadratic loss function with no elastic net penalty .",
    "we let @xmath105 denote partial residuals ( where @xmath106 denotes elementwise multiplication , @xmath107 the vector of fitted values and @xmath108 the @xmath53th predictor ) , and we assume that @xmath109 . for linear regression ,",
    "the kkt conditions are known as the normal equations and can be written as @xmath110 the all - pairs lasso solution satisfies @xmath111}{\\vert x_j*x_k\\vert^2},\\ ] ] where @xmath112 denotes the soft - thresholding operator defined by @xmath113 .",
    "written this way , we see that the lasso is similar to linear regression , but all coefficients are shrunken toward 0 , with some coefficients ( those for which @xmath114 ) set to zero .",
    "it is instructive to examine the corresponding statements for the strong and weak hierarchical lasso methods.=-1    [ prop : constraint ] the coefficients of the strong and weak hierarchical lassos with @xmath115 and taking @xmath98 to be the quadratic loss ( with no elastic net penalty ) satisfy :    * strong : @xmath116}{\\vert x_j*x_k\\vert^2};\\end{aligned}\\ ] ] * weak : @xmath117}{\\vert x_j*x_k\\vert^2}\\end{aligned}\\ ] ]    for some @xmath118 , @xmath59 with @xmath119 when @xmath120 ( and likewise for @xmath121 ) .",
    "see section  1 of the supplementary materials [ @xcite ] .",
    "the @xmath122 appearing in the above two properties are optimal dual variables corresponding to the @xmath53th hierarchy constraint for the strong and weak hierarchical lasso problems , respectively .",
    "when @xmath123 , we have @xmath119 ( or @xmath124 ) by complementary slackness .",
    "comparing these expressions to those of the all - pairs lasso gives insight into the effect of the constraint .",
    "property [ prop : constraint ] reveals that the overall form of the all - pairs lasso and hierarchical lasso methods is identical .",
    "the difference is that the hierarchy constraint leads to a reduction in the shrinkage of certain main effects and an increase in the shrinkage of certain interactions . in particular , we see that when the hierarchy constraints are loose at the solution , that is , @xmath120 , the weak hierarchical lasso s optimality conditions become identical to the all - pairs lasso ( since @xmath124 ) for all coefficients involving @xmath108 . for the strong hierarchical lasso ,",
    "when both the @xmath53th and @xmath2th constraints are loose , the optimality conditions match those of the all - pairs lasso for the coefficients of @xmath108 , @xmath125 and @xmath126 .",
    "the methods differ when constraints are active , that is , when @xmath127 , which allows @xmath128 ( or @xmath121 ) to be nonzero .",
    "intuitively , this case corresponds to the situation in which hierarchy would not have held `` naturally '' ( i.e. , without the constraint ) , and the corresponding dual variable plays the role of reducing @xmath129 in @xmath26-norm and increasing @xmath130 until the constraint is satisfied . the way in which the weak and strong hierarchical lasso methods perform this shrinkage is different , but both are identical to the all - pairs lasso when all constraints are loose .      in section",
    "[ sec : our - proposed - method ] , we showed that adding the constraint @xmath131 would guarantee that hierarchy holds .",
    "however , we have not yet shown that the same is true of the convex relaxation s constraint , @xmath132 . in particular , while @xmath133 , we could still have @xmath134 .",
    "this would correspond to a model in which @xmath135 is used in the model , but @xmath79 is not .",
    "intuitively , we would expect that if @xmath74 , then @xmath136 is analogous to getting an exact zero in linear regression ( i.e. , a zero probability event ) . in this section",
    ", we establish that this is in fact the case .",
    "in particular , we study ( [ eq : hiernet ] ) and ( [ eq : weak - hiernet ] ) where @xmath98 includes an elastic net term .",
    "the importance of this modification is that it ensures uniqueness , simplifying the analysis .",
    "as noted in remark  [ rem2 ] , we think of @xmath137 as a small , fixed proportion of @xmath88 rather than as a separate tuning parameter .    [",
    "prop : strong - hier ] suppose @xmath37 is absolutely continuous with respect to the lebesgue measure on @xmath138 . if @xmath139 solves ( [ eq : hiernet ] ) , where @xmath98 is the quadratic loss with an @xmath87 ridge penalty , then strong hierarchy holds with probability 1 , that is , @xmath140    see appendix  [ app : proof - hier ] .    to understand how dropping the symmetry constraint leads to the `` or '' statement required of weak hierarchy , note that @xmath135 is in the weak hierarchical lasso model if and only if @xmath141 .",
    "this holds only if @xmath62 or @xmath142 .",
    "[ prop : weak - hier ] suppose @xmath37 is absolutely continuous with respect to the lebesgue measure on @xmath138 .",
    "if @xmath139 solves ( [ eq : weak - hiernet ] ) , where @xmath98 is the quadratic loss with an @xmath87 ridge penalty , then weak hierarchy holds with probability 1 , that is , @xmath143    see appendix  [ app : proof - hier ] .      in classical statistics ,",
    "the degrees of freedom of a procedure refer to the dimension of the space over which its fitted values can vary .",
    "it is useful in that it provides a measure of how much `` fitting '' the procedure is doing .",
    "this notion can be generalized to adaptive procedures such as the lasso [ @xcite , @xcite , @xcite , @xcite ] .",
    "see ( ryan ) @xcite for a thorough discussion .",
    "if given data @xmath28 , a procedure @xmath144 produces fitted values @xmath145 , the degrees of freedom of the procedure @xmath144 is defined to be @xmath146    [ prop : df ] suppose @xmath147 .",
    "an unbiased estimate of the degrees of freedom of the strong hierarchical lasso , with quadratic loss and no ridge penalty , is given by @xmath148 where @xmath149 with @xmath36 containing the interactions , and @xmath150 is a projection matrix which depends on the sign pattern of @xmath139 and on the set of hierarchy constraints that are tight .",
    "see appendix  [ app : df ] .",
    "figure  [ fig : df ] provides a numerical evaluation of how well @xmath151 estimates @xmath152 .",
    "we fix @xmath153 and @xmath9 , and we generate @xmath154 monte carlo replicates @xmath155 . for each replicate",
    ", we fit the strong hierarchical lasso along a grid of @xmath88 values to get @xmath156 and @xmath157 . from these values , we compute monte carlo estimates of @xmath152 from the definition in ( [ eq : df - def ] ) and of @xmath158 $ ] .",
    "estimates @xmath152 .",
    "monte carlo estimates of @xmath158 $ ] ( @xmath37-axis ) versus monte carlo estimates of @xmath152 ( @xmath159-axis ) for a sequence of @xmath88 values ( circular ) are shown .",
    "one - standard - error bars are drawn and are hardly visible .",
    "our bound on the unbiased estimate is plotted with diamonds . ]    while @xmath160 can be calculated from the data and is therefore useful as an unbiased way of calibrating the amount of fitting the strong hierarchical lasso is doing , this expression is difficult to interpret .",
    "however , it turns out that we can _ bound _ @xmath160 by a quantity that does make more sense :    [ prop : df - bound ] let @xmath161 , @xmath162 and @xmath163 .",
    "then , @xmath164 holds almost surely , where @xmath165 .",
    "see appendix  [ app : df ] .",
    "by contrast , for the all - pairs lasso in the case that @xmath166 and the design matrix is full rank , we have @xmath167 $ ] [ @xcite ] .",
    "in other words , the strong hierarchical lasso does not `` pay '' ( in terms of fitting ) for those main effects , @xmath168 , that are forced into the model by the hierarchy constraint to accommodate a strong interaction .",
    "notice that we do pay for a nonzero main effect if _ both _ @xmath169 and @xmath170 are nonzero .",
    "this makes sense since the constraint could be satisfied with just one of these variables nonzero , but in this case it is advantageous to the fit to make both nonzero . in figure",
    "[ fig : df ] , we find that this bound is in expectation visually indistinguishable from @xmath171 $ ] .",
    "there has been considerable interest in fitting interaction models in statistics and related fields .",
    "we focus here on an overview of methods that aim at forming predictive models that satisfy the hierarchical interactions restriction .",
    "many statistics textbooks discuss a simple stepwise procedure in which one iteratively considers adding or removing the `` best '' variable ( whether it be main effect or interaction ) ; they add that one should only consider including an interaction if its main effects are in the model [ e.g. , see backward elimination in @xcite , section  6.1.3 ] . in doing so , they are enforcing the strong hierarchy restriction . such procedures",
    "are ubiquitous [ @xcite , @xcite ] as are more recent versions [ @xcite , @xcite , @xcite , @xcite ] .",
    "another approach is to perform model selection first without considering hierarchy and then to include any lower - order terms necessary to satisfy hierarchy as a post - processing step [ @xcite ] .",
    "finally , @xcite and @xcite consider modifying the lars algorithm [ @xcite ] so that hierarchy is enforced .",
    "another set of procedures for building hierarchical interaction models comes from a bayesian viewpoint .",
    "@xcite adapts the _ stochastic search variable selection _ ( ssvs ) approach of @xcite to produce strong or weak hierarchical interaction models .",
    "ssvs makes use of a hierarchical normal mixture model to perform variable selection in regression .",
    "every variable has a latent binary variable indicating whether it is `` active . ''",
    "conditional on this latent variable , each coefficient is a 0-mean normal with variance determined by the latent importance of the coefficient .",
    "the original ssvs paper chooses a prior in which the importance of each variable is an independent bernoulli .",
    "@xcite introduces dependence into the prior so that @xmath172 is important only if @xmath173 and/or @xmath174 is important as well .",
    "@xcite formulate a nonconvex optimization problem to get sparse hierarchical interaction models .",
    "they write @xmath175 , where @xmath54 are the main effect coefficients and then apply @xmath26 penalties on @xmath54 and @xmath176 .",
    "notice that @xmath177 implies @xmath178 and @xmath179 .",
    "the nonconvexity arises in writing @xmath172 as the product of optimization variables .",
    "most similar to this paper s proposal is a series of methods which formulate convex optimization problems to give sparse hierarchical interaction models . @xcite",
    "modify the nonnegative garrote [ @xcite ] by adding linear inequality constraints to enforce hierarchy . in this sense ,",
    "our method can be seen as the adaption of their approach to the lasso .    finally ,",
    "as discussed in section  [ sec : sparsity - lasso ] , another set of convex methods makes use of the group lasso penalty [ @xcite ] .",
    "@xcite [ and , relatedly , @xcite ] describe composite absolute penalties ( cap ) , a very broad class of penalties that can achieve group and hierarchical sparsity . to achieve `` hierarchical selection ,",
    "'' they put forward the principle that a penalty of the form @xmath180 , with @xmath181 , induces @xmath182 to be zero only when @xmath183 is zero as well .",
    "for hierarchical interaction models , they suggest a penalty of the form @xmath184 $ ] .",
    "this framework has been developed in the structured sparsity literature [ e.g. , @xcite ] .",
    "@xcite introduce vanish , which uses this nested - group principle to achieve hierarchical sparsity in the context of nonlinear interactions .",
    "their penalty in the setting of ( [ eq : model ] ) is @xmath185 $ ] .",
    "as noted in remark  [ re3 ] , our proposal is closer to cap and vanish than it may first appear .",
    "our problem can be rewritten to have a penalty of the form @xmath186.$ ] in this sense , the penalty is in the spirit of cap and related methods although it does not quite fall into the class of cap ( since ours involves a sum of norms of norms ) .",
    "it is most similar to vanish in that it combines all of @xmath61 into the term involving @xmath173 .",
    "our main interest in this section is to study the advantages and disadvantages of restricting one s interaction models to those that honor hierarchy . clearly , the effectiveness of such a strategy depends on the true model generating the data .",
    "we take @xmath187 and @xmath188 ( 435 two - way interactions ) and consider four scenarios :    truth is hierarchical : @xmath189 ;    truth is anti - hierarchical : @xmath190 ;    truth only has interactions : @xmath191 for all @xmath53 ;    truth only has main effects : @xmath192 for all @xmath193 .    in cases ( i ) , ( ii ) , ( iv ) , we set 10 elements of @xmath54 to be nonzero ( with random sign ) , and , in cases ( i ) , ( ii ) , ( iii ) , we set 20 elements of the submatrix of @xmath10 to be nonzero . the signal - to - noise ratio ( snr ) for the main effects part of the signal is about 1.5 whereas the snr for the interactions part is about 1 .",
    "we study the effectiveness of the hierarchy constraint in the context of both the lasso and forward stepwise regression .",
    "forward stepwise regression refers to a greedy strategy for generating a sequence of linear regression models in which we start with an intercept - only model and then at each step add the variable that leads to the greatest decrease in the residual sum of squares .",
    "we choose forward stepwise as a basis of comparison since it has a simple modification that we think may be the hierarchical interactions approach most commonly used by statisticians .",
    "the modification is to restrict the set of interactions that could be added at a given step to only those between main effect variables currently in the model .",
    "a backward stepwise version of this approach is suggested in @xcite .",
    "we compare six methods , corresponding to each cell of the following table :    [ cols=\"<,^,^,^\",options=\"header \" , ]     we will refer to this in the proofs that follow .      including the elastic net penalty , @xmath194 , is equivalent to replacing @xmath38 and @xmath37 in ( [ eq : linear ] ) by @xmath195 suppose we solve ( [ eq : linear ] ) with the above design matrix . by lemma  [ lem : project ] ,",
    "@xmath196 for some @xmath197 satisfying @xmath198>0\\ ] ] for all @xmath199 .",
    "let @xmath200 be the linear operator that selects the part of a vector corresponding to the variable @xmath201 .",
    "now , @xmath202 implies that @xmath203 and @xmath204 .",
    "we showed earlier that we can not have @xmath205 and @xmath206 .",
    "this means that for any @xmath193 , there must be an @xmath207 for which @xmath208 corresponds to @xmath209 or @xmath210 .",
    "thus , @xmath211 means that @xmath212 or @xmath213 for each @xmath193 .",
    "this implies that @xmath214 and thus @xmath215 .",
    "we show now that @xmath216 in terms of our above notation , this is @xmath217 where @xmath218 is the vector with all zeros except for @xmath219 .",
    "let @xmath220 , and consider the set @xmath221^t\\bigl(e_j^+-e_j^-\\bigr)=0 , \\\\ & & \\hspace*{32pt}\\bigl[({\\widetilde{x}}_\\varepsilon p)^+\\bigl(z-\\bigl(p{\\widetilde{x}}_\\varepsilon^t \\bigr)^+w\\bigr)\\bigr]^te_j^+>0 , \\\\ & & \\hspace*{77pt}\\bigl[({\\widetilde{x}}_\\varepsilon p)^+\\bigl(z-\\bigl(p{\\widetilde{x}}_\\varepsilon^t \\bigr)^+w\\bigr)\\bigr]^te_j^->0\\bigr\\}.\\end{aligned}\\ ] ] in light of ( [ eq : sets ] ) , fixing @xmath222 automatically specifies @xmath223 .",
    "the outer union is restricted to those subsets @xmath222 of @xmath224 elements that would have @xmath225 and @xmath226 . the event in ( [ eq : zeroprob ] )",
    "is contained in @xmath227 since it corresponds to the case in which @xmath222 is @xmath228 .",
    "we begin by showing that @xmath228 is in this restricted union with probability one .",
    "we have already argued that at a solution we must have @xmath229 for all @xmath193 .",
    "now , @xmath74 and @xmath75 together imply that @xmath230 since otherwise we could lower the objective by reducing @xmath169 and @xmath170 without leaving the feasible set .",
    "therefore , it would be sufficient to show that @xmath231 .",
    "we do so by observing that @xmath232 is a finite union of zero probability sets .",
    "we begin by establishing that @xmath233 .",
    "to do so , we write @xmath234 for some @xmath235 .",
    "now , @xmath236 , so we can write @xmath237 , where @xmath238 and thus @xmath239 . since @xmath240",
    ", it follows that @xmath241 .",
    "write @xmath242 for @xmath243 , and observe that @xmath244 $ ] so that @xmath245 \\\\",
    "& = & \\varepsilon\\bigl[u_1^tu_2-u_1^{\\theta^+t}u_2^{\\theta^-}-u_1^{\\theta ^-t}u_2^{\\theta^+ } \\bigr ] \\\\ & = & -\\varepsilon\\biggl[0+\\sum_{jk}\\bigl[u_1^{\\theta^+ } \\bigr]_{jk}\\bigl[u_2^{\\theta ^-}\\bigr]_{jk}+ \\bigl[u_1^{\\theta^-}\\bigr]_{jk}\\bigl[u_2^{\\theta^+ } \\bigr]_{jk}\\biggr].\\end{aligned}\\ ] ] now , for each @xmath193 we must have @xmath246 since @xmath247 .",
    "thus , for each @xmath193 , there is a r4 or r5 row in @xmath248 and thus @xmath249 implies that @xmath250_{jk}=0 $ ] or @xmath251_{jk}=0 $ ] for each @xmath193 and likewise @xmath252_{jk}=0 $ ] or @xmath253_{jk}=0 $ ] .",
    "therefore , @xmath254 .",
    "now , @xmath255 now , @xmath256 since @xmath257 and @xmath258 and @xmath259 has full column rank .",
    "thus , @xmath260 .",
    "this completes the first part of the proof .",
    "next , we show that @xmath261 as long as @xmath262\\times e_j^\\pm>0 $ ] . now , since @xmath263 , the only row of @xmath264 that has @xmath265 is the r1 row ; but clearly @xmath266",
    ". thus , @xmath267 and @xmath268 .",
    "it follows that @xmath269    putting these two parts of the proof together establishes that@xmath270 .",
    "thus , @xmath227 is a finite union of lebesgue measure 0 sets .",
    "this shows that @xmath231 as long as @xmath37 is absolutely continuous with respect to the lebesgue measure on @xmath138 .",
    "an argument nearly identical to that of the previous section establishes that @xmath271 with probability one .",
    "thus , if @xmath134 and @xmath272 , then both @xmath273 and @xmath274 .",
    "it follows then that @xmath275 .",
    "this establishes weak hierarchy .",
    "the fit in terms of the active set is given by @xmath276 where @xmath277 .",
    "of course , @xmath278 , and we can solve the kkt conditions to get the rest of the optimal dual variables in terms of the active set @xmath279+c,\\ ] ] where @xmath280 satisfies @xmath281+c\\ge0.$ ]    note that @xmath282 and thus @xmath228 and @xmath283 depend on @xmath37 even though we do not write this explicitly . we will continue writing @xmath284 to mean specifically @xmath285 . for @xmath286 in a neighborhood of @xmath37 , we might guess that @xmath287 and @xmath288 , where @xmath289+c , \\\\ g\\bigl(y'\\bigr)_{{\\mathcal{a}}({\\hat\\phi } ) } & = & 0.\\end{aligned}\\ ] ] to verify this guess , we need to check that the pair @xmath290 satisfies the optimality conditions at @xmath286 , @xmath291 first of all , @xmath292 holds since @xmath293 and @xmath294 .",
    "likewise , @xmath295 .",
    "now @xmath296 , so by continuity of @xmath297 , we have @xmath298 for all @xmath286 in a small enough neighborhood , @xmath299 , of @xmath37 .",
    "this establishes that @xmath300 .",
    "now @xmath301 , so complementary slackness holds . to see that the first optimality condition holds , we can simply plug @xmath302 into the left - hand side .",
    "all that remains is to show that @xmath303 . if we knew that @xmath304 , then by continuity of @xmath305 we could argue that over a small enough neighborhood , @xmath306 , @xmath307 .",
    "however , it could be the case that @xmath308 for some @xmath207 , that is , @xmath309 .",
    "nonetheless , one can show that there is a set @xmath310 of measure 0 for which @xmath311 implies that @xmath312 and @xmath313 for all @xmath286 in a neighborhood of @xmath37 .",
    "lemma 9 of ( ryan ) @xcite proves this result for a nearly identical situation .",
    "the fit @xmath314 is a piecewise affine function of @xmath37 . using stein s formula for the degrees of freedom [ as described in ryan , @xcite ]",
    ", we get that @xmath315=e\\bigl[{\\operatorname{tr}}\\bigl\\{({\\widetilde{x}}p ) ( { \\widetilde{x}}p)^+\\bigr\\ } \\bigr]=e\\bigl[\\operatorname{rank}({\\widetilde{x}}p)\\bigr],\\ ] ] where @xmath277 .",
    "we bound this by an estimate that is more interpretable : @xmath316 .",
    "clearly , r2r7 are linearly independent rows .",
    "thus , the rank of @xmath317 is at least @xmath318 .",
    "now , an r1 row is linearly independent of r2r8 precisely when @xmath319 has @xmath320 . to see this , note that if @xmath321 , then r1 is certainly linearly independent of r2r8 and likewise for @xmath322 ; however if @xmath323 , then @xmath324 for all @xmath325 , and therefore this row of r1 lies in the span of r3r7 .",
    "thus , this means there are @xmath326 additional linearly independent rows .",
    "finally , we consider r8 .",
    "clearly , r8 lies in the span of r4r5 for @xmath324 since @xmath327 at a solution .",
    "but if @xmath328 , then it is linearly independent of r1r8",
    ". therefore , r8 adds @xmath329 to the rank where we have used that @xmath330 at a solution ( since @xmath331 ) and recalling that @xmath332 for the rows of r8 . in summary , we have shown that the row - rank is @xmath333 recalling that there are @xmath224 columns , we get that @xmath334",
    "the lagrangian of ( [ eq : prox ] ) is given by @xmath335 where @xmath336 is the dual variable corresponding to the hierarchy constraints and @xmath337 are the dual variables corresponding to the nonnegativity constraints . for notational convenience , we have written @xmath338 for @xmath61 , and we have dropped the subscripts on @xmath339 .",
    "the kkt conditions are @xmath340 where @xmath341 is a subgradient of the absolute value function evaluated at @xmath342 .",
    "the three conditions involving @xmath337 implies that @xmath343_+$ ] .",
    "the stationarity condition involving @xmath344 implies that @xmath345 .",
    "now , define @xmath346_+-[\\tilde\\beta^-+t\\alpha]_+.$ ] the remaining kkt conditions involve @xmath347 alone : @xmath348 observing that @xmath297 is nonincreasing in @xmath336 and piecewise linear suggests finding @xmath347 as done in algorithm  [ alg : onerow ] .",
    "_ inputs : _ @xmath349 .    1 .",
    "find @xmath350 define @xmath351_+-[\\tilde\\beta_j^-+t\\alpha]_+.$ ] 1 .",
    "if @xmath352 , take @xmath353 and go to step 2 .",
    "form knot the set @xmath354 and let @xmath355 .",
    "3 .   evaluate @xmath356 for @xmath357 .",
    "4 .   if @xmath358 for some @xmath357 , take @xmath359 and go to step 2 .",
    "find adjacent knots , @xmath360 , such that @xmath361 .",
    "take @xmath362/(p_2-p_1).\\ ] ] 2 .",
    "return @xmath363 $ ] and @xmath364_+$ ] .",
    "we thank will fithian , max grazier - gsell , brad klingenberg , balasubramanian narasimhan and ryan tibshirani for useful conversations and two referees and an associate editor for helpful comments ."
  ],
  "abstract_text": [
    "<S> we add a set of convex constraints to the lasso to produce sparse interaction models that honor the hierarchy restriction that an interaction only be included in a model if one or both variables are marginally important . </S>",
    "<S> we give a precise characterization of the effect of this hierarchy constraint , prove that hierarchy holds with probability one and derive an unbiased estimate for the degrees of freedom of our estimator . </S>",
    "<S> a bound on this estimate reveals the amount of fitting `` saved '' by the hierarchy constraint .    </S>",
    "<S> we distinguish between _ parameter sparsity_the number of nonzero coefficients  and _ practical sparsity_the number of raw variables one must _ measure _ to make a new prediction . </S>",
    "<S> hierarchy focuses on the latter , which is more closely tied to important data collection concerns such as cost , time and effort . </S>",
    "<S> we develop an algorithm , available in the ` r ` package ` hiernet ` , and perform an empirical study of our method .    , </S>"
  ]
}