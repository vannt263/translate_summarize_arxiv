{
  "article_text": [
    "cross - disciplinary fields that combine information theory with statistical mechanics have developed rapidly in recent years and achievements in these have become the center of attention .",
    "the employment of methods derived from statistical mechanics has resulted in significant progress in providing solutions to several problems in information theory , including problems in error correction @xcite , spreading codes @xcite and compression codes @xcite . above all ,",
    "data compression plays an important role as one of the base technologies in many aspects of information transmission .",
    "data compression is generally classified into lossless compression and lossy compression @xcite .",
    "lossless compression is aimed at reducing the size of message under the constraint of perfect retrieval . in lossy compression ,",
    "on the other hand , the length of message can be reduced by allowing a certain amount of distortion .",
    "the theoretical framework for lossy compression scheme is called rate distortion theory , which consists partly of shannon s information theory @xcite .    several lossy compression codes , whose schemes saturate the rate distortion function that represents an optimal performance , were discovered in the case where the code length becomes infinity . for instance , low density generator matrix ( ldgm ) code @xcite and using a nonmonotonic perceptron @xcite were proposed . in these compression codes , a decoder is first defined to retrieve a reproduced message from a codeword . in the encoding problem , for a given source message",
    ", we must find a codeword that minimizes the distortion between the reproduced message and the source message .",
    "therefore , fundamentally , the computational cost of compressing a message is of exponential order of a codeword length .",
    "it is important to understand properties of various lossy compression codes saturating the optimal performance for the development of more useful codes .",
    "since a multilayer network includes a nonmonotonic perceptron as a special case , we employ tree - like committee machine and parity machine as typical multilayer networks @xcite to lossy compression and analytically evaluate their performance .",
    "let us start by defining the concepts of the rate distortion theory @xcite .",
    "let @xmath3 be a discrete random variable with source alphabet @xmath4 .",
    "we will assume that the alphabet is finite .",
    "an source message of @xmath5 random variables , @xmath6 , is compressed into a shorter expression , where the operator @xmath7 denotes the transpose . here",
    ", the encoder describes the source sequence @xmath8 by a codeword @xmath9 .",
    "the decoder represents @xmath10 by a reproduced message @xmath11 , as illustrated in fig .",
    "[ fig : framework ] .",
    "note that @xmath5 represents the length of a source sequence , while @xmath12 represents the length of a codeword .",
    "the code rate is defined by @xmath13 in this case .",
    "a distortion function is a mapping @xmath14 from the set of source alphabet - reproduction alphabet pair into the set of non - negative real numbers . in most cases ,",
    "the reproduction alphabet @xmath15 is the same as the source alphabet @xmath4 .",
    "after this , we set @xmath16 .",
    "an example of common distortion functions is hamming distortion given by @xmath17 which results in the probability of error distortion , since @xmath18=p[y\\ne\\hat{y}]$ ] , where @xmath19 and @xmath20 represent the expectation and the probability of its argument respectively .",
    "the distortion between sequences @xmath21 is defined by @xmath22 .",
    "therefore , the distortion associated with the code is defined as @xmath23 $ ] , where the expectation is with respect to the probability distribution on @xmath4 .",
    "a rate distortion pair @xmath24 is said to be _ achievable _ if there exists a sequence of rate distortion codes @xmath25 with @xmath26 \\le d$ ] in the limit @xmath27 .",
    "we can now define a function to describe the boundary called the _ rate distortion function_. the rate distortion function @xmath28 is the infimum of rates @xmath29 such that @xmath24 is in the rate distortion region of the source for a given distortion @xmath30 and all rate distortion codes .",
    "the infimum of rates @xmath29 for a given distortion @xmath30 and given rate distortion codes @xmath25 is called the _ rate distortion property _ of @xmath25 .",
    "we restrict ourselves to a boolean source @xmath31 .",
    "we assume that the source sequence is not biased to rule out the possibility of compression due to redundancy .",
    "the non - biased boolean message in which each component is generated independently from an identical distribution @xmath32 . for this simple source ,",
    "the rate distortion function for an unbiased boolean source with hamming distortion is given by @xmath33 where @xmath34 called the binary entropy function .",
    "to simplify notations , let us replace all the boolean representations @xmath35 with the ising representation @xmath36 throughout the rest of this paper .",
    "we set @xmath37 as the binary alphabets .",
    "we consider an unbiased source message in which a component is generated independently from an identical distribution : @xmath38 for simplicity .",
    "first let us define a decoder .",
    "we can construct a nonlinear map @xmath39 from codeword @xmath40 to reproduced message @xmath41 . for a given source message @xmath42 , the role of the encoder is to find a codeword @xmath40 that minimizes the distortion between its reproduced message @xmath43 and the source message @xmath10 .",
    "we choose a nonlinear map @xmath44 utilizing tree - like multilayer perceptrons , i.e. , a tree - like committee machine ( committee tree ) and a tree - like parity machine ( parity tree ) .",
    "figure [ fig : tree ] shows its architecture .",
    "the codeword @xmath45 is divided into @xmath46-dimensional @xmath0 disjoint vectors @xmath47 as @xmath48 .",
    "the @xmath49th hidden unit receives the vector @xmath50 .",
    "the outputs of the committee tree and the parity tree are a majority decision and a parity of hidden unit outputs , respectively .",
    "input units and @xmath0 hidden units . ]    the @xmath51th bit of the reproduced message @xmath52 is defined by utilizing the committee tree as @xmath53 where @xmath54 are fixed @xmath46-dimensional vectors and the map @xmath55 is a transfer function .",
    "function @xmath56 denotes the sign function taking 1 for @xmath57 and -1 for @xmath58 .",
    "similarly , the @xmath51th bit @xmath52 of the reproduced message is also defined by utilizing the parity tree as @xmath59 the decoder @xmath44 from the codeword @xmath45 to the reproduced message @xmath60 is described as @xmath61 in this framework , the encoder @xmath62 from the original message @xmath10 to the codeword @xmath45 can be written as @xmath63 with respect to the case of both the committee tree and the parity tree . employing the ising representation , where the length of the codeword is infinite , the average hamming distortion can be represented as @xmath64 = \\sum_{\\mu=1}^m [ 1- \\theta ( y^\\mu \\hat{y}^\\mu ) ] , \\ ] ] where the function @xmath65 denotes the step function taking 1 for @xmath57 and 0 otherwise . since we assume the unbiased source message in this paper , we set @xmath66 .",
    "this encoding scheme is essentially the same as a learning of the multilayer perceptrons because of a following reason .",
    "we first assign the random input vector @xmath67 to each bit of the original message @xmath68 .",
    "the encoder must find a weight vector @xmath45 which satisfies input - output relations @xmath69 as much as possible .",
    "then we use this optimal weight vector @xmath45 as a codeword .",
    "therefore , in a lossless case of @xmath70 , an evaluation of the rate distortion property of these codes is entirely identical to the calculation of the storage capacity @xcite .",
    "we analytically evaluate the typical performance , according to hosaka et al @xcite , for the proposed compression scheme using the replica method .",
    "the minimum permissible average distortion @xmath30 is calculated , when the code rate @xmath29 is fixed . for a given original message @xmath10 and the input vectors @xmath71",
    ", the number of codewords @xmath45 , which provide a fixed hamming distortion @xmath72 , can be expressed as @xmath73 where @xmath74 denotes kronecker s delta taking 1 if @xmath75 and 0 otherwise . since the original message @xmath10 and the input vectors @xmath76",
    "are randomly generated predetermined variables , the quenched average of the entropy per bit over these parameters , @xmath77 is naturally introduced for investigating the typical properties , where @xmath78 denotes the average over @xmath10 and @xmath71 .",
    "we calculate the entropy @xmath79 by the replica method ( see appendix [ appendix.replicamethod ] ) .",
    "the rate - distortion region can be represented by @xmath80 .",
    "therefore , a minimum code rate @xmath29 for a fixed distortion @xmath30 is given by a solution of @xmath81 .",
    "note that a minimum code rate @xmath29 for @xmath70 coincides with a reciprocal of the critical storage capacity of a multilayer perceptron , i.e. , the critical storage capacity @xmath82 can be obtained by @xmath83 .        in the lossy compression",
    "using the committee tree , we obtain average entropy @xmath84 as @xmath85 where @xmath86 with @xmath87 ( see appendix [ appendix.rm.ct.generalk ] ) . for any @xmath0 , we can obtain a minimum code rate @xmath29 , which gives @xmath88 for a fixed distortion @xmath30 .",
    "we concentrate in the following on the simple case of large @xmath0 , where the @xmath0-multiple integrals can be reduced to a single gaussian integral .",
    "we assume that the number of hidden units @xmath0 is large but still @xmath89 . using the central limit theorem ,",
    "the averaged entropy is given by @xmath90 where @xmath91 ^ 2 = \\frac 2{\\pi } \\sin^{-1 } q$ ] and @xmath92 ( see appendix [ appendix.rm.ct.largek ] ) .",
    "figure [ fig : ct ] shows that the limit of achievable code rate @xmath29 expected for @xmath93 plotted versus the distortion @xmath30 for @xmath94 and @xmath1 . for a fixed code rate @xmath29 , the achievable distortion decreases as the number of hidden units @xmath0 increases .",
    "however , it does not saturate shannon s limit even if in the limit @xmath1 . for large @xmath0",
    ", the ea order parameter @xmath95 , which means the average overlap between different codewords , does not converge to zero . since this means that codewords are correlated , the distribution of codewords is biased in @xmath96 .",
    "note that a nonzero ea order parameter does not mean that the reproduced message has a nonzero average due to the random input vector which have a zero average .",
    "expected for @xmath93 plotted versus the distortion @xmath30 for @xmath97 ( dotted line ) , @xmath98 ( short dashed line ) and @xmath1 ( long dashed line ) .",
    "solid line denotes rate - distortion function @xmath28 for binary sequences by shannon . ]      in the lossy compression using the parity tree , on the other hand , we hence obtain averaged entropy @xmath99 as @xmath100 where @xmath101 \\biggr ) .\\ ] ] for cases utilizing a committee tree and a parity tree , only terms @xmath102 and @xmath103 are different .",
    "since both the order parameters @xmath95 and @xmath104 at the saddle - point of ( [ eq : s_pt ] ) are less than one , the average entropy can be expanded with respect to @xmath105 ( < 1)$ ] .",
    "solutions of the saddle - point equation derived from the expanded form of average entropy are obtained as @xmath106 in the case @xmath107 ( see appendix [ appendix.rm.pt ] ) . for @xmath97 ,",
    "@xmath108 holds .",
    "note that for @xmath97 , a parity tree is equivalent to a committee tree . for @xmath107 ,",
    "the order parameter @xmath95 becomes zero , namely all codewords are uncorrelated and distributed all round in @xmath96 .",
    "where @xmath2 , substituting ( [ eq : k>=2ptspe ] ) into ( [ eq : s_pt ] ) , average entropy is obtained as @xmath109 a minimum code rate @xmath29 for a fixed distortion @xmath30 and @xmath2 is given by @xmath110 . solving this equation with respect to @xmath29",
    ", we obtain @xmath111 which is identical to the rate - distortion function for uniformly unbiased binary sources ( [ eq : rdf ] ) .",
    "however , since calculation is based on the rs ansatz , we verify the at stability to confirm the validity of this solution . as the rs solution to lossy compression using a parity tree with @xmath112 hidden units can be simply expressed as ( [ eq : k>=2ptspe ] ) , the stability condition is analytically obtained as @xmath113 where boundary @xmath114 is called the at line ( see appendix [ appendix.atline ] ) . for @xmath115 , the rs solution does not exhibit the at instability throughout the achievable region of the rate - distortion pair @xmath24 .",
    "figure [ fig : pt ] shows the limit of achievable distortion @xmath30 expected for @xmath93 plotted versus code rate @xmath29 for @xmath97 and @xmath107 . in the case @xmath107 , the limit of achievable distortion is identical to the rate - distortion function . the dash - dotted line in fig .",
    "[ fig : pt ] denotes the at line for @xmath112 .",
    "the region above the at line denotes that the rs solution is stable . for @xmath112",
    ", we found that for distortion @xmath116 , @xmath117 can become smaller than @xmath118 .",
    "nevertheless this instability may not be serious in practice , because the region where the rs solution becomes unstable is narrow .",
    "the annealed approximation of the entropy ( [ eq : def_s ] ) gives a lower bound to the rate distortion property .",
    "it coincides with the rate distortion function . according to opper s discussion @xcite",
    ", the entropy ( [ eq : def_s ] ) can be represented by the information entropy formally .",
    "the annealed information entropy can give a upper bound to the rate distortion property .",
    "however , its evaluation is difficult ( see appendix [ appendix.lower_bound ] ) .",
    "expected for @xmath93 plotted versus the distortion @xmath30 for @xmath97 ( dashed line ) and @xmath2 ( solid line ) .",
    "solid line also denotes rate - distortion function , which is identical to limit of achievable distortion for @xmath2 .",
    "dash - dotted line denotes at line for @xmath112 . for @xmath115 ,",
    "rs solution does not exhibit at instability throughout achievable region . ]",
    "it has already been shown that both compression using a sparse matrix and compression using a nonmonotonic perceptron also achieve optimal performance known as shannon s limit @xcite .",
    "all these schemes and compression using a parity tree with @xmath107 hidden units becomes the common ea order parameter @xmath119 . in compression using a nonmonotonic perceptron , the @xmath51th bit of the reproduced message is defined as @xmath120 , where @xmath121 is the transfer function with mirror symmetry , i.e. , @xmath122 @xcite . due to the mirror symmetry of @xmath121 , both @xmath45 and @xmath123 provide identical output for any @xmath124 .",
    "hence , the ea order parameter is likely to become zero .",
    "the transfer function @xmath121 with parameter @xmath125 is defined as taking 1 for @xmath126 and @xmath127 otherwise .",
    "figure [ fig : distribution ] shows the relationship between a codeword and a bit of the reproduced message .",
    "figure [ fig : distribution ] ( a ) is the case of compression using a nonmonotonic perceptron .    in compression using a parity tree , on the other hand , the @xmath51th bit of the reproduced message is @xmath128 for @xmath97 , i.e. , a parity tree is identical to a monotonic perceptron , @xmath129 holds . here , the ea order parameter becomes @xmath108 .",
    "therefore , the distribution of codewords is biased in @xmath96 .",
    "compression using a parity tree with @xmath97 hidden unit can not achieve shannon s limit .",
    "figure [ fig : distribution ] ( b ) shows the case of compression using a monotonic perceptron , i.e. , a committee tree and a @xmath97 parity tree .",
    "however , for an even number of hidden units @xmath0 , a parity tree also has the same effect as mirror symmetry .",
    "we will next discuss the case of @xmath107 .",
    "let @xmath130 be a set of vectors that reversed the signs of an arbitrary even number of blocks of a codeword @xmath131 , e.g. , @xmath132 .",
    "the cardinality of the set @xmath133 is @xmath134 where @xmath135 is the largest integer @xmath136 .",
    "according to ( [ eq : decoder_pm ] ) , all @xmath137 provide identical output for any @xmath138 .",
    "the summation of all @xmath137 becomes @xmath139 this means that @xmath140 vectors with the same distortion as codeword @xmath45 are distributed throughout @xmath96 .",
    "for instance , fig .",
    "[ fig : distribution ] ( c ) shows the distribution of codewords obtained by compression using a @xmath112 parity tree .",
    "the set @xmath96 is divided by two @xmath141-dimensional hyperplanes whose normal vectors are orthogonal to each other . for the @xmath51th bit of the reproduced message , the normal vectors of hyperplanes are @xmath142 and @xmath143 .",
    "figure [ fig : distribution ] ( d ) shows the case of compression using a @xmath98 parity tree . here , although the same effect as mirror symmetry can not be seen , nevertheless , ea order parameter @xmath95 becomes zero for the reason mentioned above .",
    "this situation is the same for @xmath144 .    with respect",
    "to ldgm code @xcite , murayama succeeded in developing a practical encoder using the thouless - anderson - palmer ( tap ) approach which introduced inertia term heuristically @xcite .",
    "the tap approach is called belief propagation ( bp ) in the field of information theory .",
    "hosaka et al applied this inertia term introduced bp to compression using a nonmonotonic perceptron @xcite . in compression using a parity tree with @xmath0 hidden units , the number of codewords which give a minimun distortion is @xmath140 .",
    "therefore , it may become easy to find codewords as the number of hidden units @xmath0 becomes large .",
    "but , in a practical encoding problem , it may not be easy to use a large @xmath0 since @xmath89 is needed .",
    "hidden units .",
    "symbol @xmath145 denotes bit of the reproduced message is @xmath146 and @xmath147 denotes that it is @xmath127 .",
    "set @xmath96 is divided by @xmath0 hyperplanes , whose normal vectors are orthogonal each other . for @xmath107 ,",
    "vectors with same distortion as codeword @xmath45 are distributed throughout @xmath96 .",
    "( a ) a nonmonotonic perceptron , @xmath119 , ( b ) a @xmath97 parity tree , @xmath108 , ( c ) a @xmath112 parity tree , @xmath119 , and ( d ) a @xmath98 parity tree , @xmath119 . ]",
    "we investigated a lossy compression scheme for unbiased boolean messages employing a committee tree and a parity tree , whose transfer functions were monotonic .",
    "the lower bound for achievable distortion in using a committee tree became small when the number of hidden units @xmath0 was large .",
    "it did not reach shannon s limit even in the case where @xmath1 .",
    "however , lossy compression using a parity tree with @xmath2 hidden units could achieve shannon s limit where the code length became infinity .",
    "we assumed the rs ansatz in our analysis using the replica method . in using a parity tree with @xmath2 ,",
    "the rs solution was unstable in the narrow region . for @xmath115",
    ", the rs solution did not exhibit the at instability throughout the achievable region .",
    "there is generally more than one code with the same distortion as a codeword .",
    "the ea order parameter , which means an average overlap between different codewords , need to be zero to reach shannon s limit like several known schemes which saturate this limit .",
    "therefore , it may be a necessary condition that the ea order parameter becomes zero to reach shannon s limit .",
    "since the encoding with our method needs exponential - time , we need to employ various efficient polynominal - time approximation encoding algorithms .",
    "it is under way to investigate the influence of the number of hidden units on the accuracy of approximation encoding algorithms . in future work , we intend to evaluate the upper bound to the rate distortion property without replica .",
    "* acknowledgements *    this work was partially supported by a grant - in - aid for scientific research on priority areas no .",
    "14084212 , and for scientific research ( c ) no .",
    "16500093 , and for encouragement of young scientists ( b ) no .",
    "15700141 from the ministry of education , culture , sports , science and technology of japan .",
    "the entropy @xmath148 can be evaluated by the replica method : @xmath149 a moment @xmath150 , which is the number of codewords with respect to an @xmath151-replicated system , can be represented as @xmath152 where @xmath153 and the superscript @xmath154 denotes a replica index . inserting an identity @xmath155 , \\end{aligned}\\ ] ] into this expression to separate the relevant order parameter . utilizing the fourier expression of kronecker s delta , @xmath156 we can calculate the average moment @xmath157 for natural numbers @xmath151 as    @xmath158 ,   \\label{eq : nn}\\end{aligned}\\ ] ]    where @xmath159 is an @xmath160 matrix having matrix elements @xmath161 and @xmath162h(y)$ ] .",
    "function @xmath163 included in the right hand side of ( [ eq : nn ] ) depends on the decoder ( details are discussed in the following sections ) .",
    "we analyze a system in the thermodynamic limit @xmath164 , while code rate @xmath29 is kept finite .",
    "this integral ( [ eq : nn ] ) will be dominated by the saddle - point of the extensive exponent and can be evaluated via a saddle - point problem with respect to @xmath165 and @xmath166 . here",
    ", we assume the replica symmetric ( rs ) ansatz : @xmath167 where @xmath168 is kronecker s delta taking 1 if @xmath169 and 0 otherwise .",
    "this ansatz means that all the hidden units are equivalent after averaging over the disorder .      in the lossy compression",
    "using the committee tree , the @xmath163 included in ( [ eq : nn ] ) is obtained as @xmath170 therefore , we obtain average entropy @xmath84 as @xmath171 where @xmath172 with @xmath87 . utilizing the fourier expression of the step function @xmath173 ,",
    "the saddle - point equations @xmath174 become @xmath175 where @xmath176 . substituting the solutions to the saddle - point equations into ( [ eq : app.s ] ) , average entropy @xmath84 is obtained .",
    "thus , for any @xmath0 , we can obtain a minimum code rate @xmath29 , which gives @xmath88 for a fixed distortion @xmath30 .",
    "we concentrate in the following on the simple case of large @xmath0 , where the @xmath0-multiple integrals can be reduced to a single gaussian integral .",
    "we assume that the number of hidden units @xmath0 is large but still @xmath89 . here , the term @xmath102 included in ( [ eq : app.s ] ) does not depend on all the individual integration variables @xmath177 but only on the combination @xmath178 $ ] . with the central limit theorem ,",
    "the term is given by @xmath179 \\nonumber \\\\ & & -\\hat{\\lambda}^2 \\biggl ( 1- \\frac 1k \\sum_l [ 2h(qt_l)-1]^2 \\biggr ) \\biggr\\}. \\nonumber \\\\\\end{aligned}\\ ] ] therefore , we obtain averaged entropy as @xmath180 where @xmath91 ^ 2 = \\frac 2{\\pi } \\sin^{-1 } q$ ] and the saddle - point equations are @xmath181 with @xmath92 and @xmath182 .",
    "in the lossy compression using the parity tree , on the other hand , the @xmath163 included in ( [ eq : nn ] ) is obtained as @xmath183 hence , we obtain averaged entropy @xmath99 as @xmath184 where @xmath101 \\biggr ) .\\ ] ] for cases utilizing a committee tree and a parity tree , only terms @xmath102 and @xmath103 are different . since both the order parameters @xmath95 and @xmath104 at the saddle - point of ( [ eq : app.s_pt ] ) are less than one , the average entropy @xmath99 can be expanded with respect to @xmath105 ( < 1)$ ] as @xmath185^{2 m } \\biggr]^k \\biggr\\ } \\nonumber \\\\ & & + \\int du \\ln 2 \\cosh \\sqrt{\\hat{q } } u - \\frac{\\hat{q}(1-q)}2 \\nonumber \\\\ & & + r^{-1}\\beta d \\biggr ) .",
    "\\end{aligned}\\ ] ] we obtain saddle - point equations using this expanded form of the averaged entropy : @xmath186^{k-1 } \\nonumber \\\\ & & \\times \\int dt ( 1 - 2h(qt))^{2m-1 } \\frac{te^{-(qt)^2/2}}{\\sqrt{2\\pi q}(1-q)^{3/2 } } , \\nonumber \\\\ & & \\label{eq : app.q^.pm } \\\\ d & = & \\frac{e^{-\\beta}}{1+e^{-\\beta } } + \\sum_{m=1}^\\infty \\frac{2e^{-\\beta}}{(1+e^{-\\beta})^2 } \\biggl ( \\frac{1-e^{-\\beta}}{1+e^{-\\beta } } \\biggr)^{2m-1 } \\nonumber \\\\ & & \\times \\biggl [ \\int dt ( 1 - 2h(qt))^{2 m } \\biggr]^k .",
    "\\label{eq : app.d.pm } \\end{aligned}\\ ] ] for @xmath107 , because of the existence of term @xmath187^{k-1}$ ] in ( [ eq : app.q^.pm ] ) , solutions to the saddle - point equations can become @xmath188 . we can find no other solutions except for @xmath188 by solving ( [ eq : app.q.pm])-([eq : app.d.pm ] ) numerically for @xmath107 . substituting this into ( [ eq : app.d.pm ] ) , we obtain @xmath189 .",
    "the hessian computed at the replica symmetric saddle - point characterizes fluctuations in the order parameters @xmath190 , @xmath191 and @xmath166 around the rs saddle - point .",
    "instability of the rs solution is signaled by a change of sign of at least one of the eigenvalues of the hessian .",
    "let @xmath192 be the exponent of the integrand of the integral ( [ eq : nn ] ) .",
    "equation ( [ eq : nn ] ) can be represented as @xmath193 we expand @xmath194 around @xmath195 , @xmath95 and @xmath104 in @xmath196 , @xmath197 and @xmath198 and then find up to second order @xmath199 where @xmath200 is the perturbation to the rs saddle - point .",
    "the hessian @xmath201 is the following @xmath202 \\times [ n+kn(n-1)]$ ] matrix : @xmath203 where @xmath160 matrix @xmath204 , @xmath205 matrix @xmath206 and @xmath207 matrices @xmath208 are @xmath209 with @xmath210 for @xmath211 to be a local maximum of @xmath194 , it is necessary for the hessian @xmath201 to be negative definite , i.e. , all of its eigenvalues must be negative .",
    "matrices @xmath212 and @xmath213 contain the quadratic fluctuations of the order parameters in the same and different hidden units , respectively .",
    "because of the block form of @xmath201 , the eigenproblem splits into an uncoupled diagonalization of the two matrices : @xmath214 and @xmath215 the eigenvectors of @xmath214 correspond to fluctuations in directions that break the permutation symmetry ( ps ) .",
    "the eigenvectors of @xmath216 represent fluctuations that do not break this symmetry .",
    "the most unstable mode corresponds to an eigenvector of @xmath216 that breaks the replica symmetry ( rs ) .",
    "we can write the eigenvalue equation as @xmath217 with @xmath218 there are three types of eigenvectors , i.e. , @xmath219 , @xmath220 and @xmath221 @xcite . the first @xmath219 has the form : @xmath222 using the orthogonality of @xmath219 and @xmath220 , the second type of eigenvector @xmath220 has the form : @xmath223 for a specific replica @xmath224 . in the limit",
    "@xmath225 this eigenvector @xmath220 converges to @xmath219 , therefore the eigenvalue of the eigenvector @xmath220 becomes degenerate with @xmath219 s .",
    "similarly , using the orthogonality of @xmath220 and @xmath221 , the third type of eigenvector @xmath221 has the form : @xmath226 for two specific replicas @xmath224 and @xmath51 . in the limit @xmath225 ,",
    "perturbations keep symmetry of the eigenvectors @xmath219 and @xmath220 across the replicas",
    ". therefore , @xmath219 and @xmath220 are irrelevant to replica symmetry breaking ( rsb ) but only determines the stability within the rs ansatz .",
    "hence , the third eigenvector @xmath221 , which is called the replicon mode , causes rsb .",
    "the eigenvalue equation @xmath227 with respect to ( [ eq : app.repliconmode ] ) splits into @xmath228 and @xmath229{\\mbox{\\boldmath $ \\mu$}}_3'=\\lambda_3{\\mbox{\\boldmath $ \\mu$}}_3'$ ] , where @xmath230 .",
    "therefore , the eigenproblem of @xmath216 is equivalent to that of @xmath231 .",
    "let us calculate the elements of @xmath212 and @xmath213 .",
    "the second derivative @xmath194 by @xmath191 related to the @xmath232 is @xmath233 where    @xmath234    for any function @xmath235 .",
    "the second derivative @xmath194 by @xmath166 related to the @xmath236 is @xmath237 where @xmath238 for any function @xmath239 .",
    "the second derivative @xmath194 by @xmath240 related to the @xmath241 is @xmath242 using gardner s method @xcite , we find that the rs stability criterion is @xmath243 where @xmath244 the line @xmath245 is called the at line . setting @xmath246 , on the other hand , the matrix @xmath231 is equal to @xmath214 .",
    "when @xmath246 , inequality @xmath247 of ( [ eq : app.at ] ) always holds .",
    "therefore , permutation symmetry breaking ( psb ) does not occur in this system .",
    "let us consider the rs stability of lossy compression using a parity tree with @xmath112 hidden units . here , @xmath163 is given by @xmath249 , therefore solutions to the saddle - point equations are @xmath250 substituting ( [ eq : app.k>=2ptspe ] ) into ( [ eq : app.pqrpqr ] ) and ( [ eq : app.p^q^r^p^q^r^ ] ) , we obtain @xmath251 therefore , using ( [ eq : app.at ] ) , the rs stability can be obtained as @xmath252 this proves ( [ eq : atline ] ) .",
    "next , let us consider the rs stability of lossy compression using a parity tree with @xmath253 hidden units . here",
    ", the solutions to the saddle - point equations are @xmath254 as well as for @xmath112 . substituting ( [ eq : app.k>=2ptspe ] ) into ( [ eq : app.pqrpqr ] ) and ( [ eq : app.p^q^r^p^q^r^ ] )",
    ", we obtain @xmath255 since the inequality @xmath256 of ( [ eq : app.at ] ) always holds , the rs solution does not exhibit the at instability throughout the achievable region for @xmath115 .",
    "in order to derive a lower bound to the rate distortion property , an upper bound to the entropy is necessary . using jensen s inequality , an upper bound to the entropy @xmath257",
    "is given by @xmath258 after a simple calculation , we obtain the upper bound to the entropy of lossy compression using a parity tree @xmath259 as @xmath260 note that this annealed entropy @xmath261 is not depend on the number of hidden units @xmath0 . solving @xmath262 with respect to @xmath29",
    ", we obtain @xmath263 this shows that the rate distortion function for uniformly unbiased binary sources ( [ eq : rdf ] ) can be also derived as a lower bound to the rate distortion property of compression using a parity tree .",
    "we next discuss a upper bound to the rate distortion property . in order to derive a upper bound to the rate distortion property , we need an lower bound to the entropy . using jensen s inequality , an upper bound to the entropy @xmath257",
    "is represented by @xmath264 this inequality can be also obtained by an annealed information entropy as follows . according to opper s discussion @xcite",
    ", we first define a function that characterizes a version space as follows : @xmath265 since this function @xmath266 is non - negative and normalized to @xmath267 , it defines a probability with respect to @xmath45 .",
    "therefore we obtain the information entropy per bit @xmath268 as @xmath269 where @xmath270 . using the identity",
    "@xmath271 we can easily confirm @xmath272 .",
    "however , it is difficult to evaluate the lower bound @xmath273 directly because @xmath274 .",
    "this difficulty is caused by a limitation of the version space due to the distortion .",
    "this limitation complicates the probability @xmath266 .",
    "12 n. sourlas , nature , * 339 * , 693 ( 1989 ) .",
    "y. kabashima , t. murayama and d. saad , phys .",
    "* 84 * , 1355 ( 2000 ) .",
    "h. nishimori and k. y. m. wong , phys .",
    "e , * 60 * , 132 ( 1999 ) . a. montanari and n. sourlas , eur .",
    "j. b , * 18 * , 107 ( 2000 ) .",
    "t. tanaka , europhys .",
    "* 54 * , 540 ( 2001 ) .",
    "t. tanaka and m. okada , ieee trans .",
    "theory , * 51 * , 2 , 700 ( 2005 ) .",
    "t. murayama and m. okada , j. phys . a : math . gen.,*36 * , 11123 ( 2003 ) .",
    "t. murayama , phys .",
    "e , * 69 * , 035105(r ) ( 2004 ) . t. hosaka , y. kabashima and h. nishimori , phys . rev .",
    "e , * 66 * , 066126 ( 2002 ) .",
    "t. hosaka and y. kabashima , j. phys .",
    "* 74 * , 1 , 488 ( 2005 ) .",
    "t. hosaka and y. kabashima , _ preprint _ , arxiv.org , cs.it/0509086 .",
    "c. e. shannon , bell syst .",
    "tech . j. , * 27 * , 379 ( 1948 ) . c. e. shannon , ire nat .",
    "rec . , * 4 * , 142 ( 1959 ) .",
    "t. m. cover and j. a. thomas , elements of information theory ( wiley , new york , 1991 ) .",
    "e. barkai , d. hansel and i. kanter , phys .",
    "* 65 * , 18 , 2312 ( 1990 ) .",
    "e. barkai and i. kanter , europhys .",
    "lett . , * 14 * , 2 , 107 ( 1991 )",
    ". e. barkai , d. hansel and h. sompolinsky , phys .",
    "a , * 45 * , 6 , 4146 ( 1992 ) .",
    "e. gardner , j. phys .",
    "a : math . gen . ,",
    "* 21 * , 257 ( 1988 ) .",
    "w. krauth and m. mzard , j. phys .",
    "( france ) , * 50 * , 3057 ( 1989 ) .",
    "m. opper , phys .",
    "e , * 51 * , 4 , 3613 ( 1995 ) .",
    "j. r. l. de almeida and d. j. thouless , j. phys .",
    "* 11 * , 5 , 983 ( 1978 ) ."
  ],
  "abstract_text": [
    "<S> [ abst ] statistical mechanics is applied to lossy compression using multilayer perceptrons for unbiased boolean messages . </S>",
    "<S> we utilize a tree - like committee machine ( committee tree ) and tree - like parity machine ( parity tree ) whose transfer functions are monotonic . for compression using committee tree , </S>",
    "<S> a lower bound of achievable distortion becomes small as the number of hidden units @xmath0 increases . </S>",
    "<S> however , it can not reach the shannon bound even where @xmath1 . for a compression using a parity tree with @xmath2 hidden units , </S>",
    "<S> the rate distortion function , which is known as the theoretical limit for compression , is derived where the code length becomes infinity . </S>"
  ]
}