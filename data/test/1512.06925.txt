{
  "article_text": [
    "nearest neighbor search in very large databases is becoming increasingly important in machine learning , computer vision , pattern recognition , and multimedia retrieval along with many applications in document , image , audio , and video retrievals  @xcite .",
    "however , it becomes difficult to efficiently store and search huge collections when the dataset size gets larger and larger ( e.g. millions or even billions ) .",
    "the idea of mapping real - valued vectors to compact codes @xcite provides a very attactive way to solve this problem .",
    "there have been many recent methods on developing hashing methods for compressing data to compact binary strings @xcite , or vector quantization based methods @xcite that compress data to compact codes .",
    "vector quantization ( vq ) was actively studied for source coding / signal compression under real - time constraints dating back to decades ago  @xcite .",
    "recently , the problem of how to apply vq techniques to efficient approximate nearest neighbor ( ann ) search has attracted a lot of attension  @xcite . while many structured vq models can be found  @xcite , in this paper , we restrict our attention to product quantization ( pq ) and residual quantization ( rq ) , which both have been successfully applied to fast nearest neighbor search @xcite .",
    "product quantization works by grouping the feature dimensions into groups , and performs quantization to each feature group .",
    "in particular , it performs a @xmath0-means clustering to each group to obtain sub - codebooks , and the global quantization codebook is generated by the cartesian products of all the small sub - codebooks . in this way",
    ", it can generate a huge number of landmark points in the space , which guarantees low quantization error ; it has achieved state of the art performance on approximate nearest neighbor search  @xcite , and can also provide a compact representation to the vectors .",
    "inspired by the success of pq , some latest works have extended pq to a more general model by finding an optimized space - decomposition to minimize its overall distortion  @xcite .",
    "a very recent work  @xcite has deployed this optimized pq within residual clusters . while it maximizes the strength of locality",
    ", it also uses extra space for multiple transformations as well as pq codebooks .",
    "different from pq , rq works by performing quantization to the whole feature space , and then recursively apply vq models to the residuals of the previous quantization level , which is a stacked quantization model . in particular",
    ", it performs a @xmath0-means clustering to the original feature vectors , and construct @xmath0 clusters . for points in each cluster , it computes the residuals between points and the cluster centers . in the next level",
    ", it aggregates all the residual vectors for all points , and performs another clustering to these residual vectors .",
    "this process is recursively applied ( stacked ) for several levels . in this way",
    ", rq produces sequential - product codebooks .",
    "a comprehensive survey of earlier rq models can be found in barnes et al .",
    "recent works have shown the effectiveness of rq to perform both indexing  @xcite and data compression  @xcite tasks in ann search problems .",
    "however , it has been observed that the effectiveness of rq might be limited to a very small number of levels  @xcite , and the randomness of residual vectors increases very quickly when we stack more and more rq layers .",
    "this results in increasingly noisy residual vectors of each level , and effective quantization at higher rq levels becomes more difficult . in other words ,",
    "the layer - wise learning in rq models provides suboptimal sub - codebooks in that each sub - codebook is learned without consideration to subsequent layers .",
    "thence several global optimization approaches have been proposed by jointly learning sub - codebooks over all layers  @xcite .",
    "different from seeking out joint optimization solutions , we here look for specific transformations to each of the residual clusters to make the resulting vectors more aligned , in order to directly address the problem of cluster misalginment and noise increase over each level after the first and improve overall quantization accordingly .      to this end , we propose a novel approach to optimizing the rq model that is motivated by the noise and shape in the residual space , as shown in figure [ figure : orq - example ] .",
    "we find that the residual vectors for each cluster have significantly different directions , which make quantization at the next level much harder .",
    "a natural idea is to align these residual vectors for each cluster , which can potentially reduce quantization error in the next level .",
    "thus , we propose to learn one rotation matrix for each residual cluster , and use them to align the residual vectors in each cluster , so as to reduce the global quantization error .",
    "then we alternate between learning rotation matrices and the residual quantization to minimize its distortion , which is mainly inspired by `` iterative quantization ( itq ) '' @xcite ; very recently itq has been successfully applied to the pq models  @xcite .",
    "different from itq and the optimized pq ( opq ) , which learn a global rotation , both ours and the locally optimized pq ( lopq )  @xcite learn one projection matrix per residual cluster . in stead of independent learning in lopq ,",
    "however , all local transformations in our method are learned associatively with a shared codebook . in this way",
    ", we successfully reduce the memory overheads about codebook usage in lopq .",
    "this offers a great flexibility to use reasonable larger codebooks in potential .",
    "we have found that by doing this iterative alignment and quantization , we can achieve significantly smaller quantization error than both vanilla rq and opq methods .",
    "in addition , we also propose a hybrid ann search method which is based on the proposed trq and pq .",
    "experimental results on several large - scale datasets have clearly demonstrated the effectiveness of our proposed methods , in particular have shown that the extra transformations only introduced very limited computation overheads when integrated with advanced indexing structures , e.g. the inverted mutli - index  @xcite . therefore , our method is able to achieve very significant gain over other state of the art methods in terms of trade - off among memory usage , search quality and speed .",
    "in this section , we review related background on two types of structured vector quantization , i.e. product quantization and residual quantization . to accurately quantize large number of points in a high dimensional space ,",
    "we need a large number of landmark points ( or centroids ) . for example , if we use the @xmath0-means clustering method to find such quantizers , when the number of cluster centers @xmath0 becomes huge ( e.g. millions or even more ) , performing @xmath0-means becomes prohibitively expensive .",
    "structured vector quantization make special assumptions about the data distribution , and try to explore such structure to generate large number of landmark points to increase the quantization accurateness @xcite .",
    "pq and rq are two structured vq families with different assumptions about data distribution .",
    "we here present discussions on structural codebook constructions and objective function formulations of the related vq models following a brief introduction of the unstructured vq .",
    "vector quantization ( vq a.k.a .",
    "@xmath0-means ) without any structure constraints is probably one of the most widely used vector quantization method . given a dataset @xmath1 ,",
    "vq is a mapping : @xmath2 where @xmath3 is a quantizer and @xmath4 is a _ centroid _ or a _ center _ from the codebook @xmath5 .",
    "according to lloyd s optimality conditions , an optimal quantizer satisfies the nearest neighbor condition : @xmath6 here @xmath7 is the distance between two vectors and euclidean distance is used in this paper .",
    "for each centroid , a set of data points will be assigned to it , and forms a cluster . according to the second optimal condition ,",
    "a centroid is computed as : @xmath8 we estimate the codebook centers to minimize the objective function : the mean squared error ( mse ) @xmath9    while the globally optimal solution of the above problem is np - hard , it can be solved by heuristic alternatives .",
    "the best known approach is the @xmath0-means algorithm  @xcite , in which the above two conditions are optimized alternatively .",
    "product quantization assumes that certain groups of features are independent with each other , and explores this assumption to generate a large number of landmark points by grouping the feature space into @xmath10 groups ( each group is a subspace ) . by quantizing each of the subspaces separately using sub - quantizer @xmath11",
    ", it produces an implicit codebook as a cartesian product of @xmath10 small sub - codebooks , @xmath12 . in this case",
    ", codebook @xmath5 can provide an exponentially large number of cluster centers while retains a linear size of storage .",
    "given a data point @xmath13 , it estimates the globally nearest center by concatenating all its nearest sub - centers from the sub - codebooks as @xmath14 .",
    "the mse of this product quantizer can be estimated as : @xmath15    the degradation of pq performance can be severe if there are substantial statistical dependences among the feature groups  @xcite ; recent works have shown that such dependences could be reduced , to some extent , by more careful space - decomposition  @xcite .",
    "their works extended the idea from iterative quantization  @xcite to the pq scheme that learn an rotation to transform the data to reduce the dependences between feature groups .",
    "for example , the work from @xcite jointly seeks a whole space rotation @xmath16 and pq codebooks @xmath5 by minimizing @xmath17 @xmath18      residual quantization ( rq ) has a different assumption in that it does not assume the features are independent , but assumes the quantization residuals of the first level quantizer can be further quantized .",
    "thus , it is a stacked quantization model . for the first level",
    ", rq simply uses a @xmath0-means clustering to quantize the data , and assign them to @xmath0 centers . for each data point , by subtracting from the assigned centroid , we can collect their residual vectors as @xmath19 .",
    "then @xmath20 will be used as the input to the next level , and we again use @xmath0-means - means is a basic choice . ] to quantize the residual vectors . by repeating this for @xmath21 times",
    ", we can have a sequential product codebook @xmath22 .",
    "given an input vector @xmath13 , by computing the nearest center at each level , we can get a sequence of indexes @xmath23 .",
    "the globally nearest center for @xmath13 here becomes a direct sum of all the sub - centers @xmath24 .",
    "simply considering one level , we represent @xmath13 at level @xmath25 as @xmath26 where @xmath27 .",
    "then the residual vector from level @xmath25 is @xmath28 the mse for a @xmath21-level rq is @xmath29 the most important advantage of rq is that it does not make the unrealistic assumption that the features are statistically independent .",
    "in addition , it holds a non - increasing property , that is , adding a level will always reduce the mse error .",
    "as discussed above , residual quantization recursively performs quantization on residual vectors from previous levels . in other words , all the residual vectors from different clusters are collected and fed into the same quantizer in the next level .",
    "as shown in figure [ figure : orq - example ] ( b ) , the residual clusters can have different shapes , orientations , or scales , which makes effective quantization of them very hard in general . to address this problem",
    ", we propose to learn one rotation per cluster to transform the residual clusters , to make them more aligned to each other , and potentially can make more fruitful residual - level quantizers ( see figure [ figure : orq - example](c ) ) .      for simplicity , we focus our analysis on a two - level rq model .",
    "we fix the first level to be an @xmath0-means clustering with @xmath30 clusters . and at this point , we are not going to specify the second - level quantizer @xmath31 considering our proposed method is general to any quantizer performed in the residual space . in this way",
    ", we have a first - level codebook @xmath32 with @xmath30 cluster centers @xmath33 .",
    "we suppose that a vector @xmath13 is assigned to @xmath34 , its residual vector is denoted as @xmath35    a residual cluster @xmath36 then is a collection of all residual vectors assigned to its center , @xmath37    the residual set @xmath38 forms the input for the next level quantizer @xmath31 . finally , the objective function for such a rq model simply follows either equation  [ eq : rq_mse ] or equation  [ eq : pq_mse ] , depending on whether another @xmath0-means or pq is selected to be the residual quantizer . to alleviate the misalignment of these clusters , we propose to apply cluster - specified transforms right after the first level so that all clusters are aligned for better adaption to the shared residual quantizer .",
    "we associate each cluster center @xmath34 with a transform @xmath39 , whose inverse @xmath40 must exist . accordingly , a transformed residual cluster is then represented as @xmath41    we dubbed the generalized rq model as transformed residual quantization ( trq ) .",
    "while our generalization only requires transforms to be invertible , the property of non - increasing mse error remain valid if transforms are orthogonal matrices , i.e. , @xmath42 , ( where @xmath43 is the identity matrix ) because applying orthogonal matrices does not change the mse error . yet , in this case , scaling inhomogeneity among residual clusters can not be accounted for by the transforms . finally , denoting a reproduction cluster as @xmath44 , and frobenius norm as @xmath45 , the mse for our trq model",
    "is given by @xmath46    0.15 in    .complexity comparison with different vq models , where @xmath10 is the number of subspaces in pq model , @xmath21 is the number of levels in rq and trq models , and @xmath25 is the number of levels having transformations involved in our trq model ; @xmath47 and @xmath48 are operation costs on distance computations and vector transformations respectively . [ cols=\"<,^,^,>\",options=\"header \" , ]     -0.1 in    * parameters *",
    "there are mainly four parameters involved , where @xmath49 and @xmath50 are used to represent the numbers of feature groups for the two different level pq models , while @xmath30 and @xmath51 as the numbers of centers .",
    "we choose @xmath52 and @xmath53 , with @xmath54 and @xmath55 , to be consistent with  @xcite and  @xcite .",
    "since the multi - index has split the features to two groups ( see @xmath56 ) , the system can simply be seen as a concatenation of two independent residual product quantization models , and each of them has @xmath57 and @xmath58 while @xmath59 and @xmath60 are retained . in our case , they are optimized as two trq models .    from now on , we only need to present how one of the trq models works during a search .",
    "note that , we are integrating trq with omulti , so the whole raw space has been transformed by , say @xmath61 , the transformation learnt in omulti .",
    "a query @xmath62 becomes @xmath63 before it probes to the first - level clusters .",
    "we split @xmath64 equally to two segments , @xmath65 . here",
    "we show only what happens to @xmath66 ; it then will be probed into the left subspace .",
    "if it is assigned to the sub - center @xmath67 , in our setting , the residual for @xmath66 is @xmath68 .",
    "finally , the concatenation @xmath69 are used for re - ranking pq - codes on the shortlist that is provided by the indexer omulti .",
    "* computation - wise discussion * a potential concern might be centered on the extra computational and storage costs required by the proposed trq model .",
    "trq applies transformations in the residual spaces and aims to improve the quantization quality and furthermore the search accuracy in the ann problem .",
    "it slightly increases memory usage due to the local transformations . in the setting for table [ table : sift1b - result ] , for instance",
    ", it requires about 2.5% extra memory space for all the transformation matrices relative to pq or opq model .",
    "this is acceptable with the substantially increased search accuracy considered .",
    "even though our presented results have been restricted to comparisons with pq and opq , we here give a short discussion about comparison between our method with lopq  @xcite considering the common feature of the locally learned transformations .",
    "lopq learn different pq codebooks for each cluster while our trq share just one pq codebook among all clusters .",
    "this results in lopq use much more extra memory than trq dose . in the same",
    "setting as table [ table : sift1b - result ] lopq produce 16384 pq codebooks that takes 2 gb ( about 10% ) extra memory compared to ours and other methods . taking advantage of the considerable memory usage , lopq",
    "have achieved about 2% higher recall than our trq on this sift1b dataset .",
    "computationally , the number of transformations or matrix multiplications in both trq and lopq for each query is determined by the number of visited cells from the first level index ; it is usually less than 100 out of 16384 ( in table 2 ) .",
    "the results in table [ table : sift1b - result ] show that the extra computational costs can be well justified given the significant improvements on search accuracy .",
    "we also use only 10 millions of the training points to speed up the codebooks learning in both of opq and trq .",
    "all the results have been produced using a single thread with an intel(r ) xeon(r ) cpu @ 3.40ghz with 25.6 m cache size , and 128 gb ram .",
    "all methods have been implemented in matlab and c ( for codebook learning and database encoding ) and c++ ( for searching ) .",
    "product quantization and residual quantization are the well - known quantization models that reduce both computation and storage complexities .",
    "we have seen that different ways to optimize these models are becoming active and fruitful in the computer vision area . in this paper , we have presented a novel approach to reducing quantization error of residual quantization for fast nearest neighbor search .",
    "we have also presented two hybrid searching architectures that are based on product quantization and the proposed transformed residual quantization .",
    "our experiments show that the proposed trq has achieved significantly smaller quantization error than previous methods , which clearly demonstrates the benefits of the proposed model .",
    "also , in large - scale ann experiments on 1 billion vectors , our method has achieved significantly better accuracy than previous methods .    despite",
    "that we have restricted our attention to the ann search problem in this paper , the approach can be extensively used for many other problems that require efficient and effective very large - scale clustering . for examples , @xmath0nn graph construction , and feature matching for image retrieval , and for object recognition to name a few .",
    "in addition , how to integrate our transformation - based approach with the global codebook learning approaches arises to be a very interesting question for residual quantization optimization .",
    "these become our future research directions ."
  ],
  "abstract_text": [
    "<S> the success of product quantization ( pq ) for fast nearest neighbor search depends on the exponentially reduced complexities of both storage and computation with respect to the codebook size . </S>",
    "<S> recent efforts have been focused on employing sophisticated optimization strategies , or seeking more effective models . </S>",
    "<S> residual quantization ( rq ) is such an alternative that holds the same property as pq in terms of the aforementioned complexities . </S>",
    "<S> in addition to being a direct replacement of pq , hybrids of pq and rq can yield more gains for approximate nearest neighbor search . </S>",
    "<S> this motivated us to propose a novel approach to optimizing rq and the related hybrid models . with an observation of the general randomness increase in a residual space </S>",
    "<S> , we propose a new strategy that jointly learns a local transformation per residual cluster with an ultimate goal to reduce overall quantization errors . </S>",
    "<S> we have shown that our approach can achieve significantly better accuracy on nearest neighbor search than both the original and the optimized pq on several very large scale benchmarks . </S>"
  ]
}