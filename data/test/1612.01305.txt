{
  "article_text": [
    "the epoch - making detections of the transient gravitational - wave ( gw ) signals gw150914 and",
    "gw151226 during the first observing run of the two advanced ligo interferometers  @xcite has marked the start of gw astronomy .",
    "gw150914 , detected with unexpectedly high signal - to - noise ( snr ) ratio ( snr@xmath024 ) and with a statistical significance greater than @xmath1 , is in excellent agreement with numerical relativity waveforms  @xcite for the final few cycles ( chirp ) , merger ( burst ) and subsequent ringdown of the coalescence of two stellar - origin black holes ( bhs ) in a binary system .",
    "gw151226 , also the result of a binary bh merger , was recovered with similar statistical significance but with a snr@xmath013 .",
    "its initial bh masses , @xmath2 and @xmath3 , are lower than in the case of gw150914 , @xmath4 and @xmath5 . as a result",
    ", gw151226 spent almost 1 s in the ligo frequency band , increasing in frequency and amplitude from 35 to 450 hz over about 55 cycles .",
    "contrary to gw150914 , matched filtering with waveform templates from general relativity was essential to detect gw151226 due to the smaller strain amplitude and the longer time interval  @xcite .",
    "at present , the two advanced ligo interferometers are being upgraded .",
    "the second observing run ( o2 ) is expected to start in late 2016 with a significant strain improvement . at the same time , the commissioning of the european detector advanced virgo  @xcite is well underway , aiming at start observing in the second half of this year , while the japanese detector kagra  @xcite is still under construction",
    ". simultaneous observational campaigns of these four detectors , five with the later addition of the recently approved ligo india , will increase considerably the rate of detections along with their statistical significance and the accuracy of the sky location of each event  @xcite .    despite the recent discoveries , noise removal remains one of the most challenging problems in gw data analysis .",
    "there exist a number of noise sources that limit the possibilities of detection  @xcite .",
    "the most limiting source of noise for frequencies below a few tens of hz is gravity gradient noise .",
    "thermal noise due to brownian motion is dominant at intermediate frequencies , while shot noise , produced by quantum fluctuations of the laser , becomes prominent at frequencies above @xmath6 hz , difficulting detection above 2 khz .",
    "nevertheless , searches for gravitational wave bursts up to frequencies of 5 khz have been performed  @xcite .",
    "these sources of noise are not stationary and the sensitivity of the detectors changes with time .",
    "to add more complexity , transient spurious noise signals ( glitches ) due to instrumental or environmental sources , may potentially disturb astrophysical signals .",
    "glitches might mimic gw signals increasing the false alarm rate and producing a decrease in the detectors duty cycles . a huge effort in commissioning and detector characterization",
    "@xcite has been done to reduce the effect of glitches . improving glitch identification and classification",
    "@xcite would improve detection efficiency but there will always be a chance for false positives in the detectors .",
    "gw detectors are designed to be sensitive to waveforms produced by different astrophysical mechanisms .",
    "sources can be separated in groups depending on how well - known and modeled their waveforms are .",
    "specific data analysis techniques have been developed for each type of signal ( for a review see  @xcite and references therein ) .",
    "transient gw signals from compact binary coalescence ( cbc ) , either from binary neutron stars ( bns ) or binary black holes ( bbh ) , are well studied and the corresponding waveforms can be calculated with high accuracy .",
    "these systems are typically modeled using the effective - one - body formalism ( eob ) @xcite which combines post - newtonian methods @xcite with numerical relativity and perturbation theory @xcite . this technique allows to generate template banks efficiently .",
    "this is the main reason matched - filtering is the most common method for cbc detection  @xcite in which filters correlate signals with templates .",
    "a trigger associated with a specific template is generated when the filter output excess a certain threshold .",
    "in addition to eob waveforms , other waveform families are needed to cover as much parameter space as possible ( see  @xcite and references therein ) .",
    "matched - filtering becomes however unpractical for well - modeled but continuous sources , like spinning neutron stars , due to the large computational resources it would require .",
    "nevertheless , as such signals are very stable and have long duration , a coherent integration can be performed .",
    "in addition , the data from all detectors can be compared , which increases the snr of this type of events .",
    "roughly speaking there are two main methods to fulfill this comparison , cross - correlation methods and coherent methods  @xcite .",
    "the former directly compare the data streams from a pair of detectors to search for a common signal within uncorrelated noise while the latter generalize the concepts of excess power and cross - correlation to take full advantage of having three or more data streams .",
    "the duration and the sky coverage ( all - sky or targeted )  @xcite can vary depending of the type of source which is sought for .",
    "in contrast with the last type of sources , the non - spherical gravitational collapse of massive stars produces a short ( @xmath0ms ) duration ( prompt ) signal ( but see  @xcite for the case of collapsars where the duration of the signal is dominated by the accretion timescale , considerably longer ) with a significant power in the khz frequency band .",
    "in addition to core - collapse supernova , other astrophysical sources as cosmic string cusps  @xcite and bbh mergers , can also produce gw transients or  bursts \" . such signals , in particular core - collapse bursts , can only be modeled imperfectly , and the computational requirements for obtaining the corresponding waveforms from numerical relativity simulations are much larger than in the case of cbc .",
    "therefore , a bank of templates can not be built with sufficient accuracy to meet the requirements of matched - filtering .",
    "for burst signals , the time - frequency analysis of the signal in all the detectors , related to each other with cross - correlation and coherent methods , is the best option . with initial detectors , a complete all - sky ,",
    "all - time burst search was performed  @xcite and has also been carried out during advanced ligo s first observing run ( o1 )  @xcite .",
    "these techniques , used in tandem with electromagnetic observations , can increase the possibilities of identifying a gw burst @xcite .",
    "the detection confidence of unmodeled astrophysical sources has significantly improved in recent years .",
    "in particular , coherent approaches over a network of gw detectors have proven to be very effective  @xcite , increasing the detection confidence of long - duration ( above several seconds ) burst signals which are insensitive to the presence of most noise transients .",
    "in contrast , short - duration bursts are more affected by detector glitches and specific pipelines based on bayesian inference have been developed to differentiate between signals and noise transients , namely coherentwaveburst  @xcite , bayeswave  @xcite , and olib  @xcite .",
    "other approaches , like those of  @xcite , have proven to be effective for estimating physical parameters and for the reconstruction of burst signal waveforms from ( gaussian ) noisy environments .",
    "apart from the standard techniques mentioned in the previous paragraphs , gw data analysis can benefit from the incorporation of new approaches from other fields .",
    "recently , we presented in  @xcite new methods for denoising gw signals based on @xmath7-norm minimization , modeling the denoising problem as a variational problem .",
    "these methods have been originally developed and fully tested in the context of image processing where they have been shown to be the best approach to solve the so - called rudin - osher - fatemi denoising model  @xcite .    in this paper",
    "we continue the work initiated in  @xcite .",
    "we assume the linear degradation model to solve the denoising problem as the estimation of the recovered signal @xmath8 from the relation @xmath9 where the measured signal is @xmath10 and @xmath11 is white gaussian noise of zero mean .",
    "our approach in  @xcite consisted in obtaining @xmath8 as the unique minimizer of the total variation norm of the signal subject to a fidelity term expressed in terms of the @xmath12-norm of the residual , i.e. , @xmath13 .",
    "here , we propose an alternative approach based on the sparse reconstruction of the signal @xmath8 over learned dictionaries , built from existing waveform catalogs .",
    "the development of models and algorithms for sparse reconstruction of signals over a dictionary has been a subject of great interest in recent years  @xcite .",
    "it appears as an alternative to the traditional signal representation based on fourier decomposition or more modern representations based on wavelets , chirplets , warplets , etc .",
    "following the terminology introduced by mallat and zhang  @xcite a _ dictionary _ is a collection of signals ( in our case waveforms ) of length @xmath14 called _ atoms _ , @xmath15 where @xmath8 is the signal to be recovered , @xmath16 $ ] is the dictionary , which is composed of @xmath17 atoms of length @xmath14 , and @xmath18 is a vector which contains the coefficients of the representation .",
    "the atoms are not set to be orthogonal unlike in other decompositions like those based in principal component analysis ( pca ) , allowing more flexibility in the representation .",
    "a dictionary can be complete , if it contains exactly @xmath19 atoms , or overcomplete if @xmath20 atoms . in the latter case",
    "the solution vector @xmath21 is not unique and can not be obtained by applying simple linear methods . in our work ,",
    "we use the _ basis pursuit decomposition _ proposed in  @xcite .",
    "an interesting review of other approaches to solve problem ( [ eq : dict_decomp ] ) can also be found in ref .",
    "@xcite .",
    "the prototype signals of a dictionary can be chosen as a predefined set of functions , like a fourier basis ( frequency dictionaries ) , several types of wavelet functions ( wavelet dictionaries ) or gabor wavelet decomposition to produce time - frequency dictionaries .",
    "however , the idea of using a dictionary learned from data has improved the denoising results considerably  @xcite .",
    "nowadays , state - of - the - art algorithms for reconstruction and denoising are being developed along this direction  @xcite , and very efficient methods have been devised to solve the challenging optimization problem inherent to learning dictionaries .",
    "the sparse reconstructions of signals over trained dictionaries we propose in this work are obtained for the same kind of gw trained signals that we did in  @xcite , namely burst signals from a catalog of rotational stellar core collapse  @xcite and chirp - burst - ringdown signals from a catalog of bbh mergers  @xcite .",
    "once the dictionaries are set , we demonstrate their utility for the denoising of gw signals embedded in gaussian noise .",
    "this paper is organized as follows : section  [ section : math ] describes the basic mathematical details of our method , namely the sparse representation and the dictionary learning problems together with the specific formulation we use to solve them . section  [ section : gw ] deals with the gw waveform catalogs we employ to assess our method . in section",
    "[ section : parameter_estimation ] we adapt the general problem to the specific case of gw signals and we obtain the optimal set of model parameters to perform the denoising of given signals . in sections  [ section : application ] and  [ section : other ] we illustrate our technique with a significant sample of test cases .",
    "section  [ section : gw150914 ] discusses the performance of our method when applied to the actual signal gw150914 .",
    "finally , the conclusions of our work and possible future extensions are presented in section  [ section : summary ] .",
    "appendix a contains a table with the correspondence between the naming of the gw signals employed in this study and that of the original gw catalogs .",
    "we start from the linear degradation model @xmath22 where @xmath23 is the measured signal , @xmath24 is the gw signal to be recovered and @xmath25 is random gaussian noise .",
    "if signal @xmath24 is a vector @xmath26 one can say that it admits a sparse approximation over an overcomplete dictionary @xmath27 , where each column contains one of the @xmath17 atoms of length @xmath14 and @xmath20 , when one can find a linear combination of a _ few _ atoms from @xmath28 that is close to @xmath24 .",
    "the classical dictionary learning techniques @xcite try to solve the variational problem associated with eq .",
    "( [ eq : dict_decomp ] ) given by , @xmath29 where @xmath30 is the @xmath31-norm to assure that we have the solution with the fewest number of nonzero coefficients .",
    "the @xmath31-norm is just the number of nonzero components of the vector .",
    "this constrained variational problem can be formulated as an unconstrained variational problem adding the @xmath31-norm term as a penalty term weighted by a lagrangian multiplier @xmath32 , @xmath33 this problem is not convex and is np - hard ( i.e.  non - deterministic polynomial - time hard ) so , in practice , it can not be solved in linear time  @xcite .",
    "a problem is in the np class if it can be solved in non - deterministic polynomial - time .",
    "algorithms that produce and approximate solutions to this problem have been proposed in the past .",
    "the simplest ones are matching pursuit ( mp ) and orthogonal matching pursuit ( omp ) ( see  @xcite and references therein for details ) .    the variational problem defined by eq .",
    "( [ eq : l0-problem ] ) can be reformulated into a convex variational formulation by substituting the @xmath31-norm by the nondifferentiable convex @xmath7-norm in the total energy .",
    "the regularization in the @xmath7-norm promotes zeros in the components of the vector coefficient @xmath21 .",
    "this problem can be solved in linear time and the solution found is the sparsest one in most cases .",
    "the variational problem thus stands as , @xmath34 which is known as _ basis pursuit _",
    "@xcite or _ lasso _  @xcite . an alternative formulation known as",
    "elastic - net _",
    "@xcite , @xmath35 adds a @xmath12-norm penalty for stability reasons , i.e. , the calculated approximate representation depends on the data as a lipschitz function .    to solve the lasso problem ( [ eq : lasso ] ) we propose a decomposition based on the split - bregman ( sb ) algorithm @xcite .",
    "the sb algoritm solves very efficiently @xmath7-norm minimization problems , like the rudin - osher - fatemi variational model @xcite , decoupling the @xmath7-norm and @xmath12-norm terms and solving them alternatively until convergence is reached . in order to achieve this goal",
    "we introduce a new independent unknown vector @xmath36 to split the minimization with respect to the @xmath7-norm . applying this splitting",
    "the problem reads as follows @xmath37 by iteratively minimizing with respect to @xmath21 and @xmath38 separately , the sb iterative procedure reads as follows @xmath39 starting with @xmath40 .",
    "the role of the auxiliar vector @xmath41 is to enforce the unknowns @xmath42 and @xmath21 be equal when convergence is reached .",
    "the iteration process uses a small positive fixed value of @xmath43 and it runs over a scale - space that reconstructs the signal as a linear combination of few elements of the dictionary .",
    "since the two parts are decoupled , they can be solved independently .",
    "the energy of the first step is smooth ( i.e.  differentiable ) and it can be solved using common techniques as the gauss - seidel method .",
    "on the other hand , @xmath36 can be explicitely computed to optimal values by using the shrinkage operator , @xmath44@xmath45 in practice we only use one iteration for the splitting steps and the final algorithm only consists of just one loop ( see @xcite for a detailed discussion ) .      up to this point",
    "we have considered that the dictionary @xmath28 is fixed and we only have to solve the problem of representation . as we want to design the dictionary to fit a given set of gw signals , we start by considering a finite number of training signals , which can be split in @xmath46 patches of length @xmath14 , i.e.  @xmath47 $ ] in @xmath48 . in most common problems ,",
    "the number of training patches @xmath46 is large compared with the length of each patch , @xmath49 . in general , the number of atoms in the dictionary is lower than the number of patches , @xmath50 , because each signal only uses a few elements in @xmath28 for the representation .    to obtain the trained dictionary , we need to add the dictionary matrix @xmath28 as a variable in the minimization problem @xmath51 where the summation index @xmath52 indicates the i - th row of @xmath53 ( now a matrix ) , which contains the coefficients of the sparse representation of each atom in the dictionary .",
    "the constraint in @xmath28 reads @xmath54 the whole problem ( [ eq : dict_learning ] ) is not jointly convex , but convex with respect to either of the two variables , @xmath55 , keeping the other one fixed . to perform the dictionary update",
    "we follow the algorithm proposed by mairal et al in  @xcite to which the reader is addressed for details .",
    "these authors use a block - coordinate descent method @xcite for solving for @xmath28 and @xmath56 iteratively , @xmath57 the main advantage of this implementation is that it is parameter - free and does not require any learning rate .",
    "in the present work we employ the same two catalogs of gw signals used in  @xcite to assess our method , namely a catalog of signals from relativistic rotational core collapse simulations  @xcite and a catalog from bbh simulations  @xcite .",
    "in addition , we also consider two extra signals , one from a core collapse catalog developed by abdikamalov et al .",
    "@xcite and a bbh signal from  @xcite .",
    "these last two signals allow us to investigate the ability of our approach to extract independent waveforms using dictionaries built from atoms that do not contain explicit information on the signals to be denoised .         in the core collapse scenario the bulk of gravitational radiation",
    "is emitted during bounce , when the quadrupole moment changes rapidly , which produces a burst of gws with a duration of about 10 ms and a maximum dimensionless amplitude of about @xmath58 at a distance of @xmath59 kpc .",
    "broadly speaking , gw signals from core collapse exhibit a distinctive morphology characterized by a steep rise in amplitude to positive values before bounce followed by a negative peak at bounce and a series of damped oscillations associated with the oscillations of the newly formed proto - neutron star around its equilibrium solution .",
    "we employ the catalog developed by dimmelmeier et al .",
    "@xcite , who obtained 128 waveforms from general relativistic simulations of rotating stellar core collapse to a neutron star using the coconut code .",
    "the core collapse template bank computed by  @xcite has also been built through axisymmetric simulations with the coconut code .",
    "the progenitors investigated have different initial angular momentum distributions in the core and the simulations include a microphysical finite - temperature equation of state , an approximate electron capture treatment during collapse , and a neutrino leakage scheme for the postbounce evolution .    regarding bbh signals we consider the bbh waveform catalog of mrou et al  @xcite which includes the late inspiral , merger , and quasi - normal mode ringdown signals for 174 different models .",
    "those waveforms have been computed using the spectral einstein code ( spec )  @xcite .",
    "in addition we employ the ` r1 ' bbh waveform computed by the gsfc group  @xcite and available at  @xcite .",
    "this waveform includes the late inspiral and merger phases of an irrotational bbh simulation performed with a grid - based numerical code , conformally flat initial data and the bssn system of equations .",
    "the numerical code and techniques are hence different to those used by  @xcite .",
    "we now turn to describe the process to generate a learned dictionary from the waveforms of both catalogs .",
    "the goal is to find the best set of dictionary parameters that produce the best denoising results .",
    "as in  @xcite we find that the results depend critically on the value of the regularization parameter @xmath32 selected .",
    "the way we build the dictionaries for the burst and bbh catalogs is similar .",
    "we divide randomly in three groups both the 128 burst waveform signals of  @xcite and the first 100 bbh waveforms of  @xcite .",
    "since the bbh signals of  @xcite are quite large , we do not use the entire bbh catalog in order to save computational resources .",
    "specifically , the bbh catalog covers binaries with total mass 20@xmath60 and mass ratios up to 1:8 , and so does our dictionary .",
    "we then use in either case @xmath61 of the waveforms for training the dictionary , @xmath62 for validation of the method , i.e.  to search the best set of parameters , and the remaining @xmath63 to test the algorithm in different situations .",
    "the numerically generated signals are embedded in non - white , gaussian noise corresponding to advanced ligo proposed broadband configuration , provided by the lsc algorithm library suite ( lal )  @xcite .",
    "the frequency ranges from 10 hz to 8192 hz ( one - sided spectrum ) .",
    "first of all , we resample the waveforms of both catalogs to the advanced ligo / virgo sampling rate of @xmath64 hz , zero padded to have the same length .",
    "the corresponding signals are also shifted to be aligned with either the minimum peak in the case of bursts or with the maximum peak in the merger part for bbh signals .",
    "we select @xmath65 samples around the corresponding alignment points to train the dictionary . with this length , the waveforms of the burst catalog fit completely in the window , while only the last cycles of the inspiral , merger and ringdown of the bbh waveforms are taken into account to perform the denoising .",
    "this late part of the bbh signal is arguably the most interesting part , hence deserving to be denoised best .",
    "below we comment on the reason for this choice and on existing alternatives to also reconstruct the early inspiral part of the signal .    to ensure the best conditions for the convergence of the algorithms and to avoid round - off errors",
    ", we also scale the amplitude of the validation signals of both catalogs so that their maximum value is set to unity .",
    "the values of the regularization parameter @xmath32 we discuss in this section are hence determined by this normalization .",
    "moreover , we scale each signal to achieve a specified value of the snr , defined as @xmath66 where @xmath67 indicates the fourier transform of signal strain @xmath68 , @xmath69 is the power spectral density ( psd ) of the noise , i.e.  the sensitivity curve of the detector , @xmath70 is each of the components of the frequency vector , @xmath71 is the number of positive frequencies , and @xmath72 and @xmath73 are the time step and frequency step , respectively .    the optimal value of the regularization parameter , @xmath74 , is defined to be the one which gives the best results according to a suitable metric function applied to the denoised signal and the original one , measuring the quality of the recovered signal . in our case",
    "we choose two estimators , namely the mean squared error , @xmath75 where @xmath76 and @xmath77 are the reconstructed and original signals , respectively , and @xmath14 is the number of samples , and the structural similarity ( ssim ) index  @xcite , which deviates from the traditional measures of error because it takes into account the structural information . the ssim index varies between 0 ( minimum similarity ) and 1 ( maximum similarity ) and is defined as @xmath78 where @xmath79 and @xmath80 are constants , @xmath81 ( @xmath82 ) is the average of @xmath83 ( @xmath84 ) , @xmath85 ( @xmath86 ) the variance of @xmath83 ( @xmath84 ) and @xmath87 the covariance of @xmath83 and @xmath84 .",
    "as mentioned before we use @xmath61 of the signals of each catalog to produce one dictionary per type of signal . to do the learning , we select @xmath88 random patches ( the starting sample is random ) of a selected length , which is a parameter to be estimated . the patches are selected uniformly from all the learning waveforms of each catalog .",
    "then , we select the @xmath17 patches with the highest energy , defined as the square of the @xmath12-norm of each patch . after that",
    ", we solve problem ( [ eq : dict_learning ] ) using a block - coordinate descent method .",
    "this step is done modifying the code developed by @xcite .",
    "[ fig : dictionary ] shows a small representation of the atoms of both dictionaries .",
    "in addition to the search of @xmath74 we must decide the best values for the size of the dictionary , i.e.  the number of atoms and their length . to this aim we calculate the mse for the reconstructed signals obtained using dictionaries of different sizes . in each case",
    ", the value @xmath89 will be the corresponding value that minimizes the mse .",
    "for this task we use the validation set of signals of the dictionaries and set the snr to @xmath90 . as",
    "the length of the atoms is always shorter than the length of the validation signals , we do the denoising with a sliding window with an overlap of @xmath91 samples , where @xmath14 is the length of the window , which agrees with the length of the atoms . with this overlap",
    ", there are many samples that are repeated on different windows .",
    "these samples must be averaged to obtain the final reconstructed signals .",
    "our initial tests show that the best reconstruction is achieved using tv - averaging ( see  @xcite ) ,    @xmath92    where @xmath93 corresponds to the current patch and @xmath94 is the total - variation norm of that patch .",
    "we calculate the mse for the validation set of burst signals with window lengths @xmath95 , 128 , 256 , and 512 .",
    "the results are shown in the left panel of fig .",
    "[ fig : err_burst ] .",
    "each vertical bar represents the value of the mse for each atom length .",
    "the figure shows that the largest value of the mse is achieved for a length of @xmath96 .",
    "this is due to the fact that if the atoms are too short the reconstructed signal is more oscillatory due to the noise .",
    "this effect can be corrected using larger lengths",
    ". however , the larger the length of the atoms the more difficult to recover the smallest oscillations of the original signal .",
    "this is the reason why the mse actually grows for the larger window length analyzed ( @xmath97 samples ) . while this is a generic trend , it is nevertheless still possible that the longest window may work better for specific signals ( e.g.  signals # 6 or # 11 in fig .",
    "[ fig : err_burst ] ) .",
    "however , in general the best results correspond to a length of @xmath98 samples .",
    "the right panel of fig .",
    "[ fig : err_burst ] displays the values of @xmath89 , i.e.  the value of the regularization parameter that minimizes the mse value .",
    "it has been obtained for a fixed window length @xmath99 .",
    "this figure reveals that the values of @xmath32 are bounded between @xmath100 and @xmath101 . therefore ,",
    "not all values of @xmath32 are possible and selecting the mean value @xmath102 will produce , on average , a good reconstruction for all burst signals .",
    "nevertheless , fine - tuning this parameter can improve the results in specific cases .",
    "we next carry out the same analysis for the case of bbh signals .",
    "as bbh waveforms are totally different to burst signals , the choices just discussed for bursts would not lead to satisfactory results if applied blindly to the bbh catalog .",
    "contrary to burst signals , bbh waveforms are significantly longer , therefore we need to increase the length of the atoms .",
    "the values obtained for the mse for the bbh catalog are shown in the left panel of fig .",
    "[ fig : bbh_error ] and correspond to atom lengths that comprise from @xmath103 to @xmath104 samples .",
    "the case of @xmath96 samples is not shown in the figure because the corresponding value of the mse is much larger .",
    "as for the case of burst signals , fig .",
    "[ fig : bbh_error ] shows that the mse decreases with the window length in most cases .",
    "therefore , to denoise bbh waveform signals we select the length of @xmath104 samples as it produces the best results .",
    "the corresponding results for the value of @xmath89 for bbh are shown in the right panel of fig .",
    "[ fig : bbh_error ] . again",
    ", the values are restricted to a small interval between @xmath105 and @xmath106 . as we show below , using the mean value , @xmath107 , yields to satisfactory denoising results in most cases .",
    "a similar study is required to determine how the results depend on the _ number _ of atoms of the dictionary @xmath17 .",
    "in general , the larger the dictionary the better the results , but at a higher computational cost . therefore",
    ", setting the size of the dictionary is often a trade - off between results quality and efficiency . to evaluate an optimal value for the number of atoms we carry out tests with the two catalogs using values from @xmath108 in the case of bursts with @xmath109 atom length and from @xmath110 in the case of bbh with @xmath111 atom length .",
    "we find that using @xmath112 and @xmath113 atoms for bursts and bbh , respectively , is a valid compromise as it produces good results at a reasonable computational cost .",
    "however , if computational resources are not an issue , there is no reason not to use larger dictionaries . for the two catalogs ,",
    "the value of @xmath89 for @xmath114 and @xmath115 atoms are bounded in a similar interval than shown before .",
    "the first test consists in studying the performance of the method when there is no signal inside the data set .",
    "the goal of this test is to check if in the absence of signal the dictionary produces spurious signals due to noise .",
    "the result of this test is shown in fig .  [",
    "fig : no_signal ] .",
    "a stream of @xmath116 s of pure non - white gaussian noise ( upper panel ) is denoised using the generic value of @xmath74 corresponding to burst signals , i.e.  @xmath117 .",
    "one can see that the resulting signal has zero amplitude throughout the frame ( lower panel ) for this specific value of @xmath32 .",
    "this is the ideal behavior of the algorithm in order to avoid false detections due to noise .",
    "we next repeat this test for 200 independent realizations of noise ( following the procedure outlined in appendix a of  @xcite ) to check if this behavior remains the same irrespective of the noise realization . for our specific value of @xmath32",
    "we find 26 false reconstructions due to noise fluctuations .",
    "we note however that the smaller the @xmath32 the more coefficients of the representation become nonzero and more structures due to noise may appear .",
    "in contrast , a large value of @xmath32 will reduce the ratio of false reconstructions , even though a true gw signal with low snr could be missed .",
    "for instance , for @xmath118 we only obtain one false reconstruction .",
    "the results reported in this section are illustrative of the typical response of the lasso algorithm on @xmath32 . a comprehensive statistical study of the dependence of the number of false reconstructions and signal misses on the parameters of the method , i.e.  value of @xmath32 , type of signal injection , snr , and noise realization , deserves further analysis .",
    "we also note that this is a fairly simple test because the noise is purely gaussian . in a more realistic scenario ,",
    "the presence of instrumental glitches in the detector data  @xcite could produce false reconstructions .",
    "next we study how the method works when applied to the eight test waveform signals of the burst catalog in a long data frame . in the figures for this and the following tests , we use the same noise realization to compare the results on an equal footing .",
    "correspondingly , in the tables reported in this section , we present results obtained with 20 different noise realizations to find out how the reconstruction is affected by noise fluctuations .",
    "the signals are embedded in gaussian noise with a snr of @xmath90 .",
    "the time of arrival is fixed and it is the same for all the signals .",
    "the value of the regularization parameter is set to @xmath119 and remains the same value for all the tests of this section .",
    "although this value is not the optimal one , i.e.  the one which produces the best results for a given signal , our goal is to determine if it is possible to recover the signal with a generic value of @xmath32 . this approach may be closer to what occurs in a realistic situation , where no information on the signal is available a priori .",
    "the quality of the denoising is measured using the mse and the ssim metric functions , and is reported in table [ table : burst ] for all test signals .",
    "this table shows the maximum and the minimum values for both mse and ssim for 20 independent noise realizations .",
    "we recall that the results depend on the value of @xmath32 .",
    "each signal embedded in different noise realizations is a new scenario , and the best results will be obtained with the optimal value of @xmath32 for each case .",
    "with snr 20 and @xmath119 the relative variations are not too large ( the highest variation in ssim is 14% for signal # 6 ) .",
    "therefore , at this snr , the reconstruction is not very affected by noise fluctuations .",
    "[ fig : burst_denoising ] shows the results for only two signals of the catalog , namely those which yield the best ( signal # 1 ; left panel ) and the worst ( signal # 6 ; right panel ) denoising results , respectively ( for the chosen value of @xmath32 and noise realization ) .",
    "the figure displays the comparison of the two original noisy signals ( upper panels ) with the recovered ones ( lower panels ) .",
    "concerning the signal on the left panel our method can accurately recover the distinctive positive and negative peaks associated with the hydrodynamical bounce that follows the collapse of the inner iron core of the star once the equation of state stiffens and the central density exceeds nuclear matter density .",
    "this is particularly clear for the peaks with the larger amplitudes , which are recovered properly .",
    "however , when the amplitude decreases ( i.e.  in the part of the temporal evolution associated with the quasi - radial oscillations of the newly formed neutron star ) the signal becomes weaker than the noise and , as a result , the method returns a zero amplitude signal .",
    "it is also worth mentioning that in the part of the time series where the data are purely noise ( no numerical relativity signal injected ) the method returns a zero signal , as it should . the same behavior is seen for the signal displayed on the right panel of fig .",
    "[ fig : burst_denoising ] , the dampened oscillations are weaker than the noise and the method sets their amplitude to zero .",
    "we note that signal # 6 is somewhat different from the common features of the dictionary . as a result , while the broad morphology is still captured to some extent , the overall result is poorer than for the signal on the left panel .",
    "even so , we note that the results can be improved by changing slightly the value of @xmath32 by adding more atoms to the dictionary .",
    "( we have checked that for @xmath120 the mse is @xmath121 and the ssim is @xmath122 . )",
    ".values ( maximum - minimum ) of the mse and ssim error estimators for the eight burst signals we use as test signals and 20 noise realizations .",
    "values are reported for both snr 20 and 10 . [ cols=\"^,^,^,^,^,^ \" , ]                              p.  jaranowski and a.  krlak ,  gravitational - wave data analysis .",
    "formalism and sample applications : the gaussian case  , living rev .",
    "relativity * 15 * , ( 2012 ) , 4 .",
    "url ( cited on 7 - 20 - 2016 ) : http://www.livingreviews.org/lrr-2012-4              b.s .",
    "sathyaprakash and b.f .",
    "schutz , `` physics , astrophysics and cosmology with gravitational waves '' , living rev .",
    "relativity 12 , ( 2009 ) , 2 .",
    "url ( accessed 7 - 15 - 2016 ) : http://www.livingreviews.org/lrr-2009-2"
  ],
  "abstract_text": [
    "<S> gravitational wave astronomy has become a reality after the historical detections accomplished during the first observing run of the two advanced ligo detectors . in the following years </S>",
    "<S> , the number of detections is expected to increase significantly with the full commissioning of the advanced ligo , advanced virgo and kagra detectors . </S>",
    "<S> the development of sophisticated data analysis techniques to improve the opportunities of detection for low signal - to - noise - ratio events is hence a most crucial effort . </S>",
    "<S> we present in this paper one such technique , dictionary - learning algorithms , which have been extensively developed in the last few years and successfully applied mostly in the context of image processing . </S>",
    "<S> however , to the best of our knowledge , such algorithms have not yet been employed to denoise gravitational wave signals . by building dictionaries from numerical relativity templates of both , binary black holes mergers and bursts of rotational core collapse , </S>",
    "<S> we show how machine - learning algorithms based on dictionaries can be also successfully applied for gravitational wave denoising . </S>",
    "<S> we use a subset of signals from both catalogs , embedded in non - white gaussian noise , to assess our techniques with a large sample of tests and to find the best model parameters . </S>",
    "<S> the application of our method to the actual signal gw150914 shows promising results . </S>",
    "<S> dictionary - learning algorithms could be a complementary addition to the gravitational wave data analysis toolkit . </S>",
    "<S> they may be used to extract signals from noise and to infer physical parameters if the data are in good enough agreement with the morphology of the dictionary atoms . </S>"
  ]
}