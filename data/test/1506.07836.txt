{
  "article_text": [
    "extreme events such as heat or cold waves have major impacts on human health and ecological sustainability .",
    "motivated by the pressing need for better risk assessment , inference for spatial extremes has developed rapidly during the last decade ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "max - stable processes are the only possible non - degenerate limits for rescaled pointwise maxima of random processes , thus motivating their use in modeling spatial extremes .",
    "flexible max - stable models have been developed for inference on spatial extremes . over a variety of applications ,",
    "the brown  resnick process  @xcite has proven to be a valuable model for extremes of environmental variables , such as rainfall , temperature , wind speed and river discharge ( see , e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "it is built using gaussian processes and its dependence structure is determined by a variogram , so well - established ideas from classical geostatistics may be exported to the extremal context .",
    "the greater challenge is inference , since the full likelihood of the brown  resnick process is tractable only in relatively low - dimensional settings @xcite .",
    "composite likelihood techniques have been successfully used to fit max - stable models to block maxima or threshold exceedances , but with reduced estimation efficiency .",
    "more efficient procedures based on full , possibly censored , likelihoods @xcite have been developed for threshold exceedances , allowing the construction of more elaborate models for spatial extremes .",
    "bayesian hierarchical models are appealing in spatial statistics because of the flexibility with which they can track the spatial variation of marginal distributions .",
    "@xcite , @xcite and @xcite constructed hierarchical models for extremes assuming conditional independence of extreme occurrences .",
    "such models may have flexible margins but can not produce realistic simulations of extreme events @xcite . @xcite and",
    "@xcite used hierarchical models that model spatial dependence using gaussian processes , a somewhat restrictive class of models that are asymptotically independent and thus may underestimate potential dependence at the most extreme levels @xcite . @xcite and",
    "@xcite proposed the use of composite likelihoods in markov chain monte carlo ( mcmc ) algorithms and applied them for bayesian inference on max - stable models , but these methods present computational challenges and do not yield exact simulation from the posterior distribution .",
    "@xcite constructed a max - stable spatial model based on a hierarchical representation of logistic extreme - value distributions @xcite .    in this paper",
    "we propose a bayesian hierarchical model based on the brown  resnick process , which exploits a special case for which the full likelihood of such a process can be calculated , using the joint distribution of componentwise maxima and their partitions in terms of individual events @xcite .",
    "we discuss the difficulties of using the partitions in practice , and we propose a new bayesian approach to fitting max - stable processes without fixed partitions .",
    "our work is motivated by the changing climate of northern fennoscandia . in northern regions of norway ,",
    "finland , russia and sweden , extreme low temperatures play a central role in ecological sustainability by limiting the extent of outbreaks of forest pests such as the autumnal moth _",
    "epirrita autumnata _ ,",
    "whose larvae have caused serious damage by defoliating mountain birch .",
    "forests in the region have mostly been protected from this pest by extreme low winter temperatures , since its eggs can not survive below @xmath0c @xcite .",
    "winter minimum temperatures are often below this threshold , but in recent years they have increased and projections suggest they will increase even more in the future ( * ? ? ?",
    "*  e.1 ) .",
    "although temperature is not the only factor limiting the spread of the moth , it is one of the most important , and thus plays a central role in the ecology of northern fennoscandia . in ",
    "[ sec : forecasts ] we discuss the possible changes in winter minimum temperatures in the region , focusing on the probability that they will be less than @xmath0c .",
    "motivated by recent disastrous heat wave events over the world , max - stable processes and related methods have been used to model temperature extremes and their potential changes in time .",
    "@xcite used the so - called @xcite process to model annual maximum temperatures in switzerland .",
    "they used latitude , longitude and elevation to estimate response surfaces for the marginal parameters of extremes .",
    "@xcite describe a bayesian spatial model for annual maximum temperatures in the us based on mixtures of gaussian distributions .",
    "@xcite used a bayesian max - stable model to model extreme temperature data over europe .",
    "section  [ sec : setting ] describes the data we study .",
    "section  [ sec : evt ] summarizes some extreme value ideas and introduces the inference methods for the brown  resnick model .",
    "the hierarchical model and the application are discussed in section  [ sec : bhmodel ] .",
    "we use temperature data from 20 meteorological stations located in northern regions of finland , norway and sweden ; see fig .",
    "[ fig : mapfinland ] . the smallest and largest distances between two stations are 26 and 471  km .",
    "daily minimum temperatures are available from 1960 to 2013 , but with data missing . in view of the motivation for our application",
    ", inference is needed only for the winter minima .",
    "we focus on modeling minimum temperatures in the months december  march and we use the daily data to investigate their co - occurrences .",
    "weather stations in the northern regions of finland , norway and sweden .",
    "the dashed lines shows the convex hull of the stations , which will be our region of interest for simulation.,scaledwidth=81.0% ]    missing daily data censor the winter minima .",
    "winter minimum values for the 20 stations are shown in fig .",
    "[ fig : annualmin ] .",
    "our dataset has at least 26 winter minima for each station , and each year has values of winter minimum for at least 15 of the 20 stations .",
    "c ) at the @xmath1 stations ( a different color for each station ) .",
    "the horizontal line shows the @xmath0c threshold . ]",
    "mean winter minimum temperatures depend on local geography and topography , and we shall model this non - stationarity in their marginal distributions using covariates and random effects , which will also allow us to extrapolate results to ungauged sites .",
    "we derived covariates that are known to influence minimum temperature in northern fennoscandia @xcite .",
    "we supplemented the coordinates of the stations with their absolute elevation above sea level and elevation relative to the surrounding areas ( calculated on a 2  km  @xmath2  2  km spatial moving average ) , a proximity index to the arctic ocean , calculated with a 160  km  @xmath2  160  km moving window assuming that the effect of the ocean reaches 80  km inland , and a lake cover index , which expresses the proportion of area covered by lakes in a 3  km  @xmath2  3  km window ( see * ? ? ? * ) .      in fig .",
    "[ fig : annualmin ] it is difficult to see the presence of a temporal trend with only 53 years of noisy data , but the figure suggests that minima may generally be slightly less extreme from 1990 onwards . the inclusion of a temporal effect in the model will help to assess this .      since we focus on modeling the winter minima , temporal dependence in the daily temperatures is not of interest in itself . but our approach requires us to partition minima in terms of independent generating events , which is complicated by the presence of temporal dependence in the daily data ; see   [ sec : declustering ] .",
    "cold waves stay over the region for several days , so minimum temperatures tend to be very dependent from day to day . preliminary analysis ( see the supplementary material , * ? ? ?",
    "* ) showed that the dependence is strong for the first few days and vanishes after approximately five to ten days at each location .",
    "we expect winter minimum temperatures to be dependent over large regions , and this is the case in our data .",
    "[ fig : annualmin ] suggests that the minima for the @xmath1 stations behave similarly for the @xmath3 years : the years 1966 and 1999 were very cold at all @xmath1 locations , whereas 1973 , 1993 and 2007 are warmer at all locations .",
    "it appears that most of the minima occurred on the same day or only a few days apart , so they may correspond to the same event , which suggests strong spatial dependence .",
    "results from a preliminary analysis ( see * ? ? ?",
    "* ) suggest that pairwise distributions are asymptotically dependent and max - stable . after accounting for differences in the marginal distributions",
    ", we will use a stationary isotropic max - stable model for modeling spatial dependence .",
    "let @xmath4 be a random process with continuous sample paths defined over a compact subset @xmath5 of @xmath6 , and let @xmath7 be independent replicates of @xmath8 . if there exist sequences of continuous functions @xmath9 and @xmath10 such that we have the distributional convergence @xmath11\\right\\}_{\\bm s\\in s } \\rightarrow \\{z(\\bm s)\\}_{\\bm s\\in s},\\quad n\\rightarrow\\infty,\\ ] ] where @xmath12 is a non - degenerate random process , we say that @xmath8 lies in the max - domain of attraction of the process @xmath13 .",
    "then the process @xmath13 must be max - stable @xcite and its marginals are of generalized extreme - value ( gev ) form , @xmath14_+^{-1/\\xi(\\bm s)}\\right ) , \\quad \\bm s\\in s,\\ ] ] denoted @xmath15 , with @xmath16 , and real location @xmath17 , scale @xmath18 and shape @xmath19 parameters ; throughout the paper , the case @xmath20 is defined as the limit for @xmath21 .",
    "every max - stable process with marginal parameters @xmath17 , @xmath22 and @xmath19 can be written as @xmath23 where @xmath13 is a simple max - stable process with unit frchet margins , i.e. , @xmath24 .",
    "thus it suffices to consider simple max - stable processes , henceforth denoted by @xmath13 .    as a consequence of max - stability , for every set of locations @xmath25 , @xmath26 where the so - called exponent function @xmath27 is homogeneous of order @xmath28 , i.e. , @xmath29 , @xmath30 , and satisfies @xmath31 for any permutation of its arguments . in particular , @xmath32 where @xmath33 $ ] is known as the extremal coefficient of @xmath13 relative to @xmath34 .",
    "the extremal coefficient can be interpreted as the effective number of independent variables among @xmath35 and is useful for model checking , since it can be estimated non - parametrically .    as they appear as limits for rescaled pointwise maxima of random processes , max - stable processes are natural models for observed block maxima . moreover",
    ", the equality @xmath36 shows that such processes also provide natural models for block minima .",
    "several parametric max - stable models have been proposed .",
    "the @xcite process was the first to be considered but its realizations are too smooth for realistic modeling of complex extremes .",
    "the @xcite process is constructed from stationary gaussian processes and provides more realistic realizations controlled by different correlation functions .",
    "the extremal-@xmath37 process extends the schlather process by introducing an additional shape parameter @xcite .",
    "here we focus on the brown  resnick process @xcite which appears to be a good model for extremes of environmental processes .",
    "@xcite constructed a stationary max - stable process , called the brown  resnick process , as @xmath38 where the @xmath39 s are the points of a unit rate poisson process on @xmath40 and the @xmath41 are independent replicates of an intrinsically stationary centered gaussian process @xmath42 with variance function @xmath43 .",
    "the dependence structure of @xmath13 is inherited from that of @xmath42 , which is controlled by the variogram @xmath44 $ ] . without loss of generality",
    ", we suppose that @xmath45 with probability one , so @xmath46 .",
    "a large class of models may be obtained by choosing different variograms .",
    "later we use the stable variogram @xmath47 , with @xmath48 and @xmath49 $ ] .",
    "the exponent function of a brown  resnick process is @xcite @xmath50 where @xmath51 is the cumulative distribution function of a @xmath52-dimensional gaussian distribution with mean vector @xmath53 and covariance matrix @xmath54 , @xmath55 , @xmath56 , @xmath57 denotes the @xmath58 vector consisting of the @xmath59th column of @xmath60 without its @xmath59th component , @xmath61 denotes the matrix @xmath60 without its @xmath59th row and @xmath59th column , and so forth .",
    "let @xmath62 be a vector stemming from a simple max - stable process @xmath13 .",
    "differentiation of   shows that the full likelihood contribution for one realization @xmath63 of @xmath64 may be written as @xmath65 \\exp\\{-v(z_1,\\ldots , z_d)\\},\\ ] ] where @xmath66 denotes the set of all partitions of @xmath67 and the subscript @xmath68 on @xmath69 denotes the partial derivative of the function @xmath27 with respect to the components in the subset @xmath68 .",
    "although the likelihood has a closed - form expression for the brown  resnick process @xcite , the size of @xmath66 equals the bell number and is larger than @xmath70 for @xmath71 , making full likelihood inference computationally unattainable in many situations . hence inference for max - stable processes",
    "has typically been based on composite likelihoods using lower - order densities @xcite",
    ". not only do composite likelihoods result in a loss of efficiency compared to full likelihood estimation , they also increase the computational burden when employed in a bayesian analysis , because the composite likelihood does not directly yield a posterior which appropriately accounts for parameter uncertainty @xcite .",
    "@xcite , @xcite and @xcite developed full likelihood inferences for threshold exceedances of processes in the max - domain of attraction of a brown  resnick process , which , while challenging , have lower computational burdens than does  .    for our likelihood",
    ", we employ the result of @xcite , who showed that , for the brown  resnick process , partial derivatives of @xmath27 with respect to the first @xmath72 indices , denoted @xmath73 , satisfy @xmath74,\\end{gathered}\\ ] ] with @xmath75 , @xmath76 , @xmath77 , @xmath78 the @xmath79 matrix consisting of the first @xmath72 rows and columns of @xmath80 , @xmath81 , @xmath82 , @xmath83 , @xmath84 , @xmath85 and @xmath86 , where @xmath87      @xcite derived a likelihood based on componentwise block maxima and their occurrence times that avoids direct differentiation of  .",
    "let @xmath88 denote independent replicates of a random vector @xmath89 .",
    "without loss of generality , suppose that the process @xmath8 has unit frchet margins and lies in the max - domain of attraction of a simple max - stable process @xmath13 .",
    "let @xmath90 denote the componentwise maxima , and let @xmath91 denote the unique partition of @xmath67 such that @xmath92 comes from the same individual observation , i.e. , for each @xmath93 , there exists @xmath94 such that @xmath95 .",
    "then @xcite showed that , as @xmath96 , the block maximum and its associated partition @xmath97 converge in distribution to @xmath98 , whose joint density is @xmath99 this suggests using   as a likelihood for inference on the joint distribution of block maxima and their occurrence times .",
    "this approach only requires the maximum and the partition to be observed , and thus represents a compromise between the block maximum and threshold exceedance approaches .",
    "the relative efficiency of stephenson  tawn and pairwise maximum likelihood estimators has been investigated by @xcite . because the partition contains information about the dependence , the stephenson ",
    "tawn estimator has smaller variance than that based only on the block maxima .",
    "however , for finite block length , the former may be more biased , because the model @xmath98 is misspecified for @xmath97 , i.e. , both for the maximum and the partition @xcite .",
    "use of the limiting distribution for @xmath97 represents a strong assumption that can lead to poor fit , whereas approaches based on the maximum @xmath100 alone are more robust .",
    "a further difficulty arises from the definition of the partition for serially dependent data .",
    "the partition in the stephenson  tawn model is defined in terms of independent generating events .",
    "while it is natural to define the partition in terms of the days of occurrence of the maxima when the daily observations are independent , its estimation is harder when data show temporal dependence . in  [ sec : declustering ] we propose a space - time declustering procedure to estimate the partition .",
    "although the use of the estimated fixed partition is theoretically appealing , it represents a practical challenge .",
    "first , the model is misspecified , which affects the estimation of the max - stable model by the introduction of potentially biased information .",
    "second , the partition must be estimated in a preliminary step , and inference based on the stephenson ",
    "tawn model is conditional on the fixed partition and ignores the uncertainty associated to its estimation .",
    "the partition is not always very well defined ( see  [ sec : declustering ] ) and its estimation is sensitive to measurement error .",
    "this motivates us to propose an approach to estimating the parameters of the brown  resnick process , without fixing the partition , which uses only the values of the maxima and can be applied when the occurrence dates are unavailable .",
    "let @xmath62 be a vector stemming from a brown  resnick process @xmath13 . to overcome the intractability of the likelihood",
    ", we propose to augment the data with the unknown partition @xmath101 that generates @xmath64 in terms of the functions @xmath102 in  , i.e. , @xmath103 is defined such that the points @xmath104 in the same set @xmath68 come from the same function @xmath105 .",
    "the joint distribution of @xmath98 is  .",
    "we impute the unknown partition @xmath101 given the observed value of @xmath64 in an mcmc algorithm , using ideas similar to @xcite , who considered the partition in the context of conditional simulation from max - stable processes .",
    "the partition is imputed from its conditional distribution given the data . from   we",
    "have @xmath106 however , the number of partitions of the set @xmath67 is the @xmath107-th bell number , so   is intractable for large @xmath107 .",
    "we use a gibbs sampler to update the partition . for a partition @xmath108",
    ", we choose a location @xmath109 at random and let @xmath110 denote the restriction of @xmath111 to @xmath112 .",
    "the size of @xmath110 is @xmath113 , or @xmath114 if @xmath115 is itself a partitioning set of @xmath111 .",
    "given @xmath110 , the partition is updated by attributing the location @xmath109 either to an existing set of @xmath110 or to a new set , giving at most @xmath116 choices . the conditional probability for a new partition @xmath117 with @xmath118 is @xmath119 the discrete distribution   has support on a set of size at most @xmath116 , so a new partition @xmath117 can easily be simulated .    in practice , inference is based on independent replicates @xmath120 , and the mcmc algorithm independently updates the corresponding latent partitions @xmath121 . given the observed values of the @xmath122 and the simulated @xmath123 , the marginal and dependence parameters of the model are updated from the joint distribution of the @xmath124 , which is a product of terms of the form  .",
    "we first fitted gev distributions separately at each of the 20 stations ; see @xcite .",
    "the estimates for the scale and shape parameters were not significantly different from site to site , but variation in the location parameter estimates suggested use of a non - stationary gev model with spatially varying location parameter and constant scale and shape parameters .",
    "we then fitted non - stationary marginal models at the 20 locations by maximizing the independence likelihood , which assumes the independence of the temperatures from site to site .",
    "non - stationarity was modeled using trend surfaces defined from the covariates mentioned in   [ sec : dataset ] and temporal trends in the location and scale parameters .",
    "we used the r package spatialextremes @xcite to fit these models , which were compared using the takeuchi information criterion , an equivalent of the akaike information criterion for misspecified models .",
    "our best model had all six covariates for the location surface , constant scale and shape parameters , and a temporal trend in the location only .",
    "its fit was assessed using marginal qq - plots : departure from the @xmath125 confidence intervals was observed for five stations .",
    "this suggests that the trend surface model is not flexible enough to model the variability in the location parameters over the 20 stations .",
    "inclusion of additional covariates did not improve the fit , motivating the use of a more flexible approach using random effects in a bayesian hierarchical model .",
    "we consider two bayesian hierarchical models based on the brown  resnick process .",
    "the first uses the stephenson  tawn model and estimates the partitions in a preliminary step through space - time declustering ; see    [ sec : stmodel ] ,  [ sec : declustering ] .",
    "the second imputes the partitions in the mcmc algorithm ; see   [ sec : estpart ] . here",
    "we describe the part of the hierarchical model for the data conditional on the partitions , which is common to both approaches .",
    "let @xmath126 ( @xmath127 ) denote the observed minimum temperatures for the @xmath128 years of data , let @xmath34 denote the @xmath129 locations of the stations such that @xmath130 , let @xmath131 denote the covariate for the day of occurrence of the winter minimum @xmath132 , defined as the number of years from 1 january 2000 , and finally let @xmath133 denote the partitions of @xmath67 for each year .",
    "when data are missing , the dimension of @xmath134 is lower than @xmath107 and so is the size of the corresponding partition .",
    "the first level of the hierarchical model gives the probability density of the observed minima and the partitions @xmath135 , conditional on the marginal and dependence parameters @xmath136 , as @xmath137 \\prod_{k=1}^{k_i } \\left[-v_{\\pi_{ik}}\\{f_{i1}(-y_{i1}),\\ldots , f_{id}(-y_{id})\\ } \\right ] \\prod_{j=1}^d f_{ij}'(-y_{ij } )   \\right ) , \\end{gathered}\\ ] ] where @xmath138 denotes the marginal transformation to the standard frchet scale for @xmath139 and the prime denotes derivative with respect to @xmath140 ; the negative signs for the @xmath132 s in   correspond to modeling negated winter minima .",
    "the second level of the hierarchical model specifies the models for the exponent measure @xmath27 and the gev parameters .",
    "we define a space - time model for @xmath141 with the covariates discussed in   [ sec : dataset ] , with a linear temporal trend , and with a gaussian prior distribution as @xmath142 where @xmath143 denotes the vector of the intercept and covariates at location @xmath144 , and @xmath145 denotes a gaussian process with mean @xmath146 , variance @xmath147 , and correlation function @xmath148 .",
    "the random effect allows the location parameters to vary smoothly over space , in addition to any effect of the covariates .",
    "we kept @xmath149 and @xmath150 constant over space and time because this appeared to suffice for a good fit .",
    "thus @xmath151 , @xmath152 , and @xmath153 for each pair of replicate and location indices @xmath154 , and the marginal gev distributions of the model at the observed data can be written as @xmath155 where @xmath156 is a gaussian vector with mean @xmath157 , where @xmath158 is a @xmath159 matrix with rows @xmath160 , and covariance matrix @xmath161 .",
    "we use a brown  resnick process with variogram @xmath162 as a model for the exponent function @xmath27 .",
    "prior distributions are assigned to the variables at the last level of a hierarchical model .",
    "we used vague priors for all parameters except those of the latent gaussian field ; see @xcite . the prior distributions for @xmath163 and @xmath147",
    "were chosen to obtain conjugate full conditional distributions for these parameters .",
    "the use of the stephenson  tawn model involves choosing the partitions in  , whose sets should correspond to independent individual events .",
    "several approaches have been proposed to identify clusters in univariate time series ( e.g. ,  ( * ? ? ?",
    "* chapter  10 ) ; @xcite ) .",
    "in the multivariate case , declustering techniques have been proposed by @xcite and @xcite . here",
    "we follow @xcite , who identified clusters of storm surges by assuming that events separated by a fixed time lag are independent .    in our application ,",
    "a difficulty arises from the use of the dates of occurrences . because of",
    "the measurement precision of the temperatures ( @xmath164c ) , the same annual minimum may occur on different days , affecting the estimation of the partition ; this is the case for 7% of the minima , with dates of occurrences separated by more than two days in 3% of cases . when a minimum has multiple days of occurrences , we choose one of these days at random to define our partition .    for our temperature data ,",
    "preliminary analysis suggests choosing a time lag of five days : two observations at the same location and separated by more than five days are assumed to be independent . to identify clusters for the 20 time series",
    ", we use a single linkage ( or friend - to - friend ) method whereby a cluster is composed of data that are dependent in pairs .",
    "thus if the minima occurred at time 1 for the first series , time 4 for the second and time 7 for the third , these three minima are considered to lie in the same cluster . with this procedure , the size of the partition for each year is at most five , and winter temperature minima usually occurred from widespread events that lasted for between five and ten days .      the mcmc algorithm for our brown ",
    "resnick model is nonstandard because the likelihood   can not be calculated exactly but must be estimated .",
    "the likelihood involves the function @xmath27 and its partial derivatives , which themselves involve the gaussian cumulative distribution function in their expressions , and thus can only be estimated .",
    "we used the approach of @xcite to use a likelihood estimator in the mcmc algorithm . under the assumption that the likelihood estimator is unbiased ,",
    "that algorithm generates a markov chain that has the targeted posterior distribution as stationary distribution .",
    "we estimated the likelihood   by replacing the gaussian cumulative distribution functions in its expression by monte carlo estimators .",
    "our estimator is not unbiased , but we constructed the estimator to minimize the potential bias in the estimation of the posterior and obtain good mixing properties for the markov chain in a reasonable time ; see @xcite . for our dataset and the fixed partitions estimated in  ",
    "[ sec : declustering ] , a single estimation of the likelihood   takes about one second on a single cpu .",
    "we used a gibbs sampler with metropolis ",
    "hastings steps to update the marginal and dependence parameters of the model .",
    "to minimize the number of likelihood estimations in the mcmc algorithm , and because some parameters were found to be very dependent , we updated the parameters of the model in blocks ; see @xcite . with our choice of blocks ,",
    "one iteration of the mcmc algorithm requires four likelihood estimations for the stephenson  tawn model . for the random partition model",
    ", the gibbs sampler for the partitions requires estimating the probabilities  , which increases the computational burden a bit more .",
    "since one iteration of the mcmc algorithm takes several seconds on a single cpu , we used parallel computing , fitting 50 independent markov chains each of length 15,000 and discarding the first 5000 iterations of each chain .",
    "the resulting chains were merged to obtain a final chain of length 500,000 .",
    "the mcmc diagnostics are discussed in @xcite .",
    "the mean of the posterior distributions were used as pointwise estimators .",
    "credible intervals were calculated from the quantiles of the markov chains .",
    "the results of a simulation study suggest that the mcmc algorithm provides sensible inferences on the model parameters : the posterior means were close to the true values and the empirical coverages of the credible intervals were close to the nominal values ; see @xcite .",
    "we compare three non - stationary brown  resnick models .",
    "the first , m1 , uses the same trend surfaces as does the best marginal model discussed in   [ sec : prelim ] and is fitted using pairwise likelihood as implemented in the r package spatialextremes ; this is our baseline model , to which we compare our new approaches .",
    "the second , m2 , is the bayesian hierarchical model that uses the fixed partitions from the declustering procedure .",
    "the third , m3 , is the bayesian hierarchical model with the random partitions .",
    ".estimated posterior mean parameters and @xmath125 confidence / credible intervals ( in parentheses ) from the trend surface model m1 , the bayesian model based on declustering m2 and the random partition bayesian model m3 .",
    "the second column is the estimated average location parameter @xmath165 over the 20 stations . [",
    "cols=\"<,^,^,^,^,^,^\",options=\"header \" , ]     we discuss the estimates in terms of the negated location parameter @xmath141 , and negated trend @xmath166 , which correspond to the original scale of minimum temperatures .",
    "the estimates for the three models are given in table  [ tab : estimates ] .",
    "we assessed the fit of the marginal distributions using qq - plots ( see * ? ? ?",
    "we assessed the fit of the dependence model in terms of empirical pairwise extremal coefficients , computed using the empirical marginal distributions of the data to avoid potential bias due to the estimation of the marginals @xcite ; see fig .  [",
    "fig : extremalcoef ] . as a global assessment of the fit in terms of the marginals and dependence , we computed the maximum of winter minimum temperatures over four different sets of stations , for the 53 years of the dataset , and compared the observed values with the predictions from the three brown  resnick models , computed by simulation using the algorithm of @xcite ; see fig .",
    "[ fig : modelchecking_max ] . further diagnostics for the prediction of the minimum and the averages of winter minimum temperatures are given in @xcite .",
    "confidence intervals in gray ) .",
    "curves are from the fitted brown  resnick models ( see table  [ tab : estimates ] ) : m1 ( dotted ) , m2 ( dashed ) , and m3 ( plain ) . ]    ): m1 ( top row ) , m2 ( middle row ) and m3 ( bottom row ) .",
    "the qq - plots compare the predictions and the observed maximum of winter minimum temperatures ( @xmath167c ) over some groups of stations , @xmath168 ( @xmath169 ) .",
    "the columns , from left to right , correspond to the group of stations @xmath170 ( western region ) , @xmath171 ( eastern region ) , @xmath172 ( northern region ) , and all 20 stations .",
    "grey dashed lines are @xmath125 overall confidence bands , and the solid grey line indicates a perfect fit.,scaledwidth=100.0% ]    for the trend surface model , m1 , the marginal qq - plots show poor fits for six stations .",
    "moreover , the model severely underestimates the dependence in terms of the pairwise extremal coefficient .",
    "this can be explained by the inadequacy of the fitted marginal distributions , which bias the dependence estimates ; @xcite also observed this when modeling wind gust data . in terms of predicting the maxima over groups of stations ,",
    "the model performs poorly : many points lie outside the @xmath125 confidence bounds . in general",
    ", this model does not predict extreme temperatures well .    for the bayesian model based on the declustering , m2 ,",
    "the marginal qq - plots show that the sample quantiles are outside the @xmath125 confidence bands for three stations , and underestimate them systematically at all stations ; the estimated scale parameter @xmath149 seems too large .",
    "the model overestimates the pairwise dependence somewhat , and tends to overestimate return levels for the maxima over different groups of stations .    for the random partition model , m3 ,",
    "the marginal qq - plots show a good fit of the gev distributions : the empirical quantiles are in the @xmath125 confidence bands for all the stations .",
    "the empirical pairwise extremal coefficients agree adequately with the curve from the model . in terms of the multivariate predictions",
    ", m3 can predict the maximum , minimum and average of the minima better than does m2 , with all the observations within the @xmath125 confidence bounds ; see fig .",
    "[ fig : modelchecking_max ] and @xcite .",
    "this model appears to represent both the marginal and spatial behavior of extreme low temperatures adequately .",
    "we further discuss the results for the random partition bayesian model m3 .",
    "[ fig : mu_mean2016](a ) shows the posterior mean of the random effects for the location parameter over the convex hull of the meteorological stations ( see fig .",
    "[ fig : mapfinland ] ) , and the mean of the gev distribution for the year 2016 .",
    "we restrict our analysis to the convex hull of the stations because extrapolation outside this region is too uncertain .",
    "the posterior mean of the random effect in the location parameters shows the variability in this parameter that can not be explained by the covariates .",
    "the posterior mean for the fitted gev distributions shows lower temperatures in the inland region .",
    "the minima decrease as latitude , longitude , relative elevation and distance from the arctic ocean increase , and increase as lake cover and elevation increase .",
    "similar covariate effects were found by @xcite for extreme low temperatures in northern fennoscandia .",
    "the estimated shape parameter , corresponding to weibull  so - called light - tailed  distributions , agrees with previous studies on extreme temperatures .",
    "the dependence parameters correspond to strong dependence , with pairwise extremal coefficient @xmath173 at 400  km , while @xmath174 implies that the realizations from the brown  resnick process are continuous but nowhere differentiable .",
    "c).,scaledwidth=100.0% ]      we base our forecasts on the random partition bayesian model m3 .",
    "changes in extremes are entirely based on the linear trend @xmath166 in the location parameter .",
    "this translates to changes in the mean of the gev distribution at location @xmath144 and year @xmath37 , which is @xmath175 the estimate @xmath176 corresponds to an increase of @xmath177c per decade in the mean winter minimum temperature . fig .",
    "[ fig : mu_mean2016](b ) shows the predicted mean winter minimum temperature for 2016 . fig .",
    "[ fig : prob36 ] shows the probability that winter minimum temperatures are higher than @xmath0c , the threshold needed for protecting northern fennoscandia forests from outbreaks of _ epirrita autumnata _ ,  for 1980 , 2016 and 2030 . there is a strong change in these probabilities . over the region defined by the stations ,",
    "the winter minimum temperatures had low probability of exceeding @xmath0c before 1980 , rising to about @xmath178 for 2016 , and then to @xmath179 for 2030 , according to the fitted model .",
    "the predictions must be interpreted with care , owing to the large uncertainty in the estimation of @xmath166 .",
    "c for 1980 , 2016 and 2030.,scaledwidth=100.0% ]    spatial simulation of winter minimum temperatures is useful for estimating joint return levels over certain regions , which could be useful for assessing the risks of pest outbreaks over a region for example .",
    "[ fig : simubr ] shows three simulations of winter minima fields from m3 , which show how the strong dependence in the brown  resnick model results in winter minima that are either low or high throughout the region .",
    "the spatial heterogeneity is mostly due to non - stationarity in the marginal distributions .",
    "conditional simulations are possible @xcite and could be useful as input to models to link past pest outbreaks with extreme temperatures , for example .",
    "c ) for 2016.,scaledwidth=100.0% ]",
    "although the stephenson  tawn approach was found to perform well on simulated data when the true partitions are known , the results of our application show that it can be inappropriate in practice . the hierarchical model for the marginals does not give a good fit to the marginal gev distributions , notwithstanding the additional flexibility provided by the random effects .",
    "we explain this by the use of the joint model for the minima and the partitions , and the declustering .",
    "the first of these requires good agreement of the data with the distribution of the block maxima and the partitions , and this is a strong assumption in practice .",
    "although the brown  resnick dependence structure is quite flexible for modeling stationary isotropic dependence structures , the distribution of the underlying partitions can be very different from the partitions obtained through the declustering . in the univariate case",
    ", previous studies have shown that declustering can bias estimators of the marginal parameters if the clusters are poorly - defined , and we can expect similar problems in the multivariate case . in our application , assuming that data separated by more than five days are independent in pairs results in partitions of small sizes each year ; see fig .",
    "[ fig : clust ] . a similar size for the true partitions for a brown  resnick process can only be obtained if the dependence in the process is very strong , and this is incompatible with the empirical extremal coefficients shown in fig .",
    "[ fig : extremalcoef ] . fig .",
    "[ fig : clust ] also shows the sizes of the random partitions in the mcmc algorithm for model m3 ; in several cases the size of the partitions from the declustering procedure have low probability for the random partition model . to further compare the partitions from the mcmc algorithm with that from the declustering procedure ,",
    "we computed the rand index @xcite , which is , for each year , the proportion of pairs of locations that are simultaneously either together or separate in both partitions ; we then averaged the rand index over the years .",
    "it varies from @xmath180 to @xmath181 , indicating that pairwise classification agrees for less than 63% of the pairs between m2 and m3 .",
    "the size of the partitions is typically larger in the random partitions , and the relation with the occurrence dates is unclear .",
    "for example , in the winter 19731974 , the winter minima at the 16 stations with no missing data occurred on four consecutive days and the declustering procedure finds a unique partitioning set , whereas just 7% of the partitions visited in the mcmc run have a unique set and 37% have four partitioning sets or more .",
    ".,scaledwidth=100.0% ]    these results suggest that a different declustering procedure must be used .",
    "we varied the number of days used in our declustering scheme , and also considered another scheme that forces the stations that are at least 150  km apart to be in different clusters .",
    "the parameter estimates changed with the scheme but the diagnostic plots were never very satisfactory .",
    "we have shown how bayesian inference can be conducted for the brown  resnick process , either using the occurrence times of maxima and a declustering procedure , or by including the partitions in an mcmc algorithm .",
    "the partitions add powerful information into the model about the dependence structure .",
    "however our application shows that using fixed partitions can badly bias the estimation of the marginal and dependence parameters of a max - stable process . in a second approach we included the partitions in the mcmc algorithm without using information on the occurrence times , and",
    "this produced superior results .",
    "a more complex approach could use the occurrence times to construct a prior distribution on the space of the partitions , to add more information into the model and improve the efficiency of the estimation , while accounting for the uncertainty on the partitions , but this might be rather complex . @xcite",
    "proposed using second - order terms in the stephenson  tawn model to reduce potential bias due to the partitions , but this seems difficult in our case because the number of terms added to the likelihood would be huge and the mcmc algorithm would become even slower",
    ".    our bayesian model can be generalized in several ways .",
    "we focused on a stationary isotropic brown  resnick process for simplicity , and it appears to be sufficient for our application .",
    "a similar bayesian hierarchical model can be constructed for the extremal-@xmath37 process using ideas of @xcite , anisotropy would be easily incorporated using the idea of @xcite , and so perhaps could the work of @xcite on non - stationary dependence models for extremes . however , the generalization of the present method to more complex models and larger datasets may be limited by computational feasibility .",
    "monte carlo estimation of the likelihood is critical to keep the running times reasonable , and is relatively fast and accurate in low dimensions , but with more sites , more simulations may be needed to obtain reliable inference .",
    "the model predicts an increase of @xmath177c per decade in the mean winter minimum temperatures in northern fennoscandia .",
    "further work to validate the predictions from our model and to quantify the possible effects of global change on boreal forests would be valuable .",
    "the linear trend in time provides sensible short - term predictions , but for longer periods more complex approaches would be necessary , though limited by the availability of reliable data .",
    "one possibility would be to link the changes in extreme temperatures to mean temperatures and use predictions from global climate models under different scenarios as inputs to forecasting .",
    "this work was undertaken while e.t .",
    "was at epfl , switzerland , with support from the swiss national science foundation .",
    "e.t . and d.s.c.s work at csu was partially supported by us national science foundation grant dms-1243102 .",
    "the temperature data are from the finnish meteorological institute database for finland , and from the european climate assessment & dataset database ( http://www.ecad.eu ) for sweden and norway .",
    "this work was motivated by discussions with seppo neuvonen , who pointed out to us the relevance of winter minimum temperatures to insect outbreaks ."
  ],
  "abstract_text": [
    "<S> the brown  resnick max - stable process has proven to be well - suited for modeling extremes of complex environmental processes , but in many applications its likelihood function is intractable and inference must be based on a composite likelihood , thereby preventing the use of classical bayesian techniques . in this paper </S>",
    "<S> we exploit a case in which the full likelihood of a brown  resnick process can be calculated , using componentwise maxima and their partitions in terms of individual events , and we propose two new approaches to inference . </S>",
    "<S> the first estimates the partitions using declustering , while the second uses random partitions in a markov chain monte carlo algorithm . </S>",
    "<S> we use these approaches to construct a bayesian hierarchical model for extreme low temperatures in northern fennoscandia .    ,    ,    , </S>"
  ]
}