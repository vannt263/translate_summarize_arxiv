{
  "article_text": [
    "the task is the following : we try to estimate a relationship @xmath0 based on a finite set @xmath1 of observations .",
    "a popular strategy is to fix a class of functions @xmath2 and to minimize the empirical risk @xmath3 over all elements @xmath4 . here",
    "@xmath5 is a loss function .",
    "sometimes , a regularization term @xmath6 is added to ( [ emprisk ] ) .",
    "we call fitting methods like this learners .",
    "popular examples for multivariate data are trees , support vector machines or smoothing splines .",
    "the choice of the learner is crucial , as too complex learners lead to overfitting , whilst weak learners fail to capture the relevant structure .",
    "the term weak learner has its seeds in the machine learning literature . in classification problems ,",
    "a weak learner is a learner that is slightly better than random guessing .",
    "( the exact definition can be found in e.g. @xcite . ) for regression problems , we might think of a learner that has a high bias compared to its variance , or a learner that has only a few degrees of freedom .",
    "+ the basic idea of boosting is to proceed stepwise and to combine weak learners in such a way that the composite  boosted  learner @xmath7 ( or @xmath8 for classification problems ) performs better than the single weak learners @xmath9 .",
    "the single learners are usually called base learners and @xmath10 is called the number of boosting iterations",
    ". the learners @xmath9 and the weights @xmath11 are chosen adaptively from the data .",
    "adaboost @xcite  the first boosting algorithm  is designed for classification problems .",
    "it is presented in algorithm [ algoadaboost ] .",
    "the weak base learner is repeatedly applied to the weighted training sample @xmath12 .",
    "points which were hard to approximate in step @xmath13 are given higher weights in the next iteration step .",
    "[ algoadaboost ]    for some learners , it is not possible to compute a weighted loss . instead , in each step we draw with replacement a sample of size @xmath14 from @xmath15 and use the weights @xmath16 as probabilities .",
    "+ it can be shown @xcite that boosting is a forward stage - wise fitting method using gradient descent techniques .",
    "more precisely , in each step we fit a weak learner to @xmath17 and the negative gradient @xmath18 of the loss function ( [ loss ] ) .",
    "the connection between boosting and gradient descent methods has lead to a wide range of new algorithms @xcite , notably for regression problems .",
    "note that if we use the quadratic loss @xmath19 the negative gradient is simply the vector of residuals , i.e. we iteratively fit the residuals using a weak learner .",
    "this method is called @xmath20boost @xcite and is presented in algorithm [ algol2boost ] .",
    "[ algol2boost ]    boosting with the loss function @xmath21 is suited for classification problems and called logitboost @xcite(see algorithm [ algologitboost ] ) .",
    "[ algologitboost ]    the function @xmath22 is an estimate of one - half of the log - odds ratio @xmath23 as a consequence , this classification algorithm also produces estimates of the class probabilities @xmath24 . generic boosting algorithms for a general loss function",
    "can be found in @xcite .",
    "+ how do we obtain the optimal number of boosting iterations ?",
    "one possibility is to use cross validation .",
    "depending on the data , this can lead to high computational costs .",
    "if we use @xmath20boost , it is possible to compute the degrees of freedom of the boosting algorithm @xcite . as a consequence",
    ", we can use model selection criteria as the akaike information criterion ( aic ) or the bayesian information criterion ( bic ) .",
    "the content of this section is condensed from @xcite .",
    "we speak of functional data if the variables that we observe are curves .",
    "let us first consider the case that only the predictor samples @xmath17 are curves , that is @xmath25 examples for this type of data are time series , temperature curves or near infra red spectra .",
    "we usually assume that the functions fulfill a regularity condition , and in the rest of the paper , we consider the hilbert space @xmath26 of all square - integrable functions @xmath27 .      in most applications , we do not measure a curve , but discrete values of a curve .",
    "an important step in the analysis of functional data is therefore the transformation of the discretized objects to smooth functions .",
    "the general approach is the following : we represent each example as a linear combination @xmath28 of a set of base functions @xmath29 .",
    "the coefficents @xmath30 are then estimated by using ( penalized ) least squares .",
    "the most frequently used base functions are fourier expansions , b - splines , wavelets and polynomials . a different possibility is to derive an orthogonal basis directly from the data .",
    "this can be done by using functional principal component analysis .",
    "we only consider linear relationships ( [ function1 ] ) , i.e. in the regression setting ( @xmath31 ) , elements @xmath32 are assumed to be linear ( up to an intercept ) and continuous . as @xmath2 is a hilbert space , it follows that any function @xmath33 is of the form @xmath34 in the two - class classification setting ( @xmath35 ) , we use @xmath8 instead of @xmath36 . as already mentioned in sect .",
    "[ secboosting ] , we estimate @xmath36 or @xmath37 by minimizing the empirical risk ( [ emprisk ] ) . note that this is an ill - posed problem , as there are ( in general ) infinitely many functions @xmath37 that fit the data perfectly .",
    "there is obviously a need for regularization , in order to avoid overfitting",
    ". we can solve this problem by using a base expansion of both the predictor variable @xmath38 as in ( [ baseexp ] ) and the function @xmath39 this transforms ( [ emprisk ] ) into a parametric problem .",
    "if we use the quadratic loss , this is a matrix problem : we set @xmath40 it follows that ( for centered data ) @xmath41 as already mentioned , we have to regularize this problem .",
    "there are two possibilities : we can either constrain the number of base functions in ( [ betaexp ] ) .",
    "that is , we demand that @xmath42 .",
    "however , we show in sect .",
    "[ secfuncboost ] that this strategy can lead to trivial results in the boosting setting .",
    "the second possibility is to add a penalty term @xmath6 to the empirical risk ( [ emprisk ] ) .",
    "if we consider functional data , it is common to use a penalty term of the form @xmath43 here @xmath44 is the @xmath45th derivative of @xmath37  provided that this derivative exists .",
    "the choice of @xmath45 depends on the data at hand and our expert knowledge on the problem . + finally ,",
    "let us briefly mention how to model a linear relationship ( [ function1 ] ) if both the predictor and response variable are functional .",
    "we consider functions @xmath46 we estimate @xmath37 by expanding @xmath47 in terms of a basis and by representing @xmath37 by @xmath48 the optimal coefficients @xmath49 are determined using the loss function @xmath50 again , we have to regularize in order to obtain smooth estimates that do not overfit .",
    "in order to apply a boosting technique to functional data , we have to extend the notion weak learner. in the classification setting , we can adopt the loose definition from sect .",
    "[ secboosting ] .",
    "a weak learner is a learner that is slightly better than random .",
    "what are examples of weak learners ?",
    "note that it is possible to apply most of the multivariate data analysis tools to functional data .",
    "we use a finite - dimensional approximation as in ( [ baseexp ] ) and simply apply any appropriate algorithm . in this way , it is possible to use stumps ( that is , classification trees with one node ) or neural networks as base learners .",
    "+ in the regression setting , we propose the following definition : a weak learner is a learner that has only a few degrees of freedom .",
    "examples include the two regularized least squares algorithms presented in sect .",
    "[ secfda ]  restriction of the number of base functions in ( [ betaexp ] ) or addition of a penalty term to ( [ emprisk ] ) .",
    "note however that the first method leads to trivial results if we use @xmath20boost .",
    "the learner is simply the projection of @xmath51 onto the space that is spanned by the columns of @xmath52 ( recall ( [ bhat ] ) ) .",
    "consequently , the @xmath51-residuals are orthogonal on @xmath52 and after one step , the boosting solution does not change anymore .",
    "another example of a weak learner is the following @xcite : in each boosting step , we only select one base function using @xmath17 and the residuals @xmath53 . to select this base function",
    ", we estimate the regression coefficients @xmath54 of @xmath55 we choose the base function that minimizes the empirical risk ( [ emprisk ] ) . for centered data ,",
    "this equals @xmath56 boosting for multivariate data with this kind of weak learner has been studied in e.g. @xcite .",
    "+ if the response variable is functional , we can adopt the same definition of weak learner as in the regression setting : a weak learner is a learner that uses only a few degrees of freedom .",
    "this example is taken from @xcite .",
    "the data consists of @xmath57 recordings of the word yes and @xmath58 recordings of the word no. one recording is represented by a discretized time series of length @xmath59 .",
    "the data can be downloaded from http://www.math.univ-montp2.fr/~biau/bbwdata.tgz .",
    "all calculations are performed using ` r ` @xcite .",
    "+ the task is to find a classification rule that assigns the correct word to each time series .",
    "we apply the logitboost algorithm to this data set .",
    "first , we represent the time series in terms of a fourier basis expansion of dimension @xmath60 .",
    "we opted to include a lot of basis functions , as experiments indicate that the results of logitboost are insensitive to the addition of possibly irrelevant basis functions .",
    "the weak learner is a classification tree with two final nodes .",
    "the misclassification rate was estimated using 10fold cross - validation ( cv ) .",
    "figure [ cverror ] shows the cross - validated error as a function of the number of boosting iterations .",
    "the minimal cv error over all boosting iterations is 0.1 , obtained after 24 boosting iterations .",
    "this is the same error rate that is reported in @xcite . there ,",
    "a functional @xmath45-nearest - neighbor - algorithm is applied to the data . finally , we remark that the cv error curve stays rather flat after the minimum is attained .",
    "this seems to be a feature of all boosting methods . as a consequence , the selection of the optimal number of boosting iterations",
    "can be done quite easily .",
    "the extension of boosting methods to functional data is straightforward . after choosing a base algorithm ( which we called a weak learner ) , we iteratively fit the data by either applying this algorithm to reweighted samples or by using a gradient descent technique . in many applications , we use a finite - dimensional expansion of the functional examples in terms of base functions .",
    "this finite - dimensional representation can then be plugged into existing algorithms as adaboost , logitboost or @xmath20boost .",
    "+ we focused on linear learning problems in sect .",
    "[ secfda ] for the sake of simplicity and briefness , but it should be noted that boosting methods can also be applied to solve nonlinear functional data problems ."
  ],
  "abstract_text": [
    "<S> we deal with the task of supervised learning if the data is of functional type . </S>",
    "<S> the crucial point is the choice of the appropriate fitting method ( also called learner ) . </S>",
    "<S> boosting is a stepwise technique that combines learners in such a way that the composite  boosted  learner outperforms the single learner . </S>",
    "<S> this can be done by either reweighting the examples or with the help of a gradient descent technique . in this paper , we explain how to extend boosting methods to problems that involve functional data .    * keywords * : functional data analysis , boosting </S>"
  ]
}