{
  "article_text": [
    "clustering of data is a well - known problem of pattern recognition , covered in textbooks such as @xcite .",
    "the problem we are looking at is defining clusters of data solely by the proximity of data points to one another .",
    "this problem is one of unsupervised learning , and is in general ill defined .",
    "solutions to such problems can be based on intuition derived from physics .",
    "a good example of the latter is the algorithm by @xcite that is based on associating points with potts - spins and formulating an appropriate model of statistical mechanics .",
    "we propose an alternative that is also based on physical intuition , this one being derived from quantum mechanics .    as an introduction to our approach",
    "we start with the scale - space algorithm by @xcite who uses a parzen - window estimator@xcite of the probability distribution leading to the data at hand .",
    "the estimator is constructed by associating a gaussian with each of the @xmath0 data points in a euclidean space of dimension @xmath1 and summing over all of them .",
    "this can be represented , up to an overall normalization by ( ) = _ i e^-(*x -x*_i)^2 2 ^ 2 where @xmath2 are the data points .",
    "roberts @xcite views the maxima of this function as determining the locations of cluster centers .",
    "an alternative , and somewhat related method , is support vector clustering ( svc ) @xcite that is based on a hilbert - space analysis . in svc",
    "one defines a transformation from data space to vectors in an abstract hilbert space .",
    "svc proceeds to search for the minimal sphere surrounding these states in hilbert space.we will also associate data - points with states in hilbert space .",
    "such states may be represented by gaussian wave - functions , in which case @xmath3 is the sum of all these states .",
    "this is the starting point of our method .",
    "we will search for the schrdinger potential for which @xmath3 is a ground - state .",
    "the minima of the potential define our cluster centers .",
    "we wish to view @xmath4 as an eigenstate of the schrdinger equation [ sch ] h ( -^2 2 ^ 2 + v(*x*))= e . here",
    "we rescaled @xmath5 and @xmath6 of the conventional quantum mechanical equation to leave only one free parameter , @xmath7 . for comparison ,",
    "the case of a single point at @xmath8 corresponds to eq .",
    "[ sch ] with @xmath9 and @xmath10 , thus coinciding with the ground state of the harmonic oscillator in quantum mechanics .    given @xmath4 for any set of data points we can solve eq .",
    "[ sch ] for @xmath6 : v(*x*)=e + ^2 2 ^ 2 = e - d 2 + 1 2 ^ 2 _ i ( -_i)^2 e^-(*x -x*_i)^22 ^2 let us furthermore require that min@xmath6=0 .",
    "this sets the value of e= - min ^2 2 ^ 2 and determines @xmath11 uniquely .",
    "@xmath12 has to be positive since @xmath6 is a non - negative function .",
    "moreover , since the last term in eq . 3 is positive definite , it follows that 0<e .",
    "we note that @xmath4 is positive - definite .",
    "hence , being an eigenfunction of the operator @xmath5 in eq .",
    "( [ sch ] ) , its eigenvalue @xmath12 is the lowest eigenvalue of @xmath5 , i.e. it describes the ground - state .",
    "all higher eigenfunctions have nodes whose number increases as their energy eigenvalues increase . as the probability distribution ,",
    "all eigenfunctions of @xmath5 have physical meaning .",
    "although this approach could be adopted , we have chosen @xmath4 as the probability distribution because of simplicity of algebraic manipulations . ]",
    "given a set of points defined within some region of space , we expect @xmath11 to grow quadratically outside this region , and to exhibit one or several local minima within the region . just as in the harmonic potential of the single point problem",
    ", we expect the ground - state wave - function to concentrate around the minima of the potential @xmath11",
    ". therefore we will identify these minima with cluster centers .    as an example we display results for the crab data set taken from ripley s book @xcite .",
    "these data , given in a five - dimensional parameter space , show nice separation of the four classes contained in them when displayed in two dimensions spanned by the 2nd and 3rd principal components@xcite ( eigenvectors ) of the correlation matrix of the data .",
    "the information supplied to the clustering algorithm contains only the coordinates of the data - points .",
    "we display the correct classification to allow for visual comparison of the clustering method with the data . starting with @xmath13",
    "we see in fig .",
    "1 that the parzen probability distribution , or the wave - function @xmath4 , has only a single maximum . nonetheless , the potential , displayed in fig .",
    "2 , shows already four minima at the relevant locations .",
    "the overlap of the topographic map of the potential with the true classification is quite amazing .",
    "the minima are the centers of attraction of the potential , and they are clearly evident although the wave - function does not display local maxima at these points . the fact that @xmath14 lies above the range where all valleys merge explains why @xmath3 is smoothly distributed over the whole domain .",
    "we ran our method also on the iris data set @xcite , which is a standard benchmark obtainable from the uci repository @xcite .",
    "the data set contains 150 instances each composed of four measurements of an iris flower .",
    "there are three types of flowers , represented by 50 instances each .",
    "clustering of these data in the space of the first two principal components is depicted in figure 3 .",
    "the results shown here are for @xmath15 which allowed for clustering that amounts to misclassification of three instances only , quite a remarkable achievement ( see , e.g. @xcite ) .",
    "the examples displayed in the previous section show that , if the spatial representation of the data allows for meaningful clustering using geometric information , quantum clustering ( qc ) will do the job . there",
    "remain , however , several technical questions to be answered : what is the preferred choice of @xmath7 ? how can qc be applied in high dimensions ?",
    "how does one choose the appropriate space , or metric , in which to perform the analysis ?",
    "we will confront these issues in the following .    in the crabs - data of we find that as @xmath7 is decreased to @xmath16 , the previous minima of @xmath11 get deeper and two new minima are formed .",
    "however the latter are insignificant , in the sense that they lie at high values ( of order @xmath12 ) , as shown in fig .",
    "if we classify data - points to clusters according to their topographic location on the surface of @xmath11 , roughly the same clustering assignment is expected for @xmath17 as for @xmath18 .",
    "one important advantage of quantum clustering is that @xmath12 sets the scale on which minima are observed .",
    "thus , we learn from fig .",
    "2 that the cores of all four clusters can be found at @xmath6 values below @xmath19 .",
    "the same holds for the significant minima of fig .",
    "4 .    by the way",
    ", the wave function acquires only one additional maximum at @xmath20 .",
    "as @xmath7 is being further decreased , more and more maxima are expected in @xmath4 and an ever increasing number of minima ( limited by @xmath0 ) in @xmath6 .",
    "the one parameter of our problem , @xmath7 , signifies the distance that we probe .",
    "accordingly we expect to find clusters relevant to proximity information of the same order of magnitude .",
    "one may therefore vary @xmath7 continuously and look for stability of cluster solutions , or limit oneself to relatively low values of @xmath7 and decide to stop the search once a few clusters are being uncovered .",
    "in all our examples data were given in some high - dimensional space and we have analyzed them after defining a projection and a metric , using the pca approach .",
    "the latter defines a metric that is intrinsic to the data , determined by their second order statistics .",
    "but even then , several possibilities exist , leading to non - equivalent results .    principal component decomposition can be applied both to the correlation matrix @xmath21 and to the covariance matrix _ = <",
    "( x_-<x > _ ) ( x_-<x > _ ) > = c _ - < x > _ _ . in both cases averaging",
    "is performed over all data points , and the indices indicate spatial coordinates from 1 to @xmath1 .",
    "the principal components are the eigenvectors of these matrices .",
    "thus we have two natural bases in which to represent the data .",
    "moreover one often renormalizes the eigenvector projections , dividing them by the square - roots of their eigenvalues .",
    "this procedure is known as  whitening \" , leading to a renormalized correlation or covariance matrix of unity .",
    "this is a scale - free representation , that would naturally lead one to start with @xmath22 in the search for ( higher - order ) structure of the data .",
    "the pca approach that we have used on our examples was based on whitened correlation matrix projections .",
    "this turns out to produce good separation of crab - data in pc2-pc3 and of iris - data in pc1-pc2 .",
    "had we used the covariance matrix @xmath23 instead , we would get similar , but slightly worse , separation of crab - data , and a much worse representation of the iris - data .",
    "our examples are meant to convince the reader that once a good metric is found , qc conveys the correct information .",
    "hence we allowed ourselves to search first for the best geometric representation , and then apply qc .",
    "in the iris problem we obtained excellent clustering results using the first two principal components , whereas in the crabs problem , clustering that depicts correctly the classification necessitates components 2 and 3 .",
    "however , once this is realized , it does not harm to add the 1st component .",
    "this requires working in a 3-dimensional space , spanned by the three leading pcs .",
    "increasing dimensionality means higher computational complexity , often limiting the applicability of a numerical method .",
    "nonetheless , here we can overcome this ",
    "curse of dimensionality \" by limiting ourselves to evaluating @xmath6 at locations of data - points only .",
    "since we are interested in where the minima lie , and since invariably they lie near data points , no harm is done by this limitation .",
    "the results are depicted in fig .",
    "5 . shown here",
    "are @xmath24 values as function of the serial number of the data , using the same symbols as in fig . 2 to allow for visual comparison .",
    "using all data of @xmath25 one obtains cluster cores that are well separated in space , corresponding to the four classes that exist in the data . only 9 of the 129 points that obey @xmath25 are misclassified by this procedure . adding higher pcs , first component 4 and then component 5 , leads to deterioration in clustering quality .",
    "in particular , lower cutoffs in @xmath24 , including lower fractions of data , are required to define cluster cores that are well separated in their relevant spaces .",
    "one may locate the cluster centers , and deduce the clustering allocation of the data , by following dynamics of gradient descent into the potential minima . defining @xmath26",
    "one follows steps of @xmath27 , letting the points @xmath28 reach an asymptotic fixed value coinciding with a cluster center .",
    "more sophisticated minimum search algorithms ( see , e.g. chapter 10 in @xcite ) can be applied to reach the fixed points faster .",
    "the results of a gradient - descent procedure , applied to the 3d analysis of the crabs data shown in fig .",
    "5 , is displayed in fig .",
    "one clearly observes the four clusters , and can compare clustering allocation with the original data classes .",
    "gradient descent calls for the calculation of @xmath6 both on the original data - points , as well as on the trajectories they follow .",
    "an alternative approach can be to restrict oneself to the original values of @xmath6 , as in the example displayed in figure 5 , and follow a hybrid algorithm to be described below . before turning to such an algorithm",
    "let us note that , in this case , we evaluate @xmath6 on a discrete set of points @xmath29 .",
    "we can then express @xmath6 in terms of the distance matrix @xmath30 as v_i = e - d 2 + 12 ^ 2_j d_ij^2 e^- d_ij^2 2 ^ 2 _ j e^-d_ij^22 ^ 2 with @xmath12 chosen appropriately so that min@xmath31=0 .",
    "this kind of formulation is of particular importance if the original information is given in terms of distances between data points rather than their locations in space . in this case",
    "we have to proceed with distance information only .",
    "applying qc we can reach results of the type of fig .",
    "5 without invoking any explicit spatial distribution of the points in question .",
    "one may then analyze the results by choosing a cutoff , e.g. @xmath32 , such that a fraction ( e.g. @xmath33 ) of the data will be included . on this subset",
    "select groups of points whose distances from one another are smaller than , e.g. , 2@xmath7 , thus defining cores of clusters . then continue with higher values of @xmath6 , e.g. @xmath34 , allocating points to previous clusters or forming new cores .",
    "since the choice of distance cutoff in cluster allocation is quite arbitrary , this method can not be guaranteed to work as well as the gradient - descent approach .",
    "our method can be easily generalized to allow for different weighting of different points , as in ( ) = _ i c_i e^-(*x -x*_i)^2 2 ^ 2 with @xmath35 .",
    "this is important if we have some prior information or some other means for emphasizing or deemphasizing the influence of data points .",
    "an example of the latter is using qc in conjunction with svc @xcite .",
    "svc has the possibility of labelling points as outliers .",
    "this is done by applying quadratic maximization to the lagrangian [ wolfe ] w=1 - _ i , j _ i _ j e^-(_i-_j)^2 2 ^ 2 over the space of all @xmath36 subject to the constraint @xmath37 .",
    "the points for which the upper bound of @xmath38 is reached are labelled as outliers .",
    "their number is regulated by @xmath39 , being limited by @xmath40 . using for",
    "the qc analysis a choice of @xmath41 will eliminate the outliers of svc and emphasize the role of the points expected to lie within the clusters .",
    "qc constructs a potential function @xmath11 on the basis of data - points , using one parameter , @xmath7 , that controls the width of the structures that we search for .",
    "the advantage of the potential @xmath6 over the scale - space probability distribution is that the minima of the former are better defined ( deep and robust ) than the maxima of the latter .",
    "however , both of these methods put the emphasis on cluster centers , rather than , e.g. , cluster boundaries .",
    "since the equipotentials of @xmath6 may take arbitrary shapes , the clusters are not spherical , as in the k - means approach .",
    "nonethelss , spherical clusters appear more naturally than , e.g. , ring - shaped or toroidal clusters , even if the data would accomodate them .",
    "if some global symmetry is to be expected , e.g. global spherical symmetry , it can be incorporated in the original schrdinger equation defining the potential function .",
    "qc can be applied in high dimensions by limiting the evaluation of the potential , given as an explicit analytic expression of gaussian terms , to locations of data points only .",
    "thus the complexity of evaluating @xmath31 is of order @xmath42 independently of dimensionality .",
    "our algorithm has one free parameter , the scale @xmath7 . in all examples we confined ourselves to scales that are of order 1 , because we have worked within whitened pca spaces .",
    "if our method is applied to a different data - space , the range of scales to be searched for could be determined by some other prior information .",
    "since the strength of our algorithm lies in the easy selection of cluster cores , it can be used as a first stage of a hybrid approach employing other techniques after the identification of cluster centers . the fact that we do not have to take care of feeble minima , but consider only robust deep minima , turns the identification of a core into an easy problem .",
    "thus , an approach that drives its rationale from physical intuition in quantum mechanics , can lead to interesting results in the field of pattern classification ."
  ],
  "abstract_text": [
    "<S> we propose a novel clustering method that is based on physical intuition derived from quantum mechanics . starting with given data points , </S>",
    "<S> we construct a scale - space probability function . viewing the latter as the lowest eigenstate of a schrdinger equation , we use simple analytic operations to derive a potential function whose minima determine cluster centers . </S>",
    "<S> the method has one parameter , determining the scale over which cluster structures are searched . </S>",
    "<S> we demonstrate it on data analyzed in two dimensions ( chosen from the eigenvectors of the correlation matrix ) . </S>",
    "<S> the method is applicable in higher dimensions by limiting the evaluation of the schrdinger potential to the locations of data points . in this case </S>",
    "<S> the method may be formulated in terms of distances between data points . </S>"
  ]
}