{
  "article_text": [
    "in recent years , combinatorial optimization problems have been tackled with hybrid methods and/or hybrid solvers @xcite .",
    "the use of problem relaxations , decomposition , cutting planes generation techniques in a constraint programming ( cp ) framework are only some examples .",
    "many hybrid approaches are based on the use of a relaxation @xmath0 , i.e. an easier problem derived from the original one by removing ( or relaxing ) some constraints .",
    "solving @xmath0 to optimality provides a bound on the original problem .",
    "moreover , when the relaxation is a linear problem , we can derive _ reduced costs _ through dual variables often with no additional computational cost .",
    "reduced costs provide an optimistic esteem ( a bound ) of each variable - value assignment cost .",
    "these results have been successfully used for pruning the search space and for guiding the search toward promising regions ( see @xcite ) in many applications like tsp @xcite , tsptw @xcite , scheduling with sequence dependent setup times @xcite and multimedia applications @xcite .",
    "we propose here a solution method , depicted in figure [ fig : tree ] , based on a two step search procedure that interleaves ( @xmath1 ) subproblem generation and ( @xmath2 ) subproblem solution . in detail",
    ", we solve a relaxation of the problem at the root node and we use reduced costs to rank domain values ; then we partition the domain of each variable @xmath3 in two sets , i.e. , the _ good _ part @xmath4 and the _ bad _ part @xmath5 .",
    "we search the tree generated by using a strategy imposing on the left branch the branching constraint @xmath6 while on the right branch we impose @xmath7 . at each leaf of the _ subproblem generation tree _ , we have a subproblem which can now be solved ( in the _ subproblem solution tree _ ) .",
    "exploring with a limited discrepancy strategy the resulting search space , we obtain that the first generated subproblems are supposed to be the most promising and are likely to contain the optimal solution .",
    "in fact , if the ranking criterion is effective ( as the experimental results will show ) , the first generated subproblem ( discrepancy equal to 0 ) @xmath8 , where all variables range on the good domain part , is likely to contain the optimal solution .",
    "the following generated subproblems ( discrepancy equal to 1 ) @xmath9 have all variables but the @xmath1-th ranging on the good domain and are likely to contain worse solutions with respect to @xmath8 , but still good .",
    "clearly , subproblems at higher discrepancies are supposed to contain the worst solutions .",
    "a surprising aspect of this method is that even by using low cardinality good sets , * we almost always find the optimal solution in the first generated subproblem*. thus , reduced costs provide extremely useful information indicating for each variable which values are the most promising .",
    "moreover , this property of reduced costs is independent of the tightness of the relaxation .",
    "tight relaxations are essential for the proof of optimality , but not for the quality of reduced costs . solving only the first subproblem , we obtain a very effective incomplete method that finds the optimal solution in almost all test instances .    to be complete , the method should solve all subproblems for all discrepancies to prove optimality . clearly ,",
    "even if each subproblem could be efficiently solved , if all of them should be considered , the proposed approach would not be applicable .",
    "the idea is that by generating the optimal solution soon and tightening the lower bound with considerations based on the discrepancies shown in the paper , we do not have to explore all subproblems , but we can prune many of them .    in this paper , we have considered as an example the travelling salesman problem and its time constrained variant , but the technique could be applied to a large family of problems .",
    "the contribution of this paper is twofold : ( @xmath1 ) we show that reduced costs provide an extremely precise indication for generating promising subproblems , and ( @xmath2 ) we show that lds can be used to effectively order the subproblems .",
    "in addition , the use of discrepancies enables to tighten the problem bounds for each subproblem .",
    "the paper is organized as follows : in section  [ sc : preliminaries ] we give preliminaries on limited discrepancy search ( lds ) , on the tsp and its time constrained variant . in section  [ sc : method ] we describe the proposed method in detail .",
    "section  [ implementation ] discusses the implementation , focussing mainly on the generation of the subproblems using lds .",
    "the quality of the reduced cost - based ranking is considered in section  [ sc : quality ] . in this section also",
    "the size of subproblems is tuned .",
    "section  [ sc : results ] presents the computational results .",
    "conclusion and future work follow .",
    "limited discrepancy search ( lds ) was first introduced by harvey and ginsberg @xcite .",
    "the idea is that one can often find the optimal solution by exploring only a small fraction of the space by relying on tuned ( often problem dependent ) heuristics .",
    "however , a perfect heuristic is not always available .",
    "lds addresses the problem of what to do when the heuristic fails .",
    "thus , at each node of the search tree , the heuristic is supposed to provide the _ good _ choice ( corresponding to the leftmost branch ) among possible alternative branches .",
    "any other choice would be _ bad _ and is called a _",
    "discrepancy_. in lds , one tries to find first the solution with as few discrepancies as possible .",
    "in fact , a perfect heuristic would provide us the optimal solution immediately .",
    "since this is not often the case , we have to increase the number of discrepancies so as to make it possible to find the optimal solution after correcting the mistakes made by the heuristic .",
    "however , the goal is to use only few discrepancies since in general good solutions are provided soon .",
    "lds builds a search tree in the following way : the first solution explored is that suggested by the heuristic .",
    "then solutions that follow the heuristic for every variable but one are explored : these solutions are that of discrepancy equal to one .",
    "then , solutions at discrepancy equal to two are explored and so on .",
    "it has been shown that this search strategy achieves a significant cutoff of the total number of nodes with respect to a depth first search with chronological backtracking and iterative sampling @xcite .",
    "let @xmath10 be a digraph , where @xmath11 is the vertex set and @xmath12 the arc set , and let @xmath13 be the cost associated with arc @xmath14 ( with @xmath15 for each @xmath16 ) .",
    "hamiltonian circuit _",
    "( _ tour _ ) of @xmath17 is a partial digraph @xmath18 of @xmath17 such that : @xmath19 and for each pair of distinct vertices @xmath20 , both paths from @xmath21 to @xmath22 and from @xmath22 to @xmath21 exist in @xmath23 ( i.e. digraph @xmath23 is _ strongly connected _ ) .",
    "the travelling salesman problem ( tsp ) looks for a hamiltonian circuit @xmath24 whose cost @xmath25 is a minimum .",
    "a classic integer linear programming formulation for tsp is as follows :    @xmath26    where @xmath27 if and only if arc @xmath28 is part of the solution . constraints ( [ eq-2 ] ) and ( [ eq-3 ] ) impose in - degree and out - degree of each vertex equal to one , whereas constraints ( [ eq-4 ] ) impose strong connectivity .",
    "constraint programming relies in general on a different model where we have a domain variable @xmath29 ( resp .",
    "@xmath30 ) that identifies cities visited after ( resp . before )",
    "node @xmath1 . domain variable @xmath31 identifies the cost to be paid to go from node @xmath1 to node @xmath29 .",
    "clearly , we need a mapping between the cp model and the ilp model : @xmath32 .",
    "the domain of variable @xmath33 will be denoted as @xmath34 .",
    "initially , @xmath35 .",
    "the travelling salesman problem with time windows ( tsptw ) is a time constrained variant of the tsp where the service at a node @xmath1 should begin within a time window @xmath36 $ ] associated to the node .",
    "early arrivals are allowed , in the sense that the vehicle can arrive before the time window lower bound .",
    "however , in this case the vehicle has to wait until the node is ready for the beginning of service .",
    "as concerns the cp model for the tsptw , we add to the tsp model a domain variable @xmath37 which identifies the time at which the service begins at node @xmath1 .    a well known relaxation of the tsp and tsptw obtained by eliminating from the tsp model constraints ( [ eq-4 ] ) and time windows constraints is the _ linear assignment problem _ ( ap ) ( see @xcite for a survey ) .",
    "ap is the graph theory problem of finding a set of _ disjoint _ subtours such that all the vertices in @xmath38 are visited and the overall cost is a minimum .",
    "when the digraph is complete , as in our case , ap always has an optimal integer solution , and , if such solution is composed by a single tour , is then optimal for tsp satisfying constraints ( [ eq-4 ] ) .",
    "the information provided by the ap relaxation is a lower bound @xmath39 for the original problem and the reduced cost matrix @xmath40 .",
    "at each node of the decision tree , each @xmath41 estimates the additional cost to pay to put arc @xmath28 in the solution .",
    "more formally , a valid lower bound for the problem where @xmath42 is @xmath43 .",
    "it is well - known that when the ap optimal solution is obtained through a _ primal - dual _ algorithm , as in our case ( we use a c++ adaptation of the ap code described in @xcite ) , the reduced cost values are obtained without extra computational effort during the ap solution .",
    "the solution of the ap relaxation at the root node requires in the worst case @xmath44 , whereas each following ap solution can be efficiently computed in @xmath45 time through a single augmenting path step ( see @xcite for details ) .",
    "however , the ap does not provide a tight bound neither for the tsp nor for the tsptw .",
    "therefore we will improve the relaxation in section  [ lagr ] .",
    "in this section we describe the method proposed in this paper .",
    "it is based on two interleaved steps : subproblem generation and subproblem solution .",
    "the first step is based on the optimal solution of a ( possibly tight ) relaxation of the original problem .",
    "the relaxation provides a lower bound for the original problem and the reduced cost matrix .",
    "reduced costs are used for ranking ( the lower the better ) variable domain values .",
    "each domain is now partitioned according to this ranking in two sets called the _ good _ set and the _ bad _ set .",
    "the cardinality of the good set is problem dependent and is experimentally defined .",
    "however , it should be significantly lower than the dimension of the original domains .    exploiting this ranking , the search proceeds by choosing at each node the branching constraint that imposes the variable to range on the good domain , while on backtracking we impose the variable to range on the bad domain . by exploring the resulting search tree by using an lds strategy",
    "we generate first the most promising problems , i.e. , those where no or few variables range on the bad sets .",
    "each time we generate a subproblem , the second step starts for optimally solving it .",
    "experimental results will show that , surprisingly , even if the subproblems are small , the first generated subproblem almost always contains the optimal solution .",
    "the proof of optimality should then proceed by solving the remaining problems . therefore , a tight initial lower bound is essential .",
    "moreover , by using some considerations on discrepancies , we can increase the bound and prove optimality fast .",
    "the idea of ranking domain values has been previously used in incomplete algorithms , like grasp @xcite .",
    "the idea is to produce for each variable the so called restricted candidate list ( rcl ) , and explore the subproblem generated only by rcls for each variable .",
    "this method provides in general a good starting point for performing local search .",
    "our ranking method could in principle be applied to grasp - like algorithms .",
    "another connection can be made with iterative broadening @xcite , where one can view the breadth cutoff as corresponding to the cardinality of our good sets .",
    "the first generated subproblem of both approaches is then the same .",
    "however , iterative broadening behaves differently on backtracking ( it gradually restarts increasing the breadth cutoff ) .      in section [ problem ]",
    "we presented a relaxation , the linear assignment problem ( ap ) , for both the tsp and tsptw .",
    "this relaxation is indeed not very tight and does not provide a good lower bound .",
    "we can improve it by adding cutting planes .",
    "many different kinds of cutting planes for these problems have been proposed and the corresponding separation procedure has been defined @xcite . in this paper , we used the sub - tour elimination cuts ( secs ) for the tsp .",
    "however , adding linear inequalities to the ap formulation changes the structure of the relaxation which is no longer an ap . on the other hand , we are interested in maintaining this structure since we have a polynomial and incremental algorithm that solves the problem . therefore , as done in @xcite ,",
    "we relax cuts in a lagrangean way , thus maintaining an ap structure .    the resulting relaxation , we call it ap@xmath46",
    ", still has an ap structure , but provides a tighter bound than the initial ap .",
    "more precisely , it provides the same objective function value as the linear relaxation where all cuts are added defining the sub - tour polytope . in many cases , in particular for tsptw instances , the bound is extremely close to the optimal solution .",
    "as described in section [ problem ] , the solution of an assignment problem provides the reduced cost matrix with no additional computational cost .",
    "we recall that the reduced cost @xmath47 of a variable @xmath48 corresponds to the additional cost to be paid if this variable is inserted in the solution , i.e. , @xmath42 .",
    "since these variables are mapped into cp variables _ next _ , we obtain the same esteem also for variable domain values .",
    "thus , it is likely that domain values that have a relative low reduced cost value will be part of an optimal solution to the tsp .",
    "this property is used to partition the domain @xmath34 of a variable @xmath29 into @xmath4 and @xmath49 , such that @xmath50 and @xmath51 for all @xmath52 . given a ratio @xmath53 ,",
    "we define for each variable @xmath29 the good set @xmath4 by selecting from the domain @xmath34 the values @xmath54 that have the @xmath55 lowest @xmath47 .",
    "consequently , @xmath56 .",
    "the ratio defines the size of the good domains , and will be discussed in section  [ sc : quality ] .",
    "note that the _ optimal _ ratio should be experimentally tuned , in order to obtain the optimal solution in the first subproblem , and it is strongly problem dependent .",
    "in particular , it depends on the structure of the problem we are solving .",
    "for instance , for the pure tsp instances considered in this paper , a good ratio is 0.05 or 0.075 , while for tsptw the best ratio observed is around 0.15 . with this ratio ,",
    "the optimal solution of the original problem is indeed located in the first generated subproblem in almost all test instances .      in the previous section , we described how to partition variable domains in a good and a bad set by exploiting information on reduced costs .",
    "now , we show how to explore a search tree on the basis of this domain partitioning . at each node corresponding to the choice of variable @xmath3",
    ", whose domain has been partitioned in @xmath4 and @xmath5 , we impose on the left branch the branching constraint @xmath6 , and on the right branch @xmath7 . exploring with a limited discrepancy search strategy this tree , we first explore the subproblem suggested by the heuristic where all variable range on the good set ; then subproblems where all variables but one range on the good set , and so on .    if the reduced cost - based ranking criterion is accurate , as the experimental results confirm , we are likely to find the optimal solution in the subproblem @xmath8 generated by imposing all variables ranging on the good set of values .",
    "if this heuristic fails once , we are likely to find the optimal solution in one of the @xmath57 subproblems ( @xmath9 with @xmath58 ) generated by imposing all variables but one ( variable @xmath1 ) ranging on the good sets and one ranging on the bad set .",
    "then , we go on generating @xmath59 problems @xmath60 all variables but two ( namely @xmath1 and @xmath54 ) ranging on the good set and two ranging on the bad set are considered , and so on .    in section  [ implementation ] we will see an implementation of this search strategy that in a sense _ squeezes _ the subproblem generation tree shown in figure  [ fig : tree ] into a constraint .",
    "if we are simply interested in a good solution , without proving optimality , we can stop our method after the solution of the first generated subproblem . in this case , the proposed approach is extremely effective since we almost always find the optimal solution in that subproblem .",
    "otherwise , if we are interested in a provably optimal solution , we have to prove optimality by solving all sub - problems at increasing discrepancies .",
    "clearly , even if all subproblems could be efficiently solved , generating and solving all of them would not be practical .",
    "however , if we exploit a tight initial lower bound , as explained in section  [ lagr ] , which is successively improved with considerations on the discrepancy , we can stop the generation of subproblems after few trials since we prove optimality fast .",
    "an important part of the proof of optimality is the management of lower and upper bounds .",
    "the upper bound is decreased as we find better solutions , and the lower bound is increased as a consequence of discrepancy increase .",
    "the idea is to find an optimal solution in the first subproblem , providing the best possible upper bound .",
    "the ideal case is that all subproblems but the first can be pruned since they have a lower bound higher than the current upper bound , in which case we only need to consider a single subproblem .    the initial lower bound @xmath61 provided by the assignment problem ap@xmath46 at the root node can be improved each time we switch to a higher discrepancy @xmath62 . for @xmath63 ,",
    "let @xmath64 be the lowest reduced cost value associated with @xmath5 , corresponding to the solution of ap@xmath46 , i.e. @xmath65 .",
    "clearly , a first trivial bound is @xmath66 for all problems at discrepancy greater than or equal to 1 .",
    "we can increase this bound : let @xmath67 be the nondecreasing ordered list of @xmath64 values , containing @xmath57 elements .",
    "$ ] denotes the @xmath1-th element in @xmath67 .",
    "the following theorem achieves a better bound improvement @xcite .",
    "[ th : bound ] for @xmath69 , @xmath70 $ ] is a valid lower bound for the subproblems corresponding to discrepancy k.    the proof is based on the concept of additive bounding procedures @xcite that states as follows : first we solve a relaxation of a problem @xmath71 .",
    "we obtain a bound @xmath39 , in our case @xmath61 and a reduced - cost matrix @xmath72 .",
    "now we define a second relaxation of @xmath71 having cost matrix @xmath72 .",
    "we obtain a second lower bound @xmath73 .",
    "the sum @xmath74 is a valid lower bound for @xmath71 . in our case , the second relaxation is defined by the constraints imposing in - degree of each vertex less or equal to one plus a linear version of the _ k - discrepancy constraint _ : @xmath75 . thus ,",
    "@xmath76 $ ] is exactly the optimal solution for this problem .",
    "note that in general reduced costs are not additive , but in this case they are . as a consequence of this result",
    ", optimality is proven as soon as @xmath78   > { \\rm ub}$ ] for some discrepancy @xmath62 , where @xmath79 is the current upper bound .",
    "we used this bound in our implementation .",
    "once the subproblems are generated , we can solve them with any complete technique . in this paper , we have used the method and the code described in @xcite and in @xcite for the tsptw . as a search heuristic",
    "we have used the one behaving best for each problem .",
    "finding a subproblem of discrepancy @xmath62 is equivalent to finding a set @xmath80 with @xmath81 such that @xmath82 the search for such a set @xmath83 and the corresponding domain assignments have been ` squeezed ' into a constraint , the _ discrepancy constraint _ discr_cst .",
    "it takes as input the discrepancy @xmath62 , the variables @xmath29 and the domains @xmath4 .",
    "declaratively , the constraint holds if and only if exactly @xmath62 variables take their values in the bad sets .",
    "operationally , it keeps track of the number of variables that take their value in either the good or the bad domain . if during the search for a solution in the current subproblem the number of variables ranging on their bad domain is @xmath62 , all other variables are forced to range on their good domain .",
    "equivalently , if the number of variables ranging on their good domain is @xmath84 , the other variables are forced to range on their bad domain .",
    "the subproblem generation is defined as follows ( in pseudo - code ) :    ....     for ( k=0 ..",
    "n ) {       add(discr_cst(k , next , d_good ) ) ;       solve subproblem ;       remove(discr_cst(k , next , d_good ) ) ;     } ....    where k is the level of discrepancy , next is the array containing @xmath29 , and d_good is the array containing @xmath4 for all @xmath63 .",
    "the command solve subproblem is shorthand for solving the subproblem which has been considered in section  [ ssc : subpr ] .",
    "a more traditional implementation of lds ( referred to as ` standard ' in table  [ tb : tree_vs_cst ] ) exploits tree search , where at each node the domain of a variable is split into the good set or the bad set , as described in section  [ ssc : generation ] . in table",
    "[ tb : tree_vs_cst ] , the performance of this traditional approach is compared with the performance of the discrepancy constraint ( referred to as discr_cst in table  [ tb : tree_vs_cst ] ) . in this table results on tsptw instances ( taken from @xcite ) are reported .",
    "all problems are solved to optimality and both approaches use a ratio of 0.15 to scale the size of the good domains . in the next section , this choice is experimentally derived .",
    "although one method does not outperform the other , the overall performance of the discrepancy constraint is in general slightly better than the traditional lds approach .",
    "in fact , for solving all instances , we have in the traditional lds approach a total time of 2.75 with 1465 fails , while using the constraint we have 2.61 seconds and 1443 fails .",
    ".comparison of traditional lds and the discrepancy constraint . [ cols= \" < ,",
    "< , < , < , < \" , ]     table [ tb : results_tsp ] shows the results for small tsp instances .",
    "time is measured in seconds , fails again denote the total number of backtracks to prove optimality .",
    "the time limit is set to 300 seconds .",
    "observe that our method ( lds ) needs less number of fails than the approach without subproblem generation ( no lds ) .",
    "this comes with a cost , but still our approach is never slower and in some cases considerably faster .",
    "the problems were solved both with a ratio of 0.05 and 0.075 , the best of which is reported in the table .",
    "observe that in some cases a ratio of 0.05 is best , while in other cases 0.075 is better . for three instances optimality",
    "could not be proven directly after solving the first subproblem .",
    "nevertheless the optimum was found in this subproblem ( indicated by objective ` obj ' is ` opt ' ) .",
    "time and the number of backtracks ( fails ) needed in the first subproblem are reported for these instances .    in table",
    "[ tb : results_tsptw ] the results for the asymmetric tsptw instances are shown .",
    "our method ( lds ) uses a ratio of 0.15 to solve all these problems to optimality .",
    "it is compared to our code without the subproblem generation ( no lds ) , and to the results by focacci , lodi and milano ( flm2002 ) @xcite . up to now , flm2002 has the fastest solution times for this set of instances , to our knowledge .",
    "when comparing the time results ( measured in seconds ) , one should take into account that flm2002 uses a pentium iii 700 mhz .",
    "our method behaves in general quite well . in many cases",
    "it is much faster than flm2002 .",
    "however , in some cases the subproblem generation does not pay off .",
    "this is for instance the case for rbg040a and rbg042a .",
    "although our method finds the optimal solution in the first branch ( discrepancy 0 ) quite fast , the initial bound lb@xmath85 is too low to be able to prune the search tree at discrepancy 1 . in those cases",
    "we need more time to prove optimality than we would have needed if we did not apply our method ( no lds ) . in such cases",
    "our method can be applied as an effective _ incomplete _ method , by only solving first subproblem .",
    "table  [ tb : lds_0 ] shows the results for those instances for which optimality could not be proven directly after solving the first subproblem . in almost all cases",
    "the optimum is found in the first subproblem .",
    "we have introduced an effective search procedure that consists of generating promising subproblems and solving them . to generate the subproblems",
    ", we split the variable domains in a good part and a bad part on the basis of reduced costs .",
    "the domain values corresponding to the lowest reduced costs are more likely to be in the optimal solution , and are put into the good set .",
    "the subproblems are generated using a lds strategy , where the discrepancy is the number of variables ranging on their bad set .",
    "subproblems are considerably smaller than the original problem , and can be solved faster .",
    "to prove optimality , we introduced a way of increasing the lower bound using information from the discrepancies .",
    "computational results on tsp and asymmetric tsptw instances show that the proposed ranking is extremely accurate . in almost all cases",
    "the optimal solution is found in the first subproblem . when proving optimality is difficult , our method can still be used as an effective incomplete search procedure , by only solving the first subproblem .",
    "some interesting points arise from the paper : first , we have seen that reduced costs represent a good ranking criterion for variable domain values .",
    "the ranking quality is not affected if reduced costs come from a loose relaxation .",
    "second , the tightness of the lower bound is instead very important for the proof of optimality .",
    "therefore , we have used a tight bound at the root node and increased it thus obtaining a discrepancy - based bound .",
    "third , if we are not interested in a complete algorithm , but we need very good solutions fast , our method turns out to be a very effective choice , since the first generated subproblem almost always contains the optimal solution .",
    "future directions will explore a different way of generating subproblems .",
    "our domain partitioning is statically defined only at the root node and maintained during the search . however , although being static , the method is still very effective .",
    "we will explore a dynamic subproblem generation .",
    "we would like to thank filippo focacci and andrea lodi for useful discussion , suggestions and ideas .",
    "g.  carpaneto , s.  martello , and p.  toth .",
    "algorithms and codes for the assignment problem . in b.",
    "simeone et  al . , editor , _ fortran codes for network optimization - annals of operations research _ , pages 193223 .",
    "1988 .",
    "f.  focacci , p.  laborie , and w.  nuijten .",
    "solving scheduling problems with setup times and alternative resources . in _ proceedings of the fifth international conference on artificial  intelligence planning and scheduling ( aips2000 )"
  ],
  "abstract_text": [
    "<S> in this paper , we propose an effective search procedure that interleaves two steps : subproblem generation and subproblem solution . </S>",
    "<S> we mainly focus on the first part . </S>",
    "<S> it consists of a variable domain value ranking based on reduced costs . exploiting the ranking , we generate , in a limited discrepancy search tree , the most promising subproblems first . </S>",
    "<S> an interesting result is that reduced costs provide a very precise ranking that allows to almost always find the optimal solution in the first generated subproblem , even if its dimension is significantly smaller than that of the original problem . concerning the proof of optimality </S>",
    "<S> , we exploit a way to increase the lower bound for subproblems at higher discrepancies . </S>",
    "<S> we show experimental results on the tsp and its time constrained variant to show the effectiveness of the proposed approach , but the technique could be generalized for other problems . </S>"
  ]
}