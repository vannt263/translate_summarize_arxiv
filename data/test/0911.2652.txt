{
  "article_text": [
    "the interpretation of astrophysical spectra rests heavily upon the availability of relevant atomic and molecular data . of critical importance are ( 1 ) the energies of the various accessible quantum states , ( 2 ) the rates of spontaneous radiative decay between states , and ( 3 ) the state - to - state rate coefficients for excitation and deexcitation in inelastic collisions .",
    "the last of these have proven quite elusive ; in many circumstances , our ability to interpret astrophysical spectra is severely limited by the availability of collisional rate coefficients . as a result of new spectroscopic facilities such as the _",
    "spitzer space telescope _",
    ", the _ herschel space observatory _ , and the planned atacama large millimetric array ( alma ) , the need for fundamental molecular data has become all the more acute . in modeling the emission from interstellar molecular clouds ,",
    "in particular , there is a critical need for rate coefficients for the rotational excitation of molecules in collisions with h@xmath3 .",
    "the theoretical calculation of such data is extremely time consuming , involving the computation of a potential energy surface for the system in question , followed by extensive scattering calculations .",
    "even for those molecules for which such calculations have been carried out , the available data are often limited to collisionally - excited collisions involving a relatively small set of low - lying states .",
    "submillimeter and infrared spectra of molecular clouds often reveal the presence of higher - lying transitions for which collisional rate coefficients have not been computed ; here , some kind of extrapolation method is needed . for the case of linear molecules ,",
    "the infinite order sudden ( ios ) and related approximations provide an obvious method of extrapolation ; for asymmetric molecules , there is no simple analogous method .    given a limited set of transitions of an asymmetric molecule for which collisional rate coefficients have been computed , previous efforts at extrapolation ( e.g.  neufeld & melnick 1987 , faure & josselin 2008 ) have typically adopted an expression with one or more adjustable parameters for how the collisional rate coefficient depends upon the relevant parameters for each transition ( energy difference , change in rotational quantum number , etc . ) the adjustable parameters are then chosen so as to optimize the fit to those transitions for which collisional rate coefficients are known , and then the expression is used to estimate the values for higher - lying transitions where calculated results are unavailable . for example , the recent study by faure & josselin ( 2008 ) considered the rate coefficients for the excitation of h@xmath3o by h@xmath3 , which will be of crucial importance to the interpretation of many _ spitzer _ and _ herschel _ spectra . here , faure & josselin extrapolated a set of rate coefficients obtained from quasi - classical trajectory calculations for transitions among the lowest 45 rotational states of water .",
    "each such state is characterized by an energy , @xmath4 , and rotational quantum numbers @xmath5 , @xmath6 and @xmath7 that describe the total angular momentum and its projection onto the principal axes of the molecule . for transitions with @xmath8 , @xmath9 and @xmath10  termed  high - propensity \" transitions ",
    "faure & josselin assumed that rate coefficients depend solely upon @xmath11 , @xmath12 and @xmath13 .",
    "for all other transitions (  low propensity transitions \" ) , they assumed the rate coefficients to be _ independent _ of @xmath11 , @xmath12 and @xmath13 and proportional to @xmath14 , where @xmath15 is adjusted to optimize the fit to the available data .",
    "as an alternative to the methods adopted previously , i have explored the use of an artificial neural network ( ann ) to accomplish the extrapolation .",
    "in contrast to previous techniques , the use of an ann does not make any _ a priori _ assumption about how the transitions might be categorized or how the rate coefficients might depend upon the parameters for each transition .",
    "the calculational method is discussed in  2 ( with the more technical details given in the appendix ) . the performance of the ann in reproducing a known set of collisional rate coefficients in presented in  3 .",
    "a discussion follows in  4 .",
    "since the mid-1980 s , artificial neural networks have been widely used to solve a wide variety of problems , particularly in pattern recognition .",
    "previous astrophysical applications have centered primarily on the automatic classification of astronomical objects based on morphology , photometry or spectroscopy ( e.g.  ball et al .",
    "2004 , collister & lahav 2004 ) , although ann have also been used previously as approximation tools ( e.g. asensio ramos & socas - navarro 2005 ) .",
    "anns consist of a set of interconnected artificial neurons .",
    "such neurons are of three types : input neurons , output neurons , and hidden neurons . given a set of input values that are fed into the input neurons , a unique set of output values emerges from the output neurons .",
    "in the context of the application considered here , the input neurons are fed the relevant parameters for each transition ( e.g. the energies and relevant quantum numbers for the upper and lower states ) , and the collisional rate coefficient emerges from a single output neuron .",
    "the hidden neurons mediate the dependence of the ann s output upon its input .",
    "each individual neuron accepts a particular input value , and outputs a value that is  in general  a non - linear function of that input .",
    "this function is called the activation function , and is typically sigmoidal in shape .",
    "( the inverse hyperbolic tangent is an often - used activation function in ann . ) with the exception of the input neurons , for which the inputs are a set of input parameters , the input for any neuron is the weighted sum of the outputs of a set of other neurons to which it is connected . during the  learning process ``",
    ", the ann is shown a ' ' training set \" of input parameters , together with the appropriate outputs ; the relevant weights of the connections are varied until the outputs ( from the output neurons ) match the appropriate values to within the desired accuracy .",
    "the arrangement of the connections between the neurons is called the  architecture \" of the network . in ",
    "feed - forward \" networks such as the one used in this study , the structure is non - recursive : this means that , when considered in the appropriate order , the input value for any neuron reflects only output values for neurons that have already been considered . in some ann , the architecture is fixed , and the learning process simply involves varying the weights of the connections . in other schemes , such as the cascade - correlation network",
    "introduced by fahlman and lebiere ( 1990 ) and used in the present study , the architecture evolves during the learning process , with hidden neurons trained and added one at a time to improve the performance of the ann .",
    "figure 1 shows a simple example of a cascade - correlation network , with 3 input neurons ( blue circles numbered 1  3 ) , 2 hidden neurons ( numbered 4 and 5 ) , and one output neuron ( number 6 ) .",
    "the @xmath16th neuron has an input value @xmath17 ( shown in red ) and an output value @xmath18 = @xmath19 ( green ) , where @xmath20 is an activation function ( which may , in principle , be different for each neuron  see appendix ) .",
    "information propagates from left to right , starting with the input values @xmath21 , @xmath22 and @xmath23 , and ending with the output value @xmath24 .",
    "the @xmath17 and @xmath18 are related by a matrix of weights , * w * , with @xmath25 . in the example shown here",
    ", * w * has 12 non - zero elements . at the end of training",
    ", the weights have assumed values that optimize the performance on the training set : in other words , @xmath24 is close to the desired output value for each given set of inputs , @xmath26 $ ] .    in this study ,",
    "i used the fast artificial neural network ( fann ) library , an open source software package developed by steffen nissen and evan nemerson .",
    "full details of my use of this software are presented in the appendix ; my intent there is to provide enough specificity to allow the reader to reproduce the calculational method exactly .",
    "a copy of the relevant computer code , written in c , will be provided upon request to the author .    in evaluating the performance of an ann ,",
    "it is essential to train the network on a  training set \" of data and then test its performance on a  test set \" _ that it has never seen .",
    "given sufficient complexity in the network ( i.e. enough hidden neurons ) , it is always possible to fit the training set to any desired degree of accuracy .",
    "the key question is how well the ann performs on new , unseen data . in this study ,",
    "i considered the collisional rate coefficients computed recently by dubernet et al .",
    "( 2009 ) for the excitation of ortho - h@xmath3o in collisions with para - h@xmath3 .",
    "the close - coupling calculations that led to these results represent the state - of - the - art in scattering calculations and are extremely expensive computationally .",
    "this dataset provides the rate coefficients for all transitions among the 45 lowest rotational states of ortho - h@xmath3o , computed for several temperatures and several values of the initial and final rotational states of the colliding para - h@xmath3 molecule . for simplicity ,",
    "i considered first the values obtained at a single temperature , 800  k , and with the initial and final h@xmath3 states being @xmath27 .",
    "this dataset then contains @xmath28 independent values , where @xmath29 is the total number of water rotational states .",
    "the factor of @xmath30 arises because the upwards and downwards rates for any transition are related by considerations of detailed balance and are therefore not independent of each other .",
    "as the training set , i adopted a subset of these 990 data points ; because the ultimate goal of this method is to estimate collisional rate coefficients for transitions that are _ higher _ in energy than those calculated previously , i chose a training set consisting of those collisional - induced transitions between the _ lowest _ @xmath31 rotational states , where @xmath32 .",
    "the training set then consists of @xmath33 data points , with the test set comprising the remaining @xmath34 data points .",
    "one key methodological choice concerns the input parameters that will be provided to the ann ; this determines the number of input neurons .",
    "each rotational state of ortho - h@xmath3o is characterized by an energy @xmath4 , and three rotational quantum numbers : @xmath5 , which describes the total rotational angular momentum , @xmath6 , which describes its projection on the principal axis for which the moment of inertia is smallest , and @xmath7 , which describes its projection on the principal axis for which the moment of inertia is largest . however , once @xmath5 and @xmath6 are specified , @xmath7 is uniquely determined .",
    "thus it is convenient to define a parameter @xmath35 , which allows each rotational state of ortho - h@xmath3o to be specified unambiguously by just two quantum numbers , @xmath5 and @xmath36 .",
    "the integer @xmath36 takes all odd values between @xmath37 and @xmath38 . in the present study ,",
    "i experimented with three choices for the input parameter set .",
    "the three cases considered were : ( 1 ) the ann is fed just the energy difference between the initial and final state , @xmath39 , and therefore has one input neuron ; ( 2 ) the ann is fed @xmath39 , @xmath13 , and @xmath40 , and therefore has 3 input neurons ; ( 3 ) the ann is fed @xmath4 , @xmath5 , and @xmath36 for both the initial and final states , requiring a total of 6 input neurons . in  3 below , the performance of the ann is described for each of these three cases . in all cases , the ann has single output neuron which provides the rate coefficient for the downward collisionally - induced transition . because the rate coefficients cover several orders of magnitude , i trained the network to output the logarithm of the rate coefficient .",
    "two other choices , more technical in nature , determine the performance of the ann .",
    "the final number of hidden neurons , @xmath41 , is discussed in the appendix . based upon",
    "the experimentation described there , i adopted @xmath42 as the optimum value . with too few hidden neurons",
    ", the ann fails to fit even the training data set well ; with too many hidden neurons , the network fits the training set perfectly but fails to generalize well to the test set . because the initial weights on the connections are randomly assigned , the results vary slightly every time the ann is trained .",
    "accordingly , i took the average of the outputs of several ann in obtaining results for the test set .",
    "as discussed in appendix a , the performance of the ann fails to improve once the number of realizations , @xmath43 , exceeds @xmath44 ; this behavior indicates that the remaining errors in the ann predictions are not a consequence of the particular choice of initial weights , but are instead a fundamental limitation of the method .",
    "the results presented in  3 below all apply to an ann with @xmath42 and @xmath45",
    "the top right , bottom left , and bottom right panels in figure 2 present the performance of an ann with @xmath46 for the cases with 1 , 3 , and 6 input neurons discussed in  2 above . in each panel ,",
    "the horizontal axis represents the logarithm of the rate coefficient ( in units of @xmath47 ) obtained by dubernet et al .",
    "( 2009 ) , and the vertical axis represents the corresponding output of the ann .",
    "red points apply to the 435 data points in the training set , while blue points apply to the 555 data points in the test set ( i.e.  to those transitions that were not seen by the ann during training ) .",
    "as expected from the discussion of high and low propensity transitions presented by faure & josselin ( 2008 ) , the ann performs much better for the cases with 3 and 6 input neurons ; only in these cases , is it informed about the rotational quantum numbers along with the transition energy .",
    "indeed , the performance in the 1-input - neuron is strikingly similar to the results obtained if the data are all fit with an expression of the form @xmath48 . adopting best - fit values of @xmath49 and @xmath15 determined from a least squares fit to the data yields an expression which is compared to the actual data in the top left panel of figure 2 .",
    "more detailed statistical information is provided in figures 3 and 4 . in figure 3 ,",
    "the frequency distribution of the errors , @xmath50 , is shown for each case .",
    "black histograms apply to the entire test set , while blue , green and red histograms apply to transitions with low , medium and high actual rate coefficients ; for this purpose ,  low \" applies to value smaller than @xmath51 ,  medium \" to value in the range @xmath52 , and ",
    "high \"  to a value greater than @xmath53 . clearly , the cases with 3 and 6 neurons yield histograms that are more strongly peaked than that obtained with 1 input neuron , indicating that the typical error in the ann output is considerably smaller .",
    "while the probability distributions for the 3- and 6-input - neuron cases are very similar when all the test data are considered together ( black histogram ) , the 6-input - neuron ann systematically underpredicts the results for transitions with high rate coefficients ( red histogram ; this behavior is also apparent from a careful inspection of figure 2 . )",
    "paradoxically , the ann performs worse when it is provided with additional information ( i.e. the individual @xmath4 , @xmath5 , @xmath36 values for the upper and lower states rather than just the differences between those values , @xmath39 , @xmath13 , @xmath40 ) . in the 6-input - neuron case",
    ", the ann is evidently fitting a trend with energy , @xmath4 , that is present for the lower energy transitions ( i.e.  for the training set ) , but extrapolates poorly to the higher - energy transitions encountered in the test set .",
    "based upon figures 2 and 3 , i adopted the 3-input - neuron ann as the optimum architecture , along with the values @xmath42 and @xmath54 discussed in the appendix . for this architecture ,",
    "the distribution of the output errors is presented in an alternative form in figure 4 . here",
    "i show the fraction of the data for which @xmath55 exceeds a given value , @xmath56 . the fraction of rate coefficients is shown on the vertical axis ( logarithmic scale ) , with the same color coding adopted in figure 3 , as a function of @xmath56 . for example",
    ", the fact that the red curve passes through the point ( 0.19 , 0.30 ) implies that one - half ( i.e.  10@xmath57 ) of those rate coefficients with high actual values ( i.e. greater than @xmath53 ) are in error by more than a factor of 1.5 ( i.e.  @xmath58 ) once again , results apply only to the test set .",
    "clearly , the ann output is less reliable for transitions with rate coefficients smaller than @xmath51 ( blue curve ) . even in this case",
    ", however , more than half of the rate coefficients are correct to within a factor 2.1    the results shown in figures 2  4 were all obtained with @xmath59 , for which the training set comprises the lowest 435 transitions and the test set contains the next 555 transitions .",
    "i have also investigated the performance of the ann as a function of the size of the training set , i.e.  as a function of @xmath31 . in figure 5 , i show the rms value of @xmath60 , as a function of n@xmath61 , for the entire set of transitions ( black points ) and for low , medium and high values of the actual rate ( blue , dark green and red as before ) .",
    "results are shown for @xmath62 , 15 , 20 , 25 , 30 , 35 , and 40 .",
    "these correspond to training sets containing 45 , 105 , 190 , 300 , 435 , 595 , and 780 data points , respectively , and test sets containing 945 , 785 , 800 , 690 , 555 , 395 , and 210 data points .",
    "the overall performance varies little once @xmath63 , i.e.  once the training set encompasses @xmath64 of the total available data .    one key application of the present study , of course ,",
    "will be to train the ann on all 990 transitions for which dubernet et al .",
    "have computed collisional rate coefficients , and to then use the ann to estimate the rates for higher - lying transitions for which no calculations have been performed .",
    "the results of figure 5 provide a gauge of the likely success of that procedure , and of the number of additional rate coefficients that can be estimated reliably .",
    "if the accuracy of the extrapolation depends upon the _ relative _ size of the training set ( i.e.  the _ ratio _ of the number of transitions in the training set to the total number for which ann estimates are to be obtained ) , we may expect that an additional @xmath65 rate coefficients can be predicted with the use of an ann .",
    "on the other hand , it is possible that the _ absolute _ size of the training set is the relevant parameter ; in that case , there is no obvious limit to how far the extrapolation can be carried .",
    "a conservative approach would be to use an ann to extrapolate no more than an additional @xmath65 rate coefficients ; the resultant data set would include all transitions among the lowest @xmath66 rotational states of o - h@xmath3o . for higher - lying transitions , the method of faure & josselin ( 2008 ) promises to be more robust : in that method , the results for high - propensity transitions are obviously well - behaved , and those for low - propensity transitions tend gracefully to zero as @xmath39 tends to infinity . with the ann , by contrast , the behavior could be unstable if one extrapolates to a @xmath39 ( or @xmath13 or @xmath40 ) that is much larger than anything it saw in the training set .",
    "while the collisional excitation of water has received a great deal of theoretical attention recently , many other molecules will be observed by alma and herschel .",
    "figure 6 shows results obtained for three other molecules for which there is a large set of collisional rate coefficients in the basecol database : so@xmath3 ( from green 1995 ) , ortho - c - c@xmath67h@xmath3 ( chandra & kegel 2000 ) , and ortho - h@xmath3co ( green 1991 ) . like water , these are asymmetric top molecules with rotational states that can be characterized by the quantum numbers @xmath5 and @xmath36 , although in these cases , the available data apply to excitation in collisions with he .",
    "i have also obtained additional results for the collisional excitation of ortho - water with para - h@xmath3 , again using rate coefficients from dubernet et al .",
    "( 2009 ) but now for cases in which the initial and/or final state of h@xmath3 is @xmath68 . in each case , the training set consisted of the lower - lying half of the entire available data set .",
    "the rms value of @xmath60 for the test set is printed on each panel ; as in the case of ortho - h@xmath3o colliding with h@xmath3 ( @xmath69 ) , typical rms values are roughly 0.35",
    "\\1 . artificial neural networks ( ann ) can be successfully trained to determine rate coefficients for the collisional excitation of astrophysical molecules .",
    "the type of ann favored here is a cascade correlation network which creates 16 hidden neurons during the course of training .",
    "there are three input neurons that inform the network of @xmath39 , @xmath13 and @xmath40 for a given transition , and one output neuron that provides the logarithm of the rate coefficient for collisional de - excitation .",
    "the performance of such an ann can be evaluated by training it on a subset of the set of transitions for which collisional rate coefficients are known , and then testing it on the remainder of that set . since the motivation for this study is to develop a method for estimating the rate coefficients for high - lying transitions for which values have not been computed , the training set involved the lowest - lying transitions among the available data .",
    "\\5 . in the primary test case considered here ,",
    "the excitation of ortho - h@xmath3 by h@xmath3 ( @xmath27 ) , the available data  consisting of 990 rate coefficients  can be modeled equally well provided that the network is trained on at least @xmath70 of the available data .",
    "the performance of the ann is summarized in table 1 , which lists the typical output errors in every case considered . here",
    "the rms and median values of @xmath71 value ) are listed for the entire test set and separately for low , medium and high values of the actual rate .",
    "overall , for ann with 16 hidden neurons and 3 input neurons that are trained on at least 20@xmath2 of the available data , the typical rms value of @xmath60 is @xmath72 ; for transitions with rate coefficients @xmath73 , the typical rms value is @xmath74 .",
    "the corresponding median values of @xmath75 are @xmath76 and @xmath77 .",
    "these median values imply that one - half of the ann outputs are typically accurate to within factors of 1.8 ( or @xmath0 in the case of rate coefficients @xmath73 . )",
    "however , the results shown in figure 4 indicate that some of the ann outputs are highly discrepant ( with 4@xmath2 in error by more than a factor 10 ) .",
    "it should be noted that the errors given here apply to individual state - to - state rate coefficients . when these are used to model molecular line emissions , the errors in the resultant line strengths can be expected to be smaller than in the state - to - state rate coefficients because a given radiative transitions may be pumped by a combination of several different collisionally - induced transitions .",
    "the calculation made use of the fast artificial neural network ( fann ) library , version 2.0.0 , an open source software package developed by steffen nissen and evan nemerson and available at http://fann.sourceforge.net .",
    "the cascade training option was adopted for this problem , and was found to result in rapid training . in this algorithm ( fahlman & lebiere 1990 ) , the training starts with an empty network devoid of hidden neurons .",
    "hidden neurons are trained and added to the network one at a time ; each new hidden neuron is chosen from a selection of candidate neurons with different initial weights and one of several possible activation functions .    in using the fann library ,",
    "the input neuron values were rescaled to small values ( relative to unity ) to ensure that the input neurons did not saturate : energies ( given as wavenumber in units of @xmath78 ) were therefore divided by @xmath79 and rotational quantum numbers were divided by 100 .",
    "no other preprocessing was performed . with these rescalings ,",
    "the input values lay in the ranges [ 0 , 0.14 ] , [ 0.03 , 0.10 ] , and [ 0.16 , 0.18 ] respectively for @xmath39 , @xmath13 , and @xmath40 .",
    "the desired output neuron values were the logarithm to base 10 of the rate coefficient in units of @xmath80 , and covered the range [ 15.3 , 9.9 ]    the cascade algorithm was used with all the default settings except for one : a linear function was selected as the activation function for the output layer .",
    "this choice was necessitated by the fact that the output neuron values lay outside the range of the other activation functions .",
    "the desired error was set to a very small value to ensure that the ann made use of the maximum number of hidden neurons specified for each test .",
    "the default settings for the fann software make six activation functions available to the algorithm , all of which are continuous functions with a domain of [ @xmath81 and a range of either [ 0 , 1 ] or [ 1 , 1 ] .",
    "the network is optimized using the i - rprop algorithm of igel & hsken ( 2000 ) , a variant of the rprop algorithm introduced by riedmiller & braun ( 1993 ) .",
    "even if the settings and training set are unchanged , the results are slightly different every time the ann is run .",
    "this is because the initial weights are randomly assigned at the start of training .",
    "i therefore averaged the output values over a set of @xmath43 runs .",
    "figure 7 shows how the performance of the network depends upon @xmath43 , for a network with 3 input neurons and 16 hidden neurons and with @xmath59 . as in figures 3",
    " 5 , black curves apply to the entire test set , while blue , green and red histograms apply to transitions with low , medium and high actual rate coefficients ; for this purpose ,  low \" applies to value smaller than @xmath51 ,  medium \" to value in the range @xmath82 , and  high \"  to a value greater than @xmath53 .",
    "the magenta curve refers to the training set .",
    "clearly , the performance on the test set is improved if multiple runs are averaged , but for @xmath43 greater than a few , no further improvements are realized .",
    "based upon the behavior exhibited in figure 7 , i selected @xmath45 as the standard parameter for the calculations that yielded the results discussed in  3 .",
    "figures 8 and 9 show how the performance depends upon the final number of hidden neurons .",
    "the color coding in figure 9 is the same as that in figure 7 .",
    "clearly , if enough hidden neurons are provided , the fit to the training set becomes extremely accurate ( lower right panel in figure 8) , with the rms error for 64 neurons being less than 3@xmath2 .",
    "however , the performance on the _ test _ set worsens somewhat once the number of hidden neurons exceeds 16 .",
    "this behavior indicates that the network is  overfitting \" the data ( e.g.  smith 1993 ) .",
    "the following analogy is presented by the use of polynomial fitting functions . given a function of one variable that has been evaluated at @xmath83 values , a perfect polynomial fit to those values can always be obtained with a polynomial of order @xmath84 .",
    "however , such a polynomial may show strong wiggles between ( and beyond ) the points where the function was evaluated ( particularly if the function evaluations have errors ) .",
    "a lower order polynomial may fit the data points imperfectly but may be preferable for the purposes of interpolation and particularly extrapolation . based upon the behavior exhibited in figure 8 and 9 , i selected @xmath85 as the standard parameter for the calculations that yielded the results discussed in  3 .",
    "i thank george fekete for helpful discussions about ann , and alexandre faure for several valuable comments on an earlier version of the manuscript .",
    "i am grateful to the anonymous referee for several useful suggestions .",
    "i gratefully acknowledge the support of funding from research support agreement 1349625 issued by nasa s jet propulsion laboratory .",
    "dubernet , m. , grosjean , a. , daniel , f. , et al .",
    "2006 , in ro - vibrational collisional excitation database : basecol  http://basecol.obspm.fr ( japan : journal of plasma and fusion research series , series 7 )                        & & & & & & & all & low@xmath86 & med .",
    "@xmath86 & high@xmath86 & all & low@xmath86 & med .",
    "@xmath86 & high@xmath86 + 2 & 3 & 435 & 555 & o - h@xmath3o & h@xmath3 00@xmath87 & 800 & 0.483 & 0.652 & 0.394 & 0.425 & 0.284 & 0.374 & 0.259 & 0.292 + 4 & 3 & 435 & 555 & o - h@xmath3o & h@xmath3 00 & 800 & 0.461 & 0.634 & 0.373 & 0.359 & 0.266 & 0.386 & 0.237 & 0.208 + 8 & 3 & 435 & 555 & o - h@xmath3o & h@xmath3 00 & 800 & 0.417 & 0.527 & 0.366 & 0.352 & 0.239 & 0.279 & 0.227 & 0.178 + 16 & 3 & 435 & 555 & o - h@xmath3o & h@xmath3 00 & 800 & 0.417 & 0.517 & 0.371 & 0.350 & 0.247 & 0.327 & 0.227 & 0.186 + 32 & 3 & 435 & 555 & o - h@xmath3o & h@xmath3 00 & 800 & 0.446 & 0.559 & 0.391 & 0.411 & 0.259 & 0.345 & 0.235 & 0.207 + 64 & 3 & 435 & 555 & o - h@xmath3o & h@xmath3 00 & 800 & 0.471 & 0.569 & 0.422 & 0.473 & 0.288 & 0.364 & 0.235 & 0.176 +   + 16 & 3 & 45 & 945 & o - h@xmath3o & h@xmath3 00 & 800 & 0.915 & 1.322 & 0.809 & 0.538 & 0.458 & 0.784 & 0.433 & 0.286 + 16 & 3 & 105 & 885 & o - h@xmath3o & h@xmath3 00 & 800 & 0.675 & 1.007 & 0.561 & 0.491 & 0.358 & 0.473 & 0.314 & 0.423 + 16 & 3 & 190 & 800 & o - h@xmath3o & h@xmath3 00 & 800 & 0.474 & 0.653 & 0.416 & 0.299 & 0.291 & 0.402 & 0.279 & 0.143 + 16 & 3 & 300 & 690 & o - h@xmath3o & h@xmath3 00 & 800 & 0.409 & 0.531 & 0.364 & 0.305 & 0.237 & 0.306 & 0.225 & 0.152 + 16 & 3 & 435 & 555 & o - h@xmath3o & h@xmath3 00 & 800 & 0.417 & 0.517 & 0.371 & 0.350 & 0.247 & 0.327 & 0.227 & 0.186 + 16 & 3 & 595 & 395 & o - h@xmath3o & h@xmath3 00 & 800 & 0.391 & 0.430 & 0.377 & 0.307 & 0.227 & 0.264 & 0.222 & 0.139 + 16 & 3 & 780 & 210 &",
    "o - h@xmath3o & h@xmath3 00 & 800 & 0.381 & 0.447 & 0.341 & 0.328 & 0.214 & 0.258 & 0.199 & 0.123 +   + 16 & 3 & 435 & 555 & o - h@xmath3o & h@xmath3 00 & 800 & 0.417 & 0.517 & 0.371 & 0.350 & 0.247 & 0.327 & 0.227 & 0.186 + 16 & 6 & 435 & 555 & o - h@xmath3o & h@xmath3 00 & 800 & 0.406 & 0.441 & 0.353 & 0.691 & 0.293 & 0.352 & 0.249 & 0.615 + 16 & 1 & 435 & 555 & o - h@xmath3o & h@xmath3 00 & 800 & 0.725 & 0.906 & 0.613 & 0.889 & 0.420 & 0.466 & 0.393 & 0.829 +   +   +   +   + & & & & & & & all & low@xmath86 & med .",
    "@xmath86 & high@xmath86 & all & low@xmath86 & med .",
    "@xmath86 & high@xmath86 + 16 & 3 & 495 & 495 & o - h@xmath3o & h@xmath3 00@xmath87 & 800 & 0.433 & 0.577 & 0.353 & 0.372 & 0.234 & 0.314 & 0.209 & 0.239 + 16 & 3 & 495 & 495 & o - h@xmath3o & h@xmath3 02 & 800 & 0.338 & 0.675 & 0.276 & 0.361 & 0.195 & 0.450 & 0.174 & 0.224 + 16 & 3 & 495 & 495 & o - h@xmath3o & h@xmath3 20 & 800 & 0.325 & 0.368 & 0.274 & n / a & 0.188 & 0.229 & 0.163 & n / a + 16 & 3 & 495 & 495 & o - h@xmath3o & h@xmath3 22 & 800 & 0.342 & 0.457 & 0.329 & 0.368 & 0.210 & 0.319 & 0.202 & 0.221 +   + 16 & 3 & 540 & 541 & o - c - c@xmath67h@xmath3 & he & 30 & 0.325 & n / a & 0.269 & 0.414 & 0.208 & n / a &",
    "0.171 & 0.316 + 16 & 3 & 602 & 603 & so@xmath3 & he & 25 & 0.341 & 0.361 & 0.327 & 0.369 & 0.205 & 0.220 & 0.187 & 0.368 + 16 & 3 & 346 & 346 & o - h@xmath3co & he & 30 & 0.288 & 0.636 & 0.270 & 0.221 & 0.168 & 0.646 & 0.155 & 0.163 +        fig .  1  simple example of a cascade - correlation network , with 3 input neurons ( blue circles numbered 1  3 ) , 2 hidden neurons ( numbered 4 and 5 ) , and one output neuron ( number 6 ) .",
    "the @xmath16th neuron has an input value @xmath17 ( red ) and an output value @xmath18 = @xmath19 ( green ) , where @xmath20 is an activation function ( which may , in principle , be different for each neuron  see appendix ) .",
    "information propagates from left to right , starting with the input values @xmath21 , @xmath22 and @xmath23 , and ending with the output value @xmath24 .",
    "the @xmath17 and @xmath18 are related by a matrix of weights , * w * , with @xmath25 .",
    "the algebraic expressions shown on the figure show that the quantities @xmath88 , @xmath89 , @xmath90 , @xmath91 , @xmath92 , @xmath93 , @xmath94 , @xmath95 , and @xmath24 can be computed non - recursively ( i.e.  explicitly , in the above order ) , given input values of @xmath21 , @xmath22 , and @xmath23 .        fig .",
    "2  the top right , bottom left , and bottom right panels present the performance of an ann with @xmath46 for the cases with 1 , 3 , and 6 input neurons discussed in  2 . in each panel , the horizontal axis represents the logarithm of the rate coefficient ( in units of @xmath47 ) obtained by dubernet et al .",
    "( 2009 ) , and the vertical axis represents the corresponding output of the ann .",
    "red points apply to the 435 data points in the training set , while blue points apply to the 555 data points in the test set ( i.e.  to those transitions that were not seen by the ann during training ) .",
    "upper right panel : analogous results if the data are all fit with an expression of the form @xmath48 .",
    "3  the frequency distribution of the errors , @xmath50 , for each of the 4 cases shown in fig .  1 .",
    "black histograms apply to the entire test set , while blue , green and red histograms apply to transitions with low , medium and high actual rate coefficients ; for this purpose ,  low \" applies to value smaller than @xmath51 ,  medium \" to value in the range @xmath82 , and  high \"  to a value greater than @xmath53 .",
    "fig .  4  fraction of the test data for which @xmath55 exceeds a given value , @xmath56 .",
    "the fraction of rate coefficients is shown on the vertical axis ( logarithmic scale ) , with the same color coding adopted in figure 2 , as a function of @xmath56 .",
    "the left panel is simply a zoomed - in version of the right panel .",
    "fig .  5  rms value of @xmath60 , as a function of n@xmath61 , for the entire set of transitions ( black points ) and for low , medium and high values of the actual rate ( blue , dark green and red as in figs .  2 and 3 ) .",
    "6  results obtained for excitation of so@xmath3 ( from green 1995 ) , ortho - c - c@xmath67h@xmath3 ( chandra & kegel 2000 ) , and ortho - h@xmath3co ( green 1991 ) in collisions with he ; and for excitation of ortho - water in collisions with para - h@xmath3 ( dubernet et al .",
    "( 2009 ) for cases in which the initial and/or final state of h@xmath3 is @xmath68 .",
    "fig .  7  dependence of the performance of the ann upon @xmath43 , for a network with 3 input neurons and 16 hidden neurons and with @xmath59 .",
    "black curves apply to the entire test set , while blue , green and red histograms apply to transitions with low , medium and high actual rate coefficients ; for this purpose ,  low \" applies to value smaller than @xmath51 ,  medium \" to value in the range @xmath82 , and  high \"  to a value greater than @xmath53 .",
    "the magenta curve refers to the training set .",
    " performance of an ann with 3 input neurons and @xmath59 .",
    "four panels shows the results for 8 , 16 , 32 , and 64 hidden neurons . in each panel",
    ", the horizontal axis represents the logarithm of the rate coefficient ( in units of @xmath47 ) obtained by dubernet et al .",
    "( 2009 ) , and the vertical axis represents the corresponding output of the ann .",
    "red points apply to the 435 data points in the training set , while blue points apply to the 555 data points in the test set ( i.e.  to those transitions that were not seen by the ann during training ) .",
    "fig .  9  dependence of the performance of the ann upon the number of hidden neurons , for a network with 3 input neurons and @xmath59 .",
    "black curves apply to the entire test set , while blue , green and red histograms apply to transitions with low , medium and high actual rate coefficients ; for this purpose ,  low \" applies to value smaller than @xmath51 ,  medium \" to value in the range @xmath82 , and ",
    "high \"  to a value greater than @xmath53 .",
    "the magenta curve refers to the training set ."
  ],
  "abstract_text": [
    "<S> an artificial neural network ( ann ) is investigated as a tool for estimating rate coefficients for the collisional excitation of molecules . </S>",
    "<S> the performance of such a tool can be evaluated by testing it on a dataset of collisionally - induced transitions for which rate coefficients are already known : the network is trained on a subset of that dataset and tested on the remainder . </S>",
    "<S> results obtained by this method are typically accurate to within a factor @xmath0 ( median value ) for transitions with low excitation rates and @xmath1 for those with medium or high excitation rates , although 4@xmath2 of the ann outputs are discrepant by a factor of 10 more . </S>",
    "<S> the results suggest that anns will be valuable in extrapolating a dataset of collisional rate coefficients to include high - lying transitions that have not yet been calculated . for the asymmetric top molecules considered in this paper , </S>",
    "<S> the favored architecture is a cascade - correlation network that creates 16 hidden neurons during the course of training , with 3 input neurons to characterize the nature of the transition and one output neuron to provide the logarithm of the rate coefficient . </S>"
  ]
}