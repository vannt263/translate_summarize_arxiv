{
  "article_text": [
    "semi - supervised learning or learning from labeled and unlabeled examples has attracted considerable attention in the last decade  @xcite .",
    "this is partially motivated by the fact that for many practical applications collecting a large number of unlabeled data is much less involved than collecting labeled data considering the expensive and tedious annotation process . moreover , as human learning often occurs in the semi - supervised learning manner ( for example , children may hear some words but do not know their exact meanings ) , research on semi - supervised learning also has the potential to uncover insights into mechanisms of human learning  @xcite .    in some machine learning applications , examples can be described by different kinds of information . for example , in television broadcast understanding",
    ", broadcast segments can be simultaneously described by their video signals and audio signals which can be regarded as information from different properties or different  views \" .",
    "multi - view semi - supervised learning , the focus of this paper , attempts to perform inductive learning under such circumstances .",
    "however , it should be noted that if there are no natural multiple views , artificially generated multiple views can still work favorably  @xcite .    in this paper",
    "we are particularly interested in multi - view semi - supervised learning approaches derived from support vector machines ( svms )  @xcite . as a state - of - the - art method in machine learning ,",
    "svms not only are theoretically well justified but also show very good performance for real applications .",
    "the transductive svms  @xcite , s@xmath0vms  @xcite and laplacian svms  @xcite have been proposed as extensions of svms from supervised learning to single - view semi - supervised learning . for multi - view learning , there are also several extensions of svms proposed such as the co - laplacian svms  @xcite and svm-2k  @xcite .",
    "regularization theory is an important technique in mathematics and machine learning  @xcite .",
    "many methods can be explained from the point of view of regularization .",
    "a close parallel to regularization theory is capacity control of function classes  @xcite .",
    "both regularization and capacity control of function classes can play a central role in alleviating over - fitting of machine learning algorithms .",
    "the new method , multi - view laplacian svms , proposed in this paper can also be explained by regularization theory and capacity control of function classes .",
    "it integrates three regularization terms respectively on function norm , manifold and multi - view regularization . as an appropriate integration of them and thus the effective use of information from labeled and unlabeled data , our method has the potential to outperform many related counterparts .",
    "the different roles of these regularization terms on capacity control will be unfolded later as a result of our empirical rademacher complexity analysis . besides giving the bound on the generalization error",
    ", we also report experimental results of the proposed method on synthetic and real - world data sets .",
    "the layout of this paper is as follows .",
    "section  [ secmvlap ] introduces the objective function of the proposed approach with concerns on different regularization terms , and its optimization .",
    "theoretical insights on the generalization error and the empirical rademacher complexity are covered by section  [ sectheana ] .",
    "then , experimental results are reported in section  [ secexp ] .",
    "finally , conclusions are drawn in section  [ seccon ] .",
    "let @xmath1 denote a set of inputs including @xmath2 labeled examples and @xmath3 unlabeled ones with label space @xmath4 . for manifold regularization",
    ", a data adjacency graph @xmath5 is defined whose entries measure the similarity or closeness of every pair of inputs .",
    "we use a typical construction of @xmath6 : @xmath7 for most pairs of inputs , and for neighboring @xmath8 the corresponding entry is given by @xmath9 where @xmath10 is the euclidean norm in @xmath11 .",
    "the manifold regularization functional acting on any function @xmath12 is defined as follows  @xcite @xmath13 it is clear that a smaller @xmath14 indicates a smoother function @xmath15 .",
    "define vector @xmath16 .",
    "then @xmath17 where matrix @xmath18 is diagonal with the @xmath19th diagonal entry @xmath20 .",
    "the matrix @xmath21 , which is arguably positive semidefinite , is called the graph laplacian of @xmath6 . in our empirical studies in section  [ secexp ] ,",
    "a normalized laplacian @xmath22 is used because this normalized one often performs as well or better in practical tasks  @xcite .      for multi - view learning",
    ", an input @xmath23 can be decomposed into components corresponding to multiple views , such as @xmath24 for an @xmath25-view representation .",
    "a function @xmath26 defined on view @xmath27 only depends on @xmath28 , while ignoring the other components @xmath29 .    for multi - view semi - supervised learning ,",
    "there is a commonly acceptable assumption that a good learner can be learned from each view  @xcite .",
    "consequently , these good learners in different views should be consistent to a large extent with respect to their predictions on the same examples .",
    "we also adopt this assumption and use the regularization idea to wipe off those inconsistent learners . given the @xmath30 examples , the multi - view regularization functional for @xmath25 functions @xmath31 can be formulated as @xmath32^{2}.\\ ] ] clearly , a smaller @xmath33 tends to find good learners in each view .",
    "as is usually assumed in multi - view learning , each view is regarded to be sufficient to train a good learner .",
    "therefore , we can write the final prediction as @xmath34 .",
    "for mvlapsvm , in this paper we concentrate on the two - view case , that is @xmath35 . in this scenario ,",
    "the objective function for mvlapsvm is defined as @xmath36+\\nonumber\\\\ & & \\gamma_1(\\|f_1\\|^{2}+\\|f_2\\|^{2})+\\frac{\\gamma_{2}}{(l+u)^2}({\\textbf{f}_1}^{\\top}l_1{\\textbf{f}_1}+\\nonumber \\\\ & & { \\textbf{f}_2}^{\\top}l_2{\\textbf{f}_2})+ \\frac{\\gamma_{3}}{(l+u)}\\sum_{i=1}^{l+u}[f_{1}(x_{i})-f_{2}(x_{i})]^{2},\\end{aligned}\\ ] ] where @xmath37 are the reproducing kernel hilbert spaces  @xcite in which @xmath38 are defined , nonnegative scalars @xmath39 are respectively norm regularization , manifold regularization and multi - view regularization coefficients , and vector @xmath40 , @xmath41 .",
    "we now concentrate on solving ( [ eqntwoviews ] ) . as an application of the representer theorem  @xcite ,",
    "the solution to problem ( [ eqntwoviews ] ) has the following form @xmath42 therefore , we can rewrite @xmath43 and @xmath44 as @xmath45 where @xmath46 and @xmath47 are @xmath48 gram matrices respective from view @xmath49 and @xmath50 , and vector @xmath51 , @xmath52 . in addition , we have @xmath53    to simplify our formulations , we respectively replace @xmath54 and @xmath55 in ( [ eqntwoviews ] ) with @xmath56 and @xmath57 . thus , the primal problem can be reformulated as @xmath58 where @xmath59 , @xmath60 .",
    "note that the additional bias terms are embedded in the weight vectors of the classifiers by using the example representation of augmented vectors .",
    "we present two theorems concerning the convexity and strong duality ( which means the optimal value of a primal problem is equal to that of its lagrange dual problem  @xcite ) of problem  ( [ eqn0514 ] ) with proofs omitted .",
    "problem  ( [ eqn0514 ] ) is a convex optimization problem .",
    "strong duality holds for problem  ( [ eqn0514 ] ) .",
    "[ corolla1 ]    suppose @xmath61 are the lagrange multipliers associated with the first two sets of inequality constraints of problem ( [ eqn0514 ] ) .",
    "define @xmath62 and @xmath63 .",
    "it can be shown that the lagrangian dual optimization problem with respect to @xmath64 and @xmath65 is a quadratic program .",
    "classifier parameters @xmath66 and @xmath67 used by ( [ eqn041702 ] ) can be solved readily after we get @xmath64 and @xmath65 .",
    "in this section , we give a theoretical analysis of the generalization error of the mvlapsvm method in terms of the theory of rademacher complexity bounds .      some important background on rademacher complexity theory",
    "is introduced as follows .    for a sample @xmath68 generated by a distribution @xmath69 on a set @xmath70 and a real - valued function class @xmath71 with domain @xmath70 ,",
    "the empirical rademacher complexity of @xmath71 is the random variable @xmath72,\\ ] ] where @xmath73 are independent uniform @xmath74-valued ( rademacher ) random variables .",
    "the rademacher complexity of @xmath71 is @xmath75=\\mathbb{e}_{s\\boldsymbol{\\sigma } } [ \\sup_{f\\in\\mathcal { f}}|\\frac{2}{l}\\sum_{i=1}^l \\sigma_i f(x_i)|].\\ ] ]    fix @xmath76 and let @xmath71 be a class of functions mapping from an input space @xmath77 ( for supervised learning having the form @xmath78 ) to @xmath79 $ ] .",
    "let @xmath80 be drawn independently according to a probability distribution @xmath81 .",
    "then with probability at least @xmath82 over random draws of samples of size @xmath2 , every @xmath83 satisfies @xmath84&\\leq & \\hat{\\mathbb{e}}[f(z ) ] + r_l(\\mathcal { f } ) + \\sqrt{\\frac{\\ln(2/\\delta)}{2l } } \\nonumber\\\\ & \\leq & \\hat{\\mathbb{e}}[f(z ) ] + \\hat{r}_l(\\mathcal { f } ) + 3\\sqrt{\\frac{\\ln(2/\\delta)}{2l } } , \\nonumber\\end{aligned}\\ ] ] where @xmath85 $ ] is the empirical error averaged on the @xmath2 examples .",
    "[ thmgererr ]    note that the above lemma is also applicable if we replace @xmath86 $ ] by @xmath87 $ ] .",
    "this can be justified by simply following the proof of lemma  [ thmgererr ] , as detailed in  @xcite .",
    "we obtain the following theorem regarding the generalization error of mvlapsvm , which is similar to one theorem in  @xcite .",
    "the prediction function in mvlapsvm is adopted as the average of prediction functions from two views @xmath88    fix @xmath76 and let @xmath71 be the class of functions mapping from @xmath89 to @xmath90 given by @xmath91 where @xmath92 and @xmath93 .",
    "let @xmath94 be drawn independently according to a probability distribution @xmath95 .",
    "then with probability at least @xmath82 over samples of size @xmath2 , every @xmath96 satisfies @xmath97 [ thmmvlaperr ]    where @xmath98 and @xmath99 .",
    "let @xmath100 be the heaviside function that returns 1 if its argument is greater than 0 and zero otherwise .",
    "then it is clear to have @xmath101 . \\label{eqn07051}\\ ] ]    consider a loss function @xmath102 $ ] , given by @xmath103 by lemma  [ thmgererr ] and since function @xmath104 dominates @xmath105 , we have  @xcite @xmath106\\leq \\mathbb{e}_{\\mathcal { d}}[\\mathcal { a}(\\tilde{f}(x , y))-1 ] \\nonumber\\\\ & & \\leq \\hat{\\mathbb{e}}[\\mathcal { a}(\\tilde{f}(x , y))-1 ] + \\hat{r}_l((\\mathcal { a}-1)\\circ\\mathcal { f } ) + 3\\sqrt{\\frac{\\ln(2/\\delta)}{2l } } . \\nonumber\\end{aligned}\\ ] ] therefore , @xmath107\\nonumber\\\\ \\leq & & \\hat{\\mathbb{e}}[\\mathcal { a}(\\tilde{f}(x , y ) ) ] + \\hat{r}_l((\\mathcal { a}-1)\\circ\\mathcal { f } ) + 3\\sqrt{\\frac{\\ln(2/\\delta)}{2l } } . \\label{eqn07052}\\end{aligned}\\ ] ]    in addition , we have @xmath108&\\leq&\\frac{1}{l}\\sum_{i=1}^{l}(1-y_i g({x}_i))_{+}\\nonumber\\\\ & = & \\frac{1}{2l}\\sum_{i=1}^{l}(1-y_i f_1({x}_i)+1-y_i f_2({x}_i))_{+ } \\nonumber\\\\ & \\leq&\\frac{1}{2l}\\sum_{i=1}^{l}[(1-y_i f_1({x}_i))_++(1-y_i f_2({x}_i))_+ ] \\nonumber\\\\ & = & \\frac{1}{2l}\\sum_{i=1}^{l}(\\xi_1^i+\\xi_2^i ) , \\label{eqn07053}\\end{aligned}\\ ] ] where @xmath109 denotes the amount by which function @xmath110 fails to achieve margin @xmath111 for @xmath112 and @xmath113 applies similarly to function @xmath114 .    since @xmath115",
    ", we can apply the lipschitz condition  @xcite of function @xmath116 to get @xmath117 it remains to bound the empirical rademacher complexity of the class @xmath71 . with @xmath118 , we have @xmath119 \\nonumber\\\\ & = & \\mathbb{e}_{\\boldsymbol{\\sigma } } [ \\sup_{g\\in\\mathcal { g}}|\\frac{2}{l}\\sum_{i=1}^l \\sigma_i y_i g(x_i)|]\\nonumber\\\\ & = & \\mathbb{e}_{\\boldsymbol{\\sigma } } [ \\sup_{g\\in\\mathcal { g}}|\\frac{2}{l}\\sum_{i=1}^l \\sigma_i g(x_i)| ] = \\hat{r}_l(\\mathcal{g } )",
    ". \\label{eqn07055}\\end{aligned}\\ ] ]    now combining ( [ eqn07051])@xmath120([eqn07055 ] ) reaches the conclusion of this theorem .      in this section",
    ", we give the expression of @xmath122 used in theorem  [ thmmvlaperr ] .",
    "@xmath122 is also important in identifying the different roles of regularization terms in the mvlapsvm approach .",
    "the techniques adopted to derive @xmath122 is analogical to and inspired by those used for analyzing co - rls in  @xcite .",
    "the loss function @xmath123 in ( [ eqntwoviews ] ) with @xmath124 $ ] satisfies @xmath125 we now derive the regularized function class @xmath126 from which our predictor @xmath127 is drawn .",
    "let @xmath128 denote the objective function in ( [ eqntwoviews ] ) .",
    "substituting the predictors @xmath129 and @xmath130 into @xmath128 results in an upper bound @xmath131 because each term in @xmath128 is nonnegative , the optimal function pair @xmath132 minimizing @xmath128 must be contained in @xmath133^{2 } \\leq 1\\},\\end{aligned}\\ ] ] where parameters @xmath134 , @xmath135 , @xmath136 are from ( [ eqn0514 ] ) , @xmath137 , @xmath138 , and @xmath139 and @xmath140 are the unnormalized graph laplacians for the graphs only involving the unlabeled examples ( to make theoretical analysis on @xmath122 feasible , we temporarily assume that the laplacians in ( [ eqntwoviews ] ) are unnormalized ) .",
    "the final predictor is found out from the function class @xmath141 : ( f_1,f_2)\\in \\mathcal { h}\\},\\ ] ] which does not depend on the labeled examples .    the complexity @xmath122 is @xmath142.\\ ] ]    to derive the rademacher complexity , we first convert from a supremum over the functions to a supremum over their corresponding expansion coefficients .",
    "then , the kahane - khintchine inequality  @xcite is employed to bound the expectation over @xmath143 above and below , and give a computable quantity .",
    "the following theorem summarizes our derived rademacher complexity .",
    "suppose @xmath144 , @xmath145 , @xmath146 , where @xmath147 and @xmath148 are respectively the first @xmath2 rows of @xmath46 and @xmath47 , and @xmath149 and @xmath150 are respectively the last @xmath3 rows of @xmath46 and @xmath47 .",
    "then we have @xmath151 with @xmath152 .",
    "we performed multi - view semi - supervised learning experiments on a synthetic and two real - world classification problems .",
    "the laplacian svm ( lapsvm )  @xcite , co - laplacian svm ( colapsvm )  @xcite , manifold co - regularization ( comr )  @xcite and co - svm ( a counterpart of the co - rls in  @xcite ) are employed for comparisons with our proposed method . for each method , besides considering the prediction function @xmath153 for the combined view , we also consider the prediction functions @xmath110 and @xmath114 from the separate views .",
    "each data set is divided into a training set ( including labeled and unlabeled training data ) , a validation set and a test set .",
    "the validation set is used to select regularization parameters from the range @xmath154 , and choose which prediction function should be used . with the identified regularization parameter and prediction function , performances on the test data and unlabeled training data",
    "would be evaluated .",
    "the above process is repeated at random for ten times , and the reported performance is the averaged accuracy and the corresponding standard deviation .",
    "this synthetic data set is generated similarly to the toy example used in  @xcite .",
    "noisy examples in two classes appear as two moons in one view and two parallel lines in the other view , and points on one moon are enforced at random to associate with points on one line ( see fig .",
    "[ figtmtl3data ] for an illustration ) .",
    "the sizes for labeled training set , unlabeled training set , validation set and test set are @xmath155 , @xmath156 , @xmath157 and @xmath157 , respectively .    0.2 in     @xmath158 & 91.40 ( 1.56 ) & 93.40 ( 3.07 ) & 91.20 ( 1.60 ) & 96.30 ( 1.95 ) & * 96.90 * ( 1.70 ) + @xmath159 & 90.60 ( 2.33 ) & 93.55 ( 2.72 ) & 90.90 ( 2.02 ) & 96.40 ( 1.61 ) & * 96.40 * ( 1.46 ) +    as in  @xcite ,",
    "a gaussian and linear kernel are respectively chosen for the two - moons and two - lines view .",
    "the classification accuracies of different methods on this data set are shown in table  [ table_tmtl3 ] , where @xmath158 and @xmath160 means accuracies on the test data and unlabeled training data , respectively , and best accuracies are indicated in bold ( if two methods bear the same accuracy , the smaller standard deviation will identify the better method ) .    from this table , we see that methods solely integrating manifold or multi - view regularization give good performance , which indicates the usefulness of these regularization concerns . moreover , among all the methods , the proposed mvlapsvm performs best both on the test set and unlabeled training set .",
    "we collected this data set from the sports gallery of the yahoo !",
    "website in 2008 .",
    "it includes 420 nba images and 420 nascar images , some of which are shown in fig .",
    "[ fignba_nascarpics ] . for each image",
    ", there is an attached short text describing content - related information .",
    "therefore , image and text constitute the two views of this data set .",
    "each image is normalized to be a @xmath161-sized gray image .",
    "feature extraction for the texts is done by removing stop words , punctuation and numbers and then applying porter s stemming  @xcite .",
    "in addition , words that occur in five or fewer documents were ignored . after this preprocessing",
    ", each text has a tfidf feature  @xcite of 296 dimensions .    the sizes for labeled training set , unlabeled training set , validation set and test set are @xmath155 , @xmath162 , @xmath163 and @xmath164 , respectively .",
    "linear kernels are used for both views .",
    "the performance is reported in table  [ table_it ] where co - svm ranks first on the test set while mvlapsvm outperforms all the other methods on the unlabeled training set .",
    "if we take the average of the accuracies on the test set and unlabeled training set , clearly our mvlapsvm ranks first .    0.2 in     @xmath158 & 99.33 ( 0.68 ) & 98.86 ( 1.32 ) & 99.38 ( 0.68 ) & * 99.43 * ( 0.59 ) & 99.38 ( 0.64 ) + @xmath159 & 99.03 ( 0.88 ) & 98.55 ( 0.67 ) & 98.99 ( 0.90 ) & 98.91 ( 0.38 ) & * 99.54 * ( 0.56 ) +      in this subsection",
    ", we consider the problem of classifying web pages .",
    "the data set consists of 1051 two - view web pages collected from the computer science department web sites at four u.s .",
    "universities : cornell , university of washington , university of wisconsin , and university of texas  @xcite .",
    "the task is to predict whether a web page is a course home page or not . within the data set",
    "there are a total of 230 course home pages .",
    "the first view of the data is the words appearing on the web page itself , whereas the second view is the underlined words in all links pointing to the web page from other pages .",
    "we preprocess each view according to the feature extraction procedure used in section  [ sectask1 ] .",
    "this results in 2332 and 87-dimensional vectors in view 1 and view 2 respectively  @xcite . finally , document vectors were normalized to tfidf features .    0.2 in     @xmath158 & 94.02 ( 2.66 ) & 93.68 ( 2.98 ) & 94.02 ( 2.24 ) & 93.45 ( 3.21 ) & * 94.25 * ( 1.62 ) + @xmath159 & 93.33 ( 2.40 ) & 93.39 ( 2.44 ) & 93.26 ( 2.19 ) & 93.16 ( 2.68 ) & * 93.53 * ( 2.04 ) +    the sizes for labeled training set , unlabeled training set , validation set and test set are @xmath165 , @xmath166 , @xmath167 and @xmath168 , respectively .",
    "linear kernels are used for both views .",
    "table  [ table_webpage ] gives the classification results obtained by different methods .",
    "mvlapsvm outperforms all the other methods on both the test data and unlabeled training data .",
    "in this paper , we have proposed a new approach for multi - view semi - supervised learning .",
    "this approach is an extension of svms for multi - view semi - supervised learning with manifold and multi - view regularization integrated .",
    "we have proved the convexity and strong duality of the primal optimization problem , and used the dual optimization to solve classifier parameters . moreover ,",
    "theoretical results on the generalization performance of the mvlapsvm approach and the empirical rademacher complexity which can indicate different roles of regularization terms have been made .",
    "experimental practice on multiple data sets has also manifested the effectiveness of the proposed method .",
    "the mvlapsvm is not a special case of the framework that rosenberg et al .",
    "formulated in  @xcite .",
    "the main difference is that they require the loss functional depends only on the combined prediction function , while we use here a slightly general loss which has a separate dependence on the prediction function from each view .",
    "their framework does not subsume our approach .      * * model selection : * as is common in many machine learning algorithms , our method has several regularization parameters to set .",
    "usually , a held out validation set would be used to perform parameter selection , as what was done in this paper . however , for the currently considered semi - supervised learning , this is not very natural because there is often a small quantity of labeled examples available .",
    "model selection for semi - supervised learning using no or few labeled examples is worth further studying . * * multi - class classification : * the mvlapsvm algorithm implemented in this paper is intended for binary classification .",
    "though the usual one - versus - rest , one - versus - another strategy , which converts a problem from multi - class to binary classification , can be adopted for multi - class classification , it is not optimal . incorporating existing ideas of multi - class svms  @xcite into the mvlapsvm approach would be a further concern . *",
    "* regularization selection : * in this paper , although the mvlapsvm algorithm obtained good results , it involves more regularization terms than related methods and thus needs more assumptions .",
    "for some applications , these assumptions might not hold . therefore , a probably interesting improvement could be comparing different kinds of regularizations and attempting to select those promising ones for each application .",
    "this also makes it possible to weight different views unequally .",
    "sindhwani , v. , niyogi , p. , belkin , m. : a co - regularization approach to semi - supervised learning with multiple views .",
    "proceedings of the workshop on learning with multiple views , international conference on machine learning ( 2005 )"
  ],
  "abstract_text": [
    "<S> we propose a new approach , multi - view laplacian support vector machines ( svms ) , for semi - supervised learning under the multi - view scenario . </S>",
    "<S> it integrates manifold regularization and multi - view regularization into the usual formulation of svms and is a natural extension of svms from supervised learning to multi - view semi - supervised learning . </S>",
    "<S> the function optimization problem in a reproducing kernel hilbert space is converted to an optimization in a finite - dimensional euclidean space . after providing a theoretical bound for the generalization performance of the proposed method </S>",
    "<S> , we further give a formulation of the empirical rademacher complexity which affects the bound significantly . </S>",
    "<S> from this bound and the empirical rademacher complexity , we can gain insights into the roles played by different regularization terms to the generalization performance . </S>",
    "<S> experimental results on synthetic and real - world data sets are presented , which validate the effectiveness of the proposed multi - view laplacian svms approach .    </S>",
    "<S> laplacian , multi - view learning , reproducing kernel hilbert space , semi - supervised learning , support vector machine </S>"
  ]
}