{
  "article_text": [
    "visual document analysis systems such as in - spire have demonstrated their applicability in managing large text corpora , identifying topics within a document and quickly identifying a set of relevant documents by visual exploration .",
    "the success of such systems depends on several factors with the most important one being the quality of the dimensionality reduction .",
    "this is obvious as visual exploration can be made possible only when the dimensionality reduction preserves the structure of the original space , i.e. , documents that convey similar topics are mapped to nearby regions in the low dimensional 2d or 3d space .",
    "standard dimensionality reduction methods such as principal component analysis ( pca ) , locally linear embedding ( lle ) @xcite , or t - distributed stochastic neighbor embedding ( t - sne ) @xcite take as input a set of feature vectors such as bag of words or tf vectors .",
    "an obvious drawback of such an approach is that such methods ignore the textual nature of documents and instead consider the vocabulary words @xmath0 as abstract orthogonal dimensions that are unrelated to each other . in this paper",
    "we introduce a general technique for incorporating domain knowledge into dimensionality reduction for text documents .",
    "in contrast to several recent alternatives , our technique is completely unsupervised and does not require any labeled data .",
    "we focus on the following type of non - euclidean geometry where the distance between document @xmath1 and @xmath2 is defined as @xmath3 here @xmath4 is a symmetric positive semidefinite matrix , and we assume that documents @xmath5 are represented as term - frequency ( tf ) column vectors . since @xmath6 can always be written as @xmath7 for some matrix @xmath8 where @xmath9 , an equivalent but sometimes more intuitive interpretation of is to compose the mapping @xmath10 with the euclidean geometry @xmath11 we can view @xmath6 as encoding the semantic similarity between pairs of words . when @xmath12 is a square matrix , it smoothes the tf vector @xmath1 by mapping observed words to unobserved related words . alternatively ,",
    "if @xmath13 , the number of rows of @xmath12 , equals to the number of existing topics , the mapping can be viewed as describing a document as a mixture of such topics .",
    "therefore , the geometry realized by or may be used to derive novel dimensionality reduction methods that are customized to text in general and to specific text domains in particular .",
    "the main challenge is to obtain the matrices @xmath12 or @xmath6 that describe the relationship among vocabulary words appropriately .",
    "we consider obtaining @xmath12 or @xmath6 using three general types of domain knowledge .",
    "the first corresponds to manual specification of the semantic relationship among words .",
    "the second corresponds to analyzing the relationship between different words using corpus statistics .",
    "the third corresponds to knowledge obtained from linguistic resources . in some cases",
    ", @xmath6 might be easier be obtain than @xmath12 .",
    "whether to specify @xmath12 directly or indirectly through @xmath6 depends on the knowledge type and is discussed in detail in section [ sec : domain ] .",
    "we investigate the performance of the proposed dimensionality reduction methods for three text domains : sentiment visualization for movie reviews , topic visualization for newsgroup discussion articles , and visual exploration of acl papers . in each of these domains",
    "we compare several different domain dependent geometries and show that they outperform popular state - of - the - art techniques . generally speaking , we observe that geometries obtained from corpus statistics are superior to manually constructed geometries and to geometries derived from standard linguistic resources such as word - net .",
    "we also demonstrate effective ways to combine different types of domain knowledge and show how such combinations significantly outperform any of the domain knowledge types in isolation .",
    "all the techniques mentioned in this paper are unsupervised , making use of labels only for evaluation purposes .",
    "despite having a long history , dimensionality reduction is still an active research area . broadly speaking",
    ", dimensionality reduction methods may be classified to projective or manifold based @xcite .",
    "the first projects data onto a linear subspace ( e.g. , pca and canonical correlation analysis ) while the second traces a low dimensional nonlinear manifold on which data lies ( e.g. , multidimensional scaling , isomap , laplacian eigenmaps , lle and t - sne ) .",
    "the use of dimensionality reduction for text documents is surveyed by @xcite who also describe current homeland security applications .",
    "dimensionality reduction is closely related to metric learning .",
    "@xcite is one of the earliest papers that focus on learning metrics of the form .",
    "in particular they try to learn matrix @xmath6 in an supervised way by expressing relationships between pairs of samples .",
    "representative paper on unsupervised metric learning for text documents is @xcite which learns a metric on the simplex based on the geometric volume of the data .",
    "we focus in this paper on visualizing a corpus of text documents using a 2-d scatter plot . while this is perhaps the most popular and practical text visualization technique ,",
    "other methods such as @xcite , @xcite , @xcite , @xcite , @xcite , @xcite exist .",
    "it is conceivable that the techniques developed in this paper may be ported to enhance these alternative visualization methods as well .",
    "dimensionality reduction methods often assume , either explicitly or implicitly , euclidean geometry .",
    "for example , pca minimizes the reconstruction error for a family of euclidean projections .",
    "lle uses the euclidean geometry as a local metric .",
    "t - sne is based on a neighborhood structure , determined again by the euclidean geometry .",
    "the generic nature of the euclidean geometry makes it somewhat unsuitable for visualizing text documents as the relationship between words conflicts with euclidean orthogonality .",
    "we consider in this paper several alternative geometries of the form or which are more suited for text and compare their effectiveness in visualizing documents .",
    "as mentioned in section  [ sec : intro ] @xmath12 smoothes the tf vector @xmath1 by mapping the observed words into observed and non - observed ( but related ) words . decomposing @xmath14 into a product of a markov morphism @xmath15 and a non - negative diagonal matrix @xmath16",
    ", we see that the matrix @xmath12 plays two roles : blending related vocabulary words ( realized by @xmath17 ) and emphasizing some words over others ( realized by @xmath18 ) .",
    "the @xmath19-th column of @xmath17 stochastically smoothes word @xmath20 into related words @xmath21 where the amount of smoothing is determined by @xmath22 .",
    "intuitively @xmath22 is high if @xmath23 are similar and @xmath24 if they are unrelated .",
    "the role of the matrix @xmath18 is to emphasize some words over others .",
    "for example , @xmath25 values corresponding to content words may be higher than values corresponding to stop words or less important words .",
    "@xmath26    it is instructive to examine the matrices @xmath17 and @xmath18 in the case where the vocabulary words cluster according to some meaningful way . figure [ fig : matrix ] gives an example where vocabulary words form two clusters .",
    "the matrix @xmath17 may become block - diagonal with non - zero elements occupying diagonal blocks representing within - cluster word blending , i.e. , words within each cluster are interchangeable to some degree .",
    "the diagonal matrix @xmath18 represents the importance of different clusters .",
    "the word clusters are formed with respect to the visualization task at hand .",
    "for example , in the case of visualizing the sentiment content of reviews we may have word clusters labeled as `` positive sentiment words '' , `` negative sentiment words '' and `` objective words '' . in general , the matrices @xmath27 may be defined based on the language or may be specific to document domain and visualization purpose .",
    "it is reasonable to expect that the words emphasized for visualizing topics in news stories might be different than the words emphasized for visualizing writing styles or sentiment content .",
    "the above discussion remains valid when @xmath28 for @xmath13 being the number of topics in the set of documents .",
    "in fact , the @xmath19-th column of @xmath17 now stochastically maps word @xmath19 to related topics @xmath29 .    applying the geometry or to dimensionality reduction",
    "is easily accomplished by first mapping documents @xmath10 and proceeding with standard dimensionality reduction techniques such as pca or t - sne .",
    "the resulting dimensionality reduction is euclidean in the transformed space but non - euclidean in the original space .    in many cases ,",
    "the vocabulary contains tens of thousands of words or more making the specification of the matrices @xmath27 a complicated and error prone task .",
    "we describe in the next section several techniques for specifying @xmath27 in practice .",
    "note , even if in some cases @xmath27 are obtained indirectly by decomposing @xmath6 into @xmath30 , the discussion of the role of @xmath27 is still of importance as the matrices can be used to come up word clusters whose quality may be evaluated manually based on the visualization task at hand .",
    "we consider four different techniques for obtaining the transformation matrix @xmath12 .",
    "each technique approaches in one of two ways : ( 1 ) separately obtain the column stochastic matrix @xmath17 which blends different words and the diagonal matrix @xmath18 which determines the importance of each word ; ( 2 ) estimate the semantic similarity matrix @xmath6 and decompose it as @xmath30 . to ensure that @xmath12 is a non - negative matrix for it to be interpretable , non - negativity matrix factorization techniques such as the one in @xcite may be applied .      in this method ,",
    "an expert user manually specifies the matrices @xmath31 based on his assessment of the relationship among the vocabulary words .",
    "more specifically , the user first constructs a hierarchical word clustering that may depend on the current text domain , and then specifies the matrices @xmath31 with respect to the cluster membership of the vocabulary .    denoting the clusters by @xmath32 ( a partition of @xmath33 )",
    ", the user specifies @xmath17 by setting the values @xmath34 appropriately .",
    "the values @xmath35 and @xmath36 together determine the blending of words from the same cluster .",
    "the value @xmath37 captures the semantic similarity between two clusters .",
    "that value may be either computed manually for each pair of clusters or automatically from the clustering hierarchy ( for example @xmath38 can be the minimal number of tree edges traversed to move from @xmath39 to @xmath40 ) .",
    "the matrix @xmath17 is then normalized appropriately to form a column stochastic matrix .",
    "the matrix @xmath18 is specified by setting the values @xmath41 where @xmath42 may indicate the importance of word cluster @xmath43 to the current visualization task .",
    "we emphasize that as with the rest of the methods in this paper , the manual specification is done without access to labeled data .    since manual clustering assumes some form of human intervention , it is reasonable to also consider cases where the user specifies @xmath31 in an interactive manner .",
    "that is , the expert specifies an initial clustering of words and @xmath31 , views the resulting visualization and adjusts his selection interactively until he is satisfied .",
    "an alternative technique which performs substantially better is to consider a transformation based on the similarity between the contextual distributions of the vocabulary words .",
    "the contextual distribution of word @xmath44 is defined as @xmath45 where @xmath1 is a randomly drawn document . in other words",
    "@xmath46 is the distribution governing the words appearing in the context of word @xmath44 .",
    "a natural similarity measure between distributions is the fisher diffusion kernel proposed by @xcite . applied to contextual distributions as in @xcite we arrive at the following similarity matrix ( where @xmath47 ) @xmath48 intuitively , the word @xmath49 will be translated or diffused into @xmath44 depending on the geometric diffusion between the distributions of likely contexts .",
    "we use the following formula to estimate the contextual distribution from a corpus of documents @xmath50 where @xmath51 is the number of times word @xmath52 appears in document @xmath1 .",
    "the contextual distribution @xmath53 or the diffusion matrix @xmath6 above may be computed in an unsupervised manner without need for labels .",
    "the contextual distribution method above may be computed based on a large collection of text documents such as the reuters rcv1 dataset .",
    "the estimation accuracy of the contextual distribution increases with the number of documents which may not be as large as required .",
    "an alternative is to estimate the contextual distributions @xmath46 from the entire @xmath54-gram content of the web .",
    "taking advantage of the publicly available google @xmath54-gram dataset - gram dataset contains @xmath54-gram counts ( @xmath55 ) obtained from google based on processing over a trillion words of running text .",
    "] we can leverage the massive size of the web to construct the similarity matrix @xmath6 . more specifically , we compute the contextual distribution by altering to account for the proportion of times two words appear together within the @xmath54-grams ( we used @xmath56 in our experiments ) .",
    "the last method we consider uses word - net , a standard linguistic resource , to specify the matrix @xmath6 in .",
    "this is similar to manual specification ( method a ) in that it builds on expert knowledge rather than corpus statistics .",
    "in contrast to method a , however , word - net is a carefully built resource containing more accurate and comprehensive linguistic information such as synonyms , hyponyms and holonyms . on the other hand ,",
    "its generality puts it at a disadvantage as method a may be used to construct a geometry suited to a specific text domain .",
    "we follow @xcite who compare five similarity measures between words based on word - net . in our experiments",
    "we use jiang and conrath s measure @xcite ( see also @xcite ) @xmath57 as it was shown to outperform the others . above , @xmath58 stands for the lowest common subsumer , that is , the lowest node in the hierarchy that subsumes ( is a hypernym of ) both @xmath59 and @xmath60 .",
    "the quantity @xmath61 is the probability that a randomly selected word in a corpus is an instance of the synonym set that contains word @xmath62 .",
    "in addition to methods a - d which constitute `` pure methods '' we also consider convex combinations @xmath63 where @xmath64 are matrices from methods a - d , and @xmath65 is a non - negative weight vector which sums to 1 .",
    "equation  [ eq : combinationthm ] allows to combine heterogeneous types of domain knowledge ( manually specified such as method a and d and automatically derived such as methods b and c ) .",
    "doing so leverages their diverse nature and potentially achieving higher performance than each of the methods a - d on its own .",
    "we evaluated methods a - d and the convex combination method by experimenting on two datasets from different domains .",
    "the first is the cornell sentiment scale dataset of movie reviews @xcite .",
    "the visualization in this case focuses on the sentiment quantity @xcite . for simplicity",
    ", we only kept documents having sentiment level 1 ( very bad ) and 4 ( very good ) .",
    "preprocessing included lower - casing , stop words removal , stemming , and selecting the most frequent 2000 words .",
    "alternative preprocessing is possible but should not modify the results much as we focus on comparing alternatives rather than measuring absolute performance .",
    "the second text dataset is 20 newsgroups .",
    "it consists of newsgroup articles from 20 distinct newsgroups and is meant to demonstrate topic visualization .    to measure the dimensionality reduction quality",
    ", we display the data as a scatter plot with different data groups ( topics , sentiments ) displayed with different markers and colors .",
    "our quantitative evaluation is based on the fact that documents belonging to different groups ( topics , sentiments ) should be spatially separated in the 2-d space .",
    "specifically , we used the following indices to evaluate different reduction methods and geometries .",
    "( i ) : :    the weighted intra - inter measure is a standard clustering quality    index that is invariant to non - singular linear transformations of the    embedded data .",
    "it equals to @xmath66 where @xmath67 is the within - cluster scatter    matrix , @xmath68 is the total scatter matrix , and    @xmath69 is the between - cluster scatter matrix @xcite . (",
    "ii ) : :    the davies bouldin index is an alternative to ( i ) that is similarly    based on the ratio of within - cluster scatter to between - cluster    scatter @xcite .",
    "( iii ) : :    classification error rate of a @xmath70-nn classifier that    applies to data groups in the 2-d embedded space . despite the fact    that we are not interested in classification per se ( otherwise we    would classify in the original high dimensional space ) , it is an    intuitive and interpretable measure of cluster separation .",
    "( iv ) : :    an alternative to ( iii ) is to project the embedded data onto a line    which is the direction returned by applying fisher s linear    discriminant analysis @xcite to the embedded data .",
    "the projected data    from each group is fitted to a gaussian whose separation is used as a    proxy for visualization quality . in particular , we summarize the    separation of the two gaussians by measuring the overlap area .",
    "while    ( iii ) corresponds to the performance of a @xmath70-nn    classifier , method ( iv ) corresponds to the performance of fisher s lda    classifier .",
    "note that the above methods ( i)-(iv ) make use of labeled information to evaluate visualization quality . the labeled data , however , is not used during the dimensionality reduction stages justifying their unsupervised behavior .",
    "the manual specification of domain knowledge ( method a ) for the 20 newsgroups domain used matrices @xmath27 that were specified interactively based on the ( manually obtained ) word clustering in figure  [ fig : manual ] . in the case of sentiment data the manual specification consisted of partitioning words into positive , negative or neutral sentiment based on the general inquirer resourceinquirer/ ] .",
    "the matrix @xmath12 was completed by assigning large weights ( @xmath25 ) for negative and positive words and small weights ( @xmath25 ) to neutral words .    [ level distance=20 mm , level 1/.style = sibling distance=22 mm , level 2/.style = sibling distance=9 mm , level 3/.style = sibling distance=8 mm , ]    childnodepolitics child nodemid east child nodeothers child    node    [ cols= \" < \" , ]      +    we also examined convex combinations @xmath71 with @xmath72 and @xmath73 .",
    "table  [ tab : convcomb ] displays three evaluation measures , the weighted intra - inter measure ( i ) , the davies - bouldin index ( ii ) , and the @xmath70-nn classifier ( @xmath74 ) accuracy on the embedded documents ( iii ) .",
    "the beginning of the section provides more information on these measures .",
    "the first four rows correspond to the `` pure '' methods a , b , c , d .",
    "the bottom row correspond to a convex combination found by minimizing the unsupervised evaluation measure ( ii ) .",
    "note that the convex combination found also outperforms a , b , c , and d on measure ( i ) and more impressively on measure ( iii ) which is a supervised measure that uses labeled data ( the search for the optimal combination was done based on ( ii ) which does not require labeled data ) .",
    "we conclude that combining heterogeneous domain knowledge may improve the quality of dimensionality reduction for visualization , and that the search for an improved convex combination may be accomplished without the use of labeled data .",
    "finally , we demonstrate the effect of linguistic geometries on a new dataset that consists of all oral papers appearing in acl 2001  2009 . for the purpose of manual specification ,",
    "we obtain 1545 unique words from paper titles , and assign each word relatedness scores for each of the following clusters : morphology / phonology , syntax / parsing , semantics , discourse / dialogue , generation / summarization , machine translation , retrieval / categorization and machine learning .",
    "the score takes value from 0 to 2 , where 2 represents the most relevant .",
    "the score information is then used to generate the transformation matrix @xmath17 .",
    "we also assign each word an importance value ranging from 0 to 3 ( larger the value , more important the word ) .",
    "this information is used to generate the diagonal matrix @xmath18 .",
    "figure [ fig : acl ] shows the projection of all 2009 papers using t - sne ( papers from 2001 to 2008 are used to estimate contextual diffusion ) .",
    "the manual specification improves over no domain knowledge by separating documents into two clusters . by examining the document i d , we find that all papers appearing in the smaller cluster correspond to either machine translation or multilingual tasks .",
    "interestingly , the contextual diffusion results in a one - dimensional manifold .",
    "in this paper we introduce several ways of incorporating domain knowledge into dimensionality reduction for visualization of text documents .",
    "the novel methods of manual specification , contextual diffusion , google @xmath54-grams , and word - net all outperform in general the original assumption @xmath75 .",
    "we emphasize that the baseline @xmath75 is the one currently in use in most text visualization systems .",
    "the two reduction methods of pca and t - sne represent a popular classical technique and a recently proposed technique that outperforms other recent competitors ( lle , isomap , mvu , cca , laplacian eigenmaps ) .",
    "our experiments demonstrate that different domain knowledge methods perform best in different situations . as a generalization , however , the contextual diffusion and google @xmath54-gram methods had the strongest performance .",
    "we also demonstrate how combining different types of domain knowledge provides increased effectiveness and that such combinations may be found without the use of labeled data ."
  ],
  "abstract_text": [
    "<S> text documents are complex high dimensional objects . to effectively visualize such data </S>",
    "<S> it is important to reduce its dimensionality and visualize the low dimensional embedding as a 2-d or 3-d scatter plot . in this paper </S>",
    "<S> we explore dimensionality reduction methods that draw upon domain knowledge in order to achieve a better low dimensional embedding and visualization of documents . </S>",
    "<S> we consider the use of geometries specified manually by an expert , geometries derived automatically from corpus statistics , and geometries computed from linguistic resources . </S>"
  ]
}