{
  "article_text": [
    "it is well - known by now that low - activity neural network models have a larger storage capacity than the corresponding models with a mean 50% activity ( see , e.g. , @xcite ) .",
    "however , this improvement is not always apparent in the basins of attraction .",
    "furthermore , for low activities the information content in a single pattern is reduced . for these reasons",
    "it is argued that a neural activity control system is needed in the dynamics of the network in order to keep its activity the same as the one for the memorized patterns during the whole retrieval process @xcite .",
    "recently , new suggestions have been put forward for the choice of threshold functions in network models in order to get an enhanced retrieval quality overlap , basin of attraction , critical capacity , information content ( see @xcite and references therein ) .",
    "diluted models @xcite , layered models @xcite and models for sequential patterns @xcite have been considered . in all cases",
    "it has been found that appropriate thresholds lead to considerable improvements of the retrieval quality .",
    "the models mentioned above have a common property . for the diluted and layered models there is no feedback in the dynamics .",
    "for the model with sequential patterns no feedback correlations are taken into account .",
    "the absence of feedback considerably simplifies the dynamics .",
    "hence , it is interesting to look at a model with feedback correlations and to see whether the introduction of a threshold in the sense described above still enhances the retrieval properties in this much more complex situation .    with these ideas in mind",
    "we consider in the sequel low activity ( or in other words sparsely coded ) fully connected neural networks .",
    "in particular , we study the application of a self - control mechanism proposed recently for a diluted network of binary patterns @xcite .",
    "self - control has been introduced in order to avoid imposing some external constraints on the network with the purpose of improving its retrieval properties .",
    "such external constraints destroy the autonomous functioning of the network .",
    "the model we look at is a fully - connected attractor neural network with neurons and patterns taking the values @xmath0 and pattern activity @xmath1 .",
    "a low - activity neural network corresponds then to the case where the pattern distribution is far from uniform , i.e. , @xmath2 .",
    "this network has the advantage that it can be generated keeping a symmetric distribution of the states since both the @xmath3 states are considered the active ones , while the @xmath4 state is the inactive one .",
    "the rest of this paper is organised as follows .",
    "the three - state network model and its order parameters are described in section ii . in order to study the retrieval quality of the model , especially in the limit of low activity the mutual information content",
    "is analysed in section iii .",
    "section iv discusses the dynamics of this network in the presence of the self - control mechanism realised through the introduction of a time - dependent threshold .",
    "evolution equations for the order parameters are written down . using these equations the influence of self - control on the retrieval quality of the network ",
    "information content , critical capacity , basins of attraction  is studied in section v. furthermore , these theoretical findings are compared with results from numerical simulations of a fully connected network of @xmath5 neurons . finally , section vi presents some concluding remarks .",
    "consider a neural network model of @xmath6 three - state neurons . at a discrete time step @xmath7 the neurons @xmath8 are updated according to the parallel deterministic dynamics @xmath9 where @xmath10 is the local field of neuron @xmath11 at time @xmath7 and @xmath12 a time - dependent threshold parameter .",
    "as usual , the transfer function @xmath13 is given by @xmath14 with @xmath15 the standard heaviside function .    the couplings @xmath16 are determined as a function of the memorized patterns @xmath17 by the hebbian learning algorithm @xmath18 with @xmath19 the loading capacity .",
    "the patterns are taken to be independent identically distributed random variables ( iidrv ) @xmath20 , chosen according to the probability distribution @xmath21 with @xmath22 the activity of the patterns . moreover ,",
    "we assume that there is no bias , i.e. , @xmath23 and that there exist no correlation between patterns such that @xmath24 .",
    "at this point we remark that the long - time behavior of this network model is governed by the spin-1 hamiltonian @xmath25 furthermore , the hopfield model can be recovered by taking the activity @xmath26 and the threshold @xmath27 .",
    "the standard order parameters of this type of models are the retrieval overlap between the @xmath28th - pattern and the microscopic state of the network @xmath29 and the neural activity of the neurons @xmath30    in the next section we use these order parameters in order to study the retrieval quality of the network .",
    "it is known that the hamming distance between the state of the network and the pattern @xmath31 , viz .",
    "@xmath32 is a good measure for the retrieval quality of a network when the patterns are uniformly distributed , i.e. , when the neural activity @xmath33 .",
    "but for low - activity networks it can not distinguish between a situation where most of the wrong neurons @xmath34 are turned off and a situation where these wrong neurons are turned on .",
    "this distinction is critical in the low - activity three - state network because the inactive neurons carry less information than the active ones @xcite .",
    "therefore the mutual information function @xmath35 has been introduced @xcite @xmath36 where @xmath37 is considered as the input and @xmath38 as the output with @xmath39 its entropy and @xmath40 its conditional entropy , viz .",
    "@xmath41\\\\         \\label{eq : enc }    s(\\sigma_{i , t}|\\xi_{i , t}^\\mu ) & = &         -\\sum_\\sigma p(\\sigma_{i , t}|\\xi_{i , t}^\\mu )                  \\ln[p(\\sigma_{i , t}|\\xi_{i , t}^\\mu)]~.\\end{aligned}\\ ] ] here @xmath42 denotes the probability distribution for the neurons at time @xmath7 and @xmath43 indicates the conditional probability that the @xmath11-th neuron is in a state @xmath38 at time @xmath7 given that the @xmath11-th site of the stored pattern to be retrieved is @xmath44 .    the calculation of the different terms of this mutual information for the model at hand proceeds as follows . as a consequence of the mean - field theory character of our model it is enough to consider the distribution of a single typical neuron so we forget about the index @xmath11 in the sequel .",
    "we also do not write the time index @xmath7 and the pattern index @xmath28",
    ".    the conditional probability that the @xmath45 neuron is in a state @xmath46 at time @xmath7 , given that the @xmath45 site of the pattern being retrieved is @xmath47 can be obtained as follows .",
    "formally writing @xmath48 for an arbitrary quantity @xmath49 and using the complete knowledge about the system @xmath50 we arrive at @xmath51    at this point we see from ( [ 3.ps ] ) that besides @xmath52 and @xmath53 the following parameter @xmath54 will play an independent role in the mutual information function .",
    "this quantity is called the activity - overlap since it determines the overlap between the active neurons , @xmath55 , and the active parts of the memorized patterns , @xmath56 .",
    "we remark that it also shows up in the alternative expression of the retrieval quality through the performance @xmath57 ( see @xcite ) .",
    "it does not play any independent role in the time evolution of the network , independent of the architecture considered  diluted , layered or fully - connected .",
    "next , one can verify that the probability @xmath58 is consistent with the averages @xmath59 these averages are precisely equal in the limit @xmath60 to the order parameters @xmath52 and @xmath53 in eq .",
    "( [ 2.mm])-([2.qn ] ) and to the activity - overlap defined in eq .",
    "( [ 2.nm]).(the fluctuations around their mean values can be neglected according to the lln , hence the average over a particular @xmath11-site distribution equals the infinite sum over @xmath11 ) .    using the probability distribution of the memorized patterns ( [ 1.px ] ) we furthermore obtain @xmath61    the expressions for",
    "the entropies defined above then become @xmath62 and the mutual information is then given by eq .",
    "( [ eq : inf ] ) .",
    "we recall that @xmath63 , @xmath64 as well as @xmath65 are needed in order to completely know the mutual information content of the network at time @xmath7 .",
    "it is known that the parallel dynamics of fully connected networks is difficult to solve , even at zero temperature , because of the strong feedback correlations @xcite . recently ,",
    "a recursive dynamical scheme has been developed which calculates the distribution of the local field at a general time step using signal - to - noise analysis techniques @xcite .",
    "recursion relations are obtained determining the full time evolution of the order parameters .",
    "we shortly review these results .",
    "suppose that the initial configuration of the network @xmath66 , is a collection of iidrv with mean @xmath67 , variance @xmath68 , and correlated with only one stored pattern , say the first one @xmath69 : @xmath70 this implies that by the law of large numbers ( lln ) one gets for the retrieval overlap and the activity at @xmath71 @xmath72 at a given time step of the dynamics , the state of a neuron , @xmath73 is determined by its local field at the previous time step .",
    "in general , in the limit @xmath60 the distribution of the local field at time @xmath74 consists out of a discrete part and a normally distributed part @xcite @xmath75           = \\mbox{var}\\left[\\lim _ { n \\to \\infty } \\frac{1}{a \\sqrt{n } }            \\sum_{i } \\xi_i^{\\nu } \\sigma_{i , t}\\right ] ,   \\nu > 1         \\label{eq : dvar }   \\\\         & & b_{i , t}= \\sum_{t'=0}^{t-1 } \\alpha           \\left[\\prod_{s = t'}^{t-1 } \\chi_s\\right ] \\ , \\sigma _ { i , t ' }             \\label{eq : mm } \\end{aligned}\\ ] ] with @xmath76 a gaussian random variable with mean zero and variance @xmath77 and @xmath78 the susceptibility @xmath79 where @xmath80 is the gaussian measure . in the above",
    "@xmath81 denotes the average both over the distribution of the embedded patterns @xmath82 and the initial configurations @xmath66 .",
    "the average over the initial configurations is hidden in an average over the local field through the updating rule ( [ 2.si ] ) .",
    "the first term on the r.h.s . of ( [ eq : init2 ] ) is the signal term produced by the pattern that is being retrieved , the rest represents the noise induced by the @xmath83 non - condensed patterns . in particular , the second term is gaussian noise and the last term @xmath84 contains discrete noise coming from the feedback correlations .",
    "the quantity @xmath85 satisfies the recursion relation @xmath86\\ ] ] for more details we refer to @xcite . using the above scheme the order parameters at a general time step",
    "can then be obtained in the limit @xmath87 from eqs .",
    "( [ 2.mm])-([2.qn ] ) and ( [ 2.si ] ) @xmath88 the activity overlap needed in order to find the mutual information can also be written as @xmath89 of course , we then also need to specify its initial value @xmath90    the idea of the self - control threshold dynamics introduced in the diluted model @xcite and studied for some other models without feedback correlations @xcite has been precisely to let the network counter the noise term in the local field at each step of the dynamics by introducing the following form @xmath91 where the function @xmath92 is a function of the pattern activity .",
    "we remark that in these cases without feedback there is no discrete noise in the local field ( the term @xmath84 in eq .",
    "( [ eq : init2 ] ) is absent ) .",
    "furthermore , also the covariance term in eq .",
    "( [ eq : drec ] ) is absent .",
    "moreover , for the diluted model @xmath93 , i.e. , only the first term in eq .",
    "( [ eq : drec ] ) is present .",
    "so this dynamical threshold has two important characteristics .",
    "first , it is a macroscopic parameter having the same value for every neuron thus no average must be taken over the microscopic random variables at each time step .",
    "secondly , it changes each time step but no statistical history intervenes in this proces .",
    "we see that the choice ( [ 2.tt ] ) is in fact related to the variance of the local field , taken for a fixed realization of the pattern which is being retrieved .",
    "it is the width of the noise produced by the non - condensed patterns .",
    "it is obvious that it can not be taken to be a function of the overlap with the pattern being retrieved . as a consequence , for the fully connected network we can not work with the exact form for @xmath85 as given in eq .",
    "( [ eq : drec ] ) because of the presence of the covariance .",
    "so , if we want to take into account some effects of feedback correlations and if we want the threshold to have the characteristic properties mentioned above , we need to approximate the covariance term in eq .",
    "( [ eq : drec ] ) such that only the previous time step is involved .",
    "this is realised by approximating this term by @xmath94\\mbox{var}[r_t^\\mu]\\}^{1/2 }   = 2\\chi_t [ q_{t+1}/a]^{1/2 } [ d_t]^{1/2}$ ] .",
    "we then easily get @xmath95 ^ 2      \\label{4.va0 } \\\\           g_t&=&\\left\\langle\\!\\left\\langle\\int { \\cal d }   z ~   z \\ ,                      f_{\\theta_t }                          \\left ( \\xi^1m^1_t + \\sqrt{\\alpha a d_t}\\,z                          \\right)\\right\\rangle\\!\\right\\rangle       \\label{4.va}\\end{aligned}\\ ] ] furthermore , we take both contributions at equal times and call @xmath96 . for more details on this approximation of the feedback correlations we refer to @xcite and references therein . finally , since @xmath97 is a function of the overlap @xmath98 , a quantity which is not available to the network we replace it by @xmath99 .",
    "what is left then is to find a form for @xmath92 .",
    "for the low - activity networks considered up to now the storage capacity could be considerably improved by taking @xmath100^{1/2}$ ] such that for the diluted model @xmath101^{-1/2}$ ] . the same form has been shown to work for the layered model @xcite .",
    "for the fully connected model considered here we again propose , a priori , this form .",
    "so , combining these results we take as self - control threshold @xmath102    finally , we make one more assumption on the dynamics . in the local field distribution ( [ eq : init2 ] ) we forget about the discrete noise @xmath84 and suppose that the noise produced by the non - condensed patterns is gaussian distributed .",
    "computer simulations have shown that this assumption is approximately valid as long as the retrieval is succesful @xcite . as a consequence",
    "we can write down recursion relations for the order parameters @xmath103^{2 }          \\nonumber \\\\            & & + ( 1-a )   \\int { \\cal d } z [ f_{\\theta_{t}}(z\\delta_{t})]^{2 }     \\label{4.qt}\\end{aligned}\\ ] ] with @xmath104 where we have already averaged over @xmath105 .",
    "we remark that the form of the last equation is different from the corresponding equation for the diluted and the layered versions of this model @xcite because of the feedback .",
    "the expressions for the overlap @xmath106 , the neural activity @xmath107 and the noise @xmath108 due to the non - condensed patterns describe the ( approximate ) macro - dynamics of the fully - connected neural network . besides the self - control model with the threshold given by eq .",
    "( [ 4.tt])-([4.dt ] ) we also consider the model with the threshold fixed at its zero time value , i.e. , @xmath109 .",
    "at this point we remark that when studying the mutual information , we want to introduce explicitly the activity - overlap parameter , @xmath110 ( recall eq .",
    "( [ 2.nm ] ) ) in the dynamics leading to the following expression for @xmath107 @xmath111 where @xmath112 is then , obviously , defined by the integral of @xmath113^{2}$ ] .",
    "this parameter @xmath114 is precisely that introduced in eq .",
    "( [ 3.ps ] ) and measures the number of active neurons in inactive condensed pattern sites .    in the following section we compare the retrieval properties of a fully connected network governed by this approximate dynamics with and without self - control with numerical simulations .",
    "the main aim is to show that self - control also works in the case of fully connected models .",
    "we have solved the time evolution of our threshold dynamics with a time - dependent self - control threshold given by eq .",
    "( [ 4.tt])-([4.dt ] ) ) and with a time - independent threshold where the neural activity is fixed by @xmath115 .",
    "we have studied the behavior of these networks in the range of pattern activities @xmath116 i.e. , from low - activities to a uniform distribution of patterns .    for both thresholds it turned out that the best results were obtained by taking @xmath117 , with @xmath118 for @xmath119 and @xmath120 for @xmath121 . at this point",
    "we remark , however , that the pure log form for @xmath92 is derived in the theoretical limit @xmath122 .",
    "so , it may be that we did not reach small enough values in our numerical analysis ( which is due to numerical complexity ) .",
    "we recall that one of the main aims of this work is to show that self - control also works for fully connected models .",
    "the important features of self - control are illustrated in figs .  1 - 5 .",
    "1 we compare the time evolution of the retrieval overlap , @xmath63 , starting from several initial values , @xmath123 , for the model with self - control , @xmath124 ( recall eq .",
    "( [ 4.tt])-([4.dt ] ) ) with the model with fixed threshold @xmath125 .",
    "an initial neural activity @xmath126 and a loading @xmath127 have been taken .",
    "we observe that the self - control forces more of the overlap trajectories to go to the retrieval attractor @xmath128 .",
    "only an initial overlap @xmath129 for the self - control model versus @xmath130 for the fixed threshold model is needed .",
    "we remark that for @xmath131 the overlap decreases in the first time step for both models .",
    "this is an expression of the fact that correlations are especially important in the first time steps leading to a decreasing neural activity @xmath64 , but the self - control threshold is able to counter these effects . near the attractor correlations seem to become less important and the gaussian character of the local field distribution dominates .",
    "since the initial overlap needed to retrieve a pattern is smaller for the self - control model , the basins of attraction of the patterns are substantially larger .",
    "this is further illustrated in figs .",
    "2 - 4 where the basin of atraction for the whole retrieval phase @xmath132 is shown for both models with an initial value @xmath126 .",
    "we have calculated the fixed - point @xmath133 of the dynamics ( [ 4.mt ] ) , ( [ 4.qt ] ) and ( [ 4.deltat ] ) and we have determined the intial conditions of the relevant parameters such that the network is able to retrieve , i.e. , such that @xmath134 .",
    "it is interesting to also give @xmath135 and/or @xmath136 separately in order to see how the activity @xmath64 is built up .    in fig .  2 we have used @xmath137",
    "the basin of attraction for the self - control model is larger , even near the border of critical storage .",
    "hence the storage capacity itself is also bigger .",
    "furthermore , a smaller initial activity - overlap @xmath135 suffices to have retrieval as is seen in fig .",
    "we start with initial conditions @xmath138 , i.e. , the smallest initial overlap possible for @xmath127 as we recall from fig .  1 , and @xmath139 or , equivalently @xmath140 .",
    "so we consider small @xmath141 running from @xmath142 to @xmath143 .",
    "we observe the peculiar behavior that for the fixed - threshold network an initial @xmath144 is needed , but even then still no retrieval is possible for low storage @xmath145 . for the self - control model a much broader region of retrieval exists .",
    "finally , the specific role of the parameter @xmath146 is displayed in fig .",
    "we start from a maximal initial overlap @xmath147 and take @xmath148 meaning that for @xmath140 to @xmath149 , @xmath64 runs from @xmath143 to @xmath77 .",
    "it can be seen that especially when @xmath136 is getting large the storage capacity of both models decreases quite drastically but again much less for the self - control model .",
    "we conclude with the observation that self - control works in a large range of pattern activities @xmath1 , as shown in fig .  5 . there",
    "the mutual information content @xmath150 is plotted as a function of the loading @xmath19 on a logaritmic scale .",
    "we observe the slow increase of @xmath11 as the activity @xmath1 decreases , saturating at a value close to @xmath151 .",
    "this behavior is typical for low activity networks @xcite .",
    "simulations have been carried out for systems with @xmath152 neurons . for every new stored pattern @xmath28",
    ", we start our dynamics with a state @xmath153 , and calculate the order parameters @xmath63 , @xmath65 and the activity overlap @xmath64 , using the definitions ( [ 2.mm ] ) , ( [ 2.qn ] ) and ( [ 2.nm ] ) . to avoid a very large computation time we have stopped the dynamics after @xmath154 time steps when no convergence was reached before .",
    "then we have averaged over windows in the @xmath155-axis in order to obtain the mutual information @xmath11 .",
    "the window size runs from @xmath156 for @xmath157 ( where we have stored @xmath158 patterns ) up to @xmath159 for @xmath160 ( where we have stored @xmath161 patterns ) .",
    "the conditions on the lln mentioned in section iva are approximatelly fullfilled for such large networks , since the fluctuations ( neglected in eqs.([3.ma1])-([3.ma ] ) ) are of order @xmath162 . however , for smaller activities @xmath1 , this quantity may be not so small .",
    "it becomes crucial in the case @xmath160 , where this quantity is @xmath163 , such that the finite size effects get relevant .",
    "this implies a kind of cut - off in the information for the self - control model as seen in fig .",
    "6 . however , the agreement with the analytic results of fig .  5 is quite good up to @xmath160 .    in order to further understand the details of the retrieval quality we plot in fig .",
    "7 all the parameters @xmath164 for the model with and without self - control in two cases : @xmath160 , a low - activity case , and @xmath157 , implying a uniform distribution of patterns .",
    "for the uniform case we do not see a big difference between the two models ( self - control and fixed threshold ) . only for larger values of @xmath19",
    ", self - control shows a little improvement .",
    "for the low - activity case , however , the main role of self - control on the neural activity is clearly noticed since @xmath165 in that case while in the fixed - threshold model it is impossible to control @xmath53 such that it stays in the neigborhood of @xmath1 . as a conseqeunce the mutual information , e.g. , is only about half of that for the model with self - control .    finally , in fig .",
    "8 we compare the simulations with the results from the fixed - points of the dynamics ( [ 4.mt ] ) , ( [ 4.qt ] ) and ( [ 4.deltat ] ) for @xmath166 .",
    "up to @xmath167 time steps are considered for both the model with and without self - control and we have averaged over a window in the @xmath155-axis of size @xmath168 . for the self - control model the small underestimation of the theoretical results can , of course , be attributed to the approximations of the noise term ( recall eqs  ( [ 4.va0 ] ) and ( [ 4.dt ] ) ) .",
    "in this paper we have introduced a self - control threshold in the dynamics of fully connected networks with three - state neurons .",
    "this leads to a large improvement of the quality of retrieval of the network .",
    "the relevant quantity in order to study this , especially in the limit of low activity is the mutual information function .",
    "the mutual information content of the network as well as the critical capacity and the basins of attraction of the retrieval solutions for three - state patterns are shown to be larger because of the self - control mechanism . furthermore , since the mutual information saturates , the critical capacity of the low - activity network behaves as @xmath169 .",
    "numerical simulations confirm these results .",
    "this idea of self - control might be relevant for various dynamical systems , e.g. , when trying to enlarge the basins of attraction and convergence times .",
    "indeed , it has been shown to work also for both diluted and layered networks .",
    "binary as well as ternary neurons and patterns have been treated . in all cases ,",
    "it turns out that in the low - activity regime the self - control threshold can be taken to be proportional to the square root of the neural activity of the network .",
    "we would like to thank g.  jongen for useful discussions .",
    "this work has been supported by the research fund of the k.u.leuven ( grant ot/94/9 ) .",
    "one of us ( d.b . ) is indebted to the fund for scientific research - flanders ( belgium ) for financial support .",
    "b.mller , j.reinhardt and m.t.strickland,_neural networks : an introduction _ , ( springer , berlin , 1995 ) .",
    "m.okada , neural networks * 9 * , 1429 ( 1996 ) .",
    "s.grosskinsky , j. phys .",
    "a : math . gen . * 32 * , 2983 ( 1999 ) .",
    "k.kitano and t.aoyagi , j. phys .",
    "a : math . gen . * 31 * , l613 ( 1998 ) .",
    "d.r.c.dominguez and d.boll , phys .",
    "lett . * 80 * , 2961 ( 1998 ) .",
    "d.boll , d.r.c.dominguez and s.amari , neural networks , to appear .",
    "d. boll and g. massolo , `` thresholds in layered neural networks with variable activity '' , preprint kuleuven , belgium , cond - mat/9909443 r.e.blahut , _ principles and practice of information theory _ , ( addison - wesley , reading , ma , 90 ) , chapter 5",
    ". h.  rieger , j. phys .",
    "* 23 * , l1273 ( 1990 ) g.m .",
    "shim , k.y.m .  wong and d.  boll ,",
    "j. phys . a : math",
    ". gen . * 30 * , 2637 ( 1997 ) e.barkai , i.kanter and h.sompolinsky , phys .",
    "a * 41 * , 590 ( 1990 ) .",
    "d. boll , g.jongen and g.m .",
    "shim , j.stat.phys , * 91 * , 125 ( 1998 ) .",
    "d.r.c.dominguez and w.k.theumann , j. phys .",
    "a * 28 * , 63 ( 1995 ) h.  nishimori and t.  ozeki , j. phys . a : math",
    "* 26 * , 859 ( 1993 ) d.boll , g.jongen and g.m .",
    "shim , cond - mat/9907390 , to appear in the _ proceedings of the international conference on mathematical physics and stochastic analysis ( lisbon , october 1998 ) _ , ed .",
    "s.  albeverio _ et al . _",
    "( world scientific , 1999 ) .",
    "c.j.perez-vicente , europhys . lett . * 10 * , 621 ( 1989 ) .",
    "h. horner , z. phys .",
    "b * 75 * , 133 ( 1989 ) ."
  ],
  "abstract_text": [
    "<S> a self - control mechanism for the dynamics of a three - state fully - connected neural network is studied through the introduction of a time - dependent threshold . </S>",
    "<S> the self - adapting threshold is a function of both the neural and the pattern activity in the network . </S>",
    "<S> the time evolution of the order parameters is obtained on the basis of a recently developed dynamical recursive scheme . in the limit of low activity </S>",
    "<S> the mutual information is shown to be the relevant parameter in order to determine the retrieval quality . due to self - control </S>",
    "<S> an improvement of this mutual information content as well as an increase of the storage capacity and an enlargement of the basins of attraction are found . </S>",
    "<S> these results are compared with numerical simulations .    </S>",
    "<S> epsf    2 </S>"
  ]
}