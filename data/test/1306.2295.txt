{
  "article_text": [
    "_ markov random fields _ ( mrfs ) , also known as undirected graphical models , or markov networks , belong to the family of probabilistic graphical models @xcite , a well - known computational framework for compact representation of joint probability distributions .",
    "these models are composed of an independence structure , and a set of numerical parameters .",
    "the independence structure is an undirected graph that encodes compactly the conditional independences among the variables in the domain .",
    "given the structure , the numerical parameters quantify the relationships in the structure .",
    "probability distributions present in practice important complexity deficiencies , with exponential space complexity of their representation , time complexity of inference , and sample complexity when learning them from data . based on the structure of independences , it is possible to represent efficiently the joint probability distribution by factorizing it into smaller functions ( or factors ) , each over a subset of the domain variables , resulting some times in exponential reductions in these complexities .",
    "this factorization can be done by using the well - known hammersley - clifford theorem @xcite .",
    "an important issue of using a graph to represent independences is that it can not encode some types of independence relations , such as the _ context - specific independences _ ( csis ) @xcite .",
    "these independences are similiar to conditional independences except that are only true for certain assignments of its conditioning set .",
    "the csis have been applied in a wide range of scenarios achieving significant improvements in time , space and sample complexities , in comparison with other approaches that only uses conditional independences encoded by the graph .",
    "@xcite . in these contributions ,",
    "the csis are encoded in alternative data structures ( e.g. , using a decision tree instead of a graph ) .",
    "this is carried out by assuming that the factors of the distribution are conditional probability distributions . in this sense ,",
    "the csis are not used to factorize the distribution , but they are used for representing efficiently the factors .",
    "the main contribution of our work is the _ context - specific hammersley - clifford _ theorem .",
    "the importance of this theoretical result lies in that it allows to factorize a distribution using csis , to obtain a more sparse representation than that obtained with conditional independences , providing theoretical guarantees . for this",
    ", a log - linear model is used as a more fine - grained representation of the mrfs @xcite . by using such models",
    "it is possible to extend the advantages of the hammersley - clifford theorem , that is , improvements in time , space and sample complexities .",
    "the remainder of this work is organized as follows .",
    "the next section provides a summary of the related work in the literature .",
    "section  [ sec : preliminaries ] presents an overview of how to factorize a distribution by exploiting its independences .",
    "section  [ sec : llmodels ] formally describes the context - specific hammersley - clifford theorem that factorizes a log - linear model according to a set of csis .",
    "the paper concludes with a summary in section  [ sec : conclusions ] .",
    "there are several works in the literature @xcite that learn log - linear models directly by presenting different procedures for selecting features from data .",
    "neither of these works discuss csis , nor present any guarantee on how the log - linear model generated is related to the underlying distribution .",
    "csis were first introduced by @xcite by coding them locally within conditional probability tables ( factors ) of bayesian networks as decision trees .",
    "their approach is hybrid , encoding conditional independencies in the directed graph and csis as decision trees over the variables of a conditional probability table .",
    "also , their work presents theoretical results for a sound graphical representation .",
    "this work instead proposes a unified representation for csis and conditional independencies into a log - linear model .",
    "as such , it requires first theoretical guarantees on how a distribution factorizes according to this model ( not needed for the work of boutlier as the factorization into conditional probability tables is not affected by the csis ) .",
    "it remains for future investigation to find an efficient graphical representation ( and theoretical guarantees thereon ) .",
    "the work of @xcite is the closests to our work , presenting an algorithm for factorizing a log - linear model according to csis .",
    "for that it introduces a statistical independence test for eliciting this independencies from data .",
    "the work assumes the underlying distribution to be a _",
    "thin junction tree_. although some theoretical results are presented that guarantee an efficient computational performance , no results are presented that guarantee the factorization proposed is sound .",
    "this section provides some background on mrfs , explaining how to factorize a distribution by exploiting its independences .",
    "let us start by introducing some necessary notation .",
    "we use capital letters for sets of indexes , reserving the @xmath0 letter for the domain of a distribution , and @xmath1 for the nodes of a graph .",
    "let @xmath2 represent a vector of @xmath3 random variables .",
    "the @xmath4 function returns all the values of the domain of @xmath5 , and @xmath6 returns all the possible values of the set of variables @xmath7 .",
    "let @xmath8 be a complete assignment of @xmath0 .",
    "the values of @xmath5 are denoted by @xmath9 , where @xmath10 .",
    "finally , we denote by @xmath11 the value taken by variables @xmath12 in the complete assignment @xmath13 .",
    "conditional independences are regularities of distributions that has been extensively studied in the field of statistics , demonstrating how they can be effectively and soundly used for reducing the dimensionality of the distribution @xcite .",
    "formally , a conditional independence is defined as follows :    * conditional independence*. let @xmath14 be two random variables , and @xmath15 be a set of variables .",
    "we say that @xmath5 and @xmath16 are _ conditionally independent _ given @xmath17 , denoted as @xmath18 , if and only if for all values @xmath19 , @xmath20 , and @xmath21 : @xmath22 whenever @xmath23 .    through the notion of conditional independence it is possible to construct a _ dependency model _",
    "@xmath24 , defined formally as follows :    * dependency model*.[def : dm ]    a dependency model @xmath25 is a discrete function that returns a truth value , given an input triplet @xmath26 , for all @xmath14 , @xmath27 .",
    "[ [ remark . ] ] remark .",
    "+ + + + + + +    an alternative viewpoint of the above definition can be obtained by considering that every triplet @xmath26 over a domain @xmath0 are implicitly conditioned by a constant assignment to some external variable of the domain @xmath28 . in this sense , all the triplets of the dependency model become to be conditioned by the assignment @xmath28 . in that sense , any probability distribution is a dependency model , because for any conditional independence assertion it is possible to test its truth value using equation  . in this work ,",
    "we are particularly interested in the set of dependency models that are _ graph - isomorph _ , that is when all its independences and dependences can be represented in an undirected graph . formally , an undirected graph @xmath29 is defined by a set of nodes @xmath30 , and a set of edges @xmath31 .",
    "each node @xmath32 is associated with a random variable @xmath33 , and each edge @xmath34 represents a direct probabilistic influence between @xmath5 and @xmath16 . a necessary and sufficient condition for dependency models to be graph - isomorph",
    "is that all its independence assertions satisfy the following independence axioms , commonly called the _",
    "pearl axioms _ @xcite :    symmetry @xmath35 decomposition @xmath36 intersection @xmath37 strong union @xmath38 transitivity",
    "@xmath39    other important property that we will need later to reconstruct graphs from dependency models is the pairwise markov property , that asserts that an undirected graph can be built from a dependency model which is graph - isomorph , as follows :    [ def : pwp ] let @xmath40 be a graph over @xmath0 .",
    "two nodes @xmath41 and @xmath42 are non - adjacent if and only if the random variables @xmath5 and @xmath16 are conditionally independent given all other variables @xmath43 , i.e. ,    @xmath44    if every independence assertion contained in a dependency model @xmath25 holds for @xmath45 , @xmath25 is said to be an _",
    "i - map _ of @xmath45 . in a similar fashion",
    ", we say that @xmath40 is also an i - map of @xmath45 .",
    "the pairwise property is necessary for those cases for which the graph can only encode a subset of the independences present in the distribution .",
    "a distribution can present additional type of independences . in this work we focus in a finer - grained type of independences : the _ context - specific independences _ ( csi ) @xcite .",
    "these independences are similar to conditional independences , but hold for a specific assignment of the conditioning set , called the _ context _ of the independence .",
    "we define csis formally as follows :    let @xmath14 be two random variables , @xmath46 be pairwise disjoint sets of variables that does not contain @xmath47 ; and @xmath48 some assignment of @xmath12 .",
    "we say that variables @xmath5 and @xmath16 are _ contextually independent _ given @xmath17 and a context @xmath49 , denoted @xmath50 , if and only if @xmath51 whenever @xmath52 .",
    "interestingly , a conditional independence assertion can be seen as a conjunction of csis , that is , the csis for all the contexts of the conditioning set of the conditional independence .",
    "since each csis is defined for a specific context , they can not be represented all together in a single undirected graph @xcite . instead",
    ", they can be captured by a dependency model @xmath25 , extended for csis by using equation   to test the validity of every assertion @xmath53 .",
    "we call this model a _ context - specific dependency model _ @xmath54 .",
    "if every independence assertion contained in @xmath54 holds for @xmath45 , @xmath54 is said to be an _",
    "csi - map _ of @xmath45 @xcite .",
    "we define formally the context - specific dependency model as follows :    context - specific dependency model.[def : csdm ] a dependency model @xmath54 is a discrete function that returns a truth value given an input triplet @xmath55 , for all @xmath56 , @xmath27 , and @xmath48 a context over the subset @xmath57 .",
    "a mrf uses an undirected graph @xmath40 and a set of numerical parameters @xmath58 to represent a distribution .",
    "the completely connected sub - graphs of @xmath40 ( a.k.a . , _ cliques _ ) can be used to factorize the distribution into a set of _ potential functions _",
    "@xmath59 of lower dimension than @xmath45 , parameterized by @xmath60 .",
    "the following theorem shows how to factorize the distribution :    [ thm : hc ] let @xmath45 be a positive distribution over the domain of variables @xmath0 , and let @xmath40 be an undirected graph over @xmath0 . if @xmath40 is an i - map of @xmath45 , then @xmath45 can be factorized as : @xmath61 where @xmath62 is a normalizing constant .",
    "a distribution factorized by the above theorem is called a _",
    "gibbs distribution_. the most nive form contains potentials @xmath63 represented by tables , where each entry corresponds to an assignment @xmath64 that has associated a numerical parameter .",
    "despite the clear benefit of the factorization described by the hammersley clifford theorem , the representation of a factor as a potential does not allow to encode csis .",
    "these patterns are more easily encoded in a more convenient representation called _ log - linear_. the log - linear model represents a gibbs distribution by using a set of _ features _",
    "@xmath65  to represent the potentials .",
    "a feature is an assignment to a subset of variables of domain .",
    "we denote a features as @xmath66 , to make more clear the distinction between the features of a log - linear and its input assignment @xmath13 .",
    "thus , a potential in a log - linear is represented as a linear combination of features as follows :    @xmath67    where @xmath68 is the kronecker delta function , that is , it equals to @xmath69 when @xmath70 , and @xmath71 otherwise . by joining the linear combinations of all the potentials and merging its indexes into a unique index @xmath72 , we can represent equation   by using the following log - linear model :    @xmath73    in the next section we present the context - specific hammersley - clifford theorem , a generalization of the hammersley clifford theorem that shows how to factorize a distribution ( represented by a log - linear ) using a context - specific dependency model @xmath54 that captures the csis .",
    "this section presents the main contribution of this work : a generalization of the hammersley - clifford theorem for factorizing a distribution represented by a log - linear based on a context - specific dependency model @xmath74 csi - map of @xmath45 . for this",
    ", we begin by defining the following corollary of the hammersley - clifford theorem :    [ thm : hc2 ]    let @xmath45 be a positive distribution , and let @xmath25 be a graph - isomorph dependency model over @xmath0 . if @xmath24  is an i - map of @xmath45 , then @xmath45 can be factorized into a set of potential functions @xmath75 , such that for any @xmath76 that is true in @xmath24 , there is no factor @xmath77 that contains both variables @xmath5 and @xmath16 in @xmath78 .    from the assmuptions ,",
    "@xmath24  is graph - isomorph and is an i - map of @xmath79 . by definition ,",
    "the former implies there exists an undirected graph @xmath80 that exactly encodes @xmath24 , and it therefore must also be i - map of @xmath79 .",
    "the assumptions of the hammersley - clifford theorem  [ thm : hc ] hold , so @xmath45 can be factorized into a set of potential functions over the cliques of @xmath40 .",
    "also , since @xmath24  is graph - isomorph , its conditional independences satisfy the pearl axiom , in particular the strong union axiom .",
    "therefore if conditional independence @xmath76 is in @xmath24 , the conditional independence @xmath81 is also in @xmath24 .",
    "using this fact in the pairwise markov property we can imply the no - edge @xmath82 ; in other words , @xmath41 and @xmath42 can not belong to the same clique .",
    "since hammersley - clifford holds , this last fact implies no factor @xmath77 can contain both variables @xmath5 and @xmath16 in @xmath78 .",
    "this corollary shows how to use a dependency model @xmath24  ( instead of a graph ) to factorize the distribution @xmath45 . in what follows",
    ", we present theoretical results that show how a context - specific dependency model @xmath54 can be used to factorize @xmath83 .",
    "the general rationale is to decompose @xmath54 into subsets of csis contextualized on certain context @xmath48 that are themselves dependency models over sub - domains , and use those to decompose the conditional distributions of @xmath45 using hammersley - clifford .",
    "let @xmath45 be a distribution over @xmath0 , @xmath48 a context over subset @xmath57 , and @xmath54 a context - specific dependency model over @xmath0 .",
    "we define the _ reduced dependency model _ @xmath84 of @xmath54 over domain @xmath85 as the rule that for each @xmath86 , each pair @xmath87 of disjoint subsets of @xmath88 , and each assignments @xmath48 of @xmath12 , assigns a truth value to a triplet @xmath55 from independence assertions in @xmath54 as follows :    @xmath89    the following proposition relates the csi - mapness of a context - specific dependency model and the i - mapness of its reduced dependency models .",
    "[ prop:1 ] let @xmath45 be a distribution over @xmath0 , @xmath48 be a context over subset of @xmath0 , and @xmath54 be a context - specific dependency model over @xmath0 .",
    "if @xmath74 is a csi - map of @xmath45 , then @xmath84 is an i - map of @xmath90 .",
    "we start arguing that @xmath84 is a csi - map of @xmath45 , and then extend the proof to show that it is an i - map of the conditional @xmath91 .",
    "that @xmath84 is a csi - map of @xmath45 follows from the fact that @xmath54 is a csi - map of @xmath45 , that implies that not only its csis holds in @xmath45 , but any csi obtained by conjoining those csis over all values of any of its variables , in particular the conjunction of equation  . that @xmath84 is an i - map of @xmath92 follows from the fact that any csi @xmath93 in @xmath45 is equivalent to a conditional independence @xmath94 in the conditional @xmath95 .    in the next auxiliary lemma",
    "it is shown how to factorize a distribution @xmath45 using a dependency model @xmath84 :    [ lem:1 ] let @xmath45 be a positive distribution over @xmath0 , @xmath74 be a dependency model over @xmath0 , and @xmath84 be a graph - isomorph dependency model over @xmath96 .",
    "if @xmath84 is an i - map of the conditional @xmath91 , then this conditional can be factorized into a set of potential functions @xmath97 over @xmath96 , such that for any @xmath53 that is true in @xmath84 , there is no factor @xmath77 that contains both @xmath41 and @xmath42 in @xmath98 .",
    "the proof consists on using corollary  [ thm : hc2 ] for the conditional @xmath90 as the distribution , and @xmath99 as the dependency model . for that , we show they satisfy the requirements of the corollary , that is , @xmath90 is positive , and @xmath99 is a graph - isomorph dependency model over domain @xmath100 that is an i - map of the conditional .",
    "the @xmath99 is an i - map of the conditional and graph - isomorph follows from the assumptions .",
    "it remains to prove then the positivity of the conditional .",
    "for that , the conditional is expanded as follows :    @xmath101    where the sum expansion of the denominator follows from the law of total probability .",
    "the conditional has been expressed then as an operation over joints , and being all positive , it follows that both the numerator and denominator , and therefore the whole quotient is positive .    with lemma [ lem:1 ]",
    ", we can present our main theoretical result , a theorem that generalizes theorem  [ thm : hc ] to factorize the features @xmath102 in a log - linear of @xmath45 according to some given context - specific dependency model @xmath54 . for this",
    ", we need to define precisely what we mean by factorization of a set of features @xmath65 .",
    "we do this in two steps , one that defines a factorization according to dependency models , and then the contextualized case for context - specific dependency models .    [ def : dmf ] let @xmath65  be a set of features over some domain @xmath0 , and @xmath84 some reduced dependency model over @xmath100 .",
    "we say features @xmath65  _ factorize _ according to @xmath84 if for each @xmath53 that is true in @xmath84 , and each feature @xmath103 such that @xmath104 , it holds that either @xmath105 or @xmath106 .",
    "[ def : ff ]    let @xmath65  be a set of features over some domain @xmath0 , and @xmath54 be a context - specific dependency model .",
    "the features @xmath65  are said to _ factorize _ according to @xmath54 if they factorize according to each reduced dependency model @xmath107 of @xmath54 ( as defined by definition  [ def : dmf ] ) , with @xmath108 , and @xmath109 .",
    "we present now our main theorem , and then discuss practical issues regarding its requirements .",
    "[ thm : cshc ] let @xmath45 be a positive distribution over @xmath0 , @xmath65  be a set of features from a log - linear of @xmath45 , and @xmath54 be a context - specific dependency model over @xmath0 , such that each of its reduced dependency models ( over all possible contexts ) is graph - isomorph . if @xmath54 is csi - map of @xmath45 then @xmath65  factorizes according to @xmath54 .    from the definition of context - specific feature factorization , the conclusion of the theorem holds if @xmath102 factorizes according to each reduced dependency model of @xmath54 .",
    "so let @xmath84 be some arbitrary reduced dependency model for context @xmath48 , and prove @xmath102 factorizes according to @xmath84 , which by definition  [ def : dmf ] requires that ( a ) for each @xmath110 that is true in @xmath84 , and ( b ) for each @xmath103 s.t .",
    "@xmath104 , it holds that ( c ) either @xmath111 or @xmath112 . to proceed",
    "then , we first apply the auxiliary lemma  [ lem:1 ] for @xmath45 , the context @xmath48 , and the reduced dependency model @xmath84 .",
    "these requirements are satisfied , that is , @xmath45 is positive and @xmath84 is both graph - isomorph and i - map of the conditional @xmath90 ( by proposition  [ prop:1 ] ) . from this",
    "we conclude the consequent of the lemma , i.e. , that the conditional @xmath91 can be factorized into a set of potencial functions @xmath97 s.t .",
    "( i ) for each @xmath50 that is true in @xmath84 ,",
    "( ii ) for each factor @xmath113 , it holds that ( iii ) either @xmath114 or @xmath115 . to conclude then",
    ", we argue that conclusions ( i ) , ( ii ) and ( iii ) of the auxiliary lemma are equivalent to the requirements ( a ) , ( b ) , and ( c ) of the factorization .",
    "clearly , conclusions ( i ) and ( iii ) matches requirement ( a ) and ( c ) of the factorization .",
    "we now show the equivalence of ( ii ) with ( b ) .",
    "a factor @xmath77 of the conditional @xmath91 is equivalent to a factor @xmath116 over the joint @xmath45 , which is composed of features @xmath117 whose values over @xmath12 matches @xmath48 , i.e. , @xmath118 .",
    "the theorem requires that each possible reduced dependency model of @xmath54 be graph - isomorph .",
    "what is the implication of this requirement ?",
    "by definition of graph - isomorphism , this implies that for each possible context @xmath48 , the reduced dependency model @xmath84 can be encoded as an undirected graph over the sub - domain @xmath96 .",
    "this provides us a mean to construct @xmath54 graphically , i.e. , constructing an undirected graph for each possible sub - domain and assignment of its complement . in practice , this may be done by experts that provide a list of csis that hold in the domain , or running a structure learning algorithm over each context .",
    "this may sound overly complex , as there are cleary an exponential number of such contexts .",
    "no doubt future works can explore this aspect , finding alternatives for simplifying this complexity on different special cases .",
    "we have presented a theoretical method for factorizing a markov random field according to the csis present in a distribution , that is formally guaranteed to be correct .",
    "this is presented by the context - specific hammersley - clifford theorem , as a generalization to csis of the hammersley - clifford theorem that applies for conditional independences . according with our theoretical result",
    ", we believe that it is worth guiding our future work in implementing algorithms for learning from data the structure of mrfs for each possible context , and then factorizing the distribution by using the learned structures .",
    "intuitively , it seems likely to achieve improvements in time , space and sample complexities , in comparison with other approaches that only uses conditional independences encoded by the graph .",
    "boutilier , c. , friedman , n. , goldszmidt , m. , and koller , d. ( 1996 ) .",
    "context - specific independence in bayesian networks . in _ proceedings of the twelfth international conference on uncertainty in artificial intelligence _ , pages 115123 .",
    "morgan kaufmann publishers inc .",
    "fierens , d. ( 2010 ) .",
    "context - specific independence in directed relational probabilistic models and its influence on the efficiency of gibbs sampling . in _",
    "european conference on artificial intelligence _ , pages 243248 ."
  ],
  "abstract_text": [
    "<S> markov random fields provide a compact representation of joint probability distributions by representing its independence properties in an undirected graph . </S>",
    "<S> the well - known hammersley - clifford theorem uses these conditional independences to factorize a gibbs distribution into a set of factors . however , an important issue of using a graph to represent independences is that it can not encode some types of independence relations , such as the context - specific independences ( csis ) . </S>",
    "<S> they are a particular case of conditional independences that is true only for a certain assignment of its conditioning set ; in contrast to conditional independences that must hold for all its assignments . </S>",
    "<S> this work presents a method for factorizing a markov random field according to csis present in a distribution , and formally guarantees that this factorization is correct . </S>",
    "<S> this is presented in our main contribution , the context - specific hammersley - clifford theorem , a generalization to csis of the hammersley - clifford theorem that applies for conditional independences . </S>"
  ]
}