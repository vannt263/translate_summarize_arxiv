{
  "article_text": [
    "analysis of large amounts of high - dimensional big data is of great interest since it illuminates the underlying phenomena . to cope with high - dimensional big data",
    ", it is sometimes assumed that there are some ( unobservable ) dependencies between the parameters of the multidimensional data points .",
    "mathematically , it means that the data is sampled from a low - dimensional manifold that is embedded in a high dimensional ambient space .",
    "dimensionality reduction methods , which rely on the presence of a manifold , map the data into a low - dimensional space while preserving certain qualities of the low - dimensional structures of the data .    a broad class of dimensionality reduction methods",
    "are based on kernel - based methods .",
    "the kernel encapsulates a measure of mutual affinities ( or similarities ) between data points .",
    "particularly , if the kernel is semi - positive definite , it can be considered as gram matrix of inner products , which correspond to an implicit mapping of the data to a high dimensional space , typically refereed to as the feature space .",
    "depending on the chosen kernel , the new geometry of the data in feature space , represents important features of the data .",
    "kernel - pca is a technique that generalizes the well known principal component analysis ( pca )  @xcite . while the latter detects principal directions of data in euclidean space and then projects the data onto them , the former does the same in the feature space .",
    "it is resulted in a low dimensional euclidean representation ( embedding ) of the data that approximates the feature space geometry .",
    "the dimensionality of the embedding space is affected by the decay rate of the kernel s spectrum .",
    "examples of kernel methods are diffusion maps ( dm )  @xcite , local linear embedding ( lle )  @xcite , laplacian eigenmaps  @xcite , hessian eigenmaps  @xcite and local tangent space alignment  @xcite .    from a practical point of view , kernel methods have a significant computational drawback : spectral analysis of the kernel matrix becomes impractical for large datasets due to high computational complexity required to manipulate a kernel matrix .",
    "their global nature is also disadvantageous .",
    "furthermore , in many applications , the analysis process is dynamic due to data accumulation over time and , as a result , the embedding has to be modified once in a while .",
    "processing a kernel matrix in memory becomes impractical for large datasets due to their sizes .",
    "a general solution scheme embeds a subset of the source data that is usually referred to as a training dataset .",
    "then , the embedding is extended to any out - of - sample data point .",
    "the nystrm method  @xcite , which is widely used in integral equations solvers , has become very popular as an out - of - sample extension method associated with dimensionality reduction methodology . for a review of spectral clustering and nystrm extension see section @xmath0 in  @xcite .",
    "the nystrm extension scheme has three significant disadvantages : ( a ) it requires diagonalization of a matrix that costs @xmath1 operations @xcite .",
    "( b ) it requires working with a matrix which may be ill - conditioned due to fast decay of its spectrum , and ( c ) it is unclear how to choose the length parameter @xmath2 since the output is sensitive to the choice of @xmath2 .",
    "some limitations of nystrm extension are overcome in @xcite .",
    "geometric harmonics ( gh )  @xcite is another out - of - sample extension method .",
    "it uses the nystrm extension of eigenfunctions of a kernel defined on the data . in order to avoid numerical instabilities",
    ", it uses only the significant spectral components . in that sense , the gh framework filters out high frequencies , which are determined by the kernel , rather than by the data",
    "this problem , additionally to the fixed interpolation distance problem , is treated in  @xcite , where a multiscale interpolation scheme is introduced .",
    "another multiscale approach , which aims to solve the aforementioned limitations , was recently introduced in  @xcite .",
    "both methods project the objective function on the eigencomponents of a series of kernels , which cover the complete spectrum of that function .",
    "the difference between these methods is in the extraction of principal components while the former is spectral and the latter is interpolative .",
    "all these methods use a kernel matrix ( or , perhaps , its low rank approximation ) as an interpolation matrix .",
    "this mechanism is strongly related to a variety of isotropic interpolation methods that employ radial basis functions ( rbf ) .",
    "such methods are used for scattered data approximation , where the data lies in a metric space .",
    "more details about rbf and scattered data approximation can be found in  @xcite and  @xcite , respectively .    in this paper",
    ", we employ the manifold assumption to establish an anisotropic out - of - sample extension .",
    "we suggest a new anisotropic interpolation scheme that ascribes for each data point a likelihood neighborhood . this likelihood is based on geometric features from the dimensionality reduction map by using pca of the maps image .",
    "incorporation of such neighborhood information produces a linear system for finding the out - of - sample extension for this data point .",
    "this method also provides an abnormality measure for a newly - mapped data point .",
    "the paper has the following structure : section [ sec : setup ] introduces the problem and the needed definitions .",
    "the construction of the out - of - sample extension is described in section [ sec : alg_const ] .",
    "section  [ sec : feature based variance ] establishes the geometric - based stochastic linear system on which the interpolant is based .",
    "three different interpolants are presented where each is based on different geometric considerations . in section [ sec : bound ] , an analysis of interpolations error is presented for the case of lipschitz mappings .",
    "computational complexity analysis of the scheme is presented in section  [ sec : complexity ] .",
    "experimental results for both synthetic data and real - life data are presented in section  [ sec : examples ] .",
    "let @xmath3 be a compact low - dimensional manifold of intrinsic dimension @xmath4 that lies in a high - dimensional ambient space @xmath5 ( @xmath6 ) , whose euclidean metric is denoted by @xmath7 .",
    "let @xmath8 be a smooth , lipschitz and dimensionality reducing function defined on @xmath3 , i.e. @xmath9 ( @xmath10 ) , where @xmath11 is a @xmath4-dimensional manifold .",
    "let @xmath12 be a finite training dataset , sufficiently dense sampled from @xmath3 , whose image @xmath13 under @xmath8 was already computed .",
    "given an out - of - sample data point @xmath14 , we aim to embed it into @xmath15 while preserving some local properties of @xmath8 .",
    "the embedding of @xmath16 into @xmath15 is denoted by @xmath17 .",
    "it is referred to as the extension of @xmath8 to @xmath16 .",
    "the proposed extension scheme is based on the local geometric properties of @xmath8 in the neighborhood of @xmath16 , denoted by @xmath18 .",
    "specifically , the influence of a neighbor @xmath19 on the value of @xmath17 depends on its distance from @xmath16 and the geometry of the image @xmath20 of @xmath18 under @xmath8 .",
    "this approach is reflected by considering @xmath17 as a random variable with mean @xmath21 and a variance @xmath22 that depends on both the distance of @xmath16 from @xmath23 and on some geometric properties of @xmath20 that will be detailedly discussed in section  [ sec : tangent space ] .",
    "mathematically , @xmath24 where @xmath25 is a random variable with mean @xmath26 and variance @xmath27 that , as previously mentioned , depends on the local geometry of @xmath8 in the neighborhood @xmath18 of @xmath16 .",
    "thus , we get @xmath28 equations for evaluating @xmath17 , one for each neighbor @xmath19 .",
    "the optimal solution then , is achieved by the generalized least squares approach described in section  [ sec : gls ] .      in this section ,",
    "we briefly describe the gls approach that will be utilized to evaluate @xmath17 . in general",
    ", the gls addresses the problem of a linear regression that assumes neither independence nor common variance between the random variables .",
    "thus , if @xmath29 are random variables that correspond to @xmath30 data points in @xmath31 , the addressed regression problem is @xmath32 where @xmath33 is an @xmath34 matrix that stores the data points as its rows and @xmath35 is an error vector . respectively to the aforementioned assumption , the @xmath36 conditional covariance matrix of the error term @xmath37 is not necessarily scalar or diagonal .",
    "the gls solution to eq .",
    "[ eq : regression ] is @xmath38 the mahalanobis distance between two random vectors @xmath39 and @xmath40 of the same distribution with conditional covariance matrix @xmath41 is @xmath42    [ obs : mahlanobis ] the mahalanobis distance in eq .",
    "[ eq : mahal_dist ] measures the similarity between @xmath39 and @xmath40 in respect to @xmath41 .",
    "if the random variables are independent , then @xmath41 is diagonal .",
    "then , it is more affected by low variance random variables and less by high variance variables .    the gls solution from eq .",
    "[ eq : gls_solution ] , minimizes the squared mahalanobis distance between @xmath43 and the estimator @xmath44 , i.e. @xmath45 further details concerning gls can for example be found in  @xcite .    in our case , for a fixed out - of - sample data point @xmath14 with its @xmath46 neighbors , a linear system of @xmath30 equations , each of the form of eq .",
    "[ eq : basic_rv ] , is solved for @xmath17 . without loss of generality , we assume that @xmath47 . the matrix formulation of such a system is @xmath48 where @xmath49^t$ ] is the @xmath50 identity blocks matrix , @xmath51 is a @xmath52-long vector , whose @xmath53-th section is the @xmath54-long constant vector @xmath55 and @xmath56 is a @xmath52-long vector , whose @xmath53-th section is the @xmath54-long vector @xmath57 .",
    "the vector @xmath56 encapsulates the images of @xmath18 under @xmath8 such as the neighborhood of @xmath17 in @xmath11 .",
    "the corresponding covariance matrix is the @xmath58 blocks diagonal matrix @xmath41 , @xmath59 whose @xmath53-th diagonal element is @xmath60 .",
    "therefore , due to eq .",
    "[ eq : gls_solution ] , the gls solution to eq .",
    "[ eq : system_rv ] is @xmath61 and it minimizes the mahalanobis distance @xmath62 that measures the similarity ( with respect to @xmath41 ) between @xmath17 and its neighbors @xmath63 in @xmath11 , which are encapsulated in @xmath56 . once @xmath41 is defined as an invertible covariance matrix @xmath17 , as defined in eq .",
    "[ eq : psi_hat_solution ] , is well posed .",
    "the definition of @xmath41 depends on the definition of @xmath64 for any @xmath65 , which can be chosen subjected to the similarity properties to be preserved by @xmath8 .",
    "these properties are discussed in section  [ sec : feature based variance ] . once eq .",
    "[ eq : psi_hat_solution ] is solved for @xmath17 , the mahalanobis distance from eq .",
    "[ eq : maha measure ] provides a measure for the disagreement between the out - of - sample extension of @xmath8 and @xmath16 with the surrounding geometry .",
    "thus , a large value of @xmath66 ( eq . [ eq : maha measure ] ) indicates that @xmath16 resides outside of @xmath3 and thus , in data analysis terminology , it can be considered as an anomalous data point .",
    "as mentioned in section  [ sec : gls ] , the gls solution minimizes the mahalanobis distances between @xmath17 and its neighbors according to the stored information in @xmath41 .",
    "thus , if the variances are determined subjected to some feature , then @xmath17 , which is defined in eq .",
    "[ eq : psi_hat_solution ] , is the closest point in @xmath11 to its neighbors with respect to this feature .",
    "the idea of algorithm [ alg : pbe ] is to get a linear approximation for the local geometry of @xmath8 and then device an out - of - sample extension that best preserves that linear demand using gls .",
    "the gls solution also provides the error , which , as described in section [ sec : gls ] , is considered as an anomalous score .    * input : * @xmath67 - training dataset .",
    "+ @xmath68 - training dataset after dimensionality reduction .",
    "+ @xmath16 - an out of sample data point . +",
    "* output : * @xmath43 - an out - of - sample extension of the data point @xmath16 that preserves the local properties of @xmath8 .",
    "+ @xmath69 - an abnormality score of the data point @xmath16 .",
    "find a set @xmath70 ( eq . [ neighborhood_epsilon ] ) of the nearest neighbors with radius @xmath71 to the data point @xmath16 in @xmath72 . for each data point @xmath73 ,",
    "construct a weighted linear system @xmath74 for @xmath43 where the construction of @xmath41 is described in section [ sec : feature based variance ] .",
    "@xmath43 is the optimal solution for the combined weighted linear system , as described in section [ sec : gls ] .",
    "when gls is solved , find the residual @xmath69 of the solution .",
    "in this section we present a construction of @xmath41 , which is the weight of the linear system for @xmath43 , such that the resulted out - of - sample extension @xmath17 agrees with principal direction of its neighborhood in @xmath11 .",
    "the neighborhood @xmath70 can be defined variously . in this paper",
    ", we use the definition @xmath75 for some positive @xmath71 , which ensures locality of the extension scheme .",
    "the parameter @xmath71 should be fixed according to the sampling density of @xmath3 such that @xmath76 .",
    "this restriction enables to detect the principal directions of the image of @xmath77 in @xmath11 .    in the rest of this section",
    ", we present the construction of @xmath41 .",
    "the first construction , presented in section  [ sec : basic ] provides a mechanism to control the rate of influence of a data point @xmath78 on the value of @xmath17 as a function of its distance from @xmath16 . the second construction for @xmath41 , presented in section  [ sec : tangent space ] , incorporates information regarding principal variance directions of @xmath70 such that the resulted out - of - sample extension @xmath17 agrees with these directions .",
    "although the definition of @xmath79 provides locality for the scheme computation , it is reasonable to require that data points in @xmath79 , which are distant from @xmath16 , affect less than close data points . for this purpose ,",
    "an affection weight @xmath80 is assigned to each data point @xmath81 .",
    "of course , any other decreasing function of the distance between @xmath16 and @xmath23 can be utilized . by defining the variance @xmath82 to be proportional to the distance @xmath83 such that @xmath84",
    ", then we get a diagonal matrix @xmath41 , whose @xmath53-th diagonal element is @xmath85 thus , due to observation  [ obs : mahlanobis ] , close data points in @xmath70 affect @xmath17 more than data points that are far away .      in this section , we present a covariance matrix that encapsulates geometric information concerning the manifold @xmath11 .",
    "the covariance matrix @xmath41 is set such that the resulted extension obeys the lipschitz property of @xmath8 .",
    "let @xmath86 be the tangential space to @xmath11 in @xmath57 and let @xmath87 be the orthogonal projection on @xmath86 .",
    "we denote the tangential component of @xmath25 by @xmath88 , and its orthogonal complement by @xmath89 , where @xmath90 is the identity transformation .",
    "proposition  [ prop : tang - perp - comp ] quantifies the tangential and perpendicular components of @xmath25 from eq .",
    "[ eq : basic_rv ] , as functions of the curvature of @xmath11 in @xmath23 and @xmath91 .",
    "[ prop : tang - perp - comp ] let @xmath92 and assume that the curvature of @xmath11 in @xmath23 is bounded by a constant @xmath93 . if @xmath8 is a lipschitz function with constant @xmath30 , then @xmath94 and @xmath95 .    without loss of generality",
    "it is assumed that @xmath96 and @xmath97 .",
    "we denote the graph of the manifold @xmath11 in the neighborhood of @xmath98 by the function @xmath99 , where the data points in @xmath11 are @xmath100 .",
    "thus we get @xmath101 and @xmath102 .",
    "let @xmath103 be a data point in the neighborhood of @xmath23 and let @xmath104 .",
    "namely , @xmath105 and @xmath106 .",
    "then , the taylor expansion of @xmath107 around @xmath98 yields @xmath108 . since @xmath8 is assumed to be a lipschitz function with constant @xmath30 , we get @xmath109 .",
    "thus , we get that @xmath110 and @xmath111 .    from eq .",
    "[ eq : basic_rv ] , proposition  [ prop : tang - perp - comp ] provides a relation between the tangential and perpendicular components of @xmath25 thus , @xmath51 from eq .",
    "[ eq : system_rv ] is the @xmath52-long vector , whose @xmath53-th section is the @xmath54-long vector @xmath112 , where its first @xmath4 entries are the tangential weights and the rest @xmath113 are the perpendicular weights .",
    "the corresponding covariance matrix is the @xmath114 blocks diagonal matrix @xmath41 , whose @xmath53-th diagonal element is the @xmath115 diagonal matrix @xmath116 where the first @xmath4 diagonal elements are @xmath117 ( see eq .",
    "[ eq : weights ] ) , and the rest @xmath113 are @xmath118.then , the solution is given by eq .",
    "[ eq : system_rv ] while @xmath119 and @xmath56 remain the same .      in real life applications ,",
    "in order to use the tangential space to @xmath11 in @xmath57 , it has to be first approximated by using its neighboring data . in this section ,",
    "we approximate the tangential space and the principle directions of the manifold @xmath11 at @xmath57 .",
    "then , these approximations are incorporated in the construction of @xmath41 to ascribe heavy weights to the tangential direction and less significant weights to the perpendicular ones .",
    "the principle directions of the data and the variance of each direction are the eigenvectors and eigenvalues of the covariance matrix of a data points @xmath57 , respectively .",
    "this covariance matrix is also known as the pca matrix .",
    "a multi - scale version of the local pca algorithm is analyzed in  @xcite and it can be used in our analysis .",
    "it is important to take at least as many data points as the dimensionality of @xmath11 .",
    "the covariance matrix of a data point @xmath57 is computed in the following way : for simplicity of calculations , we take as the set of neighbors of @xmath57 the set @xmath120 . then , we form the @xmath34 matrix @xmath33 , whose rows in the aforementioned set are @xmath121 accordingly , we define @xmath122 since we take the same set of data points then for all @xmath123 and @xmath53 we have @xmath124 . to make the calculation and stability issues easier",
    "we add the @xmath125 component to all the diagonal components .",
    "consequently , we define : @xmath126    since @xmath127 is positive definite , it is invertible .",
    "we notice that it was possible to add the @xmath128 weight components only to the least significant directions of the covariance matrix by computing the svd @xcite of the covariance matrix .",
    "this does not improve the accuracy significantly but adds more complexity to the computation .",
    "@xmath41 is a block diagonal matrix with the same structure as appears in eq .",
    "[ eq : w ] .",
    "another option is to make different estimations for the tangential space in different data points by using different sets of data points in the covariance matrix computation . while this estimation should be more accurate , it is more computationally expensive .",
    "in this section , we prove that the error of the out - of - sample extension is bounded in both cases of distance - based weights ( eq .  [ eq : simple w ] ) and the tangential - based weights(eq .",
    "[ eq : tang weights ] ) .",
    "it means that for any function @xmath129 , which agrees on a given set of data points and satisfies certain conditions , the out - of - sample extension @xmath17 of the data point @xmath16 is close to @xmath130 .",
    "first , we prove the consistency of the algorithm . in other words ,",
    "the out - of - sample extension of data points , which coverage to an already known data point , will converge to its already known image .",
    "[ lemma:5 ] assume @xmath131 . if @xmath132 then @xmath133 .",
    "an intuition for the proof of lemma [ lemma:5 ] is that the distance from the point @xmath134 is inversely proportional to the weight of the equation @xmath135 in eq .",
    "[ eq : system_rv ] , therefore , when @xmath136 , the distance tends to @xmath98 and the weight tends to @xmath137 . notice that when @xmath138 then , according to eq .",
    "[ eq : weights ] , @xmath139 and the out - of - sample extension is undefined .",
    "the dataset @xmath140 is called a @xmath141-net of the manifold @xmath3 if for any data point @xmath142 there is @xmath143 such that @xmath144 .",
    "assume that @xmath72 is a @xmath141-net of @xmath3 .",
    "let @xmath145 be a lipschitz function with a constant @xmath146 . if @xmath147 and @xmath148 is computed using the weights in eq .",
    "[ eq : simple w ] , then @xmath149 .",
    "we denote by @xmath150 the set of data points in the @xmath151 neighborhood of @xmath16 .",
    "it is easy to see that all the data points of @xmath152 are inside a ball @xmath153 of radius @xmath154 .",
    "therefore , the out - of - sample extension @xmath43 is also in this ball , namely @xmath155 since @xmath8 is a lipschitz function and @xmath156 , we have @xmath157 by combining eqs . and , we get @xmath158 .    next , we show an identical result for the case where the weights from eq .",
    "[ eq : tang weights ] are utilized to construct the covariance matrix @xmath41 .",
    "moreover , the approximations of the tangential spaces converge to the correct tangential space as @xmath159 tends to @xmath98 .",
    "[ thm : main ] let @xmath72 be a @xmath141-net of @xmath3 and let @xmath145 be a lipschitz function with a constant @xmath146 . if @xmath147 and @xmath148 is computed using the weights in eq .",
    "[ eq : tang weights ] , then @xmath160 .",
    "let @xmath150 be the @xmath161 neighborhood of @xmath16 .",
    "then , the weight matrix becomes @xmath162    by using eq .",
    ", we get @xmath163 where the @xmath127 matrices are defined in eq .",
    "[ eq : tang weights ] .",
    "the structure of the @xmath127 matrices allows us to find a basis in which all the matrices @xmath164 become diagonal .",
    "let us denote the diagonal form of @xmath127 by @xmath165 .",
    "then @xmath166 where @xmath167 is the transformation matrix .",
    "we can rewrite eq . to become @xmath168 since all @xmath165 are diagonal",
    ", we get a weighted average of the data points @xmath152 in the new basis , which is known to be in convex hull .",
    "thus , it is located inside a ball that contains all the data points .",
    "it means that @xmath148 is inside a ball of radius @xmath154 that contains all @xmath152 .",
    "therefore , @xmath169",
    "recall that the dataset @xmath72 consists of @xmath170 data points and assume that the number of data points in the neighborhood of @xmath16 is @xmath30 .",
    "the covariance matrix of a data point @xmath57 from eq .",
    "[ eq : cov xj ] is also computed once for each data point in @xmath72 , considering each of its @xmath30 neighbors .",
    "the complexity of the neighborhood computation is @xmath171 operations .",
    "then , the covariance matrix is computed in @xmath172 operations .",
    "thus , the complexity of this pre - computation stage is @xmath173 operations . for each data point , we multiply vectors of size @xmath174 by matrices of size @xmath175 or @xmath176 . thus , the out - of - sample extension complexity is @xmath177 operations .",
    "the function @xmath178 \\times [ 0,\\pi ] \\rightarrow \\mathbbm{r}^3 $ ] maps the spherical coordinates @xmath179 into a 3-d sphere of radius @xmath180 .",
    "more specifically , @xmath181 we generate @xmath182 data points angularly equally distributed where we have @xmath183 data points on each axis as a training dataset .",
    "we generate 100 random data points for which we compute the out - of - sample extension .",
    "the results from the application of the algorithm using weights as defined in eq .",
    "[ eq : simple w ] , are shown in fig . [",
    "fig : sphere ] . in fig .",
    "[ fig : sphere_3estimations ] , we can see three different results from an out - of - sample extension using different weights as presented in section [ sec : feature based variance ] . in table",
    "[ algorithm_error ] , we show how the results get better for more advanced weight algorithms .",
    "we display an accurate error mean for the algorithm .",
    "we also show the improvement of the results when we take @xmath184 data points angularly equally distributed with @xmath185 data points on each axis :    .the mean error performances of the algorithms for different number of data points and different weights [ cols=\"<,<,<,<\",options=\"header \" , ]     .,width=566 ]    .",
    "blue - the original data set , green - the correct images .",
    "yellow - the out - of - sample extension computed using the algorithm with weights from eq .",
    "[ eq : simple w ] .",
    "red - the out - of - sample extension computed using the algorithm with weights from eq .",
    "[ eq : tang weights ] .",
    "black - the out - of - sample extension computed using the weights from eq .",
    "[ eq : tang weights ] , but with different estimations for the tangential space at each data point.,width=566 ]      darpa datasets @xcite from 1998 and 1999 are utilized here to find anomalies in them . all the activities and non - activities are labeled and published . these datasets contain different types of cyber attacks that we consider as anomalies .",
    "we use this dataset to evaluate the performance of the out - of - sample extension using weights from eq .",
    "[ eq : tang weights ] and the mahalanobis distance from eq .",
    "[ eq : maha measure ] . the experiment done by following the example in @xcite .",
    "we use the same data and same mapping that was developed in @xcite .",
    "diffusion maps ( dm ) @xcite , which was applied to darpa data , reduces the dimensionality by embedding @xmath186 to @xmath187 such that @xmath188 .",
    "we present two experiments using this data to evaluate the performance of the out - of - sample extension .",
    "the first experiment is an out - of - sample extension for non - anomalous data points by comparing the original results from the dm embedding .",
    "the second experiment evaluates the anomaly detection of the algorithm .      in this experiment",
    ", we use 800 data points and an embedding function @xmath189 that was described before . by taking a random subset @xmath190 of data points and by using the values @xmath191 , we approximate @xmath8 on @xmath185 data from the 800 .",
    "we compare the approximated result to the correct values of @xmath8 and measure the error .",
    "to evaluate the performance of our method , we compare these results to the results from other leading methods such as the classic nystrm method , the multiscale data sampling and function extension ( mse ) method described in @xcite and the auto - adaptative laplacian pyramids method described in @xcite .",
    "to make the presentation self contained , nystrm , mse and auto - adaptative laplacian pyramids methods are outlined next .",
    "nystrm method : : :    the nystrm method @xcite is vastly used for an out - of - sample    extension in dimensionality reduction methods .",
    "it is a numerical    scheme for the extension of integral operator eigenfunctions .",
    "it finds    a numerical approximation for the eigenfunction problem    @xmath192 where @xmath193 is an eigenfunction and    @xmath194 is the corresponding eigenvalue .",
    "given a set of    equidistant points @xmath195 $ ] .",
    "assume that @xmath196 is similarity matrix that    is defined on the data whose @xmath197 entry measures the    similarity between the data points @xmath198 and    @xmath199 , namely @xmath200.\\ ] ]    a gaussian function is a popular choice for @xmath201 , and it is    given by @xmath202    where @xmath203 constitutes a metric on    the space .",
    "then , eq . [ nys1 ] can be approximated by a quadrature rule    to become @xmath204 .",
    "then , the nystrm extension of    @xmath193 to a new data point @xmath205 is    @xmath206 .",
    "+    if @xmath196 is symmetric , then its normalized eigenfunctions    @xmath207 constitute an orthonormal    basis to @xmath208 .",
    "thus , any vector    @xmath209^t$ ] , ( @xmath210 )    can be decomposed into a superposition of its eigenvectors    @xmath211 . then",
    ",    the nystrm extension of @xmath212 to @xmath205    becomes @xmath213 .",
    "mse method : : :    .",
    "[ alg : rand_id ]    +    use a random number generator to form a real @xmath214    matrix @xmath196 whose entries are i.i.d gaussian random    variables of zero mean and unit variance . compute the    @xmath214 product matrix @xmath215 .",
    "+    apply the pivoted qr routine to @xmath41 ( algorithm 5.4.1 in    @xcite ) , @xmath216 , where @xmath217 is an    @xmath218 permutation matrix , @xmath219 is an    @xmath220 orthogonal matrix , and @xmath221 is an    @xmath222 upper triangular matrix , where the absolute values of the    diagonal are ordered decreasingly .",
    "+    split @xmath221 s.t .",
    "@xmath223    +    where @xmath224 is @xmath220 ,    @xmath225 is @xmath226 and @xmath227 is    @xmath228 .",
    "+    [ step4 ] define the @xmath220 matrix    @xmath229 .",
    "+    [ sampling ] from step [ step4 ] , the columns of @xmath230    constitute a subset of the columns of @xmath41 .",
    "in other words ,    there exists a finite sequence @xmath231    of integers such that , for any @xmath232 , the    @xmath53th column of @xmath230 is the @xmath233th    column of @xmath41 .",
    "the corresponding columns of    @xmath234 are collected into a real @xmath235    matrix @xmath236 , so that , for any    @xmath232 , the @xmath53th column of    @xmath236 is the @xmath233th column of @xmath234 .    then , the sampled dataset is    @xmath237 .",
    "+    +    apply svd to @xmath238 , s.t .",
    "@xmath239 .",
    "+    calculate the pseudo - inverse    @xmath240 of @xmath238 .",
    "+    calculate the coordinates vector    @xmath241 of the    orthogonal projection of @xmath242 on the range of    @xmath238 in the basis of @xmath238 s columns .",
    "[ sse : step6 ] calculate the orthogonal projection    @xmath243 of @xmath212 on    @xmath238 . .",
    "+    [ sse : step7 ] calculate the extension of @xmath242 to    @xmath16 s.t .",
    "@xmath244 .",
    "+    set the scale parameter @xmath245 ,    @xmath246 and @xmath247 .",
    "+    form the gaussian kernel @xmath248 on @xmath249 ( see    @xmath250 ) , with    @xmath251 .",
    "+    estimate the numerical rank @xmath252 of @xmath248    using    @xmath253 .",
    "+    apply algorithm [ alg : rand_id ] to @xmath248 and    @xmath252 to get an @xmath254 matrix    @xmath238 and sampled dataset @xmath255 .",
    "+    apply algorithm [ alg : sse ] to @xmath238 and    @xmath256 .",
    "we get the approximation    @xmath242 to @xmath257 at    scale @xmath258 , and its extension @xmath259 to    @xmath16 .",
    "+    set @xmath260 , @xmath261 .",
    "+    @xmath262 and @xmath263 .",
    "laplacian pyramid method : : :    the laplacian pyramid is a multi - scale algorithm for extending an    empirical function @xmath212 , which is defined on a dataset    @xmath264 , to new data points . mutual distances between the    data points in @xmath264 are used to approximate    @xmath212 in different resolutions .",
    "+    @xmath264 is a set of @xmath265 data points in    @xmath266 and @xmath212 is a function defined    on @xmath264 .",
    "a gaussian kernel is defined on    @xmath264 as @xmath267 . normalizing    @xmath268 by    @xmath269 where    @xmath270 yields a smoothing operator    @xmath271 . at a finer scale @xmath272 , the gaussian    kernel @xmath273 yields the    smoothing operator @xmath274 .",
    "+    for any function @xmath275 , the    laplacian pyramid representation of @xmath212 is defined    iteratively as follows : @xmath276 the differences @xmath277 are input for this algorithm at level @xmath272 .",
    "+    equation approximates a given function @xmath212 in a    multi - scale manner , where    @xmath278 .",
    "an admissible error    should be set a - priori and the iterations in eq .",
    "stop when    @xmath279    +    we extend @xmath212 to a new point    @xmath280 in the following way :    +    @xmath281    +    the extension of @xmath212 to the point @xmath43 is    evaluated from eq . as @xmath282 .",
    "the performance results of the 4 methods are shown in fig.[fig : oos_comperison ] .          in this experiment ,",
    "all the 1321 available data points are used as our training dataset .",
    "we show in fig .",
    "[ fig : darpa_orig_image ] the image of these data points after the embedding by @xmath8 .",
    "the normal behaved manifold in the embedded space in fig .",
    "[ fig : darpa_orig_image ] has the  horseshoe \" shape .",
    "we can see a few data points , which are classified as anomalous , are the labeled attacks .",
    "then , a set of newly arrived data points are assigned with coordinates in the embedded space via the application of nystrm extension as can be seen in the left image in fig .",
    "[ fig : darpa_new_image ] .",
    "it is also done by the applicaion of the mse algorithm in @xcite .",
    "data point # 51 , which is a newly arrived data point , is an anomalous that can be seen as an outlier on the left side of the normal (  horseshoe \" ) manifold .",
    "we apply our out - of - sample extension algorithm using weights from eq .",
    "[ eq : tang weights ] , to the same set of newly arrived data points .",
    "the results are shown on the right image in fig .",
    "[ fig : darpa_new_image ] . to find anomalies ,",
    "we compute the mahalanobis distance of the extension using eq . for each of the newly arrived data points .",
    "we see that data point # @xmath283 emerged as having a much higher residual error ( @xmath284 ) than the other data points whose average residual error is @xmath285 .",
    "all the mahalanobis distance values are shown in fig .",
    "[ fig : residuals ] .",
    "after its embedding into @xmath286 .",
    ", width=566 ]    , is presented by the red data point which is the data point # 51.,width=566 ]",
    "in this paper , we present an efficient out - of - sample extension ( interpolation ) scheme for dimensionality reduction maps that are widely used in the field of data analysis .",
    "the computational cost of such maps is high .",
    "therefore , once such a map is computed over a training set , an efficient extension scheme is needed .",
    "the presented scheme is based on the manifold assumption , which is widely used in the field of dimensionality reduction .",
    "it provides an optimal solution from a stochastic geometric - based linear equations system that is determined by the application of local pca of the embedded data .",
    "moreover , the scheme enables to detect abnormal data points .",
    "the interpolation error was analyzed by assuming that the original map is a lipschitz function .",
    "the scheme was applied to both synthetic and real - life data to provide good results by mapping data from the manifold to the image manifold and by detection of abnormal data points .",
    "this research was partially supported by the israeli ministry of science & technology ( grants no . 3 - 9096 , 3 - 10898 ) ,",
    "us - israel binational science foundation ( bsf 2012282 ) , blavatnik computer science research fund and icrc funds ."
  ],
  "abstract_text": [
    "<S> dimensionality reduction methods are very common in the field of high dimensional data analysis </S>",
    "<S> . typically , algorithms for dimensionality reduction are computationally expensive . </S>",
    "<S> therefore , their applications for the analysis of massive amounts of data are impractical . </S>",
    "<S> for example , repeated computations due to accumulated data are computationally prohibitive . in this paper , an out - of - sample extension scheme , which is used as a complementary method for dimensionality reduction , is presented . </S>",
    "<S> we describe an algorithm which performs an out - of - sample extension to newly - arrived data points . </S>",
    "<S> unlike other extension algorithms such as nystrm algorithm , the proposed algorithm uses the intrinsic geometry of the data and properties for dimensionality reduction map . </S>",
    "<S> we prove that the error of the proposed algorithm is bounded . additionally to the out - of - sample extension , </S>",
    "<S> the algorithm provides a degree of the abnormality of any newly - arrived data point . </S>"
  ]
}