{
  "article_text": [
    "supernovae ia ( sne  ia ) are regarded as the best ( relative ) distance indicators out to redshifts of @xmath4 .",
    "the first observational evidence of the accelerated cosmic expansion rate was provided by sne  ia more than a decade ago @xcite .",
    "subsequently , it was supported by results from cosmic microwave background observations @xcite and baryon acoustic oscillations @xcite , that confirmed the presence of a component other than matter in the universe .",
    "general relativity can accommodate the detected acceleration as an elastic and smooth fluid ( dark energy ) exerting a repulsive gravity @xcite .",
    "it fails , however , on giving a deeper understanding about its cause .",
    "other possibilities , such as quintessence models @xcite and non - standard cosmologies describing the acceleration as a manifestation of new gravitational physics @xcite , have been suggested as alternative explanations for the acceleration .",
    "hence , we face a situation where a large variety of cosmological models has been proposed to account for cosmic acceleration . the standard approach of testing each model individually and determining its best fit parameters can not account for unexpected features in the _ true _ underlying cosmology and leads only to a smaller but still vast class of models allowed by the data .",
    "given the lack of a consistent physical framework to explain the energy component responsible for the cosmic acceleration , we aim at determining the dependence on the cosmic expansion with redshift ( _ hubble _ parameter , _ h _ ) making use of only minimum hypotheses .",
    "the idea of a model - independent reconstruction extracted from distance measurements has been widely discussed in the literature .",
    "it was proposed by @xcite and since then other reconstructions of the kind have been carried out . @xcite and",
    "@xcite proposed a procedure for smoothing supernova data over redshift with gaussian kernels .",
    "@xcite added constraints from measurements of baryon acoustic oscillations ( bao ) to the sn  ia data , and @xcite combined sne ia luminosity distances with angular - diameter distances from radio galaxies . @xcite",
    "have also tested the significance of cosmic expansion directly from sn  ia data in a model - independent way . other non - parametric approaches to reconstruct the expansion history and equation of state of dark energy use gaussian processes for smoothing the data ( see e.g. * ? ? ?",
    "* ) .    in this work ,",
    "we use the method presented by @xcite and further developed by @xcite .",
    "it belongs to the class of purely geometrical approaches which only assumes a friedman - robertson - walker metric , similar in philosophy to the analysis presented by @xcite .",
    "the method takes into account our expectations towards the underlying cosmology in order to define an optimal basis for the reconstruction , but this does not prevent it from being able to handle unexpected features that might be present in the data . at the same time , it provides a framework where subtle features can be addressed with a small number of free parameters and consequent reasonable uncertainties ( we refer the reader to * ? ? ?",
    "* ; * ? ? ?",
    "* and references therein for a detailed analysis of the method ) .",
    "as expected from a completely geometrical approach , our method does not provide direct constraints on specific cosmological parameters .",
    "however , it returns an expected form of the expansion rate as a function of redshift with corresponding uncertainties . having such a function",
    ", we can compare our results with @xmath0cdm cosmologies and point to the most likely cosmological parameters able to reproduce the resulting _",
    "hubble _ parameter behaviour .",
    "we have applied the method to the union2.1 sne ia data set @xcite and found a suitable reconstruction for _ h _ using only one coefficient for the basis .",
    "the _ hubble _ constant , @xmath5 , is left as a free parameter to fit together with the expansion coefficients .",
    "it is worth mentioning that the sn data have to be standardized before extracting their absolute magnitudes , and subsequently distances .",
    "several light - curve fitters available in literature approach the issue in different ways , their correction parameters having different physical meanings .",
    "the standardization process is a prior and separate step independent of the work presented in this paper .",
    "the data used here have been calibrated by the union team @xcite within the salt2 ( spectral adaptive lightcurve template ) paradigm .",
    "salt2 @xcite is an empirical model based on a combination of photometric light curves and spectra of both nearby and distant sn  ia .",
    "the photometry for each sn light curve is fitted to the model to determine a shape - luminosity parameter , a color parameter , and an overall flux normalization .",
    "the global nuisance parameters are fitted simultaneously with the cosmological parameters while the _ hubble _ constant is marginalised over @xcite .",
    "the union2.1 data set consists of a compilation of 580 sne from several surveys providing redshift , distance moduli and errors in distance moduli .    translating our results for a flat @xmath0cdm scenario leads to a cosmological constant model where matter energy density is in close agreement with the recently released planck - satellite results for the cosmic microwave background @xcite .",
    "given the general agreement that the _ true _ underlying cosmological model should not differ much from a cosmological - constant model ( at least as long as the cosmic dynamics is concerned ) , we believe our results show that going beyond the parametrized analysis is fundamental to tackle small deviations present in the data .",
    "this letter is organized as follows . in section  [",
    "sec : method ] , the essential aspects of the model - independent methodology are reviewed .",
    "the application of the method to luminosity - distance measurements and a comparison with @xmath0cdm models is presented in section  [ sec : real data ] .",
    "finally , conclusions are drawn and future perspectives are discussed in section  [ conclusions ] .",
    "we used the method presented in @xcite and further developed in @xcite .",
    "it aims at recovering the expansion rate in a model - independent fashion , i.e. without reverting to any assumptions on the dynamics of the universe .",
    "this is achieved by transforming the luminosity distance definition , assuming a robertson - walker metric ,    @xmath6    into a volterra integral of the second kind . here",
    "@xmath7 is the inverse of the expansion function . in order to do so the derivative of eq .",
    "( [ eq : lum_dist ] ) is taken with respect to the scale factor @xmath8 .",
    "re - arranging terms we obtain    @xmath9    note that , for the sake of simplicity , this expression is derived for a flat universe .",
    "this choice , however , does not affect the fundamental method and can be dropped without change of principle if needed and @xmath5 is considered a free parameter whenever a reconstruction is confronted with the data . ] .",
    "equation  ( [ eq : volterra ] ) has proven to be uniquely solved in terms of a neumann series @xcite ,    @xmath10    consequently , in order to perform the reconstruction from noisy data we need a well behaved determination for @xmath11 ( equation [ eq : lum_dist ] ) and its derivative ( to evaluate equation [ eq : volterra ] ) . therefore , we need to first properly smooth the data by fitting an adequate function @xmath12 to the measurements in a model - independent way .",
    "this is conveniently done through an expansion of the luminosity distance into a series of orthonormal functions , @xmath13    the @xmath14 coefficients @xmath15 are those which minimize the @xmath16 statistic function when fitting to the data .",
    "the number of terms to be included in the expansion depends strongly on the choice of the orthonormal basis and the quality of the data .",
    "therefore , although the basis would be arbitrary with ideal data , it will not be in practice . in @xcite",
    ", we saw that by choosing a completely arbitrary basis , obtained via gram - schmidt orthonormalization of the linearly - independent set @xmath17 , we needed at least three coefficients to fully reconstruct the expansion rate from union2 data @xcite . despite this limitation , we were able to produce an acceptable fit of @xmath18 , although we observed a systematic trend on its slope at intermediate and high redshifts when compared with @xmath0cdm or dark energy models .",
    "this indicates that the estimation of the derivatives was not as accurate as one should expect . in this letter",
    ", we make use of an optimal basis system derived from a principal component analysis which minimizes the number of coefficients required and orders them according to their information content .",
    "it also removes any possible bias introduced by the choice of the basis .      principal component analysis ( pca )",
    "is a well known statistical tool which aims at reducing the dimensionality of an initially very large parameter space .",
    "the algorithm looks for directions of maximum variance within the data and constructs an orthonormal basis representing directions ( the principal components , pcs ) of maximum clustering , or along which most of the information is contained . after the pcs are determined ,",
    "the original data can be re - written as a linear combination of some pcs , usually a number much smaller than the dimensionality of the original parameter space ( for a careful review see * ? ? ?",
    "different approaches using pca have already been proposed to reconstruct the dark energy equation of state @xmath19 ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) , the hubble parameter @xcite and the cosmic star formation history @xcite .",
    "here we follow the method described in @xcite to obtain an optimal basis system for a given cosmological data set , in this case a luminosity - distance sne catalog .",
    "we shall use the pca approach to substitute the arbitrary orthonormal basis mentioned in section [ sec : fitting ] , as proposed by @xcite .",
    "it is important to emphasise that , although we only present results of applying the method to sne ia data , it could be also used to analyse any other observable which delivers standard candle or standard ruler measurements ( e.g. cmb or bao ) .",
    "a general parameterization ( with independent parameters regardless the underlying physical assumptions ) could be achieved by considering the principal components as cosmological eigen - cosmologies . in this context",
    ", observations would `` excite '' ( i.e. make visible ) a given number of modes according to their accuracy .",
    "we start by defining a 1d vector which collects the redshift / scale factor values for which there is a luminosity distance measurement in our catalog , @xmath20 .",
    "the next step is to choose a group of models we believe spans the set of viable cosmologies .",
    "suppose we chose initially @xmath21 different cosmologies : for each one of them , we calculate the luminosity distance at the values of scale factor in @xmath20 , producing for each model a vector , @xmath22 .",
    "this ensemble of models , @xmath23 , referred as the _ training set _ , initializes the method .",
    "each training vector @xmath22 corresponds to a particular behaviour of the observable as a function of scale factor .",
    "the matrix @xmath24 represents a convolution of all our expectations towards the underlying cosmology and will act as a synthetic data set in order to determine an ideal orthonormal basis .",
    "once the training set of models is defined , we built the so called _ scatter matrix _",
    "@xmath25 , which contains the differences between each training vector and a given reference vector that defines the origin of the parameter space .",
    "this reference model ( @xmath26 ) may be any combination of models within @xmath27 , and is usually set to be the mean of the training set @xmath28 .",
    "a different choice will only be reflected in the final number of pcs used in the reconstruction .",
    "if the reference model was chosen wisely , maybe one pc shall be enough to account for the deviations from the reference model present in the data .",
    "otherwise , we may need a larger number of components in order to achieve the same reconstruction .",
    "however , the choice of the reference model is arbitrary and does not affect the nature of the basis or the reconstruction of @xmath29 .",
    "the principal components are found by solving the usual eigenvalue problem @xmath30 where @xmath31 and @xmath32 are the eigenvalues and the eigenvectors , respectively . the linear transformation leading to these pcs is defined in such a way that it concentrates in only a few features all the information ( or variance ) regarding the deviations of the models in the training set from the reference vector .",
    "the eigenvector with the largest eigenvalue corresponds to the direction of maximum variance ( first pc ) .",
    "the second pc corresponds to the direction defined by the eigenvector with second largest eigenvalue and so on .",
    "an important issue when working with pca is to determine how many pcs one should take into account @xcite .",
    "the number of pcs to be included in our reconstruction can be based on the cumulative percentage of total variance represented by a set of @xmath33 pcs ,    @xmath34    where @xmath35 is the total number of pcs and @xmath33 the number to be included in the reconstruction . in this way",
    ", the question of how many principal components to use translates into what percentage of variance we are willing to consider .    after constructing the orthonormal basis and deciding how many pcs to include in the final analysis ( @xmath33 )",
    ", we express the corrections to the reference model as linear combinations of the first @xmath33 pcs , @xmath36 following what was done in the previous section , the final values for the coefficients @xmath15 are determined by confronting this expression for the luminosity distance with the data through a @xmath16 minimisation . in this step ,",
    "the _ hubble _",
    "constant @xmath5 is considered a free parameter to be minimised along with the coefficients @xmath15 .",
    "subsequently , we approximate the derivative in eq .",
    "( [ eq : volterra ] ) as @xmath37 due to the linearity of eq .",
    "( [ eq : volterra ] ) , it is possible to compute it for each mode @xmath38 of the basis separately .",
    "thus , the final solution in terms of neumann series is @xmath39 [ eq : final_ex ] that is , the measured coefficients give the solution for the expansion function .",
    "it is also important to stress that the method is able to constrain other cosmologies which are not explicitly included in the original training set .",
    "this again can be achieved by using of a larger number of pcs ( see section 4.2 of * ? ? ?",
    "the errors in our method arises mainly from the uncertainty in the determination of the expansion coefficients ( @xmath15 ) and @xmath5 ( which is left as a free parameter ) due to the minimisation .",
    "the errors in the coefficients then propagate into the estimate of @xmath40 as follows @xmath41 ^ 2=\\sum^j_{j=0}\\left[\\frac{\\delta e(a)}{\\delta c_j}\\right]^2(\\delta c_j)^2=\\sum^j_{j=0}\\left[e^{(j)}(a)\\right]^2(\\delta c_j)^2.\\ ] ]    the final errors on the expansion rate @xmath42 are    @xmath43 ^ 2=\\frac{[\\delta e(a)]^2}{e^4(a)}\\ ] ]    the error contribution due to the minimisation of @xmath5 is added to the previous one in quadrature .",
    "moreover , the uncertainty in our ability to determine the principal components is given by @xmath44 @xcite .",
    "in this way , the total error budget is expressed as @xmath45 .",
    "we applied the method described above to the largest homogeneously reduced sn  ia sample publicly available , the union2.1 @xcite .",
    "this sample contains 580 sne and includes data from snls @xcite , essence @xcite and sdss @xcite surveys , low redshift samples @xcite as well as _ hubble space telescope _",
    "data @xcite .",
    "cdm models with the following sampling of the cosmological parameters : @xmath46 and @xmath47.,width=347,height=264 ]    in fig .",
    "[ fig : pcs ] we show the principal components obtained using the redshift coverage of union2.1 and a training set of 11 flat @xmath0cdm models with the sampling @xmath46 and @xmath47 .",
    "the first principal component for this sample already accounts for @xmath48 of the total variance and its determination carries an uncertainty of @xmath49 .",
    "this means it already contains the main properties of the expansion of the universe and accounts alone for a great part of the total variance sampled in the scatter matrix .",
    "therefore , we restrict ourselves to only one principal component when performing the reconstruction .    fitting the luminosity distance data to the expression in eq .",
    "( [ eq : rec_dl ] ) with 1 pc returns @xmath50 and @xmath1 .",
    "these specific values were obtained considering @xmath51 .",
    "however we did test other reference models and , although the value for @xmath52 depends slightly on the choice of @xmath26 , the final reconstruction and the minimised @xmath5 do not ( this is not the case in general and is due to the fact that the space covered by the training set is in a volume tightly enclosing the data ) . it is worth mentioning that the errors we obtain for @xmath5 are purely statistical and only due to the minimization process .",
    "they are negligible compared with the calibration error .",
    "however , we present our best fit here to demonstrate that the reconstructed zero - point of @xmath53 points towards a lower value than the standard approach . this trend has been confirmed by the planck results @xcite .    the gray area in fig .",
    "( [ fig : union2 ] ) represents the reconstructed expansion history within 3@xmath54 errors . for the sake of comparison",
    ", the figure also shows the best - fit cosmology found by the original union2.1 analysis , @xmath55 @xcite and the latest result reported by the planck satellite team , @xmath56 @xcite . in order to avoid confusion , only the best - fit curves are shown in fig .",
    "( [ fig : union2 ] ) .",
    "both results are in marginal agreement with the behaviour we found for @xmath18 .    for the reasons exposed above ,",
    "it is clear that our approach does not output fits to specific cosmological parameters .",
    "however , we can put our reconstruction in the context of @xmath0cdm models and find the range of @xmath57 values allowed by the behaviour we found for @xmath18 . keeping fixed the value we found for @xmath5 , we obtain @xmath58 ( red curve in fig .  [",
    "fig : union2 ] ) .",
    "we emphasise that the magnitude of the error does not carry the same meaning as in the standard parametric analysis shown in fig .",
    "[ fig : union2 ] .",
    "the determination of range of values for @xmath57 is merely a strategy to better compare our results . unlike other analyses we are tracking only dynamical deviations from @xmath59 , which causes the error bars to be small .",
    "the results we found are significantly higher than the best - fit value obtained by the union2.1 team ( @xmath60 without systematics ; blue curve in fig .",
    "[ fig : union2 ] ) , and are in close agreement with the value reported by planck .",
    "we show in fig .",
    "( [ fig : gaussian ] ) a more clear comparison of our results with others from the sne ia literature ( all using salt2 light curve fitter ; * ? ? ? * ) .",
    "we believe that the shift in our results towards the planck values is an indication that sne ia cosmology should move beyond the parametrized approaches if it aims at dealing with small deviations from the standard values of the cosmological parameters present in the data .",
    "we want to stress again that we are not biased by any theoretical opinion towards a cosmological model , since we do not assume any specific form of the friedmann equations .",
    "in fact , the method is a powerful tool to evaluate non - standard cosmologies as we tentatively showed in @xcite and plan to explore in a more rigorous way in future work .",
    "error , extracted from the union2.1 sample using the optimal basis system with one pc .",
    "the red ( solid ) line represents our best - fit to @xmath0cdm paradigm .",
    "the blue ( dashed ) line is the best - fit obtained by the union team for this sample and the green ( dot - dashed ) line corresponds to results from planck.,width=347,height=264 ]    our model - independent method to constrain the expansion history has other interesting applications .",
    "for instance , it offers a complementary way of detecting possible systematic effects ",
    "e.g. the uncertainties introduced by the light curve calibration , host galaxy extinction or intrinsic variations corresponding to different sn ia populations  which could affect the data and would be overlooked within a traditional analysis based on physical parameterizations .",
    "the method is also a valuable tool that can be used to plan future type ia supernova cosmology campaigns , by testing redshift ranges in which it would be more relevant to collect data ( see * ? ? ?",
    "in this letter , we have applied a method to recover the expansion history of the universe in a model - independent fashion .",
    "the luminosity - distance measurements , obtained from sne  ia , depend only on space - time geometry , and can be directly related to the _ hubble _ function without assuming any dynamical model .",
    "we argue that , as long as the nature of dark energy remains unknown , model - independent analyses of the kind described here have more significance in deriving cosmological parameters than traditional parametric studies .",
    "this is due to the fact that no specific form of the equation of state @xmath19 or the _ hubble _ function is fixed in our approach .",
    "our only assumption relies on the argument that the luminosity distance can be expanded into a series of orthonormal functions .",
    "this basis is chosen to be derived from pca , in an attempt to control the number of coefficients to be included in our reconstruction in a rigorous way .",
    "our analysis shows that sne data do point to a higher value of @xmath57 , in contradiction with what was found with standard methods .",
    "furthermore , this is in agreement with the last results driven by planck and might be an indication that it is important to move beyond parametric fits .",
    "the ultimate goal of this work is to discriminate among different cosmological models , such as @xmath61 or dgp theories , based on very different physical assumptions , and , in this way , break the current degeneracy in the cosmological parameters .",
    "further analysis on simulated data might point to caveats not appearing in real data analysis .",
    "cdm scenario , @xmath58 ( gray region ) , and others from the literature .",
    "the green ( dot - dashed ) line correspond to results reported by the first year of sdss data , @xmath62 @xcite .",
    "cyan ( dashed ) line stands for results reported by union2.1 team , @xmath55 @xcite , the pink ( dotted ) line represents outcomes from snls3 , @xmath63 @xcite and the red ( dot - dot - dashed ) line represents recent results from planck , @xmath56 @xcite .",
    "only statistical errors are considered in this plot.,width=347,height=264 ]    the model - independent method offers a complementary way of detecting possible systematic errors .",
    "this is especially relevant if one considers the dependence introduced by the light - curve fitters on the derived distances moduli from sne  ia .",
    "it is important to test the performance of the available light - curve fitters on the base of model - independent approaches .",
    "a similar analysis of the same data set with different light curve fitters is important and will certainly be presented in a subsequent work .",
    "finally , is it worthwhile noting the potential of the method for the analysis of possible local inhomogeneities through comparison of the expansion history in different directions .",
    "it is a pleasure to thank a. shafieloo and a. kim for providing helpful comments and suggestions .",
    "e.e.o.i . thanks the brazilian agency fapesp ( 2011/09525 - 3 ) for financial support .",
    "this work was also partially supported by the deutsche forschungsgemeinschaft via the transregional collaborative research center `` the dark universe '' ( trr  33 ) , the emmy noether program ( ro 3676/1 - 1 ) , and the excellence cluster `` origin and structure of the universe '' ( exc  153 ) ."
  ],
  "abstract_text": [
    "<S> we perform a model independent reconstruction of the cosmic expansion rate based on type ia supernova data . using the union 2.1 data set , </S>",
    "<S> we show that the _ hubble _ parameter behaviour allowed by the data without making any hypothesis about cosmological model or underlying gravity theory is consistent with a flat @xmath0cdm universe having @xmath1 and * @xmath2 , * weakly dependent on the choice of initial scatter matrix . </S>",
    "<S> this is in closer agreement with the recently released planck results ( @xmath3 ) than other standard analyses based on type ia supernova data . </S>",
    "<S> we argue this might be an indication that , in order to tackle subtle deviations from the standard cosmological model present in type ia supernova data , it is mandatory to go beyond parametrized approaches .    </S>",
    "<S> [ firstpage ]    supernovae : general  cosmology : cosmological parameters  cosmology : observations  cosmology : theory </S>"
  ]
}