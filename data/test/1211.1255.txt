{
  "article_text": [
    "the brain is a computational device for information processing and its flexible and adaptive behaviors emerge from a system of interacting neurons depicting very complex networks @xcite .",
    "many biological evidences suggest that the neocortex implements a common set of algorithms to perform `` intelligent '' behaviors like learning and prediction .",
    "in particular , two important related aspects seem to represent the crucial core for learning in biological neural networks : the hierarchical information processing and the abstraction process @xcite .",
    "the hierarchical architecture emerges from anatomical considerations and is fundamental for associative learning ( e.g. multisensory integration ) .",
    "the abstraction instead leads the inference of concepts from senses and perceptions ( fig . [ fig:1]d ) .",
    "+ specifically , information from sensory receptors ( eyes , skin , ears , etc . )",
    "travels into the human cortical circuits following subsequent abstraction processes .",
    "for instance , elementary sound features ( e.g. frequency , intensity , etc . )",
    "are first processed in the primary stages of human auditory system ( choclea ) .",
    "subsequently sound information gets all the stages of the auditory pathway up to the cortex where higher level features are extracted ( fig .",
    "[ fig:1]e - f ) . in this way information passes from raw data to objects ,",
    "following an abstraction process in a hierarchical layout .",
    "thus , biological neural networks perform generalization and association of sensory information .",
    "for instance , we can associate sounds , images or other sensory objects that present together as it happens in many natural and experimental settings like during pavlovian conditioning . biological networks process these inputs following a hierarchical order . in a first stations inputs from distinct senses",
    "are separately processed accomplishing data abstraction .",
    "this process is repeated in each subsequent higher hierarchical layer .",
    "doing so , in some hierarchical layer , inputs from several senses converge showing associations among sensory inputs .",
    "+ recent findings indicate that neurons can perform invariant recognitions of their input activity patterns producing specific modulations of their synaptic releases @xcite .",
    "although the comphrension of such neuronal mechanisms is still elusive , these hints can drive the development of algorithms closer to biology than spiking networks or other brain - inspired models appear to be .",
    "+ in this work , we propose a learning framework based on these biological considerations , called inductive conceptual network ( icn ) , and we tested the accuracy of this network on the mnist and usps datasets .",
    "the icn represents a general biological plausible model of the learning mechanisms in neuronal networks .",
    "the invariant pattern recognition that occurs in the hierarchy nodes is achieved by modeling node inputs by variable - order markov models ( vmms ) @xcite .",
    "the methods of this work are based on a set of considerations extracted primarily from the memory - prediction framework proposed by jeff hawkins in his book _ on intelligence_. therefore in this section we first present crucial aspects of brain information processing .      as preliminary step",
    "we introduce few theoretical concepts about learning and memory experiences in nervous systems",
    ". the human brain massively elaborates sensory information . through some elusive mechanism",
    ", the brain builds models ( formal representation ) from observations . in such models , pattern recognition and abstraction play a crucial role @xcite .",
    "the former allows for the capture of patterns from observations , the latter allows for transforming raw observations into abstract concepts .",
    "for instance , listening to sequence of unknown songs from an unknown singer we perform both pattern recognition and abstraction , respectively when we identify sound features ( e.g. beats per minute ) and when we infer abstract information concerning the new singer ( e.g. he / she plays jazz ) .",
    "+ key features of these brain processes can be translated in algorithms @xcite .",
    "jeff hawkins et al .",
    "recently proposed a new learning framework ( memory - prediction @xcite ) based on abstraction processes and pattern recognitions .",
    "this paradigm claims that abstraction represents one of the most important tasks underlying learning in the brain and that occurs through the recognition of invariances .",
    "moreover , he suggested that sensory inputs are processed hierarchically : each layer propagates to the next layer the invariant recognized patterns . in propagating only invariances and discarding everything else , data are compressed with size decreasing at every next layer .",
    "this is finely promoted by a pyramidal shape .",
    "hawkins et al .",
    "implemented the memory - prediction framework into a set of software libraries specialized in image processing ( hierarchical temporal memory , htm @xcite ) which exhibits invariant recognition by a complex hierarchy of node implementing the hidden markov model algorithm @xcite .",
    "we propose a different realization of the memory - prediction framework , called inductive conceptual network ( icn ) , where biological neurons are individually identified by nodes ( see figure [ fig:1]a - b ) and invariant recognition is performed by modeling inputs with variable - order markov models ( vmm ) @xcite .",
    "the former assumption allowed us to pin down the icn model into adequate biological background and to evaluate not only its learning ability but also its neurophysiological matching with neuronal population dynamics .",
    "the latter assumption addresses the problem of invariant recognition in a powerful and computational efficient way @xcite .",
    "+ the inductive conceptual network is a hierarchical spiking network working as online unsupervised learning algorithm . in common with htm",
    ", the icn displays a tree - shaped hierarchy of nodes ( figure [ fig:1]d - f ) .",
    "formally , icn is a triplet @xmath0 where @xmath1 is the vector that contains the number of nodes in each layer such that @xmath2 .",
    "let @xmath3 be the total number of nodes , and @xmath4 is the @xmath5x@xmath5 adjacency matrix representing the connections between nodes and @xmath6 is the maximum markov order , an indicator of the memory power of each node .",
    "for the construction of @xmath7 that is initially set to @xmath8 , we proceeded iteratively following these two steps for nodes in each layer @xmath9 :    1 .",
    "a set of deterministic assignations : \\{@xmath10 } with @xmath11 and @xmath12 ; 2 .   a set of random assignations : @xmath13    where @xmath14 is the number of nodes in the generic layer @xmath9 and @xmath15 is the discrete uniform distribution .",
    "layers handle inputs from the immediately preceding layer ( layer below ) except for the first that handles the raw input data .",
    "the matrix @xmath4 is semi - randomly assigned respecting the multilayer architecture : each node receives the downstairs - layer input both from their neighbour nodes and from a small set of randomly chosen ones ( figure [ fig:1]c ) .",
    "+ nodes read inputs from their _ dendrites _ ( figs .",
    "[ fig:1]a - b ) and an algorithm estimates the joint probability distribution of the observed inputs ( see below , vmm ) . whether the observed input is the most expected ( or is very close to ) the node produces a @xmath16 ( representing a spike ) towards their output nodes otherwise does nothing .",
    "the icn is a general purpose learning framework , and although it has not been tested on non - visual tasks it can however be used for other sensory information processing .",
    "the learning of spatiotemporal patterns is the subject of study of sequential data learning that usually involves very common methods , like hidden markov models ( hmm ) .",
    "in fact , hmm are able to model complex symbolic sequences assuming hidden states that control the system dynamics .",
    "however , hmm training suffers from local optima and their accuracy performance has been overcome by vmms .",
    "other techniques like @xmath17-gram models ( or @xmath17-order markov models ) compute the frequency of each @xmath17 long subsequence but in these models the number of possible model states grows exponentially with @xmath17 . therefore ,",
    "both computational space and time issues arise .",
    "+ in this perspective , the observed symbolic ( binary ) sequence is assumed to be generated by a stationary unknown symbol source @xmath18 where @xmath19 is the symbol alphabet and @xmath20 is the probability distribution of symbols . a vmm ( also known as variable length markov chains ) ,",
    "given the maximum order @xmath21 of conditional dependencies and a training sequence @xmath22 generated by @xmath23 , returns a model for the source @xmath23 that s an estimation @xmath24 of probability distribution @xmath20 . applying vmms , instead of @xmath17-gram models",
    ", takes several advantages .",
    "a vmm estimation algorithm builds efficiently a model for @xmath23 .",
    "in fact , only the occurred @xmath21-grams are stored and their conditional probabilities @xmath25 are estimated .",
    "this trick saves lots of memory and computational time and makes feasible to model sequences with very long dependencies ( @xmath26 $ ] ) on current personal computers .",
    "we consider the inputs from dendrites that each neuron ( node ) sees as binary symbols emitted by a discrete source which releases outcomes following an unknown non - stationary probability distribution @xmath20 .",
    "the aim of each node is to learn its source as best as possible so that it can recognize correctly recurrent patterns assigning to them highest probabilities .",
    "the vmms are typically used for this task being able to model dependencies among symbols up to an arbitrary order .",
    "vmms can be estimated by many algorithms .",
    "we took into consideration a famous efficient lossless compression algorithm , the prediction by partial matching ( ppm ) @xcite , implemented in an open - source java software library @xcite .",
    "+ formally , a node reads a binary input ( at each step ) @xmath27 of length @xmath28 that represents the all - or - none dendritic activity .",
    "let @xmath29 be the maximum dependency allowed among input symbols , then each node builds its probability model feeding @xmath6-tuples of the received @xmath28-ary input @xmath22 into the ppm algorithm .",
    "each node has its own instance of the ppm algorithm .",
    "after this first learning phase , the node passes into the prediction mode and looks if it observes in @xmath22 the most expected pattern ( pattern that has the highest probability assignment ) . if it happens , the node produces @xmath16 as output in correspondence to the salient patterns thus preserving the spatial structural organization of inputs . we introduce the further condition that a @xmath16 is produced in correspondence of patterns having hamming distance @xcite very close to the most expected one .",
    "we make this choice to introduce a sort of noise tolerance in the pattern recognition process .",
    "in other words , during the coding ( and second ) stage , a node processes its input by @xmath6 symbols at time .",
    "if the current @xmath6-tuple pattern is the highest probable ( or is very close to , by hamming distance ) a @xmath16 is inserted into the output code , otherwise it marks a @xmath30 . + for instance , let be @xmath31 and @xmath32 the most expected pattern . let @xmath33 be the current input that updates the probability distribution @xmath20 . finally , the node produces the output sequence @xmath34 where @xmath16 corresponds to the two occurrences of the most expected pattern ( @xmath32 ) .",
    "the pseudo - code of the algorithms governing respectively nodes and the hierarchy are the following :    _ algorithm for nodes _    .... node ( )     read input s = ( s_1, ...",
    ",s_n ) ;     for each k - tuple in s :        update p by ppm(s_i, ... ,s_(i+k ) ) ;        if hammingdistance((s_i, ...",
    ",s_(i+k)),bestpattern ) < gamma :              output(1 ) ;        else              output(0 ) ;        update bestpattern ;     end end ....    where the function ` ppm ( ) ` updates the probabilistic model @xmath24 with the new input s_i, ... ,s_(i+k ) .",
    "the function ` hammingdistance(\\cdot,\\cdot ) ` computes the hamming distance between two binary strings and the function ` ppm_best ( ) ` returns the current most probable pattern .    _",
    "algorithm for icn _    .... icn ( )    for each image in dataset :        bw = binarizeimage(image ) ;        assigninputtofirstlayer(bw ) ;        foreach layer in icn :             setinput(bw ) ;             learn ( ) ;             bw = predict ( ) ;        end        collect(bw )",
    "end end ....",
    "where in the ` learn ( ) ` function the distribution @xmath24 of each node are updated and in the ` predict ( ) ` function the spiking activity of the current layer is returned .",
    "+ before evaluating the performance of icn on handwritten digit datasets , we evaluated the learning capabilities of a single node by a simple experiment .",
    "we provided a sequence of 1000 binary 5-tuples as input to a node with 5 dendrites and @xmath35 ( figs .",
    "the input sequence of 5-tuple is generated randomly inserting at each time with probability 0.25 a fixed pattern ( equal to @xmath36 ) . simulating a hebbian setup where at each of the five dendrites",
    "is associated a weight increased in case of positive temporal correlation between pre- and post - synaptic spikes and decreased otherwise @xcite , we make sure that the end of proposed sequence weights of first and fourth dendrites are strengthen to the detriment of other ones ( figure [ fig:2]a ) .    ) and the hebb@xmath37s rule .",
    "( a ) starting with equal weights ( 0.5 ) and assigning a positive increment ( 0.01 ) whether pre - synaptic spike precedes the post - synaptic spike in 2 timesteps at most . otherwise the synaptic weight incurs in a negative reward ( -0.01 ) .",
    "the sequence of input patterns is composed by randomly generated binary inputs ( with probability 0.75 ) plus a fixed input equal to @xmath36 ( with probability 0.25 ) .",
    "the simulation lasts 1000 timesteps where , at the end , the recurrent pattern @xmath36 was recognized assigning strong weights to the first and fourth synapses depressing the other ones .",
    "( b ) in detail , the raster plot of the simulation where the activity of nodes 1 - 5 matches the activity of the 5 presynaptic inputs and the activity of node 6 is the output of node in examination .",
    "( b@xmath37 ) an enlargement of the first 100 timesteps .",
    "( b@xmath38 ) the evolution of the most expected pattern according to the ppm estimation in the node .",
    "after the first 31 timesteps , the fixed pattern @xmath36 becomes the most expected . ]      in the current form , icn can perform unsupervised learning . to evaluate the learning capabilities of such framework",
    ", we gave as input to the first layer , the images of the mnist ( or usps ) dataset ( handwritten digits , 0 to 9 ) .",
    "here we use the mnist test set which contains 10000 examples and the whole 11000 sample of the usps .",
    "the chosen instantiation of icn was composed by 4 layers with respectively 50 , 20 , 5 and 1 node in each layer .",
    "the maximum markov order @xmath6 was set to @xmath39 for all icn nodes .",
    "all parameters in this section have been chosen empirically to best match the right classification of the digits .",
    "we expected that digit images were correctly grouped with respect to the represented number .",
    "mnist images are represented by 28x28 ( 784 ) pixels of 8-bit gray level matrix .",
    "instead , usps images , are represented by 16x16 ( 256 ) pixels .",
    "images were binarized setting a threshold on the 8-bit gray - level values to 80 .",
    "as explained above , nodes produce bits and the result of this unsupervised learning is valuable in the outputs of the top - most node .",
    "in fact , this node retains the most abstract information regarding the observed images .",
    "namely , something likes the concept of number . after some empirical tuning of parameters ( number of nodes , layer and maximum markov order ) , icn was able to discriminate digits by the top - most node output code .",
    "for instance in some experiments , giving an image of digit 0 , the icn emitted the binary code @xmath40 .",
    "in the same experiments , the code @xmath41 was reserved to the digit 1 and so on . obviously , the icn made errors and digit - to - code associations were not unique , e.g. some seven digits can be incorrectly classified with code @xmath41 reserved for the 1 . to estimate the learning error , we chose a representative code for each digit class .",
    "the representatives were selected as most frequents for each class .",
    "thus , the learning error was computed by counting mismatchs between labels and representative codes .",
    "the icn algorithm has been developed following strict and recently found biological criteria from the neurophysiology of neuronal networks .",
    "once ascertained that icn nodes perform a sort of hebbian plasticity ( see section [ node_physiology ] ) we challenged the icn with the mnist dataset ( handwritten digit images ) .",
    "the mnist dataset represents a sort of _ casting - out - nines _ for learning systems ; in fact , new proposed algorithms are tested on this dataset to check their attitude to learn .",
    "+ the learning capabilities of icn were tested by its clustering efficiency over the mnist dataset . before submitted to icn every digit image was binarized by applying a threshold . subsequently each image was fed into the first layer nodes .",
    "invariant recognized patterns are then propagated , layer - by - layer up to the highest , following the execution of algorithm-2 ( see methods ) . as a whole , an input image elicits a bit ( spike ) flux in the bottom layer , a code transmitted to the upper layer .",
    "the top - most layer , composed by only one node , finally generates its binary codes each corresponding to a digit ( class ) of the image input .",
    "we ascertained that at the best tuning of parameters the icn model got an average error of 5.73% , an acceptable score in an unsupervised environment , remarkably not requiring any preprocessing stages such as image alignment , centering or dimensionality reduction . for the usps dataset ,",
    "however harder to learn , the best achieved error was of 12.56% .",
    "+ eventually , we further investigated the influence of dataset size in the learning performance .",
    "for this reason , we repeated the same experiments randomly subsampling both datasets to 1000 and 5000 samples .",
    "for both datasets , performance improved increasing the dataset size as shown in table [ tb1 ] .",
    "even convolutional neural networks ( cnns ) @xcite are biologically inspired by the pioneer works of hubel et al . on the retinotopies of the cat s visual cortex @xcite .",
    "indeed , cnns exploit the fact that nearby pixels are more tightly correlated than more distant ones .",
    "furthermore by using a cascading structure of convolutional and subsampling layers , these networks show successfully invariant recognition of handwritten digits subjected to certain transformations ( scaling , rotation or elastic deformation ) .",
    "altough cnns are bio - inspired by the local receptive fields which constitute the local features , the learning mechanism of the whole network does not appear to have a biological counterpart .",
    "vice versa , the proposed network ( icn ) implements invariant recognition exhibiting a spiking behavior in each node which represents a clear correspondence with biological networks .",
    "furthermore , the algorithm governing nodes is the same in the whole network .",
    "since the electrophysiological properties of neurons are quite similar , our network appears to be more plausible than cnns where a set of special layers ( and nodes ) exclusively perform the invariant recognition .",
    "+ the performance of each node is based on the ppm algorithm that requires @xmath42 during learning and @xmath43 during prediction as computational time complexities @xcite .",
    "although the quadratic complexity , each node receives only small fractions of inputs keeping @xmath28 within small values . thus the overall time complexity for each processed image raises to @xmath44 , where @xmath45 is the number of nodes .",
    "interestingly , the node executions within each layer can be computed in parallel .",
    "even the space complexity is dictated by the complexity of the ppm algorithm that is @xmath46 , where @xmath6 is the chosen markov order , in the worst case .",
    "therefore , the icn algorithm requires @xmath47 in space complexity .",
    "the mnist dataset is a standard test to evaluate learning accuracy for both linear and non - linear classifiers .",
    "we show here that icn is apt to carry out unsupervised learning tasks with an error rate of 5.73% for mnist and 12.56% for usps at most .",
    "the percentage may appear weaker , in comparison with other learning methods , seemingly showing better error rates thanks , however to training and preprocessing ( check for instance the performance of convolutional nets scoring down to 0.35% error rate ) .",
    "furthermore , in comparison with other clustering techniques , our method does not fail into the _ curse of dimensionality _",
    "any classical unsupervised learning techniques , such as k - means , expectation - maximization or support vector machines generally require an _ ad hoc _ dimensionality reduction ( e.g. by independent or principal component analysis ) , a procedure that reduces the algorithm general purposiveness @xcite .",
    "however , these networks do not acknowledge biological modeling , where icn is instead adequately biologically oriented .",
    "+ in conclusion , the proposed model achieves interesting preliminary results .",
    "nevertheless further experiments with other machine learning datasets are required to strengthen its validity . moreover",
    ", future developments can allow for effective multi - input integrations : for instance , two different sources of input ( like sounds and images ) could be associated by similar output codes even in presence of inputs from a single source .",
    "2 bassett , d. s. , greenfield , d. l. , meyer - lindenberg , a. , weinberger , d. r. , moore , s. w. , bullmore , e. t. ( 2010 ) .",
    "efficient physical embedding of topologically complex information processing networks in brains and computer circuits .",
    "( k. j. friston , ed.)plos computational biology , 6(4 ) , e1000748 .",
    "public library of science .",
    "dicarlo , j. , zoccolan , d. , rust , n. ( 2012 ) how does the brain solve visual object recognition ? .",
    "neuron 73:415 - 434 .",
    "takahashi , n. , kitamura , k. , matsuo , n. , mayford , m. , kano , m. , matsuki , n. , ikegaya , y. ( 2012 ) .",
    "locally synchronized synaptic inputs .",
    "science 335:353 - 356 .",
    "kleindienst , t. , winnubst , j. , roth - alpermann , c. , bonhoeffer , t. , lohmann , c. ( 2011 ) .",
    "activity - dependent clustering of functional synaptic inputs on developing hippocampal dendrites .",
    "neuron 72(6):1012 - 1024 .",
    "makino , h. , malinow , r. ( 2011 ) .",
    "compartmentalized versus global synaptic plasticity on dendrites controlled by experience .",
    "neuron 72(6):1001 - 1011 .",
    "meyers , e.m . , qi , x.l . ,",
    "constantinidis , c. ( 2012 ) .",
    "incorporation of new information into prefrontal cortical activity after learning working memory tasks .",
    "usa doi : 10.1073/pnas.1201022109 .",
    "branco , t. , clark , b.a . ,",
    "hausser , m. ( 2010 ) .",
    "dendritic discrimination of temporal input sequences in cortical neurons .",
    "science 329(5999):1671 - 1675 .",
    "bhlmann p , wyner aj , ( 1999 ) . variable length markov chains .",
    "the annals of statistics 27(2 ) , 480 - 513 .",
    "begleiter , r. , el - yaniv , r. , yona , g. ( 2004 ) . on prediction",
    "using variable order markov models .",
    "_ journal of artificial intelligence research _ 22:385 - 421 .",
    "hawkins , j. , blackeslee , s. ( 2004 ) . on intellingence .",
    "new york , times books .",
    "dileep , g. , hawkins , j. ( 2009 ) . towards a mathematical theory of cortical micro - circuits .",
    "plos comput biol 5(10 ) : e1000532 .",
    "baum , l. e. , petrie , t. ( 1966 ) .",
    "statistical inference for probabilistic functions of finite state markov chains .",
    "the annals of mathematical statistics 37 ( 6 ) : 15541563 .",
    "baum , l. e. , petrie , t. , soules , g. , weiss , n. ( 1970 ) . a maximization technique occurring in the statistical analysis of probabilistic functions of markov chains .",
    "the annals of mathematical statistics 41 : 164 .",
    "universal coding , information , prediction , and estimation , jorma rissanen , _ ieee transactions on information theory _ , vol 30 , no . 4 , 1984 , pp .",
    "629 - 636 .",
    "cleary , j. , witten , i. ( 1984 ) .",
    "data compression using adaptive coding and partial string matching . _ ieee transactions on communications _ 32(4):396 - 402 .",
    "teahan , w. ( 1995 ) .",
    "probability estimation for ppm .",
    "proceedings of the new zealand computer science research students conference , university of waikato , hamilton , new zealand . `",
    "ronbeg / vmm / code_index.html ` .",
    "hamming , r.w .",
    "error detecting and error correcting codes , bell system technical journal 29 ( 2):147 - 160 .",
    "hebb , d.o .",
    "the organization of behavior ; a neuropsychological theory .",
    "wiley , new york .",
    "hubel , d. , wiesel , t. ( 1968 ) .",
    "receptive fields and functional architecture of monkey striate cortex .",
    "_ journal of physiology _ , 195:215 - 243 .",
    "lecun , y. , boser , b. , denker , j. s. et al .",
    "( 1990 ) handwritten digit recognition with a back - propagation network . in david touretzky .",
    "advances in neural information processing systems 2 ( nips 1989 ) .",
    "denver : morgan kaufman .",
    "lecun , y. , bengio , y. , ( 1995 ) .",
    "convolutional networks for images , speech , and time - series . in arbib ,",
    "m. a. the handbook of brain theory and neural networks . mit press .",
    "powell , w.b .",
    "approximate dynamic programming : solving the curse of dimensionality .",
    "wiley , new york .",
    "kotsiantis , s. , kanellopoulos , d. , pintelas , p. ( 2006 ) .",
    "data preprocessing for supervised leaning , international journal of computer science 1(2):111 - 117 ."
  ],
  "abstract_text": [
    "<S> the human brain processes information showing learning and prediction abilities but the underlying neuronal mechanisms still remain unknown . recently , many studies prove that neuronal networks are able of both generalizations and associations of sensory inputs . </S>",
    "<S> + in this paper , following a set of neurophysiological evidences , we propose a learning framework with a strong biological plausibility that mimics prominent functions of cortical circuitries . </S>",
    "<S> we developed the inductive conceptual network ( icn ) , that is a hierarchical bio - inspired network , able to learn invariant patterns by variable - order markov models implemented in its nodes . </S>",
    "<S> the outputs of the top - most node of icn hierarchy , representing the highest input generalization , allow for automatic classification of inputs . </S>",
    "<S> we found that the icn clusterized mnist images with an error of 5.73% and usps images with an error of 12.56% .    </S>",
    "<S> recognition ; handwritten digits ; abstraction process ; hierarchical network </S>"
  ]
}