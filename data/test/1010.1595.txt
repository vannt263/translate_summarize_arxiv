{
  "article_text": [
    "the metropolis  hastings ( mh ) algorithm provides an iterative and converging scheme to sample from a complex target density @xmath0 .",
    "each iteration of the algorithm generates a new value of the markov chain that relies on the result of the previous iteration .",
    "the underlying markov principle is well - understood and leads to a generic convergence principle as described , e.g. , in @xcite . however , due to its markovian nature , this algorithm is not straightforward to parallelize , which creates difficulties in slower languages like r @xcite . nevertheless , the increasing number of parallel cores that are available at a very low cost drives more and more interest in `` parallel - friendly '' algorithms , that is , in algorithms that can benefit from the available parallel processing units on standard computers ( see .",
    "e.g. , @xcite , @xcite , @xcite ) .",
    "different techniques have already been used to enhance some degree of parallelism in generic metropolis  hastings ( mh ) algorithms , beside the basic scheme of running @xmath1 mcmc algorithms independently in parallel and merging the results .",
    "for instance , a natural entry is to rely on renewal properties of the markov chain @xcite , waiting for all @xmath1 chains to exhibit a renewal event and then using the blocks as iid , but the constraint of markovianity can not be removed .",
    "@xcite also points out the difficult issue of accounting for the burn - in time : while , for a single mcmc run , the burn - in time is essentially negligible , it does create a significant bias when running parallel chains ( unless perfect sampling can be implemented ) .",
    "@xcite mix antithetic coupling and stratification with perfect sampling .",
    "using a different approach , @xcite rely on @xmath1 parallel chains to build an adaptive mcmc algorithm , considering in essence that the product of the target densities over the chains is their target , a perspective that obviously impacts the convergence properties of the multiple chain . @xcite",
    "take advantage of parallelization to build a non - reversible algorithm that can avoid the scaling effect of specific neighborhood structures , hence focussing on a very special type of problem .",
    "a particular family of mh algorithm is the independent metropolis  hastings ( imh ) algorithm , where the proposal distribution ( and hence the proposed value ) does not depend on the current state of the markov chain . due to this characteristic , this specific algorithm is easier to parallelize and can therefore be considered as a good building block toward efficient parallel markov chain monte carlo algorithms , as will be explained in section [ sec : imh ] .",
    "we will focus on cases where the computation of the likelihood function constitutes the major part of the execution time in the mh algorithm .",
    "a most realistic example of this setting is provided in @xcite , where the model is based on a very complex fortran program translating the results of several cosmological experiments , hence highly demanding in computing time . in this model ,",
    "@xcite use adaptive importance sampling and massive parallelization , rather than mcmc .",
    "the fundamental idea in the current paper is that one can take advantage of the parallel abilities of arbitrary items of computing machinery , from cloud computing to graphical cards ( gpu ) , in the case of the generic imh algorithm , producing an output that corresponds to a much improved monte carlo approximation machine at the same computational cost .",
    "the techniques presented here are related with those explained in @xcite and more closely to those in @xcite ( @xcite , section 3.1 ) , since these authors condition upon the order statistic of the values proposed by the imh algorithm , although in those earlier papers the links with parallel computation were not established and hence the implementation of the rao - blackwellization scheme became problematic for long chains .",
    "the plan of the paper is as follows : the standard imh algorithm is recalled in section [ sec : imh ] , followed by a description of our improving scheme , called here `` block independent metropolis ",
    "hastings '' ( block imh ) .",
    "this improvement depends on a choice of permutations on @xmath2 that is described in details in section [ sec : permut ] .",
    "we demonstrate the connections between block imh and rao  blackwellization in section [ sec : rb ] .",
    "results for a toy example are presented throughout the paper and a realistic probit regression example is described in section [ sec : applications ] as an illustration of the method .",
    "we recall here the basic imh algorithm , assuming the availability of a proposal distribution that we can sample , and which probability density @xmath3 is known up to a normalization constant . the independent metropolis  hastings algorithm , described in algorithm [ algo : imh ] , generates a markov chain with invariant density @xmath0 , corresponding to the target distribution .",
    "set @xmath4 to an arbitrary value generate @xmath5 [ basichmacpt ] compute the ratio : @xmath6 set @xmath7 with probability @xmath8 ; otherwise set @xmath9    in the larger picture of monte carlo and mcmc algorithms , the imh algorithm holds a rather special status .",
    "it has certainly been studied more often than other mcmc schemes @xcite , but it is undoubtedly a less practical solution than the more generic random walk metropolis  hastings algorithm .",
    "for instance , it is rather rarely used by itself because it requires the derivation of a tolerably good approximation to the true target , approximation that most often is unavailable . on the other hand ,",
    "first - order approximations and metropolis - within - gibbs schemes are not foreign to calling for imh local moves based on gaussian representations of the targets .",
    "the reason theoretical studies of the imh algorithm abound is that it has strong links with the non - markovian simulation methods such as importance sampling .",
    "contrary to random - walk metropolis ",
    "hastings schemes , imh algorithms may enjoy very good convergence properties and may also reach acceptance probabilities that are close to one .",
    "furthermore , the potentially large gain in variance reduction provided by the parallelization scheme developped in this paper may counteract the lesser efficiency of the original imh compared with the random walk metropolis  hastings algorithm .",
    "an important feature of the imh algorithm , when addressing parallelism , is that it can not work but in an iterative manner , since the outcome of step @xmath10 , namely the value @xmath11 , is required to compute the acceptance ratio at step @xmath12 .",
    "this sequential construction is compulsory for the validation of the algorithm given the markov property at its core @xcite .",
    "nonetheless , given that , in the imh algorithm , the proposed values @xmath13 are generated independently from the current state of the markov chain , @xmath11 , it is altogether possible to envision the generation of @xmath14 proposed values @xmath15 first , along with the computation of the associated ratios @xmath16 .",
    "once this computation requirement is completed , only the acceptance steps need to be considered iteratively .",
    "this two - step perspective makes for a huge saving in computing time when the simulation of the @xmath15 s and the derivation of the @xmath17 s can be achieved in parallel since both the remaining computation of the ratios @xmath8 given the @xmath17 s and their subsequent comparison with uniform draws typically are orders of magnitude faster .    in this respect",
    "the imh algorithm strongly differs from the random walk metropolis  hastings ( rwmh ) algorithm , for which acceptance ratios can not be processed beforehand because the proposed simulated values depend on the current value of the markov chain .",
    "the universal availability of parallel processing schemes may thus lead to a new surge of popularity for the imh algorithm .",
    "indeed , when taking advantage of @xmath1 parallel processing units , an imh can be run for @xmath1 times as many iterations as rwmh , at almost the same computing cost since rwmh can not be directly parallelized .    in order to better describe this increased computing power , we first note that , once @xmath14 successive values of a markov chain have been produced , the sequence is usually processed as a regular monte carlo sample to obtain an approximation of an expectation under the target distribution , @xmath18 $ ] say , for some arbitrary functions @xmath19 .",
    "we propose in this paper a technique that improves the precision of the estimation of this expectation by taking advantage of parallel processing units without jeopardizing the markov property .    before presenting our improvement scheme",
    ", we introduce the notation @xmath20 ( read `` or '' ) for the operator that represents a single step of the imh algorithm . using this notation ,",
    "given the current value @xmath11 and a sequence of @xmath1 independent proposed values @xmath21 , the imh algorithm goes from step @xmath10 to step @xmath22 according to the diagram in figure [ fig : imh ] .",
    "imh steps between iteration @xmath10 and iteration @xmath22.,scaledwidth=100.0% ]      we propose to take full advantage of the simulated proposed values and of the computation of their corresponding @xmath23 ratios . to this effect",
    ", we introduce the _ block imh algorithm _ , made of successive simulations of blocks of size @xmath24 . in this alternative scheme , the number of blocks @xmath25 is such that the number of desired iterations @xmath14 is equal to @xmath26 , in order to keep the comparison with a standard imh output fair .",
    "usually @xmath1 needs not be calibrated since it represents the number of physical parallel processing units that can be exploited by the code .",
    "however , in principle , this number @xmath1 can be set arbitrarily high and based on virtual parallel processing units , the drawback being then an increase in the computing cost .",
    "( note that the block imh algorithm can also be implemented with no parallel abilities , still it provides a gain in variance that may counteract the increase in time . ) in the following examples , we take @xmath1 varying from @xmath27 to @xmath28 .",
    "we first explain how a block is simulated , and then how to move from one block to the next .",
    "a @xmath29 block consists in the generation of @xmath1 parallel generations of @xmath1 values of markov chains , all starting at time @xmath10 from the current state @xmath11 and all based on the _ same _ proposed simulated values @xmath30 .",
    "the different between the @xmath1 floes is the orders in which those @xmath31 s are included .",
    "for instance , these orders may be the @xmath1 circular permutations of @xmath30 , or they may be instead random permutations , as discussed in detail ( and compared ) in section [ sec : permut ] . the block imh algorithm is illustrated in figure [ fig : imhblock ] for the circular set of permutations .",
    "block simulation from step @xmath12 to step @xmath22 . here",
    ", circular permutations of the proposed values are used for illustration purposes.,scaledwidth=100.0% ]    it should be clear that each of the @xmath1 parallel chains found in this block is a valid mcmc sequence of length @xmath1 when taken separately . as such",
    ", it can be processed as a regular mcmc output . in particular , if @xmath11 is simulated from the stationary distribution , any of the subsequent @xmath32 is also simulated from the stationary distribution .",
    "however , the point of the @xmath1 parallel flows is double :    * it aims at integrating out part of the randomness resulting from the ancillary order in which the @xmath33 s are chosen , getting close to the conditioning on the order statistics of the @xmath33 s advocated by @xcite ; * it also aims at partly integrating out the randomness resulting from the generation of uniform variables in the selection process , since the block implementation results in drawing @xmath34 uniform realizations instead of @xmath1 uniform realizations for a standard imh setting .",
    "both of those points essentially amount to implementing a new rao  blackwellization technique ( a more precise connection is drawn in section [ sec : rb ] ) . in an independent setting ,",
    "each of the @xmath33 s occurs a number @xmath35 of times across the @xmath1 steps of the @xmath1 parallel chains , i.e.  for a number @xmath34 of realizations .",
    "therefore , when considering the standard estimator @xmath36 of @xmath18 $ ] , based on a _ single _ mcmc chain , @xmath37this estimator necessarily has a larger variance than the double average @xmath38where @xmath39 and @xmath40 is the number of times @xmath11 is repeated .",
    "( the proof for the reduction of the variance from @xmath36 to @xmath41 easily follows from a double integration argument . )",
    "we again insist on the compelling feature that computing @xmath41 using @xmath1 parallel processing units does not cost more time than computing @xmath36 using a single processing unit .",
    "the block imh algorithm runs @xmath1 parallel chains during @xmath1 steps , then picks one of the final values ( represented by the black squares ) and iterates .",
    "( an alternative transition mechanism involves sampling randomly one of the @xmath34 terms within the block.),scaledwidth=100.0% ]    in order to preserve its markov validation , the algorithm must properly continue at time @xmath22 . an obvious choice is to pick one of the @xmath1 sequences at random and to take the corresponding @xmath42 as the value of @xmath43 , starting point of the next parallel block .",
    "this mechanism is represented in figure [ fig : imhnextblock ] . while valid from a markovian perspective , since the sequences are marginally produced by a regular imh algorithm , this means that the chain deduced from the block imh algorithm is converging at _ exactly _ the same speed as the original imh algorithm .",
    "an alternative choice for the starting points of the blocks takes advantage of the weights @xmath44 on the @xmath33 s that are computed via the block structure .",
    "indeed , those weights essentially act as importance weights and they allow for a selection of any of the @xmath34 @xmath45 s as the starting point of the incoming block , which corresponds to choosing one of the proposed @xmath33 s with probability proportional to @xmath44 .",
    "while this proposal does reduce the length of the resulting chain , it does not impact the estimation aspects ( which still involve all of the @xmath34 values ) and it could improve convergence , given that the weighted @xmath33 s behave like a discretized version of a sample from the target density @xmath0 .",
    "we will not cover this alternative any further .",
    "the original version of the block imh algorithm is described in algorithm [ algo : blockimh ] , the algorithm is made of a loop on the @xmath25 blocks and an inner loop on the @xmath1 parallel chains of each block .",
    "the @xmath1 steps of this inner loop are actually meant to be implemented in parallel .",
    "the output of algorithm [ algo : blockimh ] is double :    * a standard markov chain of length @xmath14 , which is made of @xmath25 chains of length @xmath1 , each of which is chosen among @xmath1 chains at line [ algo : statepickindex ] of algorithm [ algo : blockimh ] , * a @xmath46 array @xmath47 , on which the estimator @xmath41 is based .",
    "set @xmath4 to an arbitrary value , compute @xmath48 set @xmath49 , @xmath50 set a block size @xmath1 , and a number of blocks @xmath25 , such that @xmath51 generate all proposed values @xmath52 [ algo : stateintensive ] compute all ratios @xmath53 [ algo : chooseperm ] choose @xmath1 permutations @xmath54 run @xmath1 steps of an imh given :    * @xmath55 * @xmath1 proposed values @xmath56 shuffled with the permutation @xmath57 * the @xmath1 corresponding ratios @xmath58 s    save as @xmath59 the resulting chain [ algo : statepickindex ] draw an index @xmath60 uniformly in @xmath61 , set @xmath62 , set @xmath63 as the corresponding ratio @xmath23 .      since the point - wise evaluation of the target density @xmath64 is usually the most computer - intensive part of the algorithm , sampling additional uniform variables has a negligible impact here , as do further costs related to the storage of vectors larger than in the original imh .",
    "this is particularly compelling since the multiple chains do not need to be stored further than during a single block execution time .",
    "that is why , although we sample @xmath1 times more uniforms in the block imh algorithm , we still consider it to be running at roughly of the same cost as the imh algorithm .",
    "the number of target density evaluations indeed is the same for both and most often represent the overwhelming part of the computing time in the metropolis  hastings algorithm . besides",
    ", pseudo - random generation of uniforms can also benefit from parallel processing , see e.g. @xcite .    in the following monte carlo experiment ,",
    "various versions of the block imh algorithm are compared one to another , as well as to standard imh and importance sampling .",
    "we stress that a straightforward reason for not conducting a comparison with a plain parallel algorithm based on @xmath1 independent parallel chains is that it does not make much sense cost - wise . indeed , running @xmath1 parallel mcmc chains of the same length @xmath14 does cost @xmath1 times more in terms of target density evaluations .",
    "obviously , if one insists on running @xmath1 independent chains , for instance as to initialize an mcmc algorithm from several well - dispersed starting points , each of those chains can benefit from our stabilizing method , which will improve the resulting estimation .",
    "the method is presented here for square blocks of dimension @xmath65 , but blocks could be rectangular as well : the algorithm is equally valid when using @xmath66 permutations , leading to @xmath67 blocks .",
    "we focus here on square blocks because when the machine at hand provides @xmath1 parallel processing units , then it is most efficient to simulate the proposed values and the uniforms , and to compute the target densities and the acceptance ratios at the @xmath1 proposed values in parallel .",
    "once again , the block imh algorithm with @xmath29 square blocks has about the same cost as the original imh algorithm , because computing target densities and acceptance ratios does more than compensate for the cost of randomly picking an index at the end of each block .",
    "this amounts to say that line [ basichmacpt ] of algorithm [ algo : imh ] and line [ algo : stateintensive ] of algorithm [ algo : blockimh ] are ( by far ) the most computationally demanding ones in the respective algorithms .",
    "we now introduce a toy example that we will follow throughout the paper .",
    "the target @xmath0 is the density of the standard @xmath68 normal distribution and the proposal @xmath3 is the density of the @xmath69 cauchy distribution .",
    "hence , the density ratio is @xmath70 ( 1 + x^2 ) we only consider the integral @xmath71 , the mean of @xmath0 equal to zero in this case .",
    "the acceptance rate of the imh algorithm for this example is around @xmath72 .",
    "( note that imh with higher acceptance rates are considered to be more efficient , in opposition to other metropolis  hastings algorithms , see @xcite . )    in all results related to the toy example presented thereafter , @xmath73 independent runs are used to compute the variance of the estimates .",
    "the value of @xmath1 represents the number of parallel processing units that are available , ranging from @xmath27 for a desktop computer to @xmath28 for a cluster or a graphics processing unit ( gpu ) ( this value could even be larger for computers equipped with multiple gpus ) .",
    "the results of the simulation experiments are presented in figures [ fig : barplotpermutations][fig : barplotvariousscale ] as barplots , which indicate the percentage of variance decrease associated with the estimators under comparison , the reference estimator being the standard imh output @xmath36 . in agreement with the block sampling perspective",
    ", the same proposed values and uniform draws were used for all the estimators that are plotted on the same graph ( that is , for a given value of @xmath1 ) , so that the comparison is not perturbed by an additional noise associated with the simulation .",
    "while the choice of permutations in line [ algo : chooseperm ] of algorithm [ algo : blockimh ] is irrelevant for the validation of the parallelization , it has important consequences on the variance improvement and we now discuss several natural choices .",
    "the idea of testing various orders of the proposed values in a imh algorithm appeared in @xcite where the permutations were chosen to be circular .",
    "we first list natural types of permutations along with some justifications , and then we empirically compare their impact on estimation performances for the toy example .",
    "let @xmath74 be the set of permutations of @xmath75 .",
    "its size is @xmath76 , therefore too large to allow for averaging over all permutations , although this solution would be ideal .",
    "we consider the simpler option of finding @xmath1 efficient permutations in @xmath74 , denoted by @xmath77 , the goal being a choice favoring the largest possible decrease in the variance of the estimator @xmath41 defined in section [ sec : imh ] .",
    "the most basic choice is to pick the same permutation on each of the @xmath1 chains : @xmath78 this selection may sound counterproductive , but we still obtain a significant decrease in the variance of @xmath41 using this set of permutations , when compared with @xmath36 .",
    "the reason for the improvement is that @xmath1 times more uniforms are used in @xmath41 than in @xmath36 , leading to a natural rao - blackwellization phenomenon that is studied in details in section [ sec : rb ] .",
    "nonetheless this simplistic set of permutations is certainly not the best choice since it does not integrate out the ancillary randomness resulting from the arbitrary ordering of the proposed values .",
    "another simple choice is to use circular permutations . for @xmath79",
    ", we define @xmath80 an appealing property of the circular permutations is that each simulated value @xmath33 is proposed and evaluated at a different step for each chain .",
    "however , a drawback is that the order is not deeply changed : for instance @xmath81 will always be proposed one step before @xmath33 except for one of the @xmath1 chains , for which @xmath33 is proposed first .",
    "a third choice is to use random orders , that is random shufflings of the sequence @xmath75 .",
    "we can either draw those random permutations with or without replacement in the set @xmath74 , but considering the cardinality of the set @xmath74 this does not make a large difference . indeed , it is unlikely to draw twice the same permutation , except for very small values of @xmath1 .",
    "a slightly different choice of permutations consists in drawing @xmath82 permutations at random ( @xmath1 is taken to be even here to simplify the notations ) .",
    "then , denoting the first @xmath82 permutations by @xmath83 , we define for @xmath84 : @xmath85 the motivation for this inversion of the orders is that , in the second half of the permutations , the opposition with the `` reversed '' first half is maximal .",
    "this choice , suggestion of art owen ( personal communication ) , aims at minimizing the possible common history among the @xmath1 parallel chains . indeed",
    "two chains with the same proposed values in reverse order can not have a common path of length more than 1 .",
    "finally we can try to draw permutations that are far from one another in the set @xmath74 .",
    "for instance we can define the lexicographic order on @xmath74 , draw indices from a low discrepancy sequence on the set @xmath86 and select the permutations corresponding to these indices . in a simpler manner",
    ", we do use here a stratified sampling scheme : we first draw a random permutation conditionally on its first element being @xmath87 , then another permutation beginning with @xmath88 , and so forth until the last permutation which begins with @xmath1 .",
    "we compare the five described types of permutations on the toy example .",
    "figure [ fig : barplotpermutations ] shows the results for various values of @xmath1 , displaying the variance reduction of @xmath41 associated with each of the permutation orders , compared to the variance of the original imh estimator @xmath36 . for each of the @xmath73 independent replications ,",
    "the block imh algorithm was launched on one single @xmath29 block , e.g. with @xmath89 using the notation of section [ sec : imh ] , since @xmath25 plays no role whatsoever in this comparison .    as mentioned above , using the same order in the @xmath33 s for each of the @xmath1 parallel chains already produces a significant decrease of about @xmath90 in the variance of the estimators .",
    "this simulation experiment shows that the three random permutations ( random , half - random half - reversed and stratified ) are quite equivalent in terms of variance improvement and that they are significantly better than the circular permutation proposal , which only slightly improves over the `` same order '' scheme .",
    "therefore , in the next monte carlo experiments , we will only use the random order solution , simplest of the random schemes .",
    "an amount of improvement like @xmath91 when @xmath92 is quite impressive when considering that it is essentially obtained cost - free for a computer with parallel abilities @xcite .",
    "another generic improvement that can be brought over classical mh algorithms is rao  blackwellization @xcite . in this section , two rao ",
    "blackwellization methods are presented , one that is computationally free and one that , on the contrary , is computationally expensive .",
    "we then implement both solutions within the block imh algorithm and explain why the `` same order '' scheme already improves upon the imh algorithm .      within the standard imh algorithm of section [ sec : imhb ]",
    ", a cost - free improvement can be obtained by a straightforward rao  blackwellization argument . given that at step @xmath93 , @xmath31 is accepted with probability @xmath94 and rejected with probability @xmath95 , the weight of @xmath31 can be updated by @xmath94 and the weight of the simulated value @xmath96 corresponding to @xmath97 can be similarly updated by the probability @xmath95 .",
    "considering next the block imh algorithm , at the beginning of each block we can define @xmath1 weights , denoted by @xmath98 , initialized at @xmath99 and then , for the first of the @xmath1 parallel chains , denoting by @xmath60 the index such that @xmath100 , we update these weights at each time @xmath93 as @xmath101 this is obviously repeated for each of the other parallel chains , ending up with @xmath102 .",
    "this leads to a new estimator @xmath103 this estimator still depends on all uniform generations created within the block , since those weights @xmath104 depend upon the acceptances and rejections of the @xmath33 s made during the block update .",
    "however , along the steps of the block , the @xmath104 s are basically updated by the expectations of the acceptance indicators conditionally upon the results of the previous iterations , whereas the @xmath44 of section [ sec : imh ] are directly updated according to the acceptance indicators .",
    "hence , the @xmath104 s have a smaller variance than the @xmath44 s by virtue of the rao ",
    "blackwell theorem , leading to @xmath105 necessarily having a smaller variance than @xmath41 .",
    "we now discuss a more involved rao - blackwellization technique first proposed by @xcite .      exploiting the rao ",
    "blackwellization technique of @xcite within each parallel chain provides via a conditioning argument an even more stable approximation of arbitrary posterior quantities . as developed in @xcite , for a single markov chain @xmath106 , a rao  blackwell weighting scheme on the proposed values @xmath15 , with weights @xmath107 ,",
    "is given by a recursive scheme @xmath108 where @xmath109 @xmath110and @xmath111 associated with the metropolis ",
    "hastings ratios @xmath112 the cumulated computation of the @xmath113 s , of the @xmath114 s and of the @xmath115 s requires an @xmath116 computing time .",
    "given that @xmath1 is usually not very large , this additional cost is not redhibitory as in the original proposal of @xcite who were considering the application of this rao  blackwellization technique over the whole chain , with a cost of @xmath117 ( see also @xcite ) .",
    "therefore , starting from the estimator @xmath41 , the weight @xmath44 counting the number of occurrences of @xmath33 in the @xmath29 block can be replaced with the expected number @xmath118 of times @xmath33 occurs in this block ( given the @xmath1 proposed values ) , which is the sum of the expected numbers of times @xmath33 occurs in each of the @xmath1 parallel chain : @xmath119 since the @xmath1 parallel chains incorporate the proposed values with different orders , the @xmath120 s may differ for each chain and must therefore be computed @xmath1 times . note that the cost is still in @xmath116 if this computation can be implemented in parallel .",
    "then , by a rao - blackwell argument , @xmath41 and @xmath121 are dominated by @xmath122 defined as follows : @xmath123therefore , this rao  blackwellization scheme involves _ no _ uniform generation for the computation of @xmath122 : the randomness associated with these uniforms is completely integrated out .",
    "the four estimators defined up to now can be summarized as follows :    * @xmath36 is the basic imh estimator of @xmath18 $ ] , * @xmath41 improves @xmath36 by averaging over permutations of the proposed values , and by using @xmath1 times more uniforms than @xmath36 , * @xmath105 improves upon @xmath41 by a basic rao - blackwell argument , * @xmath122 improves upon the above by a further rao - blackwell argument , integrating out the ancillary uniform variables , but at a cost of @xmath116 .",
    "note that these four estimators all involve the same number @xmath1 of target density evaluations , which again represent the overwhelming part of the computing time .",
    "figure [ fig : barplotrb ] gives a comparison between the variances of the three improved estimators defined above and the variance of the basic imh estimator .",
    "the permutations are random in this case .",
    "as was already apparent on figure [ fig : barplotpermutations ] , the block estimator @xmath41 is significantly better than @xmath36 for any value of @xmath1 .",
    "moreover , both rao - blackwellization modifications seem to improve only very slightly the estimation when compared with @xmath41 , even though the improvement increases with @xmath1 .",
    "recall that the `` same order '' scheme already provided a significant decrease in the variance of the estimation . in the light of our results ,",
    "our interpretation is that using @xmath1 parallel chains with the same proposed values acts like a `` poor man '' rao  blackwellization technique since @xmath1 times more uniforms are used .",
    "specifically , each of the @xmath1 proposed values is proposed @xmath1 times instead of once , thus reducing the impact of each single uniform draw on the overall estimation .    when we use rao",
    " blackwellization on top of the block imh , in the estimators @xmath105 and @xmath122 , we try indeed to integrate out a randomness that already is partly gone .",
    "this explains why , although rao ",
    "blackwellization techniques provide a significant improvement over standard imh , the improvement is much lower and thus rather unappealing when used in the block imh setting .",
    "this outcome was at first frustrating since rao ",
    "blackwellization is indeed affordable at a cost of only @xmath116 .",
    "however , this shows _ in fine _ that the improvement brought by the block imh algorithm roughly provides the same improvement as the rao  blackwell solution , at a much lower cost .",
    "the proposal density @xmath3 may also be used to construct directly an importance sampling ( is ) estimator @xmath124 where the values @xmath15 are drawn from @xmath3 .",
    "it therefore makes sense to compare the imh algorithm with an is approximation because is is similarly easy to parallelize , and straightforward to program .",
    "furthermore , since the is estimator does not involve ancillary uniform variables , it is comparable to the rao ",
    "blackwellized version of imh , and hence to the block imh .",
    "obviously , is can not necessarily be used in the settings when imh algorithms are used , because the latter are also considered for approximating simulations from the target density @xmath0 . in particular , when considering metropolis - within - gibbs algorithms , is can not be used in a straightforward manner , even for approximating integrals .    before giving numerical results for a comparison run on the toy example ,",
    "we now explain why in this comparison we took the number of blocks to be larger than @xmath87 .",
    "the previous comparisons were computed with @xmath125 , i.e.  on a single @xmath126 block and for a large number of independent runs .",
    "the choice of @xmath25 was then irrelevant since we were comparing methods that were exploiting in different ways the @xmath1 proposed values generated in each block .",
    "when considering the block imh algorithm as a whole , as explained in section [ sec : imh ] , the end of each block sees a new starting value chosen from the current block .",
    "this ensures that the algorithm produces a valid markov chain .",
    "however , our construction also implies that the successive blocks produced by the algorithm are correlated , which should lead to lesser performances than for the is estimator .    in the comparison between imh and",
    "is , we therefore need to take into account this correlation between successive blocks . to this effect",
    ", we produce the variance reductions for several values of @xmath25 .",
    "those reductions are presented in figure [ fig : barplotis ] for @xmath127 and different values of @xmath128 .",
    "once again , the permutations in the block imh algorithm are chosen to be random .",
    "figure [ fig : barplotis ] shows the a priori surprising result that , when selecting @xmath89 in the experiment , the variance results are in favor of the block imh solutions over the is estimator , but , for any realistic application , @xmath25 is ( much ) larger than @xmath87 . for all @xmath129 ,",
    "the is estimator has a smaller variance than the three alternative block imh estimators , if only by a small margin .",
    "( note that the variance improvement over the original mcmc estimator is slightly increasing with @xmath25 despite the correlation between blocks , given that the correlation between the @xmath34 terms involved in the block imh estimators is lower than the correlation in the original mcmc chain . )",
    "this experiment thus shows that the block imh solution gets very close to the is estimator , while preserving the markovian features of the original imh algorithm .",
    "in order to evaluate the performances of the parallel processing presented in this paper on a realistic example , we examine its implementation for the probit model already analyzed in @xcite for the comparison of model choice techniques because the  plug - in \" normal distribution based on mle estimates of the first two moments works perfectly as an independent proposal .",
    "a probit model can be represented as a natural latent variable model in that , if we consider a sample @xmath130 of @xmath131 independent latent variables associated with a standard regression model , i.e.  such that @xmath132 , where the @xmath133 s are @xmath1-dimensional covariates and @xmath134 is the vector of regression coefficients , then @xmath135 such that @xmath136 is a probit sample .",
    "indeed , given @xmath134 , the @xmath31 s are independent bernoulli rv s with @xmath137 where @xmath138 is the standard normal cdf .",
    "the choice of a prior distribution for the probit model is open to debate , but the above connection with the latent regression model induced @xcite to suggest a @xmath139-prior model , @xmath140 , with @xmath131 as the @xmath139 factor and @xmath141 as the regressor matrix .",
    "while a gibbs sampler taking advantage of the latent variable structure is implemented in @xcite and earlier references @xcite , a straightforward metropolis ",
    "hastings algorithm may be used as well , based on an independent proposal @xmath142 , where @xmath143 is the mle estimator , @xmath144 its asymptotic variance , and @xmath145 a scaling factor .",
    "as in @xcite and @xcite , we use the r pima indian benchmark dataset @xcite , which contains medical information about @xmath146 pima indian women with seven covariates and one explained binary diabetes variable .    for the purpose of illustrating the implementation of the block imh algorithm",
    ", we only consider here three covariates , namely plasma glucose concentration ( with coefficient @xmath147 ) , diastolic blood pressure ( with coefficient @xmath148 ) and diabetes pedigree function ( with coefficient @xmath149 ) .",
    "we are interested in the posterior mean of those three regression parameters . in our experiment",
    ", we ran @xmath73 independent replications of our algorithm to produce a reliable evaluation of the variance of the estimators under comparison . in figure",
    "[ fig : barplotprobit ] we present the variance comparison of the four estimators described in section [ sec : rb ] , for @xmath150 and @xmath151 and for each of the three regression parameters . in the independent proposal ,",
    "the scale factor is chosen to be @xmath152 since pilot runs showed that it led to an acceptance rate around @xmath153 , with thus enough rejections to exhibit improvement by rao  blackwellization .",
    "the results shown in figure [ fig : barplotprobit ] confirm the huge decrease in variance previously observed in the toy example .",
    "the gains represented in those figures indicate that the block estimator @xmath41 is nearly as good ( in terms of variance improvement ) as the rao  blackwellized block estimators @xmath105 and @xmath122 , especially when @xmath1 moves from @xmath27 to @xmath154 .",
    "this confirms the previous interpretation given in section [ sec : rb ] that the block imh algorithm provides a cost - free rao  blackwellization as well as a partial averaging over the order of the proposed values .",
    "the fact that the toy example showed decreases in the variance that were around @xmath91 whereas the probit regression shows decreases around @xmath155 is worth discussing .",
    "the amount of decrease in the variance differs from one example to the other , but it is more importantly depending on the acceptance rate of the metropolis  hastings algorithm .",
    "in fact , rao  blackwellization and permutations of the proposed values are useless steps if the acceptance rate is exactly @xmath87 . on the opposite",
    ", it may result in a significant improvement when the acceptance rate is low ( since the part of the variance due to the uniform draws would then be much more important ) .    to illustrate the connection between the observed improvement and the acceptance rate , we propose in figure [ fig : barplotvariousscale ] a variance comparison for @xmath127 and two scaling factors @xmath145 of the proposal covariance matrix in the probit regression model . in the previous experiment ,",
    "we have used @xmath156 , which leads to an acceptance ratio around @xmath153 .",
    "here , if we take @xmath157 , the acceptance ratio rises to @xmath158 , and hence almost all the proposed values are accepted . in this case permuting the proposed values and using rao ",
    "blackwellization techniques does not bring much of a variance decrease ( figure [ fig : barplotvariousscale ] , top ) . on the other hand ,",
    "if we take @xmath159 , the acceptance ratio drops down to @xmath160 and the observed decrease in variance is huge .",
    "in this second case using all the proposed values gives much better results than relying on the standard imh estimator , which is only based on @xmath160 of the proposed values that were accepted ( figure [ fig : barplotvariousscale ] , bottom ) .",
    "the difference observed with this range of scaling factors is thus in agreement with the larger decrease in variance observed for the probit regression .",
    "this is a positive feature of the block imh method , since in a complex model , the target distribution is most often poorly approximated by the proposal and thus the acceptance rate of the imh algorithm is quite likely to be low .",
    "the monte carlo experiments produced in this paper have shown that the proposed method improves significantly the precision of the estimation , when compared with the standard imh algorithm . beyond these examples ,",
    "we see multiple situations where the block imh algorithm can be used to improve the estimation in challenging problems .",
    "first , as already stated , the imh algorithm can be used in metropolis - within - gibbs algorithms @xcite .",
    "obviously if a single imh step is performed for each component of the state , then the block imh technique can not be applied without incurring additional costs .",
    "however , it is also correct to update each component multiple times instead of once .",
    "furthermore , an uniform gibbs scan is rarely the optimal way to update the components and more sophisticated schemes have been studied , resulting in random scan gibbs samplers and adaptive gibbs samplers @xcite , where the probability of updating a given component depends on the component and is learned along the iterations of the algorithm .",
    "hence if a component is updated @xmath131 times more often than another , @xmath131 imh can be performed in a row , which allows the use of the block imh technique with @xmath161 .",
    "imh steps are also used within sequential monte carlo ( smc ) samplers ( @xcite , @xcite ) , to diversify the particles after resampling steps . in this context",
    ", an independent proposal can be designed by fitting a ( usually gaussian ) distribution on the particles .",
    "if the move step is repeated multiple times in a row , for instance to ensure a satisfying particle diversification , then the block imh algorithm can be used .",
    "related to smc , another context where the variance reduction provided by block imh might be valuable is the class of particle markov chain monte carlo methods @xcite . for these methods , a particle filter is computed at each iteration of the mh algorithm to estimate the target density , and hence it is paramount to make the most out of the expensive computations involved by those estimates .",
    "this is thus a natural framework for parallelization .    as a final message ,",
    "the block imh method is close to being @xmath162 parallel ( except for the random draw of an index at the end of each block ) .",
    "since parallel computing is getting increasingly easy to use , the free improvement brought by @xmath105 is available for all implementations of the imh algorithm .",
    "furthermore , even without considering parallel computing , since the method uses the most of each target density evaluation , it brings significant improvement when computing the target density is very costly . in such settings ,",
    "the cost of drawing @xmath34 instead of @xmath1 uniform variates is negligible and the block imh algorithm thus runs in about the same time as the standard imh algorithm .",
    "we note that the time required to complete a block in the algorithm is essentially the maximum of the @xmath1 times required to calculate the density ratios @xmath163 .",
    "therefore , if these times widely vary , there could be a diminishing saving in computation time as @xmath1 increases for both the standard imh and the block imh algorithms .",
    "nonetheless , even in such extreme cases , using @xmath105 in the block imh algorithm would bring a significant variance improvement at essentially no additional cost .",
    "the work of the second author ( cpr ) was partly supported by the agence nationale de la recherche ( anr , 212 , rue de bercy 75012 paris ) through the 2009 project anr-08-blan-0218 bigmc and the 2009 project anr-09-blan-01 emile .",
    "pierre jacob is supported by a phd fellowship from the axa research fund . since this research",
    "was initiated during the valencia 9 bayesian statistics conference , the paper is dedicated to jos miguel bernardo for the organization of this series of unique meetings since 1979 .",
    "discussions of the second author with participants during a seminar in stanford university in august 2010 were quite helpful , in particular the suggestion made by art owen to include the  half - inversed half - random \" permutations .",
    "the authors are grateful to julien cornebise for helpful discussions , in particular those leading to the stratified strategy , and to franois perron for his advice on the permutations .",
    "comments and suggestions from the editorial team of jcgs were most helpful in improving the presentation of our results ."
  ],
  "abstract_text": [
    "<S> in this paper , we consider the implications of the fact that parallel raw - power can be exploited by a generic metropolis  </S>",
    "<S> hastings algorithm if the proposed values are independent from the current value of the markov chain . </S>",
    "<S> in particular , we present improvements to the independent metropolis  </S>",
    "<S> hastings algorithm that significantly decrease the variance of any estimator derived from the mcmc output , at a null computing cost since those improvements are based on a fixed number of target density evaluations that can be produced in parallel . </S>",
    "<S> the techniques developed in this paper do not jeopardize the markovian convergence properties of the algorithm , since they are based on the rao  </S>",
    "<S> blackwell principles of @xcite , already exploited in @xcite , @xcite and @xcite . </S>",
    "<S> we illustrate those improvements both on a toy normal example and on a classical probit regression model , but stress the fact that they are applicable in any case where the independent metropolis  hastings is applicable . </S>",
    "<S> * keywords : * mcmc algorithm , independent metropolis  </S>",
    "<S> hastings , parallel computation , rao - blackwellization , permutation . </S>"
  ]
}