{
  "article_text": [
    "demand for large - scale networks consisting of multiple agents ( i.e. , nodes ) with different objectives is steadily growing due to their increased efficiency and scalability compared to centralized distributed structures @xcite .",
    "a wide range of problems in the context of distributed and parallel processing can be considered as a minimization of a sum of objective functions , where each function ( or information on each function ) is available only to a single agent or node @xcite . in such practical applications , it is essential to process the information in a decentralized manner since transferring the objective functions as well as the entire resources ( e.g. , data ) may not be feasible or possible @xcite .",
    "for example , in a distributed data mining scenario , privacy considerations may prohibit sharing of the objective functions @xcite .",
    "similarly , in a distributed wireless network , energy considerations may limit the communication rate between agents @xcite . in such settings ,",
    "parallel or distributed processing algorithms , where each node performs its own processing and share information subsequently , are preferable over the centralized methods @xcite .    here , we consider minimization of a sum of unknown convex objective functions , where each agent ( or node ) observes only its particular objective function via the stochastic gradient oracles .",
    "particularly , we seek to minimize this sum of functions with a limited number of gradient oracle calls at each agent . in this framework , we introduce a distributed online convex optimization algorithm based on the sgd iterates that efficiently minimizes this cost function .",
    "specifically , each agent uses a time - dependent weighted combination of the sgd iterates and achieves the presented performance guarantees , which matches with the lower bounds presented in @xcite , only with a relatively small excess term caused by the unknown network model .",
    "the proposed method is comprehensive , in that any communication strategy , such as the diffusion @xcite and the consensus @xcite strategies , are incorporated into our algorithm in a straightforward manner as shown in the paper .",
    "we compare the performance of our algorithm respect to the state - of - the - art methods @xcite in the literature and present the outstanding performance improvements for various well - known network topologies and benchmark data sets .",
    "distributed networks are successfully used in wireless sensor networks @xcite , and recently used for convex optimization via projected subgradient techniques @xcite . in @xcite ,",
    "the authors illustrate the performance of the least mean squares ( lms ) algorithm over distributed networks using different diffusion strategies .",
    "we emphasize that this problem can also be casted as a distributed convex optimization problem , hence our results can be applied to these problems in a straightforward manner . in @xcite ,",
    "the authors consider the cooperative optimization of the cost function under convex inequality constraints .",
    "however , the problem formulation as well as the convergence results in this paper are significantly different than the ones in @xcite . in @xcite ,",
    "the authors present a deterministic analysis of the sgd iterates and our results builds on them by illustrating a stronger convergence bound in expectation while also providing msd analyses of the sgd iterates . in @xcite ,",
    "the authors consider the distributed convex optimization problem and present the probability-@xmath4 and mean square convergence results of the sgd iterates . in this paper , on the other hand , we provide the expected convergence rate of our algorithm and the msd of the sgd iterates at any time instant .",
    "similar convergence analyses are recently illustrated in the computational learning theory @xcite . in @xcite ,",
    "the authors provide deterministic bounds on the learning performance ( i.e. , regret ) of the sgd algorithm . in @xcite ,",
    "these analyses are extended and a regret - optimal learning algorithm is proposed . in the same lines , in @xcite ,",
    "the authors describe a method to make the sgd algorithm optimal for the strongly convex optimization . however , these approaches rely on the smoothness of the optimization problem . in @xcite , a different method to achieve the optimal convergence rate",
    "is proposed and its performance is analyzed .",
    "on the other hand , in this paper , the convex optimization is performed over a network of localized learners , unlike @xcite .",
    "our results illustrate the convergence rates over any unknown communication graph , and in this sense build upon the analyses of the centralized learners . furthermore , unlike @xcite , our algorithm does not require the optimization problem to be sufficiently smooth .    distibuted convex optimization appears in a wide range of practical applications in wireless sensor networks and real - time control systems @xcite . we introduce a comprehensive approach to this setup by proposing an online algorithm , whose expected performance is asymptotically the same as the performance of the optimal centralized processor .",
    "our results are generic for any probability distribution on the data , not necessarily gaussian unlike the conventional works in the literature @xcite .",
    "furthermore , our performance bounds are optimal in a strong deterministic sense up to constant terms .",
    "our experiments over different network topologies , various data sets and cost functions illustrate the superiority and robustness of our approach with respect to the state - of - the - art methods in the literature .",
    "our main contributions are as follows .    1 .",
    "we introduce a distributed online convex optimization algorithm based on the sgd iterates that achieves an optimal convergence rate of @xmath0 after @xmath1 gradient updates , for each and every node on the network .",
    "we emphasize that this convergence rate is optimal since it achieves the lower bounds presented in @xcite up to constant terms .",
    "we show that the average sgd iterate achieves a msd of @xmath3 after @xmath1 gradient updates .",
    "our analyses can be extended to analyze the performances of the diffusion and consensus strategies in a straightforward manner as illustrated in our paper .",
    "we illustrate the highly significant performance gains of the introduced algorithm with respect to the state - of - the - art methods in the literature under various network topologies and benchmark data sets .",
    "the organization of the paper is as follows . in section [ sec : prob ] , we introduce the distributed convex optimization framework and provide the notations .",
    "we then introduce the main result of the paper , i.e. , a sgd based convex optimization algorithm , in section [ sec : main ] and analyze the convergence rate of the introduced algorithm . in section [ sec : sim ] , we demonstrate the performance of our algorithm with respect to the state - of - the - art methods through simulations and then conclude the paper with several remarks in section [ sec : conc ] .",
    "throughout the paper , all vectors are column vectors and represented by boldface lowercase letters .",
    "matrices are represented by boldface uppercase letters . for a matrix @xmath5",
    ", @xmath6 is the frobenius norm . for a vector @xmath7",
    ", @xmath8 is the @xmath9-norm . here ,",
    "@xmath10 ( and @xmath11 ) denotes a vector with all zero ( and one ) elements and the dimensions can be understood from the context . for a matrix @xmath5",
    ", @xmath12 represents its entry at the @xmath13th row and @xmath14th column . for a convex set @xmath15",
    ", @xmath16 denotes the euclidean projection onto @xmath15 , i.e. , @xmath17 .",
    "we consider the problem of distributed strongly convex optimization over a network of @xmath2 nodes . at each time @xmath18 , each node @xmath13 observes a pair of regressor vectors and data , i.e. , @xmath19 and @xmath20 , where the pairs @xmath21 are independent and identically distributed ( i.i.d . ) for all @xmath22 , which is a common framework in the conventional studies in the literature @xcite . here",
    ", the distributions of the data pairs can differ from one node to another and we do not have any information on the underlying distributions .",
    "in such a framework , the aim of each node is to minimize a strongly convex cost function @xmath23 over a convex set @xmath15 .",
    "however , @xmath23 s are now known and each node accesses to its @xmath23 only via a stochastic gradient oracle , which given some @xmath24 , produces a random vector @xmath25 , whose expectation @xmath26 is a subgradient of @xmath27 at @xmath28 . using these stochastic gradient oracles , each node estimates a parameter of interest @xmath29 on a common convex set @xmath15 and calculates an estimate of the output as @xmath30 i.e. , by a first order linear method . after observing the true data at time @xmath18 ,",
    "node @xmath13 suffers a loss of @xmath31 , where @xmath32 is a strongly convex loss function . as an example",
    ", our cost function can be defined as follows @xmath33 where @xmath34 is a lipschitz - continuous convex loss function with respect to the first variable , where is extensively studied in the literature @xcite as a strongly convex loss function involving regularity terms .",
    "nodes , where each agent communicates with a set of nodes in its neighborhood.,title=\"fig : \" ] +    here , the aim of each node is to minimize its expected loss over the convex set @xmath15 . to continue with our example in , the aim of each node is to minimize @xmath35 where we drop the notational dependency of @xmath36 to the time index @xmath18 since the data pairs are assumed to be independent over time . we emphasize that the formulation in covers a wide range of practical loss functions . as an example , when @xmath37 , we consider the regularized squared error loss and when @xmath38 , we consider the hinge loss . since we make no assumptions on the loss function @xmath39 other than strong convexity , one can also use different loss functions with their corresponding gradients and our results still hold .    in this framework , at each time",
    "@xmath18 , after observing the true data @xmath20 , each node exchanges ( i.e. , diffuses ) information with its neighbors in order to produce the next estimate of the parameter vector @xmath40 .",
    "for example , in fig .",
    "[ fig : network ] , we have a distributed network consisting of @xmath41 nodes , where each node communicates with its neighbors . although the aim of each node is to minimize its expected loss in , the ultimate goal of the distributed network is to minimize the total expected loss , i.e. , @xmath42 for each node @xmath13 , with at most @xmath18 stochastic gradient oracle calls at each agent , where @xmath2 is the number of nodes in the distributed network . here , we emphasize that the expected total loss in is defined with respect to the data statistics at every node , whereas the parameter vector is trained using the gradient oracle calls at @xmath13th node and the information diffusion over the distributed network .",
    "thus , our aim is to minimize a global , strongly convex cost function at each node over a distributed network , where each learner is allowed to use at most @xmath18 calls to the gradient oracle until time @xmath18 .",
    "in this section , we present the main result of the paper , where we introduce an algorithm based on the sgd updates that achieves an expected regret upper bound of @xmath0 after @xmath1 iterates .",
    "the proposed method uses time dependent weighted averages of the sgd updates at each node together with the adapt - then - combine diffusion strategy @xcite to achieve this performance .",
    "however , as later explained in the paper ( i.e. , in section [ sec : ext ] ) , our algorithm can be extended the consensus strategy in a straightforward manner .",
    "the complete description of the algorithm can be found in algorithm [ alg ] .",
    "@xmath43 % estimation @xmath44 % gradient oracle call @xmath45 % sgd update @xmath46 % projection @xmath47 % diffusion @xmath48 % weighting    to achieve the aforementioned result , we first introduce the following lemma , which provides an upper bound on the performance of the average parameter vector . +",
    "* lemma 1 * _ assume that ( i ) each @xmath27 is @xmath49-strongly convex over @xmath15 , where @xmath50 is a closed convex set ; ( ii ) the norms of any two subgradients has a finite covariance , i.e. , @xmath51 \\leq g^2 $ ] , for any @xmath52 . then defining @xmath53 algorithm [ alg ] yields @xmath54 + 4g^2\\mu_t^2.\\end{aligned}\\ ] ] + _    this lemma provides an upper bound on the regret and the squared deviation of the average parameter vector .",
    "it provides an intermediate step to relate the performance of the parameter vector at each agent to the best parameter vector .",
    "the assumptions in lemma 1 are widely used to analyze the convergence of online algorithms such as in @xcite .",
    "particularly , the first assumption in lemma 1 is satisfied for many loss functions , such as the hinge loss , that are widely used in the literature . even though this assumption may not hold for some loss functions such as the square loss",
    ", one can incorporate a small regularization term in order to make it strongly convex @xcite .",
    "the second assumption in lemma 2 is practically a boundedness condition that is widely used to analyze the performance of sgd based algorithms @xcite .",
    "we emphasize that our algorithm does not need to know this upper bound and it is only used in our theoretical derivations .",
    "+ * proof * in order to efficiently manage the recursions , we first consider the projection operation and let @xmath55 and @xmath56 . then , we can compactly represent the averaged estimation parameter @xmath57 in a recursive manner as follows @xcite @xmath58 { \\nonumber}\\\\      & = { { \\mbox{\\boldmath${w}$}}}_t + \\frac{1}{n } \\sum_{i=1}^n \\left ( { { \\mbox{\\boldmath${x}$}}}_{t , i } - \\mu_t { { \\mbox{\\boldmath${g}$}}}_{t , i } \\right),\\end{aligned}\\ ] ] where the last line follows since @xmath5 is right stochastic , i.e. , @xmath59 .",
    "hence , the squared deviation of these average iterates with respect to @xmath60 can be obtained as follows @xmath61    we first upper bound the second term in the right hand side ( rhs ) of as follows @xmath62 we then note that @xmath63 where the second line follows since @xmath64 thus , we can rewrite as follows @xmath65 where the last line follows since @xmath66 \\leq g^2 $ ] for any @xmath67 according to the assumption .",
    "we next turn our attention to @xmath68 $ ] term in and upper bound this term as follows @xmath69 where @xmath70 , follows from the @xmath49-strong convexity , i.e. , @xmath71 also follows from the @xmath49-strong convexity , i.e. , @xmath72 and follows from the cauchy - schwarz inequality .",
    "summing from @xmath73 to @xmath2 and taking expectation of both sides , we obtain @xmath74 { \\nonumber}\\\\      & \\hspace{.6 cm } \\leq { { \\mathbbm{e}}}\\big [ f({{\\mbox{\\boldmath${w}$}}}^ * ) - f({{\\mbox{\\boldmath${w}$}}}_t ) + 2 g \\sum_{i=1}^n { \\left|\\left|{{\\mbox{\\boldmath${w}$}}}_t-{{\\mbox{\\boldmath${w}$}}}_{t , i}\\right|\\right| } { \\nonumber}\\\\      & \\hspace{1.2 cm } - \\frac{\\lambda n}{2 } { \\left|\\left|{{\\mbox{\\boldmath${w}$}}}_t-{{\\mbox{\\boldmath${w}$}}}^*\\right|\\right|}^2 \\big],\\end{aligned}\\ ] ] where the first inequality follows due to the assumption and the last inequality follows from the jensen s inequality due to the convexity of the norm operator .",
    "we finally turn our attention to @xmath75 term in and write this term as follows @xmath76 where the inequality follows from the definition of the euclidean projection .",
    "taking the expectation of both sides , we can upper bound this term as follows @xmath77 { \\nonumber}\\\\      & \\leq g \\mu_t \\ , { { \\mathbbm{e}}}{\\left|\\left|{{\\mbox{\\boldmath${w}$}}}_t-{{\\mbox{\\boldmath${\\psi}$}}}_{t+1,i}\\right|\\right|}.\\end{aligned}\\ ] ]    putting , , and back in , we obtain @xmath78 + 4g^2\\mu_t^2.\\end{aligned}\\ ] ] this concludes the proof of lemma 1 .",
    "@xmath79 + having obtained an upper bound on the performance of the average parameter vector , we then consider the msd of the parameter vectors at each node from the average parameter vector .",
    "this lemma will then be used to relate the performance of each individual node to the performance of the fully connected distributed system . + * lemma 2 * _ in addition to the assumption in lemma 1 , assume that ( i ) the communication graph @xmath5 forms a doubly stochastic matrix such that @xmath5 is irreducible and aperiodic ; ( ii ) the initial weights at each node are identically initialized to avoid any bias , i.e. , @xmath80 , @xmath81 .",
    "then algorithm [ alg ] yields @xmath82 and @xmath83 where @xmath84 is the second largest singular value of matrix @xmath5 . + _",
    "we emphasize that the assumptions in lemma 2 are not restrictive and previous analyses in the literature also use similar assumptions @xcite .",
    "the first assumption in lemma 2 indicates that the convergence of the communication graph is geometric .",
    "this assumption holds when the communication graph is strongly connected , doubly stochastic , and each node gives a nonzero weight to the iterates of its neighbors .",
    "this holds for many communication strategies such as the metropolis rule @xcite .",
    "the second assumption in lemma 2 is basically an unbiasedness condition , which is reasonable since the objective weight @xmath60 is completely unknown to us .",
    "even though the initial weights are not identical , our analyses still hold , however with small additional excess terms .",
    "+ * proof * we first let @xmath85 $ ] , @xmath86 $ ] , and @xmath87 $ ] .",
    "then , we obtain the recursion on @xmath88 as follows @xmath89 letting @xmath90 denote the basis function for the @xmath13th dimension , i.e. , only the @xmath13th entry of @xmath90 is @xmath4 whereas the rest is @xmath91 , we have @xmath92 where the third line follows due to the unbiased initialization assumption , i.e. , @xmath80 , @xmath93 .",
    "we first consider the term @xmath94 of and define the matrix @xmath95 .",
    "then , we can write @xmath96 where the last line follows since @xmath97 , @xmath98 .    here , we let @xmath99 denote the singular values of a matrix @xmath100 and @xmath101 denote the eigenvalues of a symmetric matrix @xmath100 .",
    "then , we can upper bound as follows @xmath102 @xmath98 .",
    "therefore , using the above recursion @xmath103 times to , we obtain @xmath104 here , we note that @xmath5 is assumed to be a doubly stochastic matrix , thus @xmath105 .",
    "therefore , the eigenspectrums of @xmath106 and @xmath107-@xmath108 are equal to the eigenspectrums of @xmath5 and @xmath109 , respectively , except the largest eigenvalues , i.e. , @xmath110 .",
    "thus , we have @xmath111 and combining , , and , we obtain @xmath112 from here on , we denote @xmath113 for notational simplicity .",
    "putting back in , we obtain @xmath114 taking the expectation of both sides and noting that @xmath115 we can rewrite as follows @xmath116    an upper bound for the term @xmath117 can be obtained as @xmath118 where the last line follows from the triangle inequality . taking expectation of both sides , we obtain the following upper bound @xmath119 this concludes the proof of lemma 2 .",
    "@xmath79 + the results in lemma 1 and lemma 2 are combined in the following theorem to obtain a regret bound on the performance of the proposed algorithm .",
    "this theorem illustrates the convergence rate of our algorithm ( i.e. , algorithm [ alg ] ) over distributed networks .",
    "the upper bound on the regret @xmath0 results since the algorithm suffers from a `` diffusion regret '' to sufficiently exchange ( i.e. , diffuse ) the information among the nodes .",
    "this convergence rate matches with the lower bounds presented in @xcite up to constant terms , hence is optimal in a minimax sense .",
    "the computational complexity of the introduced algorithm is on the order of the computational complexity of the sgd iterates up to constant terms .",
    "furthermore , the communication load of the proposed method is the same as the communication load of the sgd algorithm . on the other hand , by using a time - dependent averaging of the sgd iterates , our algorithm achieves a significantly improved performance as shown in theorem 1 and illustrated through our simulations in section [ sec : sim ] . + * theorem 1 * _ under the assumptions in lemma 1 and lemma 2 , algorithm [ alg ] with learning rate @xmath120 and weighted parameters @xmath121 , when applied to any independent and identically distributed regressor vectors and data , i.e. , @xmath122 for all @xmath22 and @xmath123 , achieves the following convergence guarantee @xmath124 \\leq",
    "\\frac{4ng^2}{\\lambda(t+1 ) } \\left ( 3 + \\frac{8\\sigma\\sqrt{n}}{1-\\sigma } \\right),\\ ] ] for all @xmath125 , where @xmath84 is the second largest singular value of matrix @xmath5 . + _",
    "this theorem illustrates that although every node uses local gradient oracle calls to train its parameter vector , each node asymptotically achieves the performance of the centralized processor through the information diffusion over the network .",
    "this result shows that each node acquires the information contained in the gradient oracles at every other node and suffers asymptotically no regret as the number of gradient oracle calls at each node increse . +",
    "* proof * according to lemma 1 and lemma 2 , we have @xmath126 { \\nonumber}\\\\      & \\hspace{.5 cm } \\leq \\frac{n}{2\\mu_t } \\ , { { \\mathbbm{e}}}\\left [ ( 1-\\lambda\\mu_t){\\left|\\left|{{\\mbox{\\boldmath${w}$}}}_t-{{\\mbox{\\boldmath${w}$}}}^*\\right|\\right|}^2 - { \\left|\\left|{{\\mbox{\\boldmath${w}$}}}_{t+1}-{{\\mbox{\\boldmath${w}$}}}^*\\right|\\right|}^2 \\right ] { \\nonumber}\\\\      & \\hspace{1.5 cm } + 3ng^2\\mu_t + 6n\\sqrt{n}g^2 \\sum_{z=1}^{t-1 } \\mu_{t - z } \\sigma^{z}.\\end{aligned}\\ ] ] from the convexity of the cost functions , we also have @xmath127 \\geq { { \\mathbbm{e}}}\\ , { { \\mbox{\\boldmath${g}$}}}_{t , i , j}^t({{\\mbox{\\boldmath${w}$}}}_t-{{\\mbox{\\boldmath${w}$}}}_{t , j}),\\ ] ] @xmath93 , where @xmath128 . here , we can rewrite as follows @xmath129 & \\leq { { \\mathbbm{e}}}\\ , { { \\mbox{\\boldmath${g}$}}}_{t , i , j}^t({{\\mbox{\\boldmath${w}$}}}_{t , j}-{{\\mbox{\\boldmath${w}$}}}_t ) { \\nonumber}\\\\      & \\leq { { \\mathbbm{e}}}\\left [ { \\left|\\left|{{\\mbox{\\boldmath${g}$}}}_{t , i , j}\\right|\\right| } { \\left|\\left|{{\\mbox{\\boldmath${w}$}}}_{t , j}-{{\\mbox{\\boldmath${w}$}}}_t\\right|\\right| } \\right ] { \\nonumber}\\\\      & \\leq g \\ , { { \\mathbbm{e}}}{\\left|\\left|{{\\mbox{\\boldmath${w}$}}}_{t , j}-{{\\mbox{\\boldmath${w}$}}}_t\\right|\\right|},\\end{aligned}\\ ] ] where the second line follows from the triangle inequality and the last line follows from the boundedness assumption . summing from @xmath73 to @xmath2 , we obtain @xmath130 \\leq n g \\ , { { \\mathbbm{e}}}{\\left|\\left|{{\\mbox{\\boldmath${w}$}}}_{t , j}-{{\\mbox{\\boldmath${w}$}}}_t\\right|\\right|}.\\ ] ] using lemma 2 in , we get @xmath131 \\leq 2 n\\sqrt{n } g^2 \\sum_{z=1}^{t-1 } \\mu_{t - z } \\sigma^z.\\ ] ] we then add and to obtain @xmath132 { \\nonumber}\\\\      & \\hspace{.5 cm } \\leq \\frac{n}{2\\mu_t } { { \\mathbbm{e}}}\\left [ ( 1-\\lambda\\mu_t){\\left|\\left|{{\\mbox{\\boldmath${w}$}}}_t-{{\\mbox{\\boldmath${w}$}}}^*\\right|\\right|}^2 - { \\left|\\left|{{\\mbox{\\boldmath${w}$}}}_{t+1}-{{\\mbox{\\boldmath${w}$}}}^*\\right|\\right|}^2 \\right ] { \\nonumber}\\\\      & \\hspace{1.5 cm } + 3ng^2\\mu_t + 8n\\sqrt{n}g^2 \\sum_{z=1}^{t-1 } \\mu_{t - z } \\sigma^{z}.\\end{aligned}\\ ] ]    multiplying both sides of by @xmath18 and summing from @xmath133 to @xmath1 yields @xcite @xmath134 { \\nonumber}\\\\      & \\hspace{.1 cm } \\leq \\frac{n(1-\\lambda\\mu_1)}{2\\mu_1 } { { \\mathbbm{e}}}{\\left|\\left|{{\\mbox{\\boldmath${w}$}}}_1-{{\\mbox{\\boldmath${w}$}}}^*\\right|\\right|}^2 - \\frac{tn}{2\\mu_t } { { \\mathbbm{e}}}{\\left|\\left|{{\\mbox{\\boldmath${w}$}}}_{t+1}-{{\\mbox{\\boldmath${w}$}}}^*\\right|\\right|}^2 { \\nonumber}\\\\      & \\hspace{.5 cm } + \\sum_{t=2}^t \\frac{n}{2 } \\left ( \\frac{t(1-\\lambda\\mu_t)}{\\mu_t } - \\frac{t-1}{\\mu_{t-1 } } \\right ) { { \\mathbbm{e}}}{\\left|\\left|{{\\mbox{\\boldmath${w}$}}}_t-{{\\mbox{\\boldmath${w}$}}}^*\\right|\\right|}^2 { \\nonumber}\\\\      & \\hspace{.5 cm } + 3ng^2\\sum_{t=1}^t t\\mu_t +   8n\\sqrt{n}g^2 \\sum_{t=1}^t t \\sum_{z=1}^{t-1 } \\mu_{t - z } \\sigma^{z}.\\end{aligned}\\ ] ] here , we observe that @xmath135 putting back in and inserting @xmath120 , we obtain @xmath136   { \\nonumber}\\\\      & \\hspace{.5 cm } \\leq - \\frac{\\lambda nt(t+1)}{4 } \\ , { { \\mathbbm{e}}}{\\left|\\left|{{\\mbox{\\boldmath${w}$}}}_{t+1}-{{\\mbox{\\boldmath${w}$}}}^*\\right|\\right|}^2 { \\nonumber}\\\\      & \\hspace{1.5 cm } + \\left ( 3ng^2 + 8n\\sqrt{n}g^2 \\frac{\\sigma}{1-\\sigma } \\right ) \\sum_{t=1}^t \\frac{2t}{\\lambda(t+1 ) } { \\nonumber}\\\\      & \\hspace{.5 cm } \\leq - \\frac{\\lambda nt(t+1)}{4 } \\ , { { \\mathbbm{e}}}{\\left|\\left|{{\\mbox{\\boldmath${w}$}}}_{t+1}-{{\\mbox{\\boldmath${w}$}}}^*\\right|\\right|}^2 { \\nonumber}\\\\      & \\hspace{1.5 cm } + \\frac{2ng^2t}{\\lambda } \\left ( 3 + 8\\sqrt{n}\\frac{\\sigma}{1-\\sigma } \\right),\\end{aligned}\\ ] ] where the last line follows since @xmath137 . dividing both sides of by @xmath138 , we obtain @xmath139 { \\nonumber}\\\\      & \\hspace{1.5 cm } \\leq - \\frac{\\lambda n}{2 } \\ , { { \\mathbbm{e}}}{\\left|\\left|{{\\mbox{\\boldmath${w}$}}}_{t+1}-{{\\mbox{\\boldmath${w}$}}}^*\\right|\\right|}^2 { \\nonumber}\\\\      & \\hspace{2.5 cm } + \\frac{4ng^2}{\\lambda(t+1 ) } \\left ( 3 + 8\\sqrt{n}\\frac{\\sigma}{1-\\sigma } \\right).\\end{aligned}\\ ] ] since @xmath27 s are convex for all @xmath140 , @xmath36 is also convex .",
    "thus , from jensen s inequality , we can write @xmath141 { \\nonumber}\\\\      & \\hspace{2 cm } \\leq { { \\mathbbm{e}}}\\left [ \\frac{2}{t(t+1 ) } \\sum_{t=1}^t t \\left ( f({{\\mbox{\\boldmath${w}$}}}_{t , j } ) - f({{\\mbox{\\boldmath${w}$}}}^ * ) \\right ) \\right].\\end{aligned}\\ ] ] combining and , we obtain @xmath142 { \\nonumber}\\\\      & \\hspace{1.5 cm } \\leq - \\frac{\\lambda n}{2 } \\ , { { \\mathbbm{e}}}{\\left|\\left|{{\\mbox{\\boldmath${w}$}}}_{t+1}-{{\\mbox{\\boldmath${w}$}}}^*\\right|\\right|}^2 { \\nonumber}\\\\      & \\hspace{2.5 cm } + \\frac{4ng^2}{\\lambda(t+1 ) } \\left ( 3 + 8\\sqrt{n}\\frac{\\sigma}{1-\\sigma } \\right).\\end{aligned}\\ ] ] this concludes the proof of theorem 1 .",
    "@xmath79 + hence , using the weighted average @xmath143 instead of the original sgd iterates , we can achieve a convergence rate of @xmath0 .",
    "the denominator @xmath1 of this regret bound follows since we use a time variable weighting of the sgd iterates .",
    "the linear dependency to the network size follows since we add @xmath2 different cost functions , i.e. , one corresponding to each node .",
    "finally , the sublinear dependency to the network size results from the diffusion of the parameter vector over the distributed network .    in the following theorem , we then consider the performance of the average sgd iterate instead of the time - variable weighted iterate in .",
    "in particular , we show that the average sgd iterate achieves a msd of @xmath3 .",
    "this msd follows due to the number of gradient oracle calls and diffusion regret over the distributed network . + * theorem 2 * _ under the assumptions in lemma 1 and lemma 2 , algorithm [ alg ] with learning rate @xmath120 and weighted parameters @xmath121 , when applied to any independent and identically distributed regressor vectors and data , i.e. , @xmath122 for all @xmath22 and @xmath123 , yields the following msd guarantee @xmath144 for all @xmath125 , where @xmath145 and @xmath84 is the second largest singular value of matrix @xmath5 .",
    "+ _    * proof * following similar lines to the proof of theorem 1 , in particular following the steps between - , we get @xmath146 { \\nonumber}\\\\      & \\hspace{1.5 cm } \\leq - \\frac{\\lambda n}{2 } \\ , { { \\mathbbm{e}}}{\\left|\\left|{{\\mbox{\\boldmath${w}$}}}_{t+1}-{{\\mbox{\\boldmath${w}$}}}^*\\right|\\right|}^2 { \\nonumber}\\\\      & \\hspace{2.5 cm } + \\frac{12ng^2}{\\lambda(t+1 ) } \\left ( 1 + 2\\sqrt{n}\\frac{\\sigma}{1-\\sigma } \\right),\\end{aligned}\\ ] ] which yields @xmath147 which is the desired result .",
    "this concludes the proof of theorem 2 .",
    "algorithm [ alg ] can be generalized into the consensus strategy in a straightforward manner , while the performance guarantee in theorem 1 still holds up to constant terms , i.e. , we still have a convergence rate of @xmath0 . for the consensus strategy ,",
    "the lines 57 of algorithm [ alg ] is replaced by the following update @xmath148 hence , we have the following recursion on the parameter vectors @xmath149 instead of the one in . according to this modification , lemma 2",
    "can be updated as follows @xmath150 this loosens the upper bounds in and by a factor of @xmath151 . therefore , diffusion strategies achieves a better convergence performance compared to the consensus strategy .",
    "in this section , we first examine the performance of the proposed algorithms for various distributed network topologies , namely the star , the circle , and a random network topologies ( which are shown in fig .",
    "[ fig ] ) . in all cases",
    ", we have a network of @xmath152 nodes where at each node @xmath123 at time @xmath18 , we observe the data @xmath153 , where the regression vector @xmath19 and the observation noise @xmath154 are generated from i.i.d .",
    "zero mean gaussian processes for all @xmath22 .",
    "the variance of the observation noise is @xmath155 for all @xmath123 , whereas the auto - covariance matrix of the regression vector @xmath156 is randomly chosen for each node @xmath123 such that the signal - to - noise ratio ( snr ) over the network varies between @xmath157db to @xmath158db ( see fig .",
    "[ fig : snr ] ) .",
    "the parameter of interest @xmath159 is randomly chosen from a zero mean gaussian process and normalized to have a unit norm , i.e. , @xmath160 .",
    "we use the well - known metropolis combination rule @xcite to set the combination weights as follows @xmath161 where @xmath162 is the number of neighboring nodes for node @xmath13 .     +    for this set of experiments , we consider the squared error loss , i.e. , @xmath163 as our loss function . in the figures ,",
    "css represents the distributed constant step - size sgd algorithm of @xcite , vss represents the distributed variable step - size sgd algorithm of @xcite , uw represents the distributed version of the uniform weighted sgd algorithm of @xcite , and tvw represents the distributed time variant weighted sgd algorithm proposed in this paper .",
    "the step - sizes of the css-1 , css-2 , and css-3 algorithms are set to @xmath164 , @xmath165 , and @xmath166 , respectively , at each node and the learning rates of the vss and uw algorithms are set to @xmath167 as noted in @xcite , whereas the learning rate of the tvw algorithm is set to @xmath168 as noted in theorem 1 , where @xmath169 .",
    "these learning rates chosen to guarantee a fair performance comparison between these algorithms according to the corresponding algorithm descriptions stated in this paper and in @xcite .    in the left column of fig .",
    "[ fig ] , we compare the normalized time accumulated error performances of these algorithms under different network topologies in terms of the global normalized cumulative error ( nce ) measure , i.e. , @xmath170 additionally , in the right column of fig . [ fig ] , we compare the performance of the algorithms in terms of the global msd measure , i.e. , @xmath171 in the figures , we have plotted the nce and mse performances of the proposed algorithms over @xmath172 independent trials to avoid any bias .    as can be seen in the fig .",
    "[ fig ] , the proposed tvw algorithm significantly outperforms its competitors and achieves a smaller error performance .",
    "this superior performance of our algorithm is obtained thanks to the time - dependent weighting of the regression parameters , which is used to obtain a faster convergence rate with respect to the rest of the algorithms .",
    "hence , by using a certain time varying weighting of the sgd iterates , we obtain a significantly improved convergence performance compared to the state - of - the - art approaches in the literature .",
    "furthermore , the performance of our algorithm is robust against the network topology , whereas the competitor algorithms may not provide satisfactory performances under different network topologies .",
    "we next consider the classification tasks over the benchmark data sets : covertypecjlin / libsvmtools / datasets/ ] and quantum .",
    "for this set of experiments , we consider the hinge loss , i.e. , @xmath173 as our loss function .",
    "the regularization constant is set to @xmath174 , where the stepsizes of the tvw , uw , and vss algorithms are set as in the previous experiment .",
    "the step - sizes of the css-1 , css-2 , and css-3 algorithms are set to @xmath175 , @xmath164 , and @xmath165 for the covertype data set , whereas the step - sizes of the css-1 , css-2 , and css-3 algorithms are set to @xmath176 , @xmath175 , and @xmath164 for the quantum data set .",
    "these learning rates are chosen to illustrate the tradeoff between the convergence speed and the steady state performance of the constant stepsize sgd methods .",
    "the network sizes are set to @xmath177 and @xmath178 for the covertype and quantum data sets , respectively .    in fig .",
    "[ fig : covtype ] and fig .",
    "[ fig : quantum ] , we illustrate the performances of the proposed algorithms for various training data lengths .",
    "in particular , we train the parameter vectors at each node using a certain length of training data and test the performance of the final parameter vector over the entire data set .",
    "we provide averaged results over @xmath179 and @xmath180 independent trials for covertype and quantum data sets , respectively , and present the mean and variance of the normalized accumulated hinge errors .",
    "these figures illustrate that the proposed tvw algorithm significantly outperforms its competitors .",
    "although the performances of the uw and vss algorithms are comparably robust over different iterations , the tvw algorithm provides a smaller accumulated loss . on the other hand ,",
    "the variance of the constant stepsize methods highly deteriorate as the stepsize increases .",
    "although decreasing the stepsize yields more robust performance for these constant stepsize algorithms , the tvw algorithm provides a significantly smaller steady - state cumulative error with respect to these methods .",
    "trials for a network size of @xmath181.,title=\"fig : \" ] +     trials for a network size of @xmath182.,title=\"fig : \" ] +",
    "we study distributed strongly convex optimization over distributed networks , where the aim is to minimize a sum of unknown convex objective functions .",
    "we introduce an algorithm that uses a limited number of gradient oracle calls to these objective functions and achieves the optimal convergence rate of @xmath0 after @xmath1 gradient updates at each node .",
    "this performance is obtained by using a certain time - dependent weighting of the sgd iterates at each node . the computational complexity and the communication load of the proposed approach",
    "is the same with the state - of - the - art methods in the literature up to constant terms .",
    "we also prove that the average sgd iterate achieves a mean square deviation ( msd ) of @xmath3 after @xmath1 gradient oracle calls .",
    "we illustrate the superior convergence rate of our algorithm with respect to the state - of - the - art methods in the literature .",
    "a.  h. sayed , s .- y .",
    "tu , j.  chen , x.  zhao , and z.  j. towfic , `` diffusion strategies for adaptation and learning over networks : an examination of distributed strategies and network behavior , '' _ ieee signal processing magazine _ , vol .",
    "30 , no .  3 , pp .",
    "155171 , may 2013 .",
    "f.  yan , s.  sundaram , s.  v.  n. vishwanathan , and y.  qi , `` distributed autonomous online learning : regrets and intrinsic privacy - preserving properties , '' _ ieee transactions on knowledge and data engineering _ , vol .",
    "25 , no .  11 , pp . 24832493 , nov 2013",
    ".    s.  sundhar  ram , a.  nedic , and v.  v. veeravalli , `` distributed stochastic subgradient projection algorithms for  convex optimization , '' _ journal of optimization theory and applications _ ,",
    "147 , no .  3 , pp .",
    "516545 , dec 2010 .          c.  g. lopes and a.  h. sayed , `` diffusion least - mean squares over adaptive networks : formulation and performance analysis , '' _ ieee transactions on signal processing _",
    "56 , no .  7 , pp .",
    "31223136 , july 2008 .",
    "tu and a.  h. sayed , `` diffusion strategies outperform consensus strategies for distributed estimation over adaptive networks , '' _ ieee transactions on signal processing _",
    "60 , no .  12 , pp .",
    "62176234 , dec 2012 .",
    "a.  agarwal , p.  l. bartlett , p.  ravikumar , and m.  j. wainwright , `` information - theoretic lower bounds on the oracle complexity of stochastic convex optimization , '' _ ieee transactions on information theory _ ,",
    "58 , no .  5 , pp .",
    "32353249 , may 2012 .",
    "a.  rakhlin , o.  shamir , and k.  sridharan , `` making gradient descent optimal for strongly convex stochastic optimization , '' in _ proceedings of the 29th international conference on machine learning ( icml-12 ) _ , 2012 , pp .",
    "449456 .",
    "k.  slavakis , g.  b. giannakis , and g.  mateos , `` modeling and optimization for big data analytics : ( statistical ) learning tools for our era of data deluge , '' _ ieee signal processing magazine _ , vol .",
    "31 , no .  5 , pp . 1831 , sept 2014 .",
    "i.  d. schizas , g.  mateos , and g.  b. giannakis , `` distributed lms for consensus - based in - network adaptive processing , '' _ ieee transactions on signal processing _ , vol .",
    "57 , no .  6 , pp .",
    "23652382 , june 2009 .",
    "g.  mateos , i.  schizas , and g.  giannakis , `` distributed recursive least - squares for consensus - based in - network adaptive estimation , '' _ signal processing , ieee transactions on _ , vol .",
    "57 , no .  11 , pp . 45834588 , nov 2009 .",
    "e.  hazan and s.  kale ,",
    "`` beyond the regret minimization barrier : optimal algorithms for stochastic strongly - convex optimization , '' _ journal of machine learning research _ , vol .  15 , pp . 24892512 , jul 2014",
    ".    s.  lacoste - julien , m.  w. schmidt , and f.  bach , `` a simpler approach to obtaining an @xmath183 convergence rate for the projected stochastic subgradient method , '' http://arxiv.org/pdf/1212.2002v2.pdf , dec 2012 ."
  ],
  "abstract_text": [
    "<S> we study diffusion and consensus based optimization of a sum of unknown convex objective functions over distributed networks . </S>",
    "<S> the only access to these functions is through stochastic gradient oracles , each of which is only available at a different node , and a limited number of gradient oracle calls is allowed at each node . in this framework , we introduce a convex optimization algorithm based on the stochastic gradient descent ( sgd ) updates . particularly , we use a carefully designed time - dependent weighted averaging of the sgd iterates , which yields a convergence rate of @xmath0 after @xmath1 gradient updates for each node on a network of @xmath2 nodes . </S>",
    "<S> we then show that after @xmath1 gradient oracle calls , the average sgd iterate achieves a mean square deviation ( msd ) of @xmath3 . </S>",
    "<S> this rate of convergence is optimal as it matches the performance lower bound up to constant terms . </S>",
    "<S> similar to the sgd algorithm , the computational complexity of the proposed algorithm also scales linearly with the dimensionality of the data . </S>",
    "<S> furthermore , the communication load of the proposed method is the same as the communication load of the sgd algorithm . </S>",
    "<S> thus , the proposed algorithm is highly efficient in terms of complexity and communication load . </S>",
    "<S> we illustrate the merits of the algorithm with respect to the state - of - art methods over benchmark real life data sets and widely studied network topologies .    distributed processing , convex optimization , online learning , diffusion strategies , consensus strategies . </S>"
  ]
}