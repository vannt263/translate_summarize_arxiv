{
  "article_text": [
    "markov chain monte carlo ( mcmc ) algorithms are powerful sampling techniques which allow to implement the estimation of complex statistical distributions @xcite .",
    "the core idea of these methods is to generate a set of samples from the distribution of interest , referred to hereinafter as the target distribution .",
    "first a markov chain whose invariant distribution is precisely the target distribution is devised .",
    "then , monte carlo estimates of this target distribution are derived via averaging over the set of generated samples .",
    "monte carlo statistical simulation methods are widely considered for tackling problems in the machine learning community , @xcite .",
    "naive monte carlo methods are often challenged by the multimodality and the high - dimensionality nature of the target distribution .",
    "the hamiltonian dynamics used in hybrid monte carlo algorithms @xcite provides an efficient alternative framework to implement the sampling of such distributions @xcite .",
    "however , these methods require a fine tuning of several parameters , while still being slow for the sampling of ill - conditioned distributions .",
    "these methods generate indeed reversible markov chains with an acceptance - reject scheme .    in physics ,",
    "recent advances were made in the field of irreversible and rejection - free mcmc simulation methods , namely event - chain monte carlo ( ecmc ) schemes @xcite .",
    "event - chain algorithms generalize the concept of lifting developed by @xcite , while drawing on the lines of the recent rejection - free monte carlo scheme of @xcite . their successes in different applications",
    "@xcite have motivated the research community to pursue a general framework for implementing irreversible markov chains monte carlo algorithms , even beyond physics @xcite .",
    "indeed , the use of the bouncy particle sampler method , equivalent to an event - chain scheme , has shown a promising acceleration in comparison to the hamiltonian mc , as reported in @xcite .",
    "however , when considering common target distributions encountered in the statistics and machine learning communities , the methods are still suffering from some random - walk behavior .    in this paper ,",
    "we introduce a new ecmc scheme , the forward event - chain monte carlo algorithm .",
    "building on the insights brought by @xcite , this method allows for a fast and global exploration of the sampling space , thanks to an efficient implementation of lifting .",
    "in addition of being rejection - free , the forward event - chain monte carlo algorithm does not require any parameters tuning .",
    "this paper is organized as follows .",
    "section [ sec : mcmc ] recalls and describes the standard markov chain monte carlo sampling methodologies , as well as classical event - chain monte carlo sampling schemes .",
    "then , section [ sec : fecmc ] introduces the original forward event - chain monte carlo sampling method proposed in the paper .",
    "section [ sec : numexp ] illustrates this simulation method for high - dimensional ill - conditioned gaussian distributions , where it exhibits speedups of several magnitudes in comparison to standard event - chain implementations and to hastings - metropolis algorithms ( local monte carlo ) .",
    "a conclusion is drawn in section [ sec : conclu ] .",
    "let us denote by @xmath0 the target distribution from which one would like to sample from .",
    "mcmc sampling techniques are implemented through the recursive application of a transition kernel or operator , denoted as @xmath1 , such that @xmath2 is an invariant distribution , among other conditions @xcite . in order to design mcmc algorithms",
    ", @xmath3 needs to satisfy a fixed - point relation for @xmath2 , namely @xmath4 and a conservation condition , @xmath5 jointly , one obtains the necessary global balance condition for the construction of @xmath3 ,    @xmath6    several transition operators @xmath3 satisfy relation ( [ eq : global ] ) for a given @xmath2 .",
    "the most common approach is to consider the sufficient stronger condition on @xmath3 and @xmath2 referred to as the detailed balance condition    @xmath7    for all couples of variables @xmath8 .",
    "standard mcmc sampling schemes like the hastings - metropolis @xcite and the gibbs sampling @xcite algorithms rely on the detailed balance condition ( [ eq : detbal ] ) , cf .",
    "the standard hastings - metropolis algorithm admits the following transition kernel @xmath3 for a given candidate distribution @xmath9 , cf .",
    "@xcite @xmath10 one can easily check that the transition kernel ( [ eq : thm ] ) satisfies the detailed balance condition ( [ eq : detbal ] ) .",
    "however , the detailed balance condition enforces a local constraint that induces rejections and a random - walk behavior which impede the efficiency of the exploration of the sampling space .",
    "indeed , rejection can often be considered as a bottleneck in mcmc simulation methodology , particularly for problems where the acceptance rates of the mcmc simulation schemes are very low .    to address this issue ,",
    "an approach consists in enforcing the irreversibility of the simulated markov chain by considering the following constraint on the transition kernel @xmath3 @xmath11 for all couples of variables @xmath8 .",
    "transition kernels @xmath3 satisfying the global balance condition ( [ eq : global ] ) and relation ( [ eq : maxglobal ] ) are said to check a maximal global balance condition",
    ". such nonreversible or irreversible markov chain samplers have been investigated in probability and statistics in @xcite and in physics in @xcite for instance .",
    "we focus in the following on the irreversible mcmc sampling algorithm introduced in @xcite , which satisfies exactly a maximal global balance condition from relations ( [ eq : global ] ) and ( [ eq : maxglobal ] ) , implemented originally for physical multi - particles interacting systems , and referred to hereinafter as event - chain monte carlo ( ecmc ) .",
    "ecmc algorithms @xcite generate maximal global - balanced markov chains by relying on a lifting framework @xcite .",
    "lifting is based on the extension of the state space by an additional variable @xmath12 , while keeping the same marginal target distribution @xmath13 .",
    "the transition kernel remains the same as in ( [ eq : thm ] ) , but the additional variable @xmath12 fixes the candidate distribution @xmath9 to , @xmath14 here @xmath15 stands for the infinitesimal nature of the candidate move . in practice ,",
    "an event - driven implementation is used @xcite .",
    "the state @xmath16 is updated in the sampling space along the same direction @xmath12 until eventually a rejection of the next infinitesimal moves occurs at the state @xmath17 . a lifting move @xmath18 then arises with a probability @xmath19 and the state is updated to @xmath20 .",
    "we explain later on how one can choose the lifting filter @xmath19 in order to satisfy the global balance condition ( [ eq : global ] ) , it is also the key to the new method we propose in this paper . thus rejections are changed into lifting moves , making the scheme rejection - free .",
    "finally , instead of sampling the rejection or acceptance of each infinitesimal move along @xmath12 , one samples directly the event time @xmath21 at which a lifting move occurs .",
    "we detail now this event time computation .",
    "we consider that the target distribution @xmath2 is of the following form @xmath22 with @xmath23 ( resp .",
    "@xmath24 ) the equivalent of an energy function ( resp .",
    "position ) in physics .",
    "we note @xmath25 the dimension of the variable @xmath24 . by denoting @xmath26^+ = \\max\\{0,a\\}$ ] , ( [ eq : thm ] ) yields for an acceptance probability @xmath27^+ )",
    "\\label{eq : thmplus}\\end{aligned}\\ ] ] sampling the time event @xmath21 comes down to sampling a cumulative positive increase in the energy @xmath28 , as the probability to accept all the infinitesimal moves but the final one over a distance @xmath21 is , @xmath29 with , @xmath30^+ ds .",
    "\\label{eq : intchemin}\\ ] ] by inversion sampling , the event time @xmath21 is sampled with a single random number @xmath31 in the event - driven approach , @xmath32 with @xmath33 $ ] @xcite . solving ( [ eq : intchemin ] ) may not seem trivial but comes down to the computation of the zeroes of @xmath34 .",
    "we develop the calculations of the event times for a gaussian distribution in appendix [ sec : timeeventcalc ] .",
    "the event - driven implementation thus yields a continuum of valid samples between two event times , so that any uniform subset of these samples can be used for computing the monte carlo estimates .",
    "it is common to use a fixed displacement @xmath35 between two successive final samples .",
    "the fixed displacement @xmath35 is usually referred to as the chain length , as there is a chain of successive events between two final samples . instead of using a displacement @xmath35",
    ", one can also devise a poisson process with a fixed rate @xcite .    the global balance condition",
    "( [ eq : global ] ) is enforced by the complementarity between the physical moves on the sampling space ( @xmath36 ) and the lifting moves ( @xmath37 ) . in ecmc methods ,",
    "the satisfaction of the conservation condition is automatically checked . regarding the fixed - point relation ,",
    "it comes down to , @xmath38 with , @xmath39^+\\right ) \\\\",
    "\\phi_{\\text{lift } } & = \\int \\rho({\\bm{e ' } } \\to { \\bm{e}})\\left(1 - \\exp\\left(-\\left[{\\bm{\\nabla}}e\\cdot { \\bm{e'}}\\right]^+\\right)\\right)d{\\bm{e ' } }    \\end{aligned }    \\label{eq : phi}\\ ] ]    thus , obeying the global balance condition ( @xmath40 ) is achieved from the moment the choice of @xmath41 satisfies the lifted global balance condition , @xmath42^+\\right)d{\\bm{e ' } } \\nonumber\\\\    = \\int \\rho({\\bm{e ' } } \\to { \\bm{e}})\\exp\\left(-\\left[{\\bm{\\nabla}}e\\cdot { \\bm{e'}}\\right]^+\\right)d{\\bm{e ' } } ,   \\label{eq : globaleth}\\end{aligned}\\ ] ] which becomes in the infinitesimal limit , @xmath43^+d{\\bm{e ' } }    \\nonumber\\\\    = \\int \\rho({\\bm{e ' } } \\to { \\bm{e}})\\left[{\\bm{\\nabla}}e\\cdot { \\bm{e'}}\\right]^+d{\\bm{e ' } } ,   \\label{eq : globale}\\end{aligned}\\ ] ] the first implementations of ecmc @xcite chose to fix , @xmath44 for the straight event - chain , and , @xmath45 with @xmath46 being the inflexive operator , @xmath47 for the bouncy event - chain .",
    "both choice of lifting filters are valid as the lifted global balance condition ( [ eq : globale ] ) becomes for the straight ec , @xmath48^+    = \\left[{\\bm{\\nabla}}e\\cdot ( -{\\bm{e}})\\right]^+,\\ ] ] and for the bouncy ec , @xmath49^+ &   = \\left[{\\bm{\\nabla}}e\\cdot r({\\bm{e}})\\right]^+\\\\    &   = \\left[-{\\bm{\\nabla}}e\\cdot { \\bm{e}}\\right]^+ .",
    "\\end{aligned}\\ ] ] for most target distributions , both the straight and bouncy ec need however an episodic resampling of the direction @xmath12 , a refreshement , in order to be ergodic ( for more details and a rigorous proof see @xcite ) . as for the outputting of the final subset samples",
    ", the direction resampling must be uniform on the continuum of valid states and can be made at fixed displacement , for instance after the chain length @xmath35 , or at the arrival time of a given poisson process .",
    "finally , the event - chain monte carlo implementation is recalled in algorithm [ alg : ecmc ] .",
    "input : @xmath23 , @xmath50 , @xmath35 , @xmath51 initialize @xmath52 , @xmath53 set current chain length to @xmath54 set direction @xmath12 randomly in the unit sphere of @xmath55 set initial sample as @xmath56 compute @xmath57 with @xmath58 $ ] compute @xmath21 such that equation ( [ eq : intchemin ] ) holds compute @xmath59 set new sample @xmath60 break the while loop compute @xmath61 update chain length to @xmath62 update direction @xmath63",
    "the classical versions of the event - chain monte carlo simulation method relying on the lifting filter ( [ eq : straightec ] ) or ( [ eq : bouncyec ] ) can be further optimized .",
    "indeed , the choice of the direction @xmath12 for the update step ( see algorithm [ alg : ecmc ] ) can be crucial regarding the speed of the sampling scheme to explore the target distribution .",
    "it has been shown that the straight ec is more efficient than the bouncy ec for multiparticle systems @xcite , as it exploits symmetry and factorization of the interparticle energy function @xmath64 . however , when such symmetry is not available",
    ", @xcite argues that the bouncy ec explores more easily the target distribution , as it avoids going backwards in the sampling space right after the sampling of a lifting event .",
    "ideally , we would like to find the right optimal set of directions @xmath65 necessary for the ergodicity and allowing for an efficient exploration of the target distribution , as was done for multiparticle systems @xcite .",
    "at the refreshment point , a new direction is drawn randomly from this set .",
    "if a direction @xmath12 is chosen randomly uniformly in the unit sphere of @xmath66 at each refreshment , it increases indeed the random - walk behavior of the exploration process of the target distribution .",
    "unfortunately , finding this optimal set is not trivial .",
    "moreover , both the classical versions of the ecmc satisfy the lifted global balance condition ( [ eq : globale ] ) by enforcing the local constraint @xmath67 or @xmath68 , which is similar to how the sufficient detailed balance condition is used to ensure the necessary global balance one .    in order to avoid the refreshment step ,",
    "we propose a new version of the event - chain method : the forward event - chain monte carlo .",
    "it relies on the following lifting filter ,    @xmath69^+}{||{\\bm{\\nabla}}e||}. \\label{eq : fec}\\ ] ]    this lifting filter is similar to the one recently proposed in physics for handling multibody interactions @xcite . thus sampling a new direction @xmath12 after a lifting event comes down to sampling a vector in the unit sphere @xmath55 and accepting it with the probability ( [ eq : fec ] ) .",
    "we propose the following original direct sampling of the new direction @xmath12 by drawing a random number @xmath70 $ ] , as ,    @xmath71    with , @xmath72 such sampling yields a random direction @xmath12 satisfying the lifted global balance condition ( [ eq : globale ] ) .",
    "even if the initial lifting filter ( [ eq : fec ] ) does not depend on the incoming direction @xmath73 , we enforce a dependence in the direct sampling of the new direction @xmath12 as to only update the direction @xmath74 , as exhibited on figure [ fig : fdirection ] .",
    "the goal is indeed to avoid introducing any unnecessary resampling of the other component @xmath75 , that may lead to a random - walk behavior and a slow exploration .",
    ", a new direction @xmath12 is sampled following ( [ eq : directe ] ) . the new direction @xmath12 preserves the components of the initial direction @xmath73 that are orthogonal to the local gradient @xmath76 . ]    eventually , while implementing this new scheme , there is no need to refresh a direction @xmath12 , as it is resampled at each lifting event .",
    "moreover , it realizes a strict irreversible mc scheme even for the energy function @xmath64 , as the energy decrease is of a different magnitude than the proposed energy increase that led to the sampling of a lifting event .",
    "finally , the forward event - chain monte carlo implementation is recalled in algorithm [ alg : ecmc2 ] .",
    "the next section focuses on comparing the performances of the new forward event - chain scheme and the classical straight ec and bouncy ec methods .",
    "input : @xmath23 , @xmath50 , @xmath35 , @xmath51 initialize @xmath52 , @xmath53 initialize direction @xmath12 randomly in the unit sphere of @xmath55 initialize current chain length to @xmath54 initialize @xmath77 set initial sample as @xmath56 compute @xmath57 with @xmath58 $ ] compute @xmath21 such that equation ( [ eq : intchemin ] ) holds compute @xmath59 set new sample @xmath78 @xmath79 update chain length to @xmath80 compute @xmath81 update chain length to @xmath62 set direction @xmath12 as in ( [ eq : directe ] )",
    "0.2 in         -0.2 in    we consider the problem of sampling from a @xmath25-dimensional ill - conditioned gaussian distribution in which the eigenvalues of the covariance matrix are log - linearly distributed between @xmath82 and @xmath83 , such as in @xcite , but with a non - diagonal covariance matrix in this case .",
    "we compare the standard hastings - metropolis sampling algorithm ( local mc ) , the bouncy event - chain ( bouncy ec , similar to the bouncy particle sampler @xcite ) , the forward event - chain ( forward ec ) and the forward refresh event - chain ( forward refresh ec ) .",
    "this latter implements on top of the direction resampling at each lifting event a step of direction refreshment as done in the bouncy ec .",
    "we also consider a straight event - chain with an optimized set of directions ( optimized ec ) as an ideal reference .",
    "regarding the implementation of those methods , there is no parameter to tune for the forward ec and the optimized ec .",
    "the candidate distribution for the local mc is tuned so that to obtain an acceptable rejection rate . as for the bouncy ec and the forward refresh ec ,",
    "the chain length @xmath35 was tuned in order to achieve the fastest decorrelation for the energy .",
    "figure [ fig : samples ] depicts the first @xmath84 samples generated from the local mc , the bouncy ec and the forward ec for a @xmath85-dimensional ill - conditioned gaussian distribution .",
    "it appears clearly that the forward ec allows for an efficient exploration of the state space and of the target distribution .",
    "indeed , the samples generated from the local mc and from the bouncy ec lie only in a fraction of the support of the target distribution and are very localized . on the other hand ,",
    "the samples generated from the forward ec are distributed more uniformly on the support of the target distribution and reach easily low - probability zones of the target distribution .    in order to quantify the possible speed up achieved by the forward ec , we monitor the correlation of the samples generated from the simulation schemes .",
    "figure [ fig : autocorr ] exhibits the autocorrelation function @xmath86 of the energy function @xmath64 and the autocorrelation function @xmath87 of the samples @xmath24 generated from the different simulation methods .",
    "the goal of mcmc simulation schemes is to obtain samples as uncorrelated as possible , ideally independently and identically distributed from the target distribution .",
    "if the autocorrelation between two samples becomes negative , the final error is reduced in comparison to a direct sampling .",
    "first for the energy function @xmath64 , the autocorrelation curves on figure [ fig : autocorr ] demonstrate that the forward ec achieves a fast decorrelation , equivalent to the one obtained by the optimized ec .",
    "but , in addition , it exhibits negative correlations , thanks to its irreversible nature , that leads to the suppression of the random - walk behavior . on the other hand ,",
    "the forward refresh ec shows also an initial fast decorrelation before a slower mode kicks in , which compares asymptotically to the bouncy ec .",
    "the initial behavior of the forward refresh ec may be explained by the fact that the energy function @xmath64 follows also an irreversible scheme , thanks to the resampling ( [ eq : directe ] ) .",
    "it is however not sufficient to allow for a lasting effect on the decorrelation .",
    "finally , the local mc exhibits a burn - in phase , followed by a slow decorrelation regime .",
    "now , regarding the sample autocorrelation , the local mc does not exhibit even a burn - in phase and remains quite slow .",
    "the forward refresh ec and the bouncy ec shows the same decorrelation rate .",
    "again the forward ec and optimized ec realizes a fast decorrelation .",
    "the former exhibits also negative autocorrelations , which will prove to be an asset for the computation of the final error , as shown on figure [ fig : tauinten ] .",
    "0.15 in    [ cols=\"<,^,^,^,^,^,>\",options=\"header \" , ]     -0.1 in    [ speedx ]    the final error can be assessed and measured through a scalar and numerical performance indicator : the integrated autocorrelation time .",
    "figure  [ fig : tauinten ] depicts the different values obtained by a binning analysis of this indicator for the energy function @xmath64 and the samples @xmath24 with respect to the dimension @xmath25 of the target distribution , and for the different simulation methods .",
    "first , it appears that the optimized ec performs as well for the energy than for the samples .",
    "the situation is quite different for the bouncy ec and the forward refresh ec , as the integrated autocorrelation time for the samples is ten times slower than for the energy .",
    "however , the forward refresh ec exhibits a small acceleration for the energy , in comparison to the bouncy ec .",
    "finally , the forward ec obtains the smallest integrated autocorrelation times for both the energy and the samples .",
    "moreover , regarding the samples , the autocorrelation time decreases with the dimension of the target distribution , due to the negative correlations as shown on figure [ fig : autocorr ] .",
    "quantitatively , the forward ec achieves clear acceleration in comparison to the other methods ( up to a few times to @xmath88 compared with the bouncy ec ) , as summarized in tables [ speede ] and [ speedx ] .",
    "moreover , as the scaling with the dimension of the target distribution is smaller than for the other methods and even negative for the samples , this acceleration increases with the dimension of the target distribution .",
    "as the bouncy ec was shown to outperform optimized versions of the hamiltionian mc method @xcite on similar distributions , the forward ec is also expected to lead to even better improvements .",
    "finally , event - chain based methods are particulary efficient for factorizable distributions as , @xmath89^+\\right),\\ ] ] leading to a factorized filter @xcite , @xmath90^+\\right).\\ ] ] the factorized filter allows to sample an independent lifting event for each factor @xmath91 .",
    "there was no need to use the factorization of the gaussian distribution here , but the forward event - chain scheme remains valid : one resamples the factor direction @xmath92 following ( [ eq : directe ] ) , when a lifting event is sampled for the factor @xmath91 .",
    "in this paper , a novel and original irreversible markov chain monte carlo simulation scheme is proposed : the forward event - chain monte carlo algorithm .",
    "this simulation scheme is rejection - free and satisfies the maximal global balance condition , thanks to an efficient implementation of the lifting framework . also , the implementation of this sampling scheme is straightforward as there is no parameters to tune like for the leapfrog integrator for hamiltonian monte carlo simulation methods or for the bouncy event - chain .",
    "numerical experiments demonstrate that the forward event - chain monte carlo algorithm allows an efficient exploration of the state space and of the support of the target distribution .",
    "moreover it brings a considerable acceleration in comparison to state - of - the - art monte carlo simulation methods ( event - chain based and standard mcmc ) .",
    "this acceleration increases with the dimensionality of the target distribution .",
    "let us assume that @xmath24 is drawn from a gaussian distribution @xmath93 . in this case , the energy function of relation ( [ eq : tardist ] )",
    "is defined as @xmath94 recalling equation ( [ eq : intchemin ] ) , one should find @xmath21 such that @xmath95^+ ds= \\delta e^ * \\label{eq : intchemin2}\\ ] ] for a given @xmath28 .",
    "relation ( [ eq : ener ] ) yields @xmath96 .",
    "thus the integrand in equation ( [ eq : intchemin2 ] ) can be written @xmath97^+ \\label{eq : integrand}\\ ] ] in this respect , solving equation ( [ eq : intchemin2 ] ) in @xmath21 depends on the sign of @xmath98 .",
    "+ if @xmath99 , integrating expression ( [ eq : integrand ] ) between 0 and @xmath21 yields @xmath100 @xmath21 is the positive root of a second - order polynomial function ( [ eq : intpos ] ) and is calculated directly by the following expression : @xmath101    if @xmath102 , integrating expression ( [ eq : integrand ] ) between 0 and @xmath21 comes down to integrating ( [ eq : integrand ] ) between @xmath103 and @xmath21 and it yields @xmath104 @xmath21 is the positive root of a second - order polynomial function ( [ eq : intneg ] ) and is calculated directly by the following expression : @xmath105    in both cases , the calculation of the event time @xmath21 for a given @xmath28 is straightforward for a gaussian distribution as it boils down to the calculation of the roots of second - order polynomials ."
  ],
  "abstract_text": [
    "<S> this paper considers event - chain monte carlo simulation schemes in order to design an original irreversible markov chain monte carlo ( mcmc ) algorithm for the sampling of complex statistical models . </S>",
    "<S> the functioning principles of mcmc sampling methods are firstly recalled , as well as standard event - chain monte carlo simulation schemes are described . then , a forward event - chain monte carlo sampling methodology is proposed and introduced . this nonreversible mcmc rejection - free simulation algorithm is tested and run for the sampling of high - dimensional ill - conditioned gaussian statistical distributions </S>",
    "<S> numerical experiments demonstrate the efficiency of the proposed approach , compared to standard event - chain and standard monte carlo sampling methods . </S>",
    "<S> accelerations up to several magnitudes are exhibited .    0.3 in </S>"
  ]
}