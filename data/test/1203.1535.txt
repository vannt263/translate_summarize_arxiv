{
  "article_text": [
    "adaptive filtering has attracted much research interest in both theoretical and applied issues for a long time@xcite . due to its good performance , easy implementation , and high robustness , least mean square ( lms ) algorithm @xcite has been widely used in various applications such as system identification , channel equalization , and echo cancelation .",
    "the unknown systems to be identified are sparse in most physical scenarios , including the echo paths @xcite and digital tv transmission channels @xcite . in other words , there are only a small number of non - zero entries in the long impulse response . for such systems ,",
    "the traditional lms has no particular gain since it never takes advantage of the prior sparsity knowledge . in recent years",
    ", several new algorithms have been proposed based on lms to utilize the feature of sparsity .",
    "m - max normalized lms ( mmax - nlms)@xcite and sequential partial update lms ( s - lms)@xcite decrease the computational cost and steady - state mean squared error ( mse ) by means of updating filter tap - weights selectively .",
    "proportionate nlms ( pnlms ) and its improved version@xcite accelerate the convergence by setting the individual step size in proportional to the respective filter weights .",
    "sparsity in adaptive filtering framework has been a long discussed topic @xcite .",
    "inspired by the recently appeared sparse signal processing branch @xcite , especially compressive sampling ( or compressive sensing , cs)@xcite , a family of sparse system identification algorithms has been proposed based on @xmath1 norm constraint .",
    "the basic idea of such algorithms is to exploit the characteristics of unknown impulse response and to exert sparsity constraint on the cost function of gradient descent .",
    "specially , za - lms @xcite utilizes @xmath2 norm and draws the zero - point attraction to all tap - weights .",
    "@xmath0-lms @xcite employs a non - convex approximation of @xmath0 norm and exerts respective attractions to zero and non - zero coefficients . the smoothed @xmath0 algorithm , which is also based on an approximation of @xmath0 norm , is proposed in @xcite and analyzed in @xcite .",
    "besides lms variants , rls - based sparse algorithms @xcite and bayesian - based sparse algorithms @xcite have also been proposed .",
    "it is necessary to conduct a theoretical analysis for @xmath0-lms algorithm .",
    "numerical simulations demonstrate that the mentioned algorithm has rather good performance compared with several available sparse system identification algorithms @xcite , including both accelerating the convergence and decreasing the steady - state msd .",
    "@xmath0-lms performs _ zero - point attraction _ to small adaptive taps and pulls them toward the origin , which consequently increases their convergence speed and decreases their steady - state bias . because most coefficients of a sparse system are zero , the overall identification performance is enhanced .",
    "it is also found that the performance of @xmath0-lms is highly affected by the predefined parameters .",
    "improper parameter setting could not only make the algorithm less efficient , but also yield steady - state misalignment even larger than the traditional algorithm .",
    "the importance of such analysis should be further emphasized since adaptive filter framework and @xmath0-lms behave well in the solution of sparse signal recovery problem in compressive sensing@xcite .",
    "compared with some convex relaxation methods and greedy pursuits @xcite , it was experimentally demonstrated that @xmath0-lms in adaptive filtering framework shows more robustness against noise , requires fewer measurements for perfect reconstruction , and recovers signal with less sparsity .",
    "considering its importance as mentioned above , the steady - state performance and instantaneous behavior of @xmath0-lms are throughout analyzed in this work .",
    "one contribution of this work is on steady - state performance analysis . because of the nonlinearity caused by the sparsity constraint in @xmath0-lms , the theoretical analysis is rather difficult . to tackle this problem and enable mathematical tractability ,",
    "adaptive tap - weights are sorted into different categories and several assumptions besides the popular independence assumption are employed .",
    "then , the stability condition on step size and steady - state misalignment are derived .",
    "after that , the parameter selection rule for optimal steady - state performance is proposed .",
    "finally , the steady - state msd gain is obtained theoretically of @xmath0-lms over the tradition algorithm , with the optimal parameter .",
    "another contribution of this work is on instantaneous behavior analysis , which indicates the convergence rate of lms type algorithms and also arouses much attention @xcite . for lms and most of its linear variants ,",
    "the convergence process can be obtained in the same derivation procedure as steady - state misalignment . however , this no longer holds for @xmath0-lms due to its nonlinearity . in a different way by utilizing the obtained steady - state msd as foundation , a taylor expansion is employed to get an approximated quantitative analysis of the convergence process . also , the convergence rates are compared between @xmath0-lms and standard lms .      in order to theoretically characterize the performance and",
    "guide the selection of the optimal algorithm parameters , the mean square analysis has been conducted for standard lms and a lot of its variants . to the best of our knowledge ,",
    "widrow for the first time proposed the lms algorithm in @xcite and studied its performance in @xcite .",
    "later , horowitz and senne @xcite established the mathematical framework for mean square analysis via studying the weight vector covariance matrix and achieved the closed - form expression of mse , which was further simplified by feuer and weinstein @xcite .",
    "the mean square performance of two variants , leaky lms and deficient length lms , were theoretically investigated in similar methodologies in @xcite and @xcite , respectively . recently , dabeer and masry @xcite put forward a new approach for performance analysis on lms without assuming a linear regression model . moreover , convergence behavior of transform - domain lms was studied in @xcite with second - order autoregressive process .",
    "a summarized analysis was proposed in @xcite on a class of adaptive algorithms , which performs linear time - invariant operations on the instantaneous gradient vector and includes lms as the simplest case .",
    "similarly , the analysis of normalized lms has also attracted much attention @xcite .",
    "however , the methodologies mentioned above , which are effective in their respective context , could no longer be directly applied to the analysis of @xmath0-lms , considering its high non - linearity .",
    "admittedly , nonlinearity is a long topic in adaptive filtering and not unique for @xmath0-lms itself .",
    "researchers have delved into the analysis of many other lms - based nonlinear variants @xcite .",
    "nevertheless , the nonlinearity of most above references comes from non - linear operations on _ the estimated error _ , rather than _ the adaptive tap - weights _ that @xmath0-lms mainly focuses on .",
    "we have noticed that the mean square deviation analysis of za - lms has been conducted @xcite . however , this work is far different from the reference .",
    "first of all , the literature did not consider the transient performance analysis while in this work the mean square behavior of both steady - state and convergence process are conducted .",
    "moreover , considering @xmath0-lms is more sophisticated than za - lms , there are more parameters in @xmath0-lms than in za - lms , which enhances the algorithm performance but increases the difficulty of theoretical analysis .",
    "last but not least , taking its parameters to a specific limit setting , @xmath0-lms becomes essentially the same as za - lms , which can apply the theoretical results of this work directly .",
    "a preliminary version of this work has been presented in conference@xcite , including the convergence condition , derivation of steady - state msd , and an expression of the optimal parameter selection .",
    "this work provides not only a detailed derivation for steady - state results , but also the mean square convergence analysis .",
    "moreover , both the steady - state msd and the parameter selection rule are further simplified and available for analysis .",
    "finally , more simulations are performed to validate the results and more discussions are conducted .",
    "this paper is organized as follows . in section [ section :",
    "review ] , a brief review of @xmath0-lms and za - lms is presented . then in section [ section : preliminaries ] , a few assumptions are introduced which are reasonable in a wide range of situations . based on these assumptions , section [ section : analysis ] proposes the mean square analysis .",
    "numerical experiments are performed to demonstrate the theoretical derivation in section [ section : simulation ] and the conclusion is drawn in section [ section : conclusion ] .",
    "the unknown coefficients and input signal at time instant @xmath3 are denoted by @xmath4^{\\rm t}$ ] and @xmath5^{\\rm t}$ ] , respectively , where @xmath6 is the filter length .",
    "the observed output signal is @xmath7 where @xmath8 denotes the additive noise .",
    "the estimated error between the output of unknown system and of the adaptive filter is @xmath9 where @xmath10^{\\rm t}$ ] denotes the adaptive filter tap - weights .    in order to take the sparsity of the unknown coefficients into account , @xmath0-lms @xcite inserts an @xmath0 norm penalty into the cost function of standard lms .",
    "the new cost function is @xmath11 where @xmath12 is a factor to balance the estimation error and the new penalty . due to the np hardness of @xmath0 norm optimization , a continuous function",
    "is usually employed to approximate @xmath0 norm .",
    "taking the popular approximation@xcite and making use of the first order taylor expansion , the recursion of @xmath0-lms is @xmath13 where @xmath14^{\\rm t}$ ] and @xmath15 the last item in ( [ w_update ] ) is called _ zero - point attraction _ @xcite , @xcite , because it reduces the distance between @xmath16 and the origin when @xmath17 is small . according to ( [ l_0gradapprx ] ) and fig . [ fig : g_l0l1](a ) , obviously such attraction is non - linear and exerts varied affects on respective tap - weights .",
    "this attraction is effective for the tap - weights in the interval @xmath18 , which is named _",
    "attraction range_. in this region , the smaller @xmath17 is , the stronger attraction affects .",
    "za - lms ( or sparse lms ) @xcite runs similar as @xmath0-lms .",
    "the only difference is that the sparse penalty is changed to @xmath2 norm .",
    "accordingly the zero - point attraction item of the former is defined as @xmath19 which is shown in fig .",
    "[ fig : g_l0l1](b ) .",
    "the recursion of za - lms is @xmath20 where @xmath21 is the parameter to control the strength of sparsity penalty . comparing the sub figures in fig .",
    "[ fig : g_l0l1 ] , one can readily accept that @xmath22 exerts the various attraction to respective tap - weight , therefore it usually behaves better than @xmath23 . in the following analysis",
    ", one will read that za - lms is a special case of @xmath0-lms and the result of this work can be easily extended to the case of za - lms .",
    "as its improvement , reweighted za - lms ( rza - lms ) is also proposed in @xcite , which modifies the zero - point attraction term to @xmath24 where parameter @xmath25 controls the similarity between ( [ rzagradapprx ] ) and @xmath0 norm . please refer to fig .",
    "[ fig : g_l0l1](c ) for better understanding the behavior of ( [ rzagradapprx ] ) . in section",
    "v , both za - lms and rza - lms are simulated for the purpose of performance comparison .",
    "-lms , ( b ) za - lms , ( c ) rza - lms.,width=307 ]      denote @xmath26 and @xmath27 as the steady - state msd and instantaneous msd after @xmath3 iterations for lms with zero - mean independent gaussian input , respectively .",
    "the steady - state msd has the explicit expression @xcite of @xmath28 where @xmath29 and @xmath30 denote the power of input signal and additive noise , respectively , and @xmath31 is a constant defined by ( [ definedeltal ] ) in appendix [ const ] . for the convergence process , the explicit expression of instantaneous msd is implied in @xcite as @xmath32    the next one turns to za - lms , @xmath33 is used to denote the steady - state msd with white gaussian input .",
    "reference @xcite reaches the conclusion that @xmath34 where @xmath35 is the solution to @xmath36 where @xmath37 denotes the number of non - zero unknown coefficients and @xmath38 is a constant defined by ( [ definedelta0 ] ) .",
    "@xmath0-lms employs steepest descent recursively and is applicable to solving sparse system identification .",
    "more generally , steepest ascent iterations are used in several algorithms in the field of sparse signal processing .",
    "for example , researchers developed smoothed @xmath0 method @xcite for sparse decomposition , whose iteration includes a steepest ascent step and a projection step .",
    "the first step is defined as @xmath39 where @xmath40 serves as step size , @xmath41^{\\rm t}$ ] denotes the negative derivative to an approximated @xmath0 norm and takes the value @xmath42 after ( [ sl0_1 ] ) , a projection step is performed which maps @xmath43 to @xmath44 in the feasible set .",
    "it can be seen that ( [ sl0_1 ] ) performs steepest ascent , which is similar to zero - point attraction in @xmath0-lms .",
    "the iteration details and performance analysis of this algorithm are presented in @xcite and @xcite , respectively .",
    "another algorithm , named iterative bayesian@xcite , also enjoys steepest ascent iteration as @xmath45 where @xmath40 denotes the step size and @xmath6 is a log posterior probability function .",
    "analysis of this algorithm and its application to sparse component analysis in noisy scenario are presented in @xcite .",
    "considering the nonlinearity of zero - point attraction , some preparations are made to simplify the mean square performance analysis .      because various affects are exerted in @xmath0-lms to the filter tap - weights according to their respective system coefficients",
    ", it would be helpful to classify the unknown parameters , correspondingly , the filter tap - weights , into several categories and perform different analysis on each category separately . according to the attraction range and their strength ,",
    "all system coefficients are classified into three categories as @xmath46 where @xmath47 . obviously , @xmath48 and @xmath49 . in the following text ,",
    "derivations are firstly carried out for the three sets separately .",
    "then a synthesis is taken to achieve the final results .",
    "the following assumptions about the system and the predefined parameters are adopted to enable the formulation .",
    "* input data @xmath50 is an _",
    "_ zero - mean gaussian signal .",
    "* tap - weights @xmath51 , input vector @xmath52 , and additive noise @xmath8 are mutually independent . * the parameter @xmath53 is so small that @xmath54 .    assumption ( i ) commonly holds while ( ii ) is the well - known independence assumption @xcite . assumption ( iii ) comes from the experimental observations , i.e. , a too large @xmath53 can cause much bias as well as large steady - state msd . therefore , in order to achieve better performance , @xmath53 should not be too large .",
    "besides the above items , several regular patterns are supposed during the convergence and the steady state .    *",
    "all tap - weights , @xmath51 , follow gaussian distribution .",
    "* for @xmath55 , the tap - weight @xmath56 is assumed to have the same sign with the corresponding unknown coefficient . *",
    "the adaptive weight @xmath56 is assumed out of the attraction range for @xmath57 , while in the attraction range elsewhere .",
    "assumption ( iv ) is usually accepted for steady - state behavior analysis @xcite .",
    "assumption ( v ) and ( vi ) are considered suitable in this work due to the following two aspects .",
    "first , there are few taps violating these assumptions in a common scenario",
    ". intuitively , only the non - zero taps with rather small absolute value may violate assumption ( v ) , while assumption ( vi ) may not hold for the taps close to the boundaries of the attraction range .",
    "for other taps which make up the majority , these assumptions are usually reasonable , especially in high snr cases .",
    "second , assumptions ( v ) and ( vi ) are proper for small steady - state msd , which is emphasized in this work .",
    "the smaller steady - state msd is , the less tap - weights differ from unknown coefficients . therefore , it is more likely that they share the same sign , as well as on the same side of the attraction range .",
    "based on the discussions above , those patterns are regarded suitable in steady state .",
    "for the convergence process , due to fast convergence of lms - type algorithms , we may suppose that most taps will get close to the corresponding unknown coefficients very quickly , so these patterns are also employed in common scenarios .",
    "as we will see later , some of the above assumptions can not always hold in whatever parameter setting and may restrict the applicability of some analysis below .",
    "however , considering the difficulties of nonlinear algorithm performance analysis , these assumptions can significantly enable mathematical tractability and help obtain results shown to be precious in a large range of parameter setting .",
    "thus , we consider these assumptions reasonable to be employed in this work .",
    "based on the assumptions above , the mean and mean - square performances of @xmath0-lms are analyzed in this section .",
    "define the misalignment vector as @xmath58 , combine ( [ expout ] ) , ( [ esterror ] ) , and ( [ w_update ] ) , one has @xmath59 taking expectation and using the assumption ( ii ) , one derives @xmath60 where _ overline _ denotes expectation .    * for @xmath57 , utilizing assumption ( vi ) , one has @xmath61 . * for @xmath62 , combining assumptions ( iii ) , ( v ) and ( vi ) , it can be derived that @xmath63 * for @xmath64 , noticing the fact that @xmath65 has the opposite sign with @xmath66 in interval @xmath67 and using assumptions ( iv ) and ( vi ) , it can be derived that @xmath61 .",
    "thus , the bias in steady state is obtained @xmath68 in steady state , therefore , the tap - weights are unbiased for large coefficients and zero coefficients , while they are biased for small coefficients .",
    "the misalignment depends on the predefined parameters as well as unknown coefficient @xmath69 itself .",
    "the smaller the unknown coefficient is , the larger the bias becomes .",
    "this tendency can be directly read from fig .",
    "[ fig : g_l0l1](a ) . in the attraction range ,",
    "the intensity of the zero - point attraction increases as tap - weights get more closing to zero , which causes heavy bias .",
    "thus , the bias of small coefficients in steady state is the byproduct of the attraction , which accelerates the convergence rate and increases steady - state msd .",
    "the condition on mean square convergence and steady - state msd are given by the following theorem .",
    "[ theo1 ] in order to guarantee convergence , step - size @xmath40 should satisfy @xmath70 and the final mean square deviation of @xmath0-lms is @xmath71 where @xmath72 are defined in ( [ beta1])@xmath73([beta3 ] ) in appendix [ const ] , respectively .",
    "the proof of theorem [ theo1 ] goes in appendix [ proof_steady_msd ] .",
    "_ remark 1 _ : the steady - state msd of @xmath0-lms is composed of two parts : the first item in ( [ msd_expression ] ) is exactly the steady - state msd of standard lms ( [ msdlms ] ) , while the latter two items compose an additional part caused by zero - point attraction .",
    "when @xmath53 equals zero , @xmath0-lms becomes the traditional lms , and correspondingly the additional part vanishes .",
    "when the additional part is negative , @xmath0-lms has smaller steady - state msd and thus better steady - state performance over standard lms .",
    "consequently , it can be deduced that the condition on @xmath53 to ensure @xmath0-lms outperforms lms in steady - state is @xmath74    _ remark 2 _ : according to theorem [ theo1 ] , the following corollary on parameter @xmath53 is derived .",
    "[ coro : opt_kappa ] from the perspective of steady - state performance , the best choice for @xmath53 is @xmath75{\\frac{\\beta_1+\\beta_2}{\\beta_1-\\beta_2}}-\\sqrt[4]{\\frac{\\beta_1-\\beta_2}{\\beta_1+\\beta_2}}\\right ) , \\label{kappabest}\\ ] ] and the minimum steady - state msd is @xmath76    the proof of corollary @xmath77 is presented in appendix [ best_parameters ] .",
    "please notice that in ( [ msd_opt ] ) , the first item is about standard lms and the second one is negative when @xmath78 is less than @xmath6 .",
    "therefore , the minimum steady - state msd of @xmath0-lms is less than that of standard lms as long as the system is not totally non - sparse .",
    "_ remark 3 _ : according to the theorem , it can be accepted that the steady - state msd is not only controlled by the predefined parameters , but also dependent on the unknown system in the following two aspects .",
    "first , the sparsity of the system response , i.e. @xmath78 and @xmath6 , controls the steady - state msd .",
    "second , significantly different from standard lms , the steady - state msd is relevant to the _ small coefficients _ of the system , considering the attracting strength appears in @xmath79 and @xmath80 .    here",
    "we mainly discuss the effect of system sparsity as well as the distribution of coefficients on the minimum steady - state msd . based on the above results , the following corollary can be deduced .",
    "[ coro : steady_q ] the minimum steady - state msd of ( [ msd_opt ] ) is monotonic increasing with respect to @xmath78 and attracting strength @xmath81 .    the validation of corollary @xmath82 is performed in appendix [ effect_sparsity ] .",
    "the _ zero - point attractor _ is utilized in @xmath0-lms to draw tap - weights towards zero .",
    "the more sparse the unknown system is , the less steady - state msd is .",
    "similarly , small coefficients are biased in steady state and deteriorate the performance , which explains that steady - state msd is increasing with respect to @xmath81 .",
    "_ remark 4 _ : according to ( [ convcondition ] ) , one knows that @xmath0-lms has the same convergence condition on step size as standard lms@xcite and za - lms@xcite .",
    "consequently the effect of @xmath40 on steady - state performance is analyzed .",
    "it is indicated in ( [ msdlms ] ) that the standard lms enhances steady - state performance by reducing step size@xcite .",
    "@xmath0-lms has a similar trend . for the seek of simplicity and practicability , a sparse system of @xmath78 far less than @xmath6",
    "is considered to demonstrate this property . utilizing ( [ convcondition ] ) in such scenario ,",
    "the following corollary is derived .",
    "[ coro : steady_mu ] for a sparse system which satisfies @xmath83 the minimum steady - state msd in ( [ msd_opt ] ) is further approximately simplified as @xmath84 where @xmath85 and @xmath86 are defined by ( [ eta6 ] ) in appendix [ const ] , and @xmath81 , defined by ( [ attractingstrength ] ) , denotes the attracting strength to the zero - point .",
    "furthermore , the minimum steady - state msd increases with respect to the step size .",
    "the proof of corollary [ coro : steady_mu ] is conducted in appendix [ effect_mu ] . due to the stochastic gradient descent and zero - point attraction , the tap - weights suffer oscillation , even in steady state , whose intensity is directly relevant to the step size .",
    "the larger the step size , the more intense the vibration .",
    "thus , the steady - state msd is monotonic increasing with respect to @xmath40 in the above scenario .",
    "_ remark 5 _ : in the scenario where @xmath87 remains a constant while @xmath88 approaches to zero , it can be readily accepted that ( [ w_update ] ) becomes totally identical to ( [ w_update_l1 ] ) , therefore @xmath0-lms becomes za - lms in this limit setting of parameters . in appendix [ relationship_sparse_standard_lms ]",
    "it is shown that the result ( [ shi_sparse_lms_msd ] ) for steady - state performance @xcite could be regarded as a particular case of theorem [ theo1 ] .",
    "as @xmath88 approaches to zero in @xmath0-lms , the attraction range becomes infinity and all non - zero taps belong to small coefficients which are biased in steady state .",
    "thus , za - lms has larger steady - state msd than @xmath0-lms , due to bias of all taps caused by uniform attraction intensity .",
    "if @xmath53 is further chosen optimal , the optimal parameter for za - lms is given by @xmath89 ( notice that @xmath90 approaches @xmath91 as @xmath88 tending to zero , as makes @xmath92 finite ) , and the minimum steady - state msd of @xmath0-lms ( [ msd_opt ] ) converges to that of za - lms . to better compare the three algorithms , the steady - state msds of lms , za - lms , and @xmath0-lms are listed in table [ table1 ] , where that of za - lms is rewritten and @xmath93 is defined in ( [ gamma_za ] ) in appendix [ relationship_sparse_standard_lms ] .",
    "it can be accepted that the steady - state msds of both za - lms and @xmath0-lms are in the form of @xmath94 plus addition items , where @xmath94 denotes the steady - state msd of standard lms .",
    "if the additional items are negative , za - lms and @xmath0-lms exceed lms in steady - state performance .",
    "[ cols=\"^,^,^,^,^ \" , ]     in the first experiment , the steady - state performance with respect to @xmath53 is considered . referring to fig .",
    "[ fig : steady_msd_sweep_kappa_40db ] , the theoretical steady - state msd of @xmath0-lms is in good agreement with the experiment results when snr is @xmath95db . with the growth of @xmath53 from @xmath96 , the steady - state msd decreases at first , which means proper zero - point attraction is helpful for sufficiently reducing the amplitude of tap - weights in @xmath97 . on the other hand ,",
    "larger @xmath53 results in more intensity of zero - point attraction item and increases the bias of small coefficients @xmath98 .",
    "overlarge @xmath53 causes too much bias , thus deteriorates the overall performance . from ( [ kappabest ] )",
    ", @xmath99 produces the minimized steady - state msd , which is marked with a square in fig .",
    "[ fig : steady_msd_sweep_kappa_40db ] .",
    "again , simulation result tallies with analytical value well . when snr is @xmath100db , referring to fig .",
    "[ fig : steady_msd_sweep_kappa_20db ] , the theoretical result also predicts the trend of msd well .",
    "however , since the assumptions ( v ) and ( vi ) do not hold well in low snr case , the theoretical result has perceptible deviation from the simulation result .    -lms ( with respect to different @xmath53 ) , where snr is @xmath95db and the solid square denotes @xmath101.,title=\"fig:\",width=384 ] +    -lms ( with respect to different @xmath53 ) , where snr is @xmath100db and the solid square denotes @xmath101.,title=\"fig:\",width=384 ] +    in the second experiment , the effect of parameter @xmath88 on steady - state performance is investigated .",
    "please refer to fig .",
    "[ fig : steady_msd_sweep_alpha_40db ] for results .",
    "rza - lms is also tested for performance comparison , with its parameter @xmath21 chosen as optimal values which are obtained by experiments . for the sake of simplicity",
    ", the parameter @xmath25 in ( [ rzagradapprx ] ) is set the same as @xmath88 .",
    "simulation results confirm the validity of the theoretical analysis .",
    "with very small @xmath88 , all tap - weights are attracted toward zero - point and the steady - state msd is nearly independent . as @xmath88 increases",
    ", there are a number of taps fall in the attraction range while the others are out of it .",
    "consequently , the total bias reduces .",
    "besides , the results for za - lms are also considered in this experiment , with the optimal parameter @xmath21 proposed in _",
    "remark 5_. it is shown that @xmath0-lms always yields superior steady - state performance than za - lms ; moreover , in scenario where @xmath88 approaches @xmath102 , the msd of @xmath0-lms tends to that of za - lms . in the parameter range of this experiment ,",
    "@xmath0-lms shows better steady - state performance than rza - lms .    ) , and @xmath0-lms ( with respect to different @xmath88 ) , where @xmath25 equals @xmath88 .",
    "parameters @xmath21 and @xmath53 are chosen as optimal for rza - lms and @xmath0-lms , respectively.,title=\"fig:\",width=384 ] +    the third experiment studies the effect of non - zero coefficients number on steady - state deviation .",
    "please refer to fig .",
    "[ fig : steady_msd_sweep_q ] .",
    "it is readily accepted that @xmath0-lms with optimal @xmath53 outperforms traditional lms in steady state .",
    "the fewer the non - zero unknown coefficients are , the more effectively @xmath0-lms draws tap - weights towards zero .",
    "therefore , the effectiveness of @xmath0-lms increases with the sparsity of the unknown system .",
    "when @xmath78 exactly equals @xmath6 , its performance with optimal @xmath53 already attains that of standard lms , indicating that there is no room for performance enhancement of @xmath0-lms for a totally non - sparse system .",
    "-lms ( with respect to different total non - zeros taps @xmath78 ) , where @xmath53 is chosen as optimal.,title=\"fig:\",width=384 ] +    the fourth experiment is designed to investigate convergence process with respect to @xmath53 . also , the learning curve of the standard lms is simulated . when snr is @xmath95db , the results in fig .",
    "[ fig : converegence_msd_sweep_kappa_40db ] demonstrate that our theoretical analysis of convergence process is generally in good accordance with simulation .",
    "it can be observed that different @xmath53 results in differences in both steady - state msd and the convergence rate . due to more intense zero - attraction force ,",
    "larger @xmath53 results in higher convergence rate ; but too large @xmath53 can have bad steady - state performance for too much bias of small coefficients .",
    "moreover , @xmath0-lms outperforms standard lms in convergence rate for all parameters we run , and also surpasses it in steady - state performance when @xmath53 is not too large . when snr is 20db , fig .",
    "[ fig : converegence_msd_sweep_kappa_20db ] also shows similar trend about how @xmath53 influences the convergence process ; however , since the low snr scenario breaks assumptions ( v ) and ( vi ) , the theoretical results and experimental results differ to some extent .    -lms ( with respect to different @xmath53 ) , where snr is @xmath95db.,title=\"fig:\",width=384 ] +    -lms ( with respect to different @xmath53 ) , where snr is @xmath100db.,title=\"fig:\",width=384 ] +    the fifth experiment demonstrates convergence process for various step sizes , with the comparison of lms and @xmath0-lms",
    ". please refer to fig .",
    "[ fig : converegence_msd_sweep_mu ] . similar to traditional lms ,",
    "smaller step size yields slower convergence rate and less steady - state msd .",
    "therefore , the choice of step size should seek a balance between convergence rate and steady - state performance . furthermore , the convergence rate of @xmath0-lms is faster than that of lms when their step sizes are identical .",
    "-lms with respect to different step sizes @xmath40 , where @xmath53 is chosen as optimal for @xmath0-lms.,title=\"fig:\",width=384 ] +",
    "the comprehensive mean square performance analysis of @xmath0-lms algorithm is presented in this paper , including both steady - state and convergence process .",
    "the adaptive filtering taps are firstly classified into three categories based on the zero - point attraction item , and then analyzed separately . with the help of some assumptions which are reasonable in a wide range ,",
    "the steady - state msd is finally deduced and the convergence of instantaneous msd is approximately predicted . moreover , a parameter selection rule is put forward to minimize the steady - state msd and theoretically it is shown that @xmath0-lms with optimal parameters is superior than traditional lms for sparse system identification .",
    "the all - round theoretical results are verified in a large range of parameter setting through numerical simulations .",
    "in order to make the main body simple and focused , the explicit expressions of some constants used in derivations are listed here .",
    "all through this work , four constants of @xmath103 are used to simplify the expressions .    to evaluate the zero - point attracting strength , with respect to the sparsity of the unknown system coefficients ,",
    "two kinds of strengthes are defined as @xmath104 which are utilized everywhere in this work .",
    "considering the attraction range , it can be readily accepted that these strengthes are only related to the small coefficients , other than the large ones and the zeros .    in lemma [ lemma ]",
    ", @xmath105 is defined as @xmath106,\\label{matrix_a}\\ ] ] and @xmath107^{\\rm t},\\label{vector_b}\\ ] ] where @xmath108 where @xmath109 is the solution to ( [ quadratic_equ ] ) .    in theorem",
    "[ theo2 ] , the constants @xmath110 and @xmath111 are @xmath112    in corollary [ coro : opt_kappa ] , the constants @xmath72 are @xmath113    in appendix [ effect_sparsity ] and [ effect_mu ] , the constants @xmath114 are @xmath115",
    "denote @xmath116 to be msd at iteration @xmath3 , and @xmath117 to be the second moment matrix of @xmath118 , respectively , @xmath119 substituting ( [ h_update ] ) into ( [ secondmoment ] ) , and expanding the term @xmath120 into three second moments using the gaussian moment factoring theorem @xcite , one knows @xmath121 using the fact that @xmath122 , one has @xmath123 consequently , the condition needed to ensure convergence is @xmath124 and ( [ convcondition ] ) is derived directly , which is the same as standard lms and similar with the conclusion in @xcite .    next the steady - state msd will be derived . using ( [ r_update ] ) and considering the @xmath125th diagonal element",
    ", one knows @xmath126 to develop @xmath127 , one should first investigate two items , namely @xmath128 and @xmath129 in ( [ msd_comp_steadystate ] ) .",
    "for @xmath57 , from assumption ( vi ) one knows @xmath130 , thus @xmath131 for small coefficients , considering assumptions ( v ) and ( vi ) , formula ( [ l_0gradapprx ] ) implies @xmath132 is a locally linear function with slope @xmath133 , which results in @xmath134 thus , it can be shown @xmath135 where @xmath136 is derived in ( [ h_steadystate_detail ] ) .    then turning to @xmath64",
    ", it is readily known that @xmath137 in this case .",
    "thus , from assumptions ( iv ) and ( vi ) , the following results can be derived from the property of gaussian distribution , @xmath138 combining assumption ( iii ) , ( [ h_steadystate_detail ] ) and ( [ ehgegg_lc])@xmath73([egg_sc ] ) , one can know the equivalency between ( [ msd_comp_steadystate ] ) and following equations for @xmath125 in @xmath139 , @xmath98 , and @xmath97 , respectively , @xmath140 where @xmath109 denotes @xmath141 for simplicity . summing ( [ msd_comp_steadystate_lc ] ) and ( [ msd_comp_steadystate_sc ] ) for all @xmath142 , and noticing that @xmath143 it could be derived that @xmath144 where @xmath81 is introduced in ( [ attractingstrength ] ) . combining ( [ msd_expression_old ] ) and ( [ msd_comp_steadystate_zc ] ) , it can be reached that @xmath109 is defined by the following equation @xmath145 finally , ( [ msd_expression ] ) is achieved after solving the quadratic equation above and a series of formula transformation on ( [ msd_expression_old ] ) .",
    "thus , the proof of theorem [ theo1 ] is completed .      the vector @xmath187 in ( [ vector_b ] )",
    "could be denoted as @xmath188 ,   \\label{b_simp}\\ ] ] where @xmath110 is defined in ( [ lambda3 ] ) and @xmath189 are constants .",
    "take @xmath190-transform for ( [ d_omega_iter ] ) , it can be derived that @xmath191 = ( z\\mathbf{i}-\\mathbf{a})^{-1 } z \\left[\\!\\!\\ !",
    "d_0\\\\ \\omega_0\\end{array}\\!\\!\\!\\right ] + ( z\\mathbf{i}-\\mathbf{a})^{-1}{\\mathbf{b}(z)},\\ ] ] where @xmath192 .",
    "then combine the definition of @xmath193 in theorem [ theo2 ] and the above results , it is further derived @xmath194 where @xmath195 and @xmath196 are constants .",
    "take the inverse @xmath190-transform and notice the definition of @xmath197 , it finally yields @xmath198 thus we have completed the proof of ( [ d_conv_closedform ] ) . by forcing the equivalence between ( [ d_conv_closedform ] ) and lemma [ lemma ] , the expression of @xmath111 could be solved as ( [ c3 ] ) .",
    "by defining @xmath146 , ( [ msd_expression ] ) becomes @xmath147 where @xmath148 is defined as @xmath149 next we want to find @xmath150 which minimizes @xmath148 . forcing the derivative of @xmath148 with respect to @xmath66 to be zero",
    ", it can be obtained that @xmath151 combining @xmath152 and substituting @xmath153 in ( [ msd_theta ] ) , corollary [ coro : opt_kappa ] can be finally achieved .      from ( [ msdlms ] ) , ( [ msd_opt ] ) , ( [ beta1 ] ) , ( [ beta2 ] ) , and ( [ eta0 ] )",
    ", it can be obtained that @xmath154 note neither the @xmath26 defined in ( [ msdlms ] ) nor @xmath155 defined in ( [ eta0 ] ) is dependent on @xmath78 or @xmath156 , thus the focus of the proof is the denominator in ( [ msd_opt_eta ] ) . in the following",
    ", we will analyze the two items in the denominator separately and obtain their monotonicity . the first item in the denominator",
    "is @xmath157 from ( [ beta1_coro : steady_q ] ) , it has already shown that @xmath158 is increasing with respect to @xmath78 and @xmath156 .",
    "next we consider the second item .",
    "it can be obtained beforehand that @xmath80 and @xmath159 equal @xmath160 and @xmath161 , respectively .",
    "thus , one has @xmath162 further notice that @xmath163 it can be proved that all of the three items in the square root of ( [ beta1beta2_coro : steady_q ] ) are increasing with respect to @xmath78 and @xmath81 ; thus the second item in the denominator is monotonic increasing with respect to @xmath78 and @xmath81 . till now , the monotonicity of @xmath164 with respect to @xmath78 and @xmath81 has been proved .",
    "last , in the special scenario where @xmath78 exactly equals @xmath6 , it can be obtained that @xmath164 is identical to @xmath165 ; thus @xmath165 is larger than the minimum steady - state msd of the scenario where @xmath78 is less than @xmath6 . in sum ,",
    "corollary [ coro : steady_q ] is proved .      for a sparse system in accordance with ( [ sparse_condition ] ) ,",
    "@xmath114 defined in appendix [ const ] are approximated by @xmath166 substituting @xmath114 in @xmath72 of ( [ msd_opt_eta ] ) , with the approximated expressions above , ( [ msd_simple_sparse_smallmu ] ) is finally derived after calculation .",
    "next we show @xmath164 in ( [ msd_simple_sparse_smallmu ] ) is monotonic increasing with respect to @xmath40 .",
    "since ( [ msd_simple_sparse_smallmu ] ) is equivalent with @xmath167 it can be directly observed from ( [ eta6 ] ) that larger @xmath40 results in larger numerator as well as smaller denominator in ( [ msd_simple_appdendix ] ) , which both contribute to the fact that @xmath164 is monotonic increasing with respect to @xmath40 .",
    "thus , the proof of corollary [ coro : steady_mu ] is arrived .",
    "define function @xmath199 then the roots of @xmath200 are eigenvalues of matrix @xmath201 . from ( [ matrix_a ] )",
    ", it can be shown @xmath202 and @xmath203 where @xmath204 denote the entries of @xmath205 .",
    "thus , we know @xmath206 and @xmath207 , which indicates that one root of quadratic function @xmath200 is within the interval @xmath208 $ ] .",
    "similarly , another root of @xmath200 is in @xmath209 .",
    "thus , it can be concluded that the eigenvalues of @xmath201 are both in @xmath210 and satisfy @xmath211 for large step size scenario of @xmath212 , ( [ lambda3 ] ) and ( [ lambda12_bound_full ] ) yield @xmath213    through comparison between ( [ d_conv_closedform ] ) and ( [ d_conv_closedform_lms ] ) , one can know for large @xmath40 , all the three transient items in msd convergence of @xmath0-lms has faster attenuation rate than lms , leading to acceleration of convergence rate .",
    "when @xmath87 remains a constant while @xmath88 approaches zero , from ( [ w_update ] ) , ( [ l_0gradapprx ] ) , and ( [ l_1gradapprx ] ) , it is obvious that the recursion of @xmath0-lms becomes that of za - lms .",
    "furthermore , one can see that @xmath168 equals @xmath169 . from the definition",
    ", it can be shown @xmath139 is an empty set when @xmath88 approaching zero .",
    "consequently , @xmath170 combine ( [ msd_expression_old ] ) , ( [ quadratic_equ ] ) , and ( [ gsc_sum_l1 ] ) , then after quite a series of calculation , the explicit expression of steady - state msd becomes @xmath171 where @xmath93 is the discriminant of quadratic equation ( [ quadratic_equ ] ) , @xmath172 through a series of calculation , it can be proved that ( [ msd_l1_expression ] ) is equivalent with ( [ shi_sparse_lms_msd ] ) obtained in @xcite .",
    "thus , the steady - state msd in za - lms could be regarded as a particular case of that in @xmath0-lms .",
    "from ( [ r_update ] ) , the update formula is @xmath173    since lms algorithm has fast convergence rate , it is reasonable to suppose most filter tap - weights will get close to the corresponding system coefficient very quickly ; thus , the classification of coefficients @xmath69 could help in the derivation of the convergence situation of @xmath174 .    for @xmath57 , from assumption ( vi ) , ( [ coeff_update ] ) takes the form @xmath175    for @xmath62 the mean convergence is firstly derived and then the mean square convergence is deduced .",
    "take expectation in ( [ h_update ] ) , and combine assumptions ( iii ) , ( v ) , and ( vi ) , one knows @xmath176 since @xmath177 , one can finally get @xmath178 combining ( [ coeff_update ] ) , ( [ mean_conv_sc ] ) and employing assumption ( iii ) , it can be achieved @xmath179    next turn to @xmath64 . from assumption ( iv ) , the following formula can be attained employing the steady state result and first - order taylor expansion @xmath180 where @xmath181 , which is the solution to equation ( [ quadratic_equ ] ) .",
    "finally , with assumption ( iii ) we have @xmath182    considering @xmath183 , and combine ( [ lc_conv]),([mean_conv_sc]),([sc_conv ] ) , and ( [ zc_conv ] ) , one can obtain ( [ d_omega_iter ] ) after a series of derivation .",
    "as for the initial value , since @xmath184 , by definition we have @xmath185 and @xmath186 . thus , lemma [ lemma ] is reached .",
    "the authors wish to thank laming chen and four anonymous reviewers for their helpful comments to improve the quality of this paper .",
    "d. angelosante , j. a. bazerque , and g. b. giannakis , `` online adaptive estimation of sparse signals : where rls meets the @xmath2-norm , '' _ ieee trans . signal process .",
    "58 , no . 7 , pp . 3436 - 3447 , jul .",
    "2010 .",
    "g. mileounis , b. babadi , n. kalouptsidis , and v. tarokh , `` an adaptive greedy algorithm with application to nonlinear communications , '' _ ieee trans .",
    "signal process .",
    "6 , pp . 2998 - 3007 , jun .",
    "2010 .",
    "y. kopsinis , k. slavakis , and s. theodoridis , `` online sparse system identification and signal reconstruction using projections onto weighted @xmath2 balls , '' _ ieee trans .",
    "signal process .",
    "905 - 930 , mar .",
    "2011 .",
    "e. cands , j. romberg , and t. tao , `` robust uncertainty principles : exact signal reconstruction from highly incomplete frequency information , '' _ ieee trans .",
    "inf . theory _ ,",
    "489 - 509 , feb . 2006 .",
    "h. zayyani , m. babaie - zadeh , and c. jutten , `` an iterative bayesian algorithm for sparse component analysis ( sca ) in presence of noise , '' _ ieee trans .",
    "signal process .",
    "4378 - 4390 , oct .",
    "j. jin , y. gu , and s. mei , `` a stochastic gradient approach on compressive sensing signal reconstruction based on adaptive filtering framework , '' _",
    "ieee j. sel .",
    "topics signal process . _ ,",
    "409 - 420 , apr . 2010 .",
    "l. horowitz and k. senne , `` performance advantage of complex lms for controlling narrow - band adaptive arrays , '' _ ieee trans .",
    "acoust . , speech , signal process .",
    "assp-29 , no .",
    "722 - 736 , jun .",
    "1981 .",
    "s. k. zhao , z. h. man , s. y. khoo , and h. r. wu,``stability and convergence analysis of transform - domain lms adaptive filters with second - order autoregressive process , '' _ ieee trans .",
    "signal process .",
    "119 - 130 , jan . 2009 .",
    "a. ahlen , l. lindbom , and m. sternad , `` analysis of stability and performance of adaptation algorithms with time - invariant gains , '' _ ieee trans .",
    "signal process .",
    "103 - 116 , jan . 2004 .",
    "s. c. chan and y. zhou , `` convergence behavior of nlms algorithm for gaussian inputs : solutions using generalized abelian integral functions and step size selection , '' _ journal of signal processing systems _ , vol .",
    "3 , pp . 255 - 265 , 2010 .",
    "s. c. chan and y. zhou , `` on the performance analysis of the least mean m - estimate and normalized least mean m - estimate algorithms with gaussian inputs and additive gaussian and additive gaussian and contaminated gaussian noises , '' _ journal of signal processing systems _ , vol .",
    "81 - 103 , 2010 .",
    "s. koike , `` convergence analysis of a data echo canceler with a stochastic gradient adaptive fir filter using the sign algorithm , '' _ ieee trans .",
    "signal process .",
    "2852 - 2861 , dec . 1995"
  ],
  "abstract_text": [
    "<S> as one of the recently proposed algorithms for sparse system identification , @xmath0 norm constraint least mean square ( @xmath0-lms ) algorithm modifies the cost function of the traditional method with a penalty of tap - weight sparsity . </S>",
    "<S> the performance of @xmath0-lms is quite attractive compared with its various precursors . </S>",
    "<S> however , there has been no detailed study of its performance . </S>",
    "<S> this paper presents comprehensive theoretical performance analysis of @xmath0-lms for white gaussian input data based on some assumptions which are reasonable in a large range of parameter setting . </S>",
    "<S> expressions for steady - state mean square deviation ( msd ) are derived and discussed with respect to algorithm parameters and system sparsity . </S>",
    "<S> the parameter selection rule is established for achieving the best performance . </S>",
    "<S> approximated with taylor series , the instantaneous behavior is also derived . </S>",
    "<S> in addition , the relationship between @xmath0-lms and some previous arts and the sufficient conditions for @xmath0-lms to accelerate convergence are set up . finally , </S>",
    "<S> all of the theoretical results are compared with simulations and are shown to agree well in a wide range of parameters .    </S>",
    "<S> * keywords : * adaptive filter , sparse system identification , @xmath0-lms , mean square deviation , convergence rate , steady - state misalignment , independence assumption , white gaussian signal , performance analysis . </S>"
  ]
}