{
  "article_text": [
    "symmetric positive definite matrices arise in many areas in a variety of guises : covariances , kernels , graph laplacians , or otherwise .",
    "a basic computation with such matrices is evaluation of the bilinear form @xmath3 , where @xmath4 is a matrix function and @xmath2 , @xmath5 are given vectors .",
    "if @xmath6 , we speak of computing a _ bilinear inverse form ( bif ) _ @xmath7 .",
    "for example , with @xmath8 ( @xmath9 canonical vector ) @xmath10 is the @xmath9 diagonal entry of the inverse .    in this paper , we are interested in efficiently computing bifs , primarily due to their importance in several machine learning contexts , e.g. , evaluation of gaussian density at a point , the woodbury matrix inversion lemma , implementation of mcmc samplers for determinantal point processes ( dpp ) , computation of graph centrality measures , and greedy submodular maximization ( see section  [ sec : motiv.app ] ) .",
    "when @xmath1 is large , it is preferable to compute @xmath7 iteratively rather than to first compute @xmath11 ( using cholesky ) at a cost of @xmath12 operations .",
    "one could think of using conjugate gradients to solve @xmath13 approximately , and then obtain @xmath14 .",
    "but several applications require precise bounds on numerical estimates to @xmath15 ( e.g. , in mcmc based dppsamplers such bounds help decide whether to accept or reject a transition in each iteration  see section  [ sec : mcdpp ] ) , which necessitates a more finessed approach .",
    "gauss quadrature is one such approach .",
    "originally proposed in @xcite for approximating integrals , gauss- and _ gauss - type quadrature _",
    "( i.e. , gauss - lobatto @xcite and gauss - radau @xcite quadrature ) have since found application to bilinear forms including computation of @xmath7 @xcite .",
    "@xcite also show that gauss and ( right ) gauss - radau quadrature yield lower bounds , while gauss - lobatto and ( left ) gauss - radau yield upper bounds on the bif @xmath7 .    however , despite its long history and voluminous existing work ( see e.g. ,  @xcite ) , our understanding of gauss - type quadrature for matrix problems is far from complete .",
    "for instance , it is not known whether the bounds on bifs improve with more quadrature iterations ; nor is it known how the bounds obtained from gauss , gauss - radau and gauss - lobatto quadrature compare with each other .",
    "_ we do not even know how fast the iterates of gauss - radau or gauss - lobatto quadrature converge .",
    "_    * contributions . *",
    "we address all the aforementioned problems and make the following main contributions :    =1em    we show that the lower and upper bounds generated by gauss - type quadrature monotonically approach the target value  ( theorems  [ thm : lowbtwn ] and [ thm : upbtwn ] ; corr .",
    "[ cor : monlow ] ) .",
    "furthermore , we show that for the same number of iterations , gauss - radau quadrature yields bounds superior to those given by gauss or gauss - lobatto , but somewhat surprisingly all three share the same convergence rate .",
    "we prove linear convergence rates for gauss - radau and gauss - lobatto explicitly  ( theorems  [ thm : rrconv ] and [ thm : lrconv ] ; corr .",
    "[ cor : loconv ] ) .",
    "we demonstrate implications of our results for two tasks : ( i ) scalable markov chain sampling from a dpp ; and ( ii ) running a greedy algorithm for submodular optimization . in these applications",
    ", quadrature accelerates computations , and the bounds aid early stopping .    indeed , on large - scale sparse problems our methods lead to even several orders of magnitude in speedup .",
    "[ [ related - work . ] ] related work .",
    "+ + + + + + + + + + + + +    there exist a number of methods for efficiently approximating matrix bilinear forms . @xcite and @xcite use extrapolation of matrix moments and interpolation to estimate the 2-norm error of linear systems and the trace of the matrix inverse . @xcite",
    "extend the extrapolation method to bifs and show that the derived one - term and two - term approximations coincide with gauss quadrature , hence providing lower bounds .",
    "further generalizations address @xmath16 for a hermitian matrix @xmath1 @xcite .",
    "in addition , other methods exist for estimating trace of a matrix function  @xcite or diagonal elements of matrix inverse  @xcite .",
    "many of these methods may be applied to computing bifs .",
    "but they do not provide intervals bounding the target value , just approximations .",
    "thus , a black - box use of these methods may change the execution of an algorithm whose decisions ( e.g. , whether to transit in a markov chain ) rely on the bif value to be within a specific interval .",
    "such changes can break the correctness of the algorithm .",
    "our framework , in contrast , yields iteratively tighter lower and upper bounds ( section  [ sec : main ] ) , so the algorithm is guaranteed to make correct decisions ( section  [ sec : algos ] ) .",
    "bifs are important to numerous problems .",
    "we recount below several notable examples : in all cases , efficient computation of bounds on bifs is key to making the algorithms practical .    * determinantal point processes . *",
    "a determinantal point process ( dpp ) is a distribution over subsets of a set @xmath17 ( @xmath18 ) . in its _ l - ensemble _ form ,",
    "a dppuses a positive semidefinite kernel @xmath19 , and to a set @xmath20 assigns probability @xmath21 where @xmath22 is the submatrix of @xmath23 indexed by entries in @xmath24 .",
    "if we restrict to @xmath25 , we obtain a @xmath26-dpp .",
    "dpp s are widely used in machine learning , see e.g. , the survey  @xcite .    exact sampling from a ( @xmath26-)dpprequires eigendecomposition of @xmath23 @xcite , which is prohibitive .",
    "for large @xmath27 , metropolis hastings ( mh ) or gibbs sampling are preferred and state - of - the - art . therein the core task is to compute transition probabilities  an expression involving bifs  which are compared with a random scalar threshold .    for mh @xcite , the transition probabilities from a current subset ( state ) @xmath24 to @xmath28 are @xmath29 for @xmath30 ; and @xmath31 for @xmath32 . in a @xmath26-dpp ,",
    "the moves are swaps with transition probabilities @xmath33 for replacing @xmath34 by @xmath35 ( and @xmath36 ) .",
    "we illustrate this application in greater detail in section  [ sec : mcdpp ] .",
    "dpps are also useful for ( repulsive ) priors in bayesian models  @xcite .",
    "inference for such latent variable models uses gibbs sampling , which again involves bifs .    * submodular optimization , sensing .",
    "* algorithms for maximizing submodular functions can equally benefit from efficient bif bounds .",
    "given a positive definite matrix @xmath37 , the set function @xmath38 is _ submodular _",
    ": for all @xmath39 $ ] and @xmath40\\setminus t$ ] , it holds that @xmath41 .",
    "finding the set @xmath42 $ ] that maximizes @xmath43 is a key task for map inference with dpps @xcite , matrix approximations by column selection @xcite and sensing @xcite .",
    "for the latter , we model spatial phenomena ( temperature , pollution ) via gaussian processes and select locations to maximize the joint entropy @xmath44 of the observed variables , or the mutual information @xmath45\\setminus s})$ ] between observed and unobserved variables .    greedy algorithms for maximizing monotone @xcite or non - monotone",
    "@xcite submodular functions rely on marginal gains of the form @xmath46 for @xmath47 and @xmath48\\backslash s$ ] .",
    "the algorithms compare those gains to a random threshold , or find an item with the largest gain .",
    "in both cases , efficient bif bounds offer speedups .",
    "they can be combined with lazy  @xcite and stochastic greedy algorithms   @xcite .",
    "* network analysis , centrality .",
    "* when analyzing relationships and information flows between connected entities in a network , such as people , organizations , computers , smart hardwares , etc .",
    "@xcite , an important question is to measure popularity , centrality , or importance of a node .",
    "several existing popularity measures can be expressed as the solution to a large - scale linear system .",
    "for example , _ pagerank _",
    "@xcite is the solution to @xmath49 , and _ bonacich centrality _",
    "@xcite is the solution to @xmath50 , where @xmath1 is the adjacency matrix . when computing local estimates , i.e. , only a few entries of @xmath51 , we obtain exactly the task of computing bifs  @xcite .",
    "moreover , we may only need local estimates to an accuracy sufficient for determining which entry is larger , a setting where our quadrature based bounds on bifs will be useful .",
    "* scientific computing . * in computational physics bifs are used for estimating selected entries of the inverse of a large sparse matrix .",
    "more generally , bifs can help in estimating the trace of the inverse , a computational substep in lattice quantum chromodynamics  @xcite , some signal processing tasks  @xcite , and in gaussian process ( gp ) regression  @xcite , e.g. , for estimating variances . in numerical linear algebra ,",
    "bifs are used in rational approximations  @xcite , evaluation of green s function  @xcite , and selective inversion of sparse matrices  @xcite .",
    "a notable use is the design of preconditioners  @xcite and uncertainty quantification  @xcite .    *",
    "benefiting from fast iterative bounds .",
    "* many of the above examples use bifs to rank values , to identify the largest value or compare them to a scalar or to each other . in such cases , we first compute fast , crude lower and upper bounds on a bif , refining iteratively , just as far as needed to determine the comparison . figure  [ fig : conv ] in section  [ sec : conv ] illustrates the evolution of these bounds , and section  [ sec : algos ] explains details .",
    "for convenience , we begin by recalling key aspects of gauss quadrature , , which contains our new results . ] as applied to computing @xmath52 , for an @xmath53 symmetric positive definite matrix @xmath1 that has _",
    "simple _ eigenvalues , arbitrary vectors @xmath54 , and a matrix function @xmath4 . for a more detailed account of the relevant background on gauss - type quadratures",
    "please refer to appendix  [ append : sec : gauss ] , or @xcite .",
    "it suffices to consider @xmath55 thanks to the identity @xmath56 let @xmath57 be the eigendecomposition of @xmath1 where @xmath58 is orthonormal . letting @xmath59 , we then have @xmath60 toward computing @xmath61 , a key conceptual step is to write the above sum as the riemann - stieltjes integral @xmath62 : = u^\\top f(a)u = \\int_{\\lambda_{\\min}}^{\\lambda_{\\max } } f(\\lambda)d\\alpha(\\lambda),\\ ] ] where @xmath63 , @xmath64 , and @xmath65 is piecewise constant measure defined by @xmath66 our task now reduces to approximating the integral  , for which we invoke the powerful idea of gauss - type quadratures  @xcite .",
    "we rewrite the integral   as @xmath67 : = q_{n } + r_{n } = { \\sum\\nolimits}_{i=1}^n \\omega_i f(\\theta_i ) + { \\sum\\nolimits}_{i=1}^m \\nu_i f(\\tau_i ) + r_{n}[f],\\ ] ] where @xmath68 denotes the @xmath69th degree approximation and @xmath70 denotes the remainder term . in representation   the _ weights _",
    "@xmath71 , @xmath72 , and quadrature _ nodes _ @xmath73 are unknown , while the values @xmath74 are prescribed and lie outside the interval of integration @xmath75 .    different choices of these parameters yield different quadrature rules : @xmath76 gives gauss quadrature  @xcite ; @xmath77 with @xmath78  ( @xmath79 ) gives left  ( right ) gauss - radau quadrature  @xcite ; @xmath80 with @xmath78 and @xmath81 yields gauss - lobatto quadrature  @xcite ; while for general @xmath82 we obtain gauss - christoffel quadrature  @xcite .",
    "the weights @xmath71 , @xmath72 and nodes @xmath73 are chosen such that if @xmath4 is a polynomial of degree less than @xmath83 , then the interpolation @xmath84 = q_{n}$ ] is _",
    "exact_. for gauss quadrature , we can recursively build the _ jacobi matrix _ @xmath85 and obtain from its spectrum the desired weights and nodes .",
    "theorem  [ thm : gauss ] makes this more precise .",
    "@xcite[thm : gauss ] the eigenvalues of @xmath86 form the nodes @xmath87 of gauss quadrature ; the weights @xmath88 are given by the squares of the first components of the eigenvectors of @xmath86 .    if @xmath86 has the eigendecomposition @xmath89 , then for gauss  quadrature thm .",
    "[ thm : gauss ] yields @xmath90 given @xmath1 and @xmath2 , our task is to compute @xmath68 and the jacobi matrix @xmath86 .",
    "for bifs , we have that @xmath91 , so becomes @xmath92 , which can be computed recursively using the lanczos algorithm  @xcite . for gauss - radau and gauss - lobatto quadrature we can compute modified versions of jacobi matrices @xmath93  ( for left gauss - radau ) , @xmath94  ( for right gauss - radau ) and @xmath95  ( for gauss - lobatto ) based on @xmath86 . the corresponding nodes and weights , and",
    "thus the approximation of gauss - radau and gauss - lobatto quadratures , are then obtained from these modified jacobi matrices , similar to gauss quadrature . aggregating all these computations yields an algorithm that iteratively obtains bounds on @xmath96 .",
    "the combined procedure , _ gauss quadrature lanczos ( gql ) _",
    "@xcite , is summarily presented as algorithm  [ algo : gql ] .",
    "the complete algorithm may be found in appendix  [ append : sec : gauss ] .",
    "@xcite[lem : bounds ] let @xmath97 , @xmath98 , @xmath99 , and @xmath100 be the @xmath101-th iterates of gauss , left gauss - radau , right gauss - radau , and gauss - lobatto quadrature , respectively , as computed by alg .",
    "[ algo : gql ] . then , @xmath97 and @xmath99 provide lower bounds on @xmath102 , while @xmath98 and @xmath100 provide upper bounds .",
    "* initialize * : @xmath103 , @xmath104 , @xmath105 update @xmath106 using a lanczos iteration solve for the modified jacobi matrices @xmath107 ,",
    "@xmath108 and @xmath109 .",
    "compute @xmath97 , @xmath99 , @xmath98 and @xmath100 with sherman - morrison formula .",
    "it turns out that the bounds given by gauss quadrature have a close relation to the approximation error of conjugate gradient ( cg ) applied to a suitable problem . since we know the convergence rate of cg",
    ", we can obtain from it the following estimate on the _ relative error _ of gauss quadrature .",
    "[ thm : gaussconv ] the @xmath101-th iterate of gauss quadrature satisfies the relative error bound @xmath110 where @xmath111 is the condition number of @xmath1 .    in other words , thm .",
    "[ thm : gaussconv ] shows that the iterates of gauss quadrature have a linear ( geometric ) convergence rate .",
    "in this section we summarize our main theoretical results . as before , detailed proofs may be found in  appendix  [ app : sec : proofs ] .",
    "the key questions that we answer are : ( i ) do the bounds on @xmath102 generated by gql improve monotonically with each iteration ; ( ii ) how tight are these bounds ; and ( iii ) how fast do gauss - radau and gauss - lobatto iterations converge ?",
    "our answers not only fill gaps in the literature on quadrature , but provide a theoretical base for speeding up algorithms for some applications ( see sections  [ sec : motiv.app ] and [ sec : algos ] ) .      our first result shows that both gauss and right gauss - radau quadratures give iteratively better lower bounds on @xmath102 .",
    "moreover , with the same number of iterations , right gauss - radau yields tighter bounds .",
    "[ thm : lowbtwn ] let @xmath112 .",
    "then , @xmath99 yields better bounds than @xmath97 but worse bounds than @xmath113 ; more precisely , @xmath114    combining theorem  [ thm : lowbtwn ] with the convergence rate of relative error for gauss quadrature ( thm .  [ thm : gaussconv ] ) we obtain the following convergence rate estimate for right gauss - radau .",
    "[ thm : rrconv ] for each iteration @xmath101 , the right gauss - radau iterate @xmath99 satisfies @xmath115      our second result compares gauss - lobatto with left gauss - radau quadrature .",
    "[ thm : upbtwn ] let @xmath112 .",
    "then , @xmath98 gives better upper bounds than @xmath116 but worse than @xmath117 ; more precisely , @xmath118    this shows that bounds given by both gauss - lobatto and left gauss - radau become tighter with each iteration .",
    "for the same number of iterations , left gauss - radau provides a tighter bound than gauss - lobatto .    combining the above two theorems",
    ", we obtain the following corollary for all four gauss - type quadratures .",
    "[ cor : monlow ] with increasing @xmath101 , @xmath97 and @xmath99 give increasingly better lower bounds and @xmath98 and @xmath100 give increasingly better upper bounds , that is , @xmath119      our next two results state linear convergence rates for left gauss - radau quadrature and gauss - lobatto quadrature applied to computing the bif @xmath96 .",
    "[ thm : lrconv ] for each @xmath101 , the left gauss - radau iterate @xmath98 satisfies @xmath120 where @xmath121 .",
    "theorem  [ thm : lrconv ] shows that the error again decreases linearly , and it also depends on teh accuracy of @xmath122 , our estimate of the smallest eigenvalue that determines the range of integration . using the relations between left gauss - radau and gauss - lobatto",
    ", we readily obtain the following corollary .",
    "[ cor : loconv ] for each @xmath101 , the gauss - lobatto iterate @xmath100 satisfies @xmath123 where @xmath124 .",
    "* remarks * all aforementioned results assumed that @xmath1 is strictly positive definite with simple eigenvalues . in  appendix  [ append : sec : general ] , we show similar results for the more general case that @xmath1 is only required to be symmetric , and @xmath2 lies in the space spanned by eigenvectors of @xmath1 corresponding to distinct positive eigenvalues .",
    "next , we empirically verify our the theoretical results shown above .",
    "we generate a random symmetric matrix @xmath125 with density @xmath126 , where each entry is either zero or standard normal , and shift its diagonal entries to make its smallest eigenvalue @xmath127 , thus making @xmath1 positive definite .",
    "we set @xmath128 and @xmath129 .",
    "we randomly sample @xmath130 from a standard normal distribution .",
    "figure  [ fig : conv ] illustrates how the lower and upper bounds given by the four quadrature rules evolve with the number of iterations .",
    "figure  [ fig : conv ] ( b ) and ( c ) show the sensitivity of the rules ( except gauss quadrature ) to estimating the extremal eigenvalues . specifically , we use @xmath131 and @xmath132 .",
    ".32    .32    .32    the plots in figure  [ fig : conv ] agree with the theoretical results .",
    "first , all quadrature rules are seen to yield iteratively tighter bounds .",
    "the bounds obtained by the gauss - radau quadrature are superior to those given by gauss and gauss - lobatto quadrature ( also numerically verified ) . notably",
    ", the bounds given by all quadrature rules converge very fast  within 25 iterations they yield reasonably tight bounds .",
    "it is valuable to see how the bounds are affected if we do not have good approximations to the extremal eigenvalues @xmath133 and @xmath134 .",
    "since gauss quadrature does not depend on the approximations @xmath135 and @xmath136 , its bounds remain the same in ( a),(b),(c ) .",
    "left gauss - radau depends on the quality of @xmath122 , and , with a poor approximation takes more iterations to converge ( figure  [ fig : conv](b ) ) .",
    "right gauss - radau depends on the quality of @xmath137 ; thus , if we use @xmath138 as our approximation , its bounds become worse ( figure  [ fig : conv](c ) )",
    ". however , its bounds are never worse than those obtained by gauss quadrature .",
    "finally , gauss - lobatto depends on both @xmath122 and @xmath137 , so its bounds become worse whenever we lack good approximations to @xmath133 or @xmath134 . nevertheless , its quality is lower - bounded by left gauss - radau as stated in  thm .",
    "[ thm : upbtwn ] .",
    "our theoretical results show that gauss - radau quadrature provides good lower and upper bounds to bifs .",
    "more importantly , these bounds get iteratively tighter at a linear rate , finally becoming exact  ( see appendix  [ app : sec : proofs ] ) .",
    "however , in many applications motivating our work ( see section  [ sec : motiv.app ] ) , we do not need exact values of bifs ; bounds that are tight enough suffice for the algorithms to proceed . as a result ,",
    "all these applications benefit from our theoretical results that provide iteratively tighter bounds .",
    "this idea translates into a _ retrospective _ framework for accelerating methods whose progress relies on knowing an interval containing the bif . whenever the algorithm takes a step ( _ transition _ ) that depends on a bif ( e.g. , as in the next section , a state transition in a sampler if the bif exceeds a certain threshold ) , we compute rough bounds on its value . if the bounds suffice to take the critical decision ( e.g. , decide the comparison ) , then we stop the quadrature .",
    "if they do not suffice , we take one or more additional iterations of quadrature to tighten the bound .",
    "algorithm  [ algo : framework ] makes this idea explicit .",
    "proceed with the original algorithm retrospectively run one more iteration of left and(or ) right gauss - radau to obtain tighter bounds .",
    "make the correct transition with bounds    we illustrate our framework by accelerating : ( i ) markov chain sampling for ( @xmath26-)dpps ; and ( ii ) maximization of a ( specific ) nonmonotone submodular function .",
    "first , we use our framework to accelerate iterative samplers for determinantal point processes .",
    "specifically , we discuss mh sampling @xcite ; the variant for gibbs sampling follows analogously .",
    "the key insight is that all state transitions of the markov chain rely on a comparison between a scalar @xmath139 and a quantity involving the bilinear inverse form . given the current set @xmath24 , assume we propose to add element @xmath140 to @xmath24 .",
    "the probability of transitioning to state @xmath141 is @xmath142 . to decide whether to accept this transition ,",
    "we sample @xmath143 ; if @xmath144 then we accept the transition , otherwise we remain at @xmath24 .",
    "hence , we need to compute @xmath145 just accurately enough to decide whether @xmath144 .",
    "to do so , we can use the aforementioned lower and upper bounds on @xmath146 .",
    "let @xmath147 and @xmath148 be lower and upper bounds for this bif in the @xmath101-th iteration of gauss quadrature . if @xmath149 , then we can safely accept the transition , if @xmath150 , then we can safely reject the transition .",
    "only if @xmath151 , we can not make a decision yet , and therefore retrospectively perform one more iteration of gauss quadrature to obtain tighter upper and lower bounds @xmath152 and @xmath153 .",
    "we continue until the bounds are sharp enough to safely decide whether to make the transition .",
    "note that in each iteration we make the same decision as we would with the exact value of the bif , and hence the resulting algorithm ( alg .",
    "[ algo : gaussdpp ] ) is an exact markov chain for the dpp . in each iteration",
    ", it calls  alg .",
    "[ algo : dpp_judge ] , which uses step - wise lazy gauss quadrature for deciding the comparison , while stopping as early as possible .",
    "randomly initialize @xmath154 pick @xmath155 , @xmath156 uniformly randomly @xmath157 compute bounds @xmath122 , @xmath137 on the spectrum of @xmath158 @xmath159 @xmath160 compute bounds @xmath122 , @xmath137 on the spectrum of @xmath161 @xmath159    run one gauss - radau iteration to get @xmath162 and @xmath163 for @xmath0 .",
    "* return * * true * * return * * false * @xmath164    if we condition the dppon observing a set of a fixed cardinality @xmath26 , we obtain a @xmath26-dpp .",
    "the mh sampler for this process is similar , but a state transition corresponds to swapping two elements ( adding @xmath140 and removing @xmath5 at the same time ) . assume the current set is @xmath165 . if we propose to delete @xmath5 and add @xmath140 to @xmath28 , then the corresponding transition probability is @xmath166 again , we sample @xmath167 , but now we must compute two quantities , and hence two sets of lower and upper bounds : @xmath168 , @xmath169 for @xmath170 in the @xmath101-th gauss quadrature iteration , and @xmath171 , @xmath172 for @xmath173 in the @xmath174-th gauss quadrature iteration . then if we have @xmath175 , we can safely accept the transition ; and if @xmath176 we can safely reject the transition ; otherwise , we tighten the bounds via additional gauss - radau iterations",
    ".    * refinements .",
    "* we could perform one iteration for both @xmath140 and @xmath5 , but it may be that one set of bounds is already sufficiently tight , while the other is loose .",
    "a straightforward idea would be to judge the tightness of the lower and upper bounds by their difference ( gap ) @xmath177 , and decide accordingly which quadrature to iterate further .",
    "but the bounds for @xmath140 and @xmath5 are not symmetric and contribute differently to the transition decision .",
    "in essence , we need to judge the relation between @xmath139 and @xmath178 , or , equivalently , the relation between @xmath179 and @xmath180 .",
    "since the left hand side is `` easy '' , the essential part is the right hand side . assuming that in practice the impact is larger when the gap is larger , we tighten the bounds for @xmath181 if @xmath182 , and otherwise tighen the bounds for @xmath146 .",
    "details of the final algorithm with this refinement are shown in appendix  [ append : sec : kdppalgo ] .      as indicated in section  [ sec : motiv.app ] , a number of applications , including sensing and information maximization with gaussian processes , rely on maximizing a submodular function given as @xmath183 .",
    "in general , this function may be non - monotone . in this case , an algorithm of choice is the double greedy algorithm of  @xcite .",
    "the double greedy algorithm starts with two sets @xmath184 and @xmath185 and serially iterates through all elements to construct a near - optimal subset . at iteration @xmath101 , it includes element @xmath101 into @xmath186 with probability @xmath187 , and with probability @xmath188 it excludes @xmath101 from @xmath189 .",
    "the decisive value @xmath187 is determined by the marginal gains @xmath190 and @xmath191 :    @xmath192_+ / [ \\delta_i^+]_+ + [ \\delta_i^-]_+ . \\end{aligned}\\ ] ]    for the log - det function , we obtain @xmath193 where @xmath194 . in other words , at iteration @xmath101 the algorithm uniformly samples @xmath156 , and then checks if @xmath195_+ \\le ( 1-p)[\\delta_i^+]_+,\\ ] ] and if true , adds @xmath101 to @xmath186 , otherwise removes it from @xmath189 .",
    "this essential decision , whether to retain or discard an element , again involves bounding bifs , for which we can take advantage of our framework , and profit from the typical sparsity of the data .",
    "concretely , we retrospectively compute the lower and upper bounds on these bifs , i.e. , lower and upper bounds @xmath196 and @xmath197 on @xmath198 , and @xmath199 and @xmath200 on @xmath201 . if @xmath202_+ \\le ( 1-p)[l_i^+]_+$ ] we safely add @xmath101 to @xmath186 ; if @xmath203_+ > ( 1-p ) [ u_i^+]_+$ ] we safely remove @xmath101 from @xmath189 ; otherwise we compute a set of tighter bounds by further iterating the quadrature .    as before , the bounds for @xmath201 and @xmath198 may not contribute equally to the transition decision .",
    "we can again apply the refinement mentioned in section  [ sec : mcdpp ] : if @xmath204_+ - [ l_i^-]_+)\\le ( 1-p)([u_i^+]_+ - [ l_i^+]_+)$ ] we tighten bounds for @xmath198 , otherwise we tighten bounds for @xmath201 .",
    "the resulting algorithm is shown in appendix  [ append : sec : dgalgo ] .",
    ".31 -)dppis initialized with random subsets of size @xmath205 and corresponding running times are averaged over 1,000 iterations of the chain .",
    "all results are averaged over 3 runs.,title=\"fig : \" ]    .31 -)dppis initialized with random subsets of size @xmath205 and corresponding running times are averaged over 1,000 iterations of the chain .",
    "all results are averaged over 3 runs.,title=\"fig : \" ]    .31 -)dppis initialized with random subsets of size @xmath205 and corresponding running times are averaged over 1,000 iterations of the chain .",
    "all results are averaged over 3 runs.,title=\"fig : \" ]    .data .",
    "for all datasets we add an 1e-3 times identity matrix to ensure positive definiteness . [ cols=\"^,^,^,^\",options=\"header \" , ]      we perform experiments on both synthetic and real - world datasets to test the impact of our retrospective quadrature framework in applications .",
    "we focus on ( @xmath26-)dppsampling and the double greedy algorithm for the log - det objective .",
    "we generate small sparse matrices using methods similar to section  [ sec : conv ] . for ( @xmath26-)dppwe generate @xmath206 matrices while for double greedy we use @xmath207 .",
    "we vary the density of the matrices from @xmath208 to @xmath209 . the running time and speedup",
    "are shown in figure  [ fig : syntime ] .",
    "the results suggest that our framework greatly accelerates both dppsampling and submodular maximization .",
    "the speedups are particularly pronounced for sparse matrices .",
    "as the matrices become very sparse , the original algorithms profit from sparsity too , and the difference shrinks a little .",
    "overall , we see that our framework has the potential to lead to substantial speedups for algorithms involving bilinear inverse forms .",
    "we further test our framework on real - world datasets of varying sizes .",
    "we selected 6 datasets , four of them are of small / medium size and two are large .",
    "the four small / medium - sized datasets are used in  @xcite .",
    "the first two of small / medium - sized datasets , abalone and wine , are popular datasets for regression , and we construct sparse kernel matrices with an rbf kernel .",
    "we set the bandwidth parameter for abalone as @xmath210 and that for wine as @xmath211 and the cut - off parameter as @xmath212 for both datasets , as in  @xcite .",
    "the other two small / medium - sized datasets are gr  ( arxiv high energy physics collaboration graph ) and hep  ( arxiv general relativity collaboration graph ) , where the kernel matrices are laplacian matrices .",
    "the final two large datasets datasets are epinions  ( who - trusts - whom network of epinions ) and slashdot  ( slashdot social network from feb .",
    "2009 )   with large laplacian matrices .",
    "dataset statistics are shown in  tab .",
    "[ tab : realdata ] .",
    "the running times in  tab .",
    "[ tab : realspeedup ] suggest that the iterative bounds from quadrature significantly accelerate ( @xmath26-)dppsampling and double greedy on real data .",
    "our algorithms lead to speedups of up to a thousand times .    on the large sparse matrices ,",
    "the `` standard '' double greedy algorithm did not finish within 24 hours , due to the expensive matrix operations involved . with our framework , the algorithm needs only 15 minutes .    to our knowledge , these results are the first time to run dppand double greedy for information gain on such large datasets .",
    "[ [ instability . ] ] instability .",
    "+ + + + + + + + + + + +    as seen in  alg .",
    "[ algo : gql ] , the quadrature algorithm is built upon lanczos iterations .",
    "although in theory lanczos iterations construct a set of orthogonal lanczos vectors , in practice the constructed vectors usually lose orthogonality after some iterations due to rounding errors . one way to deal with this problem is to reorthogonalize the vectors , either completely at each iteration or selectively  @xcite . also , an equivalent lanczos iteration proposed in  @xcite which uses a different expression to improve local orthogonality .",
    "further discussion on numerical stability of the method lies beyond the scope of this paper .",
    "[ [ preconditioning . ] ] preconditioning .",
    "+ + + + + + + + + + + + + + + +    for gauss quadrature on @xmath102 , the convergence rate of bounds is dependent on the condition number of @xmath1 .",
    "we can use preconditioning techniques to get a well - conditioned submatrix and proceed with that . concretely , observe that for non - singular @xmath213 , @xmath214 thus , if @xmath215 is well - conditioned , we can use it with the vector @xmath216 in gauss quadrature .",
    "there exists various ways to obtain good preconditioners for an spd matrix .",
    "a simple choice is to use @xmath217^{-1/2}$ ] .",
    "there also exists methods for efficiently constructing sparse inverse matrix  @xcite .",
    "if @xmath23 happens to be an sdd matrix , we can use techniques introduced in  @xcite to construct an approximate sparse inverse in near linear time .",
    "in this paper we present a general and powerful computational framework for algorithms that rely on computations of bilinear inverse forms .",
    "the framework uses gauss quadrature methods to lazily and iteratively tighten bounds , and is supported by our new theoretical results .",
    "we analyze properties of the various types of gauss quadratures for approximating the bilinear inverse forms and show that all bounds are monotonically becoming tighter with the number of iterations ; those given by gauss - radau are superior to those obtained from other gauss - type quadratures ; and both lower and upper bounds enjoy a linear convergence rate .",
    "we empirically verify the efficiency of our framework and are able to obtain speedups of up to a thousand times for two popular examples : maximizing information gain and sampling from determinantal point processes .",
    "[ [ acknowledgements ] ] acknowledgements + + + + + + + + + + + + + + + +    this research was partially supported by nsf career award 1553284 and a google research award .",
    "62 [ 1]#1 [ 1]`#1 ` urlstyle [ 1]doi : # 1    anari , nima , gharan , shayan  oveis , and rezaei , alireza .",
    "onte carlo markov chain algorithms for sampling strongly rayleigh distributions and determinantal point processes . in _ colt _ , 2016 .",
    "atzori , luigi , iera , antonio , and morabito , giacomo .",
    "the internet of things : a survey . _ computer networks _ , 540 ( 15):0 27872805 , 2010 .",
    "bai , zhaojun and golub , gene  h. bounds for the trace of the inverse and the determinant of symmetric positive definite matrices .",
    "_ annals of numerical mathematics _ , pp .   2938 , 1996 .",
    "bai , zhaojun , fahey , gark , and golub , gene  h. some large - scale matrix computation problems .",
    "_ journal of computational and applied mathematics _ , pp . 7189 , 1996 .",
    "bekas , constantine , kokiopoulou , effrosyni , and saad , yousef .",
    "an estimator for the diagonal of a matrix .",
    "_ applied numerical mathematics _ , pp .   12141229 , 2007 .",
    "bekas , constantine , curioni , alessandro , and fedulova , irina .",
    "low cost high performance uncertainty quantification . in _ proceedings of the 2nd workshop on high performance computational finance _ , 2009 .",
    "belabbas , mohamed - ali and wolfe , patrick  j. spectral methods in machine learning and new strategies for very large datasets .",
    "_ proceedings of the national academy of sciences _ , pp . 369374 , 2009 .",
    "benzi , michele and golub , gene  h. bounds for the entries of matrix functions with applications to preconditioning .",
    "_ bit numerical mathematics _ , pp .   417438 , 1999 .",
    "benzi , michele and klymko , christine .",
    "total communicability as a centrality measure .",
    "_ j. complex networks _ , pp .",
    "124149 , 2013 .",
    "bonacich , phillip .",
    "power and centrality : a family of measures",
    ". _ american journal of sociology _ , pp .   11701182 , 1987 .",
    "boutsidis , christos , mahoney , michael  w. , and drineas , petros .",
    "an improved approximation algorithm for the column subset selection problem . in _ soda _ , pp .   968977 , 2009 .",
    "brezinski , claude .",
    "error estimates for the solution of linear systems .",
    "_ siam journal on scientific computing _ , pp .   764781 , 1999 .",
    "brezinski , claude , fika , paraskevi , and mitrouli , marilena .",
    "estimations of the trace of powers of positive self - adjoint operators by extrapolation of the moments .",
    "_ electronic transactions on numerical analysis _ , pp .",
    "144155 , 2012 .",
    "buchbinder , niv , feldman , moran , naor , joseph , and schwartz , roy . a tight linear time",
    "( 1/2)-approximation for unconstrained submodular maximization . in _ focs _ , 2012 .",
    "dong , shao - jing and liu , keh - fei .",
    "stochastic estimation with @xmath218 noise",
    ". _ physics letters b _ , pp .   130136 , 1994 .",
    "estrada , ernesto and higham , desmond  j. network properties revealed through matrix functions .",
    "_ siam review _ , pp .   696714 , 2010 .",
    "fenu , caterina , martin , david  r. , reichel , lothar , and rodriguez , giuseppe .",
    "network analysis via partial spectral factorization and gauss quadrature .",
    "_ siam journal on scientific computing _ , pp .",
    "a2046a2068 , 2013 .",
    "fika , paraskevi and koukouvinos , christos .",
    "stochastic estimates for the trace of functions of matrices via hadamard matrices .",
    "_ communications in statistics - simulation and computation _ , 2015 .",
    "fika , paraskevi and mitrouli , marilena .",
    "estimation of the bilinear form @xmath219 for hermitian matrices",
    ". _ linear algebra and its applications _ , 2015 .",
    "fika , paraskevi , mitrouli , marilena , and roupa , paraskevi .",
    "estimates for the bilinear form @xmath220 with applications to linear algebra problems . _",
    "electronic transactions on numerical analysis _ , pp .   7089 , 2014 .",
    "freericks , james  k. transport in multilayered nanostructures . _ the dynamical mean - field theory approach , imperial college , london _ , 2006 .",
    "frommer , andreas , lippert , thomas , medeke , bjrn , and schilling , klaus . _ numerical challenges in lattice quantum chromodynamics : joint interdisciplinary workshop of john von neumann institute for computing , jlich , and institute of applied computer science , wuppertal university , august 1999 _ , volume  15 .",
    "springer science & business media , 2012 .",
    "gauss , carl  f. _ methodus nova integralium valores per approximationem inveniendi_. apvd henricvm dieterich , 1815 .",
    "gautschi , walter .",
    "a survey of gauss - christoffel quadrature formulae . in _",
    "eb christoffel _ , pp .",
    "springer , 1981 .",
    "gillenwater , jennifer , kulesza , alex , and taskar , ben .",
    "near - optimal map inference for determinantal point processes . in _ nips _",
    ", 2012 .",
    "gittens , alex and mahoney , michael  w. revisiting the nystrm method for improved large - scale machine learning .",
    "_ icml _ , 2013 .",
    "golub , gene  h. some modified matrix eigenvalue problems .",
    "_ siam review _ , pp .   318334 , 1973 .",
    "golub , gene  h. and meurant , grard .",
    "matrices , moments and quadrature ii ; how to compute the norm of the error in iterative methods .",
    "_ bit numerical mathematics _ , pp .",
    "687705 , 1997 .",
    "golub , gene  h. and meurant , grard .",
    "_ matrices , moments and quadrature with applications_. princeton university press , 2009 .",
    "golub , gene  h. and welsch , john  h. calculation of gauss quadrature rules . _ mathematics of computation _ , pp .   221230 , 1969 .",
    "golub , gene  h. , stoll , martin , and wathen , andy .",
    "approximation of the scattering amplitude and linear systems .",
    "_ elec . tran . on numerical analysis _ , pp .   178203 , 2008 .",
    "hestenes , magnus  r. and stiefel , eduard",
    ". methods of conjugate gradients for solving linear systems .",
    "_ j. research of the national bureau of standards _ , pp .",
    "409436 , 1952 .",
    "hough , j.  ben , krishnapur , manjunath , peres , yuval , and virg , blint .",
    "determinantal processes and independence .",
    "_ probability surveys _ , 2006 .",
    "kang , byungkon .",
    "fast determinantal point process sampling with application to clustering . in _",
    "nips _ , pp .   23192327 , 2013 .",
    "krause , andreas , singh , ajit , and guestrin , carlos .",
    "near - optimal sensor placements in gaussian processes : theory , efficient algorithms and empirical studies .",
    "_ jmlr _ , pp .   235284 , 2008 .",
    "kulesza , alex and taskar , ben .",
    "determinantal point processes for machine learning .",
    "_ arxiv:1207.6083 _ , 2012 .",
    "kwok , james  t. and adams , ryan  p. priors for diversity in generative latent variable models . in _ nips _ , pp .   29963004 , 2012 .",
    "lanczos , cornelius .",
    "_ an iteration method for the solution of the eigenvalue problem of linear differential and integral operators_. united states governm .",
    "press office los angeles , ca , 1950 .",
    "lee , christina  e. , ozdaglar , asuman  e. , and shah , devavrat . solving systems of linear equations : locally and asynchronously .",
    "_ arxiv _ , abs/1411.2647 , 2014 .",
    "leskovec , jure , lang , kevin  j. , dasgupta , anirban , and mahoney , michael  w. statistical properties of community structure in large social and information networks . in _ www _ , pp .   695704 , 2008 .",
    "lin , lin , yang , chao , lu , jianfeng , and ying , lexing . a fast parallel algorithm for selected inversion of structured sparse matrices with application to 2d electronic structure calculations .",
    "_ siam journal on scientific computing _ , pp .   13291351 , 2011 .",
    "lin , lin , yang , chao , meza , juan  c. , lu , jianfeng , ying , lexing , and e , weinan .",
    "an algorithm for selected inversion of a sparse symmetric matrix .",
    "_ acm transactions on mathematical software _ , 2011 .",
    "lobatto , rehuel .",
    "_ lessen over de differentiaal - en integraal - rekening : dl . 2 integraal - rekening _ , volume  1 .",
    "van cleef , 1852 .",
    "meurant , grard .",
    "the computation of bounds for the norm of the error in the conjugate gradient algorithm .",
    "_ numerical algorithms _ , pp .   7787 , 1997 .",
    "meurant , grard .",
    "numerical experiments in computing bounds for the norm of the error in the preconditioned conjugate gradient algorithm . _ numerical algorithms _ , pp .   353365 , 1999 .",
    "meurant , grard . _",
    "the lanczos and conjugate gradient algorithms : from theory to finite precision computations _ ,",
    "volume  19 .",
    "siam , 2006 .",
    "minoux , michel . accelerated greedy algorithms for maximizing submodular set functions . in _",
    "optimization techniques _ , pp .   234243 .",
    "springer , 1978 .",
    "mirzasoleiman , baharan , badanidiyuru , ashwinkumar , karbasi , amin , vondrk , jan , and krause , andreas .",
    "lazier than lazy greedy . in _ aaai _ , 2015 .",
    "nemhauser , george  l .. , wolsey , laurence  a. , and fisher , marshall  l. an analysis of approximations for maximizing submodular set functions ",
    "_ mathematical programming _ , pp .",
    "265294 , 1978 .    page , lawrence , brin , sergey , motwani , rajeev , and winograd , terry .",
    "_ the pagerank citation ranking : bringing order to the web .",
    "_ stanford infolab , 1999 .",
    "radau , rodolphe .",
    "tude sur les formules dapproximation qui servent  calculer la valeur numrique dune intgrale dfinie .",
    "_ j. de mathmatiques pures et appliques _ , pp . 283336 , 1880 .",
    "rasmussen , carl  e. and williams , christopher k.  i. _ gaussian processes for machine learning_. mit press , cambridge , ma , 2006 .",
    "rockov , veronika and george , edward  i. determinantal priors for variable selection , 2015 .",
    "scott , john .",
    "_ social network analysis_. sage , 2012 .",
    "sherman , jack and morrison , winifred  j. adjustment of an inverse matrix corresponding to a change in one element of a given matrix . _ the annals of mathematical statistics _ , pp .   124127 , 1950 .",
    "shewchuk , jonathan  r. an introduction to the conjugate gradient method without the agonizing pain , 1994 .",
    "sidje , roger  b. and saad , yousef .",
    "rational approximation to the fermi ",
    "dirac function with applications in density functional theory .",
    "_ numerical algorithms _ , pp .",
    "455479 , 2011 .",
    "stoer , josef and bulirsch , roland . _ introduction to numerical analysis _ ,",
    "volume  12 .",
    "springer science & business media , 2013 .",
    "sviridenko , maxim , vondrk , jan , and ward , justin .",
    "optimal approximation for submodular and supermodular optimization with bounded curvature . in _ soda _ , 2015 .",
    "tang , jok  m. and saad , yousef .",
    "a probing method for computing the diagonal of a matrix inverse .",
    "_ numerical linear algebra with applications _ , pp .",
    "485501 , 2012 .",
    "wasow , wolfgang  r. a note on the inversion of matrices by random walks .",
    "_ mathematical tables and other aids to computation _ , pp .",
    "7881 , 1952 .",
    "wilf , herbert  s. _ mathematics for the physical sciences_. wiley , new york , 1962 .",
    "we present below a more detailed summary of material on gauss quadrature to make the paper self - contained .",
    "we ve described that the riemann - stieltjes integral could be expressed as @xmath221 : = q_{n } + r_{n } = { \\sum\\nolimits}_{i=1}^n \\omega_i f(\\theta_i ) + { \\sum\\nolimits}_{i=1}^m \\nu_i f(\\tau_i ) + r_{n}[f],\\ ] ] where @xmath68 denotes the @xmath69th degree approximation and @xmath70 denotes a remainder term .",
    "the weights @xmath71 , @xmath72 and nodes @xmath73 are chosen such that for all polynomials of degree less than @xmath83 , denoted @xmath222 , we have _ exact _ interpolation @xmath84 = q_{n}$ ] .",
    "one way to compute weights and nodes is to set @xmath223 for @xmath224 and then use this exact nonlinear system .",
    "but there is an easier way to obtain weights and nodes , namely by using polynomials orthogonal with respect to the measure @xmath225 .",
    "specifically , we construct a sequence of _ orthogonal polynomials _",
    "@xmath226 such that @xmath227 is a polynomial in @xmath228 of degree exactly @xmath26 , and @xmath229 , @xmath230 are orthogonal , i.e. , they satisfy @xmath231 the roots of @xmath232 are distinct , real and lie in the interval of @xmath233 $ ] , and form the nodes @xmath87 for gauss quadrature  ( see , e.g. ,  ( * ? ? ?",
    "* ch .  6 ) ) .",
    "consider the two _ monic polynomials _ whose roots serve as quadrature nodes : @xmath234 where @xmath235 for consistency .",
    "we further denote @xmath236 , where the sign is taken to ensure @xmath237 on @xmath238 $ ] .",
    "then , for @xmath239 , we calculate the quadrature weights as @xmath240 , \\quad    \\nu_j = i\\biggl[{\\rho_m^+(\\lambda)\\pi_n(\\lambda ) \\over ( \\rho_m^+)'(\\tau_j)\\pi_n(\\tau_j)(\\lambda - \\tau_j)}\\biggr],\\end{aligned}\\ ] ] where @xmath241 denotes the derivative of @xmath4 with respect to @xmath228 .",
    "when @xmath76 the quadrature degenerates to gauss quadrature and we have @xmath242.\\end{aligned}\\ ] ]    although we have specified how to select nodes and weights for quadrature , these ideas can not be applied to our problem because the measure @xmath225 is unknown . indeed ,",
    "calculating the measure explicitly would require knowing the entire spectrum of @xmath1 , which is as good as explicitly computing @xmath243 , hence untenable for us .",
    "the next section shows how to circumvent the difficulties due to unknown @xmath225 .",
    "the key idea to circumvent our lack of knowledge of @xmath225 is to recursively construct polynomials called _",
    "lanczos polynomials_. the construction ensures their orthogonality with respect to @xmath225 .",
    "concretely , we construct lanczos polynomials via the following three - term recurrence : @xmath244 while ensuring @xmath245",
    ". we can express   in matrix form by writing @xmath246 where @xmath247^\\top$ ] , @xmath248 is @xmath69th canonical unit vector , and @xmath86 is the tridiagonal matrix @xmath249 this matrix is known as the _ jacobi matrix _ , and is closed related to gauss quadrature .",
    "the following well - known theorem makes this relation precise .",
    "[ append : thm : gauss ] the eigenvalues of @xmath86 form the nodes @xmath87 of gauss - type quadratures .",
    "the weights @xmath88 are given by the squares of the first elements of the normalized eigenvectors of @xmath86 .",
    "thus , if @xmath86 has the eigendecomposition @xmath250 , then for gauss  quadrature thm .",
    "[ append : thm : gauss ] yields @xmath251    [ [ specialization . ] ] specialization .",
    "+ + + + + + + + + + + + + + +    we now specialize to our main focus , @xmath6 , for which we prove more precise results . in this case , becomes @xmath252_{1,1}$ ] .",
    "the task now is to compute @xmath68 , and given @xmath1 , @xmath2 to obtain the jacobi matrix @xmath86 .",
    "fortunately , we can efficiently calculate @xmath86 iteratively using the _ lanczos algorithm _  @xcite .",
    "suppose we have an estimate @xmath106 , in iteration @xmath253 of lanczos , we compute the tridiagonal coefficients @xmath254 and @xmath255 and add them to this estimate to form @xmath256 . as to @xmath68 , assuming we have already computed @xmath257_{1,1}$ ] , letting @xmath258 and invoking the sherman - morrison identity  @xcite we obtain the recursion : @xmath259_{1,1 } = [ j_i^{-1}]_{1,1 } + \\frac{\\beta_i^2 ( [ j_i]_{1})^2}{\\alpha_{i+1 } - \\beta_i^2 [ j_i]_i},\\end{aligned}\\ ] ] where @xmath260_1 $ ] and @xmath260_i$ ] can be recursively computed using a cholesky - like factorization of @xmath106  @xcite .    for gauss - radau quadrature , we need to modify @xmath106 so that it has a prescribed eigenvalue . more precisely , we extend @xmath106 to @xmath107 for left gauss - radau  ( @xmath108 for right gauss - radau ) with @xmath261 on the off - diagonal and @xmath262  ( @xmath263 ) on the diagonal , so that @xmath107  ( @xmath108 ) has a prescribed eigenvalue of @xmath122  ( @xmath137 ) .    for gauss - lobatto quadrature ,",
    "we extend @xmath106 to @xmath109 with values @xmath264 and @xmath265 chosen to ensure that @xmath109 has the prescribed eigenvalues @xmath122 and @xmath137 . for more detailed on the construction , see  @xcite .    for all methods ,",
    "the approximated values are calculated as @xmath266_{1,1}$ ] , where @xmath267 is the modified jacobi matrix . here",
    "@xmath268 is constructed at the @xmath101-th iteration of the algorithm .",
    "the algorithm for computing gauss , gauss - radau , and gauss - lobatto quadrature rules with the help of lanczos iteration is called _ gauss quadrature lanczos _ ( gql ) and is shown in  @xcite .",
    "we recall its pseudocode in  alg .",
    "[ algo : gql ] to make our presentation self - contained ( and for our proofs in section  [ sec : main ] ) .",
    "* initialize * : @xmath269 , @xmath103 , @xmath270 , @xmath271 , @xmath272 , @xmath273 , @xmath274 , @xmath275 , @xmath276 , @xmath277 , @xmath105 @xmath278 @xmath279 @xmath280 @xmath281 @xmath282 @xmath283 @xmath284 , @xmath285 , @xmath286 @xmath287 , @xmath288 @xmath289 , @xmath290 @xmath291 , @xmath292 , @xmath293 @xmath164    the error of approximating @xmath84 $ ] by gauss - type quadratures can be expressed as @xmath294 = { f^{(2n+m)}(\\xi)\\over ( 2n+m ) ! } i[\\rho_m\\pi_n^2],\\ ] ] for some @xmath295 $ ]  ( see , e.g. ,  @xcite ) .",
    "note that @xmath296 does not change sign in @xmath238 $ ] ; but with different values of @xmath82 and @xmath297 we obtain different ( but fixed ) signs for @xmath298 $ ] using @xmath299 and @xmath300 . concretely , for gauss quadrature @xmath76 and @xmath298 \\ge 0 $ ] ; for left gauss - radau @xmath77 and @xmath78 , so we have @xmath298 \\le 0 $ ] ; for right gauss - radau we have @xmath77 and @xmath79 , thus @xmath298 \\ge 0 $ ] ; while for gauss - lobatto we have @xmath80 , @xmath78 and @xmath81 , so that @xmath298 \\le 0 $ ] .",
    "this behavior of the errors clearly shows the ordering relations between the target values and the approximations made by the different quadrature rules .",
    "lemma  [ lem : bounds ] ( see e.g. , @xcite ) makes this claim precise .",
    "[ append : lem : bounds ] let @xmath97 , @xmath98 , @xmath99 , and @xmath100 be the approximations at the @xmath101-th iteration of gauss , left gauss - radau , right gauss - radau , and gauss - lobatto quadrature , respectively . then",
    ", @xmath97 and @xmath99 provide lower bounds on @xmath102 , while @xmath98 and @xmath100 provide upper bounds .",
    "the final connection we recall as background is the method of conjugate gradients .",
    "this helps us analyze the speed at which quadrature converges to the true value ( assuming exact arithmetic ) .",
    "while gauss - type quadratures relate to the lanczos algorithm , lanczos itself is closely related to conjugate gradient ( cg )  @xcite , a well - known method for solving @xmath301 for positive definite @xmath1 .",
    "we recap this connection below .",
    "let @xmath302 be the estimated solution at the @xmath26-th cg iteration . if @xmath303 denotes the true solution to @xmath301 , then the _ error _ @xmath304 and _ residual _ @xmath305 are defined as @xmath306 at the @xmath26-th iteration , @xmath302 is chosen such that @xmath305 is orthogonal to the @xmath26-th _ krylov space _ , i.e. , the linear space @xmath307 spanned by @xmath308 .",
    "it can be shown  @xcite that @xmath305 is a scaled lanczos vector from the @xmath26-th iteration of lanczos started with @xmath309 . noting the relation between lanczos and gauss quadrature applied to appoximate @xmath310 , one obtains the following theorem that relates cg with gql .",
    "[ append : thm : relation ] let @xmath304 be the error as in  , and let @xmath311 .",
    "then , it holds that @xmath312_{1,1 } - [ j_k^{-1}]_{1,1}),\\end{aligned}\\ ] ] where @xmath313 is the jacobi matrix at the @xmath26-th lanczos iteration starting with @xmath309 .    finally , the rate at which @xmath314 shrinks has also been well - studied , as noted below .",
    "[ append : thm : cgconv ] let @xmath304 be the error made by cg at iteration @xmath26 when started with @xmath315 .",
    "let @xmath316 be the condition number of @xmath1 , i.e. , @xmath317 .",
    "then , the error norm at iteration @xmath26 satisfies @xmath318    due to these explicit relations between cg and lanczos , as well as between lanczos and gauss quadrature , we readily obtain the following convergence rate for relative error of gauss quadrature .",
    "[ append : thm : gaussconv ] the @xmath101-th iterate of gauss quadrature satisfies the relative error bound @xmath319    this is obtained by exploiting relations among cg , lanczos and gauss quadrature .",
    "set @xmath320 and @xmath321 .",
    "then , @xmath322 and @xmath323 .",
    "an application of  thm .",
    "[ append : thm : relation ] and thm .",
    "[ append : thm : cgconv ] thus yields the bound @xmath324_{1,1 } - [ j_i^{-1}]_{1,1 } ) = g_n - g_i\\\\ & \\le\\quad2\\bigl(\\frac{\\sqrt{\\kappa } - 1}{\\sqrt{\\kappa } + 1}\\bigr)^i\\|{\\varepsilon}_0\\|_a = 2\\bigl(\\frac{\\sqrt{\\kappa } - 1}{\\sqrt{\\kappa } + 1}\\bigr)^i u^\\top a^{-1 } u = 2\\bigl(\\frac{\\sqrt{\\kappa } - 1}{\\sqrt{\\kappa } + 1}\\bigr)^i g_n\\end{aligned}\\ ] ] where the last equality draws from  lemma  [ append : lem : exact ] .    in other words , thm .",
    "[ append : thm : gaussconv ] shows that the iterates of gauss quadrature converge linearly .",
    "we begin by proving an exactness property of gauss and gauss - radau quadrature .",
    "[ append : lem : exact ] with @xmath1 being symmetric positive definite with simple eigenvalues , the iterates @xmath325 , @xmath326 , and @xmath327 are exact .",
    "namely , after @xmath27 iterations they satisfy @xmath328    observe that the jacobi tridiagonal matrix can be computed via lanczos iteration , and lanczos is essentially essentially an iterative tridiagonalization of @xmath1 . at the @xmath101-th iteration we have @xmath329 , where @xmath330 are the first @xmath101 lanczos vectors  ( i.e. , a basis for the @xmath101-th krylov space ) .",
    "thus , @xmath331 where @xmath332 is an @xmath53 orthonormal matrix , showing that @xmath333 has the same eigenvalues as @xmath1 . as a result @xmath334 , and it follows that the remainder @xmath335 = { f^{(2n)}(\\xi)\\over ( 2n ) ! } i[\\pi_n^2 ] = 0,\\ ] ] for some scalar @xmath295 $ ] , which shows that @xmath325 is exact for @xmath102 . for left and right gauss - radau quadrature",
    ", we have @xmath336 , @xmath337 , and @xmath338 , while all other elements of the @xmath339-th row or column of @xmath340 are zeros .",
    "thus , the eigenvalues of @xmath340 are @xmath341 , and @xmath342 again equals @xmath343 . as a result , the remainder satisfies @xmath335 = { f^{(2n)}(\\xi)\\over ( 2n ) ! }",
    "i[(\\lambda - \\tau_1)\\pi_n^2 ] = 0,\\ ] ] from which it follows that both @xmath327 and @xmath326 are exact .    the convergence rate in  thm .",
    "[ append : thm : cgconv ] and the final exactness of iterations in  lemma  [ append : lem : exact ] does not necessarily indicate that we are making progress at each iterations .",
    "however , by exploiting the relations to cg we can indeed conclude that we are making progress in each iteration in gauss quadrature .",
    "[ append : thm : monogauss ] the approximation @xmath97 generated by gauss quadrature is monotonically nondecreasing , i.e. , @xmath344    at each iteration @xmath345 is taken to be orthogonal to the @xmath101-th krylov space : @xmath346 .",
    "let @xmath347 be the projection onto the complement space of @xmath348 .",
    "the residual then satisfies @xmath349 where the last inequality follows from @xmath350 .",
    "thus @xmath351 is monotonically nonincreasing , whereby @xmath352 is monotonically decreasing and thus @xmath97 is monotonically nondecreasing .    before we proceed to gauss - radau ,",
    "let us recall a useful theorem and its corollary .",
    "[ append : thm : lanczospoly ] let @xmath353 be the vector generated by  alg .",
    "[ algo : gql ] at the @xmath101-th iteration ; let @xmath229 be the lanczos polynomial of degree @xmath101",
    ". then we have @xmath354    from the expression of lanczos polynomial we have the following corollary specifying the sign of the polynomial at specific points .",
    "assume @xmath355 .",
    "if @xmath101 is odd , then @xmath356 ; for even @xmath101 , @xmath357 , while @xmath358 for any @xmath112 .",
    "since @xmath329 is similar to @xmath1 , its spectrum is bounded by @xmath122 and @xmath137 from left and right .",
    "thus , @xmath359 is positive semi - definite , and @xmath360 is negative semi - definite . taking @xmath361 into consideration",
    "we will get the desired conclusions .",
    "we are ready to state our main result that compares ( right ) gauss - radau with gauss quadrature .",
    "[ append : thm : lowbtwn ] let @xmath112 .",
    "then , @xmath99 gives better bounds than @xmath97 but worse bounds than @xmath113 ; more precisely , @xmath362    we prove inequality   using the recurrences satisfied by @xmath97 and @xmath99 ( see alg .",
    "[ algo : gql ] )    _ upper bound : @xmath363 .",
    "_ the iterative quadrature algorithm uses the recursive updates @xmath364 it suffices to thus compare @xmath263 and @xmath254 .",
    "the three - term recursion for lanczos polynomials shows that @xmath365 where @xmath366 is the original lanczos polynomial , and @xmath367 is the modified polynomial that has @xmath137 as a root . noting that @xmath368",
    ", we see that @xmath369 . moreover , from  thm .",
    "[ append : thm : monogauss ] we know that the @xmath97 s are monotonically increasing , whereby @xmath370 .",
    "it follows that @xmath371 and from this inequality it is clear that @xmath363 .",
    "_ lower - bound : @xmath372 . _ since @xmath373 and @xmath374 , we readily obtain @xmath375    combining  thm .  [",
    "append : thm : lowbtwn ] with the convergence rate of relative error for gauss quadrature ( thm .",
    "[ append : thm : gaussconv ] ) immediately yields the following convergence rate for right gauss - radau quadrature :    [ append : thm : rrconv ] for each @xmath101 , the right gauss - radau @xmath99 iterates satisfy @xmath115    this results shows that with the same number of iterations , right gauss - radau gives superior approximation over gauss quadrature , though they share the same relative error convergence rate .",
    "our second main result compares gauss - lobatto with ( left ) gauss - radau quadrature .",
    "[ append : thm : upbtwn ] let @xmath112 . then , @xmath98 gives better upper bounds than @xmath116 but worse than @xmath117 ; more precisely , @xmath118    we prove these inequalities using the recurrences for @xmath98 and @xmath100 from  alg .",
    "[ append : algo : gql ] .    _",
    "@xmath376 _ : from  alg .",
    "[ append : algo : gql ] we observe that @xmath377",
    ". thus we can write @xmath98 and @xmath100 as @xmath378 to compare these quantities , as before it is helpful to begin with the original three - term recursion for the lanczos polynomial , namely @xmath379 in the construction of gauss - lobatto , to make a new polynomial of order @xmath380 that has roots @xmath122 and @xmath137 , we add @xmath381 and @xmath382 to the original polynomial to ensure @xmath383 since @xmath255 , @xmath384 , @xmath385 and @xmath386 are all greater than @xmath387 , @xmath388 . to determine the sign of polynomials at @xmath122 ,",
    "consider the two cases :    1 .",
    "odd @xmath101 . in this case",
    "@xmath389 , @xmath356 , and @xmath390 ; 2 .",
    "even @xmath101 . in this case",
    "@xmath391 , @xmath392 , and @xmath393 .",
    "thus , if @xmath394 , where the signs take values in @xmath395 , then @xmath396 , @xmath397 and @xmath398 .",
    "hence , @xmath399 must hold , and thus @xmath400 given that @xmath401 for @xmath112 .    using @xmath402 with @xmath403 , an application of monotonicity of the univariate function @xmath404 for @xmath405 to the recurrences defining @xmath98 and @xmath100 yields the desired inequality @xmath406 .    _",
    "@xmath407 _ : from recursion formulas we have @xmath408 establishing @xmath409 thus amounts to showing that ( noting the relations among @xmath97 , @xmath98 and @xmath100 ) : @xmath410 where the last inequality is obviously true ; hence the proof is complete .    in summary",
    ", we have the following corollary for all the four quadrature rules :    [ append : cor : monlow ] as the iteration proceeds , @xmath97 and @xmath99 gives increasingly better asymptotic lower bounds and @xmath98 and @xmath100 gives increasingly better upper bounds , namely @xmath411    directly drawn from  thm .",
    "[ append : thm : monogauss ] ,  thm .",
    "[ append : thm : lowbtwn ] and  thm .",
    "[ append : thm : upbtwn ] .    before proceeding further to our analysis of convergence rates of left gauss - radau and gauss - lobatto ,",
    "we note two technical results that we will need .",
    "[ append : lem : delta ] let @xmath254 and @xmath262 be as in alg .",
    "[ algo : gql ] .",
    "the difference @xmath412 satisfies @xmath413 .    from the lanczos polynomials in the definition of left gauss - radau quadrature we have",
    "@xmath414 rearrange this equation to write @xmath415 , which can be further rewritten as @xmath416    lemma  [ append : lem : delta ] has an implication beyond its utility for the subsequent proofs : it provides a new way of calculating @xmath254 given the quantities @xmath417 and @xmath262 ; this saves calculation in  alg .",
    "[ append : algo : gql ] .",
    "the following lemma relates @xmath418 to @xmath419 , which will prove useful in subsequent analysis .",
    "[ append : lem : comp ] let @xmath419 and @xmath418 be computed in the @xmath101-th iteration of alg .",
    "[ algo : gql ] .",
    "then , we have the following : @xmath420    we prove   by induction . since @xmath300 , @xmath421 and @xmath422 we know that @xmath423 .",
    "assume that @xmath424 is true for all @xmath425 and considering the @xmath426-th iteration : @xmath427    to prove , simply observe the following @xmath428    with aforementioned lemmas we will be able to show how fast the difference between @xmath98 and @xmath97 decays .",
    "note that @xmath98 gives an upper bound on the objective while @xmath97 gives a lower bound .",
    "[ append : lem : diffconv ] the difference between @xmath98 and @xmath97 decreases linearly .",
    "more specifically we have @xmath429 where @xmath430 and @xmath316 is the condition number of @xmath1 , i.e. , @xmath431 .",
    "we rewrite the difference @xmath432 as follows @xmath433 where @xmath412 .",
    "next , recall that @xmath434 . since @xmath97 lower bounds @xmath325 , we have @xmath435 thus , we can conclude that @xmath436 now we focus on the term @xmath437 . using  lemma  [ append : lem :",
    "delta ] we know that @xmath413 .",
    "hence , @xmath438 finally we have @xmath439    [ append : thm : lrconv ] for left gauss - radau quadrature where the preassigned node is @xmath122 , we have the following bound on relative error : @xmath120 where @xmath121 .",
    "write @xmath440 .",
    "since @xmath441 , using lemma [ append : lem : diffconv ] to bound the second term we obtain @xmath442 from which the claim follows upon rearrangement .    due to the relations between left gauss - radau and gauss - lobatto",
    ", we have the following corollary :    [ append : cor : loconv ] for gauss - lobatto quadrature , we have the following bound on relative error : @xmath443 where @xmath124 .",
    "in this section we consider the case where @xmath2 lies in the column space of several top eigenvectors of @xmath1 , and discuss how the aforementioned theorems vary . in particular , note that the previous analysis assumes that @xmath1 is positive definite . with our analysis in this section",
    "we relax this assumption to the more general case where @xmath1 is symmetric with simple eigenvalues , though we require @xmath2 to lie in the space spanned by eigenvectors of @xmath1 corresponding to positive eigenvalues .",
    "we consider the case where @xmath1 is symmetric and has the eigendecomposition of @xmath444 where @xmath445 s are eigenvalues of @xmath1 increasing with @xmath101 and @xmath187 s are corresponding eigenvectors .",
    "assume that @xmath2 lies in the column space spanned by top @xmath26 eigenvectors of @xmath1 where all these @xmath26 eigenvectors correspond to positive eigenvalues .",
    "namely we have @xmath446 and @xmath447 .    since we only assume that @xmath1 is symmetric , it is possible that @xmath1 is singular and thus we consider the value of @xmath448 , where @xmath449 is the pseudo - inverse of @xmath1 . due to the constraints on @xmath2 we have @xmath450 where @xmath451 .",
    "namely , if @xmath2 lies in the column space spanned by the top @xmath26 eigenvectors of @xmath1 then it is equivalent to substitute @xmath1 with @xmath452 , which is the truncated version of @xmath1 at top @xmath26 eigenvalues and corresponding eigenvectors .",
    "another key observation is that , given that @xmath2 lies only in the space spanned by @xmath453 , the krylov space starting at @xmath2 becomes @xmath454 this indicates that lanczos iteration starting at matrix @xmath1 and vector @xmath2 will finish constructing the corresponding krylov space after the @xmath26-th iteration .",
    "thus under this condition , alg .",
    "[ algo : gql ] will run at most @xmath26 iterations and then stop . at that time",
    ", the eigenvalues of @xmath313 are exactly the eigenvalues of @xmath452 , thus they are exactly @xmath455 of @xmath1 . using similar proof as in  lemma  [ append : lem : exact ] , we can obtain the following generalized exactness result .",
    "@xmath456 , @xmath457 and @xmath458 are exact for @xmath459 , namely @xmath460    the monotonicity and the relations between bounds given by various gauss - type quadratures will still be the same as in the original case in  section  [ sec : main ] , but the original convergence rate can not apply in this case because now we probably have @xmath461 , making @xmath316 undefined .",
    "this crash of convergence rate results from the crash of the convergence of the corresponding conjugate gradient algorithm for solving @xmath462 .",
    "however , by looking at the proof of , e.g. ,  @xcite , and by noting that @xmath463 , with a slight modification of the proof we actually obtain the bound @xmath464 ^ 2 \\|{\\varepsilon}^0\\|_a^2,\\end{aligned}\\ ] ] where @xmath465 is a polynomial of order @xmath101 . by using properties of chebyshev polynomials and following the original proof  ( e.g. ,  @xcite or  @xcite ) we obtain the following lemma for conjugate gradient .",
    "let @xmath466 be as before ( for conjugate gradient ) .",
    "then , @xmath467    following this new convergence rate and connections between conjugate gradient , lanczos iterations and gauss quadrature mentioned in section  [ sec : main ] , we have the following convergence bounds .",
    "[ append : cor : spec_conv ] under the above assumptions on @xmath1 and @xmath2 , due to the connection between gauss quadrature , lanczos algorithm and conjugate gradient , the relative convergence rates of @xmath97 , @xmath99 , @xmath98 and @xmath100 are given by @xmath468 where @xmath469 and @xmath470 is a lowerbound for nonzero eigenvalues of @xmath452 .",
    "we present the details of the function dpp - judgegauss(@xmath471 )  ( mentioned in section  [ sec : mcdpp ] ) in  alg .",
    "[ append : algo : dpp_judge ] .",
    "@xmath269 , @xmath103 , @xmath472 , @xmath473 , @xmath474 @xmath475 , @xmath476 @xmath477 , @xmath478 , @xmath479 @xmath480 , @xmath481 @xmath482 , @xmath483 , @xmath484 @xmath485 @xmath486 , @xmath487 @xmath488 , @xmath489 @xmath490 , @xmath491 return _ true _ return _ false _ @xmath492 , @xmath164",
    "we present details of a _ retrospective markov chain monte carlo ( mcmc ) _ in  alg .",
    "[ append : algo : gausskdpp ] and  alg .",
    "[ append : algo : kdpp_judge ] that samples for efficiently drawing samples from a @xmath26-dpp , by accelerating it using our results on gauss - type quadratures .",
    "randomly initialize @xmath154 where @xmath25 pick @xmath493 and @xmath494 uniformly randomly pick @xmath156 uniformly randomly @xmath495 get lower and upper bounds @xmath122 , @xmath137 of the spectrum of @xmath158 @xmath496    @xmath269 , @xmath103 , @xmath497 , @xmath498 , @xmath499 @xmath500 , @xmath501 , @xmath502 , @xmath503 , @xmath504 run one more iteration of gauss - radau on @xmath0 to get tighter @xmath505 and @xmath506 @xmath507 run one more iteration of gauss - radau on @xmath508 to get tighter @xmath509 and @xmath510 @xmath511 return _ true _ return _ false _",
    "@xmath269 , @xmath103 , @xmath497 , @xmath498 , @xmath499 @xmath500 , @xmath501 , @xmath502 , @xmath503 , @xmath504 @xmath512 @xmath513 , @xmath514 @xmath515 , @xmath516 , @xmath517 @xmath518 , @xmath519 @xmath520 , @xmath521 , @xmath522 @xmath523 @xmath524 , @xmath525 @xmath526 , @xmath527 @xmath528 , @xmath529 @xmath525 , @xmath530 , @xmath507 @xmath531 @xmath532 , @xmath533 @xmath534 , @xmath535 , @xmath536 @xmath537 , @xmath538 @xmath539 , @xmath540 , @xmath541 @xmath542 @xmath543 , @xmath544 @xmath545 , @xmath546 @xmath547 , @xmath548 @xmath544 , @xmath549 , @xmath511 return _ true _ return _ false _",
    "we present details of _ retrospective stochastic double greedy _ in  alg .",
    "[ append : algo : gaussdg ] and  alg .",
    "[ append : algo : dg_judge ] that efficiently select a subset @xmath550 that approximately maximize @xmath551 .",
    "@xmath499 , @xmath504 run one more iteration of gauss - radau on @xmath0 to get tighter lower and upper bounds @xmath559 , @xmath560 for @xmath561 @xmath562 run one more iteration of gauss - radau on @xmath563 to get tighter lower and upper bounds @xmath564 , @xmath565 for @xmath566 @xmath567 return _ true _ return _ false _"
  ],
  "abstract_text": [
    "<S> we present a framework for accelerating a spectrum of machine learning algorithms that require computation of _ bilinear inverse forms _ @xmath0 , where @xmath1 is a positive definite matrix and @xmath2 a given vector . </S>",
    "<S> our framework is built on gauss - type quadrature and easily scales to large , sparse matrices . </S>",
    "<S> further , it allows retrospective computation of lower and upper bounds on @xmath0 , which in turn accelerates several algorithms . </S>",
    "<S> we prove that these bounds tighten iteratively and converge at a linear ( geometric ) rate . to our knowledge , </S>",
    "<S> ours is the first work to demonstrate these key properties of gauss - type quadrature , which is a classical and deeply studied topic . </S>",
    "<S> we illustrate empirical consequences of our results by using quadrature to accelerate machine learning tasks involving determinantal point processes and submodular optimization , and observe tremendous speedups in several instances . </S>"
  ]
}