{
  "article_text": [
    "one of the important challenges in computer vision applications is obtaining high resolution depth maps of observed scenes .",
    "a number of common tasks , such as object reconstruction , robotic navigation , and automotive driver assistance can be significantly improved by complementing intensity information from optical cameras with high resolution depth maps . however , with current sensor technology , direct acquisition of high - resolution depth maps is very expensive .",
    "the cost and limited availability of such sensors imposes significant constraints on the capabilities of vision systems and has dampened the adoption of methods that rely on high - resolution depth maps .",
    "thus , the literature has flourished with methods that provide numerical alternatives to boost the spatial resolution of the measured depth data .",
    "-@xmath0 slices of the color and depth sequences , respectively , at a fixed @xmath1 ; ( c)(e ) @xmath1-@xmath0 slices at @xmath2 ; ( f)(h ) @xmath1-@xmath0 slices at @xmath3 ; ( c ) and ( f ) input color images ; ( d ) and ( g ) input low - resolution and noisy depth images ; ( e ) and ( h ) estimated depth images.,width=302 ]    one of the most popular and widely investigated class of techniques for improving the spatial resolution of depth is guided depth superresolution .",
    "these techniques jointly acquire the scene using a low - resolution depth sensor and a high - resolution optical camera .",
    "the information acquired from the camera is subsequently used to superresolve the low - resolution depth map .",
    "these techniques exploit the property that both modalities share common features , such as edges and joint texture changes .",
    "thus , such features in the optical camera data provide information and guidance that significantly enhances the superresolved depth map .    to - date , most of these methods operate on a single snapshot of the optical image and the low - resolution depth map .",
    "however , most practical uses of such systems acquire a video from the optical camera and a sequence of snapshots of the depth map . the key insight in our paper",
    "is that information about one particular frame is replicated , in some form , in nearby frames .",
    "thus , frames across time can be exploited to superresolve the depth map and significantly improve such methods .",
    "the challenge is finding this information in the presence of scene , camera , and object motion between frames .",
    "figure  [ fig : mainfigure ] provides an example , illustrating the similarity of images and depth maps across frames .",
    "a key challenge in incorporating time into depth estimation is that depth images change significantly between frames .",
    "this results in abrupt variations in pixel values along the temporal dimension and may lead to significant degradation in the quality of the result .",
    "thus , it is important to compensate for motion during estimation . to that end , the method we propose exploits space - time similarities in the data using motion adaptive regularization . specifically , we identify and group similar depth patches , which we superresolve and regularize using a rank penalty .",
    "our method builds upon prior work on patch - based methods and low - rank regularization , which were successfully applied to a variety of practical estimation problems .",
    "it further exploits the availability of optical images which provide a very robust guide to identify and group similar patches , even if the depth map has very low resolution .",
    "thus , the output of our iterative algorithms is robust to operating conditions .",
    "our key contributions are summarized as follows :    * we provide a new formulation for guided depth superresolution , incorporating temporal information . in this formulation , the high resolution depth",
    "is determined by solving an inverse problem that minimizes a cost .",
    "this cost includes a quadratic data - fidelity term , as well as a new motion adaptive regularizer based on a low - rank penalty on groups of similar patches .",
    "* we develop two optimization strategies for solving our estimation problem .",
    "the first approach is based on exact optimization of the cost via alternating direction method of multipliers ( admm ) .",
    "the second approach uses a simplified algorithm that alternates between enforcing data - consistency and low - rank penalty .",
    "* we validate our approach experimentally and demonstrate it delivers substantial improvements .",
    "in particular , we compare several algorithmic solutions to the problem and demonstrate that : ( a ) availability of temporal information significantly improves the quality of estimated depth ; ( b ) motion adaptive regularization is crucial for avoiding artifacts along temporal dimension ; ( c ) using intensity during block matching is essential for optimal performance .",
    "in the last decade , guided depth superresolution has received significant attention .",
    "early work by diebel and thrun  @xcite showed the potential of the approach by modeling the co - occurence of edges in depth and intensity with markov random fields ( mrf ) .",
    "kopf  @xcite and yang  @xcite have independently proposed an alternative approach based on joint bilaterial filtering , where intensity is used to set the weights of the filter .",
    "the bilaterial filtering approach was further refined by chan  @xcite who incorporated the local statistics of the depth and by liu  @xcite who used geodesic distances for determining the weights .",
    "dolson  @xcite extended the approach to dynamic sequences to compensate for different data rates in the depth and intensity sensors .",
    "he  @xcite proposed a guided image filtering approach , improving edge preservation .",
    "more recently , sparsity - promoting regularization  an essential component of compressive sensing  @xcite  has provided more dramatic improvements in the quality of depth superresolution . for example , li  @xcite demonstrated improvements by combining dictionary learning and sparse coding algorithms .",
    "ferstl  @xcite relied on weighted total generalized variation ( tgv ) regularization for imposing a piecewise polynomial structure on depth .",
    "gong  @xcite combined the conventional mrf approach with an additional term promoting transform domain sparsity of the depth in an analysis form . in their recent work ,",
    "huang  @xcite use the mrf model to jointly segment the objects and recover a higher quality depth .",
    "schuon  @xcite performed depth superresolution by taking several snapshots of a static scene from slightly displaced viewpoints and merging the measurements using sparsity of the weighted gradient of the depth .",
    "many natural images contain repetitions of similar patterns and textures .",
    "current state - the - art image denoising methods such as nonlocal means ( nlm )  @xcite and block matching and 3d filtering ( bm3d )  @xcite take advantage of this redundancy by processing the image as a structured collection of patches .",
    "the original formulation of nlm was extended by yang and jacob  @xcite to more general inverse problems via introduction of specific nlm regularizers .",
    "similarly , danielyan  @xcite have proposed a variational approach for general bm3d - based image reconstruction that inspired the current work . in the context of guided depth",
    "superresolution , nlm was used by huhle  @xcite and park  @xcite for reducing the amount of noise in the estimated depth .",
    "lu  @xcite combined a block - matching procedure with low - rank constraints for enhancing the resolution of a single depth image .",
    "our paper extends prior work on depth supperresolution by introducing a new variational formulation that imposes low - rank constraints in the regularization .",
    "furthermore , our formulation is motion - adaptive , resulting in substantial improvement of the quality of the estimated depth .",
    "our approach estimates the high - resolution depth map by minimizing a cost function that  as typical in such problems  combines a data - fidelity term and a regularizer .",
    "specifically , we impose a quadratic data fidelity term that controls the error between the measured and estimated depth values .",
    "the regularizer groups similar depth patches from multiple frames and penalizes the rank of the resulting structure .",
    "since we use patches from multiple frames , our method is implicitly motion adaptive .",
    "thus , by effectively combining multiple views of the scene , it yields improved depth estimates .",
    "is centered at the reference patch .",
    "search is also conducted in the same window position in multiple temporally adjacent frames .",
    "similar patches are grouped together to construct a block @xmath4.,width=302 ]      the depth sensing system collects a set of measurements denoted @xmath5}$ ] .",
    "each measurement is considered as a downsampled version of a higher resolution depth map @xmath6 using a subsampling operator @xmath7 .",
    "our end goal is to recover this high - resolution depth map @xmath8 for all @xmath9 .    in the remainder of this work",
    ", we use @xmath10 to denote the number of pixels in each frame , @xmath11 to denote the number of temporal frames , and @xmath12 to denote the total number of depth measurements .",
    "furthermore , @xmath13 denotes the vector of all the measurements , @xmath14 the complete sequence of high - resolution depth maps , and @xmath15 the complete subsampling operator .",
    "we also have available the sequence of high - resolution intensity images from the optical camera , denoted @xmath16 .",
    "using the above , a forward model for the depth recovery problem is given by @xmath17 where @xmath18 denotes the measurement noise .",
    "thus , our objective becomes to recover high - resolution depth given the measured data @xmath19 and @xmath20 , and the sampling operator @xmath21 .",
    "as typical in such problems , we formulate the depth estimation task as an optimization problem @xmath22 where @xmath23 enforces data fidelity and @xmath24 is a regularization term that imposes prior knowledge about the depth map .",
    "we form the regularization term by constructing sets of patches from the image . specifically",
    ", we first define an operator @xmath25 , for each set of patches @xmath26 $ ] , where @xmath27 is the number of such sets constructed .",
    "the operator extracts @xmath28 patches of size @xmath29 pixels from the depth image frames in @xmath30 . as illustrated in fig .",
    "[ fig : blocmatching ] , each block @xmath31 is obtained by first selecting a reference patch and then finding @xmath32 similar patches within the current frame as well as the adjacent temporal frames .    to determine similarity and to group similar patches together we use the intensity image as a guide . to reduce the computational complexity of the search , we restrict it to a space - time window of fixed size around the reference patch .",
    "we perform the same block matching procedure for the whole space - time image by moving the reference patch and by considering overlapping patches in each frame .",
    "thus , each pixel in the signal @xmath30 may contribute to multiple blocks .",
    "the adjoint @xmath33 of @xmath25 simply corresponds to placing the patches in the block back to their original locations in @xmath30 .",
    "it satisfies the following property @xmath34 where @xmath35 and @xmath36 denotes the total number of references to the @xmath37th pixel by the matrices @xmath38 .",
    "therefore , the depth image @xmath30 can be expressed in terms of an overcomplete representation using the blocks @xmath39      each block , represented as a matrix , contains multiple similar patches , i.e. , similar columns .",
    "thus , we expect the matrix to have a low rank , making rank a natural regularizer for the problem @xmath40    by seeking a low - rank solution to  , we exploit the similarity of blocks to guide superresolution while enforcing consistency with the observed data . however , the rank regularizer   is of little practical interest since its direct optimization is intractable .",
    "the most popular approach around this , first proposed by fazel in  @xcite , is to convexify the rank by replacing it with the nuclear norm : @xmath41 where @xmath42 denotes the @xmath43th largest singular value of @xmath44 and @xmath45 is a parameter controlling the amount of regularization .",
    "in addition to its convexity , the nuclear norm is an appealing penalty to optimize because it also has a closed form proximal operator : @xmath46 where @xmath47 is the singular value decomposition ( svd ) of @xmath19 and @xmath48 is the soft - thresholding function applied to the diagonal matrix @xmath49 .",
    "-shrinkage operator @xmath50 for a fixed @xmath51 at two values of @xmath52 .",
    "for @xmath53 the shrinkage is equivalent to soft - thresholding , while for @xmath54 it approaches hard - thresholding , width=302 ]    recent work has shown that nonconvex regularizers consistently outperform nuclear norm by providing stronger denoising capability without losing important signal compoenents  @xcite . in this paper , we use the nonconvex generalization to the nuclear norm proposed by chartrand  @xcite @xmath55 here , the scalar function @xmath56 is designed to satisfy @xmath57 where @xmath58 is the @xmath52-huber function @xmath59 with @xmath60 .",
    "although @xmath56 is nonconvex and has no closed form formula , its proximal operator does admit a closed form expression @xmath61 where @xmath62 is a poitwise @xmath52-shrinkage operator defined as @xmath63 for @xmath53 , @xmath52-shrinkage   is equivalent to conventional soft thresholding ( see illustration in fig .  [",
    "fig : shrink ] ) .",
    "when @xmath54 , it approaches hard thresholding , which is similar to principal component analysis ( pca ) in the sense that it retains the significant few principal components .",
    "thus , the regularizer   is a computationally tractable alternative to the rank penalty .",
    "while the regularizer is not convex , it can still be efficiently optimized due to closed form of its proximal operator  .",
    "note that due to nonconvexity of our regularizer for @xmath64 , it is difficult to theoretically guarantee global convergence .",
    "however , we have empirically observed that our algorithms converge reliably over a broad spectrum of examples presented in section  [ sec : experiments ] .      to solve the optimization problem   under the rank regularizer  ,",
    "we first simplify notation by defining an operator @xmath65 and a vector @xmath66 .",
    "the minimization is performed using an augmented - lagrangian ( al ) method  @xcite .",
    "specifically , we seek the critical points of the following al    @xmath67    where @xmath68 is the quadratic parameter and @xmath69 is the dual variable that imposes the constraint @xmath70 . traditionally , an al scheme solves   by alternating between a joint minimization step and an update step as    @xmath71    however , the joint minimization step   is typically computationally intensive . to reduce complexity , we separate   into a succession of simpler steps using the well - established by now alternating direction method of multipliers ( admm )  @xcite .",
    "the steps are as follows    [ eq : admm ] @xmath72    by ignoring the terms that do not depend on @xmath30 ,   amounts to solving a quadratic problem @xmath73 where @xmath74 .",
    "solving this quadratic equation is efficient since the inversion is performed on a diagonal matrix .",
    "similarly ,   is solved by @xmath75 with @xmath76 .",
    "this step can be solved via block - wise application of the proximal operator   as @xmath77 for all @xmath26 $ ] .",
    "the algorithm in section  [ sec : iterativeoptimization ] can be significantly simplified by decoupling the enforcement of the data - fidelity from the enforcement of the rank - based regularization .",
    "the simplified algorithm reduces computational complexity while making estimation more uniform across the whole space - time depth image .",
    "in particular , danielyan  @xcite has argued that , due to inhomogeneous distribution of pixel references generated by matching across the image , using a penalty with a single regularization parameter highly penalizes pixels with a large number of references .",
    "the resulting nonuniform regularization makes the algorithm potentially oversensitive to the choice of the parameter @xmath78 .",
    "instead , we rely on the simplified algorithm    [ eq : dec ] @xmath79    where @xmath80 , and @xmath45 is the regularization and @xmath68 is the quadratic parameters .    to solve   we apply the proximal operator @xmath81 for all @xmath26 $ ] .",
    "next ,   reduces to a linear step @xmath82 there are substantial similarities between algorithms   and  .",
    "the main differences are that we eliminated the dual variable @xmath69 and simplified the quadratic subproblem  .",
    "[ cols=\"<,^,^,^,^,^,^,^,^,^,^,^,^ \" , ]      downsized version at @xmath83 db input snr .",
    "we plot the reconstruction snr against the video frame number .",
    "the plot illustrates potential gains that can be obtained by fusing intensity and depth information in a motion - adaptive way.,width=302 ]     downsized version at @xmath83 db input snr .",
    "we plot the reconstruction snr against the video frame number .",
    "the plot illustrates potential gains that can be obtained by fusing intensity and depth information in a motion - adaptive way.,width=302 ]                to verify our development , we report results on extensive simulations using our guided depth superresolution algorithms . in particular , we compare results of both the admm approach ( denoted _ admm-3d _ ) and its simplified variant ( denoted _ gds-3d _ ) against six alternative methods .    as the first and the simplest reference method , we consider standard linear interpolation ( _ linear _ ) .",
    "in addition to linear interpolation , we consider methods relying in some form of total variation ( tv ) regularization , one of the most widely used regularizers in the context of image reconstruction due to its ability to reduce noise while preserving image edges  @xcite . specifically , we consider depth interpolation using tv - regularized least squares on a frame - by - frame basis ( _ tv-2d _ ) .",
    "we also consider the weighted - tv formulation proposed by ferstl  @xcite ( _ wtgv-2d _ ) , also operating on a frame - by - frame basis .",
    "this formulation uses a weighted anisotropic total generalized variation , where weighting is computed using the guiding intensity image , thus promoting edge co - occurrence in both modalities .",
    "finally , we consider a weighted - tv formulation which includes time , i.e. , multiple frames ( _ wtv-3d _ ) , with weights computed using the guiding intensity image , as before .",
    "we also compare these methods to two variations of our algorithm .",
    "to illustrate the potential gains of our method due to temporal information , we run our simplified algorithm on a frame - by - frame basis ( _ gds-2d _ ) , i.e. , using no temporal information . similarly , to illustrate the gains due to intensity information",
    ", we also run the simplified algorithm by performing block matching on the low - resolution depth as opposed to intensity ( _ ds-3d _ ) .",
    "this last approach conceptually corresponds to denoising the initial depth using our motion adaptive low - rank prior .    to improve convergence speed , we initialized all the iterative algorithms with the solution of linear interpolation",
    ". the regularization parameters of tv-2d , wtgv-2d , and wtv-3d were optimized for the best signal - to - noise ratio ( snr ) performance .",
    "similarly , the regularization parameters @xmath78 for gds-2d , ds-3d , admm-3d , and gds-3d were optimized for the best snr performance .",
    "however , in order to reduce the computational time , the selection was done from a restricted set of three predetermined parameters .",
    "the methods tv-2d , wtv-3d , gds-2d , ds-3d , admm-3d , and gds-3d were all run for a maximum of @xmath84 iterations with an additional stopping criterion based on measuring the relative change of the solution in two successive iterations @xmath85 where @xmath86 is the desired tolerance level .",
    "we selected other parameters of wtgv-2d as suggested in the code provided by the authors ; in particular , we run the algorithm for a maximum of @xmath87 iterations with the stopping tolerance of @xmath88 . in all experiments ,",
    "the patch size was set to @xmath89 pixels , the space - time window size to @xmath90 pixels , and the number of similar patches was fixed to @xmath91 .",
    "parameters @xmath52 and @xmath92 were hand selected to @xmath93 and @xmath94 , respectively .    for quantitative comparison of the algorithms ,",
    "we rely on the data - set by zhang  @xcite , which consists of three video sequences _ flower _ , _ lawn _ , and _ road _ , containing both intensity and depth information on the scenes .",
    "we considered images of size @xmath95 with @xmath96 time frames .",
    "the ground truth depth was downsized by factors of @xmath97 , @xmath98 , @xmath99 , and @xmath100 , and was corrupted by additive gaussian noise corresponding to snr of @xmath83 db .",
    "table  [ tab : table ] reports the snr of superresolved depth for all the algorithms and downsizing factors . figures  [ fig : result2_plot ] and  [ fig : result1_plot ] illustrate the evolution of snr against the frame number for _ road _ and _ flower _ , respectively , at downsizing factor of @xmath98 .",
    "the effectiveness of the proposed approach can also be appreciated visually in figs .",
    "[ fig : result2_img ] and  [ fig : result1_img ] .    for additional validation ,",
    "we test the algorithm on the kitti dataset  @xcite .",
    "the dataset contains intensity images from pointgray flea2 cameras and 3d point clouds from a velodyne hdl-64e , which have been calibrated a priori using specific known targets .",
    "we consider a region of interest of @xmath101 pixels with @xmath102 time frames .",
    "the @xmath103 total lidar measurements are randomly and uniformly split into a reconstruction and a validation sets . note that this implies a depth measurement rate of just @xmath104 .",
    "we then estimate depth from the reconstruction set using wtv-3d , as well as gds-3d , and use the validation set to evaluate the quality of the results ; these are illustrated in fig .",
    "[ fig : result3_img ] .    the examples and simulations in this section ,",
    "validate our claim : the quality of estimated depth can be considerably boosted by properly incorporating temporal information into the reconstruction procedure .",
    "comparison of gds-2d against gds-3d highlights the importance of additional temporal information .",
    "the approach we propose is implicitly motion adaptive and can thus preserve temporal edges substantially better than alternative approaches such as wtv-3d .",
    "moreover , comparison between ds-3d and gds-3d highlights that the usage of intensity significantly improves the performance of the algorithm . note also the slight improvement in snr of gds-3d over admm-3d .",
    "this is consistent with the arguments in  @xcite that suggest to decouple data - fidelity from the enforcement of prior constraints when using block - matching - based methods .",
    "we presented a novel motion - adaptive approach for guided superresolution of depth maps .",
    "our method identifies and groups similar patches from several frames , which are then supperresolved using a rank regularizer . using this approach",
    ", we can produce high - resolution depth sequences from significantly down - sized low - resolution ones .",
    "compared to the standard techniques , the proposed method preserves temporal edges in the solution and effectively mitigates noise in practical configurations .    while our formulation has higher computational complexity than standard approaches that process each frame individually , it allows us to incorporate a very effective regularization for stabilizing the inverse problem associated with superresolution .",
    "the algorithms we describe enable efficient computation and straightforward implementation by reducing the problem to a succession of straightforward operations .",
    "our experimental results demonstrate the considerable benefits of incorporating time and motion adaptivity into inverse - problems for depth estimation .",
    "j.  diebel and s.  thrun , `` an application of markov random ffield to range sensing , '' in _ proc .",
    "advances in neural information processing systems 18 _ , vol .",
    "18 , vancouver , bc , canada , december 5 - 8 , 2005 , pp . 291298 .",
    "q.  yang , r.  yang , j.  davis , and d.  nister , `` spatial - depth super resolution for range images , '' in _ proc .",
    "ieee conf .",
    "computer vision and pattern recognition ( cvpr ) _ , minneapolis , mn , usa , june 17 - 22 , 2007 , pp .",
    "d.  chan , h.  buisman , c.  theobalt , and s.  thrun , `` a noise - aware filter for real - time depth upsampling , '' in _ eccv workshop on multicamera and multimodal sensor fusion algorithms and applications _ , 2008 .",
    "liu , o.  tuzel , and y.  taguchi , `` joint geodesic upsampling of depth images , '' in _ proc .",
    "ieee conf .",
    "computer vision and pattern recognition ( cvpr ) _ , portland , or , usa , june 23 - 28 , 2013 , pp . 169176 .",
    "j.  dolson , j.  baek , c.  plagemann , and s.  thrun , `` upsampling range data in dynamic environments , '' in _ proc .",
    "ieee conf .",
    "computer vision and pattern recognition ( cvpr ) _ , vol .",
    "23 , san francisco , ca , usa , june 13 - 18 , 2010 , pp . 11411148 .",
    "e.  j. cands , j.  romberg , and t.  tao , `` robust uncertainty principles : exact signal reconstruction from highly incomplete frequency information , '' _ ieee trans .",
    "inf . theory _ ,",
    "52 , no .  2 ,",
    "pp . 489509 , february 2006 .",
    "d.  ferstl , c.  reinbacher , r.  ranftl , m.  ruether , and h.  bischof , `` image guided depth upsampling using anisotropic total generalized variation , '' in _ proc .",
    "ieee conf .",
    "computer vision and pattern recognition ( cvpr ) _ , sydney , nsw , australia , december 1 - 8 , 2013 , pp .",
    "9931000 .",
    "s.  schouon , c.  theobalt , j.  davis , and s.  thrun , `` lidarboost : depth superresolution for tof 3d shape scanning , '' in _ proc .",
    "ieee conf .",
    "computer vision and pattern recognition ( cvpr ) _ , miami , fl , usa , june 20 - 25 , 2009 , pp . 343350 .",
    "k.  dabov , a.  foi , v.  katkovnik , and k.  egiazarian , `` image denoising by sparse 3-d transform - domain collaborative filtering , '' _ ieee trans .",
    "image process .",
    "_ , vol .",
    "16 , no .",
    "16 , pp . 20802095 , august 2007 .",
    "b.  huhle , t.  schairer , p.  jenke , and w.  strasser , `` fusion of range and color images for denoising and resolution enhancement with a non - local filter , '' _ comput .",
    "image underst .",
    "114 , no .  12 , pp . 13361345 , december 2010 .",
    "j.  park , h.  kim , y .- w .",
    "tai , m.  s. brown , and i.  kweon , `` high quality depth map upsampling for 3d - tof cameras , '' in _ proc .",
    "conf . comp .",
    "barcelona , spain , november 6 - 13 , 2011 , pp .",
    "16231630 .",
    "s.  lu , x.  ren , and f.  liu , `` depth enhancement via low - rank matrix completion , '' in _ proc .",
    "ieee conf .",
    "computer vision and pattern recognition ( cvpr ) _ , columbus , oh , usa , june 23 - 28 , 2014 , pp . 33903397 .",
    "h.  yoon , k.  s. kim , d.  kim , y.  bresler , and j.  c. ye , `` motion adaptive patch - based low - rank approach for compressed sensing cardiac cine mri , '' _ ieee trans . med .",
    "_ , vol .",
    "33 , no .  11 , pp . 20692085 ,",
    "november 2014 .",
    "s.  boyd , n.  parikh , e.  chu , b.  peleato , and j.  eckstein , `` distributed optimization and statistical learning via the alternating direction method of multipliers , '' _ foundations and trends in machine learning _",
    ", vol .  3 , no .  1 ,",
    "1122 , 2011 ."
  ],
  "abstract_text": [
    "<S> spatial resolution of depth sensors is often significantly lower compared to that of conventional optical cameras . </S>",
    "<S> recent work has explored the idea of improving the resolution of depth using higher resolution intensity as a side information . in this paper , we demonstrate that further incorporating temporal information in videos can significantly improve the results . </S>",
    "<S> in particular , we propose a novel approach that improves depth resolution , exploiting the space - time redundancy in the depth and intensity using motion - adaptive low - rank regularization . </S>",
    "<S> experiments confirm that the proposed approach substantially improves the quality of the estimated high - resolution depth . </S>",
    "<S> our approach can be a first component in systems using vision techniques that rely on high resolution depth information . </S>"
  ]
}