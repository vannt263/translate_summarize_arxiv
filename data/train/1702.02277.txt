{
  "article_text": [
    "the cerebellar model articulation controller ( cmac ) was proposed by j. s. albus in 1975 @xcite .",
    "parallel at this time in the history , the concept of perceptron @xcite had already been popular , whereas effective learning schemes to tune perceptrons @xcite were not on the stage yet . in 1969 ,",
    "minsky and papert also pointed out the limitations that the exclusive disjunction logic can not be solved by the perceptron model in their book _ perceptrons : an introduction to computational geometry_. these facts made it less promising to consider cmac as a neural network form .",
    "consequently , although the name of cmac appears bio - inspired enough , and the theory that the cerebellum is analogous to a perceptron has been proposed earlier @xcite , cmac was emphasized to be understood as a table referring technique that can adaptive to real - time control system . nevertheless , the underlying bioscience mechanism was addressed again in 1979 by albus @xcite , which always gives the cmac model two different ways of interpretation .",
    "the structure of cmac was originally described as two inter - layer mappings illustrated in fig .",
    "[ pcmac ] .",
    "the control functions are represented in the weighted look - up table , rather than by solution of analytic equations or by analog @xcite .",
    "if we use @xmath0 to denote sensory input vectors , and let @xmath1 and @xmath2 stand for association cells and response output vectors respectively , both cmac and multilayer percptrons ( mlp ) model can be formalized as :    f : + g :    the final output can be calculated as : @xmath3 where the asterisk denotes a link or activation between certain association cell and the output .",
    "the nuance between mlp and cmac model is that , for mapping @xmath4 , mlp model is fully connected but cmac restricts the association in a certain neighboring range .",
    "this property of mapping significantly accelerates the learning process of cmac , which is considered a main advantage of it comparing to other neural network models .",
    "it is notable that cmac may not represent accurately how the human cerebellum works , even at a most simplified level .",
    "for instance , recent biological evidence from eyelid conditioning experiments suggests that the cerebellum is capable of computing exclusive disjunction @xcite .",
    "however , cmac is still an important computational model , because the restriction on mapping function effectively decreased the chance of been trapped in a local minimum during learning process .    despite the advantage aforementioned ,",
    "cmac has the following disadvantages as well @xcite :    * many more weight parameters are needed comparing to normal mlp model * local generalization mechanism may cause some training data to be incorrectly interpolated , especially when data is sparse considering the size of cmac * cmac is a discrete model , analytical derivatives do not exist    as a response to these problems , modified or high order cmac , storage space compressing , and fast convergent learning algorithms are continuously studied .",
    "these discoveries will be elaborated in the following sections .",
    "recent advances in big data and computing power seem to have watered down these problems .",
    "but in many physical scenarios , the computing power are still restricted and high speed responses are required .",
    "this serves for the reason to further study cmac - like models , though it has been in and out of fashion for several times .",
    "although there has been few other pioneer review works on cmac , for instance by mohajeri et al . in 2009 @xcite ,",
    "this article is inventive for its chronological perspective . rather than emphasizing on detailed techniques .",
    "the remainder of the article is organized as follows : section [ archi ] provides the evolution trajectory of cmac structure and efficient storage techniques .",
    "section [ lrn ] discusses the learning algorithms .",
    "section [ app ] presents various circumstances that cmac model has been applied .",
    "section [ disc ] summarizes the paper , and instigates discussions about potential improvements that can be made on cmac model .",
    "before the cmac was proposed in the 1970s , the anatomy and physiology of cerebellum has been studied for a long time .",
    "it is widely agreed that many different type of nerve cells are involved in cerebellar functioning .",
    "[ cerebellum ] shows the computational model proposed by mauk and donegan @xcite . a more simple and implementable model proposed by albus @xcite is exhibited in fig . [ cerealbus ] .            based on the computational model of cerebellum ( fig . [ cerealbus ] ) , the primary cmac structure as shown in fig . [ pcmac ]",
    "is conceived . while it is obvious that high dimensional proximity of association rules can not be captured in this primary form , because the nodes are arranged into one dimensional array .",
    "a simple solution to this problem is to introduce some nonlinearity to the first mapping . as a result ,",
    "another layer called `` conceptual memory '' was soon added to the cmac structure , which involves one additional mapping to the primary structure .",
    "the function of conceptual memory is illustrated in fig .",
    "[ newcmac ] .",
    "if we use @xmath1 to represent the actual memory ( association cells ) , @xmath5 is the conceptual memory to encode @xmath0 .",
    "then the conceptual mapping @xmath4 is more sparse and constrained within a certain range , but mapping @xmath6 could be random .",
    "f : + g : + h :    when it comes to implementation , the connectivism perspective to recognize cmac as a neural network and the table referring perspective are equivalent . fig . [ cmp ] illustrates the difference at a conceptual level @xcite . to the upper part",
    "is a two input one output neural network structure , to the lower part is a two input one output table look - up structure .",
    "an intuitive observation from both fig .",
    "[ newcmac ] and fig .",
    "[ cmp ] is that the number of weights will increase exponentially with the number of input variables .",
    "this problem brings out two challenges : 1 ) the storage of weights become space consuming ; 2 ) the training process becomes difficult to converge and waiting time before termination will lengthen .",
    "a previously used technique to solve the first challenge is called tile coding , which latter was developed to an adaptive version as well @xcite .",
    "the advantage of tile coding is that we can strictly control the number of features through tile split .",
    "another commonly employed trick is called hashing .",
    "this technique is applied to cmac in the 1990s , and maps a large virtual space @xmath1 to a smaller physical address @xmath7 .",
    "many common hashing function @xmath8 can be used , for instance from md5 or des , which are fundamental methods in cryptography . however ,",
    "how to reduce the collision for specific problems according to the data property is still considered an art .    literature @xcite introduced a hardware implementation which uses selected binary bits of indexes as the hashing code .",
    "whereas other research , e.  g. @xcite , claims that due to the learning rate diminishing and slower convergence , hashing is not effective for enhancing the capability of approximation by cmac .",
    "therefore , many other attempts have been made , such as neighbor sequential , cmac with general basis functions , and adaptive coding @xcite , which is a similar idea to hashing in the sense of weight space compression .      since simply increasing the cmac size",
    "gives diminishing returns , two directions of modification are undertaken to push forward the research on cmac .",
    "the _ first _",
    "consideration is to combine multiple low dimensional cmacs .",
    "second _ consideration is to introduce other properties , for example spline neurons , fuzzy coding or cooperative pid controller .",
    "cascade cmac architecture was firstly proposed in 1997 for the purpose of printer calibration @xcite ( fig .",
    "[ ccmac ] ) .",
    "input variables are sequentially added to keep each of the cmac component two dimensional .        another method to combine multiple cmacs",
    "can be realized by voting technique .",
    "if we regard the cascade cmac model as a fusion of input information at a feature level , then the voting cmacs can be reckoned as a fusion at decision level .",
    "each small cmac just accept a subset of the whole input space . in this case",
    "an important antecedent is that input data is well partitioned .",
    "the reason to this requirement is that voting lift can only be achieved by heterogeneous expert networks . taken this into account , some prior knowledge of input data or unsupervised clustering techniques can be applied in this stage .    if we make more efforts for dimension reduction ,",
    "multiple levels of voting can be used .",
    "then the architecture can be reckoned as a hierarchical cmac ( h - cmac ) , which is described by tham @xcite in 1996 .",
    "h - cmac has several advantages , such as less storage space and fast adaptation for learning non - linear functions . in fig .",
    "[ hcmac ] , a two level h - cmac is illustrated .",
    "it is noticeable that each conventional cmac components in different layers plays different role .",
    "the gating network works at a higher level .",
    "these architectures can be employed at the same time with more fundamental modifications for the second consideration . in 1992 ,",
    "lane et al .",
    "@xcite descried high order cmac as cmac with binary neuron output replaced by spline functions .",
    "this modification brings about more parameters of splines , but makes the output derivable , which sometimes gives a better performance because the learning phase goes deeper .",
    "sharing the idea to allow more meticulous transfer function , a similar modification can be made by introducing linguistic rules and fuzzy logic .",
    "linguistic cmac ( lcmac ) was proposed by he and lawry based on label semantics , rather than mapping functions in 2009 .",
    "a cascade of lcmac series was further developed in 2015 @xcite .",
    "borrowing the terminology of `` focal element '' from evidence theory , the properties used to activate it is represented as membership function ( usually trapezoidal ) of several attributes .",
    "therefore , for each input tuple , the excited neurons form a hypercube in the matrix of all the weights as memory .",
    "the responsive output can be distributionally depicted as : @xmath9 where @xmath10 is the probability of some memory unit @xmath11 been activated given the input vector @xmath12 .",
    "@xmath13 denotes focal element .",
    "@xmath14 is the index of linguistic attributes .",
    "@xmath15 is the hidden weights for @xmath13 called `` mass assignment '' .",
    "fuzzy cmac ( fcmac ) is yet another form of fuzzy coding .",
    "the intuition to use fuzzy representation is similar to using spline function . for most well defined problems ,",
    "the nature of cmac approximation is using multiple steps to emulate a smooth surface .",
    "proper selection of fuzzy membership function would obviously relief the pressure of weight storage and training . from my understanding",
    ", fcmac is an inverse structure of many established neuro - fuzzy inference systems .",
    "usually , two extra fuzzy / defuzzy layers are added next to the association layer , the consequents can be mamdani type , tsk type , weights , or a hybrid of them , e. g. parametric - fcmac ( pfcmac ) @xcite",
    ". more advanced fcmac models , maybe inspired by spline methods , use interpolation to solve the discrete inference problem . in 2015 ,",
    "zhou and quek proposed fie - fcmac @xcite , which adds fuzzy interpolation and extrapolation to a rigid cmac structure .",
    "recently , cmac applications to more specific scenarios are studied .",
    "for example , for control of time - varying nonlinear system , a combination of radial basis function network and the local learning feature of cmac is proposed ( rbfcmac ) .",
    "it is reported that using rbfs can prevent parameter drift and accelerate synchronization speed to the changing system @xcite . for this type of combination , beside rbf ,",
    "wavelet neural network ( wnn ) , fuzzy rule neuron and recurrent mechanism @xcite or a mingle of them can also be employed with cmac model simultaneously .",
    "previous works , such as @xcite , have provided evidence that these features are effective for modeling complicated dynamic systems .    in a broader scenario ,",
    "cmac can be applied with other control systems as well ( fig .",
    "[ combine ] ) .",
    "traditionally , the role of cmac at the primary stage is to assist the output of a main controller .",
    "as the training proceed , error between cmac and the actual model decays .",
    "cmac takes charge of the main controller .",
    "back - forward signal acceptor or conjugated cmacs are often used to accelerate this process @xcite .",
    "more precisely , this arrangement is a change of information flow but not a change of architecture .",
    "similarly , cmac structures that modifies the storage optimizing methods , for example quantization @xcite and multi - resolution @xcite , will only result in architectural difference from a hardware implementation sense . according to my personal understanding , they are the same thing regarding the conceptual structure , though these techniques are of sufficient interests to be discussed in section [ lrn ] .",
    "the original form of learning proposed albus is based on backward propagation of errors .",
    "the fast convergence of this algorithm is proved mathematically by succeeding researches .",
    "specifically , the convergence rate is only governed by the size of receptive fields for association cells @xcite .",
    "some study @xcite suggests that it would be useful to distinguish between target training and error training , despite they share the same mathematical form . in the weights updating rule",
    ", @xmath16 @xmath17 is the epoch times , @xmath18 $ ] is the learning rate , @xmath19 is a state indicator of activation .",
    "the learning process is theoretically faster with a larger @xmath20 , while overshooting may occur .",
    "if the difference between output and the desired value @xmath21 can well define the error , target training and error training will be equivalent . in certain cases ,",
    "@xmath22 is a popular cost function as well .",
    "based on the original learning algorithm , many developments are further derived .",
    "most of them can be categorized into two directions of improvement .",
    "the first relies on extra supervisory signals or value assignment mechanism based on statistics .",
    "the second endeavors to optimize the use of memory . from a more practical sense",
    ", the dichotomy can be understood as learning to adjust value of weights , and learning to adjust number or size of weights .      facing the trade off between speed of convergence and learning stability",
    ", it is intuitive to consider using a relatively large @xmath20 at the beginning , and slow down the weight adjustment near the optimum point .",
    "this improvement is called adaptive learning rate @xcite .",
    "it can be achieved by using two cmac components , one as main controller , another as supervisory controller or compensated controller .",
    "another way to achieve this adaption is by imposing a resistant term to the weight updating rule : @xmath23 in the above rule , @xmath24 and @xmath25 are constants , @xmath26 denotes the average activation times of the memory units that have been activated by training sample @xmath27 .    for both fixed learning rate and adaptive learning rate ,",
    "weights adjustment start from a randomized set of parameters .",
    "experiments suggest that for sparse data , even if the training samples are within the local generalization range , perfect linear interpolation may not be achieved @xcite . as a result",
    ", the approximation may appears to have many small zigzag patterns .",
    "therefore , the weight smoothing technique is proposed .",
    "after each iteration , the weights are globally adjusted according to : @xmath28 where @xmath29 is the proportional coefficient measuring the share of the weight with index @xmath30 that needs to be replaced by the average of all activated weights @xmath31 .",
    "+ while the weight smoothing technique tries to affect as much memory as possible in one iteration , repeated activation of the same units may not be a good thing . in 2003 , research @xcite pointed out that equally assign the error to each weight is not a meticulous method . during the learning process ,",
    "if an associative memory is activated many times , which means many relevant samples are already learned , the weight should be more close to the desired value . in other words ,",
    "the weight value is more  credible \" . in this situation",
    ", less error should be assigned to it so that other memory units can learn faster .",
    "this rule is called learning based on credit assignment : @xmath32 where @xmath6 is a parameter regarding the degree of local generalization , @xmath33 records the times memory unit @xmath27 has been activated .",
    "further research has proved that the convergence is guaranteed with learning rate @xmath34 .",
    "the hardware implementation of cmac memory enables other interpretations of the associative architecture .",
    "if we recognize it as a self - organizing feature map ( sofm ) , competitive learning algorithm can be realized . with input variables",
    "@xmath35 , the output can directly be the weight of the winning neuron .",
    "the weights updating rule can be formalized according to hebbian theory : @xmath36 note that the index of @xmath37 can be time dependent .",
    "a modified version of the aforementioned rule involves not only inputs , but also errors as feedback : @xmath38 using this learning mechanism , the sofm - like cmac is named mcmac . in 2000 , ang and quek @xcite proposed learning with momentum , neighborhood , and averaged fuzzy output for both cmac and mcmac . for cmac and mcmac with momentum , the weights updating rules can be written as :    w_j(k+1 ) =  w_j(k ) +  _ j(k ) y_j(k ) + w_,(k+1)=  w_,(k)+(1-)[y(k)-w_,(k ) ]    where the first term represents a momentum , the second term is a back propagation term with learning rate @xmath39 and local gradient @xmath40 .",
    "@xmath30 is the index for activated weights .",
    "therefore , given a sequential learning process , the aggregational weights adjustment can be derived from the above rules : @xmath41 when the sign of @xmath42 keeps unchanged , @xmath43 is accelerating ; while if the sign is reversing , @xmath43 will slow down to stabilize the learning process .",
    "the neighborhood learning rule proposed by ang and quek @xcite serves the same purpose as weight smoothing technique .",
    "however , they used the gaussian function to put more attention to those neurons surrounding the winning neuron instead of evenly adjusting each weight regardless of the distance .",
    "weights updating rule for mcmac with momentum and neighborhood in singular form is : @xmath44\\ ] ] where @xmath45 is the distance metric between neuron @xmath30 and the winning neuron @xmath27 .    beside using additional terms such as momentum and neighborhood , kernel method",
    "can also be applied to cmac learning . in 2007 ,",
    "kernel cmac ( kcmac ) was proposed @xcite to reduce the cmac dimension which usually hazard the training speed and convergence .",
    "kcmac treats the association memory as the feature space of a kernel machine , thus weights can be determined by solving an optimization problem .",
    "the supervised learning using error @xmath46 as slack variables is to achieve : @xmath47 where @xmath48 denotes weights , coefficient @xmath49 serves as penalty parameter , @xmath50 is the mapping function and @xmath51 is the kernel function .    the standard procedure to this problem is to solve the maxima - minima of lagrangian function . though other learning method can be employed as well , for instance , using bayesian ying - yang ( byy ) learning as proposed in 2013 by tian et al @xcite .",
    "the key idea behind byy learning can be represented by harmonizing the joint probability with different product form of bayesian components . in this specific kcmac case ,",
    "@xmath52 and output @xmath53 are observable , while @xmath48 is a hidden variable .",
    "the joint distribution can either be written as @xmath54 form or @xmath55 form .",
    "p_ying(,,)=p ( ) p(|)p(| , ) + p_yang(,,)=p ( ) p(|)p(| , )    our goal is to maximize @xmath56 , @xmath57 in practice , @xmath58 is frequently calculated depending on how the conditional probabilities are estimated and maximum of @xmath59 is achieved by searching heuristically .      adjusting number of weights",
    "is chiefly realized by introducing multi - resolution and dynamic quantization techniques . as section [ intro ] has explained , cmac was firstly used for real time control system .",
    "consequently , the structural design fits the hardware implementation well .",
    "many cmac variants , such as lcmac and fcmac , inherit the memory units division with a lattice - based manner .",
    "inputs are generally built on grids with equal space .",
    "this characteristic adds local constraints to the value of adjacent units .",
    "if we consider this problem from a function approximation perspective , it is also rather intuitive that local complex shape needs a larger number of low - order elements to approach .",
    "naturally , multi - resolution lattice techniques @xcite were proposed in the 1990s .    with our prior knowledge",
    ", some metrics can be used to determine the resolution , for example , the variation of output in certain memory unit areas , which can be formalized as following : @xmath60 where @xmath61 is the number of output samples , @xmath62 is the variance .",
    "other attempts use a tree structure to manage the resolution hierarchically . new high resolution lattice is generated only if the threshold of variance is exceeded .",
    "the concept of quantization is almost identical to resolution .",
    "the nuance may be that the terminology _ quantization _ puts more emphasis on the discretion of continuous signal .",
    "furthermore , increasing resolution will also cause the number of weights to grow .",
    "quantization deals with a given memory capacity problem .    to the best of my knowledge ,",
    "the idea of adaptive quantization was initially proposed in 2006 @xcite .",
    "the algorithm used to interpolate points looks on change of slopes , which is also similar to the variance metric discussed above .",
    "the input space is initialized with uniform quantization . for each point @xmath19 ,",
    "the slopes are calculated by neighbor points .",
    "@xmath63 here @xmath4 stands for the mapping function between input vector and association cells .",
    "the change of sign for corresponding directions indicates finer local structure , or fluctuation in other words , thus a new critical point is added to split the interval .",
    "the termination condition is that , within all intervals , the difference of output values should not exceed an experimental constant @xmath64 @xcite . the adaptive quantization technique is later developed to the pseudo self evolving cmac ( psecmac ) model @xcite , which further introduced neighborhood activation mechanism .",
    "though cmac was firstly proposed for manipulator control , during the past decades it has been proved effective in robotic arm control , fractionator control , piezoelectric actuator control @xcite , mechanic system , signal processing , intelligent driver warning system , fuel injection , canal control automation , printer color reproduction , atm cash demand @xcite , and many other fields @xcite .",
    "this can be attributed to the fast learning and stable performance of cmac models .",
    "moreover , engineering - oriented software and toolkit also help to promote the application of camc . in the following part , cmac application to two emerging engineering fields",
    "are elaborated .",
    "the practice side of financial engineering has employed various instruments to model the price and risk of bond , stock , option , future and other derivatives . in most cases ,",
    "historic data can be partitioned into chunks of selected time span to enable a supervised learning process . in 2008 ,",
    "teddy et al @xcite proposed an improved cmac variant ( psecmac ) to model the price of currency exchange american call option .",
    "three variables are taken into consideration : difference between strike price and current price , time of maturity , and pricing volatility .",
    "thus the pricing function is formulated as : @xmath65    the article reported psecmac as the best - performing model among several cmac - like systems .",
    "it is also reported that most of the cmac models produce better result than using black - scholes model in sense of rmse .",
    "though for american option , black - scholes model is not a good benchmark because it is sensitive to details of calculation .",
    "this work further constructed an arbitrage system based on the pricing model .",
    "positions are adjusted according to the delta hedging ratio .",
    "experiments suggested the model has a marginal positive roi , omitting transaction costs .",
    "the application of cmac on commercial devices may be more advantageous .",
    "the pressure to reduce cost and demand of embedded controller make cmac - like models a good choice .",
    "recently , studies have been carried on adaptive control of disabled wheelchair and seatless unicycle @xcite .",
    "this type of problems can be formalized as controlling a set of variables in certain range ( e. g. speed and balance angle ) with a set of unknown and varying variables ( e. g. friction to the ground and weight of the rider ) .",
    "li et al @xcite proposed a tsk type fcmac to synthesis the equations of adaptive steering control .",
    "the back - stepping error is associated with the torque @xmath66 , which can simultaneously effect on critical moments .",
    "= |a ( , ) + |b()(--c  ` sgn ` ( ) ) + = |c ( , ) + |d()(--c  ` sgn ` ( ) )    as the output adapting to the moment parameters , the balance angle is controlled near zero .",
    "the performance of fcmac is benchmarked with a linear - quadratic regulator for differential equations to describe the state of the motor .",
    "simulations suggest that lqr is not able to converge speed and angle of balance , while fcmac provides satisfactory result .",
    "referring to section [ app ] , cmac was proved to be effective in many classic control problem and has been applied to emerging engineering problems . however , this model seems have encountered a bottleneck because of the lack of fundamental breakthrough during the past decade .",
    "nowadays , issues discussed are mainly focused on trivial modification on memory structure and learning algorithm .",
    "the framework of error propagation or minimization of loss function is kept unchanged .    according to my understanding",
    ", the limitation of current cmac models can be ascribe to its over simplification of the cerebellum structure .",
    "therefore , the next generation cerebellar model may adopt new discoveries from neuroscience .",
    "for example , the associative memory cells may take different roles rather than been treated identically .",
    "in fact , anatomical models usually feature several types of elementary cerebellar processing units .",
    "meanwhile , the theory of spike timing dependent plasticity ( stdp ) suggests that the learning process of firing neurons may be ordered @xcite .",
    "this feature can introduce far more complexity to the current learning algorithm .",
    "j. s. albus , `` a new approach to manipulator control : the cerebellar model articulation controller ( cmac ) , '' _ trans .",
    "asme , series g. journal of dynamic systems , measurement and control _ , 97 , pp . 220-233 , 1975 .",
    "li , c .- c .",
    "tsai , f .-",
    "tai and h .- s .",
    "yap , `` adaptive steering control using fuzzy cmac for electric seatless unicycles , '' _ ieee international conference on control & automation _ , pp . 556561 , 2014 .",
    "lin and h .- y .",
    "li , `` self - organizing adaptive wavelet cmac backstepping control system design for nonlinear chaotic systems , '' _ nonlinear analysis : real world applications _",
    ", 14(1 ) , pp . 206223 , 2013 .",
    "peng , r .- j .",
    "wai and c .- m .",
    "lin , `` adaptive cmac model reference control system for linear piezoelectric ceramic motor , '' in _ ieee international symposium on computational intelligence in robotics and automation _",
    ", 2003 .",
    "su , t. tao , and t .- h . hung , `` credit assigned cmac and its application to online learning robust controllers , '' _ ieee transactions on systems , man , and cybernetics , part b ( cybernetics ) _ , 33(2 ) , 2003 .",
    "w. j. zhou , d. l. maskell and c. quek , `` fie - fcmac : a novel fuzzy cerebellum model articulation controller ( fcmac ) using fuzzy interpolation and extrapolation technique , '' in _ international joint conference on neural networks ( ijcnn ) _ , 2015 ."
  ],
  "abstract_text": [
    "<S> the cerebellar model articulation controller ( cmac ) is an influential brain - inspired computing model in many relevant fields . since its inception in the 1970s , </S>",
    "<S> the model has been intensively studied and many variants of the prototype , such as kcmac , mcmac , and lcmac , have been proposed . </S>",
    "<S> this review article focus on how the cmac model is gradually developed and refined to meet the demand of fast , adaptive , and robust control . </S>",
    "<S> two perspective , cmac as a neural network and cmac as a table look - up technique are presented . </S>",
    "<S> three aspects of the model : the architecture , learning algorithms and applications are discussed . in the end , some potential future research directions on this model are suggested . </S>"
  ]
}