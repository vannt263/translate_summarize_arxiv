{
  "article_text": [
    "in this paper we consider the analysis of longitudinal data that arise in the form of a small number of long sequences .",
    "our interest in this problem is motivated by the naturalistic teenage driving study ( ntds ) , an observational study of teenage driving performance and characteristics [ simons - morton et al .",
    "( @xcite ) ] . in the study ,",
    "42 newly licensed teenage drivers in virginia were monitored continuously during their first 18 months of independent driving using in - vehicle data recording systems .",
    "the instrumentation included accelerometers , video cameras , a global positioning system , a front radar and a lane tracker .",
    "the ntds is a first of its kind , at least for teenage drivers in the united states .",
    "the study provides valuable information on risky driving behavior , which can be assessed in terms of elevated gravitational force ( @xmath0-force ) events ( rapid start , hard stop , hard turn and yaw ) .",
    "counts of @xmath0-force events are available for each trip ( defined as ignition on to ignition off ) , and their incidence rates represent different aspects of risky driving behavior .",
    "the ntds data set comprises more than 68,000 trips by the 42 teen subjects , with an average of 1,626 trips per subject .",
    "figure [ fig1 ] provides a summary of the ntds data by subject in terms of the number of trips made and the total number of miles driven .",
    "an important goal in our analysis of the ntds data is to understand how risky driving is associated with subject - level covariates ( i.e. , individual characteristics such as gender ) as well as trip - level or time - dependent covariates ( e.g. , time since licensure , presence of passengers ) .",
    "there is an extensive literature on longitudinal data analysis ; see , for example , @xcite , @xcite and @xcite .",
    "common approaches to longitudinal data analysis include random effect models [ @xcite , @xcite ] and generalized estimating equations ( gee ) [ @xcite , @xcite , @xcite ] .",
    "in general , the two approaches have different interpretations ( subject - specific versus marginal ) , although that distinction is not important when the log link is used for count data . for analyzing the ntds data ,",
    "it is natural to consider generalized linear mixed models ( glmm ) with appropriate random effects to account for population heterogeneity , serial correlation and/or possible overdispersion [ @xcite , @xcite , @xcite ] .",
    "however , a realistic glmm for the ntds data would involve several random components , including a latent process that induces serial correlation ( see section [ analysis ] ) , and the computational demand of such a glmm analysis can be prohibitive with thousands of trips per subject . even if the computation is feasible , the resulting inference may be sensitive to modeling assumptions that are difficult to verify .",
    "the computational burden can be reduced by resorting to a marginal analysis using the gee approach , especially under a working independence assumption .",
    "the gee approach only requires specification of the first two moments and not the entire distribution .",
    "the robust variance estimate can be used to make asymptotically valid inference that only requires correct specification of the mean structure , assuming that the number of subjects is large relative to the number of observations per subject .",
    "the exact opposite situation occurs in the ntds , and @xcite have shown that the robust variance estimate can perform poorly in such a situation and that the model - based variance estimate may be preferable .",
    "numerous methods have been proposed to improve upon the robust variance estimate , including jackknife [ @xcite , @xcite ] , bias correction [ @xcite ] and window subsampling techniques [ @xcite , @xcite ] .",
    "given the well - known advantages and potential issues of the gee approach , it is natural to ask how to perform a simple and valid marginal analysis of the ntds data with reasonable efficiency and robustness .",
    "we attempt to address this question in the present paper by examining some existing methods and developing new ones .",
    "we find that it is generally helpful to include a fixed effect for each subject in the gee model .",
    "once the subject effects are estimated , they can be treated as the response variable in a subsequent linear regression analysis for estimating the effects of subject - level covariates .",
    "these fixed effects also help with trip - level covariates by removing the correlation due to population heterogeneity .",
    "if the counts are large ( with a marginal mean of 1 , say ) , the effects of trip - level covariates can be estimated from a conventional gee analysis using an estimated covariance matrix together with the robust variance estimate . for small counts ( with a marginal mean of 0.1 ) , the conventional approach is not satisfactory , and we explore a within - cluster resampling ( wcr ) approach [ @xcite , @xcite ] .",
    "the wcr approach was originally proposed to deal with informative cluster sizes , which is not of concern in the ntds .",
    "our motivation for considering wcr is to improve the performance of gee methods by altering the data structure . to this end",
    ", we consider extensions of wcr that reduce serial correlation within clusters or increase the number of ( approximately ) independent clusters .",
    "this leads to a wcr method involving separated blocks which performs better than the conventional methods .",
    "the rest of the paper is organized as follows . in the next section we set up the notation ,",
    "formulate the problem , and discuss potential issues .",
    "then we examine the standard gee methods in section [ gee ] and explore some wcr methods in section  [ mo ] .",
    "the ntds data are analyzed in section [ analysis ] using the appropriate methods .",
    "the paper concludes with a discussion in section [ disc ] .",
    "let @xmath1 ( @xmath2 ; @xmath3 ) denote the number of events that occur during the @xmath4th trip by the @xmath5th subject . for reasons that will become clear later ,",
    "we distinguish subject - level covariates such as gender from trip - level covariates such as time since licensure , writing @xmath6 for the former group of covariates and @xmath7 for the latter .",
    "note that @xmath7 may include interactions between subject - level and trip - level covariates . with @xmath8",
    "denoting the mileage of the @xmath4th trip by the @xmath5th subject , the marginal model of interest to us may be written as @xmath9 where @xmath10 , @xmath11 and @xmath12 are unknown parameters , the latter two being of primary importance . without loss of generality , we treat the covariates as fixed .",
    "estimation of the parameters in model ( [ 10 ] ) is challenged by the fact that the @xmath1 from the same subject tend to be correlated due to considerable variability between drivers as well as serial correlation ( over time ) within drivers .",
    "these sources of correlation are often accounted for using random effects [ e.g. , @xcite , @xcite ] .",
    "for example , one might postulate that , conditional on the random effects @xmath13 , @xmath14 and  @xmath15 , the @xmath1 are independent and each follows a poisson distribution with conditional mean @xmath16 where @xmath13 induces some heterogeneity between drivers ( beyond that explained by  @xmath6 ) , @xmath14 generates serial correlation among trips close in time , and @xmath15 accounts for any additional overdispersion relative to the poisson model .",
    "it is often assumed that the @xmath14 for a given subject arise from a subject - specific stochastic process @xmath17 through the relationship @xmath18 , where @xmath19 denotes the time of the @xmath4th trip .",
    "note that model ( [ 20 ] ) is consistent with model  ( [ 10 ] ) because it implies the same mean structure after integrating out the random effects , provided the distributions of the random effects do not depend on the covariates .    for the ntds data , it seems reasonable to equip model ( [ 20 ] ) with the following distributional assumptions .",
    "one might assume that @xmath20 , @xmath21 , and @xmath17 is a zero - mean gaussian process with a covariance structure given by @xmath22 the random effects @xmath13 and @xmath15 and the process @xmath17 are assumed independent of each other , although the @xmath18 are necessarily correlated within each subject .",
    "the parameter @xmath23 determines how rapidly the serial correlation decreases with the gap time .",
    "the process @xmath17 is known as the ornstein  uhlenbeck process [ @xcite ] , and the above distributional assumptions will be collectively referred to as the gauss  ornstein ",
    "poisson ( goup ) model .",
    "it is possible to perform a maximum likelihood analysis under the goup model [ e.g. , @xcite ] .",
    "however , such an analysis can be computationally demanding because of the serial correlation , and the resulting inference may be sensitive to the distributional assumptions involved . the methods to be considered for",
    "our marginal analysis will not require the full strength of the goup model , and they may or may not involve the distributional assumptions in the goup model .",
    "some of the methods we consider do incorporate such distributional assumptions into estimating equations through a working covariance matrix , in which case we also assess the robustness of the resulting inference against misspecification of the correlation structure . our goal is to make valid and reasonably efficient inference on @xmath11 and @xmath12 in model ( [ 10 ] ) with minimal dependence on the distributional assumptions in the goup model .    without the distributional assumptions , model ( [ 20 ] )",
    "does play a crucial role in this paper , as a way to conceptualize the different sources of variability .",
    "we assume that the different subjects are independent of each other and that the random effects @xmath13 and @xmath24 and the random process @xmath17 are independent of each other within each subject .",
    "it follows that the @xmath1 ( @xmath25 ) are conditionally independent given @xmath13 and @xmath17 .",
    "another important implication is that=-1 @xmath26=0 where @xmath27=\\mbox { const.}+\\alpha'z_i+b_i.\\ ] ] this shows that the correlation due to @xmath13 can be removed by treating subject as a fixed effect .",
    "obviously , this approach would not work in the usual asymptotic theory assuming a large number of subjects and a limited number of observations per subject .",
    "however , it might be appropriate when the number of subjects is small and the number of observations per subject is large as in the ntds .",
    "note that the subject - specific model ( [ 30 ] ) does not directly involve @xmath11 , which has been absorbed into the subject - specific intercept  @xmath28 ; thus ,  @xmath11 can not be estimated directly by fitting model ( [ 30 ] ) .",
    "nonetheless , equation  ( [ 40 ] ) suggests that once @xmath28 has been estimated , say , by @xmath29 , it should be possible to estimate @xmath11 from a subject - level linear model regressing @xmath29 on  @xmath6 .",
    "assuming working independence , a  standard gee analysis can be performed with or without fixed subject effects ( fse ) , using either the robust variance estimate or the model - based variance estimate .",
    "this gives rise to four possible methods for estimating  @xmath12 . as discussed earlier ,",
    "@xmath11 is not directly estimable from a gee analysis with fse but can be recovered from a subsequent linear regression analysis based on ( [ 40 ] ) .",
    "let @xmath29 denote the gee estimate of @xmath28 , and assume that @xmath29 is approximately unbiased , that is , @xmath30 .",
    "this , together with ( [ 40 ] ) , implies that @xmath31 an approximately unbiased estimate of @xmath11 can then be obtained from a  subject - level linear model regressing @xmath29 on @xmath6 . a standard least squares ( ls )",
    "algorithm can be used to fit this model if the @xmath29 can be treated as independent . in general , the @xmath29 are correlated due to correlated errors in gee estimation , even though the @xmath28 are indeed independent of each other .",
    "this correlation can be taken into account using an iteratively reweighted least squares ( irls ) algorithm described in appendix [ appa ] .",
    "thus , under working independence , @xmath11  can also be estimated using four different methods ( ls and irls with fse , robust and model - based without fse ) .",
    "these methods are evaluated and compared in a simulation study mimicking the ntds .",
    "specifically , each simulated data set consists of 40 subjects and 60,000 trips ( 1500 per subject ) .",
    "the study duration is rescaled to the unit interval , over which the trips are uniformly distributed .",
    "the offset  @xmath32 follows a normal distribution with mean  1 and variance  1 ,  @xmath6 is generated as a bernoulli variable with success probability  0.5 , and  @xmath7 is taken to be the trip time ( i.e. , @xmath33 ) .",
    "given the covariates , the outcome  @xmath1 is generated according to the goup model described in section  [ form ] , with @xmath34 and @xmath35 or 300 , corresponding to longer- and shorter - lived serial correlation , respectively .",
    "these values are based on the ntds data ( see section [ analysis ] ) and the wide range for @xmath36 reflects a large amount of variability in its estimation ( see section [ ecm ] ) .",
    "we set @xmath37 for simplicity because the results change little over a range of realistic values for these parameters .",
    "we choose @xmath38 in ( [ 20 ] ) such that the marginal mean of  @xmath1 equals a  specified value ( 0.1 or 1 ) .",
    "this range for @xmath39 covers most kinematic measures in the ntds with varying thresholds .",
    "some measures are associated with larger counts , which are generally easier to deal with . in each scenario",
    "( combination of parameter values ) , 1,000 replicate samples are generated and analyzed using the methods described in the preceding paragraph .",
    "@lcccd2.2ccc@ & & & & & & & + & & & & & & & + 1&short&no&robust&0.00&0.39&0.33&0.90 + & & no&model - based&0.00&0.40&0.03&0.14 + & & yes&ls&0.00&0.33&0.32&0.95 + & & yes&irls&0.01&0.32&0.31&0.95 + & long&no&robust&0.00&0.42&0.34&0.90 + & & no&model - based&-0.01&0.42&0.03&0.11 + & & yes&ls&-0.01&0.33&0.33&0.94 + & & yes&irls&0.00&0.32&0.32&0.95 + 0.1&short&no&robust&0.02&0.41&0.33&0.90 + & & no&model - based&0.01&0.40&0.04&0.17 + & & yes&ls&-0.01&0.35&0.33&0.94 + & & yes&irls&-0.01&0.36&0.32&0.93 + & long&no&robust&-0.01&0.42&0.34&0.89 + & & no&model - based&0.00&0.40&0.04&0.15 + & & yes&ls&0.01&0.35&0.33&0.94 + & & yes&irls&0.00&0.33&0.32&0.95 +    table [ tb1 ] compares the four methods for estimating @xmath11 in terms of empirical bias , standard deviation , median standard error and coverage probability of intended 95% confidence intervals .",
    "all methods in table [ tb1 ] are nearly unbiased , suggesting that bias is not of concern here . in terms of efficiency ,",
    "the most important factor is clearly the use of fse , which consistently results in smaller standard deviations .",
    "with fse in the model , one might expect the irls estimate of @xmath11 , which accounts for the correlation among the @xmath40 , to be more efficient than the ls estimate .",
    "however , their difference seems rather small in table [ tb1 ] , for two reasons .",
    "first , the variability due to estimating the @xmath28 is dominated by the variability in the @xmath28 , resulting in weak correlation among the @xmath29 in this particular example .",
    "second , a referee pointed out that any correlation that does exist among the @xmath29 should be approximately exchangeable , in which case the ls estimate is still efficient .",
    "the precision in estimating @xmath11 appears insensitive to the length of the serial correlation ( specified through @xmath36 ) , although the fse estimates do seem to become more variable for low counts [ @xmath41 .",
    "the use of fse also helps with variance estimation . without fse ,",
    "the model - based variance estimate is clearly disastrous , as expected , and even the robust variance estimate is unsatisfactory , with sub - nominal coverage ( @xmath4290% ) . including fse in the gee model leads to reasonable variance estimates and nearly correct coverage , under both ( ls and irls ) approaches .",
    "thus , it seems that the key to valid and efficient inference about @xmath11 in similar situations is to include fse in a  gee analysis followed by a subject - level linear regression analysis for the estimated fse .",
    "@lcccd2.2cccc@ & & & & & & & & + & & & & & & & & +   + 1&short&no&robust&0.00&0.12&0.09&0.91&0.0 + & & no&model - based&0.00&0.12&0.05&0.67&0.0 + & & yes&robust&-0.01&0.12&0.09&0.90&0.0 + & & yes&model - based&0.00&0.12&0.04&0.47&0.0 + & long&no&robust&0.01&0.21&0.16&0.92&0.0 + & & no&model - based&-0.01&0.21&0.05&0.41&0.0 + & & yes&robust&-0.01&0.21&0.16&0.91&0.0 + & & yes&model - based&-0.01&0.21&0.04&0.28&0.0 + 0.1&short&no&robust&0.00&0.13&0.10&0.90&0.0 + & & no&model - based&-0.01&0.13&0.07&0.74&0.0 + & & yes&robust&0.00&0.13&0.10&0.90&0.0 + & & yes&model - based&0.00&0.13&0.06&0.65&0.0 + & long&no&robust&0.02&0.21&0.17&0.90&0.0 + & & no&model - based&0.00&0.21&0.07&0.49&0.0 + & & yes&robust&-0.01&0.21&0.17&0.91&0.0 + & & yes&model - based&-0.01&0.21&0.06&0.42&0.0 + [ 3pt ] + 1&short&yes&robust&0.00&0.07&0.07&0.95&0.0 + & & yes&model - based&-0.01&0.07&0.07&0.95&0.0 + & long&yes&robust&-0.01&0.12&0.12&0.94&0.0 + & & yes&model - based&0.00&0.12&0.11&0.90&0.1 + 0.1&short&yes&robust&0.00&0.09&0.10&0.92&4.6 + & & yes&model - based&0.00&0.10&0.10&0.95&4.9 + & long&yes&robust&0.00&0.15&0.16&0.90&3.5 + & & yes&model - based&-0.01&0.15&0.12&0.87&3.9 + [ 6pt ] + 1&varying&yes&robust&-0.02&0.09&0.09&0.93&0.0 + & & yes&model - based&-0.03&0.09&0.08&0.92&0.0 + 0.1&varying&yes&robust&-0.02&0.11&0.12&0.90&3.7 + & & yes&model - based&-0.02&0.11&0.11&0.93&2.9 +    estimation of @xmath12 is a different story , as shown in the top section of table  [ tb2 ] .",
    "as in the case of estimating @xmath11 , bias is not a major issue in estimating  @xmath12 .",
    "the precision in estimating @xmath43 depends mostly on the length of the serial correlation , with better precision for shorter - lived serial correlation , and seems insensitive to other factors ( e.g. , fse , mean count ) .",
    "this is clearly different from the situation in table [ tb1 ] , and an intuitive explanation is the following . in general , estimating the effect of a subject - level covariate is essentially comparing one group of subjects with another , while estimating the effect of a trip - level covariate is essentially comparing one group of trips with another group of trips by the same subjects . with @xmath33 ,",
    "estimation of @xmath12 is basically a comparison of later trips with earlier trips , and it seems natural that serial correlation has a larger impact on this comparison than on a comparison of boys with girls , say .",
    "the latter comparison , on the other hand , is more likely to benefit from the use of fse to separate the relevant information ( i.e. , overall incidence rates of individual drivers ) from the noise ( i.e. , within - subject variation ) . in table",
    "[ tb2 ] , none of the four methods is satisfactory in terms of coverage , though the robust variance estimate performs better than the model - based one .",
    "several alternatives to the robust variance estimate have also been explored with little success ( see section [ disc ] ) .",
    "we now consider gee methods incorporating the covariance matrix for the @xmath1 . under the goup model described in section [ form ] , it is straightforward to show , by appealing to well - known properties of normal and poisson distributions , that @xmath44,\\nonumber\\\\[-8pt]\\\\[-8pt ] & & \\eqntext{(j\\not = j').}\\end{aligned}\\ ] ] these expressions provide the marginal covariance matrix relevant in a  gee analysis without fse .",
    "with fse in the model , we need to condition on @xmath13 and the relevant formulas become @xmath45,\\nonumber\\\\[-8pt]\\\\[-8pt ] & & \\eqntext{(j\\not = j'),}\\end{aligned}\\ ] ] with @xmath28 defined in section [ form ] .",
    "note that @xmath46 is not involved in the covariance matrix in a gee analysis with fse .",
    "unknown parameters in the covariance matrix ( @xmath47 , @xmath48 , @xmath36 and possibly @xmath46 ) can be estimated by applying moment methods and nonlinear regression techniques to residuals from a preliminary gee analysis with working independence ( see appendix [ appb ] for details ) .",
    "this preliminary gee analysis can be performed with or without fse , regardless of the primary gee analysis for estimating @xmath12 .",
    "with fse in the preliminary gee analysis , the aforementioned techniques do not provide an estimate of  @xmath46 .",
    "if desired , an estimate of  @xmath46 can be obtained from the irls estimation of  @xmath11 or simply as the error variance in the ls analysis , ignoring the fact that @xmath29 is an error - prone estimate of @xmath28 ( see section [ wind ] and appendix [ appa ] for details ) .",
    "thus , the covariance matrix can be estimated using three methods : fse - ls , fse - irls and no fse , with the first two differing only in the estimate of  @xmath46 .",
    "table [ tb3 ] shows the performance of the above three methods for estimating parameters in the covariance structure , in terms of bias , standard deviation and convergence .",
    "the methods are evaluated in the same scenarios as the previous simulation experiments , with the addition of a higher mean count  ( 10 ) to show a more complete picture . for a mean count of 10 ,",
    "the fse methods are nearly unbiased for the variance components but biased for @xmath36 in a way that results in underestimation of the length of the serial correlation .",
    "the bias for @xmath36 is larger for longer - lived serial correlation .",
    "the no fse method is more biased for @xmath36 ( in the same direction ) and even visibly biased for @xmath46 . with decreasing counts",
    ", the bias problem becomes more severe , especially for @xmath36 , to the extent that estimation of the serial correlation is meaningless for a mean count of @xmath49 .",
    "the problem is compounded by a  large amount of variability in estimating @xmath36 .",
    "another issue with low counts is convergence in the nonlinear regression . for a mean count of @xmath49 ,",
    "the convergence failure rates are approximately 8% and 13% ( with short- and long - lived serial correlation , resp . ) for the fse methods and even worse for the no fse method .",
    "the fse methods appear more reasonable than the no fse method , and we will use fse - ls , which is simpler than fse - irls , in the subsequent simulations .",
    "gee methods based on covariance matrices estimated through fse - ls are evaluated in the same setting described in section [ wind ] .",
    "when the nonlinear regression in fse - ls fails to converge , the data - driven initial values will be taken as the final estimates [ see equation ( [ b10 ] ) in appendix [ appb ] ] . with thousands of trips per subject ,",
    "inverting a covariance matrix is time - consuming , and iterating until convergence in the usual fisher scoring algorithm [ @xcite , section 3.2.4 ] would be prohibitive .",
    "we therefore settle for a one - step estimate of the regression coefficients based on just one iteration in the fisher scoring algorithm .",
    "specifically , we start with a gee analysis with working independence , use the residuals to estimate parameters in the covariance matrix , and update the regression coefficients just once using the estimated covariance matrix . under this approach , it is necessary to include fse in each gee analysis ; otherwise meaningful simulation results can not be produced .",
    "this is not surprising because inversion of large covariance matrices can be difficult when the correlation is strong and poorly estimated .",
    "this is not a serious limitation because the use of fse leads to better efficiency and coverage anyway , which we have observed in simulations where the true covariance matrix is used in gee ( results not shown ) .",
    "once fse are included , numerical problems become much less frequent ( 35% for a mean count of 0.1 , @xmath501% for higher counts ) .    we have found that incorporating the covariance matrix into the estimating equation does little to improve the estimation of @xmath11 in terms of efficiency and coverage . we therefore omit the results for estimating @xmath11 and henceforth focus on estimating @xmath12 .",
    "the results for the latter , reported in the middle section of table [ tb2 ] , show that better efficiency is attained using an estimated covariance matrix than under working independence . in terms of coverage , the robust variance estimate performs well for higher counts but not for lower ones , especially when the serial correlation is long - lived .",
    "the model - based variance estimate performs well for short - lived serial correlation and poorly for long - lived serial correlation .",
    "the former case shows that underestimating the length of the serial correlation has little effect when the serial correlation is already negligibly short .",
    "although not shown in table [ tb2 ] , the model - based variance estimate has been found to work well in all cases if the true covariance matrix is used .",
    "thus , the root of the problem appears to be the difficulty in estimating @xmath36 .",
    "it is natural to ask how the gee methods based on the goup model would perform if the model is misspecified . for the ntds",
    ", the goup model seems plausible for the most part .",
    "it is possible , however , that the length of the serial correlation , which is assumed constant , might change over time . to formalize this notion of changing length , note that the covariance structure given by ( [ 50 ] ) can be generalized into @xmath51 for a function @xmath52\\to(0,\\infty)$ ] . for a constant @xmath36",
    ", this reduces to the original goup model . in the next set of simulation experiments",
    ", we will generate data using the generalized goup model with @xmath36 taken to be a straight line from @xmath53 to @xmath54 .",
    "thus , instead of working exclusively with short- or long - lived serial correlation , we allow it to be short - lived at the beginning and to gradually become long - lived at the end .",
    "this is motivated by the conjecture that teenage driving will tend to be haphazard at the beginning and will become more consistent over time .",
    "unfortunately , it appears difficult to verify this conjecture directly using the data .",
    "indeed , even when  @xmath36 is a constant , we have seen in table [ tb3 ] that it can be really difficult to estimate , especially when the counts are low .",
    "it seems reasonable to expect that a time - varying @xmath36 will be even more difficult to estimate .",
    "we therefore take a sensitivity analysis approach to this issue .",
    "the bottom section of table [ tb2 ] shows the performance in the present setting of gee methods based on the original goup model .",
    "the point estimates are more ( or less ) efficient in this setting than if the serial correlation is consistently long - lived ( or short - lived ) .",
    "the coverage property of the model - based variance estimate is also intermediate between the two extremes .",
    "as before , the robust variance estimate works well for large counts but not for small ones .      to summarize , valid inference on @xmath11 can be made through a gee analysis with fse ( regardless of the covariance matrix ) followed by a  linear regression analysis , while inference on @xmath12 requires more work . for large counts , valid inference on @xmath12",
    "can be made by estimating the covariance matrix and using the robust variance estimate ; in this case , the inference seems robust with respect to the serial correlation structure . for small counts ,",
    "the robust variance estimate is unsatisfactory and the model - based variance estimate performs well only for short - lived serial correlation . because the serial correlation is difficult to estimate with small counts , one would not know in practice whether the model - based variance estimate is appropriate for a particular application .",
    "therefore , it is necessary to develop methods that perform well ( or better ) for small counts with short- and long - lived serial correlation .",
    "this is the focus of the next section , where we explore wcr - type methods .",
    "wcr is a resampling approach for marginal analysis of clustered data , originally designed to deal with informative cluster sizes [ @xcite , @xcite ] . in its original form",
    ", the approach can be applied to the present problem as follows .",
    "suppose we sample a trip from each subject randomly and form a subset of nonclustered data @xmath55 where @xmath56 is uniformly distributed on @xmath57 , independently across @xmath5 .",
    "there is no correlation in the subsample ( [ 110 ] ) , which can therefore be analyzed using a quasi - poisson model with mean structure given by ( [ 10 ] ) .",
    "this can be repeated a large number ( say , @xmath58 ) of times to yield @xmath59 , where @xmath60 denotes the point estimate of @xmath61 based on the @xmath62th subsample , and @xmath63 is the associated variance estimate .",
    "then the wcr estimate of  @xmath64 is given by @xmath65 and the associated variance estimate is @xmath66 the first term on the right - hand side of ( [ 120 ] ) measures the variability within subsamples , while the second term measures the variability between subsamples . in general , for any wcr method to work , it is essential that valid estimates @xmath67 be obtained from each subsample .",
    "note that the estimates  @xmath68 are identically distributed .",
    "if @xmath60 is biased for @xmath64 , then @xmath69 given by  ( [ 115 ] ) is also biased by the same amount .",
    "similarly , if @xmath70 underestimates the sampling variance of  @xmath71 , then @xmath72 given by ( [ 120 ] ) will underestimate the sampling variance of @xmath73 by a similar amount , because the second term on the right - hand side of ( [ 120 ] ) will be approximately unbiased for large @xmath58 .",
    "furthermore , in terms of relative bias , the underestimation problem of @xmath72 is generally worse than that of @xmath63 , simply because the true variance of @xmath69 is smaller than that of @xmath60 . in light of these observations",
    ", we will study wcr methods in two steps , always considering a single subsample before moving to repeated sampling .",
    "there is no reason why a wcr subsample must consist of exactly one observation from each subject .",
    "in fact , to fit a model with fse , there has to be more than one observation from each subject . without fse ,",
    "the standard wcr approach is feasible but , as we shall see , does not work well .",
    "we therefore consider an extended wcr approach where a simple random sample ( srs ) is taken from each subject to form an wcr subsample .",
    "thus , instead of a single number @xmath56 , we take an srs @xmath74 from the index set @xmath57 for each @xmath5 .",
    "the resulting subsample is @xmath75 for which a standard gee analysis can be performed , under working independence or using an estimated covariance matrix , with or without fse , using the robust or model - based variance estimate .",
    "this can be repeated a  large number of times , and the resulting estimates can be combined using  ( [ 115 ] ) and  ( [ 120 ] ) .",
    "if the covariance matrix for the @xmath1 is used , it will be estimated once from the full sample and the estimate will be applied to all subsamples .",
    "@lccd3.0d3.0d3.2d4.2ccc@ & & & & & & & & + & & & & & & & & & +   + short&no&robust&1&1&-15.40&357.22&1.59&0.64&0.4 + & & & 5&1&-0.02&1.44&0.99&0.86&0.0 + & & & 25&1&0.03&0.64&0.52&0.91&0.0 + & & & 100&1&-0.02&0.37&0.28&0.91&0.0 + & & & 500&1&-0.01&0.20&0.15&0.92&0.0 + & & & 100&500&0.00&0.13&0.00&0.16&0.0 + & yes&robust&5&1&0.09&1.59&1.16&0.87&0.2 + & & & 25&1&0.01&0.68&0.50&0.87&0.1 + & & & 100&1&-0.02&0.37&0.28&0.90&0.1 + & & & 500&1&0.00&0.18&0.14&0.91&0.0 + & & & 100&500&0.00&0.12&0.00&0.18&0.0 + long&no&robust&1&1&32.12&2011.24&1.64&0.64&0.8 + & & & 5&1&-0.08&1.43&1.00&0.89&0.0 + & & & 25&1&-0.01&0.72&0.53&0.89&0.0 + & & & 100&1&0.01&0.42&0.31&0.92&0.0 + & & & 500&1&0.00&0.24&0.20&0.91&0.0 + & & & 100&500&-0.01&0.21&0.10&0.52&0.0 + & yes&robust&5&1&-0.62&19.13&1.13&0.85&0.1 + & & & 25&1&-0.02&0.71&0.52&0.89&0.1 + & & & 100&1&0.00&0.39&0.30&0.90&0.0 + & & & 500&1&-0.01&0.24&0.20&0.90&0.0 + & & & 100&500&-0.01&0.21&0.10&0.56&0.0 +    @lccd3.0d3.0d2.2cccc@ & & & & & & & & + & & & & & & & & & +   + short&yes&robust&5&1&0.03&1.44&0.89&0.75&0.4 + & & & 25&1&0.02&0.59&0.48&0.85&1.4 + & & & 100&1&0.01&0.30&0.29&0.91&3.3 + & & & 500&1&-0.01&0.15&0.15&0.92&4.6 + & & & 100&500&0.00&0.09&0.21&0.70&2.6 + & & model - based&5&1&-0.09&1.49&1.19&0.91&0.2 + & & & 25&1&0.02&0.58&0.55&0.94&1.1 + & & & 100&1&0.00&0.29&0.30&0.94&2.2 + & & & 500&1&0.00&0.15&0.15&0.97&4.1 + & & & 100&500&0.01&0.10&0.12&0.68&1.4 + long&yes&robust&5&1&-0.04&1.48&0.93&0.76&0.4 + & & & 25&1&0.00&0.61&0.51&0.87&1.2 + & & & 100&1&0.00&0.31&0.30&0.90&2.3 + & & & 500&1&-0.01&0.18&0.19&0.90&3.1 + & & & 100&500&0.00&0.15&0.24&0.69&2.8 + & & model - based&5&1&-0.02&1.67&1.22&0.92&0.4 + & & & 25&1&0.01&0.59&0.54&0.93&1.9 + & & & 100&1&-0.01&0.32&0.31&0.94&2.4 + & & & 500&1&0.00&0.18&0.17&0.92&3.2 + & & & 100&500&0.00&0.16&0.16&0.72&2.6 +    this wcr - srs approach is evaluated in table [ tb4 ] . given the findings in the last section , this evaluation is focused on small counts . under working independence , only the robust variance estimate is considered because the model - based variance estimate is clearly invalid . as before , no meaningful results are available when an estimated covariance matrix is used without fse . for each method , we begin by analyzing a single subsample , with @xmath76 and different values of @xmath77 ( size of the srs from each subject ) .",
    "table [ tb4 ] shows poor performance for @xmath78 ( i.e. , standard wcr ) , apparently due to erratic estimates , which are likely to arise when the subsample is very small . even for @xmath79 , the point estimate",
    "is visibly biased in some cases , probably because the subsample is still small .",
    "the coverage probability generally increases with @xmath77 , at least for @xmath80 .",
    "the best coverage is seen when an estimated covariance matrix is used together with fse and the model - based variance estimate . as explained in section [ ecm ] , this method tends to work well when the serial correlation is weak , which is precisely what happens to a randomly selected subset of trips , which tend to be farther apart from each other than in the full sample .",
    "this explains why better coverage is seen here than in the corresponding portion of table [ tb2 ] .",
    "of course , this trend will be reversed when the number of selected trips gets too large , which explains the slight drop in coverage probability when @xmath77 increases further to 500 .",
    "overall , it appears that @xmath81 would be a reasonable choice in terms of bias and coverage .",
    "efficiency is not a major consideration here because the efficiency loss due to @xmath82 can be recovered through repeated sampling .",
    "table  [ tb4 ] also shows the performance of the wcr - srs method with @xmath83 and @xmath81 , whose efficiency levels appear similar to the corresponding results in table  [ tb2 ] , as expected . for the working independence methods ,",
    "the wcr variance estimate is seriously biased downward and the coverage probability is far below the nominal level , confirming the conjecture in section [ smo ] that the variance underestimation problem of  @xmath72 is generally worse than that of @xmath63 in terms of relative bias . for methods based on an estimated covariance matrix ,",
    "the wcr variance estimate is not necessarily biased downward but the coverage properties are not good .",
    "this behavior is probably due to erratic estimates which arise frequently in repeated analyses of subsamples when a poorly estimated covariance matrix is used .",
    "table [ tb4 ] does suggest a potential benefit of wcr , that is , the ability to produce a sparse subset of trips with weaker serial correlation .",
    "this advantage can be exploited further by selecting trips that are separated from each other by a specified amount .",
    "specifically , for a given positive integer @xmath84 , we can choose a trip randomly among the first @xmath85 trips and every @xmath86st trip from there on , until the end of the sequence .",
    "this can be done for each subject and the process can be repeated many times as before .",
    "this approach does lead to good coverage in single outputation when an estimated covariance matrix is used together with fse and the model - based variance estimate ( results not shown ) .",
    "however , upon moving to wcr , this approach exhibits the same strange behavior as seen in table [ tb4 ] ( results not shown ) , probably for the same reason .",
    "the previous experiments with wcr show that variance underestimation can be a serious problem , especially when the variance of @xmath87 is much smaller than that of @xmath60 .",
    "the latter difference can be reduced by including more observations in each subsample , and the question then becomes how to ensure reasonable performance of the variance estimate based on a subsample . to this end , we propose the following wcr approach involving separated blocks .",
    "with fse in the model , the only source of correlation is serial correlation , which is weaker for trips farther apart .",
    "thus , with an appropriate amount of separation , trips from the same subject can be treated as approximately independent .",
    "let @xmath88 denote the block size ( i.e. , the number of consecutive trips to be analyzed together as a block ) and @xmath84 the separation ( i.e. , the number of trips used to separate the blocks ) . for a given subject ,",
    "one possible set of separated blocks can be formed by taking the first @xmath88 trips , skipping the next @xmath84 trips , taking the next  @xmath88 , skipping the next @xmath84 , and so on .",
    "each group of @xmath88 trips sampled together will be treated as a block and the different blocks will be treated as independent in the subsequent gee analysis .",
    "a random shift can be added to this sampling process , and the random sampling process can be repeated many times as before .",
    "preliminary simulation results , as well as those in table [ tb4 ] , suggest that a reasonable block size would be @xmath89 .",
    "simple calculations based on ( [ 80 ] ) and ( [ 90 ] ) show that , with @xmath90 , the correlation between trips in different blocks is typically well below 0.05 in all scenarios considered in this paper . with these choices for @xmath88 and  @xmath84 , the wcr - sb approach is evaluated through simulations and the results are presented in table [ tb5 ] .",
    "@ld2.0ccccc@ & & & & & & + & & & & & & + short&1&0.00&0.15&0.13&0.94&0.0 + & 50&0.00&0.12&0.10&0.92&0.0 + long&1&0.00&0.22&0.19&0.92&0.0 + & 50&0.01&0.20&0.17&0.92&0.0 + varying&1&0.00&0.18&0.15&0.92&0.1 + & 50&0.00&0.15&0.13&0.92&0.0 +    we focus on working independence to avoid numerical stability issues , and only consider the robust variance estimate .",
    "the method appears to have better coverage when applied to a single set of separated blocks than to the full sample ( table [ tb2 ] ) , apparently owing to a larger number of approximately independent clusters ( 400 vs. 40 ) .",
    "note that the efficiency based on a single outputation is quite close to that in a full sample analysis . considering this and the computational demand ,",
    "the wcr version is implemented with @xmath91 repetitions , and the resulting wcr - sb estimates are virtually as efficient as the full sample estimates . in the case of long - lived serial correlation ,",
    "better coverage ( 92% ) is achieved here using the working independence method than in any of the previous simulations .",
    "the improvement ( 2% over table  [ tb2 ] ) is not dramatic but still substantial , and similar phenomena have been observed consistently in other simulation experiments under similar scenarios .",
    "table [ tb5 ] also shows the performance of the wcr - sb method when the serial correlation is generated according to ( [ 100 ] ) rather than ( [ 50 ] ) .",
    "the only noticeable effect of this change is an intermediate level of efficiency ( between the extreme cases of short- and long - lived serial correlation ) .",
    "our analysis of the ntds data concerns the following elevated @xmath0-force events : rapid start ( longitudinal @xmath92 ) , hard stop ( longitudinal @xmath93 ) , hard left / right turn ( lateral deceleration/@xmath94 ) , yaw ( @xmath955 degrees within 3 seconds ) , as well as a composite measure defined as the totality of these 5 types of events .",
    "yaw is a measure of correction after a turn and is calculated as the absolute change in angle between an initial turn and the correction . for each specific @xmath0-force event , the threshold ( in parenthesis )",
    "is chosen to allow meaningful differences between drivers and different driving conditions to be revealed and estimated with a sufficient number of events .",
    "the mean counts based on the chosen thresholds are 0.13 ( rapid start ) , 0.18 ( hard stop ) , 0.20 ( hard left turn ) , 0.15 ( hard right turn ) , 0.04 ( yaw ) and 0.70 ( composite measure ) .",
    "the mean count for yaw is relatively low , perhaps too low for the methods discussed in this paper , but the corresponding threshold is already the lowest for which we have data available .",
    "this limitation should be kept in mind when interpreting the results .    from the viewpoint of behavior science",
    ", it is reasonable to expect some serial correlation because risky driving may be related to the weather and the driver s mental and physical conditions ( e.g. , mood and fatigue ) .",
    "all of these factors could result in serial correlation unless they are all included in the model , which is impossible in the ntds due to the lack of this information . to explore",
    "the nature of the serial correlation in the ntds data , a  marginal model with fse and all relevant covariates ( to be described later ) is fit for the composite measure under working independence .",
    "pairwise products of standardized residuals are calculated for consecutive trips ( because the number of all possible pairs is too large ) . after sorting by gap time ,",
    "these pairs are grouped into 100 bins for further reduction . within each bin",
    ", we calculate the median gap time and the mean product of standardized residuals , and the results are plotted in figure [ fig2 ] together        with a  lowess smooth .",
    "figure [ fig2 ] suggests that some serial correlation is present in the data , although a precise characterization of the serial correlation seems difficult . under the goup model and using the techniques described in appendix [ appb ] , the estimated variance components are generally close to 1 ( for the composite measure and other @xmath0-force events ) , while the estimate of @xmath36 varies wildly and depends heavily on the initial value .    -force events ( rapid start , hard stop , hard left / right turn , yaw , and the composite measure ) as functions of time since licensure , estimated using regression splines ( one node for every 3 months ) . ]",
    "figure [ fig3 ] presents the estimated incidence rate ( ir ) per 100 miles of each @xmath0-force event as a function of time since licensure .",
    "the ir is defined by  ( [ 10 ] ) with @xmath8 replaced by 100 , @xmath6 empty , and @xmath7 consisting of indicators of calendar month and a set of regression splines for @xmath19 ( one knot for every 3 months ) .",
    "calendar month is included in the model to adjust for a possible seasonal effect , which may confound with the effect of time since licensure because the enrollment of subjects was not uniform with respect to calendar month , with more subjects enrolled in the fall and fewer in the spring .",
    "the estimates shown in figure [ fig3 ] represent geometric means of the monthly estimates .",
    "note that the ir considered here is a marginal quantity which involves the marginal intercept @xmath10 and which should therefore be estimated in a model without fse .",
    "the estimates in figure [ fig3 ] are obtained under working independence , because the goup model without fse is problematic to fit and its performance is not well understood ( see section [ ecm ] ) . under the regression spline model , there is some ambiguity as to whether the ir for the composite measure changes over time .",
    "the @xmath96-value for the regression splines is 0.49 based on the robust variance estimate under working independence , although the validity of this test may be questionable ( see section [ gee ] ) . under the goup model with fse ,",
    "the ir for the composite measure does appear to change over time ( @xmath97 for the robust variance estimate , @xmath98 for the model - based variance estimate ) .",
    "the latter observation is in contrast to a previous report that risky driving largely remains constant [ @xcite ] , and the difference is probably due to the lower thresholds used here , which lead to more events and perhaps more relevant information . despite the ambiguity about statistical significance ,",
    "the main conclusion from our marginal analysis is fairly consistent with the findings of @xcite .",
    "the latter reference reports irs for adults / parents that are consistently and substantially lower than those for novice teenagers driving the same vehicles . in figure",
    "[ fig3 ] , the estimated ir of the composite measure for teenage drivers increases over the first 6 months , declines over the next few months , and then increases again , suggesting the establishment of a risky driving style . this general impression is reinforced by the plots for the specific @xmath0-force events , which show some temporal changes but no clear trends .    as noted by a referee , some instances of risky driving ( e.g.",
    ", mistakes due to the lack of skills ) may depend more on the amount of practice , which can be quantified by accumulated mileage , than on time since licensure .",
    "this raises the question of which measure is more appropriate to adjust for .",
    "accumulated mileage is a measure of driving experience , while time since licensure reflects both experience and maturity . from a public health perspective , we believe that time since licensure is more relevant to policy makers .",
    "nonetheless , all analyses in this section have been repeated with time since licensure replaced by accumulated mileage , and the results are very similar and therefore omitted . with regard to figure [ fig3 ] , this similarity suggests that persistent risky driving among teenage drivers is not only due to inexperience but also to immaturity ( i.e. , aspects of adolescent development that motivate novice young drivers to seek excitement and under - recognize risks ) .",
    "each @xmath0-force event is further analyzed in a larger model that simultaneously adjusts for the following covariates : driver s gender , risky friends , time of day , passenger condition , calendar month , and time since licensure .",
    "the risky behavior of a teenage driver s friends is assessed using a 7-item index asking `` how many of your friends would you estimate   smoke cigarettes , drink alcohol , get drunk at least once a week , use marijuana , drive after having two or more drinks in the previous hour , exceed speed limits , and do not use safety belts ( none , a few , some , most , all ) . ''",
    "the assessment was made at 4 time points ( baseline , 6 , 12 and 18 months ) , the 4 scores were averaged for each driver , and the average score was then dichotomized according to the median split among all drivers in the study .",
    "night driving was determined from video data by visual observation of the ambient natural lighting at the start of the trip .",
    "late night was defined as 10 pm to 6 am , and night trips that did not start at late night were considered early night trips .",
    "the presence and relative age ( adult or teen ) of each passenger was determined by the coders examining the video data . in summary ,",
    "gender and risky friends enter the marginal model as subject - level binary covariates , time of day is a  trip - level covariate with 3 levels ( day , early night and late night ) , passenger condition is also a trip - level covariate with 3 levels ( none , teens , at least one adult ) , calendar month is a trip - level covariate with 12 levels , and time since licensure is represented by the same set of regression splines as before .",
    "no signs of model misspecification are observed in residual plots ( not shown ) .",
    "table [ tb6 ] presents estimates of incidence rate ratios ( irr ) that quantify the association of each @xmath0-force event with the aforementioned covariates ( except calendar month and time since licensure ) .",
    "an irr is just the result of exponentiating the corresponding regression coefficient in the marginal model .",
    "the estimates are obtained using standard gee methods ( working independence or the goup model , robust or model - based variance estimate ) as well as the proposed wcr - sb method ( working independence , @xmath89 , @xmath90 , @xmath91 ) .",
    "all of these methods include fse . in a sensitivity analysis , a different separation size ( @xmath99 ) for the wcr - sb method",
    "is used to produce similar results , which are therefore omitted .",
    "since the point estimates depend primarily on the working correlation structure , only 2 sets of point estimates are shown in table [ tb6 ] .",
    "for the subject - level covariates , the ls method is used to obtain 2 sets of 95% confidence intervals ( for working independence and the goup model ) . for the trip - level covariates ,",
    "table  [ tb6 ] presents 4 sets of 95% confidence intervals corresponding to the robust variance estimate and the wcr - sb method under working independence as well as the robust and model - based variance estimates under the goup model .",
    "table [ tb6 ] contains some unrealistically large values ( @xmath100100 , labeled as  @xmath101 ) for the irrs associating risky friends and gender with yaw and rapid start , probably due to very few events in the subgroup of teenage drivers acting as the denominator .",
    "despite some numerical differences between the different methods , the results in table [ tb6 ] indicate clearly that teenage risky driving is not associated with gender , positively associated with risky friends , negatively associated with early night , and negatively associated with the presence of passengers ( more strongly for adults than for teens ) .",
    "the evidence is not conclusive with regard to late night , whose association with risky driving is significant in some cases but not in others .",
    "the association of risky friends with risky driving points to potentially destructive effects of risk - accepting social norms and social identity on risky behavior . both the perception and the actuality of having risk - taking friends could contribute to the perceived acceptability of risky behavior .",
    "the association of early night with risky driving suggests that teens do recognize the danger of night driving ( at least partially ) and drive more carefully ( or rather , less carelessly ) at night .",
    "there is no contradiction between this finding and the apparent ambiguity about late night because late night trips often take place under unusual circumstances .",
    "finally , it is worth noting that passengers , especially adult passengers , appear protective with respect to risky driving .",
    "further research is warranted to confirm , better characterize and utilize such protective effects .",
    "even with a great variety of methods available for longitudinal data analysis , it can still be difficult to analyze longitudinal data in long sequences , especially when the serial correlation is strong and long - lived . in this paper",
    ", we examine standard gee methods and propose new ones for marginal analysis of longitudinal count data in a small number of very long sequences .",
    "the methods are evaluated and compared in simulation experiments mimicking the ntds , and the main findings can be summarized as follows .",
    "we consider the use of fse in this particular situation , a  simple technique with important practical implications .",
    "it allows the effects of subject - level covariates to be estimated easily from a linear regression analysis for the estimated fse , and it also helps with trip - level covariates by removing the correlation due to population heterogeneity . for trip - level covariates , we find that a standard gee analysis under working independence can lead to inefficient estimates and serious undercoverage , and that both problems can be alleviated by incorporating a properly specified correlation structure .",
    "the latter approach works well for large counts but not for small counts , mainly because the serial correlation is hard to estimate with small counts .",
    "we therefore explore an alternative approach ( wcr ) for the case of small counts .",
    "the original version of wcr and an extension to simple random sampling seem unsatisfactory , however , because of numerical instability in repeated analyses of small samples and a bias magnification effect of wcr that results in variance underestimation . to address these issues ,",
    "we propose an wcr - sb approach that involves separated blocks and that performs better than all of the previously considered methods ( wcr and standard gee ) .    in table",
    "[ tb6 ] the wcr - sb analyses are not dramatically different from the other analyses .",
    "this is reassuring to the scientists , but it also suggests that the ntds data set is not ideal for demonstrating the advantage of the wcr - sb approach . to put the latter point in perspective",
    ", we note that better performance in the frequentist sense does not imply better results in every realization .",
    "the performance of the wcr - sb approach has been evaluated in simulation experiments which are , in fact , designed to mimic the ntds .",
    "most of the uncertainty in designing the simulation experiments is associated with the length of the serial correlation , which is very difficult to estimate with small counts , as shown in table [ tb3 ] .",
    "it is certainly possible that the serial correlation in the ntds is not sufficiently long - lived for the new method to make a material difference . in that case ,",
    "a better example than the ntds might be a proposed follow - up study that involves 100 teenage drivers to be followed for at least 3 years .",
    "the larger amount of data from the latter study will afford a better understanding of both the nature and the length of the serial correlation .",
    "furthermore , as teenage drivers gain experience and perspective over time , their driving behavior may become less haphazard and more stable , in which case the serial correlation will gradually become longer - lived toward the end of the ( longer ) study duration .",
    "thus , in terms of the length of the serial correlation , the follow - up study will provide a better opportunity than the ntds to demonstrate the advantage of the wcr - sb approach .",
    "the wcr - sb approach is designed for longitudinal data in a small number of long sequences .",
    "the strength of this approach relative to standard gee methods is the ability to handle strong and long - lived serial correlation when the mean count is low .",
    "the weaknesses of the wcr - sb approach include increased computational demand ( relative to standard gee methods ) and the need for information about the length of the serial correlation ( in order to specify the separation size @xmath84 ) .",
    "because @xmath36 can be very difficult to estimate , one may need to consult the subject - matter scientist or perform a sensitivity analysis with different values of @xmath84 , as we did in analyzing the ntds data ( see section [ analysis ] ) .    as mentioned earlier at the end of section [ wind ] , we have actually explored several existing alternatives to the robust variance estimate in the present situation .",
    "no appreciable improvement has been achieved using a standard bootstrap procedure ( i.e. , sampling subjects with replacement ) and jackknife methods [ @xcite , @xcite ] .",
    "a  block bootstrap procedure [ @xcite ] appears to work well for short - lived serial correlation but not for long - lived serial correlation . in a simple",
    "setting that permits closed - form calculations of the finite - sample target of the robust variance estimate ( with sample quantities replaced by their population counterparts ) under working independence , we have found that the target can be far below the sampling variance of the point estimate observed in simulations , suggesting that the asymptotic theory may not provide a reasonable approximation in this situation .",
    "given that , it seems unlikely that a substantial improvement can be made using the available bias correction methods [ e.g. , @xcite ] .",
    "another possible approach to variance estimation is window subsampling [ @xcite , @xcite , @xcite ] .",
    "a practical difficulty with this approach is specification of the window size , which has been found to have a  large impact on the variance estimate . @xcite have suggested taking the maximum among variance estimates based on several window sizes , but it remains unclear how to choose the set of window sizes for the maximization . maximizing over a large set of window sizes can lead to a very conservative variance estimate , as the maximum among several underestimators need not be an underestimator itself .    under the wcr approach ,",
    "we have considered various sampling schemes based on time .",
    "an interesting possibility , suggested by a referee , is to sample trips using a mechanism that involves other covariates than time and possibly the outcome . in the latter case",
    ", appropriate adjustments will be necessary to account for the outcome - dependent nature of the subsample .",
    "further research is needed to explore the potential benefits of the more sophisticated sampling schemes .",
    "this article has been focused on valid inference in the sense of ( nearly ) correct coverage .",
    "we have not discussed the issue of generalization from a sample of subjects to the target population , which is always important and especially so when the number of subjects is small .",
    "such generalizations should be easier to justify when the within - subject variability is large relative to the between - subject variability , which appears to be the case in the ntds .",
    "write @xmath102 and @xmath103 , and let @xmath104 denote the variance estimate from the gee analysis",
    ". as the notation indicates , @xmath105 estimates the conditional variance @xmath106 because the gee model with fse is conditional on @xmath107 .",
    "the marginal variance of @xmath108 is given by @xmath109 where @xmath110 denotes the @xmath111 identity matrix .",
    "a natural estimate of @xmath112 can be obtained as @xmath113 provided a reasonable estimate @xmath114 is available .",
    "this suggests the following irls algorithm .",
    "we start by obtaining an initial estimate of @xmath11 , say , the ls estimate",
    ". then we proceed to the following steps :    calculate a moment estimate of @xmath46 , say , the mean squared residual minus the mean diagonal element of @xmath115 ;    substitute the estimate of @xmath46 into ( [ a10 ] ) and re - estimate @xmath11 from a  weighted least squares analysis based on the updated @xmath116 .",
    "these steps can be iterated a number of times or until convergence .",
    "let @xmath117 denote the fitted values from a preliminary gee analysis without fse .",
    "then ( [ 60 ] ) shows that a moment estimate of the sum of @xmath46 , @xmath47 and @xmath118 can be obtained as @xmath119,\\ ] ] where @xmath120 is the total number of observations .",
    "further , equation  ( [ 70 ] ) suggests that @xmath46 , @xmath47 and @xmath121 can be estimated from a nonlinear regression analysis with @xmath122 as the response variable and with mean function @xmath123 the above quantities could be modified using bias correction adjustments [ e.g. , @xcite , section 3.2 ] and/or smoothing techniques prior to the nonlinear regression analysis .",
    "however , such modifications have not been found helpful in our experiments .",
    "ideally , the nonlinear regression analysis should include all pairs of trips within subjects",
    ". however , this may be impractical for the ntds data because a subject with thousands of trips would contribute millions of pairs , resulting in too many data points . as a compromise",
    ", we propose to base the nonlinear regression analysis on two types of pairs : consecutive trips ( with @xmath124 ) and symmetric trips ( with @xmath125 ) .",
    "the pairs of consecutive trips are informative about short - lived serial correlation , while the pairs of symmetric trips help characterize the overall correlation over the entire range of the gap time @xmath126 .",
    "similar techniques can be used for a preliminary gee analysis with fixed subject effects . with @xmath127 denoting the fitted values with fixed subject effects , the sum of @xmath47 and @xmath48 can be estimated by @xmath128,\\ ] ] while the separate values of @xmath47 and @xmath36 can be estimated from another nonlinear regression analysis with @xmath129 as the response variable and with mean function @xmath130 these are justified by ( [ 80 ] ) and ( [ 90 ] ) , respectively .",
    "initial values of the unknown parameters are required for fitting the above nonlinear regression models . in our experience ,",
    "a reasonable way to obtain initial values appears to be the following linear regression method .",
    "consider the case with fse , and note that equation ( [ 90 ] ) can be rewritten as @xmath131 to take advantage of this relationship , we can allocate the pairs of trips into bins according to the value of @xmath126 and treat each bin as approximately homogeneous with respect to the gap time . within each bin",
    ", we can replace  @xmath132 with @xmath133 and expectation with sample average on the left - hand side of ( [ b10 ] ) , replace @xmath126 with a typical value ( say , the median ) on the right - hand side , and run a linear regression analysis with each bin as a data point .",
    "initial estimates of @xmath47 and @xmath36 can then be obtained by exponentiating the estimated intercept and negating the estimated slope , respectively .",
    "this linear regression approach would not work for the case without fse , for which we could use as initial estimates the final estimates from a gee analysis with fse .",
    "we thank the editor , associate editor and two anonymous referees for constructive comments that have greatly improved the manuscript ."
  ],
  "abstract_text": [
    "<S> most of the available methods for longitudinal data analysis are designed and validated for the situation where the number of subjects is large and the number of observations per subject is relatively small . motivated by the naturalistic teenage driving study ( ntds ) , which represents the exact opposite situation , we examine standard and propose new methodology for marginal analysis of longitudinal count data in a small number of very long sequences . </S>",
    "<S> we consider standard methods based on generalized estimating equations , under working independence or an appropriate correlation structure , and find them unsatisfactory for dealing with time - dependent covariates when the counts are low . </S>",
    "<S> for this situation , we explore a within - cluster resampling ( wcr ) approach that involves repeated analyses of random subsamples with a final analysis that synthesizes results across subsamples . </S>",
    "<S> this leads to a novel wcr method which operates on separated blocks within subjects and which performs better than all of the previously considered methods . </S>",
    "<S> the methods are applied to the ntds data and evaluated in simulation experiments mimicking the ntds .    ,    . </S>"
  ]
}