{
  "article_text": [
    "the study of the cosmic microwave background ( cmb ) anisotropies is providing strong constraints on theories of structure formation .",
    "these theories are statistical in essence , so the extraction of the information must be done in a statistical way . in particular ,",
    "the standard method for analysing a cmb experiment is the maximum likelihood estimator ( ml ) .",
    "the procedure is straightforward : maximise the probability of the parameters of the model given the data , p(parameters@xmath2data ) , over the allowed parameter space .",
    "usually , we take the prior probability for the parameters to be constant , so this is equivalent to maximising the likelihood , p(data@xmath2parameters ) , via the bayes theorem .",
    "the ml method has been widely applied in cmb analyses , for power spectrum or parameters estimation , @xcite . when computing the likelihood in these problems , we have to deal with the inversion of the covariance matrix of the data , which usually involves @xmath3 operations , being @xmath4 the number of pixels of the map .",
    "the increasing size of the datasets makes this method computationally costfull for new experiments , so other methods have been investigated in the last few years to confront the problem",
    ". there have been several proposals on this matter .",
    "pioneering work on the problem of power spectrum estimation @xcite , based on an evaluation of the @xmath5 s coefficients of the multipole expansion of the observed map in the spherical harmonics basis , have been applied to cobe data @xcite .",
    "quadratic estimators have been proposed by several authors @xcite as statistics that give the same parameters that maximise the likelihood , but requiring less computational work .",
    "nevertheless , alternative statistical methods are required in the field to extract the cosmological information from future cmb experiments ( as planck ) where the number of data points will be very large ( see , e.g. @xcite for an estimation of the scaling of the computing time with the dataset size ) .    here , we propose a new statistical method to analyse a cmb map . in order to illustrate it",
    ", we will use the two - point correlation function ( cf ) .",
    "we first replace the likelihood of the full map by the likelihood of the fluctuations of an estimator of the cf .",
    "then , we derive the cosmological parameters from it in an efficient manner .",
    "if we assume gaussianity for the primordial cmb fluctuations , the cf completely characterises the statistical properties of the field . in this line , it has been suggested @xcite that it can be used to obtain the power spectrum or for parameter estimation , because it encodes all the relevant information for that purpose .",
    "this approach of considering the cf in cmb analyses has been recently used by other authors @xcite to estimate the power spectrum .",
    "they obtain the cf using different estimators , and integrate it , projecting over the legendre polynomials , to obtain the @xmath6 s .",
    "the advantage of this estimator is that it only needs at the most @xmath7 operations to be computed , and not @xmath3 , as is required for the likelihood .",
    "we construct a modified version of the standard @xmath0 test , using the cf evaluated at a certain set of points .",
    "the estimate of the parameters of the model is given by the minimum of this statistic , as in the standard analysis .",
    "we give a very good approximation to the distribution function of this modified @xmath0 , so the confidence limits can be obtained without using simulations , by integration below that curve , as for the ml .",
    "we show that , in several problems , choosing a large enough set of points to evaluate the cf , our method has the same power as the maximum likelihood , while being two different methods .",
    "in this section we will introduce the test , using for this purpose the two - point cf .",
    "nevertheless , all the procedure described below can be applied to any other estimator .    for a certain map of the cmb anisotropies , @xmath8 with @xmath4 pixels , and errors",
    "@xmath9 , we can estimate the cf , @xmath10 , in a set of @xmath11 angular distances , @xmath12 . in this work",
    "we have used the following estimator ,    @xmath13 = \\frac { \\sum _ { i , j \\in \\ { k \\ } }   \\ ;   x_i x_j } { \\sum _ { i , j \\in \\ { k \\ } } 1 } \\label{est_cf}\\ ] ]    where @xmath14 stands for the set of all pixel pair @xmath15 such that their angular distance is @xmath16 , but our proposal and techniques can be applied to other estimators for the cf ( see , for example , @xcite , or @xcite ) .",
    "hereafter , we will write the estimate of a certain parameter as @xmath17 $ ] . if we have a map with zero mean and no noise , eq .",
    "( [ est_cf ] ) is a unbiased estimator of the theoretical two - point cf , which for a experiment with a symmetric beam is given by    @xmath18    where @xmath19 stands for the window function of the experiment .",
    "we have explicitly removed the dipole contribution in the previous equation , because these coefficients are dominated by the kinematic dipole in a real map .",
    "if the cmb signal is gaussian , the power spectrum ( or its fourier transform , the cf ) , encodes all the information about the model .",
    "so , under this assumption , we can parameterise a model through the @xmath6 s themselves , or through the cosmological parameters , by writing @xmath20 . in general , we will write @xmath21 , being @xmath22 the parameters of the model .    as an example of the general procedure that we propose in this paper , we consider in detail the estimator derived from the following statistic :    @xmath23 - c(\\theta_k|\\hat{m } ) - c_n(\\theta_k ) ) ^2 } { \\sigma^2 ( c(\\theta_k ) ) } \\label{chi2}\\ ] ]    where @xmath24 s are certain weights to be defined below , and @xmath25 stands for the variance of the estimator @xmath26 $ ] .",
    "@xmath27 represents the discrete cf of the noise .",
    "if we have an experiment with uncorrelated noise , this function takes the form    @xmath28    this method is a modification of the standard form of a @xmath0-test , for the case when the error of each of the estimates entering ( [ chi2 ] ) are independent and gaussianly distributed ( hereafter , we mean by standard @xmath0-test the case when @xmath29 , @xmath30 , and all the terms of the sum in ( [ chi2 ] ) are independent ) . in the present case , the @xmath31 $ ] quantities follow very closely a gaussian , but are correlated .",
    "for this reason , we have introduced some weights ( @xmath24 ) , that will be determined by minimising the dispersion of the estimator derived from equation ( [ chi2 ] ) , as we will see in the next section .",
    "those quantities will account for the different degree of correlation between terms , and in a general problem , they will be a function @xmath32 , where @xmath33 stands for the correlation matrix between the errors of the estimates of @xmath34 and @xmath35 , i.e. @xmath36 - c(\\theta_i|\\hat{m } )   - c_n(\\theta_i ) \\bigg ) \\times \\\\ \\times \\bigg ( e [ c(\\theta_j ) ] - c(\\theta_j|\\hat{m } )   - c_n(\\theta_j ) \\bigg ) >   \\label{cprima}\\end{aligned}\\ ] ]    so the variance @xmath25 is related to the @xmath37-matrix by @xmath38    the brackets @xmath39 represent an average over an ensemble of universes , i.e. , an average over realizations for one fixed cmb model .    in principle",
    ", our construction seems to miss information about the correlations when compared to the usual @xmath0 procedure to analyse correlated datasets , because in equation ( [ chi2 ] ) only the diagonal terms of the covariance matrix ( @xmath37 ) are explicitly shown . nevertheless , as we will see in the next section , the @xmath24 weights depend on the full covariance matrix and not on its inverse ,",
    "so all the correlations implicitly enter in that expression .",
    "we will not make a detailed comparison between the usual @xmath0 method with uses the full covariance matrix c ( we will refer to this method as the `` usual @xmath0 '' ) and our @xmath1 .",
    "however , we will illustrate this with an example ( see section 6.1 ) .",
    "in addition , in appendix a we present a brief comparison of some characteristics of both methods ( @xmath1 and the usual @xmath0 ) for the case of linear problems .",
    "it is an interesting result that , for gaussian linear problems , if we are estimating only one parameter , the estimates from both methods are exactly equal , while being different statistics ( i.e. they will give different probability contours ) .",
    "hereafter , we will concentrate in our method , and its application to cmb problems .",
    "it is worth to notice that our @xmath1 statistic is a different approach to the ml , in the sense that it provides different estimates and probability contours .",
    "however , in the case of cmb analyses , we will see below that it has a similar power to the ml , but avoiding the problem of the inversion of the covariance matrix .",
    "there is however a minor sense in which our test may formally be considered as an approximation to the ml .",
    "it is well - known that the ml is an asymptotically efficient estimator for our problem .",
    "thus , in the limit of infinite size of the sample ( or for those problems where an efficient estimator exists , as in linear gaussian problems ) , then the ml is the only one statistic which renders the minimum variance , and thus any other estimator may be regarded as approximate .",
    "once we have constructed the @xmath1 test , the estimate for this method is given by the set of values for the parameters that minimise eq .",
    "( [ chi2 ] ) , i.e. the solution to the set of equations @xmath40 . for a given problem ,",
    "we proceed as follows .",
    "if we want to estimate a set of @xmath41 parameters , @xmath42 , we first compute the @xmath37 matrix by assuming an initial value for those parameters , @xmath43 . using this matrix , we obtain our estimate by solving the following system of equations ,    @xmath44    when computing these derivatives with respect to @xmath22 , we neglect the dependence of @xmath37 on the parameters , which is equivalent to assuming that @xmath25 and @xmath24 are constants in the derivation .",
    "this process is iterated until convergence .",
    "the reason to keep the @xmath37 matrix fixed in the derivation is that we want to have an unbiased estimator of the parameters .",
    "the evaluation of the @xmath37 matrix can be done by monte carlo simulations , but we also propose an analytical approach .",
    "it is possible to evaluate equation ( [ cprima ] ) using the quantities @xmath45 , for a multivariate - gaussian field , as is the case for the cmb ( see appendix b ) .    in the particular case of power spectrum estimation , the set of parameters we have to determine are the @xmath6 s themselves , or the band powers in a certain number of multipole bands centred at multipoles @xmath46 ( i.e. , @xmath47 ) .",
    "as the theoretical cf ( [ cf_th ] ) is linear in the @xmath6 s , we have that eq . ( [ ps_est ] ) is a linear system of @xmath41 equations , and the solution can easily be found .    as an example , we present here the equations for the determination of the total power measured by a certain experiment . in this case",
    ", we only have one parameter , @xmath48 , defined as    @xmath49    which is essentially a normalisation of the spectrum . from here",
    ", we define the function @xmath50 , which is independent of @xmath51 .",
    "we can now obtain the analytic expression for the estimate of @xmath51 by minimising eq .",
    "( [ chi2 ] ) with respect to @xmath51 , which in this case takes the form    @xmath52 - \\sigma_{sky}^2 f(\\theta_k ) ) ^2 } { \\sigma^2 ( c(\\theta_k ) ) } \\ ] ]    for simplicity , we will not write the term of the noise cf , but it can be easily included inside the true cf . inserting the previous expression in equation ( [ ps_est ] ) ,",
    "we obtain , for fixed @xmath24 , an equation for @xmath51 .",
    "it must be noted that due to the dependence of @xmath25 on @xmath51 , this equation is not exactly linear .",
    "we could solve it iteratively , starting with certain fixed value for @xmath25 .",
    "however , for all the values of these quantities within the current limits , a first iteration is enough , as we will see , so that the equation determining @xmath51 is effectively linear , and therefore its solution is given by    @xmath53 =   \\bigg ( \\sum_i \\frac { p_i f_i^2 } { \\sigma^2 ( c(\\theta_i ) ) } \\bigg ) ^{-1 }   \\sum_i p_i f_i \\frac { e [ c(\\theta_i ) ] } { \\sigma^2 ( c(\\theta_i ) ) }   \\label{e_ssky}\\ ] ]    the @xmath54 of this estimator is given by    @xmath55 ) =   \\bigg ( \\sum_i \\frac { p_i f_i^2 } { \\sigma^2 ( c(\\theta_i ) ) } \\bigg ) ^{-1 } \\times \\\\   \\qquad \\times \\bigg [ \\sum_i \\frac{p_i^2 f_i^2}{\\sigma^2 ( c(\\theta_i ) ) } + \\sum_{i \\neq j } \\frac { f_i f_j p_i p_j c'_{ij } } { \\sigma^2 ( c(\\theta_i ) ) \\sigma^2 ( c(\\theta_j ) ) } \\bigg]^{1/2 } \\label{rms_ssky}\\end{aligned}\\ ] ]    where we have defined @xmath56 .",
    "the expression within large parentheses in this equation is the @xmath54 of the second sum in equation ( [ e_ssky ] ) .",
    "the first sum within the parentheses correspond to the quadratic addition of the contributions of each term in ( [ e_ssky ] ) , which is present even when the random variables @xmath31 $ ] are independently distributed .",
    "the second sum is due to the correlations between any pair of these variables .",
    "it should be noted that equation ( [ rms_ssky ] ) has been obtained assuming that the quantities @xmath31 $ ] follow a multivariate gaussian distribution .",
    "this is a good approximation if there are enough pixel pairs entering in the sum in ( [ est_cf ] ) ( see , for example , @xcite , for the cf of the cobe data ) .",
    "similar calculations for the standard @xmath0 and the likelihood function can be found in @xcite ( hereafter , b93 ) .",
    "the matricial expression of the estimate and the rms for a general linear problem are shown in appendix a.",
    "the @xmath24 s weights in equation ( [ chi2 ] ) are introduced in order to take into account the different degree of correlation of the terms of the sum .",
    "their expression can be obtained once we define exactly what we are interested in .",
    "for example , one common criteria for one parameter estimation is to use the estimator which has the minimum @xmath54 .",
    "we will consider this criteria here .",
    "for the problem of one parameter estimation described in the previous section , once we have the analytic expression for the @xmath54 , and an initial guess for the @xmath37 matrix , we can obtain the optimum set of @xmath24 s using the `` minimum @xmath54 criteria '' .",
    "we minimise eq .",
    "( [ rms_ssky ] ) with respect to the @xmath24 s quantities .",
    "we obtain that the @xmath24 s quantities are given by the solution to the implicit set of equations    @xmath57 @xmath58 which can be solved numerically , using a newton - raphson scheme for nonlinear systems of equations .",
    "it should be noted that in the case when @xmath59 , @xmath60 , equation ( [ p_k_ssky ] ) has the trivial solution @xmath29 , as we expected for the standard case without correlations .",
    "the estimates obtained with these @xmath24 give us a better guess for @xmath37 , that could be used in equation ( [ p_k_ssky ] ) to obtain more appropriate values of the @xmath24 .",
    "however , in practice , we have checked that for all the cases that we consider in this paper , this iteration is not necessary , since over the a priori uncertainty region of the parameter , the variation of the @xmath24 is negligible .    the previous expression , derived for the problem of total power estimation , can also be applied to any problem of one parameter estimation , as follows .",
    "let @xmath61 be the parameter we are interested in .",
    "if we expand the cf in a taylor series around an initial guess , @xmath62 , we obtain , up to first order ,    @xmath63 @xmath64 with @xmath65 , so we can use equation ( [ p_k_ssky ] ) , with    @xmath66    if we use an initial guess close to the real value , this linear approximation will give good results .",
    "the @xmath67 s can be obtained numerically for each problem .",
    "when we deal with a problem of several parameters estimation , it is not well defined what has to be minimised . a reasonable criteria for these problems ,",
    "if we want to estimate the set @xmath68 , is to minimise @xmath69 ) $ ] , where we define @xmath70)$ ] as the @xmath54 for each individual parameter .",
    "linearising around our initial guess , @xmath43 , we can derive a simple expression for the @xmath54 of each parameter . in this case , we have that    @xmath71    where we define    @xmath72    the estimate of the parameters is given by the solution to the linear system of equations @xmath73 , where we have again neglected the dependence of @xmath74 on the parameters .",
    "the general expression of the covariance matrix of the parameters is shown , for a general linear problem , in appendix a. if we expand those matrices , we obtain that the general form of this estimate , for our problem , is given by    @xmath75 = \\frac{1}{r } \\sum_{k } j_{k , i } p_k   \\frac { e [ c(\\theta_k ) ] - c(\\theta_k,\\hat{m}_0 ) } { \\sigma^2(c(\\theta_k ) ) } ,   \\qquad i=1, ... ,p \\label{est_m_k}\\ ] ]    where @xmath76 = e[m_i ] - ( m_0)_i$ ] , and @xmath77 and @xmath78 are numbers obtained from the @xmath79 s . from ( [ est_m_k ] ) we can infer the general expression for the @xmath54 in the case of several parameters , obtaining    @xmath80 ) =   \\frac{1}{r } \\bigg [ \\sum_{k , j } j_{k , i } j_{j , i } p_k p_j \\frac { c'_{kj } } { \\sigma^2(c(\\theta_k ) )   \\sigma^2(c(\\theta_j ) ) } \\bigg]^{1/2 } \\label{rms_several}\\ ] ]    where @xmath81 . using the previous equation , we can obtain the @xmath24 quantities for any problem , just minimising the product @xmath82 ) $ ] numerically .",
    "summarising , we will have a different expression for the @xmath24 s for each particular problem .",
    "an application of these equations for the problem of two parameters estimation can be found in section 6.1 .",
    "the @xmath1 proposed in eq .",
    "( [ chi2 ] ) corresponds to a sum of quantities which are not independent .",
    "if we had set @xmath83 , we would have the standard @xmath0 statistic , but still with correlations among the terms .",
    "therefore , its distribution function will not be the standard one .",
    "the formal expression for the distribution of a @xmath0 constructed from variables which are distributed following a multivariate gaussian distribution is given in appendix c. this distribution , when applied to cmb analyses , was studied in b93 .",
    "there , they proposed that the distribution function for the statistic ( [ chi2 ] ) is given by an standard ( rescaled ) @xmath0 function , but with an effective number of degrees of freedom .",
    "this proposal is not exact ( see also appendix c ) , but it turns out to be a very good approximation for the true distribution function , as we shall see in the following section . in a general case , the error in the distribution function using our approximation will be a few percent .",
    "the basis of the approximation is to assume that correlations only reduce the degrees of freedom , but do not change the shape of the distribution . quantifying this argument",
    ", there exist a certain constant , @xmath84 , which makes the statistic    @xmath85    to be distributed as an ordinary @xmath0 , with an effective number of degrees of freedom , @xmath86 .",
    "this number , and the constant @xmath84 , are obtained just by imposing the new distribution to have the mean and the variance of a standard @xmath0 , i.e. ,    @xmath87    @xmath88    where @xmath89 means mean square .",
    "summarising , our proposal is that once the first and second moments are fixed , the whole distribution will follow a @xmath0 very closely . for our problem ,",
    "we obtain    @xmath90    @xmath91    in this calculation , we needed to compute the @xmath89 of eq .",
    "( [ chi2 ] ) .",
    "this result is obtained using the fact that the data points follow a multivariate gaussian distribution , and it can be found in b93 ( see also appendix b for similar calculations ) .    in general , @xmath86 is a real number , so we have to consider the analytic extension of a standard @xmath0 distribution ( which is known as the gamma distribution function , see for example @xcite ) ,    @xmath92 @xmath93 just by replacing the factorial with the gamma function ( @xmath94 ) . in ( [ dfchi2 ] ) , @xmath95 is the probability of finding a value for @xmath96 between @xmath97 and @xmath98 , and @xmath99 stands for the probability density function .",
    "once we know the distribution function , the confidence limits are given by integration bellow this curve .",
    "we assign a weight to each hypothesis as in a standard @xmath0 analysis , integrating the distribution from the obtained value up to infinite ,    @xmath100 @xmath101 i.e. , the probability of finding a value of @xmath97 bigger than or equal to @xmath102 . here",
    ", we explicitly write where the dependence in the data ( @xmath103 ) and in the parameters ( @xmath22 ) is .",
    "in this section , we will test the whole method in the problem of one parameter estimation , but first , we will study the quality of our approximation to the distribution function of a @xmath0 with correlations .",
    "these two points will be done by means of monte carlo simulations . in order to do that",
    ", we have chosen the jb - iac 33 ghz interferometer @xcite as the reference experiment .",
    "this experiment is a two element interferometer , which operates at @xmath104  ghz , at the teide observatory .",
    "it has two configurations , with angular resolutions @xmath105 ( @xmath106 ) , and @xmath107 ( @xmath108 ) , respectively .",
    "the window function in both configurations is very narrow , so the results are quoted in terms of total power inside the band ( band power ) .",
    "the experiment has given measurements on the power spectrum on both scales @xcite , which are consistent with the boomerang data @xcite .",
    "we have the likelihood analysis implemented for this experiment , so the comparison with the new method will be straightforward . in our analyses ,",
    "we have used the compact configuration , and only one of the two channels ( i.e. , the real part of the complex visibility ) .",
    "the cmb realizations have been done assuming the following values for the cosmological parameters : @xmath109 , @xmath110 , @xmath111 , @xmath112 and @xmath113 . for this model ,",
    "the total power inside the window function ( or band power ) is @xmath114 for the short configuration ( @xmath115 ) .",
    "this number is related with @xmath116 by a conversion factor , which is obtained using the flat band power approximation ( i.e. @xmath117 constant inside the window function ) in equation ( [ def_ssky ] ) . for our instrument , this conversion factor is @xmath118 , which gives @xmath119 for the previous model . for this experiment ,",
    "the sensitivity in a @xmath120s integration is given by @xmath121 , where @xmath122 is the number of observing days .",
    "therefore , the signal - to - noise ratio is given by @xmath123 .       with correlations with @xmath124 terms .",
    "we show the histogram with the frequencies for the rescaled @xmath0 , obtained from @xmath125 realizations , for four different cases , varying @xmath126 ( signal - to - noise ratio ) .",
    "we use @xmath127 bins of equal size to sample the distribution function . in all the figures , the dots represent the numbers coming from the realizations , and the error bars show their sampling error .",
    "the solid line is our approximation to the distribution function , using the value of @xmath86 from the formula ( shown within parentheses in the figure ) .",
    "it is also shown , using dot - dashed lines , the distribution function of a ( rescaled ) @xmath0 with @xmath124 .",
    "we can see the effect of the correlations as @xmath126 increases . ]",
    "the first point is to check the validity of our approximation to the distribution function for a @xmath0 with correlations .",
    "we will study the case when the @xmath128 quantities entering in the @xmath0 follow a multivariate gaussian distribution .",
    "this is the case for an ( ideal ) cmb map , where the temperature at each pixel has two contributions , one coming from gaussian noise , and another one from the cosmological fluctuation field , which is supposed to be also gaussian . using the cmb terminology from section 2",
    ", we will study the distribution of the statistic    @xmath129    and we will compare it with the proposed approximation .",
    "this is a particular case of ( [ chi2 ] ) , when @xmath83 .",
    "we will study in detail this case here , but our results are completely general .",
    "it should be noted that our proposal is exact , by definition , in the two limit cases of no correlations at all , and totally correlated points .",
    "the first one correspond to the definition of the @xmath0 distribution function , and the second one is the case of a @xmath0 with @xmath130 .    in appendix c",
    "we present the formal aspect of the distribution function for ( [ chi3 ] ) , and we study in detail , analytically , the case @xmath131 .",
    "the cases with low @xmath4 turn out to be the critical ones , because the shape of the distribution function differs strongly from a gaussian . in the limit of high @xmath4 , both our approximation and the real distribution function tend to a gaussian distribution ( the same one , by definition ) , as a consequence of the central limit theorem .",
    "therefore , it is interesting to test our proposal for a intermediate range of values of @xmath4 .",
    "we have done so , and we will present here , as an example , the case for @xmath124 .",
    "we generate cmb realizations with noise , for a ten pixels map . from each simulation , we compute ( [ chi3 ] ) , and from the whole set of values obtained , we study the histogram with the frequencies , and we compare it with the proposed one , for several values of the signal - to - noise ratio .",
    "we show these results in figure [ fig_histog ] . in general ,",
    "the asymptotic shape of the distribution is very well reproduced , and the largest differences always occur for low values of @xmath0 .",
    "this is precisely the kind of approximation we need , because in a statistical analysis we usually are interested in the tail of the distributions .",
    "we can see that the distribution of the @xmath0 with correlations among terms is compatible , inside the numerical precision , with an standard ( rescaled ) @xmath0 , with an effective number of degrees of freedom , smaller than @xmath4 .",
    "we have also checked that the numerical values for @xmath86 and @xmath84 are correctly given by equations ( [ neff ] ) and ( [ a ] ) . in table",
    "[ tabla2 ] we compare the values obtained from the simulations with the predicted ones given by the theoretical formulae .",
    "their difference is in all cases smaller than the sampling errors , so we conclude that our expressions give the correct values for these parameters .",
    ".values of @xmath86 and @xmath84 , for @xmath124 .",
    "[ cols=\"^,^,^,^,^,^ \" , ]     @xmath132 adopted values for the @xmath24 s . the last row is the optimum set of @xmath24 values ( see figure [ figura2 ] ) .",
    "+ @xmath133 results from 100 monte carlo simulations of cobe data ( see details in the text ) .",
    "the first number is the average value for the parameter , and the second one is the @xmath54 from the simulations .",
    "+ @xmath134 @xmath54 values obtained analytically , using the linear approximation to the cf ( see section 3 ) .",
    "we will use the @xmath24 quantities given by the minimum of the function @xmath135 ) \\times rms(e[q_{rms - ps}])$ ] , as we have discussed in section 3 .",
    "those @xmath54 s can be derived , using the linear approximation to the cf , from equation ( [ rms_several ] ) . in this problem ,",
    "the @xmath77 and @xmath78 quantities are given by    @xmath136    @xmath137    where @xmath138 stands for @xmath11 , and @xmath139 for @xmath140 .",
    "the equation for @xmath141 can be obtained from @xmath142 , just interchanging @xmath143 .    in order to check the previous expressions for the @xmath54",
    ", we use 100 of the above mentioned realizations of cobe like maps ( @xmath144 ; @xmath145 ) , and we analyse them using several sets of @xmath24 s . in this way , we can obtain the real value of the @xmath54 , and compare it with the number coming from the formula .",
    "the results are summarised in table [ tabla3 ] . in all cases ,",
    "the theoretical numbers obtained from equation ( [ rms_several ] ) are in agreement with the numerical results , so we conclude that the linear approximation to the true cf works well in computing the @xmath54 .",
    "the largest differences occur when we obtain a large @xmath54 , due to the fact that , in that case , fails the linear approximation to the cf .    the average values recovered for @xmath11 and @xmath146 from monte - carlo simulations show that the estimator is unbiased , as we would expect .",
    "it should be noticed that , in table [ tabla3 ] , the effective number of degrees of freedom is quite small . in all cases ,",
    "we obtain @xmath147 , but we are using @xmath148 .",
    "the reason is that the cf contains long - range terms , coming from low multipoles ( @xmath149 ) .",
    "this fact reduces the degrees of freedom drastically , so the choice of the @xmath24 will be critical in this problem .",
    "the numbers obtained when we consider the whole cf and @xmath29 are compatible with those in bhs94 , but slightly better because we consider the noise of the 4-year cobe map . in that paper",
    ", they obtained , using the same galactic cut ( @xmath150 ) , and the noise from the 2-year map , the values @xmath151 ) = 0.96 $ ] and @xmath152 ) = 253\\mu k^2 $ ] ( in our units ) .",
    "nevertheless , we see that considering only the first points , and setting to zero the others , strongly reduces the @xmath54 of the estimate , even below the values obtained when they do not consider noise and incomplete sky coverage ( they have @xmath151 ) = 0.36 $ ] and @xmath152 ) = 175 \\mu k^2 $ ] ) .",
    "s for the @xmath153 4-year _ cobe _ map ( see details in the text ) , using the galactic cut @xmath150 .",
    "these values were obtained by numerical minimisation of the product @xmath151 ) \\times rms ( e[q_{rms - ps } ] ) $ ] , using the linear approximation to the cf .",
    "each value for @xmath154 ( @xmath155 ) corresponds to an angular distance , given by @xmath156 .",
    "an explanation of this peculiar structure can be found in the text ( section 6.1 ) . ]    finally , we can obtain the optimum set of @xmath24 s by numerical minimisation of the product of @xmath54 s .",
    "we have used the fortran program @xmath157 @xcite .",
    "the obtained value for these quantities is shown in figure [ figura2 ] .",
    "these numbers do not depend on the initial guess for @xmath11 and @xmath140 within the a priori region of uncertainty .",
    "the values obtained have a peculiar form , but it can be understood as follows .",
    "we see that the terms which contribute to the optimum estimator ( minimum @xmath54 ) correspond to the first @xmath158 degrees , i.e. the first part of the cf , as we would expect .",
    "but there are also two regions , one at @xmath159 , and another at @xmath160 , which contribute to the estimator .",
    "these peaks are located just in the zeros of the quadrupole ( @xmath161 ) which is the multipole with the largest cosmic variance . by using those points ,",
    "we add some information to the estimator ( coming from higher @xmath162 values ) , but we do not increase its variance . in any case , to consider or not those points do not affect too much to the power of the method ( in table [ tabla3 ] , the values obtained when using the first 10 points from the cf are close to those obtained with the optimal set ) . following this interpretation , one could think about other combinations of the @xmath24 parameters using others points ( for example , taking two points at @xmath163 and @xmath164 .",
    "we have explored this possibility as an illustration , and we find the values @xmath165 and @xmath166 , which are similar but slightly higher than those obtained for our optimal @xmath24 .",
    "thus , we can conclude that for this problem , we can find a set of estimators ( one for each one of these sets of @xmath24 values ) which give similar rms values when estimating @xmath11 and @xmath146 , and as we show below , these values are comparable to those given by the likelihood method .",
    "finally , we have applied our @xmath1-test for the cf to the actual 4-year cobe data .",
    "our estimates , from the analysis of the @xmath153 map , using the galactic cut @xmath150 , a step of @xmath167 to sample the cf , and the optimum set of @xmath24 s , are @xmath168 = 1.08 $ ] , and @xmath169 = 15.2 \\mu k$ ] , so our estimate of the parameters , using the true @xmath54 from @xmath170 realizations , will be @xmath171 , and @xmath172 .",
    "this result has to be compared with the likelihood analysis using these data ( see @xcite , table 1 ) .",
    "they obtain for this map @xmath173 , @xmath174 .",
    "the estimates from both methods are nearly the same , and now the error bars coming from the cf are compatible in size with those coming from the maximum likelihood method . for comparison , using no weights ( @xmath29 ) , we would obtain @xmath168 = 1.10 \\pm 0.54 $ ] , and @xmath169 = ( 16.0 \\pm 4.8 ) \\mu k$ ] , so we can see that using the @xmath24 s for this problem is essential .      finally , we will apply our method to obtain band power spectra for the cobe data .",
    "we have used the realizations from the previous subsection ( @xmath144 ; @xmath145 ) , and the same cobe map .",
    "we will compare our results with those from @xcite ( see table 2 in that paper ) .",
    "we have used exactly the same @xmath162 range : four multipole bands between @xmath161 and @xmath175 .",
    "those bands are : @xmath176 ; @xmath177 ; @xmath178 and @xmath179 . using those realizations , we have checked that the method is unbiased when applied to power spectrum estimation .    when applied to the @xmath153 4-year cobe map",
    ", we obtain the results that are shown in table [ tabla4 ] .",
    "we quote the band power values in terms of the quadrupole normalisation expected for a scale - invariant power - law spectrum within the specified range of @xmath162 .",
    "the quoted values for our method have been obtained from the optimal set of @xmath24 for this problem . when compared with the ml data",
    ", we can see that the error bars are of the same order in both cases , as in the previous subsection .",
    "so we again obtain a method of a similar power to the likelihood on the pixel map .",
    "@ccccc@ & +   + method & @xmath176 & @xmath177 & @xmath178 & @xmath179 + @xmath1 , optimal & @xmath180 & @xmath181 & @xmath182 & @xmath183 + ml @xmath133 & @xmath184 & @xmath185 & @xmath186 & @xmath187 +    @xmath132 these band power amplitudes are expressed in terms of @xmath188 , i.e. the quadrupole normalisation expected for a scale - invariant power - law spectrum within the specified range of @xmath162 .",
    "the units are @xmath189 .",
    "+ @xmath133 ml values from @xcite for the same map .",
    "the estimates from both methods are consistent in all bins , except the apparent inconsistency at the second one , where the two estimates differ in more than 3 times the size of one error bar .",
    "nevertheless , the statistical significance of that deviation has to be computed as follows : given that we do not know the true value for the band power , when comparing two results we have to consider the difference of both estimates , and so we have to compute the variance of the difference .",
    "in this case , the difference is @xmath190 , and the @xmath54 of the difference of those two estimates is @xmath191 .",
    "here we are assuming the fact that both methods are almost independent , following the results for the case of one parameter estimation , with signal - to - noise ratios of the order of @xmath138 .",
    "therefore , we find a deviation at the @xmath192-sigma level , which corresponds to a fluctuation of 1 in 47 ( for a normal distribution ) .",
    "this can be understood given that the ml and the @xmath1 are two different methods : the first is based on the full map , and the second on the correlation function .",
    "therefore , the estimates will be different in general , although as we can see , both methods have similar power , so there is no reason to consider any one of them as `` the estimate '' .",
    "summarising , we have seen that it is possible with our method to perform an analysis with a similar power to the ml , even for the case of several parameters .",
    "in this paper we have presented an statistical method to analyse cmb maps .",
    "it consists in a variation of an standard @xmath0-test for the case when we have correlated points . here",
    ", our test has been constructed from the two - point correlation function , following the proposal from other authors @xcite that the cf contains all the relevant information concerning the cosmological parameter estimation . in this line",
    ", we propose a @xmath1 based on the cf , which is a different approach from the `` usual @xmath0 '' method ( which uses the full covariance matrix @xmath37 ) .",
    "our proposal explicitly uses only the diagonal terms of the covariance matrix of the data , but we introduce certain weights , which now implicitly contain all the correlations .",
    "this approach has two important computational advantages compared with the `` usual @xmath0 '' , or with a likelihood based on the cf :    * we do not need to invert the covariance matrix , @xmath37 ; all the quantities ( the @xmath1 , @xmath86 , @xmath84 and the expression for the @xmath54 ) depend on @xmath37 directly . *",
    "even more , if we have a problem with a low value for @xmath86 , the effective number of @xmath24 to use ( i.e. the number of @xmath24 s which are significantly different from zero ) will be also small , typically , of order @xmath193 .",
    "so it is not even necessary to use all the terms of the diagonal .",
    "these advantages are more important when comparing our method with the standard ml analysis on the full map .",
    "we do not need to invert the covariance matrix of the map ( @xmath194 ) , and we only need to concentrate on a few numbers , so the problem is computationally accessible if we have to deal with large datasets .",
    "it it important to stress here that the @xmath1 is not an approximation to the ml on the full map , but a different approach ( i.e. both methods will give different estimates and probability contours for a given problem ) .",
    "so the @xmath1 can be applied to any problem , but it has to be checked , by means of monte carlo simulations , that the method has a similar power to the maximum likelihood . as we have seen , this is the case for several cmb common problems ( power spectrum estimation , and cosmological parameter estimation ) .",
    "the largest computational effort in our method has to be done estimating the cf , which is a @xmath195 operation .",
    "nevertheless , there are estimators for the cf more efficient @xcite , so our procedure could be applied to current _ wmap _ data , and _ planck _",
    "simulated data , but this will be treated in detail in future works .",
    "our method can be extended for general noise covariance matrices .",
    "we only need to compute @xmath196 in the same way as the cf , and introduce it in equation ( [ chi2 ] ) .",
    "if we have the noise matrix , it is straightforward to obtain @xmath196 .",
    "but , if we do not have the noise matrix , we can obtain an estimate of the correlation function of the noise using mc simulations .",
    "this idea has been used recently by other authors @xcite .",
    "it must be noted that if the noise changes substantially from pixel to pixel , then we would have to use weights in equation ( [ est_cf ] ) to compute the cf in a more efficient way .",
    "we have also presented an approximation which gives very accurately the distribution function for a @xmath0 constructed from a set of multivariate gaussian variables .",
    "this proposal can be extended to approximate the distribution function of any quantity made of a sum of squares , each of them distributed ( exactly or approximately ) following a gaussian distribution .    to conclude , we propose that , if we are interested in obtaining a certain set of parameters , @xmath22 , we can use an unbiased estimator of certain quantities depending on these parameters , provided that they contain all the relevant information to these parameters ( in our case , we have used the cf at certain angles ) . from it , we may obtain , by varying the @xmath24 s , the best estimator of those parameters . in this paper , we have tested this proposal , using the two - point cf as the reference estimator , in cmb problems . for the case of one parameter estimation ( the normalisation of the spectrum ) ,",
    "our method , with the cf , turns out to be as powerful as the ml .",
    "when applied to cobe data , we have shown the importance of choosing the right set of @xmath24 s . in the optimum case",
    ", we obtain a value for the @xmath54 two or three times smaller than the one obtained without weighting at all . in this problem ,",
    "the @xmath24 s are critical because the effective number the degrees of freedom is very small .",
    "the reason is that when we have strong correlations ( @xmath86 small compared with @xmath11 ) , the structure of these correlations , which is encoded in the @xmath24 s , will be relevant , so there will be a considerable difference between the optimal weights and @xmath197 . when analysing cmb data which contain large scales ( low multipoles ) , we are considering correlations over long distances .",
    "all the points are correlated with the others due to these low multipoles ( quadrupole , octupole , ... ) . in the case of boomerang data @xcite",
    ", the effective number of degrees of freedom will be larger ( if we throw away the large scales ) , so the @xmath24 s will be closer to 1 , and a standard @xmath0 test based on the cf should produce good results .",
    "we would like to thank m.p.hobson , r.a.watson , r.rebolo , c.gutirrez and r.kneissl for their useful comments , and b.barreiro for her cobe - like maps simulation program .",
    "in this appendix we will study the relationship between the usual @xmath0 analysis ( the standard approach for the case of correlated data points , which uses the full covariance matrix ) , and our approach , for gaussian problems in which the model depends linearly on the parameters , and with a covariance matrix independent of the parameters .",
    "this case is not exactly equal to the usual one in cmb analyses , but it is very close and is particularly suitable for illustration .",
    "we will use here the following notation .",
    "let @xmath198 = @xmath199 be a @xmath200 matrix containing the @xmath11 data points , which , by hypothesis , are distributed following a multivariate gaussian distribution .",
    "let @xmath201 = @xmath202 be a @xmath203 matrix whose elements are the @xmath154 parameters of the model .",
    "let @xmath204 be a @xmath205 matrix , also given by the model .",
    "their elements are defined so that the mean value of @xmath198 , @xmath206 , is given by the matrix multiplication @xmath207 .",
    "finally , let @xmath208 be the covariance matrix of the problem , defined as @xmath209 where @xmath210 stands for the transpose .",
    "using the previous definitions , the usual @xmath0 and our @xmath1 are given , respectively , by    @xmath211    @xmath212    where we have defined the matrix @xmath213 using the diagonal of the covariance matrix , and our weights ( @xmath214 ) , in the following way : @xmath213@xmath215 , for @xmath216 , and @xmath213@xmath217 for @xmath60 .",
    "it should be noted that the @xmath213 matrix depends implicitly on the weights ( @xmath214 ) .    for this family of models under consideration ,",
    "the optimum estimator is the maximum likelihood , which is given by    @xmath218    until this point , we have not made use of the fact that the covariance matrix is independent of the parameters .",
    "if we use it now , from the last equation we have that the maximum likelihood reduces to the usual @xmath0 for this problem . the estimate for both methods is obtained by minimising the previous expressions with respect to the parameters , so we have    @xmath219 = ( { \\bf { \\hat{y } } } { \\bf { \\hat{m}}}^{-1 } { \\bf { \\hat{x}}}^t )   ( { \\bf { \\hat{x}}}{\\bf { \\hat{m}}}^{-1 } { \\bf { \\hat{x}}}^t ) ^{-1 } \\label{estimate1}\\ ] ]    @xmath220 = ( { \\bf { \\hat{y } } } { \\bf { \\hat{v } } \\it{}}^{-1 } { \\bf { \\hat{x}}}^t )   ( { \\bf { \\hat{x}}}{\\bf { \\hat{v } } \\it{}}^{-1 } { \\bf { \\hat{x}}}^t ) ^{-1 } \\label{estimate2}\\ ] ]    hereafter in this section , we will use subscript @xmath138 for the standard ( usual @xmath0 ) method , and @xmath139 for the @xmath1 .",
    "we compute now the covariance matrices of the parameters , @xmath221 , which are given by    @xmath222 - < e_1 [ { \\bf { \\hat{\\alpha } } } ] > ) =   ( { \\bf { \\hat{x}}}{\\bf { \\hat{m}}}^{-1 } { \\bf { \\hat{x}}}^t ) ^{-1}\\ ] ]    @xmath223 - < e_2 [ { \\bf { \\hat{\\alpha } } } ] > ) = \\ ] ] @xmath224    using this notation , the @xmath89 for each parameter is given by the corresponding element in the diagonal of @xmath221 .    the following point is to compare the estimates of both methods . for the case of one parameter estimation , it can be argued that both method give the same estimate , and therefore have the same @xmath54 .",
    "the argument is as follows : in this case , the estimate for both methods is a linear combination of the @xmath11 quantities @xmath198 .",
    "so it could be possible , in principle , to fix the @xmath11 quantities ( @xmath214 ) to equalise the @xmath11 coefficients in expressions ( [ estimate1 ] ) and ( [ estimate2 ] ) . given that for this particular problem the usual @xmath0 is the optimal method ( i.e. the one with the minimum variance ) , and that the corresponding estimator is the only linear one for which this variance is obtained , those @xmath214 quantities which set equal the coefficients in ( [ estimate1 ] ) and ( [ estimate2 ] ) , are exactly the same that would be obtained by minimisation of the @xmath54 of that parameter .",
    "we have checked this statement for the critical case where we have a @xmath0 with only @xmath225 terms . in figure [ comparachi2s ]",
    "we present several particular examples , showing that for the set of @xmath214 quantities that minimise the @xmath54 for the @xmath1 , we always obtain the same @xmath54 as in the case of the usual @xmath0 .",
    "we have also checked that , for those values of the @xmath214 , the estimates from both methods are exactly the same .",
    "method with correlations , and our @xmath1 method , for several linear gaussian models with only one parameter , @xmath226 , and @xmath225 variables , named ( @xmath227,@xmath228 ) , normally distributed with means @xmath229 and @xmath230 .",
    "we also assume a covariance matrix independent on the parameter , and we parameterise it as @xmath231 @xmath232 , @xmath233 @xmath234 , and @xmath235 @xmath236 @xmath237 , but these results are completely general .",
    "we plot here four typical cases for this problem .",
    "the dot - dashed line in all four panels corresponds to the usual @xmath0 value for the rms in the estimate of @xmath226 .",
    "the solid line is the rms obtained for the @xmath1 , using the shown value of @xmath238 .",
    "the @xmath239 is obtained from the normalisation equation @xmath240 .",
    "we see that it is always possible to find a certain value of @xmath238 , for which we have exactly the same rms for both methods . for that @xmath238 value ,",
    "the estimates are also obtained to be equal ( see text for details ) . ]",
    "let s consider now the case of several ( @xmath241 ) parameters .",
    "for this problem , it is not possible in general to obtain a set of @xmath214 quantities which render the @xmath54 of the usual @xmath0 . in any case",
    ", we have checked that it is always possible to choose those quantities to make the estimate for just one of the parameters exactly equal , and so its @xmath54 .",
    "we have studied this problem in detail for the case of @xmath242 terms in the @xmath0 , and @xmath243 parameters .",
    "the results are the following :    * if we choose to minimise the @xmath54 of only one parameter to find the optimal @xmath214 quantities , we can always make the estimate for that parameter exactly equal to the usual @xmath0 . for those @xmath214 values , the estimator for the other parameter is very close to the optimum , in the sense that the @xmath54 for the other parameter at the most @xmath244 bigger that the optimal one . *",
    "if we minimise the product of the two @xmath54 s , we find in all cases that both estimates are close to the optimal ones , and the largest relative differences between @xmath245 and @xmath246 are smaller than @xmath247 .",
    "therefore , we can conclude that the criteria to obtain the @xmath214 quantities has an small ambiguity , in the sense that if we are interested in one parameter in particular , we should minimise the @xmath54 for that parameter only . in practice , this ambiguity is not relevant because , for this problem , the differences between the estimates and the obtained @xmath54 values are negligible .",
    "therefore , we will maintain the original proposal of minimising the product of @xmath54 s , which in some sense is equivalent to minimise the average size of the confidence region .",
    "all these arguments can be applied to cases with @xmath248 parameters .",
    "the probability density function for a multivariate gaussian distribution of n variables ( @xmath249mgd ) , @xmath250 , is given by    @xmath251    where @xmath252 is called the _ covariance matrix _",
    ", @xmath253 stands for its determinant , and @xmath254 is the mean of @xmath103 .",
    "the elements of @xmath252 are given by    @xmath255    we define @xmath256 .",
    "when computing the mean square of equation ( [ chi2 ] ) , or the @xmath37 matrix given in ( [ est_cf ] ) , we need to know the following quantities : @xmath257 , @xmath258 and @xmath259 .",
    "we will obtain them here . for a @xmath260mgd",
    ", we can obtain the quantity    @xmath261 @xmath262 } dx_1 dx_2 = \\ ] ] @xmath263    for a @xmath264mgd , we obtain    @xmath265 @xmath266    where @xmath267 stands for the elements of the inverse of the c matrix . using the same notation , for a @xmath268mgd , we obtain    @xmath269 @xmath270 @xmath271 @xmath272",
    "the moment generating function of the quadratic form @xmath273 , where the @xmath250 variables follow a @xmath249mgd with zero mean ( eq .",
    "[ mgd ] ) , is given by ( see @xcite )      in this equation , @xmath275 are the eigenvalues of the matrix @xmath276 , where @xmath252 is the covariance matrix ( [ cov_matrix ] ) , and @xmath277 is defined as @xmath278 , being @xmath279 the kronecker - delta . from this expression ,",
    "the distribution function can be obtained by the laplace inverse transform .",
    "formally , we have      where @xmath281 stands for the inverse laplace transform @xcite , and we use the notation @xmath282 for the exact distribution function of the @xmath0 . as an example , we will study in detail the case @xmath225 , comparing the exact distribution function with our approximation .",
    "we compare both the distribution function and the @xmath154-order moments up to @xmath289 with the values obtained using our approximation .",
    "the results quoted here correspond to the case @xmath290 , and @xmath291 , so @xmath292 varies in the range @xmath293 $ ] .",
    "nevertheless , the results are completely general .    for our problem",
    ", we can write @xmath294 , @xmath295 , and @xmath296 . in figure [ app1 ] we present the @xmath297 function for the value of @xmath292 which gives us the maximum percentage difference between the exact and the approximated functions .",
    "this value corresponds to @xmath298 .",
    "the largest percentage difference in the distribution function for this case is reached at @xmath299 , and has a value of @xmath300 . in terms of the weights ,",
    "we obtain for this point a difference of a @xmath301 ( @xmath302 , and @xmath303 ) .",
    "nevertheless , the power of our approximation is that the largest differences always occur at low values of @xmath0 . the asymptotic shape of the exact distribution function is well reproduced , as we need for a @xmath0 analysis .",
    "to conclude , we show in figure [ app2 ] the third and fourth order moments , both for the real distribution and the approximation , in the whole range of values for @xmath292 . by definition ,",
    "the first and second moments are equal for the true and the approximate distribution .",
    "we see again that the approximation follows quite closely the true function , as we have found from the simulations in section 5.1 .     with correlations , with @xmath304 terms .",
    "we have used @xmath290 , and @xmath305 .",
    "we show the case @xmath298 , because for that value we have the largest percentage difference between the true function and our approximation , which is given by @xmath306 and @xmath307 .",
    "we can see that the largest differences occur at low values of @xmath0 .",
    "the asymptotic values are well reproduced . ]"
  ],
  "abstract_text": [
    "<S> we present a new general procedure for determining a given set of quantities . to this end , we define certain statistic , that we call modified @xmath0 ( @xmath1 ) , because of its similarity with the standard @xmath0 . </S>",
    "<S> the terms of this @xmath1 are made up of the fluctuations of an unbiased estimator of some statistical quantities , and certain weights . </S>",
    "<S> only the diagonal terms of the covariance matrix explicitly appear in our statistic , while the full covariance matrix ( and not its inverse ) is implicitly included in the calculation of the weights . choosing these weights we may obtain , through minimising the @xmath1 , the estimator that provides the minimum rms , either for those quantities or for the parameters on which these quantities depend . in this paper , we describe our method in the context of cosmic microwave background experiments , in order to obtain either the statistical properties of the maps , or the cosmological parameters . </S>",
    "<S> the test here is constructed out of some estimator of the two - point correlation function at different angles . for the problem of one parameter estimation , </S>",
    "<S> we show that our method has the same power as the maximum likelihood method . </S>",
    "<S> we have also applied this method to monte carlo simulations of the cobe - dmr data , as well as to the actual 4-year data , obtaining consistent results with previous analyses . </S>",
    "<S> we also provide a very good analytical approximation to the distribution function of our statistic , which could also be useful in other contexts .    </S>",
    "<S> [ firstpage ]    methods : statistical  cosmology : cosmic microwave background  cosmology : cosmological parameters </S>"
  ]
}