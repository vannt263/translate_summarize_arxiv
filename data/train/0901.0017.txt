{
  "article_text": [
    "the em algorithm of dempster laird and rudin ( 1977 ) is a widely applicable methodology for computing likelihood maximizers or at least stationary points .",
    "it has been extensively studied over the years and many useful generalizations have been proposed including , for instance , the stochastic em algorithm of delyon , lavielle and moulines ( 1999 ) and kuhn and lavielle ( 2004 ) ; the px - em accelerations of liu , rubin and wu ( 1998 ) ; the mm generalization of lange and hunter ( 2004 ) and approaches using extrapolation such as proposed in varadhan and roland ( 2007 ) .    in recent years",
    ", much attention has been given to the problem of variable selection for multiparameter estimation , for which the desired solution is sparse , i.e. many of the parameters are zero .",
    "several approaches have been proposed for recovering sparse models .",
    "a large number of contributions are based on the use of non - differentiable penalties like the lasso ( tibshirani ( 1996 ) and cand ` es and plan ( 2008 ) ) , isle ( friedman and popescu ( 2003 ) ) and `` hidden variable''-type approach developed by figueiredo and nowak ( 2003 ) .",
    "other contributions are for instance sparse bayes learning ( tipping ( 2001 ) ) , information theoretic based prior methods of barron ( 1999 ) , empirical bayes ( johnstone and silverman ( 2004 ) ) . among recent alternatives",
    "is the new dantzig selector of cands and tao ( 2008 ) . on the other hand",
    ", only a few attempts have been made to use of non - differentiable penalization for more complex models than the linear model ; for some recent progress , see koh , kim , and boyd ( 2007 ) for the case of logistic regression ; and khalili and chen ( 2007 ) for mixture models .    in the present paper",
    ", we develop new extensions of the em algorithm that incorporate a non - differentiable penalty at each step . following previous work of the first two authors",
    ", we use a kullback proximal interpretation for the em - iterations and prove stationarity of the cluster points of the methods using nonsmooth analysis tools .",
    "our analysis covers coordinate by coordinate methods such as space alternating extensions of em and kullback proximal point ( kpp ) methods .",
    "such component - wise versions of em - type algorithms can benefit from acceleration of convergence speed ( fessler and hero ( 1994 ) ) .",
    "the kpp method was applied to gaussian mixture models in celeux _",
    "the main result of this paper is that any cluster point of the space alternating kpp method satisfies a nonsmooth karush - kuhn - tucker condition .",
    "the paper is organized as follows . in section 2",
    "we review penalized kullback proximal point methods and introduce componentwise pkpp algorithms with new differentiable penalties . in section 3 ,",
    "our main asymptotic results are presented . in section 4 ,",
    "we present a space alternating implementation of the penalized em algorithm for a problem of model selection in a finite mixture of linear regressions using the scad penalty introduced in fan and li ( 2001 ) and further studied in khalili and chen ( 2007 ) .",
    "the problem of maximum likelihood ( ml ) estimation consists of solving the maximization @xmath0 where @xmath1 is an observed sample of a random variable @xmath2 defined on a sample space @xmath3 and @xmath4 is the log - likelihood function defined by @xmath5 defined on the parameter space @xmath6 , and @xmath7 denotes the density of @xmath2 at @xmath1 parametrized by the vector parameter @xmath8 .",
    "the standard em approach to likelihood maximization introduces a complete data vector @xmath9 with density @xmath10 . consider the conditional density function @xmath11 of @xmath9 given @xmath1 @xmath12 as is well known , the em algorithm then consists of alternating between two steps .",
    "the first step , called the e(xpectation ) step , consists of computing the conditional expectation of the complete log - likelihood given @xmath2 .",
    "notice that the conditional density @xmath13 is parametrized by the current iterate of the unknown parameter value , denoted here by @xmath14 for simplicity .",
    "moreover , the expected complete log - likelihood is a function of the variable @xmath8 .",
    "thus the second step , called the m(aximization ) step , consists of maximizing the obtained expected complete log - likelihood with respect to the variable parameter @xmath8 .",
    "the maximizer is then accepted as the new current iterate of the em algorithm and the two steps are repeated until convergence is achieved .",
    "consider now the general problem of maximizing a concave function @xmath15 .",
    "the original proximal point algorithm introduced by martinet ( 1970 ) is an iterative procedure which can be written @xmath16 the influence of the quadratic penalty @xmath17 is controlled by the sequence of positive parameters @xmath18 .",
    "rockafellar ( 1976 ) showed that superlinear convergence of this method occurs when the sequence @xmath18 converges to zero . a relationship between proximal point algorithms and em algorithms",
    "was discovered in chrtien and hero ( 2000 ) ( see also chrtien and hero ( 2008 ) for details ) .",
    "we review the em analogy to kpp methods to motivate the space alternating generalization .",
    "assume that the family of conditional densities @xmath19 is regular in the sense of ibragimov and khasminskii ( 1981 ) , in particular @xmath20 and @xmath21 are mutually absolutely continuous for any @xmath8 and @xmath14 in @xmath22 .",
    "then the radon - nikodym derivative @xmath23 exists for all @xmath24 and we can define the following kullback leibler divergence : @xmath25.\\ ] ] let us define @xmath26 as the domain of @xmath27 , @xmath28 the domain of @xmath29 and @xmath30 the domain of @xmath31 . using the distance - like function @xmath32 ,",
    "the kullback proximal point algorithm is defined by @xmath33 the following was proved in chrtien and hero ( 2000 ) .",
    "[ chrtien and hero ( 2000 ) proposition 1 ] .",
    "[ equiem ] in the case where @xmath34 is the log - likelihood , the em algorithm is a special instance of the kullback - proximal algorithm with @xmath35 , for all @xmath36 .      in what follows , and in anticipation of component",
    "- wise implementations of penalized kpp , we will use the notation @xmath37 for the local decomposition at @xmath8 defined by @xmath38 , @xmath39 where @xmath40 are subspaces of @xmath41 and @xmath42 .",
    "then , the space alternating penalized proximal point algorithm is defined as follows .    [ spalt ] let @xmath43 : @xmath44 be a continuously differentiable mapping and let @xmath45 denote its @xmath46 coordinate .",
    "let @xmath47 be a sequence of positive real numbers and @xmath48 be a positive real vector in @xmath49 .",
    "let @xmath50 be a possibly nonsmooth penalty function with bounded clarke - subdifferential ( see the appendix for details ) on compact sets .",
    "then , the space alternating penalized kullback proximal algorithm is defined by @xmath51 where @xmath26 is the domain of @xmath27 and @xmath28 is the domain of @xmath29 .",
    "the standard kullback - proximal point algorithms as defined in chrtien and hero ( 2008 ) is obtained as special case by selecting @xmath52 , @xmath53 , @xmath54 .",
    "the mappings @xmath45 will simply be the projection onto the subspace @xmath55 , @xmath39 in the sequel but the proofs below allow for more general mappings too .",
    "the notation @xmath56 will be used to denote the norm on any previously defined space .",
    "the space on which the norm operates should be obvious from the context .",
    "for any bivariate function @xmath34 , @xmath57 will denote the gradient with respect to the first variable .",
    "for the convergence analysis , we will make the following assumptions . for a locally lipschitz function @xmath10",
    ", @xmath58 denotes the clarke subdifferential of @xmath10 at @xmath59 ( see the appendix ) .",
    "[ ass1 ] ( i ) @xmath27 is differentiable and @xmath60 converges to @xmath61 whenever @xmath62 tends to @xmath63 .",
    "+ ( ii ) the domain @xmath28 of @xmath64 is a subset of the domain @xmath26 of @xmath65 .",
    "+ ( iii ) @xmath66 is a convergent nonnegative sequence of real numbers whose limit is denoted by @xmath67",
    ". + ( iv ) the mappings @xmath45 are such that @xmath68 for all @xmath8 in @xmath69 , all @xmath70 and @xmath71 sufficiently small so that @xmath72 , @xmath39 .",
    "this condition is satisfied for linear projection operators .",
    "we will also impose one of the two following sets of assumptions on the distance - like function @xmath32 in ( [ kullb ] ) .",
    "[ ass2 ] ( i ) there exists a finite dimensional euclidean space @xmath73 , a differentiable mapping @xmath74 and a functional @xmath75 such that kl divergence ( [ kullb ] ) satisfies @xmath76 where @xmath77 denotes the domain of @xmath78 .",
    "+ ( ii ) for any @xmath79 there exists @xmath80 such that @xmath81 . moreover , we assume that @xmath82 for any bounded set @xmath83 .",
    "+ for all @xmath84 in @xmath85 , we will also require that + ( iii ) ( positivity ) @xmath86 , + ( iv ) ( identifiability ) @xmath87 , + ( v ) ( continuity ) @xmath78 is continuous at @xmath88 + and for all @xmath89 belonging to the projection of @xmath85 onto its second coordinate , + ( vi ) ( differentiability ) the function @xmath90 is differentiable at @xmath89 .    in the case where the kullback divergence @xmath32 is not defined everywhere ( for instance if its domain of definition is the positive orthant )",
    ", we need stronger assumptions to prove the desired convergence properties .",
    "[ ass3 ] ( i ) there exists a differentiable mapping @xmath91 such that the kullback distance - like function @xmath32 is of the form @xmath92 where for all @xmath93 and @xmath94 , @xmath95 is continuously differentiable on its domain of definition , @xmath96 is a function from @xmath3 to @xmath97 , the set of positive real numbers , + ( ii ) the function @xmath98 is a non negative differentiable convex function defined @xmath99 and such that @xmath100 if and only if @xmath101 .",
    "+ ( iii ) there exists @xmath102 such that @xmath103 + ( iv ) the mapping @xmath89 is injective on each @xmath55 .    in the context of assumptions [ ass3 ] , @xmath30 is simply the set @xmath104 notice that if @xmath105 and @xmath106 for all @xmath93 and all @xmath94 , the functions @xmath32 turn out to reduce to the well known @xmath98 divergence defined in csiszr ( 1967 ) .",
    "assumptions [ ass3 ] are satisfied by most standard examples ( for instance gaussian mixtures and poisson inverse problems ) with the choice @xmath107",
    ".    assumptions [ ass1](i ) and ( ii ) on @xmath27 are standard and are easily checked in practical examples , e.g. they are satisfied for the poisson and additive mixture models .    finally we make the following general assumption .",
    "[ ass4 ] the kullback proximal iteration ( [ kullprox ] ) is well defined , i.e. there exists at least one maximizer of ( [ kullprox ] ) at each iteration @xmath13 .    in the em case , i.e. @xmath108 , this last assumption is equivalent to the computability of m - steps .",
    "in practice it suffices to show the inclusion @xmath109 for @xmath110 in order to prove that the solution is unique .",
    "then assumption [ ass1](i ) is sufficient for a maximizer to exist .",
    "these technical assumptions play an important role in the theory developed below . assumption ( i ) 1 on differentiability of the log - likelihood is important for establishing the karush - kuhn - tucker optimality conditions for cluster points .",
    "the fact that the objective should decrease to negative infinity as the norm of the parameter goes to infinity is often satisfied , or can be easily imposed , and is used later to garantee boundedness of the sequence of iterates .",
    "assumption 1 ( ii ) is only needed in order to simplify the analysis since , otherwise , each iterate would lie in the intersection of @xmath26 and @xmath30 and this would lead to asymptotic complications ; this assumption is always satisfied in the models we have encountered in practice .",
    "assumption 1 ( iii ) is standard .",
    "assumption 1 ( iv ) is satisfied when @xmath45 is a projection onto @xmath111 and simplifies the proofs .",
    "assumption 2 imposes natural conditions on the `` distance '' @xmath32 .",
    "assumption 2 ( ii ) ensures that the `` distance '' @xmath32 is large between points whose euclidean distance goes to @xmath63 , thus weakening the assumption that @xmath32 should grow to @xmath63 in such a case .",
    "assumptions 3 are used to obtain the karush - kuhn - tucker conditions in theorem 2 . for this theorem , we require @xmath32 to behave like a standard kullback - leibler `` distance '' and therefore that @xmath32 has a more constrained shape .",
    "assumption 3 ( iii ) is a simplification of assumption 2 ( ii ) .",
    "assumption 3 ( iv ) is a natural injectivity requirement .",
    "under assumptions [ ass1 ] , we state basic properties of the penalized kullback proximal point algorithm . the most basic property is the monotonicity of the penalized likelihood function and the boundedness of the penalized proximal sequence @xmath112 .",
    "the proofs of the following lemmas are given , for instance , in chrtien and hero ( 2000 ) for the unpenalized case ( @xmath54 ) and their generalizations to the present context is straightforward .",
    "we start with the following monotonicity result .",
    "[ truit ] for any iteration @xmath113 , the sequence @xmath112 satisfies @xmath114    [ boundu ] the sequence @xmath112 is bounded .",
    "the next lemma will also be useful and its proof in the unpenalized case where @xmath54 is given in chrtien and hero ( 2008 ) lemma 2.4.3 .",
    "the generalization to @xmath115 is also straightforward .",
    "[ yal ] assume that in the space alternating kpp sequence @xmath112 , there exists a subsequence @xmath116 belonging to a compact set @xmath117 included in @xmath26 .",
    "then , @xmath118    one important property , which is satisfied in practice , is that the distance between two successive iterates decreases to zero .",
    "this property is critical to the definition of a stopping rule for the algorithm .",
    "this property was established in chrtien and hero ( 2008 ) in the case @xmath119 .",
    "[ chrtien and hero ( 2008 ) proposition 4.1.2 ] [ nondegphi ] the following statements hold .",
    "\\(i ) for any sequence @xmath112 in @xmath120 and any bounded sequence @xmath121 in @xmath120 , if @xmath122 then @xmath123 for all @xmath93,@xmath94 such that @xmath124 .",
    "\\(ii ) if @xmath122 and one coordinate of one of the two sequences @xmath112 and @xmath121 tends to infinity , so does the other s same coordinate .      the results of this subsection state that any cluster point @xmath125 such that @xmath126 lies on the closure of @xmath30 satisfies a modified karush - kuhn - tucker type condition .",
    "we first establish this result in the case where assumptions [ ass2 ] hold in addition to assumptions [ ass1 ] and [ ass2 ] for the kullback distance - like function @xmath32 .    for notational convenience ,",
    "we define @xmath127    [ bord ] assume that assumptions [ ass1 ] , [ ass2 ] and [ ass4 ] hold and if @xmath128 , then , for each @xmath39 , @xmath89 is injective on @xmath55 .",
    "assume that the limit of @xmath47 , @xmath67 , is positive .",
    "let @xmath125 be a cluster point of the space alternating penalized kullback - proximal sequence ( [ kullprox ] ) .",
    "assume that all the functions @xmath95 are differentiable at @xmath125 . if @xmath125 lies in the interior of @xmath26 , then @xmath125 is a stationary point of the penalized log - likelihod function @xmath4 , i.e. @xmath129    * proof*. we consider two cases , namely the case where @xmath52 and the case where @xmath128",
    ".    a. if @xmath52 the proof is analogous to the proof of theorem 3.2.1 in chrtien and hero ( 2008 ) . in particular , we have @xmath130 for all @xmath8 such that @xmath131 . since @xmath132 is differentiable at @xmath125 , the result follows by writing the first order optimality condition at @xmath125 in ( [ a ] ) .",
    "b. assume that @xmath128 and let @xmath133 be a subsequence of iterates of ( [ kullprox ] ) converging to @xmath125 .",
    "moreover let @xmath39 and @xmath134 .",
    "for each @xmath13 , let @xmath135 the smallest index greater than @xmath136 , of the form @xmath137 , with @xmath138 and @xmath139 .",
    "using the fact that @xmath89 is injective on every @xmath55 , @xmath39 , lemma [ yal ] and the fact that @xmath47 converges to @xmath140 , we easily conclude that @xmath141 and @xmath142 also converge to @xmath125 .    for @xmath13 sufficiently large",
    ", we may assume that the terms @xmath143 and @xmath144 belong to a compact neighborhood @xmath145 of @xmath126 included in @xmath30 . by definition [ spalt ] of the space alternating penalized",
    "kullback proximal iterations , @xmath146 therefore , @xmath147 continuity of @xmath148 follows directly from the proof of theorem 3.2.1 in chrtien and hero ( 2008 ) , where in that proof @xmath136 has to be replaced by @xmath135 .",
    "this implies that @xmath149 for all @xmath150 such that @xmath151 .",
    "finally , recall that no assumption was made on @xmath8 , and that @xmath145 is a compact neighborhood of @xmath125 .",
    "thus , using the assumption [ ass1](i ) , which asserts that @xmath4 tends to @xmath61 as @xmath62 tends to @xmath63 , we may deduce that ( [ xbz ] ) holds for any @xmath152 such that @xmath153 and , letting @xmath154 tend to zero , we see that @xmath125 maximizes @xmath155 for all @xmath152 such that @xmath156 belongs to @xmath30 as claimed .    to conclude the proof of theorem [ bord ] , take @xmath157 in @xmath41 and decompose @xmath157 as @xmath158 with @xmath159",
    "then , equation ( [ xbz ] ) implies that the directional derivatives satisfy @xmath160 for all @xmath39 . due to assumption [ ass1 ] ( iv ) ,",
    "the directional derivative of @xmath161 in the direction @xmath157 is equal to the sum of the partial derivatives in the directions @xmath162 and , since all other terms in the definition of @xmath163 are differentiable , we obtain using ( [ posdirdif ] ) , that @xmath164 therefore , using characterization ( [ carac ] ) ( in the appendix ) of the subdifferential , the desired result follows .",
    "@xmath165    next , we consider the case where assumptions [ ass3 ] hold .    [ bordp ]",
    "assume that in addition to assumptions [ ass1 ] and [ ass4 ] , assumptions [ ass3 ] hold .",
    "let @xmath125 be a cluster point of the space alternating penalized kullback proximal sequence .",
    "assume that all the functions @xmath95 are continuously differentiable at @xmath125 .",
    "let @xmath166 denote the index of the active constraints at @xmath125 , i.e. @xmath167 . if @xmath125 lies in the interior of @xmath26 , then @xmath125 satisfies the following property : there exists a family of subsets @xmath168 and a set of real numbers @xmath169 , @xmath170 , @xmath39 such that @xmath171    the condition ( [ weakkkt ] ) resembles the traditional karush - kuhn - tucker conditions of optimality but is in fact weaker since the vector @xmath172 in equation ( [ weakkkt ] ) does not necessarily belong to the normal cone at @xmath125 to the set @xmath173 .",
    "* proof of theorem [ bordp]*. let @xmath174 denote the bivariate function defined by @xmath175    as in the proof of theorem [ bord ] , let @xmath133 be a subsequence of iterates of ( [ kullprox ] ) converging to @xmath125",
    ". moreover let @xmath39 and @xmath134 . for each @xmath13",
    ", let @xmath135 be the next index greater than @xmath136 such that @xmath176 . using the fact that @xmath89 is injective on every @xmath55 , @xmath39 , lemma [ yal ] and the fact that @xmath47 converges to @xmath140",
    ", we easily conclude that @xmath141 and @xmath142 also converge to @xmath125 .    due to assumption [ ass3 ] ( iv ) ,",
    "the first order optimality condition at iteration @xmath135 can be written @xmath177 with @xmath178 .",
    "moreover , claim a in the proof of theorem 4.2.1 in chrtien and hero ( 2008 ) , gives that for all @xmath179 such that @xmath180 @xmath181 let @xmath182 be a subset of indices such that the family @xmath183 is linearly independent and spans the linear space generated by the family of all projected gradients @xmath184 . since this linear independence",
    "are preserved under small perturbations ( continuity of the gradients ) , we may assume , without loss of generality , that the family @xmath185 is linearly independent for @xmath13 sufficiently large . for such @xmath13 , we may thus rewrite equation ( [ frst ] ) as @xmath186 where @xmath187    * claim*. _ the sequence @xmath188 has a convergent subsequence for all @xmath179 in @xmath189 . _    * proof of the claim*. since the sequence @xmath112 is bounded , @xmath43 is continuously differentiable and the penalty @xmath50 has bounded subdifferential on compact sets , there exists a convergent subsequence @xmath190 with limit @xmath191 .",
    "now , using equation ( [ claima ] ) , this last equation implies that @xmath192 converges to the coordinates of a vector in the linearly independent family @xmath193 .",
    "this concludes the proof .",
    "@xmath165    the above claim allows us to finish the proof of theorem [ bordp ] .",
    "since a subsequence @xmath194 is convergent , we may consider its limit @xmath195 . passing to the limit",
    ", we obtain from equation ( [ frst ] ) that @xmath196 using the outer semi - continuity property of the subdifferential of locally lipschitz functions ( see appendix ) we thus obtain that @xmath197 .",
    "now , summing over @xmath198 in ( [ clustr ] ) , we obtain @xmath199 moreover , since @xmath200 tends to zero if @xmath201 , i.e. if the constraint on component @xmath179 is not active , equation ( [ paille ] ) implies that @xmath202 where @xmath203 is the subset of active indices of @xmath182 , i.e. @xmath204 . since @xmath205 , this implies that @xmath206 which establishes theorem [ bordp ] once we define @xmath207 . @xmath165",
    "the result ( [ prekkt ] ) can be refined to the classical karush - kuhn - tucker type condition under additional conditions such as stated below .",
    "[ coro ] if in addition to the assumptions of theorem [ bordp ] we assume that either @xmath208 or @xmath209 for all @xmath210 , i.e. such that @xmath211 , then there exists a set of subsets @xmath168 and a family of real numbers @xmath169 , @xmath170 , @xmath39 such that the following karush - kuhn - tucker condition for optimality holds at cluster point @xmath125 : @xmath212",
    "[ mixreg ]    variable subset selection in regression models is frequently performed using penalization of the likelihood function , e.g. using aic , akaike ( 1973 ) and bic , schwarz ( 1978 ) penalties .",
    "the main drawback of these approaches is lack of scalability due to a combinatorial explosion of the set of possible models as the number of variables increases .",
    "newer methods use @xmath213-type penalties of likelihood functions , as in the lasso , tibshirani ( 1996 ) and the dantzig selector of cands and tao ( 2007 ) , to select subsets of variables without enumeration .",
    "computation of maximizers of the penalized likelihood function can be performed using standard algorithms for nondifferentiable optimization such as bundle methods , as introduced in hiriart - urruty and lemarchal ( 1993 ) .",
    "however general purpose optimization methods might be difficult to implement in the situation where , for instance , log objective functions induce line - search problems . in certain cases , the em algorithm , or a combination of em type methods with general purpose optimization routines might be simpler to implement .",
    "variable selection in finite mixture models , as described in khalili and chen ( 2007 ) , represents such a case due to the presence of very natural hidden variables .    in the finite mixture estimation problem considered here ,",
    "@xmath214 are realizations of the response variable @xmath2 and @xmath215 are the associated realizations of the @xmath216-dimensional vector of covariates @xmath9 .",
    "we focus on the case of a mixture of linear regression models sharing the same variance , as in the baseball data example of section 7.2 in khalili and chen ( 2007 ) , i.e. @xmath217 with @xmath218 and @xmath219 .",
    "the main problem discussed in khalili and chen ( 2007 ) is model selection for which a generalization of the smoothly clipped absolute deviation ( scad ) method of fan and li ( 2001,2002 ) is proposed using an mm - em algorithm in the spirit of hunter and lange ( 2004 ) .",
    "no convergence property of the mm algorithm was established .",
    "the purpose of this section is to show that the space alternating kpp em generalization is easily implemented and that stationarity of the cluster points is garanteed by the theoretical analysis of section [ theo ] .",
    "the scad penalty , studied in khalili and chen ( 2007 ) is a modification of the @xmath213 penalty which is given by @xmath220 where @xmath221 is specified by @xmath222 for @xmath223 in @xmath224 .",
    "define the missing data as the class labels @xmath225 of the mixture component from which the observed data point @xmath226 was drawn .",
    "the complete log - likelihood is then @xmath227 setting @xmath228 , the penalized @xmath229-function is given by @xmath230-p_n(\\beta_1,\\ldots,\\beta_k)\\ ] ] where @xmath231    the computation of this @xmath229-function accomplishes the e - step .",
    "moreover , a penalty of the form @xmath232 can be added to the log - likelihood function in order to ensure that assumptions 1(i ) ( convergence of the penalized log - likelihood to @xmath61 for parameter values with norm growing to @xmath63 ) is satisfied for the case where @xmath9 is not invertible . due to the fact that the penalty @xmath50 is a function of the mixture probabilities @xmath233 ,",
    "the m - step estimate of the @xmath234 vector is not given by the usual formula @xmath235 this , however , is the choice made in khalili and chen ( 2007 ) in their implementation .",
    "moreover , optimizing jointly over the variables @xmath236 and @xmath233 is clearly a more complicated task than independently optimizing with respect to each variable .",
    "we implement a componentwise version of em consisting of successively optimizing with respect to the @xmath233 s and alternatively with respect to the vectors @xmath236 .",
    "optimization with respect to the @xmath233 s can be easily performed using standard differentiable optimization routines and optimization with respect to the @xmath236 s can be performed by a standard non - differentiable optimization routine , e.g. as provided by the function optim of scilab using the nd ( standing for non - differentiable ) option .",
    "we now turn to the description of the kullback proximal penalty @xmath32 defined by ( [ kullb ] ) .",
    "the conditional density function @xmath237 is @xmath238 and therefore , the kullback distance - like function @xmath239 is @xmath240    we have @xmath241 subsets of variables with respect to which optimization will be performed successively .",
    "all components of assumptions [ ass1 ] and [ ass3 ] are trivially satisfied for this model .",
    "validation of assumption [ ass3 ] ( iv ) is provided by lemma 1 of celeux _ et al_. ( 2001 ) . on the other hand , since @xmath242 implies that @xmath243 and @xmath243 implies @xmath244 for all @xmath245 and @xmath246 and @xmath247 it follows that @xmath248 if @xmath111 is the vector space generated by the probability vectors @xmath234 and @xmath249 otherwise .",
    "therefore , corollary [ coro ] applies .",
    "we illustrate this algorithm on real data ( available at    _ http://www.amstat.org/publications/jse/v6n2/datasets.watnik.html_ ) .",
    "khalili and chen ( 2007 ) report that a model with only two components was selected by the bic criterion in comparison to a three components model . here",
    ", two alternative algorithms are compared : the approximate em using ( [ approxpiupdates ] ) and the plain em using the optim subroutines .",
    "the results for @xmath250 and @xmath251 are given in figures [ fig1 ] .     and",
    "the first plot is the vector @xmath223 obtained for the single component model .",
    "the second ( resp .",
    "third ) plot is the vector of the optimal @xmath252 ( resp .",
    "@xmath253 ) .",
    "the fourth plot is the euclidean distance to the optimal @xmath125 versus iteration index .",
    "the starting value of @xmath254 was .3 ]    the results shown in figure [ fig1 ] establish that the approximate em algorithm has similar properties to the plain em algorithm for small values of the threshold parameters @xmath255 .",
    "moreover , the larger the values of @xmath255 , the closer the probability of the first component is to 1 .",
    "one important fact to notice is that with the plain em algorithm , the optimal probability vector becomes singular , in the sense that the second component has zero probability , as shown in figure [ degpi ] .",
    "figure [ nondegpi ] demonstrates that the approximate em algorithm of khalili and chen ( 2007 ) does not produce optimal solutions .     and",
    "the plot shows the probability @xmath254 of the first component versus iteration index .",
    "the starting value of @xmath254 was .3 ]     and @xmath251 .",
    "the plot shows the probability @xmath254 of the first component versus iteration index .",
    "the starting value of @xmath254 was .3 ]",
    "in this paper we analyzed the expectation maximization ( em ) algorithm with non - differentiable penalty . by casting the em algorithm as a kullback proximal penalized ( kpp ) iteration , we proved the stationarity of the cluster points and showed that any cluster point of the space alternating kpp method satisfies a nonsmooth karush - kuhn - tucker condition .",
    "the theory was applied to a space alternating implementation of the penalized em algorithm for a problem of model selection in a finite mixture of linear regressions .",
    "since we are dealing with non differentiable functions , the notion of generalized differentiability is required .",
    "the main references for this appendix are clarke ( 1990 ) and rockafellar and wets ( 2004 ) .",
    "a locally lipschitz function @xmath10 : @xmath256 always has a generalized directional derivative @xmath257 : @xmath258 in the sense given by clarke , i.e. @xmath259 the clarke subdifferential of @xmath10 at @xmath8 is the convex set defined by @xmath260            a point @xmath8 is said to be a _ stationary point _ of @xmath10",
    "if @xmath267 consider now the problem @xmath268 subject to @xmath269^t\\geq 0\\ ] ] where all the functions are locally lipschitz from @xmath41 to @xmath224",
    ". then , a necessary condition for optimality of @xmath8 is the karush - kuhn - tucker condition , i.e. there exists a vector @xmath270 such that @xmath271 convex functions are in particular locally lipschitz .",
    "the main references for these facts are rockafellar ( 1970 ) and hiriart - urruty and lemarchal ( 1993 ) .",
    "3 h.  akaike ( 1973 ) .",
    "information theory and an extension of the maximum likelihood principle , in _ second international symposium on information theory _ , eds .",
    "b.n . petrox and f. caski , budapest : akademiai kiado , p.267 .",
    "s. alliney and s.  a. ruzinsky ( 1994 ) .",
    "an algorithm for the minimization of mixed @xmath213 and @xmath272 norms with application to bayesian estimation .",
    "ieee trans .",
    "signal processing , vol .",
    "3 , pp . 618627 .    a. r. barron ( 1999 ) .",
    "information - theoretic characterization of bayes performance and the choice of priors in parametric and nonparametric problems .",
    "bayesian statistics , 6 ( alcoceber , 1998 ) , 2752 , oxford univ .",
    "press , new york , 1999 ."
  ],
  "abstract_text": [
    "<S> the em algorithm is a widely used methodology for penalized likelihood estimation . </S>",
    "<S> provable monotonicity and convergence are the hallmarks of the em algorithm and these properties are well established for smooth likelihood and smooth penalty functions </S>",
    "<S> . however , many relaxed versions of variable selection penalties are not smooth . in this paper </S>",
    "<S> we introduce a new class of space alternating penalized kullback proximal extensions of the em algorithm for nonsmooth likelihood inference . </S>",
    "<S> we show that the cluster points of the new method are stationary points even when they lie on the boundary of the parameter set . </S>",
    "<S> we illustrate the new class of algorithms for the problems of model selection for finite mixtures of regression and of sparse image reconstruction . </S>"
  ]
}