{
  "article_text": [
    "in the age of deep learning and big data , we face a situation where we train ever more complicated models with increasing amounts of data . we have different models for different tasks trained on different datasets ,",
    "each of which is an expert on its own domain , but not on others . in a typical",
    "setting , each new task comes with its own dataset . learning a new task ,",
    "say scene classification based on a pre - existing object recognition network trained on imagenet , requires adapting the model to the new set of classes and fine - tuning it with the new data .",
    "the newly trained network performs well on the new task , but has a degraded performance on the old ones .",
    "this is called _ catastrophic forgetting _  @xcite , and is a major problem facing life long learning techniques @xcite , where new tasks and datasets are added in a sequential manner .",
    "ideally , a system should be able to operate on different tasks and domains and give the best performance on each of them .",
    "for example , an image classification system that is able to operate on generic as well as fine - grained classes , and in addition performs action and scene classification .",
    "if all previous training data were available , a direct solution would be to jointly train a model on all the different tasks or domains . each time a new task arrives along with its own training data ,",
    "the model is retrained on all the tasks and new layers / neurons are added , if needed .",
    "such a solution has three main drawbacks .",
    "the first is the risk of the negative inductive bias when the tasks are not related or simply adversarial .",
    "second , a shared model might fail to capture specialist information for particular tasks as joint training will encourage a hidden representation beneficial for all tasks .",
    "third , each time a new task is to be learned , the whole network needs to be re - trained .",
    "apart from the above drawbacks , the biggest constraint with joint training is that keeping all the data from the previous tasks is a difficult requirement to be met , especially in the era of big data .",
    "for example , ilsvrc  @xcite has 1000 classes , with over a million images , amounting to 200 gb of data . yet",
    "the alexnet model trained on the same dataset , is only 200 mb , a difference in size of three orders of magnitude . with increasing amounts of data collected , it becomes less and less feasible to store all the training data , and more practical to just store the models learned from the data .    without storing the data ,",
    "one can consider strategies like using the previous model to generate virtual samples ( i.e. use the soft outputs of the old model on new task data to generate virtual labels ) and use them in the retraining phase @xcite .",
    "this works to some extent , but is unlikely to scale as repeating this scheme a number of times causes a bias towards the new tasks and an exponential buildup of errors on the older ones , as we show in our experiments . instead of having a network that is jack of all trades and master of none",
    ", we stress the need for having different specialist or expert models for different tasks , as also advocated in  @xcite .",
    "a new expert model is added to our network of experts whenever a new task arrives by transferring knowledge from previous models . with an increasing number of task specializations , the number of expert models increases .",
    "modern gpus , used to speed up training and testing of neural nets , have limited memory ( compared to cpus ) , and can only load a relatively small number of models at a time .",
    "we obviate the need for loading all the models by learning a gating mechanism that uses the test sample to decide which expert to activate ( see figure [ fig : system ] ) .",
    "for this reason , we call our method _ expert gate_.    unlike @xcite , who train one uber network for performing vision tasks as diverse as semantic segmentation , object detection and human body part detection , our work focuses on tasks with a similar objective . for example , imagine a drone trained to fly through an environment using its frontal camera . for optimal performance , it needs to deploy different models for different environments such as indoor , outdoor or forest .",
    "our gating mechanism then selects a model on the fly based on the input video .",
    "another application could be a visual question answering system , that has multiple models trained using images from different domains . here too , our gating mechanism could use the data itself to select the associated task model .    even if we could deploy all the models simultaneously ,",
    "selecting the right expert model is not straightforward .",
    "just using the output of the highest scoring expert is no guarantee for success as neural networks can erroneously give high confidence scores , as shown in @xcite .",
    "we also demonstrate this in our experiments .",
    "training a discriminative classifier to distinguish between tasks is also not an option since that would again require storing all training data .",
    "what we need is a task recognizer that can tell the relevance of its associated task model for a given test sample .",
    "this is exactly what our gating mechanism provides .",
    "we solve the above problems by using an undercomplete autoencoder as a gating mechanism .",
    "we learn for each new task or domain , a gating function that captures the shared characteristics among the different training samples and can recognize similar samples at test time .",
    "we do so using a one layer under - complete autoencoder .",
    "each autoencoder is trained along with the corresponding expert model and maps the training data to its own lower dimensional subspace . at test time",
    ", each task autoencoder projects the sample to its learned subspace and measures the reconstruction error due to the projection .",
    "the autoencoder with the lowest reconstruction error is used like a switch , to select the relevant specialist model ( see figure [ fig : system ] ) . note that we do not need class labels to train the autoencoder as model selector .",
    "this could be useful in the case of semi - supervised training and limited annotation budgets .",
    "interestingly , such autoencoders can be used not just as a gate at test time , but also to evaluate _",
    "task relatedness _ at training time , which in turn can be used to determine which prior model is more relevant to a new task .",
    "we show how , based on this information , expert gate can decide which specialist model to transfer knowledge from when learning a new task and whether to use fine - tuning or learning - without - forgetting  @xcite . to summarize , our contributions are the following .",
    "we develop expert gate , a lifelong learning system that can sequentially deal with new tasks without storing all previous data .",
    "it automatically selects the most related prior task to aid learning of the new task . at test time , the appropriate model is loaded automatically to deal with the task at hand .",
    "we evaluate our gating network on image classification and video prediction problems .",
    "the rest of the paper is organized as follows .",
    "we discuss related work in section [ relwork ] .",
    "expert gate is detailed in section  [ method ] .",
    "experiments evaluating expert gate over image classification and video prediction problems are presented in section [ experiments ] .",
    "we finish with concluding remarks and future work in section [ conclusions ] .",
    "* multi - task learning * our end goal is to develop a system that can reach expert level performance on multiple tasks , with tasks learned sequentially . as such",
    ", it lies at the intersection between multi - task learning and life long learning .",
    "standard multi - task learning @xcite aims at learning multiple tasks in a joint manner .",
    "the objective is to use knowledge from different tasks , the so called inductive bias @xcite , in order to improve performance on individual tasks .",
    "often one shared model is used for all tasks . however",
    ", this typically leads to suboptimal performance on the individual tasks . on the other hand ,",
    "multiple models can be learned , that are each optimal for their own tasks , but utilize inductive bias / knowledge from other models @xcite .    to determine which related tasks to utilize ,  @xcite cluster the tasks based on the mutual information gain when using the information from one task while learning another .",
    "this is an exhaustive process . as an alternative , @xcite",
    "assume that the parameters of the related task models lie close by in the original space or in a lower dimensional subspace and thus cluster the tasks parameters .",
    "they first learn task models independently , then use the tasks within the same cluster to help improving or relearning their models .",
    "we avoid the need for learning independent models first , by using our tasks autoencoders , that are fast to train , to identify related tasks .    * multiple models for multiple tasks * one of the first examples of using multiple models , each one handling a subset of tasks , was by jacobs et al .",
    "they trained an adaptive mixture of experts ( each a neural network ) for multi - speaker vowel recognition and used a separate gating network to determine which network to use for each sample .",
    "they showed that this setup outperformed a single shared model .",
    "a downside , however , was that each training sample needed to pass through each expert , for the gating function to be learned . to avoid this issue ,",
    "it has been proposed not to use a gating function but rather a mixture of one generalist model and many specialist models  @xcite . at test time",
    ", the generalist model acts as a gate to forward the sample to the correct network . unlike our model , these approaches require all the data to be available for learning the generalist model , which needs to be retrained each time new tasks arrive . *",
    "efficient lifelong learning without catastrophic forgetting * in sequential lifelong learning , knowledge from previous tasks is leveraged to improve the training of new tasks , while taking care not to forget old tasks , i.e. preventing catastrophic forgetting @xcite .",
    "our system obviates the need for storing all the training data collected during the lifetime of an agent , by learning task autoencoders that learn the distribution of the task data , and hence , also capture the meta knowledge of the task .",
    "this is one of the desired characteristics of a lifelong learning system , as outlined by silver et al .",
    "the constraint of not storing all previous training data has been looked at previously by silver and mercer  @xcite . in their work",
    ", the output of the previous task networks given new training data , called virtual samples , is used to regularize the training of the networks for new tasks .",
    "this improves the new task performance by using the knowledge of the previous tasks . more recently , li and hoiem  @xcite propose the learning without forgetting which basically use a similar regularization strategy , but learn a single network for all tasks : they finetune a previously trained network ( with additional task outputs ) for new tasks . the contribution of previous tasks / networks in the training of new networks is determined by task relatedness metrics in @xcite , while in @xcite , all previous knowledge is used for training new tasks .",
    "@xcite demonstrated sequential training of a network for only two tasks . in our experiments , we show that training a shared model gets worse when extended to more than two tasks , especially when task relatedness is low .",
    "like us , two recent architectures , namely the progressive network @xcite and the modular block network @xcite , also use multiple networks , a new one for each new task .",
    "they add new networks as additional columns with lateral connections to the previous nets .",
    "these lateral connections mean that each layer in the new network is connected to not only its previous layer in the same column , but also to previous layers from all previous columns .",
    "this allows the networks to transfer knowledge from older to newer tasks .",
    "however , in these works , choosing which column to use for a particular task at test time is done manually , and the authors leave its automation as future work . here , we propose to use an autoencoder to determine which model , and consequently column , is to be selected for a particular test sample .",
    "we consider the case of lifelong learning or sequential learning where tasks and their corresponding data come one after another .",
    "for each task , we learn a specialized model ( expert ) by transferring knowledge from previous tasks  in particular , we build on the _ most related _ previous task . simultaneously we learn a gating function that captures the characteristics of each task .",
    "this gate forwards the test data to the corresponding expert resulting in a high performance over all learned tasks .",
    "the question then is : how to learn such a gate function to differentiate between tasks , without having access to the training data of previous tasks ? to this end , we learn a low dimensional representation where each task or domain lies . at test time",
    "we then select the representation that best fits the test sample .",
    "we do that using an undercomplete autoencoder per task .",
    "below , we first describe this autoencoder in more detail ( section  [ sec : autoencoder ] ) .",
    "next , we explain how to use them for selecting the most relevant expert ( section  [ sec : selection ] ) and for estimating task relatedness ( section  [ sec : relatedness ] ) .",
    "an autoencoder @xcite is a neural network that learns to produce an output similar to its input @xcite .",
    "the network is composed of two parts , an encoder @xmath0 , which maps the input @xmath1 to a code @xmath2 and a decoder @xmath3 , that maps the code to a reconstruction of the input .",
    "the loss function @xmath4 is simply the reconstruction error .",
    "the encoder learns , through a hidden layer , a lower dimensional representation ( undercomplete autoencoder ) or a higher dimensional representation ( overcomplete autoencoder ) of the input data , guided by regularization criteria to prevent the autoencoder from copying its input .",
    "a linear autoencoder with a euclidean loss function learns the principal subspace .",
    "however , autoencoders with non - linear functions yield better dimensionality reduction compared to pca   @xcite .",
    "this motivates our choice for this model .",
    "autoencoders are usually used to learn feature representations in an unsupervised manner or for dimensionality reduction . here , we use them for a different goal .",
    "the lower dimensional subspace learned by one of our undercomplete autoencoders will be maximally sensitive to variations observed in the task data but insensitive to changes orthogonal to the manifold . in other words",
    ", it represents only the variations that are needed to reconstruct relevant samples .",
    "our main hypothesis is that the autoencoder of one domain / task should thus be better at reconstructing the data of that task than the other autoencoders .",
    "comparing the reconstruction errors of the different tasks autoencoders then allows to successfully forward a test sample to the most relevant expert network .",
    "an autoencoder can be composed of multiple layers or even be a convolutional network , as the stacked autoencoder of  @xcite .",
    "however , this would lead to a gate that is as heavy as the original task model . as our aim is to use the autoencoder as a gating function that avoids the need for loading all experts at the same time , an autoencoder that is as complex and memory intensive as the expert networks would defeat our goal .",
    "hence , we design a simple autoencoder that is no more complex than one layer in a deep model . * preprocessing * we start from a robust image representation @xmath1 . in our experiments",
    ", we use @xmath5 features for this  @xcite . before the encoding layer",
    ", we pass the input through a preprocessing step . in preprocessing ,",
    "the input data is standardized , followed by a sigmoid function .",
    "the standardization of the data i.e. subtracting the mean and dividing the result by the standard deviation is essential as it increases the robustness of the hidden representation to input variations .",
    "normally , standardization is done using the data that a network is trained on , but in this case , this is not a good strategy .",
    "this is because , at test time , we compare the relative reconstruction errors of the different autoencoders .",
    "different standardization regimes lead to non - comparable reconstruction errors .",
    "instead , we use the statistics of imagenet for the standardization of each autoencoder .",
    "since this is a large dataset it gives a good approximation of the distribution of natural images .",
    "after standardization , we apply the sigmoid function to map the input to a range of @xmath6 $ ] . * network architecture * based on our goal of simplicity , we design an autoencoder with a one layer encoder / decoder ( see figure  [ fig : structure ] ) .",
    "the encoding step consists of one fully connected layer followed by relu  @xcite .",
    "we make use of relu activation units as they are fast and easy to optimize .",
    "relu also introduces sparsity in the hidden units which leads to better generalization . for decoding",
    ", we use again one fully connected layer , but now followed by a sigmoid . the sigmoid yields values between @xmath7 $ ] , which allows us to use cross entropy as the loss function .",
    "another detail is that we first learn an autoencoder for imagenet ilsvrc2012 data @xcite and then use it as initialization for learning other domain encoders .",
    "this results in a good starting point , leading to faster convergence and better results",
    ". please refer to the supplementary materials for a comparison of different design choices and initializations of our autoencoder .      at test time , and after learning the autoencoders for the different tasks , we add a softmax layer that takes as input the reconstruction errors @xmath8 from the different tasks autoencoders given a test sample @xmath1 .",
    "the reconstruction error @xmath8 of the @xmath9-th autoencoder is the output of the loss function given the input sample @xmath1 .",
    "the softmax layer gives a probability @xmath10 for each task autoencoder indicating its confidence : @xmath11 @xmath12 is the temperature .",
    "we use a temperature value of 2 as in @xcite leading to softer probability values . given these confidence values , we load the expert model associated with the most confident autoencoder . note that for tasks that have some overlap , it may be convenient to activate more than one expert model instead of taking the max score only .",
    "this can be done by setting a threshold on the confidence values , see e.g. section [ sec : expres - gate ] .      given a new task @xmath13 associated with its data @xmath14 , we first learn an autoencoder for this task @xmath15 .",
    "suppose that @xmath16 is a previous task with associated autoencoder @xmath17 .",
    "we want to measure the task relatedness between task @xmath13 and task @xmath16 .",
    "since we do not have access to the previous task @xmath16 data , we use the validation data from the current task @xmath13 .",
    "we compute the average reconstruction error @xmath18 on the current task data made by the current task autoencoder @xmath15 and , likewise , the average reconstruction error @xmath19 made by the previous task autoencoder @xmath17 on the current task data .",
    "the relatedness between the two tasks is then computed : @xmath20 note that the relatedness value is not symmetric . applying this to every previous task",
    ", we get a relatedness value to each previous task .",
    "we exploit task relatedness in two ways .",
    "first , we use it to select the most related task to be used as prior model for learning the new task .",
    "second , we exploit the level of task relatedness to determine which transfer method to use : fine - tuning or learning - without - forgetting ( lwf )  @xcite .",
    "we found in our experiments that lwf only outperforms fine - tuning when the two tasks are sufficiently related .",
    "when this is not the case , enforcing the new model to give similar outputs for the old task may actually hurt performance .",
    "fine - tuning , on the other hand , only uses the previous task parameters as a starting point and is less sensitive to the level of task relatedness .",
    "therefore , we apply a threshold on the task relatedness value to decide when to use lwf and when to fine - tune .",
    "algorithm [ algo ] shows the main steps of our expert gate in both training and test phase .    *",
    "_ training phase _ * input : expert - models @xmath21 , tasks - autoencoders @xmath22 , new task ( @xmath13 ) , data ( @xmath14 ) ; output : @xmath23 @xmath15 = train - task - autoencoder @xmath24 ( _ rel , rel - val_)=select - most - related - task(@xmath14,@xmath15,\\{a } )    @xmath23=lwf@xmath25 @xmath23=fine - tune@xmath25 * _ test phase _ * input : @xmath1 ; output : prediction @xmath9=select - expert@xmath26 prediction = activate - expert@xmath27",
    "[ sec : expres ] first , we compare our method against various baselines on a set of three image classification tasks ( section  [ sec : expres - baselines ] ) .",
    "next , we analyze our gate behavior in more detail on a bigger set of tasks ( section  [ sec : expres - gate ] ) , followed by an analysis of our task relatedness measure ( section  [ sec : expres - related ] ) . finally , we test expert gate on a video prediction problem ( section  [ sec : expres - video ] ) . *",
    "implementation details * we use @xmath5 features @xcite , i.e. the output of the last convolutional layer of an alexnet pre - trained with imagenet as image representation for our autoencoders .",
    "@xmath28 features are standard in many image recognition / retrieval pipelines today .",
    "we experimented with the size of the hidden layer in the autoencoder , trying sizes of 10 , 50 , 100 , 200 and 500 , and found an optimal value of 100 neurons .",
    "this is a good compromise between complexity and performance .",
    "we use the matconvnet framework @xcite in all our experiments .",
    "we start with the sequential learning of three image classification tasks : in order we train on mit _ scenes _",
    "@xcite for scene classification , caltech - ucsd _ birds _",
    "@xcite for fine - grained bird classification and oxford _ flowers _",
    "@xcite for fine - grained flower classification . to simulate a scenario in which an agent or robot has some prior knowledge , and is then exposed to datasets in a sequential manner , we start off with an alexnet model pre - trained on imagenet .",
    "we compare against the following baselines : + _ 1 . multiple fine - tuned models _ : distinct alexnet models ( pretrained on imagenet ) finetuned separately for each task .",
    "+ _ 2 . a single fine - tuned model _ : one alexnet model ( pre - trained on imagenet ) sequentially fine - tuned on each task .",
    "+ _ 3 . a single jointly - trained model _ : assuming all training data is always available , this model is jointly trained ( by finetuning on alexnet model pretrained on imagenet ) for both previous and new tasks each time a new task comes in .",
    "+ _ 4 . a single lwf model _ : learning - without - forgetting  @xcite sequentially applied to multiple tasks .",
    "each new task is learned with all the outputs of the previous network for the new training samples as soft targets .",
    "so , a network ( pre - trained on imagenet ) is first trained for task 1 data without forgetting imagenet ( i.e. using the pretrained alexnet predictions as soft targets ) .",
    "then , this network is trained with task 2 data , now using imagenet and task 1 specific layers outputs as soft targets ; and so on .",
    "multiple lwf models _ : distinct models are learned with lwf , one model per new task , always using alexnet pre - trained on imagenet as previous task . for baselines with multiple models ( 1 and 5 ) , we rely on an oracle gate to select the right model at test time .",
    "so reported numbers for these are upper bounds of what can be achieved in practice .",
    "the same holds for baseline 3 , as it assumes all previous training data is stored and available . for our expert gate system and for each new task , we first select the most related previous task ( including imagenet ) and then learn the new task expert model by transferring knowledge from the most related task model . if the relatedness degree is higher than a threshold , we use lwf ; otherwise , we use fine-tuning.the threshold was set to @xmath29 .",
    "table [ tab : sequential ] shows the classification accuracy achieved on the test sets of the different tasks . for the _ single fine - tuned model _ and _ single lwf model _ , we also report intermediate results in the sequential learning .",
    "when learning multiple models ( one per new task ) , lwf improves over vanilla fine - tuning for scenes and birds , as reported also by @xcite .",
    "however , for flowers , performance degrades compared to fine - tuning .",
    "we measure a lower degree of task relatedness to imagenet for flowers compared to birds or scenes ( see figure [ fig : rel3 ] ) which might explain this effect .    r0.2     comparing the _ single fine - tuned model _",
    "( learned sequentially ) with the _ multiple fine - tuned models _ , we observe an increasing drop in performance on older tasks : sequentially fine - tuning a single model for new tasks shows catastrophic forgetting and is not a good strategy for lifelong learning .",
    "the _ single lwf model _ is less sensitive to forgetting on previous tasks .",
    "however , it is still inferior to training exclusive models for those tasks ( _ multiple fine - tuned models _ ) , both for older as well as newer tasks .",
    "lower performance on previous tasks is because of a buildup of errors and degradation of the soft targets of the older tasks .",
    "this results in lwf failing to compensate for forgetting in a sequence involving more than 2 tasks .",
    "this also adds noise in the learning process of the new task .",
    "further , the previous tasks have varying degree of task relatedness . on these datasets",
    ", we systematically observed the largest task relatedness values for imagenet ( see figure [ fig : rel3 ] ) . treating all the tasks",
    "equally prevents the new task from getting the same benefit of imagenet as in the _ multiple lwf models _ setting . our _ expert gate _",
    "always correctly identifies the most related task , i.e. imagenet .",
    "based on the relatedness degree , it used lwf for birds and scenes while fine - tuning was used for flowers . as a result",
    ", the best expert models were learned for each task . at test time , our gate mechanism succeeds to select the correct model for @xmath30 of the test samples .",
    "this leads to superior results to those achieved by the other two sequential learning strategies ( _ single fine - tuned model _ and _ single lwf model _ ) .",
    "we achieve comparable performance on average to the _ joint training _ that has access to all the tasks data . also , performance is on par with _ multiple fine - tuned models _ or _ multiple lwf models _ that both assume having the task label for activating the associated model .",
    ".[tab : sequential]classification accuracy for the sequential learning of 3 image classification tasks .",
    "methods with * assume all previous training data is still available , while methods with * * use an oracle gate to select the proper model at test time . [ cols=\"<,^,^,^\",options=\"header \" , ]",
    "in the context of lifelong learning , most work has focused on how to exploit knowledge from previous tasks and transfer it to a new task .",
    "little attention has gone to the related and equally important problem of how to select the proper ( i.e. most relevant ) model at test time .",
    "this is the topic we tackle in this paper . to the best of our knowledge , we are the first to propose a solution that does not require storing data from previous tasks .",
    "surprisingly , expert gate s autoencoders can distinguish different tasks equally well as a discriminative classifier trained on all data .",
    "moreover , they can be used to select the most related task and the most appropriate transfer method during training .",
    "combined , this gives us a powerful method for lifelong learning , that outperforms not only the state - of - the - art but also joint training of all tasks simultaneously .",
    "our current system uses only the most related model for knowledge transfer . as future work",
    ", we will explore the possibility of leveraging multiple related models for the training of new tasks  for instance , by exploring new strategies for balancing the contribution of the different tasks by their relatedness degree rather than just varying the learning rates . also a mechanism to decide when to merge tasks with high relatedness degree rather than adding a new expert model , seems an interesting research direction ."
  ],
  "abstract_text": [
    "<S> in this paper we introduce a model of lifelong learning , based on a network of experts . </S>",
    "<S> new tasks / experts are learned and added to the model sequentially , building on what was learned before . </S>",
    "<S> to ensure scalability of this process , data from previous tasks can not be stored and hence is not available when learning a new task . </S>",
    "<S> a critical issue in such context , not addressed in the literature so far , relates to the decision of which expert to deploy at test time . </S>",
    "<S> we introduce a gating autoencoder that learns a representation for the task at hand , and is used at test time to automatically forward the test sample to the relevant expert . </S>",
    "<S> this has the added advantage of being memory efficient as only one expert network has to be loaded into memory at any given time . </S>",
    "<S> further , the autoencoders inherently capture the relatedness of one task to another , based on which the most relevant prior model to be used for training a new expert with fine - tuning or learning - without - forgetting can be selected . </S>",
    "<S> we evaluate our system on image classification and video prediction problems . </S>"
  ]
}