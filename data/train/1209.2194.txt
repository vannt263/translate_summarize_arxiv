{
  "article_text": [
    "widespread deployment of mobile sensors is expected to revolutionize our ability to monitor and control physical environments . however , for these networks to reach their full range of applicability they must be capable of operating in uncertain and unstructured environments . realizing the full potential of networked sensor systems will require the development of protocols that are fully distributed and adaptive in the face of persistent faults and time - varying , unpredictable environments .",
    "our direct motivation is the problem of tracking a direction from chemical gradients .",
    "a network of mobile sensors needs to move in a direction @xmath0 ( understood as a vector on the unit circle ) , which none of the sensors initially knows ; however , intermittently some sensors are able to obtain a sample of @xmath0",
    ". the sensors can observe the velocity of neighboring sensors but , as the sensors move , the set of neighbors of each sensor changes ; moreover , new sensors occasionally join the network and current sensors sometimes permanently leave the network .",
    "the challenge is to design a protocol by means of which the sensors can adapt their velocities based on the measurements of @xmath0 and observations of the velocities of neighboring sensors so that every node s velocity converges to @xmath0 as fast as possible .",
    "this challenge is further complicated by the fact that all estimates of @xmath0 as well as all observations of the velocities of neighbors are assumed to be noisy .",
    "we will consider a natural generalization in the problem , and instead consider the problem of learning an arbitrary vector @xmath0 by a network of mobile nodes subject to time - varying ( and unpredictable ) inter - agent connectivity , and intermittent , noisy measurements .",
    "we begin by formally stating the problem for a fixed number of nodes .",
    "we consider @xmath1 autonomous nodes engaged in the task of learning a vector @xmath2 . at each time",
    "@xmath3 we denote by @xmath4 the graph of inter - agent communications at time @xmath5 : two nodes are connected by an edge in @xmath6 if and only if they are able to exchange messages at time @xmath5 . note that by definition the graph @xmath6 is undirected .",
    "if @xmath7 then we will say that @xmath8 and @xmath9 are neighbors at time @xmath5 .",
    "we will adopt the convention that @xmath6 contains no self - loops .",
    "we will assume the graphs @xmath6 satisfy a standard condition of uniform connectivity over a long - enough time scale : namely , there exists some constant positive integer @xmath10 ( unknown to any of the nodes ) such that the graph sequence @xmath6 is @xmath10-connected , i.e. the graphs @xmath11 are connected for each integer @xmath12 . .",
    "each node maintains an estimate of @xmath0 ; we will denote the estimate of node @xmath8 at time @xmath5 by @xmath13 . at time @xmath5",
    ", node @xmath8 can update @xmath13 as a function of the noise - corrupted estimates @xmath14 of its neighbors .",
    "we will use @xmath15 to denote the noise - corrupted estimate of the offset @xmath16 available to neighbor @xmath8 at time @xmath5 : @xmath17 here @xmath18 is a zero - mean random vector every entry of which has variance @xmath19 , and all @xmath18 are assumed to be independent of each other , as well as all other random variables in the problem ( which we will define shortly ) .",
    "these updates may be the result of a wireless message exchange or may come about as a result of sensing by each node . physically ,",
    "each node is usually able to sense ( with noise ) the relative difference @xmath20 , for example if @xmath13 represent velocities and measurements by the agents are made in their frame of reference .",
    "alternatively , it may be that nodes are able to measure the absolute quantities @xmath21 and then @xmath18 is the sum of the noises in these two measurements .",
    "occasionally , some nodes have access to a noisy measurement @xmath22 where @xmath23 is a zero - mean random vector every entry of which has variance @xmath24 ; we assume all vectors @xmath23 are independent of each other and of all @xmath18 . in this case ,",
    "node @xmath8 incorporates this measurement into its updated estimate @xmath25 .",
    "we will refer to a time @xmath5 when at least one node has a measurement as a _",
    "measurement time_. for the rest of the paper , we will be making an assumption of uniform measurement speed , namely that fewer than @xmath26 steps pass between successive measurement times ; more precisely , letting @xmath27 be the times when at least one node makes a measurement , we will assume that @xmath28 and @xmath29 for all positive integers @xmath30 .",
    "it is useful to think of this formalization in terms of our motivating scenario , which is a collection of nodes - vehicles , uavs , mobile sensors , or underwater gliders - which need to learn and follow a direction .",
    "updated information about the direction arrives from time to time as one or more of the nodes takes measurements , and the nodes need a protocol by which they update their velocities @xmath13 based on the measurements and observations of the velocities of neighboring nodes .",
    "this formalization also describes the scenario in which a moving group of animals must all learn which way to go based on intermittent samples of a preferred direction and social interactions with near neighbors .",
    "an example is collective migration where high costs associated with obtaining measurements of the migration route suggest that the majority of individuals rely on the more accessible observations of the relative motion of their near neighbors when they update their own velocities @xcite .",
    "we now describe the protocol which we analyze for the remainder of this paper .",
    "if at time @xmath5 node @xmath8 does not have a measurement of @xmath0 , it nudges its velocity in the direction of its neighbors : @xmath31 where @xmath32 is the set of neighbors of node @xmath8 at time @xmath5 , @xmath33 is the cardinality of @xmath32 , and @xmath34 is a stepsize which we will specify later .    on the other hand ,",
    "if node @xmath8 does have a measurement @xmath35 , it updates as @xmath36    intuitively , each node seeks to align its estimate @xmath13 with both the measurements it takes and estimates of neighboring nodes .",
    "as nodes align with one another , information from each measurement slowly propagates throughout the system .",
    "our protocol is motivated by a number of recent advances within the literature on multi - agent consensus . on the one hand ,",
    "the weights we accord to neighboring nodes are based on metropolis weights ( first introduced within the context of multi - agent control in @xcite ) and are chosen because they lead to a tractable lyapunov analysis as in @xcite . on the other hand , we introduce a stepsize @xmath34 which we will later choose to decay to zero with @xmath5 at an appropriate speed by analogy with the recent work on multi - agent optimization @xcite .",
    "our protocol is also motivated by models used to analyze collective decision making and collective motion in animal groups @xcite .",
    "our time varying stepsize rule is similar to models of context - dependent interaction in which individuals reduce their reliance on social cues when they are progressing towards their target @xcite .",
    "we now proceed to set the background for our main result , which bounds the rate at which the estimates @xmath13 converge to @xmath0 .",
    "we first state a proposition which assures us that the estimates @xmath13 do indeed converge to @xmath0 almost surely .",
    "[ thm : conv ] if the stepsize @xmath34 is nonnegative , nonincreasing and satisfies @xmath37 then for any initial values @xmath38 , we have that with probability @xmath39 @xmath40    we remark that this proposition may be viewed as a generalization of earlier results on leader - following and learning , which achieved similar conclusions either without the assumptions of noise , or on fixed graphs , or with the assumption of a fixed leader ( see @xcite as well as the related @xcite ) .",
    "our protocol is very much in the spirit of this earlier literature .",
    "all the previous protocols ( as well as ours ) may be thought of as consensus protocols driven by inputs , and we note there are a number of other possible variations on this theme which can accomplish the task of learning the unknown vector @xmath0 .",
    "the sum of the squared distances from the final limit : @xmath41 we will refer to @xmath42 as the variance at time @xmath5 . before we state our main theorem , we introduce some notation .",
    "first , we define the the notion of the lazy metropolis walk on an undirected graph : this is the random walk which moves from @xmath8 to @xmath9 with probability @xmath43 whenever @xmath8 and @xmath9 are neighbors .",
    "moreover , given a random walk on a graph , the hitting time from @xmath8 to @xmath9 is defined to be the expected time until the walk visits @xmath9 starting from @xmath8 .",
    "we will use @xmath44 to refer to the largest degree of any node in the sequence @xmath6 and @xmath45 to refer to the largest number of nodes that have a measurement at any one time ; clearly both @xmath44 and @xmath45 are at most @xmath1 .",
    "finally , @xmath46 denotes the smallest integer which is at least @xmath47 , and recall that @xmath48 is the dimension of @xmath0 and all @xmath13 . with this notation in place , we now state our main result .",
    "[ mainthm ] let the stepsize be @xmath49 for some @xmath50 .",
    "suppose each of the graphs @xmath6 is connected and let @xmath51 be the largest hitting time from any node to any node in a lazy metropolis walk on any of the graphs @xmath6 . if @xmath5 satisfies the lower bound @xmath52^{1/\\epsilon},\\ ] ] then we have the following decay bound on the expected variance : @xmath53   \\leq    15 { \\cal h }",
    "t l \\frac{m \\sigma^2 + n t(\\sigma')^2}{(t / t-1)^{1 -   \\epsilon } }   + z(1 ) e^{-\\frac{(t / t-1)^\\epsilon - 2}{24 \\mathcal{h } t\\epsilon}}.\\ ] ]    in the general case when each @xmath6 is not necessarily connected but the sequence @xmath6 is @xmath10-connected , we have that if @xmath5 satisfies the lower bound @xmath54^{1/\\epsilon}\\ ] ] then we have the following decay bound on the expected variance : @xmath55 \\leq 2 n^{2 } d_{\\rm max } \\max(t , b ) l \\frac { m \\sigma^2 + 2n ( 1 +   \\max(t , b ) )   ( \\sigma')^2}{\\left ( t/\\max(t , b ) \\right)^{1 - \\epsilon } }   + z(1 ) e^{- \\frac{\\left(t/\\max(t , b ) \\right)^\\epsilon - 2}{32n^2 d_{\\rm max }   \\left ( 1 + \\max(t , b ) \\right ) \\epsilon}}.\\ ] ]    our theorem provides a quantitative bound on the convergence time of the repeated alignment process of eq .",
    "( [ nonmeasuringupdate ] ) and eq .",
    "( [ measuringupdate ] ) .",
    "we believe this is the first time a convergence time result has been demonstrated in the setting of time - varying ( not necessarily connected ) graphs , intermittent measurements by possibly different nodes , and noisy communications among nodes .",
    "the convergence time expressions are somewhat unwieldy , and we pause now to discuss some of their features .    first , observe that the convergence times are a sum of two terms : the first which decays with @xmath5 as @xmath56 and the second which decays as @xmath57 ( here @xmath58-notation hides all terms that do not depend on @xmath5 ) . in the limit of large @xmath5",
    ", the second will be negligible and we may focus our attention solely on the first .",
    "thus our finding is that it is possible to achieve a nearly linear decay with time by picking a stepsize @xmath59 with @xmath60 close to zero .",
    "moreover , examining eq .",
    "( [ eq : general ] ) , we find that for every choice of @xmath50 , the scaling with the number of nodes @xmath1 is polynomial .",
    "moreover , in analogy to some recent work on consensus @xcite , better convergence time bounds are available when the largest degree of any node is small .",
    "this is somewhat counter - intuitive since higher degrees are associated with improved connectivity .",
    "a plausible intuitive explanation for this mathematical phenomenon is that low degrees ensure that the influence of new measurements on nodes does not get repeatedly diluted in the update process .",
    "furthermore , while it is possible to obtain a nearly linear decay with the number of iterations @xmath5 as we just noted , such a choice blows up the bound on the transient period before the asymptotic decay bound kicks in .",
    "every choice of @xmath60 then provides a trade off between the transient size and the asymptotic rate of decay .",
    "this is to be contrasted with the usual situation in distributed optimization ( see e.g. , @xcite ) where a specific choice of stepsize usually results in the best bounds .",
    "finally , in the case when all graphs are connected , the effect of network topology on the convergence time comes through the maximum hitting time @xmath51 in all the individual graphs @xmath6 .",
    "there are a variety of results on hitting times for various graphs which may be plugged into theorem [ mainthm ] to obtain precise topology - dependent estimates .",
    "we first mention the general result that @xmath61 for the metropolis chain on an arbitrary connected graph from @xcite . on a variety of reasonably connected graphs ,",
    "hitting times are considerably smaller .",
    "a recent preprint @xcite shows that for many graphs , hitting times are proportional to the inverse degrees . in a 2d or 3d grid , we have that @xmath62 @xcite , where the notation @xmath63 is the same as ordinary @xmath58-notation with the exception of hiding multiplicative factors which are polynomials in @xmath64 .",
    "we illustrate the convergence times of theorem [ mainthm ] with a concrete example .",
    "suppose we have a collection of nodes interconnected in ( possibly time - varying ) 2d grids with a single ( possibly different ) node sampling at every time .",
    "we are interested how the time until @xmath65 $ ] falls below @xmath66 scales with the number of nodes @xmath1 as well as with @xmath66 .",
    "let us assume that the dimension @xmath48 of the vector we are learning as well as the noise variance @xmath24 are constants independent of the number of nodes . choosing a step size @xmath67 , we have that theorem [ mainthm ] implies that variance @xmath68 $ ] will fall below @xmath66 after @xmath69 steps of the protocol .",
    "the exact bound , with all the constants , may be obtained from eq .",
    "( [ eq : connected ] ) by plugging in the hitting time of the 2d grid @xcite .",
    "moreover , the transient period until this exact bound applies ( from eq .",
    "( [ transient ] ) ) has length @xmath70 .",
    "we can obtain a better asymptotic decay approaching @xmath71 by picking a more slowly decaying stepsize , at the expense of lenghtening the transient period .",
    "we believe that our paper is the first to the problem of cooperative multi - agent learning by a network unpredictable communication disruptions and intermittent measurements .",
    "the key features of our model are 1 ) its cooperative nature ( many nodes working together ) 2 ) its reliance only on distributed and local observations 3 ) the incorporation of time - varying communication restrictions .    naturally , our work is not the first attempt to fuse learning algorithms with distributed control or multi - agent settings .",
    "we refer the reader to the recent papers @xcite which study multi - agent learning in a game - theoretic context .",
    "we mention especially the recent surveys .",
    "our work here is very much in the spirit of the recent literature on distributed filtering @xcite and especially @xcite .",
    "these works consider the problem of tracking a time - varying signal from local measurements by each node , which are then repeatedly combined through a consensus - like iteration .",
    "the above - referenced papers consider a variety of schemes to this effect and obtain bounds on their performance , usually stated in terms of solutions to certain lyapunov equations , or in terms of eigenvalues of certain matrices on fixed graphs .",
    "our work is most closely related to a number of recent papers on distributed detection @xcite which seek to evaluate protocols for networked cooperative hypothesis testing and related problems .",
    "like the previously mentioned work on distributed filtering , these papers use the idea of local iterations which are combined through a distributed consensus update , termed `` consensus plus innovations '' ; a similar idea is called `` diffusion adaptation '' in @xcite .",
    "this literature clarified a number of distinct phenomena in cooperative filtering and estimation ; some of the contributions include working out tight bounds on error exponents for choosing the right hypothesis and other performance measures for a variety of settings ( e.g. , @xcite ) , as well as establishing a number of fundamental limits for distributed parameter estimation @xcite .    in this work ,",
    "we consider the related ( and often simpler ) question of learning a static unknown vector .",
    "however , we derive results which are considerably stronger compared to what is available in the previous literature , obtaining convergence rates in settings when the network is time - varying , measurements are intermittent , and communication is noisy . _",
    "most importantly , we are able to explicitly bound the speed of convergence to the unknown vector @xmath0 in these unpredictable settings in terms of network size and combinatorial features of the networks .",
    "_      we now outline the remainder of the paper .",
    "section [ sec : conv ] , which comprises most of our paper , contains the proof of proposition [ thm : conv ] as well as the main result , theorem [ mainthm ] . the proof is broken up into several distinct pieces since some steps are essentially lengthy exercises in analysis .",
    "we begin in section [ subsec : aclass ] which contains some basic facts about symmetric substochastic matrices which will be useful .",
    "the following section [ subsec : decay ] is devoted solely to analyzing a particular inequality .",
    "we will later show that the expected variance satisfies this inequality and apply the decay bounds we derived in that section .",
    "we then begin analyzing properties of our protocol in section [ subsec : sieve ] , before finally proving proposition [ thm : conv ] and theorem [ mainthm ] in section [ sec : proof ] .",
    "finally , section [ sec : simul ] contains some simulations of our protocol and section [ sec : concl ] concludes with a summary of our results and a list of several open problems .",
    "the purpose of this section is to prove theorem [ mainthm ] ; we prove proposition [ thm : conv ] along the way .",
    "we note that the first several subsections contain some basic results which we will have occasion to use later ; it is only in section [ sec : proof ] that we begin directly proving theorem [ mainthm ] .",
    "we begin with some preliminary definitions .",
    "given a nonnegative matrix @xmath72 , we will use @xmath73 to denote the graph whose edges correspond to the positive entries of @xmath74 in the following way : @xmath73 is the directed graph on the vertices @xmath75 with edge set @xmath76 . note that if @xmath74 is symmetric then the graph @xmath73 will be .",
    "we will use the standard convention of @xmath77 to mean the @xmath8th basis column vector and @xmath78 to mean the all - ones vector .",
    "finally , we will use @xmath79 to denote the row sum of the @xmath8th row of @xmath80 and @xmath81 .",
    "when the argument matrix @xmath74 is clear from context , we will simply write @xmath82 and @xmath83 for @xmath84 .      in this subsection",
    "we prove a few lemmas which we will find useful in the proofs of our main theorem .",
    "our first lemma gives a decomposition of a symmetric matrix and its immediate corollary provides a way to bound the change in norm arising from multiplication by a symmetric matrix .",
    "similar statements were proved in @xcite,@xcite , and @xcite .    for any symmetric matrix @xmath74 ,",
    "@xmath85_{kl } ( \\e_k - \\e_l ) ( \\e_k - \\e_l)^t.\\ ] ] [ lemma : decomposition ]    observe that each term @xmath86 in the sum on the right - hand side has row sums of zero , and consequently both sides of the above equation have identical row sums .",
    "moreover , both sides of the above equation are symmetric .",
    "this implies it suffices to prove that all the @xmath87-entries of both sides with @xmath88 are the same .",
    "but on both sides , the @xmath87th element when @xmath88 is @xmath89_{ij}$ ] .",
    "[ sievebound ] for any symmetric matrix @xmath74 , @xmath90_{kl}(x_k - x_l)^2.\\ ] ]    by lemma [ lemma : decomposition ] , @xmath91_{kl } x^t ( \\e_k - \\e_l ) ( \\e_k - \\e_k)^t x \\\\ & = & \\sum_{j=1}^n r_j x_j^2 - \\sum_{k < l } [ a^2]_{kl } ( x_k - x_l)^2 . \\end{aligned}\\ ] ] thus the decrease in squared norm from @xmath47 to @xmath92 is @xmath93_{kl}(x_k - x_l)^2.\\ ] ]    we now introduce a measure of graph connectivity which we call the _ sieve constant _ of a graph , defined as follows . for a nonnegative , stochastic matrix @xmath72 , the sieve constant @xmath94 is defined as @xmath95 for an undirected graph @xmath96 , the sieve constant @xmath97 denotes the sieve constant of the metropolis matrix , which is the stochastic matrix with @xmath98 the sieve constant is , as far as we are aware , a novel graph parameter : we are not aware of any previous works making use of it .",
    "our name is due to the geometric picture inspired by the above optimization problem : one entry of the vector @xmath47 must be held close to zero while keeping it close to all the other entries , with @xmath94 measuring how much `` sieves '' through the gaps .    the sieve constant will feature prominently in our proof of theorem [ mainthm ] ; we will use it in conjunction with lemma [ sievebound ] to bound how much the norm of a vector decreases after multiplication by a substochastic , symmetric matrix .",
    "we will require a bound on how small @xmath94 can be in terms of the combinatorial features of the graph @xmath73 ; such a bound is given by the following lemma .",
    "[ lemma : snonnegativity ] @xmath99 .",
    "moreover , if the smallest entry of the matrix @xmath74 , which we will denote by @xmath100 , satisfies @xmath101 and if the graph @xmath73 is weakly connected we have @xmath102 where @xmath103 is the ( weakly - connected ) diameter of @xmath73 .",
    "it is evident from the definition of @xmath94 that it is necessarily nonnegative .",
    "we will show that for any @xmath104 , @xmath105 this then implies the lemma immediately from the definition of the sieve constant .    indeed",
    ", we may suppose @xmath106 without loss of generality .",
    "suppose the minimum in the above optimization problem is achieved by the vector @xmath47 ; let @xmath107 be the index of the component of @xmath47 with the largest absolute value ; without loss of generality , we may suppose that the shortest path connecting @xmath39 and @xmath107 is @xmath108 ( we can simply relabel the nodes to make this true ) .",
    "moreover , we may also assume @xmath109 ( else , we can just replace @xmath47 with @xmath110 ) .",
    "imply that @xmath111 or @xmath112 and applying cauchy - schwarz @xmath113 or @xmath114      we continue here with some preliminary results which we will use in the course of proving theorem [ mainthm ] .",
    "the proof of that theorem will proceed by arguing that @xmath115 $ ] will satisfy the inequality @xmath116 for some increasing integer sequence @xmath27 and some positive constants @xmath117 .",
    "we will not turn to deriving this inequality for @xmath68 $ ] now ; this will be done later in section [ sec : proof ] .",
    "the current subection is instead devoted to analyzing the consequences of the inequality , specifically deriving a bound on how fast @xmath118 decays as a function of @xmath117 and the sequence @xmath27 .",
    "the only result from this subsection which will be used later is corollary [ effdecay ] ; all the other lemmas proved here are merely steps on the way of the proof of that corollary .",
    "we begin with a lemma which bounds some of the products we will shortly encounter .",
    "suppose @xmath119 $ ] and @xmath50 and for @xmath120 define @xmath121 moreover , we will adopt the convention that @xmath122 when @xmath123 .",
    "then we have @xmath124 [ phibound ]    taking the logarithm of the definition of @xmath125 , and using the inequality @xmath126 , @xmath127 where , in the last inequality , we applied the standard technique of lower - bounding a nonincreasing nonnegative sum by an integral .",
    "we next turn to a lemma which proves yet another bound we will need , namely a lower bound on @xmath5 such that the inequality @xmath128 holds .",
    "suppose @xmath129 and @xmath130 . then @xmath131 . [ betabound ]    on the one hand",
    ", the inequality holds at @xmath132 : @xmath133 on the other hand , the derivative of @xmath134 is nonnegative for @xmath135 , so that the inequality continues to hold for all @xmath136 .    another useful bound is given in the following lemma .",
    "suppose @xmath50 and @xmath137 . then @xmath138 [ epsilonpower ]    note that for fixed @xmath137 the expression on the left is a convex function of @xmath60 and we have equality for both @xmath139 and @xmath140 .",
    "this implies we have equality for all @xmath141 $ ] .",
    "we now combine lemma [ phibound ] and lemma [ epsilonpower ] to obtain a convenient bound on @xmath125 whenever @xmath142 is not too close to @xmath143 .",
    "suppose @xmath119 $ ] and @xmath50",
    ". then if @xmath142 satisfies @xmath144 , we have [ cubedecay ] @xmath145    indeed , observe that as a consequence of lemma [ epsilonpower ] , @xmath146 and consequently @xmath147 the claim now follows by lemma [ phibound ] .",
    "the previous lemma suggests that as long as the distance between @xmath142 and @xmath143 is at least @xmath148 , then @xmath125 will be small .",
    "the following lemma provides a bound on how long it takes until the distance from @xmath149 to @xmath143 is at least this large .",
    "suppose @xmath150^{1/\\epsilon}$ ] , @xmath119 $ ] , and @xmath50",
    ". then @xmath151 .",
    "[ halflemma ]    rearranging , we need to argue that @xmath152 setting @xmath153 this is equivalent to @xmath154 which , by lemma [ betabound ] , occurs if @xmath155 or @xmath156^{1/\\epsilon}.\\ ] ]    for simplicity of presentation , we will henceforth adopt the notation @xmath157^{1/\\epsilon}.\\ ] ]    with all these lemmas in place , we now turn to the main goal in this subsection , which is to analyze how a sequence satisfying eq .",
    "( [ exampledecay ] ) decays with time .",
    "our next lemma does this for the special choice of @xmath158 .",
    "the proof of this lemma relies on all the results derived previously in this subsection .",
    "[ decay ] suppose @xmath159 where @xmath119 $ ] , @xmath160 , and @xmath161 and the initial condition @xmath162 are all nonnegative .",
    "then for @xmath163 we have @xmath164 [ decay0 ]    let us adopt the shorthand @xmath165 .",
    "we have that @xmath166 we will break this sum up into three pieces : @xmath167 we will bound each piece separately .    the first piece can be bounded by using lemma [ halflemma ] to argue that each of the terms @xmath168 is at most @xmath169 , the quantity @xmath170 is upper bounded by @xmath39 , and there are at most @xmath171 terms in the sum . consequently , @xmath172we bound the second piece by arguing that all the terms @xmath168 are bounded above by @xmath161 , whereas the sum of @xmath170 over that range is at most @xmath173 due to lemma [ cubedecay ] .",
    "thus @xmath174 finally , the last term is bounded straightforwardly by lemma [ phibound ] .",
    "putting these three bounds together gives the statement of the current lemma .",
    "finally , we turn to the main result of this subsection , which is the extension of the previous corollary to the case of general sequences @xmath27 .",
    "the following result is the only one which we will have occasion to use later , and its proof proceeds by an appeal to lemma [ decay0 ] .",
    "suppose @xmath175 where @xmath119 $ ] , @xmath161 and the initial condition @xmath162 are nonnegative , @xmath50 and @xmath27 is some increasing integer sequence satisfying @xmath176 and @xmath177 where @xmath26 is some positive integer .",
    "then if @xmath178^{1/\\epsilon } , \\ ] ] we will have @xmath179[effdecay ]    define @xmath180 .",
    "then @xmath181 since @xmath182 , we have that @xmath183 applying lemma [ decay0 ] , we get that for @xmath184^{1/\\epsilon},\\ ] ] we have @xmath185 the corollary now follows since @xmath186 and @xmath176 .      with all the results of the previous subsections in place , we can now turn to the analysis of our protocol .",
    "we do not begin the actual proof of theorem [ mainthm ] in this subsection , but rather we derive some bounds on the decrease of @xmath42 at each step .",
    "it is in the next section [ sec : proof ] , we will make use of these bounds to prove theorem [ mainthm ] .    for the remainder of section [ subsec : sieve ]",
    ", we will assume that @xmath187 , i.e , @xmath0 and all @xmath13 belong to @xmath188 .",
    "we will then define @xmath189 to be the vector that stacks up @xmath190 .",
    "the following proposition describes a convenient way to write eq ( [ nonmeasuringupdate ] ) .",
    "we omit the proof ( which is obvious ) .    [",
    "remark : eqrewrite ] we can rewrite eq .",
    "( [ nonmeasuringupdate ] ) and eq .",
    "( [ measuringupdate ] ) as follows : @xmath191 where :    1 .",
    "if @xmath192 and @xmath193 are neighbors in @xmath6 , @xmath194 as a consequence , @xmath195 is a symmetric matrix .",
    "2 .   if node @xmath8 does not have a measurement of @xmath0 at time @xmath5 , then @xmath196 on the other hand , if node @xmath8 does have a measurement of @xmath0 at time @xmath5 , @xmath197 thus @xmath195 is a diagonally dominant matrix and its graph is merely the intercommunication graph at time @xmath5 : @xmath198 . moreover ,",
    "if no node has a measurement at time @xmath5 , @xmath195 is stochastic .",
    "3 .   if node @xmath8 does not have a measurement of @xmath0 at time @xmath5 , then @xmath199 .",
    "if node @xmath8 does have a measurement of @xmath0 at time @xmath5 , then @xmath200 .",
    "if node @xmath8 has a measurement of @xmath0 at time @xmath5 , @xmath201 is a random variable with mean zero and variance @xmath202 .",
    "else , @xmath203 . each @xmath201 is independent of all @xmath189    putting it all together , we may write our update as @xmath204    let us use @xmath205 for the set of agents who have a measurement at time @xmath5 .",
    "we use this notation in the next lemma , which bounds the decrease in @xmath206 from time @xmath5 to @xmath207 .      observe that , for any @xmath5 , the vector @xmath212 satisfies @xmath213 and therefore , @xmath214 we now apply corollary [ sievebound ] which involves the entries and row - sums of the matrix @xmath215 which we lower - bound as follows . because @xmath195 is diagonally dominant , we have that if @xmath216 then @xmath217_{kl } \\geq [ a(t)]_{kk } [ a(t)]_{kl } \\geq \\frac{1}{2 } \\frac{1}{4 \\max ( d_k(t ) , d_l(t ) ) } \\geq \\frac{1}{8 \\max ( d_k(t ) , d_l(t))}.\\ ] ] moreover , if @xmath30 has a measurement of @xmath0 then the row sum of the @xmath30th row of @xmath74 @xmath218 , which implies that the @xmath30th row sum of @xmath80 is also @xmath218",
    ". consequently , corollary [ sievebound ] implies    @xmath219    next ,    @xmath220    since @xmath221= 0 , \\aor{e[b(t)]=0}$ ] and @xmath222 \\leq   ( m\\sigma^2 + n(\\sigma')^2)/16}$ ] independently of @xmath189 , this immediately implies the first inequality in the statement of the lemma .",
    "the second inequality is then a straightforward consequence of the definition of the sieve constant .      with all the lemmas of the previous subections in place",
    ", we finally begin the proof of our main result , theorem [ mainthm ] . along the way",
    ", we will prove the basic convergence result of proposition [ thm : conv ] .",
    "our first observation in this section is that it suffices to prove it in the case when @xmath187 , ( i.e. , when @xmath0 is a number ) .",
    "indeed , observe that the update equations eq .",
    "( [ nonmeasuringupdate ] ) and ( [ measuringupdate ] ) are separable in the entries of the vectors @xmath13 .",
    "therefore , if theorem [ mainthm ] is proved under the assumption @xmath187 , we may apply it to each component to obtain it for the general case .",
    "thus we will therefore be assuming without loss of generality that @xmath187 for the remainder of this paper .",
    "our first step is to prove the basic convergence result of proposition [ thm : conv ] .",
    "our proof strategy is to repeatedly apply lemma [ lemma : measurementdecrease ] to bound the the decrease in @xmath42 at each stage .",
    "this will yield a decrease rate for @xmath42 which will imply almost sure convergence to the correct @xmath0 .",
    "we first claim that there exists some constant @xmath223 such that if @xmath224 , then @xmath225 \\leq ( 1- c \\delta(t_{k+1 } ) ) z(\\aor{t_k } ) +   \\max(t , b ) \\delta(t_k)^2 \\aor { ( m \\sigma^2 + n ( \\sigma')^2)}.\\ ] ] as a consequence of our assumptions on @xmath34 , we have the following three facts : @xmath226 @xmath227 moreover , eventually it is true that @xmath228 .",
    "implies @xmath229 with probability @xmath39 .",
    "( [ constantdecay ] ) .",
    "consequently , eq . ( [ constantdecay ] ) follows from the assertion @xmath231 + \\sum_{k \\in s(m ) } e[(v_k(m ) - \\mu)^2 ~|~ v(t_k ) ] } { \\sum_{i=1}^n ( v_i(t_k ) - \\mu)^2 } > 0\\ ] ] where the infimum is taken over all vectors @xmath232 such that @xmath233 and over all possible sequences of undirected communication graphs and measurements between time @xmath27 and @xmath234 satisfying the conditions of uniform connectivity and uniform measurement speed .",
    "now since @xmath235 \\geq e[x]^2 $ ] , we have that @xmath236 + \\sum_{k \\in s(m ) } e[(v_k(m ) - \\mu)^2 ~|~ v(t_k ) ] } { \\sum_{i=1}^n ( v_i(t_k ) - \\mu)^2 } \\\\ & & \\geq   \\inf ~~ \\frac{\\sum_{m = t_k}^{t_{k+1}-1 } \\sum_{(k , l ) \\in e(m ) } e[v_k(m ) - v_l(m ) ~|~ v(t_k)]^2 + \\sum_{k \\in s(m ) } e[v_k(m ) - \\mu ~|~ v(t_k)]^2 } { \\sum_{i=1}^n ( v_i(t_k ) - \\mu)^2}.   \\end{aligned}\\ ] ] we will that this last infimum is positive .",
    "let us @xmath237 $ ] for @xmath238 .    clearly , we need to argue that @xmath239 where the infimum is taken over all sequences of undirected communication graphs satisfying the conditions of uniform connectivity and measurement speed and @xmath240 ( which in turn determines all the @xmath241 with @xmath238 through eq .",
    "( [ zequation ] ) ) .    from eq .",
    "( [ zequation ] ) , we have that the expression within the infimum in eq .",
    "( [ zinf ] ) is invariant under scaling of @xmath240 .",
    "so we can conclude that , for any sequence of graphs @xmath6 and measuring sets @xmath205 , the infimum is always achieved by some vector @xmath240 with @xmath242 .",
    "now given a sequence of graphs @xmath6 and a sequence of measuring sets @xmath205 , suppose @xmath240 is a vector that achieves this infimum ; let @xmath243 be the set of indices @xmath8 with @xmath244 , @xmath245 be the set of indices @xmath8 with @xmath246 , and @xmath247 be the set of indices with @xmath248 . since @xmath249 we have that at least one of @xmath250 is nonempty . without loss of generality ,",
    "suppose that @xmath245 is nonempty .",
    "due to the conditions of uniform connectivity and uniform measurement speed , there is a first time @xmath251 when at least one of the following two events happens : ( i ) some node @xmath252 is connected to a node @xmath253 ( ii ) some node @xmath252 has a measurement of @xmath0 .    in the former case ,",
    "@xmath254 and @xmath255 and consequently @xmath256 will be positive ; in the latter case , @xmath254 and consequently @xmath257 will be positive . in either case , the infimum of eq .",
    "( [ zinf ] ) will be strictly positive .",
    "having established proposition [ thm : conv ] , we now turn to the proof of theorem [ mainthm ] .",
    "we will split the proof into several chunks .",
    "recall that theorem [ mainthm ] has two bounds : eq .",
    "( [ eq : connected ] ) which holds when each graph @xmath6 is connected and eq .",
    "( [ eq : general ] ) which holds in the more general case when the graph sequence @xmath6 is @xmath10-connected .",
    "we begin by analyzing the first case .",
    "our first lemma towards that end provides an upper bound on the eigenvalues of the matrices @xmath195 corresponding to connected @xmath6 .",
    "suppose each @xmath6 is connected and at least one node makes a measurement at every time @xmath5 .",
    "then the largest eigenvalue of the matrices @xmath195 satisfy @xmath258 [ conneig ] where , recall , @xmath51 is an upper bound on the hitting times in the lazy metropolis walk on @xmath6 .",
    "we then have that if @xmath266 is a stochastic vector ( meaning it has nonnegative entries which sum to one ) , then @xmath267 generated by eq .",
    "( [ aupdate ] ) has the following interpretation : @xmath268 is the probability that a random walk taking steps according to @xmath260 is at node @xmath9 at time @xmath30 .",
    "this is easily seen by induction : clearly it holds at time @xmath269 , and if it holds at time @xmath30 , then it holds at time @xmath270 since @xmath271_{ij } = [ a']_{ij}$ ] if @xmath272 and @xmath263_{i',j}=0 $ ] for all @xmath273 and @xmath274 .",
    "next , we note that @xmath264 is an absorbing set for the markov chain with transition matrix @xmath260 , and moreover @xmath275 is the probability that the random walk starting at @xmath266 is not absorbed in @xmath264 by time @xmath30 . defining @xmath276 to be the expected time until a random walk in @xmath260 starting from @xmath8 is absorbed in the set @xmath264 , we have that by markov s inequality , @xmath277 for any stochastic @xmath266 .",
    "thus for any nonnegative integer @xmath30 , @xmath278 now by convexity of the @xmath39-norm , we have that in fact for all initial vectors @xmath266 ( not necessarily stochastic ones ) , @xmath279 by the perron - frobenius theorem , @xmath280 is real and its corresponding eigenvector is real .",
    "plugging it in for @xmath266 , we get that @xmath281 or @xmath282 where we used the inequality @xmath283 for all @xmath284 $ ] .",
    "it remains to rephrase this bound in terms of the hitting times in the lazy metropolis walk on @xmath285 .",
    "we simply note that @xmath263_{i , i ' } = 1/4 $ ] by construction , so @xmath286 thus @xmath287 combining this bound with eq .",
    "( [ lambdap ] ) proves the lemma .",
    "we use proposition [ remark : eqrewrite ] to write a bound for @xmath288 $ ] in terms of the largest eigenvalue @xmath289 . indeed , as we remarked previously in eq .",
    "( [ yminusmu ] ) , @xmath290 we therefore have that @xmath291 where we used the fact that @xmath292 since @xmath195 is a substochastic matrix .",
    "next , we have @xmath293 and finally @xmath294 \\leq \\left [ 1 - \\delta(t ) \\left ( 1 - \\lambda_{\\rm max}(a(t ) ) \\right ) \\right ] z(t ) + \\delta(t_{k})^2 \\frac{\\aor { m\\sigma^2 + n(\\sigma')^2}}{16}.\\ ] ] let @xmath27 be the times when a node has had a measurement . applying the above equation at times @xmath295 and using the eigenvalue bound of lemma [ conneig ] at time @xmath296 and the trivial bound @xmath292 at times @xmath297 , we obtain , @xmath298 & \\leq & \\left ( 1 - \\frac{1}{24 \\mathcal{h } t_k^{1-\\epsilon } } \\right ) z(t_k ) + \\frac { m \\sigma^2 + nt ( \\sigma')^2 } { 16 t_k^{2 - 2 \\epsilon } } \\\\   & \\leq & \\left ( 1 - \\frac{1}{24 \\mathcal{h }   t_{k+1}^{1-\\epsilon } } \\right ) z(t_k ) + \\frac{m \\sigma^2 + nt(\\sigma')^2}{16 t_k^{2 - 2 \\epsilon}}.\\end{aligned}\\ ] ] iterating expectations and applying corollary [ effdecay ] , we obtain that for @xmath299^{1/\\epsilon},\\ ] ] we have @xmath300 \\leq \\frac{9 ( 1/16 )   ( m\\sigma^2 + nt(\\sigma')^2)24 { \\cal h } t}{k^{1 - \\epsilon } }   + z(1 ) e^{-(k^\\epsilon - 2)/(24 \\mathcal{h } t\\epsilon)}.\\ ] ] using the inequality @xmath301 , @xmath302 \\leq 14 { \\cal h } t \\frac {    m \\sigma^2 + n t(\\sigma')^2}{(t_k / t)^{1 - \\epsilon } }   + z(1 ) e^{-((t_k / t)^\\epsilon - 2)/(24 \\mathcal{h } t\\epsilon)}.\\ ] ] finally , for any @xmath5 satisfying @xmath303^{1/\\epsilon}\\ ] ] there is some @xmath27 with @xmath30 satisfying eq .",
    "( [ kbound ] ) within the last @xmath26 steps before @xmath5 .",
    "we can therefore get an upper bound on @xmath304 $ ] applying eq .",
    "( [ expect - tk ] ) to that last @xmath27 , and noting that the expected increase from that @xmath305 to @xmath42 is bounded as @xmath306 - e[z(t_k ) ~|~ v(1 ) ] \\leq \\frac{nt ( \\sigma')^2}{t_k^{2 - 2\\epsilon } } \\leq \\frac{n t ( \\sigma')^2}{t_k^{1-\\epsilon } }   \\leq \\frac{n t ( \\sigma')^2}{(t / t-1)^{1-\\epsilon}}\\ ] ] this implies that for @xmath5 satisfying eq .",
    "( [ tlower ] ) , we have @xmath307   \\leq   15 { \\cal h } t \\frac{m \\sigma^2 + n t(\\sigma')^2}{(t / t-1)^{1 - \\epsilon } }   + z(1 ) e^{-((t / t-1)^\\epsilon - 2)/(24 \\mathcal{h } t\\epsilon)}.\\ ] ] after some simple algebra , this is exactly the bound of the theorem .",
    "we now turn to the proof of the second part of theorem [ mainthm ] , namely eq .",
    "( [ eq : general ] ) . its proof requires a certain inequality between quadratic forms in the vector @xmath189 which we separate into the following lemma .",
    "let @xmath28 and @xmath308 for @xmath309 , and assume that the entries of the vector @xmath232 satisfy @xmath310 further , let us assume that none of the @xmath311 equal @xmath0 , and let us define @xmath312 to be the largest index such that @xmath313 and @xmath314 to be the smallest index such that @xmath315 .",
    "we then have @xmath316 +   \\sum_{k \\in s(m ) } e[(v_k(m ) - \\mu)^2 ~|~ v(t_k ) ]   & \\geq & \\nonumber\\\\   ( v_{p_-}(t_k ) - \\mu)^2 + ( v_{p+}(t_k ) - \\mu)^2 +   \\sum_{i=1 , \\ldots , n , ~~ i",
    "\\neq p_{- } }   ( v_i(t_k ) - v_{i+1}(t_k))^2    & &   \\label{eq : quantdecbound }   \\end{aligned}\\ ] ] [ lemma : quantdecbound ]    as in the statement of lemma [ lemma : quantdecbound ] , we choose @xmath176 , and @xmath308 for @xmath309 .",
    "@xmath317 +    \\sum_{k \\in s(m ) } e [ ( v_k(m ) - \\mu)^2 ~|~ v(t_k ) ]   \\geq   \\frac{1}{4 } \\kappa(l_n ) z(t_k).\\ ] ] because @xmath34 is decreasing and the degree of any vertex at any time is at most @xmath44 , this in turn implies    @xmath318}{\\max ( d_k(m ) , d_l(m ) ) }   + \\frac{\\delta(m)}{4   }   \\sum_{k \\in s(m ) } e [ ( v_k(m ) - \\mu)^2 ~|~ v(t_k ) ]   \\geq \\frac{\\delta(t_{k+1})}{32 d_{\\rm max } } \\kappa(l_n ) z(t_k).\\ ] ]    now appealing to eq .",
    "( [ decreaseineq ] ) , we have @xmath319 \\leq \\left(1 - \\frac{\\delta(t_{k+1 } ) \\kappa(l_n)}{32 d_{\\rm max } } \\right ) z(t_k ) +   \\delta(t_k)^2 \\frac{\\aor { m \\sigma^2 + n \\max(t , b ) ( \\sigma')^2}}{16}.\\ ] ] now applying corollary [ effdecay ] as well as the bound @xmath320 from lemma [ lemma : snonnegativity ] , we get that for @xmath321^{1/\\epsilon}\\ ] ] we will have @xmath322 \\leq ( 9/16 ) n^2 d_{\\rm max } t \\frac { m \\sigma^2 + n ( 1+\\max(t , b ) )   ( \\sigma')^2}{k^{1 - \\epsilon } }   + z(1 ) e^{-(k^\\epsilon - 2)/(32n^2 d_{\\rm max }   \\left ( 1 + \\max(t , b ) \\right ) \\epsilon)}.\\ ] ] now using @xmath323 for @xmath324 , we have @xmath325 \\leq n^2 d_{\\rm max } \\max(t , b ) \\frac { m \\sigma^2 + n \\max(t , b )   ( \\sigma')^2}{\\left ( 1 + t_k/\\max(t , b ) \\right)^{1 - \\epsilon } }   + z(1 ) e^{- ( \\left(1 + t_k/\\max(t , b)\\right)^\\epsilon - 2)/(32n^2 d_{\\rm max }   \\left ( 1 + \\max(t , b ) \\right ) \\epsilon)}.\\ ] ]    for a general time @xmath5 , we have that as long as @xmath326^{1/\\epsilon}\\ ] ] we have that there exists a @xmath327 with @xmath30 satisfying the lower bound of eq .",
    "( [ finalkbound ] ) .",
    "moreover , the increase from this @xmath328 $ ] to @xmath329 $ ] is upper bounded by @xmath330 .",
    "thus : @xmath307 \\leq 2 n^2 d_{\\rm max } \\max(t , b ) \\frac { m \\sigma^2",
    "+ n ( 1+\\max(t , b ) )   ( \\sigma')^2}{\\left ( t/\\max(t , b ) \\right)^{1 - \\epsilon } }   + z(1 ) e^{- ( \\left(t/\\max(t , b)\\right)^\\epsilon - 2)/(32n^2 d_{\\rm max }   \\left ( 1 + \\max(t , b ) \\right ) \\epsilon)}.\\ ] ] after some simple algebra , this is exactly what we sought to prove .",
    "we report here on several simulations of our learning protocol .",
    "these simulations confirm the broad outlines of the bounds we have derived ; the convergence to @xmath0 takes place at a rate broadly consistent with inverse polynomial decay in @xmath5 and the scaling with @xmath1 appears to be polynomial as well .",
    "figure [ threeplots ] shows plots of the distance from @xmath0 for the complete graph , the line graph , and the star graph , each on @xmath331 nodes .",
    "these are the three graphs in the left column of the figure .",
    "we caution that there is no reason to believe these charts capture the correct asymptotic behavior as @xmath332 .",
    "intriguingly , the star graph and the complete graph appear to have very similar performance . by contrast",
    ", the performance of the line graph is an order of magnitude inferior to the performance of either of these ; it takes the line graph on @xmath331 nodes on the order of @xmath333 iterations to reach roughly the same level of accuracy that the complete graph and star graph reach after about @xmath334 iterations .",
    "moreover , figure [ threeplots ] also shows the scaling with the number of nodes @xmath1 on the graphs in the right column of the figure .",
    "the graphs show the time until @xmath335 decreases below a certain threshold as a function of number of nodes .",
    "we see scaling that could plausibly be superlinear for the line graph and essentially linear for the complete graph and essentially linear for the star graph over the range shown .",
    "@xmath336{complete1.eps } & \\includegraphics[width=2.5in]{complete1.eps }   \\\\",
    "\\includegraphics[width=2.5in]{star1.eps } &   \\includegraphics[width=2.5in]{star2.eps }   \\\\ \\includegraphics[width=2.5in]{line1.eps }   & \\includegraphics[width=2.5in]{line2.eps }   \\end{array}$ ]    finally , we include a simulation for the lollipop graph , defined to be a complete graph on @xmath337 vertices joined to a line graph on @xmath337 vertices .",
    "the lollipop graph often appears as an extremal graph for various random walk properties ( see , for example , @xcite ) .",
    "the scaling with the number of nodes is considerably worse than for the other graphs we have simulated here .",
    "@xmath336{lollipop1.eps } & \\includegraphics[width=2.5in]{lollipop2.eps }   \\end{array}$ ]",
    "we have proposed a model for cooperative learning by multi - agent systems facing time - varying connectivity and intermittent measurements . we have proved a protocol capable of learning an unknown vector from independent measurements in this setting and provided quantitative bounds on its learning speed .",
    "crucially , these bounds have a dependence on the number of agents @xmath1 which grows only polynomially fast , leading to reasonable scaling for our protocol .",
    "we note that the sieve constant of a graph , a new measure of connectivity we introduced , played a central role in our analysis . on sequences of connected graphs ,",
    "the largest hitting time turned out to be the most relevant combinatorial primitive .",
    "our research points to a number of intriguing open questions .",
    "our results are for undirected graphs and it is unclear whether there is a learning protocol which will achieve similar bounds ( i.e. , a learning speed which depends only polynomially on @xmath1 ) on directed graphs .",
    "it appears that our bounds on the learning speed are loose by several orders of magnitude when compared to simulations , so that the learning speeds we have presented in this paper could potentially be further improved .",
    "moreover , it is further possible that a different protocol provides a faster learning speed compared to the one we have provided here .",
    "finally , and most importantly , it is of interest to develop a general theory of decentralized learning capable of handling situations in which complex concepts need to be learned by a distributed network subject to time - varying connectivity and intermittent arrival of new information .",
    "consider , for example , a group of uavs all of which need to learn a new strategy to deal with an unforeseen situation , for example , how to perform formation maintenance in the face .",
    "given that selected nodes can try different strategies , and given that nodes can observe the actions and the performance of neighboring nodes , is it possible for the entire network of nodes to collectively learn the best possible strategy ?",
    "a theory of general - purpose decentralized learning , designed to parallel the theory of pac ( provably approximately correct ) learning in the centralized case , is warranted .",
    "an earlier version of this paper published in the cdc proceedings had an incorrect decay rate with @xmath5 in the main result .",
    "the authors are grateful to sean meyn for pointing out this error .",
    "r. alur , a. dinnocenzo , k. h. johansson , g. j. pappas , g. weiss , `` compositional modeling and analysis of multi - hop control networks , '' _ ieee transactions on automatic control _ , vol .",
    "2345 - 2357 , 2011 .",
    "d. bajovic , d. jakovetic , j.m.f .",
    "moura , j. xavier , b. sinopoli `` large deviations performance of consensus+innovations distributed detection with non - gaussian observations , '' _ ieee transactions on signal processing _",
    "5987 - 6002 , 2012 .",
    "d. bajovic , d. jakovetic , j. xavier , b. sinopoli , j.m.f .",
    "moura , `` distributed detection via gaussian running consensus : large deviations asymptotic analysis , '' _ ieee transactions on signal processing _ , vol .",
    "9 , pp . 4381 - 4396 , 2011 .",
    "a. k. chandra , p. raghavan , w. l. ruzzo , r. smolensky , p. tiwari , `` the electrical resistance of a graph captures its commute and cover time , '' _ computational complexity _ , vol . 6 , no .",
    "312 - 340 , 1996 .",
    "v. guttal and i. d. couzin , `` social interactions , information use , and the evolution of collective migration , '' _ proceedings of the national academy of sciences _ , vol .",
    "37 , pp . 16172 - 16177 , 2010 .",
    "m. huang and j. m. manton , `` coordination and consensus of networked agents with noisy measurments : stochastic algorithms and asymptotic behavior , '' _ siam journal on control and optimization _ , vol .",
    "134 - 161 , 2009 .",
    "d. jakovetic , j.m.f .",
    "moura , j. xavier , `` consensus+innovations detection : phase transition under communication noise , '' _ proceedings of the 50th annual allerton conference on communication , control and computing _",
    ", 2012 .",
    "s. kar , j.m.f .",
    "moura , `` convergence rate analysis of distributed gossip ( linear parameter ) estimation : fundamental limits and tradeoffs , '' _ ieee journal on selected topics on signal processing _ , vol . 5 , no .",
    "674 - 690 , 2011 .",
    "n. e. leonard , t. shen , b. nabet , l. scardovi , i. d. couzin and s. a. levin ,  decision versus compromise for animal groupsin motion \" , _ proceedings of the national academy of sciences _ , vol .",
    "227 - 232 , 2012 .",
    "s. martinez , f. bullo , j. corts , e. frazzoli , `` on synchronous robotic networks - part i : models , tasks and complexity , '' _ ieee transactions on automatic control _",
    "2199 - 2213 , 2007 .",
    "z. meng , w. ren , y. cao , z. you , `` leaderless and leader - following consensus with communication and input delays under a directed network topology , '' _ ieee transactions on systems , man , and cybernetics - part b _",
    "41 , no . 1 , 2011 .                    s.s .",
    "ram , a. nedic , v.v .",
    "veeravalli , `` distributed stochastic subgradient projection algorithms for convex optimization , '' _ journal of optimization theory and applications , _ vol .",
    "516 - 545 , 2010 .",
    "d. spanos , r. olfati - saber , r. m. murray , `` approximate distributed kalman filtering in sensor networks with quantifiable performance , '' _ proceedings of the fourth international symposium on information processing in sensor networks _",
    ", 2005 .            c. torney",
    ", z. neufeld and i. d. couzin , `` context - dependent interaction leads to emergent search behavior in social aggregates , '' _ proceedings of the national academy of sciences _ , vol .",
    "22055 - 22060 , 2009 .",
    "b. touri and a. nedic , `` on existence of a quadratic comparison function for random weighted averaging dynamics and its implications , '' _ proceedings of the 50th ieee conference on decision and control , _ 2011 ."
  ],
  "abstract_text": [
    "<S> motivated by the problem of tracking a direction in a decentralized way , we consider the general problem of cooperative learning in multi - agent systems with time - varying connectivity and intermittent measurements . </S>",
    "<S> we propose a distributed learning protocol capable of learning an unknown vector @xmath0 from noisy measurements made independently by autonomous nodes . </S>",
    "<S> our protocol is completely distributed and able to cope with the time - varying , unpredictable , nature of inter - agent communication , and intermittent noisy measurements of @xmath0 . </S>",
    "<S> our main result bounds the learning speed of our protocol in terms of the size and combinatorial features of the ( time - varying ) networks connecting the nodes .    </S>",
    "<S> multi - agent systems , learning theory , distributed control .    93e35 , 93a14 . </S>"
  ]
}