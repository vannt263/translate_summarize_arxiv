{
  "article_text": [
    "a central mission of theoretical computer science is to understand which computational problems can be solved efficiently , which ones can not , and what it is about a problem that makes it easy or hard . to illustrate these kind of questions ,",
    "let us consider the following parameters of an undirected @xmath0-regular graph   @xmath1 :    * the _ smallest connected component of @xmath2 _ is the size of the smallest non - empty set @xmath3 such that @xmath4 .",
    "* the _ independent - set number of @xmath2 _ is the size of the largest set @xmath3 such that @xmath5 . *",
    "the _ ( edge ) expansion  of @xmath2 _ , denoted @xmath6 , is the minimum _ expansion _",
    "@xmath7 of a vertex set @xmath3 with size @xmath8 , where @xmath9 the expansion @xmath7 measures the probability that a step of the random walk on @xmath2 leaves @xmath10 conditioned on starting in @xmath10 .",
    "all these parameters capture different notions of well - connectedness of the graph @xmath2 .",
    "computing these can be very useful in many of the settings in which we use graphs to model data , whether it is communication links between servers , social connections between people , genes that are co - expressed together , or transitions between states of a system .",
    "the computational complexity of the first two parameters is fairly well understood .",
    "the smallest connected component is easy to compute in time linear in the number @xmath11 of vertices by using , for example , breadth - first search from every vertex in the graph .",
    "the independent - set number is @xmath12-hard to compute , which means that , assuming the widely believed conjecture that @xmath13 , it can not be computed in time polynomial in @xmath14 .",
    "in fact , under stronger ( but still widely believed ) quantitative versions of the @xmath15 conjecture , for every @xmath16 it is infeasible to decide whether or not the maximum independent set is larger than @xmath16 in time @xmath17  @xcite and hence we can not significantly beat the trivial @xmath18-time algorithm for this problem .",
    "similarly , while we can approximate the independent - set number trivially within a factor of @xmath14 , assuming such conjectures , there is no polynomial - time algorithm to approximate it within a factor of @xmath19 where @xmath20 is some function tending to zero as @xmath14 grows  @xcite .",
    "so , connectivity is an easy problem and independent set a hard one , but what about expansion ?",
    "here the situation is more complicated .",
    "we know that we ca nt efficiently compute @xmath6 exactly , and we ca nt even get an arbitrarily good approximation  @xcite , but we actually do have efficient algorithms with non - trivial approximation guarantees for @xmath6 .",
    "discrete versions of _ cheeger s inequality _",
    "@xcite yield such an estimate , namely @xmath21 where @xmath22 denotes the ( efficiently computable ) second largest eigenvalue of the @xmath2 s adjacency matrix .",
    "is the @xmath23 matrix @xmath24 with @xmath25 entries such that @xmath26 iff @xmath27 . ] in particular , we can use ( [ eq : cheeger ] ) to efficiently distinguish between graphs with @xmath6 close to @xmath28 and graphs with @xmath6 bounded away from @xmath28 . but",
    "can we do better ? for example , could we efficiently compute a quantity @xmath29 such that @xmath30",
    "? we simply do nt know . up to factors depending on the number @xmath14 of vertices , which give better guarantees than ( [ eq : cheeger ] ) for graphs where @xmath6 is sufficiently small as a function of @xmath14 . ]",
    "this is not an isolated example , but a pattern that keeps repeating . over the years",
    ", computer scientists have developed sophisticated tools to come up with algorithms on one hand , and hardness proofs showing the limits of efficient algorithms on the other hand .",
    "but those two rarely match up .",
    "moreover , the cases where we do have tight hardness results are typically in settings , such as the independent set problem , where there is no way to significantly beat the trivial algorithm .",
    "in contrast , for problems such as computing expansion , where we already know of an algorithm giving non - trivial guarantees , we typically have no proof that this algorithm is _",
    "optimal_. in other words , the following is a common theme :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ if you already know an algorithm with non - trivial approximation guarantees for a problem , it s very hard to rule out that cleverer algorithms could nt get even better guarantees .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    in 2002 , subhash khot formulated a conjecture , known as the _ unique games conjecture ( ugc ) _  @xcite . a large body of",
    "follow up works has shown that this conjecture ( whose description is deferred to section  [ sec : ugc - sseh ] below ) implies many hardness results that overcome the above challenge and match the best - known algorithms even in cases when they achieve non - trivial guarantees .",
    "in fact , beyond just resolving particular questions , this line of works obtained far - reaching complementary _ meta algorithmic _ and _ meta hardness _ results . by this",
    "we mean results that give an efficient _ meta algorithm _",
    "@xmath31 ( i.e. , an algorithm that can be applied to a family of problems , and not just a single one ) that is _ optimal _ within a broad domain @xmath32 , in the sense that ( assuming the ugc ) there is no polynomial - time algorithm that performs better than @xmath31 on any problem in @xmath32 .",
    "it is this aspect of the unique games conjecture result that we find most exciting , and that shows promise of going beyond the current state where the individual algorithmic and hardness results form `` isolated islands of knowledge surrounded by a sea of ignorance '' into a more unified theory of complexity .",
    "the meta - algorithm that the ugc predicts to be optimal is based on _",
    "semidefinite programming _ and it uses this technique in a very particular and quite restricted way .",
    "( in many settings , this meta - algorithm can be implemented in near - linear time  @xcite . )",
    "we will refer to this algorithm as the _ ugc meta - algorithm_. it can be viewed as a common generalization of several well known algorithms , including those that underlie cheeger s inequality , grothendieck s inequality  @xcite , the goemans ",
    "williamson @xmath33  algorithm  @xcite , and the lovsz  @xmath34 function  @xcite .",
    "as we ve seen for the example of cheeger s inequality , in many of those settings this meta - algorithm gives _ non - trivial approximation guarantees _ which are the best known , but there are no hardness results ruling out the existence of better algorithms .",
    "the works on the ugc has shown that this conjecture ( and related ones ) imply that this meta - algorithm is _ optimal _ for a vast number of problems , including all those examples above .",
    "for example , a beautiful result of raghavendra  @xcite showed that for every constraint - satisfaction problem ( a large class of problems that includes many problems of interest such as max @xmath16-sat , @xmath16-coloring , and max - cut ) , the ugc meta - algorithm gives the best estimate on the maximum possible fraction of constraints one can satisfy .",
    "similarly , the ugc ( or closely related variants ) imply there are no efficient algorithms that give a better estimate for the sparsest cut of a graph than the one implied by cheeger",
    "s inequality  @xcite and no better efficient estimate for the maximum correlation of a matrix with @xmath35-valued vectors than the one given by grothendieck s inequality .- valued vectors even though we do nt actually know the numerical value of this factor ( known as grothendieck s constant ) . ] to summarize :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ if true , the unique games conjecture tells us not only _ which _ problems in a large class are easy and which are hard , but also _ why _ this is the case .",
    "there is a _ single unifying reason _ , captured by a concrete meta - algorithm , that explains all the easy problem in this class .",
    "moreover , in many cases where this meta - algorithm already gives non - trivial guarantees , the ugc implies that no further efficient improvements are possible . _",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    all this means that the unique games conjecture is certainly a very attractive proposition , but the big question still remains unanswered  is this conjecture actually true ? while some initial results supported the ugc , more recent works , although still falling short of disproving the conjecture , have called it into question . in this survey",
    "we discuss the most promising current approach to refute the ugc , which is based on the _ sum of squares ( sos ) method _",
    "the sos method could potentially refute the unique games conjecture by beating the guarantees of the ugc meta - algorithm on problems on which the conjecture implies the latter s optimality .",
    "this of course is interesting beyond the ugc , as it means we would be able to improve the known guarantees for many problems of interest . alas , analyzing the guarantees of the sos method",
    "is a very challenging problem , and we still have relatively few tools to do so .",
    "however , as we will see , we already know that at least in some contexts , the sos method can yield better results than what was known before .",
    "the sos method is itself a meta algorithm , so even if it turns out to refute the ugc , this does not mean we need to give up on the notion of explaining the complexity of wide swaths of problems via a single algorithm ; we may just need to consider a different algorithm .",
    "to summarize , regardless of whether it refutes the ugc or not , understanding the power of the sos method is an exciting research direction that could advance us further towards the goal of a unified understanding of computational complexity .      instead of the unique games conjecture , in this survey we focus on a related conjecture known as the _ small - set expansion hypothesis ( sseh )",
    "_  @xcite .",
    "the sseh implies the ugc  @xcite , and while there is no known implication in the other direction , there are several results suggesting that these two conjectures are probably equivalent  @xcite . at any rate ,",
    "most ( though not all ) of what we say in this survey applies equally well to both conjectures , but the sseh is , at least in our minds , a somewhat more natural and simpler - to - state conjecture .    recall that for a @xmath0-regular graph @xmath1 and a vertex set @xmath3 , we defined its expansion as @xmath36 . by cheeger s inequality ( [ eq : cheeger ] ) , the second largest eigenvalue yields a non - trivial approximation for the minimum expansion @xmath37 , but it turns out that eigenvalues and similar methods do not work well for the problem of approximating the minimum expansion of smaller sets .",
    "the small - set expansion hypothesis conjectures that this problem is inherently difficult .    for every @xmath38 there exists @xmath39 such that given any graph @xmath1 , it is @xmath40-hard to distinguish between the case * ( i ) * that there exists a subset @xmath3 with @xmath41 such that @xmath42 and the case * ( ii ) * that @xmath43 for every @xmath10 with @xmath44 .",
    "as mentioned above , the sseh implies that ( [ eq : cheeger ] ) yields an optimal approximation for @xmath6 .",
    "more formally , assuming the sseh , there is some absolute constant @xmath45 such that for every @xmath46 , it is @xmath40-hard to distinguish between the case that a given graph @xmath2 satisfies @xmath47 and the case that @xmath48  @xcite . given that the sseh conjectures the difficulty of approximating expansion , the reader might not be so impressed that it also implies the optimality of cheeger s inequality . however , we should note that the sseh merely conjectures that the problem becomes harder as @xmath49 becomes smaller , without postulating any quantitative relation between @xmath49 and @xmath50 , and so it is actually surprising ( and requires a highly non - trivial proof ) that it implies such quantitatively tight bounds .",
    "even more surprising is that ( through its connection with the ugc ) the sseh implies tight hardness result for a host of other problems , including every constraint satisfaction problem , grothendieck s problem , and many others , which a priori seem to have nothing to do with graph expansion .    while we will stick to the sseh in this survey , for completeness we present here the definition of the unique games conjecture .",
    "we will not use this definition in the proceeding and so the reader can feel free to skip this remark .",
    "the ugc can be thought of as a more structured variant of the sseh where we restrict to graphs and sets that satisfy some particular properties . because we restrict both the graphs and the sets , a priori it is not clear which of these conjectures should be stronger .",
    "however it turns out that the sseh implies the ugc  @xcite .",
    "it is an open problem whether the two conjectures are equivalent , though the authors personally suspect that this is the case .",
    "we say that an @xmath14-vertex graph @xmath1 is _ @xmath49-structured _ if there is a partition of @xmath51 into @xmath52 sets @xmath53 each of size @xmath54 , such that for every @xmath55 , either @xmath56 or @xmath57 is a _ matching _ ( namely for every @xmath58 there is exactly one @xmath59 such that @xmath27 ) .",
    "we say a set @xmath3 is _",
    "@xmath49-structured _ if @xmath60 for all @xmath61 ( and so in particular , @xmath62 ) .",
    "the unique games conjecture states that for every @xmath38 there exists a @xmath39 such that it is @xmath40 hard , given a @xmath49-structured @xmath2 , to distinguish between the case * ( i ) * that there exists a @xmath49-structured @xmath10 such that @xmath42 and the case * ( ii ) * that every @xmath49-structured @xmath10 satisfies @xmath43 .",
    "the conjecture can also be described in the form of so - called `` two prover one round games '' ( hence its name ) ; see khot s surveys  @xcite .      in the rest of this survey",
    "we describe the sum of squares algorithm , some of its applications , and its relation to the unique games and small - set expansion conjectures .",
    "we start by defining the sum of squares algorithm , and how it relates to classical questions such as hilbert @xmath63 problem .",
    "we will demonstrate how the sos algorithm is used , and its connection to the ugc / sseh , by presenting cheeger s inequality ( [ eq : cheeger ] ) as an instance of this algorithm .",
    "the sseh implies that the sos algorithm can not yield better estimates to @xmath6 than those obtained by ( [ eq : cheeger ] ) . while we do not know yet whether this is true or false , we present two different applications where the sos does beat prior works finding a planted sparse vector in a random subspace , and _",
    "sparse coding_ learning a set of vectors @xmath24 given samples of random sparse linear combinations of vectors in @xmath24 .",
    "we then discuss some of the evidence for the ugc / sseh , how this evidence is challenged by the sos algorithm and the relation between the ugc / sseh and the problem of ( approximately ) finding sparse vectors in arbitrary ( not necessarily random ) subspaces .",
    "much of our discussion is based on the papers  @xcite .",
    "see also  @xcite for informal overviews of some of these issues .    for the reader interested in learning more about the unique games conjecture , there are three excellent surveys on this topic .",
    "khot s ccc survey  @xcite gives a fairly comprehensive overview of the state of knowledge on the ugc circa 2010 , while his icm survey  @xcite focuses on some of the techniques and connections that arose in the works around the ugc .",
    "trevisan  @xcite gives a wonderfully accessible introduction to the ugc , using the max - cut problem as a running example to explain in detail the ugc s connection to semidefinite programming . as",
    "a sign of how rapidly research in this area is progressing , this survey is almost entirely disjoint from  @xcite . while the former surveys mostly described the implications of the ugc for obtaining very strong hardness and `` meta hardness '' results ,",
    "the current manuscript is focused on the question of whether the ugc is actually true , and more generally understanding the power of the sos algorithm to go beyond the basic lp and sdp relaxations .",
    "our description of the sos algorithm barely scratches the surface of this fascinating topic , which has a great many applications that have nothing to do with the ugc or even approximation algorithms at large .",
    "the volume  @xcite and the monograph  @xcite are good sources for some of these topics .",
    "the sos algorithm was developed in slightly different forms by several researchers , including shor  @xcite , nesterov  @xcite , parrilo  @xcite , and lasserre  @xcite .",
    "it can be viewed as a strengthening of other `` meta - algorithms '' proposed by  @xcite ( also known as linear and semi - definite programming hierarchies ) .",
    "our description of the sos meta algorithm follows parrilo s , while the description of the dual algorithm follows lasserre , although we use the pseudoexpectation notation introduced in  @xcite instead of lasserre s notion of `` moment matrices '' .",
    "the positivstellensatz / sos proof system was first studied by grigoriev and vorobjov  @xcite and grigoriev  @xcite proved some degree lower bounds for it , that were later rediscovered and expanded upon by  @xcite .",
    "all these are motivated by the works in real geometry related to hilbert s @xmath64 problem ; see reznick s survey  @xcite for more on this research area .",
    "one difference between our focus here and much of the other literature on the sos algorithm is that we are content with proving that the algorithm supplies an _ approximation _ to the true quantity , rather than exact convergence , but on the other hand are much more stringent about using only very low degree ( preferably constant or polylogarithmic in the number of variables ) .",
    "one of the most common ways of proving that a quantity is non - negative is by expressing it as a _ sum of squares _ ( sos ) .",
    "for example , we can prove the arithmetic - mean geometric - mean inequality @xmath65 by the identity @xmath66 .",
    "thus a natural question , raised in the late @xmath67 century , was whether _ any _ non - negative ( possibly multivariate ) polynomial can be written as a sum of squares of polynomials .",
    "this was answered negatively by hilbert in 1888 , who went on to ask as his @xmath64 problem whether any such polynomial can be written as a sum of squares of _ rational _ functions .",
    "a positive answer was given by artin  @xcite , and considerably strengthened by krivine and stengle .",
    "in particular , the following theorem is a corollary of their results , which captures much of the general case .",
    "[ thm : psatz ] let @xmath68=\\mathbb{r}[x_1,\\ldots , x_n]$ ] be multivariate polynomials .",
    "then , the system of polynomials equations @xmath69 has no solution over @xmath70 if and only if , there exists polynomials @xmath71 $ ] such that @xmath72 $ ] is a _ sum of squares _ of polynomials and @xmath73    we say that the polynomials @xmath74 in the conclusion of the theorem form an _ sos proof _ refuting the system of polynomial equations  @xmath75 .",
    "clearly the existence of such polynomials implies that @xmath75 is unsatisfiable  the interesting part of theorem  [ thm : psatz ] is the other direction .",
    "we say that a sos refutation @xmath76 has",
    "@xmath77 if the maximum degree of the polynomials @xmath78 involved in the proof is at most @xmath77  @xcite . by writing down the coefficients of these polynomials ,",
    "we see that a degree-@xmath77 sos proof can be written using @xmath79 numbers . into sums of squares will not require more than @xmath80 terms ; also in all the settings we consider , there are no issues of accuracy in representing real numbers , and so a degree @xmath77-proof can be written down using @xmath79 bits . ]    in the following lemma , we will prove a special case of theorem  [ thm : psatz ] , where the solution set of @xmath75 is a subset of the hypercube @xmath81 . here , the degree of sos refutations is bounded by @xmath82 .",
    "( this bound is not meaningful computationally because the size of degree-@xmath83 refutations is comparable to the number of points in @xmath81 . )",
    "[ lem : hypercube - pss ] let @xmath84 for some @xmath85 $ ] . then , either the system @xmath75 is satisfiable or it has a degree-@xmath82 sos refutation .",
    "suppose the system is not satisfiable , which means that @xmath86 for all @xmath87 .",
    "since @xmath81 is a finite set , we may assume @xmath88 over @xmath81 .",
    "now interpolate the real - valued function @xmath89 on @xmath90 as a multilinear ( and hence degree at most @xmath14 ) polynomial in @xmath91 $ ] . then",
    ", @xmath92 is a polynomial of degree at most @xmath82 that vanishes over @xmath81 .",
    "( since we can replace @xmath93 by @xmath94 in any monomial , we can assume without loss of generality that @xmath95 is multilinear and hence has degree at most @xmath14 . )",
    "this means that we can write @xmath92 in the form @xmath96 for polynomials @xmath97 with @xmath98 .",
    "( this fact can be verified either directly or by using that @xmath99 is a grbner basis for @xmath81 . ) putting things together , we see that @xmath100 , which is a sos refutation for @xmath75 of the form in theorem  [ thm : psatz ] .      the sum of squares algorithm is based on the following theorem , which was discovered in different forms by several researchers :    [ thm : sos ] if there is a degree-@xmath77 sos proof refuting @xmath101 , then such a proof can be found in @xmath79 time .",
    "we can view a degree-@xmath77 sos refutation @xmath102 for @xmath75 as a system of linear equations in @xmath79 variables corresponding to the coefficients of the unknown polynomials @xmath74 .",
    "we only need to incorporate the non - linear constraint that @xmath10 is a sum of squares .",
    "but it is not hard to see that a degree-@xmath77 polynomial @xmath10 is a sum of squares if and only if there exists a positive - semidefinite matrix @xmath103 such that @xmath104 , where @xmath105 and @xmath106 range over all monomials @xmath107 and @xmath108 of degree at most @xmath109 .",
    "thus , the task of finding a degree-@xmath77 sos refutation reduces to the task of solving linear systems of equations with the additional constraint that matrix formed by some of the variables is positive - semidefinite .",
    "_ semidefinite programming _ solves precisely this task and is computationally efficient .    in the applications we are interested in , the number of variables @xmath14 corresponds to our `` input size '' .",
    "the equation systems @xmath75 we consider can always be solved via a `` brute force '' algorithm running in @xmath110 time , and so degree-@xmath77 sos proofs become interesting when @xmath77 is much smaller than @xmath14 .",
    "ideally we would want @xmath111 , though @xmath112 or even , say , @xmath113 , is still interesting .",
    "theorem  [ thm : sos ] yields the following _ meta algorithm _ that can be applied on any problem of the form @xmath114 where @xmath115 $ ] are polynomials .",
    "the algorithm is parameterized by a number @xmath77 called its _ degree _ and operates as follows :    we call @xmath116 the _",
    "degree-@xmath77 sos estimate _ for ( [ eq : poly - opt ] ) , and by theorem  [ thm : sos ] it can be computed in @xmath117 time . for the actual minimum value @xmath118 of ( [ eq : poly - opt ] ) , the corresponding system of equations @xmath119 is satisfiable , and hence in particular can not be refuted by an sos proof .",
    "thus , @xmath120 for any @xmath77 .",
    "since higher degree proofs are more powerful ( in the sense that they can refute more equations ) , it holds that @xmath121 ( we can assume degrees of sos proofs to be even . ) as we ve seen in lemma  [ lem : hypercube - pss ] , for the typical domains we are interested in computer science , such as when the set of solutions of @xmath122 is equal to @xmath123 , this sequence is finite in the sense that @xmath124 .",
    "the sos algorithm uses semidefinite programming in a much more general way than many previous algorithms such as  @xcite .",
    "in fact , the ugc meta - algorithm is the same as the base case ( i.e. , @xmath125 ) of the sos algorithm .",
    "recall that the ugc and sseh imply that in many settings , one can not improve on the approximation guarantees of the ugc meta - algorithm without using @xmath126 time .",
    "thus in particular , if those conjectures are true then in those settings , using the sos meta algorithm with degree , say , @xmath127 ( or even @xmath112 or @xmath128 ) will not yield significantly better guarantees than @xmath125 .",
    "another approach to optimize over non - linear problems such as ( [ eq : poly - opt ] ) is to use local - search algorithms such as gradient descent that make local improvement steps , e.g. , in the direction of the gradient , until a local optimum is reached .",
    "one difference between such local search algorithms and the sos algorithm is that the latter sometimes succeeds in optimizing highly non - convex problems that have exponential number of local optima . as an illustration ,",
    "consider the polynomial @xmath129 .",
    "its unique global minimum is the point @xmath130 , but it is not hard to see that it has an exponential number of local minima ( for every @xmath131 , @xmath132 for every @xmath133 with @xmath134 $ ] , and so there must be a local minima in the ball of radius @xmath135 around @xmath136 ) .",
    "hence , gradient descent or other such algorithms are extremely likely to get stuck in one of these suboptimal local minima . however , since @xmath137 is in fact a sum of squares with constant term @xmath28 , the degree-@xmath138 sos algorithm will output @xmath137 s correct global minimum value .",
    "suppose we want to show that the level-@xmath77 sos meta - algorithm achieves a good approximation of the minimum value of @xmath95 over the set @xmath139 for a particular kind of polynomials @xmath140 $ ] . since the estimate @xmath116 always lower bounds this quantity , we are to show that @xmath141 for some particular function @xmath142 ( satisfying @xmath143 ) which captures our approximation guarantee .",
    "( e.g. , a factor @xmath144 approximation corresponds to the function @xmath145 . )    if we expand out the definition of @xmath116 , we see that to prove equation  ( [ eq : meta - works ] ) we need to show that for every @xmath118 if there does not exists a degree-@xmath77 proof that @xmath146 for all @xmath147 , then there exists an @xmath148 such that @xmath149 .",
    "so , to prove a result of this form , we need to find ways to use the _ non - existence _ of a proof . here , _",
    "duality _ is useful .    _",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ pseudodistributions are the dual object to sos refutations , and hence the _ non - existence _ of a refutation implies the _ existence _ of a pseudodistribution . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    we now elaborate on this , and explain both the definition and intuition behind pseudodistributions . in section  [ sec : sos - cheeger ] we will give a concrete example , by showing how one can prove that degree-@xmath150 sos proofs capture cheeger s inequality using such an argument .",
    "results such as the analysis of the goemans - williamson @xmath33  algorithm  @xcite , and the proof of grothendieck s inequality  @xcite can be derived using similar methods .",
    "let @xmath151_{\\ell}$ ] denote the set of polynomials in @xmath151 $ ] of degree at most @xmath77 .",
    "a _ degree-@xmath77 pseudoexpectation operator for @xmath151 $ ] _ is a linear operator @xmath152 that maps polynomials in @xmath151_{\\ell}$ ] into @xmath153 and satisfies that @xmath154 and @xmath155 for every polynomial @xmath137 of degree at most @xmath109 .",
    "the term pseudoexpectation stems from the fact that for every distribution @xmath156 over @xmath70 , we can obtain such an operator by choosing @xmath157 for all @xmath158 $ ] .",
    "moreover , the properties @xmath159 and @xmath160 turn out to capture to a surprising extent the properties of distributions and their expectations that we tend to use in proofs .",
    "therefore , we will use a notation and terminology for such pseudoexpectation operators that parallels the notation we use for distributions .",
    "in fact , all of our notation can be understood by making the thought experiment that there exists a distribution as above and expressing all quantities in terms of low - degree moments of that distribution ( so that they also make sense if we only have a pseudoexpectation operator that does nt necessarily correspond to a distribution ) .    in the following",
    ", we present the formal definition of our notation .",
    "we denote pseudoexpectation operators as @xmath161 , where @xmath156 acts as index to distinguish different operators .",
    "if @xmath161 is a degree-@xmath77 pseudoexpectation operator for @xmath151 $ ] , we say that @xmath156 is a _",
    "degree-@xmath77 pseudodistribution _ for the indeterminates @xmath136 . in order to emphasize or change indeterminates , we use the notation @xmath162 . in case",
    "we have only one pseudodistribution @xmath156 for indeterminates @xmath136 , we denote it by @xmath163 . in that case , we also often drop the subscript for the pseudoexpectation and write @xmath164 for @xmath165 .",
    "we say that a degree-@xmath77 pseudodistribution @xmath163 satisfies a system of polynomial equations @xmath122 if @xmath166 for all @xmath167 $ ] and all polynomials @xmath168 $ ] with @xmath169 .",
    "we also say that @xmath170 satisfies the constraint @xmath171 if there exists some sum - of - squares polynomial @xmath72 $ ] such that @xmath170 satisfies the polynomial equation @xmath172 .",
    "it is not hard to see that if @xmath170 was an actual distribution , then these definitions imply that all points in the support of the distribution satisfy the constraints .",
    "we write @xmath173 to denote that @xmath137 is a sum of squares of polynomials , and similarly we write @xmath174 to denote @xmath175 .    the duality between sos proofs and pseudoexpectations is expressed in the following theorem .",
    "we say that a system @xmath75 of polynomial equations is _ explicitly bounded _ if there exists a linear combination of the constraints in @xmath75 that has the form @xmath176 for @xmath177 and @xmath72 $ ] a sum - of - squares polynomial .",
    "( note that in this case , every solution @xmath178 of the system @xmath75 satisfies @xmath179 . )",
    "[ thm : duality ] let @xmath101 be a set of polynomial equations with @xmath180 $ ] .",
    "assume that @xmath75 is explicitly bounded in the sense above .",
    "then , exactly one of the following two statements holds : ( a ) there exists a degree-@xmath77 sos proof refuting  @xmath75 , or ( b ) there exists a degree-@xmath77 pseudodistribution @xmath163 that satisfies  @xmath75 .",
    "first , suppose there exists a degree-@xmath77 refutation of the system @xmath75 , i.e. , there exists polynomials @xmath181 $ ] and a sum - of - squares polynomial @xmath91 $ ] so that @xmath182 and @xmath183 .",
    "let @xmath163 be any pseudodistribution .",
    "we are to show that @xmath163 does not satisfy @xmath75 .",
    "indeed , @xmath184 , which means that @xmath185 for at least one @xmath167 $ ] . therefore , @xmath163 does not satisfy @xmath75 .",
    "next , suppose there does not exist a degree-@xmath77 refutation of the system @xmath75 .",
    "we are to show that there exists a pseudodistribution that satisfies @xmath75 .",
    "let @xmath32 be the cone of all polynomials of the form @xmath186 for sum - of - squares @xmath187 and polynomials @xmath97 with @xmath188 . since @xmath75 does not have a degree-@xmath77 refutation , the constant polynomial @xmath189 is not contained in @xmath32 .",
    "we claim that from our assumption that the system  @xmath75 is explicitly bounded it follows that @xmath189 also can not lie on the boundary of @xmath32 .",
    "assuming this claim , the hyperplane separation theorem implies that there exists a linear form @xmath190 such that @xmath191 but @xmath192 for all @xmath193 . by rescaling",
    ", we may assume that @xmath194 .",
    "now this linear form satisfies all conditions of a pseudoexpectation operator for the system  @xmath75 .",
    "_ proof of claim .",
    "_ we will show that if @xmath189 lies on the boundary of @xmath32 , then also @xmath195 . if @xmath189 is on the boundary of @xmath32",
    ", then there exists a polynomial  @xmath196_\\ell$ ] such that @xmath197 for all @xmath38 ( using the convexity of @xmath32 ) .",
    "since @xmath75 is explicitly bounded , for every polynomial  @xmath196_\\ell$ ] , the cone @xmath32 contains a polynomial of form @xmath198 for a sum - of - square @xmath187 and a number @xmath199 .",
    "( here , the polynomial  @xmath200 is a certificate that @xmath201 over the solution set of @xmath75 .",
    "such a certificate is easy to obtain when @xmath75 is explicitly bounded .",
    "we are omitting the details . ) at this point , we see that @xmath189 is a nonnegative combination of the polynomials  @xmath202 , @xmath198 , and @xmath187 for @xmath203 .",
    "since these polynomials are contained in @xmath32 , their nonnegative combination @xmath189 is also contained in the cone  @xmath32 .",
    "[ [ recipe - for - using - pseudoexpectations - algorithmically . ] ] recipe for using pseudoexpectations algorithmically .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in many applications we will use the following dual form of the sos algorithm :    theorem  [ thm : duality ] shows that in the cases we are interested in , both variants of the sos algorithm will output the same answer .",
    "regardless , a similar proof to that of theorem  [ thm : sos ] shows that the dual form of the sos algorithm can also be computed in time @xmath117 .",
    "thus , when using the sos meta - algorithm , instead of trying to argue from the non - existence of a proof , we will use the existence of a pseudodistribution . specifically , to show that the algorithm provides an @xmath204 approximation in the sense of ( [ eq : meta - works ] ) , what we need to show is that given a degree-@xmath77 pseudodistribution @xmath170 satisfying the system @xmath205 , we can find some particular @xmath206 that satisfies @xmath207 .",
    "our approach to doing so ( based on the authors paper with kelner  @xcite ) can be summarized as follows :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ solve the problem pretending that @xmath170 is an _ actual distribution _ over solutions , and if all the steps you used have low - degree sos proofs , the solution still works even when @xmath163 is a low - degree _ pseudodistribution_. _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    it may seem that coming up with an algorithm for the actual distribution case is trivial , as any element in the support of the distribution would be a good solution . however note that even in the case of a real distribution , the algorithm does not get sampling access to the distribution , but only access to its low - degree moments . depending on the reader s temperament , the above description of the algorithm , which `` pretends '' pseudodistributions are real ones , may sound tautological or just wrong . hopefully it will be clearer after the next two sections , where we use this approach to show how the sos algorithm can match the guarantee of cheeger s inequality for computing the expansion , to find planted sparse vectors in random subspaces , and to approximately recover sparsely used dictionaries .",
    "recall that the _ expansion _",
    ", @xmath6 , of a @xmath0-regular graph @xmath1 is the minimum of @xmath208 over all sets @xmath10 of size at most @xmath209 . letting @xmath210",
    "be the characteristic vector of the set @xmath10 the expression @xmath211 can be written as @xmath212 which is a quadratic polynomial in @xmath136 . therefore ,",
    "for every @xmath16 , computing the value @xmath213 can be phrased as the question of minimizing a polynomial @xmath95 over the set of @xmath136 s satisfying the equations @xmath214 and @xmath215 .",
    "let @xmath216 be the degree-@xmath77 sos estimate for @xmath217 .",
    "we call @xmath218 the degree-@xmath77 sos estimate for @xmath6 . note that @xmath219 can be computed in @xmath220 time . for the case @xmath125",
    ", the following theorem describes the approximation guarantee of the estimate @xmath219 .",
    "[ thm : sparsest - cut ] there exists an absolute constant @xmath144 such that for every graph @xmath2 @xmath221    before we prove theorem  [ thm : sparsest - cut ] , let us discuss its significance . theorem  [ thm : sparsest - cut ] is essentially a restatement of cheeger s inequality in the sos language  the degree @xmath150-sos algorithm is the ugc meta algorithm which is essentially the same as the algorithm based on the second - largest eigenvalue .",
    "such that there exists a degree-@xmath150 pseudodistribution satisfying the more relaxed system @xmath222 .",
    "] there are examples showing that  ( [ eq : sparsest - cut - sdp ] ) is tight , and so we can not get better approximation using degree @xmath150 proofs . but",
    "can we get a better estimate using degree @xmath138 proofs ? or degree @xmath223 proofs ?",
    "we do nt know the answer , but if the small - set expansion hypothesis is true , then beating the estimate ( [ eq : sparsest - cut - sdp ] ) is @xmath12 -hard , which means ( under standard assumptions ) that to do so we will need to use proofs of degree at least @xmath224 .",
    "this phenomenon repeats itself in other problems as well .",
    "for example , for both the grothendieck inequality and the @xmath33   problems , the sseh ( via the ugc ) predicts that beating the estimate obtained by degree-@xmath150 proofs will require degree @xmath225 . as in the case of expansion",
    ", we have not been able to confirm or refute these predictions .",
    "however , we will see some examples where using higher degree proofs _ does _ help , some of them suspiciously close in nature to the expansion problem .",
    "one such example comes from the beautiful work of arora , rao and vazirani  @xcite who showed that @xmath226 which is better than the guarantee of theorem  [ thm : sparsest - cut ] for @xmath227 .",
    "however , this is not known to contradict the sseh or ugc , which apply to the case when @xmath6 is a small constant .    as we will see in section  [ sec : ssevssos ] , for the small set expansion problem of approximating @xmath7 for small sets @xmath10 , we can beat the degree @xmath150 bounds with degree @xmath228 proofs where @xmath229 is a parameter tending to zero with the parameter @xmath50 of the sseh  @xcite .",
    "this yields a sub - exponential algorithm for the small - set expansion problem ( which can be extended to the unique games problem as well ) that `` barely misses '' refuting the sseh and ugc .",
    "we will also see that degree @xmath230 proofs have surprising power in other settings that are closely related to the sseh / ugc , but again at the moment still fall short of refuting those conjectures .",
    "this proof is largely a reformulation of the standard proof of a discrete variant of cheeger s inequality , phrased in the sos language of pseudodistributions , and hence is included here mainly to help clarify these notions , and to introduce a tool sampling from a distribution matching first two moments of a pseudodistribution that will be useful for us later on . by the dual formulation , to prove theorem  [ thm : sparsest - cut ] we need to show that given a pseudodistribution @xmath170 over characteristic vectors of size-@xmath16 sets @xmath10 of size @xmath231 with @xmath232 , we can find a particular set @xmath233 of size at most @xmath234 such that @xmath235 . for simplicity",
    ", we consider the case @xmath236 ( the other cases can be proven in a very similar way ) .",
    "the distribution @xmath170 satisfies the constraints @xmath237 , @xmath238 for all @xmath61 , and @xmath239 .",
    "the algorithm to find @xmath233 is quite simple :    1 .   choose @xmath240 from a random gaussian distribution with the same quadratic moments as @xmath163 so that @xmath241 and @xmath242 for all @xmath243 $ ] .",
    "( see details below . ) 2 .",
    "output the set @xmath244 ( which corresponds to the 0/1 vector closest to @xmath133 ) .",
    "we remark that the set produces by the algorithm might have cardinality larger than @xmath234 , in which case we will take the complement of @xmath233 .",
    "[ [ sampling - from - a - distribution - matching - two - moments . ] ] sampling from a distribution matching two moments .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we will first give a constructive proof the well - known fact that for every distribution over @xmath70 , there exists an @xmath14-dimensional gaussian distribution with the same quadratic moments .",
    "given the moments of a distribution @xmath170 over @xmath70 , we can sample a gaussian distribution @xmath245 matching the first two moments of @xmath246 as follows .",
    "first , we can assume @xmath247 for all @xmath61 by shifting variables if necessary .",
    "next , let @xmath248 and @xmath249 be the eigenvectors and eigenvalues of the matrix @xmath250 .",
    "( note that @xmath103 is positive semidefinite and so @xmath251 . )",
    "choose i.i.d random standard gaussian variables @xmath252 and define @xmath253 .",
    "since @xmath254 equals @xmath94 if @xmath255 and equals @xmath28 otherwise , @xmath256 one can verify that if @xmath170 is a degree-@xmath150 pseudodistribution then the second moment matrix @xmath103 of the shifted version of @xmath136 ( such that @xmath257 for all @xmath61 ) is positive - semidefinite , and hence the above can be carried for pseudodistributions of degree at least @xmath150 as well . concretely , if we let @xmath258 be the mean of the pseudodistribution , then @xmath259 .",
    "this matrix is positive semidefinite because every test vector @xmath260 satisfies @xmath261 .",
    "[ [ analyzing - the - algorithm . ] ] analyzing the algorithm .",
    "+ + + + + + + + + + + + + + + + + + + + + + + +    the analysis is based on the following two claims : ( i ) the set @xmath233 satisfies @xmath262 with constant probability and ( ii ) in expectation @xmath263 .",
    "we will focus on two extreme cases that capture the heart of the arguments for the claims . in the first case , all variables @xmath264 have very small",
    "variance so that @xmath265 . in this case , because our constraints imply that @xmath266 , every variable satisfies either @xmath267 or @xmath268 , which means that the distribution of the set @xmath233 produced by the algorithm is concentrated around a particular set , and it is easy to verify that this set satisfies the two claims . in the second ,",
    "more interesting case , all variables @xmath264 have large variance , which means @xmath269 in our setting .    in this case , each event @xmath270 has probability @xmath271 and therefore @xmath272 . using that the quadratic moments of @xmath273 satisfy @xmath274 and @xmath275",
    ", one can show that these events can not be completely correlated , which allows us to control the probability of the event @xmath276 and establishes ( i ) .",
    "for the second claim , it turns out that by convexity considerations it suffices to analyze the case that all edges contribute equally to the term @xmath277 so that @xmath278 for all @xmath279 .",
    "so we see that @xmath280 is a 2-dimensional gaussian distribution with mean @xmath281 and covariance @xmath282 thus , in order to bound the expected value of @xmath283 we need to bound the probability of the event `` @xmath284 and @xmath285 '' for this particular gaussian distribution , which amounts to a not - too - difficult calculation that indeed yields an upper bound of @xmath286 on this probability .",
    "in this section , we illustrate the computational power of the sum - of - squares method with applications to two basic problems in unsupervised learning . in these problems",
    ", we are given samples of an unknown distribution from a fixed , parametrized family of distributions and the goal is to recover the unknown parameters from these samples .",
    "despite the average - case nature of these problems , most of the analysis in these applications will be for deterministic problems about polynomials that are interesting in their own right .",
    "the first problem is sparse vector recovery . here",
    ", we are given a random basis of a @xmath0-dimensional linear subspace @xmath287 of the form @xmath288 where @xmath289 is a sparse vector and @xmath290 are independent standard gaussian vectors .",
    "the goal is to reconstruct the vector @xmath289 .",
    "this is a natural problem in its own right , and is also a useful subroutine in various settings ; see  @xcite .",
    "demanet and hand  @xcite gave an algorithm ( based on  @xcite ) that recovers @xmath289 by searching for the vector @xmath136 in @xmath291 that maximizes @xmath292 ( which can be done efficiently by @xmath14 linear programs ) .",
    "it is not hard to show that @xmath289 has to have less than @xmath293 coordinates for it to be maximize this ratio , below for a related statement . ] and hence this was a limitation of prior techniques .",
    "in contrast , as long as @xmath0 is not too large ( namely , @xmath294 ) , the sos method can recover @xmath289 as long as it has less than @xmath295 coordinates for some constant @xmath296  @xcite .",
    "the second problem we consider is sparse dictionary learning , also known as sparse coding . here , we are given independent samples @xmath297 from an unknown distribution of the form @xmath298 , where @xmath299 is a matrix and @xmath136 is a random @xmath300-dimensional vector from a distribution over sparse vectors .",
    "this problem , initiated by the work olshausen and field  @xcite in computational neuroscience , has found a variety of uses in machine learning , computer vision , and image processing ( see , e.g.  @xcite and the references therein ) .",
    "the appeal of this problem is that intuitively data should be sparse in the `` right '' representation ( where every coordinate corresponds to a meaningful feature ) , and finding this representation can be a useful first step for further processing , just as representing sound or image data in the fourier or wavelet bases is often a very useful primitive .",
    "while there are many heuristics use to solve this problem , prior works giving rigorous recovery guarantees such as  @xcite all required the vector @xmath136 to be _ very _ sparse , namely less than @xmath301 nonzero entries .",
    "consists of @xmath300 independent random variables then better guarantees can be achieved using _ independent component analysis ( ica ) _  @xcite .",
    "see  @xcite for the current state of art in this setting .",
    "however we are interested here in the more general case . ] in contrast , the sos method can be used to approximately recover the dictionary matrix @xmath24 as long as @xmath136 has @xmath302 nonzero ( or more generally , significant ) entries  @xcite .",
    "we say a vector @xmath136 is @xmath303-sparse if the 0/1 indicator @xmath304 of the support of @xmath136 has norm - squared @xmath305 .",
    "the ratio @xmath306 is the fraction of non - zero coordinates in @xmath136 .",
    "[ thm : sparse - recovery ] there exists a polynomial - time approximation algorithm for sparse vector recovery with the following guarantees : suppose the input of the algorithm is an arbitrary basis of a @xmath307-dimensional linear subspace @xmath287 of the form @xmath308 such that @xmath289 is a @xmath303-sparse unit vector with @xmath309 and @xmath290 are standard gaussian vectors orthogonal to @xmath289 with @xmath310 .",
    "then , with probability close to @xmath94 , the algorithm outputs a unit vector  @xmath136 that has correlation @xmath311 with @xmath289 .",
    "our algorithm will follow the general recipe we described in section  [ sec : pseudo - dist ] :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ find a system of polynomial equations @xmath75 that captures the intended solution @xmath289 , then pretend you are given a distribution @xmath312 over solutions of @xmath75 and show how you could recover a single solution @xmath313 from the low order moments of @xmath312 .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    specifically , we come up with a system @xmath75 so that desired vector @xmath289 satisfies all equations , and it is essentially the only solution to @xmath75 . then , using the sos algorithm , we compute a degree-@xmath138 pseudodistribution @xmath312 that satisfies @xmath75 . finally , as in section  [ sec : sparsest - cut : proof ] , we sample a vector @xmath313 from a gaussian distribution that has the same quadratic moments as the pseudodistribution @xmath312 .",
    "[ [ how - to - encode - this - problem - as - a - system - of - polynomial - equations ] ] how to encode this problem as a system of polynomial equations ?",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    by cauchy ",
    "schwarz , any @xmath303-sparse vector @xmath136 satisfies @xmath314 for all @xmath315 with @xmath316 . in particular , for @xmath317 , such vectors satisfy @xmath318 .",
    "this fact motivates our encoding of sparse vector recovery as a system of polynomial equations .",
    "if the input specifies subspace @xmath287 , then we compute the projector @xmath137 into the subspace @xmath291 and choose the following polynomial equations : @xmath319 and @xmath320 , where @xmath321 .",
    "( we assume here the algorithm is given @xmath322 as input , as we can always guess a sufficiently close approximation to it . )",
    "[ [ why - does - the - sum - of - squares - method - work ] ] why does the sum - of - squares method work ?",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the analysis of algorithm has two ingredients .",
    "the first ingredient is a structural property about projectors of random subspaces .",
    "[ lem : random - norm - bound ] let @xmath323 be a random @xmath0-dimensional subspace with @xmath310 and let @xmath324 be the projector into @xmath325 . then , with high probability , the following sum - of - squares relation over @xmath326 $ ] holds for @xmath327 , @xmath328    we can write @xmath329 where @xmath330 is a @xmath331 matrix whose rows are an orthogonal basis for the subspace @xmath325 . therefore , @xmath332 where @xmath333 , and so to prove lemma  [ lem : random - norm - bound ] it suffices to show that under these conditions , @xmath334 .",
    "the matrix @xmath335 will be very close to having random independent gaussian entries , and hence , up to scaling , @xmath336 will be ( up to scaling ) , close to @xmath337 where @xmath338 are chosen independently at random from the standard gaussian distribution .",
    "the expectation of @xmath339 is equal @xmath340 .",
    "therefore , to prove the lemma , we need to show that for @xmath341 , the polynomial @xmath342 is with high probability close to its expectation , in the sense that the @xmath343 matrix corresponding to @xmath344 s coefficients is close to its expectation in the spectral norm .",
    "this follows from standard matrix concentration inequalities , see  ( * ? ? ?",
    "* theorem  7.1 ) ) .",
    "the following lemma is the second ingredient of the analysis of the algorithm .",
    "[ lem : sparse - vector - recovery ] let @xmath323 be a linear subspace and let @xmath324 be the projector into @xmath325 .",
    "let @xmath345 be a @xmath303-sparse unit vector orthogonal to @xmath325 and let @xmath346 and @xmath137 the projector on @xmath291 .",
    "let @xmath347 be a degree-@xmath138 pseudodistribution that satisfies the constraints @xmath348 and @xmath349 , where @xmath350 .",
    "suppose @xmath351 is a sum - of - squares relation in @xmath326 $ ] .",
    "then , @xmath347 satisfies @xmath352    note that the conclusion of lemma  [ lem : sparse - vector - recovery ] implies that a vector @xmath313 sampled from a gaussian distribution with the same quadratic moments as the computed pseudodistribution also satisfies @xmath353 and @xmath354 . by markov inequality , @xmath355 holds with probability at least @xmath356 .",
    "since @xmath357 is gaussian , it satisfies @xmath358 with probability at least @xmath271 . if both events occur , which happens with probability at least @xmath359 , then @xmath360 , thus establishing theorem  [ thm : sparse - recovery ] .    [ [ proof - of - lemmalemsparse - vector - recovery ] ] proof of lemma  [ lem : sparse - vector - recovery ] + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    there are many ways in which pseudodistributions behave like actual distributions , as far as low degree polynomials are concerned . to prove lemma  [ lem : sparse - vector - recovery ]",
    ", we need to establish the following two such results :    suppose @xmath361 and @xmath362 are nonnegative integers that sum to a power of @xmath150 .",
    "then , every degree-@xmath363 pseudodistribution @xmath364 satisfies @xmath365    the proof of the general case follows from the case @xmath366 by an inductive argument .",
    "the proof for the case @xmath367 follows from the fact that the polynomial @xmath368 $ ] is a sum of squares for all @xmath369 and choosing @xmath370 and @xmath371    [ lem : pseudo - hoelder ] let @xmath364 be a degree-@xmath138 pseudodistribution .",
    "then , @xmath372    the inequality is invariant with respect to the measure used for the inner norm @xmath373 . for simplicity ,",
    "suppose @xmath374 . then , @xmath375 . let @xmath376 and @xmath377 . then , lemma  [ lem : pseudo - hoelder ] allows us to bound the pseudoexpectations of the terms @xmath378 , so that as desired @xmath379    we can now prove lemma  [ thm : sparse - recovery ] .",
    "let @xmath380 $ ] . by construction ,",
    "the polynomial identity @xmath381 holds over @xmath326 $ ] . by the triangle inequality for pseudodistribution @xmath382 norm , for @xmath383 and @xmath384 @xmath385 by the premises of the lemma , @xmath386 and @xmath387 . together with the previous bound",
    ", it follows that @xmath388 .",
    "since @xmath389 and @xmath347 satisfies @xmath319 , we have @xmath390 . finally , using @xmath391 , we derive the desired bound @xmath392 thus establishing lemma  [ lem : pseudo - hoelder ] and theorem  [ thm : sparse - recovery ] .      a _ @xmath393-overcomplete dictionary",
    "_ is a matrix @xmath299 with @xmath394 and isotropic unit vectors as columns ( so that @xmath395 ) .",
    "we say a distribution @xmath163 over @xmath396 is @xmath397-nice if it satisfies @xmath398 and @xmath399 for all @xmath400 $ ] , and it satisfies that non - square monomial degree-@xmath0 moments vanish so that @xmath401 for all non - square degree-@xmath0 monomials @xmath107 , where @xmath402 for @xmath403 . for @xmath404 and @xmath405 , a nice distribution satisfies",
    "that @xmath406 which means that it is approximately sparse in the sense that the square of the entries of @xmath136 has large variance ( which means that few of the entries have very big magnitude compared to the rest ) .",
    "for every @xmath38 and @xmath407 , there exists @xmath0 and @xmath229 and a quasipolynomial - time algorithm algorithm for sparse dictionary learning with the following guarantees : suppose the input consists of @xmath408 independent samples moments of @xmath136 are bounded by @xmath408 . ] from a distribution @xmath409 over @xmath70 , where @xmath410 is a @xmath393-overcomplete dictionary and the distribution @xmath163 over @xmath396 is @xmath397-nice .",
    "then , with high probability , the algorithm outputs a set of vectors with hausdorff distance of @xmath24 by @xmath411 might not affect the input distribution ) , we measure the hausdorff distance after symmetrizing the sets , i.e. , replacing the set @xmath10 by @xmath412 . ] at most @xmath50 from the set of columns of @xmath24 .    [ [ encoding - as - a - system - of - polynomial - equations . ] ] encoding as a system of polynomial equations .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    let @xmath413 be independent samples from the distribution @xmath409 .",
    "then , we consider the polynomial @xmath414_d$ ] . using the properties of nice distributions , a direct computation shows that with high probability @xmath137 satisfies the relation @xmath415 ( here , we are omitting some constant factors , depending on @xmath0 , that are not important for the following discussion . )",
    "it follows that @xmath416 for every column @xmath417 of @xmath24 .",
    "it s also not hard to show that every unit vector @xmath418 with @xmath419 is close to one of the columns of @xmath24 .",
    "( indeed , every unit vector satisfies @xmath420 .",
    "therefore , @xmath419 implies that @xmath421 , which is close to @xmath94 for @xmath422 . )",
    "what we will show is that pseudodistributions of degree @xmath423 allow us to find all such vectors .",
    "[ [ why - does - the - sum - of - squares - method - work-1 ] ] why does the sum - of - squares method work ? + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in the following , @xmath38 and @xmath407 are arbitrary constants that determine constants @xmath424 and @xmath425 ( as in the theorem ) .",
    "[ lem : isolate ] let @xmath426 $ ] be a degree-@xmath0 polynomial with @xmath427 for some @xmath393-overcomplete dictionary @xmath24 .",
    "let @xmath156 be a degree-@xmath423 pseudodistribution that satisfies the constraints @xmath348 and @xmath428 .",
    "let @xmath429 $ ] be a product of @xmath423 random linear forms$ ] where @xmath430 is a random unit vector in @xmath70 . ] .",
    "then , with probability at least @xmath431 over the choice of @xmath432 , there exists a column @xmath417 of @xmath24 such that @xmath433    if @xmath161 is a pseudoexpectation operator , then @xmath434 is also a pseudoexpectation operator ( as it satisfies linearity , normalization , and nonnegativity ) .",
    "( this transformation corresponds to reweighing the pseudodistribution @xmath156 by the polynomial @xmath435 . ) hence , the conclusion of the lemma gives us a new pseudodistribution @xmath436 such that @xmath437 .",
    "therefore , if we sample a gaussian vector @xmath418 with the same quadratic moments as @xmath436 , it satisfies @xmath438 with probability at least @xmath356 . at the same time , it satisfies @xmath439 with probability at least @xmath271 . taking these bounds together",
    ", @xmath418 satisfies @xmath440 with probability at least @xmath359 .",
    "lemma  [ lem : isolate ] allows us to reconstruct one of the columns of @xmath24 . using similar ideas , we can iterate this argument and recover one - by - one all columns of @xmath24 .",
    "we omit the proof of lemma  [ lem : isolate ] , but the idea behind it is to first give an sos proof version of our argument above that maximizers of @xmath137 must be close to one of the @xmath441 s .",
    "we then note that if a distribution @xmath156 is supported ( up to noise ) on at most @xmath300 different vectors , then we can essentially isolate one of these vectors by re - weighing @xmath156 using the product of the squares of @xmath442 random linear forms .",
    "it turns out , this latter argument has a low degree sos proof as well , which means that in our case that given @xmath156 satisfying the constraint @xmath428 , we can isolate one of the @xmath417 s even when @xmath156 is not an actual distribution but merely a pseudodistribution .",
    "so far we have discussed the small - set expansion hypothesis and the sum of squares algorithm .",
    "we now discuss how these two notions are related .",
    "one connection , mentioned before , is that the sseh predicts that in many settings the guarantees of the degree-@xmath150 sos algorithm are best possible , and so in particular it means that going from degree @xmath150 to say degree @xmath443 should not give any substantial improvement in terms of guarantees .",
    "another , perhaps more meaningful connection is that there is a candidate approach for refuting the sseh using the sos algorithm . at the heart of this approach is the following observation :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ the small - set expansion problem is a special case of the problem of finding `` sparse '' vectors in a linear subspace . _",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    this may seem strange , as a priori , the following two problem seem completely unrelated : * ( i ) * given a graph @xmath1 , find a `` small '' subset @xmath3 with low expansion @xmath7 , and * ( ii ) * given a subspace @xmath444 , find a `` sparse '' vector in @xmath432 .",
    "the former is a combinatorial problem on graphs , and the latter a geometric problem on subspaces .",
    "however , for the right notions of `` small '' and `` sparse '' , these turn out to be essentially the same problem .",
    "intuitively , the reason is the following : the expansion of a set @xmath10 is proportional to the quantity @xmath445 where @xmath136 is the characteristic vector of @xmath10 ( i.e. @xmath446 equals @xmath94 if @xmath447 and equals @xmath28 otherwise ) , and @xmath190 is the _ laplacian matrix _ of @xmath2 ( defined as @xmath448 where @xmath449 is the identity , @xmath0 is the degree , and @xmath24 is @xmath2 s adjacency matrix ) .",
    "let @xmath450 be the eigenvectors of @xmath190 and @xmath451 the corresponding eigenvalues .",
    "then @xmath452    therefore if @xmath445 is smaller than @xmath453 and @xmath144 is a large enough constant , then most of the mass of @xmath136 is contained in the subspace @xmath454 . since @xmath10 is small , @xmath136 is sparse , and so we see that there is a sparse vector that is `` almost '' contained in @xmath432 .",
    "moreover , by projecting @xmath136 into @xmath432 we can also find a `` sparse '' vector that is actually contained in @xmath432 , if we allow a slightly softer notion of `` sparseness '' , that instead of stipulating that most coordinates are zero , only requires that the distribution of coordinates is very `` spiky '' in the sense that most of its mass is dominated by the few `` heavy hitters '' .    concretely , for @xmath455 and @xmath456 , we say that a vector @xmath178 is _",
    "@xmath457-sparse _ if @xmath458 .",
    "note that a characteristic vector of a set of measure @xmath49 is @xmath457-sparse for any @xmath459 .",
    "the relation between small - set - expansion and finding sparse vectors in a subspace is captured by the following theorem :      1 .   _",
    "( non - expanding small sets imply sparse vectors . ) _ if there exists @xmath3 with @xmath462 and @xmath463 then there exists an @xmath464-sparse vector @xmath465 where for every @xmath466 , @xmath467 denotes the span of the eigenvectors of @xmath190 with eigenvalue smaller than @xmath466 .",
    "( sparse vectors imply non - expanding small sets . )",
    "_ if there exists a @xmath464-sparse vector @xmath468 , then there exists @xmath3 with @xmath469 and @xmath470 for some constant @xmath471 depending on @xmath118 .",
    "the first direction of theorem  [ thm : ssevsnorms ] follows from the above reasoning , and was known before the work of  @xcite .",
    "the second direction is harder , and we omit the proof here .",
    "the theorem reduces the question of determining whether there for small sets @xmath10 , the minimum of @xmath7 is close to one or close to zero , into the question of bounding the maximum of @xmath472 over all unit vectors in some subspace .",
    "the latter question is a polynomial optimization problem of the type the sos algorithm is designed for !",
    "thus , we see that we could potentially resolve the sseh if we could answer the following question :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ what is the degree of sos proofs needed to certify that the @xmath473-norm is bounded for all ( euclidean norm ) unit vectors in some subspace @xmath432 ? _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    we still do nt know the answer to this question in full generality , but we do have some interesting special cases .",
    "lemma  [ lem : random - norm - bound ] of section  [ sec : sparse - vector - recovery ] implies that if @xmath432 is a random subspace of dimension @xmath474 then we can certify that @xmath475 for all @xmath476 via a degree-@xmath138 sos proof . this is optimal , as the @xmath138-norm simply wo nt be bounded for dimensions larger than @xmath301 :        let @xmath137 be the matrix corresponding to the projection operator to the subspace @xmath432 . note that @xmath137 has @xmath0 eigenvalues equalling @xmath94 and the rest equal @xmath28 , and hence @xmath480 and the frobenius norm squared of @xmath137 , defined as @xmath481 , also equals @xmath0 . let @xmath482 where @xmath483 is the @xmath484 standard basis vector .",
    "then @xmath485 is the trace of @xmath137 which equals @xmath0 and hence using cauchy - schwarz @xmath486 on the other hand , @xmath487 therefore , by the inequality @xmath488 , there exists an @xmath61 such that if we let @xmath489 then @xmath490 hence , just the contribution of the @xmath484 coordinate to the expectation achieves @xmath491      [ cor : dimbound ] let @xmath492 , and @xmath432 be subspace of @xmath70 .",
    "if @xmath493 , then there is an @xmath494-degree sos proof for this fact .",
    "( the constants in the @xmath495 notation can depend on @xmath459 but not on @xmath14 . )    by lemma  [ lem : dimbound ] , the condition implies that @xmath496 , and it is known that approximately bounding a degree-@xmath230 polynomial on the @xmath0-dimensional sphere requires an sos proof of at most @xmath497 degree ( e.g. , see  @xcite and the references therein ) .    combining corollary  [ cor : dimbound ] with theorem  [ thm : ssevsnorms ] implies that for every @xmath498 there exists some @xmath229 ( tending to zero with @xmath50 ) , such that if we want to distinguish between the case that an @xmath14-vertex graph @xmath2 satisfies @xmath42 for every @xmath499 , and the case that there exists some @xmath10 of size at most @xmath52 with @xmath43 , then we can do so using a degree @xmath500 sos proofs , and hence in @xmath501 time .",
    "this is much better than the trivial @xmath502 time algorithm that enumerates all possible sets .",
    "similar ideas can be used to achieve an algorithm with a similar running time for the problem underlying the unique games conjecture  @xcite .",
    "if these algorithms could be improved so the exponent @xmath229 tends to zero with @xmath14 for a fixed @xmath50 , this would essentially refute the sseh and ugc .",
    "thus , the question is whether corollary  [ cor : dimbound ] is the best we could do .",
    "as we ve seen , lemma  [ lem : random - norm - bound ] shows that for random subspaces we can do much better , namely certify the bound with a constant degree proof .",
    "two other results are known of that flavor .",
    "barak , kelner and steurer  @xcite showed that if a @xmath0-dimensional subspace @xmath432 does not contain a @xmath503-sparse vector , then there is an @xmath230-degree sos proof that it does not contain ( or even almost contains ) a vector with @xmath504 nonzero coordinates . if the dependence on @xmath0 could be eliminated ( even at a significant cost to the degree )",
    ", then this would also refute the sseh .",
    "barak , brando , harrow , kelner , steurer and zhou  @xcite gave an @xmath230-degree sos proof for the so - called  bonami - beckner - gross @xmath505 hypercontractivity theorem ",
    "( see  ( * ? ? ? *",
    "chap .  9 ) ) .",
    "this is the statement that for every constant @xmath16 , the subspace @xmath506 containing the evaluations of all degree @xmath507 polynomials on the points @xmath508 does not contain an @xmath509-sparse vector , and specifically satisfies for all @xmath510 , @xmath511 on its own this might not seem so impressive , as this is just one particular subspace",
    ". however , this particular subspace underlies much of the evidence that has been offered so far in support of both the ugc and sseh conjectures .",
    "the main evidence for the ugc / sseh consists of several papers such as  @xcite that verified the predictions of these conjectures by proving that various natural algorithms indeed fail to solve some of the computational problems that are hard if the conjectures are true .",
    "these results all have the form of coming up with a `` hard instance '' @xmath2 on which some algorithm @xmath31 fails , and so to prove such a result one needs to do two things : * ( i ) * compute ( or bound ) the true value of the parameter on @xmath2 , and * ( ii ) * show that the value that @xmath31 outputs on @xmath2 is ( sufficiently ) different than this true value .",
    "it turns out that all of these papers , the proof of * ( i ) * can be formulated as low degree sos proof , and in fact the heart of these proofs is the bound ( [ eq : hyper - contract ] ) .",
    "therefore , the results of  @xcite showed that all these `` hard instances '' can in fact be solved by the sos algorithm using a constant degree .",
    "this means that at the moment , we do nt even have any example of an instance for the problems underlying the sseh and ugc that can be reasonably _ conjectured _",
    "( let alone proved ) hard for the constant degree sos algorithm .",
    "this does not mean that such instances do not exist , but is suggestive that we have not yet seen the last algorithmic word on this question .",
    "b.  barak .",
    "structure vs. combinatorics in computational complexity .",
    ", ( 112):115126 , february 2014 . b.  barak , f.  g. s.  l. brando , a.  w. harrow , j.  a. kelner , d.  steurer , and y.  zhou .",
    "hypercontractivity , sum - of - squares proofs , and their applications . in _",
    "stoc _ , pages 307326 , 2012 .",
    "m.  x. goemans and d.  p. williamson .",
    "improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming .",
    ", 42(6):11151145 , 1995 . n.  goyal , s.  vempala , and y.  xiao .",
    "fourier pca . in _ stoc _ , 2014 . also available as arxiv report 1306.5825 .",
    "m.  laurent .",
    "sums of squares , moment matrices and optimization over polynomials . in _ emerging applications of algebraic geometry _ , pages 157270 .",
    "springer , 2009 .",
    "l.  lovsz . on the shannon capacity of a graph .",
    ", 25(1):17 , 1979 ."
  ],
  "abstract_text": [
    "<S> in order to obtain the best - known guarantees , algorithms are traditionally tailored to the particular problem we want to solve . </S>",
    "<S> two recent developments , the _ unique games conjecture ( ugc ) _ and the _ sum - of - squares ( sos ) method _ , surprisingly suggest that this tailoring is not necessary and that a single efficient algorithm could achieve best possible guarantees for a wide range of different problems .    </S>",
    "<S> the _ unique games conjecture ( ugc ) _ is a tantalizing conjecture in computational complexity , which , if true , will shed light on the complexity of a great many problems . </S>",
    "<S> in particular this conjecture predicts that a _ single concrete algorithm _ provides optimal guarantees among all efficient algorithms for a large class of computational problems .    </S>",
    "<S> the _ sum - of - squares ( sos ) method _ is a general approach for solving systems of polynomial constraints . </S>",
    "<S> this approach is studied in several scientific disciplines , including real algebraic geometry , proof complexity , control theory , and mathematical programming , and has found applications in fields as diverse as quantum information theory , formal verification , game theory and many others .    </S>",
    "<S> we survey some connections that were recently uncovered between the unique games conjecture and the sum - of - squares method . </S>",
    "<S> in particular , we discuss new tools to rigorously bound the running time of the sos method for obtaining approximate solutions to hard optimization problems , and how these tools give the potential for the sum - of - squares method to provide new guarantees for many problems of interest , and possibly to even refute the ugc .    </S>",
    "<S> primary 68q25 ; secondary 90c22 .    </S>",
    "<S> sum of squares , semidefinite programming , unique games conjecture , small - set expansion </S>"
  ]
}