{
  "article_text": [
    "high dimensional and functional data analysis are interesting domains which do not have stopped growing for many years . to consider these kinds of data , it is more and more important to think about methods which take into account the high dimension as well as the possibility of having large samples . in this paper",
    ", we focus on an usual stochastic optimization problem which consists in estimating @xmath0 , \\ ] ] where @xmath1 is a random variable taking values in a space @xmath2 and @xmath3 , where @xmath4 is a separable hilbert space . in order to build an estimator of @xmath5 ,",
    "an usual method was to consider the solver of the problem generated by the sample , i.e to consider @xmath6-estimates ( see @xcite and @xcite among others ) . in order to build these estimates ,",
    "deterministic convex optimization algorithms ( see @xcite ) are often used ( see @xcite , @xcite in the case of the median ) , and these methods are really efficient in small dimensional spaces .    nevertheless , in a context of high dimensional spaces ,",
    "this kind of method can encounter many computational problems .",
    "the main ones are that it needs to store all the data , which can be expensive in term of memory and that they can not deal online with the data . in order to overcome this , stochastic gradient algorithms ( @xcite ) are efficient candidates since they do not need to store the data into memory , and they can be easily updated , which is crucial if the data arrive sequentially ( see @xcite , @xcite , @xcite or @xcite among others ) . in order to improve the convergence , @xcite and @xcite introduced its averaged version ( see also @xcite for the weighted version ) .",
    "these algorithms have become crucial to statistics and modern machine learning ( @xcite , @xcite , @xcite ) .",
    "there are already many results on these algorithms in the literature , that we can split into two parts : asymptotic results , such as almost sure rates of convergence ( see for instance @xcite , @xcite , @xcite ) and non asymptotic ones , such as quadratic error bounds ( see @xcite , @xcite,@xcite ) .",
    "nevertheless , the framework proposed to get non asymptotic results in @xcite forces the function we would like to minimize on being uniformly strongly convex . in order to obtain asymptotic results , @xcite and @xcite",
    "give a general framework for which no strongly convexity assumption is needed , but the results are only proven in finite dimensional spaces .    in a recent work , @xcite introduce a new framework , with only locally strongly convexity assumptions ( and non global ones ) , in general hilbert spaces , which allows to obtain almost sure rates and @xmath7 rates of convergence . in keeping with it , and in order to have a deeper study of the stochastic gradient algorithm as well as of its averaged version ( up to a new assumption )",
    ", we first give the asymptotic normality of the estimates . in a second time , since a central limit theorem is often unusable without an estimation of the variance , we introduce a recursive algorithm , inspired by @xcite , which enables us to estimate the asymptotic variance of the averaged estimator , and we establish the rates of convergence of these estimates.as far as we know , there was no efficient and recursive estimate of the asymptotic variance in the literature . finally , we will give two applications in robust statistics : the geometric median ( see @xcite or @xcite ) and the geometric quantiles ( see @xcite and @xcite ) , which are useful robust indicators in statistics . indeed , they are often used in data depth and outliers detection ( @xcite , @xcite ) , as well as for robust estimation of the mean and variance ( see @xcite ) , or for robust principal component analysis ( @xcite , @xcite , @xcite ) .",
    "the paper is organized as follows : section [ sectionassumption ] recalls the framework introduced by @xcite before giving two new assumptions which allow to get the rate of convergence of the estimators of the asymptotic variance . in section [ sectiontlc ] , the stochastic gradient algorithm as well as its averaged version",
    "are introduced and their asymptotic normality are given .",
    "the recursive estimator of the asymptotic variance is given in section [ sectionvariance ] and its almost sure as well as its quadratic mean rates of convergence are established .",
    "an application on the recursive estimations of the geometric median and of the geometric quantiles as well as a short simulation study are given in section [ sectionapplication ] .",
    "finally , the proofs are postponed in section [ sectionproof ] and in an appendix .",
    "let @xmath4 be a separable hilbert space such as @xmath8 or @xmath9 ( for some closed interval @xmath10 ) , we denote by @xmath11 its inner product and by @xmath12 the associated norm .",
    "let @xmath1 be a random variable taking values in a space @xmath2 , and let @xmath13 be the function we would like to minimize , defined for all @xmath14 by @xmath15,\\ ] ] where @xmath16 .",
    "moreover , let us suppose that the functional @xmath17 is convex .",
    "finally , let us introduce the space of linear operators on @xmath4 , denoted by @xmath18 , equipped with the frobenius ( or hilbert - schmidt ) inner product , which is defined by @xmath19 where @xmath20 is an orthonormal basis of @xmath4 .",
    "we denote by @xmath21 the associated norm , and @xmath18 is then a separable hilbert space .",
    "let us recall the framework introduced by @xcite :    * the functional @xmath22 is frechet - differentiable for the second variable almost everywhere .",
    "moreover , @xmath17 is differentiable and denoting by @xmath23 its gradient , there exists @xmath24 such that @xmath25 * the functional @xmath17 is twice continuously differentiable almost everywhere and for all positive constant @xmath26 , there is a positive constant @xmath27 such that for all @xmath28 , @xmath29 where @xmath30 is the hessian of the functional @xmath17 at @xmath31 and @xmath32 is the usual spectral norm for linear operators .",
    "* there exists a positive constant @xmath33 such that for all @xmath34 , there is an orthonormal basis of @xmath4 composed of eigenvectors of @xmath30 .",
    "moreover , let us denote by @xmath35 the limit inf of the eigenvalues of @xmath36 , then @xmath35 is positive . finally , for all @xmath34 , and for all eigenvalue @xmath37 of @xmath30",
    ", we have @xmath38 .",
    "* there are positive constants @xmath39 such that for all @xmath34 , @xmath40 * * * there is a positive constant @xmath41 such that for all @xmath14 , @xmath42 \\leq l_{1 } \\left ( 1 + \\left\\| h - m \\right\\|^{2 } \\right ) .\\ ] ] * * there is a positive constant @xmath43 such that for all @xmath14 , @xmath44 \\leq l_{2 } \\left ( 1 + \\left\\| h - m \\right\\|^{4 } \\right ) .\\ ] ] * * for all integer @xmath45 , there is a positive constant @xmath46 such that for all @xmath14 , @xmath47 \\leq l_{q } \\left ( 1 + \\left\\| h - m \\right\\|^{2q } \\right ) .\\ ] ]    let us now introduce two new assumptions .",
    "* let @xmath48 be the functional defined for all @xmath14 by @xmath49 .\\ ] ] * * the functional @xmath50 is continuous at @xmath5 with respect to the frobenius norm : @xmath51 - \\mathbb{e}\\left [ \\nabla_{h } g \\left ( x , h \\right ) \\otimes \\nabla_{h}g \\left ( x , h \\right ) \\right ] \\right\\|_{f } = 0 .\\ ] ] * * the functional @xmath50 is locally lipschitz on a neighborhood of @xmath5 : there are positive constants @xmath39 , such that for all @xmath52 , @xmath53 \\right\\|_{f } \\leq c_{\\epsilon } \\left\\| h - m \\right\\| .\\ ] ]    note that assumptions * ( a1 ) * to * ( a5b ) * are discussed in @xcite .",
    "assumption * ( a6b ) * can be verified by giving a bound , on a neighborhood of @xmath5 , of the derivative of the functional @xmath50 .",
    "this last assumption allows to give the rate of convergence of the estimators of the asymptotic variance .",
    "let @xmath54 , the linear operator @xmath55 is defined for all @xmath56 by @xmath57 .",
    "moreover , note that @xmath58",
    "in what follows , let @xmath59 be independent random variables with the same law as @xmath1 .",
    "the stochastic gradient algorithm is defined recursively for all @xmath60 by @xmath61 with @xmath62 bounded and @xmath63 is a step sequence of the form @xmath64 , with @xmath65 and @xmath66 .",
    "moreover , let @xmath67 be the sequence of @xmath68-algebras defined for all @xmath60 by @xmath69 .",
    "then , the algorithm can be considered as a noisy ( or stochastic ) gradient algorithm since it can be written as @xmath70 where @xmath71 , defined for all @xmath60 by @xmath72 , is a martingale differences sequence adapted to the filtration @xmath73 .",
    "finally , note that under assumptions * ( a1 ) * to * ( a5a ) * , it was proven in @xcite that for all positive constant @xmath74 , @xmath75 moreover , assuming also that * ( a5b ) * is fulfilled , for all positive integer @xmath76 , there is a constant @xmath77 such that for all @xmath60 , @xmath78 \\leq \\frac{c_{p}}{n^{p\\alpha}}.\\ ] ] in order to get a deeper study of this estimate , we now give its asymptotic normality .",
    "[ tlcgrad ] suppose assumptions * ( a1 ) * to * ( a5a ) * and * ( a6a ) * hold .",
    "then , we have the convergence in law @xmath79 with @xmath80 .\\end{aligned}\\ ] ]    note that an analogous result is given by @xcite in the particular case of finite dimensional spaces .",
    "nevertheless , the proof of this result can not be directly applied in the infinite case .",
    "for example , it remains on the fact that the hessian admits finite dimensional eigenspaces , or on the fact that the trace of a matrix is well defined .",
    "[ remlyap ] note that taking a step sequence of the form @xmath81 with @xmath82 is possible , and one can obtain the following asymptotic normality ( see @xcite for the case of finite dimensional spaces ) @xmath83 nevertheless , knowing the conditions on @xmath84 needs to have some information on the hessian @xmath36 , which can be as difficult as to estimate @xmath5 .",
    "moreover , @xmath85 is not the optimal covariance ( see @xcite and @xcite for instance ) .      as mentioned in remark",
    "[ remlyap ] , having the parametric rate of convergence ( @xmath86 ) with the robbins - monro algorithm is possible taking a good choice of step sequence @xmath63 .",
    "nevertheless , this choice is often complicated and the asymptotic variance which is obtained is not optimal .",
    "then , in order to improve the convergence , let us now introduce the averaged algorithm ( see @xcite and @xcite ) defined for all @xmath60 by @xmath87 this can be written recursively for all @xmath60 as @xmath88 it was proven in @xcite that under assumptions * ( a1 ) * to * ( a5a ) * , for all @xmath89 , @xmath90 suppose assumption * ( a5b ) * is also fulfilled , for all positive integer @xmath76 , there is a positive constant @xmath91 such that for all @xmath60 , @xmath92 \\leq \\frac{c_{p}'}{n^{p}}.\\ ] ] finally , in order to have a deeper study of this estimate , we now give its asymptotic normality .",
    "[ theotlc ] suppose assumptions * ( a1 ) * to * ( a5a ) * and * ( a6a ) * are verified .",
    "then , we have the convergence in law @xmath93 with @xmath94 , and @xmath95 $ ] .    note that as for the case of the robbins - monro algorithm , this result exists in the particular case of finite dimensional spaces in @xcite , but the proof can not be directly applied in general hilbert spaces .",
    "a first naive method to estimate the asymptotic variance could be to estimate the hessian @xmath36 and the variance @xmath96 as follows @xmath97 but the main problem is that under assumptions * ( a2 ) * , * ( a3 ) * and * ( a5 ) * , if @xmath4 is an infinite dimensional spaces , then @xmath98 an other problem is that , in order to get a recursive estimator of the asymptotic variance , it needs to invert a matrix at each iteration , which costs much calculus time in high dimensional spaces .",
    "a second estimator of the asymptotic variance was introduced in @xcite .",
    "it is defined for all @xmath60 by @xmath99 nevertheless , suppose assumptions * ( a1 ) * to * ( a6b ) * hold , then @xmath100 = o \\left ( \\frac{1}{\\ln n}\\right ) .\\ ] ] thus , this estimator faces two main problems : it is not recursive and it converges very slowly .",
    "finally , in order to solve the second problem , a faster algorithm was introduced by @xcite , defined for all @xmath60 by @xmath101 with @xmath102 and @xmath103 . in the case of finite dimensional spaces , the following convergence in probability",
    "is given ( under some assumptions ) @xmath104{\\mathbb{p } } 0 , \\ ] ] with @xmath105 .",
    "a first technical problem is that only the convergence in probability is given , in the case of finite dimensional spaces , and for the usual spectral norm .",
    "a second one is that it is not recursive and it can not be easily updated .",
    "we now give a recursive version of the algorithm defined by ( [ algopel ] ) to estimate the asymptotic variance in separable hilbert spaces , before establishing its rates of convergence ( almost sure and in quadratic mean ) .",
    "this algorithm is defined by @xmath106 with @xmath107 this can be written recursively for all @xmath60 as @xmath108 with @xmath109 . then",
    ", contrary to previous algorithms , this one does not need to store all the estimations into memory and can be easily updated , which is crucial when the data arrive sequentially .",
    "finally , the following theorem ensures that it is quite fast .",
    "[ vitesselpps ] suppose assumptions * ( a1 ) * to * ( a5a ) * and * ( a6b ) * hold .",
    "then , the sequence @xmath110 defined by ( [ defisigman ] ) verifies for all positive constant @xmath111 , @xmath112 moreover , suppose * ( a5b ) * holds too , there is a positive constant @xmath113 such that for all @xmath60 , @xmath114 \\leq \\frac{c}{n^{1-s}}\\ ] ]    the proof is given in section [ sectionproof ] .",
    "suppose assumptions * ( a1 ) * to * ( a5a ) * and * ( a6b ) * hold .",
    "then , for all positive constant @xmath111 , @xmath115 moreover , suppose * ( a5b ) * holds too , there is a positive constant @xmath113 such that for all @xmath60 , @xmath116 \\leq \\frac{c}{n^{1-s}}\\ ] ]    operator @xmath117 is self - adjoint and non negative , so that it admits a spectral decomposition @xmath118 , where @xmath119 is the sequence of ordered eigenvalues associated to the orthonormal eigenvectors @xmath120 , @xmath121 . using the karhunen - love s expansion of @xmath117",
    ", we directly get that @xmath122 where @xmath123 are _ i.i.d .",
    "_ centered gaussian variables with unit variance .",
    "thus , the distribution of @xmath124 is a mixture of independent chi - square random variables with one degree of freedom .",
    "then it is interesting to estimate the main eigenvalues to build precise confidence balls .",
    "let @xmath125 be the @xmath45 main eigenvalues , it is then possible to estimate recursively @xmath126 as well as the associated normalized eigenvectors @xmath127 as follows : @xmath128 combined with an orthogonalization by deflation of @xmath129 .",
    "this recursive algorithm is based on ideas developed by @xcite that are related to the power method for extracting eigenvectors .",
    "the estimated eigenvectors @xmath129 tend ( up to the sign change ) to @xmath130 .",
    "let @xmath4 be a separable hilbert space and let @xmath1 be a random variable taking values in @xmath4 .",
    "let @xmath131 such that @xmath132 , the geometric quantile @xmath133 corresponding to the direction @xmath134 ( see @xcite ) is defined by @xmath135 - \\left\\langle h , v \\right\\rangle , \\ ] ] and in a particular case , the geometric median @xmath5 ( see @xcite ) corresponds to the case where @xmath136 .",
    "we consider from now that the following usual assumptions are fulfilled ( @xcite ) :    * the random variable @xmath1 is not concentrated on a straight line : for all @xmath14 , there is @xmath137 such that @xmath138 and @xmath139 * the random variable @xmath1 is not concentrated around single points : for all positive constant @xmath26 , there is a positive constant @xmath27 such that for all @xmath140 , @xmath141 \\leq c_{a } , & \\mathbb{e}\\left [ \\frac{1}{\\left\\| x - h \\right\\|^{2 } } \\right ] \\leq c_{a } .\\end{aligned}\\ ] ]    let us now denote by @xmath142 and @xmath143 the functions defined for all @xmath144 by @xmath145 - \\left\\langle h , v \\right\\rangle , & g_{v}(x , h ) : =   \\left\\| x - h \\right\\| - \\left\\| x \\right\\|   - \\left\\langle h , v \\right\\rangle .\\end{aligned}\\ ] ] under * ( h1 ) * and * ( h2 ) * , the functional @xmath146 is twice frchet - differentiable and it was proven ( see @xcite , @xcite and @xcite ) that assumptions * ( a1 ) * to * ( a5b ) * are verified . finally , the following lemma ensures that assumptions * ( a6a ) * and * ( a6b ) * are also fulfilled .",
    "[ lemquantile ] suppose assumption * ( h2 ) * holds .",
    "then , there are positive constants @xmath147 such that for all @xmath148 , @xmath149 \\right\\|_{f } \\leq c_{\\epsilon } \\left\\| h - m^{v } \\right\\| .\\ ] ]    the proof is given in an appendix .",
    "then , it is possible to estimate simultaneously and recursively the geometric quantile @xmath133 as well as the asymptotic variance of the averaged estimator as follows : @xmath150      we consider from now that @xmath1 is a random variable taking values in @xmath8 , with @xmath151 , and following a uniform law on the unit sphere @xmath152 .",
    "then , the geometric median @xmath5 is equal to @xmath153 , and assumptions * ( h1 ) * and * ( h2 ) * are verified ( see lemma a.1 in @xcite ) . moreover , the hessian of the functional @xmath154 at @xmath5 verifies @xmath155 = i_{d } -   \\mathbb{e } \\left [ x \\otimes x \\right ] = \\frac{d-1}{d}i_{d}.\\ ] ] finally , the asymptotic variance of the averaged estimates verified @xmath156 \\gamma_{m}^{-1 }   = \\frac{d}{(d-1)^2 } i_{d}.\\ ] ]    in what follows , we take a dimension @xmath157 and a stepsequence @xmath63 verfying for all @xmath60 , @xmath158 .",
    "first , let us study the quality of the gaussian approximation of @xmath159 , where @xmath160 .",
    "figure [ fig0 ] seems to confirm theorem [ theotlc ] since we can see that the estimated density of each coordiniate is closed to the density of @xmath161 , which is also confirmed by a kolmogorov - smirnov test .",
    "( in blue ) compared to the standard gaussian density ( in red ) . ]    in figures [ fig1 ] and [ fig2 ] , we consider the evolution of the quadratic mean error , for the frobenius norm as well as for the spectral norm , of the estimates @xmath162 of @xmath117 defined by ( [ defisigman ] ) , with regard to the sample size .",
    "it confirms the fact that the estimates of the asymptotic variance converge quickly , and so , even for moderate dimensions and sample sizes .",
    "the results are obtained with the help of @xmath163 generated samples .     with respect to the frobenius norm . ]     with respect to the spectral norm . ]",
    "in order to simplify the proofs , let us now give some decompositions of the algorithms .",
    "let us recall that the stochastic gradient algorithm can be written as @xmath164 linearizing the gradient , it comes @xmath165 where @xmath166 is the remainder term in the taylor s expansion of the gradient .",
    "thanks to previous decomposition and with the help of an induction ( see @xcite or @xcite for instance ) , one can check that for all @xmath60 , @xmath167 with @xmath168 for all @xmath60 and @xmath169 . finally , the asymptotic variance can be seen as the almost sure limit of the sequence of random variables @xmath170 ( see the proof of theorem [ theotlc ] ) .",
    "then , in order to prove the convergence of the estimates , we need to exhibit this sequence . in this aim , one can rewrite equation ( [ decdelta ] ) as @xmath171 with @xmath172      summing equalities ( [ decdeltabis ] ) and dividing by @xmath173 , we obtain the following decomposition of the averaged estimator @xmath174 finally , by linearity and applying an abel s transform to the first term on the right - hand side of previous equality ( see @xcite or @xcite for instance ) , @xmath175      in order to simplify the proof of theorem [ vitesselpps ] , we will introduce a new estimator of the variance . in this aim ,",
    "let us now introduce the sequences @xmath176 and @xmath177 defined for all @xmath60 by @xmath178 and @xmath179 .",
    "then , thanks to decomposition ( [ decdeltabis ] ) , let @xmath180 in order to simplify several proofs , we now give @xmath7 upper bounds of the terms on the right - hand side of previous equality .",
    "[ lemtech ] suppose assumptions * ( a1 ) * to * ( a5b ) * hold .",
    "then , for all positive integer @xmath76 , @xmath181 =   o \\left ( \\exp \\left ( \\frac{pn^{1-s}}{1-s } \\right ) n^{p\\alpha } \\right ) , \\\\ & \\mathbb{e}\\left [ \\left\\| \\sum_{k=1}^{n } a_{k } \\delta_{k } \\right\\|^{2p } \\right ] = o \\left ( \\exp \\left ( \\frac{pn^{1-s}}{1-s } \\right ) n^{p(s-\\alpha } \\right ) , \\\\ & \\mathbb{e}\\left [ \\left\\| \\sum_{k=1}^{n } a_{k}\\xi_{k+1 } \\right\\|^{2p}\\right ] = o \\left ( \\exp \\left ( \\frac{pn^{1-s}}{1-s } \\right ) n^{p s } \\right)\\end{aligned}\\ ] ]    the proof of this lemma as well as an analogous lemma which gives the asymptotic almost sure behavior of these terms are given in appendix .",
    "we can now introduce the following estimator @xmath182 and one can decompose @xmath183 as follows : @xmath184      let us recall that the robbins - monro algorithm can be written for all @xmath60 as ( see ( [ decbeta ] ) ) @xmath185 it was proven in @xcite that under assumptions * ( a1 ) * to * ( a5a ) * , for all @xmath186 , @xmath187 then , we just have to apply theorem 5.1 in @xcite to the last term on the right - hand side of equality ( [ decbeta ] ) .",
    "more precisely , let @xmath188 be an orthonormal basis of @xmath4 composed of eigenvectors of @xmath36 and let @xmath189 for all @xmath190 , we have to prove that the following equalities are verified .",
    "@xmath191 @xmath192 @xmath193    * proof of ( [ in1rm ] ) .",
    "* let @xmath194 , applying markov s inequality , @xmath195 .\\end{aligned}\\ ] ] first , since each eigenvalue @xmath196 of @xmath36 verifies @xmath197 , there is a rank @xmath198 such that for all positive integer @xmath199 verifying @xmath200 , @xmath201 for the sake of simplicity , we consider from now that @xmath202 ( one can see the proof of lemma 3.1 in @xcite for an analogous and more detailed proof ) .",
    "then , applying lemmas [ lemmajxi ] and [ sumexppart ] , there is a positive constant @xmath113 such that for all @xmath60 , @xmath203 &   \\leq \\sum_{k=1}^{n}\\left\\| \\beta_{n}\\beta_{k}^{-1 } \\right\\|_{op}^{4}\\gamma_{k}^{4}\\mathbb{e}\\left [ \\left\\| \\xi_{k+1 } \\right\\|^{4 } \\right ]   \\\\ & \\leq c \\sum_{k=1}^{n } \\exp \\left ( - 4 \\lambda_{\\min}\\sum_{j = k+1}^{n}\\gamma_{j } \\right ) \\gamma_{k}^{4 } \\\\ & = o \\left ( \\gamma_{n}^{3}\\right ) , \\end{aligned}\\ ] ] which concludes the proof of ( [ in1rm ] ) .    * proof of ( [ in2rm ] ) . * since @xmath204 we just have to prove that @xmath205 first , note that by linearity @xmath206 \\left(\\beta_{n}\\beta_{k}^{-1}\\gamma_{k } \\right ) \\\\ & + \\sum_{k=1}^{n}\\left ( \\beta_{n}\\beta_{k}^{-1}\\gamma_{k}\\right)\\epsilon_{k+1}\\left(\\beta_{n}\\beta_{k}^{-1}\\gamma_{k } \\right ) , \\end{aligned}\\ ] ] with @xmath207 $ ] . note that @xmath208 is a sequence of martingale differences adapted to the filtration @xmath209 .",
    "we now prove that the two last terms on the right - hand side of previous equality converge almost surely to @xmath153 .",
    "first , as in @xcite and @xcite , one can check that @xmath210 let us now rewrite @xmath211 $ ] as @xmath212 = \\sigma ' + \\left ( \\mathbb{e}\\left [ \\nabla_{h}g \\left ( x_{k+1 } , m_{k } \\right ) \\otimes \\nabla_{h}g \\left ( x_{k+1 } , m_{k } \\right ) |\\mathcal{f}_{k } \\right ] - \\sigma ' \\right ) - \\phi \\left ( m_{k}\\right ) \\otimes \\phi \\left ( m_{k } \\right ) .\\end{aligned}\\ ] ] then , let @xmath213 moreover , since there is a positive constant @xmath113 such that for all @xmath60 , @xmath214 , @xmath215 thus , applying inequalities ( [ vitesseasrm ] ) and ( [ majbeta ] ) as well as lemma [ sumexppart ] , for all @xmath216 , @xmath217 in the same way , @xmath218 - \\sigma ' \\right ) \\left ( \\beta_{n } \\beta_{k}^{-1}\\gamma_{k } \\right ) \\right\\|_{f } \\\\ & \\leq \\frac{1}{\\gamma_{n } }   \\sum_{k=1}^{n } \\left\\|   \\beta_{n } \\beta_{k}^{-1}\\gamma_{k } \\right\\|_{op}^{2}\\left\\| \\mathbb{e}\\left [ \\nabla_{h}g \\left ( x_{k+1 } , m_{k } \\right ) \\otimes \\nabla_{h}g \\left ( x_{k+1 } , m_{k } \\right ) |\\mathcal{f}_{k } \\right ] - \\sigma ' \\right\\|_{f } \\\\ & \\leq \\frac{1}{\\gamma_{n } }   \\sum_{k=1}^{n } \\gamma_{k}^{2 } \\exp \\left ( - 2 \\lambda_{\\min } \\sum_{j = k+1}^{n}\\gamma_{j } \\right ) \\left\\| \\mathbb{e}\\left [ \\nabla_{h}g \\left ( x_{k+1 } , m_{k } \\right ) \\otimes \\nabla_{h}g \\left ( x_{k+1 } , m_{k } \\right ) |\\mathcal{f}_{k } \\right ] - \\sigma ' \\right\\|_{f } .\\end{aligned}\\ ] ] then , with the help of assumption * ( a6a ) * , lemma [ sumexppart ] and toeplitz s lemma , one can check that @xmath219 - \\sigma ' \\right ) \\left ( \\beta_{n } \\beta_{k}^{-1}\\gamma_{k } \\right ) \\right\\|_{f } = 0 \\quad a.s.\\ ] ] in order to verify equality ( [ convbeta ] ) , we have to prove @xmath220 let @xmath20 be an orthonormal basis of @xmath4 composed of eigenvectors of @xmath36 , and let @xmath221 be the set of the associated eigenvalues . then , let us rewrite @xmath222 as @xmath223 and it comes , by linearity and by dominated convergence , @xmath224 \\\\ & = \\mathbb{e}\\left[\\frac{1}{\\gamma_{n}}\\sum_{k=1}^{n}\\gamma_{k}^{2 } \\left (   \\sum_{i \\in i}\\left\\langle \\nabla_{h}g \\left ( x , m \\right ) , e_{i } \\right\\rangle \\prod_{j = k+1}^{n } \\left ( 1-\\gamma_{k}\\lambda_{i } \\right)e_{i } \\right ) \\otimes \\left ( \\sum_{i \\in i}\\left\\langle \\nabla_{h}g \\left ( x , m \\right ) , e_{i } \\right\\rangle \\prod_{j = k+1}^{n } \\left ( 1-\\gamma_{k}\\lambda_{i } \\right)e_{i } \\right ) \\right ] .\\end{aligned}\\ ] ] in the same way , @xmath225 ds \\\\ & = \\mathbb{e}\\left[\\int_{0}^{\\infty } \\left(\\sum_{i \\in i}\\left\\langle \\nabla_{h}g \\left ( x , m \\right ) , e_{i } \\right\\rangle e^{-\\lambda_{i}s } e_{i } \\right ) \\otimes \\left(\\sum_{i \\in i}\\left\\langle \\nabla_{h}g \\left ( x , m \\right ) , e_{i } \\right\\rangle e^{-\\lambda_{i}s } e_{i } \\right )   ds \\right ] .\\end{aligned}\\ ] ] in order to conclude the proof , let us now introduce the following lemma , which allows to give a bound of @xmath226 .",
    "[ lempelunif ] there is a positive sequence @xmath227 such that for all @xmath60 and for all @xmath228 , @xmath229 and @xmath230 .",
    "the proof is given in appendix .    thanks to previous lemma , let @xmath231   \\\\ & \\leq a_{n } \\mathbb{e}\\left [ \\sqrt{\\sum_{i , i '",
    "\\in i } \\left\\langle \\nabla_{h}g \\left ( x , m \\right ) , e_{i } \\right\\rangle^{2}\\left\\langle \\nabla_{h}g \\left ( x , m \\right ) , e_{i ' } \\right\\rangle^{2 } } \\right ] \\\\ & = a_{n } \\mathbb{e}\\left [ \\sum_{i \\in i } \\left\\langle \\nabla_{h}g \\left ( x , m \\right ) , e_{i } \\right\\rangle^{2}\\right ] .\\end{aligned}\\ ] ] under assumption * ( a5a ) * , @xmath232 \\\\ & =   a_{n}\\mathbb{e}\\left [ \\left\\| \\nabla_{h } g \\left ( x , m \\right ) \\right\\|^{2 } \\right ] \\\\ & \\leq l_{1}a_{n}.\\end{aligned}\\ ] ] since @xmath233 converges to @xmath153 , this concludes the proof of inequality ( [ in2rm ] ) .",
    "* proof of inequality ( [ in3rm ] ) * let @xmath234 , applying markov s inequality , @xmath235 \\\\ & = \\frac{1}{\\gamma_{n}\\epsilon^{2 } } \\sum_{k=1}^{n } \\sum_{j = n}^{\\infty } \\mathbb{e}\\left [ \\mathbb{e}\\left [ \\left\\langle \\beta_{n}\\beta_{k}^{-1}\\xi_{k+1}\\otimes \\beta_{n}\\beta_{k}^{-1}\\xi_{k+1 } |\\mathcal{f}_{k } \\right ] ( e_{j } ) , e_{j } \\right\\rangle^{2 } \\right ]   \\\\ & = \\frac{1}{\\epsilon^{2 } }   \\sum_{j = n}^{\\infty}\\frac{1}{\\gamma_{n}}\\sum_{k=1}^{n } \\mathbb{e}\\left [ \\mathbb{e}\\left [ \\left\\langle \\beta_{n}\\beta_{k}^{-1}\\xi_{k+1}\\otimes \\beta_{n}\\beta_{k}^{-1}\\xi_{k+1 } |\\mathcal{f}_{k } \\right ] ( e_{j } ) , e_{j } \\right\\rangle^{2 } \\right]\\end{aligned}\\ ] ] since @xmath236 converges almost surely to @xmath237 with respect to the frobenius norm and by dominated convergence , @xmath238 moreover , since @xmath239 and since @xmath240 for all @xmath241 , @xmath242 which concludes the proof .",
    "let us recall that the averaged algorithm can be written as @xmath243 it is proven in @xcite that @xmath244 in order get the asymptotic normality of the martingale term @xmath245 , let us check that assumptions of theorem 5.1 in @xcite are fulfilled , i.e let @xmath20 be an orthonormal basis of @xmath4 and @xmath246 for all @xmath190 , we have to verify @xmath247 @xmath248 @xmath249    * proof of ( [ cond1 ] ) * let @xmath194 , applying markov s inequality , @xmath250 .\\end{aligned}\\ ] ] then , applying lemma [ lemmajxi ] , there is a positive constant @xmath113 such that @xmath251    * proof of ( [ cond2 ] ) .",
    "* first , note that @xmath252 + \\frac{1}{n}\\sum_{k=1}^{n } \\epsilon_{k+1},\\ ] ] with @xmath253 $ ] .",
    "remark that @xmath254 is a sequence of martingale differences adapted to the filtration @xmath73 , and one can check that @xmath255 let us now prove that the sequence of operators @xmath256 \\right)$ ] converges almost surely to @xmath96 , with respect to the frobenius norm .",
    "note that @xmath257 - \\sigma ' \\right\\| & = \\left\\| \\mathbb{e}\\left [ \\nabla_{h } g \\left ( x_{k+1 } , m_{k } \\right ) \\otimes \\nabla_{h } g \\left ( x_{k+1 } , m_{k } \\right)|\\mathcal{f}_{k } \\right ] - \\sigma ' - \\phi \\left ( m_{k } \\right ) \\otimes \\phi \\left(m_{k } \\right)\\right\\|_{f } \\\\ & \\leq \\left\\| \\mathbb{e}\\left [ \\nabla_{h } g \\left ( x_{k+1 } , m_{k } \\right ) \\otimes \\nabla_{h } g \\left ( x_{k+1 } , m_{k } \\right)|\\mathcal{f}_{k } \\right ] - \\sigma ' \\right\\|_{f } + \\left\\|   \\phi \\left ( m_{k } \\right ) \\otimes \\phi \\left(m_{k } \\right)\\right\\|_{f}.\\end{aligned}\\ ] ] then , thanks to assumption * ( a6a ) * , since @xmath258 and since @xmath259 converges to @xmath5 almost surely ( see @xcite ) , @xmath260 - \\sigma ' \\right\\|_{f } = 0 \\quad a.s , \\\\ & \\lim_{n \\to \\infty } \\left\\|   \\phi \\left ( m_{k } \\right ) \\otimes \\phi \\left(m_{k } \\right)\\right\\|_{f } = \\lim_{n \\to \\infty } \\left\\|   \\phi ( m_{k } ) \\right\\|^{2 } = 0 \\quad a.s.\\end{aligned}\\ ] ] in a particular case , for all @xmath190 , @xmath261 ( e_{i } ) , e_{j } \\right\\rangle = \\psi_{i , j } : = \\left\\langle \\sigma ' ( e_{i } ) , e_{j } \\right\\rangle \\quad a.s .\\ ] ] thus , applying toeplitz s lemma , @xmath262 ( e_{i } ) , e_{j } \\right\\rangle = \\psi_{i , j } \\quad a.s.\\ ] ] finally , for all @xmath263 , @xmath264    * proof of ( [ cond3 ] ) .",
    "* let @xmath234 , applying markov s inequality , @xmath265 \\\\ & = \\frac{1}{n\\epsilon^{2 } } \\sum_{k=1}^{n } \\sum_{j = n}^{\\infty } \\mathbb{e}\\left [ \\mathbb{e}\\left [ \\left\\langle \\xi_{k+1 } , e_{j } \\right\\rangle^{2}|\\mathcal{f}_{k } \\right ] \\right ] .\\end{aligned}\\ ] ] since for all @xmath241 , @xmath266 , and by linearity @xmath267 \\right ] \\\\ & = \\frac{1}{\\epsilon^{2 } }   \\sum_{j = n}^{\\infty } \\frac{1}{n}\\sum_{k=1}^{n } \\mathbb{e}\\left [    \\left\\langle \\mathbb{e}\\left [ \\xi_{k+1 } \\otimes \\xi_{k+1 } |\\mathcal{f}_{k } \\right ] ( e_{j } ) , e_{j } \\right\\rangle   \\right ] .\\end{aligned}\\ ] ] since @xmath268 $ ] converges almost surely to @xmath96 and by dominated convergence , @xmath269 moreover , since @xmath270 $ ] , thanks to assumption * ( a5a ) * , @xmath271 \\right\\|_{f }   \\leq \\mathbb{e}\\left [ \\left\\| \\nabla_{h } g \\left ( x , m \\right ) \\right\\|^{2 } \\right ]   \\leq l_{1 } .\\end{aligned}\\ ] ] thus , since for all @xmath241 , @xmath272 , @xmath273 which concludes the proof .",
    "let us recall that equation ( [ defisigman ] ) can be written as @xmath274 in order to prove theorem [ vitesselpps ] , we just have to give the rates of convergence of the terms on the right - hand side of previous equality .",
    "the following lemma gives the almost sure and the rate of convergence in quadratic mean of the first term on the right - hand side of previous equality .",
    "[ majopasbelle0 ] suppose assumptions * ( a1 ) * to * ( a5a ) * and * ( a6b ) * hold .",
    "then , for all @xmath186 , @xmath275 moreover , suppose assumption * ( a5b ) * holds too .",
    "then , @xmath276   = o \\left ( \\frac{1}{n^{1-s } } \\right ) .\\ ] ]    the proof is given in appendix .",
    "the following lemma gives the almost sure and the rate of convergence in quadratic mean of the second term on the right - hand side of equality ( [ decsigman ] ) .    [ lemmajopasbelle ] suppose assumptions * ( a1 ) * to * ( a5a ) * and * ( a6b ) * hold .",
    "then , for all @xmath186 , @xmath277 moreover , suppose assumption * ( a5b ) * holds too .",
    "then @xmath278 = o \\left ( \\frac{1}{n^{2(1-s)}}\\right ) .\\ ] ]    the proof is given in appendix .",
    "finally , the following proposition gives the almost sure and the rate of convergence in quadratic mean of the last term on the right - hand side of equality ( [ decsigman ] ) .    [ vitsigmanbarre ] suppose assumptions * ( a1 ) * to * ( a5a ) * and * ( a6b ) * hold .",
    "then , there is a positive constant @xmath111 such that @xmath279 suppose assumption * ( a5b ) * holds too .",
    "then , there is a positive constant @xmath113 such that for all @xmath60 , @xmath280 \\leq \\frac { c}{n^{1-s}}.\\ ] ]    applying equality ( [ normf ] ) , one can check that @xmath281 where @xmath282 are defined in ( [ defiai ] ) .",
    "the following lemma gives the rate of convergence in quadratic mean of the first terms on the right - hand side of previous inequality .",
    "[ lempleinmaj ] suppose assumptions * ( a1 ) * to * ( a6b ) * hold .",
    "then , for all @xmath283 , @xmath284 = o \\left ( \\frac{1}{n^{1-s } } \\right ) , \\\\ & \\mathbb{e}\\left [ \\left ( \\frac{1}{\\sum_{k=1}^{n}k^{-\\delta}}\\sum_{k=1}^{n}\\frac{1}{k^{\\delta}b_{k } } \\left\\| a_{i , k } \\right\\| \\left\\| m_{k+1 } \\right\\| \\right)^{2}\\right ] = o \\left ( \\frac{1}{n^{1-s } } \\right ) .\\end{aligned}\\ ] ]    the proof of this lemma as well as its `` almost sure version '' are given in appendix .    then , we just have to bound the last term on the right - hand side of inequality ( [ majbourrinsigmanbarre ] ) .",
    "first let us decompose @xmath285 as @xmath286 note that for all @xmath287 , @xmath288 is @xmath289-measurable and @xmath290 = 0 $ ] .",
    "moreover , @xmath291 the end of the proof consists in giving a bound of the quadratic mean of each term on the right - hand side of previous equality .",
    "note that the almost sure rates of convergence are not proven since it is quite analogous .",
    "* bounding @xmath292 $ ] . * first , note that @xmath293 moreover , with the help of an integral test for convergence , one can check that there is a positive constant @xmath113 such that for all positive integers @xmath294 , @xmath295 furthermore , since @xmath296 is a sequence of martingale differences adapted to the filtration @xmath297 , let @xmath298 \\\\ & = \\mathbb{e}\\left [ \\left\\| \\frac{1}{\\sum_{k=1}^{n}k^{-\\delta}}\\sum_{k=1}^{n } \\left ( \\sum_{j = k}^{n } \\frac{1}{k^{\\delta}}\\frac{1}{b_{k } } \\right ) a_{k}\\xi_{k+1}\\otimes m_{k } \\right\\|_{f}^{2}\\right ] \\\\ & = \\left ( \\frac{1}{\\sum_{k=1}^{n}k^{-\\delta}}\\right)^{2}\\sum_{k=1}^{n }   \\left ( \\sum_{j = k}^{n } \\frac{1}{k^{\\delta}}\\frac{1}{b_{k } } \\right)^{2 } a_{k}^{2}\\mathbb{e}\\left [ \\left\\| \\xi_{k+1}\\otimes m_{k } \\right\\|_{f}^{2}\\right]\\end{aligned}\\ ] ] then , applying equality ( [ normf ] ) and cauchy - schwarz s inequality , @xmath299 \\\\ & \\leq \\left ( \\frac{1}{\\sum_{k=1}^{n}k^{-\\delta}}\\right)^{2}\\sum_{k=1}^{n } \\left ( \\sum_{j = k}^{n } \\frac{1}{k^{\\delta}}\\frac{1}{b_{k } } \\right)^{2 } a_{k}^{2}\\sqrt{\\mathbb{e}\\left [ \\left\\| \\xi_{k+1 } \\right\\|^{4}\\right ] \\mathbb{e}\\left [ \\left\\| m_{k } \\right\\|^{4 } \\right]}.\\end{aligned}\\ ] ] finally , applying lemmas [ lemtech ] and [ lemmajxi ] as well as inequality ( [ sumpartbk ] ) , @xmath300 with analogous calculus , one can check @xmath301   = o \\left ( \\frac{1}{n^{1-s}}\\right ) .\\ ] ]    * bounding @xmath302 $ ] . * first , note that @xmath303 note that @xmath304 is a sequence of martingale differences adapted to the filtration @xmath305 .",
    "furthermore , @xmath306 \\\\ & = \\left ( \\frac{1}{\\sum_{k=1}^{n}k^{-\\delta}}\\right)^{2}\\sum_{k=1}^{n}\\frac{1}{k^{2\\delta } } \\frac{1}{b_{k}^{2}}\\mathbb{e}\\left [ \\left\\| \\sum_{j'=2}^{k}\\sum_{j=1}^{j'-1}a_{j}a_{j ' } \\xi_{j+1}\\otimes \\xi_{j'+1 }   \\right\\|_{f}^{2 } \\right ] \\\\ & + \\left ( \\frac{1}{\\sum_{k=1}^{n}k^{-\\delta}}\\right)^{2}\\mathbb{e}\\left [ \\sum_{k=2}^{n}\\sum_{j=1}^{k-1}b_{k}^{-1}k^{-\\delta}b_{j}^{-1}j^{-\\delta } \\left\\langle \\sum_{j''=2}^{j}\\sum_{j'=1}^{j''-1}a_{j'}a_{j''}\\xi_{j'+1}\\otimes \\xi_{j''+1 } , \\sum_{i''=2}^{k}\\sum_{i'=1}^{i''-1}a_{i'}a_{i''}\\xi_{i'+1}\\otimes \\xi_{i''+1 } \\right\\rangle \\right ] .\\end{aligned}\\ ] ] then end of the proof consists in bounding the two terms on the right - hand side of previous equality .",
    "first , since @xmath304 is a sequence of martingale differences adapted to the filtration @xmath305 , let @xmath307 \\\\ & = \\left ( \\frac{1}{\\sum_{k=1}^{n}k^{-\\delta}}\\right)^{2}\\sum_{k=1}^{n}\\frac{1}{k^{2\\delta } } \\frac{1}{b_{k}^{2 } } \\sum_{j'=2}^{k}\\mathbb{e}\\left [ \\left\\|\\sum_{j=1}^{j'-1}a_{j}a_{j ' } \\xi_{j+1}\\otimes \\xi_{j'+1 }   \\right\\|_{f}^{2 } \\right ] .\\end{aligned}\\ ] ] then , applying equality ( [ normf ] ) and cauchy - schwarz s inequality , @xmath308 \\\\ & \\leq \\left ( \\frac{1}{\\sum_{k=1}^{n}k^{-\\delta}}\\right)^{2}\\sum_{k=1}^{n}\\frac{1}{k^{2\\delta } } \\frac{1}{b_{k}^{2 } } \\sum_{j'=2}^{k}a_{j'}^{2}\\sqrt{\\mathbb{e}\\left [ \\left\\| \\xi_{j'+1 }   \\right\\|^{4 } \\right]}\\sqrt { \\mathbb{e}\\left [ \\left\\|\\sum_{j=1}^{j'-1}a_{j } \\xi_{j+1}\\right\\|^{4}\\right ]   } \\end{aligned}\\ ] ] finally , applying lemma [ lemmajxi ] , [ lemsumexp ] and [ lemtech ] , @xmath309 then , since @xmath310 , @xmath311 = o \\left ( \\frac{1}{n^{1-s } } \\right ) .\\ ] ] in the same way , by linearity , let @xmath312   \\\\ & = \\left ( \\frac{1}{\\sum_{k=1}^{n}k^{-\\delta}}\\right)^{2}\\sum_{k=2}^{n}\\sum_{j=1}^{k-1}b_{k}^{-1}k^{-\\delta}b_{j}^{-1}j^{-\\delta}\\mathbb{e}\\left [   \\left\\langle \\sum_{j''=2}^{j}\\sum_{j'=1}^{j''-1}a_{j'}a_{j''}\\xi_{j'+1}\\otimes \\xi_{j''+1 } , \\sum_{i''=2}^{j}\\sum_{i'=1}^{i''-1}a_{i'}a_{i''}\\xi_{i'+1}\\otimes \\xi_{i''+1 } \\right\\rangle_{f } \\right ] \\\\ & + \\left ( \\frac{1}{\\sum_{k=1}^{n}k^{-\\delta}}\\right)^{2}\\sum_{k=2}^{n}\\sum_{j=1}^{k-1}b_{k}^{-1}k^{-\\delta}b_{j}^{-1}j^{-\\delta } \\mathbb{e}\\left [ \\left\\langle \\sum_{j''=2}^{j}\\sum_{j'=1}^{j''-1}a_{j'}a_{j''}\\xi_{j'+1}\\otimes \\xi_{j''+1 } , \\sum_{i''=j+1}^{k}\\sum_{i'=1}^{i''-1}a_{i'}a_{i''}\\xi_{i'+1}\\otimes \\xi_{i''+1 } \\right\\rangle_{f } \\right ] .",
    "\\end{aligned}\\ ] ] since @xmath313 is a sequence of martingale differences adapted to the filtration @xmath314 , @xmath315   \\\\ & = \\sum_{k=2}^{n } \\sum_{j=1}^{k-1}b_{k}^{-1}k^{-\\delta}b_{j}^{-1}j^{-\\delta}\\sum_{j''=2}^{j}\\sum_{j'=1}^{j''-1}\\sum_{i''=j+1}^{k}\\sum_{i'=1}^{i''-1}a_{i'}a_{i''}a_{j'}a_{j '' } \\mathbb{e}\\left [ \\left\\langle \\xi_{j'+1}\\otimes \\xi_{j''+1 } , \\xi_{i'+1}\\otimes \\xi_{i''+1 } \\right\\rangle_{f } \\right ] \\\\ & = \\sum_{k=2}^{n } \\sum_{j=1}^{k-1}b_{k}^{-1}k^{-\\delta}b_{j}^{-1}j^{-\\delta}\\sum_{j''=2}^{j}\\sum_{j'=1}^{j''-1}\\sum_{i''=j+1}^{k}\\sum_{i'=1}^{i''-1}a_{i'}a_{i''}a_{j'}a_{j '' } \\mathbb{e}\\left [ \\left\\langle \\xi_{j'+1}\\otimes \\xi_{j''+1 } , \\xi_{i'+1}\\otimes \\mathbb{e}\\left [ \\xi_{i''+1 } |\\mathcal{f}_{i '' } \\right ] \\right\\rangle_{f } \\right ] \\\\ & = 0 .\\end{aligned}\\ ] ] furthermore , since @xmath316 is a sequence of martingale differences adapted to the filtration @xmath317 and applying equality ( [ normf ] ) , @xmath318 \\\\ & = \\left ( \\frac{1}{\\sum_{k=1}^{n}k^{-\\delta}}\\right)^{2}\\sum_{k=2}^{n}\\sum_{j=1}^{k}b_{k}^{-1}k^{-\\delta}b_{j}^{-1}j^{-\\delta }    \\sum_{j''=1}^{j } \\mathbb{e}\\left [ \\left\\| \\sum_{j'=1}^{j''-1}a_{j'}a_{j''}\\xi_{j'+1}\\otimes \\xi_{j''+1 } \\right\\|_{f}^{2 } \\right ] \\\\ & = \\left ( \\frac{1}{\\sum_{k=1}^{n}k^{-\\delta}}\\right)^{2}\\sum_{k=2}^{n}\\sum_{j=1}^{k}b_{k}^{-1}k^{-\\delta}b_{j}^{-1}j^{-\\delta }    \\sum_{j''=1}^{j}a_{j''}^{2 } \\mathbb{e}\\left [ \\left\\| \\sum_{j'=1}^{j''-1}a_{j'}\\xi_{j'+1}\\right\\|^{2 } \\left\\| \\xi_{j''+1 } \\right\\|^{2 } \\right ] .\\end{aligned}\\ ] ] applying cauchy - schwarz s inequality as well as lemmas [ lemmajxi ] and [ lemtech ] , @xmath319 \\mathbb{e}\\left [ \\left\\| \\xi_{j''+1 } \\right\\|_{f}^{4 } \\right ] } \\\\ & = o \\left ( \\left ( \\frac{1}{\\sum_{k=1}^{n}k^{-\\delta}}\\right)^{2}\\sum_{k=1}^{n}\\sum_{j=1}^{k}b_{k}^{-1}k^{-\\delta}b_{j}^{-1}j^{-\\delta}\\sum_{j''=1}^{j}a_{j''}^{4}j''^{s } \\right ) .",
    "\\end{aligned}\\ ] ] finally , applying lemma [ lemsumexp ] , @xmath320 thus , @xmath321 = o \\left ( \\frac{1}{n^{1-s}}\\right ) .\\ ] ] moreover , with analogous calculus , one can check @xmath322 = o \\left ( \\frac{1}{n^{1-s}}\\right ) .\\ ] ]    * bounding @xmath323 . * first , note that @xmath324 - \\sigma \\right ) \\\\ & + \\frac{1}{\\sum_{k=1}^{n}k^{-\\delta } } \\sum_{k=1}^{n } \\frac{1}{k^{\\delta}b_{k } } \\sum_{j=1}^{k}a_{k}^{2 } \\left ( \\xi_{k+1 } \\otimes \\xi_{k+1 } - \\mathbb{e}\\left [ \\xi_{k+1 } \\otimes \\xi_{k+1 } |\\mathcal{f}_{k } \\right ] \\right)\\end{aligned}\\ ] ] the end of the proof consists in bounding the quadratic mean of the terms on the right - hand side of previous equality .",
    "first , applying lemma [ lemsum ] , let @xmath325 - \\sigma \\right ) \\right\\|_{f}^{2 } \\right ] \\\\ & \\leq \\left ( \\frac{1}{\\sum_{k=1}^{n}k^{-\\delta}}\\right)^{2 } \\left ( \\sum_{k=1}^{n } \\frac{1}{k^{\\delta}b_{k } } \\sqrt{\\mathbb{e}\\left [ \\left\\| \\sum_{j=1}^{k}a_{j}^{2 } \\left ( \\mathbb{e}\\left [ \\xi_{k+1 } \\otimes \\xi_{k+1}|\\mathcal{f}_{k } \\right ] - \\sigma \\right ) \\right\\|_{f}^{2}\\right ] } \\right)^{2 } \\\\ & \\leq \\left ( \\frac{1}{\\sum_{k=1}^{n}k^{-\\delta}}\\right)^{2 } \\left ( \\sum_{k=1}^{n } \\frac{1}{k^{\\delta}b_{k } }   \\sum_{j=1}^{k } a_{j}^{2 } \\sqrt{\\mathbb{e}\\left [ \\left\\| \\mathbb{e}\\left [ \\xi_{k+1 } \\otimes \\xi_{k+1}|\\mathcal{f}_{k } \\right ] - \\sigma    \\right\\|_{f}^{2}\\right ] } \\right)^{2}\\end{aligned}\\ ] ] then , applying inequality ( [ vitlprm ] ) and corollary [ lemmajxisigma ] , @xmath326 } \\right)^{2 } \\right ) \\\\ & = o \\left ( \\left ( \\frac{1}{\\sum_{k=1}^{n}k^{-\\delta}}\\right)^{2 } \\left ( \\sum_{k=1}^{n } \\frac{1}{k^{\\delta}b_{k } }   \\sum_{j=1}^{k } a_{j}^{2 } j^{-\\alpha /2 } \\right)^{2 } \\right ) .\\end{aligned}\\ ] ] furthermore , thanks to lemma [ lemsumexp ] , @xmath327 thus , since @xmath328 , @xmath329 - \\sigma \\right ) \\right\\|_{f}^{2 } \\right ] = o \\left ( \\frac{1}{n^{1-s}}\\right ) .\\ ] ]    moreover , applying lemma [ lemsum ] , let @xmath330 \\right ) \\right\\|_{f}^{2 } \\right ] \\\\ & \\leq \\left ( \\frac{1}{\\sum_{k=1}^{n}k^{-\\delta}}\\right)^{2}\\ \\left ( \\sum_{k=1}^{n } \\frac{1}{k^{\\delta}b_{k } } \\sqrt{\\mathbb{e}\\left [ \\left\\| \\sum_{j=1}^{k}a_{j}^{2 } \\left ( \\xi_{k+1 } \\otimes \\xi_{k+1 } - \\mathbb{e}\\left [ \\xi_{k+1 } \\otimes \\xi_{k+1 } |\\mathcal{f}_{k } \\right ] \\right ) \\right\\|_{f}^{2 } \\right ] } \\right)^{2}.\\end{aligned}\\ ] ] furthermore , since @xmath331 - \\xi_{k+1}\\otimes \\xi_{k+1 } \\right)$ ] is a sequence of martingale differences adapted to the filtration @xmath209 and applying lemma [ lemmajxi ] , @xmath332 \\right ) \\right\\|_{f}^{2 } \\right ] } \\right)^{2 } \\\\ & = o \\left ( \\left ( \\frac{1}{\\sum_{k=1}^{n}k^{-\\delta}}\\right)^{2}\\ \\left ( \\sum_{k=1}^{n } \\frac{1}{k^{\\delta}b_{k } } \\sqrt { \\sum_{j=1}^{k}a_{j}^{4 } } \\right)^{2 } \\right ) .\\end{aligned}\\ ] ] then , applying lemma [ lemsumexp ] , @xmath333 finally , @xmath334 \\right ) \\right\\|_{f}^{2 } \\right ]    = o \\left ( \\frac{1}{n^{1-s } } \\right ) , \\ ] ] which concludes the proof .      in order to simplify the proof , we recall or give some technical lemmas .",
    "the following one ensures that the sequence @xmath71 admits uniformly bounded @xmath335-moments .",
    "[ lemmajxi ] suppose assumptions * ( a1 ) * to * ( a5a ) * hold , there is a positive constant @xmath336 such that for all @xmath337 , @xmath338 \\leq k\\ ] ] moreover , suppose assumption * ( a5b ) * holds too .",
    "then , for all positive integer @xmath76 , there is a positive constant @xmath339 such that for all @xmath337 , @xmath340 \\leq k_{p}\\ ] ] as a particular case , since for all eigenvalue @xmath196 of @xmath36 , @xmath197 , for all @xmath60 , @xmath341 \\leq k_{p}\\lambda_{\\min}^{-2p}.\\ ] ]    [ lemmajxisigma ] suppose assumptions * ( a1 ) * to * ( a6b ) * hold .",
    "then , there is a positive constant @xmath113 such that for all @xmath60 , @xmath342 - \\sigma   \\right\\|_{f}^{2 } \\leq c \\left\\| m_{n } - m \\right\\|^{2}\\ ] ]    the proof is not given since it is a direct application of assumption * ( a6b ) * and lemma  [ lemmajxi ] .",
    "the following lemma gives upper bounds of the sums of exponential terms which appears in several proofs .",
    "[ lemsumexp ] for all constants @xmath343 such that @xmath344 , there is a positive constant @xmath345 such that @xmath346    the proof is not given it is a direct application of an integral test for convergence . as a corollary",
    ", one can obtain the following bound ( lower and upper ) of @xmath347 .",
    "[ corbn ] there are positive constants @xmath348 such that for all @xmath60 , @xmath349    the following lemma is really useful in the proof of theorem [ tlcgrad ] .",
    "[ sumexppart ] let @xmath350 be non - negative constants such that @xmath351 , and @xmath352 , @xmath353 be two sequences defined for all @xmath60 by @xmath354 with @xmath355 .",
    "thus , there is a positive constant @xmath356 such that for all @xmath60 , @xmath357    finally , we recall the following results , which enables us to upper bound the @xmath7 moments of a sum of random variables in normed vector spaces .    [ lemsum ] let @xmath358 be random variables taking values in a normed vector space such that for all positive constant @xmath45 and for all @xmath359 , @xmath360 < \\infty$ ] .",
    "thus , for all constants @xmath361 and for all integer @xmath76 , @xmath362 \\leq \\left ( \\sum_{k=1}^{n } \\left| a_{k } \\right| \\left ( \\mathbb{e}\\left [ \\left\\| y_{k } \\right\\|^{p } \\right ] \\right)^{\\frac{1}{p } } \\right)^{p}.\\ ] ]",
    "* bounding @xmath363 $ ] . * applying an abel s transform , @xmath364 first , @xmath365 = o \\left ( 1 \\right)$ ] .",
    "moreover , applying inequality ( [ vitlprm ] ) , @xmath366 & \\leq \\exp \\left ( \\frac{pn^{1-s}}{1-s } \\right)c_{\\gamma}^{-1}n^{2p\\alpha } \\frac{c_{p}\\lambda_{\\min}^{-2p}}{n^{p\\alpha } } \\\\ & \\leq \\exp \\left ( \\frac{pn^{1-s}}{1-s } \\right)c_{p}c_{\\gamma}^{-1}\\lambda_{\\min}^{-2p}n^{p\\alpha}.\\end{aligned}\\ ] ] furthermore , one can check that there is a positive constant @xmath113 such that for all @xmath60 , @xmath367 and applying lemma [ lemsum ] and inequality ( [ vitlprm ] ) , @xmath368 & \\leq \\left ( \\sum_{k=2}^{n }   \\left| \\frac{a_{k}}{\\gamma_{k } } - \\frac{a_{k-1}}{\\gamma_{k-1 } } \\right| \\left ( \\mathbb{e}\\left [ \\left\\| t_{k } \\right\\|^{2p } \\right ] \\right)^{\\frac{1}{2p}}\\right)^{2p } \\\\ & \\leq c^{2p}c_{p}\\lambda_{\\min}^{-2p } \\left ( \\sum_{k=2}^{n } k^{-s+\\alpha}\\exp \\left ( \\frac{k^{1-s}}{2(1-s ) } \\right)k^{-\\alpha / 2 } \\right)^{2p } .\\end{aligned}\\ ] ] finally , applying lemma [ lemsumexp ] , @xmath369 = o \\left ( \\exp \\left ( \\frac{pn^{1-s}}{1-s } \\right ) n^{p\\alpha } \\right ) .\\ ] ]    * bounding @xmath370 $ ] . * since there is a positive constant @xmath371 ( see @xcite ) such that for all @xmath60 , @xmath372 , applying lemma [ lemsum ] and inequality ( [ vitlprm ] ) , @xmath373 & \\leq c_{m}^{2p}\\lambda_{\\min}^{-4p } \\left ( \\sum_{k=1}^{n } a_{k } \\left ( \\mathbb{e}\\left [ \\left\\| m_{n } - m \\right\\|^{4p } \\right ] \\right)^{\\frac{1}{2p } } \\right)^{2p } \\\\ & \\leq c_{m}^{2p}\\lambda_{\\min}^{-4p}c_{2p } \\left ( \\sum_{k=1}^{n } a_{k}k^{-\\alpha }   \\right)^{2p }   \\end{aligned}\\ ] ] applying lemma [ lemsumexp ] , @xmath374 = o \\left ( \\exp \\left ( \\frac{pn^{1-s}}{1-s } \\right ) n^{2p ( s- \\alpha ) } \\right)\\ ] ]    * bounding @xmath375 $ ] . *",
    "first , since @xmath376 is a sequence of martingale differences , and thanks to lemma [ lemsumexp ] , @xmath377 & = \\sum_{k=1}^{n } a_{k}^{2}\\mathbb{e}\\left [ \\left\\| \\xi_{k+1 } \\right\\|^{2 } \\right ] \\\\ & = o \\left ( \\exp \\left ( \\frac{n^{1-s}}{1-s } \\right ) n^{s } \\right ) .\\end{aligned}\\ ] ] with the help of an induction on @xmath76 ( see the proof of theorem 4.2 in @xcite for instance ) , one can check that for all integer @xmath378 , @xmath379 = o \\left ( \\exp \\left ( p\\frac{n^{1-s}}{1-s } \\right ) n^{ps } \\right ) , \\ ] ] which concludes the proof .",
    "let @xmath221 be the eigenvalues of the hessian @xmath36 .",
    "first , let @xmath380 let us recall that there is a positive constant @xmath113 such taht for all @xmath381 , @xmath382 .",
    "then , let @xmath198 be an integer such that for all @xmath383 , @xmath384 , and it comes , for all @xmath383 , @xmath385 . then",
    ", with the help of the taylor s expansion of the functional @xmath386 , one can check that for all @xmath381 and for all @xmath383 , @xmath387 with @xmath388 . then , for all @xmath389 , @xmath390 with the help of an integral test for convergence , @xmath391 then , @xmath392 we now give an upper bound of @xmath393 .",
    "since @xmath394 for all @xmath381 , there is a rank @xmath198 , only depending on @xmath395 and @xmath396 , such that the functional @xmath397 defined for all @xmath398 by @xmath399 is increasing on @xmath400 $ ] . for the sake of simplicity ,",
    "let us consider that @xmath401 . then , with the help of an integral test for convergence , @xmath402_{0}^{n } \\\\",
    "\\label{inegintpourri } &   + c_{\\gamma}\\frac{1}{\\lambda_{i } + \\lambda_{i ' } } \\int_{0}^{n } e^ { -\\left ( \\lambda_{i } + \\lambda_{i ' } \\right ) \\frac{c_{\\gamma}}{1-\\alpha}\\left ( ( t+1)^{1-\\alpha } - n^{1-\\alpha } \\right ) } \\left (   t^{-1-\\alpha } - \\left ( \\lambda_{i } + \\lambda_{i ' } \\right )   c_{\\gamma}t^{-2\\alpha}\\right )    e^ { -\\left ( \\lambda_{i } + \\lambda_{i ' } \\right )   c_{\\gamma}(t+1)^{- \\alpha } } dt\\end{aligned}\\ ] ] then , since for all @xmath381 , @xmath403 , one can check that there is a positive sequence @xmath404 only depending on @xmath405 such that @xmath406 with analogous calculus , on can check that there is a positive sequence @xmath407 only depending on @xmath408 such that @xmath409 which concludes the proof .",
    "we only give the bound of the quadratic mean error since the almost sure rate of convergence is quite straightforward .",
    "first , since @xmath410 and by linearity , let @xmath411 then , we have to bound the three terms on the right - hand side of previous equality .",
    "* bounding @xmath412 $ ] .",
    "* first , applying lemma [ lemsum ] and equality ( [ normf ] ) , let @xmath413 \\\\ & \\leq \\left ( \\frac{1-\\delta}{n^{1-\\delta}}\\right)^{2 } \\left (   \\sum_{k=1}^{n } \\frac{1}{k^{\\delta + s}}\\exp \\left ( - \\frac{k^{1-s}}{1-s}\\right ) \\sqrt{\\mathbb{e}\\left [ \\left\\|   \\left ( \\sum_{j=1}^{k } e^{\\frac{j^{1-s}}{2(1-s)}}\\left ( m_{j } - m \\right ) \\right ) \\otimes \\left ( \\sum_{j=1}^{k } e^{\\frac{j^{1-s}}{2(1-s)}}\\left ( \\overline{m}_{j } - m \\right ) \\right ) \\right\\|_{f}^{2 } \\right]}\\right)^{2 } \\\\ & \\leq \\left ( \\frac{1-\\delta}{n^{1-\\delta}}\\right)^{2 } \\left (   \\sum_{k=1}^{n } \\frac{1}{k^{\\delta + s}}\\exp \\left ( - \\frac{k^{1-s}}{1-s}\\right ) \\sqrt{\\mathbb{e}\\left [ \\left\\|    \\sum_{j=1}^{k } e^{\\frac{j^{1-s}}{2(1-s)}}\\left ( m_{j } - m \\right )   \\right\\|^{2 } \\left\\|   \\sum_{j=1}^{k } e^{\\frac{j^{1-s}}{2(1-s)}}\\left ( \\overline{m}_{j } - m \\right )   \\right\\|^{2 } \\right]}\\right)^{2 } .\\end{aligned}\\ ] ] applying cauchy - schwarz s inequality , @xmath414 \\right)^{\\frac{1}{4 } } \\left ( \\mathbb{e}\\left [ \\left\\|    \\sum_{j=1}^{k } e^{\\frac{j^{1-s}}{2(1-s)}}\\left ( \\overline{m}_{j } - m \\right )   \\right\\|^{4}\\right]\\right)^{\\frac{1}{4 } } \\right)^{2}.\\end{aligned}\\ ] ] first , note that thanks to lemma [ lemtech ] @xmath415 = o \\left ( \\exp \\left ( \\frac{2k^{1-s}}{1-s}\\right ) k^{2s } \\right ) .\\ ] ] furthermore , applying lemmas [ lemsum ] and lemma [ lemsumexp ] as well as inequality ( [ vitlpmoy ] ) , @xmath416 & \\leq \\left (   \\sum_{j=1}^{k } e^{\\frac{j^{1-s}}{2(1-s)}}\\left ( \\mathbb{e}\\left [ \\left\\| \\overline{m}_{j } - m \\right\\|^{4 } \\right]\\right)^{\\frac{1}{4}}\\right)^{4 } \\\\ \\notag & \\leq c_{2 } ' \\left (   \\sum_{j=1}^{k } e^{\\frac{j^{1-s}}{2(1-s)}}\\frac{1}{j^{1/2}}\\right)^{4 }",
    "\\\\ \\label{nouvmaj } & = o \\left (   \\exp \\left ( 2\\frac{k^{1-s}}{(1-s ) } \\right)k^{4s}k^{-2 } \\right ) .\\end{aligned}\\ ] ] then , applying lemma [ lemsumexp ] , @xmath417        * bounding @xmath419 $ ] . *",
    "first , applying lemma [ lemsum ] and equality ( [ normf ] ) , let @xmath420 \\\\ & \\leq \\left ( \\frac{1-\\delta}{n^{1-\\delta } } \\sum_{k=1}^{n } \\frac{1}{k^{\\delta + s } } e^ { - \\frac{k^{1-s}}{1-s } } \\sqrt{\\mathbb{e}\\left [ \\left\\| \\left ( \\sum_{j=1}^{k }",
    "e^{\\frac{j^{1-s}}{2(1-s)}}\\left ( \\overline{m}_{j } - m \\right ) \\right ) \\otimes \\left ( \\sum_{j=1}^{k } e^{\\frac{j^{1-s}}{2(1-s)}}\\left ( \\overline{m}_{j } - m \\right ) \\right ) \\right\\|_{f}^{2}\\right ] } \\right)^{2 } \\\\ & =   \\left ( \\frac{1-\\delta}{n^{1-\\delta } } \\sum_{k=1}^{n } \\frac{1}{k^{\\delta + s } } e^ { - \\frac{k^{1-s}}{1-s } } \\sqrt{\\mathbb{e}\\left [ \\left\\|   \\sum_{j=1}^{k } e^{\\frac{j^{1-s}}{2(1-s)}}\\left ( \\overline{m}_{j } - m   \\right ) \\right\\|_{f}^{4}\\right ] } \\right)^{2}.\\end{aligned}\\ ] ] then , applying inequality ( [ nouvmaj ] ) and corollary [ corbn ] , @xmath421 which concludes the proof .",
    "we just give the proof for the rate of convergence in quadratic mean , the proof of the almost sure rate of convergence is quite straightforward .",
    "let @xmath422 we now bound the quadratic mean of each term on the right - hand side of previous equality .",
    "first , note that with the help of an integral test for convergence , @xmath423 then , @xmath424 then , applying lemma [ lemsum ] , there is a positive constant @xmath113 such that for all @xmath60 , @xmath425 \\\\ & \\leq \\frac{c}{n^{4 - 4\\delta}}\\left ( \\sum_{k=1}^{n } \\frac{1}{k^{\\delta + s}}\\exp \\left ( - \\frac{k^{1-s}}{1-s}\\right ) \\sqrt{\\mathbb{e}\\left\\|",
    "\\left ( \\sum_{j=1}^{k } e^{\\frac{j^{1-s}}{2(1-s)}}\\left ( m_{j } - m \\right ) \\right ) \\otimes \\left ( \\sum_{j=1}^{k } e^{\\frac{j^{1-s}}{2(1-s)}}\\left ( m_{j } - m \\right ) \\right ) \\right\\|_{f}^{2 } } \\right)^{2 } .\\end{aligned}\\ ] ] furthermore , applying equality ( [ normf ] ) @xmath426 finally , applying lemma [ lemtech ] ans since @xmath427 , @xmath428    in the same way , with the help of an integral test for convergence , @xmath429 thus , one can check that there is a positive constant @xmath84 such that for all @xmath60 , @xmath430 then , @xmath431 thus , applying lemma [ lemsum ] , there is a positive constant @xmath113 such that for all @xmath60 , @xmath432 \\\\ & \\leq \\left ( \\frac{1}{\\sum_{k=1}^{n}k^{-\\delta } } \\right)^{2}\\left ( \\sum_{k=1}^{n } \\frac{1}{k^{\\delta}}k^{-1}e^ { - \\frac { k^{1-s}}{1-s } } \\sqrt { \\mathbb{e}\\left [ \\left\\| \\left ( \\sum_{j=1}^{k } e^{\\frac{j^{1-s}}{2(1-s)}}\\left ( m_{j } - m \\right ) \\right ) \\otimes \\left ( \\sum_{j=1}^{k } e^{\\frac{j^{1-s}}{2(1-s)}}\\left ( m_{j } - m \\right ) \\right ) \\right\\|_{f}^{2 } \\right ] } \\right)^{2 } .\\end{aligned}\\ ] ] finally , applying equality ( [ normf ] ) and lemma [ lemtech ] , @xmath433 } \\right)^{2}\\\\   & = o \\left ( \\left ( \\frac{1}{\\sum_{k=1}^{n}k^{-\\delta } } \\right)^{2}\\left ( \\sum_{k=1}^{n } \\frac{1}{k^{\\delta + 1-s } } \\right)^{2 } \\right ) \\\\ & = o \\left ( \\frac{1}{n^{2(1-s ) } } \\right ) , \\end{aligned}\\ ] ] which concludes the proof .",
    "this proof is a direct application of lemma [ lemtech ] . in order to convince the reader",
    ", we just give one proof , and the other ones are analogous . applying lemma [ lemsum ] and [ lemtech ] as well as corollary [ corbn ] , @xmath434 & \\leq \\left(\\frac{1 } { \\sum_{k=1}^{n}k^{-\\delta}}\\right)^{2}\\left ( \\sum_{k=1}^{n}\\frac{1}{k^{\\delta}b_{k } } \\sqrt { \\mathbb{e}\\left [   \\left\\| a_{1,k } \\right\\|^{4 } \\right ] } \\right)^{2 } \\\\ & = o \\left (   \\left(\\frac{1 } { \\sum_{k=1}^{n}k^{-\\delta}}\\right)^{2}\\left ( \\sum_{k=1}^{n}\\frac{1}{k^{\\delta } } k^{\\alpha -s }    \\right)^{2}\\right ) \\\\ & = o \\left ( \\frac{1}{n^{2(s- \\alpha)}}\\right ) , \\end{aligned}\\ ] ] which concludes the proof .",
    "suppose assumptions * ( a1 ) * to * ( a5a ) * hold .",
    "then , for all @xmath283 , and for all @xmath186 , @xmath284 = o \\left ( \\frac { ( \\ln n)^{\\gamma}}{n^{1-s } } \\right ) , \\\\ & \\mathbb{e}\\left [ \\left ( \\frac{1}{\\sum_{k=1}^{n}k^{-\\delta}}\\sum_{k=1}^{n}\\frac{1}{k^{\\delta}b_{k } } \\left\\| a_{i , k } \\right\\| \\left\\| m_{k+1 } \\right\\| \\right)^{2}\\right ] = o \\left ( \\frac{(\\ln n)^{\\gamma}}{n^{1-s } } \\right ) .\\end{aligned}\\ ] ]        first , for all @xmath14 , let us define the function @xmath435 \\longrightarrow \\mathcal{s}(h)$ ] , defined for all @xmath436 $ ] by @xmath437 \\\\ & = \\mathbb{e}\\left [ \\frac{x - m^{v}+t \\left ( h - m^{v } \\right)}{\\left\\| x - m^{v}+t \\left ( h - m^{v } \\right ) \\right\\| } \\otimes \\frac{x - m^{v}+t \\left ( h - m^{v } \\right)}{\\left\\| x - m^{v}+t \\left ( h - m^{v } \\right ) \\right\\| } \\right ] .\\end{aligned}\\ ] ] in what follows , we will denote @xmath438 .",
    "note that @xmath439 & \\varphi_{h}(1 ) = \\mathbb{e}\\left [ \\nabla_{h}g_{v } \\left ( x , h \\right ) \\otimes \\nabla_{h}g_{v } \\left ( x , h \\right ) \\right]\\end{aligned}\\ ] ] and that the functional @xmath440 is differentiable , and its derivative is defined for all @xmath441 $ ] by @xmath442 + \\mathbb{e}\\left [ \\frac{1}{\\left\\| a(t ) \\right\\|^{2}}\\left ( h - m^{v } \\right ) \\otimes a(t ) \\right ] \\\\ & + \\mathbb{e}\\left [ \\frac{1}{\\left\\| a(t ) \\right\\|^{2 } } a(t ) \\otimes \\left ( h - m^{v } \\right ) \\right ] .\\end{aligned}\\ ] ] then , applying cauchy - schwarz s inequality , @xmath443 \\left\\| m^{v } - h \\right\\| .\\end{aligned}\\ ] ] thus , let @xmath234 , thanks to assumption * ( h2 ) * , there is a positive constant @xmath444 such that for all @xmath441 $ ] and for all @xmath148 , @xmath445 finally , @xmath446 - \\mathbb{e}\\left [ \\nabla_{h}g_{v } \\left ( x , h \\right ) \\otimes \\nabla_{h}g_{v } \\left ( x , h \\right ) \\right ] \\right\\|_{f } & = \\left\\| \\varphi_{h}(1 ) - \\varphi_{h}(0 ) \\right\\|_{f } \\\\ & = \\left\\| \\int_{0}^{1}\\varphi_{h}'(t ) dt \\right\\|_{f } \\\\ & \\leq c_{\\left\\| m^{v } \\right\\| + \\epsilon } \\left\\| m^{v } - h \\right\\| , \\end{aligned}\\ ] ] which concludes the proof .",
    "kemperman , j. ( 1987 ) . the median of a finite measure on a banach space . in",
    "statistical data analysis based on the @xmath447-norm and related methods ( neuchtel , 1987 ) _ , pages 217230 .",
    "north - holland , amsterdam .",
    "minsker , s. , srivastava , s. , lin , l. , and dunson , d. ( 2014 ) .",
    "scalable and robust bayesian inference via the median posterior . in _ proceedings of the 31st international conference on machine learning ( icml-14 ) _ , pages 16561664 ."
  ],
  "abstract_text": [
    "<S> stochastic gradient algorithms are more and more studied since they can deal efficiently and online with large samples in high dimensional spaces . in this paper </S>",
    "<S> , we first establish a central limit theorem for these estimates as well as for their averaged version in general hilbert spaces . moreover , </S>",
    "<S> since having the asymptotic normality of estimates is often unusable without an estimation of the asymptotic variance , we introduce a recursive algorithm of the asymptotic variance of the averaged estimator , and we establish its almost sure rate of convergence as well as its rate of convergence in quadratic mean . finally , an example in robust statistics is given : the estimation of geometric quantiles and of the geometric median .    </S>",
    "<S> * keywords : * stochastic gradient algorithm , averaging , central limit theorem , asymptotic variance , geometric median , geometric quantiles . </S>"
  ]
}