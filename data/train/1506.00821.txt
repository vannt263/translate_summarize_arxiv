{
  "article_text": [
    "multi - object tracking refers to the problem of jointly estimating the number of objects and their trajectories from sensor data .",
    "driven by aerospace applications in the 1960 s , today multi - object tracking lies at the heart of a diverse range of application areas , see for example the texts bsf88,bp99 , mah07 , mallickbook12 , mahler2014advances .",
    "the most popular approaches to multi - object tracking are the joint probabilistic data association filter @xcite , multiple hypothesis tracking bp99 , and more recently , random finite set ( rfs ) @xcite .",
    "the rfs approach has attracted significant attention as a general systematic treatment of multi - object systems and provides the foundation for the development of novel filters such as the probability hypothesis density ( phd ) filter @xcite , cardinalized phd ( cphd ) filter @xcite , and multi - bernoulli filters mah07,vvc09,vvps10 .",
    "while these filters were not designed to estimate the trajectories of objects , they have been successfully deployed in many applications including radar / sonar @xcite , tobiaslanterman08 , @xcite , computer vision mtc_csvt08,hvvs_pr12,hvv_tsp13 , cell biology @xcite , autonomous vehicle @xcite , automotive safety @xcite , sensor scheduling rv10,rvc11,hv14sencon , gostar13 and sensor network zhang_tac11,bcf_stsp13,ucj_stsp13 .",
    "the introduction of the generalized labeled multi - bernoulli ( glmb ) rfs in @xcite has led to the development of the first tractable rfs - based multi - object tracker - the @xmath0-glmb filter .",
    "the @xmath1-glmb filter is attractive in that it exploits the conjugacy of the glmb family to propagate forward in time the ( labeled ) multi - object filtering density exactly @xcite .",
    "each iteration of this filter involves an update operation and a prediction operation , both of which result in weighted sums of multi - target exponentials with intractably large number of terms .",
    "the first implementation of the @xmath0-glmb filter truncate these sums by using the @xmath2-shortest path and ranked assignment algorithms , respectively , in the prediction and update to determine the most significant components @xcite .    while the original two - staged implementation is intuitive and highly parallelizable , it is structurally inefficient as it requires many intermediate truncations of the @xmath0-glmb densities . specifically , in the update , truncation is performed by solving a ranked assignment problem for each predicted @xmath0-glmb component . since truncation of the predicted @xmath0-glmb sum is performed separately from the update , in general , a significant portion of the predicted components would generate updated components with negligible weights .",
    "hence , computations are wasted in solving a large number of ranked assignment problems , each of which has cubic complexity in the number of measurements .    in this paper , we present a new implementation by formulating a joint prediction and update that eliminates inefficient truncation procedures in the original approach .",
    "the key innovation is the exploitation of the direct relationship between the components of the @xmath0-glmb filtering densities at consecutive iterations to circumvent solving a ranked assignment problem for each predicted component .",
    "in contrast to the original implementation , the proposed joint implementation only requires one truncation per component in the filtering density .",
    "naturally , the joint prediction and update allows truncation of the @xmath0-glmb filtering density ( without explicitly enumerating all the components ) using the ranked assignment algorithm @xcite , @xcite , @xcite .",
    "more importantly , it admits a very efficient approximation of the @xmath0-glmb filtering density based on markov chain monte carlo methods .",
    "the key innovation is the use of gibbs sampling to generate significant updated @xmath0-glmb components , instead of deterministically generating them in order of non - increasing weights .",
    "the advantages of the proposed stochastic solution compared to the rank assignment algorithm are two - fold .",
    "first , it eliminates unnecessary computations incurred by sorting the  components , and reduces the complexity from cubic to linear in the number of measurements .",
    "second , it automatically adjusts the number of significant components generated by exploiting the statistical characteristics of the component weights .",
    "the paper is organized as follows .",
    "background on labeled rfs and the @xmath3-glmb filter is provided in section [ sec : bg ] .",
    "section sec : fast_impl presents the joint prediction and update formulation and the gibbs sampler based implementation of the @xmath0-glmb filter .",
    "numerical results are presented in section [ sec : sim ] and concluding remarks are given in section [ sec : sum ] .",
    "this section summarizes the labeled rfs and the glmb filter implementation .",
    "we refer the reader to the original work @xcite for detailed expositions .    for the rest of the paper , single - object states",
    "are represented by lowercase letters , e.g. @xmath4 , @xmath5 while multi - object states are represented by uppercase letters , e.g. @xmath6 , @xmath7 , symbols for labeled states and their distributions are bolded to distinguish them from unlabeled ones , e.g. @xmath5 , @xmath7 , @xmath8 , etc , spaces are represented by blackboard bold e.g. @xmath9 , @xmath10 , @xmath11 , @xmath12 , etc , and the class of finite subsets of a space @xmath9 is denoted by @xmath13 .",
    "we use the standard inner product notation @xmath14 and the following multi - object exponential notation @xmath15 , where @xmath16 is a real - valued function , with @xmath17 by convention .",
    "we denote a generalization of the kronecker delta that takes arbitrary arguments such as sets , vectors , etc , by @xmath18and the inclusion function , a generalization of the indicator function , by@xmath19we also write @xmath20 in place of @xmath21 when @xmath6 = @xmath22 .",
    "a labeled rfs is simply a finite set - valued random variable where each single - object dynamical state is augmented with a unique label that can be stated concisely as follows    a labeled rfs with state space @xmath9 and ( discrete ) label space @xmath11 is an rfs on @xmath23 such that each realization has distinct labels .",
    "let @xmath24 be the projection @xmath25 , then a finite subset set @xmath7 of @xmath23 has distinct labels if and only if @xmath7 and its labels @xmath26 have the same cardinality , i.e. @xmath27 . the function @xmath28 @xmath29 is called the _ distinct label indicator_.    the set integral defined for any function @xmath30 is given by@xmath31 where the integral of a function @xmath32 is : @xmath33    the notion of labeled rfs enables the incorporation of individual object identity into multi - object system and the bayes filter to be used as a tracker of these multi - object states .",
    "suppose that at time @xmath34 , there are @xmath35 target states @xmath36 , each taking values in the ( labeled ) state space @xmath37 . in the random finite set formulation",
    "the set of targets is treated as the _ multi - object state _",
    "@xmath38 each state @xmath39 either survives with probability @xmath40 and evolves to a new state @xmath41 or dies with probability @xmath42 .",
    "the dynamics of the survived targets are encapsulated in the multi - object transition density @xmath43 ) .    for a given multi - object state @xmath44 , each state @xmath45 at time @xmath34",
    "is either detected with probability @xmath46 and generates an observation @xmath47 with likelihood @xmath48 or missed with probability @xmath49 .",
    "the _ multi - object observation _ at time @xmath34 , @xmath50 , is the superposition of the observations from detected states and poisson clutters with intensity @xmath51 . assuming that , conditional on @xmath52 , detections are independent , and that clutter is independent of the detections and is distributed as a poisson rfs , the multi - object likelihood is given by voglmb13,vvp_glmb13 @xmath53^{\\mathbf{x}_k }   \\label{eq : rfsmeaslikelihood0}\\]]where @xmath54 is a function such that @xmath55 implies @xmath56 , and @xmath57    @xmath58 is called an _ association map _ since it provides the mapping between tracks and observations , i.e. which track generates which observation , with undetected tracks assigned to 0 .",
    "the condition @xmath55 implies @xmath56 ensures that a track can generate at most one measurement at a point of time .    the image of a set @xmath59 through the map @xmath60 is denoted by @xmath61 , i.e. @xmath62 while the notation @xmath63 is used to denote the collection of all eligible association maps on domain @xmath64 , i.e. @xmath65    if the clutter is distributed as an iid cluster rfs , i.e. @xmath66^k$ ] , the multi - object likelihood is @xmath67 where @xmath68 is the association map from @xmath69 to @xmath70 and    given a multi - object system as described above , the objective is to find the _ multi - object filtering density _ , denoted by latexmath:[$\\mathbf{\\pi } _ { k+1}(\\mathbf{x }",
    "_ { }    measurements upto time @xmath34 ] , which captures all information on the number of targets and individual target states at time @xmath72 . in multi - object baysian filtering , the multi - object filtering density is computed recursively in time according to the following prediction and update , commonly referred to as _ multi - object bayes recursion _ @xcite @xmath73    note , however , that the bayes filter is intractable since the set integrals in - have no analytic solution in general .",
    "the @xmath1-glmb rfs , a special class of labeled rfs , provides an exact solution to - .",
    "this is because the @xmath1-glmb rfs is closed under the multi - object chapman - kolmogorov equation with respect to the multi - object transition kernel and is conjugate with respect to the multi - object likelihood function @xcite .",
    "a @xmath1-glmb rfs is a labeled rfs with state space @xmath9 and ( discrete ) label space @xmath11 , distributed according to @xmath74 ^{\\mathbf{x}},\\]]where @xmath75 is a discrete space while @xmath76 and @xmath77 satisfy @xmath78    the @xmath1-glmb density is essentially a mixture of multi - object exponentials , in which each components is identified by a pair @xmath79 .",
    "each @xmath80 is a set of tracks labels while @xmath81 represents a history of association maps @xmath82 .",
    "the pair @xmath79 can be interpreted as the hypothesis that the set of tracks @xmath64 has a history of @xmath83 association maps and corresponding kinematic state densities @xmath84 .",
    "the weight @xmath85 , therefore , can be considered as the probability of the hypothesis @xmath86 .",
    "denote the collection of all label sets with @xmath87 unique elements by @xmath88 , the cardinality distribution of a @xmath1-glmb rfs is given by @xmath89    a @xmath0-glmb is completely characterized by the set of parameters @xmath90 . for implementation",
    "it is convenient to consider the set of parameters as an enumeration of all @xmath0-glmb components ( with positive weight ) together with their associated weights and track densities @xmath91 , as shown in figure  [ fig : paramtable ] , where @xmath92 and @xmath93 .",
    "-glmb parameter set with each component indexed by an integer @xmath16 .",
    "the hypothesis  for component @xmath16 is @xmath94 while its weight and associated track densities are @xmath95 and @xmath96 , @xmath97 . ]    given a @xmath1-glmb initial density , all subsequent multi - object densities are @xmath1-glmbs and can be computed exactly by a tractable filter called the @xmath0-glmb filter .",
    "the @xmath0-glmb filter recursively propagates a @xmath0-glmb density forward in time via the bayes recursion equations and .",
    "closed form solutions to the prediction and update of the @xmath0-glmb filter are given in the following propositions @xcite .",
    "[ prop_ck_strong ] if the multi - target posterior at time @xmath98 is a @xmath1-glmb of the form , i.e. @xmath99^{\\!\\mathbf{x}_{}}\\!\\!\\!,\\end{aligned}\\ ] ] and the birth density @xmath100 is defined on @xmath101 according to @xmath102^{\\!\\mathbf{y}},\\ ] ] then the multi - target prediction density to the next time is a @xmath0-glmb given by @xmath103^{\\!\\mathbf{x}_{}}\\!\\!\\!,\\end{aligned}\\ ] ] where@xmath104^{\\!l}\\!\\!\\!\\sum_{i_{_{\\!}k_{\\!}-_{\\!}1_{\\!}}\\supseteq_{_{\\!}}l}\\!\\!\\left[_{\\!}1\\!-\\!\\eta_{_{\\!}s\\!}^{_{\\!}(_{\\!}\\xi_{_{\\!}k_{\\!}-_{\\!}1\\!})\\!}\\right]^{\\!i_{_{\\!}k_{\\!}-_{\\!}1\\!}-l}\\label{eq : propckstrongws } \\\\",
    "\\!\\!\\!\\!\\!\\eta_{_{\\!}s\\!}^{_{\\!}(_{\\!}\\xi_{_{\\!}k_{\\!}-_{\\!}1\\!})\\!}(_{\\!}\\ell)\\!\\!\\!\\!\\!&=&\\!\\!\\!\\!\\!\\left\\langle p_{_{\\!}s}(\\cdot , \\ell ) , p_{_{\\!}k_{\\!}-_{\\!}1}^{_{\\!}(_{\\!}\\xi_{_{\\!}k_{\\!}-_{\\!}1\\!})\\!}(_{\\!}\\cdot , \\ell)\\right\\rangle   \\label{eq : propckstrong_eta } \\\\ \\!\\!\\!\\!\\!p_{_{\\!}k_{_{\\!}}|_{_{\\!}}k_{\\!}-_{\\!}1_{\\!}}^{_{\\!}(_{\\!}\\xi_{_{\\!}k_{\\!}-_{\\!}1\\!})_{\\!}}(_{\\!}x,\\ell_{_{\\!}})\\!\\!\\!\\!\\!&=&\\!\\!\\!\\!\\!1_{\\mathbb{l}_{\\!}}(_{\\!}\\ell_{_{\\!}})p_{_{\\!}s}^{_{\\!}(_{\\!}\\xi_{_{\\!}k_{\\!}-_{\\!}1\\!})_{\\!}}(_{\\!}x,\\ell_{_{\\!}})+1_{_{\\!}\\mathbb{b}_{\\!}}(_{\\!}\\ell_{_{\\!}})p_{_{\\!}b_{_{\\!}}}(_{\\!}x,\\ell_{_{\\!}})\\label{eq : propckstrongpp } \\\\",
    "\\!\\!\\!\\!\\!p_{_{\\!}s}^{_{\\!}(_{\\!}\\xi_{_{\\!}k_{\\!}-_{\\!}1\\!})_{\\!}}(_{\\!}x,\\ell_{_{\\!}})\\!\\!\\!\\!\\!&=&\\!\\!\\!\\!\\!\\frac{\\left\\langle p_{_{\\!}s}(\\cdot , \\ell)f_{_{\\!}k_{_{\\!}}|_{_{\\!}}k_{\\!}-_{\\!}1_{\\!}}(x|\\cdot , \\ell ) , p_{_{\\!}k_{\\!}-_{\\!}1}^{_{\\!}(_{\\!}\\xi_{_{\\!}k_{\\!}-_{\\!}1\\!})\\!}(\\cdot , \\ell ) \\right\\rangle}{\\eta_{_{\\!}s\\!}^{_{\\!}(_{\\!}\\xi_{_{\\!}k_{\\!}-_{\\!}1\\!})\\!}(\\ell ) }   \\label{eq : propckstrongps}\\end{aligned}\\ ] ]    [ propbayes_strong ] given the prediction density in , the multi - target posterior is a @xmath0-glmb given by@xmath105 ^{\\!\\mathbf{x}_{}}\\!\\!\\ ! ,   \\label{eq : propbayes_strong0}\\end{aligned}\\ ] ] where @xmath106^{\\!i_{_{\\!}k } } ,   \\label{eq : propbayes_strong1 } \\\\",
    "\\eta_{_{\\!}z_{_{\\!}k}}^{_{\\!}(_{\\!}\\theta_{_{\\!}k\\!})}(\\ell)\\!\\!\\ ! & = & \\!\\!\\!\\left\\langle p_{_{\\!}k_{_{\\!}}|_{_{\\!}}k_{\\!}-_{\\!}1{\\!}}^{_{\\!}(_{\\!}\\xi_{_{\\!}k_{\\!}-_{\\!}1\\!})\\!}(\\cdot , \\ell ) , \\psi_{_{\\!}z_{_{\\!}k}\\!}(\\cdot , \\ell ; \\theta_{_{\\!}k\\ ! } ) \\right\\rangle ,   \\label{eq : propbayes_strong2 } \\\\",
    "p_{k}^{_{\\!}(_{\\!}\\xi_{_{\\!}k_{\\!}-_{\\!}1\\!},\\theta_{_{\\!}k\\!})}(x,\\ell |z_k)\\!\\!\\ ! & = & \\!\\!\\!\\frac{p_{_{\\!}k_{_{\\!}}|_{_{\\!}}k_{\\!}-_{\\!}1{\\!}}^{_{\\!}(_{\\!}\\xi_{_{\\!}k_{\\!}-_{\\!}1\\!})\\!}(x,\\ell)\\psi_{_{\\!}z_{_{\\!}k}\\!}(x,\\ell;\\theta_{_{\\!}k\\!})}{\\eta_{_{\\!}z_{_{\\!}k}}^{_{\\!}(_{\\!}\\theta_{_{\\!}k\\!})}(\\ell ) } .",
    "\\label{eq : propbayes_strong3}\\end{aligned}\\ ] ]    the propagations of @xmath1-glmb components through prediction and update are illustrated in fig .",
    "[ fig : pred_propag ] and fig .",
    "[ fig : upd_propag ] , respectively .",
    "it is clear that the the number of components grows exponentially with time .",
    "specifically , a component in the filtering density at time @xmath98 generates a large number of predicted components , of which each one in turn produces a new set of multiple @xmath1-glmb components in the filtering density at time @xmath34 .",
    "hence , it is necessary to reduce the number of @xmath1-glmb components in both prediction and update densities at every time step .",
    "-glmb prediction @xcite : component @xmath16 in the prior generates a large set of predicted components with @xmath107 , i.e. @xmath108 ,  , @xmath109 , and @xmath110 . ]    -glmb update @xcite : component @xmath16 in the predicted density generates a ( large ) set of update components with @xmath111 and weights @xmath112,  ,@xmath113 . ]",
    "the simplest way to truncate a @xmath1-glmb density is discarding components with smallest weights .",
    "the following proposition asserts that this strategy minimizes the @xmath114-distance between the true density and the truncated one @xcite    [ prop_l1_error]let @xmath115 denote the @xmath114-norm of @xmath30 , and for a given @xmath116 let @xmath117 ^{\\mathbf{x}}\\]]be an unnormalized @xmath0-glmb density .",
    "if @xmath118 then@xmath119",
    "in this section , we briefly review the original implementation of the @xmath1-glmb filter in subsection  [ subsec : orig_scheme ] and propose a new implementation strategy with joint prediction and update in subsection  [ subsec : new_scheme ] .",
    "the first implementation of the @xmath1-glmb filter , detailed in @xcite , recursively calculates the filtering density by sequentially computing the predicted and update densities at each iteration based on proposition  [ prop_ck_strong ] and proposition  [ propbayes_strong ] . since direct implementation of equations and",
    "is difficult due to the sum over supersets in , the predicted and update densities are rewritten as and , respectively , with @xmath120^{\\!i_{_{\\!}k_{\\!}-_{\\!}1\\!}-_{\\!}l\\!}\\!\\left[_{\\!}\\eta_{_{\\!}s}^{_{\\!}(_{\\!}\\xi_{_{\\!}k_{\\!}-_{\\!}1\\!})\\!}\\right]^{\\!l\\!}.\\ ] ]    c _ _ k__|__k_-_1(___|__z__k_-_1)=_(__)_i__k_-_1,__k_-_1 , l , j 1___(_i__k_-_1)(_l_)1___(___)(_j)__k_-_1^_(_i__k_-_1,__k_-_1)__s^_(_i__k_-_1,__k_-_1)(_l_)__b(_j)___lj__((__))^_[eq : propck_strong3 ] + _ _ k(__z__k)= [ eq : propbayes_strong4 ]    in the prediction stage , each component @xmath121 with weight @xmath122 generates a set of prediction components @xmath123 with weight @xmath124 where @xmath125 and",
    "@xmath126 represent two disjoint label sets for survival and birth tracks , respectively .",
    "since the weight of the prediction component can be factorized into two factors , @xmath127 and @xmath128 , which depend on two mutually exclusive sets ; truncating the predicted density is performed by solving two separate @xmath2-shortest path problems for each set of tracks .",
    "this is because running only one instance of the @xmath2-shortest path based on the augmented set of existing and birth tracks generally favours the selection of survival tracks over new births and typically results in poor track initiation .    in the update stage ,",
    "each prediction component @xmath123 generates a ( large ) set of update components @xmath129 .",
    "these update components are truncated without having to exhaustively compute all the components by solving a ranked assignment problem .",
    "although the original two - staged implementation is intuitive and highly parallelizable , it is has several drawbacks .",
    "first , since truncation of the predicted @xmath0-glmb density is performed separately from the update based purely on _ a priori _ knowledge ( e.g. survival and birth probabilities ) , in general , a significant portion of the predicted components would generate updated components with negligible weights .",
    "hence , computations are wasted in solving a large number of ranked assignment problems , each of which has cubic complexity in the number of measurements .",
    "second , it would be very difficult to determine the final approximation error of the truncated filtering density as the implementation involves least three separate truncating processes : one for existing tracks , one for birth tracks , and one for predicted tracks .    in the following subsections",
    ", we will introduce the joint prediction and update as a better alternative to the original two - staged approach .",
    "the joint strategy eliminates the need for separate prediction truncating procedures , thus involves only one truncation per iteration .",
    "consequently , the new implementation yields considerable computational savings while preserving the filtering performance as well as the parallelizability of the original implementation .      instead of computing the filtering density in two steps ,",
    "the new strategy aims to generate the components of the filtering density in one combined step by formulating a direct relationship between the component of the current filtering density with those of the previous density .",
    "specifically , we will derive a new formulation for @xmath130 that does not involve prediction induced variables @xmath125 and @xmath126 .",
    "this can be done via an _ extended association map _ , denoted by @xmath131 , and defined as follows    the extended association map is a function @xmath132 such that @xmath133 for @xmath134 implies @xmath56 .",
    "the new map , in essence , only extends the original map to include a new association , @xmath135 .",
    "in particular , @xmath131 is identical to @xmath136 except for non - survival and unconfirmed birth tracks , i.e. @xmath137    the image of a set @xmath138 through the extended map @xmath139 and the collection of all eligible extended association maps on domain @xmath64 are denoted by @xmath140 and @xmath141 , respectively .",
    "based on the notion of extended association map , the following proposition establishes the direct relationship between two consecutive filtering densities at time @xmath34 and @xmath98 . for simplicity",
    ", we assume that target births are modeled by ( labeled ) multi - bernoulli rfs s , i.e. @xmath142^{\\mathbb{b}-j}[r(\\cdot)]^j$ ] with @xmath143 denotes the existence probability of track @xmath144 .",
    "[ prop_joint ] if the multi - target posterior at time @xmath98 is a @xmath0-glmb of the form and the set of targets born at the next time is distributed as a labeled multi - bernoulli rfs , then the multi - target posterior at the next time is a @xmath0-glmb given by @xmath145^{\\!\\mathbf{x}_{}}\\end{aligned}\\ ] ] where @xmath146 and @xmath147^{\\!i_{_{\\!}k_{\\!}-_{\\!}1\\!}\\cup_{_{\\ ! } } \\mathbb{b}}\\\\ & \\!\\gamma_{_{\\!}z_{_{\\!}k}}^{_{\\!}(_{\\!}\\tilde{\\theta}_{_{\\!}k\\!}(_{\\!}\\ell^{_{\\!}})_{\\!})\\!}(_{\\!}\\ell^{_{\\!}})\\!=\\!\\begin{cases } 1_{\\!}-_{\\!}{\\eta}_{_{\\!}s}^{_{\\!}(_{\\!}\\xi_{k\\!-\\!1\\!}^{})}\\!(_{\\!}\\ell^{_{\\ ! } } ) & \\!\\forall\\ell\\!\\in\\ ! i_{_{\\!}k_{\\!}-_{\\!}1}\\!\\ ! : \\tilde{\\theta}_{_{\\!}k_{\\!}}(_{\\!}\\ell^{_{\\!}})\\!=\\!|z_{\\!k}|\\!+\\!1 , \\\\",
    "{ \\eta}_{_{\\!}s}^{_{\\!}(_{\\!}\\xi_{k\\!-\\!1\\!}^{})}\\!(_{\\!}\\ell^{_{\\!}}){\\eta}_{_{\\!}z_{_{\\!}k}}^{_{\\!}(_{\\!}{\\theta}_{_{\\!}k\\!})\\!}(_{\\!}\\ell^{_{\\ ! } } ) & \\!\\forall\\ell\\!\\in\\!i_{_{\\!}k_{\\!}-_{\\!}1}\\!\\ ! : \\tilde{\\theta}_{_{\\!}k_{\\!}}(_{\\!}\\ell^{_{\\!}})\\!<\\!|z_{\\!k}|\\!+\\!1 , \\\\   1_{\\!}-_{\\!}r(_{\\!}\\ell^{_{\\ ! } } ) & \\!\\forall\\ell\\!\\in\\!\\mathbb{b}\\ ! : \\tilde{\\theta}_{_{\\!}k_{\\!}}(_{\\!}\\ell^{_{\\!}})\\!=\\!|z_{\\!k}|\\!+\\!1 , \\\\ r(_{\\!}\\ell^{_{\\!}}){\\eta}_{_{\\!}z_{_{\\!}k}}^{_{\\!}(_{\\!}{\\theta}_{_{\\!}k\\!})\\!}(_{\\!}\\ell^{_{\\ ! } } ) & \\!\\forall\\ell\\!\\in\\!\\mathbb{b}\\ ! : \\tilde{\\theta}_{_{\\!}k_{\\!}}(_{\\!}\\ell^{_{\\!}})\\!<\\!|z_{\\!k}|\\!+\\!1",
    ". \\label{eq : gamma_def } \\end{cases}\\end{aligned}\\ ] ]    we now proceed to detail an efficient implementation of the @xmath1-glmb filter based on the result in proposition  [ prop_joint ] .",
    "let the sets of existing tracks , birth tracks , and measurements be enumerated by @xmath148 , @xmath149 , and @xmath150 , respectively .",
    "given the @xmath16-th component @xmath151 , the objective is to find @xmath152 extended associations @xmath153 that produce highest update weights .",
    "denote by @xmath154 a @xmath155 matrix whose entries are either @xmath156 or @xmath157 such that the sum of each row is exactly @xmath156 while the sum of each column is at most @xmath156 .",
    "the matrix @xmath154 is called an _",
    "assignment matrix _ since it represents a valid extended association map @xmath158 .",
    "hence , finding the desirable extended associations is equivalently translated to finding the corresponding assignment matrices . the simplest way to determine these matrices without exhaustingly computing",
    "all of the update weights is to assign an appropriate cost for each assignment matrix and then rank these matrices in non - decreasing order of their costs via murty s algorithm .",
    "let @xmath159 be a matrix whose @xmath160 entry , with @xmath161 and @xmath162 , is defined as follows . @xmath163 as depicted in fig .",
    "[ fig : contr_mat ] , the matrix @xmath159 contains all possible values of @xmath164 that any valid extended associations @xmath165 can generate .",
    "intuitively speaking , each entry of @xmath159 is an indicator of how likely an extended association is assigned to a track ; hence , the matrix @xmath166 is called the cost matrix and cost function for a particular assignment matrix @xmath154 is given by @xmath167 it is straightforward to show that the ( unnormalized ) weight of the @xmath168-generated component in the filtering density at time @xmath34 is @xmath169.\\ ] ]    based on and , an implementation of the @xmath1-glmb filter with joint prediction and update is given in algorithm  [ alg : joint_murty ] , where the subroutine @xmath170 uses murty s algorithm @xcite to generate a sequence of @xmath152 assignment matrices that yield lowest costs ( or equivalently , highest update weights ) without exhaustively navigating the whole assignment space .    @xmath171 @xmath172 compute @xmath173 according to @xmath174 @xmath175 compute @xmath176 according to compute @xmath177 according to normalize weights @xmath178    similar to the original implementation , the joint prediction and update also operates independently on each components in the filtering density , thereby is highly parallelizable .      in multi - target tracking ,",
    "truncating procedures based on the original murty s algorithm with complexity @xmath179 , where @xmath180 is the number of assignments and @xmath181 is the number of the measurements , have been proposed in danchicknewnam93 , @xcite , @xcite .",
    "more efficient algorithms with @xmath182 complexity have been proposed in @xcite , pascoaletal03 , @xcite , with the latter showing better efficiency for large @xmath183 .",
    "the main drawback of these approaches is that a significant amount of computation is used to sort the data associations in a particular order despite the fact that order is effectively discarded after the update .",
    "furthermore , the number of desired components must be predetermined , generally by a large enough number to capture all important associations .",
    "if the number of significant components in the filtering density is much smaller than the chosen threshold , many insignificant components are generated that waste a lot of computation at the next iteration .",
    "conversely if the number of significant component exceeds the chosen threshold , the filtering performance will likely degrades in subsequent iterations .",
    "nonetheless a deterministic polynomial time solution is thus appealing in the sense that convergence and reproducibility is guaranteed without having to enumerate all possible solutions .    in this subsection",
    ", we propose an alternative to the ranked assignment based solution .",
    "our proposed solution is based on stochastic simulation or markov chain monte carlo methods via a gibbs sampler which directly address the above mentioned drawbacks .",
    "conceptually , instead of ranking extended associations in non - increasing order of their weights , each extended association is treated as a realization of a ( discrete ) random variable , where the probability of each extended association is proportional to the weight of its associated @xmath1-glmb component in the next filtering density .",
    "candidate extended associations are then generated by sampling from this discrete distribution . extended associations with high weights are chosen more often than those with low weights in a statistically consistent manner .",
    "consequently these samples of extended associations are more statistically diverse than those from obtained from a deterministic approach such as the ranked optimal assignment .    using the same enumeration for tracks and measurement as in the previous section , a valid extended association map @xmath131 for each component",
    "@xmath184 can be represented as a vector @xmath185^t \\in \\{0,1,\\ldots , m\\!+\\!1\\}^{p}$ ] .",
    "the key idea of the stochastic based approach is that @xmath131 can be considered as realizations of a random variable in the space @xmath186 with the following distribution @xmath187where @xmath188,\\label{eq : theta_req}\\\\ \\omega _ { k}^{(i_{k-1}^{(h)},\\xi_{k-1}^{(h)},\\tilde{\\theta})}&\\propto \\omega _ { k\\!-\\!1}^{(i_{k\\!-\\!1}^{(h)},\\xi _ { k\\!-\\!1}^{(h)})}\\!\\left[\\gamma_{_{\\!}z_{_{\\!}k}\\!}^{_{\\!}(_{\\!}\\tilde{\\theta}(\\cdot)_{\\!})\\!}(\\cdot ) \\right ] ^{\\!i_{k\\!-\\!1}\\cup \\mathbb{b}},\\label{eq : omeg_theta}\\end{aligned}\\]]with @xmath189 and @xmath190 is given in .",
    "thus the probability of a valid extended association is proportional to the weight of the corresponding @xmath1-glmb component in the next filtering density while zero probability is allocated to extended associations which do not satisfy the constraint that each measurement is assigned to at most one track .",
    "however , sampling directly from the distribution is very difficult since we can not exhaustively compute all of the values of @xmath191 .",
    "a common solution to this kind of problem is to use markov chain monte carlo ( mcmc ) methods such as the gibbs sampler to obtain samples from without having to directly compute @xmath191 .",
    "the gibbs sampler is a very efficient method to sample a difficult distribution if its conditional marginals can be computed in a simple closed form @xcite with proven convergence under generally standard assumptions @xcite .",
    "the main theoretical contribution in this section is stated in the following proposition , which allows conditional marginals to be computed via the entries of the matrix @xmath159 .",
    "[ marg_cond ] denote by @xmath192 the @xmath87-th element of @xmath131 , i.e. @xmath193 , and @xmath194 all the other elements except @xmath192 , i.e. @xmath195^t$ ] .",
    "then , the conditional marginal @xmath196 is given by @xmath197\\!\\gamma_{_{\\!}z_{_{\\!}k}}^{_{\\!}(_{\\!}\\tilde{\\theta}_{_{\\!}n\\!})\\!}(_{\\!}\\ell_{_{\\!}n\\!}).\\ ] ]    propostion  [ marg_cond ] simply states that the conditional probability @xmath196 are zero when @xmath198 is inconsistent with @xmath199 , thus ensuring that each measurement can be assigned to at most on track .",
    "the conditional probabilities are otherwise proportional to @xmath200 .    with these conditionals the pseudo code for @xmath0-glmb filtering with the gibbs sampler",
    "is given in algorithm  [ alg : gibbs ] where the gibbs sampler is used directly in place of the ranked optimal assignment .",
    "@xmath201 @xmath202 @xmath203 compute @xmath173 according to initialize @xmath204^t\\gets\\left[\\tilde{\\theta}_{_{\\!}k}^{_{\\!}(_{\\!}h,0_{\\!})\\!}(_{\\!}\\ell_{_{\\!}1\\ ! } ) , \\ldots,\\tilde{\\theta}_{_{\\!}k}^{_{\\!}(_{\\!}h,0_{\\!})\\!}(_{\\!}\\ell_{_{\\!}p\\!})\\right]^t$ ] compute @xmath205 according to @xmath206 @xmath207 @xmath208 @xmath209 $ ] @xmath210    in order to produce one sample , algorithm  [ alg : gibbs ] requires a gibbs sequence of length @xmath211 , starting from an arbitrary initialization @xmath212^t$ ] , to be generated .",
    "alternatively , we can sample a long gibbs sequence of length @xmath213 and then extract every @xmath211-th sample @xcite .",
    "the length of the gibbs sequence , roughly speaking , depends on the convergence rate of the gibbs sampler and the distance from the initial point to the true sample space .",
    "if we start with a good initialization right in the true sample space , we can use all the samples from the gibbs sequence @xcite . in practice ,",
    "one example of good initialization that allows us to use all of the samples from the resulting gibbs sequence is the optimal assignment , which can be obtained via either munkres @xcite or jonker - volgenant algorithm @xcite .",
    "otherwise , we can start with all zeros assignment ( i.e. all tracks are misdetected ) that is also valid sample and requires no additional computation .    in terms of computational complexity ,",
    "sampling from a discrete distribution is linear with the weight s length @xcite , therefore the total complexity of the gibbs sampling procedure presented in algorithm  alg : gibbs is @xmath214 . in comparison ,",
    "the fastest ranked optimal assignment algorithm is @xmath215 @xcite . for general multi - target tracking problems in practice",
    ", we usually have @xmath216 , thus the gibbs sampling algorithm will generally be much faster than the ranked assignment given the same @xmath217 .",
    "in this section we first compare the performance of the joint prediction and update approach with its traditional separated counterpart , both employ the ranked assignment algorithm for fair comparison .",
    "then , we illustrate the superior performance of the gibbs sampler based truncation to the conventional ranked assignment via a difficult tracking scenario with low detection probability and very high clutter rate .",
    "the first numerical example is based on a scenario adapted from vvp_glmb13 in which a varying number targets travel in straight paths and with different but constant velocities on the two dimensional region @xmath218",
    "m \\times [ -1000,1000]m $ ] .",
    "the duration of the scenario is @xmath219 .",
    "there is a crossing of 3 targets at the origin at time @xmath220 , and a crossing of two pairs of targets at position @xmath221 at time @xmath222 .",
    "the region and tracks are shown in figure  [ fig : traj ] .     plane .",
    "start / stop positions for each track are shown with @xmath223/@xmath224 . ]",
    "the kinematic target state is a vector of planar position and velocity @xmath225^t$ ] .",
    "measurements are noisy vectors of planar position only @xmath226^t$ ] .",
    "the single - target state space model is linear gaussian according to transition density @xmath227 and likelihood @xmath228 with parameters @xmath229 where @xmath230 and @xmath231 denote the @xmath232 identity and zero matrices respectively , @xmath233 is the sampling period , @xmath234 and @xmath235 are the standard deviations of the process noise and measurement noise .",
    "the survival probability is @xmath236 = 0.99 and the birth model is a labeled multi - bernoulli rfs with parameters @xmath237 where @xmath238 and @xmath239 with @xmath240^t , m^{\\!(\\!2\\!)}_b= [ -100,0,-100,0]^t , m^{\\!(\\!3\\!)}_b= [ 100,0,-100,0]^t , p_b = \\text{diag } ( [ 10 , 10 , 10 , 10 ] ^t)^2 $ ] .",
    "the detection probability is @xmath241 and clutter follows a poisson rfs with an average intensity of @xmath242 giving an average of @xmath243 false alarms per scan .",
    "first , we compare the performance of the traditional separated and the proposed joint prediction and update approaches . for a fair comparison , both approaches are capped to the same maximum components .",
    "results are shown over 100 monte carlo trials .",
    "figures  [ fig : card ] shows the mean and standard deviation of the estimated cardinality versus time .",
    "figures  fig : ospa_comb and [ fig : ospa_sep ] show the ospa distance @xcite and its localization and cardinality components for @xmath244 and @xmath245 .",
    "it can be seen that both approaches estimate the cardinality equally well .",
    "similarly , in terms of ospa distance , the performance of the two approach is virtually the same .",
    "second , we demonstrate the fast implementation via the gibbs sampler . in this example , we keep all parameters the same as in the previous example except that the clutter rate is now increased to average @xmath246 false alarms per scan .",
    "the performance of the gibbs sampler implementation is compared with that of a ranked assignment based implementation with the same maximum number of posterior hypotheses .",
    "the average ospa distances over 100 monte carlo trials are presented in fig .",
    "[ fig : ospa_100 ] .    ) . ]",
    "it is obvious that the gibbs sampler has a better ospa from around time @xmath247 onward .",
    "the reason is in difficult scenario ( e.g. high clutter rate , low detection probability ) , if the number of existing targets are high the gibbs sampling technique is expected to pick up the new born target better than the ranked assignment algorithm given the same number of samples / hypotheses due to its randomized behaviour .",
    "this is clearly illustrated in the cardinality statistics for both approaches in fig .",
    "fig : card_gibbs and fig .",
    "[ fig : card_murty ] . as expected , however , the joint approach averaged run time is significantly lower that that of the original approach .",
    "reductions in execution time of 1 to 2 orders of magnitude are typical .",
    "in this paper we propose a new implementation scheme for the @xmath1-glmb filter that allows joint prediction and update .",
    "in contrast to the conventional two - staged implementation , the joint approach use _ a posteriori _ information to construct cost matrices for every individual track , thereby requires only one truncation in each iteration due to the elimination of inefficient intermediate steps . more importantly",
    ", this joint strategy provides the platform for the development of an accelerated randomized truncation procedure that achieves superior performance as compared to that of its traditional deterministic counterpart .",
    "the proposed method is also applicable to approximations of the @xmath1-glmb filter such as the labeled multi - bernoulli ( lmb ) filter @xcite .",
    "m.  tobias and a.  d. lanterman , `` probability hypothesis density - based multitarget tracking with bistatic range and doppler observations , '' _ iee proceedings - radar , sonar and navigation _ , vol .",
    "152 , no .  3 , pp .",
    "195205 , june 2005 .      d.  e. clark and j.  bell , `` bayesian multiple target tracking in forward scan sonar images using the phd filter , '' _ iee proceedings - radar , sonar and navigation _ , vol .",
    "152 , no .  5 , pp .",
    "327334 , october 2005 .",
    "r.  hoseinnezhad , b .-",
    "vo , and b.  t. vo , `` visual tracking in background subtracted image sequences via multi - bernoulli filtering , '' _ ieee trans .",
    "signal process .",
    "_ , vol .",
    "61 , no .  2 ,",
    "392397 , jan 2013 .",
    "s.  rezatofighi , s.  gould , b.  vo , b.  vo , k.  mele , and r.  hartley , `` multi - target tracking with time - varying clutter rate and detection profile : application to time - lapse cell microscopy sequences , '' _ ieee trans . med .",
    "_ , vol .",
    "34 , no .  6 , pp .",
    "13361348 , 2015 .      c.  lundquist , l.  hammarstrand , and f.  gustafsson , `` road intensity based mapping using radar measurements with a probability hypothesis density filter , '' _ ieee trans . signal process .",
    "_ , vol .",
    "59 , no .  4 , pp . 13971408 , april 2011 .",
    "g.  battistelli , l.  chisci , s.  morrocchi , f.  papi , a.  benavoli , a.  di  lallo , a.  farina , and a.  graziano , `` traffic intensity estimation via phd filtering , '' in _ proc .",
    "2008 european radar conference ( eurad ) _ , oct 2008 , pp . 340343 .",
    "a.  gostar , r.  hoseinnezhad , and a.  bab - hadiashar , `` robust multi - bernoulli sensor selection for multi - target tracking in sensor networks , '' _ ieee signal process .",
    "_ , vol .  20 , no .  12 , pp .",
    "11671170 , dec 2013 .",
    "g.  battistelli , l.  chisci , c.  fantacci , a.  farina , and a.  graziano , `` consensus cphd filter for distributed multitarget tracking , '' _ ieee j. sel .",
    "topics signal process . _ , vol .  7 , no .  3 , pp .",
    "508520 , june 2013 .",
    "m.  pascoal , m.  captivo , and j.  clmaco , `` a note on a new variant of murty s ranking assignments algorithm , '' _",
    "4or : quarterly journal of the belgian , french and italian operations research societies _ , vol .  1 , no .  3 , pp .",
    "243255 , 2003 .",
    "i.  cox and s.  hingorani , `` an efficient implementation of reid s multiple hypothesis tracking algorithm and its evaluation for the purpose of visual tracking , '' _ ieee trans . pattern anal .",
    "_ , vol .  18 , no .  2 , pp .",
    "138150 , 1996 .",
    "a.  frigessi , p.  d. stefano , c .- r .",
    "hwang , and s .- j .",
    "sheu , `` convergence rates of the gibbs sampler , the metropolis algorithm and other single - site updating dynamics , '' _ journal of the royal statistical society .",
    "series b ( methodological ) _ , vol .",
    "55 , no .  1 ,",
    "pp . 205219 , 1993 .",
    "g.  roberts and a.  smith , `` simple conditions for the convergence of the gibbs sampler and metropolis - hastings algorithms , '' _ stochastic processes and their applications _ ,",
    "49 , no .  2 ,",
    "pp . 207216 , 1994 .",
    "c.  j. geyer and e.  a. thompson , `` constrained monte carlo maximum likelihood for dependent data , '' _ journal of the royal statistical society .",
    "series b ( methodological ) _ , vol .",
    "54 , no .  3 , pp .",
    "657699 , 1992 ."
  ],
  "abstract_text": [
    "<S> this paper proposes an efficient implementation of the generalized labeled multi - bernoulli ( glmb ) filter by combining the prediction and update into a single step . </S>",
    "<S> in contrast to the original approach which involves separate truncations in the prediction and update steps , the proposed implementation requires only one single truncation for each iteration , which can be performed using a standard ranked optimal assignment algorithm . furthermore , we propose a new truncation technique based on markov chain monte carlo methods such as gibbs sampling , which drastically reduces the complexity of the filter . </S>",
    "<S> the superior performance of the proposed approach is demonstrated through extensive numerical studies .    </S>",
    "<S> random finite sets , delta generalized labeled multi - bernoulli filter </S>"
  ]
}