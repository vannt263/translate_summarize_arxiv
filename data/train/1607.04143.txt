{
  "article_text": [
    "consider a set @xmath2 of @xmath0 discrete memoryless sources with a known joint probability mass function .",
    "subsets of @xmath1 sources are sampled at each time instant , and jointly processed with the objective of reconstructing _ all _ the @xmath0 sources as compressed representations , within a specified level of distortion",
    ". how should the sampler optimally sample the sources in a causal manner to yield the best compression rate for a given distortion level ? what are the tradeoffs",
    " under optimal processing  among the sampling procedure ,",
    "compression rate and distortion level ?",
    "this paper is our preliminary attempt at answering these questions .",
    "the study of problems of combined sampling and compression has a rich and varied history in diverse contexts .",
    "highlights include : classical sampling and processing , rate distortion theory , multiterminal source coding , wavelet - based compression , and compressed sensing , among others .",
    "rate distortion theory @xcite rules the compression of a given sampled signal and its reconstruction within a specified distortion level . on the other hand , compressed sensing @xcite provides a random linear encoding of nonprobabilistic analog sources marked by a sparse support , with lossless recovery as measured by a block error probability ( with respect to the distribution of the encoder ) . upon placing the problem of lossless source coding of analog sources in an information theoretic setting , with a probabilistic model for the source that need",
    "not be encoded linearly , rnyi dimension is known to determine fundamental performance limits @xcite ( see also @xcite ) .",
    "several recent studies consider the compressed sensing of a signal with an allowed detection error rate or quantization distortion @xcite , or with denoising @xcite ; of multiple signals followed by distributed quantization @xcite , including a study of scaling laws @xcite ; or of sub - nyquist rate sampled signals followed by lossy reconstruction @xcite .",
    "closer to the line of our work , the rate distortion function has been characterized when multiple gaussian signals from a random field are sampled and quantized ( centralized or distributed ) in @xcite . also , in a series @xcite ( see also @xcite , @xcite ) , various aspects of random field - sampling and reconstruction for special models are considered . in a setting of distributed acoustic sensing and reconstruction , centralized as well as distributed coding schemes and sampling lattices are studied , and their performance is compared with corresponding rate distortion bounds @xcite . in @xcite , considering a gaussian random field on the interval @xmath3 $ ] and i.i.d . in time , reconstruction of the entire field from compressed versions of @xmath4 sampled sequences under the mean - squared error distortion criterion is studied . in a different formulation , for the case of @xmath5 sources , each of which is sampled for a fixed proportion of time , the rate distortion function and associated sampling mechanism are characterized in @xcite .",
    "our work differs from the approaches above in that we allow randomized sampling that can depend on the observed source values , and no sparsity assumption is made on the sources .",
    "it bears emphasis that we deal with centralized  and not distributed  processing of the sources .",
    "our contributions are as follows .",
    "we consider a new formulation involving a _ sampling rate distortion function _",
    "( srdf ) , which combines a sampling of sources and lossy compression , to address the questions posed at the outset . as a basic ingredient",
    ", the sampling rate distortion function is characterized for a fixed sampling set of size @xmath1 .",
    "this characterization is based on prior work by dobrushin - tsybakov @xcite ( see also berger @xcite , @xcite and yamamoto - itoh @xcite ) on the rate distortion function for a `` remote '' source - receiver model in which the encoder and receiver lack direct access to the source and decoder outputs , respectively .",
    "for the special case of the probability of error distortion criterion , we show that the optimal procedure can be simplified to a rate distortion code for the sampled sources followed by maximum a posteriori estimation of the remaining sources .",
    "best fixed - set sampling can be strictly inferior to random sampling . considering an independent random sampler , in which the sampling does not depend on the source outputs and",
    "is independent ( but not necessarily identically distributed ) in time , we show that the corresponding srdf remains the same regardless of whether or not the decoder is provided information regarding the sequence of sampled sets .",
    "this surprising property does not hold for any causal sampler , in general .",
    "next , we consider a generalization , namely a memoryless random sampler whose output can depend on the source values at each time instant .",
    "the associated formula for srdf is used now to study the structural characteristics of the optimal sampler .",
    "specifically , we show when the decoder too is aware of the sequence of sampled sets that the optimal sampler is characterized by a conditional point - mass ; this has the obvious benefit of a reduction in the search space for an optimal sampler .",
    "we also show that such a memoryless sampler can outperform strictly a random sampler that lacks access to source values .",
    "finally , in a setting in which the decoder is unaware of the sampled sequence , an upper bound for the srdf is seen to have an optimal conditional point - mass sampler .",
    "our models are described in section [ s : preliminaries ] .",
    "the main results , along with examples , are stated in section [ s : results ] .",
    "section [ s : proofs ] contains the proofs .",
    "presented first are the achievability proofs that are built successively in the order of increasing complexity of the samplers .",
    "the converse proofs follow in reverse order in a unified manner .",
    "let @xmath6 and @xmath7 be a @xmath8-valued rv where each @xmath9 is a finite set",
    ". it will be convenient to use the following compact notation . for a nonempty set @xmath10 ,",
    "we denote by @xmath11 the rv @xmath12 with values in @xmath13 , and denote @xmath14 repetitions of @xmath11 by @xmath15 with values in @xmath16 , where @xmath17 takes values in the @xmath14-fold product space @xmath18 . for @xmath19 ,",
    "let @xmath20 be the set of all @xmath4-sized subsets of @xmath2 and let @xmath21 .",
    "all logarithms and exponentiations are with respect to the base 2 .",
    "consider a discrete memoryless multiple source ( dmms ) @xmath22 consisting of i.i.d .",
    "repetitions of the rv @xmath23 with given pmf @xmath24 of assumed full support @xmath25 .",
    "let @xmath26 where @xmath27 is a finite reproduction alphabet for @xmath28 .",
    "[ d : rs ] a @xmath4-_random sampler _ ( @xmath4-rs ) , @xmath19 , collects causally at each @xmath29 , random samples@xmath30    @xmath31 from @xmath32 , where @xmath33 is a rv with values in @xmath34 with ( conditional ) pmf @xmath35 , with @xmath36 and @xmath37 .",
    "such a @xmath4-rs is specified by a ( conditional ) pmf @xmath38 with the requirement @xmath39 the output of a @xmath4-rs is @xmath40 where @xmath41 .",
    "successively restrictive choices of a @xmath4-rs in corresponding to @xmath42 and @xmath43 will be termed the @xmath4-_memoryless _ and the @xmath4-_independent random samplers _ and denoted by @xmath4-mrs and @xmath4-irs , respectively .",
    "[ d : encoder ] an @xmath14-length block code with @xmath4-rs for a dmms @xmath44 with alphabet @xmath45 and reproduction alphabet @xmath46 is a triple @xmath47 where @xmath48 is a @xmath4-rs as in , and @xmath49 are a pair of mappings where the encoder @xmath50 maps the output of the @xmath4-rs into some finite set @xmath51 and the decoder @xmath52 maps @xmath53 into @xmath54 .",
    "we shall use the compact notation @xmath55 suppressing @xmath14 .",
    "the rate of the code with @xmath4-rs @xmath56 is @xmath57 .    for",
    "a given ( single - letter ) finite - valued distortion measure @xmath58 , an @xmath14-length block code with @xmath4-rs @xmath56 will be required to satisfy the expected fidelity criterion ( @xmath59 , @xmath60 ) , i.e. , @xmath61     \\triangleq   \\mathbb{e } \\bigg [ \\dfrac{1}{n } \\sum_{t=1}^{n } d \\bigg ( x_{{{\\mathcal m}}t } , \\big ( \\varphi \\big ( f ( s^{n } , x_{s}^{n } ) \\big ) \\big ) _ { t }   \\bigg ) \\bigg ] \\leq \\delta .",
    "\\end{split}\\end{aligned}\\ ] ]    we shall consider also the case where the decoder is informed of the sequence of sampled sets @xmath62 . denoting such an _ informed decoder _ by @xmath63 , the expected fidelity criterion will use the augmented @xmath64 instead of @xmath65 the earlier decoder ( that is not informed ) will be termed an _",
    "uninformed decoder_.    [ d : rdf ]    a number @xmath66 is an achievable @xmath4-sample coding rate at average distortion level @xmath60 if for every @xmath67 and sufficiently large @xmath14 , there exist @xmath14-length block codes with @xmath4-rs of rate less than @xmath68 and satisfying the expected fidelity criterion @xmath69 ; and @xmath70 will be termed an achievable @xmath4-sample rate distortion pair .",
    "the infimum of such achievable rates is denoted by @xmath71 for an informed decoder , and by @xmath72 for an uninformed decoder .",
    "we shall refer to @xmath73 as well as @xmath74 as the _ sampling rate distortion function _",
    "( srdf ) , suppressing the dependence on @xmath4 .",
    "_ remarks _ : ( i ) clearly , @xmath75 and both are nonincreasing in @xmath4",
    ". + ( ii ) for a dmms @xmath22 , the requirement on the sampler renders @xmath76 and thereby also @xmath77 to be memoryless sequences .",
    "single - letter characterizations of the srdfs in this paper involve , as an ingredient , a characterization of @xmath73 with @xmath78 , where @xmath79 is a _ fixed _ set with @xmath80 .",
    "denote the corresponding @xmath81 by @xmath82 ( with an abuse of notation ) .",
    "the _ fixed - set _",
    "srdf @xmath83 in effect , is the ( standard ) rate distortion function for the dmms @xmath84 using a modified distortion measure @xmath85 defined by @xmath86.\\end{aligned}\\ ] ]    [ th : k - fs - rd ] for a dmms @xmath87 , the fixed - set srdf for @xmath79 is @xmath88 \\leq { \\delta } }   i \\big ( x_{a } \\wedge   y_{{{\\mathcal m } } } \\big ) \\end{aligned}\\ ] ] for @xmath89 , and equals 0 for @xmath90 , where @xmath91 , \\ \\ \\ \\ { \\delta}_{\\rm max } = \\min_{y_{{{\\mathcal m } } } \\in { \\cal y}_{{{\\mathcal m } } } }   \\mathbb{e } \\big [ d ( x_{{{\\mathcal m } } } , y_{{{\\mathcal m } } } ) \\big ] =   \\min_{y_{{{\\mathcal m } } } \\in { \\cal y}_{{{\\mathcal m } } } } \\mathbbm{e } [ d_{a}(x_{a } , y_{{{\\mathcal m } } } ) ] .",
    "\\end{split}\\end{aligned}\\ ] ]    [ cor : discrete - prob - error ] with @xmath92 , for the probability of error distortion measure @xmath93 the srdf is @xmath94 where the minimum in is subject to @xmath95 \\leq \\delta - ( 1 - \\mathbbm{e } [ \\alpha(x_{a } ) ] ) \\ ] ] with @xmath96 and @xmath97 , \\ \\ \\ \\ \\delta _ { { \\rm max } } = 1 - \\max_{x_{{{\\mathcal m } } } \\in { \\cal x}_{{{\\mathcal m}}}}p_{x_{{{\\mathcal m}}}}(x_{{{\\mathcal m}}}).\\end{aligned}\\ ] ]    _ remarks _ : ( i ) the minimum in exists by virtue of the continuity in @xmath98 of @xmath99 over the compact set @xmath100 \\leq \\delta    \\}$ ] .",
    "+ ( ii ) the corollary relies on showing that the minimum in is attained now by a pmf @xmath101 under a longer markov chain @xmath102 interestingly , the achievability proof entails in a first step a mapping of @xmath103 in @xmath104 into its codeword @xmath105 , from which in a second step a reconstruction @xmath106 of @xmath107 is obtained as a maximum a posteriori ( map ) estimate .",
    "the @xmath4-irs affords a more capable mechanism than the fixed - set sampler of proposition [ th : k - fs - rd ] , with the sampling sets possibly varying in time .",
    "surprisingly , the srdf for a @xmath4-irs , displayed as @xmath108 , remains the same regardless of whether or not the decoder is provided information regarding the sequence of sampled sets .",
    "[ th : k - irs - informed - decoder ] for a @xmath4-irs , the srdf is @xmath109 for @xmath110 , where the minimum is with respect to @xmath111 and @xmath112 \\leq { \\delta}$ ] , with @xmath113\\end{aligned}\\ ] ] and @xmath114 as in .    a convenient equivalent expression for @xmath108 in is given by    [ prop : k - irs - equiv ] for a @xmath4-irs , @xmath115 where the minimum is with respect to @xmath116    _ proof .",
    "_ for every @xmath117 , in , @xmath118 \\leq \\delta } { \\min } i(x_{s } \\wedge y_{{{\\mathcal m } } } | s ) & = \\underset { p_{x_{{{\\mathcal m } } } } p_{s } p_{y_{{{\\mathcal m } } } |s x_{s } } \\atop \\mathbbm{e}[d(x_{{{\\mathcal m}}},y_{{{\\mathcal m } } } ) ] \\leq \\delta } { \\min } \\sum_{a \\in { \\cal a}_{k } } p_{s}(a ) i(x_{a } \\wedge y_{{{\\mathcal m}}}|s = a )   \\\\     & = \\underset { p_{s } , \\atop   \\delta_{a } :    \\sum \\limits_{a \\in { \\cal a}_{k } } p_{s } ( a ) \\delta_{a } \\leq \\delta   } { \\min }   \\   \\sum_{a \\in { \\cal a}_{k } } p_{s}(a ) \\underset { p_{y_{{{\\mathcal m } } } | s = a , x_{a } }    \\atop \\mathbbm{e}[d ( x_{{{\\mathcal m}}},y_{{{\\mathcal m } } } ) | s = a ] = \\delta_{a } } { \\min } i(x_{a } \\wedge y_{{{\\mathcal m } } } | s = a ) \\label{eq : th1-ach - eq2 } \\\\      & = \\underset { p_{s } , \\atop \\delta_{a } :     \\sum \\limits_{a \\in { \\cal a}_{k } } p_{s } ( a ) \\delta_{a } \\leq \\delta } { \\min } \\ \\sum_{a \\in { \\cal a}_{k } } p_{s}(a ) r_{a}(\\delta_{a}),\\end{aligned}\\ ] ] where @xmath119 is used to denote @xmath120 for compactness . the validity of follows by the introduction of the @xmath121s and observing that the order of the minimization does not alter the value of the minimum .",
    "the last step obtains upon noting that the value of the inner minimum in is the same upon replacing the equality in @xmath122 = \\delta_{a}$ ] with `` @xmath123 '' .",
    "_ remark _ : by proposition [ prop : k - irs - equiv ] , the srdf for a @xmath4-irs is the lower convex envelope of the set of srdfs @xmath124 and thus is convex in @xmath125 .",
    "furthermore , @xmath126 additionally , a @xmath4-irs can outperform strictly the best fixed - set sampler .",
    "for instance , if there is no fixed - set srdf for any @xmath127 that is uniformly best for all @xmath60 , then the previous inequality can be strict .",
    "this is illustrated by the following example .    with @xmath128 and @xmath129",
    "let @xmath130 be i.i.d .",
    "bernoulli(@xmath131 ) rvs , and @xmath132 with @xmath133 @xmath134 for @xmath135 , @xmath136 whereas @xmath137 where @xmath138 is the binary entropy function .",
    "clearly , @xmath108 is strictly smaller than @xmath139 for @xmath140 see fig .",
    "[ fig : irs_fixed_set ] . note that while the distortion measure @xmath59 in definition [ d : encoder ] is taken to be finite - valued , the event @xmath141 above is accommodated by assigning ( optimally ) zero probability to it .",
    "a @xmath4-mrs is more powerful than a @xmath4-irs in that sampling with the former at each time instant can depend on the current dmms realization .",
    "the srdf for a @xmath4-mrs can improve with an informed decoder unlike for a @xmath4-irs .",
    "[ th : k - mrs - informed - decoder ] for a @xmath4-mrs with informed decoder , the srdf is @xmath142 for @xmath110 , where the minimum is with respect to @xmath143 and @xmath144 \\leq { \\delta}$ ] , with @xmath145    \\right ] , \\end{aligned}\\ ] ] @xmath146     \\right ] , \\",
    "\\ \\ \\end{aligned}\\ ] ] and @xmath147 being a @xmath148-valued rv with @xmath149 .    _ remark _ : analogously as in proposition [ prop : k - irs - equiv ] , the srdf @xmath150 can be expressed as @xmath151 = \\delta_{u } } \\min i(x_{s } \\wedge y_{{{\\mathcal m } } } | s , u = u ) \\label{eq : k - mrs_equiv_form}\\end{aligned}\\ ] ] and thereby equals a lower convex envelope of functions of @xmath60 .",
    "the optimal sampler that attains the srdf in theorem [ th : k - mrs - informed - decoder ] has a simple structure .",
    "it is easy to see that each of @xmath152 and @xmath114 in and , respectively , is attained by a sampler for which @xmath153 takes the form of a conditional point - mass .",
    "such samplers , in fact , are optimal for every distortion level @xmath154 and will depend on @xmath60 , in general .",
    "[ d : point_mass_sampler ] given a mapping @xmath155 , the ( conditional point - mass ) mapping @xmath156 is defined by @xmath157    the following reduction of theorem [ th : k - mrs - informed - decoder ] shows the optimality of conditional point - mass samplers for a @xmath4-mrs which will be seen to play a material role in the achievability proof of theorem [ th : k - mrs - informed - decoder ] .    [",
    "th : k - mrs - informed - alternative ] for a @xmath4-mrs with informed decoder , the srdf equals @xmath158 for @xmath159 with @xmath152 and @xmath114 as in and , respectively , where the minimum is with respect to @xmath160 of the form @xmath161 with @xmath144   \\leq { \\delta},$ ] where the ( time - sharing ) rv @xmath147 takes values in @xmath148 with @xmath162 .",
    "the structure of the optimal sampler in theorem [ th : k - mrs - informed - alternative ] implies that the search space for minimization now can be reduced to the corner points of the simplexes of the conditional pmfs @xmath163 .",
    "the srdf in is thus the lower convex envelope of the srdfs for conditional point - mass samplers . in general ,",
    "time - sharing between such samplers will be seen to achieve the best compression rate for a given distortion level .",
    "finally , for a @xmath4-mrs with uninformed decoder , we provide an upper bound for the srdf .",
    "[ th : k - mrs - uninformed - decoder ] for a @xmath4-mrs with uninformed decoder , @xmath164 for @xmath110 , where the minimum is with",
    "respect to @xmath165 and + @xmath166 \\leq \\delta,$ ] with @xmath167 being as in ( [ eq : delta - min - mrs ] ) and .    the ( achievability ) proof of theorem [ th : k - mrs - uninformed - decoder ] is along the lines of proposition [ th : k - fs - rd ] .",
    "the lack of a converse is due to the inability to prove or disprove the convexity of the right - side of in @xmath60 .",
    "( convexity would imply equality in . )",
    "the optimal sampler can , however , be shown to be a conditional point - mass sampler along the lines of theorem [ th : k - mrs - informed - alternative ] .",
    "note that the same conditional point - mass sampler need not be the best in and .",
    "strong forms of the @xmath4-mrs and @xmath4-irs are obtained by allowing time - dependence in sampling . specifically , and can be strengthened , respectively , to @xmath168 and @xmath169 surprisingly , this does not improve srdf for the @xmath4-mrs ( with decoder informed ) or the @xmath4-irs .",
    "[ prop : smrs_sirs ] for a strong @xmath4-mrs in and a strong @xmath4-irs in , the corresponding srdfs @xmath170 and @xmath171 equal the right - sides of and , respectively .",
    "finally , standard properties of the srdf for the fixed - set sampler , @xmath4-irs and @xmath4-mrs with informed decoder are summarized in the following    [ l : srdf_convexity ] for a fixed @xmath24 , the right - sides of , and are finite - valued , nonincreasing , convex , continuous functions of @xmath60 .",
    "we close this section with an example showing that ( i ) the srdf for a @xmath4-mrs with informed decoder can be strictly smaller than that of a @xmath4-irs ; and ( ii ) furthermore , unlike for a @xmath4-irs , a @xmath4-mrs with informed decoder can outperform strictly that with an uninformed decoder , uniformly for all feasible distortion values .    with @xmath172 and",
    "@xmath173 consider a dmms with @xmath174 represented by a virtual binary symmetric channel ( bsc ) shown in figure [ fig : virtual_bsc ] .",
    "fix @xmath175 and @xmath176 i.e. , @xmath177 and @xmath178 are independent .",
    "let @xmath59 correspond to the probability of error criterion , i.e. , @xmath179    \\(i ) considering a @xmath4-mrs , @xmath180 , with informed decoder , we obtain by theorem [ th : k - mrs - informed - alternative ] that @xmath181 and the ( conditional point - mass ) sampler    @xmath182    is uniformly optimal for all @xmath183 and @xmath184 to obtain @xmath185 the srdfs for fixed - set samplers are @xmath186 and @xmath187 since @xmath188 uniformly in @xmath189 it is a simple exercise to show that @xmath190 clearly , @xmath191 , with @xmath114 for the former being @xmath152 for the latter , as shown in figure [ fig : mrs_irs ] .",
    "-mrs vs. @xmath4-irs , width=283,height=226 ]    \\(ii ) the conditional pmf @xmath153 in represents a @xmath192-@xmath192 map between the values of @xmath23 and @xmath193 , and can be seen also to be the optimal choice in the right - side of for all @xmath194 the remaining minimization in , with respect to @xmath195 , renders the right - side to be convex in @xmath196 consequently , as observed in the passage following theorem [ th : k - mrs - uninformed - decoder ] , the bound in is tight .",
    "the resulting values of @xmath150 and @xmath197 are plotted for @xmath198 in figure [ fig : mrs_inf_vs_uninf ] .",
    "+    -mrs , width=389,height=264 ]",
    "our achievability proofs successively build upon each other in the order : fixed - set sampler , @xmath4-irs and @xmath4-mrs .",
    "the achievability proof of proposition [ th : k - fs - rd ] for a fixed - set sampler forms a basic building block for subsequent application . relying on this",
    ", the srdf for a @xmath4-irs is shown to be achieved in theorem [ th : k - irs - informed - decoder ] without the decoder being informed of the sequence of sampled sets .",
    "next , for a @xmath4-mrs with informed decoder , we prove first theorem [ th : k - mrs - informed - alternative ] which shows that the optimal sampler is deterministic in that the corresponding @xmath199 is a point - mass .",
    "this structure enables an achievability proof of theorem [ th : k - mrs - informed - decoder ] which builds on that of proposition [ th : k - fs - rd ] .",
    "lastly , for a @xmath4-mrs with uninformed decoder , the achievability proof of theorem [ th : k - mrs - uninformed - decoder ] rests on the preceding proofs .    * proposition [ th : k - fs - rd ] * : observe first that @xmath200 \\\\   & = \\underset { x_{a^{c } } { { \\ - \\!\\!\\circ\\!\\ ! - \\ } } x_{a } { { \\ - \\!\\!\\circ\\!\\ ! - \\ } } y_{{{\\mathcal m } } } } \\min \\mathbbm{e } \\big [ \\mathbbm{e}[d(x_{{{\\mathcal m } } } , y_{{{\\mathcal m}}})|x_{a } ] \\big ] \\\\   & = \\underset{p _ { x_{a } y_{{{\\mathcal m } } } } } \\min \\mathbbm{e}[d_{a}(x_{a } , y_{{{\\mathcal m } } } ) ]   \\ \\ \\ \\text{by } \\eqref{eq : modified_distortion_a } \\text { and since } x_{a^{c } } { { \\ - \\!\\!\\circ\\!\\ ! - \\ } } x_{a } { { \\ - \\!\\!\\circ\\!\\ ! - \\ } } y_{{{\\mathcal m } } } \\\\   & =    \\mathbbm{e } \\big [ \\   \\underset{y_{{{\\mathcal m } } } \\in { { \\mathcal y}_{\\mathcal m } } } \\min d_{a}(x_{a } , y_{{{\\mathcal m } } } ) \\big ] \\end{aligned}\\ ] ] and @xmath201 \\\\   & = \\underset { p_{x_{{{\\mathcal m } } } } p_{y_{{{\\mathcal m } } } }   } \\min \\mathbbm{e}[d(x_{{{\\mathcal m } } } , y_{{{\\mathcal m } } } ) ] \\\\   & = \\underset { y_{{{\\mathcal m } } } \\in { { \\mathcal y}_{\\mathcal m } } } \\min \\mathbbm{e}[d(x_{{{\\mathcal m } } } , y_{{{\\mathcal m}}})].\\end{aligned}\\ ] ]    next , note that for every @xmath202 , @xmath203 \\leq \\delta } { \\min } i(x_{a } \\wedge y_{{{\\mathcal m } } } ) = \\underset { \\mathbbm{e } [ { d}_{a } ( x_{a},y_{{{\\mathcal m } } } ) ] \\leq \\delta } { \\min } i(x_{a } \\wedge y_{{{\\mathcal m } } } ) .\\end{aligned}\\ ] ]    clearly every feasible @xmath204 on the left - side above gives a feasible @xmath205 on the right - side .",
    "similarly every feasible @xmath205 on the right - side leads to a feasible @xmath101 on the left - side of the form @xmath206 .",
    "given @xmath207 , consider a ( standard ) rate distortion code @xmath208 for the dmms @xmath209 with distortion measure @xmath210 , of rate @xmath211 and with expected distortion @xmath212",
    "\\leq \\delta + { \\epsilon}$ ] for all @xmath213 say .",
    "the code @xmath208 also satisfies @xmath214 & = \\dfrac{1}{n } \\mathbbm{e } \\left [ \\sum_{t=1}^{n } \\mathbbm{e } \\big [ d \\big ( x_{{{\\mathcal m}}t } , \\big ( \\varphi(f(x_{a}^{n } ) ) \\big ) _",
    "{ t } \\big)\\big | x_{a}^{n } \\big ] \\right ]   \\\\    & =   \\dfrac{1}{n } \\mathbbm{e } \\left [ \\sum_{t=1}^{n } \\mathbbm{e } \\big [ d \\big ( x_{{{\\mathcal m}}t } , \\big ( \\varphi(f(x_{a}^{n } ) ) \\big ) _ { t } \\big)\\big | x_{a t } \\big ] \\right ]   \\\\   & = \\mathbbm{e } \\left [ { d}_{a } \\big ( x_{a}^{n } , \\varphi \\big ( f(x_{a}^{n } ) \\big ) \\big ) \\right ]   \\\\   & \\leq \\delta + { \\epsilon},\\end{aligned}\\ ] ] thereby yielding achievability in the proposition .    turning to the corollary , for every @xmath215 satisfying the constraints in , consider the pmf @xmath216 defined by @xmath217 where @xmath218 is the maximum a posteriori estimate of @xmath219 given @xmath220 according to @xmath221 .",
    "observe that @xmath222 satisfies @xmath223 and @xmath224   & = p(x_{{{\\mathcal m } } } \\neq y_{{{\\mathcal m } } } ) \\\\   & = p(x_{a } \\neq y_{a } ) + p(x_{a } = y_{a } ) p(x_{a^{c } } \\neq y_{a^{c}}|x_{a } = y_{a } ) \\\\   & = q(x_{a } \\neq y_{a } ) + q(x_{a } = y_{a } ) p(x_{a^{c } } \\neq y_{a^{c}}|x_{a } = y_{a } ) \\\\   & \\geq q(x_{a } \\neq y_{a } ) + q(x_{a } = y_{a } ) q(x_{a^{c } } \\neq y_{a^{c}}|x_{a } = y_{a } ) \\\\   & = q(x_{{{\\mathcal m } } } \\neq y_{{{\\mathcal m } } } ) = \\mathbbm{e}_{q}[d(x_{{{\\mathcal m } } } , y_{{{\\mathcal m } } } ) ] , \\label{eq : cor_prf_distortion}\\end{aligned}\\ ] ] where the inequality is by , and the optimality of the map estimator .",
    "also , it is readily checked that @xmath225 = 1 - \\mathbbm{e } [ \\alpha(x_{a } ) ] + \\mathbbm{e } [ \\alpha(x_{a } ) \\mathbbm{1}(x_{a } \\neq y_{a } ) ] .\\end{aligned}\\ ] ] furthermore , @xmath226 putting together - and comparing with establishes the corollary .",
    ",    it is interesting to note that the form of @xmath227 \\leq \\delta - ( 1 - \\mathbbm{e } [ \\alpha(x_{a } ) ] ) } i(x_{a } \\wedge y_{a } )   \\end{aligned}\\ ] ] leads to a simpler and direct proof of achievability of the corollary .",
    "specifically , for a given @xmath60 , first @xmath103 is mapped into ( only ) its corresponding codeword @xmath105 but under a modified distortion measure @xmath228 and a corresponding reduced threshold as indicated by .",
    "next , the codewords @xmath105 serve as sufficient statistics from which ( the unsampled ) @xmath107 is reconstructed as @xmath229 under @xmath230 the corresponding estimation error coincides with the reduction in the threshold .",
    "* theorem [ th : k - irs - informed - decoder ] * : the equivalent expression for @xmath108 given by proposition [ prop : k - irs - equiv ] suggests an achievability scheme using a concatenation of fixed - set sampling rate distortion codes from proposition [ th : k - fs - rd ] . let @xmath231 and @xmath232 yield the minimum in proposition [ prop : k - irs - equiv ] .",
    "a sequence of sampling sets @xmath62 are constructed _ a priori _ with @xmath233 repeatedly for approximately @xmath234 time instants , for each @xmath235 in @xmath236 correspondingly , sampling rate distortion codes of blocklength @xmath237  with distortion @xmath238 and of rate @xmath239  are concatenated .",
    "this predetermined selection of sampling sets does not require the decoder to be additionally informed .    for a fixed @xmath110 , let @xmath231 and @xmath240 attain the minimum in . fix @xmath241 and @xmath242 order ( in any manner ) the elements of @xmath243 as @xmath244 with @xmath245 for @xmath246 and @xmath247 ,",
    "define the `` time - sets '' @xmath248 as @xmath249 the time - sets cover @xmath250 , i.e. , @xmath251 and satisfy @xmath252    now , a @xmath4-irs is chosen with a deterministic sampling sequence @xmath253 according to @xmath254 by proposition [ th : k - fs - rd ] , for each @xmath255 in @xmath256 , there exists a code @xmath257 , @xmath258 and @xmath259 of rate @xmath260 and with @xmath261 = \\mathbbm{e } \\left [ d_{a_{i } } \\big ( x_{a_{i}}^{\\nu_{a_{i } } } , \\varphi_{a_{i } } \\big ( f_{a_{i}}(x_{a_{i}}^{\\nu_{a_{i } } } ) \\big ) \\big )   \\right ]    \\leq \\delta_{a_{i } } + \\frac{{\\epsilon}'}{2 } \\end{aligned}\\ ] ] for all @xmath262 ( cf .",
    "proof of proposition [ th : k - fs - rd ] ) .    consider a ( composite ) code @xmath208 as follows .",
    "for the deterministic sampling scheme defined above , the encoder @xmath263 consists of a concatenation of encoders defined by @xmath264 which maps the output of the @xmath4-irs into the set @xmath265 .",
    "the decoder @xmath266 is given by @xmath267 and is aware of the sampling sequence without being informed additionally of it .",
    "the rate of the code is @xmath268 where the previous inequality holds for all @xmath14 large enough . denoting the decoder output by @xmath269",
    ", we have that @xmath270 & = \\mathbbm{e } \\left [   \\dfrac{1}{n } \\sum_{t=1}^{n } d(x_{{{\\mathcal m}}t},y_{{{\\mathcal m}}t } )   \\right ] \\\\                                             & = \\dfrac{1}{n }   \\sum \\limits _ { i   = 1}^{m_{k } } |\\nu_{a_{i}}|   \\mathbbm{e } \\left [ d \\left ( x_{\\cal m}^{\\nu_{a_{i } } } , \\varphi_{a_{i } } \\left ( f_{a_{i}}(x_{a_{i}}^{\\nu_{a_{i } } } ) \\right ) \\right )   \\right ] \\\\                         & \\leq \\sum \\limits _ { i   = 1}^{m_{k } } \\left ( p_{s}(a_{i } ) + \\frac{1}{n } \\right ) \\left ( \\delta_{a_{i } } + \\frac{{\\epsilon}'}{2 } \\right ) \\\\                        & = \\sum \\limits_{i=1}^{m_{k } } p_{s}(a_{i } ) \\delta_{a_{i } } + \\frac{m_{k}}{n } \\left ( \\frac{{\\epsilon}'}{2 } + \\delta_{\\max } \\right ) + \\frac{{\\epsilon}'}{2 } \\\\                        &",
    "\\leq \\delta + { \\epsilon}\\label{eq : distortion_eq1}\\end{aligned}\\ ] ] by and for all @xmath14 large enough .",
    "the proof is completed by noting that and hold simultaneously for all @xmath14 large enough .    next , we establish theorem [ th : k - mrs - informed - alternative ] .",
    "the structure of the conditional point - mass sampler therein will be used next in the achievability proof of theorem [ th : k - mrs - informed - decoder ] to follow .    * theorem [ th : k - mrs - informed - alternative ] * : denoting the minima in and by @xmath271 and @xmath272 , respectively , clearly @xmath273 in fact",
    ", equality will be shown to hold , thereby proving the theorem .",
    "first , since @xmath271 and @xmath272 are convex in @xmath60 by lemma [ l : srdf_convexity ] , by ( * ? ? ?",
    "* lemma 8.1 ) they can be expressed in terms of their lagrangians as @xmath274 where @xmath275 and @xmath276 are the respective minima of @xmath277   \\label{eq : alternative_ach_eq1}\\end{aligned}\\ ] ] over @xmath278 and @xmath279 . by the conditional version of topse s identity (",
    "* lemma 8.5 ) , the expression in equals @xmath280 .",
    "\\label{eq : alternative_ach_eq2}\\end{aligned}\\ ] ]    in @xmath275 , the minimum of the expression in also over @xmath281 is not altered by changing the order of minimization with @xmath282 being the innermost .",
    "using this fact , it is shown in appendix that the minimizing @xmath282 is of the form @xmath283 , whereby @xmath284 hence , equality holds in .    * theorem [ th : k - mrs - informed - decoder ] * : by , using the result of theorem [ th : k - mrs - informed - alternative ] , @xmath285 where @xmath286 \\leq \\delta_{u }   }   \\min i(x_{s } \\wedge y_{{{\\mathcal m } } } | s ) , \\ \\ \\ \\delta_{\\min } \\leq \\delta_{u } \\leq \\delta_{\\max } \\label{eq : k - mrs - informed - sum - component}\\end{aligned}\\ ] ] with the pmf @xmath287 being understood as @xmath288 . to simplify notation ,",
    "the conditioning on @xmath289 will be suppressed except when needed .",
    "it suffices to show the existence of a code of rate @xmath290 with distortion @xmath291 \\ \\substack{\\sim \\\\ \\leq \\\\\\ } \\ \\delta_{u}$ ] .",
    "a concatenation of such codes indexed by @xmath292 yields , in effect , suitable time - sharing among them , leading to the achievability of . by theorem [ th : k - mrs - informed - alternative ] , in view of the optimality of point - mass samplers , concatenating fixed - set sampling rate distortion codes for conditional sources @xmath293 , will suffice .    given any @xmath294 , for the minimizer in ,",
    "consider the corresponding @xmath295   \\text { and }    i(x_{a_{i } } \\wedge y_{{{\\mathcal m } } } | s = a_{i } ) , \\ \\",
    "i \\in { \\cal m}_{k}.\\end{aligned}\\ ] ] the associated @xmath296 is an i.i.d .",
    "sequence ( cf . remark ( ii ) following definition [ d : rdf ] ) .",
    "the sampling sets characterized by the conditional point - mass sampler above and the dmms realizations @xmath297 , are denoted as @xmath298 , and hence @xmath299 .",
    "the idea behind the remainder of the proof below for each @xmath289 is the following .",
    "we collect all those time instants at which a particular @xmath300 in @xmath34 is sampled , with the objective of applying a fixed - set sampling rate distortion code .",
    "since the size of this time - set will vary according to @xmath301 in @xmath302 , the rate of such a code , too , will vary accordingly .",
    "however , since we seek fixed rate codes ( rather than codes with a desired average rate ) , we apply fixed - set sampling codes to subsets of predetermined lengths from among typical sampling sequences in @xmath303    fix @xmath304 and @xmath305 ordering the elements of @xmath34 as in the proof of theorem [ th : k - irs - informed - decoder ] , for @xmath247 , the sets @xmath306 cover @xmath307 ; denote the set of the first @xmath308 time instants in @xmath309 by @xmath310 . for the ( typical ) set @xmath311 @xmath312 for all @xmath313 , say .",
    "along the lines of proof of theorem [ th : k - irs - informed - decoder ] , for each dmms with ( conditional ) pmf @xmath314 , there exists a code @xmath315 and @xmath316 of rate @xmath317 and with @xmath318 \\leq \\delta_{a_{i } } + \\frac{{\\epsilon}'}{2 }    \\end{aligned}\\ ] ] for all @xmath319    a ( composite ) code @xmath320 , with @xmath263 taking values in @xmath321 is constructed as follows .",
    "the encoder @xmath263 consists of a concatenation of encoders defined by @xmath322    for @xmath323 and @xmath324 the informed decoder @xmath63 is given by @xmath325 where @xmath326 is a fixed but arbitrary symbol in @xmath327    the rate of the code is @xmath328    defining @xmath329 , and with @xmath330 denoting the output of the decoder , we have @xmath331 & =   \\ \\mathbbm{e } \\left [   \\mathbbm{e } \\left [ d(x_{{{\\mathcal m}}}^{n } , y_{{{\\mathcal m}}}^{n } ) \\big | s^{n } \\right ] \\right ] \\\\    & = \\ \\sum_{s^{n } \\in { \\cal t}_{{\\epsilon}'}^{(n ) } } p_{s^{n } } ( s^{n } )   \\mathbbm{e } \\big [ d(x_{{{\\mathcal m}}}^{n } , y_{{{\\mathcal m}}}^{n } ) \\big | s^{n } = s^{n } \\big ] \\   +   \\sum_{s^{n } \\notin { \\cal t}_{{\\epsilon}'}^{(n ) } } p_{s^{n}}(s^{n } ) \\mathbbm{e } \\left [ d(x_{{{\\mathcal m}}}^{n } , y_{{{\\mathcal m}}}^{n } ) \\big | s^{n } = s^{n } \\right ]   \\\\ & \\leq \\ \\sum_{s^{n } \\in { \\cal t}_{{\\epsilon}'}^{(n ) } } p_{s^{n } } ( s^{n } ) \\sum _ { i = 1 } ^{m_{k } } \\frac{|\\nu_{a_{i}}| } { n } \\mathbbm{e } \\left [ d \\big ( x_{\\cal m}^{\\nu_{a_{i } } } , \\varphi_{a_{i } } \\big ( f_{a_{i}}(x_{a_{i}}^{\\nu_{a_{i } } } ) \\big ) \\big ) \\big | s^{\\nu_{a_{i } } } = a_{i}^{\\nu_{a_{i } } } \\right ] \\ \\\\ & \\ \\ \\ \\   + \\ \\frac{1}{n } \\sum_{s^{n } \\in { \\cal t}_{{\\epsilon}'}^{(n ) } } p_{s^{n } } ( s^{n } )   \\sum \\limits_{i = 1}^{m_{k } } \\sum \\limits_{t \\in \\tau_{s^{n}}(a_{i } ) \\setminus \\nu_{a_{i } } }   \\mathbbm{e } \\left [ d(x_{{{\\mathcal m } } } , y_{{{\\mathcal m}}t } ) | s^{n } = s^{n } \\right ]   \\   +   \\sum_{s^{n } \\notin { \\cal t}_{{\\epsilon}'}^{(n ) } } p_{s^{n}}(s^{n } ) { d}_{\\max } \\\\ & \\leq \\   \\sum _ { i = 1 } ^{m_{k } }   p_{s}(a_{i } ) \\left ( \\delta_{a_{i } } + \\frac{{\\epsilon}'}{2 } \\right )    \\   + \\ \\left ( 1 - \\frac{\\sum \\limits_{i=1}^{m_{k } } |\\nu_{a_{i}}|   } { n }   \\right ) d_{\\max }   \\   +    \\frac{{\\epsilon}'}{2 } d_{\\max } \\\\ & \\leq \\",
    "\\delta _ { { u } } + \\frac{{\\epsilon}'}{2 }      \\   + \\ \\left ( 2 m_{k } { { \\epsilon } ' }   \\right ) d_{\\max }   \\   +    \\frac{{\\epsilon}'}{2 } d_{\\max } \\\\ & < \\delta_u +   \\epsilon \\label{eq : thm2-ach - eq2}\\end{aligned}\\ ] ] for all @xmath14 large enough .",
    "the proof is completed by noting that for @xmath14 large enough and hold simultaneously and time - sharing between the codes corresponding to @xmath332 completes the proof .",
    "* theorem [ th : k - mrs - uninformed - decoder ] : * the proof is similar to that of proposition [ th : k - fs - rd ] with the i.i.d . sequence @xmath333 replaced by the i.i.d .",
    "sequence @xmath334 with joint pmf @xmath335 obtained from and a modified distortion measure @xmath336 $ ] .",
    "the details , identical to those in the achievability proof of proposition [ th : k - fs - rd ] , are omitted .",
    "separate converse proofs can be provided for proposition [ th : k - fs - rd ] and proposition [ prop : smrs_sirs ] . however , in order to highlight the underlying ideas economically , we develop the proofs in a unified manner .",
    "specifically , in contrast with the achievability proofs above , our converse proofs are presented in the order of weakening power of the sampler , viz . ,",
    "@xmath4-mrs , @xmath4-irs and fixed - set sampler .",
    "we begin with the proof of lemma [ l : srdf_convexity ] followed by pertinent technical results before turning to proposition [ th : k - fs - rd ] and proposition [ prop : smrs_sirs ] .",
    "* lemma [ l : srdf_convexity ] * : we need to prove only that the right - sides of , and are convex and continuous , since they are evidently finite - valued and nonincreasing in @xmath196 the convexity of the right - side of on @xmath337 $ ] is a standard consequence of the convexity of @xmath338 in @xmath339 and the convexity of the constraint set in .",
    "the convexity of the right - sides of and is immediate by the remarks following proposition [ prop : k - irs - equiv ] and theorem [ th : k - mrs - informed - decoder ] , and their continuity for @xmath340 is a consequence .",
    "continuity at @xmath341 in , and holds , for instance , as in ( @xcite , lemma 7.2 ) .",
    "[ l : iid - converse ] let the finite - valued rvs @xmath342 be such that @xmath343 are mutually independent and satisfy @xmath344 and @xmath345 where @xmath346 then , the following hold for @xmath347 : @xmath348 @xmath349 and @xmath350    * proof : * first , is true by the following simple observation : for @xmath351 @xmath352 where the first term in the sum above is zero by the mutual independence of @xmath353 and the second term equals zero by .",
    "next , the claim and the markov property imply that for @xmath323 @xmath354    the claim now follows , since @xmath355 where the first term in the sum above is zero by , and the latter two terms are zero by and , respectively .    now using , @xmath356 \\\\     & = \\sum \\limits_{t=1}^{n } i ( b_{t } \\wedge b^{t-1 } , a^{n \\setminus t},c_{t+1}^{n } , d_{t } | a_{t } , c^{t } ) \\ \\ \\ \\text{by }    \\eqref{eq : abcd_lemma_claim2 } \\\\     & \\geq \\sum_{t=1}^{n } i(b_{t } \\wedge d_{t } |a_{t } , c^{t}),\\end{aligned}\\ ] ] so that the claim follows .",
    "we now prove proposition [ prop : smrs_sirs ] which , in effect , implies the converse proofs for theorem [ th : k - mrs - informed - decoder ] , theorem [ th : k - irs - informed - decoder ] and proposition [ th : k - fs - rd ] .",
    "specifically , a converse is fashioned for @xmath357 with those for @xmath171 and @xmath358 emerging along the way .",
    "let @xmath359 be an @xmath14-length strong @xmath4-mrs block code with decoder output + @xmath360 and satisfying @xmath361 \\leq \\delta$ ] .",
    "the hypothesis of lemma [ l : iid - converse ] with @xmath362 is met since @xmath363 and by , @xmath364 then by lemma [ l : iid - converse ] , for @xmath347 , @xmath365 @xmath366 and @xmath367    denoting by @xmath368 the cardinality of the range space of the encoder @xmath263 , the rate @xmath369 of the code satisfies @xmath370 where follows from .",
    "denote @xmath371 $ ] by @xmath372 .    for the strong @xmath4-mrs code above , in using and , we get @xmath373 = \\delta_{t } } \\min i(x_{s_{t } } \\wedge y_{{{\\mathcal m}}t } | s_{t } , s^{t-1 } ) \\label{eq : converse_generic_eq3 } \\\\   & \\geq \\",
    "\\underset {   p_{u_{t } } p_{x_{{{\\mathcal m}}t } }   p_{s_{t}| x_{{{\\mathcal m}}t } u_{t } }   p_{y_{{{\\mathcal m}}}| s_{t } x_{s_{t } } u_{t } } \\atop \\mathbbm{e}[d(x_{{{\\mathcal m}}t } , y_{{{\\mathcal m}}t } )   ] \\leq \\delta_{t } } \\min i(x_{s_{t } } \\wedge y_{{{\\mathcal m}}t } | s_{t } , u_{t } ) , \\label{eq : converse_generic_eq4 } \\end{aligned}\\ ] ] where @xmath374 is a rv taking values in a set of cardinality @xmath375 the existence of the minima in and comes from the continuity of the conditional mutual information terms over compact sets of pmfs .    by the carathodory theorem @xcite ,",
    "every point in the convex hull of the set @xmath376 , i(x_{s } \\wedge y_{{{\\mathcal m } } } | s ) \\big ) : x_{{{\\mathcal m } } } { { \\ - \\!\\!\\circ\\!\\ ! - \\ } } s , x_{s } { { \\ - \\!\\!\\circ\\!\\ ! - \\ } } y_{{{\\mathcal m } } } \\big \\ } \\subset \\mathbbm{r}^{2 } $ ] can be represented as a convex combination of at most three points in @xmath377 hence , to describe every element in the set @xmath378 , i(x_{s_{t } } \\wedge y_{{{\\mathcal m}}t } | s_{t } , u_{t } ) \\big )   : p_{u_{t } x_{{{\\mathcal m}}t } s_{t } y_{{{\\mathcal m}}t } } =   p_{u_{t } } p_{x_{{{\\mathcal m}}t } } p_{s_{t}|x_{{{\\mathcal m}}t } u_{t } } p_{y_{{{\\mathcal m}}t}|s_{t } x_{s_{t } } u_{t } } \\big \\ } , \\end{aligned}\\ ] ] it suffices to consider a rv @xmath374 with support of size three .",
    "( for @xmath379 , this assertion is straightforward . ) consequently , the right - side of equals @xmath380 ( cf . ) .",
    "using the convexity of @xmath150 in @xmath60 , we get from that @xmath381 i.e. , @xmath382 thereby completing the converse proof for a strong @xmath4-mrs and theorem [ th : k - mrs - informed - decoder ] .    next",
    ", an @xmath14-length strong @xmath4-irs code and fixed - set sampler code can be viewed as restrictions of the strong @xmath4-mrs code above .",
    "specifically , the strong @xmath4-irs and fixed - set sampler respectively entail replacing @xmath383 by @xmath384 and @xmath385 .",
    "counterparts of and hold with the mentioned replacements . for a strong @xmath4-irs , upon replacing @xmath383 with @xmath384 ,",
    "we observe that the right - side of , viz .",
    "@xmath386 \\leq \\delta_{t } } \\min i(x_{s_{t } } \\wedge y_{{{\\mathcal m}}t } | s_{t } , u_{t } ) \\end{aligned}\\ ] ] is now the lower convex envelope of the srdf for a @xmath4-irs , already convex in distortion , and hence , equals @xmath387 itself .",
    "thus , becomes @xmath388 \\leq \\delta_{t } } \\min i(x_{s_{t } } \\wedge y_{{{\\mathcal m}}t } | s_{t } ) = r_{i } ( \\delta_{t } ) .",
    "\\label{eq : converse_generic_eq5}\\end{aligned}\\ ] ] combining and , we get along the lines of that @xmath389 which gives the converse proof for a strong @xmath4-irs and theorem [ th : k - irs - informed - decoder ] .    in a manner analogous to a strong @xmath4-irs , for a fixed - set sampler the convexity of @xmath358 in @xmath60 implies that the counterpart of the right - side of , with @xmath390 replaced by @xmath391 , simplifies to @xmath392 . as in",
    ", it follows that @xmath393 which gives the converse for proposition [ th : k - fs - rd ] .",
    "our new framework of sampling rate distortion describes the centralized sampling of fixed - size subsets of the components of a dmms , followed by encoding and lossy reconstruction of the full dmms . specifically , we examine the tradeoffs between sampling strategy , optimal encoding rate and distortion in reconstruction as characterized by a sampling rate distortion function .",
    "three sampling strategies are considered : fixed - set sampling , independent random sampling and memoryless random sampling ; in the latter two settings , the decoder may or may not be informed of the sampling sequence .",
    "single - letter characterizations of the srdf are provided for the sampling strategies above but for a memoryless random sampler with uninformed decoder . in the last case , an achievability proof yields an upper bound for the srdf whose tightness is unknown .",
    "this upper bound in theorem [ th : k - mrs - uninformed - decoder ] can be convexified by means of a time - sharing random variable whereupon the modified bound becomes tight .",
    "however , it remains open whether such time - sharing is necessary for convexification .",
    "the authors thank himanshu tyagi for many helpful discussions and for suggesting the model with strong sampler .    20    t.  berger , _ rate distortion theory : a mathematical basis for data compression _ , prentice - hall , englewood cliffs , nj , 1971 .",
    "t.  berger , `` multiterminal source coding , '' in _ the information theory approach to communications _ , g. longo , ed .",
    "vienna / new york : springer - verlag , 1978 , vol .",
    "229 , cism courses and lectures , pp . 171231 .",
    "v.  p.  boda and p.  narayan , `` sampling rate distortion , '' _ proceedings of the ieee international symposium on information theory ( isit ) , 2014 _ , pp .",
    "30573061 , june 29-july 4 2014 .",
    "v.  p.  boda and p.  narayan , `` memoryless sampling rate distortion , '' _",
    "53rd annual allerton conference on communication , control , and computing , 2015 _ , pp .",
    "919 - 923 , sept .",
    "30-oct . 2 2015 .",
    "v.  p.  boda and p.  narayan , `` independent and memoryless sampling rate distortion , '' _ proceedings of the ieee international symposium on information theory ( isit ) , 2016 _ ,",
    "pp.2968 - 2972 , july 10 - 15 2016 .",
    "e.  j. cands , j.  romberg and t.  tao , `` robust uncertainty principles : exact signal reconstruction from highly incomplete frequency information , '' _ ieee trans .",
    "inform . theory _",
    "52 , no .  2 , pp .",
    "489509 , feb .",
    "e.  j. cands and t.  tao , `` near - optimal signal recovery from random projections : universal encoding strategies , '' _ ieee trans .",
    "inform . theory _",
    "52 , no .  12 , pp .  54065425 , dec .",
    "z.   chen , j.  ranieri , r.  zhang and m.  vetterli ,  dass : distributed adaptive sparse sensing  , _ ieee trans .",
    "wireless comm .",
    "_ , vol .  14 , no .  5 , pp",
    "25712583 , may 2015 .",
    "t.  m.  cover and a.  j.  thomas , _ elements of information theory _ , john wiley & sons , 2012 .",
    "i.  csiszr and j.  krner , _ information theory : coding theorems for discrete memoryless systems _ , cambridge university press , 2011 .",
    "r.  l.  dobrushin and b.  s.  tsybakov ,  information transmission with additional noise , \" _ ire trans .",
    "inform . theory _",
    ", vol .  8 , no .  5 , pp . 293304 , sept .",
    "d.  l. donoho , `` compressed sensing , '' _ ieee trans .",
    "inform . theory _",
    "52 , no .  4 , pp",
    ".  12891306 , april 2006 .",
    "a.  k. fletcher , s.  rangan , v.  k. goyal and k.  ramachandran ,  denoising by sparse approximation : error bounds based on rate - distortion theory , \" _ eurasip j. adv . sig .",
    "119 , dec . 2006 .",
    "a.  k. fletcher , s.  rangan and v.  k. goyal ,  on the rate - distortion performance of compressed sensing , \" _ proceedings of ieee international conference on acoustics , speech and signal processing _ , vol .  3 , pp .",
    "iii-885iii-888 , 15 - 20 april 2007 .",
    "a.  hormati , o.  roy , y.  m. lu and m.  vetterli ,  distributed sampling of signals linked by sparse filtering : theory and applications , \" _ ieee trans .",
    "_ , vol .",
    "58 , no .  3 , pp .",
    "10951109 , march 2010 .",
    "p.  ishwar , a.  kumar and k.  ramachandran ,  on distributed sampling in dense sensor networks : a  bit conservation principle , \" _ international symposium on information processing in sensor networks _ ( ipsn ) ,",
    "palo alto , ca , april 2003 .",
    "a.  kashyap , l.  a. lastras - montano , c.  xia and l.  zhen ,  distributed source coding in dense sensor networks , \" _ proceedings of data compression conference , 2005 _ , pp .",
    "1322 , 29 - 31 march 2005 .",
    "t.  kawabata and a.  dembo ,  the rate - distortion dimension of sets and measures , \" _ ieee trans .",
    "inform . theory _",
    "40 , no .  5 , pp .",
    "15641572 , dec . 1994 .",
    "a.  kipnis , a.  j.  goldsmith , y.  c.  eldar and t.  weissman ,  distortion rate function of sub - nyquist sampled gaussian sources , \" _ ieee trans .",
    "inform . theory _",
    "62 , no .  1 ,",
    "401429 , jan .",
    "r.  l. konsbruck , e.  telatar and m.  vetterli ,  on sampling and coding for distributed acoustic sensing , \" _ ieee trans .",
    "inform . theory _",
    "58 , no .  5 , pp .",
    "31983214 , may 2012 .",
    "x.  liu , o.  simeone and e.  erkip ,  lossy computing of correlated sources with fractional sampling , \" _ proceedings of the ieee information theory workshop ( itw ) , 2012 _ , pp .",
    "232236 , 3 - 7 sept . 2012 .",
    "d.  l. neuhoff and s.  s. pradhan ,  information rates of densely sampled gaussian data , \" _ proceedings of the ieee international symposium on information theory proceedings ( isit ) , 2011 _ , pp .",
    "27762780 , july 31 2011-aug . 5 2011 .",
    "d.  l. neuhoff and s.  s. pradhan ,  rate - distortion behavior at low distortion for densely sampled gaussian data , \" _ proceedings of the ieee international symposium on information theory proceedings ( isit ) , 2012 _ , pp .",
    "358362 , 1 - 6 july 2012 .",
    "s.  s. pradhan and d.  l. neuhoff , ",
    "transform coding of densely sampled gaussian data , \" _ proceedings of the ieee international symposium on information theory ( isit ) , 2007 _ , pp .",
    "11111114 , 24 - 29 june 2007 .",
    "j.  ranieri , a.   chebira and m.  vetterli ,  near - optimal sensor placement for linear inverse problems ",
    ", _ ieee trans .",
    "_ , vol .",
    "62 , no .  5 , pp .",
    "11351146 , march 2014 .",
    "g.  reeves and m.  gastpar ,  the sampling rate - distortion tradeoff for sparsity pattern recovery in compressed sensing , \" _ ieee trans .",
    "inform . theory _",
    "58 , no .  5 , pp .",
    "30653092 , may 2012 .",
    "j.  z. sun and v.  k. goyal , `` intersensor collaboration in distributed quantization networks , '' _ ieee trans .",
    "_ , vol .",
    "61 , no .  9 , pp . 39313942 , sept . 2013 .",
    "j.  unnikrishnan and m.  vetterli ,  sampling and reconstructing spatial fields using mobile sensors , \" _ proceedings of the ieee international conference on acoustics , speech and signal processing ( icassp ) , 2012 _ , pp .",
    "37893792 , 25 - 30 march 2012 .",
    "j.  unnikrishnan and m.  vetterli ,  on sampling a high - dimensional bandlimited field on a union of shifted lattices , \" _ proceedings of the ieee international symposium on information theory ( isit ) , 2012 _ , pp .",
    "14681472 , 1 - 6 july 2012 .",
    "j.  unnikrishnan and m.  vetterli ,  sampling high - dimensional bandlimited fields on low - dimensional manifolds , \" _ ieee trans .",
    "inform . theory _",
    "59 , no .  4 , pp . 21032127 , april 2013 .",
    "c.  weidmann and m.  vetterli ,  rate distortion behavior of sparse sources , \" _ ieee trans .",
    "inform . theory _",
    "58 , no .  8 , pp .",
    "49694992 , aug .",
    "y.  wu and s.  verd ,  optimal phase transitions in compressed sensing , \" _ ieee trans .",
    "inform . theory _",
    "58 , no .",
    "62416263 , oct .",
    "y.  wu and s.  verd ,  rnyi information dimension : fundamental limits of almost lossless analog compression , \" _ ieee trans .",
    "inform . theory _",
    "56 , no .  8 , pp . 37213748 , aug .",
    "h.  yamamoto and k.  itoh , `` source coding theory for multiterminal communication systems with a remote source , '' _ ieice trans .",
    "e63-e , no .",
    "10 , pp . 700706 , 1980 .",
    "[ s : appendix ]",
    "we show that the minimum of with respect to @xmath278 is attained by @xmath282 of the form @xmath394 .",
    "the lagrangian is @xmath395     \\label{eq : topsoe_usage } \\\\   & = \\underset { p_{u } , q_{y_{{{\\mathcal m}}}| s u } , \\atop   p _",
    "{ y_{{{\\mathcal m } } } | s x_{s}u } } \\min \\",
    "\\sum \\limits_{u , x_{{{\\mathcal m } } } } p_{u x_{{{\\mathcal m}}}}(u , x_{{{\\mathcal m } } } )   \\underset { p_{s|x_{{{\\mathcal m}}}u }    } \\min \\sum \\limits_{s \\in { { \\mathcal a}_{k } } } p_{s|   x_{{{\\mathcal m } } } u}(s|x_{{{\\mathcal m}}},u )     \\bigg ( \\mathbbm{e } \\big [ \\log \\frac { p_{y_{{{\\mathcal m}}}|s x_{s } u } ( y_{{{\\mathcal m } } } | s , x_{s},u ) } { q_{y_{{{\\mathcal m}}}|su } ( y_{{{\\mathcal m}}}| s , u ) }   \\\\ & \\hspace*{10.7 cm } +   \\lambda d(x_{{{\\mathcal m } } } , y_{{{\\mathcal m } } } ) \\big | s = s , x_{s } = x_{s } , u = u \\big ] \\bigg )      \\end{aligned}\\ ] ] where the expectation above is with respect to @xmath396 noting that the term in @xmath397 is a function of @xmath398 we get @xmath399 \\bigg )   \\\\    & =    \\underset { p_{u } ,    q_{y_{{{\\mathcal m}}}|    s u } , \\atop    p _ { y_{{{\\mathcal m } } } | s x_{s}u } } { \\min } \\",
    "\\sum \\limits_{u , x_{{{\\mathcal m } } } } p_{u x_{{{\\mathcal m}}}}(u , x_{{{\\mathcal m } } } ) \\",
    "\\underset { \\delta_{h ( \\cdot ) } } \\min   \\sum \\limits_{s \\in { { \\mathcal a}_{k } } } \\delta_{h(x_{{{\\mathcal m } } } , u ) } ( s )   \\bigg ( \\mathbbm{e } \\big [ \\log \\frac { p_{y_{{{\\mathcal m}}}|s x_{s } u } ( y_{{{\\mathcal m } } } | s , x_{s},u ) } { q_{y_{{{\\mathcal m}}}|su } ( y_{{{\\mathcal m}}}| s , u ) }   \\\\ & \\hspace*{9 cm } + \\lambda d(x_{{{\\mathcal m } } } , y_{{{\\mathcal m } } } ) \\big | s = s , x_{s } = x_{s } , u = u \\big ] \\bigg )     \\\\      & = \\underset { p_{u } ,   \\delta _ { h ( \\cdot ) } ,   q _ { y_{{{\\mathcal m } } } |   s u } , \\atop   p _ { y_{{{\\mathcal m } } } | s x_{s}u } } \\min \\     d \\left (   p_{y_{{{\\mathcal m}}}|sx_{s } u } \\big",
    "| \\big | q_{y_{{{\\mathcal m}}}|   s u } \\big | p_{s x_{s } u } \\right )   +   \\lambda \\mathbbm{e } \\big [ d(x_{{{\\mathcal m } } } , y_{{{\\mathcal m } } } )   \\big ]     \\\\      & = g_{r } ( \\lambda ) .",
    "\\label{eq : alternative_ach_eq3}\\end{aligned}\\ ] ]"
  ],
  "abstract_text": [
    "<S> consider a discrete memoryless multiple source with @xmath0 components of which @xmath1 possibly different sources are sampled at each time instant and jointly compressed in order to reconstruct all the @xmath0 sources under a given distortion criterion . </S>",
    "<S> a new notion of sampling rate distortion function is introduced , and is characterized first for the case of fixed - set sampling . </S>",
    "<S> next , for independent random sampling performed without knowledge of the source outputs , it is shown that the sampling rate distortion function is the same regardless of whether or not the decoder is informed of the sequence of sampled sets . </S>",
    "<S> furthermore , memoryless random sampling is considered with the sampler depending on the source outputs and with an informed decoder . </S>",
    "<S> it is shown that deterministic sampling , characterized by a conditional point - mass , is optimal and suffices to achieve the sampling rate distortion function . for memoryless random sampling with an uninformed decoder , an upper bound for the sampling rate distortion function </S>",
    "<S> is seen to possess a similar property of conditional point - mass optimality . </S>",
    "<S> it is shown by example that memoryless sampling with an informed decoder can outperform strictly any independent random sampler , and that memoryless sampling can do strictly better with an informed decoder than without .    </S>",
    "<S> discrete memoryless multiple source , independent random sampler , memoryless random sampler , random sampling , rate distortion , sampling rate distortion function . </S>"
  ]
}