{
  "article_text": [
    "in this paper , we study the problem of state estimation under both bad data and observation noise . in state estimation problems , the observations may be corrupted with abnormally large measurement errors , called bad data , in addition to the usual additive observation noise . more specifically ,",
    "suppose we want to estate the state @xmath2 described by an @xmath3-dimensional real - numbered vector , and we make @xmath4 measurements , then these measurements can be written as an @xmath4-dimensional vector @xmath5 , which is related to the state vector through the measurement equation @xmath6 where @xmath7 is a nonlinear function relating the measurement vector to the state vector , and @xmath8 is the vector of measurement noise , and @xmath9 is the vector of bad data imposed on the measurement . in this paper , we assume that @xmath8 is an @xmath3-dimensional vector with i.i.d .",
    "zero mean gaussian elements of variance @xmath10 .",
    "we also assume that @xmath9 is a vector with only @xmath11 nonzero entries , and the nonzero entries can take arbitrary real - numbered values , reflecting the nature of bad data .",
    "it is well known that least square ( ls ) method can be used to suppress the effect of observation noise on state estimations . in ls method",
    ", we try to find a vector @xmath2 minimizing @xmath12 however , the ls method generally only works well when there are no bad data @xmath9 corrupting the observation @xmath5 .    in this paper , a mixed least @xmath0 norm and least square convex programming",
    "is used to simultaneously detect bad data and subtract additive noises from the observations . in our theoretical analysis of the decoding performance , we assume @xmath7 is a linear transformation @xmath13 with @xmath14 as an @xmath15 matrix with i.i.d .",
    "standard zero mean gaussian entries . through using the almost euclidean property for the linear subspace generated by @xmath14",
    ", we derive a new performance bound for the state estimation error under sparse bad data and additive observation noises . in our analysis , using the `` escape - through - a - mesh '' theorem from geometric functional analysis @xcite , we are able to significantly improve on the bounds for the almost euclidean property of a linear subspace , which may be interesting in a more general mathematical setting . compared with earlier analysis on the same optimization problem in @xcite , the analysis is new using the almost euclidean property rather than the restricted isometry conditions used in @xcite , and we are able to give explicit bounds on the error performance , which is sharper than the analysis using the restricted isometry conditions in @xcite .    inspired by bad data detection methods for linear systems , we further propose an iterative convex programming approach to perform combined bad data detection and denoising in nonlinear electrical power networks .",
    "the static state of an electric power network can be described by the vector of bus voltage magnitudes and angles in power networks . however , in smart grid power networks , the measurement of these quantities can be corrupted due to errors in the sensors , communication errors in transmitting the measurement results , and adversarial compromises of the meters .",
    "so the state estimation of power networks needs to detect , identify , and eliminate large measurement errors @xcite .",
    "since the probability of large measurement errors occurring is very small , it is reasonable to assume that bad data are only present in a small fraction of the available meter measurements results .",
    "so bad data detection in power networks can be viewed as a sparse error detection problem , which shares similar mathematical structures as sparse recoveries problem in compressive sensing @xcite .",
    "however , this problem in power networks has several unique properties when compared with ordinary sparse error detection problem @xcite . in fact , @xmath7 in ( [ eq : powermodel ] ) is a nonlinear mapping instead of a linear mapping in @xcite .",
    "our iterative convex programming based algorithms work is shown by numerical examples working well in this nonlinear setting .",
    "compared with @xcite , which proposed to apply @xmath0 minimization in bad data detection in power networks , our approach offers a better decoding error performance when both bad data and additive observation noises are present .",
    "@xcite@xcite considered state estimations under malicious data attacks , and formulated the problem of state estimation under malicious attacks as a hypothesis testing problem by assuming a prior probability distribution on the state @xmath2 .",
    "in contrast , our approach does not rely on any prior information on the signal @xmath2 itself , and the performance bounds hold for arbitrary state @xmath2 .",
    "the rest of this paper is organized as follows . in section [ sec : condition ] , we introduce the convex programming to perform joint bad data detection and denoising , and derive the performance bound on the decoding error based on the almost euclidean property of linear subspaces .",
    "in section [ sec : boundingeuclidean ] , a sharp bound on the almost euclidean property is given through the `` escape - through - mesh '' theorem . in section [ sec : evaluating ] , we will present explicit bounds on the decoding error . in section [ sec : numerical ] , we introduce our algorithm to perform bad data detection in nonlinear systems , and present simulation results of its performance in power networks .",
    "in this section , we will introduce a convex programming formulation to do bad data detection in a linear systems , and give a characterization of its decoding error performance . in a linear system ,",
    "the @xmath16 observation vector is @xmath17 , where @xmath2 is the @xmath18 signal vector ( @xmath19 ) , @xmath9 is a sparse error vector with @xmath11 nonzero elements , @xmath8 is a noise vector with @xmath20 . in",
    "what follows , we denote the part of any vector @xmath21 over any index set @xmath22 as @xmath23 .",
    "we solve the following optimization problem involving optimization variables @xmath24 and @xmath25 , and we then estimate the state @xmath2 to be @xmath26 , which is the optimizing value for @xmath24 .",
    "@xmath27 we are now ready to give the main theorem which bounds the decoding error performance of ( [ eq : bus ] ) .",
    "let @xmath5 , @xmath14 , @xmath2 , @xmath9 and @xmath8 are specified as above .",
    "suppose that the minimum nonzero singular value of @xmath14 is @xmath28 .",
    "let @xmath29 be a real number larger than @xmath30 , and suppose that every vector @xmath21 in the subspace generated by the matrix @xmath14 satisfies @xmath31 for any subset @xmath32 with cardinality @xmath33 , where @xmath11 is an integer , and @xmath34 .",
    "we also assume the subspace generated by @xmath14 satisfies the _ almost euclidean _",
    "property for a constant @xmath35 , namely @xmath36 holds for every @xmath21 in the subspace generated by @xmath14    then the solution @xmath26 satisfies @xmath37 [ thm : bound ]    suppose that one optimal solution set to ( [ eq : bus ] ) is @xmath38 .",
    "since @xmath39 , we have @xmath40 .",
    "since @xmath41 and @xmath42 is a feasible solution for ( [ eq : bus ] ) , then @xmath43    applying the triangle inequality to @xmath44 , we further obtain @xmath45    denoting @xmath46 as @xmath47 , because @xmath9 is supported on a set @xmath48 with cardinality @xmath49 , by the triangle inequality for @xmath0 norm again , @xmath50    so we have @xmath51    with @xmath52 , we know @xmath53    combining this with ( [ eq : differencebounded ] ) , we obtain @xmath54    by the almost euclidean property @xmath55 , it follows : @xmath56    by the definition of singular values , @xmath57 so combining ( [ eq : wl2norm ] ) , we get @xmath37    note that when there are no sparse errors present , the decoding error bound satisfies @xmath58 , theorem [ thm : bound ] shows that the decoding error of ( [ eq : bus ] ) is oblivious to the presence of bad data , no matter how large in amplitude these bad data can be .",
    "this phenomenon also observed in @xcite by using the restricted isometry condition for compressive sensing .",
    "we remark that , for given @xmath5 and given @xmath59 , by strong lagrange duality theory , the solution @xmath26 to ( [ eq : bus ] ) will correspond to the solution to @xmath2 in the following problem ( [ eq : lambdaduality ] ) for some lagrange duality variable @xmath60 . as @xmath61 increases , the corresponding @xmath62 that produces the same solution to @xmath2 will correspondingly decrease .",
    "@xmath63 in fact , when @xmath64 , ( [ eq : lambdaduality ] ) approaches @xmath65 and when @xmath66 , ( [ eq : lambdaduality ] ) approaches @xmath67 thus , ( [ eq : lambdaduality ] ) can be viewed as a weighed version of @xmath0 minimization and @xmath1 minimization ( or equivalently the ls method ) .",
    "we will later use numerical experiments to show that in order to recover a sparse vector from measurements with both noise and errors , this weighted version outperforms both @xmath0 minimization and the ls method .    in the next two sections",
    ", we will aim at explicitly computing @xmath68 , which will denote @xmath69 later in this paper .",
    "the appearance of the @xmath70 factor is to compensate for the energy scaling of large random matrices and its meaning will be clear in later context .",
    "next , we will compute explicitly the almost euclidean property constant @xmath71 .",
    "in this section , we would like to give a quantitative bound on the almost euclidean property constant @xmath71 such that with high probability ( with respect to the measure for the subspace generated by the random @xmath14 ) , @xmath55 holds for every vector @xmath47 from the subspace generated by @xmath14 . here",
    "we assume that each element of @xmath14 is generated from the standard gaussian distribution @xmath72 .",
    "so the subspace generated by @xmath14 is a uniformly distributed @xmath73-dimensional subspaces from the high dimensional geometry .    to ensure that the subspace generated from @xmath14 satisfies the almost euclidean property with @xmath74",
    ", we must have the event that the subspace generated by @xmath14 does not intersect the set @xmath75 , where @xmath76 is the euclidean sphere in @xmath77 . to evaluate the probability that this event happens",
    ", we will need the following `` escape - through - mesh '' theorem .",
    "@xcite [ thm : escapethroughmesh ] let @xmath78 be a subset of the unit euclidean sphere @xmath76 in @xmath77 .",
    "let @xmath79 be a random @xmath3-dimensional subspace of @xmath80 , distributed uniformly in the grassmanian with respect to the haar measure .",
    "let @xmath81 , where @xmath82 is a random column vector in @xmath80 with i.i.d .",
    "@xmath72 components .",
    "assume that @xmath83 .",
    "then @xmath84    from theorem [ thm : escapethroughmesh ] , we can use the following programming to get an estimate of the upper bound of @xmath85 . because the set @xmath75 is symmetric , without loss of generality , we assume that the elements of @xmath82 follow i.i.d",
    ". half - normal distributions , namely the distribution for the absolute value of a standard zero mean gaussian random variables . with @xmath86 denoting the @xmath87-th element of @xmath82 , this is equivalent to @xmath88    following the method from @xcite , we use the lagrange duality to find an upper bound for the objective function of ( [ eq : maxproductoptimization ] ) .",
    "@xmath89 where @xmath62 is a vector @xmath90 .",
    "first , we maximize ( [ eq : maxmin ] ) over @xmath91 , @xmath92 for fixed @xmath93 , @xmath94 and @xmath62 . by making the derivatives to be zero ,",
    "the minimizing @xmath91 is given by @xmath95    plugging this back , we get @xmath96    next , we minimize ( [ eq : insidemin ] ) over @xmath97 .",
    "it is not hard to see the minimizing @xmath98 is @xmath99 and the corresponding minimized value is @xmath100    then , we minimize ( [ eqn : insidemin2 ] ) over @xmath60 .",
    "given @xmath82 and @xmath101 , it is easy to see that the minimizing @xmath62 is    @xmath102 and the corresponding minimized value is @xmath103    now if we take any @xmath101 , ( [ eq : upperboundinstance ] ) serves as an upper bound for ( [ eq : maxmin ] ) . since @xmath104 is a concave function , by jensen s inequality , we have for any given @xmath105 , @xmath106 since @xmath82 has i.i.d .",
    "half - normal components , the righthand side of ( [ eqn : upper ] ) equals to @xmath107 where @xmath108 is the error function .",
    "one can check that ( [ eqn : boundu2 ] ) is convex in @xmath109 . given @xmath71 ,",
    "we minimize ( [ eqn : boundu2 ] ) over @xmath101 and let @xmath110 denote the minimum value .",
    "then from ( [ eqn : upper ] ) and ( [ eqn : boundu2 ] ) we know @xmath111 given @xmath112 , we pick the largest @xmath113 such that @xmath114 . then as @xmath4 goes to infinity , it holds that @xmath115 then from theorem [ thm : escapethroughmesh ] , with high probability @xmath116 holds for every vector @xmath47 in the subspace generated by @xmath14 .",
    "we numerically calculate how @xmath113 changes over @xmath117 and plot the curve in fig .",
    "[ fig : alpha ] .",
    "for example , when @xmath118 , @xmath119 , thus @xmath120 for all @xmath47 in the subspace generated by @xmath14 .     over @xmath121",
    "note that when @xmath122 , we get @xmath123 . that is much larger than the known @xmath71 used in @xcite , which is approximately @xmath124 ( see equation ( 12 ) in @xcite ) . when applied to the sparse recovery problem considered in @xcite",
    ", we will be able to recover any vector with no more than @xmath125 nonzero elements , which are @xmath126 times more than the @xmath127 bound in @xcite .",
    "if the elements in the measurement matrix @xmath14 are i.i.d . as the unit real gaussian random variables @xmath72 , following upon the work of marchenko and pastur @xcite , geman@xcite and silverstein @xcite proved that for @xmath128 , as @xmath129 , the smallest nonzero singular value @xmath130 almost surely as @xmath131",
    "now that we have already explicitly bounded @xmath71 and @xmath28 , we now proceed to characterize @xmath29 .",
    "it turns out that our earlier result on the almost euclidean property can be used to computed @xmath29 .",
    "suppose an @xmath4-dimensional vector @xmath47 satisfies @xmath132 . then if for some set @xmath32 with cardinality @xmath133 , @xmath134 then @xmath135 must be a number satisfying @xmath136 [ thm : almosteuclideanb ]    without loss of generality , we let @xmath137 .",
    "then by the cauchy - schwarz inequality , @xmath138    at the same time , by the almost euclidean property , @xmath139 so we must have @xmath136    if a nonzero @xmath4-dimensional vector @xmath47 satisfies @xmath132 , and for any set @xmath32 with cardinality @xmath133 , if @xmath140 for some number @xmath141 , then @xmath142 where @xmath143 .",
    "if @xmath144 , we have @xmath145 so by theorem [ thm : almosteuclideanb ] , @xmath146 satisfies @xmath147    this is equivalent to @xmath148    solving this inequality for @xmath149 , we get ( [ eq : largestsparsity ] ) .",
    "so for a sparsity @xmath149 , this corollary can be used to find @xmath29 such that @xmath150 . combining these results on computing @xmath28 , @xmath71 and @xmath29 , we",
    "can then compute the bound @xmath151 in theorem [ thm : bound ] .",
    "for example , when @xmath152 , we plot the bound @xmath69 as a function of @xmath149 in fig . [",
    "fig : varpi ]     versus @xmath149 ]",
    "* experiment 1 : * we first consider recovering a signal vector from gaussian measurements .",
    "we generate the measurement matrix @xmath154 with i.i.d . @xmath155 entries and a vector @xmath156 with i.i.d gaussian entries .",
    "let @xmath157 be the signal vector .",
    "let @xmath158 and @xmath159 .",
    "we first consider the recover performance when the number of erroneous measurements is fixed .",
    "we randomly choose twelve measurements and flip the signs of these measurements .",
    "for each measurement , we also independently add a gaussian noise from @xmath160 .",
    "for a given @xmath161 , we apply ( [ eq : lambdaduality ] ) to estimate @xmath162 using @xmath62 from 0 to 13 , and pick the best @xmath163 with which the estimation error is minimized . for each @xmath161 , the result",
    "is averaged over fifty runs .",
    "[ fig : gaussianlambda2sigma ] shows the curve of @xmath163 against @xmath161 .",
    "when the number of measurements with bad data is fixed , @xmath163 decreases as the noise level increases .     versus @xmath161 for gaussian measurements ]",
    "we next fix the noise level and consider the estimation performance when the number of erroneous measurements changes .",
    "each measurement has a gaussian noise independently drawn from @xmath164 .",
    "let @xmath165 denote the percentage of erroneous measurements .",
    "given @xmath165 , we randomly choose @xmath166 measurements , and each such measurement is added with a gaussian error independently drawn from @xmath167 . the estimation result is averaged over fifty runs .",
    "[ fig : gaussianfixlambda ] shows how the estimation error changes as @xmath165 increases for different @xmath62 .",
    "@xmath168 has the best performance in this setup compared with a large value @xmath169 and a small value @xmath170 .     versus @xmath165 for gaussian measurements ]    * experiment 2 : * we also consider estimating the state of the power system from available measurements and known system configuration .",
    "the state variables are the voltage magnitudes and the voltage angles at each bus .",
    "the measurements can be the real and reactive power injections at each bus , and the real and reactive power flows on the lines .",
    "all the measurements are corrupted with noise , and a small fraction of the measurements contains errors .",
    "we would like to estimate the state variables from the corrupted measurements .",
    "the relationship between the measurements and the state variables for a @xmath171-bus system can be stated as follows @xcite : @xmath172 @xmath173 @xmath174 @xmath175 where @xmath176 and @xmath177 are the real and reactive power injection at bus @xmath87 respectively , @xmath178 and @xmath179 are the real and reactive power flow from bus @xmath87 to bus @xmath180 , @xmath181 and @xmath182 are the voltage magnitude and angle at bus @xmath87 . @xmath183 and @xmath184 are the magnitude and phase angle of admittance from bus @xmath87 to bus @xmath180 , @xmath185 and @xmath186 are the magnitude and angle of the shunt admittance of line at bus @xmath87 .",
    "given a power system , all @xmath183 , @xmath184 , @xmath185 and @xmath186 are known .    for a @xmath171-bus system , we treat one bus as the reference bus and set the voltage angle at the reference bus to be zero .",
    "there are @xmath187 state variables with the first @xmath171 variables for the bus voltage magnitudes @xmath181 and the rest @xmath188 variables for the bus voltage angles @xmath189 .",
    "let @xmath190 denote the state variables and let @xmath191 denote the @xmath4 measurements of the real and reactive power injection and power flow .",
    "let @xmath192 denote the noise and @xmath193 denote the sparse error vector .",
    "then we can write the equations in a compact form , @xmath194 where @xmath195 denotes @xmath4 nonlinear functions defined in ( [ eqn : sys1 ] ) to ( [ eqn : sys2 ] ) .",
    "an estimate of the state variables , @xmath196 , can be obtained by solving the following minimization problem , @xmath197 where @xmath196 is the optimal solution @xmath162 .",
    "@xmath198 is a fixed parameter .",
    "when @xmath64 , ( [ eqn : nonlinear ] ) approaches @xmath199 and when @xmath66 , ( [ eqn : nonlinear ] ) approaches @xmath200    since @xmath201 is nonlinear , we linearize the equations and apply an iterative procedure to obtain a solution .",
    "we start with the initial state @xmath202 where @xmath203 for all @xmath204 , and @xmath205 for all @xmath206 . in the @xmath11th iteration ,",
    "let @xmath207 , then we solve the following convex optimization problem , @xmath208 where @xmath154 is the jacobian matrix of @xmath201 evaluated at @xmath209 .",
    "let @xmath210 denote the optimal solution @xmath211 to ( [ eqn : linear ] ) , then the state estimation is updated by @xmath212 we repeat the process until @xmath213 .",
    "we evaluate the performance on the ieee 30-bus test system .",
    "[ fig : bus ] shows the structure of the test system .",
    "then the state vector contains fifty - nine variables .",
    "we take one hundred measurement including the real and reactive power injection at each bus and some of the real and reactive power flows on the lines .",
    "we first consider how the estimation performance changes as the noise level increases when the erroneous measurements are fixed .",
    "the errors of the measurements are simulated by inverting the sign of the real power injection at bus 2 , bus 3 , bus 5 , bus 26 and bus 30 , and inverting the sign of the reactive power injection at bus 30 .",
    "each measurement also contains a gaussian noise independently drawn from @xmath160 . for a fixed noise level @xmath161",
    ", we solve ( [ eqn : nonlinear ] ) by the iterative procedure using different @xmath62 ( from 0.5 to 12 ) .",
    "the estimation performance is measured by @xmath214 , where @xmath215 is the true state variable and @xmath196 is our estimation . for a fixed @xmath161 , we choose the @xmath163 to be the one with which @xmath214 is minimal among all the @xmath62 s we consider . the result",
    "is averaged over fifty runs .",
    "[ fig : lambda2sigma ] shows how @xmath163 changes as @xmath161 increases from 0 to 0.2 .",
    "when the noise level is low , i.e. the measurements basically only contain errors , then the estimation performance is better when we use a larger @xmath62 .",
    "when the noise level is high , a smaller @xmath62 leads to a better performance .     versus @xmath161 ]",
    "we also study how the estimation performance changes as the number of erroneous measurements increases .",
    "each of the one hundred measurements contains random gaussian noise independently drawn from @xmath217 .",
    "let @xmath165 denote the percentage of erroneous measurements with bad data . for fixed @xmath165 , we randomly choose the set @xmath218 of erroneous measurements with cardinality @xmath219 .",
    "each erroneous measurement contains an additional gaussian error independently drawn from @xmath220 .",
    "we than calculate the solution @xmath196 of ( [ eqn : nonlinear ] ) and the estimation error @xmath214 .",
    "[ fig : fixlambda ] shows how the estimation error changes as @xmath165 increases .",
    "the results are averaged over fifty runs .",
    "when @xmath62 is small ( @xmath221 ) , ( [ eqn : nonlinear ] ) approaches ( [ eqn : l1 ] ) , and the estimation error is relatively large if @xmath165 is small , i.e. the measurements basically contain only noise .",
    "when @xmath62 is large ( @xmath222 ) , ( [ eqn : nonlinear ] ) approaches ( [ eqn : l2 ] ) , and the estimation error is relatively large if @xmath165 is large , i.e. the measurements contains errors besides noise .",
    "in contrast , if we choose @xmath62 to be 7 in this case , the estimation error is relatively small for all @xmath165 among the three choices of @xmath62 .    ]",
    "in this paper , we study state estimation through observations corrupted with both bad data and additive observation noises .",
    "a mixed @xmath0 and @xmath1 convex programming is used to separate both sparse bad data and additive noises from the observations .",
    "we used the almost euclidean property of a linear subspace to provide sharp bounds on this convex programming based state estimation method .",
    "we also give sharp bounds for the almost euclidean property of a linear subspace using the `` escape - through - a - mesh '' theorem from geometric functional analysis @xcite .",
    "we then propose an iterative convex programming based methods to perform state estimation with bad data detection in the nonlinear electrical power network problems .",
    "simulation results confirm the effectiveness of the algorithms in denoising and detecting bad data at the same time .",
    "the research is supported by nsf under ccf-0835706 and onr under n00014 - 11 - 1 - 0131 .",
    "m. stojnic , `` various thresholds for @xmath0-optimization in compressed sensing , '' _ http://arxiv.org/abs/0907.3666 _",
    "s. geman , `` a limit theorem for the norm of random matrices , '' _ annals of probability _ , vol . 8 , no .",
    "2 , pp.252 - 261 , 1980 j. silverstein , `` the smallest eigenvalue of a large dimensional wishart matrix , '' _ annals of probability _ , vol . 13 , pp . 1364 - 1368 , 1985 v.  a.  marenko and l.  a.  pastur , `` distributions of eigenvalues for some sets of random matrices , '' math .",
    "ussr - sbornik , vol .",
    "1 , pp . 457 - 483 , 1967 .",
    "e. cands and p. randall , `` highly robust error correction by convex programming , '' _ ieee transactions on information theory _ , vol .",
    "2829 - 2840 .",
    "w. kotiuga and m. vidyasagar , `` bad data rejection properties of weighted least absolute value techniques applied to static state estimation , '' _ ieee trans .",
    "power app .",
    "pas-101 , pp .",
    "844 - 853 , apr .",
    "1982 . o. kosut , l. jia , r. j. thomas , and l. tong , `` on malicious data attacks on power system state estimation , '' _ proceedings of upec_,cardiff , wales , uk , 2010 . o. kosut , l. jia , r. j. thomas , and l. tong , ",
    "malicious data attacks on smart grid state estimation : attack strategies and countermeasures \" _ proceedisng of ieee 2010 smartgridcomm _ , 2010 .",
    "yin zhang , `` a simple proof for recoverability of @xmath0-minimization : go over or under ? ''",
    "technical report , http://www.caam.rice.edu/  yzhang / reports / tr0509.pdf"
  ],
  "abstract_text": [
    "<S> in this paper , we consider the problem of state estimation through observations possibly corrupted with both bad data and additive observation noises . </S>",
    "<S> a mixed @xmath0 and @xmath1 convex programming is used to separate both sparse bad data and additive noises from the observations . through using the almost euclidean property for a linear subspace </S>",
    "<S> , we derive a new performance bound for the state estimation error under sparse bad data and additive observation noises . </S>",
    "<S> our main contribution is to provide sharp bounds on the almost euclidean property of a linear subspace , using the `` escape - through - a - mesh '' theorem from geometric functional analysis . </S>",
    "<S> we also propose and numerically evaluate an iterative convex programming approach to performing bad data detections in nonlinear electrical power networks problems . </S>"
  ]
}