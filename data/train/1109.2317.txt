{
  "article_text": [
    "we live in an age of data deluge . a study sponsored by the information storage company _ emc _",
    "estimated that the world s data is more than doubling every two years , reaching 1.8 zettabytes ( 1 zb = @xmath0 bytes ) of data to be stored in 2011 .",
    "this includes various digital data continuously being generated by individuals as well as business and government organizations , who all need scalable solutions to store data reliably and securely",
    ".    storage technology has been evolving fast in the last quarter of a century to meet the numerous challenges posed in storing an increasing amount of data and catering to diverse applications with different workload characteristics . in 1988 , raid ( redundant arrays of inexpensive disks ) was proposed @xcite , which combines multiple storage disks ( typically from two to seven ) to realize a single logical storage unit .",
    "data is stored redundantly , using replication , parity , or more recently erasure codes .",
    "such redundancy makes a raid logical unit significantly more reliable than the individual constituent disks . besides meeting cost effective reliable storage ,",
    "raid systems provide good throughput by leveraging parallel i / o at the different disks , and , more recently , geographic distributions of the constituent disks to achieve resilience against local events ( such as a fire ) that could cause correlated failures .    while raid has evolved and stayed an integral part of storage solutions to date , new classes of storage technology have emerged , where multiple logical storage units ( simply referred to as ` storage nodes ' ) are assembled together to scale out the storage capacity of a system .",
    "the massive volume of data involved means that it would be extremely expensive , if not impossible , to build single pieces of hardware with enough storage as well as i / o capabilities . by the term ` networked '",
    ", we refer to these storage systems that pool resources from multiple interconnected storage nodes , which in turn may or not use raid .",
    "the data is distributed across these interconnected storage units and hence the name ` networked distributed storage systems ' ( ndss ) .",
    "it is worth emphasizing at this juncture that though the term ` raid ' is now also used in the literature for ndss environments , for instance , hdfs - raid @xcite and ` distributed raid ' , but in this article we use the term raid to signify traditional raid systems where the storage nodes are collocated , and the number of parity blocks per data object is few , say one or two ( raid-1 to raid-6 ) . unlike in traditional raid systems where the storage disks are collocated , all data objects are stored in the same set of storage disks , and these disks share an exclusive communication bus within a stand - alone unit , in ndss , a shared interconnect is used across the storage nodes , and different objects may be stored across arbitrarily different ( possibly intersecting ) subsets of storage nodes , and thus there is competition and interference in the usage of the network resources .",
    "ndss come in many flavors such as data centers and peer - to - peer ( p2p ) storage / backup systems .",
    "while data centers comprise thousands of compute and storage nodes , individual clusters such as that of google file system ( gfs ) @xcite are formed out of hundreds up to thousands of nodes .",
    "p2p systems like wuala , in contrast , formed swarms of tens to hundreds of nodes for individual files or directories , but would distribute such swarms arbitrarily out of hundreds of thousands of peers .",
    "while p2p systems are geographically distributed and connected through an arbitrary topology , data center interconnects have well defined topologies and are either collocated or distributed across a few geographic regions .",
    "furthermore , individual p2p nodes may frequently go offline and come back online ( temporary churn ) , creating unreliable and heterogeneous connectivity . on the contrary ,",
    "data centers use dedicated resources with relatively infrequent temporary outages .     despite these differences , ndss share several common characteristics . while i / o of individual nodes continues to be a potential bottleneck , available bandwidth , both at the network s edges and within",
    "the interconnect becomes a critical shared resource . also , given the system scale , failure of a significant subset of the constituent nodes , as well as other network components , is the norm rather than the exception . to enable a highly available overall service ,",
    "it is thus essential to tolerate both short - term outages of some nodes and to provide resilience against permanent failures of individual components .",
    "fault - tolerance is achieved using redundancy , while long - term resilience relies on replenishment of lost redundancy over time .    a common practice to realize redundancy",
    "is to keep three copies of an object to be stored ( called 3-way replication ) : when one copy is lost , the second copy is used to regenerate the first one , and hopefully , not both the remaining copies are lost before the repair is completed .",
    "there is of course a price to pay : redundancy naturally reduces the efficiency , or alternatively put , increases the overheads of the storage infrastructure .",
    "the cost for such an infrastructure should be estimated not only in terms of the hardware , but also of real estate and maintenance of a data center .",
    "a us environmental protection agency report of 2007 indicates that the us used 61 billion kilowatt - hours of power for data centers and servers in 2006 . that is 1.5 percent of the us electricity use , and it cost the companies that paid those bills more than $ 4.5 billion .",
    "there are different ways to reduce these expenses , starting from the physical media , which has witnessed a continuous shrinking of physical space and cost per unit of data , as well as reductions in terms of cooling needs .",
    "this article focuses on a different aspect , that of the trade - off between fault - tolerance and efficiency in storage space utilization via coding techniques , or more precisely erasure codes .",
    "an erasure code @xmath1 transforms a sequence of @xmath2 symbols into a longer sequence of @xmath3 symbols . adding extra @xmath4 symbols helps in recovering the original data in case some of the @xmath5 symbols are lost .",
    "an @xmath1 induces a @xmath6 overhead .",
    "erasure codes were designed for data transmitted over a noisy channel , where coding is used to append redundancy to the transmitted signal to help the receiver recover the intended message , even when some symbols are erasured / corrupted by noise ( see figure  [ fig : ecforcomm ] ) .",
    "codes offering the best trade - off between redundancy and fault - tolerance , called maximum distance separable ( mds ) codes , tolerate @xmath4 erasures , that is , no matter which group of @xmath4 symbols are lost , the original data can be recovered .",
    "the simplest examples are the repetition code @xmath7 ( given @xmath8 symbol , repeat it @xmath5 times ) , which is the same as replication , and the parity check code @xmath9 ( compute one extra symbol which is the sum of the first @xmath2 symbols ) .",
    "the celebrated reed - solomon codes @xcite are another instance of such codes : consider a sequence of @xmath2 symbols as a degree @xmath10 polynomial , which is evaluated in @xmath5 symbols .",
    "conversely , given @xmath5 symbols , or in fact at least ( any ) @xmath2 symbols , it is possible to interpolate them to recover the polynomial and decode the data .",
    "think of a line in the plane .",
    "given any @xmath11 or more points , the line is completely determined , while with only one point , the line is lost .",
    "symbols is encoded into @xmath5 fragments before transmission over an erasure channel . as long as at least @xmath12 symbols arrive at destination",
    ", the receiver can decode the message . ]",
    "this same storage overhead / fault tolerance trade - off has also long been studied in the context of raid storage units .",
    "while raid 1 uses replication , subsequent raid systems integrate parity bits , and reed - solomon codes can be found in raid 6 .",
    "notable examples of new codes designed to suit the peculiarities of raid systems include weaver codes @xcite , array codes @xcite as well as other heuristics @xcite .",
    "optimizing the codes for the nuances of raid systems , such as physical proximity of storage devices leading to clustered failures are natural aspects @xcite gaining traction .",
    "note that even though we do not detail here those codes optimized for traditional raid systems , they may nonetheless provide some benefits in the context of ndss , and vice - versa .",
    "a similar evolution has been observed in the world of ndss , and a wide - spectrum of ndss have started to adopt erasure codes : for example , the new version of google s file system , microsoft s windows azure storage @xcite as well as other storage solution companies such as cleversafe and wuala .",
    "this has happened due to a combination of several factors , including years of implementation refinements , ubiquity of significantly powerful but cheap hardware , as well as the sheer scale of the data to be stored .",
    "we will next elaborate how erasure codes are used in ndss , and while mds codes are optimal in terms of fault - tolerance and storage overhead tradeoffs , why there is a renewed interest in the coding theory community to design new codes that take into account maintenance of ndss explicitly .",
    "in an ndss , if one object is stored using an erasure code and each ` encoded symbol ' is stored at a different node , then the object stays available as long as the number of node failures does not exceed the code recovery capability .",
    "now let individual storage nodes fail according to an i.i.d .",
    "random process with the failure probability being @xmath13 .",
    "the expected number of independent node failures is binomially distributed , hence the probability of losing an object with an @xmath1 mds erasure code is @xmath14 .",
    "in contrast , it is @xmath15 with @xmath16-way replication .",
    "for example , if the probability of failure of individual nodes is @xmath17 , then for the same storage overhead of 3 , corresponding to @xmath18 for replication and to an @xmath19 erasure code , the probabilities of losing an object are @xmath20 and @xmath21 respectively .",
    "such resilience analysis illustrates the high fault - tolerance that erasure codes provide . using erasure codes ,",
    "however , means that a larger number of storage nodes are involved in storing individual data objects .",
    "there is however a fundamental difference between a communication channel , where erasures occur once during transmission , and an ndss , where faults accumulate over time , threatening data availability in the long run .",
    "traditionally , erasure codes were not designed to reconstruct subsets of arbitrary encoded blocks efficiently . when a data block encoded by an mds erasure code is lost and has to be recreated , one would typically first need data equivalent in amount to recreate the whole object in one place ( either by storing a full copy of the data , or else by downloading an adequate number of encoded blocks ) , even in order to recreate a single encoded block , as illustrated in figure  [ fig : ecdrawback ] .    in recent years",
    ", the coding theory community has thus focused on designing codes which better suit ndss nuances , particularly with respect to replenishing lost redundancy efficiently .",
    "the focus of such works has been on ( i ) bandwidth , which is typically a scarce resource in ndss , ( ii ) the number of storage nodes involved in a repair process , ( iii ) the number of disk accesses ( i / o ) at the nodes facilitating a repair , and ( iv ) the repair time , since delay in the repair process may leave the system vulnerable to further faults .",
    "note that these aspects are often interrelated .",
    "there are numerous other aspects , such as data placement , meta - information management to coordinate the network , as well as interferences among multiple objects contending for resources , to name a few prominent ones , which all together determine an actual system s performance .",
    "the novel codes we describe next are yet to go through a comprehensive benchmarking across this wide spectrum of metrics . instead",
    ", we hope to make these early and mostly theoretical results accessible to practitioners , in order to accelerate the process of such further investigations",
    ".     thus the rest of this article assumes a network of @xmath22 nodes , storing one object of size @xmath2 , encoded into @xmath5 symbols , also referred to as encoded blocks or fragments , each of them being stored at distinct @xmath5 nodes out of the @xmath22 choices . when a node storing no symbol corresponding to the object being repaired participates in the repair process by downloading data from nodes owning data ( also called _ live nodes _ ) , it is termed a _",
    "newcomer_. typical values of @xmath5 and @xmath2 depend on the environments considered : for data centers , the number of temporary failures is relatively low , thus small @xmath23 values such as @xmath24 or @xmath25 ( with respective overheads of 1.5 and 1.3 ) are generally fine @xcite . in p2p systems such as wuala , larger parameters like",
    "@xmath26 are desirable to guarantee availability since nodes frequently go temporarily offline . when discussing the repair properties of a code , it is also important to distinguish which repair strategy is best suited : in p2p systems , a lazy approach ( where several failures are tolerated before triggering repair ) can avoid unnecessary repairs since nodes may be temporarily offline .",
    "data centers might instead opt for immediate repairs . yet",
    ", proactive repairs can lead to cascading failures .",
    "thus in all cases , ability to repair multiple faults simultaneously is essential .",
    "in summary , codes designed to optimize the maintenance process should take into account different code parameters , repair strategies , the ability to replenish single as well as multiple lost fragments , and repair time .",
    "recent coding works aimed in particular at :    * ( i ) * minimize the absolute amount of data transfer needed to recreate one lost encoded block at a time when storage nodes fail . _",
    "regenerating codes _",
    "@xcite form a new family of codes achieving the minimum possible repair bandwidth ( per repair ) given an amount of storage per node , where the optimal storage - bandwidth trade - off is determined using a network coding inspired analysis , assuming that each new - coming node contacts @xmath27 arbitrary live nodes for each repair .",
    "regenerating codes , like mds erasure codes , allow data retrievability from any arbitrary set of @xmath2 nodes .",
    "collaborative regenerating codes @xcite are a generalization allowing simultaneous repair of multiple faults .    *",
    "( ii ) * minimize the number of nodes to be contacted for recreating one encoded block , referred to as fan - in . reduction in the number of nodes needed for one repair typically increases the number of ways repair may be carried out , thus avoiding bottlenecks caused by stragglers .",
    "it also makes multiple parallel repairs possible , all in turn translating into faster system recovery . to the best of our knowledge , _ self - repairing codes _ @xcite were the first instances of @xmath1 code families achieving a repair fan - in of 2 for up to @xmath28 simultaneous and arbitrary failures .",
    "since then , such codes have become a popular topic of study under the nomenclature of ` locally repairable codes ' - the name being reminiscent of a relatively well established theoretical computer science topic of locally decodable codes .",
    "other specific instances of locally repairable code families such as @xcite , as well as study of the fundamental trade - offs and achievability of such codes @xcite have commenced in the last years .",
    "local repairability come at a price , since either nodes store the minimum possible amount of data , in which case the mds property has to be sacrificed ( if one encoded symbol can be repaired from other two , any set of @xmath2 nodes including these 3 nodes will not be adequate to reconstruct the data ) , or the amount of data stored in each node has to be increased . a _ resilience analysis _ of self - repairing codes @xcite has shown that object retrieval is little impaired by it , and in fact , the mds property might not be as critical for ndss as it is for communication , since ndss have the option of repairing data .",
    "there are other codes which fall somewhere ` in between ' these extremes .",
    "prominent among these are hierarchical and pyramid codes which we summarize first before taking a closer look at regenerating and locally repairable codes .",
    "consider an object comprising eight data blocks @xmath29 . create three encoded fragments @xmath30 , @xmath31 and @xmath32 using the first two blocks , and repeat the same process for blocks @xmath33 and @xmath34 ( for @xmath35 ) .",
    "one can then build another layer of encoded blocks , namely @xmath36 and @xmath37 .",
    "the fragment @xmath32 may be viewed as providing _ local redundancy _ , while @xmath36 achieves _",
    "global redundancy_. the same idea can be iterated to build a hierarchy ( figure [ fig : hierarchical ] ) , where the next level global redundant fragment is @xmath38 .        consequently ,",
    "when some of the encoded fragments are lost , localized repair is attempted , and global redundancy is used only if necessary .",
    "for instance , if the node storing @xmath30 is lost , then nodes storing @xmath31 and @xmath32 are adequate for repair . however ,",
    "if nodes storing @xmath30 and @xmath32 are both lost , one may first reconstruct @xmath32 by retrieving @xmath36 and @xmath39 , and then rebuild @xmath30 .",
    "this basic idea can be extended to realize more complex schemes , where ( any standard ) erasure coding technique is used in a _ bottom - up _ manner to create local and global redundancy at a level , and the process is iterated .",
    "that is the essential idea behind _ hierarchical codes _",
    "for the same example , one may also note that if both @xmath30 and @xmath31 are lost , then repair is no longer possible .",
    "this illustrates that the different encoded pieces have unequal importance .",
    "because of such assymmetry , the resilience of such codes have only been studied with simulations in @xcite .",
    "in contrast , _ pyramid codes _",
    "@xcite were designed in a top - down manner , but aiming again to have local and global redundancy to provide better fault - tolerance and improve read performance by trading storage space efficiency for access efficiency .",
    "such local redundancy can naturally be harnessed for efficient repairs as well . a new version of pyramid codes , where the coefficients used in the encoding have been numerically optimized , namely locally reconstructable codes @xcite has more recently been proposed and is being used in the azure @xcite system .",
    "we use an example to illustrate the design of a simple pyramid code . take an @xmath40 mds code ,",
    "say a reed - solomon code with generator matrix @xmath41 , of the form @xmath42=[{\\bf",
    "o}_1,\\ldots,{\\bf o}_8,c_1,c_{2},c_{3}].\\ ] ]    a pyramid code can be built from this base code , by retaining the pieces @xmath29 , and two of the other pieces ( without loss of generality , lets say , @xmath43 ) .",
    "additionally , split the data blocks into two groups @xmath44 and @xmath45 , and compute some more redundancy coefficients for each of the two groups , which is done by picking a first symbol @xmath46 corresponding to @xmath47 by setting @xmath48 and @xmath49 corresponding to @xmath47 with @xmath50 .",
    "this results in an @xmath51 , whose codewords look like @xmath52\\ ] ] where @xmath53 is equal to the original code s @xmath47 :    @xmath54    for both hierarchical and pyramid codes , at each hierarchy level , there is some ` local redundancy ' which can repair lost blocks without accessing blocks outside the subgroup , while if there are too many errors within a subgroup , then the ` global redundancy ' at that level will be used .",
    "one moves further up the pyramid until repair is eventually completed .",
    "use of local redundancy means that a small number of nodes is contacted , which translates into a smaller bandwidth footprint .",
    "furthermore , if multiple isolated ( in the hierarchy ) failures occur , they can be repaired independently and in parallel .",
    "in contrast to hierarchical codes , where analysis of the resilience has not been carried out , pyramid codes top - down approach allows to discern distinct failure regimes under which data is recoverable , and regimes when data is not recoverable .",
    "for instance , in the example above , as long as there are three or fewer failures , the object is always reconstructable . likewise ,",
    "if there are five or more failures , then the data can not be reconstructed .",
    "however , there is also an intermediate region , in this simple case , it being the scenario of four arbitrary failures , in which , for certain combinations of failures , data can not be reconstructed , while for others , it can be .    coinciding with these works ,",
    "researchers from the network coding community started studying the fundamental limits and trade - offs of bandwidth usage for regeneration of a lost encoded block vis - a - vis the storage overhead ( subject to the mds constraint ) culminating in a new family of codes , broadly known as _ regenerating codes _ , discussed next .",
    "the repair of lost redundancy in a storage system can be abstracted as an _ information flow graph _ @xcite .",
    "first , the data object is encoded using an @xmath1 mds code and the encoded blocks are stored across @xmath5 storage nodes . each storage node is assumed to store an amount @xmath55 of data ( meaning that the size of an encoded block is at most @xmath55 , since only one object is stored ) .",
    "when one node fails , new nodes contact @xmath56 live nodes and download @xmath57 amount of data from each contacted node in order to perform the repair .",
    "if several failures occur , the model @xcite assumes that repairs are taken care of one at a time .",
    "information flows from the data owner to the data collector as follows ( see figure [ fig : rgccloud ] for an illustration ) : ( 1 ) the original placement of the data distributed over @xmath5 nodes is modeled as directed edges of weight @xmath55 from the sources ( data owners ) to the original storage nodes .",
    "( 2 ) a storage node is denoted by @xmath58 and modeled as two logical nodes @xmath59 and @xmath60 , which are connected with a directed edge @xmath61 with weight @xmath55 representing the storage capacity of the node .",
    "the data flows from the data owner to @xmath59 , then from @xmath59 to @xmath60 .",
    "( 3 ) the regeneration process consists of directed edges of weight @xmath57 from @xmath62 contacted live nodes to the @xmath59 of the newcomer .",
    "( 4 ) finally , the reconstruction / access of the whole object is abstracted with edges of weight @xmath55 to represent the destination ( data collector ) downloading data from arbitrary @xmath2 live storage nodes .",
    "then , the maximum information that can flow from the source to the destination is determined by the max - flow over a min - cut of this graph . for the original object to be reconstructible at the destination , this flow needs to be at least as large as the size of the original object .",
    "any code that enables the information flow to be actually equal to the object size is called a regenerating code ( rgc ) @xcite .",
    "now , given @xmath2 and @xmath5 , the natural question is , _ what are the minimal storage capacity @xmath55 and bandwidth @xmath63 needed for repairing an object of a given size _ ?",
    "this can be formulated as a linear non - convex optimization problem : minimize the total download bandwidth @xmath64 , subject to the constraint that the information flow equals the object size .",
    "the optimal solution is a piecewise linear function , which describes a trade - off between the storage capacity @xmath55 and the bandwidth @xmath57 as shown in figure [ fig : rgctradeoffcurve ] [ @xmath65 , and has two distinguished boundary points : the minimal storage repair ( msr ) point ( when @xmath55 is equal to the object size divided by @xmath2 ) , and the minimal bandwidth repair ( mbr ) point .",
    "the trade - off analysis only determines what can best be achieved , but in itself does not provide any specific code construction .",
    "several codes have since been proposed , most of which operate either at the msr or mbr points of the trade - off curve , e.g. , @xcite .",
    "the specific codes need to satisfy the constraints determined by the max - flow min - cut analysis , however there is no constraint or need to regenerate precisely the same ( bitwise ) data as was lost ( see figure  [ fig : examplergcs ] ( a ) ) . when the regenerated data is in fact not the same as that lost , but nevertheless provides equivalent redundancy , it is called _ functional regeneration _ , while if it is bitwise identical to what was lost , then it is called _ exact regeneration _",
    ", as illustrated in figure  [ fig : examplergcs ] ( b ) .",
    "note that the proof of storage - bandwidth trade - off determined by the min - cut bound does not depend on the type of repair ( functional / exact ) .",
    "the original model @xcite has since been generalized @xcite to show that in case of multiple faults , the new nodes carrying out regenerations can collaborate among themselves to perform several repairs in parallel , which was in turn shown to reduce the overall bandwidth needed per regeneration ( figure [ fig : rgctradeoffcurve ] @xmath66 representing the number of failures / new collaborating nodes ) .",
    "instances of codes for this setting , referred to as collaborative regenerating codes ( crgc ) are rarer than classical regenerating codes , and up to now , only a few code constructions are known @xcite .",
    "( collaborative ) regenerating codes stem from a precise information theoretical characterization .",
    "however , they also suffer from algorithmic and system design complexity inherited from network coding , which is larger than even traditional erasure codes , apart from the added computational overheads .",
    "the value of fan - in @xmath62 for regeneration has also practical implications . with a high fan - in @xmath62 even a small number of slow or overloaded nodes",
    "can thwart the repairs .",
    "the codes proposed in the context of network coding aim at reducing the repair bandwidth , and can be seen as the combination of an mds code and a network code .",
    "hierarchical and pyramid codes instead tried to reduce the repair degree or fan - in ( i.e. , the number of nodes needed to be contacted to repair ) by using ",
    "erasure codes on top of erasure codes \" .",
    "we next present some recent families of locally repairable codes ( lrc ) @xcite , which minimize the repair fan - in @xmath62 , trying to achieve @xmath67 such as @xmath68 or @xmath69 .",
    "forcing the repair degree to be small has advantages in terms of repair time and bandwidth , however , it might affect other code parameters ( such as its rate , or storage overhead ) .",
    "we will next elaborate a few specific instances of locally repairable codes .",
    "the term `` locally repairable '' is inspired by @xcite , where the repair degree @xmath62 of a node is called the  locality @xmath62 \" of a codeword coordinate , and is reminiscent of _ locally decodable _ and _ locally correctable _ codes , which are well established topics of study in theoretical computer science .",
    "self - repairing codes ( src ) @xcite were to our knowledge the first @xmath23 codes designed to achieve @xmath68 per repair for up to @xmath28 simultaneous failures .",
    "other families of locally repairable codes based on projective geometric construction ( projective self - repairing codes ) @xcite and puncturing of reed - mueller codes @xcite have been very recently proposed .",
    "some instances of these latter codes can achieve a repair degree of either 2 or 3 .    with @xmath68 resources of at most two live nodes",
    "may get saturated due to a repair .",
    "thus simultaneous repairs can be carried out in parallel , which in turn provides fast recovery from multiple faults .",
    "for example , in figure  [ fig : hsrc ] if the 7th node fails , it can be reconstructed in 3 different ways , by contacting either @xmath70 , or @xmath71 , or @xmath72 .",
    "if both the 6th and 7th node fail each of them can still be reconstructed in two different ways .",
    "one newcomer can contact first @xmath73 and then @xmath74 to repair @xmath75 , while another newcomer can in parallel contact first @xmath76 then @xmath73 to repair @xmath77 .",
    "figure  [ fig : psrc ] shows another example illustrating how the fan - in can be varied to achieve different repair bandwidths while using src . if a node , say @xmath74 , fails , then the lost data can be reconstructed by contacting a subset of live nodes . two different strategies with different fan - ins @xmath68 and @xmath78 and correspondingly different total bandwidth usage have been shown to demonstrate some of the flexibilities of the regeneration process .",
    "notice that the optimal storage - bandwidth trade - off of regenerating codes does not apply here , since the constraint @xmath79 is relaxed .",
    "thus better trade - off points in terms of total bandwidth usage for a repair can also be achieved ( not illustrated here , see @xcite for details ) .",
    "recall that if a node can be repaired with @xmath80 other nodes then there exist dependencies among them .",
    "the data object can be recovered only out of @xmath2 independent encoded pieces , and hence when the @xmath2 nodes include @xmath81 nodes with mutual dependency , then the data can not be recovered from them .",
    "lrcs however allow recovery of the whole object using many specific combinations of @xmath2 encoded fragments . from the closed form and numerical analyses of @xcite and @xcite , respectively",
    ", one can observe that while there is some deterioration of the static resilience with respect to mds codes of equivalent storage overhead , the degradation is rather marginal .",
    "this can alternatively be interpreted as that for a specific desired value of fault - tolerance , the storage overhead for using lrc is negligibly higher than mds codes .",
    "an immediate caveat emptor that is needed at this juncture is that , the rates of the known instances of locally repairable codes in general , and self - repairing codes in particular , are pretty low , and much higher rates are desirable for practical usage .",
    "the static resilience of such relatively higher rate locally repairable codes , if and when such codes are invented , will need to be revisited to determine their utility .",
    "such trade - offs are yet to be fully understood , though some early works have recently been carried out @xcite .",
    "all the coding techniques we have seen so far address the repairability problem at the granularity of isolated objects that are stored using erasure coding .",
    "however , a simple heuristic of superimposing two codes , one over individual objects , and another across encoded pieces from multiple objects @xcite as shown in figure [ fig : raid4onec ] , can provide good repairability properties as well .",
    "consider @xmath82 objects @xmath83 to be stored .",
    "for @xmath84 , object @xmath85 is erasure encoded into @xmath5 encoded pieces @xmath86 , to be stored in @xmath87 distinct storage nodes .",
    "additionally , _ parity groups _ formed by @xmath82 encoded pieces ( with one encoded piece chosen from each of the @xmath82 objects ) can be created , together with a parity piece ( or xor ) , where w.l.o.g , a parity group is of the form @xmath88 for @xmath89 , and the parity piece @xmath90 is @xmath91 .",
    "the parity pieces are then stored in additional @xmath5 distinct storage nodes .",
    "such an additional redundancy is akin to raid-4 .",
    "code , while a parity bit is computed vertically across @xmath82 objects , where @xmath82 is a design parameter . ]    this code design , called is similar to a two - dimensional product code  @xcite in that the coding is done both horizontally and vertically . in the context of raid systems ,",
    "similar strategy has also been applied to create intra - disk redundancy @xcite .",
    "the design objectives here are somewhat different , namely : ( i ) the horizontal layer of coding primarily achieves fault - tolerance by using an @xmath23 erasure coding of individual objects , while ( ii ) the vertical single parity check code mainly enables cheap repairs ( by choosing a suitable @xmath82 ) by creating raid-4 like parity of the erasure encoded pieces from different objects .",
    "the number of objects @xmath82 that are cross - coded indeed determines the fan - in for repairing isolated failures independently of the code parameters @xmath5 and @xmath2 .",
    "if @xmath92 , it can be shown that the probability that more than one failure occurs per column is small , and thus repair using the parity bit is often enough - resulting in cheaper repairs , while relatively infrequently repairs may have to be performed using the @xmath23 code .",
    "the choice of @xmath82 determines trade - offs between repairability , fault - tolerance and storage overheads which have been formally analyzed in @xcite .",
    "somewhat surprisingly , the analysis demonstrates that for many practical parameter choices , this cross - object coding achieves better repairability while retaining equivalent fault - tolerance as maximum distance separable erasure codes incurring equivalent storage overhead .",
    "such a strategy also leads to other practical concerns as well as opportunities , such as the issues of object deletion or updates , which need further rigorous investigation before considering them as a practical option .",
    "the coding techniques presented in this paper have so far undergone only partial evaluation and benchmarking , and more rigorous evaluation of even the stand - alone approaches is ongoing work for most .",
    "thus , it is somewhat premature to provide results from any comparative study , though some preliminary works on the same have also recently been carried out @xcite taking into consideration realistic settings where multiple objects are collocated in a common pool of storage nodes , and multiple storage nodes may potentially fail simultaneously , all creating interferences between the different repair operations competing for the limited and shared network resources . instead , we give one example of a theoretical result by considering the repair bandwidth per repair in the presence of multiple failures for some of these codes , and we provide an overview of what a system designer may expect from all these codes in table-[tab : parametersummary ] .",
    "we further enumerate several other metrics that need to be studied to better understand their applicability .",
    "[ cols=\"<,<,^ , < , < , < \" , ]      per lost block for various choices of @xmath93 ( @xmath94 is the size of the stored object ) for ( n=31,k=8 ) encoding schemes . for parallel repairs using erasure codes",
    "the traffic is @xmath95 ( not shown ) .",
    "the src code parameters are denoted as src(n , k ) . ]",
    "one would not allow in practice failures to accumulate indefinitely , and instead a regeneration process will have to be carried out .",
    "if this regeneration is triggered when precisely @xmath93 out of the @xmath5 storage nodes are still available , then the total bandwidth cost to regenerate each of the @xmath96 failed nodes is depicted in figure [ fig : gamma ] . note that delayed repair where multiple failures are accumulated may be a design choice , as in p2p systems with frequent temporary outages , or an inevitable effect of correlated failures where multiple faults accumulate before the system can respond .    for locally repairable codes such as src the repairs can be done in sequence or in parallel , denoted @xmath97 and @xmath98 respectively in the figure .",
    "this is compared with mds erasure codes ( @xmath99 ) when the repairs are done in sequence , as well as with rgc codes at msr point ( @xmath100 ) for a few choices of @xmath62 .",
    "the bandwidth need has been normalized with the size of one encoded fragment .",
    "we notice that for up to a certain point , self - repairing codes have the least ( and a constant of 2 ) bandwidth need for repairs even when they are carried out in parallel .    for larger number of faults ,",
    "the absolute bandwidth usage for traditional erasure codes and regenerating codes is lower than that of self - repairing codes .",
    "however given that erasure codes and regenerating codes need to contact @xmath2 and @xmath56 nodes respectively , some preliminary empirical studies have shown the regeneration process for such codes to be slow @xcite which can in turn make the system vulnerable . in contrast , because of an extremely small fan - in @xmath68 , self - repairing codes can support fast and parallel repairs @xcite while dealing with a much larger number of simultaneous faults .",
    "comparison with some other codes such as hierarchical and pyramid codes has been excluded here due to the lack of necessary analytical results , as well as the fact that the different encoded pieces have assymetrical importance , and thus , just the number of failures does not adequately capture the system state for such codes .    given that repair processes run continuously or as and when deemed necessary ,",
    "the static resilience is not the most relevant metric of interest for storage system designers .",
    "often , another metric , namely _",
    "mean time to data loss _",
    "( mttdl ) is used to characterize the reliability of a system .",
    "mttdl is determined by taking into account the cumulative effect of the failures along with that of the repair processes . for the novel codes discussed in this manuscript ,",
    "such study of mttdl is yet to be carried out in the literature .",
    "a qualitative remark worth emphasizing is that , precisely because of the better repair characteristics such as fast repairs , some of these codes are likely to improve mttdl significantly .",
    "whether the gains outweigh the drawbacks , such as the lack of mds property ( and consequent poorer static resilience ) , is another open issue .",
    "there is a long tradition of using codes for storage systems .",
    "this includes traditional erasure codes as well as turbo and low density parity check codes ( ldpc ) coming from communication theory , rateless ( digital fountain and tornado ) codes originally designed for content distribution centric applications , or locally decodable codes emerging from the theoretical computer science community to cite a few .",
    "the long believed mantra in applying codes for storage has been ` _ _ the storage device is the erasure channel _ _ ' .",
    "such a simplification ignores the maintenance process in ndss for long term reliability .",
    "this realization has led to a renewed interest in designing codes tailor - made for ndss .",
    "this article surveys the major families of novel codes which emphasize primarily better repairability .",
    "there are many other system aspects which influence the overall performance of these codes , that are yet to be benchmarked .",
    "this high level survey is aimed at exposing the recent theoretical advances providing a single and easy point of entry to the topic .",
    "those interested in further mathematical details depicting the construction of these codes may refer to a longer and a more rigorous survey @xcite in addition to the respective individual papers .",
    "a. datta s work was supported by moe tier-1 grant rg29/09 .",
    "f. oggier s work was supported by the singapore national research foundation under research grant nrf - crp2 - 2007 - 03 .",
    "99 apache.org .",
    "hadoopfs - raid .",
    "http://wiki.apache.org/hadoop/hdfs-raid , 2012 .",
    "b. calder , et al .",
    ", `` windows azure storage : a highly available cloud storage service with strong consistency '' twenty - third acm symposium on operating systems principles , sosp 2011 .",
    "y. cassuto , j. bruck , `` low - complexity array codes for random and clustered 4-erasures '' , ieee transactions on information theory , 01/2012 .",
    "a. datta and f. oggier , ",
    "redundantly grouped cross - object coding for repairable storage `` , asia - pacific workshop on systems , apsys 2012 .",
    "a. dholakia , e. eleftheriou , x - y .",
    "hu , i. iliadis , j. menon , k.k .",
    "rao , `` a new intra - disk redundancy scheme for high - reliability raid storage systems in the presence of unrecoverable errors '' , acm transactions on storage , 2008 .",
    "a. g. dimakis , p. b. godfrey , y. wu , m. wainwright and k. ramchandran , ' ' network coding for distributed storage systems \" ieee transactions on information theory , vol .",
    "56 , issue 9 , sept . 2010 .",
    "a. duminuco , e. biersack ,  hierarchical codes : how to make erasure codes attractive for peer - to - peer storage systems \" , eighth international conference on in peer - to - peer computing , p2p 2008 .",
    "p. elias ,  error - free coding `` , transactions on information theory , vol .",
    "4 , september 1954 .",
    "s. ghemawat , h. gobioff , s - t . leung , ' ' the google file system \" , acm symposium on operating systems principles , sosp 2003",
    ".        j. l. hafner , `` weaver codes : highly fault tolerant erasure codes for storage systems '' , 4th conference on usenix conference on file and storage technologies , fast 2005 .",
    "h. d. l. hollmann , `` storage codes - coding rate and repair locality '' , international conference on computing , networking and communications , icnc 2013 .",
    "c. huang , m. chen , and j. li ,  pyramid codes : flexible schemes to trade space for access efficiency in reliable data storage systems \" , sixth ieee international symposium on network computing and applications , nca 2007 . c. huang , h. simitci , y. xu , a. ogus , b. calder , p. gopalan , j. lin , s. yekhanin ,  erasure coding in windows azure storage \" , usenix conference on annual technical conference , usenix atc 2012 .",
    "kermarrec , n. le scouarnec , g. straub ,  repairing multiple failures with coordinated and adaptive regenerating codes \" , the 2011 international symposium on network coding , netcod 2011 .",
    "f. oggier , a. datta ,  self - repairing homomorphic codes for distributed storage systems \" , the 30th ieee international conference on computer communications , infocom 2011 . extended version at http://arxiv.org/abs/1107.3129 f. oggier , a. datta ,  self - repairing codes for distributed storage  a projective geometric construction \" , ieee information theory workshop , itw 2011",
    ". f. oggier , a. datta ,  homomorphic self - repairing codes for agile maintenance of distributed storage systems \" , http://arxiv.org/abs/1107.3129 f. oggier , a. datta , `` coding techniques for repairability in networked distributed storage systems '' , http://sands.sce.ntu.edu.sg/codingfornetworkedstorage/pdf/longsurvey.pdf , september 2012 . l. pamies - juarez , e. biersack ,  cost analysis of redundancy schemes for distributed storage systems \" ,",
    "arxiv:1103.2662 , 2011 . l. pamies - juarez , f. oggier , a. datta , `` an empirical study of the repair performance of novel coding schemes for networked distributed storage systems '' , arxiv:1206.2187 , 2012 .",
    "d. a. patterson , g. gibson , r. h. katz  a case for redundant arrays of inexpensive disks ( raid ) \" acm sigmod international conference on management of data , 1988 .",
    "k. v. rashmi , n. b. shah , p. vijay kumar , k. ramchandran , `` explicit construction of optimal exact regenerating codes for distributed storage '' , allerton 2009 .",
    "i. s. reed , g. solomon ,  polynomial codes over certain finite fields \" , journal of the society for industrial and appl .",
    "mathematics , no 2 , vol 8 , siam , 1960 .",
    "k. w. shum ,  cooperative regenerating codes for distributed storage systems \" , ieee international conference on communications , icc 2011 ."
  ],
  "abstract_text": [
    "<S> the increasing amount of digital data generated by today s society asks for better storage solutions . </S>",
    "<S> this survey looks at a new generation of coding techniques designed specifically for the maintenance needs of networked distributed storage systems ( ndss ) , trying to reach the best compromise among storage space efficiency , fault - tolerance , and maintenance overheads . </S>",
    "<S> four families of codes , namely , pyramid , hierarchical , regenerating and locally repairable codes such as self - repairing codes , along with a heuristic of cross - object coding to improve repairability in ndss are presented at a high level . </S>",
    "<S> the code descriptions are accompanied with simple examples emphasizing the main ideas behind each of these code families . </S>",
    "<S> we discuss their pros and cons before concluding with a brief and preliminary comparison . </S>",
    "<S> this survey deliberately excludes technical details and does not contain an exhaustive list of code constructions . instead , it provides an overview of the major novel code families in a manner easily accessible to a broad audience , by presenting the big picture of advances in coding techniques for maintenance of ndss . + * keywords : coding techniques , networked distributed storage systems , hierarchical codes , pyramid codes , regenerating codes , locally repairable codes , self - repairing codes , cross - object coding . * </S>"
  ]
}