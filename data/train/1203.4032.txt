{
  "article_text": [
    "the density  @xmath6 of particles undergoing anomalous subdiffusion satisfies the integrodifferential equation  @xcite @xmath7 for a parameter  @xmath8 in the range @xmath9 , where @xmath10 is a generalized diffusivity and @xmath11 is a homogeneous term .",
    "we consider   for  @xmath12 and for  @xmath13 in a bounded , convex  or @xmath14 domain  @xmath15 , subject to homogeneous boundary conditions , either of dirichlet type , @xmath16 or of neumann type , @xmath17 where @xmath18 denotes the outward unit normal for  @xmath19 .",
    "in addition , we specify the initial condition @xmath20 in the limit as  @xmath21 , the evolution equation   reduces to the classical diffusion equation , @xmath22 , in which the flux  @xmath23 depends only on the instantaneous value of the gradient .",
    "by contrast , in   the flux depends on the entire history of the gradient , and this fact leads to significant computational challenges , particularly if the spatial dimension  @xmath24 .    in  @xcite , we solved the foregoing initial - boundary value problem using a piecewise - constant , discontinuous galerkin ( dg ) method for the time discretization , combined with a standard , continuous piecewise - linear finite element discretization in space . for simplicity , in the present work",
    "we assume that both the spatial mesh and the time steps are quasiuniform . defining the elliptic partial differential operator  @xmath25",
    ", we assume that for some @xmath26 $ ] the solution  @xmath27 satisfies regularity estimates of the form  @xcite @xmath28 for @xmath29 .",
    "the dg solution  @xmath30 therefore satisfies an error bound  ( * ? ? ?",
    "* theorem  3 ) in the norm of  @xmath31 , @xmath32 where @xmath33 is the @xmath34th time level , @xmath35 , @xmath36 is the maximum time step , @xmath37 is the maximum diameter of the spatial finite elements and @xmath38 .",
    "when @xmath39 , we could achieve full first - order accuracy with respect to  @xmath36 ( ignoring logarithmic factors ) by relaxing the assumption that the time steps are quasi - uniform , but to do so would complicate our fast solution procedure .",
    "section  [ sec : dg method ] provides a precise description of the dg method , which can be interpreted as a type of implicit euler scheme .",
    "denote the @xmath40th _ free _ node by  @xmath41 , and let @xmath42 . at the @xmath34th time step , we compute the vector of nodal values , @xmath43\\in{\\mathbb{r}}^m$ ] , by solving a linear system @xmath44 here , @xmath45  and @xmath46 are the mass and stiffness matrices arising from the spatial discretization",
    ", @xmath47 is the length of the @xmath34th time interval , @xmath48 is the average value of the load vector for  @xmath49 , and the weights are given by @xmath50 and @xmath51 the condition number of the matrix  @xmath52 is @xmath53 , and we assume the use of an efficient elliptic solver costing @xmath54  operations . by comparison ,",
    "computing the right - hand side in the obvious way requires @xmath55  operations .",
    "moreover , we must keep the vector  @xmath56 in active memory for _ all _ the previous time levels @xmath57 , @xmath58 , ",
    ", @xmath59 , requiring @xmath55  locations .",
    "thus , the total cost for @xmath0  time steps is @xmath2  operations and @xmath3  active memory locations , whereas applying the same dg method to a _ classical _ diffusion equation ( which has no memory term ) costs only @xmath3  operations and @xmath54  active locations . in other words",
    ", solving the fractional diffusion equation in this way costs @xmath0  times as much as solving a classical diffusion equation .",
    "cuesta , lubich and palencia  @xcite studied the time discretization of   by convolution quadrature , and schdle , lpez - fernndez and lubich  @xcite developed a fast solution algorithm costing @xmath4  operations and using @xmath5  active memory locations .",
    "the purpose of this paper is to present a fast summation algorithm for the dg method   that likewise costs @xmath4  operations and @xmath5  active memory locations .    the algorithm is closely related to the panel clustering technique for boundary element methods , introduced by hackbusch and nowak  @xcite . to explain the basic strategy ,",
    "suppose that instead of @xmath60 the integral term had a degenerate kernel  @xmath61 .",
    "in this case , @xmath62 where @xmath63  and @xmath64 .",
    "since @xmath65 and since @xmath66 , evaluating the right - hand side of the linear system   would cost only @xmath67  operations and there would be no need to retain in active memory the solution at all previous time levels .",
    "our kernel  @xmath60 is not degenerate , but it can be approximated to high accuracy by a degenerate kernel if we restrict @xmath68  and @xmath69 to suitable , well - separated intervals .",
    "consequently , if @xmath33  and @xmath70 are restricted in the same way , then @xmath71 can be approximated to high accuracy by a sum  @xmath72 of the form  , leading to a fast method to evaluate the sum that occurs on the right - hand side of  .    in section  [ sec : dg method ] we investigate the effect of perturbing the dg method in this way , and section  [ sec : low - rank ] presents a simple scheme for generating the @xmath72 via taylor expansion .",
    "section  [ sec : cluster ] describes the _ cluster tree _ ,",
    "whose nodes are contiguous families of time intervals , used to appropriately restrict @xmath33  and @xmath70 .",
    "the fast summation algorithm is then defined via the concept of an _ admissible covering _ , and we present an error estimate in theorem  [ thm : error ] .",
    "further investigation of the cluster tree in section  [ sec : cost ] allows us to prove , in theorem  [ thm : operations ] , that the algorithm requires @xmath73  operations . in section  [ sec : memory ] , we present a memory management strategy and show , in theorem  [ thm : active memory ] , that with this strategy the fast summation algorithm uses at most @xmath5  active memory locations during each time step .",
    "section  [ sec : example ] presents a numerical example , and the paper concludes with three technical appendices .",
    "although presented here only for equation   discretized in time using a piecewise - constant dg method , the fast summation algorithm does not depend in any essential way on this specific choice and could be used for many other time stepping procedures for evolution problems with memory .",
    "the key requirements are that the quadrature weights are computed via local averages of the kernel and that , away from the diagonal , the derivatives of the kernel exist and decay appropriately . also , the approximation scheme of section  [ sec : low - rank ] , based on taylor expansions , is only one possibility , chosen because it is simple to analyse for the kernel in  .",
    "we could instead use an interpolation scheme that requires a user to supply only pointwise values of the kernel .",
    "we define @xmath74 and denote the riemann  liouville fractional differentiation operator of order  @xmath75 by @xmath76 and let @xmath77 so that , suppressing the dependence on  @xmath13 , @xmath78 we also denote the inner product in  @xmath31 and the bilinear form associated with  @xmath79 by @xmath80 the weak form of   is then @xmath81 where @xmath82 is the sobolev space  @xmath83 in the case of dirichlet boundary conditions  , and @xmath84 in the case of neumann boundary conditions  .",
    "the numerical solution  @xmath85 is a piecewise - constant function of  @xmath68 , with coefficients belonging to a continuous , piecewise - linear finite element space  @xmath86 .",
    "thus , @xmath87 generally has a jump discontinuity at each time level  @xmath33 , and we adopt the usual convention of treating @xmath88 as continuous for  @xmath68 in the half - open interval  @xmath89 $ ] , writing @xmath90^n = u^n_+-u^n.\\ ] ] the dg time - stepping procedure is determined by the variational equation @xmath91\\,dt\\\\      = { \\langleu^{n-1},v^{n-1}_+\\rangle}+\\int_{i_n}{\\langlef(t),v(t)\\rangle}\\,dt,\\end{gathered}\\ ] ] which must hold for every piecewise - constant test function  @xmath92 with coefficients in  @xmath93 . in our piecewise - constant case , @xmath94 so this variational equation reduces to finding @xmath95 such that @xmath96 where @xmath97 and @xmath98 thus , the vector of nodal values of  @xmath99 satisfies  , with weights given by  and .",
    "our fast algorithm approximates @xmath100 with @xmath101 where @xmath102 for @xmath103 , and yields @xmath104 satisfying the perturbed equation @xmath105 with @xmath106 .",
    "if , for every real - valued , piecewise - constant function  @xmath92 , @xmath107 then the perturbed problem   is stable : @xmath108    this estimate follows by a simple energy argument  ( * ? ? ?",
    "* theorem  1 ) .    in appendix",
    "[ sec : lower bound ] , we prove a lower bound @xmath109 where the constant  @xmath110 depends only on  @xmath8 , allowing us to show the following .",
    "[ cor : stability ] the perturbed dg method   is stable if @xmath111 and @xmath112    write @xmath113 and observe that since  @xmath114 , @xmath115    to estimate the effect on the solution  @xmath99 of perturbing the weights  @xmath71 , we introduce the piecewise - constant interpolation operator @xmath116 in the next result , for simplicity we treat the semi - discrete dg method in which there is no spatial discretization ; the fully - discrete method could be analysed using the methods of mclean and mustapha  ( * ? ? ?",
    "* section  5 ) .",
    "[ thm : accuracy ] assume that @xmath117 and @xmath118 .",
    "if the perturbed equation   is stable , and if @xmath119 then @xmath120    since @xmath121 satisfies   and @xmath27 satisfies @xmath122 the difference  @xmath123 satisfies @xmath124 where @xmath125 .",
    "noting that @xmath126 , stability of the perturbed equation implies that @xmath127 and from  and we see that @xmath128    the second term on the right - hand side of   does not involve  @xmath72 and is @xmath129  @xcite .",
    "recall from   that @xmath130 , and from   that for @xmath103 , @xmath131      \\,ds      = -\\int_{i_j}\\int_{i_n}\\omega_{\\nu-1}(t - s)\\,dt\\,ds.\\ ] ] denote the midpoint of the interval  @xmath132 by @xmath133 , and define @xmath134 for @xmath135 and @xmath136 .",
    "the approximate weights  @xmath72 are determined as follows . see appendices [ sec : eval ]  and [ sec : computing low ] for notes on the stable evaluation of these quantities .",
    "[ thm : taylor expansion ] let @xmath137 and suppose that @xmath138 with @xmath139,\\qquad i_n\\subseteq(c , d],\\qquad \\frac{b - a}{c - b}\\le\\eta.\\ ] ] denote the midpoint of  @xmath140 $ ] by @xmath141 , and define @xmath142 and @xmath143 then @xmath144    taylor expansion about  @xmath145 gives @xmath146 so , by  , integrating over @xmath147  and @xmath148 gives @xmath149 with @xmath150 since @xmath151 $ ] and @xmath152 $ ] , we have @xmath153 , so @xmath154 if @xmath155 , then @xmath156 so @xmath157 . however , if @xmath158 then @xmath159 so @xmath160 also , @xmath161 because @xmath162 $ ] , and assumption   implies that @xmath163 .",
    "thus , @xmath164 and since @xmath165 , it follows that @xmath166 the identity @xmath167 shows that @xmath168 and because @xmath169 , @xmath170    whereas @xmath171 depends on  @xmath140 $ ] ( via @xmath145 ) , in the following alternative expansion @xmath172 depends only on @xmath173  and @xmath174 .",
    "however , the sum that defines @xmath175 is susceptible to loss of precision .",
    "[ cor : phi psi star ] if we define @xmath176 then @xmath177    the binomial expansion gives @xmath178 so @xmath179",
    "we introduce the notation @xmath180 and refer to any such set of consecutive subintervals as a  _ cluster_. a _ cluster tree _ for the mesh  @xmath181 is a tree  @xmath182 having the following properties :    1 .",
    "each node of  @xmath182 is a cluster ; 2 .",
    "the root node of  @xmath182 is @xmath183 ; 3 .",
    "each node is either a leaf or else equals the disjoint union of its children .",
    "let @xmath184 denote the set of leaves in the cluster tree  @xmath182 , and observe that for any two distinct nodes @xmath185 of  @xmath182 exactly one of the following holds  ( * ? ? ?",
    "* remark  3.4 ) : @xmath186 ; @xmath187 ; or @xmath188 .    in the obvious way",
    ", we define the _ length _ of a cluster  @xmath189 to be the length of the underlying interval  @xmath190 ; thus , @xmath191 the _ distance _ between two clusters is the euclidean distance between the underlying point sets in  @xmath192 , @xmath193 so in particular , @xmath194 if @xmath195 .",
    "we define the _ history _ of a cluster to be the entire preceding half - open interval : @xmath196\\quad\\text{if $ { \\textnormal{\\textsf{c}}}={\\textnormal{\\textsf{c}}}(j , n)$.}\\ ] ]    given a leaf  @xmath197 , and a parameter  @xmath198 in the range  @xmath137 , we say that a cluster  @xmath189 is @xmath199-_admissable _ if @xmath200 thus , the conclusions of theorem  [ thm : taylor expansion ] hold for @xmath201 and @xmath202 with @xmath203 $ ]  and @xmath204 $ ] .",
    "notice that if @xmath189 is @xmath199-admissible , then so are all the descendents of  @xmath189 .",
    "an _ @xmath199-admissible cover _ is a set  @xmath205 of clusters such that    1 .",
    "each cluster  @xmath206 is a node of  @xmath182 ; 2 .",
    "@xmath207 ; 3 .",
    "if @xmath208 , @xmath209 with  @xmath185 , then @xmath188 ; 4 .",
    "if @xmath206 then either @xmath189 is @xmath199-admissible , or else @xmath210 .",
    "a trivial example of an @xmath199-admissible cover is the set of leaves in the history of  @xmath211 , that is , @xmath212    [ lem : non - ad leaves ] all @xmath199-admissible covers contain the same non - admissible leaves .",
    "let @xmath213 and @xmath214 be @xmath199-admissible covers , and suppose that @xmath215 is not @xmath199-admissible . since @xmath216",
    ", there exist @xmath217 such that @xmath218 intersects at least one interval  @xmath219 .",
    "since @xmath220 is a leaf of  @xmath182 , it follows that @xmath221 and hence @xmath189 is not @xmath199-admissible .",
    "thus , @xmath189 must be a leaf , because @xmath214 is an @xmath199-admissible cover , and we conclude that @xmath222 .    algorithm  [ alg : divide ] defines a recursive procedure , @xmath223 , that we use to define a family of clusters  @xmath224 , as follows :    @xmath225 , @xmath226 @xmath227 @xmath228    this construction is a modified version of  ( * ? ? ? * ( 3.8b ) ) .",
    "[ thm : minc ] @xmath224 is the unique minimal @xmath199-admissible cover .",
    "suppose that @xmath205 is an @xmath199-admissible cover .",
    "let @xmath229 and let @xmath189 be any node of  @xmath205 that intersects @xmath230",
    ". if @xmath230 is a leaf , then @xmath230 is not @xmath231-admissible so @xmath232 by lemma  [ lem : non - ad leaves ] . if @xmath230 is not a leaf , then either @xmath189 is a leaf , in which case @xmath233 , or else @xmath189 is @xmath199-admissible and @xmath234 by the construction of  @xmath224 using algorithm  [ alg : divide ] . hence , in all cases",
    "@xmath234 , so either @xmath235 or else @xmath236 .",
    "determine @xmath237 , @xmath238 , @xmath239 , @xmath240 such that @xmath203 $ ] and @xmath204 $ ] @xmath226 @xmath241 @xmath241 @xmath242    the minimal admissible cover  @xmath224 is the disjoint union of the sets @xmath243 given @xmath34 , there is a unique leaf  @xmath244 containing @xmath132 , and we define @xmath245 and @xmath246 using the formulae of theorem  [ thm : taylor expansion ] with @xmath247=\\bigcup{\\textnormal{\\textsf{c}}}$ ] and @xmath248=\\bigcup{\\textnormal{\\textsf{l}}}_n$ ] .",
    "we then define @xmath249 and @xmath250 so that @xmath251 where @xmath252 to evaluate @xmath253 , we compute @xmath254    the results of sections [ sec : dg method ]  and [ sec : low - rank ] now yield the following estimate for the additional error incurred by using the approximating sum  .",
    "recall that @xmath255 is the constant appearing in the lower bound  .",
    "[ thm : error ] define @xmath72 according to theorem  [ thm : taylor expansion ] ,  and , and put @xmath256 .",
    "the perturbed problem   is stable if @xmath257 in which case the error estimate   for the semidiscrete dg method holds with @xmath258    recalling , we have @xmath259 and @xmath260}{\\gamma(\\nu+1 ) }      \\le\\frac{k_j^\\nu}{\\gamma(\\nu+1)}.\\end{aligned}\\ ] ] thus , by theorem  [ thm : taylor expansion ] , @xmath261 and @xmath262 therefore , corollary  [ cor : stability ] shows that the perturbed scheme is stable if @xmath263 and since @xmath264 the error bound follows at once from theorem  [ thm : accuracy ] .    since @xmath265",
    "because the mesh is quasiuniform , we have stability if @xmath266 , and the error factor satisfies",
    "@xmath267    the alternative expansion from corollary  [ cor : phi psi star ] gives @xmath268  and @xmath269 such that @xmath270 so @xmath271 if @xmath189 is not a leaf , then @xmath189 is the union of its children , so @xmath272 thus , once we compute @xmath273 for every leaf  @xmath189 of  @xmath182 , we can compute @xmath273 for the remaining nodes  @xmath189 by aggregation .",
    "we now seek to estimate the number of operations required to evaluate the right - hand side of  .",
    "let @xmath274 denote the _ generation _ of the node  @xmath275 , defined recursively by @xmath276 and @xmath277 put @xmath278 and note the @xmath279 since the root node of the cluster tree can not be @xmath199-admissible .",
    "we formlate the following regularity condition for the cluster tree .    for integers @xmath280 and @xmath281 , we say that @xmath182 is _ @xmath282-uniform _ if , for some constants @xmath283 and for every node  @xmath275 ,    1 .",
    "@xmath284 ; 2 .",
    "@xmath285 whenever @xmath286 ; 3 .",
    "@xmath287 whenever @xmath288 ; 4 .   @xmath289 $ ] for @xmath290 .",
    "for example , given a uniform mesh with @xmath291  subintervals and @xmath292 , recursive bisection of  @xmath293 $ ] for @xmath294  generations leads to a @xmath295-uniform cluster tree in which each leaf contains @xmath296  subintervals .",
    "we assume henceforth that @xmath182 is @xmath282-uniform .",
    "since @xmath297 and @xmath298 , we see that @xmath299 , implying that @xmath300 also , @xmath301 so @xmath302 the next result shows that @xmath303 .",
    "[ lem : minc count ] suppose that @xmath197 and @xmath304 . if @xmath203 $ ] and @xmath204 $ ] , then @xmath305 whereas @xmath306 therefore , @xmath307    if @xmath308 , then @xmath189 is @xmath199-admissible so @xmath309 and thus @xmath310 .",
    "suppose for a contradiction that @xmath311 , and let @xmath312 .",
    "if @xmath313 $ ] , then @xmath314 and @xmath315 , so @xmath316 and @xmath317 showing that @xmath318 is @xmath199-admissible , which is impossible because @xmath319 .",
    "thus , @xmath320 and @xmath321 .",
    "now let @xmath322 and suppose for a contradiction that @xmath323 .",
    "since @xmath324 it follows that @xmath325 so @xmath189 is @xmath199-admissible , which is impossible because @xmath189 is a leaf .",
    "thus , @xmath326 and @xmath327 .    as a straight forward consequence ,",
    "we obtain the desired operation counts .",
    "[ thm : operations ] if @xmath182 is @xmath282-uniform , then the right - hand side of   can be computed for  @xmath328 in order @xmath329 operations .",
    "if @xmath330 , then the number of subintervals in  @xmath189 is @xmath331 so computing @xmath332 requires @xmath333  operations . since @xmath334 and @xmath322 whenever @xmath335 , we see that the total cost for all of the near - field sums is @xmath336 .",
    "the sum  @xmath253 costs @xmath67  operations , and since @xmath337 the total cost for all of the far - field sums is @xmath338  operations . in addition , computing @xmath339 for every  @xmath189 with  @xmath330 costs @xmath3  operations , so computing this sum for @xmath340 and @xmath341 costs @xmath342  operations .",
    "thus , the overall cost for @xmath0  time steps is of order @xmath343  operations .    if we choose @xmath344 so that @xmath345 and each leaf contains only a single subinterval , then the cost is @xmath346 , as claimed in the introduction . in practice ,",
    "the overheads associated with the tree data structure mean that it may be more efficient to choose @xmath347 .",
    "from   we have @xmath348 , which implies that to store @xmath349 for all @xmath350  and @xmath340 we require @xmath351  memory locations . storing @xmath104 for  @xmath328 requires a further @xmath3  locations .",
    "however , at the @xmath34th time step only a small fraction of this memory is active , in the sense that the data it holds play a role in computing  @xmath104 .",
    "figure  [ fig : clusters ] illustrates a @xmath352-uniform cluster tree with @xmath353  and @xmath354 .",
    "the black cluster is the current leaf  @xmath355 , and the red clusters belong to the minimal @xmath356-admissible cover  @xmath357 .",
    "as we will now explain , memory associated with the green and red clusters is active , whereas that associated with the blue and magenta clusters is not .     shown in black , the minimal admissible cover  @xmath357 in red and the other active clusters in green",
    ". the blue clusters are not yet active and the magenta clusters are no longer active . ]    for each cluster  @xmath275 , either there is no  @xmath34 such that @xmath358 , or else there is a unique smallest  @xmath359 such that @xmath358 . moreover ,",
    "if @xmath360 for some @xmath361 , then an ancestor of  @xmath189 must belong to  @xmath357 and we have @xmath362 for every @xmath363 .",
    "hence , there is also a unique @xmath364 such that @xmath365 so , if @xmath189 is not a leaf , the sums @xmath366\\ ] ] contribute to the far - field sum  @xmath367 if and only if @xmath368 .",
    "we can therefore deallocate the @xmath67  memory locations used to store  @xmath369 once @xmath104 has been computed for  @xmath364 .    for each  @xmath34 ,",
    "define a subtree @xmath370 so that @xmath349 includes a term in  @xmath104 if and only if @xmath371 . in algorithm  [ alg : time stepping ]",
    ", after computing @xmath104 we update all far - field sums  @xmath372 with  @xmath371 , so that @xmath104 is subsequently needed only for computing near - field sums . in this way , we can deallocate the @xmath373  memory locations used to store @xmath56 for @xmath374 once @xmath104 has been computed for  @xmath375 .",
    "algorithm  [ alg : free ] defines a recursive procedure @xmath376 that deallocates the memory associated with the children of  @xmath189 , and with their descendants if not already deallocated .",
    "find @xmath357 using algorithm  [ alg : divide ] @xmath377 compute @xmath378 using allocate @xmath104 and solve write @xmath104 to disk allocate @xmath369 and initialize to 0 .",
    "@xmath379    @xmath380 deallocate @xmath369    [ thm : active memory ] the number of active memory locations used during the execution of algorithm  [ alg : time stepping ] is never more than @xmath381 .",
    "suppose that the memory associated with  @xmath189 is active during the @xmath34th time step , and that @xmath203 $ ] and @xmath382 $ ] .",
    "( so in figure  [ fig : clusters ] , @xmath189 is green or red or black . )",
    "if @xmath330 , then by lemma  [ lem : minc count ] , @xmath383 and since @xmath384  and @xmath385 , we see that the number of such clusters is at most @xmath386}{\\lambda tq^{-\\ell } }      & \\le\\frac{\\lambda}{\\lambda}\\bigl(q^{\\ell - g}+1+(1+\\eta^{-1})q\\bigr)\\\\      & \\le \\frac{\\lambda}{\\lambda}(1+\\eta^{-1})(q+1).\\end{aligned}\\ ] ] storing @xmath349 requires @xmath387  memory locations , so the desired estimate follows after adding the contributions for  @xmath388 .    since @xmath389 , theorem  [ thm : active memory ] justifies the claim in the introduction that the memory requirements are proportional to  @xmath390 .",
    "theorems [ thm : operations ]  and [ thm : active memory ] show that  for a given choice of @xmath1  and @xmath0 and a given cluster tree  the computational cost , both with respect to the number of operations and to the number of active memory locations , is proportional to  @xmath391 . at the same time , by theorem  [ thm : error ] , to achieve the desired accuracy we must ensure that @xmath392 is sufficiently small .",
    "the next result shows the relation between @xmath393  and @xmath198 that is optimal in the sense of achieving a given accuracy for the least computational cost .",
    "[ prop : eta ] for a given  @xmath394 , the ratio  @xmath391 is minimised subject to the constraint @xmath395 by choosing @xmath396    introducing the lagrangian @xmath397 , we obtain the necessary conditions @xmath398=0 \\quad\\text{and}\\quad -r\\eta^{-2}+\\mu(r+1)r\\eta^{r-1}2^{-r}=0,\\ ] ] so @xmath399 and @xmath400 .",
    "thus , we should choose successive values of  @xmath393 until the second inequality in   holds , with  @xmath198 given by  .",
    "since @xmath401 , the computational cost is then proportional to  @xmath393 .",
    ".performance of slow and fast methods with @xmath402 time steps and @xmath403 spatial degrees of freedom . [ cols=\"^,^,^,^,^\",options=\"header \" , ]     consider a simple test problem in @xmath404  spatial dimensions , with  @xmath405 , @xmath406 , @xmath407 and homogeneous dirichlet boundary conditions  .",
    "we take @xmath408 so that the smallest eigenvalue of the elliptic operator  @xmath25 is @xmath409 .",
    "we choose the initial data and source term @xmath410 where @xmath411 is an eigenfunction of  @xmath79 with eigenvalue  @xmath412 .",
    "the exact solution of   then has the separable form  @xmath413 , and we can compute the time - dependent factor  @xmath414 to high accuracy by applying gauss quadrature to an integral representation  ( * ? ? ? *",
    "section  6 ) .",
    "moreover , the regularity estimates   hold with @xmath415 , so by   the @xmath416-error in  @xmath99 is of order  @xmath417 .",
    "table  [ tab : results ] shows some results of computations performed using a single - threaded fortran code running on a desktop pc with an intel core - i7  860 processor ( 2.80ghz ) and 8 gb of ram . in all cases",
    "the spatial discretization used bilinear finite elements on a uniform @xmath418  rectangular mesh , so the number of degrees of freedom was  @xmath419 .",
    "we solved the linear system   using fast sine transforms . taking @xmath402  time steps",
    ", we first computed the ( slow ) dg solution  @xmath87 and then the perturbed ( fast ) solution  @xmath121 for @xmath420 , 5 , 6 choosing @xmath198 as in proposition  [ prop : eta ] .",
    "the table shows the maximum nodal error @xmath421 , and also the cpu times in seconds , broken down into three parts .",
    "the setup phase covers computing the @xmath71 or @xmath72 , and for the fast method the cost of constructing the cluster tree and admissible covers .",
    "the rhs phase covers the computation of the right - hand side of  , and the solver phase is the total cpu time used by the elliptic solver .",
    "the cluster tree was @xmath352-uniform for @xmath353  and @xmath422 , so there were @xmath423 leaves .",
    "we see from the table that if @xmath424 then the fast summation algorithm evaluates the right - hand side ( rhs ) in 17.7  seconds , compared to 911  seconds for a direct evaluation , while maintaining the accuracy of the dg solution .",
    "for any real - valued , piecewise - constant  @xmath92 , @xmath425 so the next theorem shows that holds .",
    "if @xmath426\\to{\\mathbb{r}}$ ] is piecewise @xmath427 then @xmath428 where @xmath429    the assumption that @xmath430 is piecewise @xmath427 ensures @xmath431 is continuous except for weak singularities at the breakpoints of  @xmath430 . using the substitution  @xmath432 for  @xmath433 , we see that it suffices to deal with the case  @xmath434 .",
    "denote the laplace transform of  @xmath27 by @xmath435 and observe that , because @xmath436 , if we extend @xmath430 by zero outside the interval  @xmath437 $ ] , then @xmath438 applying the plancherel theorem , and noting that @xmath439 because @xmath430 is real - valued , we have @xmath440 in particular , @xmath441 and @xmath442    the estimate @xmath443 implies that , for any  @xmath444 , @xmath445 and therefore by  , @xmath446 thus , for  @xmath447 , @xmath448 which , in combination with  , implies that the desired inequality holds with @xmath449 the choice  @xmath450 maximises @xmath255 and gives the formula  .",
    "since the diagonal weights present no difficulty , we assume throughout this appendix that @xmath103 . denoting the distance between the centres of @xmath451  and @xmath132 by @xmath452 , we see from   that @xmath453 and so @xmath454 although we can easily evaluate these integrals analytically , the resulting expressions are susceptible to loss of precision when @xmath455  and @xmath456 are small compared to  @xmath457 .",
    "consider the problem of computing @xmath458 when @xmath36 is small compared to  @xmath68 , so that we have a difference of nearly equal numbers .",
    "the c99 standard library provides the functions @xmath459  and @xmath460 that approximate @xmath461  and @xmath462 accurately even when  @xmath13 is close to zero , so we can avoid the loss of precision by noting that @xmath463 and evaluating @xmath464 as @xmath465 .",
    "however , even though we can compute @xmath466 accurately , we still face the problem that @xmath467 is again a difference of nearly equal numbers , as is the alternative formula @xmath468 or equivalently @xmath469 when  @xmath455 is small compared to  @xmath457 , the following sum gives a more accurate value for the weight  @xmath71 .    if @xmath103 , then there exists @xmath470 such that @xmath471    we use the first integral representation in  .",
    "the taylor expansion @xmath472 implies that @xmath473 with the error term given by  @xmath474 , where @xmath475 by the integral mean value theorem , there exists @xmath476 $ ] such that @xmath477 since @xmath478 and @xmath479 , by the intermediate value theorem there exists @xmath470 such that @xmath480    by starting from the _ second _ integral representation in  , we obtain an expansion in odd powers of  @xmath456 , instead of  @xmath455 . for practical meshes",
    ", we generally have @xmath481 , so the series in the theorem will be preferable .    to determine the speed of convergence of the series , denote the @xmath173th term by @xmath482 and note that since @xmath483 , @xmath484 we find that the ratio of successive terms is @xmath485 and , since @xmath486 as @xmath487 for @xmath488 , @xmath489 for instance , in the case of a uniform grid @xmath490 , this limiting ratio is @xmath491^{-2}$ ] , giving acceptable convergence for @xmath492 . if @xmath493 , then the limiting ratio is @xmath494 , so the convergence is relatively slow .",
    "we see from   that @xmath495 ,      \\quad\\text{for $ x = k_{n-1}/k_n$,}\\end{aligned}\\ ] ] and from symmetry we may assume @xmath496 and thus @xmath497 . to evaluate the difference in square brackets , we write @xmath498}{\\nu}\\biggr)\\ ] ] computing @xmath499 as @xmath500 . in this way , @xmath501,\\ ] ] and we compute @xmath502 as @xmath503 $ ] .",
    "we remark that in the special case  @xmath405 one can evaluate @xmath71 more easily .",
    "firstly , @xmath504 and if we write @xmath505 for @xmath506 , @xmath507 , then @xmath508",
    "recall from theorem  [ thm : taylor expansion ] that @xmath509 so gives @xmath510 since @xmath511 and @xmath512 , we have @xmath513 and the @xmath514 can be computed via the recursion @xmath515 likewise , @xmath516\\\\      & = \\frac{k_j}{p!}\\sum_{q=0}^{p-1 } ( t_j-{\\bar s})^q      ( t_{j-1}-{\\bar s})^{p-1-q},\\end{aligned}\\ ] ] giving @xmath517 and @xmath518"
  ],
  "abstract_text": [
    "<S> we solve a fractional diffusion equation using a piecewise - constant , discontinuous galerkin method in time combined with a continuous , piecewise - linear finite element method in space . </S>",
    "<S> if there are @xmath0  time levels and @xmath1  spatial degrees of freedom , then a direct implementation of this method requires @xmath2  operations and @xmath3  storage , owing to the presence of a memory term : at each time step , the discrete evolution equation involves a sum over _ all _ previous time levels . </S>",
    "<S> we show how the computational cost can be reduced to @xmath4  operations and @xmath5  active memory locations . </S>"
  ]
}