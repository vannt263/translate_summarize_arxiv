{
  "article_text": [
    "compressive sensing is a recently emerged technique of signal sampling and reconstruction , the main purpose of which is to recover sparse signals from much fewer linear measurements @xcite @xmath0 where @xmath1 is the sampling matrix with @xmath2 , and @xmath3 denotes the @xmath4-dimensional sparse signal with only @xmath5 nonzero coefficients .",
    "such a problem has been extensively studied and a variety of algorithms that provide consistent recovery performance guarantee were proposed , e.g. @xcite . in practice ,",
    "sparse signals usually have additional structures that can be exploited to enhance the recovery performance .",
    "for example , the atomic decomposition of multi - band signals @xcite or audio signals @xcite usually results in a block - sparse structure in which the nonzero coefficients occur in clusters .",
    "in addition , a discrete wavelet transform of an image naturally yields a tree structure of the wavelet coefficients , with each wavelet coefficient serving as a `` parent '' for a few `` children '' coefficients @xcite . a number of algorithms , e.g. , block - omp @xcite , mixed @xmath6 norm - minimization @xcite , group lasso @xcite , structomp @xcite , and model - based cosamp @xcite were proposed for recovery of block - sparse signals , and their recovery behaviors were analyzed in terms of the model - based restricted isometry property ( rip ) @xcite and the mutual coherence @xcite .",
    "analyses suggested that exploiting the inherent structure of sparse signals helps improve the recovery performance considerably .",
    "these algorithms , albeit effective , require the knowledge of the block structure ( such as locations and sizes of blocks ) of sparse signals _ a priori_. in practice , however , the prior information about the block structure of sparse signals is often unavailable .",
    "for example , we know that images have structured sparse representations but the exact tree structure of the coefficients is unknown to us . to address this difficulty , a hierarchical bayesian `` spike - and - slab '' prior model",
    "is introduced in @xcite to encourage the sparseness and promote the cluster patterns simultaneously .",
    "nevertheless , for both works @xcite , the posterior distribution can not be derived analytically , and a markov chain monte carlo ( mcmc ) sampling method has to be employed for bayesian inference . in @xcite , a graphical prior , also referred to as the `` boltzmann machine '' , was used to model the statistical dependencies between atoms .",
    "specifically , the boltzmann machine is employed as a prior on the support of a sparse representation .",
    "however , the maximum a posterior ( map ) estimator with such a prior involves an exhaustive search over all possible sparsity patterns . to overcome the intractability of the combinatorial search",
    ", a greedy method @xcite and a variational mean - field approximation method @xcite were proposed to approximate the map .",
    "recently , a sparse bayesian learning method was proposed in @xcite to address the sparse signal recovery problem when the block structure is unknown . in @xcite ,",
    "the components of the signal are partitioned into a number of overlapping blocks and each block is assigned a gaussian prior .",
    "an expanded model is then used to convert the overlapping structure into a block diagonal structure so that the conventional block sparse bayesian learning algorithm can be readily applied .    in this paper , we develop a new bayesian method for block - sparse signal recovery when the block - sparse patterns are entirely unknown .",
    "similar to the conventional sparse bayesian learning approach @xcite , a bayesian hierarchical gaussian framework is employed to model the sparse prior , in which a set of hyperparameters are introduced to characterize the gaussian prior and control the sparsity of the signal components .",
    "conventional sparse learning approaches , however , assume independence between the elements of the sparse signal .",
    "specifically , each individual hyperparameter is associated independently with each coefficient of the sparse signal . to model the block - sparse patterns , in this paper , we propose a coupled hierarchical gaussian framework in which the sparsity of each coefficient is controlled not only by its own hyperparameter , but also by the hyperparameters of its immediate neighbors .",
    "such a prior encourages clustered patterns and suppresses `` isolated coefficients '' whose pattern is different from that of its neighboring coefficients .",
    "an expectation - maximization ( em ) algorithm is developed to learn the hyperparameters characterizing the coupled hierarchical model and to estimate the block - sparse signal .",
    "our proposed algorithm not only admits a simple iterative procedure for bayesian inference .",
    "it also demonstrates superiority over other existing methods for block - sparse signal recovery .",
    "the rest of the paper is organized as follows . in section [ sec :",
    "model ] , we introduce a new coupled hierarchical gaussian framework to model the sparse prior and the dependencies among the signal components . an expectation - maximization ( em ) algorithm is developed in section [ sec : inference ] to learn the hyperparameters characterizing the coupled hierarchical model and to estimate the block - sparse signal .",
    "section [ sec : inference - un ] extends the proposed bayesian inference method to the scenario where the observation noise variance is unknown .",
    "relation of our work to other existing works are discussed in [ sec : discussions ] , and an iterative reweighted algorithm is proposed for the recovery of block - sparse signals .",
    "simulation results are provided in section [ sec : simulation ] , followed by concluding remarks in section [ sec : conclusion ] .",
    "we consider the problem of recovering a block - sparse signal @xmath7 from noise - corrupted measurements @xmath8 where @xmath1 ( @xmath9 ) is the measurement matrix , and @xmath10 is the additive multivariate gaussian noise with zero mean and covariance matrix @xmath11 .",
    "the signal @xmath3 has a block - sparse structure but the exact block pattern such as the location and size of each block is unavailable to us .    in the conventional sparse bayesian learning framework , to encourage the sparsity of the estimated signal",
    ", @xmath3 is assigned a gaussian prior distribution @xmath12 where @xmath13 , and @xmath14 are non - negative hyperparameters controlling the sparsity of the signal @xmath3 . clearly , when @xmath15 approaches infinity , the corresponding coefficient @xmath16 becomes zero . by placing hyperpriors over @xmath17 , the hyperparameters @xmath17",
    "can be learned by maximizing their posterior probability .",
    "we see that in the above conventional hierarchical bayesian model , each hyperparameter is associated independently with each coefficient .",
    "the prior model assumes independence among coefficients and has no potential to encourage clustered sparse solutions .    to exploit the statistical dependencies among coefficients",
    ", we propose a new hierarchical bayesian model in which the prior for each coefficient not only involves its own hyperparameter , but also the hyperparameters of its immediate neighbors .",
    "specifically , a prior over @xmath3 is given by @xmath18 where @xmath19 and we assume @xmath20 and @xmath21 for the end points @xmath22 and @xmath23 , @xmath24 is a parameter indicating the relevance between the coefficient @xmath16 and its neighboring coefficients @xmath25 . to better understand this prior model , we can rewrite ( [ prior ] ) as @xmath26^{\\beta}[p(x_i|\\alpha_{i-1})]^{\\beta } \\label{eq2}\\end{aligned}\\ ] ] where @xmath27 for @xmath28 .",
    "we see that the prior for @xmath16 is proportional to a product of three gaussian distributions , with the coefficient @xmath16 associated with one of the three hyperparameters @xmath29 for each distribution .",
    "when @xmath30 , the prior distribution ( [ eq2 ] ) reduces to the prior for the conventional sparse bayesian learning .",
    "when @xmath31 , the sparsity of @xmath16 not only depends on the hyperparameter @xmath15 , but also on the neighboring hyperparameters @xmath32 .",
    "hence it can be expected that the sparsity patterns of neighboring coefficients are related to each other .",
    "also , such a prior does not require the knowledge of the block - sparse structure of the sparse signal .",
    "it naturally has the tendency to suppress isolated non - zero coefficients and encourage structured - sparse solutions .",
    "following the conventional sparse bayesian learning framework , we use gamma distributions as hyperpriors over the hyperparameters @xmath17 , i.e. @xmath33 where @xmath34 is the gamma function .",
    "the choice of the gamma hyperprior results in a learning process which tends to switch off most of the coefficients that are deemed to be irrelevant , and only keep very few relevant coefficients to explain the data .",
    "this mechanism is also called as `` automatic relevance determination '' . in the conventional sparse bayesian framework , to make the gamma prior non - informative , very small values , e.g. @xmath35 , are assigned to the two parameters @xmath36 and @xmath37 .",
    "nevertheless , in this paper , we use a more favorable prior which sets a larger @xmath36 ( say , @xmath38 ) in order to achieve the desired `` pruning '' effect for our proposed hierarchical bayesian model .",
    "clearly , the gamma prior with a larger @xmath36 encourages large values of the hyperparameters , and therefore promotes the sparseness of the solution since the larger the hyperparameter , the smaller the variance of the corresponding coefficient .",
    "we now proceed to develop a sparse bayesian learning method for block - sparse signal recovery . for ease of exposition",
    ", we assume that the noise variance @xmath39 is known _",
    "a priori_. extension of the bayesian inference to the case of unknown noise variance will be discussed in the next section . based on the above hierarchical model ,",
    "the posterior distribution of @xmath3 can be computed as @xmath40 where @xmath14 , @xmath41 is given by ( [ eq3 ] ) , and @xmath42 it can be readily verified that the posterior @xmath43 follows a gaussian distribution with its mean and covariance given respectively by @xmath44 where @xmath45 is a diagonal matrix with its @xmath46th diagonal element equal to @xmath47 , i.e. @xmath48 given a set of estimated hyperparameters @xmath17 , the maximum a posterior ( map ) estimate of @xmath3 is the mean of its posterior distribution , i.e. @xmath49    our problem therefore reduces to estimating the set of hyperparameters @xmath17 . with hyperpriors placed over @xmath50 , learning the hyperparameters becomes a search for their posterior mode , i.e. maximization of the posterior probability @xmath51 .",
    "a strategy to maximize the posterior probability is to exploit the expectation - maximization ( em ) formulation which treats the signal @xmath3 as the hidden variables and maximizes the expected value of the complete log - posterior of @xmath52 , i.e. @xmath53 $ ] , where the operator @xmath54 $ ] denotes the expectation with respect to the distribution @xmath55 . specifically , the em algorithm produces a sequence of estimates @xmath56 , @xmath57 , by applying two alternating steps , namely , the e - step and the m - step @xcite .",
    "* e - step : * given the current estimates of the hyperparameters @xmath56 and the observed data @xmath58 , the e - step requires computing the expected value ( with respect to the missing variables @xmath3 ) of the complete log - posterior of @xmath52 , which is also referred to as the q - function ; we have @xmath59 \\nonumber\\\\ = & \\int p(\\boldsymbol{x}|\\boldsymbol{y},\\boldsymbol{\\alpha}^{(t)})\\log p(\\boldsymbol{\\alpha}|\\boldsymbol{x } ) d\\boldsymbol{x } \\nonumber\\\\ = & \\int p(\\boldsymbol{x}|\\boldsymbol{y},\\boldsymbol{\\alpha}^{(t)})\\log [ p(\\boldsymbol{\\alpha})p(\\boldsymbol{x}|\\boldsymbol{\\alpha})]d\\boldsymbol{x}+ c \\label{q - function - org}\\end{aligned}\\ ] ] where @xmath60 is a constant independent of @xmath52 . ignoring the term independent of @xmath52 , and recalling ( [ eq3 ] )",
    ", the q - function can be re - expressed as @xmath61 since the posterior @xmath62 is a multivariate gaussian distribution with its mean and covariance matrix given by ( [ eq4 ] ) , we have @xmath63=\\hat{\\mu}_i^2+\\hat{\\phi}_{i , i}\\end{aligned}\\ ] ] where @xmath64 denotes the @xmath46th entry of @xmath65 , @xmath66 denotes the @xmath46th diagonal element of the covariance matrix @xmath67 , @xmath65 and @xmath67 are computed according to ( [ eq4 ] ) , with @xmath52 replaced by the current estimate @xmath56 . with the specified prior ( [ alpha - prior ] ) , the q - function can eventually be written as @xmath68    * m - step : * in the m - step of the em algorithm , a new estimate of @xmath52 is obtained by maximizing the q - function , i.e. @xmath69 for the conventional sparse bayesian learning , maximization of the q - function can be decoupled into a number of separate optimizations in which each hyperparameter @xmath15 is updated independently .",
    "this , however , is not the case for the problem being considered here .",
    "we see that the hyperparameters in the q - function ( [ q - function ] ) are entangled with each other due to the logarithm term @xmath70 . in this case , an analytical solution to the optimization ( [ m - opt ] ) is difficult to obtain .",
    "gradient descend methods can certainly be used to search for the optimal solution .",
    "nevertheless , such a gradient - based search method , albeit effective , does not provide any insight into the learning process . also , gradient - based methods involve higher computational complexity as compared with an analytical update rule . to overcome the drawbacks of gradient - based methods , we consider an alternative strategy which aims at finding a simple , analytical sub - optimal solution of ( [ m - opt ] ) .",
    "such an analytical sub - optimal solution can be obtained by examining the optimality condition of ( [ m - opt ] ) .",
    "suppose @xmath71 is the optimal solution of ( [ m - opt ] ) , then the first derivative of the q - function with respect to @xmath52 equals to zero at the optimal point , i.e. @xmath72 where @xmath73 , @xmath74 , and for @xmath75 , we have @xmath76 note that for notational convenience , we allow the subscript indices of the notations @xmath64 and @xmath66 in ( [ omega ] ) equal to @xmath77 and @xmath78 .",
    "although these notations @xmath79 do not have any meaning , they can be used to simplify our expression .",
    "clearly , they should all be set equal to zero , i.e. @xmath80 . recalling the optimality condition , we therefore have @xmath81 where @xmath82 , @xmath83 , and @xmath84 since all hyperparameters @xmath17 and @xmath85 are non - negative , we have @xmath86 hence the term on the left - hand side of ( [ eq5 ] ) is lower and upper bounded respectively by @xmath87 where @xmath88 for @xmath89 , and @xmath90 for @xmath91 . combining ( [ eq5])([eq6 ] ) , we arrive at @xmath92\\quad \\forall i=1,\\ldots , n \\label{eq12}\\end{aligned}\\ ] ] with @xmath38 , and @xmath93 , a sub - optimal solution to ( [ m - opt ] ) can be obtained as @xmath94 for some @xmath95 within the range @xmath96 .",
    "we see that the solution ( [ hp - update ] ) provides a simple rule for the hyperparameter update .",
    "also , notice that the update rule ( [ hp - update ] ) resembles that of the conventional sparse bayesian learning work @xcite except that the parameter @xmath97 is equal to @xmath98 for the conventional sparse bayesian learning method , while for our case , @xmath97 is a weighted summation of @xmath99 for @xmath100 .    for clarity , we now summarize the em algorithm as follows .    1 .   at iteration @xmath101 ( @xmath102 ) : given a set of hyperparameters @xmath103 , compute the mean @xmath65 and covariance matrix @xmath67 of the posterior distribution @xmath104 according to ( [ eq4 ] ) , and compute the map estimate @xmath105 according to ( [ posterior - mean ] ) .",
    "2 .   update the hyperparameters @xmath106 according to ( [ hp - update ] ) , where @xmath97 is given by ( [ omega ] ) .",
    "3 .   continue the above iteration until @xmath107 , where @xmath108 is a prescribed tolerance value .",
    "_ remarks : _ although the above algorithm employs a sub - optimal solution ( [ hp - update ] ) to update the hyperparameters in the m - step , numerical results show that the sub - optimal update rule is quite effective and presents similar recovery performance as using a gradient - based search method .",
    "this is because the sub - optimal solution ( [ hp - update ] ) provides a reasonable estimate of the optimal solution when the parameter @xmath36 is set away from zero , say , @xmath38 .",
    "numerical results also suggest that the proposed algorithm is insensitive to the choice of the parameter @xmath95 in ( [ hp - update ] ) as long as @xmath95 is within the range @xmath109 $ ] for a properly chosen @xmath36 .",
    "we simply set @xmath110 in our following simulations .",
    "the update rule ( [ hp - update ] ) not only admits a simple analytical form which is computationally efficient , it also provides an insight into the em algorithm . the bayesian occam s razor which contributes to the success of the conventional sparse bayesian learning method also works here to automatically select an appropriate simple model . to see this",
    ", note that in the e - step , when computing the posterior mean and covariance matrix , a large hyperparameter @xmath15 tends to suppress the values of the corresponding components @xmath111 for @xmath100 ( c.f .",
    "( [ eq4 ] ) ) . as a result , the value of @xmath97 becomes small , which in turn leads to a larger hyperparameter @xmath15 ( c.f .",
    "( [ hp - update ] ) ) .",
    "this negative feedback mechanism keeps decreasing most of the entries in @xmath112 until they reach machine precision and become zeros , while leaving only a few prominent nonzero entries survived to explain the data .",
    "meanwhile , we see that each hyperparameter @xmath15 not only controls the sparseness of its own corresponding coefficient @xmath16 , but also has an impact on the sparseness of the neighboring coefficients @xmath25 .",
    "therefore the proposed em algorithm has the tendency to suppress isolated non - zero coefficients and encourage structured - sparse solutions .",
    "in the previous section , for simplicity of exposition , we assume that the noise variance @xmath39 is known _ a priori_. this assumption , however , may not hold valid in practice . in this section ,",
    "we discuss how to extend our previously developed bayesian inference method to the scenario where the noise variance @xmath39 is unknown .    for notational convenience ,",
    "define @xmath113 following the conventional sparse bayesian learning framework @xcite , we place a gamma hyperprior over @xmath114 : @xmath115 where the parameters @xmath60 and @xmath116 are set to small values , e.g. @xmath117 . as we already derived in the previous section , given the hyperparameters @xmath52 and the noise variance @xmath39 , the posterior @xmath118 follows a gaussian distribution with its mean and covariance matrix given by ( [ eq4 ] ) .",
    "the map estimate of @xmath3 is equivalent to the posterior mean .",
    "our problem therefore becomes jointly estimating the hyperparameters @xmath52 and the noise variance @xmath39 ( or equivalently @xmath114 ) .",
    "again , the em algorithm can be used to learn these parameters via maximizing their posterior probability @xmath119 .",
    "the alternating em steps are briefly discussed below .    * e - step * : in the e - step , given the current estimates of the parameters @xmath120 and the observed data @xmath58 , we compute the expected value ( with respect to the missing variables @xmath3 ) of the complete log - posterior of @xmath121 , that is , @xmath122 $ ] , where the operator @xmath123 $ ] denotes the expectation with respect to the distribution @xmath124 . since @xmath125 the q - function can be expressed as a summation of two terms @xmath126 \\nonumber\\\\ & + e_{\\boldsymbol{x}|\\boldsymbol{y},\\boldsymbol{\\alpha}^{(t)},\\gamma^{(t)}}[\\log p(\\gamma)p(\\boldsymbol{y}|\\boldsymbol{x},\\gamma ) ] \\label{eq7}\\end{aligned}\\ ] ] where the first term has exactly the same form as the q - function ( [ q - function - org ] ) obtained in the previous section , except with the known noise variance @xmath39 replaced by the current estimate @xmath127 , and the second term is a function of the variable @xmath114 .    * m - step * : we observe that in the q - function ( [ eq7 ] ) , the parameters @xmath52 and @xmath114 to be learned are separated from each other .",
    "this allows the estimation of @xmath52 and @xmath114 to be decoupled into the following two independent problems : @xmath128 \\label{opt-2 } \\\\",
    "\\gamma^{(t+1 ) } = & \\arg\\max_{\\gamma } e_{\\boldsymbol{x}|\\boldsymbol{y},\\boldsymbol{\\alpha}^{(t)},\\gamma^{(t)}}[\\log p(\\gamma)p(\\boldsymbol{y}|\\boldsymbol{x},\\gamma ) ] \\label{opt-3}\\end{aligned}\\ ] ] the first optimization problem ( [ opt-2 ] ) has been thoroughly studied in the previous section , where we provided a simple analytical form ( [ hp - update ] ) for the hyperparameter update .",
    "we now discuss the estimation of the parameter @xmath114 .",
    "recalling ( [ gamma - prior ] ) , we have @xmath129 \\nonumber\\\\ = & \\frac{m}{2}\\log\\gamma-\\frac{\\gamma}{2}e_{\\boldsymbol{x}|\\boldsymbol{y},\\boldsymbol{\\alpha}^{(t)},\\gamma^{(t ) } } \\left[\\|\\boldsymbol{y}-\\boldsymbol{a}\\boldsymbol{x}\\|_2 ^ 2\\right]+c\\log\\gamma - d\\gamma \\label{eq13}\\end{aligned}\\ ] ] computing the first derivative of ( [ eq13 ] ) with respect to @xmath114 and setting it equal to zero , we get @xmath130 where @xmath131 \\nonumber\\end{aligned}\\ ] ] note that the posterior @xmath124 follows a gaussian distribution with mean @xmath65 and covariance matrix @xmath67 , where @xmath65 and @xmath67 are computed via ( [ eq4 ] ) with @xmath114 ( i.e. @xmath39 ) and @xmath52 replaced by the current estimates @xmath132 . hence @xmath133 can be computed as @xmath134+e[\\boldsymbol{x}^t\\boldsymbol{a}^t \\boldsymbol{a}\\boldsymbol{x } ] \\nonumber\\\\ = & \\boldsymbol{y}^t\\boldsymbol{y}-2\\boldsymbol{\\hat{\\mu}}^t\\boldsymbol{a}^t\\boldsymbol{y}+ \\boldsymbol{\\hat{\\mu}}^t\\boldsymbol{a}^t \\boldsymbol{a}\\boldsymbol{\\hat{\\mu}}+\\text{tr}\\left(\\boldsymbol{\\hat{\\phi}}\\boldsymbol{a}^t\\boldsymbol{a}\\right ) \\nonumber\\\\ \\stackrel{(a)}{= } & \\|\\boldsymbol{y}-\\boldsymbol{a}\\boldsymbol{\\hat{\\mu}}\\|_2 ^ 2+(\\gamma^{(t)})^{-1}\\sum_{i=1}^n\\rho_i \\label{eq10}\\end{aligned}\\ ] ] where the last equality @xmath135 follows from @xmath136 in which @xmath137 is given by ( [ d - definition ] ) with @xmath52 replaced by the current estimate @xmath56 , and @xmath138 note that @xmath139 and @xmath140 are set to zero when computing @xmath141 and @xmath142 . substituting ( [ eq10 ] ) back into ( [ eq11 ] ) , a new estimate of @xmath114 , i.e. the optimal solution to ( [ opt-3 ] ) , is given by @xmath143 the above update formula has a similar form as that for the conventional sparse bayesian learning ( c.f .",
    "* equation ( 50 ) ) ) .",
    "the only difference lies in that @xmath144 are computed differently : for the conventional sparse bayesian learning method , @xmath145 is computed as @xmath146 , while @xmath145 is given by ( [ rho ] ) for our algorithm .    the sparse bayesian learning algorithm with unknown noise variance is now summarized as follows .    1 .",
    "at iteration @xmath101 ( @xmath102 ) : given the current estimates of @xmath56 and @xmath147 , compute the mean @xmath65 and the covariance matrix @xmath67 of the posterior distribution @xmath148 via ( [ eq4 ] ) , and calculate the map estimate @xmath105 according to ( [ posterior - mean ] ) .",
    "2 .   compute a new estimate of @xmath52 , denoted as @xmath106 , according to ( [ hp - update ] ) , where @xmath97 is given by ( [ omega ] ) ; update @xmath114 via ( [ gamma - update ] ) , which yields a new estimate of @xmath114 , denoted as @xmath149 .",
    "3 .   continue the above iteration until @xmath107 , where @xmath108 is a prescribed tolerance value .",
    "sparse bayesian learning is a powerful approach for regression , classification , and sparse representation .",
    "it was firstly introduced by tipping in his pioneering work @xcite , where the regression and classification problem was addressed and a sparse bayesian learning approach was developed to automatically remove irrelevant basis vectors and retain only a few ` relevant ' vectors for prediction .",
    "such an automatic relevance determination mechanism and the resulting sparse solution not only effectively avoid the overfitting problem , but also render superior regression and classification accuracy . later on in @xcite , sparse bayesian learning was introduced to solve the sparse recovery problem . in a series of experiments ,",
    "sparse bayesian learning demonstrated superior stability for sparse signal recovery , and presents uniform superiority over other methods .    in @xcite ,",
    "sparse bayesian learning was generalized to solve the simultaneous ( block ) sparse recovery problem , in which a group of coefficients sharing the same sparsity pattern are assigned a multivariate gaussian prior parameterized by a common hyperparameter that controls the sparsity of this group of coefficients . specifically , we have @xmath150 where @xmath151 denotes the group of coefficients that share a same sparsity pattern , @xmath15 is the hyperparameter controlling the sparsity of @xmath151 . in @xcite ,",
    "the above model was further improved to accommodate temporally correlated sources @xmath152 in which @xmath153 is a positive definite matrix that captures the correlation structure of @xmath151 .",
    "we see that , in both models @xcite , each coefficient is associated with only one sparseness - controlling hyperparameter .",
    "this explicit assignment of each coefficient to a certain hyperparameter requires to know the exact block sparsity pattern _ a priori_. in contrast , for our hierarchical bayesian model , each coefficient",
    "is associated with multiple hyperparameters , and the hyperparameters are somehow related to each other through their commonly connected coefficients .",
    "such a coupled hierarchical model has the potential to encourage block - sparse patterns , while without imposing any stringent or pre - specified constraints on the structure of the recovered signals .",
    "this property enables the proposed algorithm to learn the block - sparse structure in an automatic manner .",
    "recently , zhang and rao extended the block sparse bayesian learning framework to address the sparse signal recovery problem when the block structure is unknown @xcite . in their work @xcite",
    ", the signal @xmath3 is partitioned into a number of overlapping blocks @xmath154 with identical block sizes , and each block @xmath151 is assigned a gaussian prior @xmath155 . to address the overlapping issue ,",
    "the original data model is converted into an expanded model which removes the overlapping structure by adding redundant columns to the original measurement matrix @xmath156 and stacking all blocks @xmath154 to form an augmented vector . in doing this way ,",
    "the prior for the new augmented vector has a block diagonal form similar as that for the conventional block sparse bayesian learning .",
    "thus conventional block sparse bayesian learning algorithms such as @xcite can be applied to the expanded model .",
    "this overlapping structure provides flexibility in defining a block - sparse pattern .",
    "hence it works well even when the block structure is unknown .",
    "a critical difference between our work and @xcite is that for our method , a prior is directly placed on the signal @xmath3 , while for the method proposed in @xcite , a rigorous formulation of the prior for @xmath3 is not available , instead , a prior is assigned to the augmented new signal which is constructed by stacking a number of overlapping blocks @xmath154 .",
    "sparse bayesian learning algorithms have a close connection with the reweighted @xmath157 or @xmath158 methods .",
    "in fact , a dual - form analysis @xcite reveals that sparse bayesian learning can be considered as a non - separable reweighted strategy solving a non - separable penalty function .",
    "inspired by this insight , we here propose a reweighted @xmath157 method for the recovery of block - sparse signals when the block structure of the sparse signal is unknown .",
    "conventional reweighted @xmath157 methods iteratively minimize the following weighted @xmath157 function ( for simplicity , we consider the noise - free case ) : @xmath159 where the weighting parameters are given by @xmath160 , and @xmath108 is a pre - specified positive parameter . in a series of experiments @xcite , the above iterative reweighted algorithm outperforms the conventional @xmath157-minimization method by a considerable margin .",
    "the fascinating idea of the iterative reweighted algorithm is that the weights are updated based on the previous estimate of the solution , with a large weight assigned to the coefficient whose estimate is already small and vice versa . as a result ,",
    "the value of the coefficient which is assigned a large weight tends to be smaller ( until become negligible ) in the next estimate .",
    "this explains why iterative reweighted algorithms usually yield sparser solutions than the conventional @xmath157-minimization method .",
    "as discussed in our previous section , the basic idea of our proposed sparse bayesian learning method is to establish a coupling mechanism such that the sparseness of neighboring coefficients are somehow related to each other . with this in mind",
    ", we slightly modify the weight update rule of the reweighted @xmath157 algorithm as follows @xmath161 we see that unlike the conventional update rule , the weight @xmath162 is not only a function of its corresponding coefficient @xmath163 , but also dependent on the neighboring coefficients @xmath164 . in doing this way ,",
    "a coupling effect between the sparsity patterns of neighboring coefficients is established .",
    "hence the modified reweighted @xmath157-minimization algorithm has the potential to encourage block - sparse solutions .",
    "experiments show that the proposed modified reweighted @xmath157 method yields considerably improved results over the conventional reweighted @xmath157 method in recovering block - sparse signals .",
    "it also serves as a good reference method for comparison with the proposed bayesian sparse learning approach .",
    "we now carry out experiments to illustrate the performance of our proposed algorithm , also referred to as the pattern - coupled sparse bayesian learning ( pc - sbl ) algorithm , and its comparison with other existing methods .",
    "the performance of the proposed algorithm will be examined using both synthetic and real data .",
    "the parameters @xmath36 and @xmath37 for our proposed algorithm are set equal to @xmath165 and @xmath93 throughout our experiments .",
    "let us first consider the synthetic data case . in our simulations , we generate the block - sparse signal in a similar way to @xcite .",
    "suppose the @xmath4-dimensional sparse signal contains @xmath5 nonzero coefficients which are partitioned into @xmath166 blocks with random sizes and random locations .",
    "specifically , the block sizes @xmath167 can be determined as follows : we randomly generate @xmath166 positive random variables @xmath168 with their sum equal to one , then we can simply set @xmath169 for the first @xmath170 blocks and @xmath171 for the last block , where @xmath172 denotes the ceiling operator that gives the smallest integer no smaller than @xmath173 .",
    "similarly , we can partition the @xmath4-dimensional vector into @xmath166 super - blocks using the same set of values @xmath168 , and place each of the @xmath166 nonzero blocks into each super - block with a randomly generated starting position ( the starting position , however , is selected such that the nonzero block will not go beyond the super - block ) .",
    "also , in our experiments , the nonzero coefficients of the sparse signal @xmath3 and the measurement matrix @xmath1 are randomly generated with each entry independently drawn from a normal distribution , and then the sparse signal @xmath3 and columns of @xmath156 are normalized to unit norm .",
    "two metrics are used to evaluate the recovery performance of respective algorithms , namely , the normalized mean squared error ( nmse ) and the success rate .",
    "the nmse is defined as @xmath174 , where @xmath112 denotes the estimate of the true signal @xmath3 .",
    "the success rate is computed as the ratio of the number of successful trials to the total number of independent runs .",
    "a trial is considered successful if the nmse is no greater than @xmath35 . in our simulations ,",
    "the success rate is used to measure the recovery performance for the noiseless case , while the nmse is employed to measure the recovery accuracy when the measurements are corrupted by additive noise .",
    "we first examine the recovery performance of our proposed algorithm ( pc - sbl ) under different choices of @xmath85 .",
    "as indicated earlier in our paper , @xmath85 ( @xmath175 ) is a parameter quantifying the dependencies among neighboring coefficients .",
    "[ fig1 ] depicts the success rates vs. the ratio @xmath176 for different choices of @xmath85 , where we set @xmath177 , @xmath178 , and @xmath179 .",
    "results ( in fig .",
    "[ fig1 ] and the following figures ) are averaged over 1000 independent runs , with the measurement matrix and the sparse signal randomly generated for each run . the performance of the conventional sparse bayesian learning method ( denoted as `` sbl '' ) @xcite and the basis pursuit method ( denoted as `` bp '' ) @xcite is also included for our comparison .",
    "we see that when @xmath30 , our proposed algorithm performs the same as the sbl .",
    "this is an expected result since in the case of @xmath30 , our proposed algorithm is simplified as the sbl . nevertheless , when @xmath31 , our proposed algorithm achieves a significant performance improvement ( as compared with the sbl and bp ) through exploiting the underlying block - sparse structure , even without knowing the exact locations and sizes of the non - zero blocks .",
    "we also observe that our proposed algorithm is not very sensitive to the choice of @xmath85 as long as @xmath31 : it achieves similar success rates for different positive values of @xmath85 . for simplicity",
    ", we set @xmath180 throughout our following experiments .",
    "for different choices of @xmath85.,width=340 ]    next , we compare our proposed algorithm with some other recently developed algorithms for block - sparse signal recovery , namely , the expanded block sparse bayesian learning method ( ebsbl ) @xcite , the boltzman machine - based greedy pursuit algorithm ( bm - map - omp ) @xcite , and the cluster - structured mcmc algorithm ( cluss - mcmc ) @xcite .",
    "the modified iterative reweighted @xmath157 method ( denoted as mrl1 ) proposed in section [ sec : discussions ] is also examined in our simulations .",
    "note that all these algorithms were developed without the knowledge of the block - sparse structure .",
    "the block sparse bayesian learning method ( denoted as bsbl ) developed in @xcite is included as well .",
    "although the bsbl algorithm requires the knowledge of the block - sparse structure , it still provides decent performance if the presumed block size , denoted by @xmath181 , is properly selected .",
    "[ fig2 ] plots the success rates of respective algorithms as a function of the ratio @xmath176 and the sparsity level @xmath5 , respectively .",
    "simulation results show that our proposed algorithm achieves highest success rates among all algorithms and outperforms other methods by a considerable margin .",
    "we also noticed that the modified reweighted @xmath157 method ( mrl1 ) , although not as good as the proposed pd - sbl , still delivers acceptable performance which is comparable to the bsbl and better than the bm - map - omp and the cluss - mcmc .",
    "we now consider the noisy case where the measurements are contaminated by additive noise .",
    "the observation noise is assumed multivariate gaussian with zero mean and covariance matrix @xmath11 .",
    "also , in our simulations , the noise variance is assumed unknown ( except for the bm - map - omp ) .",
    "the nmses of respective algorithms as a function of the ratio @xmath176 and the sparsity level @xmath5 are plotted in fig .",
    "[ fig3 ] , where the white gaussian noise is added such that the signal - to - noise ratio ( snr ) , which is defined as @xmath182 , is equal to @xmath183db for each iteration .",
    "we see that our proposed algorithm yields a lower estimation error than other methods in the presence of additive gaussian noise .      in this subsection",
    ", we carry out experiments on real world images . as it is well - known ,",
    "images have sparse ( or approximately sparse ) structures in certain over - complete basis , such as wavelet or discrete cosine transform ( dct ) basis .",
    "moreover , the sparse representations usually demonstrate clustered structures whose significant coefficients tend to be located together ( see fig .",
    "[ fig6 ] ) .",
    "therefore images are suitable data sets for evaluating the effectiveness of a variety of block - sparse signal recovery algorithms .",
    "we consider two famous pictures ` lena ' and ` pirate ' in our simulations . in our experiments , the image is processed in a columnwise manner : we sample each column of the @xmath184 image using a randomly generated measurement matrix @xmath185 , recover each column from the @xmath186 measurements , and reconstruct the image based on the @xmath187 estimated columns . fig .",
    "[ fig4 ] and [ fig5 ] show the original images ` lena ' and ` pirate ' and the reconstructed images using respective algorithms , where we set @xmath188 and @xmath189 respectively .",
    "we see that our proposed algorithm presents the finest image quality among all methods .",
    "the result , again , demonstrates its superiority over other existing methods .",
    "the reconstruction accuracy of respective algorithms can also be observed from the reconstructed wavelet coefficients .",
    "we provide the true wavelet coefficients of one randomly selected column from the image ` lena ' , and the wavelet coefficients reconstructed by respective algorithms .",
    "results are depicted in fig .",
    "it can be seen that our proposed algorithm provides reconstructed coefficients that are closest to the groundtruth .    [ cols=\"^,^,^,^,^,^ \" , ]",
    "we developed a new bayesian method for recovery of block - sparse signals whose block - sparse structures are entirely unknown .",
    "a pattern - coupled hierarchical gaussian prior model was introduced to characterize both the sparseness of the coefficients and the statistical dependencies between neighboring coefficients of the signal .",
    "the prior model , similar to the conventional sparse bayesian learning model , employs a set of hyperparameters to control the sparsity of the signal coefficients . nevertheless , in our framework , the sparsity of each coefficient not only depends on its corresponding hyperparameter , but also depends on the neighboring hyperparameters .",
    "such a prior has the potential to encourage clustered patterns and suppress isolated coefficients whose patterns are different from their respective neighbors .",
    "the hyperparameters , along with the sparse signal , can be estimated by maximizing their posterior probability via the expectation - maximization ( em ) algorithm .",
    "numerical results show that our proposed algorithm achieves a significant performance improvement as compared with the conventional sparse bayesian learning method through exploiting the underlying block - sparse structure , even without knowing the exact locations and sizes of the non - zero blocks .",
    "it also demonstrates its superiority over other existing methods and provides state - of - the - art performance for block - sparse signal recovery .",
    "a.  drmeau , c.  herzet , and l.  daudet , `` boltzmann machine and mean - field approximation for structured sparse decompositions , '' _ ieee trans .",
    "signal processing _",
    "60 , no .  7 ,",
    "34253438 , july 2012 .",
    "z.  zhang and b.  d. rao , `` extension of sbl algorithms for the recovery of block sparse signals with intra - block correlation , '' _ ieee trans . signal processing _ , vol .",
    "61 , no .  8 , pp . 20092015 , apr .",
    "2013 .",
    "z.  zhang and b.  d. rao , `` sparse signal recovery with temporally correlated source vectors using sparse bayesian learning , '' _ ieee journal of selected topics in signal processing _",
    ", vol .  5 , no .  5 , pp .",
    "912926 , sept .",
    "d.  wipf and s.  nagarajan , `` iterative reweighted @xmath157 and @xmath158 methods for finding sparse solutions , '' _ ieee journals of selected topics in signal processing _ , vol .  4 , no .  2 ,",
    ". 317329 , apr . 2010 ."
  ],
  "abstract_text": [
    "<S> we consider the problem of recovering block - sparse signals whose structures are unknown _ a priori_. block - sparse signals with nonzero coefficients occurring in clusters arise naturally in many practical scenarios . </S>",
    "<S> however , the knowledge of the block structure is usually unavailable in practice . in this paper </S>",
    "<S> , we develop a new sparse bayesian learning method for recovery of block - sparse signals with unknown cluster patterns . </S>",
    "<S> specifically , a pattern - coupled hierarchical gaussian prior model is introduced to characterize the statistical dependencies among coefficients , in which a set of hyperparameters are employed to control the sparsity of signal coefficients . unlike the conventional sparse bayesian learning framework in which each individual hyperparameter is associated independently with each coefficient , in this paper , the prior for each coefficient not only involves its own hyperparameter , but also the hyperparameters of its immediate neighbors . in doing this way , </S>",
    "<S> the sparsity patterns of neighboring coefficients are related to each other and the hierarchical model has the potential to encourage structured - sparse solutions . </S>",
    "<S> the hyperparameters , along with the sparse signal , are learned by maximizing their posterior probability via an expectation - maximization ( em ) algorithm . </S>",
    "<S> numerical results show that the proposed algorithm presents uniform superiority over other existing methods in a series of experiments .    </S>",
    "<S> sparse bayesian learning , pattern - coupled hierarchical model , block - sparse signal recovery . </S>"
  ]
}