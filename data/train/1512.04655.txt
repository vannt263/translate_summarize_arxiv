{
  "article_text": [
    "in astronomy , the technique of difference image analysis ( dia ) aims to measure changes , from one image to another , in the objects ( e.g. stars , galaxies , etc . ) observed in a particular field .",
    "typically these changes consist of variations in flux and/or position .",
    "however , the variations in the object properties that we are interested in are entangled with the differences in the sky - to - detector ( or scene - to - image ) transformation between pairs of images .",
    "therefore , the dia method must carefully model the changes in astrometry , throughput , background , and blurring between an image pair in order to extract the required astronomical information .",
    "the state of the art in dia has evolved substantially over the last decade and a half .",
    "possibly the most complicated part of dia is the optimal modelling of the convolution kernel describing the changes in point - spread function ( psf ) between images .",
    "the seminal paper by @xcite set the current framework for doing this by detailing the expansion of the kernel as a linear combination of basis functions .",
    "@xcite subsequently showed how to model a spatially varying convolution kernel by modelling the coefficients of the kernel basis functions as polynomials of the image coordinates .",
    "the most important ingredient then in constructing a kernel solution in the alard dia framework is the definition of the set of kernel basis functions .",
    "the main developments in this area were achieved by @xcite , who defined the gaussian basis functions , @xcite and @xcite who introduced the delta basis functions ( dbfs ) , and @xcite ( hereafter be12 ) who conceived of the regularised dbfs . a detailed discussion of the kernel basis functions presented in the dia literature may be found in @xcite ( hereafter br13 ) .",
    "the traditional gaussian basis functions require the specification of numerous parameters while demanding precise sub - pixel image registration for optimal results , as do many other sets of kernel basis functions ( e.g. the network of bicubic b - spline functions introduced by @xcite ) .",
    "consequently , the optimal choice of parameters for generating such sets of basis functions is not obvious , although some investigation into this issue has been performed ( @xcite ) .",
    "in contrast , the dbfs have the ultimate flexibility to represent a discrete kernel of any form while requiring the absolute minimal user specification ; namely the kernel size and shape ( or equivalently the set of `` active '' kernel pixels ) .",
    "they may even be used to model fractional pixel offsets between images , avoiding the need for image resampling in the absence of other image misalignments ( rotation , scale , shear and distortion ) .",
    "unsurprisingly then , dia photometry for kernels employing dbfs has been shown to be better than that produced for kernels using gaussian basis functions ( @xcite ) . however , the use of dbfs yields somewhat noisier kernel solutions than is desirable due to the relatively large number of parameters in the kernel model . to tackle this weakness of the dbfs , be12 developed the regularised dbfs through the elegant application of tikhonov regularisation to the kernel model .",
    "this refined approach produces very clean and low - noise kernel solutions at the expense of introducing an extra parameter @xmath0 into the kernel definition , where the value of @xmath0 controls the strength of the regularisation .",
    "be12 recommend values of @xmath0 between 0.1 and 1 for square kernels of size 19@xmath119 pixels although they caution that the optimal value will likely be data set dependent .    the next logical step in the development of dia",
    "is to investigate how the properties of the image pair under consideration influence the composition of the optimal kernel model ( i.e. the optimal set of dbfs , the optimal values of their coefficents , and the optimal value of @xmath0 ) . in this context",
    ", `` optimality '' refers both to the principle of parsimony , in that the optimal kernel model should constitute the simplest configuration of dbfs that provides a sufficiently good fit to the data , and to appropriate / relevant model performance measure(s ) .",
    "the proposed investigation may be accomplished both by generating and analysing a comprehensive set of simulated images , and by testing on a wide variety of real image data .",
    "neither of these tasks have yet been attempted .",
    "various model selection criteria have been developed from different statistical view - points as implementations of the principle of parsimony ( e.g. the aikaike information criterion - @xcite , the bayesian information criterion - @xcite , etc . ) and each one may be used to automatically select a parsimonious model from a set of models . due to the sheer number of possible combinations of dbfs that may constitute the kernel model , the set of models that can be considered",
    "will be limited to a set of feasible candidate kernel models defined via the adoption of an appropriate kernel design algorithm .",
    "the performance of each model selection criterion may then be assessed by measuring the quality of the corresponding kernel solution with respect to one or more desired metric(s ) .",
    "the final result will then be a recommendation , dependent on the properties of the image pair under consideration , as to which model selection criterion should be adopted to consistently yield the best kernel solutions for the specified kernel design algorithm .    in this paper , we report on the results of having carried out the proposed investigation for both the unregularised and regularised dbfs ( section  [ sec : methods_dia ] ) using simulated images ( section  [ sec : simsec ] ) and real data ( section  [ sec : realsec ] ) .",
    "we restrict attention to the case of solving for a spatially - invariant convolution kernel .",
    "the performance of three proposed kernel design algorithms ( section  [ sec : ker_design ] ) coupled with up to eight model selection criteria ( section  [ sec : model_selection_criteria ] ) was assessed with regards to model error ( simulations only ) , fit quality , and photometric accuracy .",
    "in total 19 methods were tested .",
    "the conclusions and recommendations from our investigation are detailed in section  [ sec : conclusions ] .",
    "in this section , we briefly describe the methods used in this paper to solve for the spatially - invariant convolution kernel matching the psf between two images of the same field .",
    "consider a pair of registered images of the same field with the same dimensions and sampled on the same pixel grid . to avoid invalidating the assumption of a spatially - invariant kernel model",
    ", the image registration should be such that at most there is a translational offset of a few pixels between the images , with no rotational ( or other ) image misalignments .",
    "let the images be referred to as the reference image @xmath2 and the target image @xmath3 with pixel values @xmath4 and @xmath5 , respectively , where @xmath6 and @xmath7 are pixel indices referring to the column @xmath6 and row @xmath7 of an image .",
    "we model the target image @xmath3 as a model image @xmath8 formed by the convolution of the reference image @xmath2 with a spatially - invariant discrete convolution kernel @xmath9 plus a spatially - invariant ( constant ) differential background @xmath10 : @xmath11_{ij } + b \\label{eqn : model1}\\ ] ] where the @xmath12 are the pixel values of the model image . as in @xcite , we model @xmath9 as a linear combination of basis functions : @xmath13 where the @xmath14 are the kernel pixel values , @xmath15 and @xmath16 are pixel indices corresponding to the column @xmath15 and row @xmath16 of the discrete kernel , @xmath17 is the number of kernel basis functions , and the @xmath18 are the pixel values of the @xmath19th discrete kernel basis function @xmath20 with corresponding coefficient @xmath21 .",
    "substitution of equation  ( [ eqn : kernel ] ) into equation  ( [ eqn : model1 ] ) yields : @xmath22_{ij } + b \\label{eqn : model2}\\ ] ] with : @xmath23_{ij } = \\sum_{rs } r_{(i+r)(j+s ) } \\kappa_{qrs } \\label{eqn : basis_image}\\ ] ] the image @xmath24_{ij}$ ] is referred to as a _ basis image_. the model image @xmath8 has @xmath25 parameters .",
    "note that equation  ( [ eqn : model2 ] ) may be derived as a special case of equation  ( 8) from br13 .",
    "assuming that the target image pixel values @xmath5 are independent observations drawn from normal ( or gaussian ) distributions @xmath26 and that the parameters @xmath21 and @xmath10 of the model image have uniform bayesian prior probability distribution functions ( pdfs ) , then the maximum likelihood estimator ( mle ) of @xmath21 and @xmath10 may be found by minimising the chi - squared : @xmath27 this is a general linear least - squares problem ( see @xcite ) with associated _ normal equations _ in matrix form : @xmath28 where the symmetric and positive - definite @xmath29 matrix @xmath30 is the least - squares matrix , the vector @xmath31 is the vector of @xmath32 model parameters , and @xmath33 is another vector . for the vector of parameters : @xmath34 the elements of @xmath30 and @xmath33 are given in terms of the basis images by : @xmath35 @xmath36 @xmath37_{ij } & \\mbox{for $ 1 \\le q \\le n_{\\kappa}$ } \\\\ 1                            & \\mbox{for $ q = n_{\\kappa } + 1 $ }      \\\\ \\end{cases } \\label{eqn : psi_pattern}\\ ] ]    cholesky factorization of @xmath30 , followed by forward and back",
    "substitution is the most efficient and numerically stable method ( @xcite ) for obtaining the solution @xmath38 to the normal equations ( i.e. @xmath39 is the vector of mles of the model parameters ) .",
    "the inverse matrix @xmath40 is the covariance matrix of the parameter estimates @xmath41_{q q^{\\,\\prime}}$ ] and consequently the uncertainty @xmath42 in each @xmath43 is given by : @xmath44_{q q } } \\label{eqn : sol_uncertainties}\\ ] ]    for the spatially - invariant kernel , the _ photometric scale factor _",
    "@xmath45 between the reference and target image is a constant : @xmath46 as noted by @xcite , it is good practice to subtract an estimate of the sky background level from @xmath2 before solving for @xmath9 and @xmath10 in order to minimise any correlation between @xmath45 and @xmath10 .",
    "we adopt a noise model for the model - image pixel uncertainties @xmath47 of : @xmath48 where @xmath49 is the ccd readout noise ( adu ) , @xmath50 is the ccd gain ( e@xmath51/adu ) , and @xmath52 is the flat - field image .",
    "the @xmath47 depend on the @xmath12 which renders our maximum likelihood problem as a non - linear problem and also requires that the mle of the model image parameters is obtained by minimising @xmath53 instead of @xmath54 .",
    "however , iterating the solution by considering the @xmath47 and @xmath12 in turn as fixed is an appropriate linearisation of the problem that still allows for the model image parameters to be determined by minimising @xmath54 at each iteration as described above ( since the @xmath47 are considered as constant whenever the model image parameters are being estimated ) .",
    "for the first iteration , we estimate the @xmath47 by approximating @xmath12 in equation  ( [ eqn : noise_model ] ) with @xmath5 .",
    "a @xmath55-sigma - clip algorithm is employed at the end of each iteration except for the first to prevent outlier target - image pixel values from influencing the solution ( e.g. cosmic rays , variable stars , etc . ) .",
    "the criterion for pixel rejection is @xmath56 , and we use @xmath57 .",
    "only 3 - 4 iterations are required for convergence and the final solution is highly insensitive to the initial choice of @xmath47 ( e.g. setting all of the @xmath47 to unity for the first iteration gives exactly the same result as setting the @xmath47 by approximating @xmath12 in equation  ( [ eqn : noise_model ] ) with @xmath5 ) .",
    "finally , it should be noted that lack of iteration introduces a bias into the kernel and differential background solution ( see br13 for a discussion and examples ) .",
    "the difference image @xmath58 is defined by : @xmath59 from which we may define a normalised difference ] in the absence of varying objects , and for a reliable noise model , the distribution of the @xmath60 values provides an indication of the quality of the difference image ; namely , the @xmath60 should follow a gaussian distribution with zero mean and unit standard deviation .",
    "if the @xmath60 follow a gaussian distribution with significant bias or standard deviation greater than unity , then systematic errors are indicated , which may be due to under - fitting .",
    "if they follow a gaussian distribution with standard deviation less than unity , then over - fitting may be indicated .",
    "if they follow a non - gaussian distribution , then an inappropriate noise model may be at least part of the cause .",
    "the final ingredient required to construct a kernel solution is the definition of the set of kernel basis functions , which in turn defines the set of basis images . in this paper",
    "we consider only the _ delta basis functions _ , which are defined by : @xmath61 where a one - to - one correspondence @xmath62 associates the @xmath19th kernel basis function @xmath20 with the discrete kernel pixel coordinates @xmath63 , and @xmath64 is the kronecker delta - function : @xmath65 as such , each dbf @xmath20 and its corresponding coefficient @xmath21 represent a single kernel pixel and its value , respectively .",
    "note that this definition of the dbfs ignores the transformation that is required when the photometric scale factor is spatially varying ( br13 ) .",
    "the dbfs have a conveniently simple expression for the corresponding basis images : @xmath23_{ij } = r_{(i+\\mu)(j+\\nu ) } \\label{eqn : delta_basis_images}\\ ] ]      for the dbfs , be12 introduced a refinement to the normal equations to control the trade - off between noise and resolution in the kernel solution .",
    "they used _",
    "tikhonov regularisation _",
    "( see @xcite ) to penalise kernel solutions that are too noisy by adding a penalty term to the chi - squared that is derived from the second derivative of the kernel surface and whose strength is parameterised by a tuning parameter @xmath0 .",
    "the addition of a penalty term to the chi - squared is equivalent to adopting a non - uniform bayesian prior pdf on the model parameters .",
    "the corresponding maximum penalised likelihood estimator ( mple ) of @xmath21 and @xmath10 is obtained by minimising : @xmath66 where @xmath67 is the number of data values from their equation  ( 12 ) . ]",
    "( i.e. target - image pixels ) and @xmath68 is an @xmath69 matrix with elements : @xmath70 we consider two dbfs to be _ adjacent _ if they share a common kernel - pixel edge , _ connected _ if they can be linked via any number of pairs of adjacent dbfs , and _ disconnected _ if they are not connected . note that the elements of the last row and column of @xmath68 , corresponding to the differential background parameter @xmath10 , are all zero .",
    "the matrix @xmath68 is the _ laplacian matrix _ representing the connectivity graph of the set of dbfs ( cf .",
    "graph theory ) .",
    "it is symmetric , diagonally dominant , and positive - semidefinite .",
    "all of the eigenvalues of @xmath68 are non - negative while @xmath71 of them are equal to zero . here",
    ", @xmath72 is the number of disconnected sets of connected dbfs within the full set of dbfs ( i.e. the number of components of the connectivity graph ) .",
    "consequently , the rank of @xmath68 is @xmath73 , as is the rank of @xmath74 , which are facts that we will use later in section  [ sec : model_selection_criteria_dia ] .",
    "it is also useful to note that if all of the dbfs are connected to each other , then @xmath68 and @xmath75 are both of rank @xmath76 . in appendix",
    "a , we present a couple of example kernels with their corresponding @xmath68 matrices .",
    "the expression in equation  ( [ eqn : pen_chi_sqr ] ) is at a minimum when its gradient with respect to each of the parameters @xmath21 and @xmath10 is equal to zero . performing the @xmath77 differentiations and rewriting the set of linear equations in matrix form we obtain the _ regularised normal equations _ : @xmath78 where : @xmath79    obtaining the solution to the regularised normal equations now proceeds as for the normal equations in section  [ sec : solving_for_kernel ] .",
    "the covariance matrix of the parameter estimates @xmath80 is similarly given by @xmath81_{q q^{\\,\\prime}}$ ] .",
    "here we describe our statistical tool - kit of model selection criteria that we will use for deciding on the best set of dbfs to be employed in the modelling of the convolution kernel .",
    "the criteria are valid for linear models , such as our model image @xmath8 in equation  ( [ eqn : model2 ] ) , and for data drawn from independent gaussian distributions , which is a valid approximation to the poissonian statistics of photon detection for ccd image data @xmath3 that only breaks down at very low signal levels ( @xmath8216  e@xmath51 ) .",
    "we direct the reader to @xcite for an essential reference on the information criteria presented below .",
    "the @xmath83-test may be used to compare two models a and b with parameter sets @xmath84 and @xmath85 , respectively , that are nested ( i.e. @xmath86 ) .",
    "the @xmath83-statistic is defined by : @xmath87 where @xmath88 and @xmath89 are the chi - squared values of models a and b , respectively ( see equation  ( [ eqn : chi_sqr ] ) ) . under the null hypothesis that model b does not provide a significantly better fit than model a",
    ", the @xmath83-statistic follows a chi - squared distribution with @xmath90 degrees of freedom ( dof ) .",
    "we set our @xmath83 threshold for rejection of the null hypothesis at 1 per cent ( e.g. @xmath91 for dof  =  1 ) .",
    "we adopt the chi - squared values of models a and b as those calculated during the first iteration of our kernel solution procedure to enable a fair comparison between models since they are both computed using the same pixel uncertainties ( i.e. the @xmath47 estimated by approximating @xmath12 in equation  ( [ eqn : noise_model ] ) with @xmath5 ) .",
    "however , the values of the model image parameters are still taken as those calculated in the final iteration of the kernel solution procedure .",
    "model selection using the @xmath83-test applies only to models a , b , ... , z with sequentially nested parameter sets @xmath92 . starting with models a and b , the @xmath54 is minimised for each model and the @xmath83-test is used to determine whether or not model b provides a significantly better fit than model a. if it does not , then model a is accepted as the correct model and the procedure terminates , otherwise the next pair of models b and c are evaluated using the same method .",
    "the procedure continues by evaluating sequential model pairs in this fashion until either the @xmath83-test indicates that the next model does not provide a significantly better fit or until there are no more models to test .",
    "the @xmath93-test may also be used to compare two nested models a and b. the @xmath93-statistic is defined by : @xmath94 where @xmath67 is the number of data values . again , under the null hypothesis that model b does not provide a significantly better fit than model a , @xmath93 follows an @xmath93-distribution with dof  @xmath95 .",
    "we set our @xmath93 threshold for rejection of the null hypothesis at 1 per cent ( e.g. @xmath96 for dof  =  ( 2,1000 ) ) and we compute the @xmath93-statistic using the chi - squared values of models a and b calculated during the first iteration of our kernel solution procedure .",
    "model selection with the @xmath93-test applies to models a , b , ... , z with sequentially nested parameter sets and proceeds in the same way as model selection with the @xmath83-test .",
    "the principal of maximum likelihood assumes a uniform prior pdf on the model parameters .",
    "a consequence of this is that as parameters are added to a model , the maximum likelihood always increases , rendering it useless for the purpose of model selection between models with different dimensionality .",
    "information criteria are used as an alternative for evaluating models with different numbers of parameters .",
    "they may be applied regardless of whether the models under consideration are nested or non - nested .",
    "the akaike information criterion ( aic ; @xcite ) is derived as an asymptotic approximation to the kullback - leibler divergence ( @xcite ) , hence favouring models with smaller numbers of parameters .",
    "] , which measures the distance of a candidate model from the true underlying model under the assumption that the true model is of infinite dimension and is therefore not represented in the set of candidate models .",
    "the aim of the aic is to evaluate models based on their prediction accuracy .    a version of the aic for gaussian linear regression problems that corrects for the small - sample bias while being asymptotically the same as the aic for @xmath98 was derived by @xcite : @xmath99 where @xmath100 is the likelihood function for the vector of model parameters @xmath101 , and @xmath102 is a vector of mles for the model parameters .",
    "model selection with the aic@xmath97 is performed by minimising @xmath103 for each model , and then minimising aic@xmath97 over the full set of models under consideration .",
    "the takeuchi information criterion ( tic ; @xcite ) is a generalisation of the aic ( @xcite ) given by : @xmath104 where tr is the matrix trace operator .",
    "the matrices @xmath105 and @xmath106 are defined as : @xmath107 @xmath108 @xmath109 where @xmath110 is the likelihood function for the @xmath6th ( single ) data point .",
    "model selection with the tic proceeds as for the aic@xmath97 .",
    "the bayesian approach to model selection is to choose the model with the largest bayesian posterior probability . by approximating the posterior probability of each model",
    ", @xcite derived the bayesian information criterion ( bic ) for model selection : @xmath111 the bic generally includes a heavier penalty than the aic@xmath97 for more complicated models ( e.g. in the regime @xmath112 and @xmath113 ) , therefore favouring models with fewer parameters than those favoured by the aic@xmath97 .",
    "model selection with the bic proceeds as for the aic@xmath97 .",
    "@xcite performed a deeper bayesian analysis to derive an improved bic : @xmath115 model selection with the bic@xmath114 proceeds as for the aic@xmath97 .",
    "it is worth mentioning that the bic and bic@xmath114 are _ consistent _ model selection criteria in that they select with high probability the true model from the set of candidate models whenever the true model is represented in the set of candidate models .",
    "the aic , aic@xmath97 , tic , bic and bic@xmath114 apply only to models estimated by maximum likelihood .",
    "@xcite derived a further generalisation of the aic and tic , called the generalised information criterion ( gic ) , that can be applied to model selection for models with parameters estimated by maximum penalised likelihood : @xmath117 where @xmath118 is a vector of mples for the model parameters , and : @xmath119 @xmath120 here @xmath121 is an @xmath122 matrix and we have used the fact that it is symmetric to slightly simplify the @xcite expressions for @xmath123 and @xmath124 ( their equation  ( 21 ) ) .",
    "model selection with the gic@xmath125 is performed by minimising gic@xmath125 over @xmath0 for each model , and then selecting the model for which gic@xmath125 is minimised over the full set of models under consideration .",
    "using the same bayesian analysis as for the derivation of the bic@xmath114 , @xcite also extended the bic@xmath114 to apply to model selection for models with parameters estimated by maximum penalised likelihood . for @xmath121 of rank @xmath126 , and denoting the product of the @xmath126 non - zero eigenvalues of @xmath121 by @xmath127 , they derived : @xmath128 model selection with the bic@xmath125 proceeds as for the gic@xmath125 .",
    "we may adapt the various information criteria from sections  [ sec : model_selection_criteria_ic_ml ]  and  [ sec : model_selection_criteria_ic_mpl ] to our problem of solving for the kernel and differential background in dia .",
    "the model image @xmath8 has @xmath25 parameters and we use the notation @xmath129 , @xmath130 and @xmath131 .",
    "firstly , we compute the log - likelihood function for data drawn from gaussian distributions @xmath26 as : @xmath132 for model selection purposes , the last term @xmath133 is constant and can be ignored .",
    "secondly , we note that since the @xmath47 are considered as constant at each iteration of the maximum likelihood problem in section  [ sec : solving_for_kernel ] , the matrices @xmath134 and @xmath135 evaluated at @xmath38 are given by : @xmath136_{q q^{\\,\\prime } } = \\frac{1}{n_{\\mbox{\\scriptsize dat } } } \\sum_{ij } \\ , \\varepsilon_{ij}^{2 } \\ , \\frac{\\psi_{qij } \\ , \\psi_{q^{\\,\\prime } ij}}{\\sigma_{ij}^{2 } } \\label{eqn : ihat}\\ ] ] @xmath137 for computational purposes it is useful to note that @xmath138 is symmetric . from these two expressions , we may derive the following results : @xmath139_{q q^{\\,\\prime } } \\ , [ \\mathbf{h}^{-1 } ] _ { q q^{\\,\\prime } } \\label{eqn : trij}\\ ] ] @xmath140 finally , we consider that the solution of the normal equations requires the computation of the cholesky factorisation @xmath141 , where @xmath142 is a lower triangular matrix with positive diagonal entries @xmath143 , from which we may immediately calculate the determinant of @xmath30 as @xmath144 . hence , with minimal extra computation , the cholesky factorisation of @xmath30 yields : @xmath145    therefore , using equations  ( [ eqn : ln_l ] ) and ( [ eqn : trij ] )  -  ( [ eqn : deth ] ) for the maximum likelihood problem in section  [ sec : solving_for_kernel ] , we have the following formulae for the relevant information criteria from section  [ sec : model_selection_criteria_ic_ml ] : @xmath146 @xmath147_{q q^{\\,\\prime } } \\ , [ \\mathbf{h}^{-1 } ] _ { q q^{\\,\\prime } } \\label{eqn : tic_use}\\ ] ] @xmath148 @xmath149    considering now the maximum penalised likelihood problem , for constant @xmath47 we have @xmath150 , which is equal to @xmath151 when evaluated at @xmath80 ( using equations  ( [ eqn : matrix_reg_normal_eqns ] )  and  ( [ eqn : reg_least_squares_matrix ] ) ) .",
    "then , using @xmath74 , the matrices @xmath152 and @xmath153 evaluated at @xmath80 are given by : @xmath154 @xmath155 writing @xmath156 , then , from these two expressions , we may derive the following results : @xmath157_{q q^{\\,\\prime } } \\ , - \\ , \\lambda^{2 } \\",
    ", \\omega_{q } \\ , \\omega_{q^{\\,\\prime } } \\right ) \\ ,                    [ \\mathbf{h_{\\mbox{\\scriptsize p}}}^{-1 } ]",
    "_ { q q^{\\,\\prime } } \\label{eqn : trijdash}\\ ] ] @xmath158 also , the cholesky factorisation of @xmath159 yields : @xmath160 finally , we note that the matrix @xmath75 is of rank @xmath161 , and hence @xmath162 .    therefore , using equations  ( [ eqn : ln_l ] ) and ( [ eqn : trijdash ] )  -  ( [ eqn : dethdash ] ) for the maximum penalised likelihood problem in section  [ sec : reg_delta_basis ] , we have the following formulae for the relevant information criteria from section  [ sec : model_selection_criteria_ic_mpl ] : @xmath163_{q q^{\\,\\prime } } \\ , - \\ , \\lambda^{2 } \\ , \\omega_{q } \\ , \\omega_{q^{\\,\\prime } } \\right ) \\ ,                                                  [ \\mathbf{h_{\\mbox{\\scriptsize p}}}^{-1 } ] _ { q q^{\\,\\prime } } \\end{aligned } \\label{eqn : gicp_use}\\ ] ] @xmath164",
    "let us introduce the concept of a _ kernel design _ , which we define as a specific choice of dbfs ( or , equivalently , kernel pixels ) to be employed in the modelling of the convolution kernel . from a master set of @xmath165 dbfs",
    ", the model selection criteria will each select a single `` best '' kernel design , which requires the evaluation of the criteria via the estimation of the model image parameters for each of the @xmath166 possible kernel designs .",
    "this computational problem is formidable and currently infeasible for values of @xmath165 that are required for typical kernel models ( e.g. a relatively small 9x9 kernel pixel grid yields @xmath1672.4@xmath110@xmath168 potential kernel designs ! ) .",
    "furthermore , branch - and - bound algorithms ( e.g. @xcite ) for speeding up this exhaustive search are only applicable to some of our model selection criteria in section  [ sec : model_selection_criteria_dia ] .    it is well known that by not considering all of the possible combinations of predictor variables in a linear regression problem ( e.g. by using stepwise regression for variable selection ) , the optimal set of predictors may be misidentified .",
    "however , in our case , we know from the nature / purpose of the kernel ( and copious amounts of prior experience ! ) that the true kernel model has a peak signal at the kernel coordinates corresponding to the translational offset between the reference and target images ( which is at the kernel origin when they are properly registered ) and that this signal decays away from the peak . there may be other peaks ( e.g. due to a telescope jump in the target image ) , but again these also have profiles that decay away from the peak(s ) .",
    "the best kernel designs are therefore generally limited to sets of dbfs in close proximity that form relatively compact and regular shapes .",
    "based on these observations , we have devised two algorithms for automatic kernel design that compare a manageable number of sensible kernel models ; the circular kernel design algorithm ( section  [ sec : simim_ckda ] ) and the irregular kernel design algorithm ( section  [ sec : simim_ikda ] ) .",
    ".the number of dbfs in a circular kernel design for different ranges of the kernel radius @xmath169 .",
    "the ranges are defined by @xmath170 .",
    "the table may be extended as appropriate for larger values of @xmath169 . [ cols=\"^,^,^,^,^,^,^,^\",options=\"header \" , ]",
    "in this appendix , we present figures  [ fig : results_reg1 ]  -  [ fig : results_reg4 ] where we plot the median mse , @xmath45 , mfb , and mfv values , and the mpb and mpv measures , for each kernel solution method for various subsets of our simulations chosen based on image sampling .",
    "these plots are referred to briefly in sections  [ sec : sim_results ]  and  [ sec : sim_disc ] .",
    "for completeness of this paper , in figures  [ fig : results_mse_map ]  -  [ fig : results_mfv_map ] we plot surfaces representing the median mse , @xmath45 , mfb , and mfv values for simulation set s10 as a function of the reference image and kernel fwhm .",
    "these plots are referred to briefly in section  [ sec : sim_fur_inv ] ."
  ],
  "abstract_text": [
    "<S> we present a selection of methods for automatically constructing an optimal kernel model for difference image analysis which require very few external parameters to control the kernel design . </S>",
    "<S> each method consists of two components ; namely , a kernel design algorithm to generate a set of candidate kernel models , and a model selection criterion to select the simplest kernel model from the candidate models that provides a sufficiently good fit to the target image . </S>",
    "<S> we restricted our attention to the case of solving for a spatially - invariant convolution kernel composed of delta basis functions , and we considered 19 different kernel solution methods including six employing kernel regularisation . </S>",
    "<S> we tested these kernel solution methods by performing a comprehensive set of image simulations and investigating how their performance in terms of model error , fit quality , and photometric accuracy depends on the properties of the reference and target images . </S>",
    "<S> we find that the irregular kernel design algorithm employing unregularised delta basis functions , combined with either the akaike or takeuchi information criterion , is the best kernel solution method in terms of photometric accuracy . </S>",
    "<S> our results are validated by tests performed on two independent sets of real data . </S>",
    "<S> finally , we provide some important recommendations for software implementations of difference image analysis .    </S>",
    "<S> [ firstpage ]    methods : statistical - techniques : image processing - techniques : photometric - methods : data analysis . </S>"
  ]
}