{
  "article_text": [
    "learning and storing temporal sequences in neural networks have long been an important research topic since the forward progression of time is a fundamental aspect of human reasoning .",
    "different approaches have already been proposed .",
    "some exploit the dynamics of spiking neurons [ [ barber]][[brea ] ] , others are based on binary neurons , similar to the mcculloch - pitts model [ [ mcpitts ] ] with binary inputs and outputs .    most learning theoretical models are based on the hebbian assumption , that is , changes in synaptic strength related to correlations of pre- and post - synaptic firing . among them , hopfield networks [ [ hopfield ] ] , known predominantly for their ability to encode static patterns , could be adapted to store temporal sequences in an incremental manner .",
    "for example , in [ [ maurer ] ] , the hopfield model is extended to encode a time series of patterns .",
    "the sequences are stored in a set of hopfield networks linked to one another through a matrix of weights .",
    "however , the limitation of hopfield networks is well known : only `` about @xmath0 states can be simultaneously remembered before error in recall is severe '' , @xmath1 being the number of units in the network [ [ hopfield ] ] .",
    "this is due to  catastrophic interference \" ( ci ) [ [ mccloskey ] ] or  catastrophic forgetting \" ( cf ) [ [ french ] ] .",
    "indeed , in hopfield - like networks , the storage of new information affects on all the connections and thus introduces noise to all the messages that have been stored .",
    "other models such as those in [ [ hinton ] ] and [ [ sutskever ] ] are based on boltzmann machines [ [ ackley ] ] , in which the weights are continuous , and these models have a similar issue with new messages affecting the ability to retrieve older ones .",
    "in contrast to the models discussed above , willshaw - type models [ [ willshaw]][[sommer ] ] consider sparse messages and binary connections instead of weighted ones .",
    "the ci issue is then better resolved : a newly stored pattern only involves a small number of connections compared to the network size .",
    "it can partially overlap with some older ones , but the number of overlapped patterns is largely smaller than the total number of ones that the network can potentially store . concerning sequential learning , it is possible to store the most elementary sequences ( those containing only two patterns , a cue pattern and a target pattern ) in a bipartite willshaw network . a sparse pattern _ a _ represented in memory _ x _",
    "is associated with another sparse pattern _ b _ in memory _",
    "y _ via hetero - associative connections .",
    "a one - step retrieval algorithm enables to retrieve _",
    "b _ provided _ a_.    by introducing an organization in clusters , a recently proposed willshaw - type neural network based on cliques and sparse representations",
    "[ [ gb1 ] ] can treat information with non binary alphabets .",
    "this is extended in [ [ aliabadi ] ] with a further sparse organization and structure .",
    "this model demonstrates large storage diversity ( the number of storable messages with a relatively high recovery rate ) and capacity ( the amount of storable information ) , as well as strong robustness with respect to erasures and errors .",
    "however , the non - oriented connections in this model are not biologically relevant .",
    "it is then of high interest to replace them with oriented ones .",
    "orientation of connections would naturally offer the network the ability to store sequential information .",
    "one may expect this structure to inherit the good properties of the clique - based networks to solve some well - known issues such as ci in sequence learning .",
    "anticipation is important in human sequential behavior .",
    "for instance , it is shown in [ [ synder ] ] that for sound sequences , induced gamma - band activity predicts tone onsets and even persists when expected tones are omitted .",
    "subsequently , [ [ leaver ] ] suggests that learning sound sequences recruits similar brain structures as motor sequence learning , in which retrieving stored sequences of any kind involves predictive readout of upcoming information before the actual sensorimotor event .",
    "anticipation in visual graphemic sequences is studied in [ [ orliaguet ] ] , in particular the between - letter context effects .",
    "it is shown that this kind of multi - model approach such as _ shape + movement _ anticipates better than a simple _ shape _ or _",
    "movement_. inspired from these experiments , the sequential anticipation in this paper exploits redundancy in two dimensions : temporal and spatial redundancy .",
    "the rest of paper is organized as follows : section [ chpt_clique_based_network ] recalls the principles of storing fixed length messages in clique - based networks , which is at the root of the work presented in this paper . in section [ chpt_chain_of_tournmt ] , the oriented sparse neural networks based on original oriented graphs , called",
    " chains of tournaments \" are demonstrated to be good material to store sequential information with an anticipated mechanism .",
    "generalization is proposed in section [ chpt_learning_seq_pat ] .",
    "section [ dbl_layer_strct ] introduces a structure combining clique - based and tournament - based networks in order to obtain accurate retrieving .",
    "finally , a conclusion is provided in section [ conclu ] .",
    "let us recall the key points of the clique - based neural networks initially introduced in [ [ gb1 ] ] and then extended in [ [ aliabadi ] ] which could be seen as a willshaw structure [ [ willshaw]][[sommer ] ] with the ability to store messages with a non binary alphabet .",
    "such a network is composed of @xmath2 binary nodes organized in @xmath3 clusters .",
    "each cluster contains a certain number of nodes , which materialize the alphabet .",
    "different clusters may reify different alphabets .",
    "for example , a cluster of 26 nodes may represent the 26 letters of the english alphabet and another cluster of 100 nodes , in the same network , may represent the decimal numbers from 1 to 100 .",
    "for the reason of simplicity , unless specifically mentioned , in the sequel we consider clusters of same size @xmath4 each .",
    "we denote by @xmath5 the @xmath6 node in the @xmath7 cluster , the value of which @xmath8 is respectively one or zero , activated or not . in our vocabulary , the nodes are called `` fanals '' , since during the storage process , only one of them in the same cluster can be activated at the same time .",
    "let @xmath9 be the set of messages to be stored by the network .",
    "each message @xmath10 is split into @xmath11 sub - messages or symbols @xmath12 .",
    "graphically , each sub - message is mapped to a unique fanal in the corresponding cluster . as a consequence ,",
    "storing a message is equivalent to storing the corresponding pattern of @xmath11 fanals . this pattern",
    "is then represented by a clique , that is , a set of fanals such that each one is connected to the others .",
    "it has been proven in [ [ gb2 ] ] that cliques are codewords of a good error correcting code . the binary edge weight between nodes @xmath5 and @xmath13 is denoted by @xmath14 .",
    "the weight of an existing connection remains unchanged even if the same pair of fanals appears in two or more messages .",
    "an example with two messages of order @xmath15 stored in a network of @xmath16 clusters is represented in fig .",
    "[ non_oriented_network ] .     are stored in a network of @xmath16 clusters in form of cliques .",
    "one edge is shared .",
    "fanals are represented respectively in 6 clusters by open circles , filled circles , open triangles , filled rectangles , open rectangles and filled triangles.,width=336 ]    formally , if we denote @xmath17 the connection set of the corresponding clique after learning message @xmath18 , the connection set of the associated graph after learning @xmath19 can therefore be defined by the union : @xmath20    at the retrieval stage , the network is provided with a message @xmath21 , possibly a distorted version of the stored message @xmath22 .",
    "several forms of distortions are envisaged .",
    "we call an _ erasure _ , the situation when a cluster that should be active is not provided with information , in other words none of the fanals in this cluster are activated ; an _ error _ , the situation when the fanal activated in a cluster is not the right one ; and an _ insertion _ , the situation when some fanals are activated in the clusters that should be silent . based on the set of existing connections , an appropriate fault tolerant decoding algorithm should be able to give an estimate @xmath23 , which would be the nearest from @xmath22 in terms of hamming distance among @xmath9 : minimum hamming distance amounts maximun correspondence between two graphical patterns",
    ".    the decoding procedure could be iterative and each iteration normally consists of two steps : _ message passing _ and _ selection of the winner _ [ [ ala ] ] .",
    "the former exchanges stimuli ( that is , some of the sub - messages @xmath24 ) within the whole network via established connections , and the contributions are added at each fanal .",
    "the latter chooses the fanals that are the most likely correct for the next iteration . in the case",
    "that all the clusters are addressed by a single message ( @xmath25 ) , a winner - take - all ( wta ) rule is performed locally within each cluster [ [ gb1 ] ] .",
    "this rule is adapted in [ [ aliabadi ] ] for storing sparse messages ( @xmath26 ) by performing global winner - take - all ( gwta ) in the whole network .    however , gwta is not suitable for an iterative decoding process .",
    "in fact , as mentioned in the original paper [ [ aliabadi ] ] , although there is indeed an advantage to so called iterative `` guided decoding '' ( the indices of the clusters involved in the target message are known beforehand , which reduces to the similar situation as in [ [ gb1 ] ] ) , on the contrary , iterations do not help for `` blind decoding '' ( when the cluster indices are not known beforehand ) .",
    "indeed , gwta will only choose the fanals strictly with the highest score .",
    "corrupted information at a certain iteration could incorrectly fire very few fanals with the highest score , all the others being turned off . at the next iteration , the very limited information provided to the decoder would activate an excessively large number of fanals .",
    "the network would then oscillate between these two states as long as the iterations continue .",
    "a more detailed study of retrieval algorithms in clique - based networks is carried out in [ [ ala ] ] [ [ jiangthesis ] ] where some improvements are proposed . among them",
    ", global winners - take - all ( gwsta ) selects @xmath27 , an expected number of fanals ( that is @xmath11 , the clique order , in the ideal case ) that have the maximal or near maximal scores .",
    "global losers - kicked - out ( glsko ) follows an opposite approach . instead of selecting the fanals with the highest score",
    ", it eliminates those with the lowest score .",
    "these two algorithms improve considerably the retrieval performance for `` blind decoding '' , especially with input distortions of type _ error _ or _ insertion_. the performance approach that of maximum likelihood ( ml ) decoding .",
    "these algorithms will be discussed and evaluated in section [ glb_decod_schm ] and section [ dbl_layer_strct ] with the adaptation to retrieval of sequences of patterns .",
    "the _ data representation _ of a neural pattern is of critical importance in neural networks .",
    "two opposite approaches are worth mentioning . on one hand ,",
    "a knowledge element can be materialized by a highly localized element , called the _ grandmother cell _ [ [ bar72 ] ] [ [ gro02 ] ] representation .",
    "the problem with this theory is the explosion of material if one would represent a very large number of knowledge elements . on the other hand ,",
    "the principle of _ distributed representation _ [ [ rhm86 ] ] states that a specific piece of information is encoded by a unique pattern of activity over a group of neurons . to make it clear",
    ", @xmath2 grandmother neurons can only represent @xmath2 knowledge elements , whereas this number is @xmath28 using the distributed approach . to our knowledge",
    ", there is more and more evidence that memories are stored in the brain using a distributed representation across numerous cortical and subcortical regions [ [ eichenbaum ] ] [ [ fuster ] ] , which means that each memory is stored across thousands of synapses and neurons , and each neuron or synapse is potentially involved in thousands of memories .",
    "currently , it is generally assumed that neural networks are structured in a columnar organization . at the lowest level , about @xmath29 neurons",
    "form a cortical microcolumn ( also called the minicolumn ) [ [ bc02 ] ] , which is a vertical cylinder through the six cortical layers of the brain .",
    "the diameter of a microcolumn is about @xmath30 m . there are about @xmath31 microcolumns in the human neocortex [ [ jl07 ] ] .",
    "neurons within a microcolumn perform various tasks : short and long range excitation , short range inhibition . from the point of view of informational organization ,",
    "the microcolumn is likely the most basic and consistent information processing unit in the brain , instead of a single neuron [ [ jones]]-[[cruz ] ] . as a consequence",
    ", we deliberately call a node in our network a `` fanal '' instead of a `` neuron '' to make it clear that a node corresponds to a microcolumn .",
    "then , dozens of cortical microcolumns form a macrocolumn , analogous to the way that nodes compose a cluster . in the literature , the name of this unit is often ambiguous :  column \" ,  macrocolumn \" and  hypercolumn \" are commonly interchangeable .",
    "finally , several macrocolumns ( clusters ) are grouped together to contribute to a functional area of the brain .",
    "the general concept of the brain as an information processing machine underlines the importance of information theory in cognition problems . a number of research efforts [ [ miller]]-[[cowan ] ] point out that the _ chunk _ of information , an abstract but nevertheless measurable unit of information , is relevant to several human cognitive features . in the clique - based network ,",
    "the division of information into clusters is in fact a way for _",
    "chunking_. a sparse message",
    "may be composed of chunks , the number of which is significantly smaller than the number of clusters in the entire network . and",
    "a hierarchical structure could be envisaged to form clusters of clusters as well as chunks of chunks . however , these perspectives are not the concern of this paper .      from the information point of view",
    ", the memory is robust and durable and therefore must be encoded _ redundantly_. the concept of _ informational redundancy _ was originally defined by shannon in the context of communication theory [ [ shannon ] ] .",
    "recently numerous studies have proposed to investigate connectivity redundancy of complex brain networks with the introduction of basic principles of graph theory , such as [ [ bullmore ] ] [ [ lanzo ] ] . however , some authors consider that redundancy can limit the amount of information carried by a neural system : `` in neuroscience , redundancy implies inefficiency '' [ [ friston ] ] . in short",
    ", redundancy has to be used with moderation and in a subtle way .",
    "the retrieval algorithms in section [ chpt_clique_based_network ] make use of the high redundancy of cliques . as a matter of fact",
    ", a clique composed of four nodes for example can be uniquely defined by two connections if well chosen , and the other four serve as redundant information ( see fig .",
    "[ clique_redundancy ] ) .",
    "one may consider degenerating a @xmath11-clique by taking off some connections . supposing the total number of connections after this degeneration is a multiple of the order @xmath11 .",
    "we denote this number @xmath32 , with @xmath33 an integer and @xmath34 .",
    "this degeneration can be achieved in several different ways , going from the inhomogeneous ( some nodes having all possible connections whereas some others having very few connections ) to the most homogeneous , which is the case for a @xmath35-connected ring graph of order @xmath11 ( see the graph on the left of fig . [ chain_of_cliques_vs_tournaments ] ) , which is denoted by @xmath36 in the sequel .",
    "our simulation ( see fig.[simu_clique_redundancy ] ) shows that homogeneity of connectivity of the regular degenerated clique , that is , @xmath36 , offers the best retrieval performance .",
    "it has been proven in [ [ gb1 ] ] that the merit factor of a clique - based code is 2 , which means this can be considered a good error correcting code",
    ". we can carry out a similar demonstration here for @xmath36 .",
    "we define the minimum hamming distance @xmath37 of the code based on @xmath36 as the minimum number of edges that differ between two @xmath35-connected ring graphs of order @xmath11 .",
    "@xmath37 is obtained in the case of only one vertex being different : @xmath38    when the size of the graph is large in comparison with that of the pattern , which is here @xmath36 , the coding rate @xmath39 is given by the ratio between the minimum number of edges that are necessary to specify a pattern and the number of edges used . in the case of @xmath36 , @xmath39 can be expressed as : @xmath40    finally , the merit factor is obtained as : @xmath41    we find exactly the same merit factor of 2 as it is for a non degenerated clique - based code : a code based on @xmath36 has a smaller minimal hamming distance , which is exactly compensated by a higher coding rate .",
    "let us denote @xmath42 the closed integer interval between @xmath43 and @xmath44 . in a non - oriented network ,",
    "the connection matrix is symmetric : @xmath45 .",
    "this is however not biologically relevant .",
    "indeed , a biological neuron is composed of a cell body and two types of branched projections : the axon and the dendrites .",
    "the dendrites of a neuron reproduce the electrochemical stimuli received from the axons of other neural cells to its cell body , which then propagates the stimuli towards its axon in certain conditions .",
    "this type of propagation of stimuli can only be achieved in this unique direction . considering oriented networks not only provides more biological credibility , but it is also more general , since in a graph , one non - oriented edge is simply the combination of two oriented edges .",
    "unidirectionality seems , at first glance , to be a handicap to obtain good performance : it requires twice as much material resources ( one has to use two bits in order to specify the connections between two nodes in an oriented graph , instead of only one bit in non - oriented one ) for lower redundancy .    if one replaces non - oriented connections by oriented ones in @xmath36 , one obtains a novel structure called  chain of tournaments \" of order @xmath11 with degree @xmath33 ( c.f .",
    "[ chain_of_cliques_vs_tournaments ] on the right , @xmath46 ) , which is denoted by @xmath47 . in graph theory ,",
    "a tournament is a directed graph obtained by assigning a direction to each edge of a clique .",
    "a tournament offers half the redundancy of a clique . and a chain of tournaments is constructed by a succession of a number of tournaments .",
    "a chain of tournaments with @xmath48 is a @xmath11-clique .    in the sequel",
    ", the parameter @xmath33 will be called equivalently in several ways : `` incident degree '' as the amount of input information , and  anticipation degree \" as the temporal anticipative span in sequence learning .",
    "unidirectionality enables the network to exhibit sequential or temporal behavior , which opens a novel horizon compared to non - oriented networks .",
    "in fact , the sequential structure imposed by the forward progression of time is omnipresent in all cognitive behaviors .",
    "cognitive sequential learning is unidirectional ( irreversible ) rather than bidirectional ( reversible ) .",
    "indeed , while learning a song , the succession of the lyrics as well as the melody follows the forward progression of time , and attempting to sing a song in reverse order is extremely difficult if not impossible for human cognitive capacities . in this subsection",
    ", we will explain how to store and decode variable length messages whose length is potentially greater than the number of clusters in the network , whereas the length of messages stored in clique - based networks can not exceed the number of clusters @xmath3 .",
    "beforehand , let us define a temporal sequence following the terminology introduced by wang and arbib [ [ wang ] ] .",
    "a temporal sequence @xmath49 of length @xmath50 is defined as :    @xmath51 +    each @xmath52 is called a component of @xmath49 , which can be just a symbol or a spatial pattern composed of several symbols in parallel .",
    "the length of a sequence is the number of components in the sequence . in this section ,",
    "we are only interested in the sequence of symbols .",
    "sequences of spatial patterns will be discussed in section [ chpt_learning_seq_pat ] .    the storage principle is illustrated by fig .",
    "[ learning_sequence_of_symbols ] .",
    "each symbol @xmath53 is mapped to a particular fanal in a corresponding cluster .",
    "we define the function @xmath54 .",
    "we call cluster @xmath55 a downstream neighbor of cluster @xmath56 if @xmath57 . during the storing process",
    ", the directed connections are established successively between the current cluster and its @xmath33 downstream neighbors .",
    "for instance , the fanal corresponding to @xmath58 in cluster 1 are connected to those corresponding to @xmath59 and @xmath60 , respectively in clusters 2 , 3 and 4 ; that of @xmath61 is connected to those of @xmath62 and @xmath63 , respectively in clusters 8 , 1 and 2 , etc .",
    "we call that two fanals @xmath5 and @xmath13 are _ paired _ by the sequence @xmath49 on passage @xmath64 , denoted by the binary relation @xmath65 , if we have @xmath66 otherwise , @xmath67",
    ". function @xmath68 maps the symbol @xmath69 to the appropriate fanal in cluster @xmath56 on the @xmath70 passage of this sequence . by abuse of language",
    ", we admit that words  symbol \" and  fanal \" are interchangeable in order to simplify notations .",
    "thus , after storing a set @xmath71 of @xmath72 sequences of length @xmath50 , the network is defined formally by : + @xmath73 + @xmath74    note that the loop structure enables the reuse of the resources .",
    "a cluster may be solicited several times , if @xmath50 , the length of the sequence surpasses the number of clusters @xmath3 . in the example illustrated in fig .",
    "[ learning_sequence_of_symbols ] , cluster 1 is solicited for three times by symbols @xmath75 and @xmath76 . in this way",
    ", the network is able to learn sequences of any length , not limited by the number of clusters .",
    "the encoded sequences can then materialize for example voluminous multimedia streams .",
    "we suppose that the stored sequences are randomly , uniformly and independently generated among all the possible ones .",
    "we assume the independence of connections in order to apply the binomial law .",
    "this approximation , verified by simulations , could be intuitively explained by the fact that with some long sequences stored in the network , two connections taken randomly from the graph are unlikely to have been added at the same time .",
    "the distribution of connections can then be seen as independent bernoulli variables of parameter @xmath77 , which is the _ density _ of the network .",
    "the value of @xmath77 can be obtained by binomial arguments .",
    "in fact , the probability of having a connection between fanals @xmath5 and @xmath13 is by construction the probability of having at least one sequence containing symbol @xmath78 at position @xmath56 and symbol @xmath79 at position @xmath55 . for reasons of simplicity",
    ", we suppose that @xmath50 is a multiple of @xmath3 . the density of the network after storing @xmath72 sequences can be expressed as :    @xmath80    it is important to note that the density is independent of @xmath33 . in fact",
    ", the number of potentially established connections for each passage is @xmath81 , which is well dependent on @xmath33 , but the total number of possible connections is also @xmath33-dependant : @xmath82 .",
    "the incremental density for each passage is roughly the ratio between these two , so @xmath33 is eliminated from the expression .      to start the decoding process ,",
    "the network should be provided with any subsequence of @xmath33 consecutive symbols , in particular the first @xmath33 symbols if one would like to retrieve the sequence from the beginning , but this is not necessary .",
    "the 3 starting symbols @xmath83 and @xmath84 are illustrated in black in fig .",
    "[ learning_sequence_of_symbols ] , the symbols to be deduced being in gray .",
    "note that if the supplied subsequence is in the middle of the sequence , the decoder should be provided with supplementary information where the corresponding clusters are .",
    "we see it rather as plausibility than a constraint : for instance , if one gets stuck in the middle of a melody when playing on the piano , it seems easier to recall it by restarting from the beginning of the tune .",
    "the decoding procedure is sequential and discrete - time such that at each step , the wta decision is made locally in the following cluster , based on contributions from the @xmath33 previous clusters .",
    "let us take an example where the trigger sequence is composed of @xmath33 symbols at the beginning .",
    "formally , the decoding can be expressed as algorithm [ seq_sym_decod ] .",
    "initialization @xmath85    an important comment concerns the anticipation effect included in this decoding principle . indeed , in fig .",
    "[ learning_sequence_of_symbols ] , while the decoding is performed in cluster 4 , given the results in clusters 1 , 2 and 3 , activity is already emerging in cluster 5 , receiving signals from clusters 2 and 3 ; and cluster 6 , receiving signals from cluster 3 .",
    "this process of anticipating information gives a rough estimate of the network dynamics in the following time window .",
    "this concept resonates with [ [ leaver ] ] , according to which the very existence of anticipatory imagery suggests that retrieving stored sequences of any kind involves predictive readout of upcoming information before the actual sensorimotor event .",
    "the clique - based networks are robust towards errors , this property being inherited to some extent by tournament - based networks which are error tolerant in sequential decoding . indeed , with a sufficient degree of temporal redundancy , specified by @xmath33",
    ", a transitory error would hopefully not propagate to future decoding decisions .",
    "however , it is important to note that the benefits of iterative decoding are definitively lost , since only one iteration is applicable in this process .",
    "let us define the symbol error rate ( sber ) as the percentage of the symbols that are not correctly retrieved .",
    "one should distinguish two different types of errors : the ones coming from an excessive network density , and thus occuring even if perfect information is provided and which we call _ structural _ symbol error rate ; and those generated because there have been errors within the previous @xmath33 positions .",
    "for an algorithm not tolerant towards errors , as soon as a single error occurs , the next decoding steps will produce errors and as a consequence , practically all errors are of the latter type .",
    "one can roughly estimate the structural error rate as the error rate at a single decoding step when perfect information is provided : @xmath86    in fact , an error will occur if and only if at least one fanal among @xmath87 in the examined cluster , other than the target one , is also connected to the @xmath33 previous fanals that are provided correctly .",
    "the probability of the existence of such a connection is the network density @xmath77 .    the theoretical error rate @xmath88 has to be compared with the simulated sber , knowing that we have always sber @xmath89 .",
    "the result is shown in fig .",
    "[ error_rate_sequence_of_symbols ] for the tournament - based network with parameters @xmath90 , @xmath91 and @xmath92 ( that is with maximal temporal redundancy ) .",
    "the length of the sequences is @xmath93 .",
    "we observe that the presence of errors in the previous positions do make the task more difficult for the decoder , but the performance is far from catastrophic .",
    "for example , for a structural sber@xmath94 , the network is able to store 15000 sequences of length 100 . by simulation",
    ", this same error rate corresponds to 13000 sequences .    ,",
    "@xmath91 and @xmath92 .",
    "the length of the sequences is @xmath93 .",
    "three curves are illustrated : simulated symbol error rate , structural symbol error rate and the network density.,scaledwidth=45.0% ]      as discussed before , giving a theoretical estimate of sber is not trivial in sequential decoding , since the occurrence of one retrieval error at a given step will provide the next @xmath33 steps with erroneous information , and thus will potentially lead to an accumulated number of retrieval errors .",
    "we are thus interested in the sequence error rate ( sqer ) , which is defined as the ratio of the number of incorrectly retrieved sequences to the total number of test sequences .",
    "test sequences are among those having been stored by the network .",
    "the retrieval will be considered as a failure as soon as a single erroneous symbol appears .",
    "each decoding step is a small segmented problem : identify the target fanal by making use of the stimuli of the @xmath33 previous active fanals and the existing connections . as expressed in ( [ innate_sym_err_rate ] ) , the probability of making a right decision for a certain decoding step is @xmath95 .",
    "there are @xmath96 steps for decoding a whole squence of length @xmath50 .",
    "sqer is then estimated by the following formula : @xmath97    we have now to answer the question : for a limited number of neural ressources ( @xmath2 fanals ) , and a given learning set ( @xmath72 sequences of length @xmath50 ) , how can we optimize the number of clusters @xmath3 in order to minimize sqer ?",
    "we fix @xmath98 , since obviously it is of interest to take @xmath33 as large as possible to minimize the error rate , the density @xmath77 being independent of @xmath33 ( see section [ storage_seq_sym ] ) .",
    "some assumptions may be made in order to simplify the demonstration . by supposing @xmath99",
    ", we deduce from ( [ den_seq_sym ] ) that @xmath100    for @xmath98 , we have @xmath101    by differentiating @xmath102 with respect to @xmath3 , we obtain @xmath103    finally , setting the derivative equal to zero gives @xmath104    for example , for @xmath105 , the optimal number of clusters is @xmath106 , for a corresponding density @xmath107 .",
    "this is acceptable for a rough estimate , since with the same set of parameters , the exact formula of density ( [ den_seq_sym ] ) gives @xmath108 .",
    "note that @xmath109 is proportional to the square of the number of fanals and inversely proportional to the sequence length and the number of sequences .",
    "each sequence is composed of @xmath110 bits .",
    "if the number of stored sequences is small compared to the total number of possible ones , what we consider in the sequel , it has been shown in [ [ grirab12 ] ] that the capacity , that is , the number of bits stored in the network approaches : @xmath111    each cluster is connected via a vectorial arrow towards each of its @xmath33 downstream neighbors .",
    "a vectorial arrow is potentially composed of @xmath112 arrows , each of which can be encoded by 1 bit .",
    "thus , @xmath113 , the quantity of memory used by the network is : @xmath114 with the same hypothesis as above , this leads to the expression of network efficiency : @xmath115    for the same configuration as in the previous subsection , we have @xmath116 .",
    "this calculation is valid in the case of @xmath117 , however , a higher efficiency is usually observed for a lower degree of anticipation @xmath33 .",
    "table [ table1 ] describes theoretical values for several different configurations of the network . for a reasonably sized network with the concern for biological plausibility , which means",
    "the number of fanals per cluster is of the order of 100 , the efficiency is around @xmath118 to @xmath119 .",
    "the efficiency tends extremely slowly to the asymptotical bound of @xmath120 [ [ knoblauch ] ] as the number of fanals tends to infinity .",
    ".maximum number of sequences ( diversity ) @xmath121 that a chain of tournaments is able to store and retrieve with an error probability smaller than 0.01 , for different values of @xmath11 , @xmath122 , @xmath33 and @xmath50 .",
    "the values of corresponding efficiency @xmath123 are also mentioned . [ cols=\"<,<,<,<,<,<\",options=\"header \" , ]     [ table1 ]",
    "the structure represented on the right in fig .",
    "[ chain_of_cliques_vs_tournaments ] can be viewed as a generalization of the clique - based networks .",
    "it becomes a clique by setting @xmath124 ( two oriented connections being equivalent to a non oriented one ) .",
    "it is still possible to generalize furthermore this topology :    1 .   a chain of tournaments",
    "is not necessarily a closed loop ; 2 .",
    "a given component of the sequence at time @xmath125 , @xmath126 , is not necessarily a single symbol , but a set of parallel symbols that corresponds to a set of fanals in different clusters .",
    "the sequence then becomes vectorial .",
    "the authors of [ [ aliabadi ] ] have introduced a mechanism of storing sparse messages in networks of neural cliques in order to satisfy the `` sparse coding '' vision of mental information [ [ foldiak ] ] [ [ olshausen ] ] .",
    "a _ sparse message _ or a _ sparse pattern _ is a message that contains few expressed elements . back to our sequential models ,",
    "a sparse pattern is mapped to a set of @xmath11 fanals , with @xmath127 , distributed in a certain number of clusters , with only one fanal per cluster . in fig .",
    "[ chain_of_tournaments_open ] , four sparse patterns with different orders are represented : black circles ( order 4 ) , grey squares ( order 3 ) , grey circles ( order 2 ) and black squares ( order 5 ) . here",
    ", we are not interested in the way to store and recall static sparse patterns , the different aspects of which are assessed in [ [ aliabadi ] ] , but in the association between these patterns to build up sequential behavior .",
    "the storage and the retrieval of sequences of sparse patterns can be implemented in the single layered structure illustrated in fig .",
    "[ chain_of_tournaments_open ] .",
    "clusters are represented by squares in the grid .",
    "each pattern generates activities in a limited number of distributed clusters .",
    "the elements of a single pattern are not associated by connections . on the other hand ,",
    "two patterns that are linked are associated through an oriented complete bipartite graph .",
    "the succession of patterns is then carried by a chain of tournaments with degree @xmath33 . in fig .",
    "[ chain_of_tournaments_open ] , we have @xmath128 , since the first pattern ( filled circles ) is connected ( via solid arrows ) to the second ( grey squares ) and to the third ( grey circles , via dotted arrows ) , but not to the fourth ( filled squares ) .",
    "this connectivity structure is similar to the sparse , distributed model of rinkus [ [ rinkus1 ] ] , although the latter only considers connections between two consecutive patterns ( no anticipation ) , and the way to organize clusters is different .",
    ".,scaledwidth=45.0% ]      to retrieve the whole sequence , the network should be provided at least with a cue sequence of any @xmath33 successive patterns ( a cue sequence of less than @xmath33 patterns makes the retrieval task not guaranteed but still possible ) , not necessarily at the beginning .",
    "the actual state of the network @xmath129 is defined by the collection of the fanals that constitute @xmath33 estimated successive patterns at time @xmath125 : @xmath130 all the fanals in @xmath129 are activated : @xmath131 a step of message passing is then done for all the fanals in the network , as defined by : @xmath132    unlike the looped tournament - based networks in section [ error_tolerant_decoding ] , the emplacement of the target pattern is unknown a priori . as a consequence , one has to process a _ global _ selection of winners rule instead of a _ local _ one . obviously , the local wta automatically leaves one or several activated fanals in every cluster .",
    "several strategies are possible for a global selection .",
    "three of them are presented here with the same example : after the message passing step , let us suppose that the set of 10 fanal scores is @xmath133 .",
    "1 .   threshold selection ( ts ) : only the fanals with a score superior or equal to a predefined threshold @xmath134 are selected .",
    "for example , if @xmath135 , the estimated pattern will be \\{b , d , e , f , g , j } ; 2 .",
    "global winner - takes - all ( gwta ) : only the fanals with the maximal score are selected .",
    "the estimated pattern will be \\{d , g , j } ; 3 .",
    "global winners - take - all ( gwsta ) : the @xmath27 fanals with the maximal or near maximal scores are selected . if several fanals have the same score as the @xmath136 does , these fanals are selected as well . for example , if @xmath137 , the estimated pattern will be \\{d , e , f , g , j}. in this case , the number of selected fanals can be slightly larger than the targeted number @xmath27 .",
    "the network state is then updated by replacing the oldest pattern in @xmath138 with the most recently decoded pattern .",
    "the decoding process then repeats , starting from ( [ network_state ] ) .",
    "ts is simple and efficient when the order @xmath11 of each pattern is identical . in this case",
    ", one can simply apply a threshold @xmath139 to reach good retrieval performance .",
    "however , the performance of ts is very sensitive to the threshold chosen . in a general case ,",
    "the optimal value of @xmath134 is @xmath140 , the cardinality of @xmath138 .",
    "this makes it not applicable to sequences with variable - order patterns , if one does not know beforehand the order of previous patterns . in a similar way ,",
    "the choice of parameter @xmath27 in gwsta is dependent on the order of the target pattern , the optimal choice being @xmath141 .",
    "nevertheless , compared to ts , gwsta is much more flexible concerning variable - order patterns .",
    "setting the value of @xmath27 equal to the smallest order @xmath142 is generally good enough , if the order of patterns does not vary very much and @xmath142 is not too small . on the other hand , gwta is totally independent of the pattern order .",
    "the counterpart is that a fanal obtaining erroneously the highest score will extinguish all the rest .",
    "[ comp_decoding_algo_single_layer ] compares different decoding schemes in terms of pattern error rate ( per ) : ts , gwta and gwsta .",
    "the simulated network is composed of 100 clusters of 64 fanals .",
    "the length of the sequences to store is 100 .",
    "two cases are simulated : all patterns contain 20 fanals ; or the pattern order varies uniformly between 10 and 20 .",
    "the parameters for each decoding scheme : @xmath134 or @xmath27 are carefully chosen .",
    "when pattern order is fixed , ts and gwsta have almost equivalent performance and both outperforms gwta . with variable - order patterns , gwsta with an optimally chosen @xmath27 outperforms once again gwta , while ts has very poor performance in this case .",
    "overall , gwsta is generally the best decoding scheme in terms of per , while gwta is more flexible , thus is a good tradeoff if the orders of patterns are completely unknown .     or @xmath27 for the corresponding decoding scheme is properly chosen.,scaledwidth=45.0% ]      the decoding strategies described in the previous section could be problematic if a fanal is shared by two or more patterns within range @xmath33 .",
    "as previously explained , the organization of the sequence storage involves two types of redundancy : spatial redundancy carried by the order of the pattern @xmath11 and temporal redundancy offered by anticipation degree @xmath33 .",
    "the lack or weakness of one may be compensated by the other .",
    "one may expect that sequences of small patterns can be better retrieved by increasing @xmath33 , but in practice the suitable range of @xmath33 for a good performance is very limited .",
    "this issue is caused by the fact that @xmath33 successive patterns may share a certain number of fanals that lead to possible auto - connections . to take a simple example ,",
    "consider the cue sequence @xmath143 .",
    "the target pattern @xmath144 should be triggered due to the established connections between @xmath143 and @xmath144 .",
    "now suppose that @xmath58 and @xmath145 share exactly the same set of fanals .",
    "in this case , @xmath145 is also connected to the cue sequence @xmath143 , since by construction , @xmath145 is already connected to @xmath146 and @xmath147 . as a result .",
    "@xmath144 and @xmath145 will both be triggered .",
    "a partial sharing of fanals among the @xmath33 successive patterns will cause the similar interference issue .    to avoid this , one trick is to set a restriction on the cluster activity during the storage and consequently during the decoding as well : the clusters involved in a pattern are not allowed to participate in the activities of the @xmath33 following ones .",
    "it is then possible to estimate sqer . at each decoding step , we deduce the estimate at time @xmath148 , denoted by @xmath149 , from the @xmath33 previous estimates @xmath150 , which are supposed to be correct : @xmath151 .",
    "three groups of fanals should be distinguished for error probability estimation :    1 .",
    "all the fanals in the clusters involved in @xmath152 , the number of which is @xmath153 .",
    "the probability of these fanals to be incorrectly stimulated is zero .",
    "in fact , cluster activity restriction forbids connections within the same cluster , that makes the scores of these fanals always less than @xmath32 .",
    "2 .   the fanals involved in the following patterns @xmath154 , the number of which is @xmath155 .",
    "these fanals deterministically receive stimuli from part of currently activated patterns .",
    "for example , the scores of the fanals in @xmath156 reach at least @xmath155 , since they are connected to @xmath157 . as a consequence , @xmath11 spurious connections ,",
    "the probability of each of them being @xmath77 , are enough to make a wrong decision .",
    "the probability of these @xmath155 fanals not to be incorrectly stimulated is thus @xmath158 .",
    "all the remaining fanals except those in the target pattern @xmath159 , the number of which is @xmath160 .",
    "the probability of these fanals not to be incorrectly stimulated is @xmath161 .",
    "this estimation repeats @xmath96 times if the @xmath33 patterns at the beginning of the sequence are provided .",
    "finally , sqer is estimated as : @xmath162^{l - r}\\ ] ] with @xmath163     and pattern order @xmath11 .",
    "the network is composed of 100 clusters of 64 fanals .",
    "the length of the sequences stored is 100 .",
    "the curves correspond to sequences of patterns of different orders @xmath11 : 4 , 6 , 8 , 10 and 12 .",
    "the number of sequences @xmath72 is adjusted for each @xmath11 in the way that the lowest point of each curve represents a sqer of about 1%.,scaledwidth=50.0% ]    fig .",
    "[ relation_optimal_r_c ] makes it clear that it is the product of @xmath33 and @xmath11 that determines the system retrieval performance .",
    "for example , in the network composed of 100 clusters of 64 fanals storing sequences of length 100 , the optimal value of this product is approximately 25 , and this is hardly dependent on different learning sets .",
    "indeed , the product @xmath32 corresponds to the quantity of available input information for the next pattern decoding . for sequences composed of patterns of large order ,",
    "only a few previous instants of information are needed to decode the next pattern , whereas for patterns with small order more temporally distant information could be useful . however , an augmentation of either @xmath11 or @xmath33 will increase network density .",
    "a tradeoff should be made between the quantity of available input information and the density .",
    "up to now , the sequential decoding in tournament - based networks did not involve iterative processing .",
    "the overall sequence is encoded by connecting the group of fanals that represents a first pattern a to those that represent a second pattern b , which then connects to a third pattern c , etc .",
    "the problem of this simple chaining process is that an error in pattern a will potentially activate fanals that do not take part of pattern b , while some part of pattern b will not be stimulated .",
    "b is transformed into a corrupted version b. therefore , the retrieval of pattern c will be even more challenging and more subject to errors .",
    "this is called `` avalanche of errors '' .",
    "we have discussed a similar issue in section [ error_tolerant_decoding ] with the decoding of sequences of symbols .",
    "our solution was to use a sufficiently large value of @xmath33 in order to assure error tolerant decoding .    some studies [ [ kleinfeld ] ] , [ [ sompolinsky ] ] suggest theoretical solutions to the error avalanche problem , which are based on the error correction properties of auto - associative networks . here",
    ", auto - association refers to the strengthening of synaptic connections between cells encoding the same memory .",
    "it was proposed that the error avalanche problem could be avoided if each hetero - associative step in the chaining process is followed by an auto - associative step , which potentially enables to transform a corrupted memory into the accurate version .",
    "but in these models , synaptic connections are weighted , which is totally different from our binary model .      in a similar approach ,",
    "we propose a double layered structure as illustrated in fig .",
    "[ double_layered_structure ] .",
    "the lower layer is _ tournament - based hetero - associative _ , similar to the single layered network which stores sequential oriented associations between patterns .",
    "an upper layer of mirror fanals is superposed to emphasize the co - occurrence of the elements belonging to the same pattern , in the form of a clique . therefore",
    ", this second layer is _ clique - based auto - associative _ , similar to the sparse clique - based networks analyzed in [ [ aliabadi ] ] .",
    "the clique - based auto - associative layer is supposed able to remove the insertions which are potentially activated because of the parasite connections in the tournament - based layer .",
    "for instance in fig .",
    "[ double_layered_structure ] , the upstream pattern represented by 4 circles activates the downstream pattern represented by 4 squares , but also an insertion , the filled square .",
    "the single layered network will in this case continue on decoding the following patterns , with the risk to amplify the error . on the contrary",
    ", the filled square does not belong to the clique in the clique - based layer , therefore will turn to be silent after some iterations .",
    "the accurate information is then retransmitted back to the tournament - based hetero - associative layer to pursue the sequential decoding . here",
    ", an iterative gwsta procedure is applied for clique decoding .",
    "[ comp_dbl_vs_single_layer ] shows the gain in terms of per of the double layered structure compared to the single layered structure .    .",
    "decoding scheme in tournament - based layer : gwsta .",
    "@xmath27 is optimally chosen for both layers .",
    ", scaledwidth=45.0% ]    however , one may note that the double layered structure uses more material than the single layered one .",
    "efficiency is a more fair mesure for performance , which can be expressed as    @xmath164    for the single layered structure , @xmath165 since there are @xmath166 potential oriented connections each of which is encoded by 1 bit , whereas for the double layered one , @xmath167 , the number of non oriented connections being @xmath168 .",
    "even though , an efficiency gain is observed for the double layered structure .",
    "for instance , in the same configuration as fig .",
    "[ comp_dbl_vs_single_layer ] , with @xmath169 , we have @xmath170 compared to @xmath171 .      the cooperation mechanism between the auto - associative and hetero - associative memory for accurate recalling is biologically plausible .",
    "it has been suggested in [ [ lisman ] ] that in the hippocampus , the dentate and ca3 networks reciprocally cooperate for accurate recall of memory sequences .",
    "auto - association occurs in ca3 to accentuate the simultaneity of elements in the same memory , whereas hetero - association occurs in the dentate to progress in the sequential decoding .",
    "several anatomical and electrophysiological observations point out that dentate granule cells have axons called mossy fibers , which powerfully excite ca3 cells .",
    "and there is also a feedback information pathway from ca3 back to the dentate [ [ lisman2 ] ] .",
    "however , the realization of this cooperation by duplicating of identical nodes may appear irrelevant . albeit mirror neurons as well as mirror systems",
    "have been directly observed in primate and other species , and it is speculated that this system provides the physiological mechanism for the perception / action coupling , it is not obvious at all that such a system also exists in the memory mechanism .",
    "to further approach biological reality , one may consider this structure as a single layer of unique nodes connected by relatively long and short synapses .",
    "the memory at one instant is represented by a pattern locally embedded in the terms of a neural clique .",
    "the information is exchanged rapidly between active nodes via short auto - associative synapses until the neural pattern stabilizes .",
    "then this information is distributively spread via long hetero - associative synapses over the network to emerge the following local pattern , etc .",
    "we have developed the possibility of tournament - based networks to process sequences of symbols or sparse patterns .",
    "particularly , we have shown that they are able to store and retrieve a large number of long sequences , which could give them the ability to encode the flows with voluminous information , such as multimedia streams .",
    "as described in section [ chpt_learning_seq_pat ] , the network made of 6400 nodes ( @xmath172 , @xmath173 ) is able to store about @xmath174 vectorial sequences composed of @xmath93 patterns of @xmath169 parallel symbols .",
    "each pattern contains @xmath175 bits , which corresponds to a total information of about 13.2 mbits .",
    "the double layered structure benefiting from iterative retrieving assures more accurate retrieving and higher efficiency .",
    "the decoding algorithm is error tolerant and anticipative , which mirrors what is known of biological neural networks .",
    "our model still possesses some degrees of freedom with respect to several network parameters , in particular the threshold @xmath134 .",
    "thresholds play a primordial role in the brain functioning . for instance , epilepsy is a common and diverse set of chronic neurological disorders , which result from abnormal , excessive neuronal activity in the brain .",
    "some medications and treatments manage to increase the threshold in order to avoid this excessive activity . in our model",
    ", one may imagine implementing a non uniformly space - variant threshold laying over the whole network , which could be different from one cluster to another .",
    "the clusters with higher thresholds are thus penalized compared to those with lower ones . at the meantime ,",
    "the threshold could also vary as long as the sequence progresses or be time - variant , and thus could  guide \" the sequence towards appropriate locations .",
    "this variable threshold should be a consequence of an intelligent training .",
    "moreover , nothing by now enables to switch from one sequence to another to create some sort of  association \" or  discrimination \" between sequences .",
    "these associations , together with the choice of sequence paths mentioned above , are probably goal - oriented in the brain . here once again , a variable threshold could be useful , since it could be encoded in the way such that this switching between sequences becomes possible .",
    "a speculation could be made that the level of intelligence would be , at least partially , based on how these kind of switchings or associations are performed in the brain .",
    "finally , the principle depicted in section [ dbl_layer_strct ] could be extended to a hierarchical structure similar to that described in [ [ starzyk ] ] [ [ starzyk2 ] ] for lecture principle , which is a four - level network that isolates letters , words , sentences , and strophes .",
    "particularly , one can imagine a hierarchical network where each level passes a part of subsampled information to the next hierarchically higher level , which then aggregates received information in the form of super level cliques and/or chains of tournaments .",
    "[ hopfield ] j.  hopfield , `` neural networks and physical systems with emergent collective computational abilities , '' _ proceedings of the national academy of sciences of the united states of america _ , 79(8):2554 , 1982 .",
    "[ mccloskey ] m.  mccloskey and n. j.  cohen ,  catastrophic interference in connectionist networks : the sequential learning problem , \" _ the psychology of learning and motivation _ , vol .",
    "23 , academic press , pp .",
    "109 - 164 , new york , 1989 .",
    "[ leaver ] a. m.  leaver , j. v.  lare , b.  zielinski , a.r .",
    "halpern , and j. p.  rauschecker , ",
    "brain activation during anticipation of sound sequences , \" _ the journal of neuroscience _",
    "29(8 ) , pp . 2477 - 2485 , feb .",
    "[ orliaguet ] j. p.  orliaguet , s.  kandel , and l. j  bo ,  visual perception of motor anticipation in cursive handwriting : influence of spatial and movement information on the prediction of forthcoming letters , \" _ perception _ , vol .",
    "905 - 912 , 1997 .",
    "[ rhm86 ] d. e.  rumelhart , g.e .",
    "hinton , and j. l.  mcclelland ,  a general framework for parallel distributed processing , \" _ parallel distributed processing : explorations in the microstructure of cognition _ , vol .",
    "1 , mit press , 1986 .                [ cruz ] l.  cruz , s. v.  buldyrev , s.  peng , d. l.  roe , b.  urbanc , h. e.  stanley and d. l.  rosene ,  a statistically based density map method for identification and quantification of regional differences in microcolumnarity in the monkey brain , \" _ journal of neuroscience methods _ , 141 , pp .",
    "321 - 332 , 2005 .",
    "[ lanzo ] c. d.  lanzo , l.  marzetti , f.  zappasodi , f. d. v.  fallani , and v. pizzella ,  redundancy as a graph - based index of frequency specific meg functional connectivity , \" _ computational and mathematical methods in medicine _ , vol .",
    "2012 , aug .",
    "2012 .",
    "[ lisman ] j. e.  lisman , l. m.  talamini , and a.  raffone,recall of memory sequences by interaction of the dentate and ca3 : a revised model of the phase precession , \" _ neural networks _ ,",
    "18 , pp . 1191 - 1201 , 2005 ."
  ],
  "abstract_text": [
    "<S> an extension to a recently introduced architecture of clique - based neural networks is presented . </S>",
    "<S> this extension makes it possible to store sequences with high efficiency . to obtain this property , </S>",
    "<S> network connections are provided with orientation and with flexible redundancy carried by both spatial and temporal redundancy , a mechanism of anticipation being introduced in the model . </S>",
    "<S> in addition to the sequence storage with high efficiency , this new scheme also offers biological plausibility . in order to achieve accurate sequence retrieval , a double layered structure combining hetero - association and auto - association </S>",
    "<S> is also proposed . </S>",
    "<S> +    associative memory , sequential memory , sparse coding , information theory , redundancy , directed graph </S>"
  ]
}