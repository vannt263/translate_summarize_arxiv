{
  "article_text": [
    "finite mixture modeling is a flexible and powerful approach to modeling data that is heterogeneous and stems from multiple populations , such as data from patter recognition , computer vision , image analysis , and machine learning .",
    "the gaussian mixture model is an important mixture model family .",
    "it is well known that any continuous distribution can be approximated arbitrarily well by a finite mixture of normal densities ( lindsay , 1995 ; mclachlan and peel , 2000 ) . however , as demonstrated by chen ( 1995 ) , when the number of components is unknown , the optimal convergence rate of the estimate of a finite mixture model is slower than the optimal convergence rate when the number is known . in practice , with too many components , the mixture may overfit the data and yield poor interpretations , while with too few components , the mixture may not be flexible enough to approximate the true underlying data structure . hence , an important issue in finite mixture modeling is the selection of the number of components , which is not only of theoretical interest , but also significantly useful in practical applications .",
    "most conventional methods for determining the order of the finite mixture model are based on the likelihood function and some information theoretic criteria , such as aic and bic .",
    "leroux ( 1992 ) investigated the properties of aic and bic for selecting the number of components for finite mixture models and showed that these criteria would not underestimate the true number of components .",
    "roeder and wasserman ( 1997 ) showed the consistency of bic when a normal mixture model is used to estimate a density function  nonparametrically \" . using the locally conic parameterization method developed by dacunha - castelle and gassiat ( 1997 ) , keribin ( 2000 ) investigated the consistency of the maximum penalized likelihood estimator for an appropriate penalization sequence .",
    "another class of methods is based on the distance measured between the fitted model and the nonparametric estimate of the population distribution , such as penalized minimum - distance method ( chen and kalbfleisch , 1996 ) , the kullback - leibler distance method ( james , priebe and marchette , 2001 ) and the hellinger distance method ( woo and sriram , 2006 ) . to avoid the irregularity of the likelihood function for the finite mixture model that emerges",
    "when the number of components is unknown , ray and lindsay ( 2008 ) suggested to use a quadratic - risk based approach to select the number of components for the finite multivariate mixture .",
    "however , these methods are all based on the complete model search algorithm and the computation burden is heavy . to improve the computational efficiency ,",
    "recently , chen and khalili ( 2008 ) proposed a penalized likelihood method with the scad penalty ( fan and li , 2001 ) for mixtures of univariate location distributions .",
    "they proposed to use the scad penalty function to penalize the differences of location parameters , which is able to merge some subpopulations by shrinking such differences to zero .",
    "however , similar to most conventional order selection methods , their penalized likelihood method can be only used for one dimensional location mixture models . furthermore ,",
    "if some components in the true / optimal model have the same location ( which is the case for the experiment in subsection 4.2 of this study ) , some of them would be eliminated incorrectly by this method .",
    "on the other hand , bayesian approaches have been also used to find a suitable number of components of the finite mixture model .",
    "for instance , variational inference , as an approximation scheme of bayesian inference , can be used to determine the number of the components in a fully bayesian way ( see , e.g. , corduneanu and c.m . bishop ( 2001 ) or chapter 10.2 of bishop ( 2006 ) ) . moreover , with suitable priors on the parameters , the maximum a posteriori ( map ) estimator can be used for model selection . in particular , ormoneit and tresp ( 1998 ) and",
    "zivkovic and van der heijden ( 2004 ) put the dirichlet prior on the mixing weights , i.e. the mixing probabilities , of the components in the gaussian mixture model , and brand ( 1999 ) applied the ",
    "entropic prior \" on the same parameters to favor models with small entropy .",
    "they then used the map estimator to drive the mixing weights associated with unnecessary components toward extinction . based on an improper dirichlet prior , figueiredo and jain ( 2002 ) suggested to use minimum message length criterion to determine the number of the components , and further proposed an efficient algorithm for learning a finite mixture from multivariate data .",
    "we would like to point out the significant difference between those approaches and our proposed method in this paper . when a component is eliminated , our suggested objective function changes continuously , while those approaches encounter a sudden change in the objective function because @xmath0 is not in the support area of the prior distribution for the mixing weights , such as the dirichlet prior .",
    "therefore , it is difficult to study statistical properties of these bayesian approaches and , especially , the consistency analysis is often missing in the literature .    in this paper ,",
    "we propose a new penalized likelihood method for finite mixture models .",
    "in particular , we focus on finite gaussian mixture models . intuitively , if some of the mixing weights or mixing probabilities are shrunk to zero , the corresponding components are eliminated and a suitable number of components is retained . by doing this , we can deal with multivariate gaussian mixture models and do not need to assume common covariance matrix for different components .",
    "popular @xmath1 types of penalty functions would suggest to penalize the mixing weights directly .",
    "however , we will show that such types of penalty functions do not penalize the mixing weights severely enough and can not shrink them to zero .",
    "instead , we propose to penalize the logarithm of mixing weights .",
    "when some mixing weights are shrunk to zero , the objective function of the proposed method changes continuously , and hence we can investigate its statistical properties , especially the consistency of the proposed penalized likelihood method .",
    "the rest of the paper is organized as follows . in section 2",
    ", we propose a new penalized likelihood method for finite multivariate gaussian mixture models . in section 3 , we derive asymptotic properties of the estimated number of components . in section 4 , simulation studies and a real data analysis are presented to illustrate the performance of our proposed methods .",
    "some discussions are given in section 5 .",
    "proof will be delegated in the appendix .",
    "gaussian mixture model ( gmm ) models the density of a @xmath2-dimensional random variable @xmath3 as a weighted sum of some gaussian densities @xmath4 where @xmath5 is a gaussian density with mean vector @xmath6 and covariance matrix @xmath7 , and @xmath8 are the positive mixing weights or mixing probabilities that satisfy the constraint @xmath9 . for identifiability of the component number ,",
    "let @xmath10 be the smallest integer such that all components are different and the mixing weights are nonzero .",
    "that is , @xmath10 is the smallest integer such that @xmath11 for @xmath12 , and @xmath13 for @xmath14 .",
    "given the number of components @xmath10 , the complete set of parameters of gmm , @xmath15 , can be conveniently estimated by maximum likelihood method via the em algorithm . to avoid overfitting and underfitting ,",
    "an important issue is to determine the number of components @xmath10 .",
    "intuitively , if some of the mixing weights are shrunk to zero , the corresponding components are eliminated and a suitable number of components is retained .",
    "however , this can not be achieved by directly penalizing mixing weights @xmath8 . by considering the indicator variables @xmath16 that show",
    "if the @xmath17th observation arises from the @xmath18th component as missing data , one can find the expected complete - data log - likelihood function ( pp .",
    "48 , mclachlan and peel , 2000 ) : @xmath19 \\right\\ } \\nonumber \\\\ & = & \\sum_{i=1}^{n }",
    "\\sum_{m=1}^{m } h_{im } \\log \\pi_m + \\sum_{i=1}^{n } \\sum_{m=1}^{m } h_{im } \\log \\phi({\\mbox{\\bf x}}_i ; { \\mbox{\\boldmath$\\mu$}}_m , { \\mbox{\\boldmath$\\sigma$}}_m),\\end{aligned}\\ ] ] where @xmath20 is the complete - data log - likelihood , and @xmath21 is the posterior probability that the @xmath17th observation belongs to the @xmath18th component .",
    "note that the expected complete - data log - likelihood involves @xmath22 , whose gradient grows very fast when @xmath8 is close to zero .",
    "hence the popular @xmath1 types of penalties may not able to set insignificant @xmath8 to zero .",
    "below we give a simple illustration on how the likelihood function changes when a mixing probability approaches to zero .",
    "in particular , a data set of 1000 points is randomly generated from a bivariate gaussian distribution ( i.e. , a gmm with only one component ) .",
    "a gmm with two components , @xmath23 , is then used to fit the data .",
    "the learned two gaussian components are depicted in figure  [ gmm2c](a ) , and @xmath24 is 0.227 .",
    "furthermore , to see how the negative likelihood function changes with respective to it , let @xmath25 gradually approach zero . for each fixed @xmath25 , we optimize all other parameters , @xmath26 , by maximizing the likelihood function . figure  [ gmm2c](b ) depicts how the minimized negative log - likelihood function changes with respective to @xmath27 .",
    "it shows that the log - likelihood function changes almost linearly along with @xmath28 when @xmath29 is close to zero , albeit some small upticks .",
    "in other words , the derivative of the log - likelihood function with respective to @xmath24 is approximately proportional to @xmath30 when @xmath24 is close to zero , and it would dominate the derivative of @xmath31 . consequently @xmath1 penalties can not set insignificant @xmath29 to zero .    [ cols=\"^,^ \" , ]     by the discussion above , we know that @xmath32-type penalized likelihood methods are not omnipotent , especially when the model is not a regular statistical model .",
    "in fact , the expected complete - data log - likelihood function ( [ e2.2 ] ) suggests that we need to consider some types of penalties on @xmath22 in order to achieve the sparsity for @xmath33 . in particular",
    ", we simply choose to penalize @xmath34 , where @xmath35 is a very small positive number , say @xmath36 or @xmath37 as the discussion of theorem 3.2 . note",
    "that @xmath38 is a monotonically increasing function of @xmath39 , and it is shrunk to zero when the mixing weight @xmath39 goes to zero .",
    "therefore , we propose the following penalized log - likelihood function , @xmath40,\\ ] ] where @xmath20 is the log - likelihood function , @xmath41 is a tuning parameter , and @xmath42 is the number of free parameters for each component .",
    "for gmm with arbitrary covariance matrices , each component has @xmath43 number of free parameters .",
    "although here @xmath42 is a constant and can be removed from the equation above , it would simplify the search range of @xmath41 in the numerical study .",
    "note our penalty function is similar to that derived with the dirichlet prior from bayesian point of view , both using logarithm function of the mixing weights of the finite mixture model as the penalty function or prior distribution function .",
    "however , for the dirichlet prior , the objective penalized likelihood function penalizes @xmath44 , and unlike our proposed penalty function @xmath45 , zero is not in the support area of the penalty function @xmath44 . in the mathematical sense",
    ", these bayesian approaches can not shrink the mixing weights to zero exactly since zero is not well defined for the objective function . in other words ,",
    "the objective function is not continuous when some of mixing weights shrunk continuously to zero .",
    "as the discussion by fan and li ( 2001 ) , such discontinuity poses challenges to investigate the statistical properties of related penalized or bayesian methods .",
    "it is be one of main reasons why there is little literature on studies the consistency of the proposed methods based on the dirichlet prior .",
    "moveover , when @xmath46 , our penalty function can not be seen as in a border family of dirichlet distributions . in fact , when @xmath46 , there is no definition for the second term of our proposed penalty function .",
    "though when @xmath47 , our proposed penalty function is similar to the one used by figueirdo and jain ( 2002 ) .",
    "however , zero is still not in the support area of their proposed improper prior function , and their method has similar problems as those bayesian approaches based on the dirichlet prior do .    as discussed in fan and li ( 2001 ) , a good penalty function should yield an estimator with three properties : unbiasedness , sparsity , and continuity .",
    "it is obvious that @xmath48 would over penalize large @xmath8 and yield a biased estimator .",
    "hence , we also consider the following penalized log - likelihood function , @xmath49.\\ ] ] compared to ( [ e2.3 ] ) , the only difference is that @xmath8 is replaced by @xmath50 in the penalty function , where @xmath51 is the scad penalty function proposed by fan and li ( 2001 ) and is conveniently characterized through its derivative : @xmath52 for some @xmath53 and @xmath54 .",
    "it is easy to see that , for a relatively large @xmath8 and @xmath55 , @xmath50 is a constant , and henceforth the estimator of this @xmath8 is expected be unbiased .      here",
    "we propose a modified em algorithm to maximize ( [ e2.3 ] ) and ( [ e2.4 ] ) iteratively in two steps .    first we introduce a modified em algorithm to maximize ( [ e2.3 ] ) . by ( [ e2.2 ] ) and ( [ e2.3 ] ) ,",
    "the expected penalized log - likelihood function is @xmath56.\\ ] ]    in the e step , we calculate the posterior probability given the current estimate @xmath57",
    "@xmath59 in the m step , we update @xmath60 by maximizing the expected penalized log - likelihood function ( [ e2.5 ] ) . note that we can update @xmath61 and @xmath62 separately as they are not intervened in ( [ e2.5 ] ) . to obtain an estimate for @xmath63 , we introduce a lagrange multiplier @xmath64 to take into account for the constraint @xmath65 , and aim to solve the following set of equations , @xmath66 = 0.\\ ] ] given @xmath35 is very close to zero , by straightforward calculations , we obtain @xmath67 \\right\\}.\\ ] ] the update equations on @xmath68 are the same as those of the standard em algorithm for gmm ( mclachlan and peel , 2000 ) .",
    "specifically , we update @xmath6 and @xmath7 as follows , @xmath69    in summary the proposed modified em algorithm works as follows : it starts with a pre - specified large number of components , and whenever a mixing probability is shrunk to zero by ( [ e2.6 ] ) , the corresponding component is deleted , thus fewer components are retained for the remaining em iterations .",
    "here we abuse the notation @xmath10 for the number of components at beginning of each em iteration , and through the updating process , @xmath10 becomes smaller and smaller . for a given em iteration step ,",
    "it is possible that none , one , or more than one components are deleted .",
    "the modified em algorithm for maximizing ( [ e2.4 ] ) is similar to the one for ( [ e2.3 ] ) , and the only difference is in the m step for maximizing @xmath70 . given the current estimate @xmath71 for @xmath70 , to solve @xmath72 = 0,\\ ] ] we substitute @xmath73 by its linear approximation @xmath74 in the equation above . then by straightforward calculations , @xmath8 can be updated as follows @xmath75 where @xmath76 if an updated @xmath8 is smaller than a pre - specified small threshold value , we then set it to zero and remove the corresponding component from the mixture model .",
    "in numerical study , ( [ pi ] ) is seldom exactly equal to zero .",
    "using such threshold method to set mixing weight to zero is only to avoid the numerical unstabilities .",
    "because of the consistent properties of such penalty function derived in section 3 , we can set this pre - specified small threshold value as small as possible , though small threshold value would increase iterative steps of em algorithm and computation time . in our numerical studies , we set this threshold as @xmath77 , but the smallest mixing weight in the following two numerical examples are @xmath78 and @xmath79 , both larger than this threshold .",
    "to obtain the final estimate of the mixture model by maximizing ( [ e2.3 ] ) or ( [ e2.4 ] ) , one needs to select the tuning parameters @xmath41 and @xmath80 ( the latter is involved ( [ e2.4 ] ) ) .",
    "our simulation studies show that the numerical results are not sensitive to the selection of @xmath80 and therefore by the suggestion of fan and li ( 2001 ) we set @xmath81 . for standard lasso and scad penalized regressions , there are many methods to select @xmath41 , such as generalized cross - validation ( gcv ) and bic ( see fan and li , 2001 and wang _ et al .",
    "_ , 2007 ) . here",
    "we define a bic value @xmath82 and select @xmath83 by @xmath84 where @xmath85 is the estimate of the number of components and @xmath86 and @xmath87 are the estimates of @xmath6 and @xmath7 for maximizing @xmath88 or @xmath89 for a given @xmath41 .",
    "it is possible to extend our proposed model selection method to more generalized mixture models , however , to illustrate the basic idea of the proposed method without many mathematical difficulties , in this section , we only show the model selection consistency of the proposed method for gaussian mixture models .",
    "first , we assume that , for the true gaussian mixture model , there are @xmath90 mixture components , @xmath91 with @xmath92 , for @xmath93 , @xmath94 , for @xmath95 . then by the idea of locally conic models ( dacunha - castelle and gassiat , 1997 and 1999 ) , the density function of the gaussian mixture model can be rewritten as @xmath96 where @xmath97 @xmath98 and @xmath99 are the true values of multivariate normal components , and @xmath100 in the original gaussian mixture model can be defined as @xmath101 and @xmath102 .",
    "similar to dacunha - castelle and gassiat ( 1997 , 1999 ) , by the restrictions imposed on the @xmath103 : @xmath104 and by the permutation , such a parametrization is locally conic and identifiable .",
    "after the parametrization , the penalized likelihood functions ( [ e2.3 ] ) and ( [ e2.4 ] ) can be written as @xmath105 \\\\ \\nonumber & { \\widehat}{= } & \\ell_p(\\theta,{\\mbox{\\boldmath$\\beta$ } } ) \\\\   & = & \\sum\\limits_{i=1}^n \\log f({\\mbox{\\bf x}}_i , \\theta,{\\mbox{\\boldmath$\\beta$ } } ) - n",
    "\\lambda d_f \\sum_{m=1}^{m } \\left[\\log(\\epsilon + \\pi_m ) - \\log(\\epsilon ) \\right],\\end{aligned}\\ ] ] and @xmath106 \\\\ \\nonumber & { \\widehat}{= } & \\ell_p(\\theta,{\\mbox{\\boldmath$\\beta$ } } ) \\\\   & = & \\sum\\limits_{i=1}^n \\log f({\\mbox{\\bf x}}_i , \\theta,{\\mbox{\\boldmath$\\beta$ } } ) - n",
    "\\lambda d_f \\sum_{m=1}^{m } \\left[\\log(\\epsilon + p_\\lambda(\\pi_m ) ) - \\log(\\epsilon ) \\right],\\end{aligned}\\ ] ] respectively .",
    "we need the following conditions to derive the asymptotic properties of our proposed method .",
    "* @xmath107 where @xmath108 and @xmath109 are large enough constants . *",
    "@xmath110 , where @xmath111 are the eigenvalues of @xmath112 and @xmath113 is a positive constant .",
    "compared to the conditions in dacunha - castelle and gassiat ( 1997 , 1999 ) , the conditions p1 and p2 are slightly stronger . without lose of generality , we assume that the parameters in the mixture model are in a bounded compact space not only for mathematical conveniences , but also for avoiding the identifiability and ill - posedness problems of the finite mixture model as discussed in bishop ( 2006 ) .",
    "those conditions are also practically reasonable for our revised em algorithm as the discussion in figueirdo and jain ( 2002 ) .    first ,",
    "even if it is known there is @xmath114 mixture components in the model , the maximum likelihood or penalized maximum likelihood solution still has a total of @xmath115 equivalent solutions corresponding to the @xmath115 ways of assigning @xmath114 sets of parameters to @xmath114 components .",
    "although this is an important issue when we wish to interpret the estimate parameter values for a selected model .",
    "however , the main focus of our paper is to determine the order and find a good density estimate with the finite mixture model .",
    "the identifiability problem is irrelevant as all equivalent solutions have same estimate of the order and the density function .",
    "second , condition ( p2 ) is imposed to guarantee the non - singularity of the covariance matrices , avoiding the ill - posedness in the estimation of the finite multivariate gaussian mixture model with unknown covariance matrices using our proposed penalized maximum likelihood method .",
    "similar as the discussion in figueirdo and jain ( 2002 ) , given a sufficient large number of the initial mixture components , say m , our proposed modified em algorithm selects components in a backwards manner by merging smaller components into a large component , and thus reducing the number of components . on the other hand ,",
    "pre - specifying an extreme large value for m should be avoided as well . in our numerical studies , the initial number of mixture components m is set to be smaller than n / p so that each estimated mixture component has a positive covariance matrix , as required by condition ( p2 ) .    under the conditions ( p1 ) and",
    "( p2 ) and if @xmath116 , @xmath117 and @xmath118 , there exists a local maximizer @xmath119 of @xmath120 , which was given in ( 3.2 ) , such that @xmath121 , and for such local maximizer , the number of the mixture components @xmath122 with probability tending to one .    under the conditions ( p1 ) and ( p2 ) and",
    "if @xmath123 and @xmath124 where @xmath125 is a constant , there exists a local maximizer @xmath119 of @xmath120 , which was given in ( 3.1 ) , such that @xmath121 , and for such local maximizer , the number of the mixture components @xmath122 with probability tending to one .",
    "* remarks : * under conditions ( p1 ) and ( p2 ) and with an appropriate tuning parameter @xmath41 , the consistency of our proposed two penalized methods have been shown by the two theorems above .",
    "although maximizing ( 3.2 ) using our proposed em algorithm is a bit complicated than that for ( 3.1 ) , it has theoretical advantages . unlike ( 3.2 ) , a mixture component with a relative large mixing weight @xmath126 is still penalized in ( 3.1 ) by the penalty function @xmath127 , and this would produce a bias model estimation and affect the consistency of the model selection as the discussion by fan and li ( 2001 ) .",
    "moreover , in practice it is easier to select an appropriate tuning parameter for ( 3.2 ) than for ( 3.1 ) to guarantee the consistency of the final model selection and estimation . in particular",
    ", the following theorem shows that the proposed bic criterion always selects the reasonable tuning parameter with probability tending one by which maximizing ( 3.2 ) selects the consistent component number of the finite gaussian mixture model .",
    "let @xmath128 denote the number of component of gaussian mixture models selected by ( 3.2 ) with the tuning parameter @xmath41 , and @xmath129 is the lambda selected by the proposed bic criterion in section 2.3 .",
    "then we have the following theorem .    under the conditions ( p1 ) and ( p2 ) , @xmath130 .",
    "the proofs of the theorems are given in the appendix .",
    "in the first example , we generate 600 samples from a three - component bivariate normal mixture with mixing weights @xmath131 , mean vectors @xmath132^t$ ] , @xmath133^t$ ] , @xmath134^t$ ] , and covariance matrices @xmath135 $ ] , @xmath136 $ ] , @xmath137 .",
    "in fact , these three components are obtained by rotating and shifting a common gaussian density @xmath138 , and together they exhibit a triangle shape .",
    "we run both of our proposed penalized likelihood methods ( [ e2.3 ] ) and ( [ e2.4 ] ) for 300 times . the maximum initial number of components is set to be 10 or 50 , the initial value for the modified em algorithms is estimated by k - means clustering , and the tuning parameter @xmath41 is selected by our proposed bic method .",
    "figure  [ fig2s1 ] shows the evolution of the modified em algorithm , with the maximum number of components as 10 .",
    "we compare our proposed methods with traditional aic and bic methods .",
    "figure  [ fig3s1](a - c ) shows the histograms of the estimated component numbers .",
    "one can see that our proposed methods perform much better in identifying the correct number of components than aic and bic methods do .",
    "in fact , both proposed methods estimate the number of components @xmath139 correctly regardless of the maximum initial number of components .",
    "figure  [ fig3s1](d ) depicts the evolution of the penalized log - likelihood function ( [ e2.3 ] ) for the simulated data set in figure  [ fig2s1](a ) in one run , and shows how our proposed modified em algorithm converges numerically .",
    "c    c    in addition , when the number of components is correctly identified , we summarize the estimation of the unknown parameters of gaussian distributions and mixing weights in table 1 and table 2 with different maximum initial number of components . for the covariance matrix ,",
    "we use eigenvalues because the three components have the same shape as @xmath140 .",
    "table 1 and table 2 show that the modified em algorithms give accurate estimate for parameters and mixing weights .",
    "the final estimate of these parameters is robust to the initialization of the maximum number of components .     &",
    "true & 0.3333 & -1 & 1 & 2 & 0.2 + & ( [ e2.3 ] ) & 0.3342(.0201 ) & -0.9911(.0861 ) & 1.0169(.1375 ) & 2.0034(.3022 ) & 0.1981(.0264 ) + & ( [ e2.4 ] ) & 0.3356(.0187 ) & -1.0022 ( .0875 ) & 1.0007(.1428 ) & 2.0205(.2769 ) & 0.1973 ( .0265 ) + & true & 0.3333 & 1 & 1 & 2 & 0.2 + & ( [ e2.3 ] ) & 0.3317(.0196 ) & 1.0151(.0845 ) & 0.9849(.1318 ) & 1.9794(.2837 ) & 0.1977(.0303 ) + & ( [ e2.4 ] ) & 0.3321 ( .0193 ) & 1.0108(.0790 ) & 0.9904(.1253 ) & 1.9825(.2980 ) & 0.1957 ( .0292 ) + & true & 0.3333 & 0 & -1.4142 & 2 & 0.2 + & ( [ e2.3 ] ) & 0.3341(.0171 ) & 0.0019(.1324 ) & -1.4112(.0405 ) & 1.9722(.2425 ) & 0.1973(.0258 ) + & ( [ e2.4 ] ) & 0.3322(.0159 ) & 0.0014(.1449)&-1.4103(.0404)&1.9505(.2424 ) & 0.1978(.0267 ) +     & true & 0.3333 & -1 & 1 & 2 & 0.2 + & ( [ e2.3 ] ) & 0.3342(.0201 ) & -1.0080(.0881 ) & 0.9854(.1372 ) & 1.9603(.2857 ) & 0.1974(.0296 ) + & ( [ e2.4 ] ) & 0.3320(.0190 ) & -1.0017 ( .0859 ) & 0.9985(.1389 ) & 1.9604(.2830 ) & 0.1960 ( .0286 ) + & true & 0.3333 & 1 & 1 & 2 & 0.2 + & ( [ e2.3 ] ) & .3347(.0170 ) & 0.9879(.0885 ) & 1.0166(.1385 ) & 1.9531(.2701 ) & 0.1981(.0283 ) + & ( [ e2.4 ] ) & 0.3345 ( .0182 ) & 0.9987(.0896 ) & 1.0044(.1402 ) & 1.9661(.2460 ) & 0.1971 ( .0248 ) + & true & 0.3333 & 0 & -1.4142 & 2 & 0.2 + & ( [ e2.3 ] ) & 0.3329(.0198 ) & 0.0210(.1329 ) & -1.4105(.0344 ) & 1.9717(.2505 ) & 0.1975(.0265 ) + & ( [ e2.4 ] ) & 0.3334(.0164 ) & 0.0117(.1302)&-1.4116(.0372)&1.9736(.2769 ) & 0.1998(.0281 ) +      in the second example , we consider a situation where the mixture components overlap and may have same means but different covariance matrices .",
    "this is a rather challenging example , and the proposed method by chen and khalili ( 2008 ) can not be applied as some components have the same mean . specifically , we generate 1000 samples with mixing weights @xmath141 , @xmath142 , mean vectors @xmath143^t$ ] , @xmath144^t$ ] , @xmath145^t$ ] , and @xmath146 ,   \\qquad{\\mbox{\\boldmath$\\sigma$}}_2&=\\left[\\begin{array}{cc } 2 & 2 \\\\ 2 &   7\\end{array}\\right ] , \\\\ { \\mbox{\\boldmath$\\sigma$}}_3&=\\left[\\begin{array}{cc } 0.5 & 0 \\\\ 0 & 4\\end{array}\\right ] , \\qquad{\\mbox{\\boldmath$\\sigma$}}_4&=\\left[\\begin{array}{cc } 0.125 & 0 \\\\ 0 & 0.125 \\end{array}\\right].\\end{aligned}\\ ] ]    similar to the first example , we run our proposed methods for 300 times .",
    "the maximum number of components is set to be 10 or 50 , the initial value for the modified em algorithms is estimated by k - means clustering , and the tuning parameter @xmath41 is selected by our proposed bic method .",
    "figure  [ fig2s2 ] shows the evolution of the modified em algorithm for ( 2.3 ) with the maximum initial number of components as 10 for one simulated data set .",
    "figure  [ fig3s2 ] shows that our proposed method can identify the number of components @xmath139 correctly , and performs much better than aic and bic methods .",
    "table 3 and table 4 show that the modified em algorithms give accurate estimates for parameters and mixing weights .",
    "similar as the first example , the final estimate of these parameters is robust to the initialization of the maximum number of components .",
    "c    c    [ t3 ]     & true & 0.3 & -2 & -2 & 0.1 & 0.2 + & ( [ e2.3 ] ) & 0.3022(.0093 ) & -2.0010(.0216 ) & -1.9989(.0291 ) & 0.0979(.0114 ) & 0.2010 ( .0242 ) + & ( [ e2.4 ] ) & 0.3009(.0095 ) & -1.9995(.0206 ) & -1.9975(.0319 ) & 0.0990(.0119 ) & 0.2003(.0226 ) + & true & 0.3 & -2 & -2 & 1.2984 & 7.7016 + & ( [ e2.3 ] ) & 0.2995(.0112 ) & -1.9989(.1133 ) & -1.9963(.1837 ) & 1.2864(.1407 ) & 7.7219(.7301 ) + & ( [ e2.4 ] ) & 0.3017(.0118 ) & -1.9995(.1202 ) & -2.0049(.1811)&1.2926(.1343)&7.5856(.7301 ) + & true & 0.3 & 2 & 0 & 0.5 & 4 + & ( [ e2.3 ] ) & 0.3019(.0083 ) & 1.9943(.0483 ) & 0.0001(.1294 ) & 0.4986(.0529 ) & 3.9951(.3496 ) + & ( [ e2.4 ] ) & 0.3012(.0087 ) & 1.9995(.0511 ) & -0.0001(.1244 ) & 0.4963(.0544 ) & 3.9998(.3911 ) + & true & 0.1 & 1 & -4 & 0.125 & 0.125 + & ( [ e2.3 ] ) & 0.0964(.0038 ) & 1.0005(.0373 ) & -3.9966(.0394 ) & 0.1143(.0245 ) & 0.1339(.0252 ) + & ( [ e2.4 ] ) & 0.0962(.0047 ) & 0.9993(.0394 ) & -4.0013(.0417 ) & 0.1167(.0259 ) & 0.1317(.0278 ) +    [ t4 ]     & true & 0.3 & -2 & -2 & 0.1 & 0.2 + & ( [ e2.3 ] ) & 0.3016(.0107 ) & -1.9982(.0223 ) & -1.9998(.0312 ) & 0.0986(.0110 ) & 0.2034 ( .0241 ) + & ( [ e2.4 ] ) & 0.3009(.0095 ) & -1.9986(.0218 ) & -1.9978(.0320 ) & 0.0993(.0110 ) & 0.2010(.0238 ) + & true & 0.3 & -2 & -2 & 1.2984 & 7.7016 + & ( [ e2.3 ] ) & 0.3002(.0128 ) & -2.0040(.1086 ) & -2.0052(.1819 ) & 1.2823(.1386 ) & 7.6696(.7538 ) + & ( [ e2.4 ] ) & 0.3017(.0115 ) & -1.9988(.1173 ) & -2.0116(.1811)&1.2757(.1318)&7.6734(.7476 ) + & true & 0.3 & 2 & 0 & 0.5 & 4 + & ( [ e2.3 ] ) & 0.3015(.0083 ) & 1.9986(.0500 ) & 0.0054(.1365 ) & 0.4998(.0531 ) & 3.9951(.3651 ) + & ( [ e2.4 ] ) & 0.3012(.0084 ) & 2.0015(.0505 ) & 0.0102(.1268 ) & 0.4915(.0524 ) & 3.9751(.3770 ) + & true & 0.1 & 1 & -4 & 0.125 & 0.125 + & ( [ e2.3 ] ) & 0.0966(.0044 ) & 0.9983(.0408 ) & -4.0019(.0431 ) & 0.1150(.0251 ) & 0.1327(.0258 ) + & ( [ e2.4 ] ) & 0.0962(.0050 ) & 1.0011(.0402 ) & -4.0019(.0425 ) & 0.1154(.0256 ) & 0.1313(.0254 ) +      we apply our proposed methods to an image segmentation data set at uci machine learning repository ( http://archive.ics.uci.edu/ml/datasets/image+segmentation ) .",
    "this data set was created from a database of seven outdoor images ( brickface , sky , foliage , cement , window , path and grass ) .",
    "each image was hand - segmented into instances of @xmath147 regions , and 230 instances were randomly drawn .",
    "for each instance , there are 19 attributes .",
    "we here only focus on four images , brickface , sky , foliage and grass , and two attributes , extra red and extra green .",
    "our objective is to estimate the joint probability density function of the two attributes ( see figure  [ figr1](a ) ) using a gaussian mixture with arbitrary covariance matrices . in other words ,",
    "we implement our proposed method to identify the number of components , and to simultaneously estimate the unknown parameters of bivariate normal distributions and mixing weights .",
    "although we consider only four images , figure  [ figr1](a ) suggests that a five - component gaussian mixture is more appropriate and the brickface image is better represented by two components .",
    "c ( a ) ( b ) ( c )    as in the simulation studies , we run our proposed method for 300 times . for each run",
    ", we randomly draw 200 instances for each images .",
    "the maximum number of components is set to be ten , and the initial value for the modified em algorithm is estimated by the k - means clustering . because there is little difference between numerical results of the two proposed methods ( [ e2.3 ] ) and ( [ e2.4 ] ) , here we only show the numerical results obtained by maximizing ( [ e2.3 ] ) .",
    "figure [ figr2 ] shows the evolution of the modified em algorithm for one run .",
    "figure [ figr3 ] shows that our proposed method selects five components with high probability . for a five - component gaussian mixture model , we summarize the estimation of parameters and mixing weights in table 5 .",
    "c    c     & & & & + 1 & sky & foliage & .4153(0.0555 ) & -27.7689(2.3336 ) & -13.7343(1.1377 ) & 12.0464(1.1726 ) & 7.3739(0.3745 ) + 2 & grass & .2617(0.0523 ) & -9.3724(0.8588 ) & 13.4992(4.0740 ) & 3.6393(1.2160 ) & 3.5632(1.3567 ) + 3 & foliage & brickface & .1447(0.0425 ) & -4.9511(1.3913 ) & -3.3625(3.5537 ) & 3.1584(0.7102 ) & 1.6728(0.4004 ) + 4 & brickface & .0824(0.0487 ) & -1.3474(0.8417 ) & -12.0906(7.2158 ) & 2.5780(1.5227 ) & 1.3302(0.7854 ) + 5 & brickface & .0936(0.0717 ) & 3.5476(1.4703 ) & -8.4996(2.1980 ) & 1.5110(1.2288 ) & 1.3293(1.6460 ) +",
    "in this paper , we propose a penalized likelihood approach for multivariate finite gaussian mixture models which integrates model selection and parameter estimation in a unified way .",
    "the proposed method involves light computational load and is very attractive when there are many possible candidate models . under mild conditions",
    ", we have , both theoretically and numerically , shown that our proposed method can select the number of components consistently for gaussian mixture models .",
    "though we mainly focus on gaussian mixture models , we believe our method can be extended to more generalized mixture models .",
    "this requires more rigorous mathematical derivations and further theoretical justifications , and is beyond the scope of this paper .    in practice ,",
    "our proposed modified em algorithm gradually discards insignificant components , and does not generate new components or split any large components . if necessary , for very complex problems , one can perform the split - and - merge operations ( ueda _ et al._. 1999 ) after certain em iterations to improve the final results .",
    "we only show the convergence of our proposed modified em algorithm through simulations , and further theoretical investigation is needed .",
    "moreover , classical acceleration methods , such as louis method , quasi - newton method and hybrid method ( mclachlan and peel , 2000 ) , may be used to improve the convergence rate of our proposed modified em algorithm .",
    "another practical issue is the selection of the tuning parameter @xmath41 for the penalized likelihood function .",
    "we propose a bic selection method , and simulation results show it works well .",
    "moreover , our simulation results show the final estimate is quite robust to the initial number of components .    in this paper ,",
    "we propose two penalized log - likelihood functions ( [ e2.3 ] ) and ( [ e2.4 ] ) .",
    "although the numerical results obtained by these two penalized functions are very similar , we believe they have different theoretical properties . we have shown the consistency of model selection and tuning parameter selection by maximizing ( [ e2.4 ] ) and the bic criterion proposed in the paper under mild conditions",
    "we have also shown the consistency of model selection by maximizing ( [ e2.3 ] ) , but we note that the conditions are somewhat restrictive . in particular , the consistency of bic criterion to select the tuning parameter for ( [ e2.3 ] ) needs further investigations .    an ongoing work is to investigate how to extend our proposed penalized likelihood method to the mixture of factor analyzers ( ghahramani and hinton , 1997 ) , and to integrate clustering and dimensionality reduction in a unified way .",
    "we now outline the key ideas of the proof for theorem 3.1 .    first assume the true gaussian mixture density is @xmath148 then define @xmath149 as the subset of functions of form @xmath150 where @xmath151 is the derivative of @xmath152 for the @xmath17th component of @xmath153 , @xmath154 is the derivative of @xmath152 for the @xmath155 component of @xmath112 . for functions in @xmath149 , @xmath156 and @xmath157",
    "satisfy the conditions p1 and p2 .",
    "the important consequence for @xmath149 is the following proposition . +",
    "* proposition a.1 .",
    "* under the conditions p1 and p2 , @xmath149 is a donsker class .",
    "* proof : * first , by the conditions p1 and p2 , it is easy to check that the conditions p1 and p2 in keribin ( 2000 ) or p0 and p1 in dacunha - castelle and gassiat ( 1999 ) are satisfied by @xmath149",
    ". then @xmath158 are within envelope functions @xmath159 , and square integrable under @xmath160 . on the other hand , by the restrictions imposed on the @xmath103 , @xmath161 and @xmath162",
    "are bounded .",
    "therefore , similar to the proof of theorem 4.1 in keribin ( 2000 ) or the proof of proposition 3.1 in dacunha - castelle and gassiat ( 1999 ) , it is straightforward to show that @xmath149 has the donsker property with the bracketing number @xmath163 where @xmath164 . @xmath165 + * proof of theorem 3.1 : * to prove the theorem , we first show that there exists a maximizer @xmath166 such that @xmath121 . in fact , it is sufficient to show that , for a large constant @xmath125 , @xmath167 where @xmath168",
    ". let @xmath169 , and notice that @xmath170 \\\\ & \\ & + n\\lambda d_f\\sum\\limits_{l=1}^q [ \\log ( \\epsilon+p_\\lambda(\\pi^0_l))-\\log(\\epsilon)],\\end{aligned}\\ ] ] and then @xmath171 \\\\ & { \\widehat}{= } & i_1+i_2.\\end{aligned}\\ ] ] for @xmath172 , because @xmath169 and by the restriction condition on @xmath173 , we have @xmath174 when @xmath175 . due to the property of the penalty function , we then have @xmath176| \\\\ & = & |-n\\lambda d_f\\sum\\limits_{m = m - q+1}^m [ \\log ( \\epsilon+a\\lambda)- \\log ( \\epsilon+a\\lambda)]| \\\\ & = & 0.\\end{aligned}\\ ] ]      noticing @xmath168 , @xmath183 and @xmath184 , and by the conditions p1 , p2 and proposition a.1 for the class @xmath149 , we have @xmath185 since @xmath186 converges uniformly in distribution to a gaussian process by proposition a.1 , and @xmath187 is of order @xmath188 by the law of large numbers , we have @xmath189 when @xmath125 is large enough , the second term of @xmath177 dominates other terms in the penalized likelihood ratio .",
    "then we have @xmath190 with probability tending to one .",
    "hence there exists a maximizer @xmath166 with probability tending to one such that @xmath191    next we show that @xmath192 or that @xmath193 , when the maximizer @xmath166 satisfies @xmath194 .",
    "in fact , when @xmath194 , we have @xmath195 by the restriction condition on @xmath196 .",
    "a lagrange multiplier @xmath64 is taken into account for the constraint @xmath197 .",
    "then it is then sufficient to show that @xmath198 with probability tending to one for the maximizer @xmath166 where @xmath199 , @xmath200 , and @xmath201 to show the equation above , we consider the partial derivatives for @xmath202 firstly .",
    "they should satisfy the following equation , @xmath203 it is obvious that the first term in the equation above is of order @xmath188 by the law of large numbers . if @xmath175 and @xmath194 , it is easy to know that @xmath204 , and hence the second term should be @xmath205 .",
    "so we have @xmath206 .",
    "next , consider @xmath207 where @xmath200 and @xmath208 .",
    "as shown for ( [ a3 ] ) , it is obvious that the first term and the third term @xmath64 in the equation above are of order @xmath188 .",
    "for the second term , because @xmath209 , @xmath116 and @xmath35 is sufficient small , we have @xmath210 with probability tending to one .",
    "hence the second term in the equation ( [ a3 ] ) above dominates the first term and the third term in the equation .",
    "therefore we proved the equation ( [ a1 ] ) , or equivalently @xmath211 with probability tending to one when @xmath212 .",
    "@xmath165 + * proof of theorem 3.2 : * to prove theorem 3.2 , similar as the proof of theorem 3.1 , we first show that there exists a maximizer @xmath166 such that @xmath121 when @xmath213 , and it is sufficient to show that , for a large constant @xmath108 , @xmath167 where @xmath214 .",
    "let @xmath215 , and as the similar step of theorem 3.1 , we have @xmath216 \\\\ & { \\widehat}{= } & i_1+i_2.\\end{aligned}\\ ] ] for @xmath172 , because of @xmath215 and by the restriction condition on @xmath173 , we have @xmath217 when @xmath175 . by the property of the penalty function",
    ", we then have @xmath218| \\\\ & = & \\left|-n\\lambda d_f\\sum\\limits_{m = m - q+1}^m \\left[\\frac{(\\pi_m-\\pi^0_{m - m+q})}{\\epsilon+\\pi^0_{m - m+q}}\\cdot(1+o(1 ) ) \\right]\\right| \\\\ & = & o(\\sqrt{n})\\cdot \\frac{q c_1}{\\sqrt{n}}(1+o(1))=o(c_1).\\end{aligned}\\ ] ]    for @xmath177 , as in the proof of theorem 3.1 , we have @xmath219 when @xmath108 is large enough , the second term of @xmath177 dominates @xmath172 and other terms in @xmath177 .",
    "hence we have @xmath190 with probability tending to one .",
    "hence there exists a maximizer @xmath166 with probability tending to one such that @xmath191      first , we show that for any maximizer @xmath223 with @xmath224 if there is @xmath225 such that @xmath226 , then there should exist another maximizer of @xmath227 in the area of @xmath228 .",
    "it means that the extreme maximizer of @xmath227 in the compact area @xmath229 should satisfy that @xmath230 for any @xmath231 .",
    "hence it is also equivalent to show for any such kind maximizer @xmath223 with @xmath232 , we always have @xmath233 with probability tending to one .",
    "similar as the analysis before , we have @xmath234-n\\lambda d_f\\log\\frac{\\epsilon+\\pi^\\ast_k}{\\epsilon } \\\\ & { \\widehat}{= } & i_1+i_2+i_3.\\end{aligned}\\ ] ] as shown before , we have @xmath235 . for @xmath236 , because @xmath124 we have @xmath237 then notice that @xmath236 is always negative and dominate @xmath177 and @xmath172 , and hence we have @xmath238 .      a lagrange multiplier @xmath64 is taken into account for the constraint @xmath197 .",
    "it is then sufficient to show that @xmath242 with probability tending to one for the maximizer @xmath166 where @xmath243 to show the equation above , we consider the partial derivatives for @xmath202 firstly .",
    "they should satisfy the following equation , @xmath244 it is obvious that the first term in the equation above is of order @xmath188 by the law of large numbers . if @xmath175 and @xmath194 , it is easy to know that @xmath204 , and hence the second term should be @xmath205 .",
    "so we have @xmath206 .",
    "next , consider @xmath245 where @xmath200 and @xmath246 .",
    "as shown for ( [ a5 ] ) , it is obvious that the first term and the third term @xmath64 in the equation above are of order @xmath188 . for the second term , because @xmath247 , @xmath213 and @xmath248 , we have @xmath249 with probability tending to one .",
    "hence the second term in the equation ( [ a6 ] ) above dominates the first term and the third term in the equation .",
    "therefore we proved the equation ( [ a4 ] ) , or equivalently @xmath211 with probability tending to one when @xmath212 .",
    "@xmath165    * proof of theorem 3.3 : * by proposition a.1 , and as in the example of gaussian case shown in keribin ( 2000 ) , we know that the conditions ( p1)-(p3 ) and ( i d ) are satisfied by multivariate gaussian mixture model . hence to prove this theorem , we can use the theoretical results obtained by keribin ( 2000 ) and follow the proof step of theorem 2 in wang _",
    "et al._. ( 2007 ) .",
    "first , given @xmath250 , by theorem 3.1 , we known that @xmath192 with probability tending to 1 , and @xmath251 is the consistent estimate of @xmath252 . hence with probability tending to 1 , we have @xmath253 where @xmath254 is the parameter estimators of the multivariate gaussian mixture model . on the other hand , when @xmath90 is known , we know its maximum likelihood estimate @xmath255 is consistent .",
    "hence we have @xmath256   \\\\ & = & \\ell({\\widehat}{{\\mbox{\\boldmath$\\theta$}}}_{mle})-n\\lambda^\\ast d_f \\cdot q \\cdot \\log \\frac{\\epsilon+a\\lambda^\\ast}{\\epsilon } \\ge \\ell_p({\\widehat}{{\\mbox{\\boldmath$\\theta$}}}_{\\lambda^\\ast}),\\end{aligned}\\ ] ] where @xmath257 is the maximum likelihood estimate of @xmath258 .",
    "then by the convex property of @xmath20 and the definition of @xmath259 , when @xmath250 we have the oracle property of the penalized estimate of @xmath260 which should be equal to @xmath261 with probability tending to one .",
    "* case 1 : * underfitted model , i.e. , @xmath262 . according to the definition of the bic criterion , we have @xmath263 where @xmath264 is the maximum likelihood estimate of the finite gaussian mixture model when the number of the components is @xmath265 .",
    "similar as keribin ( 2000 ) , we know that @xmath266 where @xmath267 is the finite gaussian mixture model space with @xmath268 mixture components",
    ". then we have @xmath269 this implies that @xmath270    * case 2 : * overfitted model , i.e. , @xmath271 . as keribin ( 2000 ) , for @xmath272 , by dacunha - castelle ( 1999 ) we know that @xmath273 converges in distribution to the following variables @xmath274 where @xmath275 and @xmath276 are subsets of a unit sphere @xmath277 of functions ( for detail definition of @xmath275 , @xmath275 and @xmath277 , see keribin , 2000 ) .",
    "hence @xmath278 and we have @xmath279 and @xmath280 combined ( [ t31 ] ) with ( [ t32 ] ) , theorem 3.3 has been proved .",
    "@xmath165                corduneanu , a. and bishop , c. m. ( 2001 ) .",
    "variational bayesian model selection for mixture distributions . in _",
    "proceedings eighth international conference on artificial intelligence and statistics _ , 2734 , morgan kaufmann .",
    "dacunha - castelle , d. and gassiat , e. ( 1999 ) .",
    "testing the order of a model using locally conic parametrization : population mixtures and stationary arma processes , _ the annals of statistics _",
    ", * 27 * , 1178 - 1209 .",
    "lindsay , b.g .",
    "_ mixture models : theory , geometry and applications_. nsf - cbms regional conference series in probability and statistics , volume 5 , institute for mathematical statistics : hayward , ca .",
    "ormoneit , d. and tresp , v. ( 1998 ) .",
    "averaging , maximum penalized likelihood and bayesian estimation for improving gaussian mixture probability density estimates .",
    "_ ieee transactions on neural networks _ , * 9*(4 ) : 10459227 ."
  ],
  "abstract_text": [
    "<S> this paper is concerned with an important issue in finite mixture modelling , the selection of the number of mixing components . </S>",
    "<S> we propose a new penalized likelihood method for model selection of finite multivariate gaussian mixture models . </S>",
    "<S> the proposed method is shown to be statistically consistent in determining of the number of components . </S>",
    "<S> a modified em algorithm is developed to simultaneously select the number of components and to estimate the mixing weights , i.e. the mixing probabilities , and unknown parameters of gaussian distributions . </S>",
    "<S> simulations and a real data analysis are presented to illustrate the performance of the proposed method .     </S>",
    "<S> tao huang is assistant professor , department of statistics , university of virginia , charlottesville , va 22904 . </S>",
    "<S> heng peng is assistant professor , department of mathematics , hong kong baptist university , kowloon tong , hong kong . </S>",
    "<S> kun zhang is research scientist , department of schlkopf , max planck institute for biological cybernetics , spemannstrasse 38 , 72076 , tbingen .    </S>",
    "<S> gaussian mixture models , model selection , penalized likelihood , em algorithm . </S>"
  ]
}