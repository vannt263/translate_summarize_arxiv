{
  "article_text": [
    "the rnyi divergence , introduced in @xcite , has been studied so far in various information - theoretic contexts ( and it has been actually used before it had a name @xcite ) . these include generalized cutoff rates and error exponents for hypothesis testing ( @xcite ) , guessing moments ( @xcite ) , source and channel coding error exponents ( @xcite ) , strong converse theorems for classes of networks @xcite , strong data processing theorems for discrete memoryless channels @xcite , bounds for joint source - channel coding @xcite , and one - shot bounds for information - theoretic problems @xcite .    in @xcite , gilardoni derived a pinsker - type lower bound on the rnyi divergence @xmath4 for @xmath5 . in view of the fact that this lower bound is not tight , especially when the total variation distance @xmath6 is large , this paper starts by considering the minimization of the rnyi divergence @xmath4 , for an arbitrary @xmath7 , subject to a given ( or minimal ) value of the total variation distance . note that the minimization here is taken over all probability measures with a total variation distance which is not below a given value ; this problem differs from the type of problems studied in @xcite and @xcite , in connection to the minimization of the relative entropy @xmath8 subject to a minimal value of the total variation distance with a fixed probability measure @xmath9 .",
    "the solution of this problem generalizes the problem of minimizing the relative entropy @xmath10 subject to a given value of the total variation distance where the latter is a special case with @xmath11 ( see @xcite ) .",
    "one possible way to deal with this problem stems from the fact that the rnyi divergence is a one - to - one transformation of the hellinger divergence @xmath12 where for @xmath13 : @xmath14 and @xmath12 is an @xmath15-divergence ; since the total variation distance is also an @xmath15-divergence , this problem can be viewed as a minimization of an @xmath15-divergence subject to a constraint on another @xmath15-divergence .",
    "the numerical optimization of an @xmath15-divergence subject to simultaneous constraints on @xmath16-divergences @xmath17 was recently studied in @xcite , where it has been shown that it suffices to restrict attention to alphabets of cardinality @xmath18 .",
    "in fact , as shown in ( * ? ? ?",
    "* ( 22 ) ) , a binary alphabet suffices if there is a single constraint ( i.e. , @xmath19 ) which is on the total variation distance . in view of",
    ", the same conclusion also holds when minimizing the rnyi divergence subject to a constraint on the total variation distance . to set notation ,",
    "the divergences @xmath20 are defined at the end of this section , being consistent with the notation in @xcite and @xcite .",
    "this paper treats this minimization problem of the rnyi divergence in a different way .",
    "we first generalize the analysis in @xcite , which was used for the minimization of the relative entropy subject to a constraint on the variational distance , for proving that it suffices to restrict attention to probability measures which are defined on a binary alphabet .",
    "furthermore , the continuation of the analysis in this paper relies on the lagrange duality , and a solution of the karush - kuhn - tucker ( kkt ) equations while asserting strong duality for the studied problem .",
    "the use of lagrange duality further simplifies the computational task of the studied minimization problem .    as complementary results to the minimization problem studied in this paper , the reader is referred to (",
    "* ; * ? ? ?",
    "* section  8) which provides upper bounds on the rnyi divergence @xmath4 for an arbitrary @xmath21 as a function of either the total variation distance or relative entropy in case that the relative information is bounded .",
    "the solution of the minimization problem of the rnyi divergence , subject to a constraint on the total variation distance , provides an elegant way for the characterization of the exact locus of the points @xmath0 where @xmath2 and @xmath3 are probability measures whose total variation distance is not below a given value @xmath22 , and @xmath9 is an arbitrary probability measure .",
    "it is further shown in this paper that all the points of this convex region can be attained by a triple of probability measures @xmath23 which are defined on a binary alphabet .    in view of the characterization of the exact locus of these points ,",
    "a geometric interpretation is provided in this paper for the minimal chernoff information between @xmath2 and @xmath3 , denoted by @xmath24 , subject to an @xmath22-separation constraint on the variational distance between @xmath2 and @xmath3 .",
    "it is demonstrated in the following that the intersection point at the boundary of the locus of @xmath0 and the straight line @xmath25 is the point whose coordinates are equal to the minimal value of @xmath24 under the constraint @xmath26 .",
    "the reader is referred to @xcite , which relies on the closed - form expression in ( * ? ? ?",
    "* proposition  2 ) for the minimization of the constrained chernoff information , and which analyzes the problem of channel - code detection by a third - party receiver via the likelihood ratio test . in the latter problem",
    ", a third - party receiver has to detect the channel code used by the transmitter by observing a large number of noise - affected codewords ; this setup has applications in security or cognitive radios , or in link adaptation in some wireless technologies .",
    "since the rnyi divergence @xmath27 forms a generalization of the relative entropy @xmath8 , where the latter corresponds to @xmath28 , the approach suggested in this paper for the characterization of the exact locus of pairs of relative entropies in view of a solution to a minimization problem of the rnyi divergence is analogous to the usefulness of complex analysis in solving real - valued problems .",
    "we consider the analysis of the considered problem as mathematically pleasing in its own right .",
    "note , however , that an operational meaning of a special point at the boundary of this locus has an operational meaning in view of @xcite ( see the previous paragraph ) .",
    "the studied problem considered here differs from the study in @xcite which considered the joint range of @xmath15-divergences for pairs ( rather than triplets ) of probability measures .    the performance analysis of linear codes under maximum - likelihood ( ml ) decoding is of interest for studying the potential performance of these codes under optimal decoding , and for the evaluation of the degradation in performance that is incurred by the use of sub - optimal and practical decoding algorithms .",
    "the reader is referred to @xcite which is focused on this topic .",
    "the second part of this paper derives an exponential upper bound on the performance of ml decoded binary linear block codes ( or code ensembles ) .",
    "its derivation relies on the gallager bounding technique ( see ( * ? ? ?",
    "* chapter  4 ) , @xcite ) , and it reproduces the shulman - feder bound @xcite as a special case .",
    "the new exponential bound derived in this paper is expressed in terms of the rnyi divergence from the normalized distance spectrum of the code ( or average distance spectrum of the ensemble ) to the binomial distribution which characterizes the average distance spectrum of the capacity - achieving ensemble of fully random block codes .",
    "this exponential bound provides a quantitative measure of the degradation in performance of binary linear block codes ( or code ensembles ) as a function of the deviation of their ( average ) distance spectra from the binomial distribution , and its use is exemplified for an ensemble of turbo - block codes .",
    "this paper is structured as follows : section  [ section : minimum of the renyi divergence subject to a fixed tv distance ] solves the minimization problem for the rnyi divergence under a constraint on the total variation distance , section  [ section : range of relative entropies subject to a minimal tv distance ] uses the solution of this minimization problem to obtain an exact characterization of the joint range of the relative entropies in the considered setting above",
    ". section  [ section : an upper bound on the ml decoding error probability with the renyi divergence ] provides a new exponential upper bound on the block error probability of ml decoded binary linear block codes , which is expressed in terms of the rnyi divergence , suggests an efficient way to apply the bound to the performance evaluation of binary linear block codes ( or code ensembles ) , and exemplifies its use . throughout this paper , logarithms are to the base  @xmath29 .",
    "we end this section by introducing the definitions and notation used in this work , which are consistent with @xcite , @xcite , and are included here for the convenience of the reader .",
    "we assume throughout that the probability measures @xmath30 and @xmath9 are defined on a common measurable space @xmath31 , and @xmath32 denotes that @xmath30 is _ absolutely continuous _ with respect to @xmath9 , namely there is no event @xmath33 such that @xmath34 .",
    "let @xmath35 denote the radon - nikodym derivative ( or density ) of @xmath30 with respect to @xmath9 .",
    "the relative entropy is given by @xmath36    the total variation distance is given by @xmath37 is given by @xmath38 the analytic extension of @xmath39 at @xmath28 yields @xmath40 ( nats ) .",
    "the _ rnyi divergence of order @xmath41 _ is given as follows :    * if @xmath42 , then @xmath43 * if @xmath44 , then @xmath45 * @xmath46 which is the analytic extension of @xmath47 at @xmath28 .",
    "* if @xmath48 then @xmath49 with @xmath50 .",
    "the chernoff information between probability measures @xmath2 and @xmath3 is expressed as follows in terms of the rnyi divergence : @xmath51 } \\bigl\\{(1-\\alpha ) d_{\\alpha}(p_1\\|p_2)\\bigr\\}\\end{aligned}\\ ] ] and it is the best achievable exponent in the bayesian probability of error for binary hypothesis testing ( see , e.g. , ( * ? ? ?",
    "* theorem  11.9.1 ) ) .",
    "in this section , we derive a tight lower bound on the rnyi divergence @xmath52 subject to an equality constraint on the total variation distance @xmath53 where @xmath54 is fixed ; alternatively , it can regarded as a minimization problem under the inequality constraint @xmath26 .",
    "it is first shown that this lower bound is attained for probability measures defined on a binary alphabet , and lagrange duality is used to further simplify the computational task of this bound . the special case where",
    "@xmath28 , which is specialized to the minimization of the relative entropy subject to a fixed total variation distance , has been studied extensively , and three equivalent forms of the solution to this optimization problem were derived in @xcite , @xcite , @xcite .    in (",
    "* corollaries  6 and 9 ) , gilardoni derived two pinsker - type lower bounds on the rnyi divergence of order @xmath5 , expressed in terms of the total variation distance . among these two bounds ,",
    "the improved lower bound is given ( in nats ) by @xmath55 where @xmath56 denotes the total variation distance between @xmath30 and @xmath9 .",
    "note that in the limit where @xmath22 tend to  2 ( from below ) , this lower bound converges to a finite value which is at most @xmath57 ; it is , however , an artifact of the lower bound in view of the next lemma .",
    "@xmath58    [ lemma : limit when the tv distance tends to 1 ]    see appendix  [ appendix : proof of lemma 1 ] .    in the following ,",
    "we derive a tight lower bound which is shown to be achievable by a restriction of the probability measures to a binary alphabet . for @xmath59 ,",
    "let @xmath60    in the following , we evaluate the function @xmath61 .",
    "in view of ( * ? ? ? * section  2 ) which characterizes the minimum of the relative entropy in terms of the total variation distance , we first extend the argument in @xcite to prove the next lemma .",
    "for an arbitrary @xmath59 , the minimization in is attained by probability measures which are defined on a binary alphabet .",
    "[ lemma : restriction of the minimization to 2-element probability distributions ]    see appendix  [ appendix : proof of lemma 2 ] .",
    "the following proposition enables to calculate @xmath61 for an arbitrary positive @xmath62 .",
    "let @xmath63 and @xmath54 .",
    "the function @xmath61 in satisfies @xmath64 \\colon |p - q| \\geq \\frac{\\varepsilon}{2 } } d_\\alpha ( p \\| q ) \\label{eq : optimization problem 1 for the renyi divergence}\\ ] ] where @xmath65 denotes the binary rnyi divergence .",
    "[ proposition : optimization problem 1 for the renyi divergence ]    this directly follows from lemma  [ lemma : restriction of the minimization to 2-element probability distributions ] .",
    "@xmath66    and @xmath67 $ , } \\\\[0.1 cm ] -\\log \\left(1-\\tfrac12 \\ , \\varepsilon \\right ) , & \\quad \\mbox{if $ \\varepsilon \\in ( 1 , 2).$ } \\end{array } \\right .",
    "\\label{eq : g for alpha=2}\\end{aligned}\\ ] ] furthermore , for @xmath5 and @xmath54 , @xmath68 and @xmath69 where @xmath70 [ proposition : skew - symmetry property of g ]    see appendix  [ appendix : proof of proposition on skew - symmetry of g ] .",
    "the lower bound on @xmath71 in provides another proof of lemma  [ lemma : limit when the tv distance tends to 1 ] since it first yields that @xmath72 for @xmath5 ; this lemma also holds for @xmath73 since @xmath74 is monotonically increasing in its order @xmath62 .    in the following , we use lagrange duality to obtain an alternative expression as a solution of the minimization problem for @xmath61 .",
    "recall that proposition  [ proposition : optimization problem 1 for the renyi divergence ] applies to every @xmath59 .",
    "the following enables to simplify considerably the computational task in calculating @xmath61 , for @xmath5 .",
    "let @xmath5 and @xmath75 .",
    "the function @xmath76 is strictly monotonically increasing , positive , continuous , and @xmath77 [ lemma : monotonicity of f ]    see appendix  [ appendix : proof of lemma on monotonicity ] .    for @xmath5 and @xmath75 ,",
    "the equation @xmath78 has a unique solution @xmath79 .",
    "it follows from lemma  [ lemma : monotonicity of f ] , and the mean value theorem for continuous functions .    since @xmath80 is strictly monotonically increasing ( see lemma  [ lemma : monotonicity of f ] )",
    ", the numerical calculation of the unique solution of equation   is easy . [",
    "remark : numerical solution of the equation ]    an alternative simplified form for the optimization problem in proposition  [ proposition : optimization problem 1 for the renyi divergence ] is next provided for orders @xmath5 . hence , proposition  [ proposition : optimization problem 1 for the renyi divergence ] applies to every @xmath7 , whereas the following is restricted to @xmath5 .",
    "this , however , proves to be very useful in the next section in terms of obtaining a significant reduction in the computational complexity of @xmath81 where only @xmath5 is of interest there .",
    "let @xmath5 , @xmath82 , and let @xmath83 .",
    "a solution of the minimization problem for @xmath84 in proposition  [ proposition : optimization problem 1 for the renyi divergence ] is obtained by calculating the binary rnyi divergence @xmath85 in while taking the unique solution @xmath86 of , and setting @xmath87 . [ proposition : efficient calculation of g_alpha ]    see appendix  [ appendix : proof of proposition on efficient calculation of g ] .    in view of proposition  [",
    "proposition : efficient calculation of g_alpha ] , the plots in figures  [ figure : renyi_divergence_alpha_025_050_075_100 ] and  [ figure : renyi_divergence_alpha09 ] provide numerical results .",
    "in this section , we address the following question :    what is the locus of the points @xmath0 if @xmath1 are arbitrary probability measures which are mutually absolutely continuous , and @xmath26 for a given value @xmath82 ?",
    "( none of the three probability measures is fixed ) .",
    "[ question ]    the present section provides an exact characterization of this locus in view of the solution to the minimization problem in section  [ section : minimum of the renyi divergence subject to a fixed tv distance ] , and the following lemma :    let @xmath1 be pairwise mutually absolutely continuous probability measures defined on a measurable space @xmath31 .",
    "then , for @xmath63 , @xmath89 where the probability measure @xmath90 is given by @xmath91 [ lemma : identity for the renyi divergence ]    see appendix  [ appendix : proof of the lemma with the identity for the renyi divergence ] .    as a corollary of lemma  [ lemma : identity for the renyi divergence ] ,",
    "the following tight inequality holds , which is attributed to van erven ( * ? ? ?",
    "* lemma  6.6 ) and shayevitz ( * ? ? ?",
    "* section  iv.b.8 ) ) .",
    "it will be useful for the continuation of this section , jointly with the results of section  [ section : minimum of the renyi divergence subject to a fixed tv distance ] .",
    "let @xmath92 be mutually absolutely continuous discrete probability measures defined on a common set @xmath93 . if @xmath5 then @xmath94 with equality if and only if , for every @xmath95 , @xmath96 for @xmath97 , inequality   is reversed with the same necessary and sufficient condition for an equality .",
    "[ corollary : shayevitz ]    the knowledge of the maximizing probability measure in is required for the characterization of the exact locus which is studied in this section .",
    "the exact locus of the points @xmath98 is determined as follows : let @xmath99 for a fixed @xmath82 , and let @xmath5 be chosen arbitrarily . by the tight lower bound in section  [ section : minimum of the renyi divergence",
    "subject to a fixed tv distance ] , we have @xmath100 where @xmath61 is expressed in . for @xmath5 and for a fixed value of @xmath82 , let @xmath101 and @xmath102 in @xmath103 be set to achieve the global minimum in ( note that , without loss of generality , one can assume that @xmath104 since if @xmath105 achieves the minimum in then also @xmath106 achieves the same minimum ) . consequently , the lower bound in is attained by probability measures @xmath107 which are defined on a binary alphabet ( see lemma  [ lemma : restriction of the minimization to 2-element probability distributions ] ) with @xmath108 from corollary  [ corollary : shayevitz ] and , , it follows that for every @xmath5 @xmath109 where equality in holds if @xmath2 and @xmath3 are the probability measures in which are defined on a binary alphabet , and @xmath9 is the respective probability measure in which is therefore also defined on a binary alphabet .",
    "hence , there exists a triple of probability measures @xmath1 which are defined on a binary alphabet and satisfy with equality , and these probability measures are easy to calculate for every @xmath5 and @xmath82 .    similarly to , since @xmath110 , it follows from that @xmath111 by multiplying both sides of by @xmath112 and relying on the skew - symmetry property in , it follows that is equivalent to @xmath113 which is when @xmath5 is replaced by @xmath114 .",
    "hence , since holds for every @xmath5 , there is no additional information in .",
    "the exact locus of @xmath115 in the setting of question  1 is the convex region whose boundary is the convex envelope of all the straight lines @xmath116",
    "( i.e. , the boundary is the pointwise maximum of the set of straight lines in for @xmath5 ) .",
    "furthermore , all the points in this convex region , including its boundary , are attained by probability measures @xmath1 which are defined on a binary alphabet . [",
    "theorem : joint range of the relative entropies ]    let @xmath1 be arbitrary probability measures which are mutually absolutely continuous and satisfy the @xmath22 separation condition for @xmath2 and @xmath3 in total variation . in view of corollary  [ corollary : shayevitz ] and since by definition @xmath117 , it follows that the point @xmath115 satisfies @xmath118 for every @xmath5 ; this implies that every such a point is either on or above the convex envelope of the parameterized straight lines in .",
    "we next prove that a point which is below the convex envelope of the lines in can not be achieved under the constraint @xmath26 .",
    "the reason for this claim is because for such a point @xmath115 , there is some @xmath5 for which @xmath119 since under the @xmath22 separation condition for @xmath2 and @xmath3 in total variation distance , @xmath117 , then for such @xmath5 , inequality   is violated ; in view of corollary  [ corollary : shayevitz ] , this yields that the point is not achievable under the constraint @xmath99 . as an interim conclusion",
    ", it follows that the exact locus of the achievable points is the set of all points in the plane @xmath115 which are on or above the convex envelope of the parameterized straight lines in for @xmath5 .",
    "the next step aims to show that an arbitrary point which is located at the boundary of this region can be obtained by a triplet of probability measures @xmath120 which are defined on a binary alphabet , and satisfy @xmath121 . to that end , note that every point which is on the boundary of this region is a tangent point to one of the straight lines in for some @xmath5 .",
    "accordingly , the proper probability measures @xmath122 , @xmath123 and @xmath124 can be determined as follows for a given @xmath82 :    a.   find the slope @xmath125 of the tangent line at the selected point on the boundary ; in view of , @xmath126 yields @xmath127 .",
    "b.   in view of proposition  [ proposition : efficient calculation of g_alpha ] , determine @xmath128 such that @xmath129 and @xmath130 .",
    "consequently , let @xmath122 and @xmath123 be the probability measures which are defined on the binary alphabet with @xmath131 and @xmath132 .",
    "c.   the respective probability measure @xmath133 is calculated from , and it is therefore also defined on the binary alphabet .    finally , we show that every interior point in the achievable region can be attained as well by a proper selection of @xmath122 , @xmath123 and @xmath134 which are defined on a binary alphabet .",
    "to that end , note that every such interior point is located at the boundary of the locus of @xmath115 under the constraint @xmath135 with some @xmath136 ; this follows from the fact that @xmath71 is a strictly monotonically increasing and continuous function in @xmath137 , which tends to infinity as we let @xmath22 tend to  2 ( see lemma  [ lemma : limit when the tv distance tends to 1 ] ) .",
    "it therefore follows that the suitable triplet of probability measures @xmath120 can be obtained by the same algorithm used for points on the boundary of this region , except for replacing @xmath22 by the larger value  @xmath138 .",
    "this concludes the proof by first characterizing the exact locus of points , and then demonstrating that every point in this convex region ( including its boundary ) is attained by probability measures which are defined on the binary alphabet ; the proof is also constructive in the sense of providing an algorithm to calculate such probability measures @xmath139 for an arbitrary point in this closed and convex region .    as it is shown in figure  [ figure : achievable_regions_epsilon_100_140_180_198 ] , the boundaries of these regions become less curvy as @xmath140 .",
    "consider the point in figure  [ figure : achievable_regions_epsilon_100_140_180_198 ] which , in the plane of @xmath0 , is the intersection of the straight line @xmath25 and the boundary of the convex region which is characterized in theorem  [ theorem : joint range of the relative entropies ] for an arbitrary @xmath82 .    in view of the proof of theorem  [",
    "theorem : joint range of the relative entropies ] , this intersection point satisfies @xmath141 for some @xmath5 , for @xmath107 which are probability measures defined on a binary alphabet with @xmath142 , and @xmath90 is given in .",
    "the equal coordinates of this intersection point are therefore equal to the chernoff information @xmath24 ( see ( * ? ? ?",
    "* section  11.9 ) ) . due to the symmetry of this region with respect to the straight line @xmath25 ( this follows from the symmetry property @xmath110 ) ,",
    "the slope of the tangent line to the boundary of the convex region at this intersection point is @xmath143 ( see figure  [ figure : achievable_regions_epsilon_100_140_180_198 ] ) .",
    "this yields that @xmath144 , and from proposition  [ proposition : skew - symmetry property of g ] , @xmath145 .",
    "hence , from with @xmath146 , the equal coordinates of this intersection point are given by @xmath147 based on ( * ? ? ?",
    "* proposition  2 ) , this value is equal to the minimum of the chernoff information subject to an @xmath22 separation constraints for @xmath2 and @xmath3 in total variation distance .",
    "we next calculate the probability measures @xmath122 , @xmath123 and @xmath134 which attain this intersection point .",
    "eq .   with @xmath148 yields @xmath149 such that",
    "@xmath150 $ ] and @xmath151 .",
    "a possible solution of this equation is @xmath152 and @xmath153 , so the respective probability measures @xmath154 which are defined on the binary alphabet satisfy @xmath155 and @xmath156 ; consequently , from , @xmath157 is the equiprobable distribution on the binary alphabet .    as a byproduct of the characterization of the convex region in theorem  [ theorem : joint range of the relative entropies ] , it follows that the straight line @xmath158 ( in the plane of figure  [ figure : achievable_regions_epsilon_100_140_180_198 ] ) intersects the boundary of the convex region which is specified in theorem  [ theorem : joint range of the relative entropies ] at the point whose coordinates are equal to the minimized chernoff information subject to the constraint @xmath99 .",
    "the equal coordinates of each of the 4 intersection points in figure  [ figure : achievable_regions_epsilon_100_140_180_198 ] , which refer to @xmath159 , are equal to @xmath160 nats , respectively .",
    "this section derives an exponential upper bound on the performance of binary linear block codes , expressed in terms of the rnyi divergence . similarly to @xcite , @xcite , @xcite , @xcite , @xcite , ( * ? ? ?",
    "* section  3.b ) , @xcite , @xcite and @xcite , the upper bound in the next theorem quantifies the degradation in the performance of block codes under ml decoding in terms of the deviation of their distance spectra from the binomially distributed ( average ) distance spectrum of the capacity - achieving ensemble of random block codes .",
    "consider a binary linear block code of length @xmath161 and rate @xmath162 where @xmath163 designates the number of codewords .",
    "let @xmath164 and , for @xmath165 , let @xmath166 be the number of non - zero codewords of hamming weight @xmath167 .",
    "assume that the transmission of the code takes place over a memoryless , binary - input and output - symmetric channel .",
    "then , the block error probability under ml decoding satisfies @xmath168 \\right ) \\label{eq : optimized error exponent}\\ ] ] where @xmath169 for @xmath170 ( with the convention that @xmath171 for @xmath172 ) , @xmath173 is the binomial distribution with parameter @xmath174 and @xmath161 independent trials ( i.e. , @xmath175 for @xmath176 ) , @xmath177 is the pmf defined by @xmath178 for @xmath179 , @xmath180 is the rnyi divergence of order @xmath181 ( i.e. , @xmath182 where @xmath183 here ) , and @xmath184 designates the gallager random coding error exponent in ( * ? ? ? * eq .",
    "( 5.6.14 ) ) .",
    "[ theorem : channel coding theorem with the renyi divergence ]    before proving theorem  [ theorem : channel coding theorem with the renyi divergence ] , we relate this exponential bound to previously reported bounds .",
    "[ remark : generalization of the shulman - feder bound ] note that the loosening of the bound by taking @xmath172 and , respectively , @xmath171 gives the upper bound @xmath185 \\right ) \\\\[0.1 cm ] & \\stackrel{(\\text{a})}{= } \\exp \\left ( -n \\ , e_{\\text{r}}\\left(r + \\frac{d_{\\infty}(p_n \\| q_n)}{n}\\right ) \\right ) \\nonumber \\\\[0.1 cm ] & \\stackrel{(\\text{b})}{= } \\exp \\left ( -n \\ , e_{\\text{r}}\\left(r + \\frac{1}{n } \\ , \\log \\max_{0 \\leq l \\leq n } \\frac{p_n(l)}{q_n(l ) } \\right ) \\right ) \\\\[0.1 cm ] & \\stackrel{(\\text{c})}{= } \\exp \\left ( -n \\ , e_{\\text{r}}\\left(r + \\frac{1}{n } \\ , \\log \\max_{0 \\leq l \\leq n } \\frac{s_l}{e^{-n(\\log 2-r ) } { n\\choose{l } } } \\right ) \\right)\\end{aligned}\\ ] ] which coincides with the shulman - feder bound @xcite . equality  ( a ) follows from the definition of the gallager random coding exponent @xmath186 in ( * ? ? ?",
    "* eq .  ( 5.6.16 ) ) where the symmetric input distribution @xmath187 is the optimal input distribution for any memoryless , binary - input output - symmetric channel , equality  ( b ) follows from the expression of the rnyi divergence of order infinity ( see , e.g. , ( * ? ? ?",
    "* theorem  6 ) ) , and equality  ( c ) follows from the definition of the pmfs @xmath177 and @xmath173 in theorem  [ theorem : channel coding theorem with the renyi divergence ] .",
    "the proof of theorem  [ theorem : channel coding theorem with the renyi divergence ] is based on the framework of the gallager bounds in ( * ? ? ?",
    "* chapter  4 ) and @xcite .",
    "specifically , it has an overlap with ( * ? ? ?",
    "* appendix  a ) .",
    "unlike the analysis in ( * ? ? ?",
    "* appendix  a ) , working with the rnyi divergence of order @xmath188 , instead of the relative entropy as a lower bound ( see ( * ? ? ?",
    "* eq .  ( a19 ) ) ) reveals a need for an optimization of the error exponent , which leads to the error exponent in theorem  [ theorem : channel coding theorem with the renyi divergence ] .",
    "namely , if the value of @xmath170 is increased then the value of @xmath189 is decreased , and therefore @xmath190 is also decreased ( unless it is zero , see ( * ? ? ?",
    "* theorem  3 ) ; note that @xmath177 and @xmath173 do not depend on the parameters @xmath191 and @xmath181 , so they stay un - affected by varying the values of these parameters ) .",
    "the maximization of the error exponent in theorem  [ theorem : channel coding theorem with the renyi divergence ] aims at finding a proper balance between the two summands @xmath192 and @xmath193 on the right - hand side of , while also performing an optimization over the second dependent variable @xmath194 $ ] .",
    "we proceed now with the proof of theorem  [ theorem : channel coding theorem with the renyi divergence ] .",
    "the proof of theorem  [ theorem : channel coding theorem with the renyi divergence ] is based on the framework of the gallager bounds in ( * ? ? ?",
    "* chapter  4 ) and @xcite .",
    "specifically , it relies on ( * ? ? ?",
    "* appendix  a ) .",
    "we explain in the following how our proof differs from the analysis in ( * ? ? ?",
    "* appendix  a ) . from (",
    "* eq .  ( a17 ) ) , we have that for every @xmath195 $ ] @xmath196    from this point , we deviate from the analysis in ( * ? ? ?",
    "* appendix  a ) .",
    "since @xmath197 where @xmath198 , we have @xmath199 & = \\exp \\left(\\frac{r \\rho'}{s } \\cdot \\log \\left ( \\sum_{l=0}^n\\ , p_n(l)^s \\ , q_n(l)^{1-s } \\right ) \\right ) \\nonumber \\\\[0.1 cm ] & = \\exp \\left(\\frac{\\rho'}{s-1 } \\cdot \\log \\left ( \\sum_{l=0}^n\\ , p_n(l)^s \\ , q_n(l)^{1-s } \\right ) \\right ) \\nonumber \\\\ & = \\exp\\bigl(\\rho ' d_s(p_n \\| q_n)\\bigr ) \\label{eq : term with renyi divergence}\\end{aligned}\\ ] ] where @xmath190 is the rnyi divergence of order @xmath181 from @xmath177 to @xmath173 .",
    "this enables to refer to the rnyi divergence of order @xmath188 , instead of lower bounding this quantity by the relative entropy , and consequently loosening the bound ( see ( * ? ? ?",
    "* eq .  ( a19 ) ) ) .",
    "note that since the rnyi divergence is monotonically increasing in its order ( see , e.g. , ( * ? ? ?",
    "* theorem  3 ) ) and the rnyi divergence of order  1 is particularized to the relative entropy , the inequality @xmath200 holds .",
    "the combination of and gives @xmath201 & = \\exp \\left(-n \\left [ e_0 \\bigl(\\rho ' , \\underline{q } = \\bigl(\\tfrac12 , \\tfrac12 \\bigr ) \\bigr ) - \\rho ' \\biggl ( r r + \\frac{d_s(p_n \\| q_n)}{n } \\biggr ) \\right ] \\right ) , \\quad 0 \\leq \\rho ' \\leq \\frac{1}{r}. \\label{eq : new upper bound on the decoding error probability}\\end{aligned}\\ ] ] a maximization of the error exponent in with respect to the parameters @xmath170 and @xmath202 $ ] ( recall that @xmath203 ) gives the upper bound in .",
    "an efficient use of theorem  [ theorem : channel coding theorem with the renyi divergence ] for the performance evaluation of binary linear block codes ( or coee ensembles ) is suggested in the following by borrowing a concept of bounding from @xcite , which has been further studied , e.g. , in @xcite , @xcite , @xcite , and combining it with the new bound in theorem  [ theorem : channel coding theorem with the renyi divergence ] . in order to utilize the shulman - feder bound for binary linear block codes in a clever way",
    ", it has been suggested in @xcite to partition the binary linear block code @xmath204 into two subcodes @xmath205 and @xmath206 where @xmath207 and @xmath208 is the all - zero codeword .",
    "the first subcode @xmath205 contains the all - zero codeword and all the codewords of @xmath204 whose hamming weights @xmath167 belong to a subset @xmath209 , while @xmath206 contains the other codewords of @xmath204 which have hamming weights of @xmath210 , together with the all - zero codeword . from the symmetry of the channel , @xmath211 where @xmath212 and @xmath213 designate the conditional ml decoding error probabilities of @xmath205 and @xmath206 , respectively , given that the all - zero codeword is transmitted .",
    "note that although the code @xmath204 is linear , its two subcodes @xmath205 and @xmath206 are in general non - linear .",
    "one can rely on different upper bounds on the conditional error probabilities @xmath212 and @xmath213 , i.e. , we may bound @xmath212 by invoking theorem  [ theorem : channel coding theorem with the renyi divergence ] , due to its tightening of the shulman - feder bound ( see remark  [ remark : generalization of the shulman - feder bound ] ) , and also rely on an alternative approach for obtaining an upper bound on @xmath213 ( e.g. , it is possible to rely on the union bound with respect to the fixed composition codes of the subcode @xmath206 ) . the idea behind this partitioning is to include in the subcode @xmath205 the codewords of all the hamming weights whose distance spectrum is close enough to the binomial distribution @xmath173 ( see theorem  [ theorem : channel coding theorem with the renyi divergence ] ) in the sense that the additional term @xmath193 in the exponent of has a marginal effect on the conditional ml decoding error probability of the subcode @xmath205 .",
    "theorem  [ theorem : channel coding theorem with the renyi divergence ] can be applied as well to _",
    "ensembles _ of binary linear block codes .",
    "the verify this claim , let @xmath204 be an ensemble of binary linear block codes .",
    "the proof of theorem  [ theorem : channel coding theorem with the renyi divergence ] follows from the duman and salehi bounding technique @xcite which leads to the derivation of ( * ? ? ?",
    "( a.11 ) ) . by taking the expectation on the rhs of (",
    "* eq .  ( a.11 ) ) with respect to the code ensemble @xmath204 and invoking jensen s inequality , the same bound holds while @xmath166 , as it is defined in theorem  [ theorem : channel coding theorem with the renyi divergence ] , is replaced by the expectation @xmath214 $ ] with respect to the code ensemble @xmath204 .",
    "this enables to replace @xmath177 on the rhs of with @xmath215 where @xmath216}{m-1 } , \\quad \\forall \\ , l \\in \\{0 , \\ldots , n\\},\\ ] ] which therefore justifies the generalization of theorem  [ theorem : channel coding theorem with the renyi divergence ] to code ensembles of binary linear block codes .",
    "as it is exemplified in section  [ subsection : turbo ] , theorem  [ theorem : channel coding theorem with the renyi divergence ] can be efficiently applied to ensembles of turbo - like codes in the same way that it was demonstrated to be efficient in @xcite . similarly to theorem  [ theorem : channel coding theorem with the renyi divergence ] , the bound in ( * ? ? ?",
    "* theorem  3.1 ) forms another refinement of the shulman - feder bound , and the novelty in the former bound is the obtained tightening of the shulman - feder bound via the use of the rnyi divergence .",
    "we conclude this section by an example which applies this bounding technique to the ensemble of uniformly interleaved turbo codes whose two component codes are chosen uniformly at random from the ensemble of ( 1072 , 1000 ) binary systematic linear block codes .",
    "the transmission of these codes takes place over an additive white gaussian noise ( awgn ) channel , and the codes are bpsk modulated and coherently detected .",
    "the calculation of the average distance spectrum of this ensemble has been performed in ( * ? ? ?",
    "* section  5.d ) , which is required for the calculation of the upper bound in where the pd @xmath177 is replaced by its expected value over the ensemble ( i.e. , the normalization of the average distance spectrum by the number of codewords , as it is defined in theorem  [ theorem : channel coding theorem with the renyi divergence ] ) . in the following , two upper bounds on the block error probability are compared under ml decoding : the first one is the tangential - sphere bound ( tsb ) of herzberg and poltyrev ( see @xcite , @xcite , ( * ? ? ?",
    "* section  3.2.1 ) ) , and the second bound follows from the suggested combination of the union bound and theorem  [ theorem : channel coding theorem with the renyi divergence ] . note that an optimal partitioning has been performed , in a way which is conceptually similar to ( * ? ? ?",
    "* algorithm  1 ) , for obtaining the tightest bound which is obtained by combining the union bound and theorem  [ theorem : channel coding theorem with the renyi divergence ] .",
    "a comparison of the two bounds shows an advantage of the latter combined bound over the tsb in a similar way to ( * ? ? ?",
    "* upper plot of fig .",
    "8) ( e.g. , providing a gain of about 0.2  db over the tsb for a block error probability of @xmath217 ) .",
    "note that the shulman - feder bound is rather loose in this case due to the significant deviation of the ensemble distance spectrum from the binomial distribution at low and high hamming weights .",
    "furthermore , we note that the advantage of the proposed bound over the tsb in this example is consistent with the analysis in @xcite and @xcite , demonstrating a gap between the random coding error exponent of gallager and the corresponding error exponents that follow from the tsb and some of its improved versions .",
    "recall that the random coding error exponent of gallager achieves the channel capacity , whereas the random coding error exponent that follows from the tsb ( or some of its improved variants ) does not achieve the capacity of a binary - input awgn channel for bpsk modulated fully random block codes , where the gap to capacity is especially pronounced for high coding rates . in this example",
    ", the rate of the ensemble is 0.8741  bits per channel use .",
    "[ appendix : proofs of lemma 1 and 2 ]      for @xmath146 , @xmath218 where @xmath219 denotes the bhattacharyya coefficient between the two pds @xmath220 .",
    "we have @xmath221 where @xmath222 ( see ,",
    "e.g. , ( * ? ? ?",
    "* proposition  1 ) ; inequality is known in quantum information theory with respect to the relation between the trace distance and fidelity ( * ? ? ? * section  9.3 ) ) .",
    "hence , implies that holds for @xmath223 .",
    "since @xmath74 is monotonically increasing in its order @xmath62 ( see ( * ? ? ?",
    "* theorem  3 ) ) , it follows that also holds for @xmath224 . finally , due to the skew - symmetry property of @xmath225 ( see ( * ? ? ?",
    "* proposition  2 ) ) where @xmath226 for @xmath5 , and since the total variation distance is a symmetric measure and @xmath227 for @xmath5 , the satisfiability of for @xmath228 yields that it also holds for @xmath229 .",
    "let @xmath230 be probability measures which are defined on a common measurable space @xmath31 .",
    "denote by @xmath231 the mapping given by @xmath232 and let @xmath233 , for @xmath234 , be given by @xmath235 consequently , we have @xmath236 & = \\bigl ( q_1(1 ) - q_2(1 ) \\bigr ) + \\bigl ( q_2(2 ) - q_1(2 ) \\bigr ) \\nonumber \\\\ & = \\sum_{j \\in \\{1 , 2\\ } } \\bigl|q_1(j)-q_2(j)\\bigr| \\nonumber \\\\ & = |q_1-q_2|.\\end{aligned}\\ ] ] from the data processing theorem for the rnyi divergence ( see ( * ? ? ? * theorem  9 ) ) , @xmath237 where @xmath238 and @xmath239 are the probability measures which are defined on the binary alphabet ( see ) .",
    "the lemma follows by combining and .",
    "[ appendix : proof of proposition on skew - symmetry of g ]    eq",
    ".   follows from the equality @xmath218 where @xmath240 is the bhattacharyya coefficient between @xmath220 , and since ( see ( * ? ? ?",
    "* proposition  1 ) ) @xmath241    to prove , note that @xmath242 where @xmath243 denotes the @xmath244-divergence between the probability measures @xmath2 and @xmath3 ( which is the hellinger divergence of order  2 ) .",
    "one can derive a closed - form expression for @xmath245 by relying on the closed - form solution of a minimization of the @xmath244-divergence @xmath246 subject to the constraint @xmath247 , which is given by ( see ( * ? ? ?",
    "* eq .  ( 58 ) ) ) @xmath248 $ , } \\\\[0.1 cm ] \\frac{\\varepsilon}{2-\\varepsilon } , & \\quad \\mbox{if $ \\varepsilon",
    "\\in ( 1,2).$ } \\end{array } \\right.\\end{aligned}\\ ] ]    eq .   follows from the skew - symmetry property of the rnyi divergence ( * ? ? ?",
    "* proposition  2 ) .",
    "the lower bound on @xmath61 in follows from , which implies that for @xmath5 and @xmath54 @xmath249 \\colon |p - q| \\geq \\frac{\\varepsilon}{2 } } \\left(p^{\\alpha } q^{1-\\alpha } + ( 1-p)^{\\alpha } ( 1-q)^{1-\\alpha } \\right ) \\bigr)}{\\alpha-1 } \\label{eq : an equivalent expression for g}\\ ] ] and , we have @xmath250 \\colon |p - q| \\geq \\frac{\\varepsilon}{2 } } \\bigl(p^{\\alpha } q^{1-\\alpha } + ( 1-p)^{\\alpha } ( 1-q)^{1-\\alpha } \\bigr ) \\nonumber \\\\[0.1 cm ] & \\leq \\max_{p , q \\in [ 0,1 ] \\colon |p - q| \\geq \\frac{\\varepsilon}{2 } } p^{\\alpha } q^{1-\\alpha } + \\max_{p , q \\in [ 0,1 ] \\colon |p - q| \\geq \\frac{\\varepsilon}{2 } } ( 1-p)^{\\alpha } ( 1-q)^{1-\\alpha } \\nonumber \\\\[0.1 cm ] & = 2 \\max_{p , q \\in [ 0,1 ] \\colon |p - q| \\geq \\frac{\\varepsilon}{2 } } p^{\\alpha } q^{1-\\alpha } \\nonumber \\\\[0.1 cm ] & = 2 \\ , \\max \\left\\ { \\bigl(1-\\tfrac12 \\ , \\varepsilon \\bigr)^{\\alpha } , \\ ,",
    "\\bigl(1-\\tfrac12 \\ , \\varepsilon \\bigr)^{1-\\alpha } \\right\\}. \\label{eq : upper bound on the max term for g}\\end{aligned}\\ ] ] the lower bound on @xmath61 in follows from the combination of and .",
    "[ appendix : proof of lemma on monotonicity ]    for @xmath5 and @xmath75 , we have @xmath251 and @xmath252 this proves the two limits in .",
    "we prove in the following that @xmath253 is strictly increasing on the interval @xmath254 , and we also prove later in this appendix that this function is monotonically increasing on the interval @xmath255 $ ] .",
    "these two parts of the proof yield that @xmath253 is strictly monotonically increasing on the interval @xmath256 .",
    "the positivity of @xmath257 on @xmath256 follows from the first limit in , jointly with the monotonicity of this function which is proved in the following .    for a proof that @xmath253 is strictly monotonically increasing on @xmath254 , this function ( see )",
    "is expressed as follows : @xmath258 where @xmath259 \\tfrac{1-\\alpha}{\\alpha } , & \\quad \\mbox{if $ t=1$. } \\end{array } \\right .",
    "\\label{eq : function u}\\end{aligned}\\ ] ] note that @xmath260 in was defined to be continuous at @xmath261 .",
    "in order to proceed , we need the following two lemmas :    let @xmath75 .",
    "the function @xmath262 in is strictly monotonically increasing on @xmath263 $ ] , and it is strictly monotonically decreasing on @xmath254 .",
    "this function is also positive on @xmath256 .",
    "[ lemma : properties of the function z ]    @xmath264 for @xmath79 since @xmath265 , and @xmath266 . in order to prove the monotonicity properties of @xmath262 , note that its derivative satisfies the equality @xmath267 which is derived by taking logarithms on both sides of , followed by their differentiation . by setting the derivative of @xmath268 ( with respect to @xmath269 ) to zero , we have @xmath270 . since @xmath264 for @xmath79 , it follows from that @xmath271 for @xmath272 , and @xmath273 for @xmath274 .",
    "hence , @xmath262 is strictly monotonically increasing on @xmath263 $ ] , and it is strictly monotonically decreasing on @xmath254 .",
    "let @xmath5 .",
    "the function @xmath260 in is strictly monotonically decreasing and positive on @xmath275 .",
    "[ lemma : properties of the function u ]    differentiation of @xmath260 in gives that for @xmath276 @xmath277 note that @xmath278 , so the derivative is zero at @xmath261 , it is positive if @xmath279 , and it is negative if @xmath280 .",
    "this implies that @xmath281 for every @xmath282 , and it is satisfied with equality if and only if @xmath261 . from",
    ", it follows that @xmath260 is strictly monotonically decreasing on @xmath275 . since @xmath283 ( see ) and @xmath260 is strictly monotonically decreasing on @xmath275 then it is positive on this interval .    from lemmas  [ lemma : properties of the function z ] and",
    "[ lemma : properties of the function u ] , it follows that @xmath262 is strictly monotonically decreasing and positive on @xmath284 , and @xmath260 is strictly monotonically decreasing and positive on @xmath275 .",
    "this therefore implies that the composition @xmath285 is strictly monotonically increasing and positive on the interval @xmath284 .",
    "hence , from , since @xmath253 is expressed as a product of two positive and strictly monotonically increasing functions on @xmath284 , also @xmath257 has these properties on this interval .",
    "this completes the first part of the proof where we show that @xmath253 is strictly monotonically increasing and positive on @xmath254 .",
    "we prove in the following that @xmath253 is also strictly monotonically increasing and positive on @xmath286 $ ] . for this purpose ,",
    "the function @xmath257 is expressed in the following alternative way : @xmath287 where @xmath262 is defined in , and @xmath288 \\tfrac{1-\\alpha}{\\alpha } , & \\quad \\mbox{if $ t=1$. } \\end{array } \\right .",
    "\\label{eq : function r}\\end{aligned}\\ ] ] note that it follows from lemma  [ lemma : properties of the function z ] and that @xmath289 so the composition @xmath290 in is independent of @xmath291 ; the value of @xmath291 is defined in to obtain the continuity of @xmath292 , which leads to the following lemma :    for @xmath5 , the function @xmath292 in is strictly monotonically increasing and positive on @xmath275 .",
    "[ lemma : properties of the function r ]    a differentiation of @xmath292 in gives @xmath293 so the sign of @xmath294 is the same as of @xmath295 . since @xmath5 , and @xmath296 it follows that the last derivative is negative for @xmath279 , zero at @xmath261 , and positive for @xmath280",
    "this implies that @xmath261 is a global minimum of the numerator of @xmath297 ( see ) , so @xmath298 and equality holds if and only if @xmath261 .",
    "it therefore follows from that @xmath299 for @xmath300 , so @xmath301 is strictly monotonically increasing on @xmath275 . since @xmath302 , the monotonicity of @xmath301 on @xmath275 yields that it is positive on this interval .    from lemmas  [ lemma : properties of the function z ] and",
    "[ lemma : properties of the function r ] , @xmath262 is strictly monotonically increasing and positive on @xmath255 $ ] , and @xmath292 is strictly monotonically increasing and positive on @xmath275 .",
    "this implies that the composition @xmath303 is strictly monotonically increasing and positive on the interval @xmath255 $ ] . from",
    ", @xmath257 is expressed as a product of two strictly increasing and positive functions on the interval @xmath263 $ ] , which implies that @xmath253 also has these properties on this interval .",
    "this completes the second part of the proof where we show that @xmath253 is strictly monotonically increasing and positive on @xmath255 $ ] .",
    "the combination of the two parts of this proof completes the proof of lemma  [ lemma : monotonicity of f ] .",
    "[ appendix : proof of proposition on efficient calculation of g ]    the proof relies on the lagrange duality and kkt conditions , where strong duality is first asserted by verifying the satisfiability of slater s condition .",
    "let @xmath5 , @xmath82 , and @xmath304 .",
    "solving is equivalent to solving the optimization problem @xmath305 , \\nonumber \\\\[0.1 cm ]    \\end{array } \\right.\\end{aligned}\\ ] ] where @xmath306 are the optimization variables .",
    "the objective function of the optimization problem is concave for @xmath5 , so this maximization problem is a convex optimization problem .",
    "since the problem is also strictly feasible at an interior point of the domain in , slater s condition yields that strong duality holds for this optimization problem ( see ( * ? ? ?",
    "* section  5.2.3 ) ) .",
    "note that the replacement of @xmath307 with @xmath308 and @xmath309 , respectively , does not affect the value of the objective function and the satisfiability of the constraints in .",
    "consequently , it can be assumed with loss of generality that @xmath104 ; together with the inequality constraint @xmath310 , it gives that @xmath311 .",
    "the lagrangian of the dual problem is given by @xmath312 and the kkt conditions lead to the following set of equations : @xmath313 - \\lambda = 0 , \\\\[0.2 cm ] & \\frac{\\partial l}{\\partial q } = ( 1-\\alpha ) \\bigl [ p^{\\alpha } q^{-\\alpha } - ( 1-p)^{\\alpha } ( 1-q)^{-\\alpha } \\bigr ] + \\lambda = 0 , \\\\[0.2 cm ] & \\frac{\\partial l}{\\partial \\lambda } = q - p+\\varepsilon ' = 0 . \\end{array } \\right .",
    "\\label{eq : set of kkt equations}\\end{aligned}\\ ] ] eliminating @xmath314 from the first equation in , and substituting it into the second equation gives @xmath315 + \\alpha \\left [ \\bigl(\\frac{p}{q}\\bigr)^{\\alpha-1 } - \\bigl(\\frac{1-p}{1-q}\\bigr)^{\\alpha-1 } \\right ] = 0 .",
    "\\label{eq : equation 1 from 1st and 2nd kkt conditions}\\ ] ] from the third equation of , substituting @xmath316 into , and re - arranging terms gives the equation @xmath317 where @xmath257 is the function in .",
    "[ appendix : proof of the lemma with the identity for the renyi divergence ] for @xmath318 , the following equalities hold : @xmath319 & \\stackrel{\\text{(a)}}{= } \\frac1{\\alpha-1 } \\ , { \\displaystyle{\\int}}_{{\\ensuremath{\\mathcal}}{a } } \\mathrm{d}q \\ , \\log \\left(\\frac{\\mathrm{d}q}{\\mathrm{d}p_2}\\right)^{\\alpha-1 } + \\frac{1}{1-\\alpha } \\ , { \\displaystyle{\\int}}_{{\\ensuremath{\\mathcal}}{a } } \\mathrm{d}q \\ , \\log \\left(\\frac{\\mathrm{d}q}{\\mathrm{d}p_1}\\right)^\\alpha + \\frac1{\\alpha-1 } \\ , { \\displaystyle{\\int}}_{{\\ensuremath{\\mathcal}}{a } } \\mathrm{d}q \\ , \\log \\left(\\frac{\\mathrm{d}q}{\\mathrm{d}q_\\alpha}\\right ) \\\\[0.1 cm ] & \\stackrel{\\text{(b)}}{= } \\frac{1}{\\alpha-1 } \\ , { \\displaystyle{\\int}}_{{\\ensuremath{\\mathcal}}{a } } \\mathrm{d}q(x ) \\ , \\log \\left ( \\left(\\frac{\\mathrm{d}p_1}{\\mathrm{d}q } \\ , ( x)\\right)^{\\alpha } \\left(\\frac{\\mathrm{d}p_2}{\\mathrm{d}q } \\ , ( x)\\right)^{1-\\alpha } \\left(\\frac{\\mathrm{d}q_\\alpha}{\\mathrm{d}q } \\ , ( x)\\right)^{-1 } \\right ) \\\\ & \\stackrel{\\text{(c)}}{= } \\frac{1}{\\alpha-1 } \\ , { \\displaystyle{\\int}}_{{\\ensuremath{\\mathcal}}{a } } \\mathrm{d}q(x ) \\ ; \\log \\left ( { \\displaystyle{\\int}}_{{\\ensuremath{\\mathcal}}{a}}{\\left(\\frac{\\mathrm{d}p_1}{\\mathrm{d}q } \\ , ( u ) \\right)^\\alpha } \\ , \\left(\\frac{\\mathrm{d}p_2}{\\mathrm{d}q } \\ , ( u)\\right)^{1-\\alpha } \\ ,",
    "\\mathrm{d}q(u)\\right ) \\\\[0.1 cm ] & \\stackrel{\\text{(d)}}{= } \\frac{1}{\\alpha-1 } \\ ; \\log \\left ( { \\displaystyle{\\int}}_{{\\ensuremath{\\mathcal}}{a}}{\\left(\\frac{\\mathrm{d}p_1}{\\mathrm{d}q } \\ , ( u ) \\right)^\\alpha } \\ , \\left(\\frac{\\mathrm{d}p_2}{\\mathrm{d}q } \\ , ( u)\\right)^{1-\\alpha } \\ ,",
    "\\mathrm{d}q(u)\\right ) \\\\[0.1 cm ] & \\stackrel{\\text{(e)}}{= } d_{\\alpha}(p_1 \\| p_2)\\end{aligned}\\ ] ] where ( a ) follows from the equality @xmath320 where @xmath321 is an arbitrary probability measure such that @xmath322 ; ( b ) holds since @xmath1 are mutually absolutely continuous which also yields that @xmath323 ( in view of ) , ( c ) follows from , ( d ) holds since @xmath9 is a probability measure , and ( e ) follows from ( recall that @xmath324 ) .",
    "t. van erven , _ when data compression and statistics disagree : two frequentist challenges for the minimum description length principle _ , ph.d .",
    "dissertation , leiden university , leiden , the netherlands , 2010 .                            c. h. hsu and a. anastasopoulos , `` capacity - achieving codes with bounded graphical complexity and maximum - likelihood decoding , '' _ ieee trans . on information theory _",
    "56 , no .  3 , pp .",
    "9921006 , march 2010 .",
    "y. mansour , m. mohri and a. rostamizadeh , `` multiple source adaptation and the rnyi divergence , '' _ proceedings of the 25th conference on uncertainty in artificial intelligence _ , pp .  367374 , montreal , canada , june 2009 .",
    "m. raginsky , `` logarithmic sobolev inequalities and strong data processing theorems for discrete channels , '' _ proceedings of the 2013 ieee international symposium on information theory _ , pp .  419423 , istanbul , turkey , july 2013 .",
    "i. sason and s. shamai , _ performance analysis of linear codes under maximum - likelihood decoding : a tutorial , _ _ foundations and trends in communications and information theory _ ,",
    "vol .  3 , no .  12 , pp .",
    "1222 , now publishers , delft , the netherlands , july  2006 .",
    "i. sason and r. urbanke , `` parity - check density versus performance of binary linear block codes over memoryless symmetric channels , '' _ ieee trans . on information theory _ , vol .",
    "49 , no .  7 , pp .  16111635 , july 2003 .",
    "i. sason and s. verd , `` upper bounds on the relative entropy and rnyi divergence as a function of total variation distance for finite alphabets , '' _ proceedings of the 2015 ieee information theory workshop ( itw 2015 ) _ , pp .  214218 , jeju island , korea , october  1115 , 2015 .",
    "m. twitto , i. sason and s. shamai , `` tightened upper bounds on the ml decoding error probability of binary linear block codes , '' _ ieee trans . on information theory _ ,",
    "53 , no .  4 , pp . 14951510 , april 2007 .",
    "n. a. warsi , `` one - shot bounds for various information - theoretic problems using smooth min and max rnyi divergences , '' _ proccedings of the 2013 ieee information theory workshop _ , pp .",
    "434438 , seville , spain , september 2013 .",
    "a. d. yardi .",
    "a. kumar , and s. vijayakumaran , `` channel - code detection by a third - party receiver via the likelihood ratio test , '' _ proceedings of the 2014 ieee international symposium on information theory _ , pp .  10511055 , honolulu , hawaii , usa , july 2014 ."
  ],
  "abstract_text": [
    "<S> this paper starts by considering the minimization of the rnyi divergence subject to a constraint on the total variation distance . </S>",
    "<S> based on the solution of this optimization problem , the exact locus of the points @xmath0 is determined when @xmath1 are arbitrary probability measures which are mutually absolutely continuous , and the total variation distance between @xmath2 and @xmath3 is not below a given value . </S>",
    "<S> it is further shown that all the points of this convex region are attained by probability measures which are defined on a binary alphabet . </S>",
    "<S> this characterization yields a geometric interpretation of the minimal chernoff information subject to a constraint on the variational distance .    </S>",
    "<S> this paper also derives an exponential upper bound on the performance of binary linear block codes ( or code ensembles ) under maximum - likelihood decoding . </S>",
    "<S> its derivation relies on the gallager bounding technique , and it reproduces the shulman - feder bound as a special case . </S>",
    "<S> the bound is expressed in terms of the rnyi divergence from the normalized distance spectrum of the code ( or the average distance spectrum of the ensemble ) to the binomially distributed distance spectrum of the capacity - achieving ensemble of random block codes . </S>",
    "<S> this exponential bound provides a quantitative measure of the degradation in performance of binary linear block codes ( or code ensembles ) as a function of the deviation of their distance spectra from the binomial distribution . </S>",
    "<S> an efficient use of this bound is considered .    * keywords * : chernoff information , distance spectrum , error exponent , maximum - likelihood decoding , relative entropy , rnyi divergence , total variation distance . </S>"
  ]
}