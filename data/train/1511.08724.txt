{
  "article_text": [
    "reinforcement learning ( rl ) is the subfield of artificial intelligence concerned with agents that have to learn a task - solving policy by exploring state - action pairs and observing rewards @xcite .",
    "off - policy algorithms such as q - learning , or on - policy algorithms such as sarsa , are well - understood and can be shown to converge towards optimal policies under quite general assumptions .",
    "these algorithms do so by updating , for every state - action pair @xmath0 , an estimate @xmath1 of the expected value of doing @xmath2 in @xmath3 .",
    "our aim in this article is expressly _ not _ to propose a more efficient or more powerful new rl algorithm .",
    "in contrast , we want to show that convergence can occur already with very simplistic algorithms .",
    "the setting of our result is that of tasks where the agent has to reach a goal state in which a reward action can be performed .",
    "actions can be nondeterministic .",
    "we like to refer to this setting as _ navigational learning_.    the learning algorithm we consider is for a simplistic agent that can only remember the states it has already visited .",
    "the algorithm is on - policy ; its only update rule is that , when a state is revisited , the policy is revised and updated with an arbitrary new action for that state .",
    "we refer to this algorithm as the _ cycle - detection algorithm_. our main result is that this algorithm converges for all tasks that we call _",
    "reducible_. intuitively , a task is reducible if there exists a policy that is guaranteed to lead to reward .",
    "we also provide a test for convergence that an outside observer could apply to decide when convergence has happened , which can be used to detect convergence in a simulation .",
    "we note that the final policy is allowed to explore only a strict subset of the entire state space .",
    "a first motivation for this work is to understand how biological organisms can be successful in learning navigational tasks .",
    "for example , animals can learn to navigate from their nest to foraging areas and back again  @xcite .",
    "reward could be related to finding food or returning home . as in standard rl ,",
    "the learning process might initially exhibit exploration , after which eventually a policy is found that leads the animal more reliably to reward . in the context of biologically plausible learning , @xcite make the following interesting observations .",
    "first , navigational learning is not restricted to physical worlds , but can also be applied to more abstract state spaces .",
    "second , the formed policy strongly depends on the experiences of the agent , and therefore the policy is not necessarily optimal .",
    "we elaborate these observations in our formal framework .",
    "we consider a general definition of tasks , which can be used to represent both physically - inspired tasks and more abstract tasks .",
    "furthermore , we do not insist on finding ( optimal ) policies that generate the shortest path to reward , but we are satisfied with learning policies that avoid cycles .    a secondary motivation for this work is to contribute towards filling an apparent gap that exists between the field of reinforcement learning and the more logic - based fields of ai and computer science .",
    "indeed , on the structural level , the notion of task as used in rl is very similar to the notion of _ interpretation _ in description logics  @xcite , or the notion of _ transition system _ used in verification  @xcite . yet",
    ", the methods used in rl to establish convergence are largely based on techniques from numerical mathematics and the theory of optimization .",
    "our aim was to give proofs of convergence that are more elementary and are more in the discrete - mathematics style common in the above logic - based fields , as well as in traditional correctness proofs of algorithms @xcite .",
    "standard rl convergence proofs assume the condition that state - action pairs are visited ( and thus updated ) infinitely often , see e.g.  @xcite . conditions of this kind are known as _",
    "conditions in the theory of concurrent processes @xcite . also for our convergence proof",
    "we need an appropriate fairness assumption to the effect that when the agent repeats some policy - updating configuration infinitely often , it must also explore all possible updates infinitely often .",
    "we note that the cycle - detection learning algorithm could be remotely related to biologically plausible mechanisms . in some models of biological learning  @xcite",
    ", a policy is represented by synaptic connections from neurons encoding ( the perception of ) states to neurons encoding actions .",
    "connections are strengthened when pre - before - post synaptic activity is combined with reward  @xcite , causing an organism to remember action preferences for encountered states .",
    "if an organism would initially have a policy that frequently leads to cycles in the task , there is a ( slow ) way to still unlearn that policy , as follows . consider a pair @xmath4 of a state @xmath5 and its preferred action @xmath6 in the policy .",
    "due to noise  @xcite , a neuron @xmath7 participating in the encoding of action @xmath6 could become activated just before state @xmath5 effectively occurs .",
    "possibly , this post - before - pre synaptic activity leads to long - term - depression  @xcite , i.e. , connections from @xmath5 to @xmath7 are weakened .",
    "is activated by noise just before state @xmath5 occurs , refractoriness could prevent state @xmath5 from subsequently activating @xmath7  @xcite .",
    "the resulting absence of a postsynaptic spike at @xmath7 fails to elicit long - term - potentiation , i.e. , connections from @xmath5 to @xmath7 are not strengthened .",
    "so , the mentioned weakening effect is not compensated .",
    "] at some synapses , the weakening effect is aided by a longer time window for long - term - depression compared to long - term - potentiation  @xcite .",
    "so , if reward would remain absent for longer periods , as in cycles without reward , noise could gradually unlearn action preferences for states . in absence of such preferences",
    ", noise could generate random actions for states .",
    "the unlearning phase followed by new random action proposals , would resemble our cycle - detection algorithm .",
    "[ [ outline ] ] outline + + + + + + +    this article is organized as follows .",
    "we discuss related work in section  [ sec : relwork ] .",
    "we formalize important concepts in section  [ sec : fundamentals ] .",
    "we present and prove our results in section  [ sec : results ] .",
    "we discuss examples and simulations in section  [ sec : examples ] , and we conclude in section  [ sec : conclusion ] .",
    "some previous work on reinforcement learning algorithms is focused on learning a policy efficiently , say , using a polynomial number of steps in terms of certain input parameters of the task  @xcite .",
    "there is also a line of work in reinforcement learning that is not necessarily aimed towards efficiently bounding the learning time . in that case , convergence of the learning process happens in the limit , by visiting task states infinitely often .",
    "some notable examples are temporal - difference learning  @xcite and q - learning  @xcite .",
    "temporal - difference learning has become an attractive foundation for biological learning models  @xcite .",
    "most previous works in numerical reinforcement learning try to find optimal policies , and their related optimal value functions  @xcite ; an optimal policy gives the highest reward in the long run .",
    "this has motivated the design of numerical learning techniques .",
    "the corresponding proof techniques do not always clearly illuminate how properties of the task state space interplay with a particular learning algorithm . with the framework introduced in this article",
    ", we hope to shed more light on properties of the task state space , in particular on the way that paths could be formed in the graph structure of the task .",
    "although our graph - oriented framework has a different viewpoint compared to standard numerical reinforcement learning , we believe that our theorem  [ theo : algvisit ] , showing that convergence always occurs on reducible tasks , and its proof contribute to making the fascinating idea of reinforcement learning more easily accessible to a wider audience . our convergence result has a similar intent as previous results showing that numerical learning algorithms converge with probability one .",
    "various papers study models of reinforcement learning in the context of neuronal agents that learn to navigate a physical environment  @xcite .",
    "interestingly , @xcite study both physical and more abstract state spaces . as an example of a physical state space",
    ", they consider a navigation task in which a simulated mouse has to swim to a hidden platform where it can rest , where resting corresponds to reward ; each state contains only the @xmath7 and @xmath8 coordinate .",
    "as an example of an abstract state space , they consider an acrobatic swinging task where reward is given when the tip of a double pendulum reaches a certain height ; this space is abstract because each state contains two angles and two angular velocities , i.e. , there are four dimensions",
    ". conceptually it does not matter how many dimensions a state space has , because the agent is always just seeking paths in the graph structure of the task .",
    "this idea of finding paths in the task state space is also explored by @xcite , in a framework based on depth - first search .",
    "their framework has a more global perspective where learning operations have access to multiple states simultaneously and where the overall search is strongly embedded in a recursive algorithm with backtracking .",
    "our algorithm acts from the local perspective of a single agent , where only one state can be observed at any time .    as remarked by @xcite , a repeated theme in reinforcement learning",
    "is to update the policy ( and value estimation ) while the agent visits states .",
    "this theme is also strongly present in the current article , because for each visited state the policy always remembers the lastly tried action for that state . the final aim for convergence , as studied in this article , is to eventually not choose any new actions anymore for the encountered states .",
    "the notion of reducibility discussed in this article is related to the principles of ( numerical ) dynamic programming , upon which a large part of reinforcement learning literature is based  @xcite . indeed , in reducibility",
    ", we defer the responsibility of obtaining reward from a given state to one of the successor states under a chosen action .",
    "this resembles the way in dynamic programming that reward prediction values for a given state can be estimated by looking at the reward prediction values of the successor states . in settings of standard numerical reinforcement learning",
    ", dynamic programming finds an optimal policy in a time that is worst - case polynomial in the number of states and actions .",
    "this time complexity is also applicable to our iterative reducibility procedure given in section  [ sub : task ] .",
    "we formalize tasks and the notion of reducibility in section  [ sub : task ] .",
    "next , in section  [ sub : navlearn ] , we use an operational semantics to formalize the interaction between a task and our cycle - detection learning algorithm . in section  [",
    "sub : convergence ] , we define convergence as the eventual stability of the policy .",
    "lastly , in section  [ sub : fairness ] , we impose certain fairness restrictions on the operational semantics .      [ [ tasks ] ] tasks + + + + +    to formalize tasks , we use nondeterministic transition systems where some transitions are labeled as being immediately rewarding , where reward is only an on - off flag . formally , a _ task _ is a five - tuple @xmath9 where @xmath10 , @xmath11 , and @xmath12 are nonempty finite sets ; @xmath13 ; @xmath14 is a nonempty subset of @xmath15 ; and , @xmath16 is a function that maps each @xmath17 to a nonempty subset of @xmath10 . the elements of @xmath10 , @xmath11 , and @xmath12 are called respectively _ states _ , _ start states _ , and _",
    "actions_. the set @xmath14 tells us which pairs of states and actions give immediate reward .",
    "function @xmath16 describes the possible successor states of applying actions to states .",
    "our formalization of tasks keeps only the graph structure of models previously studied in reinforcement learning ; essentially , compared to finite markov decision processes  @xcite , we omit transition probabilities and we simplify the numerical reward signals to boolean flags",
    ". we do not yet study negative feedback signals , so performed actions give either reward or no reward , i.e. , the feedback is either positive or neutral . in our framework , the agent can observe states in an exact manner , which is a commonly used assumption  @xcite .",
    "we mention negative feedback signals and partial information as topics for further work in section  [ sec : conclusion ] . @xmath18    [ [ reducibility ] ] reducibility + + + + + + + + + + + +    let @xmath19 be a task as above .",
    "we define the set @xmath20 we refer to the elements of @xmath21 as _ goal states_. intuitively , for a goal state there is an action that reliably gives immediate reward .",
    "each task has at least one goal state because the set @xmath14 is always nonempty .",
    "the agent could learn a strategy to reduce all encountered states to goal states , and then perform a rewarding action at goal states .",
    "this intuition is formalized next .",
    "let @xmath22 .",
    "we formalize how states can be reduced to @xmath23 .",
    "let @xmath24 denote the set of natural numbers without zero .",
    "first , we define the infinite sequence @xmath25 of sets where @xmath26 , and for each @xmath27 , @xmath28 we call @xmath29 , @xmath30 , etc , the _ ( reducibility ) layers_. we define @xmath31 .",
    "note that @xmath32 . because @xmath10 is finite",
    ", there is an index @xmath33 for which @xmath34 , i.e. , @xmath35 is a _",
    "fixpoint_. letting @xmath36 , we say that @xmath37 is _ reducible to @xmath23 _ if @xmath38 . intuitively , each state in @xmath37 can choose an action to come closer to @xmath23 .",
    "we also say that a single state @xmath39 is _ reducible to @xmath23 _ if @xmath40 .",
    "now , we say that task @xmath19 is _ reducible ( to reward ) _ if the state set @xmath10 is reducible to @xmath21 .",
    "we use the abbreviation @xmath41 where @xmath42 .",
    "reducibility formalizes a sense of solvability of tasks .",
    "we illustrate the notion of reducibility with the following example .    [",
    "ex : reduce ] we consider the task @xmath43 defined as follows : @xmath44 ; @xmath45 ; @xmath46 ; @xmath47 ; and , regarding @xmath16 , we define @xmath48 task @xmath19 is visualized in figure  [ fig : reduce ] .",
    "note that the task is reducible , by assigning the action @xmath49 to both state @xmath50 and state @xmath51 .",
    "the reducibility layers up to and including the fixpoint , are : @xmath52    for simplicity , the assignments @xmath53 and @xmath54 form a deterministic strategy to reward .",
    "but we could easily extend task @xmath19 to a task in which the strategy to reward is always subjected to nondeterminism , by adding a new state @xmath55 with the new mappings @xmath56 , @xmath57 .",
    "@xmath18    .",
    "states and action applications are represented by circles and boxes respectively .",
    "start states are indicated by an arrow without origin .",
    "goal states and their rewarding actions are highlighted by a double circle and double box , respectively.,scaledwidth=50.0% ]    reducibility formalizes the intuition of an acyclic solution .",
    "this appears to be a natural notion of solvability , even in state graphs that contain cycles  @xcite .",
    "we would like to emphasize that reducibility is a notion of progress in the task transition graph , but it is not the same as determinism because each action application , i.e. , transition , remains inherently nondeterministic .",
    "we may think of reducibility as onion layers in the state space : the core of the onion consists of the goal states , where immediate reward may be obtained , and , for states in outer layers there is an action that leads one step down to an inner layer , closer to reward .",
    "when traveling from an outer layer to an inner layer , the nondeterminism manifests itself as unpredictability on the exact state that is reached in the inner layer .",
    "we describe a cycle - detection learning algorithm that operates on tasks , by means of an operational semantics that describes the steps taken over time .",
    "we first give the intuition behind the cycle - detection algorithm , and then we proceed with the formal semantics .",
    "we want to formally elaborate the intuition of path learning .",
    "our aim therefore is not necessarily to design another efficient learning algorithm .",
    "it seems informative to seek only the bare ingredients necessary for navigational learning .",
    "how would such a simple algorithm look like ?    as a first candidate , let us consider the algorithm that is given some random initial policy and that always follows the policy during execution .",
    "there would not be any exploration , and no learning , since the policy is always followed and never modified .",
    "in general , the policy might not even lead to any reward at all , and the agent might run around in cycles without obtaining reward .    at the opposite end of the spectrum , there could be a completely random process , that upon each visit to a task state always chooses some random action . if the agent is lucky then the random movement through the state space might occasionally , but unreliably , lead to reward .",
    "there is no sign of learning here either , because there is no storage of previously gained knowledge about where reward can be obtained .",
    "now we consider the following in - between strategy : the algorithm could only choose random actions when it detects a cycle in the state space before reaching reward .",
    "if the agent does not escape from the cycle then it might keep running around indefinitely without ever reaching reward .",
    "more concretely , we could consider a cycle - detection algorithm , constituted by the following directives :    * starting from a given start state , we continuously remember all encountered states . each time when reward is obtained , we again forget about which states we have seen .",
    "* whenever we encounter a state @xmath5 that we have already seen before , we perform some random action , and we store that action in the policy ( for that state @xmath5 ) .",
    "the cycle - detection algorithm is arguably amongst the simplest learning algorithms that one could conceive .",
    "the algorithm might be able to gradually refine the policy to avoid cycles , causing the agent to eventually follow an acceptable path to reward .",
    "the working memory is a set containing all states that are visited before obtaining reward .",
    "the working memory is reset whenever reward is obtained .",
    "we now formalize the cycle - detection algorithm . in the following ,",
    "let @xmath43 be a task .",
    "[ [ configurations ] ] configurations + + + + + + + + + + + + + +    a _ configuration _ of @xmath19 is a triple @xmath58 , where @xmath39 ; @xmath59 maps each @xmath39 to an element of @xmath12 ; and @xmath60 .",
    "the function @xmath59 is called the _",
    "policy_. the set @xmath61 is called the _ working memory _ and it contains the states that are already visited during the execution , but we will reset @xmath61 to @xmath62 whenever reward is obtained .",
    "we refer to @xmath5 as the _ current state _ in the configuration , and we also say that @xmath63 _ contains _ the state @xmath5 .",
    "note that there are only a finite number of possible configurations .",
    "the aim of the learning algorithm is to refine the policy during trials , as we formalize below .",
    "[ [ transitions ] ] transitions + + + + + + + + + + +    we formalize how to go from one configuration to another , to represent the steps of the running algorithm .",
    "let @xmath58 be a configuration .",
    "we say that @xmath63 is _ branching _ if @xmath64 ; this means that configuration @xmath63 represents a revisit to state @xmath5 , and that we want to generate a new action for the current state @xmath5 .",
    "next , we define the set @xmath65 as follows : letting @xmath66 if @xmath63 is branching and @xmath67 otherwise , we define @xmath68 intuitively , @xmath65 contains the _ options _ of actions and successor states that may be chosen directly after @xmath63 . if @xmath63 is branching then all actions may be chosen , and otherwise we must restrict attention to the action stored in the policy for the current state . note that the successor state depends on the chosen action .    next , for a configuration @xmath69 and a pair @xmath70 , we define the _ successor configuration _",
    "@xmath71 that results from the application of @xmath72 to @xmath73 , as follows :    * @xmath74 ; * @xmath75 and @xmath76 for all @xmath77 ; and , * @xmath78 .",
    "we emphasize that only the action and visited - status of the state @xmath79 is modified , where @xmath79 is the state that is departed from .",
    "we denote the successor configuration as @xmath80 .",
    "a _ transition _ @xmath81 is a four - tuple @xmath82 , also denoted as @xmath83 , where @xmath69 and @xmath71 are two configurations , @xmath84 , and @xmath85 .",
    "we refer to @xmath73 and @xmath86 as the _ source configuration _ and _ target configuration _ , respectively .",
    "we say that @xmath81 is a _ reward transition _ if @xmath87 .",
    "note that there are only a finite number of possible transitions because there are only a finite number of possible configurations .",
    "[ [ trials - and - runs ] ] trials and runs + + + + + + + + + + + + + + +    a _ chain _",
    "@xmath88 is a sequence of transitions where for each pair @xmath89 of subsequent transitions , the target configuration of @xmath90 is the source configuration of @xmath91 .",
    "chains could be finite or infinite .",
    "a _ trial _ is a chain @xmath88 where either _",
    "( i ) _  the chain is infinite and contains no reward transitions ; or , _ ( ii ) _  the chain is finite , ends with a reward transition , and contains no other reward transitions . to rephrase , if a trial is finite then it ends at the first occurrence of reward ; and , if there is no reward transition than the trial must be infinite .    in a trial",
    ", we say that an occurrence of a configuration is _ terminal _ if that occurrence is the last configuration of the trial , i.e. , the occurrence is the target configuration of the only reward transition .",
    "note that an infinite trial contains no terminal configurations .",
    "a _ start configuration _ is any configuration @xmath58 where @xmath92 and @xmath93 ; no constraints are imposed on the policy @xmath59 .",
    "now , a _ run _",
    "@xmath94 on the task @xmath19 is a sequence of trials , where    1 .",
    "the run is either an infinite sequence of finite trials , or the run consists of a finite prefix of finite trials followed by one infinite trial ; 2 .",
    "the first configuration of the first trial is a start configuration ; 3 .",
    "[ enu : run - trial - succession ] whenever one ( finite ) trial ends with a configuration @xmath69 and the next trial starts with a configuration @xmath71 , we have _ ( i ) _  @xmath95 ; _ ( ii )",
    "_  @xmath96 ; and , _",
    "( iii ) _",
    "@xmath97 ; satisfies the definition of start configuration . ] and , 4 .",
    "[ enu : run - trial - starts ] if the run contains infinitely many trials then each state @xmath92 is used at the beginning of infinitely many trials .",
    "we put condition  ( [ enu : run - trial - succession ] ) in words : when one trial ends , we start the next trial with a start state , we reuse the policy , and we again reset the working memory . by resetting the working memory ,",
    "we forget which states were visited before obtaining the reward .",
    "the policy is the essential product of a trial .",
    "condition  ( [ enu : run - trial - starts ] ) , saying that each start state is used at the beginning of infinitely many trials , expresses that we want to learn the whole task , with all possible start states .    to refer to a precise occurrence of a trial in a run , we use the ordinal of that occurrence , which is a nonzero natural number .    in the above operational semantics",
    ", the agent repeatedly navigates from start states to goal states . after obtaining immediate reward at a goal state",
    ", the agent s location is always reset to a start state .",
    "one may call such a framework episodic  @xcite .",
    "we note that our framework can also be used to study more continuing operational processes , that do not always enforce a strong reset mechanism from goal states back to remote start states .",
    "indeed , a task could define the set of start states simply as the set of all states . in that case , there are runs possible where some trials start at the last state reached by the previous trial , as if the agent is trying to obtain a sequence of rewards ; but we still reset the working memory each time when we begin a new trial .",
    "we now define a convergence property to formalize when learning has stopped in a run .",
    "consider a task @xmath43 .",
    "let @xmath94 be a run on @xmath19 .",
    "we say that a state @xmath39 _ ( eventually ) becomes stable _ in @xmath94 if there are only finitely many non - terminal occurrences of branching configurations containing @xmath5 .",
    "an equivalent definition is to say that after a while there are no more branching configurations at non - terminal positions containing @xmath5 .",
    "intuitively , eventual stability of @xmath5 means that after a while there is no risk anymore that @xmath5 is paired with new actions , so @xmath5 will definitely stay connected to the same action .",
    "is terminal in a trial , @xmath63 can not influence the action of its contained state because there is no subsequent transition anymore .",
    "] note that states appearing only a finite number of times in @xmath94 always become stable under this definition .",
    "we say that the run @xmath94 _ converges _ if _",
    "all trials terminate ( with reward ) , and _",
    "eventually all states become stable .",
    "we say that the task @xmath19 is _ learnable _ if all runs on @xmath19 converge .    in a run that converges , note that the policy will eventually become fixed because the only way to change the policy is through branching configurations at non - terminal positions .",
    "the lastly formed policy in a run is called the final policy , which is studied in more detail in section  [ sub : final ] .",
    "we emphasize that a converging run never stops , because runs are defined as being infinite ; the final policy remains in use indefinitely , but it is not updated anymore .",
    "we would also like to emphasize that in a converging run , eventually , the trials contain no cycles before reaching reward : the only moment in a trial where a state could be revisited , is in the terminal configuration , i.e. , in the target configuration of the reward transition .",
    "@xmath18      there are two choice points in each transition of the operational semantics :    * if the source configuration of the transition is branching , i.e. , the current state is revisited , then we choose a new random action for the current state ; and , * whenever we apply an action @xmath6 to a state @xmath5 , we can in general choose among several possible successor states in @xmath98 .",
    "fairness assumptions are needed to give the learning algorithm sufficient opportunities to detect problems and try better policies  @xcite .",
    "intuitively , in both choice points , the choice should be independent of what the policy and working memory say about states other than the current state .",
    "this intuition is related to the markov assumption , or _",
    "independence of path _",
    "assumption  @xcite .",
    "below , we formalize this intuition as a fairness notion for the operational semantics of section  [ sub : opsem ] .",
    "we say that a run @xmath94 is _ fair _ if for each configuration @xmath63 that occurs infinitely often at non - terminal positions , for each @xmath99 , the following transition occurs infinitely often : @xmath100 we say that a task @xmath19 is _ learnable under fairness _ if all fair runs of @xmath19 converge .",
    "there is always a fair run for any task , as follows . for each possible configuration @xmath63",
    ", we could conceptually order the set @xmath65 . during a run",
    ", we could also keep track for each occurrence @xmath101 of a configuration @xmath63 how many times we have already seen configuration @xmath63 in the run , excluding the current occurrence ; we denote this number as @xmath102 .",
    "we begin the first trial with a random start configuration @xmath73 , i.e. , we choose a random start state and a random policy .",
    "we next choose the option @xmath103 with the first ordinal in the now ordered set @xmath104 .",
    "now , for all the subsequent occurrences @xmath105 of a configuration @xmath63 in the run , we choose the option with ordinal @xmath106 in the set @xmath65 .",
    "so , if a configuration occurs infinitely often at non - terminal positions then we continually rotate through all its options .",
    "naturally , trials end at the first occurrence of reward , and then we choose another start state ; taking care to use all start states infinitely often .",
    "the cycle - detection learning algorithm formalized in section  [ sub : opsem ] continually marks the encountered states as visited . at the end of trials ,",
    "i.e. , after obtaining reward , each state is again marked as unvisited .",
    "if the algorithm encounters a state @xmath5 that is already visited within the same trial , the algorithm proposes to generate a new action for @xmath5 . intuitively ,",
    "if the same state @xmath5 is encountered in the same trial , the agent might be running around in cycles and some new action should be tried for @xmath5 to escape from the cycle .",
    "it is important to avoid cycles if we want to achieve an eventual upper bound on the length of a trial , i.e. , an upper bound on the time it takes to reach reward from a given start state .",
    "repeatedly trying a new action for revisited states might eventually lead to reward , and thereby terminate the trial . in this learning process",
    ", the nondeterminism of the task can be both helpful and hindering : nondeterminism is helpful if transitions choose successor states that are closer to reward , but nondeterminism is hindering if transitions choose successor states that are further from reward or might lead to a cycle . still , on some suitable tasks , like reducible tasks , the actions that are randomly tried upon revisits might eventually globally form a policy that will never get trapped in a cycle ever again ( see theorem  [ theo : algvisit ] below ) .",
    "the outline of this section is as follows . in section  [ sub : sufficient ] , we present a sufficient condition for tasks to be learnable under fairness . in section  [",
    "sub : final ] we discuss how a simulator could detect that convergence has occurred in a fair run . in section  [",
    "sub : necessary ] we present necessary conditions for tasks to be learnable under fairness .      intuitively ,",
    "if a task is reducible then we might be able to obtain a policy that on each start state leads to reward without revisiting states in the same trial . as long as revisits occur , we keep searching for the acyclic flow of states that is implied by reducibility .",
    "we can imagine that states near the goal states , i.e. , near immediate reward , tend to more quickly settle on an action that leads to reward .",
    "subsequently , states that are farther removed from immediate reward can be reduced to states near goal states , and this growth process propagates through the entire state space .",
    "this intuition is confirmed by the following convergence result :    [ theo : algvisit ] all reducible tasks are learnable under fairness .",
    "let @xmath43 be a reducible task .",
    "let @xmath94 be a fair run on @xmath19 .",
    "we show convergence of @xmath94 . in part  1 of the proof ,",
    "we show that all trials in @xmath94 terminate ( with reward ) . in part  2 , we show that eventually all states become stable in @xmath94 .    *",
    "part 1 : trials terminate .",
    "* let @xmath107 be the reducibility layers for @xmath19 as defined in section  [ sub : task ] , where @xmath108 .",
    "let @xmath109 be a trial in @xmath94 .",
    "to show finiteness of @xmath109 , and thus termination of @xmath109 , we show by induction on @xmath110 that the states in @xmath111 occur only finitely many times in @xmath109 . because @xmath19 is reducible , there is an index @xmath33 for which @xmath112 , and therefore our inductive proof shows that every state only occurs a finite number of times in the trial @xmath109 ; hence , @xmath109 is finite .",
    "before we continue , we recall that a state @xmath5 is marked as visited after its first occurrence in the trial ; any occurrence of @xmath5 after its first occurrence is therefore in a branching configuration .    * base case .",
    "* let @xmath113 . towards a contradiction ,",
    "suppose @xmath5 occurs infinitely often in trial @xmath109 , making @xmath109 infinite . because there are only a finite number of possible configurations , there is a configuration @xmath63 containing @xmath5 that occurs infinitely often in @xmath109 at non - terminal positions ( because the trial is now infinite ) .",
    "configuration @xmath63 is branching because it occurs more than once . in the trial .",
    "that occurrence represents a revisit to @xmath5 , so @xmath5 is in the working memory set of @xmath63 . ] by definition of @xmath114 , there is an action @xmath115 such that @xmath116 . since always @xmath117 , we can choose some @xmath118 .",
    "we have @xmath99 because @xmath63 is branching . by fairness",
    ", the following transition must occur infinitely often in the trial : @xmath119 but this transition is a reward transition , so the trial would have already ended at the first occurrence of this transition .",
    "hence @xmath5 can not occur infinitely many times ; this is the desired contradiction .    * inductive step . *",
    "let @xmath27 , and let us assume that states in @xmath120 occur only finitely many times in @xmath109 .",
    "let @xmath121 .",
    "by definition of @xmath111 , there is some action @xmath115 such that @xmath122 . towards a contradiction ,",
    "suppose @xmath5 occurs infinitely often in @xmath109 , making @xmath109 infinite .",
    "like in the base case , there must be a branching configuration @xmath63 containing @xmath5 that occurs infinitely often in the trial ( at non - terminal positions ) . since always @xmath117 , we can choose some @xmath123 .",
    "we have @xmath99 because @xmath63 is branching . by fairness",
    ", the following transition must occur infinitely often in the trial : @xmath119 but then @xmath124 would appear infinitely often in trial @xmath109 .",
    "this is the desired contradiction , because the induction hypothesis says that all states in @xmath120 ( including @xmath124 ) occur only finitely many times in @xmath109 .    *",
    "part 2 : stability of states .",
    "* we now show that all states eventually become stable in the fair @xmath94 .",
    "let @xmath107 again be the reducibility layers for @xmath19 as above , where @xmath108 .",
    "we show by induction on @xmath125 that states in @xmath111 become stable in @xmath94 .",
    "since @xmath19 is reducible , there is an index @xmath33 such that @xmath112 , so our inductive proof shows that all states eventually become stable .",
    "before we continue , we recall that part  1 of the proof has shown that all trials are finite .",
    "so , whenever we say that a configuration occurs infinitely often in the run , this means that the configuration occurs in infinitely many trials .",
    "similarly , if a transition occurs infinitely often in the run , this means that the transition occurs in infinitely many trials .    * base case . * let @xmath113 . towards a contradiction ,",
    "suppose @xmath5 would not become stable .",
    "this means that there are infinitely many non - terminal occurrences of branching configurations containing @xmath5 .",
    "would occur only a finite number of times in the run then we can immediately see in the definition of stability that @xmath5 becomes stable .",
    "] because there are only finitely many possible configurations , there must be a branching configuration @xmath63 containing @xmath5 that occurs infinitely often at non - terminal positions .    by definition of @xmath114",
    ", there is an action @xmath115 such that @xmath116 . since always @xmath117 , we can choose some @xmath118 .",
    "we have @xmath99 because @xmath63 is branching . by fairness ,",
    "the following transition @xmath81 must occur infinitely often in the run : @xmath119 transition @xmath81 is a reward transition because @xmath116 .",
    "let @xmath126 be the index of a trial containing transition @xmath81 ; this implies that @xmath81 is the last transition of trial @xmath126 .",
    "we now show that any non - terminal occurrences of @xmath5 after trial @xmath126 must be in a non - branching configuration .",
    "hence , @xmath5 becomes stable ; this is the desired contradiction .",
    "consider the first trial index @xmath127 after @xmath126 in which @xmath5 occurs again at a non - terminal position .",
    "let configuration @xmath69 with @xmath128 be the first occurrence of @xmath5 in trial @xmath127 ( which is at a non - terminal position ) .",
    "note that @xmath129 because _ ( i ) _",
    "trial @xmath126 ends with the assignment of action @xmath6 to @xmath5 ( through transition @xmath81 ) , and _",
    "( ii ) _  the trials between @xmath126 and @xmath127 could not have modified the action of @xmath5 .",
    "further , configuration @xmath73 is not branching because @xmath5 is not yet flagged as visited at its first occurrence in trial @xmath127 .",
    "this means that at any occurrence of @xmath73 , trial @xmath127 must select an option @xmath130 , with action @xmath6 and @xmath131 , and perform the corresponding transition @xmath132 : @xmath133 again , since @xmath116 , trial @xmath127 ends directly after transition @xmath132 ; no branching configuration containing @xmath5 can occur in trial @xmath127 at a non - terminal position .",
    "is directly revisited from itself , it does not matter whether the terminal configuration of the trial is branching or not .",
    "] this reasoning can now be repeated for all following trials to see that there are no more non - terminal occurrences of branching configurations containing @xmath5 .",
    "* inductive step .",
    "* let @xmath27 .",
    "we assume for each @xmath134 that @xmath124 eventually becomes stable .",
    "now , let @xmath121 . by definition of @xmath111 , there is an action @xmath115 such that @xmath122 .",
    "towards a contradiction , suppose that @xmath5 does not become stable .",
    "our aim is to show that now also at least one @xmath118 does not become stable , which would contradict the induction hypothesis .",
    "regarding terminology , we say that a chain is a _",
    "@xmath4-chain _ if _ ( i ) _  the chain contains only non - reward transitions and _ ( ii ) _  the chain has the following desired form : @xmath135 denoting @xmath136 for each @xmath137 , where @xmath138 and @xmath139 . note that such a chain starts and ends with an occurrence of @xmath5 ,",
    "so @xmath5 is revisited in the chain .",
    "moreover , the first transition performs the action @xmath6 from above .",
    "next , we say that a trial is a _",
    "@xmath4-trial _ if the trial contains a @xmath4-chain . in principle",
    ", each @xmath4-trial could embed a different @xmath4-chain .    to see that there are infinitely many occurrences of @xmath4-trials in @xmath94 ,",
    "we distinguish between the following two cases .    *",
    "suppose that in @xmath94 there are infinitely many occurrences of trials that end with a policy @xmath59 where @xmath140 , i.e. , action @xmath6 is assigned to @xmath5 .",
    "let @xmath126 be the index of such a trial occurrence .",
    "because by assumption @xmath5 does not become stable , we can consider the first trial index @xmath127 after @xmath126 in which @xmath5 occurs in a branching configuration at a non - terminal position .",
    "note that trials between trial @xmath126 and trial @xmath127 do not modify the action of @xmath5 .",
    "now , the first occurrence of @xmath5 in trial @xmath127 is always non - branching , and thus we perform action @xmath6 there . the subsequence in trial @xmath127",
    "starting at the first occurrence of @xmath5 and ending at some branching configuration of @xmath5 at a non - terminal position , is a @xmath4-chain : the chain starts and ends with @xmath5 , its first transition performs action @xmath6 , and it contains only non - reward transitions because it ends at a non - terminal position .",
    "hence , trial @xmath127 is a @xmath4-trial . *",
    "conversely , suppose that in @xmath94 there are only finitely many occurrences of trials that end with a policy @xmath59 where @xmath140 .",
    "let @xmath141 be an ( infinite ) suffix of @xmath94 in which no trial ends with action @xmath6 assigned to @xmath5 . because by assumption @xmath5 does not become stable , and because the number of possible configurations is finite , there is a branching configuration @xmath63 containing @xmath5 that occurs infinitely often at non - terminal positions in @xmath141 .",
    "choose some @xmath118 .",
    "we have @xmath99 because @xmath63 is branching . by fairness ,",
    "the following transition @xmath81 occurs infinitely often in @xmath141 : @xmath142 let @xmath126 be the index of a trial occurrence in @xmath141 that contains transition @xmath81 ; there are infinitely many such indexes because all trials in @xmath94 are finite ( see part 1 of the proof ) . since transition @xmath81 attaches action @xmath6 to @xmath5 , we know by definition of @xmath141 that any occurrence of @xmath81 in trial @xmath126 is followed by at least one other transition from state @xmath5 that attaches an action @xmath49 to @xmath5 with @xmath143",
    "; this implies that after each occurrence of transition @xmath81 in trial @xmath126 there is a branching configuration of @xmath5 at a non - terminal position . in trial @xmath126 ,",
    "a subsequence starting at any occurrence of @xmath81 and ending with the first subsequent branching configuration of @xmath5 at a non - terminal position , is a @xmath4-chain : the chain starts and ends with @xmath5 , its first transition performs action @xmath6 , and the chain contains only non - reward transitions because it ends at a non - terminal position .",
    "hence , trial @xmath126 is a @xmath4-trial .",
    "we have seen above that there are infinitely many occurrences of @xmath4-trials in @xmath94 . because there are only a finite number of possible configurations ,",
    "there is a configuration @xmath63 containing @xmath5 that is used in infinitely many occurrences of @xmath4-trials as the last configuration of a @xmath4-chain .",
    "note that @xmath63 occurs infinitely often at non - terminal positions since @xmath4-chains contain no reward transitions .",
    "next , we can choose from some occurrence of a @xmath4-trial in the run some @xmath4-chain @xmath88 where in particular the last configuration of @xmath88 is the configuration @xmath63 . formally , we write @xmath88 as @xmath135 where @xmath144 , and denoting @xmath136 for each @xmath137 , where @xmath138 and @xmath139 .",
    "we recall that all transitions of @xmath88 are non - reward transitions .",
    "note that @xmath145 : we have @xmath146 because @xmath147 and @xmath148 .    in chain @xmath88 ,",
    "we have certainly marked state @xmath5 as visited after its first occurrence , causing configuration @xmath149 to be branching .",
    "this implies @xmath150 , where @xmath151 is the same option as taken by the first transition of @xmath88 , since @xmath139 .",
    "also , since @xmath145 , we have certainly marked state @xmath152 as visited after its first occurrence in @xmath88 ; this implies @xmath153 .",
    "next , since the configuration @xmath144 occurs infinitely often at non - terminal positions ( see above ) , the following transition also occurs infinitely often by fairness : @xmath154 where @xmath155 . because @xmath156 and @xmath157 , configuration @xmath158 is branching",
    "moreover , we know that @xmath159 since no transition of @xmath88 is a reward transition , including the first transition .",
    "so , the branching configuration @xmath158 occurs infinitely often at non - terminal positions .",
    "hence , @xmath152 would not become stable .",
    "yet , @xmath148 , and the induction hypothesis on @xmath120 says that @xmath152 does become stable ; this is the desired contradiction .",
    "[ remark : theo - algvisit ] by theorem  [ theo : algvisit ] , the trials in a fair run on a reducible task eventually contain a number of non - terminal configurations that is at most the number of states ; otherwise at least one state would never become stable .",
    "that in infinitely many trials occurs in a branching configuration on a non - terminal position ; this state @xmath5 does not become stable by definition .",
    "] so , we get a relatively good eventual upper bound on trial length .",
    "however , theorem  [ theo : algvisit ] provides no information on the waiting time before that upper bound will emerge , because that waiting time strongly depends on the choices made by the run regarding start states of trials , tried actions , and successor states ( see also section  [ sec : conclusion ] ) .    because we seek a policy that avoids revisits to states in the same trial ,",
    "an important intuition implied by theorem  [ theo : algvisit ] is that for reducible tasks eventually the trials of a run follow paths without cycles through the state space .",
    "the followed paths are still influenced by nondeterminism , but they never contain a cycle . also , a path followed in a trial is not necessarily the shortest possible path to reward , because the discovery of paths depends on experience , i.e. , on the order in which actions were tried during the learning process .",
    "the experience dependence was experimentally observed , e.g.  by @xcite . @xmath18",
    "the order in which states become stable in a fair run does not necessarily have to follow the order of the reducibility layers of section  [ sub : task ] .",
    "in general , it seems possible that some states that are farther removed from goal states could become stable faster than some states nearer to goal states ; but , to become stable , the farther removed states probably should first have some stable strategy to the goal states .    to see that simulations do not exactly follow the inductive reasoning of the proof of theorem  [ theo : algvisit ] , one could compare , in the later section  [ sec : examples ] , the canonical policy implied by reducibility in figure  [ fig : corridor_template ] with an actual final policy in figure  [ fig : forward_backward ] .",
    "@xmath18    the following example illustrates the necessity of the fairness assumption in theorem  [ theo : algvisit ] .",
    "so , although the convergence result for reducible tasks appears natural , the example reveals that subtle notions , like the fairness assumption , should be taken into account to understand learning .",
    "[ ex : markov ] consider again the task @xmath19 from example  [ ex : reduce ] , that is also visualized in figure  [ fig : reduce ] . in the following , for ease of notation",
    ", we will denote configurations as triples @xmath160 , where @xmath7 is the current state ; @xmath8 is the action assigned by the policy to the specific state @xmath50 , with action @xmath2 assigned to all other states ; and @xmath161 is the set of visited states as before .",
    "consider now the following trial @xmath162 where the initial policy has assigned action @xmath2 to all states , including the start state @xmath50 : @xmath163 this is indeed a valid trial because the last transition is a reward transition .",
    "note also that a revisit to state @xmath50 occurs in the first transition .",
    "the configuration @xmath164 is thus branching , which implies that the option @xmath165 may be chosen there . at the end of trial @xmath162",
    ", action @xmath166 is assigned to state @xmath50 and action @xmath2 is assigned to the other states .",
    "consider also the following trial @xmath167 where the initial policy has assigned action @xmath166 to state @xmath50 and @xmath2 to all other states : @xmath168 the last transition is again a reward transition .",
    "note that a revisit occurs to state @xmath50 in the second transition .",
    "the configuration @xmath169 is therefore branching , which implies that the option @xmath170 may be chosen there . at the end of trial @xmath167",
    ", action @xmath2 is assigned to all states , including state @xmath50 .",
    "now , let @xmath94 be the run that alternates between trials @xmath162 and @xmath167 and that starts with trial @xmath162 .",
    "the state @xmath50 never becomes stable in @xmath94 because we assign action @xmath2 and action @xmath166 to state @xmath50 in an alternating fashion .",
    "so , run @xmath94 does not converge because there are infinitely many non - terminal occurrences of branching configurations containing state @xmath50 .",
    "although run @xmath94 satisfies all requirements of a valid run , @xmath94 is not fair .",
    "for example , although the configuration @xmath169 occurs infinitely often ( due to repeating trial @xmath167 ) , this configuration is never extended with the valid option @xmath165 that could propagate revisits of state @xmath50 to revisits of state @xmath51 in the same trial ; revisits to state @xmath51 could force state @xmath51 to use the other action @xmath166 , which in turn could aid state @xmath50 in becoming stable .    in conclusion , because task @xmath19 is reducible and yet the valid ( but unfair ) run @xmath94 does not converge , we see that theorem  [ theo : algvisit ] does not hold in absence of fairness .",
    "@xmath18      we refer to the lastly formed policy of a run as the final policy . for an increased understanding of what convergence means , it appears interesting to say something about the form of the final policy . in particular , we would like to understand what kind of paths are generated by the final policy . as an additional benefit , recognizing the form of the final policy allows us to detect the convergence point in a simulation .. ]    we syntactically characterize the final policy in theorem  [ theo : final ] . in general",
    ", verifying the syntactical property of the final policy requires access to the entire set of task states . in this subsection",
    ", we do not require that tasks are reducible .",
    "we first introduce the two key parts of the syntactical characterization , namely , the so - called _ forward _ and _ backward _ sets of states induced by a policy . as we will see below , the syntactical property says that the forward set should be contained in the backward set .",
    "[ [ forward - and - backward ] ] forward and backward + + + + + + + + + + + + + + + + + + + +    let @xmath43 be a task that is learnable under fairness . to make the notations below easier to read , we omit the symbol",
    "@xmath19 from them",
    ". it will always be clear from the context which task is meant .",
    "let @xmath171 be a policy , i.e. , each @xmath39 is assigned an action from @xmath12 .",
    "first , we define @xmath172 this is the set of all goal states that are assigned a rewarding action by the policy .",
    "next , we define two sets @xmath173 and @xmath174 , as follows . for the set @xmath175",
    ", we consider the infinite sequence @xmath176 , @xmath177 ,   of sets , where @xmath178 and for each @xmath27 , @xmath179 we define @xmath180 .",
    "note that @xmath173 .",
    "intuitively , the set @xmath175 contains all states that are reachable from the start states by following the policy . in the definition of @xmath181 with @xmath27",
    ", we remove @xmath182 from the extending states because we only want to add states to @xmath175 that can occur at non - terminal positions of trials .",
    "are still in @xmath175 because those states are also reachable from states outside @xmath182 . ]    for the set @xmath183 , we consider the infinite sequence @xmath184 , @xmath185 ,   of sets , where @xmath186 and for each @xmath27 , @xmath187 we define @xmath188 .",
    "note that @xmath174 .",
    "intuitively , @xmath183 is the set of all states that are reduced to the goal states in @xmath182 by the policy .    for completeness",
    ", we remark that the infinite sequences @xmath176 , @xmath177 ,  , and @xmath184 , @xmath185 ,  , each have a fixpoint because @xmath10 is finite .    [ [ final - policy ] ] final policy + + + + + + + + + + + +    we formalize the final policy .",
    "let @xmath19 be a task that is learnable under fairness .",
    "let @xmath94 be a fair run on @xmath19 , which implies that @xmath94 converges .",
    "we define the _ convergence - trial _ of @xmath94 as the smallest trial index @xmath105 for which the following holds : trial @xmath105 terminates and after trial @xmath105 there are no more branching configurations at non - terminal positions .",
    "this implies that after trial @xmath105 the policy can not change anymore , because to change the action assigned to a state @xmath5 , the state @xmath5 would have to occur again in branching configuration at a non - terminal position .",
    "we define the _ final policy _ of @xmath94 to be the policy at the end of the convergence - trial . in principle",
    ", different converging runs can have different final policies .",
    "now , we can recognize the final policy with the following property , that intuitively says that any states reachable by the policy are also safely reduced by the policy to reward :    [ theo : final ] let @xmath19 be a task that is learnable under fairness .",
    ", we do not require that @xmath19 is reducible .",
    "] let @xmath94 be a converging fair run of @xmath19 .",
    "a policy @xmath59 occurring in run @xmath94 at the end of a trial is the final policy of @xmath94 if and only if @xmath189    we show in two separate parts that @xmath190 is _",
    "( i ) _  a sufficient and _ ( ii ) _  a necessary condition for @xmath59 to be the final policy of run @xmath94 .",
    "* part 1 : sufficient condition .",
    "* let @xmath59 be a policy occurring in run @xmath94 at the end of a trial .",
    "assume that @xmath190 .",
    "we show that @xmath59 is the final policy of @xmath94 .    concretely",
    ", we show that any trial starting with policy @xmath59 will _",
    "use @xmath59 in all its configurations , including the terminal configuration ; and , _ ( ii ) _ , does not contain branching configurations at non - terminal positions .",
    "this implies that the first trial ending with @xmath59 is the convergence - trial , so @xmath59 is the final policy .",
    "let @xmath109 be a trial in @xmath94 that begins with policy @xmath59 .",
    "we explicitly denote trial @xmath109 as the following finite chain of transitions : @xmath191 for each @xmath192 , we denote @xmath193 .",
    "let @xmath176 , @xmath177 , ",
    "be the infinite sequence of sets previously defined for @xmath175 .",
    "we show by induction on @xmath194 that    a.   [ enu : save - pol ] @xmath195 ; b.   [ enu : state - in - forward ] @xmath196 ; c.   [ enu : non - branching ] @xmath197 is non - branching .    at the end of the induction",
    ", we can also see that @xmath198 : first , we have @xmath199 because configuration @xmath200 is non - branching by property  ( [ enu : non - branching ] ) ; is non - branching , we have @xmath201 , which , combined with @xmath202 for each @xmath203 , gives @xmath199 . ]",
    "second , @xmath204 by property  ( [ enu : save - pol ] )",
    ".    * base case .",
    "* let @xmath205 . for property  ( [ enu :",
    "save - pol ] ) , we have @xmath206 because the trial starts with policy @xmath59 . for property  ( [ enu : state - in - forward ] )",
    ", we see that @xmath207 . for property  ( [ enu : non - branching ] ) , we know that @xmath73 is non - branching because the first configuration in a trial still has an empty working memory of visited states",
    ".    * inductive step . *",
    "let @xmath27 , with @xmath208 .",
    "assume that the induction properties are satisfied for the configurations @xmath73 , ",
    ", @xmath209 .",
    "we now show that the properties are also satisfied for @xmath197 .",
    "property  ( [ enu : save - pol ] ) : :    by applying the induction hypothesis for    property  ( [ enu : non - branching ] ) to @xmath209 , namely    that @xmath209 is non - branching , we know    @xmath210 . by subsequently applying the    induction hypothesis for property  ( [ enu : save - pol ] ) to    @xmath209 , namely    @xmath211 , we know    @xmath195 .",
    "property  ( [ enu : state - in - forward ] ) : :    to start , we note that    @xmath212    because @xmath209 is non - branching by the induction    hypothesis for property  ( [ enu : non - branching ] ) . by subsequently    applying the induction hypothesis for property  ( [ enu :",
    "save - pol ] ) to    @xmath209 , namely    @xmath211 , we know    @xmath213 .",
    "moreover , since @xmath214 , the transition    @xmath215 ,    where @xmath216 , is a    non - reward transition . hence ,    @xmath217    and thus @xmath218 .    +",
    "lastly , by applying the induction hypothesis for    property  ( [ enu : state - in - forward ] ) to @xmath209 , we    overall obtain that    @xmath219 .    combined with    @xmath213",
    ",    we see that @xmath196 .",
    "property  ( [ enu : non - branching ] ) : :    towards a contradiction , suppose that configuration    @xmath197 is branching .",
    "this means that state    @xmath220 is revisited in    @xmath197 .. ]    let @xmath221 .",
    "note that    @xmath222 , which implies    @xmath223 . by applying the induction hypothesis for    property  ( [ enu : state - in - forward ] ) to configurations",
    "@xmath73 ,  , @xmath209 , we know that    @xmath224 .",
    "we now show    that @xmath225 ,    which would imply    @xmath226 ;    this is the desired contradiction .",
    "+    let @xmath184 , @xmath185 ,  be the infinite    sequence of sets defined for    @xmath183 above .",
    "we show by    induction on @xmath227 that    @xmath228 , which then overall implies    @xmath225 .",
    "+    * base case : @xmath229 . by definition ,",
    "let    @xmath230 .",
    "let @xmath231 be    the smallest index for which @xmath232 , i.e. ,    configuration @xmath233 represents the first occurrence    of @xmath5 in the trial . by applying the outer induction    hypothesis for properties ( [ enu : save - pol ] ) and ( [ enu : non - branching ] )    to @xmath233 , we know that    @xmath234 . but    since @xmath235 , we know that transition    @xmath236    is not a reward transition , implying    @xmath237",
    ". hence ,    @xmath238 , and overall    @xmath239 .    *",
    "inductive step .",
    "let @xmath240 .",
    "assume    @xmath241 .",
    "towards a contradiction ,    suppose @xmath242 .",
    "take some    @xmath243 . if @xmath244    then we would immediately have a contradiction with the induction    hypothesis .",
    "henceforth , suppose    @xmath245 , which , by definition of    @xmath246 , means    @xmath247 .",
    "we will    now show that    @xmath248 , which    would give @xmath249 ; this is the    desired contradiction .",
    "+    since @xmath230 , there is some smallest    @xmath231 such that    @xmath232 . using a similar reasoning as in the base    case ( @xmath229 ) , by applying the outer induction hypothesis    for properties ( [ enu : save - pol ] ) and ( [ enu : non - branching ] ) to    configuration @xmath233",
    ", we can see that    @xmath250 .",
    "this implies    @xmath251 .    as a last step ,",
    "we show that @xmath252 , which    gives    @xmath253 .",
    "we distinguish between the following cases :    * * if @xmath254 then @xmath255 , and    surely @xmath252 by definition of    @xmath256 .    * * if @xmath257 then we know    @xmath258 because configuration    @xmath197 revisits state @xmath220 ( see    above ) .    * part 2 : necessary condition .",
    "* let @xmath59 be the final policy of @xmath94 .",
    "we show that @xmath190 . by definition of final policy , @xmath59 is the policy at the end of the convergence - trial , whose trial index we denote as @xmath105 . by definition of convergence - trial , after trial @xmath105 there are no more branching configurations at non - terminal positions .",
    "note in particular that the policy no longer changes after trial @xmath105 .    towards a contradiction ,",
    "suppose that @xmath226 .",
    "let @xmath259 .",
    "note that @xmath223 . we show that there is a state @xmath230 that occurs at least once in a branching configuration at a non - terminal position after the convergence - trial @xmath105",
    "; this would be the desired contradiction .",
    "we provide an outline of the rest of the proof .",
    "the reasoning proceeds in two steps .",
    "first , we show for each @xmath230 that @xmath260 .",
    "this means that if we are inside set @xmath256 , we have the option to stay longer inside @xmath256 if we follow the policy @xmath59 .",
    "now , the second step of the reasoning is to show that we can stay arbitrarily long inside @xmath256 even after the convergence - trial @xmath105 , causing at least one state of @xmath256 to occur in a branching configuration at a non - terminal position after trial @xmath105 .    * step 1 .",
    "* let @xmath230 .",
    "we show @xmath260 . towards a contradiction ,",
    "suppose that @xmath261 .",
    "our strategy is to show that @xmath262 , which , by definition of @xmath183 , implies that there is some index @xmath263 such that @xmath264 .",
    "therefore @xmath265 .",
    "but that is false because @xmath266 ; this is the desired contradiction .    we are left to show that @xmath262 .",
    "first , we show @xmath267 . by definition , @xmath268 .",
    "since @xmath269 , there is some index @xmath263 such that @xmath270 .",
    "moreover , since @xmath271 and @xmath272 , we have @xmath238 .",
    "overall , @xmath273 , which implies that @xmath274 .",
    "now , we can complete the reasoning by combining @xmath267 with our assumption @xmath261 , to see the following : @xmath275    * step 2 .",
    "* we now show that after convergence - trial @xmath105 there is at least one occurrence of a branching configuration at a non - terminal position .    we first show that each @xmath276 occurs infinitely often at non - terminal positions after trial @xmath105 .",
    "recall that @xmath268 .",
    "we show by induction on @xmath227 that states in @xmath277 occur infinitely often at non - terminal positions after trial @xmath105 .",
    "* base case : @xmath229 . by definition ,",
    "@xmath178 . because @xmath94 is a valid run , each state of @xmath11 is used in infinitely many trials as the start state , also after trial @xmath105 ( see section  [ sub : opsem ] ) .",
    "is assumed to converge , which , by definition of convergence , implies that @xmath94 is an infinite sequence of finite trials .",
    "] moreover , the first configuration of a trial is always at a non - terminal position because each trial contains at least one transition . *",
    "inductive step .",
    "let @xmath240 .",
    "assume that each state in @xmath278 occurs infinitely often at non - terminal positions after trial @xmath105 .",
    "let @xmath279 .",
    "this implies that there is some @xmath280 for which @xmath281 .",
    "+ by applying the induction hypothesis , we know that @xmath282 occurs infinitely often at non - terminal positions after trial @xmath105 . because there are only a finite number of possible configurations , there is a configuration @xmath63 containing @xmath282 that occurs infinitely often at non - terminal positions after trial @xmath105 .",
    "because trial @xmath105 is the convergence - trial , we can make two observations about configuration @xmath63 : first , @xmath63 contains the final policy @xmath59 because the policy no longer changes after trial @xmath105 ; and , second , @xmath63 is non - branching because no branching configurations occur at non - terminal positions after trial @xmath105 .",
    "these observations imply @xmath283 .",
    "+ now , since @xmath63 occurs infinitely often at non - terminal positions after trial @xmath105 , the following transition @xmath81 occurs infinitely often after trial @xmath105 by fairness : @xmath284 lastly , we know that @xmath285 because @xmath286 , so transition @xmath81 is a non - reward transition . therefore state @xmath5 occurs infinitely often at non - terminal positions after trial @xmath105 .",
    "now take some @xmath287 . since @xmath224",
    ", we know from above that @xmath79 occurs infinitely often at non - terminal positions after trial @xmath105 .",
    "because there are only finitely many possible configurations , there is a configuration @xmath73 containing @xmath79 that occurs infinitely often at non - terminal positions after trial @xmath105 . after trial @xmath105 ,",
    "the policy no longer changes and only non - branching configurations may occur at non - terminal positions .",
    "so @xmath73 contains the final policy @xmath59 and is non - branching .",
    "moreover , since @xmath287 , we know from step  1 above that there is some @xmath288 .",
    "overall , we can see that @xmath289 . by fairness ,",
    "the following transition @xmath90 occurs infinitely often after trial @xmath105 : @xmath290 where @xmath291 . because @xmath287 we have @xmath292 , so transition @xmath90 is a non - reward transition .",
    "then @xmath293 , which is false since @xmath287 . ]",
    "therefore configuration @xmath86 occurs infinitely often at non - terminal positions after trial @xmath105 . denoting @xmath71 , note that @xmath294 .",
    "we now make a similar reasoning for @xmath86 as we did for @xmath73 . since @xmath295 , we know again from step  1 that there is some @xmath296 .",
    "configuration @xmath86 contains the final policy @xmath59 because @xmath86 occurs after trial @xmath105 , and @xmath86 is also non - branching because it occurs after trial @xmath105 at a non - terminal position .",
    "therefore @xmath297 .",
    "now , since @xmath86 occurs infinitely often at non - terminal positions after trial @xmath105 ( see above ) , the following transition occurs infinitely often after trial @xmath105 by fairness : @xmath298 where @xmath299 .",
    "this transition is also non - rewarding because @xmath295 implies @xmath300 .",
    "denoting @xmath301 , note that @xmath302 .",
    "we emphasize that more states of @xmath256 are now marked as visited .",
    "we can now complete the reasoning . by following the final policy @xmath59 from a state in @xmath256",
    ", we can always stay inside @xmath256 without reaching reward .",
    "so , the above procedure can be repeated @xmath303 times in total , to show the existence of a configuration @xmath58 with @xmath230 that occurs infinitely often at non - terminal positions after trial @xmath105 , and where @xmath64 .",
    "configuration @xmath63 is therefore branching , and thus the existence of @xmath63 gives the desired contradiction , as explained at the beginning of part  2 of the proof .",
    ", we are not guaranteed to see all of @xmath256 , but we still know that after at most @xmath303 transitions we have to revisit a state of @xmath256 at a non - terminal position ( after trial @xmath105 ) ; that first revisit forms the desired contradiction . ]      in section  [ sub : sufficient ] , we have seen that reducibility is a sufficient property for tasks to be learnable under fairness  ( theorem  [ theo : algvisit ] ) . in this subsection , we also show necessary properties for tasks to be learnable under fairness .",
    "this provides a first step towards characterizing the tasks that are learnable under fairness .",
    "thinking about such a characterization is useful because it allows us to better understand the tasks that are not learnable under fairness .",
    "we first introduce some auxiliary concepts .",
    "let @xmath43 be a task .",
    "we say that there is a _",
    "( simple ) path _ from a state @xmath5 to a state @xmath124 if there is a sequence of actions @xmath304 , with possibly @xmath305 , and a sequence of states @xmath306 such that    * @xmath128 ; * @xmath307 ; * @xmath308 for each @xmath192 ; * @xmath309 for each @xmath192 ; and , * @xmath310 for each @xmath311 with @xmath312 .",
    "we emphasize that the path does not contain reward transitions and does not repeat states .",
    "we also denote a path as @xmath313 note that there is always a path from a state to itself , namely , the empty path where @xmath305 .",
    "we say that a state @xmath5 is _ reachable _ if there is a path from a start state @xmath314 to @xmath5 .",
    "next , letting @xmath22 , we say that a state @xmath5 _ has a path to @xmath23 _ if there is a path from @xmath5 to a state @xmath315 .",
    "note that if @xmath5 has a path to @xmath23 , it is not guaranteed that the action sequence of that path always ends in @xmath23 because the transition function @xmath16 is nondeterministic .",
    "we can now note the following necessary properties for tasks to be learnable under fairness :    [ prop : necessary ] tasks @xmath19 that are learnable under fairness  satisfy the following properties :    a.   [ enu : path ] the reachable states have a path to @xmath21 .",
    "b.   [ enu : start - reduce ] the start states are reducible to @xmath21 .",
    "the property  ( [ enu : path ] ) is related to the assumption that reward can be reached from every state , see e.g.  @xcite .",
    "we show the two properties separately .",
    "denote @xmath43 .",
    "* property  ( [ enu : path ] ) . * towards a contradiction , suppose that there is a reachable state @xmath5 that has no path to @xmath21 .",
    "note that @xmath316 because otherwise the empty path from @xmath5 to itself would be a path to @xmath21 . because @xmath5 is reachable",
    ", there is a path @xmath317 where @xmath318 and @xmath319 .",
    "we can consider a policy @xmath59 where , for each @xmath192 , we set @xmath320 ; the other state - action mappings may be arbitrary .",
    "with @xmath321 occurs only once on the path . ] below we will consider a fair run @xmath94 whose first trial is given @xmath79 as start state and @xmath59 as the initial policy .",
    "first , we consider the following chain @xmath88 : @xmath322 where for each @xmath321 we define @xmath323 where @xmath324 . as @xmath62 . ]",
    "note that this chain is indeed valid : for each @xmath192 , configuration @xmath197 is ( constructed to be ) non - branching and therefore @xmath325 ; this means that we do not modify the policy during the transition @xmath326 , but we only mark @xmath220 as visited , which gives @xmath327 .",
    "note that configuration @xmath158 contains the state @xmath5 .    consider a fair run @xmath94",
    "whose first trial starts with the chain @xmath88 .",
    "we show that the first trial never terminates ; this is the desired contradiction because we had assumed that task @xmath19 is learnable under fairness .",
    "for the first trial to terminate , the trial must extend chain @xmath88 to a chain @xmath328 that terminates with a reward transition .",
    "but then the existence of @xmath328 would imply that there is a path from @xmath5 to a state @xmath329 , which is false .",
    "* property  ( [ enu : start - reduce ] ) . *",
    "we show that the start states are reducible to @xmath21 .",
    "let @xmath94 be a fair run on @xmath19 , which implies that @xmath94 converges . because @xmath94 converges",
    ", there is a final policy @xmath59 , as defined in section  [ sub : final ] . then by theorem  [ theo : final ] , we know that @xmath190 .",
    "we have @xmath330 because @xmath331 by definition of @xmath175 .",
    "letting @xmath332 be the set of states that are reducible to @xmath21 , we show below that @xmath333 ; this implies @xmath334 , as desired .    in section  [ sub :",
    "task ] , we have defined @xmath335 where @xmath108 and for each @xmath27 , @xmath336 also recall the definition @xmath188 from section  [ sub : final ] .",
    "we show by induction on @xmath125 that @xmath337 .",
    "* base case : @xmath205 . by definition ,",
    "hence , @xmath339 . *",
    "inductive step .",
    "let @xmath27 , and let us assume that @xmath340 .",
    "let @xmath341 , which , by definition of @xmath342 , implies @xmath343 . by applying the induction hypothesis",
    ", we see @xmath344 .",
    "this implies @xmath345 , and , overall , @xmath337 .",
    "this completes the proof .",
    "we recall from theorem  [ theo : algvisit ] that reducibility is a sufficient property for tasks to be learnable under fairness .",
    "note that reducibility implies the necessary properties in proposition  [ prop : necessary ] for tasks to be learnable under fairness .",
    "we now discuss the gap between these sufficient and necessary properties .",
    "let @xmath43 be a task .",
    "letting @xmath332 be the set of states that can be reduced to @xmath21 ( as in section  [ sub : task ] ) , we define @xmath346 we call @xmath347 the _ unstable set _ of @xmath19 . note that for each state @xmath348 , for each action @xmath115 , we always have @xmath349",
    ". then @xmath350 and subsequently @xmath351 , which is false . ]",
    "so , once we are inside @xmath347 , we can never reliably escape @xmath347 : escaping @xmath347 depends on the nondeterministic choices regarding successor states .",
    "this intuition has also appeared in part  2 of the proof of theorem  [ theo : final ] , but in that proof we were focusing on just a single fixed action for each state , as assigned by the final policy at hand .",
    "the following example illustrates how a nonempty unstable set could prevent convergence .",
    "in particular , the example illustrates that the necessary properties of proposition  [ prop : necessary ] are not sufficient for a task to be learnable under fairness .",
    "[ ex : unstable ] consider the task @xmath43 defined as follows : @xmath44 ; @xmath45 ; @xmath46 ; @xmath352 ; and , regarding @xmath16 , we define @xmath353 the task @xmath19 is visualized in figure  [ fig : unstable ] . note that @xmath354 , giving @xmath355 note in particular that @xmath356 and @xmath357 .",
    "task @xmath19 satisfies the necessary properties of proposition  [ prop : necessary ] : _ (",
    "i ) _  the reachable states , which are all states in this case , have a path to @xmath21 , and _",
    "( ii ) _  the start state @xmath50 is reducible to @xmath21 . however , task @xmath19 is not learnable under fairness , as we now illustrate .",
    "consider a trial @xmath109 of the following form : starting with an initial policy that assigns action @xmath2 to state @xmath50 , we first go from state @xmath50 to state @xmath51 ; next , we stay at least two consecutive times in state @xmath51 ; and , lastly , we proceed to state @xmath358 and obtain reward there . because there are no revisits to state @xmath50 in trial @xmath109 , state @xmath50 remains connected to action @xmath2 .",
    "we now see that we can make a fair run @xmath94 by repeating trials of the form of @xmath109 : state @xmath50 is never revisited and stays connected to action @xmath2 , and we keep revisiting state @xmath51 . this way , there are infinitely many branching configurations containing state @xmath51 at non - terminal positions .",
    "so , run @xmath94 does not converge .",
    "@xmath18    , with unstable set @xmath359 .",
    "the graphical notation is explained in figure  [ fig : reduce].,scaledwidth=50.0% ]    by looking at example  [ ex : unstable ] , it seems that runs would somehow have to learn to avoid entering the set @xmath347 .",
    "first , we define @xmath360 the set @xmath361 contains those states that could enter @xmath347 ; we call such states _ border states_. let @xmath362 , and let @xmath256 be the subset of @xmath347 that is reachable from @xmath5 . now",
    ", one idea could be to demand for each @xmath287 that there is some @xmath363 such that @xmath295 is reachable from @xmath79 and @xmath364 , i.e. , there is some escape option to return from @xmath256 back to border state @xmath5 .",
    "this way , we can revisit that precise border state @xmath5 in the same trial , so that under fairness we can choose a new action for @xmath5 to avoid a future entrance into @xmath347 .",
    "the possibility to revisit border states in the same trial is exactly what is missing from example  [ ex : unstable ] .",
    "the characterization of tasks that are learnable under fairness  might be a class of tasks that satisfy the necessary properties of proposition  [ prop : necessary ] and that additionally specify assumptions on the transition function to ensure the possibility of revisits to border states . illuminating",
    "the role of unstable sets in learning is an interesting avenue for further work ( see section  [ sec : conclusion ] ) .",
    "theorem  [ theo : algvisit ] tells us that all reducible tasks are learnable under fairness , and theorem  [ theo : final ] allows us to detect when the final policy has been formed . to illustrate these theorems ,",
    "we now consider two examples of tasks that are reducible , in section  [ sub : grid ] and section  [ sub : chain ] , respectively .",
    "our aim is not to show practical efficiency of the cycle - detection learning algorithm , but rather to illustrate the theoretical insights . indeed ,",
    "because the considered examples are reducible , they are learnable under fairness  by theorem  [ theo : algvisit ] .",
    "next , aided by theorem  [ theo : final ] , we can experimentally measure how long it takes for the learning process to convergence . in section  [ sec : conclusion ] , for further work , we identify aspects where the learning algorithm could be improved to become more suitable for practice .      our first example is a navigation task in a grid world  @xcite . in such a task , it is intuitive to imagine how paths are formed and what they mean .",
    "below we formalize a possible version of such a grid task .",
    "the grid is represented along two axes , the @xmath365- and @xmath366-axis . at any time , the agent is inside only one grid cell @xmath367 .",
    "we let the state set @xmath10 be a subset of @xmath368 .",
    "let @xmath369 be a subset of cells , called _",
    "goal cells_. the agent could apply the following actions to each grid cell : finish , left , right , up , down , left - up , left - down , right - up , and right - down .",
    "let @xmath370 denote the set containing these actions .",
    "the finish action is a non - movement action that gives immediate reward when applied to a goal cell .",
    "the finish action intuitively says that the agent believes it has reached a goal cell and claims to be finished .",
    "activating the finish action in any non - goal cell will just leave the agent in that cell without reward .",
    "the actions other than finish will be referred to as movement actions .    for every movement action @xmath6",
    ", there is noise from the environment .",
    "we formalize this with a noise offset function that maps each movement action to the possible relative movements that it can cause .",
    "for example , @xmath371 , and @xmath372 .",
    "intuitively , noise adds one left - rotating option and one right - rotating option to the main intended direction .",
    "the offsets of the other movement actions can be similarly defined ( see appendix  [ app : examples ] ) . for uniformity",
    "we define @xmath373 .",
    "now , we say that a task @xmath43 is a _ grid navigation task _",
    "if @xmath378 , @xmath13 , @xmath379 , there exists some nonempty subset @xmath369 such that @xmath380 and for each @xmath17 we have @xmath381 note that we only perform a movement action if the set of child - cells is fully contained in the set of possible grid cells ; otherwise the agent remains stationary .",
    ", [ fig : corridor_template ] , and [ fig : forward_backward ] . ]    the assumption of reducibility can additionally be imposed on grid navigation tasks . in figure",
    "[ fig : box ] , we visualize a grid navigation task that , for illustrative purposes , is only partially reducible .    , either _",
    "function @xmath16 defines @xmath6 to stay stationary for the cell , or _",
    "( ii ) _  @xmath6 could lead to another non - reducible cell .",
    "naturally , if all non - reducible cells are removed then the resulting task is reducible .",
    ", scaledwidth=50.0% ]    we now discuss two simulation experiments that we have performed on such grid navigation tasks .",
    "some details of the experiments can be found in appendix  [ app : examples ] .",
    "let @xmath24 denote the set of natural numbers without zero .",
    "our first experiment measures the convergence - trial index .",
    "first we discuss the general setup of the experiment .",
    "we recall from section  [ sub : final ] that the convergence - trial index of a fair run is the first trial index where the final policy occurs at the end . for a given task @xmath19",
    ", we can simulate fair runs , and we stop each run when the final policy is detected through the characterization of theorem  [ theo : final ] ; we remember the convergence - trial index .",
    "each run is started with a random policy , where each state is assigned a random action by uniformly sampling the available actions .",
    "fairness depends on the mechanism for choosing among successor states , which is also based on a uniform distribution ( see appendix  [ app : examples ] ) .",
    "interestingly , we do not have to simulate infinite runs because we always eventually detect the final policy ; when we stop the simulation after finite time , the finite run may be thought of as a prefix of an infinite fair run .    in principle",
    ", there is no upper bound on the convergence - trial index in the simulation .",
    "fortunately , our experiments demonstrate that for the studied reducible tasks , there appears to be a number @xmath101 such that the large majority of simulated runs has a convergence - trial index below @xmath105 . possibly , there are outliers with a very large convergence - trial index , although such outliers are relatively few in number .",
    "we can exclude outliers from a list of numbers by considering a @xmath382-quantile with @xmath383 .",
    ", we give the precise definition of @xmath382-quantile that we have used in our analysis . ]",
    "we wanted to see if the convergence - trial index depends on the distance between the start cells and the goal cells .",
    "for this purpose , we have considered grid tasks with the form of a corridor , as shown in figure  [ fig : corridor_template ] .",
    "there is only one start cell .",
    "the parameter that we can change is the distance from the start cell to the patch of goal cells .",
    "all other aspects remain fixed , including the width of the corridor and the number and the location of the goal cells .",
    "the arrows in figure  [ fig : corridor_template ] show reducibility of this kind of task . for a number @xmath384",
    ", we define the @xmath385-corridor as the corridor navigation task where the distance between the start cell and the goal cells is equal to @xmath385 .",
    "now , for some lengths @xmath385 , we have simulated runs on the @xmath385-corridor . for each @xmath385-corridor separately , we have simulated @xmath386 runs and we have computed the @xmath387-quantile for the measured convergence - trial indexes ; this gives an empirical upper bound on the convergence - trial index for the @xmath385-corridor .",
    "figure  [ fig : grid_convergence ] shows the quantiles plotted against the corridor lengths .",
    "we observe that longer corridors require more time to learn .",
    "this is probably because at each application of a movement action to a cell , there could be multiple successor cells due to nondeterminism .",
    "intuitively , the nondeterminism causes a drift away from any straight path to the goal cells .",
    "the policy has to learn a suitable action for each of the cells encountered due to drift .",
    "so , when the corridor becomes longer , more cells are encountered due to nondeterminism , and therefore the learning process takes more time to learn a suitable action for each of the encountered cells . figure  [ fig : grid_convergence ] suggests an almost linear relationship between corridor length and the empirical upper bound on the convergence - trial index based on the 0.9-quantile . in section  [ sub : chain ] , we will see another example , where the relationship is not linear .            for a fixed grid navigation task",
    ", we also wanted to test if trial length decreases as the run progresses .",
    "a decreasing trial length would demonstrate that the learning algorithm is gradually refining the policy to go more directly to reward from the start states , avoiding cycles in the state space .    for the fixed corridor length of @xmath388",
    ", we have simulated the first 2000 trials of 1000 runs ; and , for each trial we have measured its length as the number of transitions .",
    "this gives a data matrix where each cell @xmath389 , with run index @xmath105 and trial index @xmath126 , contains the length of trial @xmath126 in the simulated run @xmath105 .",
    "for each trial index we have computed the @xmath387-quantile of its length measurements in all runs . by plotting the resulting empirical upper bound on trial length against the trial index ,",
    "we arrive at figure  [ fig : grid_trial_length ] .",
    "we can see that trial length generally decreases as the run progresses .",
    "a similar experimental result is also reported by @xcite , in the context of neurons learning a grid navigation task .",
    "we recall from theorem  [ theo : final ] that , within the context of a specific task , the final policy @xmath59 in a converging run satisfies the inclusion @xmath190 . for the corridor in figure  [ fig : corridor_template ] , for one simulated run",
    ", we visualize the forward state set and the backward state set of the final policy in figure  [ fig : forward_backward ] .",
    "note that the forward state set is indeed included in the backward state set .",
    "interestingly , in this case , the simulated run has initially learned to perform the finish action in some goal cells , as witnessed by the backward state set , but eventually some of those goal cells are no longer reached from the start state , as witnessed by the forward state set .",
    ".45 , of the final policy in one simulated run of the corridor navigation task shown in figure  [ fig : corridor_template ] .",
    "the states belonging to a set are marked with their action in the final policy ; the empty grid cells are not part of the set .",
    "each arrow shows only the main intended direction of the action ; we do not show the left - rotating noise option and the right - rotating noise option .",
    "goal cells that are assigned the finish action by the final policy are marked with the `` f '' sign.,title=\"fig:\",scaledwidth=100.0% ]    .45 , of the final policy in one simulated run of the corridor navigation task shown in figure  [ fig : corridor_template ] .",
    "the states belonging to a set are marked with their action in the final policy ; the empty grid cells are not part of the set .",
    "each arrow shows only the main intended direction of the action ; we do not show the left - rotating noise option and the right - rotating noise option .",
    "goal cells that are assigned the finish action by the final policy are marked with the `` f '' sign.,title=\"fig:\",scaledwidth=100.0% ]      in addition to the concrete grid tasks of section  [ sub : grid ] , we have also considered a slightly more abstract form of task , that we call chain task . the general form of a chain task is shown in figure  [ fig : chain ] .",
    "the parameter @xmath390 tells us how long the chain is ; the states of the chain are , in order , @xmath391 and one final state @xmath392 . to obtain reward from the start state ,",
    "we should in the worst case perform all actions @xmath393 , in sequence , finished by one arbitrary action . for each @xmath192 ,",
    "the action @xmath394 should be applied to state @xmath105 .",
    "but there is forward nondeterminism that , for each pair @xmath395 could send us to an arbitrary state later in the chain , closer to state @xmath392 .",
    "also , there are backward deterministic transitions that take us back to the start state whenever we apply the wrong action to a state . formally , for a fixed value of @xmath33",
    ", we obtain a graph @xmath396 , defined as follows : @xmath397 , @xmath45 , @xmath398 , @xmath399 , and regarding @xmath16 , we define      note that for each @xmath33 , the task @xmath404 is reducible : conceptually , the reducibility iterations first assign action @xmath405 to state @xmath390 , then action @xmath406 to state @xmath407 , and so on , until state @xmath50 is assigned action @xmath408 .",
    "denotes the set of all actions @xmath409 , and the symbol @xmath410 with @xmath192 stands for the set of all actions except action @xmath394 .",
    "the graphical notation of tasks is explained in figure  [ fig : reduce].,scaledwidth=90.0% ]        using the same experimental procedure as for grid tasks , we have simulated runs for some chain lengths . for each chain length separately , we have simulated 400 runs , and we have computed the @xmath387-quantile of the measured convergence - trial indexes . by plotting the resulting empirical upper bound on the convergence - trial index against the chain length , we arrive at figure  [ fig : chain_convergence ] .",
    "we see that the convergence - trial index rises faster than linear in terms of the chain length ; this is in contrast to figure  [ fig : grid_convergence ] for grid corridors .",
    "one possible explanation , is that the forward nondeterminism on the chain causes each state to be visited less frequently in a simulated run . for a fixed length @xmath390 ,",
    "the effect is that a state @xmath411 could stay connected longer to a bad action , leading back to the start state @xmath50 .",
    "of course , at the end of each trial , the start state should be connected to action @xmath408 , because otherwise no progress could have been made ; but the other states could in principle have any action .",
    "so , we might not yet encounter the final policy for a long time , as recognizable through the syntactic characterization of theorem  [ theo : final ] .      using the same experimental procedure as for grid tasks , for the fixed chain length of @xmath412",
    ", we have simulated the first 2000 trials of 1000 runs , and we have computed the @xmath387-quantile on trial length as explained for the grid corridor experiment . by plotting the resulting empirical upper bound on trial length against the trial index , we arrive at figure  [ fig : chain_trial_length ] .",
    "again , this figure suggests that the learning algorithm is able to gradually improve the policy over trials .",
    "we describe the form of the final policy , rather than visualizing it , because the form is very restricted .",
    "consider the final policy @xmath59 in a simulated run of the chain task with length @xmath33 . in the simulation ,",
    "we know that each trial should have ended with action @xmath408 assigned to state @xmath50 because otherwise the state @xmath392 could not have been reached .",
    "this property also applies to the convergence - trial , so @xmath413 .",
    "hence , @xmath175 is the set of all states due to the forward nondeterminism along the chain .",
    "we recall from theorem  [ theo : final ] that the final policy satisfies @xmath190 . therefore , @xmath183 is also the set of all states .",
    "we can now see that the final policy @xmath59 has to satisfy @xmath414 for all @xmath192 . towards a contradiction ,",
    "let @xmath105 be the largest state number for which @xmath415 . then , using the iterations for computing @xmath183 , we can see that @xmath416 but @xmath417 . is started with the set @xmath418 .",
    "] this would be the desired contradiction , because @xmath183 is the set of all states .",
    "we compute the quantiles with the statistics package r. for completeness , we recall here the definition of quantiles that we have used in our analysis ; this definition is called definition 1 by @xcite .",
    "let @xmath421 be a nonempty list of numbers , possibly containing duplicates .",
    "let @xmath422 denote the length of @xmath421 .",
    "for an index @xmath423 , we write @xmath424}$ ] to denote the number at index @xmath105 in @xmath421 .",
    "we write @xmath425 to denote the ordered version of @xmath421 , where the numbers are sorted in ascending fashion ; we keep duplicates , so @xmath426 .",
    "let @xmath427 with @xmath383 .",
    "the @xmath382-quantile of a nonempty list @xmath421 of numbers , denoted @xmath428 is defined as follows : denoting @xmath429 where @xmath430 , @xmath431 } & \\text{if } p\\cdot n - j > 0\\\\             { { { \\mathit{order}}(l)}[j ] } & \\text{if } p\\cdot n - j = 0 .",
    "\\end{cases}\\ ] ]    intuitively , using the index @xmath429 in the ordered list @xmath425 is a good attempt at finding a number @xmath432 such that a fraction @xmath382 of @xmath421 is smaller than or equal to @xmath432 .",
    "the assumptions @xmath433 and @xmath434 ensure that we only apply valid indexes in the range @xmath435 to the list @xmath425 .",
    "the simulation was written with java development kit 8 . in our experimental results ,",
    "we have measured only the number of transitions and the number trials in runs . since no wall - clock time was needed , the exact running time of the simulation in seconds was not measured . during every transition , we have used a uniform sampling from the possible successor states . concretely , given an array of successor states , we have used the function ` math.random ( ) ` to generate random indexes in this array , as follows : the double precision number returned by ` math.random ( ) ` can be converted into an integer by multiplying with the array length and subsequently truncating the resulting number with the ` math.floor ( ) ` function .",
    "we have studied the fascinating idea of reinforcement learning in a non - numeric framework , where the focus lies on the interaction between the graph structure of the task and a learning algorithm .",
    "we have studied the graph property of reducibility , that implies the existence of a policy that makes steady progress towards reward despite nondeterminism in the task .",
    "interestingly , reducibility , combined with a natural fairness assumption , enables our simple learning algorithm to learn the task .",
    "we have also characterized the final policy for converging runs , which allows the precise detection of the convergence - trial in simulations .",
    "we now discuss some avenues for further work .",
    "we have seen a sufficient property ( theorem  [ theo : algvisit ] ) and necessary properties ( proposition  [ prop : necessary ] ) for tasks to be learnable under fairness .",
    "the gap between the sufficient and necessary properties seems strongly related to the unstable set of a task ( see section  [ sub : necessary ] ) .",
    "perhaps it is possible to characterize the tasks that are learnable under fairness  by imposing some additional constraints on the manner by which the unstable set is connected to the reducible states , in addition to the already identified necessary properties .",
    "related to remark  [ remark : theo - algvisit ] and to the simulations in section  [ sec : examples ] , one could try to theoretically provide an upper bound on the convergence - trial index , for some class of tasks .",
    "we could also make assumptions regarding the probability distributions underlying the random actions proposed for a state and the choice of successor states when applying an action . for a given task",
    ", the result could be a probability distribution on the convergence - trial index , or on the total number of transitions before convergence .      in some models of biologically plausible learning ,",
    "the activation between any pair of connected neurons is represented by an eligibility trace  @xcite . when obtaining reward , the value of this trace is applied to the synaptic weight between the neurons . in realistic scenarios ,",
    "the traces fade , with the advantage that a simulation or practical setup does not have to keep remembering all the information of past states before obtaining reward .",
    "the current article may be viewed as studying eligibility traces that are non - fading , because the working memory of the cycle - detection learning algorithm is effectively stored until the end of the trial .",
    "it appears interesting to let the working memory fade , perhaps by modeling the working memory as a first - in - first - out queue , where a newly entering state would remove the oldest state from the queue once a size limit has been reached .      in this article ,",
    "any path through the state space from a given start state to reward is good .",
    "however , in some applications we want to avoid certain states .",
    "for example , in a navigation task , an organism might want to reach its nest from a certain starting location , but on the way to the nest the organism should avoid hazardous locations like pits or swamps .",
    "it appears interesting to formally investigate such cases .",
    "information about hazards could be incorporated by extending the framework of this article with an explicit set @xmath419 in tasks , which contains the state - action pairs which should be avoided ; this is the opposite of the set @xmath14 .",
    "the framework could further be extended to differentiate between trials that terminate with reward and trials that terminate with a hazard .",
    "a run could be said to convergence if eventually all trials terminate with reward , and all states eventually become stable .",
    "the framework studied in this article provides complete information to the learning algorithm because each individual state can be mapped to its own action . in real - world applications , such as robot navigation  @xcite",
    ", the agent can only work with limited sensory information available in each time step . in that case , the agent should first build concepts for states in order to differentiate them .",
    "these concepts should be made by remembering sensory information over time , and in general multiple states will remain grouped together under the same concept because sensory information is not sufficiently accurate to differentiate between all states .",
    "this issue was also raised as an item for future work by @xcite , who initially also have considered a framework in which complete information is available to the learning agent .",
    "there is ongoing work on partially observable tasks , see e.g.  @xcite .",
    "building concepts is related to the problem of generalization  @xcite because in real - world tasks there might be too many states to store in the policy .",
    "it would be useful to collect states in conceptual groups and then assign an action to each group .",
    "bonet , b. and geffner , h. ( 2006 ) .",
    "learning depth - first search : a unified approach to heuristic search in deterministic and non - deterministic settings , and its application to mdps . in _ proceedings of the sixteenth international conference on automated planning and scheduling _ ,",
    "pages 142151 .",
    "littman , m. , cassandra , a. , and kaelbling , l. ( 1995 ) .",
    "learning policies for partially observable environments : scaling up . in _ proceedings of the twelfth international conference on machine learning _ , pages 362370 .",
    "vasilaki , e. , frmaux , n. , urbanczik , r. , senn , w. , and gerstner , w. ( 2009 ) .",
    "spike - based reinforcement learning in continuous state and action space : when policy gradient methods fail .",
    ", 5(12):e1000586 ."
  ],
  "abstract_text": [
    "<S> we consider a reinforcement learning framework where agents have to navigate from start states to goal states . we prove convergence of a cycle - detection learning algorithm on a class of tasks that we call reducible . </S>",
    "<S> reducible tasks have an acyclic solution . </S>",
    "<S> we also syntactically characterize the form of the final policy . </S>",
    "<S> this characterization can be used to precisely detect the convergence point in a simulation . </S>",
    "<S> our result demonstrates that even simple algorithms can be successful in learning a large class of nontrivial tasks . </S>",
    "<S> in addition , our framework is elementary in the sense that we only use basic concepts to formally prove convergence . </S>"
  ]
}