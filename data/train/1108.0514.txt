{
  "article_text": [
    "artificial neural networks are now becoming a more popular tools for handling astronomical data .",
    "it is very important to have some automatic means of analyzing large databases like the surveys and upcoming space missions which would release terabytes of data to the community .",
    "that is why , anns are widely used as an automatic tools for analyzing astronomical data .",
    "some of the previous attempts on anns in astronomy are : @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite .",
    "artificial neural network techniques used in the field of astronomy have been mostly supervised algorithms . here",
    "we introduce an application of unsupervised technique to identify different classes of objects and try to make a cluster of similar type of stars using this technique .",
    "the @xcite library is used for classification using som algorithm as unsupervised ann .",
    "this algorithm configures output into a topological presentation of the original multi - dimensional data , producing a som in which input vectors with similar features are mapped to the same map unit or nearby units . in this way at the output map we will have the clusters of similar objects at different positions of the map .",
    "the expected clusters in this case are the stellar spectral type ranging from o to m.    the self- organizing map algorithm is explained in section 2 . in section 3",
    "we describe the input data and their preprocessing .",
    "the result of classification and discussion are presented in sections 4 and 5 respectively .",
    "self - organizing map neural net developed by @xcite is important from many point of view as it pays attention to spatial order unlike the traditional neural models and that is why this technique acquired a very special position in neural network theory .",
    "the following section is the description of this technique in details .",
    "neural network learning is not restricted to supervised learning , wherein training pairs are provided , that is , input and target output pairs .",
    "a second major type of learning for neural network is unsupervised learning , in which the net seeks to find patterns or regularities in the input data .",
    "self - organizing map ( som ) , developed by kohonen , groups the input data clusters , a common use for unsupervised learning .",
    "the adaptive resonance theory networks , are also clustering type networks .",
    "is the assigned weight vector of the unit on the map ]    self - organizing map consists of two layers : a one dimensional input layer and a two dimensional competitive layer , organized as a 2d grid of units as shown in the figure [ somnetwork ] .",
    "som lattice structure composed of @xmath0 neural units .",
    "this layer can neither be called hidden nor an output layer .",
    "each input is connected to all output neurons in the map .",
    "a weight vector with the same dimensionality as the input vectors is attached to every neuron in the map .",
    "the learning algorithm for the som accomplishes two important things :    1 .   clustering the input data 2 .",
    "spatial ordering of the map so that input patterns tend to produce a response in units that are close to each other in the grid .    since every unit in the competitive layer represents a cluster , the number of clusters that can be formed , will be limited by the number of units in the competitive layer .    the weight vector for a competitive layer unit in a clustering net serves as a representative , exemplar , or codebook vector for the input patterns , which the net has placed on that cluster .",
    "while training , the net determines the output units that is the best match for the current input vector ; then the weight vector of the winner is adjusted according to net s learning algorithm .",
    "the beauty of the self - organizing maps is their ability to find regularities and correlations in the input layer and group them into vectors without any external adjustment or prior knowledge of the expected outcomes .",
    "kohonen s som is one of the most referenced mapping techniques because of its ability to flatten high dimensional input into two or three dimensional data .",
    "one of the most important attribute of kohonen s som is that , it does data compression without loss to relative distance between data points .",
    "the neurons are connected to adjacent neuron by a neighborhood relation , which decides the topology , or structure , of the map .",
    "the figure [ lattice ] shows the rectangular and hexagonal lattice structures and their discrete neighborhoods ( size 0,1 and 2 ) of the centermost unit .",
    "the som is trained iteratively . in each training step",
    "one vector @xmath1 from the input data set is randomly chosen and then the distance between this vector and all the weight vectors of the map will be calculated and this distance measurement , typically is an euclidian distance .",
    "the neuron in the map whose weight vector is closest to the input vector , @xmath1 , is called best - matching unit ( bmu ) , here denoted by c :    @xmath2    after finding the bmu , the weight vector of the map are updated such that the bmu is moved closer to the input vector in the input space .",
    "the topological neighbors of the bmu are treated similarly .",
    "this process of adaptation of weight vectors intern moves the bmu and its topological neighbors towards the input sample vector @xmath1 .",
    "the updating scheme aims at performing a stronger weight adaptation at the bmu location than in its neighborhood .",
    "this update rule in som for the weight vector of _ i _ is : @xmath3 , \\ ] ] where @xmath4 presents the time , @xmath5 is randomly chosen an input vector from the input data set at time @xmath4 , @xmath6 is the neighborhood kernel around the @xmath7 unit @xmath8 and @xmath9 is the learning rate at time @xmath4 .",
    "the function @xmath6 has a very important role ; it is defined over the lattice points . to achieve convergence",
    "it is necessary that @xmath10 as t@xmath11 .",
    "classically , a gaussian function is used , leading to :    @xmath12    here , the euclidian norm is chosen and @xmath13 is the 2d location for the @xmath14 neuron in the network . @xmath15 species the width of the neighborhood during time @xmath4 .",
    "the learning rate @xmath16 must always be selected much less than 1 , typically at most 0.4 .",
    "best results are usually achieved by slowly decreasing it as training progresses , the radius of the neighborhood around a cluster unit ( winner ) also decreases as the clustering process progresses , this decreasing of the learning rate and radius of neighborhood must be monotonic for better performance of the network .",
    "up to a point , large learning rate will indeed make training process faster .",
    "if the learning rate selected to be very large then convergence may never occur , and in this case weight vector may oscillate highly and it can lead to instability .",
    "the @xcite library is used as a data set for the classification and input to the som network .",
    "the jhc library covers the wavelength range of 3510 - 7427.2    for various o to m stellar type and luminosity classes v , iii , and i. this library contains 161 spectra of individual stars , we selected 158 of them as an input to the som classifier .",
    "the jhc library has a resolution of 4.5    with one sample per 1.4   .",
    "each spectrum of this library is having 2799 number of data points and they are normalized to the unity before feeding into the network .",
    "the som technique is actually mapping 2799 dimensional data into two dimensional output map .",
    "the 158 input spectra are labled as the jhc s atlas list , specifying their spectral , subspectral and luminosity types .",
    "these lables are require while analysing the output map .",
    "the lables are placed beside every node ( neuron ) on the output map which indicates , that specific neuron is activated by the corresponding input spectra .",
    "hence we can point out the location of each input spectra in the map and study the behavior of the network in making clusters of similar spectral type .",
    "in this work the classification of the library of stellar spectra , @xcite using unsupervised artificial neural network , self - organizing map algorithm ( som ) is presented .",
    "the 158 spectra of the jhc library with 2799 data points each , are the input vectors of the network and mapped into two dimensional output grid .",
    "a problem with large soms is that when the map size is increased , the time it takes to do any operations on the map increases linearly .",
    "when the grid size becomes very large , finding the best matching unit takes longer and longer .",
    "the size of the som determines the resolution of the visualization it produces . on a small som",
    ", lots of data samples will be projected to each som unit , whereas on a large som the similarity relationships of the samples are more readily visible .",
    "the user must select the map size in advance .",
    "this may lead to many experiments with different sized maps , trying to obtain the optimal result .",
    "different map size and itterations are tried out and their performances analysed some of which are given in table 1 .",
    "the map size of the @xmath17 with highest performance is selected for detailed analysis .",
    ".list of map sizes with number if itterations and their performances [ cols=\"^,^,^\",options=\"header \" , ]",
    "as it is seen from table 2 and figure [ som3 ] misclassified patterns belong to their neighboring clusters . for example",
    "12b0v which is misclassified as o spectral type , is at the neighburhood of its true class .",
    "likewise if we check all the misclassified patterns , we see that they are misclassified to their neighbouring true classes .",
    "the jhc libraray is very detailed and it also includes the subspectral types from 0 - 9 . the misclassified patterns could be looked in detail to consider the subspectral types as well .",
    "it is found that the misclassified patterns are very close to their true class .",
    "for example , 80f0iii is wrongly classified as spectral type a , but it is close to the neighbouring class , 30a9v .",
    "this is true with most of the misclassified patterns and it shows that the classification accuracy can be higher than 92.4% .",
    "this technique and algorithm can be used for even larger unknown set of stellar spectra .",
    "specially where we have different astronomical objects and need a pre - classification to identify the object types first , and then do further analysis on each cluster .",
    "the beauty of this algorithm is that we do nt need any exemplars to train the network but it finds the similarities by its own without any supervision and makes the clusters of similar objects .",
    "the author wishes to thank lukasz wyrzykowski for his constructive comments ."
  ],
  "abstract_text": [
    "<S> we present an automatic , fast , accurate and robust method of classifying astronomical objects . </S>",
    "<S> the self organizing map ( som ) as an unsupervised artificial neural network ( ann ) algorithm is used for classification of stellar spectra of stars . </S>",
    "<S> the som is used to make clusters of different spectral classes of jacoby , hunter and christian ( jhc ) library . </S>",
    "<S> this ann technique needs no training examples and the stellar spectral data sets are directly fed to the network for the classification . </S>",
    "<S> the jhc library contains 161 spectra out of which , 158 spectra are selected for the classification . </S>",
    "<S> these 158 spectra are input vectors to the network and mapped into a two dimensional output grid . </S>",
    "<S> the input vectors close to each other are mapped into the same or neighboring neurons in the output space . </S>",
    "<S> so , the similar objects are making clusters in the output map and making it easy to analyze high dimensional data .    after running the som algorithm on 158 stellar spectra , with 2799 data points each , </S>",
    "<S> the output map is analyzed and found that , there are 7 clusters in the output map corresponding to o to m stellar type . </S>",
    "<S> but , there are 12 misclassifications out of 158 and all of them are misclassified into the neighborhood of correct clusters which gives a success rate of about 92.4% . </S>"
  ]
}