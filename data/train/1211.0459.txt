{
  "article_text": [
    "covariance matrix estimation is of fundamental importance in multivariate analysis . driven by a wide range of applications in science and engineering , the high - dimensional setting , where the dimension @xmath0 can be much larger than the sample size @xmath1 , is of particular current interest . in such a setting , conventional methods and results based on fixed @xmath0 and large @xmath1",
    "are no longer applicable , and in particular , the commonly used sample covariance matrix and normal maximum likelihood estimate perform poorly .",
    "a number of regularization methods , including banding , tapering , thresholding and @xmath2 minimization , have been developed in recent years for estimating a large covariance matrix or its inverse .",
    "see , for example , @xcite , @xcite , @xcite , @xcite , bickel and levina ( @xcite ) , @xcite , @xcite , friedman , hastie and tibshirani ( @xcite ) , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite and @xcite , among many others .",
    "let @xmath3 be @xmath1 independent copies of a @xmath0 dimensional gaussian random vector @xmath4 .",
    "the goal is to estimate the covariance matrix @xmath5 and its inverse @xmath6 based on the sample @xmath7 .",
    "it is now well known that the usual sample covariance matrix @xmath8 where @xmath9 , is not a consistent estimator of the covariance matrix  @xmath5 when @xmath10 , and structural assumptions are required in order to estimate @xmath5 consistently .",
    "one of the most commonly considered classes of covariance matrices is the `` bandable '' matrices , where the entries of the matrix decay as they move away from the diagonal .",
    "more specifically , consider the following class of covariance matrices introduced in @xcite : @xmath11 \\\\[-8pt ] \\nonumber & & \\hspace*{80pt}\\mbox{and } 0 < m_0^{-1 } \\le \\lambda_{\\min } ( \\sigma ) , \\lambda_{\\max } ( \\sigma)\\le m_0 \\biggr\\}.\\end{aligned}\\ ] ] such a family of covariance matrices naturally arises in a number of settings , including temporal or spatial data analysis .",
    "see @xcite for further discussions .",
    "several regularization methods have been introduced for estimating a bandable covariance matrix @xmath12 .",
    "@xcite suggested banding the sample covariance matrix @xmath13 and estimating @xmath5 by @xmath14 where @xmath15 is a banding matrix @xmath16 and @xmath17 represents the schur product , that is , @xmath18 for two matrices of the same dimensions .",
    "see figure  [ figbandtaper](a ) for an illustration .",
    "@xcite proposed to choose @xmath19 and showed that the resulting banding estimator attains the rate of convergence @xmath20 uniformly over @xmath21 , where @xmath22 stands for the spectral norm .",
    "this result indicates that even when @xmath10 , it is still possible to consistently estimate @xmath12 , so long as @xmath23 .    @xcite established the minimax rate of convergence for estimation over @xmath21 and introduced a tapering estimator @xmath24 where the tapering matrix @xmath25 is given by @xmath26 with @xmath27 .",
    "see figure  [ figbandtaper](b ) for an illustration .",
    "it was shown that the tapering estimator @xmath28 with @xmath29 achieves the rate of convergence @xmath30 uniformly over @xmath21 , which is always faster than the rate in ( [ blrate ] ) .",
    "this implies that the rate of convergence given in  ( [ blrate ] ) for the banding estimator with @xmath31 is in fact sub - optimal .",
    "furthermore , a lower bound argument was given in @xcite which showed that the rate of convergence given in ( [ optrate ] ) is indeed optimal for estimating the covariance matrices over  @xmath21 .",
    "[ cols=\"^,^ \" , ]      the rationale behind our block thresholding approach is that although the sample covariance matrix @xmath13 is not a reliable estimator of @xmath5 , its submatrix , @xmath32 , could still be a good estimate of  @xmath33 .",
    "this observation is formalized in the following theorem .",
    "[ thunion ] there exists an absolute constant @xmath34 such that for all @xmath35 , @xmath36 in particular , we can take @xmath37 .",
    "theorem  [ thunion ] enables one to bound the estimation error @xmath38 block by block .",
    "note that larger blocks are necessarily far away from the diagonal by construction .",
    "for bandable matrices , this means that larger blocks are necessarily small in the spectral norm . from theorem  [ thunion ] , if @xmath39 , with overwhelming probability , @xmath40 for blocks with sufficiently large sizes .",
    "as we shall show in section  [ secproof ] , @xmath41 and @xmath42 in the above inequality can be replaced by their respective sample counterparts .",
    "this observation suggests that larger blocks are shrunken to zero with our proposed block thresholding procedure , which is essential in establishing  ( [ eqlarge ] ) .",
    "the treatment of smaller blocks is more complicated . in light of theorem  [ thunion ] ,",
    "blocks of smaller sizes can be estimated well , that is , @xmath32 is close to @xmath33 for @xmath43 of smaller sizes . to translate the closeness in such a blockwise fashion into the closeness in terms of the whole covariance matrix",
    ", we need a simple yet useful result based on a matrix norm compression transform .",
    "we shall now present a so - called norm compression inequality which is particularly useful for analyzing the properties of the block thresholding estimators .",
    "we begin by introducing a matrix norm compression transform .",
    "let @xmath44 be a @xmath45 symmetric matrix , and let @xmath46 be positive integers such that @xmath47 .",
    "the matrix @xmath44 can then be partitioned in a block form as @xmath48 where @xmath49 is a @xmath50 submatrix .",
    "we shall call such a partition of the matrix @xmath44 a regular partition and the blocks @xmath49 regular blocks . denote by @xmath51 a norm compression transform @xmath52",
    "the following theorem shows that such a norm compression transform does not decrease the matrix norm .",
    "[ thcomp ] for any @xmath45 matrix @xmath44 and block sizes @xmath53 such that @xmath47 , @xmath54    together with theorems  [ thunion ] and  [ thcomp ] provides a very useful tool for bounding @xmath55 .",
    "note first that theorem  [ thcomp ] only applies to a regular partition , that is , the divisions of the rows and columns are the same .",
    "it is clear that @xmath56 corresponds to regular blocks of size @xmath57 with the possible exception of the last row and column which can be of a different size , that is , @xmath58 .",
    "hence , theorem  [ thcomp ] can be directly applied .",
    "however , this is no longer the case when @xmath59 .    to take advantage of theorem  [ thcomp ] ,",
    "a new blocking scheme is needed for @xmath60 .",
    "consider the case when @xmath61 .",
    "it is clear that @xmath62 does not form a regular blocking .",
    "but we can form new blocks with @xmath58 , that is , half the size of the original blocks in @xmath63 .",
    "denote by the collection of the new blocks @xmath64 .",
    "it is clear that under this new blocking scheme , each block @xmath43 of size @xmath65 consists of four elements from @xmath64 .",
    "thus @xmath66    applying theorem  [ thcomp ] to the regular blocks @xmath64 yields @xmath67 which can be further bounded by @xmath68 where @xmath69 stands for the matrix @xmath2 norm .",
    "observe that each row or column of @xmath70 has at most 12 nonzero entries , and each entry is bounded by @xmath71 because @xmath72 implies @xmath73 .",
    "this property suggests that @xmath74 can be controlled in a block - by - block fashion .",
    "this can be done using the concentration inequalities given in section  [ secconcentration ] .",
    "the case when @xmath75 can be treated similarly .",
    "let @xmath76 and @xmath77 for @xmath78 it is not hard to see that each block @xmath43 in @xmath79 of size @xmath80 occupies up to four blocks in this regular blocking . and following the same argument as before , we can derive bounds for @xmath81 . the detailed proofs of theorems  [ thmain ] and  [ thmain1 ] are given in section  [ secproof ] .",
    "the block thresholding estimator @xmath82 proposed in section  [ secmeth ] is easy to implement . in this section",
    "we turn to the numerical performance of the estimator .",
    "the simulation study further illustrates the merits of the proposed block thresholding estimator .",
    "the performance is relatively insensitive to the choice of @xmath83 , and we shall focus on @xmath84 throughout this section for brevity .",
    "we consider two different sets of covariance matrices .",
    "the setting of our first set of numerical experiments is similar to those from @xcite .",
    "specifically , the true covariance matrix @xmath85 is of the form @xmath86 where the value of @xmath87 is set to be @xmath88 to ensure positive definiteness of all covariance matrices , and @xmath89 are independently sampled from a uniform distribution between @xmath90 and @xmath91 .",
    "the second settings are slightly more complicated , and the covariance matrix @xmath5 is randomly generated as follows .",
    "we first simulate a symmetric matrix @xmath92 whose diagonal entries are zero and off - diagonal entries @xmath93 ( @xmath94 ) are independently generated as @xmath95 .",
    "let @xmath96 be its smallest eigenvalue .",
    "the covariance matrix @xmath5 is then set to be @xmath97 to ensure its positive definiteness .",
    "for each setting , four different combinations of @xmath0 and @xmath1 are considered , @xmath98 and @xmath99 , and for each combination , 200 simulated datasets are generated . on each simulated dataset , we apply the proposed block thresholding procedure with @xmath100 . for comparison purposes , we also use the banding estimator of @xcite and tapering estimator of @xcite on the simulated datasets . for both estimators",
    ", a tuning parameter @xmath101 needs to be chosen .",
    "the two estimators perform similarly for the similar values of @xmath101 . for brevity , we report only the results for the tapering estimator because it is known to be rate optimal if @xmath101 is appropriately selected based on the true parameter space .",
    "it is clear that for both our settings , @xmath12 with @xmath102 .",
    "but such knowledge would be absent in practice . to demonstrate the importance of knowing the true parameter space for these estimators and consequently the necessity of an adaptive estimator such as the one proposed here , we apply the estimators with five different values of @xmath103 , @xmath104 and @xmath91 .",
    "we chose @xmath105 for the tapering estimator following @xcite.the performance of these estimators is summarized in figures  [ figsimres ] and  [ fignewsimres ] for the two settings , respectively .     and dimension @xmath0 .",
    "in each panel , boxplots of the estimation errors , measured in terms of the spectral norm , are given for the block thresholding estimator with @xmath100 and the tapering estimator with @xmath106 , @xmath107 and @xmath91 . ]     and dimension @xmath0 . in each panel , boxplots of the estimation errors , measured in terms of the spectral norm , are given for the block thresholding estimator with @xmath100 and the tapering estimator with @xmath106 , @xmath107 and @xmath91 . ]",
    "it can be seen in both settings that the numerical performance of the tapering estimators critically depends on the specification of the decay rate @xmath103 .",
    "mis - specifying  @xmath103 could lead to rather poor performance by the tapering estimators .",
    "it is perhaps not surprising to observe that the tapering estimator with @xmath102 performed the best among all estimators since it correctly specifies the true decay rate and therefore , in a certain sense , made use of the information that may not be known a priori in practice .",
    "in contrast , the proposed block thresholding estimator yields competitive performance while not using such information .",
    "in this paper we introduced a fully data - driven covariance matrix estimator by blockwise thresholding of the sample covariance matrix .",
    "the estimator simultaneously attains the optimal rate of convergence for estimating bandable covariance matrices over the full range of the parameter spaces @xmath21 for all @xmath108 .",
    "the estimator also performs well numerically .",
    "as noted in section  [ thresholdingsec ] , the choice of the thresholding constant @xmath100 is based on our theoretical and numerical studies .",
    "similar to wavelet thresholding in nonparametric function estimation , in principle other choices of @xmath109 can also be used .",
    "for example , the adaptivity results on the block thresholding estimator holds as long as @xmath110 where the value @xmath111 comes from the concentration inequality given in theorem  [ thunion ] .",
    "our experience suggests the performance of the block thresholding estimator is relatively insensitive to a small change of @xmath109 .",
    "however , numerically the estimator can sometimes be further improved by using data - dependent choices of @xmath109 . throughout the paper ,",
    "we have focused on the gaussian case for ease of exposition and to allow for the most clear description of the block thresholding estimator .",
    "the method and the results can also be extended to more general subgaussian distributions .",
    "suppose that the distribution of the @xmath112 s is subgaussian in the sense that there exists a constant @xmath113 such that @xmath114 let @xmath115 denote the collection of distributions satisfying both  ( [ paraspace ] ) and  ( [ subgau ] ) . then for any given @xmath116 , the block thresholding estimator @xmath82 adaptively attains the optimal rate of convergence over @xmath115 for all  @xmath103 , @xmath117 and @xmath118 whenever @xmath109 is chosen sufficiently large .    in this paper",
    "we have focused on estimation under the spectral norm . the block thresholding procedure ,",
    "however , can be naturally extended to achieve adaption under other matrix norms .",
    "consider , for example , the frobenius norm . in this case , it is natural and also necessary to threshold the blocks based on their respective frobenius norms instead of the spectral norms .",
    "then following a similar argument as before , it can be shown that this frobenius norm based block thresholding estimator can adaptively achieve the minimax rate of convergence over every @xmath21 for all @xmath108 .",
    "it should also be noted that adaptive estimation under the frobenius norm is a much easier problem because the squared frobenius norm is entrywise decomposable , and the matrix can then be estimated well row by row or column by column .",
    "for example , applying a suitable block thresholding procedure for sequence estimation to the sample covariance matrix , row - by - row would also lead to an adaptive covariance matrix estimator .",
    "the block thresholding approach can also be used for estimating sparse covariance matrices . a major difference in this case from that of estimating bandable covariance matrices",
    "is that the block sizes can not be too large . with suitable choices of the block size and thresholding level",
    ", a fully data - driven block thresholding estimator can be shown to be rate - optimal for estimating sparse covariance matrices .",
    "we shall report the details of these results elsewhere .",
    "in this section we shall first prove theorems  [ thunion ] and [ thcomp ] and then prove the main results , theorems  [ thmain ] and [ thmain1 ] . the proofs of some additional technical lemmas are given at the end of the section .",
    "let @xmath124 and @xmath125 be @xmath1 independent copies of @xmath126 .",
    "let @xmath127 be its sample covariance matrix .",
    "it is clear that @xmath128 .",
    "hence @xmath129 note that @xmath130 therefore , @xmath131 observe that @xmath132 similarly , @xmath133 the proof is now complete .      without loss of generality , assume that @xmath136 .",
    "let @xmath44 be a @xmath137 matrix , @xmath138 and @xmath139 where @xmath140 is the unit sphere in the @xmath141 dimensional euclidean space .",
    "observe that @xmath142 where as before , we use @xmath22 to represent the spectral norm for a matrix and @xmath143 norm for a vector .",
    "as shown by brczky and wintsche [ ( @xcite ) , e.g. , corollary  1.2 ] , there exists an @xmath144-cover set @xmath145 of @xmath140 such that @xmath146 for some absolute constant @xmath147 .",
    "note that @xmath148 in other words , @xmath149    now consider @xmath150 .",
    "let @xmath151 and @xmath152 .",
    "then @xmath153 where @xmath154 similarly , @xmath155 .",
    "therefore , @xmath156 clearly the distributional properties of @xmath44 are invariant to the mean of @xmath157 . we shall therefore assume without loss of generality that @xmath158 in the rest of the proof .",
    "for any fixed @xmath159 , we have @xmath160 where @xmath161 , @xmath162 , and similarly , @xmath163 , @xmath164 .",
    "it is not hard to see that @xmath165 and @xmath166 is simply the difference between the sample and population covariance of @xmath167 and @xmath168 .",
    "we now appeal to the following lemma :    applying lemma  [ lecorr ] , we obtain @xmath169 where @xmath170 . by the tail bound for",
    "@xmath171 random variables , we have @xmath172 see , for example , lemma  1 of laurent and massart ( @xcite ) . in summary , @xmath173          denote by @xmath182 the left and right singular vectors corresponding to the leading singular value of @xmath44 , that is , @xmath183 .",
    "let @xmath184 and @xmath185 be partitioned in the same fashion as @xmath157 , for example , @xmath186 .",
    "denote by @xmath187 and @xmath188 .",
    "it is clear that @xmath189 .",
    "therefore , @xmath190      with the technical tools provided by theorems  [ thunion ] and [ thcomp ] , we now show that @xmath191 is an adaptive estimator of @xmath5 as claimed by theorem  [ thmain ] .",
    "we begin by establishing formal error bounds on the blocks using the technical tools introduced earlier .",
    "first treat the larger blocks . when @xmath12 , large blocks can all be shrunk to zero because they necessarily occur far away from the diagonal and therefore are small in spectral norm .",
    "more precisely , we have :      together with theorem  [ thunion ] , this suggests that @xmath195 with probability at least @xmath196 .",
    "therefore , when @xmath197 for a large enough constant @xmath147 , @xmath198 the following lemma indicates that we can further replace @xmath199 and @xmath200 by their respective sample counterparts .",
    "in the light of lemma  [ letune ] ,  ( [ eqshrunken ] ) implies that , with probability at least @xmath205 , for any @xmath192 such that @xmath206 and  ( [ eqdefbig ] ) holds , @xmath207 whenever @xmath208 is sufficiently large . in other words , with probability at least @xmath205 , for any @xmath192 such that ( [ eqdefbig ] ) holds , @xmath209 .",
    "observe that by the definition of @xmath191 , @xmath211 by lemma  [ letune ] , the spectral norm of @xmath212 and @xmath213 appeared in the first term on the rightmost - hand side can be replaced by their corresponding population counterparts , leading to @xmath214 where we used the fact that @xmath215 .",
    "this can then be readily bounded , thanks to theorem  [ thunion]:@xmath216 together with  ( [ eqncompsmall ] ) , we get @xmath217      to put the bounds on both small and big blocks together , we need only to choose an appropriate cutoff @xmath218 in  ( [ eqsmalllarge ] ) .",
    "in particular , we take @xmath219 where @xmath220 stands for the smallest integer that is no less than @xmath221 .      if @xmath222 , all blocks are small . from the bound",
    "derived for small blocks , for example , equation  ( [ eqsmallbd ] ) , we have @xmath223 with probability at least @xmath205 .",
    "hereafter we use @xmath224 as a generic constant that does not depend on @xmath0 , @xmath1 or @xmath103 , and its value may change at each appearance .",
    "thus @xmath225 it now suffices to show that the second term on the right - hand side is @xmath226 . by the cauchy ",
    "schwarz inequality , @xmath227 observe that @xmath228 where @xmath229 stands for the frobenius norm of a matrix .",
    "thus , @xmath230      when @xmath231 and @xmath232 , by the analysis from section  [ seclarge ] , all large blocks will be shrunk to zero with overwhelming probability , that is , @xmath233 when this happens , @xmath234 recall that @xmath69 stands for the matrix @xmath2 norm , that is , the maximum row sum of the absolute values of the entries of a matrix .",
    "hence , @xmath235 as a result , @xmath236 it remains to show that @xmath237 by the cauchy ",
    "schwarz inequality , @xmath238 observe that @xmath239 where the second inequality follows from the fact that @xmath240 or @xmath241 .",
    "it is not hard to see that @xmath242 on the other hand , @xmath243 therefore , @xmath244 together with theorem  [ thunion ] , we conclude that @xmath245      finally , when @xmath0 is very large in that @xmath246 , we can proceed in the same fashion .",
    "following the same argument as before , it can be shown that @xmath247 the smaller blocks can also be treated in a similar fashion as before . from equation  ( [ eqsmallbd ] ) , @xmath248 with probability at least @xmath205 .",
    "thus , it can be calculated that @xmath249 combining these bounds , we conclude that @xmath250 in summary , @xmath251 for all @xmath108 . in other words , the block thresholding estimator @xmath191 achieves the optimal rate of convergence simultaneously over every @xmath21 for all @xmath108 .",
    "observe that @xmath252 where @xmath253 denotes the smallest eigenvalue of a symmetric matrix . under the event that @xmath254 @xmath191 is positive definite and @xmath255 .",
    "note also that @xmath256 therefore , @xmath257 by theorem  [ thmain ] .",
    "on the other hand , @xmath258 note that @xmath259 it suffices to show that @xmath260      now consider the case when @xmath263 observe that for each @xmath264 , @xmath265 it can then be deduced from the norm compression inequality , in a similar spirit as before , that @xmath266 by the triangle inequality , @xmath267 and @xmath268 under the event that @xmath269 we have @xmath270 now by lemma  [ letail ] , @xmath271 for some constant @xmath147 , which concludes the proof .",
    "note that for any @xmath192 , there exists an integer @xmath274 such that @xmath275 .",
    "we proceed by induction on @xmath276 . when @xmath277 , it is clear by construction , blocks of size @xmath278 are at least one @xmath279 block away from the diagonal .",
    "see figure  [ figblock ] also .",
    "this implies that the statement is true for @xmath277 . from  @xmath280 to @xmath281 , one simply observes that all blocks of size @xmath282 is at least one @xmath283 block away from blocks of size @xmath284 .",
    "therefore , @xmath285 which implies the desired statement .",
    "we are now in position to prove lemma  [ leblocknorm ] which states that big blocks of the covariance matrix are small in spectral norm .",
    "recall that the matrix @xmath2 norm is defined as @xmath286 .",
    "similarly the matrix @xmath287 norm is defined as @xmath288 immediately from lemma  [ lemdist ] , we have @xmath289 which implies @xmath290 .      for any @xmath291 , write @xmath292 .",
    "then the entries of @xmath293 are independent standard normal random variables . from the concentration bounds on the random matrices [ see , e.g. , @xcite ] , we have @xmath294 with probability at least @xmath295 where @xmath296 is the sample covariance matrix of @xmath293 . applying the union bound to all @xmath291 yields that with probability at least @xmath297 , for all @xmath298 @xmath299 observe that @xmath300 thus @xmath301 which implies the desired statement ."
  ],
  "abstract_text": [
    "<S> estimation of large covariance matrices has drawn considerable recent attention , and the theoretical focus so far has mainly been on developing a minimax theory over a fixed parameter space . in this paper , we consider adaptive covariance matrix estimation where the goal is to construct a single procedure which is minimax rate optimal simultaneously over each parameter space in a large collection . </S>",
    "<S> a fully data - driven block thresholding estimator is proposed . </S>",
    "<S> the estimator is constructed by carefully dividing the sample covariance matrix into blocks and then simultaneously estimating the entries in a block by thresholding . </S>",
    "<S> the estimator is shown to be optimally rate adaptive over a wide range of bandable covariance matrices . </S>",
    "<S> a  simulation study is carried out and shows that the block thresholding estimator performs well numerically . </S>",
    "<S> some of the technical tools developed in this paper can also be of independent interest . </S>"
  ]
}