{
  "article_text": [
    "the beowulf project began in 1994 at the nasa goddard space flight center .",
    "more about the history of the project and the current status can be found at the web site http://www.beowulf.org .",
    "there are about 100 clusters listed on the beowulf home page ( which is clearly not a complete list ) . within the milc collaboration",
    ", we have access to at least five clusters at our universities .",
    "we also have done production work on six other clusters at national supercomputer centers .",
    "there are several advantageous characteristics often cited for clusters .",
    "chief among these is the use of commodity hardware to produce a very cost - effective computer .",
    "processors such as the intel pentia and celeron and the amd athlon or k6 , fastethernet network cards and switches have been used to build quite cost - effective machines . however ,",
    "other choices such as the compaq alpha processor and higher speed networks such as myrinet , giganet and quadrics qsnet have also been used to build very powerful clusters .",
    "another characteristic of clusters is the use of commodity software such as linux , gnu , mpich and pbs to keep software costs close to zero .",
    "a third advantage of the cluster approach is their programmability and flexibility .",
    "message passing interface or mpi , has become a standard in commercial parallel computers .",
    "the milc code had been compiled under mpi well before being run on a beowulf cluster .",
    "the port to beowulf required minimal effort .",
    "all of our pc cluster benchmarks have been done without any assembly code .",
    "practical calculations can be done on current clusters with a granularity that is well suited to fft routines .",
    "clusters have a community of users and developers .",
    "new system administration tools frequently become available , as do advances in parallel file systems , schedulers and other useful software .",
    "thus , one can take advantage of the vigor of the community and avoid spending a large amount of time developing software unrelated to the physics . finally ,",
    "because of the short design time of clusters , one can take quick advantage of the many developments in pc hardware .",
    "it is not necessary to lock oneself into a technology either well in advance of it s actually being available , or a well developed technology that will be outdated by the time a large system can be constructed and commissioned .",
    "one can often avoid the problem of having a single source for key items .",
    "if you can no longer get a particular motherboard , there will be another vendor with a similar ( or superior ) offering .",
    "there are also potential disadvantages of clusters . with a standard supercomputer",
    ", one can get a maintenance contract , and there is somebody to yell at when things go wrong .",
    "( however , as a long - time user of supercomputers , i know that having a vendor does nt assure that the problem will be fixed . )",
    "recently , a number of vendors have been selling clusters .",
    "so the problem of not having anyone to yell at may be avoided .",
    "of course , there is still no assurance that yelling ( or even asking politely ) will result in the problem being solved .",
    "another disadvantage of the cluster approach is that of having to rely on the design effort of others . if vendors are not producing hardware with the specifications that you need , you may not be able to build a well optimized system . on the other hand , most physicists are neither skilled at nor interested in vlsi design or pcb layout and would rather spend their time thinking about physics , so why not take advantage of the labor of computer engineers ?",
    "the indiana university physics department received $ 50,000 in 1998 to build a 32-node linux cluster .",
    "the machine we built is called candycane , which stands for cpus and network do your calculation and nothing else .",
    "candycane is an appropriate name because it was designed for the `` sweet spot , '' that is , components were picked to give the best price - performance ratio attainable .",
    "it is used by several research groups in the department , but usually only one or two jobs are running at the same time . in september 1998 , a four - node prototype cluster was built and tested .",
    "three different ethernet cards were tested to see if the higher priced cards could be justified by superior performance .",
    "detecting no difference in performance , we selected the least expensive card for the production cluster . in october , the purchasing department put out a request for bids on the desired components . in november , just before thanksgiving , the last of the components arrived .",
    "( several vendors were used to get the best price on each component . ) on the wednesday and friday of thanksgiving break , 34 nodes were built , the software was installed and everything was placed on shelves and connected .",
    "one node serves as a console , and one as a spare .",
    "the cost per node was $ 693 for a pentium ii 350 , with a 4.3 gb hard drive and 64 mb of ecc ram .",
    "each node has a floppy drive and a fastethernet card .",
    "however , the compute nodes have no keyboard , video card or cdrom .",
    "there are a few video cards and an extra keyboard that can be used if a node does not reboot on its own .",
    "the 40-port hp procurve switch cost about $ 2,000 , so the total cost was about $ 25,000 . currently , ( october , 2000 ) it would be possible to build this system for @xmath0$300 per node , or for approximately $ 11,500 .",
    "an even more attractive alternative would be a diskless athlon 600 mhz system for which the per node cost is about $ 250 .",
    "this node would have much better performance than the pii 350 ; however , the fastethernet would be a bottleneck on the milc code with kogut - susskind quarks .",
    "even so , a 32 node system with a minimum performance of 1280 and 1660 mflops , for @xmath1 and @xmath2 sites per node , respectively , could be built for under $ 10,000 .",
    "this works out to a cost / mf of between $ 6.00 and $ 7.80 .    in sec .  2",
    ", we describe the key issues for good performance and in sec .  3 , we present benchmarks for the milc code on various supercomputers and clusters .",
    "section 4 gives rough cost - performance ratios for a number of platforms .",
    "additional information about emerging technologies for clusters and user experiences can be found in the on - line presentations from a session that i organized at the march , 2000 aps meeting @xcite .",
    "a very simple approach to achieving good performance for domain decomposition codes like lattice qcd codes is to optimize single node performance and to try to avoid degrading performance too much when one has to communicate boundary values to neighboring nodes .",
    "the single node performance is likely to depend upon such issues as the quality of the cpu , the performance and size of cache(s ) , the bandwidth to main memory and the quality of the compiler . for message passing performance ,",
    "key issues are the latency , peak bandwidth , processor overhead and the message passing software .",
    "focusing first on single node performance , we note that it is easy to waste a lot of money on a poor system design . to illustrate this",
    ", we consider the various speed amd athlon processors available and their prices on a particular day .",
    "although we focus on athlon here , the same considerations apply to intel or other processors .",
    "figure [ fig : price_vs_speed ] shows that processor price is a rapidly increasing function of speed . in fig",
    "[ fig : value ] , we divide the price by the speed of the chip and see that the relative expense rises rapidly for the faster chips . at the time this graph was produced , there was an apparent sweet spot at 600 mhz .",
    "the faster chips have a higher price - performance ratio . depending upon the costs of the other components of the system",
    ", the entire system may have a higher or lower price - performance ratio .",
    "for our qcd codes , access to memory is quite important . with the benchmarks below we",
    "demonstrate that performance does not increase in proportion to the speed of the chip .",
    "this is because memory speed is fixed by the 100 mhz front side bus for both 500 mhz and 600 mhz athlons .",
    "this work was supported by the u.s .",
    "doe under grant de - fg02 - 91er 40661 .",
    "special thanks to the milc collaboration , and especially r.  sugar for reading the manuscript .",
    "we thank the albuquerque high performance computer center , indiana university , llnl , national center for supercomputing applications , pittsburgh supercomputer center and san diego supercomputer center ."
  ],
  "abstract_text": [
    "<S> since the development of the beowulf project to build a parallel computer from commodity pc components , there have been many such clusters built . the milc qcd code has been run on a variety of clusters and supercomputers . </S>",
    "<S> key design features are identified , and the cost effectiveness of clusters and supercomputers are compared . </S>"
  ]
}