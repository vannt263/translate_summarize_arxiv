{
  "article_text": [
    "given an undirected graph @xmath2 containing @xmath3 nodes , determining whether any simple cycles of length @xmath3 exist in the graph solves the hamiltonian cycle problem .",
    "simple cycles of length @xmath3 are known as hamiltonian cycles .",
    "this paper is concerned with finding a hamiltonian cycle ( hc ) of a graph by finding a global minimizer of a smooth function .",
    "we associate a variable @xmath4 with each ( directed ) arc @xmath5 .",
    "define a matrix @xmath0 , whose @xmath6th element is @xmath4 if @xmath5 , or 0 otherwise .",
    "it was shown in @xcite that a longest cycle of a graph is a global minimizer of the problem :    @xmath7      \\subject      &    p(x ) \\in \\mathcal{s } , \\quad x \\ge 0 ,   \\end{array}\\ ] ]    where @xmath8 is the set of stochastic matrices .",
    "we shall refer to the linear constraints that arise from this restriction on @xmath0 as the @xmath8 constraints .",
    "it follows we may also restrict @xmath9 , where @xmath10 is the set of doubly stochastic matrices , since if a hc exists and the solution to ( [ eqn - det ] ) is defined to be @xmath11 then @xmath12 is a permutation matrix .",
    "it is these two forms of the problem that we investigate .",
    "the elements of @xmath12 that are 1 denote the arcs in the hc .",
    "although we introduced @xmath1 as the nonzero elements of @xmath0 , in practice @xmath1 is a vector .",
    "we number the indices by row .",
    "so if there are 3 arcs from the first node there will be @xmath13 , @xmath14 and @xmath15 in the first row of @xmath0 .",
    "the first nonzero element of @xmath0 in the next row is @xmath16 and so on .",
    "@xmath0 is symmetric in the pattern of the nonzero elements but is not a symmetric matrix .",
    "the elements in the upper triangular half correspond to arcs in one direction and their lower triangular half reflection is the arc being taken in the opposite direction .",
    "there would be no reason not to label arcs so that the ( 1,2 ) element was always nonzero .",
    "the corresponding ( 2,1 ) element must be zero if @xmath17 at the solution is nonzero .",
    "however , it is possible for both to be zero .    when @xmath18 it is possible to replace the objective function in ( [ eqn - det ] ) by the negative determinant of the leading principal minor of @xmath19 .",
    "the result follows from among other things that the restrictions we place on @xmath1 and @xmath0 ensure the lu factorization of @xmath20 exists without the need to permute the rows or columns .",
    "also @xmath20 has rank @xmath21 and the leading principal minor is full rank .",
    "the proof was given in @xcite .",
    "unfortunately this does not hold when @xmath22 . using the leading principal minor",
    "has the advantage that the rank - one modification @xmath23 is not required , which makes calculating the gradient and the hessian a little simpler .",
    "a method for efficiently computing the gradient and hessian of the negative determinant of the leading principal minor was provided in @xcite , and was proved in @xcite to be more numerically stable than the objective function in ( [ eqn - det ] ) .",
    "another benefit is that the maximum value is independent of the size of the graph , eliminating the need to scale any parameters by the size of the graph .    in the @xmath10 case",
    "the problem of interest is of the form @xmath24 subject to @xmath25 where @xmath26 , @xmath27 is the leading principle minor of @xmath19 , @xmath28 is the set of nodes reachable in one step from node @xmath29 . constraints ( [ eq - ds1])([eq - ds3 ] ) are called the _ doubly - stochastic _ constraints . for neatness , we refer to constraints ( [ eq - ds1])([eq - ds3 ] ) as the @xmath10 constraints .",
    "it is assumed that any graph considered is simple and undirected .",
    "although this is a classical linearly constrained problem it is different in character from those whose variables are not related to a binary - variable problem .",
    "a consequence of the multiple global minimizers ( and we believe this is true in many other problems in discrete variables such as assignment problems that are relaxed ) is the presence of saddlepoints that are almost minimizers . indeed",
    "potentially the number of saddlepoints can be much larger than the number of global minimizers .",
    "it can be shown that there exists a path between any two isolated global minimizers that contains a feasible saddlepoint . moreover ,",
    "there exists a saddlepoint for which that has only one negative eigenvalue .",
    "there is a potential for the number of such points to grow exponentially with the number of global minimizers .",
    "it was shown in @xcite how to compute @xmath30 and its first and second derivatives very efficiently .",
    "this is critical since we show that directions of negative curvature are essential to solving this problem and they play a much more critical role than is typically the case .",
    "a key issue is symmetry .",
    "obviously for _ every _ hc there is a hc obtained by reversing the direction .",
    "this symmetry reveals itself in the problem variables .",
    "if there is a hc with @xmath31 then there exists a reverse cycle that is also a hc in which @xmath32 and its twin is 1 .",
    "we need to set the initial value of these two variables to be identical in order not to introduce bias ( they may both be 0 in another hc ) .",
    "quite frequently ( and this almost always happens with some pairs ) when using only descent many of these twin variables remain equal . in such circumstances",
    "it is only the use of a direction of negative curvature that breaks the tie .",
    "while such behavior is possible for general problems it is quite rare .",
    "consequently , in this class of problem directions of negative curvature play a more important role and often more important than that of using a direction of descent .",
    "again unlike the general case where we usually observe no directions of negative curvature in the neighborhood of the solution here they are always present , which is one reason why the solution is at a vertex .",
    "what is happening is that from our current iterate there are two equally attractive minimizers so it steers a course going to neither unless directions of negative curvature are used .",
    "the symmetry reveals itself also in the problem function and derivatives .",
    "if the iterates to solve the problem are denoted by @xmath33 then at @xmath34 is it usually the case that @xmath35 the gradient of @xmath30 at @xmath34 is orthogonal to the eigenvectors corresponding to negative eigenvalues of the hessian of @xmath30 at @xmath34 .",
    "consequently when are solving for the newton step using the conjugate gradient algorithm it will not be detect when hessian is indefinite .",
    "typically in an optimization problem it is better to have more constraints if such constraints can be added even if these are inequalities and are known not to be active at the solution .",
    "however , it is not always the case .",
    "we have a choice of either @xmath36 or @xmath37 ( eliminate equations [ eq - ds2 ] or [ eq - ds3 ] ) .",
    "note here when solving with @xmath36 we are adding more equality constraints without adding extra variables and hence we are reducing the degrees of freedom in the problem .",
    "however , for this particular problem there are some theoretical differences that alter the usual picture of potentially reducing the search space but adding to the complexity of computing the iterates .",
    "it was shown in @xcite that when @xmath36 that the lu factors of @xmath19 exist regardless of the pivoting order .",
    "this has many beneficial consequences not the least of which is the objective of the problem may be recast to be @xmath38 , where @xmath27 is the leading principle minor of @xmath19 . although @xmath39 is singular its leading principle minor is nonsingular as a consequence of the existence of the lu factorization .",
    "note that @xmath19 is typically very sparse ( if it is not finding a hc is usually trivial ) .",
    "it also has other benefits , the main one being that it enables the problem to be recast in a wide variety of ways .",
    "we shall show in section [ sec - prelim ] that this property is also true even when @xmath37 so this is not a reason for preferring @xmath36 .",
    "in @xcite it was shown as part of the proof that when @xmath37 , if a variable is not 0 or 1 altering it to one of them reduces the objective .",
    "one consequence is that all local minimizers are binary .",
    "it has not been shown that this result is true for @xmath36 .",
    "indeed it seems likely it is not true .",
    "the issue that makes it more complicated is that altering a variable in the @xmath36 usually requires altering many or all of the other variables in order to retain feasibility .",
    "there are many ways that could be done .",
    "it will be seen that one of the steps we propose in our algorithm is deletion or deflation , which occur when one or more of the variables is set to 0 or 1 respectively . for the @xmath37 case it is simple to adjust the corresponding variables in a row of @xmath40 to satisfy the constraint simply by scaling the relevant row . for the @xmath36 case it more complex and either an lp or qp needs to be solved .",
    "it is made more complicated by the need to determine a strictly interior point and sometimes one does not exist . usually one benefit of more constraints is the reduced hessian is smaller and the linear system needed to be solved at each iteration is also smaller .    finding both a sufficient descent direction and a direction of sufficient negative curvature requires finding a null space matrix . if @xmath41 is the constraint coefficients we require a matrix @xmath42 such that @xmath43 and the matrix @xmath44 is full rank .",
    "the matrix @xmath42 is almost always dense .",
    "consequently , the smaller the dimension of @xmath42 the better . however , it was shown in @xcite that there exists a @xmath42 for the @xmath10 case that is sparse and structured .",
    "paradoxically the larger @xmath42 for the @xmath8 case is simpler and sparser .",
    "this alters the balance when computing the search directions needed to solve our problem .",
    "we show that the lu factorization of @xmath19 and of @xmath45 exists when p is a stochastic matrix . as already noted this",
    "was shown to be true for a doubly stochastic matrix . to determine the determinant of the objective we need to compute the lu factorization of a matrix and this result implies no pivoting is required .",
    "a stochastic matrix may have either rows or columns that sum to unity . in forming the lu factorization",
    "it is common to assume row interchanges rather than column interchanges .",
    "this is just convention and there is no advantage to doing it one way or the other .",
    "however , for sparse matrices the manner the sparse elements are stored does matter when performing the lu factorization . since",
    "when forming such matrices it is assumed that row interchanges will be done that impacts how best to store the sparse matrix in compact form . in the proof",
    "we assume row interchanges may be made and this causes us to prefer to assume that @xmath40 has unit columns .",
    "the converse result for unit rows follows immediately from this result .",
    "a matrix @xmath41 is said to have property @xmath46 if    1 .",
    "@xmath47 for @xmath48 @xmath29 2 .",
    "@xmath49 for @xmath48 @xmath50 3 .",
    "@xmath51 .    if @xmath41 has property @xmath46 an lu factorization of @xmath41 exists .    * proof *    if @xmath52 then the first row of @xmath53 is @xmath54 and the first row of @xmath55 is the same as the first row of @xmath41",
    ". consequently , there is no loss of generality if we assume that @xmath56 .",
    "note that @xmath57 is the element of largest magnitude in the first column of @xmath41 .",
    "after one step of standard gaussian elimination ( ge ) we get @xmath58 we have @xmath59 , which implies , since @xmath60 that @xmath61 it follows that @xmath62 and @xmath63 .",
    "by definition we have @xmath64 since @xmath65 and @xmath66 it follows that @xmath67 @xmath48 @xmath50 . from this result and",
    "@xmath62 it follows that @xmath68 and that @xmath69 has property @xmath46 .",
    "we can now proceed with next step of ge .",
    "note that if @xmath70 we must have the first row and column of @xmath69 be zero and we can skip the steps of ge until we have a nonzero diagonal element of @xmath69 .",
    "regardless of the rank of @xmath41 we have @xmath71 this follows from @xmath69 having property @xmath46 and the only @xmath72 matrix ( the size of @xmath69 for the last step of ge ) with that property being 0 .    if @xmath41 has rank @xmath73 then @xmath74 .    if @xmath41 has rank @xmath73 the leading principle minor is nonsingular .",
    "when performing ge the elements being eliminated are not larger in magnitude than the pivot .",
    "this implies that @xmath75 .",
    "this property implies that @xmath53 is about as well conditioned as it can be and that if software to perform ge is used even if it performs row interchanges when needed they will never be required and the lu factorization of @xmath41 will be obtained and not that of @xmath76 , where @xmath40 is a permutation matrix .",
    "[ nonsing ] the matrix @xmath77 is nonsingular when @xmath41 has rank @xmath73 and @xmath78 .",
    "* proof *    we have @xmath79 where @xmath80 .",
    "note that @xmath81 is upper triangular .",
    "moreover , the @xmath82th element of @xmath81 is @xmath83 . since @xmath84 we get @xmath85 , which implies @xmath86 is nonsingular .",
    "a matrix @xmath41 is said to have property @xmath87 if    1 .",
    "@xmath47 for @xmath48 @xmath29 2 .",
    "@xmath49 for @xmath48 @xmath50 3 .",
    "@xmath88 .    if @xmath41 has property @xmath87 an lu factorization of @xmath41 exists .",
    "this follows from the fact that the lu factors of @xmath89 exist .",
    "lu factors of @xmath41 can be obtained from the transpose of these factors .",
    "note that although this is an lu factorization it differs from that typically found since it is now @xmath55 that has unit diagonal elements .",
    "the basic approach used is similar to that due to murray and ng @xcite , who first relax the problem and then solve a sequence of problems in which a strictly convex function is added to the objective together with a nonconvex function that attempts to force the variables to be binary . initially the strictly convex function dominates the objective and in the limit the nonconvex term dominates .",
    "our approach is a simplification since the nonconvex term is not needed .",
    "also since we are applying this general approach to a specific problem with significant structure the algorithm can be modified to improve not only efficiency , but also to improve the likelihood of obtaining a global minimizer and hence a hc .",
    "how the individual problems in the sequence are solved is the main focus .",
    "it will be seen a much heavier use of negative curvature is made and with less emphasis on the use of descent directions , which is the reverse of what optimization algorithms usually do .",
    "a peculiarity of the problem , which we think may be true of most problems with multiple global minimizers , is the gradient at the iterates is often spanned by the eigenvectors corresponding to the positive eigenvalues of the hessian even though the hessian is indefinite .",
    "this corresponds to the so - called  hard case \" in trust region methods . typically in such methods little or no attention",
    "is paid to it since it is considered very unlikely to arise and essentially impossible to keep arising .    in both the stochastic and doubly stochastic case",
    "we are interested in solving a problem of the form : @xmath90 strictly speaking the upper bounds on @xmath1 are not required since the equality constraints and the lower bounds ensure that the upper bound on @xmath1 holds .",
    "however , for now we shall leave them in .",
    "we are interested in the global minimizer and a typical descent algorithm will converge to the local minimizer associated with the initial point .",
    "murray and ng propose adding a strictly convex function @xmath91 to the objective , where @xmath92 is a positive scalar . a sequence of problems is then solved for a sequence of strictly monotonically decreasing values of @xmath92 . for @xmath93 with certain continuity properties the trajectory of minimizers @xmath94 is a unique , continuous , and smooth trajectory .",
    "when the initial value of @xmath92 is sufficiently large the new objective is also strictly convex and has a unique and therefore global minimizer .",
    "consequently , the minimizer found by this algorithm is the one whose trajectory is linked to the initial unique global minimizer .",
    "a feature of the problem is it has what we term  twin variables \" . in the definition of the variables as elements of the matrix @xmath40 ,",
    "if @xmath95 is not always zero then neither is its twin is @xmath96 . in terms of the graph",
    "this is the same edge except in the opposite direction . since the reverse of a hc is itself a hc twin variables have an equal probability of being in a hc .",
    "it is essential that the minimizer of @xmath93 is a neutral point with regard to the minimizers of the original problem .",
    "for example , if the feasible region is a hypercube the unique neutral point is the center . since we know the binary minimizers are extreme points of the feasible region the  center \" of the feasible region is such a point .",
    "one way of achieving such a point is to choose :    @xmath97 an alternative is @xmath98 these are well known barrier functions used in interior point methods . by using either of these functions",
    "we have transformed the original problem into minimizing a sequence of barrier functions .",
    "note the reason here for using such functions is not eliminating inequality constraints , that is simply a side benefit . solving the original problem using say an active set method is efficient especially since we do not expect the size of the problems to be extremely large ( 100,000 variables or more ) .",
    "our motivation is different and consequently it impacts how the initial @xmath92 is chosen and how it is subsequently adjusted . since we seek a neutral point @xmath99 ( @xmath30 dropped from the objective ) .",
    "a test of whether the choice of @xmath93 leads to a neutral initial point is whether the twin variable have the same initial value and this is observed in the numerical testing of the barrier function .    since the barrier function removes the need for the inequality constraints the algorithms requires the solution of a sequence of linearly _ equality _ constrained optimization problems .",
    "the choice of method is dictated by the need to converge to points that at least satisfy second - order optimality conditions .",
    "this requires the algorithm to determine whether the reduced hessian is positive semidefinite . to obtain the reduced hessian matrix we need the null space matrix @xmath42 , which is such that @xmath100 and @xmath44 is full rank .",
    "the reduced hessian is then given by @xmath101 , where @xmath102 is the hessian of @xmath30 .",
    "we use a line search method based on determining a descent direction and when available a direction of negative curvature .",
    "a sequence @xmath33 of improving estimates is generated from an initial feasible estimate @xmath103 from    @xmath104 where @xmath105 is a steplength that ensures a sufficient decrease , @xmath106 is a sufficient descent direction , and @xmath107 is a direction of sufficient negative curvature .",
    "it was shown by forsgren and murray @xcite that this sequence converges to a point that satisfies the second - order optimality conditions .",
    "typically such methods combine a direction of descent with a direction of negative curvature when the latter exists .",
    "our observation is when a direction of negative curvature does exist and is used purely as the search direction then at almost every subsequent iteration a direction of negative curvature exists and is usually getting stronger .",
    "consequently , when we get a direction of negative curvature we do not bother computing the direction of descent .    given the importance of the direction of negative curvature we depart from normal practice and apply the modified cholesky algorithm @xcite to the following matrix @xmath108 where @xmath109 is an estimate of the smallest eigenvalue of @xmath101 when it is thought @xmath101 is indefinite , otherwise @xmath109 is negative and very small in magnitude .",
    "the rational is that when @xmath101 is indefinite this leads to a very good direction of sufficient negative curvature .",
    "when @xmath101 is positive definite the small shift ensures that the matrix has a condition number that is sufficiently small to ensure sufficient accuracy in the direction of descent .",
    "if no modification is made in the modified cholesky factorization then @xmath110 is positive definite and we compute a direction of sufficient descent by solving : @xmath111 where @xmath112 is the upper triangular factor , and @xmath113 is the gradient of @xmath30 .",
    "the sufficient descent direction is then given by @xmath114 , where @xmath115 .",
    "if a modification is made then @xmath110 is indefinite and the following system is solved @xmath116 where the index @xmath117 is obtained during the modified cholesky factorization .",
    "it can be shown that @xmath118 , where @xmath119 is a direction of sufficient negative curvature . moreover , we have @xmath120 we can improve this direction of negative curvature by minimizing @xmath121 .",
    "we reduce the value by doing a sweep of univariate minimization of this function .",
    "this cost is roughly the same as a matrix - vector multiplication and so can be repeated if need be .",
    "we use the improved value as the estimate of @xmath109 in the following iteration . note that the sign of @xmath118 is always chosen so that @xmath122 .",
    "if @xmath109 is not small in magnitude we will not know if negative curvature exists when no modification is made in the modified cholesky algorithm",
    ". however we will know that the smallest eigenvalue is bigger than @xmath109 .",
    "we repeat the modified cholesky factorization with @xmath123 . if after a small number of reductions we still get no modification then @xmath109 is set to the default small value .",
    "we use a very crude linesearch along either @xmath114 or @xmath118 .",
    "we compute the maximum step to the boundary and take a step @xmath124 times the value .",
    "if that is not a lower point we multiply the step by 0.5 until we succeed . typically @xmath125 and",
    "almost always is successful .",
    "a key difference with the use of a barrier function here compared to solving problems unrelated to relaxed discrete problems is that @xmath30 behaves in an unusual way . after a strictly feasible point",
    "is found this is used to minimize the barrier function alone ( equivalent to setting @xmath126 ) .",
    "this is an easy function to minimize and to do so accurately .",
    "this is necessary to avoid bias unlike when minimizing a regular function where we are often able to provide an initial point reasonably close to the solution .",
    "indeed we are attempting to find the initial point to be as far away as possible from the solutions . in some cases such as for cubic graphs the minimizer of the barrier function",
    "is known ( @xmath127 ) .",
    "typically we want to reduce @xmath92 at a slow rate .",
    "however , another feature of the hc problem is the point that minimizes the barrier function is either a saddle point of the determinant function or very close to it .",
    "again this rarely if ever happens when using a barrier function for normal problems .",
    "the consequence is that moving the iterates from their current location requires changing @xmath92 sufficiently to make the current reduced hessian indefinite .",
    "quite how much is not difficult to estimate .",
    "the hessian of the barrier function is a well conditioned diagonal at the minimizer .",
    "it is usually less than 2 and for cubic graphs is 1 . in both the stochastic and doubly stochastic case",
    "the matrix @xmath42 has a low condition number .",
    "consequently , given an estimate of the smallest eigenvalue of either @xmath128 , the hessian of determinant function , or of @xmath129 it is easy to find a good estimate of the change needed in @xmath92 .",
    "if it is not sufficient then we can simply divide by 10 until it is . in our testing this was never needed .",
    "once we get negative curvature we usually never reduce @xmath92 again since either we succeed in finding a hc without needing to , or we fail .    an alternative to solving",
    "the standard problem is to use the primal dual approach .",
    "the standard approach means that the newton direction is poor when @xmath92 is small .",
    "there are two reasons not to use the primal dual method .",
    "firstly , we do not need to have @xmath92 very small since we know the solution is converging to a binary point and so we can round and test the solution . ill - conditioning arises due to a variable becoming close to a bound .",
    "should that happen such a variable can be removed from the problem . how to do this is described in the sections on deletion and deflation .",
    "secondly , we need to use directions of negative curvature but the hessian in the primal dual formulation is not assured to give a direction of negative curvature except in the neighborhood of a stationary point .",
    "a common way of defining @xmath42 , such that @xmath100 and @xmath44 is full rank , is to first partition the columns of @xmath130 $ ] , where @xmath86 is nonsingular",
    ". then we can define    @xmath131.\\ ] ]    if only stochastic constraints are required , the matrix @xmath41 can be quite simply defined .",
    "if the graph has @xmath3 vertices , where vertex @xmath29 has degree @xmath132 , then we can define    @xmath133,\\ ] ]    where @xmath134 is an @xmath135 matrix .",
    "it is easy to see that the condition number of @xmath41 is bounded above by @xmath3 , by checking that @xmath136 $ ] , and therefore the condition number of @xmath41 is the ratio of the largest degree to the smallest degree .    in order to define the null space matrix for @xmath41 , it is trivial to reorder the columns of @xmath41 such that the first column of each @xmath137 submatrix appears first .",
    "the reordered matrix is @xmath138 $ ] , where @xmath139 is defined as    @xmath140,\\ ] ]    and @xmath141 is an @xmath142 matrix .",
    "then the null space matrix for @xmath143 is    @xmath144,\\ ] ]    and appropriately reordering the rows of @xmath145 provides the null space matrix for @xmath41 .",
    "one advantage of defining the null space matrix in this way is that the sparsity inherent in difficult instances is retained in @xmath42 , and non - zero entries are all either + 1 or -1 . the condition number of @xmath42 is equal to @xmath146 .",
    "the only operations involving @xmath42 that are required are matrix - vector products .",
    "then for a given vector @xmath147 , the product @xmath148 can be computed very efficiently .",
    "if doubly - stochastic constraints are desired , the matrix @xmath41 defines constraints on both the rows of @xmath40 and the columns of @xmath40 .",
    "we first define a matrix @xmath149 , corresponding to the row constraints , to be identical to the @xmath41 matrix for the stochastic case .",
    "next we define a matrix @xmath150 , corresponding to the column constraints .",
    "suppose each variable @xmath34 corresponds to an arc @xmath151 .",
    "then @xmath150 is defined as    @xmath152_{jk } : = \\left\\{\\begin{array}{lcl}1 & & \\mbox{if } \\exists\\ ; i\\;\\ ; \\mbox { s.t . }",
    "a_k = ( i , j)\\\\ 0 & & \\mbox{otherwise}\\end{array}\\right.\\ ] ]    then , we can define @xmath41 to be    @xmath153.\\ ] ]    the matrix @xmath41 defined in this way is certain to be rank deficient . in order to construct a null space matrix ,",
    "we want to delete enough rows to obtain a full rank matrix , and then reorder the matrix to obtain @xmath154 $ ] , where @xmath86 is a _ triangular _ matrix .",
    "this can be achieved by using the following algorithm .",
    "* input * : @xmath155 + * output * : @xmath156 +   + * begin * + count @xmath157 0 + rows @xmath157 rank@xmath158 + @xmath159 _ with rows removed to make _",
    "@xmath143 _ full rank _ + cols @xmath157 columns@xmath160 + @xmath161 rows + @xmath162cols@xmath163 + @xmath164 + c @xmath157",
    "_ identify a set of columns @xmath165 such that _",
    "@xmath166 + @xmath167 + @xmath29 * from * 1 * to * @xmath168 + count @xmath157 count + 1 + @xmath169 $ ] _ ( moving _ @xmath170 _ into position _ count _ )",
    "_ + @xmath171 _ ( moving column @xmath172 to column _ count _ ) _ + @xmath173 _",
    "reorder the rows to get a 1 in positive _ @xmath174count ) +   + @xmath161 rows - count +   + @xmath175 _ reverse the order of the first _",
    "rows _ entries in _ @xmath176 + @xmath171 _ ( reverse the order of the first _",
    "rows _ columns in @xmath41 ) _",
    "+ @xmath177 + @xmath178 + * end * +   +     +    then the null space for @xmath143 is defined to be    @xmath179.\\ ] ] unlike typical problems the @xmath42 constructed in this way is sparse ( similar to that of @xmath41 ) and does not require the lu factorization of @xmath86 since @xmath86 is lower triangular .",
    "moreover , @xmath86 has elements that are either 0 or 1 .",
    "consequently operations with @xmath42 do not require any multiplication .",
    "if at any stage , one or more of the @xmath4 variables approach their extremal values ( 0 or 1 ) , we fix these values and remove the variables from the problem .",
    "this process takes two forms : _ deletion _ and _ deflation _ , that is , setting @xmath4 to 0 or 1 , respectively .",
    "note that we use the term deflation because in practice the process of fixing @xmath180 results in two nodes being combined to become a single node , reducing the total number of nodes in the graph by 1 .",
    "deletion is a simple process of fixing a variable to 0 by simply removing its associated arc from the graph .",
    "when a variable @xmath4 is close to 1 , we perform a deflation step by combining nodes @xmath29 and @xmath117 by removing node @xmath29 from the graph . then , we redirect any arcs @xmath181 that previously went into node @xmath29 to become @xmath182 , unless this creates a self - loop arc . after deletion or deflation",
    "we construct the new constraint matrix @xmath41 and update @xmath42 appropriately .",
    "the thresholds for the deletion or deflation process to take place can be set as input parameters .    during deflation",
    ", we not only fix one variable ( @xmath4 ) to have the value 1 , but also fix several other variables to have the value 0 .",
    "namely , we fix all variables corresponding to arcs @xmath183 for @xmath184 , @xmath182 for @xmath185 , and @xmath186 to have the value 0 .",
    "whenever we perform deflation the information about the deflated arcs are stored in order to construct a hc in the original graph once a hc is found in the reduced graph .    after performing deletion or deflation ,",
    "a reduced vector @xmath187 is obtained , which is infeasible in the resultant smaller dimension problem . in the stochastic case obtaining a feasible point",
    "is trivial since only the variables in the specific rows where fixing has occurred need to adjusted . the simplest way is to multiply the remaining variables in an impacted rows by @xmath188 , where @xmath189 is the variable that has been fixed .",
    "note that this increases the remaining variables so will not trigger another deletion in the row .",
    "it is possible it triggers a deflation , but this is unlikely . in the doubly stochastic case many or",
    "all of the variables may be impacted even for a single variable being deleted .",
    "define @xmath190 to be the error induced by such a process . note that @xmath191 is a nonnegative vector in the case of both deletion and deflation .",
    "then , we find a new @xmath1 such that @xmath192 , and @xmath193 , where the size of @xmath194 depends on the deletion or deflation thresholds chosen .",
    "the interpretation of @xmath1 is that it is a point that satisfies the @xmath10 constraints , and is as close as possible to the point we obtained after deleting or deflating .",
    "we determine @xmath1 by first defining @xmath195 and @xmath196 so that @xmath197 .",
    "define @xmath198 as the smallest element of @xmath187 .",
    "then , we solve @xmath199    where @xmath200 is chosen large enough that @xmath201 is reduced to 0 whenever possible",
    ". constraints ( [ eq - lp_scaling3 ] ) are designed to ensure that @xmath202 .",
    "however , it may be impossible to satisfy the above constraints for a value of @xmath203 because some variables may need to be 0 or a value very close to 0 . in this case",
    ", we reduce @xmath198 and solve the lp again , continuing this process until we obtain a solution with @xmath203 .",
    "if @xmath204 unless we set @xmath205 for some @xmath29 and @xmath117 , then we delete these variables , as they can not be nonzero in a hamiltonian cycle ( or in fact any @xmath10 point ) containing the currently fixed arcs .",
    "at each iteration we test if a hc can be obtained by a simple rounding procedure . in the @xmath8 case",
    "we set the largest element in each row to one , starting with the row with the largest overall element .",
    "if the largest element happens to already have a unit element in the column we set the second largest in that row to one and so on .",
    "if there is no element available we have not identified a hc . after each setting of a variable to one",
    "we rebalance the constraints .",
    "a similar procedure is used in the @xmath10 case except in this case setting an element to unity induces more elements being set to zero .",
    "moreover , we now fail to satisfy the @xmath10 constraints and so rebalancing is not done .",
    "obviously , we could use more sophisticated rounding methods , which may allow us to identify a hc earlier .",
    "one potential improvement of this method would be to solve a heuristic at the completion of each iteration , using the current point @xmath206 , that tries to find a nearby hc .",
    "such a hybrid approach was considered in @xcite , with promising results .",
    "this has not been explored since we are interested in testing our algorithm to the limit .",
    "below we outline the structure of the algorithm , which we term dipa ( determinant interior point algorithm )",
    "in order to investigate the character and to test the performance of the algorithm we generated a test set of 350 problems . specifically , we randomly generated 50 problems for each of 20 , 30 , .... , and 80 nodes with node degree between 3 and 6 .",
    "the computer used for performing all the experiments was a pc with intelcore@xmath207 i7 - 4600u cpu , 2.70 ghz , 16 gb of ram , and running on the operating system windows 8.1 enterprise .",
    "dipa was implemented in matlab r2014b , with all lps solved by ibm ilog cplex optimization studio v12.6 via its concert interface to matlab .",
    "the choice of initial @xmath92 and the rate of reduction of @xmath92 did not prove to be critical .",
    "for successful runs once @xmath92 had been reduced to a sufficient level for the reduced hessian to be indefinite it almost always remained indefinite until it stopped .",
    "indeed no further reduction in @xmath92 was required . in figure [ fig - paths ] we show how the determinant function behaves as it goes from the initial point to all of the hcs of a 20 node graph . it can be seen all curves are remarkably similar and that each can be reached by going down a direction of negative curvature . also the degree of curvature increases the closer the point gets to the hc .",
    "we attempted to solve the 350 problems in the test set with the algorithm applied to the stochastic and doubly stochastic form of the problem . in both cases",
    "an attempt was made to perform neither a deletion or deflation by setting extreme values for the relevant parameters that invoke these steps within the algorithm .",
    "the results given in table [ tab - results ] are unambiguous .",
    "it is clear that the doubly stochastic form of the problem is far superior .",
    "we did do further tests on the stochastic case by varying the adjustable parameters but the gap in performance was far too large to bridge .",
    ".numbers of graphs solved when deletions and deflations are suppressed for the stochastic and doubly - stochastic forms of the problem . [ cols=\"^,^,^,^,^,^,^,^ \" , ]     not using deflation typically increased the number of iterations about 50% over using deflation . in figure",
    "[ fig - descent ] we show how our algorithm typically converges when a hc is found .",
    "since we are converging to a vertex the nature of converges differs considerably from a typical minimization algorithm where the reduction made in the object slows as the solution is approached .",
    "clearly the results indicate that the problem with @xmath10 constraints yields far better results than the problem with @xmath8 constraints .",
    "it also supports the view that different forms of a problem with identical complexity properties can have quite differing performance .",
    "it was not the intent of this paper to show or suggest that this approach is competitive with alternative algorithms to find a hamiltonian cycle .",
    "however , it is quite distinct from other methods and there is much that can be done to improve its performance .",
    "more constraints can be added .",
    "for example , @xmath208 and we know twin variables , say @xmath209 and @xmath210 must satisfy @xmath211 and @xmath212 .",
    "the product from the latter constraints can be added directly to the objective and can be used initially to give a strictly convex problem .",
    "however , we plan to try and find a form of the problem that eliminates the occurrence of reverse cycles and so eliminates twin variables from the problem",
    ". the new variables would be the elements of @xmath213 , where @xmath214 .",
    "knowing @xmath213 it is trivial to find the elements of @xmath40 .",
    "the current problem has a dense hessian matrix .",
    "although we have shown how all the elements can be computed efficiently it still leaves a dense matrix , which has computational implications when computing the search direction for large problems .",
    "we are investigating transformations that should lead to the hessian being sparse .",
    "also if the conjugate gradient algorithm is used to compute the search direction and direction of negative curvature it may be possible to compute @xmath215 efficiently even when @xmath102 is dense .",
    "although we have addressed the hamiltonian cycle problem an equally important interest is developing algorithms to determine global minimizers . in particular problems that have arisen from relaxing discrete problems",
    "many of the issues that arise in such problems are identical to those arising in the hc problem .",
    "for example , lots of global minimizers and hence lots of stationary points that have reduced hessians that are almost positive definite ( one negative eigenvalue ) . moreover , symmetry is also present .",
    "problems such as the frequency assignment problem have an equally good solution simply from any permutation of a known solution .",
    "also solutions are typically at a highly degenerate vertex .",
    "we are encouraged by the success of the algorithm we have developed , which has demonstrated the ability to find global minimizers of highly nonlinear and nonconvex problems with several hundred binary variables .",
    "a key requirement when solving problems with relaxed discrete variables is to have an unbiased initial point .",
    "as already noted a common technique used in global optimization is to use multiple starting points.the approach we advocate requires a neutral starting point .",
    "we have demonstrated that an equally good alternative is to vary some of the parameters and options that algorithms to solve such problems typically have .",
    "we have shown that very small changes both to the strategy and flexible parameters leads to distinct solution enabling us to reduce significantly the number of problems on which we fail to find a global minimizer . moreover",
    ", these variations do not lead to less efficient methods ."
  ],
  "abstract_text": [
    "<S> it has been shown that the global minimizer of a smooth determinant of a matrix function reveals the largest cycle of a graph . </S>",
    "<S> when it exists this is a hamiltonian cycle . </S>",
    "<S> finding global minimizers even of a smooth function is a challenge . </S>",
    "<S> the difficulty is often exacerbated by the existence of many global minimizers . </S>",
    "<S> one may think this would help but in the case of hamiltonian cycles the ratio of the number of global minimizers to the number of local minimizers is typically astronomically small . </S>",
    "<S> we describe efficient algorithms that seek to find global minimizers . </S>",
    "<S> there are various equivalent forms of the problem and here we describe the experience of two . </S>",
    "<S> the matrix function contains a matrix @xmath0 , where @xmath1 are the variables of the problem . </S>",
    "<S> @xmath0 may be constrained to be stochastic or doubly stochastic . </S>",
    "<S> more constraints help reduce the search space but complicate the definition of a basis for the null space . even so we derive a definition of the null space basis for the doubly stochastic case that is as sparse as the constraint matrix and contains elements that are either 1 , -1 or 0 . </S>",
    "<S> such constraints arise in other problems such as forms of the quadratic assignment problem .    </S>",
    "<S> keywords : hamiltonian cycle , barrier functions , interior - point methods , negative curvature . </S>"
  ]
}