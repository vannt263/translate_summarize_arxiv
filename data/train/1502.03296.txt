{
  "article_text": [
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  _ ...",
    "language in use can not be studied without statistics ",
    "_ gustav herdan ( 1964 )  @xcite _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    in the past 100 years regularities in the frequency of text constituents have been summarized in the form of _",
    "linguistic laws_. for instance , zipf s law states that the frequency @xmath0 of the @xmath1-th most frequent word in a text is inversely proportional to its rank : @xmath2  @xcite .",
    "this and other less famous linguistic laws are one of the main objects of study of _ quantitative linguistics _  @xcite .",
    "linguistic laws have both theoretical and practical importance .",
    "they provide insights on the mechanisms of text ( language , thought ) production and are also crucial in applications of statistical natural language processing ( e.g. , information retrieval ) . both the generative and data - analysis views of linguistic laws are increasingly important in modern applications .",
    "data - mining algorithms profit from accurate estimations of the vocabulary size of a collection of texts ( corpus ) , e.g. , through heaps law discussed in the next section .",
    "methods for the automatic generation of natural language can profit from knowing the linguistic laws underlying usual texts .",
    "for instance , linguistic laws may be included as ( additional ) constraints in the space of possible ( markov generated ) texts  @xcite and can thus be considered as constrains to the creativity of authors .        besides giving an overview on various examples of linguistic laws ( sec .",
    "[ sec.observations ] ) , in this paper we focus on their probabilistic interpretation ( sec .",
    "[ sec.interpretation ] ) , we discuss different statistical methods of data analysis ( sec .",
    "[ sec.data ] ) , and the possibilities of connecting different laws ( sec .",
    "[ sec.relation ] ) .",
    "the modern availability of large text databases allows for an improved view on linguistic laws that requires a careful discussion of their interpretation .",
    "typically , more data confirms the observations motivating the laws  mostly based on visual inspection  but makes increasingly difficult for the laws to pass statistical tests designed to evaluate their validity .",
    "this leads to a seemingly contradictory situation : while the laws allow for an estimation of the general behavior ( e.g. , they are much better than alternative descriptions ) , they are strictly - speaking falsified .",
    "the aim of this contribution is to present this problem and discuss alternative interpretations of the results .",
    "we argue that the statistical analysis of linguistic laws often shows long - range correlations and large ( topical ) fluctuations .",
    "we conclude that null models accounting for these observations are often ignored yet crucial in the tests of the validity of linguistic laws .",
    "an insightful introduction to linguistic laws is given in ref .",
    "@xcite by khler , who distinguishes between three kinds of laws as follows :    1 .",
    "`` the first kind takes the form of probability distributions , i.e. it makes predictions about the number of units of a given property . ''",
    "`` the second kind of law is called the functional type , because these laws link two ( or more ) variables , i.e. properties . ''",
    "`` the third kind of law is the developmental one . here , a property is related to time . ''",
    "( time may be measured in terms of text length )    we use the term linguistic law to denote quantitative relationships between measurements obtained in a written text or corpus , in contrast to syntactic rules and to phonetic and language - change laws ( e.g. , grimm s law ) .",
    "we assume that the laws make statements about individual texts ( corpus ) and are exact in an appropriate limit ( e.g. , large corpus ) .",
    "each law contains parameters which we denote by greek letters @xmath3 , and often refer to the frequency @xmath4 of a quantity @xmath5 in the text ( with @xmath6 ) .",
    "probabilities are denoted by @xmath7 .",
    "next we discuss in detail one representative example of each of the three types of laws mentioned above : zipf , menzerath - altmann , and heaps laws , respectively , see fig .",
    "[ fig.1 ] .    [ cols=\"<,^,^,^\",options=\"header \" , ]     [ [ representation - matters . ] ]",
    "representation matters .",
    "+ + + + + + + + + + + + + + + + + + + + + + +    equivalent formulations of the linguistic laws lead to _ different _ statistical analysis and conclusions  @xcite .",
    "one example of this point is the use of transformations before the fitting is performed , such as the linear fit of zipf s law in logarithmic scale discussed in sec .",
    "[ ssec.graphical ] .",
    "the variables used to represent the linguistic law are also crucial when likelihood methods are used , as discussed above for the case of zipf s law represented in @xmath8 or @xmath9 . while asymptotically these formulations are equivalent , the likelihood computed in both cases is different . in the likelihood of @xmath9",
    ", an observation corresponds to the frequency of a word _ type_.",
    "this means that the most frequent words in the database count the same as words appearing only once ( the hapax - legomenan ) . in practice ,",
    "the part of the distribution that matters the most in the fitting ( and in the likelihood ) are the words with very few counts , which contribute very little to the total text . in the likelihood of @xmath8",
    "the observational quantity is the rank @xmath1 of each occurrence of the word meaning that each word _ token _ counts the same .",
    "this means that the frequent words contribute more and the fitting of @xmath8 is robust against rare words .",
    "linear regression in log - log plot counts every point in the plot the same and , since there are more points for large @xmath1 , low - frequency words dominate the fit . using logarithmic binning , as suggested in ref .",
    "@xcite , equalize the importance of words across @xmath10 . in summary , while fitting a straight line in log - log scale using logarithmic binning gives the same value for words across the full spectrum ( in a logarithmic scale ) , the statistical rigorous methods of maximum likelihood will be dominated either by the most frequent ( in case of fitting in @xmath8 ) or least frequent ( in case of fitting in @xmath9 ) words .    beyond zipf s law",
    ", the reasoning above shows that even if asymptotically ( i.e. infinite data ) different formulations of a law are equivalent , the representation in which we test the law matters because it assumes a sampling process of the data .",
    "this in turn leads to different results when applied to finite and often noisy data and has to be taken into account when interpreting the results .",
    "[ [ application - fitting - zipfs - law . ] ] application : fitting zipf s law .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in fig .",
    "[ fig.zipf ] and tab .",
    "[ tab.books ] we compare the different fitting methods described above .",
    "the visual agreement between data and the fitted curves reflects the different weights given by the methods to different regions of the distribution as discussed above ( high - frequency words for @xmath8 and low - frequency words for the other two cases ) .",
    "not surprisingly , tab .",
    "[ tab.books ] shows that the estimated exponent @xmath11 varies from method to method .",
    "this variation is larger than the variation across different databases .",
    "large values of @xmath12 computed in the linear fit , usually interpreted as an indication of good fitting , are observed also when the p - value are very low .",
    "[ [ correlated - samples ] ] correlated samples + + + + + + + + + + + + + + + + + +    the failure of passing significance tests for increasing data size is not surprising because any small deviation from the null model becomes statistically significant .",
    "a possible conclusion emerging from these analysis is that power - law distributions are not as widely valid as previously claimed ( see also refs .",
    "@xcite ) , but often are better than alternative ( simple ) descriptions ( see our previous publication ref .  @xcite in which we consider two - parameter generalizations of zipf s law ) .",
    "the main criticism we have on this widely used framework of analysis is that it ignores the presence of correlations in the data : the computation of the likelihood in eq .",
    "( [ eq.likelihood ] ) assumes independent observations .",
    "furthermore , this assumption leads to an underestimation of the expected fluctuations ( e.g. ks - distance ) in the calculation of the p - value when assessing the validity of the law .",
    "it is thus unclear in which extent a negative result in the validity test ( e.g. , p - value@xmath13 ) is due to a failure of the proposed law or , instead , is due to the violation of the hypothesis of _ independent _ sampling .",
    "this hypothesis is known to be violated in texts  @xcite : the sequence of words and letters are obviously related to each other . in fig .",
    "[ fig.4 ] we show that these correlations affect the estimation of the frequency of individual words , which show fluctuations much larger than those expected not only based on the independent random usage of words ( poisson or bag of word models ) but also in a null model in which burstiness is included  @xcite .",
    "altogether , this shows that the independence assumption  used to write the likelihood  ( [ eq.likelihood ] )  is strongly violated and affects both the analysis based on @xmath8 ( correlation throughout a book ) and @xmath9 ( @xmath0 can be thought as a finite size estimation as the ones shown in fig .",
    "[ fig.4 ] ) .",
    "one approach to take into account correlations is to estimate a time for which two observations are independent , and then consider observations only after this time ( a smaller effective sample size ) .",
    "alternative approaches considered statistical tests for specific classes of stochastic processes ( correlated in time )  @xcite or based on estimations of the correlation coming from the data  @xcite .",
    "the application of these methods to linguistic laws is not straightforward because these methods fail in cases in which no characteristic correlation time exist .",
    "books show such long - range correlations  @xcite , also in the position of individual words in books  @xcite , in agreement with the observations reported in fig .",
    "[ fig.4 ] . more generally , correlations lead to a slower convergence to asymptotic values and it is thus possible to create processes of text generation that comply to a linguistic law asymptotically but that ( in finite samples ) violate statistical tests based on independent sampling .",
    "the problem affects also model comparison and fitting because these problems are also based on the likelihood ( in these cases , correlation affects all models and therefore it is unclear the extent in which it impacts the choice of the best model ) .",
    "in view of the different laws proposed to describe text properties , a natural question is the relationship between them ( e.g. , whether one law can be derived from another or whether there are generative processes that account for more than one law simultaneously ) .",
    "for instance , ref .",
    "@xcite clarifies how the long - range correlation of texts is related to the skewed distribution of recurrence time between words  @xcite ( a consequence of burstiness  @xcite ) .",
    "another well - known relation is the connection between heaps law and zipf s law  @xcite ( see refs .",
    "@xcite for other examples ) . here",
    "again the importance of fluctuations and an underlying null model is often neglected .",
    "the need for a null model is evident if we consider a text in which all possible words appear once in the very beginning of the text , violating heaps law , even though their frequency over the full text is still compatible with zipf s law .",
    "a typical null model is to consider that every word is used independently from the others with a probability equal to its global frequency .",
    "this probability is usually taken to be constant throughout the text ( poisson process ) , but alternative formulations considering time - dependent frequencies lead to similar results .",
    "for this generative model , zipf s law  ( [ eq.zipf2 ] ) leads to a heaps law  ( [ eq.heaps ] ) with parameters @xmath14  @xcite .",
    "similar null models are implicitly or explicitly assumed in different derivations  @xcite .",
    "figure  [ fig.5 ] shows that the connection between zipf s and heaps law using the independent usage of words fails to reproduce the fluctuations observed in data .",
    "in particular , the fluctuations around the average vocabulary size @xmath15 predicted from heaps law scales linearly with @xmath16 , and not as @xmath17 as predicted by the independence assumption ( through the central limit theorem ) . in ref .",
    "@xcite we have shown that this scaling  also known as taylor s law  @xcite  is a result of correlations in the usage among different words induced by the existence of topical structures inside and across books .",
    "it is common to find claims that a particular linguistic law is valid in a language or corpus .",
    "a closer inspection for the statistical support of these claims is often disappointing . in this chapter",
    "we performed a critical discussion of linguistic laws , the sense in which they can be considered valid , and the extent in which the evidence support its validity .",
    "we argued that linguistic laws have to be interpreted in a statistical sense .",
    "therefore , model selection ( also fitting ) and the compatibility to data have to be performed computing statistical tests based on the likelihood ( plausibility ) of the observations .",
    "the statistical analysis is far from being free of choices , both in terms of the methods employed and also about additional assumptions not contained in the original law , as discussed below .",
    "the analysis we presented above is intended to show that these choices matter and should be carefully discussed .",
    "the picture that emerges from the straight applications of the statistical tests above is that : ( i ) the linguistic laws are often the best simple description of the data , but ( ii ) the data is not generated according to it so that in a strict sense the validity of the law is falsified .",
    "this interpretation suggests that linguistic laws are useful and capture some of the ingredients seen in language , but are unable to describe observations in full detail even in the limit of large texts ( possibly because of the existence of additional processes ignored by the law ) .",
    "the main limitation of the methods we described , and thus of the conclusions summarized above , is that they were based not only on the statement of the law but also on the hypothesis that observations are independent and identically distributed .",
    "this hypothesis is known to be violated in almost all observations of written language .",
    "it is thus unclear in which extent the rejection of the null model ( small p - value ) can be considered a falsification of the linguistic law . on the one hand",
    ", this reasoning shows the limitation of the statistical methods and the necessity to apply and develop tests able to deal with ( long - range ) correlated data . on the other hand",
    ", it shows that the usual statements of linguistic laws are incomplete because they can not be properly tested .",
    "a meaningful formulation of a linguistic law allows for the computation of the likelihood of the observations , e.g. , it should be accompanied by a prediction of the fluctuations , a generative model for the relevant variables , or , ultimately , a model for the generation of texts .",
    "such models are usually interpreted as an explanation of the origin of the laws  @xcite and are absent from the statement of the linguistic laws , despite the fact that herdan already drew attention to this point  @xcite : _ `` the quantities which we call statistical laws being only expectations , they are subject to random fluctuations whose extent must be regarded as part of the statistical law . '' _ in the same sense that a scientific law can not be judged separated from a theory , linguistic laws are only fully defined once a generative process is given .",
    "the existence of long - range correlations , burstiness , and topical variations lead to strong fluctuations in the estimations of observables in texts , including the quantities described by linguistic laws .",
    "our findings have consequences to applications in information retrieval and text generation .",
    "for instance , our results show that strong fluctuations around specific laws are observed and that results obtained using the independence assumption ( e.g. , bag - of - words models ) have a limited applicability .",
    "therefore , statistical laws should not be imposed too strictly in the generation of artificial texts or in the analysis of unknown databases .",
    "large fluctuations are as much a characteristic of language as the laws themselves and therefore the creativity in the generation of texts is much larger than the one obtained if laws are imposed as strict constraints .    finally we would like to mention that our conclusions apply also to other statistical laws beyond linguistic .",
    "invariably , the increase of data size leads to a rejection of null - models , e.g. many recent works emphasize that claims of power - law distributions do not survive rigorous statistical tests  @xcite .",
    "however , the statistical tests employed in these references , and in most likelihood - based analysis , rely on the independence assumption of the observations ( known to be violated in many of the treated cases ) . nevertheless , we are not aware that this point has been critically discussed in the large number of publications on power - law fitting .",
    "the crucial role of mechanistic models in the fitting and statistical analysis of scaling laws was emphasized in ref .",
    "@xcite for urban - economic data .",
    "* acknowledgments : * we thank roger guimer , francesc font - clos , and anna deluca for insightful discussions .",
    "the books listed in tab .",
    "[ tab.books ] were obtained from project gutenberg ( http://www.gutenberg.org ) .",
    "the books and data filtering are the same as the ones used in ref .",
    "@xcite ( see the supplementary information of that paper for further details ) . we removed capitalization and all symbols except the letters `` a - z '' , the number `` 0 - 9 '' , the apostrophe , and the blank space .",
    "a string of symbols between two consecutive blank spaces was considered to be a word .",
    "the english wikipedia data was obtained from wikimedia dumps ( http://dumps.wikimedia.org/ ) .",
    "the filtering was the same as the one used in ref .",
    "@xcite , in which we removed capitalization and kept only those words ( i.e. sequences of symbols separated by blank space ) which consisted exclusively of the letters `` a - z '' and the apostrophe .    the computation of menzerath - altmann law appearing in figs .",
    "[ fig.1 ] ,  [ fig.ma ] , and tab .",
    "[ tab.ma ] was done starting from the unique words ( word type ) in the database discussed in the previous paragraphs . for each word @xmath18 we applied the following steps :    1 .",
    "lemmatize using the wordnetlemmatizer ( http://wordnet.princeton.edu in the nltk python package http://www.nltk.org/ ) .",
    "2 .   count the number of syllables @xmath19 based on the _ moby hyphenation list by grady ward _",
    ", available at http://www.gutenberg.org/ebooks/3204 3 .",
    "count the number of phonemes @xmath20 based on _ the cmu pronouncing dictionary _",
    ", version 0.7b available at www.speech.cs.cmu.edu/cgi-bin/cmudict    for the book _ moby dick _ by h. melville , this procedure allowed to compute @xmath19 and @xmath20 for @xmath21 words , @xmath22 of the total number of words ( before lemmatization ) . for the wikipedia",
    ", we obtain @xmath23 words , @xmath24 of the total number .",
    "the low success in wikipedia is due to the size of the database ( large number of rare words ) and the results depend more strongly on the procedure described above than on the database itself .",
    "j. lijffijt , p. papapetrou , k. puolamki , h. mannila , analyzing word frequencies in large text corpora using inter - arrival times and bootstrapping , chapter in machine learning and knowledge discovery in databases , lecture notes in computer science 6912 , 341 ( 2011 ) .",
    "b. mandelbrot , 1961 on the theory of word frequencies and on related markovian models of discourse _ structure of language and its mathematical aspects : proceedings of symposia in applied mathematics vol .",
    "( providence , ri : american mathematical society )                                              m. cristelli , m. batty , and l. pietronero , there is more than a power law in zipf , scientific reports 2 , 812 .",
    "m.p.h . stumpf and m. a. porter .",
    "critical truths about power laws .",
    "science , 335 , 665666 ( 2012 ) ."
  ],
  "abstract_text": [
    "<S> zipf s law is just one out of many universal laws proposed to describe statistical regularities in language . here </S>",
    "<S> we review and critically discuss how these laws can be statistically interpreted , fitted , and tested ( falsified ) . </S>",
    "<S> the modern availability of large databases of written text allows for tests with an unprecedent statistical accuracy and also a characterization of the fluctuations around the typical behavior . </S>",
    "<S> we find that fluctuations are usually much larger than expected based on simplifying statistical assumptions ( e.g. , independence and lack of correlations between observations ) . </S>",
    "<S> these simplifications appear also in usual statistical tests so that the large fluctuations can be erroneously interpreted as a falsification of the law . instead </S>",
    "<S> , here we argue that linguistic laws are only meaningful ( falsifiable ) if accompanied by a model for which the fluctuations can be computed ( e.g. , a generative model of the text ) . </S>",
    "<S> the large fluctuations we report show that the constraints imposed by linguistic laws on the creativity process of text generation are not as tight as one could expect .    </S>",
    "<S> proceedings of the _ flow machines workshop : creativity and universality in language _ , paris , june 18 to 20 , 2014 . </S>"
  ]
}