{
  "article_text": [
    "in many applications , datasets used for supervised learning contain a large number of continuous features , with a large number of samples .",
    "an example is web - marketing , where features are obtained from bag - of - words scaled using tf - idf @xcite , recorded during the visit of users on websites .",
    "a well - known trick  @xcite in this setting is to replace each raw continuous feature by a set of binary features that one - hot encodes the interval containing it , among a list of intervals partitioning the raw feature range .",
    "this leads to a non - linear decision function with respect to the raw continuous features space , and can therefore improve prediction .",
    "however , this trick is prone to over - fitting , since it increases significantly the dimension of the problem .    [ [ a - new - penalty . ] ] a new penalty .",
    "+ + + + + + + + + + + + + +    to overcome this problem , we introduce a new penalization called _ binarsity _ , that penalizes the model weights learned from such grouped one - hot encodings ( one group for each raw continuous feature ) . since the binary features within these groups are naturally ordered , the binarsity penalization combines a group total - variation penalization , with an extra linear constraint in each group to avoid collinearity between the one - hot encodings .",
    "this penalization forces the weights of the model to be as constant ( with respect to the order induced by the original feature ) as possible within a group , by selecting a minimal number of relevant cut - points .",
    "moreover , if the model weights are all equal within a group , then the full block of weights is zero , because of the extra linear constraint .",
    "this allows to perform raw feature selection .",
    "[ [ sparsity . ] ] sparsity .",
    "+ + + + + + + + +    to address the high - dimensionality of features , sparse inference is now an ubiquitous technique for dimension reduction and variable selection , see for instance  @xcite and @xcite among many others .",
    "the principle is to induce sparsity ( large number of zeros ) in the model weights , assuming that only a few features are actually helpful for the label prediction .",
    "the most popular way to induce sparsity in model weights is to add a @xmath0-penalization ( lasso ) term to the goodness - of - fit  @xcite .",
    "this typically leads to sparse parametrization of models , with a level of sparsity that depends on the strength of the penalization .",
    "statistical properties of @xmath0-penalization have been extensively investigated , see for instance  @xcite for linear and generalized linear models and  @xcite for compressed sensing , among others",
    ".    however , the lasso ignores features ordering . in @xcite ,",
    "a structured sparse penalization is proposed , known as fused lasso , which provides superior performance in recovering the true model in such applications where features are ordered in some meaningful way .",
    "it introduces a mixed penalization using a linear combination of the @xmath0-norm and the total - variation penalization , thus enforcing sparsity in both the weights and their successive differences .",
    "fused lasso has achieved great success in some applications such as comparative genomic hybridization  @xcite , image denoising  @xcite , and prostate cancer analysis  @xcite .",
    "[ [ features - discretization - and - cuts . ] ] features discretization and cuts .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    for supervised learning , it is often useful to encode the input features in a new space to let the model focus on the relevant areas  @xcite .",
    "one of the basic encoding technique is _ feature discretization _ or _ feature quantization _",
    "@xcite that partitions the range of a continuous feature into intervals and relates these intervals with meaningful labels .",
    "recent overviews of discretization techniques can be found in  @xcite or  @xcite .",
    "obtaining the optimal discretization is a np - hard problem  @xcite , and an approximation can be easily obtained using a greedy approach , as proposed in decision trees : cart  @xcite and c4.5  @xcite , among others , that sequentially select pairs of features and cuts that minimize some purity measure ( intra - variance , gini index , information gain are the main examples ) .",
    "these approaches build decision functions that are therefore very simple , by looking only at a single feature at a time , and a single cut at a time .",
    "ensemble methods ( boosting  @xcite , random forests  @xcite ) improve this by combining such decisions trees , at the expense of models that are harder to interpret .",
    "[ [ organization - of - the - paper . ] ] organization of the paper .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + +    the main contribution of this paper is the idea to use a total - variation penalization , with an extra linear constraint , on the weights of a model trained on a binarization of the raw continuous features , leading to a procedure that selects multiple cut - points per feature , looking at all features simultaneously . the proposed methodology is described in section  [ section : methodology ] .",
    "section  [ sec : theoretical_results ] establishes an oracle inequality for generalized linear models .",
    "section  [ section : btv - experiments ] highlights the results of the method on various datasets and compares its performances to well known classification algorithms . finally , we discuss the obtained results in section  [ section : discussion ] .",
    "[ [ notations . ] ] notations .",
    "+ + + + + + + + + +    throughout the paper , for every @xmath1 we denote by @xmath2 the usual @xmath3-quasi norm of a vector @xmath4 namely @xmath5 , and @xmath6 .",
    "we also denote @xmath7 , where @xmath8 stands for the cardinality of a finite set @xmath9 .",
    "for @xmath10 , we denote by @xmath11 the hadamard product @xmath12 for any @xmath13 and any @xmath14 we denote by @xmath15 is the vector in @xmath16 satisfying @xmath17 for @xmath18 and @xmath19 for @xmath20 .",
    "we write @xmath21 ( resp .",
    "@xmath22 ) for the vector of @xmath16 having all coordinates equal to one ( resp .",
    "finally , we denote by @xmath23 the sub - differential of the function @xmath24 , namely @xmath25 if @xmath26 , @xmath27 if @xmath28 and @xmath29 $ ] .",
    "consider a supervised training dataset @xmath30 containing features @xmath31 and labels @xmath32 , that are independent and identically distributed samples of @xmath33 with unknown distribution @xmath34 .",
    "let us denote @xmath35_{1 \\leq i \\leq n ; 1 \\leq j \\leq p}$ ] the @xmath36 features matrix vertically stacking the @xmath37 samples of @xmath38 raw features .",
    "let @xmath39 be the @xmath40-th feature column of @xmath41 .",
    "the binarized matrix @xmath42 is a matrix with an extended number @xmath43 of columns , where the @xmath40-th column @xmath39 is replaced by @xmath44 columns @xmath45 containing only zeros and ones .",
    "its @xmath46-th row is written @xmath47 in order to simplify presentation of our results , we assume in the paper that all raw features @xmath39 are continuous , so that they are transformed using the following one - hot encoding .",
    "we consider a partition of intervals @xmath48 of the range of the coordinates of @xmath39 and define @xmath49 for @xmath50 and @xmath51 .",
    "a natural choice of intervals is given by quantiles , namely @xmath52 $ ] and @xmath53 $ ] for @xmath54 , where @xmath55 denotes a quantile of order @xmath56 $ ] for @xmath39 . in practice , if there are ties in the estimated quantiles for a given feature , we simply choose the set of ordered unique values to construct the intervals .",
    "this principle of binarization is a well - known trick  @xcite , that allows to construct a non - linear decision with respect to the raw features space .",
    "if training data contains also unordered qualitative features , one - hot encoding with @xmath0-penalization can be used for instance .      given a loss function @xmath57",
    ", we consider the goodness - of - fit term @xmath58 where @xmath59 and @xmath60 with @xmath61 .",
    "we then have @xmath62 , with @xmath63 corresponding to the group of coefficients weighting the binarized raw @xmath40-th feature .",
    "we focus on generalized linear models  @xcite , where the conditional distribution @xmath64 has a density @xmath65 with respect to a reference measure which is either the lebesgue measure ( e.g. in the gaussian case ) or the counting measure ( e.g. in the logistic or poisson cases ) , leading to a loss function of the form @xmath66 the density described in   encompasses several distributions , see table  [ table : glm - paper ] .",
    "only @xmath67 is unknown in the density  , the parameter @xmath68 is a scale parameter , and it is assumed that @xmath69 is three times continuously differentiable . it is standard to notice that @xmath70 = \\int yf^0(y | x ) dy = b'(m^0(x)),\\ ] ] where @xmath71 stands for the derivative of @xmath72 .",
    "this formula explains how @xmath71 links the conditional expectation to the unknown @xmath73 .",
    "the results given in section  [ sec : theoretical_results ] rely on the following assumption .",
    ".examples of standard distributions that fit in the considered setting of generalized linear models , with the corresponding constants in assumption  [ assumption - constant - glm ] . [",
    "cols=\"^,^,^,^,^,^,^\",options=\"header \" , ]      then , we get @xmath78 where @xmath79 . applying lemma  [ lemma : self - concordance ] with @xmath80",
    ", we obtain @xmath81 for all @xmath82 .",
    "taking @xmath83 leads to @xmath84 a short calculation gives that @xmath85 it is clear that @xmath86 = 0 $ ] .",
    "then @xmath87 now choose @xmath88 , and using assumption  [ assumption - constant - glm ] and in lemma  [ lemma : control_inner_ball ] , we have @xmath89 hence , we obtain @xmath90 with @xmath91 it entails that @xmath92      recall that for all @xmath60 , @xmath93 and @xmath94 according to proposition  [ proposition : kkt - conditions ] , equation   involves that there is @xmath95 , @xmath96 and @xmath97 such that @xmath98 for all @xmath60 , which can be written @xmath99 for any @xmath100 such that @xmath101 and @xmath102 the monotony of the subdifferential mapping implies @xmath103 @xmath104 and @xmath105 .",
    "therefore @xmath106 we consider now the function @xmath107 defined by @xmath108 by differentiating @xmath109 three times with respect @xmath110 , we obtain @xmath111 using lemma  [ lemma : control_inner_ball ] , we have @xmath112 .",
    "applying now lemma  [ lemma : self - concordance ] with @xmath113 , we obtain @xmath114 for all @xmath115 . taking @xmath83 and",
    "@xmath116 implies @xmath117 moreover , we have @xmath118 then , we deduce that @xmath119 then , with equation  , one has @xmath120 as @xmath121 , it implies that @xmath122 if @xmath123 it follows that @xmath124 then theorem  [ theorem - oracle - prediction - logistic - v2 ] holds . from now on , let us assume that @xmath125 we first derive a bound on @xmath126 let us define the block diagonal matrix @xmath127 , with @xmath128 , defined in  , being invertible .",
    "we denote its inverse @xmath129 which is defined by the @xmath130 lower triangular matrix with entries @xmath131 if @xmath132 and @xmath133 otherwise .",
    "we set @xmath134 . using @xmath135 , we focus on finding out a bound of @xmath136 in one hand , using that @xmath137 , one has @xmath138 where @xmath139 is the @xmath140-th column of the matrix @xmath141 let us consider the event @xmath142 then , on @xmath143 , we have @xmath144 in another hand , from the definition of the subgradient @xmath145 ( see equation  )",
    ", one can choose @xmath146 such that @xmath147 for all @xmath148 and @xmath149 for all @xmath150 .",
    "using a triangle inequality and the fact that @xmath151 , we obtain @xmath152 combining inequalities   and  , we get @xmath153 on @xmath143 .",
    "hence @xmath154 this means that @xmath155 see   and  .",
    "now , going back to   and taking into account  , the compatibility of @xmath156 ( see  ) , on @xmath143 the following holds @xmath157 then @xmath158 where @xmath159 such that @xmath160 for all @xmath161 and @xmath162 next , we find an upper bound for @xmath163 .",
    "we have @xmath164 note that @xmath165 .",
    "we write the set @xmath166 and we set @xmath167 for @xmath168 with the convention that @xmath169 and @xmath170",
    ". then @xmath171 therefore @xmath172    [ remark - leastsquares ] for the case of least squares regression where @xmath173 has gaussian distribution with mean @xmath174 and variance @xmath175 . using inequalities   and  , we get @xmath176 using the fact that @xmath177 it yields @xmath178 hence , we derive the following sharp oracle inequality @xmath179 where @xmath180    now for generalized linear models , we use the connection between the empirical norm and the kullback - leibler divergence .",
    "first , we have @xmath181 therefore , by lemma  [ lemma - connection - l2-kl ] , we get @xmath182 we now use the elementary inequality @xmath183 with @xmath184 . therefore becomes @xmath185 by choosing @xmath186 we get @xmath187 setting @xmath188 we get the desired result in  .",
    "finally , we have to compute the probability of the complementary of the event @xmath143 .",
    "this is given by the following : @xmath189 & \\leq \\sum_{j=1}^p \\sum_{k=2}^{d_j } { \\mathds{p}}\\big[\\frac{1}{n}\\big|{\\langle \\big({\\boldsymbol{x}^{{b}}}_{\\bullet , j } t_{j}\\big)_{\\bullet , k } , { { \\boldsymbol{y}}}- b'(m^0({{\\boldsymbol x } } ) ) \\rangle}\\big| \\geq \\hat w_{j , k}\\big]\\\\ & \\leq \\sum_{j=1}^p \\sum_{k=2}^{d_j } { \\mathds{p}}\\big[\\sum_{i=1}^n\\big|({\\boldsymbol{x}^{{b}}}_{\\bullet , j } t_{j})_{i , k}(y_i - b'(m^0(x_i)))\\big| \\geq { n\\hat w_{j , k } } \\big ] .",
    "\\end{split}\\ ] ] let @xmath190 and @xmath191 note that conditionally on @xmath192 , the random variables @xmath193 are independent .",
    "it can be easily shown ( see theorem 5.10 in  @xcite ) that the moment generating function of @xmath194 ( copy of @xmath195 ) is given by @xmath196 = \\exp\\big(\\phi^{-1}\\big\\{b(m^0(x ) + t ) - tb'(m^0(x ) - b(m^0(x)))\\big\\}\\big).\\ ] ] applying lemma 6.1 in  @xcite , using   and assumption  [ assumption - constant - glm ] , we can derive the following chernoff - type bounds @xmath197   \\leq 2\\exp\\big(- \\frac{n^2\\hat w^2_{j , k } } { 2u_n\\phi{\\|\\xi_{\\bullet , j , k}\\|}_2 ^ 2}\\big),\\ ] ] where @xmath198 we have @xmath199 therefore , @xmath200 : x_{i , j } \\in \\bigcup_{r = k}^{d_j } i_{j , r } \\big\\ } \\big ) = n\\hat \\pi_{j , k}.\\ ] ] using weights @xmath201 ( see   in theorem  [ theorem - oracle - prediction - logistic - v2 ] ) , and together with  , we find that the probability of the complementary event @xmath202 is smaller than @xmath203 this concludes the proof of theorem  [ theorem - oracle - prediction - logistic - v2 ] . @xmath204"
  ],
  "abstract_text": [
    "<S> this paper deals with the problem of large - scale linear supervised learning in settings where a large number of continuous features are available . </S>",
    "<S> we propose to combine the well - known trick of one - hot encoding of continuous features with a new penalization called _ </S>",
    "<S> binarsity_. in each group of binary features coming from the one - hot encoding of a single raw continuous feature , this penalization uses total - variation regularization together with an extra linear constraint to avoid collinearity within groups . </S>",
    "<S> non - asymptotic oracle inequalities for generalized linear models are proposed , and numerical experiments illustrate the good performances of our approach on several datasets . </S>",
    "<S> it is also noteworthy that our method has a numerical complexity comparable to standard @xmath0 penalization . </S>"
  ]
}