{
  "article_text": [
    "compressed sensing ( cs ) @xcite , @xcite is a powerful sub - nyquist sampling theory for the acquisition and recovery of sparse signals , that has received special attention in signal and image processing as well as other related fields such as statistics and computer science .",
    "the cs theory states that if the unknown signal is inherently sparse , then it is possible to acquire and reconstruct signal ( by solving a convex optimization problem ) with a much lower number of measurements that would be otherwise needed under the existing nyquist sampling scheme . in image processing",
    ", the cs theory is particularly relevant in several applications , such as magnetic resonant imaging ( mri ) @xcite or hyper - spectral imaging @xcite , where acquisition time and/or sensing hardware cost play a significant role .",
    "also , the sparsity assumption typically holds due to , for example , inherent wavelet structure in images @xcite .    in recent years",
    ", the cs literature has seen seen significant advances in both theory @xcite and applications @xcite ( many of which are collected in the cs repository ) .",
    "there are also a variety of specialized solvers for the cs recovery problem , which are developed from different angles , such as pursuit algorithms @xcite , @xcite , @xcite , optimization algorithms @xcite , a complexity regularization algorithm @xcite , and bayesian methods @xcite .    in this work ,",
    "we focus on a particular aspect of cs recovery , wherein the emphasis is on robustness .",
    "this is originally raised by pham and venkatesh @xcite .",
    "they recognize that existing cs recovery schemes can be statistically inefficient when the corruption of cs measurements is modeled as impulsive noise .",
    "such impulsive corruption can occur due to bit errors in transmission , malfunctioning pixels , faulty memory locations @xcite , and buffer overflow @xcite , and has been raised in many image processing works @xcite . to address this problem ,",
    "pham and venkatesh @xcite proposed a new formulation , known as robust cs , which combines traditional robust statistics @xcite and existing cs into a single framework to effectively suppress outliers in the recovery . whilst the focus of @xcite is on the theoretical justification of the new formulation",
    ", they also suggested a provably convergent algorithm to solve their robust cs formulation .",
    "this majorization minimization ( mm ) algorithm finds the robust cs solution by iteratively solving a number of cs problems , the solutions from which converge to the true solution .",
    "however , this is not computationally efficient because each iteration involves a full cs recovery , which is always iterative in nature .    to overcome the computational limitation of the original robust cs algorithm proposed in @xcite , we propose two new algorithms that directly solve the robust cs formulation .",
    "they both have only one main loop and iteratively majorize the original robust cs objective function .",
    "one algorithm is adapted from the fast iterative shrinkage thresholding ( fista ) framework developed by beck and teboulle @xcite , which shares the same spirit as an unpublished work of nesterov @xcite .",
    "the other algorithm is based on a framework known as alternating direction method of multipliers ( admm ) @xcite .",
    "even though the original fista scheme was derived for the original cs problem , it can be used for robust cs .",
    "our contribution is a theoretical result that allows one to compute the lipchitz constant for the application of fista .",
    "additionally , we also derive a generalized admm algorithm for solving the robust cs formulation efficiently , which differs from the fista algorithm in that operator splitting and approximation updates are used .",
    "this results in a method that has same update complexity as fista , but is more flexible to extend .",
    "furthermore , we also extend robust cs in a number of directions , including additional affine constraints , @xmath0-norm loss function , mixed - norm regularization , and multi - tasking .",
    "we show that the admm is a powerful optimization framework for the robust cs problem as it can be modified or generalized to cope with these extensions , where often other cs techniques , including fista , find impossible to do so .",
    "we show that the derived algorithms are simple to implement , provably convergent under the admm theory , and that they effectively solve complex robust cs formulations .",
    "the paper is organized follows .",
    "section ii gives some background on robust cs , whilst section iii describes the fista and admm algorithms for solving the robust cs formulation .",
    "section iv presents four extensions of the robust cs formulation and derive computationally efficient algorithms for solving them .",
    "section v contains numerical experiments to demonstrate the computational efficiency of the proposed algorithms .",
    "finally , section vi concludes the paper .",
    "all matlab code to implement our methods described in this paper and reproduce our results is readily available at the following website http://www.computing.edu.au/~dsp/code.php .",
    "in compressed sensing ( cs ) , one is interested in the recovery of a sparse signal @xmath1 though the compressed measurement @xmath2 here , @xmath3 is the cs matrix that represents the compressive sampling operation and @xmath4 is additive noise .",
    "the cs matrix is required to some stable embedding conditions for stable recovery @xcite . as @xmath5 in the cs setting ,",
    "the recovery of @xmath6 from @xmath7 is generally ill - posed .",
    "the cs theory has established that under an assumption that @xmath6 is sparse , it is possible to recover @xmath6 reliably from @xmath7 with an error upper bounded by the noise strength . among various approaches to solve the cs recovery problem ,",
    "the optimization formulation often provides the best achievability for a given cs matrix @xmath8 in the normal cs setting , the noise in ( [ equ_cs_model ] ) is often considered gaussian with bounded norm @xmath9 and thus the maximum error induced by a cs recovery is @xmath10 .",
    "however , pham and venkatesh @xcite have discovered that when the noise is indeed impulsive , such a result will still hold for normal cs recovery but is rather inefficient .",
    "thus , they propose a modification to the cs formulation , known as robust cs , to appropriately address the characteristics of the underlying additive noise .",
    "this is achieved by considering the robust loss function instead of the quadratic cost function in ( [ equ_cs_opt ] ) @xmath11 here , @xmath12 and @xmath13 is the huber s penalty function ( soft limiter ) given as follows @xmath14 and its derivative is given by @xmath15 the parameter @xmath16 of the huber s penalty function is determined by the fraction of the outliers whilst the scale parameter @xmath17 is often estimated from some statistic of the median , such as the median of the absolute deviation ( mad ) . for detail , see @xcite .",
    "as @xmath13 is quadratic or linear depending on the actual value of @xmath18 , solving ( [ equ_robust_opt ] ) directly is not trivial .",
    "pham and venkatesh suggested that instead of solving ( [ equ_robust_opt ] ) , a better alternative is to solve a series of the normal cs problems .",
    "the idea is to replace @xmath19 with an approximate quadratic function at every outer iteration with the general form @xmath20 where @xmath21 @xmath22 pham and venkatesh detailed two options for @xmath23 , which are commonly used in the robust statistics literature    * modified residuals ( mr ) : @xmath24 * iteratively reweighted : @xmath25 , @xmath26 .",
    "when using @xmath27 as shown in ( [ equ_lx ] ) for @xmath19 in ( [ equ_robust_opt ] ) , the resultant problem is essentially a normal cs problem and thus considered solved .",
    "whilst the above strategy will work , it is inefficient because each outer iteration involves a full cs problem and it is known that the cs problem needs to be solved iteratively as well .",
    "therefore , the double loops are the main computational deficiency of the above strategy . to address this limitation , we consider bypassing the inner cs step and thus there will be only one loop for the overall algorithm .",
    "there are two powerful optimization frameworks that are suitable for this purpose , which we describe next .",
    "fast iterative shrinkage thresholding ( fista ) is an optimization approach that effectively decouples the variables from the smooth loss function in the compressed sensing objective .",
    "this approach was proposed by @xcite , which also shares the same philosophy as an unpublished work of @xcite . technically , fista is a variant of majorization minimization ( mm ) algorithms @xcite and has a special choice for the quadratic majorization as well updates that involve historical points .",
    "consider minimizing a convex optimization of the form @xmath28 where @xmath29 here , @xmath19 is a smooth loss function , but the variables in this loss function are coupled .",
    "the core idea of fista is to consider a quadratic majorization of @xmath19 , denotes as @xmath30 , such that it effectively decouples the variables .",
    "if such decoupling is possible , the approximate problem is then easier to solve even when the regularization term @xmath31 is possibly non - smooth ( such as @xmath32 ) , because it can be decomposed into a number of univariate optimization problems whose solution is analytical .",
    "the first trick of fista is to decouple the variables by considering the majorization at iteration @xmath16 and approximation point @xmath33 @xmath34 here , @xmath33 is used as the approximation point rather than @xmath35 as it involves historical updates of @xmath35 by a careful choice , which is subsequently show in ( [ equ_fista_z ] ) . also , @xmath36 is the lipchitz constant of the gradient of the loss function @xmath19 to ensure that @xmath30 is a proper majorization of @xmath19 .",
    "thus , at iteration @xmath16 , fista finds @xmath35 via @xmath37 where @xmath38 . for the quadratic loss function @xmath39",
    ", it can be shown that @xmath40 , and @xmath41 .",
    "for the @xmath0-norm regularization as in the case of cs , this results in @xmath42 this problem can be solved element - wise and its solution is @xmath43 where the soft - thresholding shrinkage operator is defined as @xmath44 the second trick of fista is to use a clever update of the approximation point to speed up convergence @xmath45    the original fista framework can be readily used for robust cs case if @xmath46 and the lipchitz constant can be computed for the robust loss function . in case of @xmath46 , it can be easily seen that @xmath47 it remains to compute the lipchitz constant .",
    "to do so , we rely on the following result :    [ lemma_l_sum ] let @xmath48 be a smooth convex function on @xmath49 and suppose that the domain @xmath49 is divided into two regions @xmath50 and @xmath51 , such that @xmath52 if @xmath53 and @xmath54 if @xmath55 , and @xmath56 , and that @xmath57 for @xmath58",
    ". denote as @xmath59 and @xmath60 the lipchitz constants of @xmath61 and @xmath62 respectively on the domains @xmath50 and @xmath51 .",
    "then the lipchitz constant of @xmath63 is bounded by @xmath64    the proof of this lemma is detailed in the appendix .",
    "the result implies that for mixed functions like the robust cs loss functions being considered , we just take the sum of lipchitz constants over each continuous and bounded domain .",
    "the lipchitz constant for the quadratic part is as before , i.e. @xmath65 , whilst for the linear part we can split into negative and positive domain . in both cases , the lipchitz constant is zero due to the fact that @xmath66 is a constant .",
    "thus , the lipchitz constant for the robust cs cost function is still @xmath67 .",
    "alternating direction method of multipliers ( admm ) is a simple but powerful framework in optimization , which is suited for today s large - scale problems arising in machine learning and signal processing .",
    "the method was in fact developed a long ago before advanced computing power was available , and re - discovered many times under different perspectives .",
    "recently , @xcite has unified the framework in a simple and concise explanation . in either the cs or robust cs problem , the main technical challenge is that the variables are coupled through @xmath68 in either the quadratic or robust loss function .",
    "this makes it rather difficult when the extra constraint with non - smooth @xmath0 norm is introduced . in principle , the problem is easier to tackle if the variables can be decoupled , so that the problem can be solved element - wise or group - wise . using a clever trick , known as operator splitting @xcite , the admm framework suggests to separate the regularization term from the smooth term by introducing an additional variable @xmath69 , which is tied to the original variable via an affine constraint : @xmath70 here , @xmath19 is the robust cs loss function . for this type of regularized objective function",
    ", admm considers the following augmented lagrangian @xmath71 here , @xmath72 is the parameter associated with the augmentation @xmath73 , and this is to improve the numerical stability of the algorithm .",
    "the strategy for minimizing this augmented lagrangian is iterative updating of the primal and dual variables . with a further normalization on the dual variable @xmath74",
    ", it is shown @xcite that as far as the primal and dual variables @xmath6 and @xmath69 are concerned @xmath75 where the constant is independent of @xmath6 and @xmath69 ( actually @xmath76 ) .",
    "note of the semi - colon , which treats @xmath77 as a parameter rather than a variable when solving for other variables .",
    "thus , the optimality point of the lagrangian can be found by iteratively updating the variables as follows : @xmath78 we note that the update steps for @xmath77 and @xmath69 are straightforward . in particular , for @xmath69 it is known that it is a soft - thresholding shrinkage operation @xmath79    due to the nature of @xmath19 , there is no exact solution for ( [ equ_x_step ] ) , and finding it always necessitates iterative algorithms .",
    "this will increase computational burden to the overall algorithm in a similar way as the previous robust cs algorithms introduced in @xcite . to alleviate the computational problem",
    ", we propose to follow a novel framework , known as generalized admm and developed by eckstein and bertsekas @xcite . in generalized admm",
    ", the update steps can be solved _",
    "approximately _ as long as the differences between the exact and approximate solutions generate a summable sequence .",
    "when such a condition is satisfied , the generalized admm theory has proved that the algorithm will converge to the solution ( * ? ? ?",
    "* theorem 8) .    to utilize the generalized admm theory ,",
    "once again we adapt an mm algorithm to solve ( [ equ_x_step ] ) , which is in the same spirit as the original robust cs @xcite . in essence , this replaces @xmath19 with a suitable quadratic majorization as discussed previously .",
    "the major difference is that we only perform the minimization of the majorization _ once _ , as opposed to iteratively as in @xcite .",
    "specifically , we propose to modify the update step for @xmath6 in ( [ equ_x_step ] ) by using the quadratic approximation of @xmath19 at iteration @xmath16 as @xmath27 ( shown in ( [ equ_lx ] ) ) @xmath80 it can be easily recognized that the solution of this problem is exact @xmath81 we note that the quadratic approximation of fista can also be used .",
    "however , the choice above leads to a better approximation and hence will converge to the true solution faster .",
    "it is also easily seen that for the mr choice of the quadratic approximation where @xmath24 , the matrix under inversion in ( [ equ_x_final ] ) is fixed @xmath82 hence , the inversion @xmath83 can be computed once and cached so that the update step in subsequent iterations can be fast .",
    "the generalized admm for the specific case being considered can be stated as follows :    consider an admm algorithm that solves the convex problem ( [ equ_robust_opt ] ) via the updates ( [ equ_x_final2 ] ) , ( [ equ_z_step ] ) , ( [ equ_u_step ] ) .",
    "denote as @xmath84 the exact solution of ( [ equ_x_step ] ) , and as @xmath85 the approximate of ( [ equ_x_step ] ) via ( [ equ_x_final2 ] ) .",
    "if the sequence @xmath86 is summable , i.e. , @xmath87 , then the above updates will generate a sequence @xmath88 that converge to the true solution of ( [ equ_robust_opt ] ) .",
    "next , we discuss the convergence stopping condition of the proposed generalized admm algorithm .",
    "when the update steps are solved exactly , the existing admm theory @xcite states that the penalty parameter @xmath72 affects both the primal residual ( defined as @xmath89 ) , and the primal residual ( defined as @xmath90 ) in an opposite manner : a large @xmath72 tends to generate a small primal residual and a large dual residual and vice versa .",
    "thus , selecting the optimal penalty parameter is typically a trade - off between primal and residual residuals with an admm algorithm , and @xmath91 generally works for most cases .",
    "however , more emphasis should be made to the primal residual in the case of the proposed generalized admm algorithm because the update step of the primal variable @xmath6 is not solved exactly .",
    "this will ensure that the approximation error in the primal variable is promptly compensated by the dual update , at the small sacrifice in convergence rate due to the residual error being slightly larger .",
    "intensive numerical studies suggest that a value for @xmath72 of between 2 and 5 for @xmath72 works rather well in many cases .",
    "we shall examine this in more detail in the experimental section , where we use @xmath92 . for stopping condition",
    ", we terminate the algorithm when the primal and dual variables are sufficiently small . for standard settings of absolute and relative tolerances",
    "please see @xcite .",
    "the fista and admm algorithms for robust cs presented tackle the optimization from slightly different angles . whilst fista solves the problem by replacing the robust cost function with a simpler quadratic approximation that decouples the variables , the admm decouples the @xmath0 regularization norm via operator splitting . whilst fista has only one approximation ,",
    "admm involves operator splitting _ and _ quadratic approximation at the step that updates @xmath6 .",
    "thus it appears that fista may have a convergence advantage due to being simpler and having less tuning requirements .",
    "however , numerical experience indicates that for a given tolerance , the admm algorithm is actually faster than fista in terms of both number of iterations or computational time to reach a given tolerance .",
    "this will be illustrated further in the experimental section .",
    "the advantage of admm is better realized when one needs to extend robust cs in similar ways as many extensions on the basic cs have been made in the literature .",
    "this is difficult , if not impossible , with the fista scheme .",
    "next , we discuss several possible extensions that can be simply achieved with the proposed admm algorithm .      in some cases",
    ", one would like to impose additional affine constraints on the optimization problem @xmath93 .",
    "this could be of prior knowledge on the power modeling ( i.e. , when @xmath94 is known a priori ) and this could potentially improve stabilization of the cs solution .",
    "thus , the lagrangian ( [ equ_lagrangian1 ] ) could be altered as follows @xmath95 here , @xmath96 and @xmath97 are the dual variables for the equality constraints .",
    "again , by scaling the dual variables @xmath98 and @xmath99 we obtain @xmath100 thus , the admm update step for @xmath6 is the solution of the problem @xmath101 once again , if this step is to be solved approximately using a quadratic majorization with @xmath24 as discussed previously then it can be shown that @xmath102 where @xmath103 .",
    "it can be shown that the updates step for @xmath69 remains the same as ( [ equ_z_final ] ) except that @xmath77 and @xmath72 are replaced with @xmath104 and @xmath105 respectively .",
    "finally , the updates of the dual variables are @xmath106 just like the basic admm algorithm , convergence is determined when both the primal and dual residuals are sufficiently small . whilst the dual residual is as before , i.e. ,",
    "@xmath107 , there are effectively two residual vectors @xmath108 and @xmath109 . depending on the desired accuracy requirement of a particular application",
    ", the stopping criterion can be determined accordingly ( see @xcite ) .      in certain situations , one may wish to impose @xmath110 regularization on the solution of the recovery .",
    "such a motivation may arise from the fact that the absolute sparse model may not be realistic , and thus it is more desirable to consider @xmath111 even in the sparse case , an additional quadratic regularization with a small @xmath112 could improve numerical stability against rank deficiency of @xmath68 .",
    "in the case of quadratic loss function , i.e. , @xmath113 , this is known as the elastic - net @xcite .",
    "thus , the proposed formulation could be interpreted as a robust version of the elastic - net .",
    "the robust cs formulation is treated a special case when @xmath114 .    for the original elastic - net",
    ", it is easily recognized that a simple algebra can convert it to a lasso ( or cs ) form , and thus it can be solved with many efficient @xmath0-regularization algorithms . for the proposed robust elastic - net",
    ", it is not possible because of the loss function @xmath19 being not quadratic .",
    "however , it is trivial to show that it is possible to modify the fista and generalized admm algorithms discussed in the previous section to cater for this additional regularization term . indeed",
    ", this regularization term only affects the update step of @xmath6 . in both fista and generalized admm",
    ", the majorization is a quadratic function and thus absorbing this extra quadratic term is straightforward .",
    "for example , in the case of the fista algorithm , we need to solve ( c.f .",
    "( [ equ_fista ] ) @xmath115 which is equivalent to @xmath116 which is of the same form and this induces the soft - thresholding shrinkage operation .",
    "likewise , in the case of the generalized admm algorithm , we need to to solve ( c.f ( [ equ_x_mm ] ) ) @xmath117 and thus this has only a slight modification compared with ( [ equ_x_final ] ) @xmath118 thus , extension to mixed - norm regularization is straightforward of the proposed admm algorithm .      in the original robust cs paper @xcite ,",
    "the huber loss is selected .",
    "this is suitable for impulsive noise being modeled as a contaminated mixture @xcite .",
    "however , the robust cs framework is not necessarily restricted to the huber loss function and indeed many loss functions in the robust statistics can be used to cater for different noise types .",
    "one particular interest is the @xmath0-norm loss function , which is optimal when the impulsive noise is modeled as a cauchy distribution @xcite . in this case ,",
    "@xmath119 and thus it is desirable to solve @xmath120 we note that the fista algorithm is not easily derived , because the loss function is not differentiable .    to overcome the difficulty associated with two parts of the objective function that are both non - differentiable , we propose to apply the operator splitting mechanism of the admm framework twice .",
    "specifically , we introduce two additional variables @xmath121 and @xmath69 and rewrite the formulation as @xmath122 thus , the augmented lagrangian is @xmath123 with the scaled dual variables @xmath98 and @xmath124 , we can rewrite @xmath125 with this form , the updates for the variables are easily computed under the admm principle . for @xmath6 , the update solves the problem @xmath126 which yields the exact solution @xmath127 for both @xmath121 and @xmath69 , it is easily recognized that the update steps are simple soft - thresholding operations . for @xmath121 ,",
    "the update step solves @xmath128 where @xmath129 .",
    "likewise , for @xmath69 the update step solves @xmath130 they both have a similar form as ( [ equ_z_step ] ) , and thus from ( [ equ_z_final ] ) we deduce ( c.f . ( [ equ_shrinkage ] ) ) @xmath131 as the updates for @xmath121 and @xmath69 .",
    "finally , the dual updates are @xmath132 the stopping criterion is when the residual vectors are sufficiently small , including @xmath133 , @xmath134 , @xmath108 , and @xmath135 .",
    "the recent literature on cs also reveals that the basic sparsity recovery scheme can be improved if one exploits further domain knowledge .",
    "such an exploitation could be based on the constraint of the sparsity models .",
    "extensions , such as model - based cs @xcite and group sparsity @xcite , are key examples of the exploitation that can effectively reduce the cs requirements for a comparable recovery error when compared with conventional cs . here",
    ", we focus on a slight variation where there are multiple cs tasks to be performed : there are multiple cs measurements @xmath136 , each follows the model @xmath137 .",
    "haar wavelet coefficients - multi random bars ]    in the image processing context , this could arise in , for example , compressed sensing of multiple video images . in these circumstances",
    ", there many be similarities between images .",
    "for example , moving images likely consist of relatively same large background and small moving objects .",
    "thus , the sparse representation of these original images may have similar sparse coefficients representing the common background part ( see fig .",
    "[ fig_mt_example ] for an illustration of a sequence of random bars images used later in the experiment ) .",
    "for that reason , it follows from the existing results on advanced cs @xcite that exploiting the shared structure between tasks is likely to improve cs recovery compared to the case where the tasks are performed independently .",
    "denote as @xmath138 $ ] the collection of sparse vectors to be recovered from the tasks , and @xmath139 $ ] the collection of cs measurements .",
    "extending the single - task robust cs , the multi - task robust cs can be formulated as follows @xmath140 here , @xmath141 and @xmath142 where @xmath143 s denote the columns of @xmath144 .",
    "clearly the loss term is the same , whilst for the regularization terms , we seek sparsity along the columns of @xmath145 but denseness along the rows of @xmath145 .",
    "this clearly reflects the prior assumption that sparse coefficients of the common parts are likely to be similar , hence the corresponding rows of @xmath145 should be dense , whilst it is sparse column - wise to respect the single - task cs s assumption .",
    "when @xmath146 is a quadratic loss function , this is a special matrix formulation of group lasso in the statistics literature @xcite .",
    "we now show that it is possible to extend both the fista and generalized admm algorithms to cater for this formulation . before doing so",
    ", we present a generalization of the soft - thresholding shrinkage operation as follows :    the optimization problem @xmath147 has the solution @xmath148    this result can be proved by simple geometrical arguments .",
    "indeed , denote @xmath149 as the solution of ( [ equ_gs ] ) , then we consider all points @xmath69 such that @xmath150 .",
    "it turns out that these points are lying on the ball with center at @xmath121 and radius @xmath151 . among these points ,",
    "only the point that satisfies @xmath152 , i.e. , intersection of the ball and the vector @xmath121 , will have minimum @xmath110 norm , which minimizes the second term in ( [ equ_gs ] ) .",
    "substituting this into ( [ equ_gs ] ) yields the form of the soft - thresholding shrinkage problem , for which the result is obtained after simple manipulations .      generalizing ( [ equ_fista ] ) for the multi - task settings , denote as @xmath153 $ ] , where @xmath154",
    "then , the update step for @xmath145 solves @xmath155 this problem can be written row - wise in the form of ( [ equ_gs ] ) and thus the solution is exact . meanwhile , the update step for @xmath156 is also similar @xmath157 with @xmath158 .",
    "we rewrite the lagrangian for the current setting as follows @xmath159 thus , the admm update steps are @xmath160 like fista , the update step of @xmath156 can easily be decomposed row - wise , each has the form of ( [ equ_gs ] ) , and thus the solution for each row of @xmath161 can be obtained immediately . for the update step of @xmath145 ,",
    "again we resort to the generalized admm principle .",
    "we approximate @xmath146 with a quadratic loss at @xmath162 @xmath163 thus , the generalized admm algorithm finds the update via @xmath164 which yields the following solution @xmath165 where @xmath153 $ ] , @xmath166 .",
    "the stopping criterion is when all primal and dual residual matrices are small , they include @xmath167 like the single - task case , one should set @xmath72 sufficiently large to obtain a smooth decrease of the objective function .      _ further extensions .",
    "_ we have presented some fundamental extensions of the cs formulation . under the admm frameworks",
    ", it appears that it is possible to consider extensions based on the combination of the basics extensions presented .",
    "for example , the @xmath0 loss could be used with affine constraint or in multi - task setting , etc .",
    "such extensions will be worthwhile investigation for future work .",
    "_ regularization path .",
    "_ in practice , the optimal value of the regularization @xmath168 is not known in advance , and thus one needs to select a proper value to do robust cs recovery .",
    "such a problem is known in statistics as model selection .",
    "typically , one needs to compute the recovery along the regularization path , and select the one which meets the @xmath0 norm constraint .",
    "this is discussed in detail in @xcite .",
    "essentially , some estimates of the noise statistics must be obtained in order to construct the bound on the residual @xmath169 .",
    "it is well - known that there exist a @xmath170 above which the solution is zero . for decreasing values of @xmath168 ,",
    "the residual @xmath171 will become smaller whilst the recovery becomes denser .",
    "the optimal @xmath168 is the maximum value of @xmath168 such that the bound constraint on the residual vector is met . in cs recovery",
    ", this happens when @xmath172 , whilst in robust cs recovery , pham and venkatesh @xcite have suggested @xmath173 , which is a generalization of the cs selection criteria for the robust case . in our implementation , we combine a coarse grid search and a fine bi - section search to find this optimal @xmath168 ( see fig . [ fig_reg_path ] for an illustration ) .",
    "regularization path of robust cs for random bars example ]    _ cholesky factorization .",
    "_ as can be seen , most update step of @xmath6 in different admm variants involves the computation of the form @xmath174 where @xmath175 is an positive definite matrix .",
    "the matrix under inversion has a size of @xmath176 and it is large in image processing application .",
    "thus , it is inefficient to compute the inversion directly to obtain the update . a much more efficient approach is to use cholesky decomposition to achieve the goal .",
    "it is known from linear algebra that if @xmath177 is a positive definite matrix then it admits the factorization @xmath178 and thus @xmath179 can be efficiently computed by solving @xmath180 first , then @xmath181 , which can be written as @xmath182 . for compressed sensing applications where @xmath68 is a fat matrix ,",
    "further exploitation can be made by reducing the dimension of the matrix for cholesky factorization .",
    "indeed , according to the matrix inversion lemma @xmath183 where @xmath184 .",
    "suppose that the cholesky factorization of @xmath185 is @xmath186 then @xmath187 we can avoid the direct inversion of @xmath175 by exploiting the fact that if @xmath188 then the matrix inversion lemma once again gives @xmath189 where @xmath190 .",
    "finally , we note that this cholesky factorization is independent of the regularization parameter @xmath168 and thus it can be cached for the whole regularization path to reduce computation .",
    "we examine the convergence property of the fista and admm algorithms and compare them with the previously proposed method in @xcite , which we refer to as nested robust cs algorithm due to the nature of the double loops inside that algorithm . as",
    "the nested robust cs algorithm @xcite is dependent on the particular cs solver being used for the inner loop , we select the admm implementation as the cs solver because it provides the best computational accuracy and speed . note that pham and venkatesh @xcite used the ` l1_ls ` algorithm originally , which is known for high - accuracy but computationally expensive .",
    "however , numerical experience shows that the inner steps do not required to be solved with high accuracy .",
    "thus , the admm implementation as a cs solver for the nested robust cs algorithm is better overall . in this case",
    ", it can be seen that the computational complexity per iteration ( regardless of inner or outer ) in all compared algorithms are approximately the same : they all involve the computation of the majorization point and the soft - thresholding shrinkage operation .    to compare the algorithms , we examine two aspects : the error versus the iterations and the computational time taken to achieve a particular tolerance . whilst the former indicates how fast an algorithm converges , the latter provides a much valuable insight for practical purpose . to do so , we let all algorithms run for sufficiently large number of iterations and measure the error ( with respect to the true value of the robust cs solution ) as iterations go on , and the computational time taken when the error reaches certain thresholds . for the admm - based cs solver used in the inner loop of the nested robust cs algorithm , we select the termination with relative tolerance of @xmath191 and absolute tolerance of @xmath192 ( see @xcite ) .",
    "this allows a reasonable convergence within the inner loops .",
    "we also choose the modified residual approach for nested robust cs as it is simpler without loosing convergence advantage .",
    "all algorithms are implemented in matlab , and roughly optimized .",
    "we revisit the random bars example in @xcite ( see also fig . [ fig_random_bars ] ) and the results of this study is shown in fig .",
    "[ fig_conv_random ] . in this example , the signal to noise ratio is 20db and the impulsive noise is modeled as a two - component gaussian mixture model where the there is 10% contamination whose variance is @xmath193 times that of the main component . here , the left subplot shows the reduction of the error versus the iterations , whilst the right plot shows the time taken to achieve the relative accuracy from initialized zeros ( as indicated by 1e0 ) to as small as @xmath194 of the initial error ( as indicated by 1e-10 ) .",
    "we note the error profile of the nested robust cs algorithm ranges considerably due to the fact that we measure with respect to the global solution of the outer loop and that within each cs inner loop the algorithm still converges normally .",
    "clearly the error profile plot indicates that the admm algorithm offers the best convergence speed per iteration , followed by the fista algorithm .",
    "for example , to achieve an accuracy of @xmath195 of the initial error , it only takes the admm algorithm less than 100 iterations , whilst the fista algorithm needs to spend more than 20 times , and the nested algorithm would need 200 times the number of iterations . in terms of the actual time taken to achieve a particular tolerance ,",
    "the right subplot further indicates the advantage of admm and fista algorithms over the nested one . in practice",
    ", one would be interested in the tolerance of between @xmath191 to @xmath196 , over which the admm and fista algorithms are observed to be 100 and 10 times faster than the nested algorithm respectively .    in fig .",
    "[ fig_random_bars ] , we shows the actual image recovery of all compared methods , including the cs , the nested robust cs , the fista robust cs , and the admm robust cs algorithms on this random bars example .",
    "the original random bars image is shown on the top left subplot , whilst its haar wavelet coefficients are shown on the top right subplot . the results clearly show that all robust cs methods achieve an psnr of about 26db , which is 1.5db better that that of conventional cs recovery .",
    "we note that there is a very minor different between robust cs algorithms , due to different convergence termination conditions , which is unavoidable .",
    "next , we examine how much improvement can be made to robust cs if the power is known .",
    "the affine robust cs formulation is slightly different to the robust cs formulation in that additional constraint @xmath93 is imposed , and here we select @xmath197 and assume that @xmath198 is known .",
    "first , we examine the convergence behavior of the affine admm robust cs algorithm to solve this formulation by revisiting the random bars example . in this case",
    ", we select @xmath199 and let the algorithm run over sufficient number of iterations . the results are shown in fig .",
    "[ fig_conv_affine ] .",
    "again , the left subplot shows the absolute error against the iterations whilst the right subplots indicates computational time taken to reach a particular accuracy .",
    "compared with those of the admm robust cs algorithm , it can be clearly seen that the affine admm robust cs algorithm takes more time to reach .",
    "this is as expected because there are only minor changes to the update steps of the primal and dual variables .",
    "next , we examine the actual image recovery of affine robust cs formulation . once again , the random bars example is used and the recovered images are shown in fig .",
    "[ fig_affine ] . here , we compare with the robust cs formulation via the admm algorithm .",
    "the result indicates that there is a slight gain in the recovery , though it is rather little . as a result ,",
    "the recovered images look similar .",
    "next , we demonstrate the robust cs algorithm with @xmath0 loss function rather than the huber s loss function used in @xcite .",
    "this is useful in situations with very impulsive corruption , where the noise is best modeled by a cauchy distribution .",
    "to do so , we revisit the random bars example , but we use cauchy noise instead . for the @xmath0 admm robust cs algorithm ,",
    "the model selection criteria is the @xmath0 norm of the residual , rather than the huber s loss function to reflect the new formulation .",
    "other than that , all other experimental settings remain the same .",
    "first , we examine the convergence behavior of the admm robust cs algorithm with @xmath0 loss . fig .",
    "[ fig_conv_l1_admm ] shows the typical convergence behavior of the algorithm in terms of accuracy versus iterations ( left ) and computational time taken to reach certain accuracy ( right ) .",
    "it is observed that the convergence is slower with modest accuracy as compared with the formulation using huber s loss function .",
    "this is as expected from admm optimization theory due to an increasing number of variables to solve the @xmath0 loss formulation .",
    "nevertheless , modest accuracy might be sufficient for many practical situations .",
    "next , we examine image recovery quality in cauchy noise .",
    "[ fig_cauchy ] shows the image recovery for cs , robust cs using nested , admm , and @xmath0-regularized admm algorithms respectively . due to cauchy noise",
    ", it is of interest to note that the cs completely fails with no meaningful pattern recovered .",
    "the other nested and admm algorithm still maintain reasonably recovery quality with an psnr of around 21db .",
    "the @xmath0-regularized admm algorithm achieves the best result with an psnr of 25db , a significant improvement compared with the other two robust cs algorithms .",
    "it is also noted that the computational time of the @xmath0-regularized admm algorithm is almost equal to that of the admm robust cs algorithm due to the fact that the update steps of the two algorithms have similar complexity .    though the @xmath0 loss is primarily used for noise modeling as the cauchy distribution",
    ", it is still of interest to examine how it behaves if the noise is modeled as from the gaussian mixture as used previously .",
    "we again revisit the settings in the previous experiment and the result is shown in fig .",
    "[ fig_gmm ] .",
    "surprisingly , the @xmath0-loss formulation provides a considerable psnr gain of 4db over the huber s loss robust cs formulation .",
    "thus , despite having less favorable convergence properties , the robust cs formulation with @xmath0 loss still appears a better performer for practical image recovery .",
    "finally , we demonstrate the usefulness of the multi - task robust cs formulation when a sequence of 10 compressed images corrupted by impulsive noise need to be recovered . whilst each image in the sequence",
    "can be recovered separately , the multi - task robust cs formulation suggests that exploiting the shared structure between the tasks may provide better recovery . to do so",
    ", we consider a sequence of random bars frames shown in the top row of fig .",
    "[ fig_mt_recovery ] . here",
    ", there are common static random bars and a moving block across the frames .",
    "obviously , the wavelet coefficients for common static bars are shared between the cs tasks .",
    "only the coefficients corresponding to the moving block distinguish between tasks .",
    "this is clearly illustrated in fig .",
    "[ fig_mt_example ] which shows an image plot of haar wavelet coefficients of all 10 random bars image in a sequence : the horizontal lines correspond to common coefficients .",
    "the settings for the recovery are the same as previous experiments . for robust cs",
    ", we select the admm algorithm , and similarly for multi - task robust cs we also select the corresponding multi - task admm algorithm .",
    "the first 4 recovered images are shown in fig .",
    "[ fig_mt_recovery ] : the second row shows cs recovery , the third row shows robust cs recovery , and finally the last row shows multi - task robust cs recovery .",
    "the actual psnrs for every frame are shown on fig .",
    "[ fig_mt_psnr ] . here , we observe clearly that , on average , the multi - task robust cs formulation does provide a significant improvement over the robust cs formulation , both of which outperform cs recovery considerably .",
    "we have presented more computationally efficient and extendable approaches to the recently proposed robust cs algorithm . we have also extended robust cs formulation in a number of ways , including affine constraints , @xmath0-loss function , and multi - task formulation . for improving computational efficiency of robust cs",
    ", we found that the ( generalized ) admm robust cs algorithm is the best , then followed by the fista robust cs algorithm .",
    "we also found that imposing affine constraint can provide improvement , though slightly .",
    "the striking result is that @xmath0 loss formulation for robust cs seems to offer considerable gain over the huber s loss formulation , despite the fact that its convergence seems slower . finally , in the case where one needs to robustly recover a sequence of compressed images , the multi - task formulation is proved to provide additional advantages in terms of both psnr output and computational speed .",
    "we start from the definition of the lipchitz constant as a term such as @xmath200 as there are two possible scenarios @xmath201 , @xmath202 , and @xmath203 and from the definition of @xmath59 and @xmath60 , we immediately have @xmath204 where @xmath205 is defined as the minimum constant such that @xmath206 let @xmath207 .",
    "for arbitrary @xmath208 and @xmath209 we construct @xmath210 such that it is a convex combination of @xmath211 and @xmath212 , so that @xmath213 and @xmath214",
    ". then using triangle inequalities and definitions of @xmath59 and @xmath60 , we have @xmath215 the proof immediate follows from ( [ equ_sup_lf ] ) and ( [ equ_sup12 ] ) .",
    "s.  boyd , n.  parikh , e.  chu , b.  peleato , and j.  eckstein , _ foundations and trends in machine learning_.1em plus 0.5em minus 0.4emnow publisher , 2011 , vol .  3 , no .  1 , ch . distributed optimization and statistical learning via the alternating direction method of multipliers , pp .",
    "e.  candes , j.  romberg , and t.  tao , `` robust uncertainty principles : exact signal reconstruction from highly incomplete frequency information , '' _ ieee transactions on information theory _ , vol .",
    "52 , no .  2 ,",
    "489509 , 2006 .",
    "r.  chan , c .- w . ho , and m.  nikolova , `` salt - and - pepper noise removal by median - type noise detectors and detail - preserving regularization , '' _ ieee transactions on image processing _ , vol .  14 , no .  10 , pp . 14791485 , 2005 .",
    "d.  donoho and g.  reeves , `` the sensitivity of compressed sensing performance to relaxation of sparsity , '' in _ proceedings of the ieee international symposium on information theory ( isit)_.1em plus 0.5em minus 0.4emieee , 2012 , pp .",
    "22112215 .",
    "j.  eckstein and d.  bertsekas , `` on the douglas - rachford splitting method and the proximal point algorithm for maximal monotone operators , '' _ mathematical programming _ , vol .",
    "55 , no .  1 ,",
    "pp . 293318 , 1992 .",
    "z.  guo , t.  wittman , and s.  osher , `` l1 unmixing and its application to hyperspectral image enhancement , '' in _ proceedings spie conference on algorithms and technologies for multispectral , hyperspectral , and ultraspectral imagery xv _ , vol . 7334 , 2009 , pp .",
    "73341m73341 m .",
    "t.  hashimoto , `` bounds on a probability for the heavy tailed distribution and the probability of deficient decoding in seequential decoding , '' _ ieee transactions on information theory _",
    "51 , no .  3 , pp .",
    "9901002 , 2005 .",
    "kim , k.  koh , m.  lustig , s.  boyd , and d.  gorinevsky , `` a method for large - scale @xmath0-regularized least squares , '' _ ieee journal on selected topics in signal processing _ , vol .  4 , no .  1 ,",
    "pp . 606617 , 2007 .",
    "n.  rao , r.  nowak , s.  wright , and n.  kingsbury , `` convex approaches to model wavelet sparsity patterns , '' in _ proceedings of the ieee international conference on image processing_.1em plus 0.5em minus 0.4emieee , 2011 , pp .",
    "19171920 .",
    "m.  yuan and y.  lin , `` model selection and estimation in regression with grouped variables , '' _ journal of the royal statistical society : series b ( statistical methodology ) _ , vol .",
    "68 , no .  1 ,",
    "4967 , 2006 ."
  ],
  "abstract_text": [
    "<S> compressed sensing ( cs ) is an important theory for sub - nyquist sampling and recovery of compressible data . </S>",
    "<S> recently , it has been extended by pham and venkatesh @xcite to cope with the case where corruption to the cs data is modeled as impulsive noise . </S>",
    "<S> the new formulation , termed as robust cs , combines robust statistics and cs into a single framework to suppress outliers in the cs recovery . to solve the newly formulated robust cs problem , </S>",
    "<S> pham and venkatesh suggested a scheme that iteratively solves a number of cs problems , the solutions from which converge to the true robust compressed sensing solution . </S>",
    "<S> however , this scheme is rather inefficient as it has to use existing cs solvers as a proxy . to overcome limitation with the original robust cs algorithm </S>",
    "<S> , we propose to solve the robust cs problem directly in this paper and drive more computationally efficient algorithms by following latest advances in large - scale convex optimization for non - smooth regularization . </S>",
    "<S> furthermore , we also extend the robust cs formulation to various settings , including additional affine constraints , @xmath0-norm loss function , mixed - norm regularization , and multi - tasking , so as to further improve robust cs . </S>",
    "<S> we also derive simple but effective algorithms to solve these extensions . </S>",
    "<S> we demonstrate that the new algorithms provide much better computational advantage over the original robust cs formulation , and effectively solve more sophisticated extensions where the original methods simply can not . </S>",
    "<S> we demonstrate the usefulness of the extensions on several cs imaging tasks . </S>"
  ]
}