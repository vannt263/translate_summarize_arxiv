{
  "article_text": [
    "the proliferation of mobile devices , ubiquity of the web , and plethora of sensors has led to an exponential increase in the amount data created , stored , managed , and processed . in march 2014 ,",
    "an ibm report claimed that 90% of the world s data had been generated in the last two years @xcite .",
    "big data characterizes the problems faced by conventional analytics systems with this dramatic expansion of data volume , velocity , and variety .    to address the challenges posed by big data ,",
    "analytical systems are shifting from shared , centralized architectures to distributed , decentralized architectures .",
    "the mapreduce framework , and its open - source variant , hadoop , exemplifies this effort by introducing a programming model to facilitate efficient , distributed algorithm execution while abstracting away lower - level details @xcite . since inception",
    ", the hadoop / mapreduce ecosystem has grown considerably in support of related big data tasks .",
    "however , these distributed frameworks are not suited for all purposes , in many cases can even result in poor performance  @xcite .",
    "algorithms that make use of multiple iterations , especially those using graph or matrix data representations , are particularly poorly suited for popular big data processing systems .",
    "graph computation is notoriously difficult to scale and parallelize , often due to inherent interdependencies within graph data @xcite .",
    "as big data drives graph sizes beyond the memory capacity of a single machine , data must be partitioned to out - of - memory storage or distributed memory .",
    "however , for sequential graph algorithms , which require random access to all graph data , poor locality and the indivisibility of the graph structure cause time- and resource - intensive pointer - chasing between storage mediums in order to access each datum .    in response to these shortcomings , new frameworks based on the _ vertex - centric programming model _",
    "have been developed with the potential to transform the ways in which researchers and practitioners approach and solve certain problems  @xcite .",
    "vertex - centric computing frameworks are platforms that iteratively execute a user - defined program over vertices of a graph .",
    "the user - defined vertex function typically includes data from adjacent vertices or incoming edges as input , and the resultant output is communicated along outgoing edges .",
    "vertex program kernels are executed iteratively for a certain number of rounds , or until a convergence property is met .",
    "as opposed to the randomly - accessible ,  global \" perspective of the data employed by conventional shared - memory sequential graph algorithms , vertex - centric frameworks employ a local , vertex - oriented perspective of computation , encouraging practitioners to  think like a vertex \" ( tlav ) .",
    "the first published tlav framework was google s pregel system @xcite , which , based off of valiant s bulk synchronous parallel ( bsp ) model @xcite , employs synchronous execution .",
    "while not all tlav frameworks are synchronous , these frameworks are first introduced here within the context of bsp in order to provide foundational understanding of tlav concepts .",
    "after spending a year with bill mccoll at oxford in 1988 , les valiant published the seminal paper on the bulk synchronous parallel ( bsp ) computing model  @xcite for guiding the design and implementation of parallel algorithms .",
    "initially touted as `` a bridging model for parallel computation , '' the bsp model was created to simplify the design of software for parallel hardware , thereby `` bridging '' the gap between high - level programming languages and multi - processor systems .",
    "as opposed to distributed shared memory or other distributed systems abstractions , bsp makes heavy use of a message passing interface ( mpi ) which avoids high latency reads , deadlocks and race conditions .",
    "bsp is , at the most basic level , a two step process performed iteratively and synchronously : 1 ) perform task computation on local data , and 2 ) communicate the results , and then repeat the two steps . in bsp",
    "each compute / communicate iteration is called a _ superstep _ , with synchronization of the parallel tasks occurring at the superstep barriers , depicted in figure  [ fig : bsp ] .          introduced in 2010 , the pregel system  @xcite is a bsp implementation that provides an api specifically tailored for graph algorithms , challenging the programmer to `` think like a vertex . ''",
    "graph algorithms are developed in terms of what each vertex has to compute based on local vertex data , as well as data from incident edges and adjacent vertices .",
    "the pregel framework , as well other synchronous tlav implementations , split computation into bsp - style supersteps .",
    "analogous to  components \" in bsp @xcite , at each superstep a vertex can execute the user - defined vertex function and then send results to neighbors along graph edges .",
    "supersteps always end with a synchronization barrier , shown in figure  [ fig : bsp ] , which guarantees that messages sent in a given superstep are received at the beginning of the next superstep .",
    "unlike the original bsp model , vertices may change status between active and inactive , depending on the overall state of execution .",
    "pregel terminates when all vertices halt and no more messages are exchanged .",
    "a comparison of tlav frameworks and bsp is presented in figure  [ fig : bspvstlav ] .",
    "bsp employs a general model of broad applicability , including graph algorithms at varying levels of granularity .",
    "underlying bsp execution is the global synchronization barrier among distributed processors .",
    "tlav frameworks utilize a vertex - centric programming model , and while pregel and its derivatives employ bsp - founded synchronous execution , other frameworks implement asynchronous execution , which has been demonstrated to improve performance in some instances @xcite .",
    "in contrast to tlav and bsp , mapreduce does not natively support iterative algorithms .",
    "several recent frameworks have extended the mapreduce model to support iterative execution @xcite , but for iterative graph algorithms , the graph topological data , which remains static , must be transferred from mappers to reducers , resulting in significant network overhead that renders iterative mapreduce frameworks uncompetitive with tlav frameworks @xcite .",
    "a theoretical comparison between mapreduce and bsp is presented in @xcite .",
    "node[xshift=0,yshift=17 mm ] tlav ; node[text width=.5cm , align = center , xshift=-6 mm ] asyn- + chro- + nous ; node[text width=.5cm , align = center , xshift=7 mm ] syn- + chro- + nous ; node [ yshift=17 mm ] bsp ; node[text width=.5cm , xshift=19 mm ] not vertex - centric ;      since pregel , several tlav frameworks have been proposed that either employ conceptually alternative framework components ( such as asynchronous execution ) , or improve upon the pregel model with various optimizations .",
    "this survey provides the first comprehensive examination into tlav framework concepts , and makes these other contributions :    1 .",
    "analyzes 4 principle components in the design of vertex programs execution in tlav frameworks , identifying the trade - offs in component implementations and providing data - driven discussion 2 .",
    "overviews approaches related to tlav system architecture , including fault tolerance on distributed systems and novel techniques for large - scale processing on single - machines 3 .   discusses how the scalability of a graph algorithm varies inversely with the algorithm s scope , illustrated by vertex - centric and related subgraph - centric , or hybrid , frameworks    this article is organized as follows : first , section  [ sec : overview ] overviews the vertex - centric programming model , including an example program and execution .",
    "section  [ sec : sys_theory ] presents the four major design decisions , or pillars , of the vertex - centric model .",
    "section  [ sec : implementation ] presents details for distributed implementation , as well as novel techniques utilized by tlav frameworks that enable large - scale graph processing on a single machine .",
    "section  [ sec : alt ] presents subgraph - centric , or hybrid , frameworks , that adopt a computational scope of the graph that is greater than a vertex ( tlav ) but less than the entire graph .",
    "section  [ sec : related ] discusses related work .",
    "finally , section  [ sec : conc ] presents a summary , conclusions , and directions for future work .",
    "first , a brief note on terminology : the tlav paradigm is described interchangeably as _ vertex - centric _ , _ vertex - oriented _ , or _",
    "think - like - a - vertex_. a _ vertex program kernel _ refers to an instance of the user - defined vertex _ program _",
    ", _ function _ , or _ process _ that is executed on a particular vertex . a graph is a data structure made up of vertices and edges , both with ( potentially empty ) data properties . as in the literature , _ graph _ and _ network _ may be used interchangeably , as may _",
    "node _ and _ vertex _ , and _ edge _ and _ link_. _ network _ may also refer to hardware connecting two or more machines , depending on context .",
    "a _ worker _ refers to a slave machine in the conventional master - worker architectural pattern , and a _ worker process _ is the program that governs worker behavior , including , but not limited to , execution of vertex programs , inter - machine communication , termination , check - pointing , etc .",
    "graphs are assumed to be directed without loss of generality .",
    "graph processing is transitioning from centralized to decentralized design patterns .",
    "sequential , shared - memory graph algorithms are inherently centralized .",
    "conventional graph algorithms , such as dijkstra s shortest path @xcite or betweenness centrality @xcite , receive the entire graph as input , presume all data is randomly accessible in memory ( _ i.e. _ , graph - omniscient algorithms ) , and a centralized computational agent processes the graph in a sequential , top - down manner . however , the unprecedented size of big data - produced graphs , which may contain hundreds of billions of nodes and occupy terabytes of data or more , exceed the memory capacity of standard machines . moreover , attempting to centrally compute graph algorithms across distributed memory results in unmanageable pointer - chasing @xcite . a more local",
    ", decentralized approach is required for processing graphs of scale .",
    "think like a vertex frameworks are platforms that iteratively execute a user - defined program over vertices of a graph .",
    "the vertex program is designed from the perspective of a vertex , receiving as input the vertex s data as well as data from adjacent vertices and incident edges .",
    "the vertex program is executed across vertices of the graph synchronously , or may also be executed asynchronously .",
    "execution halts after either a specified number of iterations , or all vertices have converged .",
    "the vertex - centric programming model is less expressive than conventional graph - omniscient algorithms , but is easily scalable with more opportunity for parallelism .",
    "the frameworks are founded in the field of distributed algorithms .",
    "although vertex - centric algorithms are local and bottom - up , they have a provable , global result .",
    "tlav frameworks are heavily influenced by distributed algorithms theory , including synchronicity and communication mechanisms @xcite .",
    "several distributed algorithm implementations , such as distributed bellman - ford single - source shortest path @xcite , are used as benchmarks throughout the tlav literature .",
    "the recent introduction of tlav frameworks has also spurred the adaptation of many popular machine learning and data mining ( mldm ) algorithms into graph representations for high - performance tlav processing of large - scale data sets @xcite .",
    "many graph problems can be solved by both a sequential , shared - memory algorithm as well as a distributed , vertex - centric algorithm .",
    "for example , the pagerank algorithm for calculating web - page importance has a centralized matrix form @xcite as well as a distributed , vertex - centric form @xcite .",
    "the existence of both forms illustrates that many problems can be solved in more than one way , by more than one approach or computational perspective , and deciding which approach to use depends on the task at hand . while the sequential , shared - memory approach is often more intuitive and easier to implement on a single machine or centralized architecture ,",
    "the limits of such an approach are being reached .",
    "vertex programs , in contrast , only depend on data local to a vertex , and reduce computational complexity by increasing communication between program kernels . as a result , tlav frameworks are highly scalable and inherently parallel , with manageable inter - machine communication .",
    "for example , runtime on the pregel framework has been shown to scale linearly with the number of vertices on 300 machines @xcite .",
    "furthermore , tlav frameworks provide a common interface for vertex - program execution , abstracting away low - level details of distributed computation , like mpi , allowing for a fast , re - usable development environment . a paradigm shift from centralized to decentralized approaches",
    "to problem solving is represented by tlav frameworks .",
    "the following describes a simple vertex program that calculates the shortest paths from a given vertex to all other vertices in a graph .",
    "in contrast to this distributed implementation example , consider a centralized , sequential , shared - memory , or ",
    "graph - omniscient , \" solution to the single - source shortest path algorithm known as djikstra s algorithm  @xcite or the more general bellman  ford algorithm  @xcite .",
    "both dijkstra s and the bellman - ford algorithms are based on repeated relaxations , which iteratively replace distance estimates with more accurate values until eventually reaching the solution .",
    "both variants are have a superlinear time complexity : djisktra s runs in @xmath0 and bellman - ford s runs in @xmath1 , where @xmath2 is the number of edges and @xmath3 is the number of vertices in the graph and typically @xmath4 .",
    "perhaps more importantly , both procedural , shared - memory algorithms keep a large state matrix resulting in a space complexity of @xmath5 .    in contrast , to solve the same single - source shortest path problem in the tlav programming model , a vertex program need only pass the minimum value of its incoming edges to its outgoing edges during each superstep .",
    "this algorithm , considered a distributed version of bellman - ford @xcite , is shown in alg .",
    "[ alg : sssp ] .",
    "the computational complexity of each vertex program kernel is less than that of the sequential solution , however a new dimension is introduced in terms of the communication complexity , or the messaging between vertices @xcite . for tlav implementation ,",
    "a user need only to write the inner - portion of alg .",
    "[ alg : sssp ] denoted by line numbers ; the outermost loop and the parallel execution is handled by the framework . because lines 1 - 10 are executed on the each vertex these lines are known as the _ vertex program_.    the tlav - solution to the single source shortest path problem has surprisingly few lines of code , and understating its execution requires a different way of thinking .",
    "figure  [ fig : minval ] depicts the execution of alg .",
    "[ alg : sssp ] for a graph with 4 vertices and 6 weighted directed edges .",
    "only the source vertex begins in an active state . in each superstep",
    ", a vertex processes its incoming messages , determines the smallest value among all messages received , and if the smallest received value is less than the vertex s current shortest path , then the vertex adopts the new value as its shortest path , and sends the new path length plus respective edge weights to outgoing neighbors .",
    "if a vertex does not receive any new messages , then the vertex becomes inactive , represented as a shaded vertex in figure  [ fig : minval ] .",
    "overall execution halts once no more messages are sent and all vertices are inactive .",
    "= [ circle , thick , draw = black!75,fill = white!20,minimum size=6 mm ] = [ circle , thick , draw = black!75,fill = gray!20,minimum size=6 mm ] = [ red ]    ( a1 ) @xmath6 ; ( b1 ) [ right of = a1 ] @xmath6 edge [ < ->,bend right ] node[above ] 2 ( a1 ) ; ( c1 ) [ right of = b1 ] 0 edge [ ->,bend left ] node[above ] 2 ( b1 ) ; ( d1 ) [ right of = c1 ] @xmath6 edge [ < - , bend right ] node[above ] 1 ( b1 ) edge [ < - > ] node[above ] 4 ( c1 ) ; ( lab1 ) [ right of = d1 , xshift=15mm , text width=4 cm ] _ superstep 0 _ + message values = 2 and 4 ;    ( a2 ) [ below of = a1 ] @xmath6 ; ( b2 ) [ right of = a2 ] 2 edge [ < ->,bend right ] ( a2 ) edge [ < - , dashed ] ( c1 ) ; ( c2 ) [ right of = b2 ] 0 edge [ ->,bend left ] ( b2 ) ; ( d2 ) [ right of = c2 ] 4 edge [ < - , bend right ] ( b2 ) edge [ < - > ] ( c2 ) edge [ < - , dashed ] ( c1 ) ; ( lab2 ) [ right of = d2 , xshift=20mm , text width=5 cm ] _ superstep 1 _ + message values = 4 , 3 , and 8 ;    ( a3 ) [ below of = a2 ] 4 edge [ < - , dashed ] ( b2 ) ; ( b3 ) [ right of = a3 ] 2 edge [ < ->,bend right ] ( a3 ) ; ( c3 ) [ right of = b3 ] 0 edge [ < - , dashed ] ( d2 ) edge [ ->,bend left ] ( b3 ) ; ( d3 ) [ right of = c3 ] 3 edge [ < - , bend right ] ( b3 ) edge [ < - > ] ( c3 ) edge [ < - , dashed ] ( b2 ) ; ( lab3 ) [ right of = d3 , xshift=15mm , text width=4 cm ] _ superstep 2 _ + message values = 6 and 7 ;    ( a4 ) [ below of = a3 ] 4 ; ( b4 ) [ right of = a4 ] 2 edge [ < ->,bend right ] ( a4 ) edge [ < - , dashed ] ( a3 ) ; ( c4 ) [ right of = b4 ] 0 edge [ < - , dashed ] ( d3 ) edge [ ->,bend left ] ( b4 ) ; ( d4 ) [ right of = c4 ] 3 edge [ < - , bend right ] ( b4 ) edge [ < - > ] ( c4 ) ; ( lab4 ) [ right of = d4 , xshift=20mm , text width=5 cm ] _ superstep 3 _ + complete , no new messages ;    with this example providing insight into tlav operation , particularly the synchronous message - passing model of pregel , the survey continues by more completely detailing tlav properties and categorizing different tlav frameworks .",
    "a tlav framework is software that supports the iterative execution of a user - defined vertex programs over vertices of a graph .",
    "frameworks are composed of several interdependent components that drive program execution and ultimate system performance .",
    "these frameworks are not unlike an analytic operating system , where component design decisions dictate how computations for a particular topology utilize the underlying hardware .",
    "this section introduces the four principle pillars of tlav frameworks .",
    "they are :    1 .",
    "timing - how user - defined vertex programs are scheduled for execution 2 .",
    "communication - how vertex program data is made accessible to other vertex programs 3 .",
    "execution model - implementation of vertex program execution and flow of data 4 .",
    "partitioning - how vertices of the graph , originally in storage , are divided up to be stored across memory of the system s multiple , while some tlav frameworks are implemented for a single machine without the specific intention of developing for a non - distributed environment ( _ e.g. _ the framework is first developed for a single machine before developing the framework for a distributed environment , like the original graphlab [ which published a distributed version 2 years later , see section  [ subsubsec : async ] ] or grace [ see section  [ subsubsec : hybrid ] ] ) , the single - machine frameworks presented in section  [ sec : arch ] are frameworks that implement particularly _ novel _ methods with the _ stated objective _ of processing , on a single machine , graphs of size that exceed the single machine s memory capacity .",
    "these single machine frameworks still partition the graph , using framework - specific methods detailed in the respective section . ]",
    "worker machines    the discussion proceeds as follows : the timing policy of vertex programs is presented in subsection  [ subsec : execution_policy ] , where system execution can be synchronous , asynchronous , or hybrid .",
    "communication between vertex programs is presented in subsection  [ subsec : communication ] , where intermediate data is shared primarily through message - passing or shared - memory .",
    "the implementation of vertex program execution is presented in subsection  [ subsec : comp_models ] , which overviews popular models of program execution and demonstrates how a particular model implementation impacts execution and performance",
    ". finally , partitioning of the graph from storage into distributed memory is presented in subsection  [ subsec : partitioning ] .",
    "each pillar is heavily interdependent with other pillars , as each design decision is tightly integrated and strongly influenced by other design decisions .",
    "while each pillar may be understood through a sequential reading of the information provided , a more efficient , yet thorough understanding may be achieved by freely forward- and cross - referencing other pillars , especially when related sections are cited .",
    "the inter - relation of the four pillars is unavoidable and indivisible , _ not unlike a graph data structure itself . _",
    "the difficulty of independently describing each pillar certainly reflects the challenge of processing a vertex in which a given result depends on the concurrent processing of neighboring vertices .",
    "this survey is restricted to a sequential presentation of information in the form of a paper . however ,",
    "each pillar , though unique , depends on , and may only be described in relation to , other pillars , so a sufficient understanding of any given pillar may only be achieved by understanding all pillars of a tlav framework , collectively .",
    "thus one may begin to understand the challenges of processing graphs ( especially large graphs , when not all  pillars \" are in the same  paper \" ) as in section  [ sec : intro ] , section  [ sec : overview ] , and @xcite .      in tlav frameworks ,",
    "the scheduling and timing of the execution is separate from the logic of the vertex program .",
    "the _ timing _ of a framework characterizes how active vertices are ordered by the scheduler for computation .",
    "timing can be synchronous , asynchronous , or a hybrid of the two models .",
    "frameworks that represent the different fundamental timing models are presented in table  [ table : timing ] .",
    "the _ synchronous _ timing model is based on the original bulk synchronous parallel ( bsp ) processing model discussed above . in this model , active vertices",
    "are executed conceptually in parallel over one or more iterations , called _",
    "supersteps_. synchronization is achieved through a global synchronization _",
    "barrier _ situated between each superstep that blocks vertices from computing the next superstep until all workers complete the current superstep .",
    "each worker coordinates with the master to progress to the next superstep .",
    "synchronization is achieved because the barrier ensures that each vertex within a superstep has access to only the data from the previous superstep . within a single processing unit",
    ", vertices can be scheduled in a fixed or random order because the execution order does not affect the state of the program .",
    "the global synchronization barrier introduces several performance trade - offs .",
    "synchronous systems are conceptually simple , demonstrate scalability , and perform exceptionally well for certain classes of algorithms . while not all tlav programs consistently converge to the same values depending on system implementation , synchronous systems are almost always deterministic , making synchronous applications easy to design , program , test , debug , and deploy .",
    "although coordinating synchronization imposes consistent overhead , the overhead becomes largely amortized for large graphs .",
    "synchronous systems demonstrate good scalability , with runtime often linearly increasing with the number of vertices @xcite . as will be discussed in section  [ subsubsec : msg ] , synchronous systems are often implemented along with message - passing communication , which enables a more efficient `` batch messaging '' method .",
    "batch messaging can especially benefit systems with lots of network traffic induced by algorithms with a low computation - to - communication ratio @xcite .",
    "although synchronous systems are conceptually straight - forward and scale well , the model is not without drawbacks .",
    "one study found that synchronization , for an instance of finding the shortest path in a highly - partitioned graph , accounted for over 80% of the total running time @xcite , so system throughput must remain high to justify the cost of synchronization , since such coordination can be relatively costly .",
    "however , when the number of active vertices drops or the workload amongst workers becomes imbalanced , system resources can become under - utilized .",
    "iterative algorithms often suffer from `` the curse of the last reducer '' otherwise known as the `` straggler '' problem where many computations finish quickly , but a small fraction of computations take a disproportionately longer amount of time @xcite .",
    "_ for synchronous systems , each superstep takes as long as the slowest vertex _ , so synchronous systems generally favor lightweight computations with small variability in runtime .    finally , synchronous algorithms may not converge in some instances . in graph coloring algorithms , for example , vertices attempt to choose colors different than adjacent neighbors @xcite and require coordination between neighboring vertices .",
    "however , during synchronous execution , the circumstance may arise where two neighboring vertices continually flip between each others color .",
    "in general , algorithms that require some type of neighbor coordination may not always converge with the synchronous timing model without the use of some extra logic in the vertex program @xcite .",
    "in the asynchronous iteration model , no explicit synchronization points , _",
    "i.e. _ , barriers , are provided , so any active vertex is eligible for computation whenever processor and network resources are available .",
    "vertex execution order can be dynamically generated and reorganized by the scheduler , and the `` straggler '' problem is eliminated . as a result , many asynchronous models outperform corresponding synchronous models , but at the expense of added complexity .",
    "theoretical and empirical research has demonstrated that asynchronous execution can generally outperform synchronous execution @xcite , albeit precise comparisons for tlav frameworks depend on a number of properties @xcite .",
    "asynchronous systems especially outperform synchronous systems when the workload is imbalanced .",
    "for example , when computation per vertex varies widely , synchronous systems must wait for the slowest computation to complete , while asynchronous systems can continue execution maintaining high throughput .",
    "one disadvantage , however , is that asynchronous execution can not take advantage of batch messaging optimizations ( see section  [ subsubsec : opts ] ) .",
    "thus , synchronous execution generally accommodates i / o - bound algorithms , while asynchronous execution well - serves cpu - bound algorithms by adapting to large and variable workloads .    many iterative algorithms exhibit asymmetric convergence .",
    "et al_. demonstrated that , for pagerank , the majority of vertices converged within one superstep , while only 3% of vertices required more than 10 supersteps @xcite .",
    "asynchronous systems can utilize prioritized computation via a dynamic schedule to focus on more challenging computations early in execution to achieve better performance @xcite .",
    "generally , asynchronous systems perform well by providing more execution flexibility , and by adapting to dynamic or variant workloads .",
    "although intelligent scheduling can improve performance , schedules resulting in sub - optimal performance are also possible . in some instances ,",
    "a vertex may perform more updates than necessary to reach convergence , resulting in excessive computation @xcite . moreover , if implementing the pull model of execution , which is commonly implemented in asynchronous systems @xcite and described in section  [ subsubsec : pushpull ] , communication becomes redundant when neighboring vertex values do nt change @xcite .",
    "the flexibility provided by asynchronous execution comes at the expense of added complexity , not only from scheduling logic , but also from maintaining data consistency .",
    "asynchronous systems typically implement shared memory , discussed in section  [ subsubsec : shared ] , where data race conditions can occur when parallel computations simultaneously attempt to modify the same data .",
    "additional mechanisms are necessary to ensure mutual exclusion , which can challenge algorithm development because framework users may have to consider low - level concurrency issues @xcite , like , for example , in graphlab where users must select a consistency model @xcite .      rather than adhering to the inherent strengths and weaknesses of a strict execution model , several frameworks work around a particular shortcoming through design improvements .",
    "one such implementation , graphhp , reduces the high fixed cost of the global synchronization barrier using _ pseudo - supersteps _ @xcite .",
    "another implementation , grace , explores dynamic scheduling within a single superstep @xcite .",
    "the powerswitch system removes the need to choose between synchronous and asynchronous execution and instead adaptively switches between the two modes to improve performance @xcite .",
    "together , these three frameworks illustrate how weaknesses with a particular execution model can be overcome through engineering and problem solving , rather than strict adoption of an execution model .    as previously discussed",
    ", synchronous systems suffer from the high , fixed cost of the global synchronization barrier . the hybrid execution model introduced by graphhp , and also used by p++ framework @xcite ,",
    "reduces the number of supersteps by decoupling intra - processor computation from the inter - processor communication and synchronization @xcite . to do this graphhp distinguishes between two types of nodes : _ boundary nodes _ that share an edge across partitions , and _ local nodes _ that only have neighboring nodes within the local partition . during synchronization",
    ", messages are only exchanged between boundary nodes . as a result , in graphhp ,",
    "a given superstep is composed of two phases : global and local .",
    "the global phase , which is executed first , runs the user program across all boundary vertices using data transmitted from other boundary vertices as well as its own local vertices .",
    "once the global phase is complete , the local phase executes the vertex program on local vertices within a pseudo - superstep ; the pseudo - superstep is different from a regular superstep in that : 1 ) pseudo - supersteps have local barriers resulting in local iterations independent of any global synchronization or communication ; and 2 ) local message passing is done through direct , in - memory message passing , which is much faster than standard mpi - style messages .    a similar approach to segmented execution , as in graphhp and p++ , is the kla paradigm @xcite , which creates a hybrid of synchronous and asynchronous execution . for graphs ,",
    "the depth of asynchronous execution is parameterized , and asynchronous execution is allowed for a certain number of levels before a synchronous round .",
    "similar to how graphhp implements a round of boundary vertex execution before several rounds of local execution , kla has multiple traversals of asynchronous execution before coordinating a round of synchronous execution .",
    "the trade - off is between expensive global synchronizations with cheap but possibly redundant asynchronous computations .",
    "kla is also similar to delta - stepping used for single source shortest path @xcite .",
    "the single - machine framework grace explores dynamic scheduling of vertices from within a single synchronous round@xcite . to do",
    "this grace exposes a programming interface that , from within a given superstep , allows for prioritized execution of vertices and selective receiving of messages outside of the previous superstep .",
    "results demonstrate comparable runtime to asynchronous models , with better scaling across multiple worker threads on a single machine .    knowing",
    "_ a priori _ which execution mode will perform better for a given problem , algorithm , system , or circumstance is challenging .",
    "furthermore , the underlying properties that give one execution model an advantage over another may change over the course of processing .",
    "for example , in the distributed single source shortest path algorithm @xcite , the process begins with few active vertices , where asynchronous execution is advantageous , then propagates to a high number of active vertices performing lightweight computations , which is ideal for synchronous execution , before finally converging amongst few active vertices @xcite .",
    "for some algorithms , one execution mode may outperform another only for certain stages of processing , and the best mode at each stage can be difficult to predict .",
    "motivated by the necessity for execution mode dynamism , powerswitch was developed to adaptively switch between synchronous and asynchronous execution modes @xcite .",
    "developed on top of the powergraph platform , powerswitch can quickly and efficiently switch between synchronous and asynchronous execution .",
    "powerswitch incorporates throughput heuristics with online sampling to predict which execution mode will perform better for the current period of computation .",
    "results demonstrate that the powerswitch s heuristics can accurately predict throughput , the switching between the two execution modes is well - timed , and overall runtime is improved for a variety of algorithms and system configurations @xcite .",
    "communication in tlav frameworks entails how data is shared between vertex programs .",
    "the two conventional models for communication in distributed systems , as well as distributed algorithms , are message passing and shared memory @xcite . in message passing systems , data is exchanged between processes through messages , whereas in shared memory systems data for one process is directly and immediately accessible by another process .",
    "this section compares and contrasts message passing and shared memory for tlav frameworks",
    ". a third method of communication , active messages , is also presented .",
    "finally , techniques to optimize distributed message passing are discussed .",
    "diagrams in figure  [ fig : comm_imp ] are referenced throughout this section to illustrate the different communication implementations .",
    "a sample graph is presented in figure  [ fig : samp_graph ] , and figures  [ fig : pregel]-[fig : gre ] depict 4 tlav communication implementations of the sample graph . for each implementation",
    ", vertices are partitioned across 2 machines , namely , vertices a , b , and c are partitioned to machine p1 , and vertices d , e , and f are put on machine p2 ( except figure  [ fig : powergraph ] and [ fig : gre ] , where the graph is cut along vertex c ) .",
    "solid arrows represent local communication and dashed arrows represent network traffic .",
    ".28       .7    .43        .5     .45        .4       in the message passing method of communication , also known as the local model of distributed computation @xcite , information is sent from one vertex program kernel to another via a message .",
    "a message contains local vertex data and is addressed to the i d of the recipient vertex . in the archetypal message - passing framework pregel @xcite ,",
    "a message can be addressed anywhere , but because vertices do not have i d information of all of other vertices , destination vertex ids are typically obtained by iterating over outgoing edges .",
    "after computation is complete and a destination i d for each message is determined , the vertex dispatches messages to the local worker process .",
    "the worker process determines whether the recipient resides on the local machine or a remote machine . in the case of the former",
    ", the worker process can place the message directly into the vertex s incoming message queue .",
    "else , the worker process looks up the worker - id of the destination vertex ) , the worker process typically has access to a local routing table , provided by the master during initialization . ] and places the message in an outgoing message buffer .",
    "the outgoing message buffer in pregel , a synchronously - timed system , is flushed when it reaches a certain capacity , sending messages over the network in batches .",
    "waiting until the end of a superstep to send all outgoing remote messages can exceed memory limits @xcite .",
    "message passing is commonly implemented with synchronized execution , which guarantees data consistency without low - level implementation details .",
    "all messages sent during superstep @xmath7 are received in superstep @xmath8 , at which point a vertex program can access the incoming message queue at the beginning of @xmath8 s program execution .",
    "synchronous execution also facilitates batch messaging , which improves network throughput . for i / o bound algorithms with lightweight computation , such as pagerank @xcite , where vertices are  always active \"",
    "so messaging is high @xcite , synchronous execution has been shown to significantly outperform asynchronous execution @xcite .",
    "message passing is depicted in figure  [ fig : pregel ] , where vertex @xmath9 sends ( an ) inter - machine message(s ) to vertices @xmath10 , @xmath11 , and @xmath12 .",
    "technically , messages are first sent from @xmath9 to the worker process of @xmath13 , which routes the messages to worker process @xmath14 , which places the message in a vertex s incoming message queue , but the worker process - related routing is omitted from the figure without loss of generality . figure  [ fig : pregel ] represents a general message passing framework , such as pregel or giraph .",
    "the three messages sent by @xmath9 across the network can be potentially reduced using optimization techniques in section  [ subsubsec : opts ] , namely , receiver - side scatter , depicted in figure  [ fig : rec_scat ] .      shared memory exposes vertex data as shared variables that can be directly read or be modified by other vertex programs . shared memory avoids the additional memory overhead constituted by messages , and does nt require intermediate processing by workers . shared memory",
    "is often implemented by tlav frameworks developed for a single machine ( see section  [ sec : arch ] ) , since challenges to a shared memory implementation arise in the distributed setting @xcite , where consistency must be guaranteed for remotely - accessed vertices .",
    "inter - machine communication for distributed shared memory still occurs through network messages .",
    "the trinity framework @xcite implements a shared global address space that abstracts away distributed memory .    for shared memory tlav frameworks",
    ", race conditions may arise when an adjacent vertex resides on a remote machine .",
    "shared memory tlav frameworks often ensure memory consistency through mutual exclusion by requiring serializable schedules .",
    "serializability , in this case , means that every parallel execution has a corresponding sequential execution that maintains consistency , _ cf .",
    "_ , the dining philosophers problem @xcite .    in graphlab @xcite",
    "border vertices are provided locally - cached _ ghost _ copies of remote neighbors , where consistency between ghosts and the original vertex is maintained using pipelined distributed locking @xcite . in powergraph @xcite",
    ", the second generation of graphlab , graphs are partitioned by edges and cut along vertices ( see vertex - cuts in section  [ subsec : partitioning ] ) , where consistency across cached _ mirrors _ of the cut vertex is maintained using parallel chandy - misra locking @xcite .",
    "giraphx is a giraph derivative with a synchronous shared memory implementation @xcite , which again provides serialization through chandy - misra locking of border vertices , although without local cached copies .",
    "the reduced overhead of shared memory compared to message passing is demonstrated by giraphx , which converges 35% faster than giraph when computing pagerank on a large web graph @xcite .",
    "moreover , some iterative algorithms perform better under serialized conditions , such as dynamic als @xcite , and popular gibbs sampling algorithms that actually require serializability for correctness @xcite .    shared",
    "memory implementations are depicted in figure  [ fig : graphlab ] and figure  [ fig : powergraph ] . in figure  [ fig : graphlab ] , ghost vertices , represented by dashed circles , are created for every neighboring vertex residing on a remote machine , as implemented by graphlab @xcite .",
    "one disadvantage of shared - memory frameworks is seen when computing on scale - free graphs which have a certain percentage of high degree vertices , such as vertex @xmath9 . in these cases",
    "the graph can be difficult to partition @xcite resulting in many ghost vertices .",
    "figure  [ fig : powergraph ] depicts shared memory with vertex cuts as implemented by powergraph @xcite .",
    "powergraph combines vertex - cuts ( discussed in section  [ subsec : partitioning ] ) with the three - phase gather - apply - scatter computational model ( see section  [ subsubsec : gas ] ) to improve processing of scale - free graphs . in figure",
    "[ fig : powergraph ] , the graph is cut along vertex @xmath9 , where @xmath15 is arbitrarily chosen as the master and @xmath16 as the mirror . for each iteration , a distributed vertex preforms computation where : ( i ) both @xmath15 and @xmath16 compute a partial result based on local neighbors , ( ii ) the partial result is sent over the network from the mirror @xmath16 to the master @xmath15 , ( iii ) the master computes the final result for the iteration , ( iv ) the master transmits the result back to the mirror over the network , then ( v ) the result is sent to local neighbors as necessary .",
    "powergraph demonstrates how the combination of advanced components , _ i.e. _ , vertex - cuts and three - phase computation , can overcome processing challenges like imbalances arising from high - degree vertices in scale - free graphs .",
    "shared memory systems are often implemented with asynchronous execution . although consistency is fundamentally maintained in synchronous message passing frameworks like pregel , asynchronous , shared memory frameworks like graphlab may execute faster because of prioritized execution and low communication overhead , but at the expense of added complexity for scheduling and maintaining consistency .",
    "the added complexity challenges scalability , for as the number of machines and partitions increase , more time and resources become devoted to locking protocols .",
    "dynamic computation addresses asymmetric convergence by only updating necessary vertices . shared memory with asynchronous execution is an effective platform for dynamic computation , because the movement of data is separated from computation , allowing vertices to access neighboring values even if the values havent changed between iterations .",
    "this implies the _ pull _ mode of information flow [ subsubsec : pushpull ] .",
    "in contrast , a vertex in a message - passing framework would need all neighboring values delivered in order to perform an update , even if some values had not changed .",
    "dynamic computation is possible with message passing in the cyclops framework , which implements a _ distributed immutable view_. cyclops is a synchronous shared memory framework @xcite , where one of the replicated vertices is designated the master , which computes updates and messages the updated state to replicas at the end of an iteration .",
    "cyclops outperforms synchronous message passing frameworks by reducing the amount of processing performed by each worker parsing messages , and is comparable to powergraph by delivering significantly fewer messages .",
    "significant deterioration in performance was noted in @xcite for larger graphs , although admittedly performance largely depends on algorithm behavior @xcite . in short , asynchronous shared memory systems",
    "can potentially outperform synchronous message passing systems , though the latter often demonstrate better scalability and generalization .      while message passing and shared memory are the two most commonly implemented forms of communication in distributed systems , a third method called _ active messages _ is implemented in the gre framework @xcite .",
    "active messaging is a way of bringing computation to data , where a message contains both data as well as the operator to be applied to the data @xcite .",
    "active messages are sent asynchronously , and executed upon receipt by the destination vertex . within the gre architecture ,",
    "active messages combine the process of sending and receiving messages , removing the need to store intermediate state , like message queues or edge data .",
    "when combined with the framework s novel agent - graph model , described below , gre demonstrates 20%55% reduction in runtime compared to powergraph across three benchmark algorithms in real and synthetic datasets , including 39% reduction in the execution time per iteration for pagerank on the twitter graph when scaled across 192 cores over 16 machines when compared to a powergraph implementation on 512 cores across 64 machines @xcite .",
    "the gre framework modifies the data graph into an agent - graph .",
    "the agent - graph is a model used internally by the framework , but is not accessible to the user .",
    "the agent - graph adds _ combiner _ and _ scatter _ vertices to the original graph in order to reduce inter - machine messaging .",
    "figure  [ fig : gre ] shows that an extra _ scatter _",
    "vertex , @xmath17 , is added to create the internal agent - graph model .",
    "the @xmath17 vertex acts as a receiver - side scatter depicted in figure  [ fig : rec_scat ] .",
    "this is useful because the new @xmath17 vertex allows @xmath9 to only send one message across the network , which @xmath17 then disperses to vertices @xmath10 , @xmath11 , and @xmath12 .",
    "combiner vertices are also added to the agent - graph in the same way as server - side aggregation depicted in figure  [ fig : send_agg ] .",
    "the agent - graph employed by gre is similar to vertex - cuts in powergraph except that gre messaging is unidirectional , and active messages are also utilized for parallel graph computation in the active pebbles framework @xcite .",
    "message passing can be costly , especially over a network .",
    "thus several message - reducing strategies have been developed in order to improve performance .",
    "some strategies are topology - driven and , as such , exploit the graph layout across machines , while other techniques are applied to specific algorithmic behavior .",
    "three topology - driven optimizations are depicted in figure  [ fig : opt ] for messaging between machines @xmath13 and @xmath14 ( or messaging from @xmath13 , @xmath14 , and @xmath18 to @xmath19 , for figure  [ fig : rec_agg ] ) .",
    ".3       .3        .3    the combiner , inspired by the mapreduce function of the same name @xcite , is a message passing optimization originally used by pregel @xcite . presuming the commutative and associative properties of a vertex function , a combiner executes on a worker process and combines many messages destined for the same vertex into a single message .",
    "for example , if a vertex function computes the sum of all incoming messages , then a combiner would detect all messages destined for a vertex @xmath20 , compute the sum of the messages , then send the new sum to @xmath20 .",
    "a combiner can especially reduce network traffic when @xmath20 is remote , shown as _ sender - side aggregation _ ( figure  [ fig : send_agg ] ) .",
    "when @xmath20 is local , a combiner can still reduce memory overhead by aggregating messages before placement into the incoming message queue , shown as _ receiver - side aggregation _ ( figure  [ fig : rec_agg ] ) .",
    "for the single - source shortest path algorithm , a combiner implementation resulted in a four - fold reduction in network traffic @xcite .",
    "a related technique is the _ receiver - side scatter_. for instances where the same message is sent to multiple vertices on the same remote machine , network traffic can be reduced by sending only one message and then having the destination worker distribute multiple copies , depicted in figure  [ fig : rec_scat ] .",
    "the strategy has been employed in multiple frameworks , including the _ large adjacency list partitioning _ in gps @xcite , ibm s x - pregel @xcite , as the _ fetch - once _ behavior in lfgraph @xcite , and through _ scatter _ nodes of the agent - graph in gre @xcite .",
    "the technique reduces network traffic by increasing memory and processing overhead , as worker - nodes must store the out - going adjacency lists of other workers . with this in mind",
    ", gps maintains a threshold where receiver - side scatter would only be applied for vertices above a certain degree .",
    "experiments showed that as the threshold is lowered , network traffic at first decreases then plateaus , while runtime decreases but then increases , demonstrating the existence of an optimal vertex - degree threshold . in x - pregel ,",
    "a ten - fold reduction in network traffic from receiver - side scatter resulted in a 1.5 times speedup @xcite .",
    "clearly , the receiver - side scatter strategy can be effective , but unlike the combiner is not guaranteed to improve performance .",
    "the three partition - driven optimizations in figure  [ fig : opt ] are related to the messaging structure of a framework , and not specific to algorithm behavior , albeit some assumptions are made regarding message computation .",
    "computation for the combiner must be commutative and associative because order can not be guaranteed , while messages for the receiver - side scatter must be identical , and independent of the adjacency list .",
    "still , the techniques are oriented around partition - level messaging and apply to the worker process , only requiring certain operational properties in order to work .",
    "the message - online - computing model proposed in @xcite , which improves memory usage by processing messages in the queue as they are delivered , also requires operations be commutative .",
    "conversely , algorithm - specific message optimizations have also been developed that restructure vertex messaging patterns for certain algorithmic behaviors @xcite . for algorithms that combine vertices into a supervertex , like boruvka s minimum spanning tree @xcite , the storing edges at subvertices ( seas )",
    "optimization implements a subroutine where each vertex tracks its parent supervertex instead of sending adjacency lists @xcite . for algorithms where vertices remove edges , like in the 1/2-approximation for maximum weight matching @xcite , the edge cleaning on demand ( ecod )",
    "optimization only deletes stale edges when , counter - intuitively , activity is requested for the stale edge @xcite . to avoid slow convergence ,",
    "ecod is only employed above a certain threshold , _",
    "e.g. _ , when more than 1% of all vertices are active .",
    "both seas and ecod exploit a trade - off between sending messages proportional to the number of vertices or proportional to the number of edges .",
    "other strategies for reducing communication , based on _ aggregate computation _ , are discussed in section  [ subsec : scope_opt ] .",
    "the model of execution for vertex - centric programs describes the implementation of the vertex function , and how data moves during computation .",
    "vertex functions have been implemented as 1 , 2 , or 3 phase - models .",
    "vertex functions have also been implemented as edge - centric functions . while the model choice does not typically impact",
    "the accuracy of the final result , combining certain implementations with other tlav components can yield improved system performance for certain graph characteristics .",
    "[ [ one - phase ] ] one phase + + + + + + + + +    the vertex programming abstraction implemented as a single function is well - characterized by the pregel framework @xcite .",
    "the single compute function of a vertex object follows the general sequence of accessing input data , computing a new vertex value , and distributing the update . in a typical pregel program ,",
    "the input data is accessed by iterating through the input message queue ( messages that may have utilized a combiner ) , applying an update function based on received data , and then sending the new value through messages addressed by iterating over outgoing edges .",
    "details based on other design decisions may vary , _ e.g. _ , input and output data may be distributed through incident edges , or neighboring vertex data may be directly accessible , but in one - phase models the general sequence of vertex execution is performed within a single , programed function .",
    "the ` vertex.compute ( ) ` function is implemented in several tlav frameworks in addition to pregel , including its open - source implementations @xcite and several related variants @xcite .",
    "the one - phase function implementation is conceptually straight - forward , but other frameworks provide opportunities for improvement by dividing up the computation .    [ [ two - phase ] ] two phase + + + + + + + + +    a two - phase vertex - oriented programming model breaks up vertex programming into two functions , most commonly referred to as the scatter - gather model . in scatter - gather ,",
    "the _ scatter _",
    "phase distributes a vertex value to neighbors , and the _ gather _ phase collects the inputs and applies the vertex update . while most single - phase frameworks _ e.g. _ , pregel , can be converted into two phases , the scatter - gather model was first explicitly put forward in the signal / collect framework @xcite .",
    "the two phase model is also presented as scatter - gather in @xcite , and is presented as the iterative vertex - centric ( ivec ) programming model in @xcite .",
    "the scatter - gather programming model commonly occurs in tlav systems where data is read / written to / from edges .",
    "ligra and polymer are frameworks implemented for single - machines ( see section  [ sec : arch ] ) that both implement a two - phase model .",
    "the user provides two functions , one function that executes across each vertex in the active subset and another function that executes all outgoing edges in the subset .",
    "the frameworks adopt a vertex - subset - centric programming model , which is similar to vertex - centric , but the framework retains a centralized view of the graph , where the whole graph is within the scope of computation , which is possible because the entire graph resides on a single machine in this case .",
    "the two phase model is executed within a program processing the whole graph .    a related two - phase programming model for message passing called scatter - combine",
    "is implemented in the gre framework @xcite .",
    "this model utilizes active messages , which are messages that include both data as well as the operator to be executed on the data @xcite . in the first phase of the model , messages are both sent ( scattered ) and the operators in the messages",
    "are executed ( combined ) at the destination vertex . in the second phase ,",
    "the combined result is used to update the vertex value .",
    "the scatter - combine model incorporates two phases differently than scatter - gather . instead of the two phase scatter - gather model of ( i ) gather - apply , and ( ii ) scatter , the scatter - combine model uses active messages to institute ( i ) scatter - gather , and then ( ii ) apply .",
    "the gre framework combines scatter - combine with a novel representation of the underlying data graph , called the agent - graph , described above , to reduce communication and improve scalability for processing graphs with scale - free degree distributions .",
    "[ [ subsubsec : gas ] ] three phase + + + + + + + + + + +    a three - phase programming model is introduced in powergraph as the gather - apply - scatter ( gas ) model @xcite .",
    "the _ gather _ phase performs a generic summation over all input vertices and/or edges , like a commutative associative combiner .",
    "the result is used in the _ apply _ phase , which updates the central vertex value .",
    "the _ scatter _",
    "phase distributes the update by writing the value to the output edges .",
    "powergraph incorporates the gas model with vertex - cut partitioning ( see section  [ subsubsec : vert_cuts ] ) to improve processing of power - law graphs .",
    "[ [ edge - centric ] ] edge - centric + + + + + + + + + + + +    the x - stream framework provides an _ edge - centric _ two phase scatter - gather programming model @xcite , as opposed to a vertex - centric programming model .",
    "the model is edge - centric because the framework iterates over edges of the graph instead of vertices .",
    "however , the framework may still be considered tlav because the two phase program operates on source and target vertices , adopting a similar local scope .",
    "x - stream leverages streaming edge data instead of random access for efficient large scale graph processing on a single machine , and is discussed in section  [ sec : arch ] in further detail .",
    "the flow of information for vertex - programs can be characterized as data being _ pushed _ or _ pulled _ @xcite . in _",
    "push _ mode , information flows from the active vertex performing the update outward to neighboring vertices , as in pregel - like message - passing . in _ pull _ mode , information flows from neighboring vertices inward to the active vertex , as in graphlab - like shared memory , when an active vertex reads neighbor s data .",
    "few tlav frameworks explicitly adopt a push or pull mode .",
    "instead , the information flow arises from other design decisions .",
    "still , analyzing a system as push or pull allows one to reason about other system properties .",
    "for example , asynchronous execution is supported by both modes , but sender - side combining is only possible in push mode @xcite .    push and pull modes",
    "are more commonly associated with databases and transactional processing , though have been more explicitly incorporated in broader graph engines and temporal frameworks ( see section  [ sec : related ] for related work ) .",
    "the galois framework , with a flexible computation model enabling the implementation of a vertex - centric interface , allows users to choose push or pull mode @xcite , as does kineograph @xcite .",
    "chronos experiments with how push and pull modes impact caching @xcite .",
    "ligra is a single - machine graph processing framework that dynamically switches between push and pull - based operators based on a threshold .",
    "the framework is in part inspired by a recently developed shared - memory breadth - first search algorithm that achieves remarkable performance by switching between push and pull modes of exploration @xcite .",
    "this algorithm , ligra , and powerswitch from section  [ subsubsec : hybrid ] exemplify how performance can be improved by dynamically adapting the processing technique to properties of the graph .",
    "the delta - caching optimization , which is introduced in powergraph @xcite , which reduces the pulling of redundant data by tracking value changes . in a three phase model ,",
    "an accumulator value is the result of gather step . with delta - caching ,",
    "a cached copy of the accumulator for each vertex is stored by the worker , requiring additional storage .",
    "if , for a given update , the change in the accumulator is minimal , then neighboring vertices are nt activated , and any change can be applied to the cached copy stored by worker .",
    "a neighboring vertex can then use the cached copy during an update . for delta - caching to be available , the apply function must be commutative , associative , and have an inverse function .",
    "delta - caching reduces redundant pulling by not activating neighboring vertices for small changes , and resulted in a 45% decrease in runtime for computing pagerank on the twitter graph @xcite .",
    "large - scale graphs must be divided into parts to be placed in distributed memory .",
    "good partitions often lead to improved performance @xcite , but expensive strategies can end up dominating processing time , leading many implementations to incorporate simple strategies , such as random placement @xcite .",
    "effective partitioning evenly distributed the vertices for balanced workload , while minimizing inter - partition edges to avoid costly network traffic , a problem formally known as _ k - way graph partitioning _ that is np - complete with no fixed - factor approximation @xcite .    leading work in graph partitioning can be broadly characterized as ( 1 ) rigorous but impractical mathematical strategies , or ( 2 ) pragmatic heuristics used in practice @xcite .",
    "practical strategies , such as those employed in the suite of algorithms known as metis @xcite , often employ a three - phase multi - level partitioning approach @xcite .",
    "partition size is often allowed to deviate in the form of a  slackness \" parameter in exchange for better cuts @xcite .",
    "graph partitioning with metis partitioning software is often considered the _ de facto _ standard for near - optimal partitioning in tlav frameworks @xcite . despite a lengthy preprocessing time",
    ", metis - algorithms significantly reduce total communication and improve overall runtime for tlav processing on smaller graphs @xcite .",
    "however , for graphs of even medium - size , the high computational cost and necessary random access the entire graph renders metis and related heuristics impractical .",
    "alternatives for large - scale graph partitioning include distributed heuristics presented in section  [ subsubsec : dist_heur ] , streaming algorithms in section  [ subsubsec : streaming ] , vertex cuts in section  [ subsubsec : vert_cuts ] , and dynamic repartitioning in section  [ subsubsec : dyn_repart ] .",
    "distributed heuristics are decentralized methods , requiring little or no centralized coordination .",
    "distributed partitioning is related to distributed community detection in networks @xcite , the two main differences being : 1 ) communities can overlap whereas partitions can not , and 2 ) partitioning requires _ a priori _ specification of the number of partitions , whereas community detection typically does not .",
    "much distributed partitioning work has been inspired by distributed community detection , namely label propagation @xcite .",
    "label propagation occurs at the vertex level , where each vertex adopts the label of the plurality of its neighbors .",
    "though the process is decentralized , label propagation for partitioning necessitates a varying amount of centralized coordination in order to maintain balanced partitions and prevent  densification \" : a cascading phenomenon where one label becomes the overwhelming preference @xcite .",
    "the densification problem is addressed in @xcite wherein a simple capacity constraint is enforced that is equal to the available capacity of the local worker divided by the number of non - local workers . in @xcite ,",
    "balanced vertex distribution is maintained by constraining label propagation and solving a linear programming optimization problem that maximizes a relocation utility function . in @xcite , vertices swap labels , either with a neighbor or possibly a random node , and simulated annealing is employed to escape local optima .",
    "the cost of centralized coordination incurred by these methods is much less than the cost of random vertex access on a distributed architecture , as with parmetis .",
    "more advanced label propagation schemes for partitioning are presented in @xcite and @xcite . in @xcite ,",
    "label propagation is used as the coarsening phase of a multi - level partitioning scheme , which processes the partitioning in blocks to accommodate multi - level partitioning for large - scale graphs . in @xcite ,",
    "several stages of label propagation are utilized to satisfy multiple partitioning objectives under multiple constraints .",
    "@xcite use a parallel multi - level partitioning algorithm for k - way balanced graphs that operates in two phases : an aggregate phase that uses weighted label propagation , and then a partition phase that performs the stepwise minimizing ratiocut method .",
    "streaming partitioning is a form of online processing that partitions a graph in a single - pass . for tlav frameworks ,",
    "streaming partitioning is especially efficient since the partitioning can be performed by the graph loader , which loads the graph from disk onto the cluster .",
    "the accepted streaming model assumes a single , centralized graph loader that reads data serially from disk and chooses where to place the data amongst available workers @xcite .",
    "centralized streaming heuristics can be adapted to run in parallel @xcite , however , depending on the heuristic , concurrency between the parallel partitioners would likely be required @xcite .",
    "one of the first online heuristics was presented by kernighan and lin and is used as a subroutine in metis @xcite .",
    "graphbuilder @xcite is a a similar library that , in addition to partitioning , supports an extensive variety of graph loading - related processing tasks .",
    "a streaming partitioner on a graph loader reads data serially from disk , receiving one vertex at a time along with its neighboring vertices . in a single look at the vertex",
    "the streaming partitioner must decide the final placement for the vertex on a worker partition , but the streaming partitioner has access to the entire subgraph of already placed vertices . in a variant of the streaming model",
    ", the partitioner has an available storage buffer with a capacity equal to that of a worker partition , so the partitioner may temporarily store a vertex and decide the partitioning later @xcite , however this buffer is not utilized by the top performing streaming partitioners . for most heuristics ,",
    "the placement of later vertices is dependent on placement of earlier vertices , so the presentation order of vertices can impact the partitioning .",
    "thus , an adverse ordering can drastically subvert partitioning efforts , however , experiments demonstrate that performance remains relatively consistent for breadth - first , depth - first , and random orderings of a graph @xcite .",
    "two top - performing streaming partitioning algorithms are greedy heuristics .",
    "the first is linear deterministic greedy ( ldg ) , a heuristic that assigns a vertex to the partition with which it shares the most edges while weighted by a penalty function linearly associated with a partition s remaining capacity .",
    "the ldg heuristic is presented in @xcite , where 16 streaming partitioning heuristics are evaluated across 21 different data sets .",
    "the use of a buffer in addition to the ldf heuristic has been adapted for streaming partitioning of massive resource description framework ( rdf ) data @xcite .",
    "another variant uses _ unweighted _ deterministic greedy instead of linear deterministic greed ( ldg ) , to perform greedy selection based on neighbors without any penalty function ; this unweighted variant has been employed for distributed matrix factorization @xcite .",
    "further analysis of ldg - related heuristics on random graphs , as well as lower bound proofs for random and adversarial stream ordering , is presented in @xcite .",
    "another top - performing streaming partitioner is fennel @xcite , which is inspired by a generalization of optimal quasi - cliques @xcite .",
    "fennel achieves high quality partitions that are in some instances comparable with near - optimal metis partitions .",
    "both fennel and ldg have been adapted to the restreaming graph partitioning model , where a streaming partitioner is provided access to previous stream results @xcite .",
    "restreaming graph partitioning is motivated by environments such as online services where the same , or slightly modified , graph is repeatedly streamed with regularity . despite adhering to the same linear memory bounds as a single - pass partitioning , the presented restreaming algorithms not only provide results comparable to metis , but are also capable of partitioning in the presence of multiple constraints and in parallel without inter - stream communication .",
    "a vertex - cut , depicted in figure  [ fig : powergraph ] , is equivalent to partitioning a graph by edges instead of vertices .",
    "partitioning by edges results in each edge being assigned to one machine , while vertices are capable of spanning multiple machines . only changes to values of cut vertices",
    "are passed over the network , not changes to edges .",
    "vertex - cuts are implemented by tlav frameworks in response to the challenges of finding well - balanced edge cuts in power - law graphs @xcite .",
    "complex network theory suggests power - law graphs have good vertex cuts in the form of nodes with high degree @xcite .",
    "a rigorous review of vertex separators is presented in @xcite .",
    "powergraph combines vertex - cuts with the three - phase gas model ( section  [ subsubsec : gas ] ) for efficient communication and balanced computation @xcite . for vertices that are cut and span multiple machines ,",
    "one copy is randomly designated the master , and remaining copies are mirrors . during an update",
    "all vertices first execute a gather , where all incoming edge values are combined with a commutative associative sum operation .",
    "then the mirrors transmit the sum value over the network to the master , which executes the apply function to produce the updated vertex value .",
    "the master then sends the result back over the network to the mirrors .",
    "finally , each vertex completes the update by scattering the result along its outgoing edges . for each update ,",
    "network traffic is proportional to the number of mirrors , therefore , breaking up high - degree vertices reduces network communication and helps to balance computation .    since its initial implementation in powergraph",
    ", the vertex - cut approach has been adopted by several other tlav frameworks .",
    "graphx is a vertex programming abstraction for the spark processing framework @xcite where the adoption of vertex - cuts demonstrated an 8-fold decrease in the platform s communication cost .",
    "graphbuilder @xcite , an open - source graph loader , supports vertex - cuts and implements grid and torus - based vertex - cut strategies that were later included in powergraph .",
    "powerlyra @xcite is a modification to powergraph that hybridizes partitioning where vertices with a degree above a user - defined threshold are cut , while vertices below the threshold are partitioned using an adaptation of the fennel streaming algorithm @xcite .",
    "powerlyra also incorporates unidirectional locality similar to gre framework ( see section  [ subsubsec : acivemsg ] ) .",
    "bigraph is a framework developed on powergraph that implements partitioning algorithms for large - scale bipartite graphs @xcite .",
    "lightgraph @xcite is a framework that optimizes vertex - cut partitions by using edge - direction - aware partitioning , and by not sending updates to mirrors with only in - edges .    several edge partitioning analyses and algorithms",
    "have recently been developed . a thorough analysis comparing expected costs of vertex partitioning and edge partitioning",
    "is presented in @xcite . in this study ,",
    "edge partitioning is empirically demonstrated to outperform vertex partitioning , and a streaming least marginal cost greedy heuristic is introduced that outperforms the greedy heuristic from powergraph .",
    "centralized hypergraph partitioning , including edge partitioning , is np - hard , and several exact algorithms have been developed @xcite . however , because of their complexity , such algorithms are too computationally expensive and not practical for large - scale graphs .",
    "centralized heuristics have been shown to be equally impractical @xcite .",
    "a large - scale vertex - cut approach for bipartite graphs based on hypergraph partitioning is presented in @xcite as part of a vertex - centric program for computing the alternating direction of multipliers optimization technique .",
    "a distributed edge partitioner was developed in @xcite that creates balanced partitions while reducing the vertex cut , based on the vertex partitioner in @xcite .",
    "good workload balance for skewed degree distributions can also be achieved with degree - based hashing @xcite . finally , as part of a non - vertex - centric bsp graph processing framework ,",
    "a distributed vertex - cut partitioner is presented in @xcite that uses a market - based model where partitions use allocated funds to buy an edge .",
    "while an effective partitioning equally distributes vertices among the partitions , for tlav frameworks , the number of active vertices performing updates on a given superstep can vary drastically over the course of computation , which creates processing imbalances and increases run time .",
    "dynamic repartitioning was developed to maintain balance during processing by migrating vertices between workers as necessary .",
    "reasons for changing active vertex sets include topological mutations to the graph and algorithmic execution properties .",
    "topological mutations may occur if the framework supports dynamic or temporal graphs ( see related work in section  [ sec : related ] ) .",
    "topology may also change due to the algorithm , such as graph coarsening @xcite .    with a static topology ,",
    "the execution pattern of the algorithm can also change the active vertex set . while vertex algorithms such as synchronous pagerank execute on every vertex for every superstep , other algorithms introduce dynamism .",
    "@xcite classifies 9 vertex algorithms as either ( i ) always active , ( ii ) traversal , or ( iii ) multi - phase , where the active vertex set of the latter two classifications can vary widely and unpredictably , depending on the graph . for dynamic repartitioning to prove beneficial",
    ", the associated overhead must be less than the additional costs stemming from processing imbalance .    according to @xcite ,",
    "a dynamic repartitioning strategy must directly address ( i ) how to select vertices to reassign , ( ii ) how and when to move the assigned vertices , and ( iii ) how to locate the reassigned vertices .",
    "other properties of a strategy include whether coordination is centralized or decentralized , and how the strategy combats  densification \" and enforces vertex balance .",
    "densification is akin to the rich - get - richer phenomenon , and can occur in greedy or decentralized protocols for partitioning / clustering , where one partition becomes over - populated as the repeated destination for migrated vertices @xcite . in response , protocols often implement constraints that prevent a partition from exceeding a certain capacity .",
    "the xpregel framework , for example , only permits the worker with the most vertices and edges to migrate vertices @xcite .",
    "table  [ table : dyn_repart ] presents 6 tlav frameworks that support dynamic repartitioning : gps @xcite , mizan @xcite , xpregel @xcite , xdgp @xcite , loggp @xcite , and the catch the wind prototype @xcite .",
    "the table includes what active vertex set imbalances are targeted by the frameworks , what metrics are used to identify vertices for reassignment , how reassigned vertices are located after migration , how densification is avoided , and whether the protocol is centralized or decentralized .    among the 6 frameworks that implement dynamic repartitioning , all",
    "are synchronous , and repartitioning occurs at the end of a superstep , separate from the updates . when a vertex is selected for migration , the worker must send all associated data to the new worker , including the vertex i d , the adjacency list , and the incoming messages to be processed in the next superstep . to avoid",
    "sending all incoming messages over the network , many dynamic repartitioning frameworks implement a form of delayed migration , where the new worker is recognized as the owner of the migrated vertex , but the vertex value remains on the old worker for an extra iteration in order to compute an update . with delayed migration , the incoming message queue does nt need to be migrated , but the new worker still receives new incoming messages @xcite .    though fundamentally sound , many experiments demonstrate that dynamic repartitioning is often not worth the high overhead .",
    "results in @xcite show that while network i / o is significantly reduced over time , overall runtime shows minor improvements .",
    "independent tests of gps show dynamic repartitioning to be detrimental for all cases in @xcite , and similar results are observed for gps and mizan in @xcite . however , one major shortcoming in these evaluations is the use of the pagerank algorithm for experimentation .",
    "dynamic repartitioning is most effective for dynamic active vertex sets , but with pagerank vertices are always active , so dynamic repartitioning performs predictably poorly .",
    "asynchronous dynamic repartitioning protocols have yet to be explored for tlav frameworks , but the added complexity and overhead for asynchrony demonstrated in section  [ subsec : execution_policy ] suggest that such an implementation is not practical .",
    "this section overviews implementation details of tlav frameworks relating to the distributed environment .",
    "these details include system architecture and fault tolerance .",
    "additionally , tlav frameworks that employ novel techniques to process large - scale graphs on single machines are surveyed .",
    "tlav frameworks generally always employ the master - slave architecture .",
    "a master node initializes the slave workers , monitors execution , and manages coordination ( and synchronization if invoked ) amongst the workers . generally , the master is responsible for graph loading and partitioning , but with a network filesystem available , the loading and partitioning can be performed in parallel @xcite .",
    "the master also stores global values , such as aggregators @xcite .",
    "the workers each execute a copy of the program on the local partitions and inform the master of runtime status .",
    "one notable exception to the general master - slave architecture is xpregel @xcite , implemented in x10 @xcite .",
    "x10 implements an asynchronous partitioned global address space ( apgas ) , which is a shared address space but with a local structure that enables highly productive distributed and parallel programming . with apgas ,",
    "the number of local  places \" is provided at runtime , which the programmer may utilize as necessary .",
    "xpregel does implement master - slave , but in x10 , the master is actually just place 0 , sans hierarchy , and opens the door for alternative architectures , like recursive structures .      for multi - core machines ,",
    "many bsp - based frameworks including pregel @xcite simply assign a partition to a given core , but frameworks can better utilize computational resources through multi - threading .",
    "xpregel @xcite supports multi - threading by dividing a partition into a user - defined number of subpartitions , assigning one thread to each subpartition .",
    "graphlab @xcite implements multi - threading and avoids deadlocks through scheduler restrictions .",
    "gps @xcite implements 3 types of threads : a thread for vertex computation , a thread for communication , and a thread for parsing .",
    "cyclops @xcite implements a hierarchical bsp model @xcite with a split design to parallelize computation and messaging while exploiting locality and avoiding synchronization contention .",
    "cyclops demonstrates that multi - threading can improve runtime relative to single - threaded execution for the same framework , at the expense of added complexity",
    ".      distributed systems must often account for the potential failure of one or more nodes over the course of computation .",
    "when a node fails , a replacement node may become available , but all data and computation performed on the failed node is lost .",
    "checkpointing is a common fault tolerance implementation , where an immutable copy of the data is written to persistent storage , such as a network filesystem .",
    "pregel implements synchronous checkpointing , where the graph is copied in between supersteps @xcite .",
    "when a failure occurs , the system rolls back to the most recently saved point , all partitions are reloaded , and the entire system resumes processing from the checkpoint .",
    "the partition of the failed node is reloaded to a new replacement node .",
    "if messaging information is also logged , then resources can be saved by only reloading and recomputing data on the replacement node .",
    "graphlab @xcite implements asynchronous vertex checkpointing , based on chandy - lamport @xcite snapshots , which need not halt the entire program and can result in slightly faster overall execution than synchronous checkpointing , minding certain program constraints .",
    "graphx is a graph processing library for apache spark , which is developed based on the resilient distributed dataset ( rdd ) abstraction @xcite .",
    "rdds are immutable , partitioned collections created through data - parallel operators , like map or reduce .",
    "rdds are either stored externally , or generated in - memory from operations on other rdds .",
    "spark maintains the lineage of operations on an rdd , so upon any node failure the rdd can be automatically recovered .",
    "graphx leverages the rdds of spark to create a graph abstraction and pregel interface .    the imitator @xcite framework implements fault - tolerance based on vertex replicas , or ghosts / mirrors used in shared memory ( see section![subsubsec : shared ] ) .",
    "the use of replicas for fault tolerance is founded in the observation that the hash partitioning of many real - world directed graphs results in the replication of over 99% of vertices @xcite . by replicating _ every _ vertex , a full copy of the graph can reside in distributed memory , enabling faster recovery times at the expense of relatively little additional memory consumption and network messaging @xcite .",
    "the efficiency of imitator is tied to the effectiveness of the partitioning ( see section  [ subsec : partitioning ] ) .",
    "imitator outperforms checkpointing for large graphs distributed over several nodes , when only one replica per vertex is required .",
    "state - of - the - art partitioning methods like metis , or a smaller number of partitions ( imitator experiments were run on 50 nodes ) , would likely lead to increased overhead for imitator .",
    "also , the number of replicas is tied to the degree of fault tolerance . to support the failure of _ k _ machines , then _",
    "k _ replicas are required , increasing overhead for each additional failure supported .",
    "a partition - based checkpoint method for fault tolerance is presented in @xcite . during execution ,",
    "a recovery executor node collects run - time statistics , and upon failure , uses heuristics to redistribute the partitions .",
    "checkpointed partitions of the failed nodes can be reassigned amongst both new and old nodes , parallelizing recovery .",
    "partitions on healthy nodes can also be reassigned for load balancing .",
    "like mapreduce , tlav frameworks are advantageous because they are highly scalable while providing a simple programming interface , abstracting away the lower level details of distributed computing .",
    "however , such environments also stipulate the availability of elaborate infrastructure , cluster management , and performance tuning , which may not be available to all users .",
    "single machine systems are easier to manage and program , but commodity machines do not have the memory capacity to process large - scale graphs in - memory .",
    "this section overviews single machine tlav frameworks that employ _ novel _ methods to process large - scale graphs .",
    "the main features of the 4 single machine frameworks in this section are presented in table  [ table : singlemachine_frameworks ] .",
    "processing large - scale graphs on a single machine requires either substantial amounts of memory , or storing part of the graph out - of - memory , in which case performance is dictated by how efficiently the graph can be fetched from storage . in @xcite",
    ", it s argued that high - end servers , offering 100 gb to 1 tb of memory or more , is enough capacity for many real and synthetic graphs reported in the literature .",
    "such machines would be capable of storing large graphs and executing relatively simple graph algorithms , though more complex algorithms would likely exhaust resources .",
    "the recommendation service at twitter @xcite , which implements a single machine graph processing system with 144 gb of ram , finds that in practice one edge occupies roughly five bytes of ram on average .",
    "compression techniques are further explored for large memory servers in @xcite .",
    "yet , graphs of scale are not practical on lower - end machines containing around 8 to 16 gb of memory @xcite . accordingly",
    ", single machine frameworks have been developed that implement the vertex - centric programming model and process a graph in parts .",
    "central to many single machine tlav frameworks are novel data layouts that efficiently read and write graph data to / from external storage .",
    "one common representation is the compressed sparse row format , which organizes graph data as out - going edge adjacency sets , allowing for the fast look - up of outgoing edge , and has been implemented in many state - of - the - art shared memory graph processors @xcite , including galois @xcite .",
    "[ [ graphchi ] ] graphchi + + + + + + + +    the seminal single machine tlav framework is graphchi @xcite , which was explicitly developed for large - scale graph processing on a commodity desktop .",
    "graphchi enables large - scale graph processing by implementing the parallel sliding window ( psw ) method , a graph data layout previously utilized for efficient pagerank and sparse - matrix dense - vector multiplication @xcite .",
    "psw partitions vertices into disjoint sets , associating with each interval a shard containing all of the interval s incoming edges , sorted by source vertex .",
    "intervals are selected to form balanced shards , and the number of intervals is chosen so any interval can fit completely in memory . a sliding window",
    "is maintained over every interval , so when vertices from one shard are updated from in - edges , the results can be sequentially written to out - edges found in sorted order in the window on other shards .",
    "graphchi may not be faster than most distributed frameworks , but often reaches convergence within an order of magnitude of the performance of distributed frameworks @xcite , which is reasonable for a desktop with an order of magnitude less ram .",
    "the graphchi framework was later extended to a general graph management system for a single machine called graphchi - db @xcite .",
    "storage concepts for single machine graph processing are further explored in @xcite through two directions .",
    "the first project investigates reducing random accesses in ssds through prefetching , in a project called rasp that later evolved into prefedge @xcite .",
    "the second project is x - stream @xcite , an edge - centric single machine graph processing framework that exploits the trade - off between random memory access and sequential access from streaming data .",
    "[ [ x - stream ] ] x - stream + + + + + + + +    streaming data from any storage medium provides much greater bandwidth than random access .",
    "experiments on the x - stream testbed , for example , demonstrate that streaming data from disk is 500 times faster than random access @xcite .",
    "x - stream combines a novel data layout , where an index is built over a storage - based edge list with an edge - centric scatter - gather programming model that includes a shuffle phase .",
    "data is read from , and updates are written to , streaming edge data . though the framework is edge - centric , a user - defined update function is executed on the destination vertex of an edge .",
    "x - stream reports that it can process a 64-billion edge graph on a single machine with a pair of 3 tb magnetic disks attached @xcite .",
    "[ [ flashgraph ] ] flashgraph + + + + + + + + + +    while graphchi and x - stream are designed for general external storage , the flashgraph framework is developed for graphs stored on any fast i / o device , such as an array of ssds .",
    "flashgraph is deployed on top of the set - associative file system ( safs ) @xcite , which includes a scalable lightweight page cache , and implements a custom asynchronous user - task i / o interface that reduces overhead for asynchronous i / o .",
    "flashgraph employs asynchronous message - passing and vertex - centric programming with the semi - external memory ( sem ) model @xcite , where vertices and algorithmic state reside in ram , but edges are stored externally . in experiments comparing graphchi and xstream ,",
    "flashgraph outperformed both by orders of magnitude even when the data for graphchi and xstream was placed into ram - disk @xcite .",
    "[ [ pathgraph ] ] pathgraph + + + + + + + + +    in addition to the path - centric programming model , further discussed in section  [ sec : alt ] , pathgraph also implements a path - centric compact storage system that improves compactness and locality @xcite . because most iterative graph algorithms involve path traversal , pathgraph stores edge traversal trees in depth - first search order . both the forward and",
    "reverse edge trees are each stored in a chunk storage structure that compresses data structure information including the adjacency set , vertex ids , and the indexing of the chunk .",
    "the efficient computational model and storage structure of pathgraph resulted in improved graph loading time , lower memory footprint , and faster runtime for certain algorithms when compared to graphchi and x - stream .",
    "the strengths of the vertex - centric programming model are also its weaknesses .",
    "whereas vertex programs may be relatively simpler to reason about since only local data is available , the algorithms are less expressive than conventional centralized algorithms . while tlav frameworks exhibit better scalability",
    ", execution can be slow because of high overhead from synchronization and message traffic that takes magnitudes longer compared to computation .",
    "several frameworks strive for the best of both worlds by adopting a scope that is greater than a vertex but less than the graph , summarized in table  [ table : alt_frameworks ] .      considering the challenges addressed by tlav frameworks , taking a subgraph - centric approach is sensible .",
    "conventional graph algorithms require the entire graph in memory , which is not possible with graphs of scale .",
    "a subgraph , though , can be partitioned into a size small enough to fit into memory ( considering computation ) while the connections between subgraphs would be no more , and likely much less , than the total number of edges .",
    "the system would better utilize processing while retaining scalability .",
    "the subgraph - centric programming model is implemented in varying degrees by several frameworks .",
    "the giraph++ @xcite , blogel @xcite , and goffish @xcite frameworks provide a subgraph - centric interface for progrmaming sequential algorithms . both giraph++ and blogel",
    "provide a subgraph - centric interface in addition to a vertex - centric interface .",
    "the results of the sequential programs can then be shared either through vertex programs on boundary nodes , or in the case of blogel , results can be shared directly between subgraphs .",
    "goffish exclusively offers a subgraph - centric interface , and implements messaging between subgraphs and also from subgraphs to specific vertices , the latter being used for traversal algorithms . by allowing subgraphs to directly message vertices ,",
    "any vertex - centric algorithm can be implemented by a subgraph - centric framework , maintaining scalability while enabling significant performance improvement .",
    "collectively , subgraph - centric frameworks dramatically outperform tlav frameworks , often by _ orders of magnitude _ in terms of computing time , number of messages , and total supersteps @xcite .",
    "the graphhp @xcite and p++ @xcite frameworks do not implement an interface for sequential programs , but do differentiate between inter - partition nodes to improve performance .",
    "in these two frameworks , supersteps are split into two phases : in the first phase messages are exchanged between vertices on partition boundaries , and in the second phase , vertices within a partition repeatedly execute the vertex program to completion , exchanging messages in memory .",
    "this method reduces communication and improves performance , however , iteratively executing intra - worker vertex programs is less efficient than executing a sequential algorithm .",
    "message - passing algorithms are typically more scalable than sequential graph algorithms , but p++ is not distributed , nor is block - based grace @xcite , an extension of @xcite , although the later demonstrates that executing vertex updates on a subgraph block basis improves locality and cache hits while reducing memory access time , which is a bottleneck for computationally light algorithms like pagerank .",
    "tlav frameworks illustrate the principal ideas for scalable graph processing , but for the best performance , users may consider subgraph - centric frameworks .",
    "subgraph frameworks leverage principles of tlav frameworks to execute sequential graph algorithms in a distributed environment .",
    "the giraph++ , blogel , and goffish frameworks reduce the scope of sequential graph algorithms for the subgraph to fit in memory while utilizing vertex or subgraph messaging to maintain scalability .",
    "together , the vertex - centric and subgraph - centric programming model , compared to sequential graph algorithms , demonstrate how scalability varies inversely with scope .",
    "while subgraph - centric frameworks illustrate the scope / scalability trade - off , several other frameworks adopt alternative computational scopes that demonstrate additional benefits .",
    "a more specific type of subgraph , a traversal tree , is used for the programming model in pathgraph @xcite .",
    "traversals are a fundamental component of many graph algorithms , including pagerank and bellman - ford shortest path .",
    "pathgraph first partitions the graph into paths , with each partition represented as two trees , a forward and reverse edge traversal . then , for the path - centric computational model , path - centric scatter and path - centric gather functions are available to the user to define an algorithm that traverse each tree .",
    "the user also defines a vertex update function , which is executed by the path - centric functions during the traversal . like block - based grace",
    ", the path - centric model utilizes locality to improve performance through reduced memory usage and efficient caching .",
    "pathgraph also implements a path - centric storage model that enables the framework to process billion node graphs on a single machine ( see section  [ sec : arch ] ) @xcite .",
    "graph processing frameworks designed for single machines can implement interfaces of unique granularity .",
    "a vertex subset interface is implemented in ligra @xcite .",
    "ligra argues that high - end servers provide enough memory for large - scale graphs , and thus implements a vertex - centric programming interface while retaining a global view of the graph .",
    "inspired by a hybrid breadth - first search ( bfs ) algorithm @xcite , ligra dynamically switches between sparse and dense representations of edge sets depending on the size of the vertex subset , which impacts whether push or pull operations are performed with the vertex subset .",
    "polymer @xcite adopts a similar interface as ligra , but with several numa - aware optimizations .",
    "galois @xcite is a shared memory framework that executes user - defined set operators while exploiting amorphous data parallelism @xcite .",
    "galois can be implement a variety of programming interfaces , including the vertex - centric paradigm @xcite .",
    "two optimizations have been introduced in @xcite for tlav frameworks that improve performance by adopting a scope of the graph other than vertex - centric . the finishing computation serially ( fcs ) method is applicable when an algorithm with a shrinking set of active vertices converges slowly near the end of execution @xcite .",
    "the fcs method is triggered when the remaining active graph can fit in the memory of a single machine ; in these instances the active portions are sent to the master and completed serially from a global , shared memory perspective of the graph .",
    "similarly , the single pivot ( sp ) optimization @xcite , first presented in @xcite , also temporarily adopts a global view . for algorithms that execute breadth - first search ( bfs ) across all vertices ,",
    "e.g. _ , the connected components algorithm , instead of executing bfs from every node , which incurs a high messaging cost , sp randomly selects one vertex from the graph and performs bfs just from that vertex .",
    "since most graphs have one big component , in addition to many small ones , the bfs from a random node can be executed until the big component is found , then bfs from every vertex that s not in the big component can execute bfs to complete the algorithm , resulting in significantly fewer total messages .",
    "this optimization adjusts scope by randomly selecting a single vertex by utilizing a global aggregator @xcite , which also adopt a scope beyond vertex .",
    "in this paper , vertex - centric graph processing systems for large - scale graphs are surveyed . in previous related work ,",
    "pregel and graphlab have been compared @xcite , and general graph processing systems have been surveyed @xcite , and 4 tlav frameworks have been empirically evaluated on 4 algorithms @xcite . a tutorial on tlav frameworks",
    "was recently delivered at an international conference @xcite .",
    "tlav frameworks intersect several subjects , including graph processing , distributed computing , big data , and distributed algorithms .",
    "several graph processing frameworks have been recently developed outside of the vertex - centric programming model .",
    "pegasus combines the bsp model with generalized matrix - vector multiplication ( gim - v ) @xcite , while turbograph introduces the pin - and - slide model to perform gim - v on a single machine @xcite .",
    "combinatorial blas @xcite and the parallel boost graph library @xcite are software libraries for high - performing parallel computation of sequential programs .",
    "piccolo performs distributed graph computation using distributed tables @xcite .",
    "graph databases , such as neo4j @xcite , hypergraphdb @xcite , and gbase @xcite , are decidedly different from tlav frameworks .",
    "both treat vertices as first class citizens , and both face related problems like partitioning , but the key distinction is that databases focus on transactional processing while tlav frameworks focus on batch processing @xcite . databases offer local or online queries , such as 1-hop neighbors , whereas tlav systems iteratively process the entire graph offline in batch .",
    "some more general graph management systems , like trinity @xcite and grace @xcite , offer suites of features that include both vertex - centric processing and queries .",
    "sensibly , a graph processing engine may be developed on top of a graph database .",
    "however the two should not be confused , and performance is incomparable .",
    "a closely related big data framework is mapreduce @xcite .",
    "mapreduce is a different programming model from tlav frameworks , but similarly enables large - scale computation and , when implemented , abstracts away the details of distributed programming .",
    "the programming model is effective for many types of computation , but addresses neither iterative processing nor graph processing @xcite .",
    "iterative computation is not natively supported , as the programming model performs only a single pass over the data with no loop awareness .",
    "moreover , i / o is read / written to / from a distributed filesystem , _",
    "e.g. , _ hdfs , rendering iterative computation inefficient @xcite . nonetheless , several frameworks have extended mapreduce to support iterative computation @xcite but such frameworks are still agnostic to the challenges of graph processing .",
    "graph computation with mapreduce has been explored @xcite , but is generally acknowledged to be lacking @xcite .",
    "a comparison of mapreduce and bsp is provided in @xcite .",
    "still , some argue that mapreduce should remain the sole  hammer \" for big data analytics because of the widespread adoption throughout industry @xcite .    similarly , in response to tlav shortcomings , such as poor out - of - core support and lengthy loading times , some frameworks rework pre - existing graph database technologies to provide a vertex - centric interface @xcite . however , many of these projects lose sight of the main problems addressed by the vertex - centric processing .",
    "tlav frameworks are ultimately big data solutions , designed large graphs to be leveraged against the memory and processing power of several machines , not single machines",
    ". moreover , tlav frameworks iteratively process the entire graph , and do not provide graph queries like 1-hop or 2-hop neighbors .",
    "tlav frameworks are not a universal solution for graph analytics , but rather provide an approach for scalable , iterative graph processing .",
    "temporal graph processing is beyond the scope of this survey , though a small number of tlav frameworks have been developed for temporal analysis @xcite .",
    "these frameworks compute temporal properties offline in batch through graph snapshots , necessitating multiple framework components , including a front - end ingress component , an analytics engine , and a storage component such as a graph database .",
    "temporal graph layout optimizations were introduced in chronos @xcite .",
    "these frameworks illustrate how advanced graph analytics systems utilize the strengths of different graph technologies for different components , _",
    "e.g. , _ graph databases for storage and online queries , and vertex - centric computation for batch analytics .",
    "dynamic graph algorithms and general analytics systems have also been surveyed @xcite .",
    "dynamic graphs are supported by many frameworks including pregel , but the topic was omitted from this survey due to widely varying support by the frameworks and broad scope of the topic .    while coined `` vertex - centric '' relative to conventional graph processing approaches , the algorithms executed by tlav frameworks are more formally known as distributed algorithms .",
    "distributed algorithms is a mature field of study @xcite , and further examples beyond figure  [ fig : minval ] may be found within the referenced frameworks .",
    "some works have explored distributed algorithms within the context of tlav frameworks @xcite , but researchers and practitioners should be aware that tlav frameworks execute distributed algorithms @xcite , which come from a field with a considerable body of work , including theory and analysis .",
    "the theoretical limits of what can be computed with vertex - centric frameworks , specifically with the synchronous , message - passing local model , has been studied @xcite .",
    "this paper surveys and compares the various components of tlav frameworks , which are a platform for executing vertex - centric algorithms . like mapreduce",
    ", these frameworks provide an interface for a user - defined function , while abstracting away the lower - level details of cluster computing . changing the components of the framework",
    "will impact system performance and run - time characteristics , but will generally not impact the design or result of the algorithm .. ]",
    "tlav frameworks have been designed in response to the challenges of processing large graphs .",
    "primary challenges include the unstructured nature of graphs , where an edge may span any two vertices , so the entire graph must be randomly accessible for conventional processing .",
    "tlav frameworks are also developed for ease of use , providing a simple vertex - centric interface while abstracting away the lower level details of cluster computing .",
    "mapreduce similarly enables highly scalable computing , but is ill - suited for iterative graph processing .    by adopting a vertex - centric programming model , the scope of computation",
    "is dramatically reduced . to perform an update",
    ", each vertex only needs data from immediate neighbors .",
    "data residing on a separate machine can be acquired directly between workers , avoiding the bottleneck of central coordination , enabling excellent scalability .",
    "the four pillars of the vertex - centric programming model , ( i ) timing , ( ii ) communication , ( iii ) the execution model , and ( iv ) partitioning , were presented and surveyed in the context of distributed graph processing frameworks . however , vertex - centric algorithms , colloquially known as distributed algorithms , have an established history and are still actively researched @xcite .",
    "several related frameworks were explored that similarly adopt a computational scope of the graph at varying granularity .",
    "these frameworks of alternative scope are like a goldilocks solution to graph processing .",
    "centralized algorithms with the entire graph in scope require too much memory , vertex - centric algorithms can scale but are less expressive and require many relatively slow messages , whereas subgraph - centric algorithms can utilize the two resources just right .",
    "a significant contribution of tlav frameworks is exposing how , for graphs , reducing the scope of a program increases scalability .    of course , expressing a particular algorithm as subgraph - centric is not trivial .",
    "the future of practical large - scale distributed graph processing may be related to finding algorithms that process a graph as independent subgraphs , such as divide - and - conquer , or algorithms that can process graphs at multiple , or even dynamic , scopes @xcite .",
    "the performance of the subgraph - centric processing is also closely tied to the effectiveness of large - scale graph partitioning , including streaming and distributed partitioning techniques .",
    "tlav frameworks are a tool for graph processing at scale .",
    "not all graphs are large enough to necessitate distributed processing , and not all graph problems need the whole graph to be computed iteratively .",
    "moreover , there is often more than one way to solve a problem , but these frameworks are simple to program , easy to distribute , and are not a bad choice for the right type of problem .",
    "subgraph - centric frameworks take vertex - centric frameworks a step further for performance .",
    "datasets will continue to grow dramatically into the new age of big data , and the design of processing systems should begin asking if they can scale out infinitely .",
    "tlav frameworks illustrate how conventional centralized systems will fail in the big data ecosystem , and how decentralized platforms must be embraced .",
    "this work was supported in part by the afosr grant # fa9550 - 15 - 1 - 0003 , as well as a department of education gaann fellowship awarded by the university of notre dame department of computer science and engineering .",
    "abou - rjeili2006 amine abou - rjeili and george karypis .",
    "2006 . . in _ proceedings of the 20th international conference on parallel and",
    "distributed processing _ _",
    "( ipdps06)_. ieee computer society , washington , dc , usa , 124124 .",
    "ahmed2013 amr ahmed , nino shervashidze , shravan narayanamurthy , vanja josifovski , and alexander  j. smola .",
    "2013 . . in _ proceedings of the 22nd international conference on world wide web _ _ ( www 13)_. international world wide web conferences steering committee , geneva , switzerland , 3748",
    "ajwani2015 deepak ajwani , marcel karnstedt , and alessandra sala . 2015 . . in _ proceedings of the 24th international conference on world wide web companion",
    "_ ( www 15 companion)_. international world wide web conferences steering committee , republic and canton of geneva , switzerland , 15451545 .",
    "bao2013 nguyen  thien bao and toyotaro suzumura",
    "in _ proceedings of the 22nd international conference on world wide web companion _ _ ( www 13 companion)_. international world wide web conferences steering committee , geneva , switzerland , 501508 .",
    "bender2010 michael  a. bender , gerth  stlting brodal , rolf fagerberg , riko jacob , and elias vicari . 2007 . .",
    "in _ proceedings of the nineteenth annual acm symposium on parallel algorithms and architectures _ _ ( spaa 07)_. acm , new york , ny , usa , 6170 .          bourse2014 florian bourse , marc lelarge , and milan vojnovic . 2014 . .",
    "in _ proceedings of the 20th acm sigkdd international conference on knowledge discovery and data mining _ _ ( kdd 14)_. acm , new york , ny , usa , 14561465 .",
    "chen qun chen , song bai , zhanhuai li , zhiying gou , bo suo , and wei pan .",
    "graphhp : a hybrid platform for iterative graph processing . retrieved july 17 , 2014 from http://wowbigdata.net.cn/paper/graphhp%ef%bc%9aa%20hybrid%20platform%20for%20iterative%20graph%20processing.pdf .",
    "( 2014 ) .",
    "chen2014 rong chen , xin ding , peng wang , haibo chen , binyu zang , and haibing guan .",
    "in _ proceedings of the 23rd international symposium on high - performance parallel and distributed computing _ _ ( hpdc 14)_. acm , new york , ny , usa , 215226 .",
    "chen2013b rong chen , jiaxin shi , yanzhe chen , and haibo chen .",
    "2015 . . in _ proceedings of the tenth european conference on computer systems _ _ ( eurosys 15)_. acm , new york , ny , usa , article 1 , 15 pages .      chen2012 rishan chen , mao yang , xuetian weng , byron choi , bingsheng he , and xiaoming li . 2012",
    ". in _ proceedings of the third acm symposium on cloud computing _",
    "_ ( socc 12)_. acm , new york , ny , usa , article 3 , 13 pages .",
    "chen2002 yen - yu chen , qingqing gan , and torsten suel .",
    "2002 . . in _ proceedings of the eleventh international conference on information and knowledge management _ _ ( cikm 02)_. acm , new york , ny , usa , 549557 .    cheng2012 raymond cheng , ji hong , aapo kyrola , youshan miao , xuetian weng , ming wu , fan yang , lidong zhou , feng zhao , and enhong chen . 2012",
    ". in _ proceedings of the 7th acm european conference on computer systems _ _ ( eurosys 12)_. acm , new york , ny , usa , 8598 .",
    "ekanayake2010 jaliya ekanayake , hui li , bingjing zhang , thilina gunarathne , seung - hee bae , judy qiu , and geoffrey fox .",
    "2010 . . in _ proceedings of the 19th acm international symposium on high performance",
    "distributed computing _ _ ( hpdc 10)_. acm , new york , ny , usa , 810818 .",
    "fan2015 jing fan , adalbert gerald  soosai raj , and jignesh  m. patel .",
    "cidr 2015 , seventh biennial conference on innovative data systems research , asilomar , ca , usa , january 4 - 7 , 2015 , online proceedings_.            gonzalez2012 joseph  e. gonzalez , yucheng low , haijie gu , danny bickson , and carlos guestrin . 2012 . .",
    "in _ proceedings of the 10th usenix conference on operating systems design and implementation _ _ ( osdi12)_. usenix association , berkeley , ca , usa , 1730 .",
    "gonzalez2014 joseph  e. gonzalez , reynold  s. xin , ankur dave , daniel crankshaw , michael  j. franklin , and ion stoica .",
    "2014 . . in _ proceedings of the 11th usenix conference on operating systems design and implementation",
    "( osdi14)_. usenix association , berkeley , ca , usa , 599613 .        gupta2013 pankaj gupta , ashish goel , jimmy lin , aneesh sharma , dong wang , and reza zadeh .",
    "2013 . . in _ proceedings of the 22nd international conference on world wide web _ _ ( www 13)_. international world wide web conferences steering committee , geneva , switzerland , 505514",
    "wook - shin han , sangyeon lee , kyungyeol park , jeong - hoon lee , min - soo kim , jinha kim , and hwanjo yu . 2013 . .",
    "in _ proceedings of the 19th acm sigkdd international conference on knowledge discovery and data mining _ _ ( kdd 13)_. acm , new york , ny , usa , 7785 .",
    "wentao hant , youshan miao , kaiwei li , ming wu , fan yang , lidong zhou , vijayan prabhakaran , wenguang chen , and enhong chen .",
    "in _ proceedings of the ninth european conference on computer systems _ _ ( eurosys 14)_. acm , new york , ny , usa , article 1 , 14 pages .",
    "harshvardhan2014 harshvardhan , adam fidel , nancy  m. amato , and lawrence rauchwerger .",
    "2014 . . in _ proceedings of the 23rd international conference on parallel architectures and compilation _ _ ( pact 14)_. acm , new york , ny , usa , 2738",
    "sungpack hong , tayo oguntebi , and kunle olukotun .",
    "2011 . . in _ proceedings of the 2011 international conference on parallel architectures and compilation techniques",
    "_ ( pact 11)_. ieee computer society , washington , dc , usa , 7888 .",
    "jain2013 nilesh jain , guangdeng liao , and theodore  l. willke .",
    "in _ first international workshop on graph data management experiences and systems _ _",
    "( grades 13)_. acm , new york , ny , usa , article 4 , 6 pages .",
    "kang2011 u. kang , hanghang tong , jimeng sun , ching - yung lin , and christos faloutsos . 2011 . .",
    "in _ proceedings of the 17th acm sigkdd international conference on knowledge discovery and data mining _ _",
    "( kdd 11)_. acm , new york , ny , usa , 10911099 .    kang2009 u. kang , charalampos  e. tsourakakis , and christos faloutsos .",
    "2009 . . in _ proceedings of the 2009 ninth ieee international conference on data mining _",
    "( icdm 09)_. ieee computer society , washington , dc , usa , 229238 .",
    "khayyat2013 zuhair khayyat , karim awara , amani alonazi , hani jamjoom , dan williams , and panos kalnis .",
    "in _ proceedings of the 8th acm european conference on computer systems _ _ ( eurosys 13)_. acm , new york , ny , usa , 169182 .            kyrola2012 aapo kyrola , guy blelloch , and carlos guestrin . 2012 . .",
    "in _ proceedings of the 10th usenix conference on operating systems design and implementation _ _",
    "( osdi12)_. usenix association , berkeley , ca , usa , 3146 .                lu1995 honghui lu , sandhya dwarkadas , alan  l. cox , and willy zwaenepoel . 1995 . . in",
    "_ proceedings of the 1995 acm / ieee conference on supercomputing _ _ ( supercomputing 95)_. acm , new york , ny , usa , article 37 .",
    "malewicz2010 grzegorz malewicz , matthew  h. austern , aart  j.c bik , james  c. dehnert , ilan horn , naty leiser , and grzegorz czajkowski . 2010 . .",
    "in _ proceedings of the 2010 acm sigmod international conference on management of data _ _ ( sigmod 10)_. acm , new york , ny , usa , 135146 .",
    "munagala1999 kameshwar munagala and abhiram ranade",
    ". 1999 . . in _ proceedings of the tenth annual acm - siam symposium on discrete algorithms",
    "_ ( soda 99)_. society for industrial and applied mathematics , philadelphia , pa , usa , 687694 .",
    "nguyen2013 donald nguyen , andrew lenharth , and keshav pingali . 2013 . .",
    "in _ proceedings of the twenty - fourth acm symposium on operating systems principles _ _ ( sosp 13)_. acm , new york , ny , usa , 456471 .",
    "nilakant2014 karthik nilakant , valentin dalibard , amitabha roy , and eiko yoneki .",
    "in _ proceedings of international conference on systems and storage _ _ ( systor 2014)_. acm , new york , ny , usa , article 4 , 12 pages .",
    "nisar2013 m.  usman nisar , arash fard , and john  a. miller . 2013 . . in _ proceedings of the 2013 ieee international congress on big data _ _ ( bigdatacongress 13)_. ieee computer society , washington , dc , usa , 255262",
    "nishimura2013 joel nishimura and johan ugander .",
    "2013 . . in _ proceedings of the 19th acm sigkdd international conference on knowledge discovery and data mining _",
    "( kdd 13)_. acm , new york , ny , usa , 11061114 .          pearce2010 roger pearce , maya gokhale , and nancy  m. amato . 2010 . .",
    "in _ proceedings of the 2010 acm / ieee international conference for high performance computing , networking , storage and analysis _ _ ( sc 10)_. ieee computer society , washington , dc , usa , 111 .",
    "pingali11 keshav pingali , donald nguyen , milind kulkarni , martin burtscher , m.  amber hassaan , rashid kaleem , tsung - hsien lee , andrew lenharth , roman manevich , mario mndez - lojo , dimitrios prountzos , and xin sui .",
    "2011 . . in _ proceedings of the acm sigplan conference on programming language design and implementation _ _ ( pldi 11)_. 1225",
    "prabhakaran2012 vijayan prabhakaran , ming wu , xuetian weng , frank mcsherry , lidong zhou , and maya haridasan .",
    "2012 . . in _ proceedings of the 2012 usenix conference on annual technical conference",
    "_ ( usenix atc12)_. usenix association , berkeley , ca , usa , 44 .        quick2012 louise quick , paul wilkinson , and david hardcastle . 2012 .",
    ". in _ proceedings of the 2012 international conference on advances in social networks analysis and mining ( asonam 2012 ) _ _ ( asonam 12)_. ieee computer society , washington , dc , usa , 457463 .",
    "rahimian2014 fatemeh rahimian , amirh .",
    "payberah , sarunas girdzijauskas , and seif haridi .",
    "2014 . . in _ distributed applications and interoperable systems",
    ", kostas magoutis and peter pietzuch ( eds . ) .",
    "springer berlin heidelberg , 186200 .",
    "rahimian2013 fatemeh rahimian , amir  h. payberah , sarunas girdzijauskas , mark jelasity , and seif haridi .",
    "2013 . . in _ proceedings of the 2013 ieee 7th international conference on self - adaptive and self - organizing systems _ _ ( saso 13)_. ieee computer society , washington , dc , usa , 5160",
    "redekopp2013 mark redekopp , yogesh simmhan , and viktor  k. prasanna .",
    "2013 . . in _ proceedings of the 2013 ieee 27th international symposium on parallel and",
    "distributed processing _ _ ( ipdps 13)_. ieee computer society , washington , dc , usa , 203214 .        salihoglu2013 semih salihoglu and jennifer widom .",
    "2013 . . in _ proceedings of the 25th international conference on scientific and statistical database management _",
    "( ssdbm)_. acm , new york , ny , usa , article 22 , 12 pages .",
    "seo2010 sangwon seo , edward  j. yoon , jaehong kim , seongwook jin , jin - soo kim , and seungryoul maeng .",
    "2010 . . in _ proceedings of the 2010 ieee second international conference on cloud computing technology and science",
    "_ ( cloudcom 10)_. ieee computer society , washington , dc , usa , 721726 .      shang2013 zechao shang and jeffrey  xu yu .",
    "2013 . . in _ proceedings of the 2013 ieee international conference on data engineering ( icde 2013 ) _ _ ( icde 13)_. ieee computer society , washington , dc , usa , 553564",
    "simmhan2013 yogesh simmhan , alok kumbhare , charith wickramaarachchi , soonil nagarkar , santosh ravi , cauligi raghavendra , and viktor prasanna .",
    "2014 . . in _ euro - par 2014 parallel processing",
    ", fernando silva , ins dutra , and vtor santos  costa ( eds . ) .",
    "lecture notes in computer science , vol . 8632 .",
    "springer international publishing , 451462 .",
    "stanton2012 isabelle stanton and gabriel kliot .",
    "2012 . . in _ proceedings of the 18th acm sigkdd international conference on knowledge discovery and data mining _",
    "( kdd 12)_. acm , new york , ny , usa , 12221230 .",
    "stutz2010 philip stutz , abraham bernstein , and william cohen .",
    "2010 . . in _ proceedings of the 9th international semantic web conference on the semantic web - volume part i_. springer - verlag , berlin , heidelberg , 764780",
    "tsourakakis2013 charalampos tsourakakis , francesco bonchi , aristides gionis , francesco gullo , and maria tsiarli .",
    "2013 . . in _ proceedings of the 19th acm sigkdd international conference on knowledge discovery and data mining _",
    "( kdd 13)_. acm , new york , ny , usa , 104112 .    tsourakakis2014 charalampos tsourakakis , christos gkantsidis , bozidar radunovic , and milan vojnovic",
    ". 2014 . . in _ proceedings of the 7th acm international conference on web search and data mining _",
    "( wsdm 14)_. acm , new york , ny , usa , 333342 .            vaquero2013 luis  m. vaquero , felix cuadrado , dionysios logothetis , and claudio martella . 2014 . .",
    "in _ proceedings of the 2014 ieee 34th international conference on distributed computing systems _ _ ( icdcs 14)_. ieee computer society , washington , dc , usa , 144153 .    voneicken1992 thorsten von eicken , david  e. culler , seth  copen goldstein , and klaus  erik schauser . 1992 . .",
    "in _ proceedings of the 19th annual international symposium on computer architecture _ _ ( isca 92)_. acm , new york , ny , usa , 256266 .              willcock2011 jeremiah  james willcock , torsten hoefler , nicholas  gerard edmonds , and andrew lumsdaine . 2011 . .",
    "in _ proceedings of the international conference on supercomputing _ _ ( ics 11)_. acm , new york , ny , usa , 235244 .",
    "xie2014 cong xie , ling yan , wu - jun li , and zhihua zhang .",
    "2014 . . in _ advances in neural information processing systems 27",
    "_ , z.  ghahramani , m.  welling , c.  cortes , n.d .",
    "lawrence , and k.q .",
    "weinberger ( eds . ) .",
    "curran associates , inc .",
    ", 16731681 .",
    "zaharia2010 matei zaharia , mosharaf chowdhury , michael  j. franklin , scott shenker , and ion stoica .",
    "in _ proceedings of the 2nd usenix conference on hot topics in cloud computing _ _",
    "( hotcloud10)_. usenix association , berkeley , ca , usa , 1010 .    zeng2012 zengfeng zeng , bin wu , and haoyu wang",
    ". 2012 . . in _ proceedings of the 1st international workshop on big data , streams and heterogeneous source mining : algorithms , systems , programming models and applications",
    "_ ( bigmine 12)_. acm , new york , ny , usa , 6168 .",
    "zhang2015 kaiyuan zhang , rong chen , and haibo chen .",
    "2015 . . in _ proceedings of the 20th acm sigplan symposium on principles and practice of parallel programming",
    "_ ( ppopp 2015)_. acm , new york , ny , usa , 183193",
    ".          zhao2014 yue zhao , kenji yoshigoe , mengjun xie , suijian zhou , remzi seker , and jiang bian .",
    "2014 . . in _ proceedings of the 2014 ieee international congress on big data _",
    "_ ( bigdatacongress 14)_. ieee computer society , washington , dc , usa , 717724 .",
    "zheng2015 da zheng , disa mhembere , randal burns , joshua vogelstein , carey  e. priebe , and alexander  s. szalay .",
    "2015 . . in _ proceedings of the 13th usenix conference on file and storage technologies",
    "( fast15)_. usenix association , berkeley , ca , usa , 4558 .",
    "zhou2014 xianke zhou , pengfei chang , and gang chen .",
    "2014 . . in _ web technologies and applications",
    ", lei chen , yan jia , timos sellis , and guanfeng liu ( eds . ) .",
    "lecture notes in computer science , vol . 8709 .",
    "springer international publishing , 401412 .",
    "zhou2008 yunhong zhou , dennis wilkinson , robert schreiber , and rong pan .",
    "2008 . . in _ proceedings of the 4th international conference on algorithmic aspects in information and management_. springer - verlag , berlin , heidelberg , 337348"
  ],
  "abstract_text": [
    "<S> the vertex - centric programming model is an established computational paradigm recently incorporated into distributed processing frameworks to address challenges in large - scale graph processing . </S>",
    "<S> billion - node graphs that exceed the memory capacity of standard machines are not well - supported by popular big data tools like mapreduce , which are notoriously poor - performing for iterative graph algorithms such as pagerank . in response , a new type of framework challenges one to  think like a vertex \" ( tlav ) and implements user - defined programs from the perspective of a vertex rather than a graph . </S>",
    "<S> such an approach improves locality , demonstrates linear scalability , and provides a natural way to express and compute many iterative graph algorithms . </S>",
    "<S> these frameworks are simple to program and widely applicable , but , like an operating system , are composed of several intricate , interdependent components , of which a thorough understanding is necessary in order to elicit top performance at scale . to this end , the first comprehensive survey of tlav frameworks is presented . in this survey , </S>",
    "<S> the vertex - centric approach to graph processing is overviewed , tlav frameworks are deconstructed into four main components and respectively analyzed , and tlav implementations are reviewed and categorized .    </S>",
    "<S> [ firstpage ] </S>"
  ]
}