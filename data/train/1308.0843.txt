{
  "article_text": [
    "the current experimental particle physics projects consist of thousands of physicists from across the globe .",
    "these large scale international collaborations have big budgets .",
    "their future endeavors and road - map to potential discoveries requires careful long term planning .",
    "the snowmass effort epitomizes the work by the high - energy physics community in the united states ( us ) to collaborate and plan out a common future keeping in mind the long - term physics aspirations and leadership of the us in the field of particle physics . via this effort",
    ", the community plans to communicate the opportunities for discoveries in high - energy physics to the broader scientific community and the us government .    for simulation of large backgrounds",
    "as expected from @xmath0 collisions at @xmath1 and @xmath2  tev , opportunistic resources are harnessed using the open science grid infrastructure ( osg )  @xcite .",
    "osg facilitates distributed computing for scientific research in the us .",
    "it is comprised of about 126 institutions with 121 recently active sites .    in order to evaluate the physics discovery potential from the recently observed higgs boson as well as new physics searches , large - statistic standard model ( sm ) background monte carlo ( mc )",
    "samples are needed to model the data expected from an integrated luminosity corresponding to 300 fb@xmath3 expected after the long shutdown-1 ( ls1 ) and 3000 fb@xmath3 for high luminosity ( hl ) large hadron collider ( lhc )  @xcite .",
    "this would require simulating billions of events calculated using exact matrix element calculations with parton shower .",
    "the lhc physics center ( lpc ) at fermilab is a hub of us physicists collaborating on the cms experiment at the lhc and plays a key role in various physics analyses and discovery process within the experiment .",
    "the lpc has a prime role to play in the snowmass process .",
    "it has vast computing resources and in - house experts on physics , experimentation , simulation , software and computing tools .",
    "being located at fermilab , the lpc members have access to the osg personnel and infrastructure .",
    "the snowmass efforts at lpc represents a collaborative effort carried out by a group of members drawn from cms and atlas experiments as well as theorists .",
    "this group has taken the lead in a very short time to successfully put together an infrastructure to generate the billions of events needed by the studies for snowmass and help drive the future course of the us hep program .",
    "the dedicated team at the lpc has been able to generate a huge amount of monte carlo data based on simulating the response of a generic snowmass detector  @xcite to the passage of particles produced from proton - proton collisions at several tev center - of - mass energies . with billions of simulated data events for standard model physics processes and new physics signals of interests for the future , the lpc team and the osg team",
    "have together enabled studies which help define the future vision of the us program for the energy frontier .    in section  [ sec : sim ] , we discuss the overview and requirements of the simulations followed by a detailed description of the mechanism used to harness resources distributed across various sites in section  [ sec : infrastructure ] . in sections",
    "[ sec : utilization ] and  [ sec : storage ] , we outline the osg resources used and storage model used for this exercise , followed by a summary in section  [ sec : summary ] .",
    "the snowmass lpc group has been running two types of jobs .",
    "the first set of jobs , `` _ event generation _ '' , runs matrix - element package , madgraph  @xcite to generate physics processes from the proton - proton collisions ( or `` events '' ) expected from the lhc .",
    "the output from these jobs then serves as input to the second class of jobs . in this second set of jobs ,",
    "`` _ detector simulation and reconstruction _ ''",
    ", the generated events are processed serially by packages named bridge  @xcite , pythia  @xcite , and delphes  @xcite , with the resulting output transferred to the storage locations .",
    "these outputs contain data similar to the data we get from actual proton - proton collisions : collections of leptons , jets and other particles , along with their characteristics ( energy , momentum , etc ) .",
    "the first set of `` _ event generation _ ''",
    "jobs needs instructions regarding the type of events to simulate .",
    "these instructions come in the form of a `` gridpack , '' and given that their size is o(10 mb ) they are transferred by htcondor with the jobs .",
    "the output of the _ event generation _ stage is in les houches accord ( lhe ) 4-vector event format , with a size of o(10 mb ) , and is transferred back to osg - xsede via htcondor .",
    "the second set of `` _ detector simulation and event reconstruction _ ''",
    "jobs simulate the accelerator conditions , detector performance , and reconstruction of the particles ( or objects ) in an event .",
    "simulation of the accelerator conditions with around 50 to 140 extra interactions per bunch crossing , termed as pile - up interactions , is one of the challenges of this step . to simulate these conditions , we need access to a large `` minimum bias '' file , which is o(1 gb ) . in an earlier iteration of our recipe",
    "the file size was significantly large , almost 50 gb . this is too large to transfer with each job and hence we developed a solution which leads to much less data transfer .",
    "this solution is detailed in the next section .",
    "job submission was done through htcondor using glideinwms @xcite .",
    "glideinwms essentially starts a job , called a _ pilot job _ , on each worker node .",
    "the pilot job advertises its existence to the wider system and eventually runs one or more science jobs depending on their length .",
    "glideinwms is the standard submission system that is recommended to new users .",
    "we shipped the science applications with the user job but did not necessarily ship the software dependencies of these applications . for the latter we used the following methods :    1 .",
    "use the software in the cmssw stack @xcite that is generally available at sites through cvmfs @xcite .",
    "cvmfs also enables caching and thus is efficient and scalable .",
    "2 .   use parrot @xcite to access the cmssw software instead of directly accessing cvmfs .",
    "a parrot client can be shipped with the job so this allows jobs to run at sites that does nt necessarily have cvmfs installed .",
    "3 .   ship the needed software dependencies with each job .",
    "this first involves determining which of the application s software dependencies are nt installed on all worker nodes .",
    "those dependencies are put into a tar archive that htcondor moves to the worker node .",
    "finally the jobs have to set up the right environment variables like ld_library_path .",
    "the minimum bias file is pre - staged to storage elements at 10 different grid sites using the osg public storage service built on top of irods  @xcite .",
    "when a job starts , it checks for the presence of the minimum bias file in a temporary directory associated with the glidein , and uses it if available . in absence of the file",
    ", the job uses the osg public storage to get the file from one of the grid sites it is staged at , and replicates it into the glidein temporary ( temp ) directory .",
    "thus , the data transfer is spread across several different sites , and the file is reused when possible .",
    "some sites do not allow installing an irods - osg client on pilot start up , so on those sites we use `` srm ''  @xcite to directly replicate the inputs with a hardcoded `` surl '' from either fnal , unl , or bnl .",
    "the osg public storage handles users and resources quotas and is capable of uploading and downloading files to the osg resources on user request .",
    "it provides a unified  namespace  for all the stored files .",
    "the user can request data without knowing the exact location of the file .",
    "this significantly simplifies the task of accessing the data .",
    "srm  @xcite is a protocol for accessing large disk or tape arrays .",
    "srm was particularly useful for snowmass because it can queue up transfer requests if several of them arrive at once , this reduces the chances of overwhelming the underlying server .",
    "resources are harnessed using an osg submit host at indiana university .",
    "jobs were submitted to all sites , mostly in the us .",
    "the top contributors are shown in table  [ tab : sites ] .",
    "the resource utilization information was obtained from the osg gratia accounting service  @xcite . since february",
    "a total of 7.8 million hours has been used by the snowmass group . figure  [ fig : hours ] demonstrates the usage of computing hours , specifically walltime , on a monthly basis .",
    "more than 100k cpu hours were used on a significant number of days starting from march to may .",
    "the phase of operations prior to may , phase 1 , yielded about 1.5 billion lhe events , and 0.5 billion output events from delphes .",
    "the lhe event generation scheme used prior to mid may was processing intensive .",
    "the `` inclusive event generation '' scheme was later changed to the `` st binned weighted event generation ''  @xcite scheme and led to much reduced processing times .",
    "figure  [ fig : jobs ] shows a monthly summary of the complted number of jobs .",
    "about 14k jobs were complted per day , with a peak of around 70 - 80k jobs run during the last week of may 2013 and first week of june 2013 .",
    "these were due to the lhe event generation at @xmath4=14 tev in may and subsequent delphes processing in early june .",
    "the delphes jobs consumed less processing time , and hence do not contribute as much of a load in the distribution of monthly hours used ( see also figure  [ fig : hours ] ) .",
    ".the number of hours contributed by the top 15 osg computing sites . [ cols=\">,>\",options=\"header \" , ]     [ tab : dataload ]",
    "new methods and tools were necessary to simulate large physics backgrounds for future endeavors and road - maps by the us high - energy physics community .",
    "opportunistic resources were harnessed at a large scale using the open science grid infrastructure .",
    "the results of the studies , involving higgs , top , and new physics searches , etc . , based on simulated events enabled by the osg resources , will be presented in the snowmass workshop at minneapolis , july 29 - aug 6 , 2013 .",
    "these studies will provide guidance on the future direction of the energy frontier to the scientific community .",
    "the help from osg and fnal lpc was crucial in the success of this program .",
    "this research was done using resources provided by the open science grid , which is supported by the national science foundation and the u.s .",
    "department of energy s office of science .",
    "the storage elements from uscms t1 at fnal , the holland computing center at unl , and the atlas t1 at bnl were used for the studies .",
    "we are thankful for help from the members of the aaa  @xcite project and osg teams with the parrot integration , the cilogon and with distributed glideinwms usage and support .",
    "d. thain , m. livny , `` parrot : an application environment for data - intensive computing '' , scalable computing : practice and experience , volume 6 , number 3 , pages 918 , 2005 .",
    "`` irods : data grids , digital libraries , persistent archives , and real - time data systems '' , https://www.irods.org/index.php/irods:data_grids,_digital_libraries,_persistent_archives,_and_real-time_data_systems          a.  avetisyan , j.  m.  campbell , t.  cohen , n.  dhingra , j.  hirschauer , k.  howe , s.  malik and m.  narain _ et al .",
    "_ , arxiv:1308.1636 [ hep - ex ] .",
    "r.  brun and f.  rademakers , `` root - an object oriented data analysis framework , '' proceedings aihenp96 workshop , lausanne , sep .",
    "1996 , nucl .",
    "instrum . and methods phys",
    "* a389 * 81 - 86 ( 1997 ) .",
    "see also http://root.cern.ch/."
  ],
  "abstract_text": [
    "<S> snowmass is a us long - term planning study for the high - energy community by the american physical society s division of particles and fields . for its simulation studies , </S>",
    "<S> opportunistic resources are harnessed using the open science grid infrastructure . </S>",
    "<S> late binding grid technology , glideinwms , was used for distributed scheduling of the simulation jobs across many sites mainly in the us . </S>",
    "<S> the pilot infrastructure also uses the parrot mechanism to dynamically access cvmfs in order to ascertain a homogeneous environment across the nodes . </S>",
    "<S> this report presents the resource usage and the storage model used for simulating large statistics standard model backgrounds needed for snowmass energy frontier studies .    </S>",
    "<S> aram avetisyan saptaparna bhattacharya , meenakshi narain sanjay padhi jim hirschauer , tanya levshina , patricia mcbride , chander sehgal , marko slyz mats rynge sudhir malik john stupak iii </S>"
  ]
}