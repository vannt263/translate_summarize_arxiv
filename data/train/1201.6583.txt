{
  "article_text": [
    "one goal of ai research is to enable artificial agents ( either virtual or physical ones ) to act `` intelligently '' in complex and difficult environments .",
    "a common view is that intelligent behavior can be `` engineered '' ; either by fully hand - coding all the necessary rules into the agent , or by relying on various optimization - based techniques to automatically generate it .",
    "for example , in modern control and dynamic programming a human designer specifies a performance signal which explicitly or implicitly encodes goals of the agent . by behaving in a way that optimizes this quantity",
    ", the agent then does what the programmer wants it to do . for many applications ,",
    "this is a perfectly reasonable approach that can lead to impressive results .",
    "however , it typically requires some prior knowledge and sometimes subtle design by the human developer to achieve sensible or desirable results .    in this paper",
    ", we investigate an approach to use the `` embodiment '' of an agent ( i.e. , the dynamics of its coupling to the environment ) to generate preferred behaviors without having to resort to specialized , hand - designed solutions that vary from task to task .",
    "our research embraces the related ideas of self - organization and self - regulation , where we aim for complex behavior to derive from simple and generic internal rules .",
    "the philosophy is that seemingly intentional and goal - driven behavior emerges as the by - product of the agent trying to satisfy universal rules rather than from optimizing externally defined rewards .",
    "examples of this kind of work include _ homeokinesis _",
    "@xcite , or the work in @xcite .",
    "the second idea is that of intrinsically motivated behavior and artificial curiosity @xcite , where an agent engages in behavior because it is inherently `` interesting '' or `` enjoyable '' , rather than as a step towards solving a specific ( externally defined ) goal .",
    "intrinsically motivated behavior may not directly help in solving a goal , but there are indications that it leads to exploration and allows an agent to acquire a broad range of abilities which can , once the need arises , be easily molded into goal - directed behavior .",
    "related relevant publications include , for example , @xcite . other related work can be found in @xcite and @xcite .    here",
    "we will consider the principle of _ empowerment _",
    "@xcite , an information - theoretic quantity which is defined as the channel capacity between an agent s actions and its sensory observations in subsequent time steps .",
    "empowerment can be regarded as `` universal utility '' which defines an a priori intrinsic reward or rather , a value / utility for the states in which an agent finds itself in .",
    "empowerment is fully specified by the dynamics of the agent - environment coupling ( namely the transition probabilities ) ; a reward does not need to be specified .",
    "it was hypothesized in @xcite that the greedy maximization of empowerment would direct an agent to `` interesting '' states in a variety of scenarios :    * for one , empowerment can be considered a stochastic generalization of the concept of _ mobility _ ( i.e. , number of options available to an agent ) which is a powerful heuristic in many deterministic and discrete puzzles and games .",
    "being in a state with high empowerment gives an agent a wide choice of actions  conversely , if an agent in `` default mode '' poises itself a  priori in a high - empowerment state , it is best equipped to quickly move from there into a variety of target states in an emergency ( for example , in the game of soccer , a goalkeeper who is about to receive a penalty kick and has no prior knowledge about the player behavior to expect naturally positions himself in the middle of the goal ) . in this regard",
    "the quantity of empowerment allows an agent to automatically ( without explicit external human input ) identify those states , even in complex environments . * in the present paper we show that , for a certain class of continuous control problems , empowerment provides a natural utility function which imbues its states with an a priori value , without an explicit specification of a reward .",
    "such problems are typically those where one tries to keep a system `` alive '' indefinitely , i.e. , in a certain goal region for as long a time as possible . on the other hand , choosing the wrong actions or doing nothing would instead lead to the `` death '' of the system ( naturally represented by zero empowerment ) .",
    "a natural example is pole - balancing . in this context",
    ", we will find the smoothness of the system informs the local empowerment gradients around the agent s state of where the most `` alive '' states are .",
    "choosing actions such that the _ local _ empowerment score is maximized would then lead the agent into those states . in the pole - balancing example",
    "this means that for a wide range of initial conditions , the agent would be made to balance the pendulum .",
    "previous studies with empowerment showed promise in various domains but were essentially limited to the case of small - scale and finite - state domains ( the ubiquitous gridworld ) and furthermore , state transition probabilities were assumed to be known a priori .",
    "the main contribution of this article is to extend previous work to the significantly more important case of ( 1 ) continuous vector - valued state spaces and ( 2 ) initially unknown state transition probabilities .",
    "the first property means that we will be able to calculate empowerment values only approximately ; more specifically , here we will use monte - carlo approximation to evaluate the integral underlying the empowerment computation .",
    "the second property considers the case where the state space is previously unexplored and implies that the agent has to use some form of online model learning to estimate transition probabilities from _ state - action - successor state _ triplets it encounters while interacting with the environment . here",
    ", we will approach model learning using gaussian process regression with iterated forecasting .    to summarize",
    ", the paper is structured into three parts as follows :",
    "1 .   the first part , section  [ sec : informal ] , gives a first , informal definition of empowerment and illustrates its general properties in a well - known finite - state domain .",
    "the second part forms the main technical portion .",
    "section  [ sect : practical computation of empowerment ] starts with a formal definition of empowerment for the continuous case and gives an algorithm for its computation based on monte - carlo approximation of the underlying high - dimensional integrals .",
    "section  [ sect : model learning ] describes model learning using gaussian process regression ( gps )  however , since this itself is a rather complex subject matter , for brevity here we can not go beyond a high - level description .",
    "3 .   the third part examines empowerment empirically in a number of continuous control tasks well known in the area of reinforcement learning .",
    "the experiments will demonstrate how empowerment can form a natural utility measure , and how states with high empowerment values coincide with the natural ( and intuitive ) choice of a goal state in the respective domain . this way ,",
    "if we incorporate empowerment into the perception - action loop of an agent , e.g. , by greedily choosing actions that lead to the highest empowered states , we can obtain a seemingly goal - driven behavior . as an application of this",
    ", we study the problem of exploration and model learning : using empowerment to guide which parts of the state - space to exlore next , the agent can quickly `` discover the goal '' and thus more efficiently explore the environment  without exhaustively sampling the state space .",
    "although a more formal definition of empowerment will follow in the next section , here we will start by motivating it through a toy example .",
    "informally , empowerment computes for any state of the environment the logarithm of the _ effective _ number of successor states the agent can induce by its actions . thus empowerment essentially measures to what extent an agent can influence the environment by its actions : it is zero if , regardless what the agent does , the outcome will be the same . and it is maximal if every action will have a _ distinct",
    "_ outcome .",
    "note that empowerment is specifically designed to allow for more general stochastic environments , of which deterministic transitions are just a special case .",
    "l0.17        as an example , consider the taxi - domain @xcite , a well - known problem in reinforcement learning with finite state and action space and stochastic transitions .",
    "the environment , shown on the left , consists of a @xmath2 gridworld with four special locations designated r,y,g,b. apart from the agent ( `` the taxi '' ) , there is a passenger who wants to get from one of the four locations to another ( selected at random ) .",
    "the state of the system is the @xmath3 coordinate of the agent , the location of the passenger ( one of r,y,g,b,in - the - car ) and its destination ( one of r,y,g,b ) .",
    "overall there are @xmath4 distinct states . usually in rl , where the interest is on abstraction and hierarchical learning , a factored representation of the state",
    "is used that explicitly exploits the structure of the domain . for our purpose , where identifying salient states is part of the problem , we do not assume that the structure of the domain is known and will use a flat representation instead .",
    "the agent has six possible elementary actions : the first four ( n,s,e,w ) move the agent in the indicated direction ( stochastically , there is a 20% chance for random movement ) .",
    "if the resulting direction is blocked by a wall , no movement occurs . the agent can also issue a pick - up and drop - off action , which require that the taxi is at the correct location and ( in the latter case ) the passenger is in the car . issuing pick - up and drop - off when the conditions are not met does not result in any changes .",
    "if a passenger is successfully delivered , the environment is reset : the agent is placed in the center and a passenger with new start and destination is generated .    using these state transition dynamics",
    ", we compute the @xmath5-step empowerment , i.e. , the _ effective _ number of successor states reachable over an action horizon of @xmath5 steps ( meaning we consider compound actions of a sequence of three elementary actions ) for every state of the system .",
    "figure  [ fig:1 ] shows some of the results : the values are ordered such that every subplot shows the empowerment values that correspond to a specific slice of the state space .",
    "for example , the top left subplot shows the empowerment value of all @xmath3 locations if the passenger is waiting at y and its destination is g , which with our labeling of the states corresponds to states 376 - 400 . inspecting the plots ,",
    "two things become apparent : for one , in general , locations in the center have high empowerment ( because the agent has freedom to move wherever it wants ) ; locations in the corners have low empowerment ( because the agent has only limited choices of what it can do ) .",
    "more interesting is the empowerment value at the designated locations : if a passenger is waiting at a certain location , its empowerment , and that of its neighbors @xmath6 steps away , increases . similarly ,",
    "if a passenger is in the car , the empowerment of the destination , and that of its neighbors @xmath6 steps away , increases .",
    "the reason is that in both situations the agent now has additional , previously unavailable , ways of affecting the environment ( plot ( c ) and ( d ) have a higher relative gain in empowerment , because they result in the end of an episode , which teleports the agent to the center ) .",
    "thus these states stand out as being `` interesting '' under the heuristic of empowerment .",
    "incidentally , these are also exactly the subgoal states if the agent s task were to transport the passenger from source to destination .",
    "note that here we did not have to specify external reward or goals , as empowerment is intrinsically computed from the transition dynamics alone .",
    "empowerment essentially `` discovers '' states where additional degrees of freedom are available , and creates a basin of attraction around them , indicating salient features of the environment of interest to the agent .",
    "it is not difficult to imagine an agent that uses empowerment as a guiding principle for exploration ; e.g. , by choosing in each state greedily the action that leads to the successor state with the highest empowerment .",
    "we expect that such an agent would traverse the state space in a far more sensible way than blind random exploration , as following the trail of increasing empowerment would quickly lead to the discovery of the salient states in the environment . in the remainder of the paper , we will develop methods for carrying over this idea into the continuum and demonstrate how empowerment supersedes typical hand - designed rewards in a number of established benchmark domains .",
    "\\(a ) p waiting at y",
    "\\(b ) p waiting at r        \\(c ) p in car , going to g        \\(d ) p in car , going to b",
    "this section defines empowerment formally and gives an algorithm for its computation .",
    "empowerment @xcite is defined for stochastic dynamic systems where transitions arise as the result of making a decision , e.g. such as an agent interacting with an environment . here",
    "we will assume a vector - valued state space @xmath7 and ( for simplicity ) a discrete action space @xmath8 .",
    "the transition function is given in terms of a density ) .",
    "this can be interpreted as modeling limited action or sensoric resolution , depending on the take .",
    "it is also a natural assumption for a robot realized in hardware . ]",
    "@xmath9 which denotes the probability of going from state @xmath10 to @xmath11 when making decision @xmath12 .",
    "while we assume the system is fully defined in terms of these @xmath13-step interactions , we will also be interested in more general @xmath14-step interactions .",
    "thus , for @xmath15 , we consider the sequence @xmath16 of @xmath14 single - step actions and the induced probability density @xmath17 of making the corresponding @xmath14-step transition .",
    "for notational convenience we can assume that , without loss of generality , @xmath13-step and @xmath14-step actions are equivalent : let the set of possible @xmath14-step actions be formed through exhaustive enumeration of all possible combinations of @xmath13-step actions . if @xmath18 is the number of possible @xmath13-step actions in every state , the number of @xmath14-step actions is then @xmath19 . with this approach",
    ", we can consider the system as evolving at the time - scale of @xmath14-step actions , so that @xmath14-step actions can be regarded as @xmath13-step actions at a higher level of decision making .",
    "this abstraction allows us to treat @xmath13-step and @xmath14-step actions on equal footing , which we will use to simplify the notation and drop references to the time index . instead of writing @xmath17 we will now just write @xmath20 to denote the transition from @xmath21 to @xmath22 under @xmath23 , irrespective of whether @xmath23 is an @xmath14-step action or @xmath13-step action .",
    "furthermore we will use the symbol @xmath24 to loop over actions @xmath23 .",
    "let @xmath25 denote the random variable associated with @xmath22 given @xmath21 .",
    "assume that the choice of a particular action @xmath23 is also random and modeled by random variable @xmath26 .",
    "the _ empowerment _ @xmath27 of a state @xmath21 ( more precisely , the @xmath14-step empowerment ) is then defined as the shannon channel capacity ( using the differential entropy ) between @xmath26 , the choice of an action sequence , and @xmath25 , the resulting successor state : @xmath28 the maximization of the mutual information is with respect to all possible distributions over @xmath26 , which in our case means vectors of length @xmath29 of probabilities .",
    "the entropy and conditional entropy are given by @xmath30 strictly speaking , the entropies in eqs .   and",
    "are differential entropies ( which could be negative ) and the probabilities are to be read as probability densities . however , as we always end up using the mutual information , i.e. the difference between the entropies , we end up with well - defined non - negative information values which are always finite due to the limited resolution / noise assumed above . using @xmath31 in eqs .   and , eq .",
    "can thus be written as @xmath32                 roll rate @xmath36 and angular velocity @xmath37 are kept in the interval @xmath38 $ ] via saturation ; roll angle @xmath39 is restricted to @xmath40 $ ] . whenever the roll angle is larger than @xmath41 in either direction , the bicycle is supposed to have fallen .",
    "this state is treated as a terminal state by defining all outgoing transitions as self - transitions , that is , once a terminal state is reached , the system stays there indefinitely , no matter what control is performed .",
    "thus , to keep the bicycle going forward , the bicycle has to be prevented from falling .",
    "the solution to the continuous - time dynamic equations in eqs .",
    "- is obained using a runge - kutta solver .",
    "the time step of the simulation is 0.2 sec , during which the applied control is kept constant .",
    "the 4-dimensional state vector is @xmath42 , the 2-dimensional control vector is @xmath43 .",
    "control variable @xmath44 was allowed to vary in @xmath45 $ ] , @xmath46 was allowed to vary in @xmath47 $ ] . since our algorithm in section  [ sec : example : gaussian model ] allows us to compute empowerment only for a finite set of possible @xmath13-step actions , we discretized the continuous control space .",
    "as in @xcite , we only consider the following 5 discrete actions : @xmath48 ."
  ],
  "abstract_text": [
    "<S> this paper develops generalizations of _ empowerment _ to continuous states . </S>",
    "<S> empowerment is a recently introduced information - theoretic quantity motivated by hypotheses about the efficiency of the sensorimotor loop in biological organisms , but also from considerations stemming from curiosity - driven learning . </S>",
    "<S> empowemerment measures , for agent - environment systems with stochastic transitions , how much influence an agent has on its environment , but only that influence that can be sensed by the agent sensors . </S>",
    "<S> it is an information - theoretic generalization of joint controllability ( influence on environment ) and observability ( measurement by sensors ) of the environment by the agent , both controllability and observability being usually defined in control theory as the dimensionality of the control / observation spaces . </S>",
    "<S> earlier work has shown that empowerment has various interesting and relevant properties , e.g. , it allows us to identify salient states using only the dynamics , and it can act as intrinsic reward without requiring an external reward . </S>",
    "<S> however , in this previous work empowerment was limited to the case of small - scale and discrete domains and furthermore state transition probabilities were assumed to be known . </S>",
    "<S> the goal of this paper is to extend empowerment to the significantly more important and relevant case of continuous vector - valued state spaces and initially unknown state transition probabilities . </S>",
    "<S> the continuous state space is addressed by monte - carlo approximation ; the unknown transitions are addressed by model learning and prediction for which we apply gaussian processes regression with iterated forecasting . in a number of well - known continuous control tasks </S>",
    "<S> we examine the dynamics induced by empowerment and include an application to exploration and online model learning .    * empowerment for continuous agent - environment systems *    </S>",
    "<S> @xmath0 + tjung@cs.utexas.edu +    * daniel polani*@xmath1 + d.polani@herts.ac.uk +    * peter stone*@xmath0 + pstone@cs.utexas.edu +    @xmath0department of computer science + university of texas at austin + 1616 guadalupe , suite 2408 + austin , texas 78701 + usa    @xmath1adaptive systems and algorithms research groups + school of computer science + university of hertfordshire + 1 college lane + hatfield al10 9ab , herfordshire + united kingdom    * empowerment for continuous agent - environment systems * + initial submission september 30 , 2010 + revision november 4 , 2010    * keywords : * information theory , learning , dynamical systems , self - motivated behavior    empowerment for continuous agent - environment systems </S>"
  ]
}