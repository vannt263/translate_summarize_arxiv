{
  "article_text": [
    "the notion of ` power - law ' distributions is not recent , and they first arose in economics in the studies of pareto  @xcite hundred years ago .",
    "later , power - law behavior was observed in various fields such as physics , biology , computer science etc .",
    "@xcite , and hence the phrase `` ubiquitous power - laws '' .",
    "though the term was first coined for distributions with a negative constant exponent , _",
    "i.e. _ , @xmath0 , the meaning of the term has expanded in due course of time to include various fat - tailed distributions , _",
    "i.e. _ , distributions decaying at a slower rate than gaussian distribution .",
    "this class is also referred to as generalized pareto distributions .    on the other hand , though the generalizations of information measures were proposed in the beginning of the birth of information theory , only ( relatively ) recently their connections with power - law distributions have been established .",
    "while maximization of shannon entropy gives rise to exponential distributions , these generalized measures give power - law distributions .",
    "this actually led to a dramatic increase in interest in generalized information measures and their application to statistics .",
    "indeed , the starting point of the theory of generalized measures of information is due to alfred rnyi  @xcite .",
    "another generalization was introduced by havrda and charvt  @xcite , and then studied by tsallis  @xcite in statistical mechanics that is known as tsallis entropy or nonextensive entropy .",
    "tsallis entropy involves a parameter @xmath1 , and it retrieves shannon entropy as @xmath2 . the shannon - khinchin axioms of shannon entropy have been generalized to this case  @xcite , and this entropy functional has been studied in information theory , statistics and many other fields .",
    "tsallis entropy has been used to study power - law behavior in different cases like finance , earthquakes and network traffic  @xcite .    in kernel",
    "based machine learning , positive definite kernels are considered as a measure of similarity between points  @xcite .",
    "the choice of kernel is critical to the performance of the learning algorithms , and hence , many kernels have been studied in literature  @xcite .",
    "kernels based on information theoretic quantities are also commonly used in text mining and image processing  @xcite . however , such kernels are defined on probability measures .",
    "probability kernels based on tsallis entropy have also been studied in  @xcite .    in this work ,",
    "we are interested in kernels based on maximum entropy distributions .",
    "it turns out that gaussian , laplacian , cauchy kernels , which have been extensively studied in machine learning , have corresponding distributions , which are maximum entropy distributions .",
    "this motivates us to look into kernels that correspond to maximum tsallis entropy distributions , also termed as tsallis distributions .",
    "these distributions have inherent advantages as they are generalizations of exponential distributions , and they exhibit power - law nature  @xcite .",
    "in fact , the value of @xmath1 controls the nature of the power - law tails .    in this paper",
    ", we propose a new kernel based on @xmath1-gaussian distribution , which is a generalization of gaussian , obtained by maximizing tsallis entropy under certain moment constraints .",
    "further , we introduce a generalization of the laplace distribution following the same lines , and propose a similar @xmath1-laplacian kernel .",
    "we give some insights into reproducing kernel hilbert spaces ( rkhs ) of these kernels .",
    "we prove that the proposed kernels are positive definite over a range of values of @xmath1 .",
    "we demonstrate the effect of these kernel by applying them to machine learning tasks : classification and regression by svms .",
    "we provide results indicating that in some cases , the proposed kernels perform better than their counterparts ( gaussian and laplacian kernels ) for certain values of @xmath1 .",
    "tsallis entropy can be obtained by generalizing the information of a single event in the definition of shannon entropy as shown in  @xcite , where natural logarithm is replaced with @xmath1-logarithm defined as @xmath3 , @xmath4 , @xmath5 , @xmath6 .",
    "tsallis entropy in a continuous case is defined as  @xcite @xmath7 this function retrieves the differential shannon entropy functional as @xmath8 .",
    "it is called nonextensive because of its pseudo - additive nature  @xcite .",
    "kullback s minimum discrimination theorem  @xcite establishes connections between statistics and information theory .",
    "a special case is jaynes maximum entropy principle  @xcite , by which exponential distributions can be obtained by maximizing shannon entropy functional , subject to some moment constraints . using the same principle , maximizing tsallis entropy under the following constraint @xmath9 results in a distribution known as @xmath1-exponential distribution  @xcite , which is of the form @xmath10 where the @xmath1-exponential , @xmath11 , is expressed as @xmath12 the condition @xmath13 in   is called the tsallis cut - off condition , which ensures existence of @xmath1-exponential .",
    "if a constraint based on the second moment , @xmath14 is considered along with  , one obtains the @xmath1-gaussian distribution  @xcite defined as @xmath15 where @xmath16 is the normalizing constant  @xcite .",
    "however , instead of  , if the constraint @xmath17 is considered , then maximization of tsallis entropy with only this constraint leads to a @xmath1-variant of the doubly exponential or laplace distribution centered at zero .",
    "a translated version of the distribution can be written as @xmath18 as @xmath2 , we retrieve the exponential , gaussian and laplace distributions as special cases of  , and  , respectively .",
    "the above distributions can be extended to a multi - dimensional setting in a way similar to gaussian and laplacian distributions , by incorporating 2-norm and 1-norm in   and  , respectively .",
    "based on the above discussion , we define the @xmath1-gaussian kernel @xmath19 , for a given @xmath20 , as @xmath21 where @xmath22 is the input space , and @xmath23 are two parameters controlling the behavior of the kernel , satisfying the conditions @xmath24 , @xmath25 and @xmath26 . for @xmath27 ,",
    "the term inside the bracket is non - negative and hence , the kernel is of the form @xmath28 where @xmath29 is the euclidean norm . on similar lines , we use   to define the @xmath1-laplacian kernel @xmath30 @xmath31 where @xmath32 is the @xmath33-norm , and",
    "@xmath34 satisfy the conditions @xmath24 , @xmath35 and @xmath36 . as before , for @xmath37 , the kernel can be written as @xmath38    -gaussian and ( b ) @xmath1-laplacian kernels with @xmath39.,scaledwidth=45.0% ]    due to the power - law tail of the tsallis distributions for @xmath40 , in case of the above kernels , similarity decreases at a slower rate than the gaussian and laplacian kernels with increasing distance .",
    "the rate of decrease in similarity is controlled by the parameter @xmath1 , and leads to better performance in some machine learning tasks , as shown later .",
    "figure  [ example_plot ] shows how the similarity decays for both @xmath1-gaussian and @xmath1-laplacian kernels in the one - dimensional case .",
    "it can be seen that as @xmath1 increases , the initial decay becomes more rapid , while towards the tails , the decay becomes slower .",
    "we now show that for certain values of @xmath1 , the proposed kernels satisfy the property of positive definiteness , which is essential for them to be useful in learning theory .",
    "berg et al .",
    "@xcite have shown that for any symmetric kernel function @xmath41 , there exists a mapping @xmath42 , @xmath43 being a higher dimensional space , such that @xmath44 , for all @xmath45 if and only if @xmath46 is positive definite ( p.d . ) , _ i.e. _ , given any set of points @xmath47 , the @xmath48 matrix @xmath49 , such that @xmath50 , is positive semi - definite .",
    "we first state some of the results presented in  @xcite , which are required to prove positive definiteness of the proposed kernel .",
    "[ pd_result1 ] for a p.d .",
    "kernel @xmath51 , @xmath52 , the following conditions are equivalent :    1 .",
    "@xmath53 is negative definite ( n.d . ) , and 2 .",
    "@xmath54 is p.d . for all @xmath55 .",
    "[ pd_result4 ] let @xmath51 be a n.d .",
    "kernel , which is strictly positive , then @xmath56 is p.d .",
    "we state the following proposition , which is a general result providing a method to generate p.d .",
    "power - law kernels , given that their exponential counterpart is p.d .",
    "[ power_pd ] given a p.d .",
    "kernel @xmath51 of the form @xmath57 , where @xmath58 for all @xmath45 , the kernel @xmath59 given by @xmath60 is p.d .",
    ", provided the constants @xmath61 and @xmath62 satisfy the conditions @xmath63 and @xmath64 .",
    "since , @xmath65 is p.d . , it follows from lemma  [ pd_result1 ] that the kernel @xmath66 is n.d .",
    "thus , for any @xmath63 , @xmath67 is n.d . , and as @xmath68 , we can say @xmath67 is strictly positive .",
    "so , application of lemma  [ pd_result4 ] leads to the fact that @xmath69 is p.d . finally , using lemma  [ pd_result1 ] , we can claim @xmath70 is p.d . for all @xmath63 and @xmath64 .    from proposition  [ power_pd ] and positive definiteness of gaussian and laplacian kernels",
    ", we can show that the proposed @xmath1-gaussian and @xmath1-laplacian kernels are p.d . for certain ranges of @xmath1 .",
    "however , strikingly , it turns out that over this range , the kernels exhibit power - law behavior .",
    "[ qg_pd ] for @xmath27 , the @xmath1-gaussian kernel , as defined in  , is positive definite .",
    "[ ql_pd ] for @xmath37 , the @xmath1-laplacian kernel , as defined in  , is positive definite for all @xmath71 .",
    "now , we show that some of the popular kernels can be obtained as special cases of the proposed kernels .",
    "the gaussian kernel is defined as @xmath72 where @xmath73 , @xmath74 .",
    "we can retrieve the gaussian kernel   when @xmath2 in  the @xmath1-gaussian kernel  .",
    "the rational quadratic kernel is of the form @xmath75 where @xmath76 , @xmath63 . substituting @xmath77 in",
    ", we obtain   with @xmath78 .",
    "the laplacian kernel is defined as @xmath79 where @xmath73 , @xmath74 .",
    "we can retrieve   as @xmath2 in  the @xmath1-laplacian kernel  .",
    "as discussed earlier , kernels map the data points to a higher dimensional feature space , also called the reproducing kernel hilbert space ( rkhs ) that is unique for each positive definite kernel  @xcite . the significance of rkhs for support vector kernels using bochner s theorem  @xcite , which provides a rkhs in fourier space for translation invariant kernels , is stated in  @xcite .",
    "other approaches also exist that lead to explicit description of the gaussian kernel  @xcite , but this approach does not work for the proposed kernels as taylor series expansion of the @xmath1-exponential function   does not converge for @xmath40 .",
    "so , we follow the bochner s approach .",
    "we state bochner s theorem , and then use the method presented in  @xcite to show how it can be used to construct the rkhs for a p.d . kernel .",
    "[ pd_result7 ] a continuous kernel @xmath80 on @xmath81 is positive definite if and only if @xmath82 is the fourier transform of a non - negative measure , i.e. , there exists @xmath83 such that @xmath84 is the inverse fourier transform of @xmath82 .",
    "then , the rkhs of the kernel @xmath65 is given by @xmath85 with the inner product defined as @xmath86 where @xmath87 is the fourier transform of @xmath88 and @xmath89 is set of all functions on @xmath90 , square integrable with respect to the lebesgue measure .",
    "it must be noted here that in our case , the existence and non - negativity of the inverse fourier transform @xmath91 is obvious due to the positive definiteness of the proposed kernels ( corollaries  [ qg_pd ] and  [ ql_pd ] ) .",
    "hence , to describe the rkhs it is enough to determine an expression for @xmath91 for both the kernels .",
    "we define the functions corresponding to the @xmath1-gaussian and @xmath1-laplacian kernels , respectively , as @xmath92 and @xmath93 where @xmath94 , @xmath71 and @xmath95 .",
    "we derive expressions for their inverse fourier transforms @xmath96 and @xmath97 , respectively .",
    "for this , we require a technical result  ( * ? ? ? * eq .",
    "4.638(3 ) ) , which is stated in the following lemma .",
    "[ pd_result8 ] let @xmath98 and @xmath99 for @xmath100 be constants , then the @xmath101-dimensional integral @xmath102    we now derive the inverse fourier transforms .",
    "we prove the result for proposition  [ qg_fourier ] .",
    "the proof of proposition  [ ql_fourier ] proceeds similarly .",
    "[ qg_fourier ] the inverse fourier transform for @xmath103 is given by @xmath104    by definition ,",
    "@xmath105 expanding the exponential term , we have @xmath106 since , both @xmath107 are @xmath103 are even functions for every @xmath108 , while @xmath109 is an odd function , hence integrating over @xmath110 , all terms with a sin component become zero .",
    "further , the remaining term is odd , and hence , the integral is same in every orthant .",
    "so the expression reduces to @xmath111 where @xmath112 .",
    "each of the cosine term can be expanded in form of an infinite series as @xmath113 substituting in   and using lemma  [ pd_result8 ] , we obtain @xmath114 where @xmath115 using expansion of gamma function for half integers , we can write   as @xmath116 substituting in   and using @xmath117 , we have @xmath118 we arrive at the claim by observing that the terms in the inner summation in   are similar to terms of multinomial expansion of @xmath119 .",
    "it can be observed that the above result agrees with the fact that inverse fourier transform of radial functions are radial in nature .",
    "we present corresponding result for @xmath1-laplacian kernel .",
    "[ ql_fourier ] the inverse fourier transform for @xmath120 is given by @xmath121 where @xmath122 with @xmath123 being the components of @xmath124 .",
    "in this section , we apply the @xmath1-gaussian and @xmath1-laplacian kernels in classification and regression . we provide insights into the behavior of these kernels through examples .",
    "we also compare the performance of the kernels for different values of @xmath1 , and also with the gaussian , laplacian ( _ i.e. _ , when @xmath2 ) , and polynomial kernels using various data sets from uci repository  @xcite .",
    "the simulations have been performed using libsvm  @xcite .",
    "table  [ dataset ] provides a description of the data sets used .",
    "the last few data sets have been used for regression .",
    ".data sets ( sets marked @xmath125 have been normalized ) . [",
    "cols=\"^,^,^,^,^\",options=\"header \" , ]     the performance of the proposed kernels have been compared with polynomial , gaussian and laplacian kernels for various values of @xmath1 using data sets 11 , 12 and 13 .",
    "the results of 5-fold cross validation using @xmath126-svr @xmath127 are shown in table  [ reg_table ] .",
    "we fixed particular @xmath128 for each data set .",
    "though laplacian kernel seems to outperform its power - law variants , the @xmath1-gaussians dominate the performance of gaussian kernel .",
    "the results further indicate that the error is a relatively smooth function of @xmath1 , and does not have a fluctuating behavior , though its trend seems to depend on the data .",
    "the relative performance of the polynomial kernels is poor .",
    "in this paper , we proposed a power - law generalization of gaussian and laplacian kernels based on tsallis distributions .",
    "they retain their properties in the classical case as @xmath2 .",
    "further , due to their power - law nature , the tails of the proposed kernels decay at a slower rate than their exponential counterparts , which in turn broadens the use of these kernels in learning tasks .",
    "we showed that the proposed kernels are positive definite for certain range of @xmath1 , and presented results pertaining to the rkhs of the proposed kernels using bochner s theorem .",
    "we also demonstrated the performance of the proposed kernels in support vector classification and regression .",
    "the power - law behavior was recognized long time back in many problems in the context of statistical analysis .",
    "recently power - law distributions have been studied in machine learning communities . as far as our knowledge , this is the first paper that introduces and studies power - law kernels , leading to the notion of a _",
    "`` fat - tailed kernel machine''_."
  ],
  "abstract_text": [
    "<S> the role of kernels is central to machine learning . motivated by the importance of power - law distributions in statistical modeling , in this paper , we propose the notion of power - law kernels to investigate power - laws in learning problem . </S>",
    "<S> we propose two power - law kernels by generalizing gaussian and laplacian kernels . </S>",
    "<S> this generalization is based on distributions , arising out of maximization of a generalized information measure known as nonextensive entropy that is very well studied in statistical mechanics . </S>",
    "<S> we prove that the proposed kernels are positive definite , and provide some insights regarding the corresponding reproducing kernel hilbert space ( rkhs ) . </S>",
    "<S> we also study practical significance of both kernels in classification and regression , and present some simulation results . </S>"
  ]
}