{
  "article_text": [
    "suppose we have a collection of units we want to rank by a certain feature of each unit : for example , we may wish to rank genes by the risk they cause of a particular condition ; we may wish to rank sportsmen by their success - rate at particular standardised trials ; we may wish to rank business opportunities by the profit they will generate .",
    "this is a very common inference problem first studied as a formal statistical problem by bechhofer ( 1954 ) and by gupta ( 1956 ) .",
    "typically , for each unit we wish to rank , we will have some data on the associated feature , but will not know the true value of that feature . based on our data",
    ", we will have a point estimate for the feature , and an associated error distribution .",
    "the amount of data we might have for different units can vary wildly , meaning that the associated error distributions can be very different for different units .",
    "this means that when we select the top units using only our point estimates , the units for which we have largest errors have a higher chance of appearing among the top units , because a large error increases the chance of the point estimate being large .",
    "we are therefore likely to select a large number of false positives if we select based solely on the point estimates .",
    "we can illustrate this with a simple example .",
    "suppose we have 300 coins , we toss 100 of them six times each , 100 of them eight times each , and the remaining 100 of them ten times each , and rank the coins by the proportion of heads observed .",
    "if the coins are all fair , then among the 100 that we toss six times each , there is likely to be at least one that achieves 100% heads . among the 100 that we toss eight times",
    ", there might be one that achieves 100% heads , and there are likely to be several that achieve 87.5% heads . among the 100 that we toss ten times each , it is fairly unlikely than any will exceed 80% , so the highest ranked units will almost certainly come from among the coins that we toss only six times",
    ". that is , the highest ranked units are almost all false - positives arising only out of chance .",
    "this is still true , even if one or more of the coins that are tossed ten times have a slightly higher probability of heads .    on the other hand ,",
    "if our main aim is to avoid false positives , we could use a testing - based approach , where for each unit , we perform an hypothesis test of whether the unit has some null status  for example whether the probability of heads is 0.5 .",
    "we can then rank by the @xmath0-values of these tests .",
    "this has the advantage of minimising false positives , but in many cases there are a large number of true positives , but only a few of them are truly important .",
    "if we apply the testing approach , we will often select the units on which we have collected most data , simply because the more data we have , the more evidence that they are not null cases .",
    "this may lead to neglecting some units which have much higher underlying value , but for which we have less data .",
    "other approaches to the problem mainly take a bayesian approach .",
    "they assume that the true values of the relevant feature fall under some distribution .",
    "we can estimate this underlying distribution from all the data points . then for each observed unit",
    ", we use this distribution as a prior to estimate a posterior distribution of the true value for this unit .",
    "we then perform our ranking based on these posterior distributions and a choice of loss function .",
    "there are a range of different methods based on different loss functions .",
    "for example , posterior expected rank ( laird and louis , 1989 ) use a loss function the squared difference between the true rank of a unit ( based on the actual value of the feature ) and the estimated rank .",
    "the @xmath1-values method ( henderson and newton , 2015 ) , corresponds to a loss function the sum of absolute differences between estimated rank and true rank .",
    "both of these loss functions are based entirely upon ranks , with no consideration of the actual true values .",
    "that is , they consider mis - ranking two units with almost identical true values to be as bad as mis - ranking units with very different true values . for the vast majority of practical ranking problems , this will not be the case .",
    "gelman and price ( 1999 ) present the interesting case of looking for spatial patterns among the top - ranked units , where artificial patterns can arise from patterns in available sample sizes . for their purposes",
    ", the ideal ranking method would be in such a way that the distribution of rank is the same for all values of standard error .",
    "for a known prior , it is possible to calculate this rank , though we are not aware of any work applying such a ranking method .",
    "however , methods with loss functions based only on rank , rather than value might be expected to perform better on this criterion , since all errors in ranking can cause this issue equally .",
    "the aim of a ranking analysis is often to maximise the average true value from the selected units .",
    "for instance , in the business profit example , the aim would be to maximise the expected total profit .",
    "for these purposes , the loss function is the difference between the largest true values and the true values of selected units .",
    "this loss function is introduced in gupta and hsiao ( 1983 ) , with some additional thought given to the situation where the loss is different for the case of omitting a variable that should be included , from the case of including a variable that should be omited .",
    "they show that for this loss function with known prior the bayes rule is to rank by posterior mean ( though they are not very explicit about this , and include some unnecessary hypotheses ) .",
    "this posterior mean ranking is used for example in aitkin and longford ( 1986 ) .",
    "a range of other loss functions have also been considered , for example , lin _ et al_. ( 2006 ) summarise a range of choices of loss function . for this paper",
    ", we will be focussing on the posterior mean ranking method , and its corresponding loss function , although many of our methods can be easily adapted to other bayesian ranking methods .",
    "the key difficulty in bayesian ranking methods is to choose the form of the prior .",
    "two common choices are the conjugate prior ( which for normal error is normal ) , and a non - parametric prior , which can be calculated using the results of laird  ( 1978 ) .",
    "figure  [ motivatingproblem ] shows the sort of problem that can arise with this approach .",
    "the lines on that figure show points that are ranked equally by posterior mean under a normal prior estimated from the whole dataset . as can be seen in that plot , a lot of emphasis gets placed on points with small variance .",
    "the reason is that the normal prior is light - tailed , so large true values are deemed implausible , and discounted",
    ". however , the true prior distribution seems to be more heavy - tailed than the normal , so larger values should not be discounted so much . for example , consider the point in the red circle .",
    "while it does have a larger standard error , it is very significantly non - zero , and it is likely that the true log - odds ratio is high .",
    "intuitively , we would probably want to rank this data point among the very top - ranked units .",
    "however , the posterior mean under the normal prior ranks it below a lot of other points which , while certainly significantly non - zero , have very small effect size . for practical purposes ,",
    "this is not desirable .",
    "we are usually interested in units with a large effect size .",
    "the aim of this paper is to study the effect that choice of prior can have on the ranking problem , and determine suitable choices of prior for such analyses . despite a fair amount of literature on bayesian ranking methods ,",
    "there has been a noticeable lack of work on the question of choice of prior . in view of the fact that selecting a suitable model for the prior distribution is a very difficult problem in model selection ,",
    "it is important to consider the effects of a misspecified prior distribution . as will become apparent later ,",
    "certain choices of prior are inherently more robust to misspecification than others .",
    "furthermore , some choices of prior are more sensitive to parameter estimation than others .",
    "we describe the objective more formally as follows .",
    "a ranking problem consists of a collection of units with unobserved parameters @xmath2 . for each unit",
    ", we have a point estimate @xmath3 for @xmath2 .",
    "we assume that @xmath3 is normally distributed with mean @xmath2 and variance @xmath4 , where @xmath5 is known .",
    "it is straightforward to adapt our approach to a number of other error distributions , but for this paper , we will focus on the normal error case .",
    "we assume that the unobserved values @xmath2 follow what we will refer to as the _ true prior _ distribution .",
    "we will rank by posterior mean using what we will refer to as the _ estimating prior _ , which may or may not be the same as the true prior .",
    "we are interested in how choice of the estimating prior affects the ranking .",
    "the structure of this paper is as follows : in section  [ theorysection ] , we develop some theory behind posterior mean ranking , and the loss from using the wrong estimating prior . in section  [ sectionisotaxes ] ,",
    "we give a visual representation of the effect of choice of estimating prior on posterior mean ranking . in section  [ sectionnonparametric ] , we show that using the non - parametric mle as an estimating prior for posterior mean ranking produces a robust ranking . in section  [ simulations ] , we apply our theory to some examples of misspecified estimating priors , and perform a simulation study to confirm the results are as expected . we show that an exponential estimating prior is a good general - purpose choice for posterior mean ranking . in section  [ sectionrealdata ] , we apply this to some real data examples where we show the difference in the ranking between using a normal distribution for the estimating prior and using an exponential distribution . in section  [ sectionconclusions ] , we make some concluding remarks and suggestions for further investigations .",
    "we suppose that our true prior distribution is continuous and has density function @xmath6 .",
    "suppose that we have a point estimate @xmath7 , whose error distribution is normal with variance @xmath8 , where @xmath9 is small .",
    "since @xmath9 is small , values of @xmath10 that are far from @xmath7 are extremely implausible , and contribute little to the posterior mean for most choices of @xmath6 .",
    "we therefore focus on the form of @xmath6 for values of @xmath10 close to @xmath7 . taking a first order taylor expansion about @xmath7",
    "gives @xmath11 using this approximation to @xmath6 gives that the posterior mean is @xmath12    this means that the key part of choice of estimating prior is to estimate the quantity @xmath13 .",
    "for the tail of the distribution , this quantity is positive , and asymptotically approaches the hazard rate . for an exponential distribution , it is constant . for heavier - tailed distributions",
    "it tends to zero as @xmath14 . for light - tailed distributions",
    ", it tends to infinity as @xmath14 .",
    "suppose we should estimate the posterior mean as @xmath15 , but in fact , we estimate it as @xmath16 , for some particular value of @xmath7 . the question is what is the average loss function resulting from this .",
    "for a ranking of all the observations , we can consider the total loss as the sum of losses due to individual mis - rankings .",
    "that is , suppose we rank the observations @xmath17},\\theta_{[2]},\\ldots,\\theta_{[n]}$ ] , when the correct ranking is @xmath18 .",
    "if we choose our selection cutoff as the first @xmath19 units , then the loss function is @xmath20})\\\\ & = \\left(\\sum_{\\substack{i\\leqslant k\\\\\\theta_{(i)}\\not\\in\\{\\theta{[1]},\\ldots,\\theta{[k]}\\ } } } \\theta_{(i)}\\right)-\\left(\\sum_{\\substack{j\\leqslant   k\\\\\\theta_{[j]}\\not\\in\\{\\theta{(1)},\\ldots,\\theta{(k)}\\ } } } \\theta_{[j]}\\right)\\\\\\end{aligned}\\ ] ]    we can move from the correct ranking to the estimated ranking by a series of transpositions of adjacent units in the current ranking . for example , if the correct ranking is @xmath21 and the estimated ranking is @xmath22 , we can change from the correct ranking to the estimated ranking via the following sequence : @xmath23    for each such transposition , exchanging the position of @xmath24 in the @xmath25th postion , with @xmath26 in the @xmath27th position , the change in loss function is @xmath28 the total loss from this mis - ranking is then given by the sum of the loss functions for each transposition .",
    "we see that the loss for each transposition is non - negative for each value of @xmath19 , so we can analyse the overall loss of a misranking by looking at the loss of each pairwise misranking .",
    "if we consider the overall loss as the total of the loss functions for all values of @xmath19 , we see that this loss function is just the sum of the loss functions for each transposition . furthermore",
    ", whatever sequence of transpositions is performed , there will be one transposition for each misranked pair .",
    "therefore the total loss function is the sum of the losses from each misranked pair .",
    "we can therefore study the total loss function by studying the misranking loss for any pair of observations . in practice",
    ", we will often consider only the loss of the upper tail of the distribution .",
    "that is , we will choose some cutoff @xmath29 and evaluate the sum of the loss function for all @xmath19 such that @xmath30 . for this",
    "we have the following proposition ( proof in appendix  [ appproofoflossfunction ] )    suppose the true prior distribution of the parameter @xmath10 has density function @xmath6 , and that we have two observations @xmath31 and @xmath32 which are normally distributed with means @xmath33 and @xmath34 and standard deviations @xmath35 and @xmath36 respectively , where @xmath33 and @xmath34 are random samples from the true prior distribution , and @xmath35 and @xmath36 are assumed to be small .    1 .   the expected loss when the estimating prior and the true prior are the same ( which we will refer to as the _ optimal expected loss _ ) is approximately given by @xmath37 2 .",
    "when the estimating prior has density @xmath38 , the difference between the expected loss and the optimal expected loss is approximately given by @xmath39 where @xmath40 .",
    "the difference between the expected loss from using the point estimate @xmath3 and the optimal expected loss is approximately given by + @xmath41    we see that for @xmath42 , @xmath43 is bounded by @xmath44 , which is the expected information of @xmath10 , and is bounded for most distributions .",
    "this means that if the estimating prior is too heavy - tailed , we can do no worse than ranking by point estimators alone . on the other hand ,",
    "if we have @xmath45 , then the integral can approach @xmath46 , which can be unbounded if the true prior has a heavy tail , but the estimating prior has a light tail . in most cases",
    ", the expression will not be unbounded .",
    "for example , for a normal estimating prior and a pareto true prior , we have that @xmath47 and @xmath48 , so @xmath49 which diverges whenever @xmath50 .",
    "thus for very heavy - tailed true priors , the loss from using a light - tailed estimating prior can diverge .",
    "we see that there is a risk of this unbounded loss whenever @xmath51 diverges .",
    "this can happen for any estimating prior with a lighter tail than an exponential distribution .",
    "we therefore suggest using an exponential distribution for the estimating prior to ensure the loss is not too great .",
    "this has the added mathematical convenience that the posterior mean is easily calculated as @xmath52 for some constant @xmath53 .",
    "if we use an improper exponential prior with density proportional to @xmath54 for all @xmath10 ( not just @xmath55 ) then this formula for the posterior mean is exact .",
    "indeed the posterior distribution is given by @xmath56 which is the density of a normal distribution with mean @xmath15 and variance @xmath8 .",
    "in this proposition , part  ( ii ) gives the measure of the cost of using the wrong estimating prior .",
    "( i ) and ( iii ) give measures of the overall difficulty of the ranking problem .",
    "( i ) is the irreducible cost of misranking .",
    "( iii ) is the additional cost from using the point estimates to rank , instead of using the posterior mean .",
    "it is an indication of the extent to which the ranking can be improved by using bayesian methods .",
    "henderson and newton ( 2015 ) describe different ranking methods in terms of the shapes of what they refer to as `` threshold functions '' , namely the functions @xmath57 which are the smallest value of @xmath7 , such that the observation @xmath58 is ranked in the top @xmath59 proportion under the ranking method in question .",
    "these threshold functions are curves joining points of equal rank : we will therefore refer these curves as _ isotaxes _ ( singular : _ isotaxis _ , from greek _ iso _ meaning equal , and _ taxis _ meaning rank ) .",
    "henderson and newton ( 2015 ) then describe their @xmath1-values procedure directly by calculating the shape of these isotaxes .",
    "we will examine the shape of the isotaxes as a method to better determine the effect of the estimating prior on ranking .    for bayesian methods ,",
    "the shape of these isotaxes depends heavily on the choice of estimating prior . for the normal estimating prior with mean 0 and variance @xmath60 , for an observation @xmath7 with standard error @xmath9 , the posterior mean is @xmath61 ,",
    "so isotaxes are given by solutions to @xmath62 for constant @xmath63 , or to @xmath64 . when plotted on a graph of @xmath8 against @xmath7 ,",
    "these are lines of varying slope , with shallower slope at higher ranks .",
    "( indeed , these lines all pass through the point @xmath65 . )    for an exponential estimating prior with hazard rate @xmath53 , as mentioned above , the posterior mean is given by @xmath15 .",
    "the isotaxes are therefore given by the equation @xmath66 , or @xmath67 , so they are lines of constant slope .",
    "for a heavy - tailed distribution , recall that we have posterior mean approximately @xmath68",
    ". therefore the isotaxes are functions of the form @xmath69 .",
    "a typical example is @xmath70 , so that @xmath71 .",
    "this means the isotaxes are curves of the form @xmath72    which gives a parabola .",
    "we plot the shapes of the isotaxes for these estimating prior distributions in figure  [ fig_isotaxis ] .",
    "0.33        0.33        0.33        we see that for the exponential and heavy - tailed estimating priors , the slopes of isotaxes are bounded away from zero , so the posterior mean can not be very far from the point estimate for @xmath7 .",
    "since by assumption , the true value also will not be so far from the point estimate , this means that the posterior mean can not be too far from the true value .    from the shapes in figure  [ fig_isotaxis ]",
    ", we see that for the normal estimating prior , the standard error becomes increasingly important as we move towards the tail of the distribution , and that the posterior mean can be arbitrarily far away from the true value . for",
    "the exponential estimating prior , the standard error remains equally important throughout .",
    "for the heavy - tailed estimating prior , the standard error becomes less important as we move to the tail of the distribution .",
    "furthermore , the standard error is most important for small standard error , and differences in standard error become less important as the standard error increases .",
    "it is also possible to calculate a non - parametric maximum likelihood estimate for the prior distribution .",
    "it was shown by laird ( 1978 ) that the prior in this case is a discrete distribution with finite support .",
    "an implementation of this non - parametric prior estimation is given in the ` rvalues ` package in ` r ` .",
    "however , this implementation is buggy , so we were unable to compare this method in section  [ simulations ] .",
    "we show that for such a choice of estimating prior , provided the support of the prior distribution includes points sufficiently close to all the observed data , then the posterior mean estimators are robust .",
    "proofs of the following lemmas are in appendix  [ nppriorproof ] .",
    "let @xmath73 be a discrete distribution with probability at least @xmath74 in the interval @xmath75 $ ] for some @xmath76 .",
    "let @xmath77 be the posterior mean for an observation @xmath7 with standard error @xmath9 .",
    "then @xmath78    this means that provided the prior distribution assigns some probability to a region near to each observed value of @xmath7 , then the posterior mean estimate will have some robustness to model misspecification .    for a sample of @xmath79 datapoints and their corresponding standard errors ,",
    "the non - parametric mle estimate for the prior distribution always assigns probability at least @xmath80 to the interval @xmath81 , for every observed data point @xmath82 .    from the preceding lemmas ,",
    "we conclude that ranking based on posterior mean under the non - parametric mle estimate for the prior is relatively robust , with @xmath83 we also know that for large @xmath79 , the non - parametric mle estimate is consistent , so the ranking will be optimal with the non - parametric mle .",
    "overall , we conclude that non - parametric estimation of prior provides a reasonable compromise between efficiency and robustness .    however , as is typically the case with non - parametric methods , there is a trade - off between bias and variance . for the non - parametric method ,",
    "the estimated ranking is asymptotically unbiassed , but can have fairly large variance for smaller sample sizes . figure  [ npsimulationexample ] gives an illustration of this .",
    "0.33        0.33        0.33        isotaxes for the upper tail of simulated data .",
    "500 data points were simulated with the true means following a normal distribution with mean @xmath84 and variance 1 .",
    "variances for the observed data points are simulated following a gamma distribution with shape parameter 2 and scale parameter 0.1 .",
    "plot ( a ) shows the isotaxes for the non - parametric mle estimate for the prior distribution . plot ( b ) shows the isotaxes for an exponential estimating prior .",
    "plot ( c ) shows the isotaxes for the true prior .",
    "points are numbered according to their rank by posterior means under the true prior .",
    "note that some points are outside the region shown , hence the missing numbers .",
    "the isotaxes shown are the ones passing through observed data points .",
    "we can see that while the non - parametric approach has the isotaxes in approximately the right direction for larger variances , they are somewhat distorted for smaller variances .",
    "this is particularly observable at the tail , because the support of the mle ( which is discrete by laird  ( 1978 ) ) is fairly sparse around the tail .",
    "this has a big effect on the posterior mean estimates for points with small standard error .",
    "however , it is worth noting that this distortion usually has limited influence on the estimated ranking .",
    "the reason for this is that the distortion is only for small standard error , compared with the standard error of the data points , so if some of the data points have small standard error , the isotaxes for posterior mean ranking based on the mle prior will be close to the correct isotaxes except for very small standard error . meanwhile , if the standard errors are large , the mle isotaxes will become further from the correct isotaxes , but not many of the observed data points will be included in this region where the isotaxes are far from optimal .",
    "the example given in figure  [ npsimulationexample ] is a typical example where the non - parametric mle prior gives a poor ranking .",
    "there are other typical examples where the mle prior does not give such a poor ranking .",
    "let @xmath73 be a discrete distribution with probability at least @xmath74 in the interval @xmath75 $ ] for some @xmath76 .",
    "let @xmath77 be the posterior mean for an observation @xmath7 with standard error @xmath9 .",
    "then @xmath78    let the support of @xmath73 be the values @xmath156 , with probabilities @xmath157 .",
    "then for the posterior distribution of @xmath10 , the probability of @xmath156 is @xmath158 the posterior mean is therefore @xmath159    we see that the difference @xmath160 is maximised when the @xmath161 all have the same sign , which we will w.l.o.g . assume to be positive .",
    "it is clear that @xmath77 is maximised by setting all the @xmath161 in the interval @xmath162 $ ] to equal @xmath29 , since this both minimises the posterior probability of the interval @xmath163 $ ] and maximises the posterior mean conditional on lying in this interval .",
    "we will therefore assume that @xmath164 , and @xmath165 , then we have @xmath166    for fixed",
    "@xmath161 , and fixed @xmath167 , if @xmath168 then @xmath169 @xmath170 this gives that the contours are linear functions in @xmath73 , so the maximum value of @xmath63 occurs at a vertex with only one non - zero value of @xmath157 .",
    "the value of @xmath171 is maximised subject to @xmath164 , @xmath165 by setting @xmath172 , @xmath173 and choosing the value of @xmath174 to maximise the resulting quantity . in this case",
    ", we have @xmath175 substituting @xmath176 , @xmath177 , this expression becomes @xmath178 the derivative of @xmath179 with respect to @xmath180 is @xmath181 .",
    "we see that for @xmath182 , @xmath183 and @xmath184 , we have @xmath185 so @xmath179 is increasing . therefore @xmath186 is decreasing .",
    "this means that assuming @xmath187 , we have that @xmath188 for all @xmath182 .",
    "meanwhile , we always have @xmath189 , so we always have @xmath190 , and therefore @xmath191    for a sample of @xmath79 datapoints and their corresponding standard errors , the mle estimate for the prior distribution always assigns probability at least @xmath80 to the interval @xmath81 , for every observed data point @xmath82 .",
    "suppose the mle assigns probability @xmath157 to point @xmath161 .",
    "we will separate the points @xmath161 into points that are in the interval @xmath192 , and points that are not .",
    "suppose the first @xmath25 points are in the interval @xmath193 and the remaining points are not .",
    "we are aiming to show that @xmath194 .",
    "suppose this is not the case .",
    "we will then show that the distribution assigning probability @xmath157 to each point @xmath161 is not the mle by constructing a prior distribution with larger likelihood .",
    "let @xmath195 .",
    "let @xmath196 be the data set , and let @xmath197 be a data point .",
    "the log - likelihood of the data can be represented as @xmath198 , i.e. as the likelihood of the point @xmath82 plus the likelihood of the remainder of the data points . for a data point",
    "@xmath199 , we will use @xmath200 to represent the conditional likelihood of @xmath201 given that its corresponding value of @xmath10 is contained in @xmath193 , and @xmath202 for the conditional likelihood of @xmath201 given that its corresponding value of @xmath10 is not contained in @xmath193 .",
    "we have that the likelihood of @xmath201 is @xmath203 .",
    "if we change the prior to have probability @xmath59 at @xmath7 and @xmath204 times the previous prior , then the log - likelihood is larger than @xmath205 the increase in log - likelihood is therefore @xmath206 for this to be an increase , we need @xmath207 if we substitute @xmath208 , where @xmath209 , then @xmath210 for large @xmath79 .",
    "we therefore need @xmath211 now we know that @xmath212 , and @xmath213 , since the prior probability of the interval @xmath193 is at most @xmath135 , and for each point @xmath161 , the likelihood is @xmath214 so @xmath215 .",
    "also @xmath216 so @xmath217 .",
    "therefore , provided that @xmath218 we will have an improvement in likelihood .",
    "we see that as @xmath219 , @xmath220 . for small @xmath221 ,",
    "the left - hand side is approximately @xmath222 . for the right - hand side",
    ", we are given that @xmath223 , so the right - hand side is less than @xmath224 so the required inequality holds .",
    "we use three simulation distributions for the priors ( both the true priors and the estimating priors ) : a normal distribution with known mean 0 and variance @xmath60 ; an exponential distribution with hazard rate @xmath53 ; and a pareto distribution with density function @xmath85 for @xmath86 where we take @xmath87 as known .",
    "( we have taken one parameter as known for the normal and pareto distributions , so that each prior has one hyperparameter to be estimated . ) for the true priors in the simulation , we set @xmath88 for the normal distribution , @xmath89 for the exponential distribution and @xmath90 for the pareto distribution .",
    "for each simulation distribution , we simulate datasets of size 1000 , 10000 , and 100000 .",
    "we simulate the standard error @xmath9 for each data set as following an exponential distribution .",
    "we present results for the mean of this exponential distribution equal to 0.02 .",
    "results for mean 0.01 , 0.05 and 0.1 are presented in the supplementary materials .",
    "the values of @xmath9 are independent of the values of @xmath10 and values of @xmath9 for different data points are independent . to avoid some computational issues caused by values of @xmath9 too close to 0",
    ", we added 0.0001 to all values of @xmath9 .",
    "we do not expect this to significantly impact the results , but we found that some numerical integration routines produced errors when the value of @xmath9 was very close to zero .    for each simulated dataset , we analyse with each of the normal , exponential and pareto distributions as the estimating prior .",
    "we will assess the performance of the ranking by the average increase in the loss function from using the given estimating prior compared to using the true prior .",
    "that is , the loss function is : @xmath91})\\ ] ] where @xmath24 is the true value of @xmath10 for the @xmath92th ranked unit under the true prior , and @xmath93}$ ] is the true value of @xmath10 for the @xmath92th ranked unit under the estimating prior .      in order to better understand issues related to parameter estimation , we examine the loss function for both optimal parameter estimates ( based on minimising the expected loss function ) and estimated parameters ( estimated from the upper tail of the data ) .",
    "we do not compare the effect of estimating the hyperparameters from the whole data set because two of the distributions used for analysis had support only on the positive real numbers , so estimating these based on the whole data including negative values might lead to strange results .",
    "even if the supports were all the same , estimating the parameters for the estimating prior based on the whole data set when the focus is on the ranking of the top units leads to suboptimal results in ranking .",
    ".expected loss functions [ cols=\"<,<,<\",options=\"header \" , ]",
    "we look at several real data sets .",
    "these datasets were studied by henderson & newton ( 2015 ) for their work on @xmath1-values . the first data set consists of gwas data for log odds ratio between snps and type-2 diabetes from morris _ et al . _  ( 2006 ) .",
    "the data are available from http://diagram-consortium.org/downloads.html .",
    "the data consists of 137,899 snps from 12,171 type 2 diabetes cases and 56,862 controls . for each snp ,",
    "an odds ratio is available along with a 95% confidence interval . following henderson & newton ( 2015 ) ,",
    "we have taken the value as the log - odds ratio , assuming this estimate follows a normal distribution , and that the standard deviation of this distribution is one - quarter of the width of the log of the 95% confidence interval provided .",
    "the resulting positive data points and isotaxes for a normal and exponential estimating prior are shown in figure  [ diabetesisotaxes ] .",
    "0.35        0.35        0.35        from this figure , we see that using a normal estimating prior with naively estimated variance , the estimated variance is small , leading to isotaxes passing close to the origin .",
    "this makes the ranking focus on values with small variance , and rank values with larger observed value and larger variance behind values with smaller variance .",
    "the exponential estimating prior provides a ranking that selects many more of the points with large estimated effect size . we can improve the performance of a normal prior by artificially inflating the variance to match the tail better .",
    "figure  [ diabetesisotaxes_c ] shows the effect of this . this does select a lot more of the points with large effect sizes , and",
    "indeed the top 1% isotaxis is very similar to the 1% isotaxis for the exponential estimating prior . on the other hand ,",
    "the higher isotaxes do put too much weight on having smaller standard error , ranking a number of points with smaller estimated effect size ahead of the point with largest estimated effect size ( circled in red ) .",
    "it is extremely implausible that this ranking is correct .",
    "overall , the ranking based on an exponential estimating prior appears more plausible to us for this dataset .",
    "next we look at the gene expression data relating to breast cancer from west _ et al . _",
    "this dataset is available in the rvalues package .",
    "the data set consists of gene expression measurements of 7129 genes accross 49 breast tumour samples  25 oestrogen receptor ( er)+ samples and 24 er@xmath94 samples . for each gene , the difference in means between the er@xmath95 and er@xmath94 groups is calculated , along with its appropriate standard error . in theory",
    "the error distribution should be modelled as a @xmath96-distribution with 47 degrees of freedom .",
    "however , for the purpose of this paper , we have used a normal distribution .",
    "the loss of accuracy should be fairly small .",
    "the resulting plot of variance against estimated effect size , and isotaxes for posterior mean with an exponential and a normal estimating prior are shown in figure  [ bcisotaxes ] .",
    "0.35        0.35        0.35        as for the diabetes data , we see that the normal estimating prior results in very flat isotaxes , and therefore gives a high ranking to observations with small variance .",
    "meanwhile , the exponential estimating prior puts a lot more weight on points with large estimated effect size .",
    "again , we see that using a normal estimating prior with inflated variance results in isotaxes that are more similar to the exponential estimating prior . in this case , the differences between the two rankings are not so clear - cut as the previous case , where some of the rankings using a normal estimating prior with increased variance were completely implausible . in this case , both the rankings for the exponential estimating prior and the normal estimating prior with inflated variance seem reasonable .",
    "we have seen that choice of estimating prior can have a very large effect on bayesian ranking methods . for the majority of ranking problems ,",
    "we are particularly interested in ranking at the upper tail of the distribution .",
    "the ranking of the upper tail can be particularly affected by choice of estimating prior .",
    "using a light - tailed estimating prior for posterior mean ranking can lead to very bad results .",
    "if the true prior is heavy - tailed , the posterior mean can be far away from the truth .",
    "conversely , if the estimating prior is too heavy - tailed , the posterior mean estimated will be between the posterior mean under the true prior and the point estimate .",
    "this can not be too far from the true posterior mean .",
    "this means that using an exponential or heavier - tailed distribution as the estimating prior should be more robust to model misspecification .",
    "in addition to being less robust to model misspecification , light - tailed estimating priors can be more sensitive to estimated parameter values . in cases",
    "where the estimating prior is misspecified , using mle estimates for hyperparameters can also be far from optimal , so this can lead to bad results even in cases with large datasets .",
    "since we are usually particularly interested in the top units , it is usually advisable to choose parameter values that fit the tail of the distribution well .",
    "using a non - parametric prior is robust , in that there is an upper bound on how far the posterior mean can be from the posterior mean under the true prior .",
    "however , using a non - parametric prior can be inefficient for smaller sample sizes , and can lead to some strange rankings .",
    "we have confirmed our results by simulation studies and real data examples . in the simulation study , we found that an exponential estimating prior performed relatively well regardless of the true prior .",
    "our simulation study also studied the effect of estimating hyperparameters on the performance .",
    "as expected , estimating hyperparameters does cause some loss .",
    "the estimation in this simulation was done by maximum likelihood . however , since the loss function we are aiming to minimise is not the standard squared error loss , this is not the optimal estimation method .",
    "we based our parameter estimation on the upper 10% of the data points .",
    "it is common for analyses which use the whole data to estimate hyperparameters",
    ". doing this could lead to far worse results when the estimating prior is misspecified .    even in the case",
    "where the true prior is not heavy - tailed , uncertainty in the hyperparameters can lead to a heavy - tailed prior when we apply model averaging over different possible values for the hyperparameters .",
    "we saw this effect in our simulation , when we looked at the results with estimated hyperparameters . particularly in the case where the prior distribution is not the true prior ,",
    "estimation of hyperparameters can have a big effect on the estimated ranking .",
    "overall , unless there is good evidence otherwise , we suggest an exponential estimating prior will be a good compromise between robustness and efficiency in most cases",
    ". it also offers easy computation of posterior mean .",
    "the most obvious direction to need improvement in this research is hyperparameter estimation .",
    "we have seen in our simulation study that estimation by mle can lead to bad ranking results .",
    "this is because the loss function from prior misspecification is different from the loss function that mle estimation aims to minimise .",
    "this suggests that a different method of estimating the hyperparameters is needed  a method specifically targeted at optimising ranking estimates .",
    "we know the loss function that we are aiming to minimise , so it should be possible to find an explicit way to solve this and derive a procedure for estimating the hyperparameter .",
    "given our recommendation to use an exponential estimating prior in most cases , finding the best hyperparameters should not prove too challenging a problem .",
    "our study has a number of limitations .",
    "we have considered only cases where ranking is by posterior mean and the error distribution is normal . in future work , we should study the problem for different error distributions , not just normal .",
    "further estimation is also needed into cases where the variance of the error distribution depends on the parameter @xmath10 .",
    "this can allow certain approximations to be applied .",
    "for example a poisson distribution can be approximated by a normal distribution where the variance depends on the mean .",
    "we should also study the problem for different methods and objective functions , e.g. @xmath1-values , posterior expected rank .",
    "we also have not considered the effect of model selection on ranking .",
    "if the estimating prior distribution is chosen based on certain model selection criteria , this may improve the ranking .",
    "however , model selection for mixture models can be difficult , so it might not provide the improvements we hope for",
    ". model selection also depends upon a good set of candidate models .",
    "our research suggests that the form of the function @xmath13 is most crucial in our choice of estimating prior , so including a sufficient range of models to allow flexibility in this function should allow us to obtain good ranking results , provided the model selection criteria are well chosen to be related to our objective function .",
    "suppose the true prior distribution of the parameter @xmath10 has density function @xmath6 , and that we have two observations @xmath31 and @xmath32 which are normally distributed with means @xmath33 and @xmath34 and standard deviations @xmath35 and @xmath36 respectively , where @xmath33 and @xmath34 are random samples from the true prior distribution , and @xmath35 and @xmath36 are assumed to be small .    1 .   the expected loss when the estimating prior and the true prior are the same ( which we will refer to as the _ optimal expected loss",
    "is approximately given by @xmath37 2 .",
    "when the estimating prior has density @xmath38 , the difference between the expected loss and the optimal expected loss is approximately given by @xmath97 where @xmath40 .",
    "the difference between the expected loss from using the point estimate @xmath3 and the optimal expected loss is approximately given by + @xmath41    \\(i ) suppose that the true parameter values are @xmath33 and @xmath34 respectively .",
    "let @xmath98 .",
    "now the loss from mis - ranking is @xmath99 if the points are mis - ranked and 0 if they are not misranked .",
    "the points are misranked if either @xmath100 and @xmath101 or if @xmath102 and @xmath103 .",
    "since the points will not plausibly be misranked if @xmath104 is large ( since @xmath31 and @xmath32 will then with high probability be far apart ) , we will assume that @xmath104 is small , so that we have @xmath105 .",
    "we will denote this common value @xmath53 .",
    "now for fixed @xmath33 and @xmath34 , suppose @xmath100 ; we want to calculate @xmath106 . we know that @xmath107 is normally distributed with mean @xmath104 and variance @xmath108 .",
    "therefore , @xmath109 on the other hand , if @xmath102 , we have @xmath110 now suppose that we fix @xmath33 , and we want to take the expected loss over the distribution of @xmath34 .",
    "this is given by @xmath111 since the probability of misranking is negligible for large @xmath104 , we will consider only small values of @xmath104 . for these small values , we can take the taylor expansion @xmath112 substituting this into the above loss function gives @xmath113    we recall that    @xmath114_{-c}^\\infty+\\int_{-c}^\\infty    \\frac{2\\sigma}{\\sqrt{2\\pi}}\\xi e^{-\\frac{\\xi^2}{2\\sigma^2}}\\,d\\xi\\\\ & = \\frac{1}{\\sqrt{2\\pi}}(\\sigma c^2 + 2\\sigma^3)e^{-\\frac{c^2}{2\\sigma^2}}\\\\ \\int_{-c}^\\infty \\frac{\\xi^2 e^{-\\frac{\\xi^2}{2\\sigma^2}}}{\\sqrt{2\\pi}\\sigma}\\,d\\xi&=\\left[-\\frac{\\sigma}{\\sqrt{2\\pi}}\\xi    e^{-\\frac{\\xi^2}{2\\sigma^2}}\\right]_{-c}^\\infty+\\int_{-c}^\\infty    \\frac{\\sigma}{\\sqrt{2\\pi } } e^{-\\frac{\\xi^2}{2\\sigma^2}}\\,d\\xi\\\\ & = \\sigma^2\\phi\\left(\\frac{c}{\\sigma}\\right)-\\frac{\\sigma c      e^{-\\frac{c^2}{2\\sigma^2}}}{\\sqrt{2\\pi}}\\\\",
    "\\int_{-c}^\\infty \\frac{\\xi e^{-\\frac{\\xi^2}{2\\sigma^2}}}{\\sqrt{2\\pi}\\sigma}\\,d\\xi&=\\frac{\\sigma e^{-\\frac{c^2}{2\\sigma^2}}}{\\sqrt{2\\pi}}\\\\ \\int_{-c}^\\infty \\frac{e^{-\\frac{\\xi^2}{2\\sigma^2}}}{\\sqrt{2\\pi}\\sigma}\\,d\\xi & = \\phi\\left(\\frac{c}{\\sigma}\\right)\\\\\\end{aligned}\\ ] ]    hence we calculate    @xmath115_0^\\infty+\\int_0^\\infty    \\frac{\\delta^3}{3}\\frac{e^{-\\frac{(c-\\delta)^2}{2\\sigma^2}}}{\\sqrt{2\\pi}\\sigma}\\,d\\delta\\\\ & = \\int_{-c}^\\infty    \\frac{(\\xi+c)^3}{3}\\frac{e^{-\\frac{\\xi^2}{2\\sigma^2}}}{\\sqrt{2\\pi}\\sigma}\\,d\\xi\\\\ & = \\frac{1}{3}\\left(\\frac{1}{\\sqrt{2\\pi}}(\\sigma c^2 + 2\\sigma^3)e^{-\\frac{c^2}{2\\sigma^2}}+3c\\left(\\sigma^2\\phi\\left(\\frac{c}{\\sigma}\\right)-\\frac{\\sigma c      e^{-\\frac{c^2}{2\\sigma^2}}}{\\sqrt{2\\pi}}\\right)+3c^2\\frac{\\sigma      e^{-\\frac{c^2}{2\\sigma^2}}}{\\sqrt{2\\pi}}+c^3\\phi\\left(\\frac{c}{\\sigma}\\right)\\right)\\\\ & = \\frac{1}{3}\\left((\\sigma c^2 + 2\\sigma^3 - 3c^2\\sigma+3c^2\\sigma)\\frac{e^{-\\frac{c^2}{2\\sigma^2}}}{\\sqrt{2\\pi}}+(3c\\sigma^2+c^3)\\phi\\left(\\frac{c}{\\sigma}\\right)\\right)\\\\ & = \\frac{1}{3}\\left((2\\sigma^3+\\sigma    c^2)\\frac{e^{-\\frac{c^2}{2\\sigma^2}}}{\\sqrt{2\\pi}}+(3c\\sigma^2+c^3)\\phi\\left(\\frac{c}{\\sigma}\\right)\\right)\\\\ \\int_{0}^\\infty \\delta\\phi\\left(\\frac{c-\\delta}{\\sigma}\\right)\\,d\\delta&=\\left[\\frac{\\delta^2}{2}\\phi\\left(\\frac{c-\\delta}{\\sigma}\\right)\\right]_0^\\infty+\\int_0^\\infty    \\frac{\\delta^2}{2}\\frac{e^{-\\frac{(c-\\delta)^2}{2\\sigma^2}}}{\\sqrt{2\\pi}\\sigma}\\,d\\delta\\\\ & = \\int_{-c}^\\infty    \\frac{(\\xi+c)^2}{2}\\frac{e^{-\\frac{\\xi^2}{2\\sigma^2}}}{\\sqrt{2\\pi}\\sigma}\\,d\\xi\\\\ & = \\frac{1}{2}\\left(\\left(\\sigma^2\\phi\\left(\\frac{c}{\\sigma}\\right)-\\frac{\\sigma c      e^{-\\frac{c^2}{2\\sigma^2}}}{\\sqrt{2\\pi}}\\right)+2c\\frac{\\sigma      e^{-\\frac{c^2}{2\\sigma^2}}}{\\sqrt{2\\pi}}+c^2\\phi\\left(\\frac{c}{\\sigma}\\right)\\right)\\\\ & = \\frac{1}{2}\\left((\\sigma^2+c^2)\\phi\\left(\\frac{c}{\\sigma}\\right)+\\frac{\\sigma c      e^{-\\frac{c^2}{2\\sigma^2}}}{\\sqrt{2\\pi}}\\right)\\\\\\end{aligned}\\ ] ]    in the loss function , we let @xmath116 and @xmath117 .",
    "substituting these into the loss function gives :    @xmath118    if we let @xmath119 , so that @xmath120 , then we have    @xmath121    ( for anyone thinking at this point that the dimensions do not work in this formula , it is worthwhile to remember that @xmath53 and @xmath73 are inversely proportional to changes in the scale of @xmath10 .",
    "that is , if we change the units so that the value of @xmath10 doubles , the values of @xmath53 and @xmath73 will be halved . )    with @xmath122 assumed to be small , we can neglect the @xmath123 term to get @xmath124 if we assume that @xmath125 is negligible , then our expression becomes @xmath126 we take the expectation of this over the distribution of @xmath33 to get that the expected loss is approximately @xmath127 we have assumed that @xmath128 is small with respect to this second term , so the expected loss is approximately @xmath129 .",
    "\\(ii ) since @xmath35 and @xmath36 are small , we can assume that @xmath130 .",
    "we will let @xmath53 denote this common value .",
    "suppose that @xmath31 is the observed value of @xmath131 and @xmath32 is the observed value of @xmath132 , and that @xmath133 , but @xmath134 so that @xmath31 and @xmath32 are mis - ranked compared to the ranking under the true prior .",
    "let @xmath10 be the underlying parameter value for @xmath31 , and let @xmath135 be the underlying parameter value for @xmath32 .",
    "the expected increase in the loss function due to this misranking , compared to using the true prior , is then @xmath136    ( where @xmath137 and @xmath138 are the posterior distributions of @xmath10 and @xmath135 given observations @xmath31 and @xmath32 respectively , under the true prior ) .",
    "similarly , if @xmath139 , then the expected increase in loss is @xmath140 .",
    "now suppose we fix @xmath141 and take the expectation of the loss over @xmath132 .",
    "the expected loss due to mis - ranking them is @xmath142 where @xmath143 is the marginal density of @xmath32 . in the case",
    "where @xmath144 , we get this by calculating expected misranking loss over all values for which @xmath31 and @xmath32 are misranked ( compared to posterior mean ranking using the true prior ) . in the case where @xmath145 , calculating the expected misranking loss over all values where @xmath31 and @xmath32 are misranked gives @xmath146 and by reversing the limits and negating the integrand",
    ", we get the formula from equation  [ eqexplossfixedx1 ] in this case also .",
    "since @xmath147 and @xmath148 are both small , we can assume that @xmath73 , @xmath149 and @xmath143 are approximately constant arround @xmath31 , so that @xmath150 for all @xmath32 in the relevant range .",
    "the integral is then approximately @xmath151    for the overall mis - ranking loss , we take the expectation of this over @xmath31 .",
    "we are usually particularly interested in the mis - ranking loss of the upper tail , that is the expected loss due to all misrankings in the upper tail , so we usually take the expectation over the distribution of @xmath31 for values @xmath152 for some chosen @xmath29 .",
    "this is given by    @xmath153    \\(iii ) we calculate this loss by substituting @xmath154 into our expression for the additional loss , we get @xmath155",
    "recall that for a distribution with density function @xmath6 , we define @xmath225 , and that the best choice of estimating prior to use for ranking is chosen to minimise @xmath226 where @xmath227 and @xmath6 are for the true prior , while @xmath228 is for the estimating prior .",
    "we evaluate this loss function for each combination of priors .",
    "if the true prior is normal with mean 0 variance @xmath60 , and the estimated prior has mean 0 , variance @xmath229 , then the loss function is given by @xmath230_a^\\infty}{4\\pi}+\\int_a^\\infty \\frac{e^{-\\frac{\\theta^2}{\\tau^2}}}{4\\pi}\\,d\\theta\\right)\\\\ & = \\left(\\frac{1}{\\tau^2}-\\frac{1}{\\hat{\\tau}^2}\\right)^2\\left(\\frac{a e^{-\\frac{a^2}{\\tau^2 } } } { 4\\pi}+\\frac{\\tau}{4\\sqrt{\\pi}}\\left(1-\\phi\\left(\\frac{\\sqrt{2}a}{\\tau}\\right)\\right)\\right)\\\\\\end{aligned}\\ ] ]              for the normal estimating prior , we have @xmath238 . for the pareto",
    "true prior , we have @xmath239 .",
    "the objective function is therefore @xmath240_a^\\infty\\\\ & = \\frac{\\alpha^2\\eta^{2\\alpha}}{a^{2\\alpha+3}}\\left(\\frac{(\\alpha+1)^2}{(2\\alpha+3)}-\\frac{2(\\alpha+1)a^2}{(2\\alpha+1)\\hat{\\tau}^2}+\\frac{a^4}{(2\\alpha-1)\\hat{\\tau}^4}\\right)\\\\\\end{aligned}\\ ] ]        we have @xmath242_a^\\infty+\\int_a^\\infty e^{-\\frac{\\theta^2}{\\tau^2}}\\,d\\theta\\\\ & = ae^{-\\frac{a^2}{\\tau^2}}+\\sqrt{\\pi}\\tau\\left(1-\\phi\\left(\\frac{\\sqrt{2}a}{\\tau}\\right)\\right)\\\\ \\int_a^\\infty \\frac{e^{-\\frac{\\theta^2}{\\tau^2}}}{2\\pi\\tau^2}\\,d\\theta&=\\frac{1-\\phi\\left(\\frac{\\sqrt{2}a}{\\tau}\\right)}{2\\sqrt{\\pi}\\tau}\\\\\\end{aligned}\\ ] ] so the expected loss is @xmath243      if the true prior is exponential with rate @xmath53 , and the estimated prior is exponential with rate @xmath244 , then the loss function is given by @xmath245_a^\\infty\\\\ & = \\frac{\\lambda\\left(\\lambda-\\hat{\\lambda}\\right)^2}{2}e^{-2\\lambda a}\\\\\\end{aligned}\\ ] ]          for the normal estimated by pareto , we have @xmath247 .",
    "the loss function is therefore @xmath248_a^\\infty+\\int_a^\\infty\\frac{1}{2\\tau^2 } e^{-\\frac{\\theta^2}{\\tau^2}}\\,d\\theta\\right)\\\\ & = \\frac{1}{2\\pi\\tau^2}\\left((\\hat{\\alpha}+1)^2\\int_a^\\infty \\frac{1}{\\theta^2}e^{-\\frac{\\theta^2}{\\tau^2}}\\,d\\theta-\\left(2\\hat{\\alpha}+\\frac{3}{2}\\right)\\frac{\\sqrt{\\pi}}{\\tau}\\left(1-\\phi\\left(\\frac{\\sqrt{2}a}{\\tau}\\right)\\right ) + \\frac{a}{2\\tau^2 } e^{-\\frac{a^2}{\\tau^2}}\\right)\\\\\\end{aligned}\\ ] ]          if the true prior is pareto with minimum @xmath250 and index @xmath59 , and the estimated prior is pareto with minimum @xmath250 and index @xmath251 , then the loss function is given by @xmath252_a^\\infty\\\\ & = \\left(\\alpha-\\hat{\\alpha}\\right)^2\\frac{\\alpha^2\\eta^{2\\alpha}}{(2\\alpha+3)a^{2\\alpha+3}}\\\\\\end{aligned}\\ ] ]      for a normal true prior , we have that the loss from using the mle ranking is @xmath253_a^\\infty+\\int_a^\\infty e^{-\\frac{\\theta^2}{\\tau^2}}\\,d\\theta\\right)=\\frac{1}{4\\pi\\tau^4}\\left(a    e^{-\\frac{a^2}{\\tau^2}}+\\sqrt{\\pi}\\tau\\left(1-\\phi\\left(\\frac{\\sqrt{2}a}{\\tau}\\right)\\right)\\right)\\\\\\end{aligned}\\ ] ]              for the pareto true prior , the expected additional loss from using the mle ranking is @xmath257_a^\\infty=\\frac{\\alpha^2(\\alpha+1)^2\\eta^{2\\alpha}}{(2\\alpha+3)a^{2\\alpha+3}}\\\\\\end{aligned}\\ ] ] substituting @xmath90 , @xmath87 and @xmath258 , we get the loss is @xmath259                        so the best choice of @xmath244 for the exponential estimating prior is @xmath270 for the simulation setting @xmath88 , @xmath254 , this is @xmath271 and the expected loss for our simulation is 0.000622064 .",
    "integrating by parts gives @xmath282_a^\\infty-\\int_a^\\infty \\frac{1}{2\\lambda\\theta^2}e^{-2\\lambda\\theta}\\,d\\theta = \\frac{e^{-2\\lambda a}}{2\\lambda a}-\\frac{1}{2\\lambda}\\int_a^\\infty \\frac{1}{\\theta^2}e^{-2\\lambda\\theta}\\,d\\theta\\\\\\end{aligned}\\ ] ]              we will assume that @xmath29 is given for each simulation , and that our objective is to estimate the parameters from the data for each estimating prior so that the distribution fits the data well on the tail .",
    "we will use maximum likelihood for this purpose .",
    "we have already seen that the loss function is different from the kullback - leibler divergence that the mle estimate attempts to optimise , so the mle is not optimal in terms of minimising our expected loss function , and further work could go into devising better estimation methods for the misspecified prior case . for the mle estimation ,",
    "the details in each case are presented here :      we have @xmath79 samples which we model as having mean @xmath2 following a normal distribution with mean 0 and variance @xmath60 , and each observation @xmath3 following a normal distribution with mean @xmath2 and variance @xmath4 .",
    "we want to maximise the log - likelihood of all the data points with @xmath287 for some cuttoff @xmath29 . to simplify this procedure , we will maximise the log - likelihood of all data points for which @xmath288 .",
    "the log - likelihood is then written @xmath289 ( the last term is because we must take the conditional log - likelihood conditional on @xmath288 . ) setting the derivative with respect to @xmath265 to zero , we get @xmath290    we can solve this numerically using newton s method .",
    "we can use the following method to obtain a good starting value .",
    "since @xmath29 is reasonably large compared to @xmath265 , we can approximate @xmath291 [ note : this is a poor approximation . using it gives fairly bad approximations for @xmath234 . the approximations for other estimating priors later are better .",
    "] so that the final term in the derivative of the log - likelihood is approximately @xmath292 we have assumed that @xmath5 is small compared to @xmath265 , so we can set        we solve for when this is equal to zero using the quadratic formula to get : @xmath295 which should give an approximation to the true value of @xmath265 . if we further make the approximation that @xmath296 is small , then we have @xmath297 which gives us @xmath298    we can compare this approximate mle estimate of @xmath60 to the theoretically best estimate for the exponential and pareto cases .",
    "if we assume that @xmath5 are all small , then the term @xmath299 is approximately @xmath300 , which is a weighted mean of the @xmath4 .",
    "therefore the expected value is the expected value of @xmath4 , so we have @xmath301 since @xmath3 is normally distributed with mean @xmath2 and variance @xmath4 , we have that @xmath302 therefore we have @xmath303 for the exponential true prior , we have that conditional on @xmath287 , we have @xmath304 follows an exponential distribution with @xmath89 and @xmath256 .",
    "therefore @xmath305 therefore , for a large sample @xmath306 this is quite far from the optimal estimate of @xmath307 .    for the pareto true prior",
    ", the variance is infinite ( since @xmath308 ) , so the distribution of the mle @xmath229 has infinite mean .",
    "this means we can not apply the law of large numbers to assert that for large sample size @xmath229 will converge in distribution to a constant .",
    "more specifically , @xmath309 follows a pareto distribution with @xmath310 and @xmath311 .",
    "the sum of pareto distributions with small @xmath59 is approximately equal to the maximum value , which has distribution function @xmath312 we also have @xmath313 we are interested in @xmath314 , because this is the value that is important for our posterior mean estimate .",
    "the survival function of @xmath314 is @xmath315 that is , @xmath314 approximately follows an exponential distribution with parameter @xmath316 .",
    "this can be quite different from the optimal @xmath317 .",
    "indeed we get @xmath318          since @xmath288 and @xmath5 is small , we can approximate @xmath321 so the log - likelihood is approximately @xmath322 however , we want the conditional log - likelihood given @xmath288 . since @xmath5 is small , we will set this approximately equal to the likelihood conditional on @xmath287 , which is @xmath323 setting the derivative with respect to @xmath53 to zero gives @xmath324 so the log - likelihood is maximised by @xmath325 ( the other zero is because the approximation @xmath326 does not hold for @xmath327 ) since @xmath5 is small , we can approximate @xmath328 which gives @xmath329    when the true prior is normal , we see that @xmath330 is the mean of a truncated normal distribution , and is given by @xmath331 substituting @xmath88 and @xmath332 , we get that @xmath333 therefore , for a large sample , our estimate @xmath244 will converge to @xmath334 .    for the pareto true prior , we have @xmath335 . despite the fact that the variance is infinite",
    ", the law of large numbers still ensures that the sample mean of the @xmath3 does converge to @xmath336 as sample size tends to infinity .",
    "we can therefore substitute @xmath336 for this sum in the expression to get @xmath337      for the pareto estimating prior , the likelihood of @xmath2 is @xmath338 and the probability of a value exceeding @xmath29 is @xmath339 .",
    "the likelihood of @xmath319 is therefore @xmath340 letting @xmath341 , this integral becomes @xmath342    so the conditional likelihood of @xmath3 given that @xmath287 is approximately @xmath343 the conditional log - likelihood is therefore @xmath344 setting the derivative with respect to @xmath59 to zero gives @xmath345    @xmath346 setting the derivative with respect to @xmath59 to zero gives @xmath347 which has solution @xmath348 assuming that @xmath349 is small .",
    "we have the approximation @xmath350 which gives the mle @xmath351    for the normal distribution , if we let @xmath352 , then @xmath353 is normally distributed with mean @xmath354 and variance @xmath355 .",
    "we have @xmath356 . for @xmath288 , we have @xmath357 , and if @xmath358 is reasonably large , then nearly all values of @xmath353 will be less than @xmath359 .",
    "this means that @xmath360 can be well approximated by the taylor expansion @xmath361 we therefore have @xmath362 for the truncated normal with mean @xmath363 , variance 1 , truncated at @xmath364 , we have @xmath365 so letting @xmath366 and @xmath367 we see that @xmath368 follows a truncated normal distribution with mean @xmath363 , variance @xmath369 and lower limit @xmath364 , @xmath370    for our specific case , the normal true prior has @xmath88 and @xmath371 .",
    "empirically , for these parameters , @xmath372 , so @xmath373 , and @xmath374 .",
    "therefore , the expected value of @xmath251 is @xmath375 since @xmath5 follows an exponential distribution with @xmath376 , so @xmath377 , which means that @xmath378        we have @xmath382 .",
    "this gives @xmath383_0^\\infty-\\int_0^\\infty   \\left(\\log(1+x)+\\frac{x}{1+x}\\right)e^{-\\lambda    x}\\,dx\\\\ & = -\\int_0^\\infty   \\frac{x}{1+x}e^{-\\lambda    x}\\,dx\\\\ & = -\\int_0^\\infty   \\left(1-\\frac{1}{1+x}\\right)e^{-\\lambda    x}\\,dx\\\\ & = \\int_0^\\infty   \\frac{e^{-\\lambda   x}}{1+x}\\,dx-\\frac{1}{\\lambda}\\\\\\end{aligned}\\ ] ] on the other hand , integration by parts gives @xmath384_0^\\infty+\\int_0^\\infty   \\frac{1}{1+x}e^{-\\lambda    x}\\,dx\\\\ & = \\int_0^\\infty   \\frac{1}{1+x}e^{-\\lambda    x}\\,dx\\\\\\end{aligned}\\ ] ] substituting this into the previous equation gives @xmath381      we have @xmath387 .",
    "this gives @xmath388 on the other hand , integration by parts gives @xmath389_0^\\infty-\\int_0^\\infty   \\frac{e^{-\\lambda    x}}{(1+x)^2}\\,dx\\\\ & = 1-\\frac{g(\\lambda)}{\\lambda}\\\\\\end{aligned}\\ ] ] this gives us @xmath390    this means that for an exponential with parameter @xmath89 and cut - off @xmath256 , @xmath391 follows an exponential distribution with rate @xmath29 , so @xmath392 , so its expected value is @xmath393 , where @xmath394 is the solution to @xmath381 similarly , @xmath395 , so @xmath396 .",
    "the expected value of @xmath251 is then @xmath397 numerically , we find @xmath398 and @xmath399 . substituting these values into the equation gives @xmath400                henderson , n. c. and newton , m. a. ( 2015 ) , making the cut : improved ranking and selection for large - scale inference .",
    "journal of the royal statistical society : series b ( statistical methodology ) .",
    "doi : 10.1111/rssb.12131          morris , a. p. , b. f. voight , t. m. teslovich , t. ferreira , a. v. segre , v. steinthorsdottir , r. j. strawbridge , h. khan , h. grallert , a. mahajan , et al .",
    "large - scale association analysis provides insights into the genetic architecture and pathophysiology of type 2 diabetes .",
    "_ nature genetics 44 , _ 981990 .    west , m. , blanchette , c. , dressman , h. , huang , e. , ishida , s. , spang , r. , zuzan , h , olson , j. a. jr .",
    ", marks , j. r. and nevins , j. r. ( 2001 ) , predicting the clinical status of human breast cancer by using gene expression profiles , _ proceedings of the national academy of sciences ( 98 ) _ , 1146211467"
  ],
  "abstract_text": [
    "<S> the ranking problem is to order a collection of units by some unobserved parameter , based on observations from the associated distribution . </S>",
    "<S> this problem arises naturally in a number of contexts , such as business , where we may want to rank potential projects by profitability ; or science , where we may want to rank variables potentially associated with some trait by the strength of the association . </S>",
    "<S> most approaches to this problem are empirical bayesian , where we use the data to estimate the hyperparameters of the prior distribution , then use that distribution to estimate the unobserved parameter values . </S>",
    "<S> there are a number of different approaches to this problem , based on different loss functions for mis - ranking units . </S>",
    "<S> however , little has been done on the choice of prior distribution . </S>",
    "<S> typical approaches involve choosing a conjugate prior for convenience , and estimating the hyperparameters by mle from the whole dataset . in this paper </S>",
    "<S> , we look in more detail at the effect of choice of prior distribution on bayesian ranking . </S>",
    "<S> we focus on the use of posterior mean for ranking , but many of our conclusions should apply to other ranking criteria , and it is not too difficult to adapt our methods to other choices of prior distributions .      empirical bayes ; posterior mean ranking ; choice of prior </S>"
  ]
}