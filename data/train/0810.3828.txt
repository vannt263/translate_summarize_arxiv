{
  "article_text": [
    "methods are generally classified into supervised , unsupervised and _ reinforcement learning _ ( rl ) . supervised learning requires explicit feedback provided by input - output pairs and gives a map from inputs to outputs .",
    "unsupervised learning only processes on the input data .",
    "in contrast , rl uses a scalar value named _ reward _ to evaluate the input - output pairs and learns a mapping from _ states _ to _ actions _ by interaction with the environment through trial - and - error . since 1980s",
    ", rl has become an important approach to machine learning @xcite-@xcite , and is widely used in artificial intelligence , especially in robotics @xcite-@xcite , @xcite , due to its good performance of on - line adaptation and powerful learning ability to complex nonlinear systems .",
    "however there are still some difficult problems in practical applications .",
    "one problem is the exploration strategy , which contributes a lot to better balancing of _ exploration _ ( trying previously unexplored strategies to find better policy ) and _ exploitation _ ( taking the most advantage of the experienced knowledge ) .",
    "the other is its slow learning speed , especially for the complex problems sometimes known as  the curse of dimensionality \" when the state - action space becomes huge and the number of parameters to be learned grows exponentially with its dimension .    to combat those problems ,",
    "many methods have been proposed in recent years .",
    "temporal abstraction and decomposition methods have been explored to solve such problems as rl and dynamic programming ( dp ) to speed up learning @xcite-@xcite .",
    "different kinds of learning paradigms are combined to optimize rl .",
    "for example , smith @xcite presented a new model for representation and generalization in model - less rl based on the self - organizing map ( som ) and standard q - learning .",
    "the adaptation of watkins q - learning with fuzzy inference systems for problems with large state - action spaces or with continuous state spaces is also proposed @xcite , @xcite , @xcite , @xcite .",
    "many specific improvements are also implemented to modify related rl methods in practice @xcite , @xcite , @xcite , @xcite , @xcite , @xcite . in spite of all these attempts",
    "more work is needed to achieve satisfactory successes and new ideas are necessary to explore more effective representation methods and learning mechanisms . in this paper",
    ", we explore to overcome some difficulties in rl using quantum theory and propose a novel quantum reinforcement learning method .",
    "quantum information processing is a rapidly developing field .",
    "some results have shown that quantum computation can more efficiently solve some difficult problems than the classical counterpart .",
    "two important quantum algorithms , the shor algorithm @xcite , @xcite and the grover algorithm @xcite , @xcite have been proposed in 1994 and 1996 , respectively .",
    "the shor algorithm can give an exponential speedup for factoring large integers into prime numbers and it has been realized @xcite for the factorization of integer 15 using nuclear magnetic resonance ( nmr ) .",
    "the grover algorithm can achieve a square speedup over classical algorithms in unsorted database searching and its experimental implementations have also been demonstrated using nmr @xcite-@xcite and quantum optics @xcite , @xcite for a system with four states .",
    "some methods have also been explored to connect quantum computation and machine learning .",
    "for example , the quantum computing version of artificial neural network has been studied from the pure theory to the simple simulated and experimental implementation @xcite-@xcite .",
    "rigatos and tzafestas @xcite used quantum computation for the parallelization of a fuzzy logic control algorithm to speed up the fuzzy inference .",
    "quantum or quantum - inspired evolutionary algorithms have been proposed to improve the existing evolutionary algorithms @xcite .",
    "hogg and portnov @xcite presented a quantum algorithm for combinatorial optimization of overconstrained satisfiability ( sat ) and asymmetric travelling salesman ( atsp ) . recently",
    "the quantum search technique has been used to dynamic programming @xcite .",
    "taking advantage of quantum computation , some novel algorithms inspired by quantum characteristics will not only improve the performance of existing algorithms on traditional computers , but also promote the development of related research areas such as quantum computers and machine learning .",
    "considering the essence of computation and algorithms , dong and his co - workers @xcite have presented the concept of _ quantum reinforcement learning _ ( qrl ) inspired by the state superposition principle and quantum parallelism . following this concept",
    ", we in this paper give a formal quantum reinforcement learning algorithm framework and specifically demonstrate the advantages of qrl for speeding up learning and obtaining a good tradeoff between exploration and exploitation of rl through simulated experiments and some related discussions .",
    "this paper is organized as follows .",
    "section ii contains the prerequisite and problem description of standard reinforcement learning , quantum computation and related quantum gates . in section iii , quantum reinforcement learning",
    "is introduced systematically , where the state ( action ) space is represented with the quantum state , the exploration strategy based on the collapse postulate is achieved and a novel qrl algorithm is proposed specifically .",
    "section iv analyzes related characteristics of qrl such as the convergence , optimality and the balancing between exploration and exploitation .",
    "section v describes the simulated experiments and the results demonstrate the effectiveness and superiority of qrl algorithm . in section",
    "vi , we briefly discuss some related problems of qrl for future work . concluding remarks are given in section vii .",
    "in this section we first briefly review the standard reinforcement learning algorithms and then introduce the background of quantum computation and some related quantum gates .",
    "standard framework of reinforcement learning is based on discrete - time , finite - state markov decision processes ( mdps ) @xcite .",
    "a markov decision process ( mdp ) is composed of the following five factors : @xmath0 , where : @xmath1 is the state space ; @xmath2 is the action space for state @xmath3 ; @xmath4 is the probability for state transition ; @xmath5 is a reward function , @xmath6 , where @xmath7 ; @xmath8 is a criterion function or objective function .    according to the definition of mdp , we know that the mdp history is composed of successive states and decisions : @xmath9 .",
    "the policy @xmath10 is a sequence : @xmath11 , when the history at @xmath12 is @xmath13 , the strategy is adopted to make a decision according to the probability distribution @xmath14 on @xmath15 .",
    "rl algorithms assume that the state @xmath1 and action @xmath15 can be divided into discrete values . at a certain step @xmath16 ,",
    "the _ agent _ observes the state of the environment ( inside and outside of the agent ) @xmath17 , and then choose an action @xmath18 .",
    "after executing the action , the agent receives a reward @xmath19 , which reflects how good that action is ( in a short - term sense ) .",
    "the state of the environment will change to next state @xmath20 under the action @xmath18 .",
    "the agent will choose the next action @xmath21 according to related knowledge .",
    "the goal of reinforcement learning is to learn a mapping from states to actions , that is to say , the agent is to learn a policy @xmath22 $ ] , so that the expected sum of discounted reward of each state will be maximized : @xmath23\\ \\ \\ \\ \\ \\ \\ \\ \\nonumber\\\\ = \\sum_{a\\in a_s}\\pi(s , a)[r_{s}^{a}+\\gamma \\sum_{s'}p_{ss'}^{a}v_{(s')}^{\\pi } ] \\ \\ \\ \\ \\\\end{aligned}\\ ] ] where @xmath24 $ ] is a discount factor , @xmath25 is the probability of selecting action @xmath26 according to state @xmath27 under policy @xmath10 , @xmath28 is the probability for state transition and @xmath29 is the expected one - step reward .",
    "@xmath30 ( or @xmath31 ) is also called the value function of state @xmath27 and the temporal difference ( td ) one - step updating rule of @xmath31 may be described as @xmath32 where @xmath33 is the learning rate .",
    "we have the optimal state - value function @xmath34\\ ] ] @xmath35 in dynamic programming , ( [ bellman ] ) is also called the bellman equation of @xmath36 .    as for state - action pairs",
    ", there are similar value functions and bellman equations , and @xmath37 stands for the value of taking the action @xmath26 in the state @xmath27 under the policy @xmath10 : @xmath38 @xmath39 let @xmath40 be the learning rate , and the one - step updating rule of q - learning ( a widely used rl algorithm ) @xcite is : @xmath41 there are many effective standard rl algorithms like q - learning , for example td(@xmath42 ) , sarsa , etc . for more details see @xcite .",
    "analogous to classical bits , the fundamental concept in quantum computation is the _ quantum bit _",
    "( _ qubit _ ) .",
    "the two basic states for a qubit are denoted as @xmath43 and @xmath44 , which correspond to the states 0 and 1 for a classical bit . however , besides @xmath43 or @xmath44 , a qubit can also lie in the superposition state of @xmath43 and @xmath44 . in other words ,",
    "a qubit @xmath45 can generally be expressed as a linear combination of @xmath43 and @xmath44 latexmath:[\\[\\label{qubit }    @xmath40 and @xmath47 are complex coefficients .",
    "this special quantum phenomenon is called _ state superposition principle _ , which is an important difference between classical computation and quantum computation @xcite .",
    "the physical carrier of a qubit is any two - state quantum system such as a two - level atom , spin-@xmath48 particle and polarized photon . for a physical qubit ,",
    "when we select a set of bases @xmath43 or @xmath44 , we indicate that an observable @xmath49 of the qubit system has been chosen and the bases correspond to the two eigenvectors of @xmath49 . for convenience , the measurement process on the observable @xmath49 of a quantum system in corresponding state @xmath45",
    "is directly called a measurement of quantum state @xmath45 in this paper .",
    "when we measure a qubit in superposition state @xmath45 , the qubit system would _ collapse _ into one of its basic states @xmath43 or @xmath44 .",
    "however , we can not determine in advance whether it will collapse to state @xmath43 or @xmath44 .",
    "we only know that we get this qubit in state @xmath43 with probability @xmath50 , or in state @xmath44 with probability @xmath51 .",
    "hence @xmath40 and @xmath47 are generally called _",
    "probability amplitudes_. the magnitude and argument of probability amplitude represent _ amplitude _ and _ phase _ , respectively .",
    "since the sum of probabilities must be equal to 1 , @xmath40 and @xmath47 should satisfy @xmath52 .    according to quantum computation theory ,",
    "a fundamental operation in the quantum computing process is a unitary transformation @xmath53 on the qubits .",
    "if one applies a transformation @xmath53 to a superposition state , the transformation will act on all basis vectors of this state and the output will be a new superposition state obtained by superposing the results of all basis vectors .",
    "it seems that the transformation @xmath53 can simultaneously evaluate the different values of a function @xmath54 for a certain input @xmath55 and it is called _",
    "quantum parallelism_. the quantum parallelism is one of the most important factors to acquire the powerful ability of quantum algorithm .",
    "however , note that this parallelism is not immediately useful @xcite since the direct measurement on the output generally gives only @xmath54 for one value of @xmath55 .",
    "suppose the input qubit @xmath56 lies in the superposition state : @xmath57 the transformation @xmath58 which describes computing process may be defined as follows : @xmath59 where @xmath60 represents the joint input state with the first qubit in @xmath56 and the second qubit in @xmath43 , and @xmath61 is the joint output state with the first qubit in @xmath56 and the second qubit in @xmath62 . according to equations ( 9 ) and ( 10 ) , we can easily obtain @xcite : @xmath63 the result contains information about both @xmath64 and @xmath65 , and we seem to evaluate @xmath66 for two values of @xmath67 simultaneously .",
    "the above process corresponds to a  _ quantum black box _ \" ( or oracle ) . by feeding quantum superposition states to a quantum black box",
    ", we can learn what is inside with an exponential speedup , compared to how long it would take if we were only allowed classical inputs @xcite .",
    "now consider an @xmath12-qubit system , which can be represented with tensor product of @xmath12 qubits : @xmath68 where ` @xmath69 ' means tensor product , @xmath70 , @xmath71 is complex coefficient and @xmath72 represents occurrence probability of @xmath73 when state @xmath74 is measured .",
    "@xmath55 can take on @xmath75 values , so the superposition state can be looked upon as the superposition of all integers from @xmath76 to @xmath77 . since @xmath53 is a unitary transformation ,",
    "computing function @xmath54 can result @xcite : @xmath78    based on the above analysis , it is easy to find that an @xmath12-qubit system can simultaneously process @xmath75 states although only one of the @xmath75 states is accessible through a direct measurement and the ability is required to extract information about more than one value of @xmath54 from the output superposition state @xcite .",
    "this is different from classical parallel computation , where multiple circuits built to compute are executed simultaneously , since quantum parallelism does nt necessarily make a tradeoff between computation time and needed physical space .",
    "in fact , quantum parallelism employs a single circuit to simultaneously evaluate the function for multiple values by exploiting the quantum state superposition principle and provides an exponential - scale computation space in the @xmath12-qubit linear physical space .",
    "therefore quantum computation can effectively increase the computing speed of some important classical functions .",
    "so it is possible to obtain significant result through fusing the quantum computation into the reinforcement learning theory .      in the classical computation ,",
    "the logic operators that can complete some specific tasks are called _ logic gates _ , such as _ not _ gate , _ and _ gate , _ xor _ gate , and so on .",
    "analogously , quantum computing tasks can be completed through _",
    "quantum gates_. nowadays some simple quantum gates such as quantum not gate and quantum _ cnot _ gate have been built in quantum computation .",
    "here we only introduce two important quantum gates , _ hadamard gate _ and _ phase gate _",
    ", which are closely related to accomplish some quantum logic operations for the present quantum reinforcement learning",
    ". the detailed discussion about quantum gates can be found in the ref .",
    "@xcite .    hadamard gate ( or hadamard transform ) is one of the most useful quantum gates and can be represented as @xcite : @xmath79 through hadamard gate , a qubit in the state @xmath43 is transformed into an equally weighted superposition state of @xmath43 and @xmath44 , i.e. @xmath80 similarly , a qubit in the state @xmath44 is transformed into the superposition state @xmath81 , i.e. the magnitude of the amplitude in each state is @xmath82 , but the phase of the amplitude in the state @xmath44 is inverted . in classical probabilistic algorithms , the phase has no analog since the amplitudes are in general complex numbers in quantum mechanics .    the other related quantum gate is the phase gate ( conditional phase shift operation ) which is an important element to carry out the grover iteration for reinforcing  good \" decision . according to quantum information theory , this transformation may be efficiently implemented on a quantum computer .",
    "for example , the transformation describing this for a two - state system is of the form : @xmath83 where @xmath84 and @xmath85 is arbitrary real number @xcite .",
    "just like the traditional reinforcement learning , a quantum reinforcement learning system can also be identified for three main subelements : a policy , a reward function and a model of the environment ( maybe not explicit ) . but quantum reinforcement learning algorithms are remarkably different from all those traditional rl algorithms in the following intrinsic aspects : representation , policy , parallelism and updating operation .",
    "as we represent a qrl system with quantum concepts , similarly , we have the following definitions and propositions for quantum reinforcement learning .",
    "select an observable of a quantum system and its eigenvectors form a set of complete orthonormal bases in a hilbert space . the states @xmath27 ( or actions @xmath26 ) in definition 1 are denoted as the corresponding orthogonal bases and are called the eigen states or eigen actions in qrl .    in the remainder of this paper",
    ", we indicate that an observable has been chosen but we do not present the observable specifically when mentioning a set of orthogonal bases . from definition 2 , we can get the set of eigen states : @xmath1 , and that of eigen actions for state @xmath3 : @xmath2 .",
    "the eigen state ( eigen action ) in qrl corresponds to the state ( action ) in traditional rl . according to quantum mechanics , the quantum state for a general closed quantum system",
    "can be represented with a unit vector @xmath45 ( dirac representation ) in a hilbert space .",
    "the inner product of @xmath86 and @xmath87 can be written into @xmath88 and the normalization condition for @xmath45 is @xmath89 .",
    "as the simplest quantum mechanical system , the state of the qubit can be described as ( [ qubit ] ) and its normalization condition is equivalent to @xmath52 .    according to the superposition principle in quantum computation , since a quantum reinforcement learning system can lie in some orthogonal quantum states , which correspond to the eigen states ( eigen actions )",
    ", it can also lie in an arbitrary superposition state .",
    "that is to say , a qrl system which can take on the states ( or actions ) @xmath90 is also able to occupy their linear superposition state ( or action ) @xmath91 it is worth noting that this is only a representation method and our goal is to take advantage of the quantum characteristics in the learning process .",
    "in fact , the state ( action ) in qrl is not a practical state ( action ) and it is only an artificial state ( action ) for computing convenience with quantum systems .",
    "the practical state ( action ) is the eigen state ( eigen action ) in qrl . for an arbitrary state ( or action ) in a quantum reinforcement learning system , we can obtain proposition 1 .",
    "an arbitrary state @xmath92 ( or action @xmath93 ) in qrl can be expanded in terms of an orthogonal set of eigen states @xmath94 ( or eigen actions @xmath95 ) , i.e. @xmath96 @xmath97 where @xmath98 and @xmath99 are probability amplitudes , and satisfy @xmath100 and @xmath101 .",
    "the states and actions in qrl are different from those in traditional rl : ( 1 ) the sum of several states ( or actions ) does not have a definite meaning in traditional rl , but the sum of states ( or actions ) in qrl is still a possible state ( or action ) of the same quantum system .",
    "( 2 ) when @xmath92 takes on an eigen state @xmath94 , it is exclusive .",
    "otherwise , it has the probability of @xmath102 to be in the eigen state @xmath94 .",
    "the same analysis also is suitable to the action @xmath93 .    since quantum computation is built upon the concept of qubit as what has been described in section ii , for the convenience of processing , we consider to use multiple qubit systems to express states and actions and propose a formal representation of them for the qrl system .",
    "let @xmath103 and @xmath104 be the number of states and actions , then choose numbers @xmath105 and @xmath12 , which are characterized by the following inequalities : @xmath106 and use @xmath105 and @xmath12 qubits to represent eigen state set @xmath107 and eigen action set @xmath108 respectively , we can obtain the corresponding relations as follows : @xmath109 @xmath110 in other words , the states ( or actions ) of a qrl system may lie in the superposition state of eigen states ( or eigen actions ) .",
    "inequalities in ( [ equation 20 ] ) guarantee that every states and actions in traditional rl have corresponding representation with eigen states and eigen actions in qrl . the probability amplitude",
    "@xmath111 and @xmath112 are complex numbers and satisfy @xmath113 @xmath114      in qrl , the agent is also to learn a policy @xmath115 $ ] , which will maximize the expected sum of discounted reward of each state .",
    "that is to say , the mapping from states to actions is @xmath116 , and we have @xmath117 where probability amplitude @xmath112 satisfies ( [ equation 26 ] ) . here",
    ", the action selection policy is based on the collapse postulate :    when an action @xmath118 is measured , it will be changed and collapse randomly into one of its eigen actions @xmath119 with the corresponding probability @xmath120 : @xmath121    according to definition 3 , when an action @xmath122 in ( [ action ] ) is measured , we will get @xmath123 with the occurrence probability of @xmath124 . in qrl algorithm",
    ", we will amplify the probability of  good \" action according to corresponding rewards .",
    "it is obvious that the collapse action selection method is not a real action selection policy theoretically .",
    "it is just a fundamental phenomenon when a quantum state is measured , which results in a good balancing between exploration and exploitation and a natural  action selection \" without setting parameters .",
    "more detailed discussion about the action selection can also be found in ref .",
    "@xcite      in proposition 1 , we pointed out that every possible state of qrl @xmath92 can be expanded in terms of an orthogonal complete set of eigen states @xmath94 : @xmath125 . according to quantum parallelism , a certain unitary transformation @xmath53 on the qubits",
    "can be implemented .",
    "suppose we have such an operation which can simultaneously process these @xmath126 states with the td(0 ) value updating rule @xmath127 where @xmath40 is the learning rate , and the meaning of reward @xmath5 and value function @xmath8 is the same as that in traditional rl .",
    "it is like parallel value updating of traditional rl over all states .",
    "however , it provides an exponential - scale computation space in the @xmath105-qubit linear physical space and can speed up the solutions of related functions . in this paper",
    "we will simulate qrl process on the traditional computer in section v. how to realize some specific functions of the algorithm using quantum gates in detail is our future work .",
    "in qrl , action selection is executed by measuring action @xmath122 related to certain state @xmath128 , which will collapse to @xmath123 with the occurrence probability of @xmath124 .",
    "so it is no doubt that probability amplitude updating is the key of recording the  trial - and - error \" experience and learning to be more intelligent .    as the action @xmath122 is the superposition of @xmath75 possible eigen actions , finding out @xmath123 is usually interacting with changing its probability amplitude for a quantum system .",
    "the updating of probability amplitude is based on the grover iteration @xcite .",
    "first , prepare the equally weighted superposition of all eigen actions @xmath129 this process can be done easily by applying @xmath12 hadamard gates in sequence to @xmath12 independent qubits with initial states in @xmath43 respectively @xcite , which can be represented into : @xmath130 we know that @xmath123 is an eigen action , irrespective of the value of @xmath26 , so that @xmath131    to construct the grover iteration we will combine two reflections @xmath132 and @xmath133 @xcite @xmath134 @xmath135 where @xmath136 is unitary matrix with appropriate dimensions and @xmath137 corresponds to the oracle @xmath138 in the grover algorithm @xcite",
    ". the external product @xmath139 is defined @xmath140 .",
    "obviously , we have @xmath141 @xmath142 where @xmath143 represents an arbitrary state orthogonal to @xmath123 .",
    "hence @xmath132 flips the sign of the action @xmath123 , but acts trivially on any action orthogonal to @xmath123 .",
    "this transformation has a simple geometrical interpretation . acting on any vector in the @xmath75-dimensional hilbert space ,",
    "@xmath132 reflects the vector about the hyperplane orthogonal to @xmath123 .",
    "analogous to the analysis in the grover algorithm , @xmath132 can be looked upon as a quantum black box , which can effectively justify whether the action is the  good \" eigen action . similarly",
    ", @xmath133 preserves @xmath144 , but flips the sign of any vector orthogonal to @xmath144 .",
    "thus one grover iteration is the unitary transformation @xcite , @xcite @xmath145 now let s consider how the grover iteration acts in the plane spanned by @xmath123 and @xmath144",
    ". the initial action in equation ( [ equation 30 ] ) can be re - expressed as @xmath146    recall that @xmath147 thus @xmath148 can be visualized geometrically by fig .  1 .",
    "flips @xmath149 into @xmath150 and @xmath133 flips @xmath150 into @xmath151 .",
    "one grover iteration @xmath152 rotates @xmath149 by @xmath153.,width=307 ]    this figure shows that @xmath144 is rotated by @xmath154 from the axis @xmath143 normal to @xmath123 in the plane .",
    "@xmath132 reflects a vector @xmath149 in the plane about the axis @xmath143 to @xmath150 , and @xmath133 reflects the vector @xmath150 about the axis @xmath144 to @xmath151 . from fig .",
    "1 we know that @xmath155 thus we have @xmath156 .",
    "so one grover iteration @xmath157 rotates any vector @xmath149 by @xmath153 .",
    "we now can carry out a certain times of grover iterations to update the probability amplitudes according to respective rewards and value functions .",
    "it is obvious that @xmath153 is the updating stepsize .",
    "thus when an action @xmath123 is executed , the probability amplitude of @xmath122 is updated by carrying out @xmath158 times of grover iterations , where @xmath159 returns the integer part of @xmath55 .",
    "@xmath160 is a parameter which indicates that the times @xmath161 of iterations is proportional to @xmath162 .",
    "the selection of its value is experiential in this paper and its optimization is an open question .",
    "the probability amplitudes will be normalized with @xmath163 after each updating . according to ref .",
    "@xcite , we know that applying grover iteration @xmath152 for @xmath161 times on @xmath164 can be represented as @xmath165| a\\rangle + \\cos [ ( 2l+1 ) \\theta ] |a^{\\perp}\\rangle\\ ] ] obviously , we can reinforce the action @xmath123 from probability @xmath166 to @xmath167 $ ] through grover iterations .",
    "since @xmath168 is a periodical function about @xmath169 and too much iterations may also cause small probability @xmath167 $ ] , we further select @xmath170 .",
    "the probability amplitude updating is inspired by the grover algorithm and the two procedures use the same amplitude amplification technique as a subroutine . here",
    "we want to emphasize the difference between the probability amplitude updating and grover s database searching algorithm .",
    "the objective of grover algorithm is to search @xmath123 by amplifying its occurrence probability to almost 1 , however , the aim of probability amplitude updating process in qrl just appropriately updates ( amplifies or shrinks ) corresponding amplitudes for  good \" or  bad \" eigen actions .",
    "so the essential difference is in the times @xmath161 of iterations and this can be demonstrated by fig .  2 .",
    "to almost 1 ; ( c ) grover iterations for reinforcing action @xmath123 to probability @xmath167$],width=288 ]          based on the above discussion , the procedural form of a standard qrl algorithm is described as fig .",
    "3 . in qrl algorithm , after initializing the state and action we can observe @xmath122 and obtain an eigen action @xmath123 .",
    "execute this action and the system can give out next state @xmath171 , reward @xmath5 and state value @xmath172 .",
    "@xmath31 is updated by td(0 ) rule , and @xmath5 and @xmath172 can be used to determine the iteration times @xmath161 . to accomplish the task in a practical computing device , we require some basic registers for the storage of related information .",
    "firstly two @xmath105-qubit registers are required for all eigen states and their state values @xmath31 , respectively .",
    "secondly every eigen state requires two @xmath12-qubit registers for their respective eigen actions stored for two times , where one @xmath12-qubit register stores the action @xmath122 to be observed and the other @xmath12-qubit register also stores the same action for preventing the memory loss associated to the action collapse .",
    "it is worth mentioning that this does not conflict with the no - cloning theorem @xcite since the action @xmath122 is a certain known state at each step .",
    "finally several simple classical registers may be required for the reward @xmath5 , the times @xmath161 , and etc .",
    "qrl is inspired by the superposition principle of quantum state and quantum parallelism .",
    "the action set can be represented with the quantum state and the eigen action can be obtained by randomly observing the simulated quantum state , which will lead to state collapse according to the quantum measurement postulate .",
    "the occurrence probability of every eigen action is determined by its corresponding probability amplitude , which is updated according to rewards and value functions .",
    "so this approach represents the whole state - action space with the superposition of quantum state and makes a good tradeoff between exploration and exploitation using probability .",
    "the merit of qrl is dual .",
    "first , as for simulation algorithm on the traditional computer it is an effective algorithm with novel representation and computation methods .",
    "second , the representation and computation mode are consistent with quantum parallelism and can speed up learning with quantum computers or quantum gates .",
    "in this section , we discuss some theoretical properties of qrl algorithms and provide some advice from the point of view of engineering .",
    "four major results are presented : ( 1 ) an asymptotic convergence proposition for qrl algorithms , ( 2 ) the optimality and stochastic algorithm , ( 3 ) good balancing between exploration and exploitation , and ( 4 ) physical realization . from the following analysis , it is obvious that qrl shows much better performance than other methods when the searching space becomes very large .",
    "in qrl we use the temporal difference ( td ) prediction for the state value updating , and td algorithm has been proved to converge for absorbing markov chain @xcite when the learning rate is nonnegative and degressive . to generally consider the convergence results of qrl",
    ", we have proposition 2 .    for any markov chain ,",
    "qrl algorithms converge to the optimal state value function @xmath173 with probability 1 under proper exploration policy when the following conditions hold ( where @xmath174 is learning rate and nonnegative ) : @xmath175       ( sketch ) based on the above analysis , qrl is a stochastic iterative algorithm .",
    "bertsekas and tsitsiklis have verified the convergence of stochastic iterative algorithms @xcite when ( [ equation 40 ] ) holds .",
    "in fact many traditional rl algorithms have been proved to be stochastic iterative algorithms @xcite , @xcite , @xcite and qrl is the same as traditional rl , and main differences lie in :    \\(1 ) exploration policy is based on the collapse postulate of quantum measurement while being observed ;    \\(2 ) this kind of algorithms is carried out by quantum parallelism , which means we update all states simultaneously and qrl is a synchronous learning algorithm .",
    "so the modification of rl does not affect the characteristic of convergence and qrl algorithm converges when ( [ equation 40 ] ) holds .",
    "most quantum algorithms are stochastic algorithms which can give the correct decision - making with probability 1-@xmath176 ( @xmath177 , close to 0 ) after several times of repeated computing @xcite , @xcite . as for quantum reinforcement learning algorithms ,",
    "optimal policies are acquired by the collapse of quantum system and we will analyze the optimality of these policies from two aspects as follows .",
    "when qrl algorithms are implemented by real quantum apparatuses , the agent s strategy is given by the collapse of corresponding quantum system according to probability amplitude .",
    "qrl algorithms can not guarantee the optimality of every strategy , but it can give the optimal decision - making with the probability approximating to 1 by repeating computation several times .",
    "suppose that the agent gives an optimal strategy with the probability @xmath178 after the agent has well learned ( state value function converges to @xmath173 ) . for @xmath179 ,",
    "the error probability is @xmath180 by repeating @xmath181 times .",
    "hence the agent will give the optimal strategy with the probability of @xmath182 by repeating the computation for @xmath181 times . the qrl algorithms on real quantum apparatuses are still effective due to the powerful computing capability of quantum system .",
    "our current work has been focused on simulating qrl algorithms on the traditional computer which also bear the characteristics inspired by quantum systems .      as mentioned above , in this paper",
    "most work has been done to develop this kind of novel qrl algorithms by simulating on the traditional computer .",
    "but in traditional rl theory , researchers have argued that even if we have a complete and accurate model of the environment s dynamics , it is usually not possible to simply compute an optimal policy by solving the bellman optimality equation @xcite . what s the fact about qrl ? in qrl ,",
    "the optimal value functions and optimal policies are defined in the same way as traditional rl .",
    "the difference lies in the representation and computing mode .",
    "the policy is probabilistic instead of being definite using probability amplitude , which makes it more effective and safer .",
    "but it is still obvious that simulating qrl on the traditional computer can not speed up learning in exponential scale since the quantum parallelism is not really executed through real physical systems .",
    "what s more , when more powerful computation is available , the agent will learn much better .",
    "then we may fall back on physical realization of quantum computation again .",
    "one widely used action selection scheme is @xmath176__-greedy _ _ @xcite , @xcite , where the best action is selected with probability ( @xmath178 ) and a random action is selected with probability @xmath176 ( @xmath179 ) .",
    "the exploration probability @xmath176 can be reduced over time , which moves the agent from exploration to exploitation .",
    "the @xmath176-greedy method is simple and effective but it has one drawback that when it explores it chooses equally among all actions .",
    "this means that it makes no difference to choose the worst action or the next - to - best action .",
    "another problem is that it is difficult to choose a proper parameter @xmath176 which can offer an optimal balancing between exploration and exploitation .",
    "another kind of action selection scheme is boltzmann exploration ( including softmax action selection method ) @xcite , @xcite , @xcite .",
    "it uses a positive parameter @xmath183 called the _ temperature _ and chooses action with the probability proportional to @xmath184 .",
    "it can move from exploration to exploitation by adjusting the  temperature \" parameter @xmath183 .",
    "it is natural to sample actions according to this distribution , but it is very difficult to set and adjust a good parameter @xmath183 . there are also similar problems with simulated annealing ( sa ) methods @xcite .",
    "we have introduced the action selecting strategy of qrl in section iii , which is called collapse action selection method .",
    "the agent does not bother about selecting a proper action consciously .",
    "the action selecting process is just accomplished by the fundamental phenomenon that it will naturally collapse to an eigen action when an action ( represented by quantum superposition state ) is measured . in the learning process",
    ", the agent can explore more effectively since the state and action can lie in the superposition state through parallel updating . when an action is observed , it will collapse to an eigen action with a certain probability .",
    "hence qrl algorithm is essentially a kind of probability algorithm .",
    "however , it is greatly different from classical probability since classical algorithms forever exclude each other for many results , but in qrl algorithm it is possible for many results to interfere with each other to yield some global information through some specific quantum gates such as hadmard gates . compared with other exploration strategy",
    ", this mechanism leads to a better balancing between exploration and exploitation .    in this paper",
    ", the simulated results will show that the action selection method using the collapse phenomenon is very extraordinary and effective .",
    "more important , it is consistent with the physical quantum system , which makes it more natural , and the mechanism of qrl has the potential to be implemented by real quantum systems .      as a quantum algorithm ,",
    "the physical realization of qrl is also feasible since the two main operations occur in preparing the equally weighted superposition state for initializing the quantum system and carrying out a certain times of grover iterations for updating probability amplitude according to rewards and value functions .",
    "these are the same operations needed in the grover algorithm .",
    "they can be accomplished using different combinations of hadamard gates and phase gates .",
    "so the physical realization of qrl has no difficulty in principle .",
    "moreover , the experimental implementations of the grover algorithm also demonstrate the feasibility for the physical realization of our qrl algorithm .",
    "to evaluate qrl algorithm in practice , consider the typical gridworld example .",
    "the gridworld environment is as shown in fig .  4 and each cell of",
    "the grid corresponds to an individual state ( eigen state ) of the environments . from any state",
    "the agent can perform one of four primary actions ( eigen actions ) : up , down , left and right , and actions that would lead into a blocked cell are not executed .",
    "the task of the algorithms is to find an optimal policy which will let the agent move from start point @xmath1 to goal point @xmath185 with minimized cost ( or maximized rewards ) .",
    "an episode is defined as one time of learning process when the agent moves from the start state to the goal state .",
    "but when the agent can not find the goal state in a maximum steps ( or a period of time ) , this episode will be terminated and start another episode from the start state again .",
    "so when the agent finds an optimal policy through learning , the number of moving steps for one episode will reduce to a minimum one .      in this @xmath186 gridworld , the initial state @xmath1 and the goal state @xmath185 is cell(1,1 ) and cell(18,18 ) and before learning the agent has no information about the environment at all .",
    "once the agent finds the goal state it receives a reward of @xmath187 and then ends this episode .",
    "all steps are punished by a reward of @xmath188 .",
    "the discount factor @xmath189 is set to 0.99 and all of the state values v(s ) are initialized as @xmath190 for all the algorithms that we have carried out . in the first experiment",
    ", we compare qrl algorithm with td(0 ) and we also demonstrate the expected result on a quantum computer theoretically . in the second experiment , we give out some results of qrl algorithm with different learning rates .",
    "for the action selection policy of td algorithm , we use @xmath176-greedy policy ( @xmath191 ) , that is to say , the agent executes the  good \" action with probability @xmath178 and chooses other actions with an equal probability . as for qrl , the action selecting policy is obviously different from traditional rl algorithms , which is inspired by the collapse postulate of quantum measurement .",
    "the value of @xmath124 is used to denote the probability of an action defined as @xmath192 . for the four cell - to - cell actions ,",
    "i.e. four eigen actions up , down , left and right , @xmath124 is initialized uniformly .",
    "learning performance for qrl algorithm compared with td algorithm in traditional rl is plotted in fig .  5 , where the cases with the good performance are chosen for both of the qrl and td algorithms . as shown in fig .  5 , the good cases in this gridworld example are respectively td algorithm with the learning rate of @xmath193 and qrl algorithm with @xmath194 .",
    "the horizontal axis represents the episode in the learning process and the number of steps required is correspondingly described by the vertical coordinate .",
    "we observe that qrl algorithm is also an effective algorithm on the traditional computer although it is inspired by the quantum mechanical system and is designed for quantum computers in the future . for their respective rather good cases in fig .  5 , qrl explores more than td algorithm at the beginning of learning phase , but it learns much faster and guarantees a better balancing between exploration and exploitation .",
    "in addition , it is much easier to tune the parameters for qrl algorithms than for traditional ones .",
    "if the real quantum parallelism is used , we can obtain the estimated theoretical results .",
    "what s more important , according to the estimated theoretical results , qrl has great potential of powerful computation provided that the quantum computer ( or related quantum apparatuses ) is available in the future , which will lead to a more effective approach for the existing problems of learning in complex unknown environments .",
    "furthermore , in the following comparison experiments we give the results of td(0 ) algorithm in qrl and rl algorithms with different learning rates , respectively . in fig .  6",
    "it illustrates the results of qrl algorithms with different learning rates : @xmath40 ( alpha ) , from 0.01 to 0.11 , and to give a particular description of the learning process , we record every learning episodes . from these figures",
    ", it can been concluded that given a proper learning rate ( @xmath195 alpha @xmath196 ) this algorithm learns fast and explores much at the beginning phase , and then steadily converges to the optimal policy that costs 36 steps to the goal state @xmath185 .",
    "as the learning rate increases from 0.02 to 0.09 , this algorithm learns faster .",
    "when the learning rate is 0.01 or smaller , it explores more but learns very slow , so the learning process converges very slowly .",
    "compared with the result of td in fig .  5",
    ", we find that the simulation result of qrl on the classical computer does not show advantageous when the learning rate is small ( alpha @xmath197 ) .",
    "on the other hand , when the learning rate is 0.11 or above , it can not converge to the optimal policy because it vibrates with too large learning rate when the policy is near the optimal policy .",
    "7 shows the performance of td(0 ) algorithm , and we can see that the learning process converges with the learning rate of 0.01 .",
    "but when the learning rate is bigger ( alpha=0.02 , 0.03 or bigger ) , it becomes very hard for us to make it converge to the optimal policy within 10000 episodes .",
    "anyway from fig .",
    "6 and fig .  7",
    ", we can see that the convergence range of qrl algorithm is much larger than that of traditional td(0 ) algorithm .",
    "all the results show that qrl algorithm is effective and excels traditional rl algorithms in the following three main aspects : ( 1 ) action selecting policy makes a good tradeoff between exploration and exploitation using probability , which speeds up the learning and guarantees the searching over the whole state - action space as well . ( 2 ) representation is based on the superposition principle of quantum mechanics and the updating process is carried out through quantum parallelism , which will be much more prominent in the future when practical quantum apparatus comes into use instead of being simulated on the traditional computers .",
    "( 3 ) compared with the experimental results in ref .",
    "@xcite , where the simulation environment is a @xmath198 gridworld , we can see that when the state space is getting larger , the performance of qrl is getting better than traditional rl in simulated experiments .",
    "the key contribution of this paper is a novel reinforcement learning framework called quantum reinforcement learning that integrates quantum mechanics characteristics and reinforcement learning theories . in this section",
    "some associated problems of qrl on the traditional computer are discussed and some future work regarded as important is also pointed out .    although it is a long way for implementing such complicated quantum systems as qrl by physical quantum systems , the simulated version of qrl on the traditional computer has been proved effective and also excels standard rl methods in several aspects . to improve this approach some issues of future work",
    "is laid out as follows , which we deem to be important .    *",
    "* model of environments * an appropriate model of the environment will make problem - solving much easier and more efficient . this is true for most of the rl algorithms .",
    "however , to model environments accurately and simply is a tradeoff problem . as for qrl ,",
    "this problem should be considered slightly differently due to some of its specialities . * * representations * the representations for qrl algorithm according to different kinds of problems would be naturally of interest ones when a learning system is designed . in this paper",
    ", we mainly discuss problems with discrete states and actions and a natural question is how to extend qrl to the problems with continuous states and actions effectively . * * function approximation and generalization * generalization is necessary for rl systems to be applied to artificial intelligence and most engineering applications .",
    "function approximation is an important approach to acquire generalization . as for qrl",
    ", this issue will be a rather challenging task and function approximation should be considered with the special computation mode of qrl .",
    "* * theory * qrl is a new learning framework that is different from standard rl in several aspects , such as representation , action selection , exploration policy , updating style , etc .",
    "so there is a lot of theoretical work to do to take most advantage of it , especially to analyze the complexity of the qrl algorithm and improve its representation and computation . *",
    "* more applications * besides more theoretical research , a tremendous opportunity to apply qrl algorithms to a range of problems is needed to testify and improve this kind of learning algorithms , especially in unknown probabilistic environments and large learning space .",
    "anyway we strongly believe that qrl approaches and related techniques will be promising for agent learning in large scale unknown environment .",
    "this new idea of applying quantum characteristics will also inspire the research in the area of machine learning .",
    "in this paper , qrl is proposed based on the concepts and theories of quantum computation in the light of the existing problems in rl algorithms such as tradeoff between exploration and exploitation , low learning speed , etc . inspired by state superposition principle",
    ", we introduce a framework of value updating algorithm .",
    "the state ( action ) in traditional rl is looked upon as the eigen state ( eigen action ) in qrl .",
    "the state ( action ) set can be represented by the quantum superposition state and the eigen state ( eigen action ) can be obtained by randomly observing the simulated quantum state according to the collapse postulate of quantum measurement .",
    "the probability of eigen state ( eigen action ) is determined by the probability amplitude , which is updated according to rewards and value functions .",
    "so it makes a good tradeoff between exploration and exploitation and can speed up learning as well . at the same time",
    "this novel idea will promote related theoretical and technical research .    on the theoretical side ,",
    "it gives us more inspiration to look for new paradigms of machine learning to acquire better performance .",
    "it also introduces the latest development of fundamental science , such as physics and mathematics , to the area of artificial intelligence and promotes the development of those subjects as well . especially the representation and essence of quantum computation",
    "are different from classical computation and many aspects of quantum computation are likely to evolve . sooner or later machine learning will also be profoundly influenced by quantum computation theory .",
    "we have demonstrated the applicability of quantum computation to machine learning and more interesting results are expected in the near future .    on the technical side ,",
    "the results of simulated experiments demonstrate the feasibility of this algorithm and show its superiority for the learning problems with huge state spaces in unknown probabilistic environments . with the progress of quantum technology",
    ", some fundamental quantum operations are being realized via nuclear magnetic resonance , quantum optics , cavity - qed and ion trap .",
    "since the physical realization of qrl mainly needs hadamard gates and phase gates and both of them are relatively easy to be implemented in quantum computation , our work also presents a new task to implement qrl using practical quantum systems for quantum computation and will simultaneously promote related experimental research @xcite .",
    "once qrl becomes realizable on real physical systems , it can be effectively used to quantum robot learning for accomplishing some significant tasks @xcite , @xcite .",
    "quantum computation and machine learning are both the study of the information processing tasks .",
    "the two research fields have rapidly grown so that it gives birth to the combining of traditional learning algorithms and quantum computation methods , which will influence representation and learning mechanism , and many difficult problems could be solved appropriately in a new way . moreover",
    ", this idea also pioneers a new field for quantum computation and artificial intelligence @xcite , @xcite , and some efficient applications or hidden advantages of quantum computation are probably approached from the angle of learning and intelligence .",
    "the authors would like to thank two anonymous reviewers , dr .",
    "bo qi and the associate editor of ieee trans .",
    "smcb for constructive comments and suggestions which help clarify several concepts in our original manuscript and have greatly improved this paper .",
    "d. dong also wishes to thank prof .",
    "lei guo and dr .",
    "zairong xi for helpful discussions .",
    "s.  g. tzafestas and g.  g. rigatos ,  fuzzy reinforcement learning control for compliance tasks of robotic manipulators , \" _ ieee transaction on system , man , and cybernetics b _ , vol.32 , no .",
    "1 , pp.107 - 113 , 2002 .",
    "m.  kaya and r.  alhajj ,  a novel approach to multiagent reinforcement learning : utilizing olap mining in the learning process , \" _ ieee transactions on systems man and cybernetics c _ , vol.35 , pp .",
    "582 - 590 , 2005 .",
    "p.  w. shor , ",
    "algorithms for quantum computation : discrete logarithms and factoring , \" in _ proceedings of the 35th annual symposium on foundations of computer science _ , pp.124 - 134 , ieee press , los alamitos , ca , 1994 .          l.  m.  k. vandersypen , m. steffen , g. breyta , c.  s. yannoni , m.  h. sherwood and i.  l. chuang ,  experimental realization of shor s quantum factoring algorithm using nuclear magnetic resonance , \" _ nature _ , vol.414 , pp.883 - 887 , 2001 .",
    "m.  sahin , u.  atav and m.  tomak ,  quantum genetic algorithm method in self - consistent electronic structure calculations of a quantum dot with many electrons , \" _ international journal of modern physics c _ ,",
    "vol.16 , no.9 , pp.1379 - 1393 , 2005 .",
    "d. dong , c. chen and z. chen ,  quantum reinforcement learning , \" in _ proceedings of first international conference on natural computation _ , _ lecture notes in computer science _ , vol.3611 , pp.686 - 689 , 2005 .",
    "j.  preskill , physics 229 : _ advanced mathematical methods of physics  quantum information and computation_. california institute of technology , 1998 .",
    "available electronically via _",
    "http://www.theory.caltech.edu/people/preskill/ph229/ _",
    "t.  s. dahl , m.  j. mataric and g.  s. sukhatme ,  emergent robot differentiation for distributed multi - robot task allocation , \" in _ proceedings of the 7th international symposium on distributed autonomous robotic systems _ , pp.191 - 200 , 2004 ."
  ],
  "abstract_text": [
    "<S> the key approaches for machine learning , especially learning in unknown probabilistic environments are new representations and computation mechanisms . in this paper , </S>",
    "<S> a novel quantum reinforcement learning ( qrl ) method is proposed by combining quantum theory and reinforcement learning ( rl ) . </S>",
    "<S> inspired by the state superposition principle and quantum parallelism , a framework of value updating algorithm is introduced . </S>",
    "<S> the state ( action ) in traditional rl is identified as the eigen state ( eigen action ) in qrl . </S>",
    "<S> the state ( action ) set can be represented with a quantum superposition state and the eigen state ( eigen action ) can be obtained by randomly observing the simulated quantum state according to the collapse postulate of quantum measurement . </S>",
    "<S> the probability of the eigen action is determined by the probability amplitude , which is parallelly updated according to rewards . </S>",
    "<S> some related characteristics of qrl such as convergence , optimality and balancing between exploration and exploitation are also analyzed , which shows that this approach makes a good tradeoff between exploration and exploitation using the probability amplitude and can speed up learning through the quantum parallelism . to evaluate the performance and practicability of qrl , </S>",
    "<S> several simulated experiments are given and the results demonstrate the effectiveness and superiority of qrl algorithm for some complex problems . </S>",
    "<S> the present work is also an effective exploration on the application of quantum computation to artificial intelligence .    </S>",
    "<S> quantum reinforcement learning , state superposition , collapse , probability amplitude , grover iteration . </S>"
  ]
}