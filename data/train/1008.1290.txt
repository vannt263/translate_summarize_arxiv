{
  "article_text": [
    "statistical model selection in the high - dimensional regime arises in a number of applications . in many data analysis problems in geophysics , radiology , genetics , climate studies , and image processing ,",
    "the number of samples available is comparable to or even smaller than the number of variables .",
    "however , it is well - known that empirical statistics such as sample covariance matrices are not well - behaved when both the number of samples and the number of variables are large and comparable to each other ( see @xcite ) .",
    "model selection in such a setting is therefore both challenging and of great interest . in order for model selection to be well - posed",
    "given limited information , a key assumption that is often made is that the underlying model to be estimated only has _ a few degrees of freedom_. common assumptions are that the data are generated according to a graphical model , or a stationary time - series model , or a simple factor model with a few latent variables .",
    "sometimes geometric assumptions are also made in which the data are viewed as samples drawn according to a distribution supported on a low - dimensional manifold .",
    "a model selection problem that has received considerable attention recently is the estimation of covariance matrices in the high - dimensional setting . as the sample covariance matrix is poorly behaved in such a regime @xcite , some form of _ regularization _ of the sample covariance is adopted based on assumptions about the true underlying covariance matrix .",
    "for example approaches based on banding the sample covariance matrix @xcite have been proposed for problems in which the variables have a natural ordering ( e.g. , times series ) , while `` permutation - invariant '' methods that use thresholding are useful when there is no natural variable ordering @xcite .",
    "these approaches provide consistency guarantees under various sparsity assumptions on the true covariance matrix .",
    "other techniques that have been studied include methods based on shrinkage @xcite and factor analysis @xcite .",
    "a number of papers have studied covariance estimation in the context of _ gaussian graphical model selection_. in a gaussian graphical model the _ inverse _ of the covariance matrix , also called the concentration matrix , is assumed to be sparse , and the sparsity pattern reveals the conditional independence relations satisfied by the variables .",
    "the model selection method usually studied in such a setting is @xmath0-regularized maximum - likelihood , with the @xmath0 penalty applied to the entries of the inverse covariance matrix to induce sparsity .",
    "the consistency properties of such an estimator have been studied @xcite , and under suitable conditions @xcite this estimator is also `` sparsistent '' , i.e. , the estimated concentration matrix has the same sparsity pattern as the true model from which the samples are generated .",
    "an alternative approach to @xmath0-regularized maximum - likelihood is to estimate the sparsity pattern of the concentration matrix by performing regression separately on each variable @xcite ; while such a method consistently estimates the sparsity pattern , it does not directly provide estimates of the covariance or concentration matrix .    in many applications throughout science and engineering",
    ", a challenge is that one may not have access to observations of all the relevant phenomena , i.e. , some of the relevant variables may be hidden or unobserved .",
    "such a scenario arises in data analysis tasks in psychology , computational biology , and economics . in general latent variables",
    "pose a significant difficulty for model selection because one may not know the number of relevant latent variables , nor the relationship between these variables and the observed variables .",
    "typical algorithmic methods that try to get around this difficulty usually fix the number of latent variables as well as the structural relationship between latent and observed variables ( e.g. , the graphical model structure between latent and observed variables ) , and use the em algorithm to fit parameters @xcite .",
    "this approach suffers from the problem that one optimizes non - convex functions , and thus one may get stuck in sub - optimal local minima .",
    "an alternative method that has been suggested is based on a greedy , local , combinatorial heuristic that assigns latent variables to groups of observed variables , based on some form of clustering of the observed variables @xcite ; however , this approach has no consistency guarantees .    in this paper",
    "we study the problem of latent - variable graphical model selection in the setting where all the variables , both observed and hidden , are jointly gaussian .",
    "more concretely let the covariance matrix of a finite collection of jointly gaussian random variables @xmath1 be denoted by @xmath2 , where @xmath3 are the observed variables and @xmath4 are the unobserved , hidden variables .",
    "the marginal statistics corresponding to the observed variables @xmath3 are given by the marginal covariance matrix @xmath5 , which is simply a submatrix of the full covariance matrix @xmath2 .",
    "however suppose that we parameterize our model by the concentration matrix @xmath6 , which as discussed above reveals the connection to graphical models .",
    "in such a parametrization , the _ marginal concentration matrix _ @xmath7 corresponding to the observed variables @xmath3 is given by the schur complement @xcite with respect to the block @xmath8 : @xmath9 thus if we only observe the variables @xmath3 , we only have access to @xmath5 ( or @xmath10 ) .",
    "the two terms that compose @xmath10 above have interesting properties .",
    "the matrix @xmath11 specifies the concentration matrix of the _ conditional statistics _ of the observed variables given the latent variables .",
    "if these conditional statistics are given by a sparse graphical model then @xmath11 is _",
    "sparse_. on the other hand the matrix @xmath12 serves as a _ summary _ of the effect of marginalization over the hidden variables @xmath13 .",
    "this matrix has small rank if the number of latent , unobserved variables @xmath13 is small relative to the number of observed variables @xmath14 ( the rank is equal to @xmath15 ) . therefore the marginal concentration matrix @xmath10 of the observed variables",
    "@xmath3 is generally _ not sparse _ due to the additional low - rank term @xmath12 . hence standard graphical model selection techniques applied directly to the observed variables @xmath3 are not useful .    a modeling paradigm that infers the effect of the latent variables @xmath4 would be more suitable in order to provide a simple explanation of the underlying statistical structure .",
    "hence we _ decompose _",
    "@xmath10 into the sparse and low - rank components , which reveals the conditional graphical model structure in the observed variables as well as the _ number _ of and effect due to the unobserved latent variables .",
    "such a method can be viewed as a blend of principal component analysis and graphical modeling . in standard graphical modeling",
    "one would directly approximate a concentration matrix by a sparse matrix in order to learn a sparse graphical model . on the other hand in principal component analysis",
    "the goal is to explain the statistical structure underlying a set of observations using a small number of latent variables ( i.e. , approximate a covariance matrix as a low - rank matrix ) . in our framework based on decomposing a concentration matrix , we learn a graphical model among the observed variables _ conditioned _ on a few ( additional ) latent variables . notice that in our setting these latent variables are _ not _ principal components , as the conditional statistics ( conditioned on these latent variables ) are given by a graphical model .",
    "therefore we refer to these latent variables informally as _",
    "hidden components_.    our first contribution in section  [ sec : iden ] is to address the fundamental question of _ identifiability _ of such latent - variable graphical models given the marginal statistics of only the observed variables .",
    "the critical point is that we need to tease apart the correlations induced due to marginalization over the latent variables from the conditional graphical model structure among the observed variables . as the identifiability problem is one of _ uniquely _ decomposing the sum of a sparse matrix and a low - rank matrix into the individual components , we study the algebraic varieties of sparse matrices and low - rank matrices .",
    "an important theme in this paper is the connection between the tangent spaces to these algebraic varieties and the question of identifiability .",
    "specifically let @xmath16 denote the tangent space at @xmath11 to the algebraic variety of sparse matrices , and let @xmath17 denote the tangent space at @xmath12 to the algebraic variety of low - rank matrices .",
    "then the _ statistical _ question of identifiability of @xmath11 and @xmath12 given @xmath10 is determined by the _",
    "geometric _ notion of _ transversality _ of the tangent spaces @xmath16 and @xmath17 .",
    "the study of the transversality of these tangent spaces leads us to natural conditions for identifiability .",
    "in particular we show that latent - variable models in which @xmath18 the sparse matrix @xmath11 has a small number of nonzeros per row / column , and @xmath19 the low - rank matrix @xmath12 has row / column spaces that are not closely aligned with the coordinate axes , are identifiable .",
    "these two conditions have natural statistical interpretations .",
    "the first condition ensures that there are no densely - connected subgraphs in the conditional graphical model structure among the observed variables @xmath3 given the hidden components , i.e. , that these conditional statistics are indeed specified by a sparse graphical model .",
    "such statistical relationships may otherwise be mistakenly attributed to the effect of marginalization over some latent variable .",
    "the second condition ensures that the effect of marginalization over the latent variables is `` spread out '' over many observed variables ; thus , the effect of marginalization over a latent variable is not confused with the conditional graphical model structure among the observed variables .",
    "in fact the first condition is often assumed in some papers on standard graphical model selection without latent variables ( see for example @xcite ) .",
    "we note here that question of parameter identifiability was recently studied for models with discrete - valued latent variables ( i.e. , mixture models , hidden markov models ) @xcite .",
    "however , this work is not applicable to our setting in which both the latent and observed variables are assumed to be jointly gaussian .    as our next contribution",
    "we propose a _",
    "regularized maximum - likelihood decomposition _",
    "framework to approximate a given sample covariance matrix by a model in which the concentration matrix decomposes into a sparse matrix and a low - rank matrix .",
    "a number of papers over the last several years have suggested that heuristics based on using the @xmath0 norm are very effective for recovering sparse models @xcite .",
    "indeed such heuristics have been effectively used , as described above , for model selection when the goal is to estimate sparse concentration matrices . in her thesis",
    "@xcite fazel suggested a convex heuristic based on the nuclear norm for rank - minimization problems in order to recover low - rank matrices .",
    "this method generalized the previously studied trace heuristic for recovering low - rank positive semidefinite matrices .",
    "recently several conditions have been given under which these heuristics provably recover low - rank matrices in various settings @xcite .",
    "motivated by the success of these heuristics , we propose the following penalized likelihood method given a sample covariance matrix @xmath20 formed from @xmath21 samples of the observed variables : @xmath22 here @xmath23 represents the gaussian log - likelihood function and is given by @xmath24 for @xmath25 , where @xmath26 is the trace of a matrix and @xmath27 is the determinant .",
    "the matrix @xmath28 provides an estimate of @xmath11 , which represents the conditional concentration matrix of the observed variables ; the matrix @xmath29 provides an estimate of @xmath12 , which represents the effect of marginalization over the latent variables .",
    "notice that the regularization function is a combination of the @xmath0 norm applied to @xmath30 and the nuclear norm applied to @xmath31 ( the nuclear norm reduces to the trace over the cone of symmetric , positive - semidefinite matrices ) , with @xmath32 providing a tradeoff between the two terms .",
    "this variational formulation is a _ convex optimization _ problem .",
    "in particular it is a regularized max - det problem and can be solved in polynomial time using standard off - the - shelf solvers @xcite .",
    "our main result in section  [ sec : main ] is a proof of the consistency of the estimator in the high - dimensional regime in which both the number of observed variables and the number of hidden components are allowed to grow with the number of samples ( of the observed variables ) .",
    "we show that for a suitable choice of the regularization parameter @xmath33 , there exists a range of values of @xmath32 for which the estimates @xmath34 have the same sparsity ( and sign ) pattern and rank as @xmath35 with high probability ( see theorem  [ theo : main ] ) .",
    "the key technical requirement is an identifiability condition for the two components of the marginal concentration matrix @xmath10 with respect to the fisher information ( see section  [ subsec : fi ] ) .",
    "we make connections between our condition and the irrepresentability conditions required for support / graphical - model recovery using @xmath0 regularization @xcite .",
    "our results provide numerous scaling regimes under which consistency holds in latent - variable graphical model selection .",
    "for example we show that under suitable identifiability conditions consistent model selection is possible even when the number of samples and the number of latent variables are on the same order as the number of observed variables ( see section  [ subsec : scal ] ) .",
    "[ [ related - previous - work ] ] related previous work + + + + + + + + + + + + + + + + + + + + +    the problem of decomposing the sum of a sparse matrix and a low - rank matrix , with no additional noise , into the individual components was initially studied in @xcite by a superset of the authors of the present paper .",
    "specifically this work proposed a convex program using a combination of the @xmath0 norm and the nuclear norm to recover the sparse and low - rank components , and derived conditions under which the convex program exactly recovers these components . in subsequent work cands et al .",
    "@xcite also studied this noise - free sparse - plus - low - rank decomposition problem , and provided guarantees for exact recovery using the convex program proposed in @xcite .",
    "the problem setup considered in the present paper is quite different and is more challenging because we are only given access to an inexact sample covariance matrix , and we are interested in recovering components that preserve both the sparsity pattern and the rank of the components in the true underlying model .",
    "in addition to proving such a consistency result for the estimator , we also provide a statistical interpretation of our identifiability conditions and describe natural classes of latent - variable gaussian graphical models that satisfy these conditions . as such",
    "our paper is closer in spirit to the many recent papers on covariance selection , but with the important difference that some of the variables are not observed .",
    "[ [ outline ] ] outline + + + + + + +    section  [ sec : bg ] gives some background on graphical models as well as the algebraic varieties of sparse and low - rank matrices .",
    "it also provides a formal statement of the problem .",
    "section  [ sec : iden ] discusses conditions under which latent - variable models are identifiable , and section  [ sec : main ] states the main results of this paper .",
    "we provide experimental demonstration of the effectiveness of our estimator on synthetic and real data in section  [ sec : sims ] .",
    "section  [ sec : conc ] concludes the paper with a brief discussion .",
    "the appendices include additional details and proofs of all of our technical results .",
    "we briefly discuss concepts from graphical modeling and give a formal statement of the latent - variable model selection problem .",
    "we also describe various properties of the algebraic varieties of sparse matrices and of low - rank matrices .",
    "the following matrix norms are employed throughout this paper :    * @xmath36 : denotes the spectral norm , which is the largest singular value of @xmath37 . *",
    "@xmath38 : denotes the largest entry in magnitude of @xmath37 . *",
    "@xmath39 : denotes the frobenius norm , which is the square - root of the sum of the squares of the entries of @xmath37 .",
    "* @xmath40 : denotes the nuclear norm , which is the sum of the singular values of @xmath37 .",
    "this reduces to the trace for positive - semidefinite matrices . *",
    "@xmath41 : denotes the sum of the absolute values of the entries of @xmath37 .    a number of _ matrix operator _",
    "norms are also used .",
    "for example , let @xmath42 be a linear operator acting on matrices",
    ". then the induced operator norm @xmath43 is defined as : @xmath44 therefore , @xmath45 denotes the spectral norm of the matrix operator @xmath46 . the only vector norm used is the euclidean norm , which is denoted by @xmath47 .",
    "a graphical model @xcite is a statistical model defined with respect to a graph @xmath48 in which the nodes index a collection of random variables @xmath49 , and the edges represent the conditional independence relations ( markov structure ) among the variables .",
    "the absence of an edge between nodes @xmath50 implies that the variables @xmath51 are independent conditioned on all the other variables .",
    "a _ gaussian graphical model _ ( also commonly referred to as a gauss - markov random field ) is one in which all the variables are jointly gaussian @xcite . in such models the sparsity pattern of the inverse of the covariance matrix , or the _ concentration _ matrix , directly corresponds to the graphical model structure .",
    "specifically , consider a gaussian graphical model in which the covariance matrix is given by @xmath52 and the concentration matrix is given by @xmath53 .",
    "then an edge @xmath54 is present in the underlying graphical model if and only if @xmath55 .",
    "our focus in this paper is on gaussian models in which some of the variables may not be observed .",
    "suppose @xmath14 represents the set of nodes corresponding to observed variables @xmath3 , and @xmath13 the set of nodes corresponding to unobserved , hidden variables @xmath4 with @xmath56 and @xmath57 .",
    "the joint covariance is denoted by @xmath58 , and joint concentration matrix by @xmath59 .",
    "the submatrix @xmath5 represents the marginal covariance of the observed variables @xmath3 , and the corresponding marginal concentration matrix is given by the schur complement with respect to the block @xmath8 : @xmath60 the submatrix @xmath11 specifies the concentration matrix of the conditional statistics of the observed variables conditioned on the hidden components .",
    "if these conditional statistics are given by a sparse graphical model then @xmath11 is sparse . on the other hand",
    "the marginal concentration matrix @xmath10 of the marginal distribution of @xmath3 is _ not _ sparse in general due to the extra correlations induced from marginalization over the latent variables @xmath4 , i.e. , due to the presence of the additional term @xmath12 .",
    "hence , standard graphical model selection techniques in which the goal is to approximate a sample covariance by a sparse graphical model are not well - suited for problems in which some of the variables are hidden . however , the matrix @xmath12 is a low - rank matrix if the number of hidden variables is much smaller than the number of observed variables ( i.e. , @xmath61 ) .",
    "therefore , a more appropriate model selection method is to approximate the sample covariance by a model in which the concentration matrix decomposes into the sum of a sparse matrix and a low - rank matrix .",
    "the objective here is to learn a sparse graphical model among the observed variables _ conditioned _ on some latent variables , as such a model explicitly accounts for the extra correlations induced due to unobserved , hidden components .      in order to analyze latent - variable model selection methods ,",
    "we need to define an appropriate notion of model selection consistency for latent - variable graphical models .",
    "notice that given the two components @xmath11 and @xmath12 of the concentration matrix of the marginal distribution , there are _ infinitely _ many configurations of the latent variables ( i.e. , matrices @xmath62 ) that give rise to the _ same _ low - rank matrix @xmath12 . specifically for any non - singular matrix @xmath63 , one can apply the transformations @xmath64 and still preserve the low - rank matrix @xmath12 . in _ all",
    "_ of these models the marginal statistics of the observed variables @xmath3 remain the same upon marginalization over the latent variables @xmath4 . the key _",
    "invariant _ is the low - rank matrix @xmath12 , which _ summarizes _ the effect of marginalization over the latent variables .",
    "these observations give rise to the following notion of consistency :    a pair of ( symmetric ) matrices @xmath65 with @xmath66 is an _ algebraically consistent _ estimate of a latent - variable gaussian graphical model given by the concentration matrix @xmath67 if the following conditions hold :    1 .",
    "the sign - pattern of @xmath30 is the same as that of @xmath11 : @xmath68 here we assume that @xmath69 .",
    "the rank of @xmath31 is the same as the rank of @xmath12 : @xmath70 3 .",
    "the concentration matrix @xmath71 can be realized as the marginal concentration matrix of an appropriate latent - variable model : @xmath72    the first condition ensures that @xmath30 provides the correct structural estimate of the conditional graphical model ( given by @xmath11 ) of the observed variables conditioned on the hidden components .",
    "this property is the same as the `` sparsistency '' property studied in standard graphical model selection @xcite .",
    "the second condition ensures that the number of hidden components is correctly estimated .",
    "finally , the third condition ensures that the pair of matrices @xmath65 leads to a realizable latent - variable model . in particular",
    "this condition implies that there exists a valid latent - variable model on @xmath73 variables in which @xmath74 the conditional graphical model structure among the observed variables is given by @xmath30 , @xmath75 the number of latent variables @xmath15 is equal to the rank of @xmath31 , and @xmath76 the extra correlations induced due to marginalization over the latent variables is equal to @xmath31 . any method for matrix factorization",
    "( see for example , @xcite ) can be used to factorize the low - rank matrix @xmath31 , depending on the properties that one desires in the factors ( e.g. , sparsity ) .",
    "we also study parametric consistency in the usual sense , i.e. , we show that one can produce estimates @xmath65 that converge in various norms to the matrices @xmath77 .",
    "notice that proving @xmath65 is close to @xmath77 in some norm does not in general imply that the support / sign - pattern and rank of @xmath65 are the same as those of @xmath77 .",
    "therefore parametric consistency is different from algebraic consistency , which requires that @xmath65 have the same support / sign - pattern and rank as @xmath77 .",
    "[ [ goal ] ] goal + + + +    let @xmath78 denote the concentration matrix of a gaussian model .",
    "suppose that we have @xmath21 samples @xmath79 of the observed variables @xmath3 .",
    "we would like to produce estimates @xmath34 that , with high - probability , are both algebraically consistent and parametrically consistent ( in some norm ) .",
    "given @xmath21 samples @xmath80 of a finite collection of jointly gaussian zero - mean random variables with concentration matrix @xmath81 , we define the sample covariance as follows : @xmath82 it is then easily seen that the log - likelihood function is given by : @xmath83 where @xmath84 is a function of @xmath85 .",
    "notice that this function is strictly concave for @xmath25 .",
    "now consider the latent - variable modeling problem in which we wish to model a collection of random variables @xmath3 ( with sample covariance @xmath86 ) by adding some extra variables @xmath4 .",
    "with respect to the parametrization @xmath65 ( with @xmath30 representing the conditional statistics of @xmath3 given @xmath4 , and @xmath31 summarizing the effect of marginalization over the additional variables @xmath4 ) , the likelihood function is given by : @xmath87 the function @xmath88 is _ jointly concave _ with respect to the parameters @xmath65 whenever @xmath89 , and it is this function that we use in our variational formulation to learn a latent - variable model .    in the analysis of a convex program involving the likelihood function",
    ", the fisher information plays an important role as it is the negative of the hessian of the likelihood function and thus controls the curvature .",
    "as the first term in the likelihood function is linear , we need only study higher - order derivatives of the log - determinant function in order to compute the hessian . letting @xmath90 denote the fisher information matrix",
    ", we have that @xcite @xmath91 for @xmath92 .",
    "if @xmath81 is a @xmath93 concentration matrix , then the fisher information matrix @xmath94 has dimensions @xmath95 .",
    "next consider the latent - variable situation with the variables indexed by @xmath14 being observed and the variables indexed by @xmath13 being hidden .",
    "the concentration matrix @xmath96 of the marginal distribution of the observed variables @xmath14 is given by the schur complement , and the corresponding fisher information matrix is given by @xmath97 notice that this is precisely the @xmath98 submatrix of the full fisher information matrix @xmath99 with respect to all the parameters @xmath100 ( corresponding to the situation in which _ all _ the variables @xmath101 are observed ) .",
    "the matrix @xmath102 has dimensions @xmath103 , while @xmath104 is an @xmath98 matrix . to summarize",
    ", we have for all @xmath105 that : @xmath106_{(i , j),(k , l ) } = { \\mathcal{i}}(k^\\ast_{(o~h)})_{(i , j),(k , l)}.\\ ] ] in section  [ subsec : fi ] we impose various conditions on the fisher information matrix @xmath104 under which our regularized maximum - likelihood formulation provides consistent estimates with high probability .      an algebraic variety is the solution set of a system of polynomial equations .",
    "the set of sparse matrices and the set of low - rank matrices can be naturally viewed as algebraic varieties . here",
    "we describe these varieties , and discuss some of their properties .",
    "of particular interest in this paper are geometric properties of these varieties such as the tangent space and local curvature at a ( smooth ) point .",
    "let @xmath107 denote the set of matrices with at most @xmath108 nonzeros : @xmath109 the set @xmath107 is an algebraic variety , and can in fact be viewed as a union of @xmath110 subspaces in @xmath111 .",
    "this variety has dimension @xmath108 , and it is smooth everywhere except at those matrices that have support size strictly smaller than @xmath108 . for any matrix @xmath112 ,",
    "consider the variety @xmath113 ; @xmath37 is a smooth point of this variety , and the tangent space at @xmath37 is given by @xmath114 in words the tangent space @xmath115 at a smooth point @xmath37 is given by the set of all matrices that have support contained within the support of @xmath37 .",
    "we view @xmath115 as a subspace in @xmath111 .",
    "next let @xmath116 denote the algebraic variety of matrices with rank at most @xmath117 : @xmath118 it is easily seen that @xmath116 is an algebraic variety because it can be defined through the vanishing of all @xmath119 minors .",
    "this variety has dimension equal to @xmath120 , and it is smooth everywhere except at those matrices that have rank strictly smaller than @xmath117 . consider a rank-@xmath117 matrix @xmath37 with svd given by @xmath121 , where @xmath122 and @xmath123 .",
    "the matrix @xmath37 is a smooth point of the variety @xmath124 , and the tangent space at @xmath37 with respect to this variety is given by @xmath125 in words the tangent space @xmath126 at a smooth point @xmath37 is the span of all matrices that have either the same row - space as @xmath37 or the same column - space as @xmath37 . as with @xmath115",
    "we view @xmath126 as a subspace in @xmath111 .    in section  [ sec : iden ]",
    "we explore the connection between geometric properties of these tangent spaces and the identifiability problem in latent - variable graphical models .",
    "the sparse matrix variety @xmath107 has the property that it has _",
    "zero _ curvature at any smooth point .",
    "consequently the tangent space at a smooth point @xmath37 is the _ same _ as the tangent space at any point in a neighborhood of @xmath37 .",
    "this property is implicitly used in the analysis of @xmath0 regularized methods for recovering sparse models .",
    "the situation is more complicated for the low - rank matrix variety , because the curvature at any smooth point is nonzero .",
    "therefore we need to study how the tangent space changes from one point to a neighboring point by analyzing how this variety curves locally .",
    "indeed the amount of curvature at a point is directly related to the `` angle '' between the tangent space at that point and the tangent space at a neighboring point . for any subspace @xmath127 of matrices ,",
    "let @xmath128 denote the projection onto @xmath127 .",
    "given two subspaces @xmath129 of the same dimension , we measure the `` twisting '' between these subspaces by considering the following quantity .",
    "@xmath130 ( n)\\|_2 .",
    "\\label{eq : rho}\\ ] ]    in appendix  [ app : matper ] we briefly review relevant results from matrix perturbation theory ; the key tool used to derive these results is the resolvent of a matrix @xcite . based on these tools we prove the following two results in appendix  [ app : rankcurv ] , which bound the twisting between the tangent spaces at nearby points .",
    "the first result provides a bound on the quantity @xmath131 between the tangent spaces at a point and at its neighbor .",
    "[ theo : tspace ] let @xmath112 be a rank-@xmath117 matrix with smallest nonzero singular value equal to @xmath132 , and let @xmath133 be a perturbation to @xmath37 such that @xmath134 .",
    "further , let @xmath135 be a rank-@xmath117 matrix .",
    "then we have that @xmath136    the next result bounds the error between a point and its neighbor in the normal direction .",
    "[ theo : nspace ] let @xmath112 be a rank-@xmath117 matrix with smallest nonzero singular value equal to @xmath132 , and let @xmath133 be a perturbation to @xmath37 such that @xmath137 .",
    "further , let @xmath135 be a rank-@xmath117 matrix",
    ". then we have that @xmath138    these results suggest that the closer the smallest singular value is to zero , the more curved the variety is locally .",
    "therefore we control the twisting between tangent spaces at nearby points by bounding the smallest nonzero singular value away from zero .",
    "in the absence of additional conditions , the latent - variable model selection problem is ill - posed . in this section",
    "we discuss a set of conditions on latent - variable models that ensure that these models are identifiable given marginal statistics for a subset of the variables .",
    "suppose that the low - rank matrix that summarizes the effect of the hidden components is itself sparse .",
    "this leads to identifiability issues in the sparse - plus - low - rank decomposition problem .",
    "statistically the additional correlations induced due to marginalization over the latent variables could be mistaken for the conditional graphical model structure of the observed variables . in order to avoid such identifiability problems the effect of the latent variables must be `` diffuse '' across the observed variables . to address this point",
    "the following quantity was introduced in @xcite for any matrix @xmath37 , defined with respect to the tangent space @xmath126 : @xmath139 thus @xmath140 being small implies that elements of the tangent space @xmath126 can not have their support concentrated in a few locations ; as a result @xmath37 can not be too sparse .",
    "this idea is formalized in @xcite by relating @xmath140 to a notion of `` incoherence '' of the row / column spaces , where the row / column spaces are said to be incoherent with respect to the standard basis if these spaces are not aligned closely with any of the coordinate axes .",
    "letting @xmath121 be the singular value decomposition of @xmath37 , the incoherence of the row / column spaces of @xmath37 ( initially proposed and studied by cands and recht @xcite ) is defined as : @xmath141 here @xmath142 denote projections , and projections onto matrix subspaces ( defined by a general linear operator ) by the calligraphic @xmath143 . ] onto the row / column spaces of @xmath37 , and @xmath144 is the @xmath145th standard basis vector . hence @xmath146 measures the projection of the most `` closely aligned '' coordinate axis with the row / column spaces . for any rank-@xmath117 matrix m we have that @xmath147 where the lower bound is achieved ( for example ) if the row / column spaces span any @xmath117 columns of a @xmath93 orthonormal hadamard matrix , while the upper bound is achieved if the row or column space contains a standard basis vector .",
    "typically a matrix m with incoherent row / column spaces would have @xmath148 .",
    "the following result ( proved in @xcite ) shows that the more incoherent the row / column spaces of @xmath37 , the smaller is @xmath149 .",
    "[ theo : xiinc ] for any @xmath112 , we have that @xmath150 where @xmath140 and @xmath146 are defined in and .",
    "based on these concepts we roughly require that the low - rank matrix that summarizes the effect of the latent variables be _ incoherent _ , thereby ensuring that the extra correlations due to marginalization over the hidden components can not be confused with the conditional graphical model structure of the observed variables .",
    "notice that the quantity @xmath151 is not just a measure of the number of latent variables , but also of the overall effect of the correlations induced by marginalization over these variables .",
    "* curvature and change in @xmath152 * : as noted previously an important technical point is that the algebraic variety of low - rank matrices is locally curved at any smooth point",
    ". consequently the quantity @xmath152 changes as we move along the low - rank matrix variety smoothly .",
    "the quantity @xmath153 introduced in also allows us to bound the variation in @xmath152 as follows .",
    "[ theo : rhotspace ] let @xmath129 be two matrix subspaces of the same dimension with the property that @xmath154 , where @xmath131 is defined in .",
    "then we have that @xmath155.\\ ] ]    this lemma is proved in appendix  [ app : rankcurv ] .",
    "an identifiability problem also arises if the conditional graphical model among the observed variables contains a densely connected subgraph .",
    "these statistical relationships might be mistaken as correlations induced by marginalization over latent variables .",
    "therefore we need to ensure that the conditional graphical model among the observed variables is sparse .",
    "we impose the condition that this conditional graphical model must have small `` degree '' , i.e. , no observed variable is directly connected to too many other observed variables conditioned on the hidden components .",
    "notice that bounding the degree is a more refined condition than simply bounding the total number of nonzeros as the _ sparsity pattern _ also plays a role . in @xcite",
    "the authors introduced the following quantity in order to provide an appropriate measure of the sparsity pattern of a matrix : @xmath156 the quantity @xmath157 being small for a matrix implies that the spectrum of any element of the tangent space @xmath115 is not too `` concentrated '' , i.e. , the singular values of the elements of the tangent space are not too large . in @xcite",
    "it is shown that a sparse matrix @xmath37 with `` bounded degree '' ( a small number of nonzeros per row / column ) has small @xmath158 .",
    "[ theo : mudeg ] let @xmath112 be any matrix with at most @xmath159 nonzero entries per row / column , and with at least @xmath160 nonzero entries per row / column . with @xmath157",
    "as defined in , we have that @xmath161      suppose that we have the sum of two vectors , each from two known subspaces .",
    "it is possible to uniquely recover the individual vectors from the sum if and only if the subspaces have a transverse intersection , i.e. , they only intersect at the origin . this simple observation leads to an appealing algebraic notion of identifiability .",
    "consider the situation in which we have the sum of a sparse matrix and a low - rank matrix .",
    "in addition to this sum , suppose that we are also given the tangent spaces at these matrices with respect to the algebraic varieties of sparse and low - rank matrices respectively .",
    "then a necessary and sufficient condition for _ local _ identifiability is that these tangent spaces have a transverse intersection .",
    "it turns out that these transversality conditions on the tangent spaces are also sufficient for the regularized maximum - likelihood convex program to provide consistent estimates of the number of hidden components and the conditional graphical model structure of the observed variables conditioned on the latent variables ( without any side information about the tangent spaces ) .    in order to quantify the level of transversality between the tangent spaces @xmath162 and",
    "@xmath127 we study the _ minimum gain _ with respect to some norm of the addition operator restricted to the cartesian product @xmath163 .",
    "more concretely let @xmath164 represent the addition operator , i.e. , the operator that adds two matrices .",
    "then given any matrix norm @xmath165 on @xmath166 , the minimum gain of @xmath167 restricted to @xmath168 is defined as follows : @xmath169 where @xmath170 denotes the projection onto the space @xmath168 , and @xmath171 denotes the adjoint of the addition operator ( with respect to the standard euclidean inner - product ) .",
    "the tangent spaces @xmath162 and @xmath127 have a _ transverse _ intersection if and only if @xmath172 . the `` level '' of transversality is measured by the magnitude of @xmath173 .",
    "note that if the norm @xmath165 used is the frobenius norm , then @xmath174 is the square of the _ minimum singular value _ of the addition operator @xmath167 restricted to @xmath175 .",
    "a natural norm with which to measure transversality is the dual norm of the regularization function in , as the subdifferential of the regularization function is specified in terms of its dual . the reasons for this will become clearer as we proceed through this paper .",
    "recall that the regularization function used in the variational formulation is given by : @xmath176 where the nuclear norm @xmath177 reduces to the trace function over the cone of positive - semidefinite matrices .",
    "this function is a norm for all @xmath178 .",
    "the dual norm of @xmath179 is given by @xmath180 the following simple lemma records a useful property of the @xmath181 norm that is used several times throughout this paper .",
    "[ theo : gg ] let @xmath162 and @xmath127 be tangent spaces at any points with respect to the algebraic varieties of sparse and low - rank matrices . then for any matrix @xmath37",
    ", we have that @xmath182 and that @xmath183 .",
    "further we also have that @xmath184 and that @xmath185 .",
    "thus for any matrices @xmath186 and for @xmath163 , one can check that @xmath187 and that @xmath188 .",
    "next we define the quantity @xmath189 as follows in order to study the transversality of the spaces @xmath162 and @xmath127 with respect to the @xmath181 norm : @xmath190 here @xmath191 and @xmath152 are defined in and .",
    "we then have the following result ( proved in appendix  [ app : rg ] ) :    [ theo : rg1 ] let @xmath192 be matrices such that @xmath193 and let @xmath194 .",
    "then we have that @xmath195 $ ] , where @xmath163 and @xmath189 is defined in . in particular",
    "we have that @xmath196 .",
    "the quantity @xmath189 being small implies that the addition operator is essentially isometric when restricted to @xmath163 .",
    "stated differently the magnitude of @xmath189 is a measure of the level of transversality of the spaces @xmath162 and @xmath127 .",
    "if @xmath197 then @xmath198 ensures that @xmath199 , which in turn implies that the tangent spaces @xmath162 and @xmath127 have a transverse intersection .    * observation * : thus we have that the smaller the quantities @xmath200 and @xmath201 , the more transverse the intersection of the spaces @xmath162 and @xmath127 .      the main focus of section  [ sec : main ] is to analyze the regularized maximum - likelihood convex program by studying its optimality conditions .",
    "the log - likelihood function is well - approximated in a neighborhood by a quadratic form given by the fisher information ( which measures the curvature , as discussed in section  [ subsec : ll ] ) .",
    "let @xmath202 denote the fisher information evaluated at the true marginal concentration matrix @xmath203 , where @xmath204 represents the concentration matrix of the full model ( see equation ) .",
    "the appropriate measure of transversality between the tangent spaces @xmath205 and @xmath206 is then in a space in which the inner - product is given by @xmath207 .",
    "specifically , we need to analyze the minimum gain of the operator @xmath208 restricted to the space @xmath163 .",
    "therefore we impose several conditions on the fisher information @xmath207 .",
    "we define quantities that control the gains of @xmath207 restricted to @xmath162 and @xmath127 separately ; these ensure that elements of @xmath162 and elements of @xmath127 are individually identifiable under the map @xmath207 .",
    "in addition we define quantities that , in conjunction with bounds on @xmath200 and @xmath201 , allow us to control the gain of @xmath207 restricted to the direct - sum @xmath209 .",
    "* @xmath207 restricted to @xmath162 * : the minimum gain of the operator @xmath210 restricted to @xmath162 is given by @xmath211 the maximum effect of elements in @xmath162 in the orthogonal direction @xmath212 is given by @xmath213 the operator @xmath207 is injective on @xmath162 if @xmath214 . the ratio @xmath215 implies the irrepresentability condition imposed in @xcite , which gives a sufficient condition for consistent recovery of graphical model structure using @xmath0-regularized maximum - likelihood .",
    "notice that this condition is a generalization of the usual lasso irrepresentability conditions @xcite , which are typically imposed on the covariance matrix .",
    "finally we also consider the following quantity , which controls the behavior of @xmath207 restricted to @xmath162 in the spectral norm : @xmath216    * @xmath207 restricted to @xmath127 * : analogous to the case of @xmath162 one could control the gains of the operators @xmath217 and @xmath218 .",
    "however as discussed previously one complication is that the tangent spaces at nearby smooth points on the rank variety are in general different , and the amount of twisting between these spaces is governed by the local curvature",
    ". therefore we control the gains of the operators @xmath219 and @xmath220 for all tangent spaces @xmath221 that are `` close to '' the nominal @xmath127 ( at the true underlying low - rank matrix ) , measured by @xmath222 being small .",
    "the minimum gain of the operator @xmath220 restricted to @xmath221 ( close to @xmath127 ) is given by @xmath223 similarly the maximum effect of elements in @xmath221 in the orthogonal direction @xmath224 ( for @xmath221 close to @xmath127 ) is given by @xmath225 implicit in the definition of @xmath226 and @xmath227 is the fact that the outer minimum and maximum are only taken over spaces @xmath221 that are tangent spaces to the rank - variety .",
    "the operator @xmath207 is injective on all tangent spaces @xmath221 such that @xmath228 if @xmath229 . an irrepresentability condition ( analogous to those developed for the sparse case ) for tangent spaces near @xmath127 to the rank variety would be that @xmath230 .",
    "finally we also control the behavior of @xmath207 restricted to @xmath221 close to @xmath127 in the @xmath231 norm : @xmath232    the two sets of quantities @xmath233 and @xmath234 essentially control how @xmath207 behaves when restricted to the spaces @xmath162 and @xmath127 _ separately _ ( in the natural norms ) .",
    "the quantities @xmath235 and @xmath236 are useful in order to control the gains of the operator @xmath207 restricted to the _ direct sum _ @xmath209 .",
    "notice that although the magnitudes of elements in @xmath162 are measured most naturally in the @xmath231 norm , the quantity @xmath235 is specified with respect to the spectral norm .",
    "similarly elements of the tangent spaces @xmath221 to the rank variety are most naturally measured in the spectral norm , but @xmath236 provides control in the @xmath231 norm .",
    "these quantities , combined with @xmath200 and @xmath201 ( defined in and ) , provide the `` coupling '' necessary to control the behavior of @xmath207 restricted to elements in the direct sum @xmath209 . in order to keep track of fewer quantities ,",
    "we summarize the six quantities as follows : @xmath237    * main assumption * there exists a @xmath238 $ ] such that : @xmath239    this assumption is to be viewed as a generalization of the irrepresentability conditions imposed on the covariance matrix @xcite or the fisher information matrix @xcite in order to provide consistency guarantees for sparse model selection using the @xmath0 norm . with this assumption we have the following proposition , proved in appendix  [ app : rg ] , about the gains of the operator @xmath207 restricted to @xmath209 .",
    "this proposition plays a fundamental role in the analysis of the performance of the regularized maximum - likelihood procedure .",
    "[ theo : irr ] let @xmath162 and @xmath127 be the tangent spaces defined in this section , and let @xmath207 be the fisher information evaluated at the true marginal concentration matrix .",
    "further let @xmath240 be as defined above .",
    "suppose that @xmath241 and that @xmath32 is in the following range : @xmath242.\\ ] ] then we have the following two conclusions for @xmath243 with @xmath244 :    1 .",
    "the minimum gain of @xmath207 restricted to @xmath245 is bounded below : @xmath246 specifically this implies that for all @xmath247 @xmath248 2 .",
    "the effect of elements in @xmath243 on the orthogonal complement @xmath249 is bounded above : @xmath250 specifically this implies that for all @xmath247 @xmath251    the last quantity we consider is the spectral norm of the marginal covariance matrix @xmath252 : @xmath253 a bound on @xmath254 is useful in the probabilistic component of our analysis , in order to derive convergence rates of the sample covariance matrix to the true covariance matrix .",
    "we also observe that @xmath255",
    "let @xmath204 denote the full concentration matrix of a collection of zero - mean jointly - gaussian observed and latent variables , let @xmath256 denote the number of observed variables , and let @xmath257 denote the number of latent variables .",
    "we are given @xmath21 samples @xmath258 of the observed variables @xmath3 .",
    "we consider the high - dimensional setting in which @xmath259 are all allowed to grow simultaneously .",
    "the quantities @xmath260 defined in the previous section are accounted for in our analysis , although we suppress the dependence on these quantities in the statement of our main result .",
    "we explicitly keep track of the quantities @xmath261 and @xmath262 as these control the complexity of the latent - variable model given by @xmath204 .",
    "in particular @xmath191 controls the sparsity of the conditional graphical model among the observed variables , while @xmath152 controls the incoherence or `` diffusivity '' of the extra correlations induced due to marginalization over the hidden variables .",
    "based on the tradeoff between these two quantities , we obtain a number of classes of latent - variable graphical models ( and corresponding scalings of @xmath259 ) that can be consistently recovered using the regularized maximum - likelihood convex program ( see section  [ subsec : scal ] for details ) . specifically we show that consistent model selection is possible even when the number of samples and the number of latent variables are on the same order as the number of observed variables .",
    "we present our main result next demonstrating the consistency of the estimator , and then discuss classes of latent - variable graphical models and various scaling regimes in which our estimator is consistent .",
    "[ theo : main ] let @xmath78 denote the concentration matrix of a gaussian model .",
    "we have @xmath21 samples @xmath79 of the @xmath272 observed variables denoted by @xmath14 .",
    "let @xmath273 and @xmath274 denote the tangent spaces at @xmath275 and at @xmath276 with respect to the sparse and low - rank matrix varieties respectively .",
    "* assumptions * : suppose that the following conditions hold :    1 .",
    "the quantities @xmath200 and @xmath201 satisfy the assumption of proposition  [ theo : irr ] for identifiability , and @xmath32 is chosen in the range specified by proposition  [ theo : irr ] .",
    "the number of samples @xmath21 available is such that @xmath277 3 .",
    "the regularization parameter @xmath33 is chosen as @xmath278 4 .",
    "the minimum nonzero singular value @xmath132 of @xmath276 is bounded as @xmath279 5 .",
    "the minimum magnitude nonzero entry @xmath280 of @xmath275 is bounded as @xmath281    * conclusions * : then with probability greater than @xmath282 we have :    1 .",
    "algebraic consistency : the estimate @xmath34 given by the convex program is algebraically consistent , i.e. , the support and sign pattern of @xmath28 is the same as that of @xmath275 , and the rank of @xmath29 is the same as that of @xmath276 .",
    "2 .   parametric consistency : the estimate @xmath34 given by the convex program is parametrically consistent : @xmath283    the proof of this theorem is given in appendix  [ app : main ] .",
    "the theorem essentially states that if the minimum nonzero singular value of the low - rank piece @xmath267 and minimum nonzero entry of the sparse piece @xmath275 are bounded away from zero , then the convex program provides estimates that are both algebraically consistent and parametrically consistent ( in the @xmath231 and spectral norms ) . in section",
    "[ subsec : cov ] we also show that these results easily lead to parametric consistency rates for the corresponding estimate @xmath284 of the marginal covariance @xmath285 of the observed variables .",
    "notice that the condition on the minimum singular value of @xmath267 is more stringent than on the minimum nonzero entry of @xmath275 .",
    "one role played by these conditions is to ensure that the estimates @xmath34 do not have smaller support size / rank than @xmath286 .",
    "however the minimum singular value bound plays the additional role of bounding the curvature of the low - rank matrix variety around the point @xmath267 , which is the reason for this condition being more stringent .",
    "notice also that the number of hidden variables @xmath287 does not explicitly appear in the bounds in theorem  [ theo : main ] , which only depend on @xmath288 .",
    "however the dependence on @xmath287 is implicit in the dependence on @xmath262 , and we discuss this point in greater detail in the following section .",
    "finally we note that algebraic and parametric consistency hold under the assumptions of theorem  [ theo : main ] for a _ range _ of values of @xmath32 : @xmath242.\\ ] ] in particular the assumptions on the sample complexity , the minimum nonzero singular value of @xmath267 , and the minimum magnitude nonzero entry of @xmath275 are governed by the lower end of this range for @xmath32 .",
    "these assumptions can be weakened if we only require consistency for a smaller range of values of @xmath32 .",
    "the following corollary conveys this point with a specific example :    [ theo : maincorl ] consider the same setup and notation as in theorem  [ theo : main ] .",
    "suppose that the quantities @xmath200 and @xmath201 satisfy the assumption of proposition  [ theo : irr ] for identifiability .",
    "suppose that we make the following assumptions :    1 .",
    "let @xmath32 be chosen to be equal to @xmath289 ( the upper end of the range specified in proposition  [ theo : irr ] ) , i.e. , @xmath290 .",
    "2 .   @xmath291 .",
    "3 .   @xmath292 .",
    "4 .   @xmath293 .",
    "5 .   @xmath294 .    then with probability greater than @xmath282 we have estimates @xmath34 that are algebraically consistent , and parametrically consistent with the error bounded as @xmath295    the proof of this corollary",
    ", one can further remove the factor of @xmath201 in the lower bound for @xmath132 .",
    "specifically the lower bound @xmath296 suffices for consistent estimation if @xmath297 bound the minimum / maximum gains of @xmath207 for _ all _ matrices ( rather than just those near @xmath127 ) , and @xmath227 bounds the @xmath207-inner - product for _ all _ pairs of orthogonal matrices ( rather than just those near @xmath127 and @xmath298 ) . ] is analogous to that of theorem  [ theo : main ] .",
    "we emphasize that in practice it is often beneficial to have consistent estimates for a range of values of @xmath32 ( as in theorem  [ theo : main ] ) .",
    "specifically the stability of the sparsity pattern and rank of the estimates @xmath34 for a range of tradeoff parameters is useful in order to choose a suitable value of @xmath32 , as prior information about the quantities @xmath261 and @xmath262 is not typically available ( see section  [ sec : sims ] ) .",
    "next we consider classes of latent - variable models that satisfy the conditions of theorem  [ theo : main ] .",
    "recall that @xmath21 denotes the number of samples , @xmath272 denotes the number of observed variables , and @xmath287 denotes the number of latent variables . recall the assumption that the quantities @xmath260 defined in section  [ subsec : fi ] remain bounded , and do not scale with the other parameters such as @xmath259 or @xmath262 or @xmath261 .",
    "in particular we focus on the tradeoff between @xmath262 and @xmath261 ( the quantities that control the complexity of a latent - variable graphical model ) , and the resulting scaling regimes for consistent estimation .",
    "let @xmath299 denote the degree of the conditional graphical model among the observed variables , and let @xmath300 denote the incoherence of the correlations induced due to marginalization over the latent variables ( we suppress the dependence on @xmath21 ) .",
    "these quantities are defined in section  [ sec : iden ] , and we have from propositions  [ theo : xiinc ] and [ theo : mudeg ] that @xmath301 since @xmath260 are assumed to be bounded , we also have from proposition  [ theo : irr ] that the product of @xmath191 and @xmath152 must be bounded by a constant .",
    "thus , we study latent - variable models in which @xmath302 as we describe next , there are non - trivial classes of latent - variable graphical models in which this condition holds .",
    "* bounded degree and incoherence * : the first class of latent - variable models that we consider are those in which the conditional graphical model among the observed variables ( given by @xmath275 ) has constant degree @xmath303 .",
    "recall from equation that the incoherence @xmath145 of the effect of the latent variables ( given by @xmath267 ) can be as small as @xmath304 . consequently latent - variable models in which @xmath305 can be estimated consistently from @xmath306 samples as long as the low - rank matrix @xmath276 is almost maximally incoherent , i.e. , @xmath307 so the effect of marginalization over the latent variables is diffuse across almost all the observed variables .",
    "thus consistent latent - variable model selection is possible even when the number of samples and the number of latent variables are on the same order as the number of observed variables .",
    "* polylogarithmic degree * the next class of models that we study are those in which the degree @xmath303 of the conditional graphical model of the observed variables grows poly - logarithmically with @xmath272 .",
    "consequently , the incoherence @xmath145 of the matrix @xmath276 must decay as the inverse of poly-@xmath308 . using the fact that maximally incoherent low - rank matrices @xmath276 can have incoherence as small as @xmath304 , latent - variable models in which @xmath309 can be consistently estimated as long as @xmath310poly-@xmath308 .",
    "the main result theorem  [ theo : main ] gives conditions under which we can consistently estimate the sparse and low - rank parts that compose the marginal concentration matrix @xmath311 .",
    "here we prove a corollary that gives rates for covariance matrix estimation , i.e. , the quality of the estimate @xmath284 with respect to the `` true '' marginal covariance matrix @xmath312 .    under the same conditions as in theorem",
    "[ theo : main ] , we have with probability greater than @xmath313 that @xmath314 ) \\lesssim \\frac{1}{\\xi(t ) } \\sqrt{\\frac{p}{n}}.\\ ] ] specifically this implies that @xmath315 .    * proof * : the proof of this lemma follows directly from duality .",
    "based on the analysis in appendix  [ app : main ] ( in particular using the optimality conditions of the modified convex program ) , we have that @xmath316 ) \\leq \\lambda_n.\\ ] ] we also have from the bound on the number of samples @xmath21 that with probability greater than @xmath317 ( see appendix  [ app : final ] ) @xmath318 ) \\lesssim \\lambda_n\\ ] ] based on the choice of @xmath33 in theorem  [ theo : main ] , we then have the desired bound .",
    "@xmath319      standard results from convex analysis @xcite state that @xmath34 is a minimum of the convex program if the zero matrix belongs to the subdifferential of the objective function evaluated at @xmath34 ( in addition to @xmath34 satisfying the constraints ) .",
    "the subdifferential of the @xmath0 norm at a matrix @xmath37 is given by @xmath320 for a symmetric positive semidefinite matrix @xmath37 with svd @xmath321 , the subdifferential of the trace function restricted to the cone of positive semidefinite matrices ( i.e. , the nuclear norm over this set ) is given by : @xmath322 ~~ \\leftrightarrow ~~ { \\mathcal{p}}_{t(m)}(n ) = u u^t , ~ { \\mathcal{p}}_{t(m)^\\bot}(n ) \\preceq i,\\ ] ] where @xmath323 denotes the characteristic function of the set of positive semidefinite matrices ( i.e. , the convex function that evaluates to @xmath324 over this set and @xmath325 outside ) .",
    "the key point is that elements of the subdifferential decompose with respect to the tangent spaces @xmath115 and @xmath126 .",
    "this decomposition property plays a critical role in our analysis .",
    "in particular it states that the optimality conditions consist of two parts , one part corresponding to the tangent spaces @xmath162 and @xmath127 and another corresponding to the normal spaces @xmath212 and @xmath298 .    consider the optimization problem with the additional ( non - convex ) constraints that the variable @xmath30 belongs to the algebraic variety of sparse matrices and that the variables @xmath31 belongs to the algebraic variety of low - rank matrices . while this new optimization problem is non - convex , it has a very interesting property . at a globally optimal solution ( and indeed at any locally optimal solution ) @xmath326 such that @xmath327 and @xmath328 are smooth points of the algebraic varieties of sparse and low - rank matrices , the first - order optimality conditions state that the lagrange multipliers corresponding to the additional variety constraints must lie in the _ normal spaces _ @xmath329 and @xmath330 .",
    "this fundamental observation , combined with the decomposition property of the subdifferentials of the @xmath0 and nuclear norms , suggests the following high - level proof strategy :    1 .",
    "let @xmath326 be the globally optimal solution of the optimization problem with the additional constraints that @xmath65 belong to the algebraic varieties of sparse / low - rank matrices ; specifically constrain @xmath30 to lie in @xmath331 and constrain @xmath31 to lie in @xmath332 .",
    "show first that @xmath326 are smooth points of these varieties .",
    "the first part of the subgradient optimality conditions of the original convex program corresponding to components _ on _ the tangent spaces @xmath333 and @xmath334 is satisfied .",
    "this conclusion can be reached because the additional lagrange multipliers due to the variety constraints lie in the normal spaces @xmath329 and @xmath330 .",
    "3 .   finally show that the second part of the subgradient optimality conditions of ( without any variety constraints ) corresponding to components in the normal spaces @xmath329 and @xmath330 is also satisfied by @xmath326 .",
    "combining these steps together we show that @xmath326 satisfy the optimality conditions of the _ original convex program _ .",
    "consequently @xmath326 is also the optimum of the convex program . as this estimate is also the solution to the problem with the variety constraints , the algebraic consistency of @xmath326",
    "can be directly concluded .",
    "we emphasize here that the variety - constrained optimization problem is used solely as an analysis tool in order to prove consistency of the estimates provided by the convex program .",
    "these steps describe our broad strategy , and we refer the reader to appendix  [ app : main ] for details .",
    "the key technical complication is that the tangent spaces at @xmath328 and @xmath267 are in general different .",
    "we bound the twisting between these tangent spaces by using the fact that the minimum nonzero singular value of @xmath267 is bounded away from zero ( as assumed in theorem  [ theo : main ] and using proposition  [ theo : tspace ] ) .",
    "in this section we give experimental demonstration of the consistency of our estimator on synthetic examples , and its effectiveness in modeling real - world stock return data .",
    "our choices of @xmath33 and @xmath32 are guided by theorem  [ theo : main ] .",
    "specifically , we choose @xmath33 to be proportional to @xmath335 . for @xmath32",
    "we observe that the support / sign - pattern and the rank of the solution @xmath34 are the same for a _ range _ of values of @xmath32",
    ". therefore one could solve the convex program for several values of @xmath32 , and choose a solution in a suitable range in which the sign - pattern and rank of the solution are stable .",
    "in practical problems with real - world data these parameters may be chosen via cross - validation . for small problem instances",
    "we solve the convex program using a combination of yalmip @xcite and sdpt3 @xcite , which are standard off - the - shelf packages for solving convex programs . for larger problem instances we use the special purpose solver logdetppa @xcite developed for log - determinant semidefinite programs .      in the first set of experiments we consider a setting in which we have access to samples of the observed variables of a latent - variable graphical model .",
    "we consider several latent - variable gaussian graphical models .",
    "the first model consists of @xmath336 observed variables and @xmath337 hidden variables .",
    "the conditional graphical model structure of the observed variables is a cycle with the edge partial correlation coefficients equal to @xmath338 ; thus , this conditional model is specified by a sparse graphical model with degree @xmath339 .",
    "the second model is the same as the first one , but with @xmath340 latent variables .",
    "the third model consists of @xmath341 latent variable , and the conditional graphical model structure of the observed variables is given by a @xmath342 nearest - neighbor grid ( i.e. , @xmath336 and degree @xmath343 ) with the partial correlation coefficients of the edges equal to @xmath344 . in all three of these models each latent variable is connected to a random subset of @xmath345 of the observed variables ( and the partial correlation coefficients corresponding to these edges are also random ) .",
    "therefore the effect of the latent variables is `` spread out '' over most of the observed variables , i.e. , the low - rank matrix summarizing the effect of the latent variables is incoherent .    for each model",
    "we generate @xmath21 samples of the observed variables , and use the resulting sample covariance matrix @xmath86 as input to our convex program .",
    "figure  [ fig : fig1 ] shows the probability of recovery of the support / sign - pattern of the conditional graphical model structure in the observed variables and the number of latent variables ( i.e. , probability of obtaining algebraically consistent estimates ) as a function of @xmath21 .",
    "this probability is evaluated over @xmath346 experiments for each value of @xmath21 .    in all of these cases standard",
    "graphical model selection applied directly to the observed variables is not useful as the marginal concentration matrix of the observed variables is not well - approximated by a sparse matrix .",
    "these experiments agree with our theoretical results that the convex program is an algebraically consistent estimator of a latent - variable model given ( sufficiently many ) samples of only the observed variables .      in the next experiment we model the statistical structure of monthly stock returns of 84 companies in the s&p 100 index from 1990 to 2007 ; we disregard 16 companies that were listed after 1990 .",
    "the number of samples @xmath21 is equal to @xmath347 .",
    "we compute the sample covariance based on these returns and use this as input to .",
    "the model learned using for suitable values of @xmath348 consists of @xmath349 latent variables , and the conditional graphical model structure of the stock returns conditioned on these hidden components consists of @xmath350 edges .",
    "therefore the number of parameters in the model is @xmath351 .",
    "the resulting kl divergence between the distribution specified by this model and a gaussian distribution specified by the sample covariance is @xmath352 .",
    "figure  [ fig : fig2 ] ( left ) shows the _ conditional _ graphical model structure .",
    "the strongest edges in this conditional graphical model , as measured by partial correlation , are between baker hughes - schlumberger , a.t.&t .",
    "- verizon , merrill lynch - morgan stanley , halliburton - baker hughes , intel - texas instruments , apple - dell , and microsoft - dell .",
    "it is of interest to note that in the standard industrial classification system for grouping these companies , several of these pairs are in different classes . as mentioned in section  [ subsec : ps ] our method estimates a low - rank matrix that summarizes the effect of the latent variables ; in order to factorize this low - rank matrix , for example into sparse factors , one could use methods such as those described in @xcite .",
    "we compare these results to those obtained using a sparse graphical model learned using @xmath0-regularized maximum - likelihood ( see for example @xcite ) , without introducing any latent variables .",
    "figure  [ fig : fig2 ] ( right ) shows this graphical model structure .",
    "the number of edges in this model is @xmath353 ( the total number of parameters is equal to @xmath354 ) , and the resulting kl divergence between this distribution and a gaussian distribution specified by the sample covariance is @xmath355 . indeed to obtain a comparable kl divergence to that of the latent - variable model described above , one would require a graphical model with over @xmath356 edges .",
    "these results suggest that a latent - variable graphical model is better suited than a standard sparse graphical model for modeling the statistical structure among stock returns .",
    "this is likely due to the presence of global , long - range correlations in stock return data that are better modeled via latent variables .",
    "we have studied the problem of modeling the statistical structure of a collection of random variables as a sparse graphical model conditioned on a few additional hidden components . as a first contribution we described conditions under which such latent - variable graphical models are identifiable given samples of only the observed variables .",
    "we also proposed a convex program based on regularized maximum - likelihood for latent - variable graphical model selection ; the regularization function is a combination of the @xmath0 norm and the nuclear norm . given samples of the observed variables of a latent - variable gaussian model we proved that this convex program provides consistent estimates of the number of hidden components as well as the conditional graphical model structure among the observed variables conditioned on the hidden components .",
    "our analysis holds in the high - dimensional regime in which the number of observed / latent variables are allowed to grow with the number of samples of the observed variables .",
    "in particular we discuss certain scaling regimes in which consistent model selection is possible even when the number of samples and the number of latent variables are on the same order as the number of observed variables .",
    "these theoretical predictions are verified via a set of experiments on synthetic data .",
    "we also demonstrate the effectiveness of our approach in modeling real - world stock return data .",
    "several research questions arise that are worthy of further investigation .",
    "while the convex program can be solved in polynomial time using off - the - shelf solvers , it is preferable to develop more efficient special - purpose solvers that can scale to massive datasets by taking advantage of the structure of the formulation .",
    "finally it would be of interest to develop a similar convex optimization formulation with consistency guarantees for latent - variable models with non - gaussian variables , e.g. , for categorical data .",
    "we would like to thank james saunderson and myung jin choi for helpful discussions , and kim - chuan toh for kindly providing us specialized code to solve larger instances of our convex program .",
    "given a low - rank matrix we consider what happens to the invariant subspaces when the matrix is perturbed by a small amount .",
    "we assume without loss of generality that the matrix under consideration is square and symmetric , and our methods can be extended to the general non - symmetric non - square case .",
    "we refer the interested reader to @xcite for more details , as the results presented here are only a brief summary of what is relevant for this paper .",
    "in particular the arguments presented here are along the lines of those presented in @xcite .",
    "the appendices in @xcite also provide a more refined analysis of second - order perturbation errors .",
    "the resolvent of a matrix @xmath37 is given by @xmath357 @xcite , and it is well - defined for all @xmath358 that do not coincide with an eigenvalue of @xmath37 .",
    "if @xmath37 has no eigenvalue with magnitude equal to @xmath359 , then we have by the cauchy residue formula that the projector onto the invariant subspace of a matrix @xmath37 corresponding to all singular values smaller than @xmath359 is given by @xmath360 where @xmath361 denotes the positively - oriented circle of radius @xmath359 centered at the origin .",
    "similarly , we have that the weighted projection onto the invariant subspace corresponding to the smallest singular values is given by @xmath362    suppose that @xmath37 is a low - rank matrix with smallest nonzero singular value @xmath132 , and let @xmath133 be a perturbation of @xmath37 such that @xmath363 .",
    "we have the following identity for any @xmath364 , which will be used repeatedly : @xmath365^{-1 } - [ m - \\zeta i]^{-1 } = - [ m - \\zeta i]^{-1 } \\delta [ ( m+\\delta ) - \\zeta i]^{-1}. \\label{eq : matexp}\\ ] ]",
    "we then have that @xmath366^{-1 } - [ m - \\zeta i]^{-1 } d \\zeta \\nonumber \\\\ & = & \\frac{1}{2 \\pi i } \\oint_{{\\mathcal{c}}_\\kappa } [ m - \\zeta i]^{-1 } \\delta [ ( m+\\delta ) - \\zeta i]^{-1 } d \\zeta .",
    "\\label{eq : projexp}\\end{aligned}\\ ] ] similarly , we have the following for @xmath367 : @xmath368^{-1 } - [ m - \\zeta i]^{-1}\\right\\ } d \\zeta \\nonumber \\\\ & = & \\frac{1}{2 \\pi i } \\oint_{{\\mathcal{c}}_\\kappa } \\zeta ~ \\left\\{[m - \\zeta i]^{-1 } \\delta [ ( m+\\delta ) - \\zeta i]^{-1}\\right\\ } d \\zeta \\nonumber \\\\ & = & \\frac{1}{2 \\pi i } \\oint_{{\\mathcal{c}}_\\kappa } \\zeta ~ [ m - \\zeta i]^{-1 } \\delta [ m - \\zeta i]^{-1 } d \\zeta \\nonumber \\\\ & & - \\frac{1}{2 \\pi i } \\oint_{{\\mathcal{c}}_\\kappa } \\zeta ~ [ m - \\zeta i]^{-1 } \\delta [ m - \\zeta i]^{-1 } \\delta [ ( m+\\delta ) - \\zeta i]^{-1 } d \\zeta .",
    "\\label{eq : wprojexp}\\end{aligned}\\ ] ]    given these expressions , we have the following two results .",
    "[ theo : projbound ] let @xmath112 be a rank-@xmath117 matrix with smallest nonzero singular value equal to @xmath132 , and let @xmath133 be a perturbation to @xmath37 such that @xmath369 with @xmath370 .",
    "then we have that @xmath371    * proof * : this result follows directly from the expression , and the sub - multiplicative property of the spectral norm : @xmath372 here , we used the fact that @xmath373^{-1}\\|_2 \\leq \\frac{1}{\\sigma - \\kappa}$ ] and @xmath374^{-1}\\|_2 \\leq \\frac{1}{\\sigma - \\frac{3\\kappa}{2}}$ ] for @xmath364 .",
    "@xmath319    next , we develop a similar bound for @xmath367 .",
    "let @xmath375 denote the invariant subspace of @xmath37 corresponding to the nonzero singular values , and let @xmath376 denote the projector onto this subspace .",
    "[ theo : wprojbound ] let @xmath112 be a rank-@xmath117 matrix with smallest nonzero singular value equal to @xmath132 , and let @xmath133 be a perturbation to @xmath37 such that @xmath369 with @xmath370 .",
    "then we have that @xmath377    * proof * : one can check that @xmath378^{-1 } \\delta [ m - \\zeta i]^{-1 } d \\zeta = ( i - p_{{u}(m ) } ) \\delta ( i - p_{{u}(m)}).\\ ] ] next we use the expression , and the sub - multiplicative property of the spectral norm : @xmath379 as with the previous proof , we used the fact that @xmath373^{-1}\\|_2 \\leq \\frac{1}{\\sigma - \\kappa}$ ] and @xmath374^{-1}\\|_2 \\leq \\frac{1}{\\sigma - \\frac{3\\kappa}{2}}$ ] for @xmath364 .",
    "@xmath319    we will use these expressions to derive bounds on the `` twisting '' between the tangent spaces at @xmath37 and at @xmath135 with respect to the rank variety .",
    "for a symmetric rank-@xmath117 matrix @xmath37 , the projection onto the tangent space @xmath126 ( restricted to the variety of symmetric matrices with rank less than or equal to @xmath117 ) can be written in terms of the projection @xmath380 onto the row space @xmath381 . for any matrix @xmath382 @xmath383 one",
    "can then check that the projection onto the normal space @xmath384 @xmath385 ( n)= ( i - p_{u(m ) } ) ~ n ~ ( i - p_{u(m)}).\\ ] ]      * proof of proposition  [ theo : nspace ] * : since both @xmath37 and @xmath135 are rank-@xmath117 matrices , we have that @xmath391 for @xmath389 .",
    "consequently , @xmath392 where we obtain the last inequality from proposition  [ theo : wprojbound ] with @xmath389 .",
    "@xmath319    * proof of lemma  [ theo : rhotspace ] * : since @xmath154 one can check that the largest principal angle between @xmath393 and @xmath394 is strictly less than @xmath395 . consequently",
    ", the mapping @xmath396 restricted to @xmath393 is bijective ( as it is injective , and the spaces @xmath129 have the same dimension ) . consider the maximum and minimum gain of the operator @xmath397 restricted to @xmath393 ; for any @xmath398 : @xmath399(m)\\|_2 \\\\ & \\in & [ 1-\\rho(t_1,t_2 ) , 1+\\rho(t_1,t_2)].\\end{aligned}\\ ] ] therefore , we can rewrite @xmath400 as follows : @xmath401(n)\\|_\\infty ] \\\\",
    "& \\leq & \\frac{1}{1 ~ - ~ \\rho(t_1,t_2 ) } ~ \\left[\\xi(t_1 ) ~ + ~ \\max_{n \\in t_1 , \\|n\\|_2 \\leq 1 } ~ \\|[{\\mathcal{p}}_{t_1 } - { \\mathcal{p}}_{t_2}](n)\\|_\\infty \\right ] \\\\ & \\leq & \\frac{1}{1 ~ - ~ \\rho(t_1,t_2 ) } ~ \\left[\\xi(t_1 ) ~ + ~ \\max_{\\|n\\|_2 \\leq 1 } ~ \\|[{\\mathcal{p}}_{t_1 } - { \\mathcal{p}}_{t_2}](n)\\|_2 \\right ] \\\\ & \\leq & \\frac{1}{1 ~ - ~ \\rho(t_1,t_2 ) } ~ \\left[\\xi(t_1 ) ~ + ~ \\rho(t_1,t_2 ) \\right].\\end{aligned}\\ ] ] this concludes the proof of the lemma . @xmath319",
    "* proof of lemma  [ theo : rg1 ] * : we have that @xmath402 ; therefore , @xmath403 .",
    "we need to bound @xmath404 and @xmath405 .",
    "first , we have @xmath406 \\\\ & \\subseteq & [ \\|s\\|_{\\infty } - \\|l\\|_{\\infty } , \\|s\\|_{\\infty } + \\|l\\|_{\\infty } ] \\\\ & \\subseteq & [ \\gamma - \\xi(t ) , \\gamma + \\xi(t)].\\end{aligned}\\ ] ] similarly , one can check that @xmath407 \\\\",
    "& \\subseteq & [ 1 - 2\\|s\\|_2 , 1 + 2 \\|s\\|_2 ] \\\\ & \\subseteq & [ 1 - 2 \\gamma \\mu(\\omega ) , 1 + 2 \\gamma \\mu(\\omega)].\\end{aligned}\\ ] ] thus , we can conclude that @xmath408.\\ ] ] where @xmath189 is defined in .",
    "@xmath319    * proof of proposition  [ theo : irr ] * : before proving the two parts of this proposition we make a simple observation about @xmath409 using the condition that @xmath410 by applying lemma  [ theo : rhotspace ] : @xmath411 here we used the property that @xmath412 in obtaining the final inequality .",
    "consequently , noting that @xmath413 $ ] implies that @xmath414    * part @xmath415 * : the proof of this step proceeds in a similar manner to that of lemma  [ theo : rg1 ] .",
    "first we have for @xmath416 with @xmath417 : @xmath418 next under the same conditions on @xmath419 , @xmath420 combining these last two bounds with , we conclude that @xmath421 where the final inequality follows from the assumption that @xmath422 $ ] .    *",
    "part @xmath339 * : note that for @xmath416 with @xmath423 @xmath424 similarly @xmath425 combining these last two bounds with the bounds from the first part , we have that @xmath426 this concludes the proof of the proposition .",
    "here we prove theorem  [ theo : main ] . throughout this section",
    "we denote @xmath427 .",
    "further @xmath273 and @xmath206 denote the tangent spaces at the `` true '' sparse matrix @xmath428 and low - rank matrix @xmath429 .",
    "we assume that @xmath242 \\label{eq : grange}\\ ] ] we also let @xmath430 denote the difference between the true marginal covariance and the sample covariance .",
    "finally we let @xmath431 throughout this section . for @xmath32 in the above range we note that @xmath432 standard facts that we use throughout this section are that @xmath412 and that @xmath433 for any matrix @xmath37 .",
    "we study the following convex program : @xmath434 - \\log\\det(s - l ) ~ + ~ \\lambda_n [ \\gamma \\|s\\|_{1 } + \\|l\\|_\\ast ] \\\\",
    "\\mbox{s.t . } & ~~~ s - l \\succ 0 . \\end{aligned } \\label{eq : sdpsy}\\ ] ] comparing with the convex program ,",
    "the main difference is that we do not constraint the variable @xmath31 to be positive semidefinite in ( recall that the nuclear norm of a positive semidefinite matrix is equal to its trace ) .",
    "however we show that the unique optimum @xmath435 of under the hypotheses of theorem  [ theo : main ] is such that @xmath436 ( with high probability ) .",
    "therefore we conclude that @xmath435 is also the unique optimum of .",
    "the subdifferential with respect to the nuclear norm at a matrix @xmath37 with ( reduced ) svd given by @xmath121 is as follows : @xmath437      1 .",
    "we show that if we solve the convex program subject to the additional constraints that @xmath438 and @xmath439 for some @xmath221 `` close to '' @xmath127 ( measured by @xmath440 ) , then the error between the optimal solution @xmath435 and the underlying matrices @xmath441 is small .",
    "this result is discussed in appendix  [ app : errbound ] .",
    "we analyze the optimization problem with the additional constraint that the variables @xmath30 and @xmath31 belong to the algebraic varieties of sparse and low - rank matrices respectively , and that the corresponding tangent spaces are close to the tangent spaces at @xmath442 .",
    "we show that under suitable conditions on the minimum nonzero singular value of the true low - rank matrix @xmath443 and on the minimum magnitude nonzero entry of the true sparse matrix @xmath444 , the optimum of this modified program is achieved at a _ smooth _ point of the underlying varieties .",
    "in particular the bound on the minimum nonzero singular value of @xmath443 helps bound the curvature of the low - rank matrix variety locally around @xmath443 ( we use the results described in appendix  [ app : rankcurv ] ) .",
    "these results are described in appendix  [ app : var ] .",
    "3 .   the next step is to show that the variety constraint can be linearized and changed to a tangent - space constraint ( see appendix  [ app : ts ] ) , thus giving us a _",
    "convex program_. under suitable conditions this tangent - space constrained program also has an optimum that has the same support / rank as the true @xmath441 .",
    "based on the previous step these tangent spaces in the constraints are close to the tangent spaces at the true @xmath441 .",
    "therefore we use the first step to conclude that the resulting error in the estimate is small .",
    "4 .   finally we show that under the identifiability conditions of section  [ sec : iden ] these tangent - space constraints are inactive at the optimum ( see appendix  [ app : final ] ) .",
    "therefore we conclude with the statement that the optimum of the convex program without any variety constraints is achieved at a pair of matrices that have the same support / rank as the true @xmath441 ( with high probability ) .",
    "further the low - rank component of the solution is positive semidefinite , thus allowing us to conclude that the original convex program also provides estimates that are consistent .",
    "consider the taylor series of the inverse of a matrix : @xmath445 where @xmath446.\\ ] ] this infinite sum converges for @xmath133 sufficiently small .",
    "the following proposition provides a bound on the second - order term specialized to our setting :      * proof * : we have that @xmath452 where the second - to - last inequality follows from the range for @xmath32 and that @xmath422 $ ] , and the final inequality follows from the bound on @xmath453 .",
    "therefore , @xmath454 here we apply the last two inequalities from above . since the @xmath455-norm is bounded above by the spectral norm @xmath456 , we have the desired result .",
    "@xmath319      next we analyze the following convex program subject to certain additional tangent - space constraints : @xmath457 - \\log\\det(s - l ) ~ + ~ \\lambda_n [ \\gamma \\|s\\|_{1 } + \\|l\\|_\\ast ]",
    "\\\\ \\mbox{s.t . } & ~~~ s - l \\succ 0 , ~~ s \\in \\omega , ~~ l \\in t ' , \\end{aligned } \\label{eq : sdpts}\\ ] ] for some subspace @xmath221 .",
    "we show that if @xmath221 is any tangent space to the low - rank matrix variety such that @xmath410 , then we can bound the error @xmath458 .",
    "let @xmath459 denote the normal component of the true low - rank matrix at @xmath221 , and recall that @xmath430 denotes the difference between the true marginal covariance and the sample covariance .",
    "the proof of the following result uses brouwer s fixed - point theorem @xcite , and is inspired by the proof of a similar result in @xcite for standard sparse graphical model recovery without latent variables .",
    "[ theo : bfpt ] let the error @xmath449 in the solution of the convex program ( with @xmath221 such that @xmath228 ) be as defined above .",
    "further let @xmath448 , and define @xmath460 , \\|{\\mathcal{c}}_{t'}\\|_2 \\right\\}.\\ ] ] if we have that @xmath461 for @xmath32 in the range given by , then @xmath462    * proof * : based on proposition  [ theo : irr ] we note that the convex program is strictly convex ( because the negative log - likelihood term has a strictly positive - definite hessian due to the constraints involving transverse tangent spaces ) , and therefore the optimum is unique .",
    "applying the optimality conditions of the convex program at the optimum @xmath463 , we have that there exist lagrange multipliers @xmath464 such that @xmath465 restricting these conditions to the space @xmath243 , one can check that @xmath466 = z_\\omega , ~~ { \\mathcal{p}}_{t ' } [ \\sigma^n_o - ( \\hat{s}_\\omega-\\hat{l}_{t'})^{-1 } ] = z_{t'},\\ ] ] where @xmath467 and @xmath468 ( we use here the fact that projecting onto a tangent space @xmath221 increases the spectral norm by at most a factor of two ) . denoting @xmath469 $ ] , we conclude that @xmath470 = z , \\label{eq : bfpteq1}\\ ] ] with @xmath471 . since the optimum @xmath463 is unique , one can check using lagrangian duality theory @xcite that @xmath463 is the unique solution of the equation . rewriting @xmath472 in terms of the errors",
    "@xmath449 , we have using the taylor series of the matrix inverse that @xmath473^{-1 } \\nonumber \\\\ & = & e_n - r_{\\sigma^\\ast_o}({\\mathcal{a}}(\\delta_s,\\delta_l ) ) + { \\mathcal{i}}^\\ast { \\mathcal{a}}(\\delta_s,\\delta_l ) \\nonumber \\\\ & = & e_n - r_{\\sigma^\\ast_o}({\\mathcal{a}}(\\delta_s,\\delta_l ) ) + { \\mathcal{i}}^\\ast { \\mathcal{a}}{\\mathcal{p}}_{\\mathcal{y}}(\\delta_s,\\delta_l ) + { \\mathcal{i}}^\\ast { \\mathcal{c}}_{t'}. \\label{eq : bfpteq2}\\end{aligned}\\ ] ]    since @xmath221 is a tangent space such that @xmath228 , we have from proposition  [ theo : irr ] that the operator @xmath474 from @xmath168 to @xmath168 is bijective and is well - defined . now consider the following matrix - valued function from @xmath475 to @xmath168 : @xmath476 - z \\right\\}.\\ ] ]",
    "a point @xmath475 is a fixed - point of @xmath477 if and only if @xmath478 = z$ ] .",
    "applying equations and above , we then see that the only fixed - point of @xmath477 by construction is the `` true '' error @xmath479 restricted to @xmath168 .",
    "the reason for this is that , as discussed above , @xmath463 is the unique optimum of and therefore is the",
    "_ unique solution _ of .",
    "next we show that this unique fixed - point of @xmath477 lies in the ball @xmath480 .    in order to prove this step , we resort to brouwer s fixed point theorem @xcite . in particular",
    "we show that the function @xmath477 maps the ball @xmath481 onto itself . since @xmath477 is a continuous function and @xmath481 is a compact set",
    ", we can conclude the proof of this proposition . simplifying the function @xmath477",
    ", we have that @xmath482 + z \\right\\}.\\ ] ] consequently , we have from proposition  [ theo : irr ] that @xmath483 - z \\right ) \\\\ & \\leq & \\frac{4}{\\alpha } ~ \\left\\{{g_\\gamma}({\\mathcal{a}}^\\dag [ e_n - r_{\\sigma^\\ast_o}({\\mathcal{a}}(\\delta_s,\\delta_l+{\\mathcal{c}}_{t ' } ) ) + { \\mathcal{i}}^\\ast { \\mathcal{c}}_{t ' } ] ) + \\lambda_n \\right\\ } \\\\ & \\leq & \\frac{r}{2 } ~~ + ~~ \\frac{4}{\\alpha } ~ { g_\\gamma}({\\mathcal{a}}^\\dag r_{\\sigma^\\ast_o}({\\mathcal{a}}(\\delta_s,\\delta_l+{\\mathcal{c}}_{t'}))),\\end{aligned}\\ ] ] where in the second inequality we use the fact that @xmath484 and that @xmath471 , and in the final inequality we use the assumption on @xmath117 .",
    "we now bound the term @xmath485 using proposition  [ theo : rem ] as @xmath447 : @xmath486 where we have used the fact that @xmath487 .",
    "hence @xmath488 by brouwer s fixed - point theorem .",
    "finally we observe that @xmath489 @xmath319      in order to prove that the solution @xmath435 of has the same sparsity pattern / rank as @xmath441 , we will study an optimization problem that explicitly enforces these constraints .",
    "specifically , we consider the following _ non - convex _ constraint set : @xmath490 recall that @xmath491 and @xmath492 .",
    "the first constraint ensures that the tangent space at @xmath30 is the same as the tangent space at @xmath444 ; therefore the support of @xmath30 is contained in the support of @xmath444 .",
    "the second and third constraints ensure that @xmath31 lives in the appropriate low - rank variety , but has a tangent space `` close '' to the tangent space @xmath127 . the final constraint roughly bounds the sum of the errors @xmath493 ; note that this does not necessarily bound the individual errors .",
    "notice that the only non - convex constraint is that @xmath494 .",
    "we then have the following nonlinear program : @xmath495 - \\log\\det(s - l ) ~ + ~ \\lambda_n [ \\gamma \\|s\\|_{1 } + \\|l\\|_\\ast ] \\\\",
    "\\mbox{s.t . } & ~~~ s - l \\succ 0 , ~~ ( s , l ) \\in \\mathcal{m}. \\end{aligned } \\label{eq : nlp}\\ ] ] under suitable conditions this nonlinear program is shown to have a unique solution .",
    "each of the constraints in @xmath496 is useful for proving the consistency of the solution of the convex program .",
    "we show that under suitable conditions the constraints in @xmath496 are actually inactive at the optimal @xmath497 , thus allowing us to conclude that the solution of is also equal to @xmath497 ; hence the solution of shares the consistency properties of @xmath497 .",
    "a number of interesting properties can be derived simply by studying the constraint set @xmath496 .      * proof * : we have by the triangle inequality that @xmath502 as @xmath503 . therefore , we have that @xmath504 , where @xmath163 .",
    "consequently , we can apply proposition  [ theo : irr ] to conclude that @xmath505 finally , we use the triangle inequality again to conclude that @xmath506 @xmath319    this simple result immediately leads to a number of useful corollaries .",
    "for example we have that under a suitable bound on the minimum nonzero singular value of @xmath429 , the constraint in @xmath496 along the normal direction @xmath298 is locally inactive .",
    "next we list several useful consequences of proposition  [ theo : nlp ] .",
    "[ theo : nlpcor ] consider _ any _ @xmath498 , and let @xmath499 .",
    "suppose @xmath32 is in the range specified by , and let @xmath507 and @xmath508 ( where @xmath509 is as defined in proposition  [ theo : nlp ] ) .",
    "let the minimum nonzero singular value @xmath132 of @xmath510 be such that @xmath511 for @xmath512 , and suppose that the smallest magnitude nonzero entry of @xmath444 is greater than @xmath513 for @xmath514 .",
    "setting @xmath515 and @xmath459 , we then have that :    1 .",
    "@xmath31 has rank equal to @xmath516 , i.e. , @xmath31 is a smooth point of the variety of matrices with rank less than or equal to @xmath516 .",
    "in particular @xmath31 has the same inertia as @xmath443 .",
    "2 .   @xmath517 .",
    "3 .   @xmath518 .",
    "4 .   @xmath519 .",
    "5 .   @xmath520 .",
    "@xmath521 .    * proof * : we note the following facts before proving each step . first @xmath522 .",
    "second @xmath412 .",
    "third we have from proposition  [ theo : nlp ] that @xmath523 .",
    "finally @xmath524 for @xmath525 $ ] .",
    "we prove each step separately .          for the fourth step let @xmath530 denote the minimum singular value of @xmath31 .",
    "consequently , @xmath531 \\geq 8 \\|\\delta_l\\|_2.\\ ] ] using the same reasoning as in the proof of the second step , we have that @xmath532 hence @xmath533        notice that this corollary applies to _ any _",
    "@xmath498 , and is hence applicable to _ any solution _",
    "@xmath497 of the @xmath496-constrained program .",
    "for now we choose an arbitrary solution @xmath497 and proceed . in the next steps",
    "we show that @xmath497 is _ the unique _ solution to the convex program , thus showing that @xmath497 is also the unique solution to .",
    "given the solution @xmath538 , we show that the solution to the convex program with the tangent space constraint @xmath539 is the same as @xmath538 under suitable conditions : @xmath540 - \\log\\det(s - l ) ~ + ~ \\lambda_n [ \\gamma \\|s\\|_{1 } + \\|l\\|_\\ast ] \\\\ \\mbox{s.t . } & ~~~ s - l \\succ 0 , ~~ s \\in \\omega , ~~ l \\in t_\\mathcal{m}. \\end{aligned } \\label{eq : sdptsm}\\ ] ]    assuming the bound of corollary  [ theo : nlpcor ] on the minimum singular value of @xmath443 the uniqueness of the solution @xmath541 is assured .",
    "this is because we have from proposition  [ theo : irr ] and from corollary  [ theo : nlpcor ] that @xmath207 is injective on @xmath542 .",
    "therefore the hessian of the convex objective function of is strictly positive - definite at @xmath541 .",
    "[ theo : avtots ] let @xmath32 be in the range specified by .",
    "suppose that the minimum nonzero singular value @xmath132 of @xmath492 is such that @xmath511 ( @xmath544 is defined in corollary  [ theo : nlpcor ] ) .",
    "suppose also that the minimum magnitude nonzero entry of @xmath444 is greater than or equal to @xmath513 ( @xmath545 is defined in corollary  [ theo : nlpcor ] ) .",
    "let @xmath546 .",
    "further suppose that @xmath547 then we have that @xmath548      1 .",
    "first we can change the non - convex constraint @xmath494 to the linear constraint @xmath549 .",
    "this is because the lower bound assumed for @xmath132 implies that @xmath550 is a smooth point of the algebraic variety of matrices with rank less than or equal to @xmath516 ( from corollary  [ theo : nlpcor ] ) .",
    "due to the convexity of all the other constraints and the objective , the optimum of this `` linearized '' convex program will still be @xmath551 .",
    "2 .   next we can again apply corollary  [ theo : nlpcor ] ( based on the bound on @xmath132 ) to conclude that the constraint @xmath552 is _ locally inactive _ at the point @xmath551 .",
    "consequently , we have that @xmath551 can be written as the solution of a _ convex program _ :",
    "@xmath553 - \\log\\det(s - l ) ~ + ~ \\lambda_n [ \\gamma \\|s\\|_{1 } + \\|l\\|_\\ast ] \\\\ \\mbox{s.t .",
    "} & ~~~ s - l \\succ 0 , ~~ s \\in \\omega , ~~ l \\in t_\\mathcal{m } , \\\\ & ~~~ { g_\\gamma}({\\mathcal{a}}^\\dag { \\mathcal{i}}^\\ast { \\mathcal{a}}(s - { s^\\ast } , { l^\\ast}- l ) ) \\leq 11 \\lambda_n .",
    "\\end{aligned } \\label{eq : sdptsmg}\\ ] ]    we now need to argue that the constraint @xmath554 is also inactive in the convex program .",
    "we proceed by showing that the solution @xmath555 of the convex program has the property that @xmath556 , which concludes the proof of this proposition .",
    "we have from corollary  [ theo : nlpcor ] that @xmath557 .",
    "since @xmath558 by assumption , one can verify that @xmath559 & \\leq & \\frac{8\\lambda_n}{\\alpha}\\left[1 + \\frac{\\nu}{3(2-\\nu ) } \\right ] \\\\ & = & \\frac{16 ( 3-\\nu ) \\lambda_n}{3 \\alpha ( 2-\\nu ) } \\\\ & \\leq & \\min\\left\\{\\frac{1}{4 c_1 } , \\frac{\\alpha \\xi(t)}{64 d \\psi c_1 ^ 2}\\right\\}.\\end{aligned}\\ ] ] the last line follows from the assumption on @xmath33 .",
    "we also note that @xmath560 from corollary  [ theo : nlpcor ] , which implies that @xmath561 . letting @xmath562",
    ", we can conclude from proposition  [ theo : bfpt ] that @xmath563 .",
    "next we apply proposition  [ theo : rem ] ( as @xmath564 ) to conclude that @xmath565    from the optimality conditions of one can also check that for @xmath566 , @xmath567 \\\\ & \\leq & 4 \\left[\\frac{2 ( 3-\\nu ) \\lambda_n}{3 ( 2-\\nu)}\\right].\\end{aligned}\\ ] ] here we used in the last inequality , and also that @xmath557 ( as noted above from corollary  [ theo : nlpcor ] ) and that @xmath568 .",
    "therefore , @xmath569 because @xmath422 $ ] . based on proposition  [ theo : irr ] ( the second part )",
    ", we also have that @xmath570 summarizing steps and , @xmath571 this concludes the proof of the proposition .",
    "[ theo : avtotscor ] under the assumptions of proposition  [ theo : avtots ] we have that @xmath572 and that @xmath573 .",
    "moreover , @xmath574 actually has the same inertia as @xmath443 .",
    "we also have that @xmath575 .",
    "the following lemma provides a simple set of sufficient conditions under which the optimal solution @xmath555 of satisfies the optimality conditions of the convex program ( without the tangent space constraints ) .",
    "[ theo : mainlemma ] let @xmath555 be the solution to the tangent - space constrained convex program .",
    "suppose that the assumptions of proposition  [ theo : avtots ] hold .",
    "if in addition we have that @xmath576 then @xmath555 is also the unique optimum of the convex program .    * proof * : recall from corollary  [ theo : avtotscor ] that the tangent space at @xmath574 is equal to @xmath577 .",
    "applying the optimality conditions of the convex program at the optimum @xmath555 , we have that there exist lagrange multipliers @xmath578 such that @xmath579 restricting these conditions to the space @xmath580 , one can check that @xmath581 = -\\lambda_n \\gamma \\mathrm{sign}({s^\\ast } ) , ~~ { \\mathcal{p}}_{t_\\mathcal{m } } [ \\sigma^n_o - ( \\hat{s}_\\omega-\\hat{l}_{t_\\mathcal{m}})^{-1 } ] = \\lambda_n u v^t,\\ ] ] where @xmath582 is a reduced svd of @xmath574 . denoting @xmath583",
    "$ ] , we conclude that @xmath584 = z , \\label{eq : tscond}\\ ] ] with @xmath585 .",
    "it is clear that the optimality condition of the convex program ( without the tangent - space constraints ) on @xmath168 is satisfied .",
    "all we need to show is that @xmath586 ) <",
    "\\label{eq : offts}\\ ] ]    rewriting @xmath587 in terms of the error @xmath588 , we have that @xmath589 restating the condition on @xmath168 , we have that @xmath590 .",
    "\\label{eq : tscond2}\\ ] ] ( recall that @xmath591 . ) a sufficient condition to show and complete the proof of this lemma is that @xmath592).\\ ] ] we prove this inequality next . recall from corollary  [ theo : nlpcor ] that @xmath557 .",
    "therefore , from equation we can conclude that @xmath593 ) ) \\\\",
    "& \\leq & \\lambda_n + 2\\left[\\frac{3 \\lambda_n \\nu}{6 ( 2-\\nu ) } \\right ] \\\\ & = & \\frac{2\\lambda_n}{2-\\nu}.\\end{aligned}\\ ] ] here we used the bounds assumed on @xmath594 and on @xmath595 .    applying the second part of proposition  [ theo : irr ] , we have that @xmath596 ) \\\\ & \\leq & \\lambda_n - { g_\\gamma}({\\mathcal{p}}_{{\\mathcal{y}}^\\bot } { \\mathcal{a}}^\\dag [ - e_n + r_{\\sigma^\\ast_o } ( { \\mathcal{a}}(\\delta_s,\\delta_l ) ) - { \\mathcal{i}}^\\ast { \\mathcal{c}}_{t_\\mathcal{m}}]).\\end{aligned}\\ ] ] here the second - to - last inequality follows from the bounds on @xmath594 , @xmath595 , and @xmath597 , and the last inequality follows from lemma  [ theo : gg ] .",
    "this concludes the proof of the lemma .",
    "@xmath319      all the analysis described so far in this section has been completely deterministic in nature .",
    "here we present the probabilistic component of our proof . specifically",
    ", we study the rate at which the sample covariance matrix converges to the true covariance matrix . the following result from @xcite plays a key role in our analysis :    [ theo : davsz ] given natural numbers @xmath598 with @xmath599 ,",
    "let @xmath600 be a @xmath601 matrix with i.i.d .",
    "gaussian entries that have zero - mean and variance @xmath602 . then the largest and smallest singular values @xmath603 and @xmath604 of @xmath600 are such that @xmath605 , \\pr\\left[{s}_p(\\gamma ) \\leq 1 - \\sqrt{\\tfrac{p}{n } } - t \\right ] \\right\\ } \\leq \\exp\\left\\{-\\tfrac{n t^2}{2}\\right\\},\\ ] ] for any @xmath606 .    using this result",
    "the next lemma provides a probabilistic bound between the sample covariance @xmath20 formed using @xmath21 samples and the true covariance @xmath285 in spectral norm .",
    "this result is well - known , and we mainly discuss it here for completeness and also to show explicitly the dependence on @xmath607 defined in .",
    "[ theo : problemma ] let @xmath607 . given any @xmath608 with @xmath609 ,",
    "let the number of samples @xmath21 be such that @xmath610 .",
    "then we have that @xmath611 \\leq 2 \\exp\\left\\{-\\tfrac{n \\delta^2}{128 \\psi^2}\\right\\}.\\ ] ]    * proof * : since the spectral norm is unitarily invariant , we can assume that @xmath312 is diagonal without loss of generality .",
    "let @xmath612 , and let @xmath613 denote the largest / smallest singular values of @xmath614 .",
    "note that @xmath614 can be viewed as the sample covariance matrix formed from @xmath21 independent samples drawn from a model with identity covariance , i.e. , @xmath615 where @xmath600 denotes a @xmath601 matrix with i.i.d .",
    "gaussian entries that have zero - mean and variance @xmath602 .",
    "we then have that @xmath616 & \\leq & \\pr\\left[\\|\\bar{\\sigma}^n ~ - i\\|_2 \\geq \\tfrac{\\delta}{\\psi}\\right ] \\\\ & \\leq & \\pr\\left[{s}_1(\\bar{\\sigma}^n ) \\geq 1 + \\tfrac{\\delta}{\\psi } \\right ] + \\pr\\left[{s}_p(\\bar{\\sigma}^n ) \\leq 1 - \\tfrac{\\delta}{\\psi } \\right ] \\\\ & = & \\pr\\left[{s}_1(\\gamma)^2 \\geq 1 + \\tfrac{\\delta}{\\psi } \\right ] + \\pr\\left[{s}_p(\\gamma)^2 \\leq 1 - \\tfrac{\\delta}{\\psi } \\right ] \\\\ & \\leq & \\pr\\left[{s}_1(\\gamma ) \\geq 1 + \\tfrac{\\delta}{4\\psi } \\right ] + \\pr\\left[{s}_p(\\gamma ) \\leq 1 - \\tfrac{\\delta}{4\\psi } \\right ] \\\\ &",
    "\\leq & \\pr\\left[{s}_1(\\gamma ) \\geq 1 + \\sqrt{\\tfrac{p}{n } } + \\tfrac{\\delta}{8 \\psi } \\right ] + \\pr\\left[{s}_p(\\gamma ) \\leq 1 - \\sqrt{\\tfrac{p}{n } } - \\tfrac{\\delta}{8 \\psi } \\right ] \\\\ & \\leq & 2 \\exp\\left\\{-\\tfrac{n \\delta^2}{128 \\psi^2}\\right\\}.\\end{aligned}\\ ] ] here we used the fact that @xmath617 in the fourth inequality , and we applied theorem  [ theo : davsz ] to obtain the final inequality by setting @xmath618 .",
    "@xmath319      [ theo : samples ] let @xmath86 be the sample covariance formed from @xmath21 samples of the observed variables . set @xmath620 . if @xmath621 , then we have with probability greater than @xmath313 that @xmath622 \\geq 1 - 2 \\exp\\{-p\\}.\\ ] ]        in this section we tie together the results obtained thus far to conclude the proof of theorem  [ theo : main ] . we only need to show that the sufficient conditions of lemma  [ theo : mainlemma ] are satisfied .",
    "it follows directly from corollary  [ theo : avtotscor ] that the low - rank part @xmath574 is positive semidefinite , which implies that @xmath555 is also the solution to the original regularized maximum - likelihood convex program with the positive - semidefinite constraint . as usual set @xmath624 , and set @xmath625 .      1 .",
    "let @xmath626 , and let the number of samples @xmath21 be such that @xmath627 note that @xmath628 .",
    "2 .   set @xmath620 , and then set @xmath33 as follows : @xmath629 note that @xmath630 .",
    "3 .   let the minimum nonzero singular value @xmath132 of @xmath443 be such that @xmath631 where @xmath544 is defined in corollary  [ theo : nlpcor ] .",
    "note that @xmath632 .",
    "4 .   let the minimum magnitude nonzero entry @xmath280 of @xmath444 be such that @xmath633 where @xmath545 is defined in corollary  [ theo : nlpcor ] .",
    "note that @xmath634 .",
    "* proof of theorem  [ theo : main ] * : we condition on the event that @xmath635 , which holds with probability greater than @xmath313 from corollary  [ theo : samples ] as @xmath621 by assumption .",
    "we note that based on the bound on @xmath21 , we also have that @xmath636.\\ ] ] in particular , these bounds imply that @xmath637 and that @xmath638 both these weaker bounds are used later .    based on the assumptions above",
    ", the requirements of lemma  [ theo : mainlemma ] on the minimum nonzero singular value of @xmath443 and the minimum magnitude nonzero entry of @xmath444 are satisfied .",
    "we only need to verify the bounds on @xmath33 and @xmath594 from proposition  [ theo : avtots ] , and the bound on @xmath639 from lemma  [ theo : mainlemma ] .",
    "finally we provide a bound on the remainder by applying propositions  [ theo : bfpt ] and [ theo : rem ] , which would satisfy the last remaining condition of lemma  [ theo : mainlemma ] . in order to apply proposition  [ theo : bfpt ] , we note that @xmath644 & \\leq & \\frac{8}{\\alpha } \\left[\\frac{\\nu}{3 ( 2-\\nu ) } + 1 \\right ] \\lambda_n \\nonumber \\\\ & = & \\frac{16(3-\\nu)\\lambda_n}{3 \\alpha ( 2-\\nu ) } \\nonumber \\\\ & = & \\frac{32(3-\\nu)d}{\\alpha \\xi(t ) \\nu } \\delta_n \\label{eq : lambound}\\\\ & \\leq & \\min\\left\\{\\frac{1}{4c_1},\\frac{\\alpha \\xi(t)}{64 d \\psi c_1 ^ 2 } \\right\\}. \\nonumber\\end{aligned}\\ ] ] in the first inequality we used the fact that @xmath568 ( from above ) and that @xmath597 is similarly bounded ( from corollary  [ theo : nlpcor ] due to the bound on @xmath132 ) . in the second equality we used the relation @xmath642 . in the final inequality we used the bound on @xmath640 from .",
    "this satisfies one of the requirements of proposition  [ theo : bfpt ] .",
    "the other condition on @xmath645 is also similarly satisfied due to the bound on @xmath132 from corollary  [ theo : nlpcor ] .",
    "specifically , we have that @xmath646 from corollary  [ theo : nlpcor ] , and use the same sequence of inequalities as above to satisfy the second requirement of proposition  [ theo : bfpt ] .",
    "thus we conclude from proposition  [ theo : bfpt ] and from that @xmath647 this bound implies that @xmath648 , which proves the parametric consistency part of the theorem .    since the bound also satisfies the condition of proposition  [ theo : rem ] ( from the inequality following above we see that @xmath649 ) , we have that @xmath650 \\frac{d \\delta_n}{\\xi(t ) } \\\\ & \\leq & \\frac{d \\delta_n}{\\xi(t ) } \\\\ & = & \\frac{\\lambda_n \\nu}{6 ( 2-\\nu)}.\\end{aligned}\\ ] ] in the final inequality we used the bound on @xmath640 , and in the final equality we used the relation @xmath642 . this concludes the algebraic consistency part of the theorem ."
  ],
  "abstract_text": [
    "<S> suppose we have samples of a _ subset _ of a collection of random variables . </S>",
    "<S> no additional information is provided about the number of latent variables , nor of the relationship between the latent and observed variables . </S>",
    "<S> is it possible to discover the number of hidden components , and to learn a statistical model over the entire collection of variables ? </S>",
    "<S> we address this question in the setting in which the latent and observed variables are jointly gaussian , with the conditional statistics of the observed variables conditioned on the latent variables being specified by a graphical model . as a first step we give natural conditions under which such latent - variable gaussian graphical models are identifiable given marginal statistics of only the observed variables . </S>",
    "<S> essentially these conditions require that the conditional graphical model among the observed variables is sparse , while the effect of the latent variables is `` spread out '' over most of the observed variables . </S>",
    "<S> next we propose a tractable convex program based on regularized maximum - likelihood for model selection in this latent - variable setting ; the regularizer uses both the @xmath0 norm and the nuclear norm . </S>",
    "<S> our modeling framework can be viewed as a combination of dimensionality reduction ( to identify latent variables ) and graphical modeling ( to capture remaining statistical structure not attributable to the latent variables ) , and it consistently estimates both the number of hidden components and the conditional graphical model structure among the observed variables . </S>",
    "<S> these results are applicable in the high - dimensional setting in which the number of latent / observed variables grows with the number of samples of the observed variables . </S>",
    "<S> the geometric properties of the algebraic varieties of sparse matrices and of low - rank matrices play an important role in our analysis .    </S>",
    "<S> * keywords * : gaussian graphical models ; covariance selection ; latent variables ; regularization ; sparsity ; low - rank ; algebraic statistics ; high - dimensional asymptotics </S>"
  ]
}