{
  "article_text": [
    "in bayesian inference , it is well known that standard markov chain monte carlo ( mcmc ) methods tend to fail when the target distribution is multimodal @xcite .",
    "these methods typically fail to move from one mode to another since such moves require passing through low probability regions .",
    "this is especially true for high dimensional problems with isolated modes .",
    "therefore , despite recent advances in computational bayesian methods , designing effective mcmc samplers for multimodal distribution has remained a major challenge . in the statistics and machine learning literature ,",
    "many methods have been proposed address this issue ( see for example , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) however , these methods tend to suffer from the curse of dimensionality @xcite .    in this paper , we propose a new algorithm , which exploits and modifies the riemannian geometric properties of the target distribution to create wormholes connecting modes in order to facilitate moving between them .",
    "our method can be regarded as an extension of hamiltonian monte carlo ( hmc ) . compared to random walk metropolis ,",
    "standard hmc explores the target distribution more efficiently by exploiting its geometric properties .",
    "however , it too tends to fail when the target distribution is multimodal since the modes are separated by high energy barriers ( low probability regions ) @xcite .    in what follows",
    ", we provide an brief overview of hmc .",
    "then , we introduce our method assuming that the locations of the modes are known ( either exactly or approximately ) , possibly through some optimization techniques ( e.g. , @xcite ) .",
    "next , we relax this assumption by incorporating a mode searching algorithm in our method in order to identify new modes and to update the network of wormholes .",
    "hamiltonian monte carlo ( hmc ) @xcite is a metropolis algorithm with proposals guided by hamiltonian dynamics .",
    "hmc improves upon random walk metropolis by proposing states that are distant from the current state , but nevertheless have a high probability of acceptance .",
    "these distant proposals are found by numerically simulating hamiltonian dynamics , whose state space consists of its _ position _ , denoted by the vector @xmath0 , and its _ momentum _ , denoted by a vector @xmath1 .",
    "our objective is to sample from the distribution of @xmath0 with the probability density function ( up to some constant ) @xmath2 .",
    "we usually assume that the auxiliary momentum variable @xmath1 has a multivariate normal distribution ( the same dimension as @xmath0 ) with mean zero .",
    "the covariance of @xmath1 is usually referred to as the _ mass matrix _ , @xmath3 , which in standard hmc is usually set to the identity matrix , @xmath4 , for convenience .",
    "based on @xmath5 and @xmath1 , we define the _ potential energy _ , @xmath6 , and the _ kinetic energy _ ,",
    "we set @xmath8 to minus the log probability density of @xmath5 ( plus any constant ) . for the auxiliary momentum variable @xmath1 , we set @xmath7 to be minus the log probability density of @xmath1 ( plus any constant ) .",
    "the _ hamiltonian _",
    "function is then defined as follows : @xmath9 the partial derivatives of @xmath10 determine how @xmath5 and @xmath1 change over time , according to _",
    "hamilton s equations _ , @xmath11 \\displaystyle   \\dot { \\boldsymbol p } & = & -\\nabla_{\\boldsymbol\\theta } h({\\boldsymbol\\theta } , { \\boldsymbol p } ) & =   & -\\nabla_{\\boldsymbol\\theta } u(\\boldsymbol\\theta )   \\end{array}\\label{rmhd}\\end{aligned}\\ ] ] note that @xmath12 can be interpreted as velocity .    in practice , solving hamiltonian s equations exactly is difficult , so we need to approximate these equations by discretizing time , using some small step size @xmath13 . for this purpose ,",
    "the _ leapfrog _ method is commonly used .",
    "we can use some number , @xmath14 , of these leapfrog steps , with some stepsize , @xmath13 , to propose a new state in the metropolis algorithm .",
    "this proposal is either accepted or rejected based on the metropolis acceptance probability .",
    "while hmc explores the target distribution more efficiently than random walk metropolis , it does not fully exploits its geometric properties .",
    "recently , @xcite proposed a new method , called riemannian manifold hmc ( rmhmc ) , that improvs the efficiency of standard hmc by automatically adapting to the local structure . to this end , they follow @xcite and propose hamiltonian monte carlo methods defined on the riemannian manifold endowed with metric @xmath15 , which is set to the fisher information matrix . more specifically",
    ", they define hamiltonian dynamics in terms of a position - specific mass matrix , @xmath3 , set to @xmath15 .",
    "the standard hmc method is a special case of rmhmc with @xmath16 . here",
    ", we use the notation @xmath17 to generally refer to a riemannian metric , which is not necessarily the fisher information . in the following section ,",
    "we introduce a natural modification of @xmath17 such that the associated hamiltonian dynamical system has a much greater chance of moving between isolated modes .",
    "consider a manifold @xmath18 endowed with a generic metric @xmath15 . given a differentiable curve @xmath19 \\to \\mathcal m$ ]",
    "one can define the arclength along this curve as @xmath20 under very general geometric assumptions , which are nearly always satisfied in statistical models , given any two points @xmath21 there exists a curve @xmath22\\to \\mathcal m$ ] satisfying the boundary conditions @xmath23 whose arclength is minimal among such curves .",
    "the length of such a minimal curve defines a distance function on @xmath18 . in euclidean space , where @xmath24 , the shortest curve connecting @xmath25 and @xmath26 is simply a straight line with the euclidean length @xmath27 .    as mentioned above ,",
    "while standard hmc algorithms explore the target distribution more efficiently , they nevertheless fail to move between isolated modes since these modes are separated by high energy barriers @xcite . to address this issue",
    ", we propose to replace the base metric @xmath17 with a new metric for which the distance between modes is shortened .",
    "this way , we can facilitate moving between modes by creating `` wormholes '' between them .",
    "let @xmath28 and @xmath29 be two modes of the target distribution .",
    "we define a straight line segment , @xmath30 , and refer to a small neighborhood ( tube ) of the line segment as a _",
    "wormhole_. next , we define a _ wormhole metric _ , @xmath31 , in the vicinity of the wormhole .",
    "the metric @xmath32 is an inner product assigning a non - negative real number to a pair of tangent vectors @xmath33 : @xmath34 . to shorten the distance in the direction of @xmath35 , we project both @xmath33 to the plane normal to @xmath35 and then take the euclidean inner product of those projected vectors .",
    "we set @xmath36 and define a _",
    "pseudo wormhole metric _",
    "@xmath37 as follows : @xmath38 { \\boldsymbol w}\\end{aligned}\\ ] ] note that @xmath39 is semi - positive definite ( degenerate at @xmath40 ) .",
    "we modify this metric to make it positive definite , and define the _ wormhole metric _ @xmath41 as follows : @xmath42 where @xmath43 is a small positive number .    to see that the wormhole metric @xmath41 in fact shortens the distance between @xmath28 and @xmath29 , consider a simple case where @xmath44 follows a straight line : @xmath45 $ ] . in this case , the distance under @xmath41 is @xmath46 which is much smaller than the euclidean distance .",
    "next , we define the overall metric , @xmath47 , for the whole parameter space of @xmath5 as a weighted sum of the base metric @xmath48 and the wormhole metric @xmath41 , @xmath49 where @xmath50 is a mollifying function designed to make the wormhole metric @xmath41 influential in the vicinity of the wormhole only . in this paper , we choose the following mollifier : @xmath51 where the _ influence factor _ @xmath52 , is a free parameter that can be tuned to modify the extent of the influence of @xmath41 : decreasing @xmath53 makes the influence of @xmath41 more restricted around the wormhole .",
    "the resulting metric leaves the base metric almost intact outside of the wormhole , while making the transition of the metric from outside to inside smooth . within the wormhole ,",
    "the trajectories are mainly guided in the wormhole direction @xmath54 : @xmath55 , so @xmath56 has the dominant eigen - vector @xmath54 ( with eigen - value @xmath57 ) , thereafter @xmath58 tends to be directed in @xmath54 .",
    "[ 2mode ]    we use the modified metric in rmhmc and refer to the resulting algorithm as wormhole hamiltonian monte carlo ( whmc ) .",
    "figure [ 2mode ] compares whmc to standard hmc based on the following illustrative example appeared in the paper by @xcite : @xmath59 here , we set @xmath60 , @xmath61 , and generate 1000 data points from the above model . in figure [ 2mode ] , the dots show the posterior samples of @xmath62 given the simulated data . while hmc is trapped in one mode , whmc moves easily between the two modes . for this example",
    ", we set @xmath63 to make whmc comparable to standard hmc .",
    "further , we use @xmath64 and @xmath65 for @xmath66 and @xmath53 respectively .    for more than two modes",
    ", we can construct a network of wormholes by connecting any two modes with a wormhole .",
    "alternatively , we can create a wormhole between neighboring modes only . in this paper",
    ", we define the neighborhood using a _ minimal spanning tree _ @xcite .",
    "the above method could suffer from two potential shortcomings in higher dimensions .",
    "first , the effect of wormhole metric could diminish fast as the sampler leaves one mode towards another mode .",
    "second , such mechanism , which modifies the dynamics in the existing parameter space , could interfere with the native hmc dynamics in the neighborhood of a wormhole .    to address the first issue , we add an external vector field to enforce the movement between modes .",
    "more specifically , we define a vector field , @xmath67 , in terms of the position parameter @xmath5 and the velocity vector @xmath68 as follows : @xmath69 with mollifier @xmath70 , where @xmath71 is the dimension , @xmath52 is the influence factor , and @xmath72 is a vicinity function indicating the euclidean distance from the line segment @xmath35 , @xmath73 the resulting vector field has three properties : 1 ) it is confined to a neighborhood of each wormhole , 2 ) it enforces the movement along the wormhole , and 3 ) its influence diminishes at the end of the wormhole when the sampler reaches another mode .     using whmc with a vector field @xmath74 to enforce moving between modes in higher dimensions.,width=528,height=192 ]    [ d100k10 ]    after adding the vector field , we modify the hamiltonian equation governing the evolution of @xmath75 as follows : @xmath76 we also need to adjust the metropolis acceptance probability accordingly since the transformation is not volume preserving .",
    "( more details are provided in the supplementary file . )",
    "figure [ d100k10 ] illustrates this approach based on sampling from a mixture of 10 gaussian distributions with dimension @xmath77 .    to address the second issue",
    ", we allow the wormholes to pass through an extra auxiliary dimension to avoid their interference with the existing hmc dynamics in the given parameter space .",
    "in particular we introduce an auxiliary variable @xmath78 corresponding to an auxiliary dimension .",
    "we use @xmath79 to denote the position parameters in the resulting @xmath80 dimensional space @xmath81 .",
    "@xmath82 can be viewed as random noise independent of @xmath0 and contributes @xmath83 to the total potential energy .",
    "correspondingly , we augment velocity @xmath84 with one extra dimension , denoted as @xmath85 . at the end of the sampling , we project @xmath86 to the original parameter space and",
    "discard @xmath82 .",
    "we refer to @xmath87 as the _ real world _ , and call @xmath88 the _ mirror world_. here , @xmath89 is half of the distance between the two worlds , and it should be in the same scale as the average distance between the modes . for most of the examples discussed here , we set @xmath90 .",
    "figure [ whpic ] illustrates how the two worlds are connected by networks of wormholes . when the sampler is near a mode @xmath91 in the real world , we build a wormhole network by connecting it to all the modes in the mirror world .",
    "similarly , we connect the corresponding mode in the mirror world , @xmath92 , to all the modes in the real world .",
    "such construction allows the sampler to jump from one mode in the real world to the same mode in the mirror world and vice versa .",
    "this way , the algorithm can effectively sample from the vicinity of a mode , while occasionally jumping from one mode to another .    ) . as an example",
    ", the cylinder shows a wormhole connecting mode 5 in the real world to its mirror image .",
    "the dashed lines show two sets of wormholes .",
    "the red lines shows the wormholes when the sampler is close to mode 1 in the real world , and the magenta lines show the wormholes when the sampler is close to mode 5 in the mirror world . ]",
    "the attached supplementary file provides the details of our algorithm ( algorithm 1 ) , along with the proof of convergence and its implementation in matlab .",
    "so far , we assumed that the locations of modes are known .",
    "this is of course not a realistic assumption in many situations . in this section",
    ", we relax this assumption by extending our method to search for new modes proactively and to update the network of wormholes dynamically . in general , however , allowing such adaptation to take place infinitely often will disturb the stationary distribution of the chain , rendering the process no longer markov @xcite . to avoid this issue , we use the _ regeneration _ method discussed by @xcite .    informally , a regenerative process `` starts again '' probabilistically at a set of times , called _ regeneration times _ @xcite . at regeneration times , the transition mechanism can be modified based on the entire history of the chain up to that point without disturbing the consistency of mcmc estimators .      the main idea behind finding regeneration times is to regard the transition kernel @xmath93 as a mixture of two kernels , @xmath94 and @xmath95 @xcite , @xmath96 where @xmath97 is an _ independence kernel _ , and the _ residual kernel _ @xmath98 is defined as follows : @xmath99 @xmath100 is the mixing coefficient between the two kernels such that @xmath101    now suppose that at iteration @xmath102 , the current state is @xmath103 . to implement this approach , we first generate @xmath104 using the original transition kernel @xmath105",
    ". then , we sample @xmath106 from a bernoulli distribution with probability @xmath107 if @xmath108 , a regeneration has occurred , then we discard @xmath104 and sample it from the independence kernel @xmath109 . at regeneration times , we redefine the dynamics using the past sample path .    ideally , we would like to evaluate regeneration times in terms of whmc s transition kernel . in general , however , this is quite difficult for such metropolis algorithms . on the other hand ,",
    "regenerations are easily achieved for the independence sampler ( i.e. , the proposed state is independent from the current state ) as long as the proposal distribution is close to the target distribution @xcite .",
    "therefore , we can specify a hybrid sampler that consists of the original proposal distribution ( here , whmc ) and the independence sampler , and adapt both proposal distributions whenever a regeneration is obtained on an independence - sampler step @xcite . in our method , we systematically alternate between whmc and the independence sampler while evaluating regeneration times based on the independence sampler only .",
    "as mentioned above , for this method to be effective , the proposal distribution for the independence sampler should be close to the target distribution .",
    "to this end , we follow @xcite and specify our independence sampler as a mixture of gaussians located at the previously identified modes . the covariance matrix for each mixture component",
    "is set to the inverse observed fisher information ( i.e. , hessian ) evaluated at the mode .",
    "the relative weight of each mixture component set to @xmath110 at the beginning , where @xmath111 is the number of previously identified modes through some initial optimization algorithm .",
    "the weights are updated at regeneration times to be proportional to the number of times the corresponding mode has been visited .",
    "more specifically , @xmath93 , @xmath100 and @xmath112 are defined as follows to satisfy : @xmath113 where @xmath114 is the independence proposal kernel , which specified using a mixture of gaussians with means fixed at the @xmath111 known modes prior to regeneration .",
    "algorithm 2 in the supplementary file shows the steps for this method .    .",
    "right panel : residual energy function at @xmath115.,title=\"fig:\",width=192,height=192 ] .",
    "right panel : residual energy function at @xmath115.,title=\"fig:\",width=192,height=192 ] .",
    "right panel : residual energy function at @xmath115.,title=\"fig:\",width=192,height=192 ]      when the chain regenerates , we can search for new modes , modify the transition kernel by including newly found modes in the mode library , and update the wormhole network accordingly . this way , starting with a limited number of modes ( identified by some preliminary optimization method ) , whmc could discover unknown modes on the fly without affecting the stationarity of the chain .",
    "to search for new modes after regeneration , we could simply apply an optimization algorithm to the original target density function @xmath2 with some random starting points .",
    "this , however , could lead to frequently rediscovering the known modes . to avoid this issue",
    ", we propose to remove / down - weight the known modes using the history of the chain up to the regeneration time and run an optimization algorithm on the resulting _ residual density _ , or equivalently , on the corresponding _ residual energy _",
    "( i.e. , minus log of density ) .",
    "to this end , we fit a mixture of gaussians with the best knowledge of modes ( locations , hessians and relative weights ) prior to the regeneration .",
    "the _ residual density _ function could be simply defined as @xmath116 with the corresponding _ residual potential energy _ as follows , @xmath117 where the constant @xmath118 is used to make the term inside the log function positive . to avoid completely flat regions ( e.g. , when a gaussian distribution provides a good approximation around the mode ) , which could cause gradient - based optimization methods to fail , we could use the following _ tempered residual potential energy _ instead : @xmath119 where @xmath120 is the temperature .",
    "figure [ engysurgt ] illustrates this concept .",
    "when the optimizer finds new modes , they are added to the existing mode library , and the wormhole network is updated accordingly .",
    "mixtures of @xmath71-dimensional gaussians . left panel : rem ( along with 95% confidence interval based on 10 mcmc chains ) for varying number of mixture components , @xmath121 , with fixed dimension , @xmath122 .",
    "right panel : rem ( along with 95% confidence interval based on 10 mcmc chains ) for varying number of dimensions , @xmath123 , with fixed number of mixture components , @xmath124.,title=\"fig:\",width=230,height=192 ]   mixtures of @xmath71-dimensional gaussians . left panel : rem ( along with 95% confidence interval based on 10 mcmc chains ) for varying number of mixture components , @xmath121 , with fixed dimension , @xmath122 .",
    "right panel : rem ( along with 95% confidence interval based on 10 mcmc chains ) for varying number of dimensions , @xmath123 , with fixed number of mixture components , @xmath124.,title=\"fig:\",width=230,height=192 ]    [ kdplot ]",
    "in this section , we evaluate the performance of our method , henceforth called wormhole hamiltonian monte carlo ( whmc ) , using three examples . the first example",
    "involves sampling from mixtures of gaussian distributions with varying number of modes and dimensions . in this example , which is also discussed by @xcite , the locations of modes are assumed to be known .",
    "the second example , which was originally proposed by @xcite , involves inference regarding the locations of sensors in a network . for our third example",
    ", we also use mixtures of gaussian distributions , but this time we assume that the locations of modes are unknown .",
    "we evaluate our method s performance by comparing it to regeneration darting monte carlo ( rdmc ) @xcite , which is one of the most recent algorithms designed for sampling from multimodal distributions based on the darting monte carlo ( dmc ) @xcite approach .",
    "dmc defines high density regions around the modes . when the sampler enters these regions , a jump between the regions",
    "will be attempted .",
    "rdmc enriches the dmc method by using the regeneration approach @xcite .",
    "we compare the two methods ( i.e. , whmc and rdmc ) in terms of relative error of mean ( rem ) @xcite , which summarizes the errors in approximating the expectation of variables across all dimensions , and its value at time @xmath102 is @xmath125 . here ,",
    "@xmath126 is the mean of mcmc samples at time @xmath102 and @xmath127 is the true mean . because rdmc uses standard hmc algorithm with a flat metric , we also use the baseline metric @xmath128 to make the two algorithms comparable .",
    "our approach , however , can be easily extended to other metrics such as fisher information .",
    "[ timeplotwsn ]      first , we evaluate the performance of our method based on sampling from @xmath129 mixtures of @xmath71-dimensional gaussian distributions with _ known _ modes .",
    "( we relax this assumption later . ) the means of these distributions are randomly generated from @xmath71-dimensional uniform distributions such that the average pairwise distances remains around 20 .",
    "the corresponding covariance matrices are constructed in a way that mixture components have different density functions .",
    "simulating samples from the resulting @xmath71 dimensional mixture of @xmath129 gaussians is challenging because the modes are far apart and the high density regions have different shapes .",
    "the left panel of figure [ kdplot ] compares the two methods for varying number of mixture components , @xmath121 , with fixed dimension ( @xmath122 ) .",
    "the right panel shows the results for varying number of dimensions , @xmath123 , with fixed number of mixture components ( @xmath124 ) . for both scenarios ,",
    "we stop the two algorithms after 500 seconds and compare their rem .",
    "as we can see , whmc has substantially lower rem compared to rdmc , especially when the number of modes and dimensions increase .      for our second example",
    ", we use a problem previously discussed by @xcite and @xcite .",
    "we assume that @xmath130 sensors are scattered in a planar region with @xmath131 locations denoted as @xmath132 .",
    "the distance @xmath133 between a pair of sensors @xmath134 is observed with probability @xmath135 .",
    "if the distance is in fact observed ( @xmath136 ) , then @xmath133 follows a gaussian distribution @xmath137 with small @xmath138 ; otherwise @xmath139 .",
    "that is , @xmath140 where @xmath141 is a binary indicator set to 1 if the distance between @xmath142 and @xmath143 is observed .",
    "given a set of observations @xmath133 and prior distribution of @xmath144 , which is assumed to be uniform in this example , it is of interest to infer the posterior distribution of all the sensor locations . following @xcite , we set @xmath145 , and add three additional base sensors with known locations to avoid ambiguities of translation , rotation , and negation ( mirror symmetry ) .",
    "the location of the 8 sensors form a multimodal distribution with dimension @xmath146 .",
    "figure [ timeplotwsn ] shows the posterior samples based on the two methods .",
    "as we can see , rdmc very rarely visits one of the modes ( shown in red in the top middle part ) ; whereas , whmc generates enough samples from this mode to make it discernible . as a result",
    ", whmc converges to a substantially lower rem ( @xmath147 ) compared to rdmc ( @xmath148 ) after 500 seconds .",
    "mixtures of @xmath71-dimensional gaussians with @xmath122 ( left panel ) and @xmath77 ( right panel).,title=\"fig:\",width=230,height=192 ]   mixtures of @xmath71-dimensional gaussians with @xmath122 ( left panel ) and @xmath77 ( right panel).,title=\"fig:\",width=230,height=192 ]    [ unknownmodes ]      we now evaluate our method s performance in terms of searching for new modes and updating the network of wormholes . for this example",
    ", we simulate a mixture of 10 @xmath71-dimensional gaussian distributions , with @xmath149 , and compare our method to rdmc . while rdmc runs four parallel hmc chains initially to discover a subset of modes and to fit a truncated gaussian distribution around each identified mode , we run four parallel optimizers ( different starting points ) using the bfgs method . at regeneration times",
    ", each chain of rdmc uses the dirichlet process mixture model to fit a new truncated gaussian around modes and possibly identify new modes .",
    "we on the other hand run the bgfs algorithm based on the residual energy function ( with @xmath115 ) to discover new modes for each chain .",
    "figure [ unknownmodes ] shows whmc reduces rem much faster than rdmc for both @xmath150 and @xmath77 . here , the recorded time ( horizontal axis ) accounts for the computational overhead for adapting the transition kernels . for @xmath150 ,",
    "our method has a substantially lower rem compared to rdmc . for @xmath77 , while our method identifies new modes over time and reduces rem substantially , rdmc fails to identify new modes so as a result its rem remains high over time .",
    "we have proposed a new algorithm for sampling from multimodal distributions . using empirical results ,",
    "we have shown that our method performs well in high dimensions .",
    "our method involves several parameters that require tuning .",
    "however , these parameters can be adjusted at regeneration times without affecting the stationary distribution .",
    "although we used a flat base metric ( i.e. , @xmath151 ) in the examples discussed in this paper , our method can be easily extended by specifying a more informative base metric ( e.g. , fisher information ) that adapts to local geometry .",
    "this material is based upon work supported by the national science foundation under grant no .",
    "we would like to thank sungin ahn for sharing his codes for the rdmc algorithm .",
    "s.  ahn , y.  chen , and m.  welling . .",
    "in _ proceedings of the 16th international conference on artificial intelligence and statistics ( ai stat ) _ , 2013 .",
    "s.  amari and h.  nagaoka .",
    "_ methods of information geometry _ , volume 191 of _ translations of mathematical monographs_. oxford university press , 2000 .    c.  j. f.  ter braak .",
    "a markov chain monte carlo version of the genetic algorithm differential evolution : easy bayesian computing for real parameter spaces .",
    "_ statistics and computing _ , 160 ( 3):0 239249 , 2006 .",
    "anthony  e. brockwell and joseph  b. kadane .",
    "identification of regeneration times in mcmc simulation , with application to adaptive schemes .",
    "_ journal of computational and graphical statistics _ , 14:0 436458 , 2005 .",
    "g.  celeux , m.  hurn , and c.  p. robert .",
    "computational and inferential difficulties with mixture posterior distributions . _ journal of the american statistical association _ , 95:0 957970 , 2000 .",
    "r.  v. craiu , jeffrey r. , and chao y. learn from thy neighbor : parallel - chain and regional adaptive mcmc . _ journal of the american statistical association _ , 1040 ( 488):0 14541466 , 2009 .",
    "s.  duane , a.  d. kennedy , b  j. pendleton , and d.  roweth . .",
    "_ physics letters b _ , 1950 ( 2):0 216  222 , 1987 .",
    "a.  e. gelfand and d.  k. dey .",
    "bayesian model choice : asymptotic and exact calculation .",
    "_ journal of the royal statistical society .",
    "series b. _ , 560 ( 3):0 501514 , 1994 .",
    "walter  r. gilks , gareth  o. roberts , and sujit  k. sahu .",
    "adaptive markov chain monte carlo through regeneration .",
    "_ journal of the american statistical association _ , 930 ( 443):0 pp . 10451054 , 1998 .",
    "issn 01621459 .",
    "m.  girolami and b.  calderhead . .",
    "_ journal of the royal statistical society , series b _ , ( with discussion ) 730 ( 2):0 123214 , 2011 .    peter  j. green .",
    "reversible jump markov chain monte carlo computation and bayesian model determination .",
    "_ biometrika _ , 82:0 711732 , 1995 .",
    "g.  e. hinton , m.  welling , and a.  mnih .",
    "wormholes improve contrastive divergence . in _ advances in neural information processing systems 16 _ , 2004 .",
    "a.  t. ihler , j.  w.  fisher iii , r.  l. moses , and a.  s. willsky .",
    "nonparametric belief propagation for self - localization of sensor networks .",
    "_ ieee journal on selected areas in communications _ , 230 ( 4):0 809819 , 2005 .",
    "s.  kirkpatrick , c.  d. gelatt , and m.  p. vecchi . .",
    "_ science , number 4598 , 13 may 1983 _ , 2200 ( 4598):0 671680 , 1983 .    j.  kleinberg and e.  tardos . _ algorithm design_. addison - wesley longman publishing co. , inc . ,",
    "boston , ma , usa , 2005 .",
    "isbn 0321295358 .",
    "s.  lan , v.  stathopoulos , b.  shahbaba , and m.  girolami . .",
    "arxiv.org/abs/1211.3759 , 2012 .",
    "k.  b. laskey and j.  w. myers . .",
    "_ machine learning _",
    ", 50:0 175196 , 2003 .",
    "b.  leimkuhler and s.  reich .",
    "_ simulating hamiltonian dynamics_. cambridge university press , 2004 .",
    "jun  s. liu .",
    "_ monte carlo strategies in scientific computing _ , chapter molecular dynamics and hybrid monte carlo .",
    "springer - verlag , 2001 .    per mykland , luke tierney , and bin yu .",
    "regeneration in markov chain samplers .",
    "_ journal of the american statistical association _ , 900 ( 429):0 pp . 233241 , 1995 .",
    "issn 01621459 .",
    "r.  m. neal .",
    "_ probabilistic inference using markov chain monte carlo methods_. technical report crg - tr-93 - 1 , department of computer science , university of toronto , 1993 .",
    "r.  m. neal .",
    "annealed importance sampling . _ statistics and computing _ , 110 ( 2):0 125139 , 2001",
    ".    r.  m. neal . .",
    "in s.  brooks , a.  gelman , g.  jones , and x.  l. meng , editors , _ handbook of markov chain monte carlo_. chapman and hall / crc , 2010",
    ".    radford  m. neal .",
    "_ bayesian learning for neural networks_. springer - verlag new york , inc . ,",
    "secaucus , nj , usa , 1996 .",
    "isbn 0387947248 .",
    "esa nummelin .",
    "_ general irreducible markov chains and non - negative operators _ , volume  83 of _ cambridge tracts in mathematics_. cambridge university press , 1984 .",
    "d.  rudoy and p.  j. wolfe .",
    "monte carlo methods for multi - modal distributions . in _ signals , systems and computers , 2006 .",
    "acssc 06 .",
    "fortieth asilomar conference on _ , pages 20192023 , 2006 .    c.  sminchisescu and b.  triggs . building roadmaps of local minima of visual models . in _ in european conference on computer vision _ ,",
    "pages 566582 , 2002 .    c.  sminchisescu and m.  welling .",
    "generalized darting monte carlo . _ pattern recognition _ , 440 ( 10 - 11 ) , 2011 .",
    "g.  r. warnes . .",
    "technical report technical report no .",
    "395 , university of washington , 2001 .",
    "m.  welling and y.  w. teh .",
    "ayesian learning via stochastic gradient langevin dynamics . in _ proceedings of the international conference on machine learning _ , 2011 .",
    "appendix    in this supplementary document , we provide details of our proposed wormhole hamiltonian monte carlo ( whmc ) algorithm and prove its convergence to the stationary distribution . for simplicity ,",
    "we assume that @xmath152 .",
    "our results can be extended to more general riemannian metrics .    in what follows",
    ", we first prove the convergence of our method when an external vector field is added to the dynamics .",
    "next , we prove the convergence for our final algorithm , where besides an external vector field , we include an auxiliary dimension along which the network of wormholes are constructed .",
    "finally , we provide our algorithm to identify regeneration times .",
    "as mentioned in the paper , in high dimensional problems with isolated modes , the effect of wormhole metric could diminish fast as the sampler leaves one mode towards another mode . to avoid this issue ,",
    "we have extended our method by including a vector field , @xmath153 , which depends on a vicinity function illustrated in figure [ picvic ] .",
    "the resulting dynamics facilitates movements between modes , @xmath154    we solve using the generalized leapfrog integrator @xcite : @xmath155 \\label{xflmc:0}\\\\ & { \\bf v}^{(\\ell+1 ) } & & = { \\bf v}^{(\\ell+1/2 ) } - \\frac{{\\varepsilon}}{2 } \\nabla_{{\\boldsymbol{\\theta } } } u({{\\boldsymbol{\\theta}}}^{(\\ell+1 ) } ) \\label{xflmc:2}\\end{aligned}\\ ] ] where @xmath156 is the index for leapfrog steps , and @xmath157 denotes the current value of @xmath158 after half a step of leapfrog .",
    "the implicit equation can be solved by the fixed point iteration .",
    "the integrator - is time reversible and numerically stable ; however , it is not volume preserving . to fix this issue",
    ", we can adjust the metropolis acceptance probability with the jacobian determinant of the mapping @xmath159 given by - in order to satisfy the detailed balance condition .",
    "denote @xmath160 .",
    "given the corresponding hamiltonian function , @xmath161 , we define @xmath162 and prove the following proposition ( see * ? ? ?",
    "[ prop : dbwda ] let @xmath163 be the proposal according to some time reversible integrator @xmath164 for dynamics . then the detailed balance condition holds given the following adjusted acceptance probability : @xmath165    to prove detailed balance , we need to show @xmath166 the following steps show that this condition holds : @xmath167     in equation ( 6 ) of our paper . ]",
    "we implement - for @xmath14 steps to generate a proposal @xmath168 and accept it with the following adjusted probability : @xmath169 where the jacobian determinant , @xmath170 , can be calculated through the following wedge product : @xmath171^{-1 } [ { \\bf i}+\\frac{{\\varepsilon}}{2 } \\nabla_{{{\\boldsymbol{\\theta}}}^{\\mathsf t } } { \\bf f}({{\\boldsymbol{\\theta}}}^{(\\ell)},{\\bf v}^{(\\ell+1/2)})]\\ ; d{{\\boldsymbol{\\theta}}}^{(\\ell ) } \\wedge d{\\bf v}^{(\\ell)}\\ ] ] with @xmath172 ( see * ? ? ? * for more details . )",
    "suppose that the current position , @xmath173 , of the sampler is near a mode denoted as @xmath174 .",
    "a network of wormholes connects this mode to all the modes in the opposite world @xmath175 .",
    "wormholes in the augmented space starting from this mode may still interfere each other since they intersect . to resolve this issue , instead of deterministically weighing wormholes by the vicinity function ( 6 )",
    ", we use the following random vector field @xmath176 : @xmath177 where @xmath13 is the stepsize , @xmath178 is the kronecker delta function , and @xmath179 . here",
    ", the vicinity function @xmath180 along the @xmath111-th wormhole is defined similarly to equation ( 6 ) , @xmath181 where @xmath182 .    for each update , @xmath176 is either set to @xmath183 or @xmath184 according to the position dependent probabilities defined in terms of @xmath185 .",
    "therefore , we write the hamiltonian dynamics in the extended space as follows : @xmath186 we use - to numerically solve , but replace with the following equation : @xmath187\\ ] ] note that this is an implicit equation , which can be solved using the fixed point iteration approach @xcite .",
    "according to the above dynamic , at each leapfrog step , @xmath156 , the sampler either stays at the vicinity of @xmath174 or proposes a move towards a mode @xmath188 in the opposite world depending on the values of @xmath189 and @xmath190 .",
    "for example , if @xmath191 , and @xmath192 , then equation becomes @xmath193 which indicates that a move to the @xmath111-th mode in the opposite wold has in fact occurred .",
    "note that the movement @xmath194 in this case is discontinuous since @xmath195 therefore , in such cases , there will be an energy gap , @xmath196 , between the two states .",
    "we need to adjust the metropolis acceptance probability to account for the resulting energy gap . does not apply because jacobian determinant is not well defined for discontinuous movement . ]",
    "further , we limit the maximum number of jumps within each iteration of mcmc ( i.e. , over @xmath14 leapfrog steps ) to 1 so the sampler can explore the vicinity of the new mode before making another jump .",
    "algorithm [ alg : whmc ] provides the details of this approach .",
    "we note that according to the definition of @xmath176 and equation , the jump occurs randomly .",
    "we use @xmath197 to denote the step at which the sampler jumps .",
    "that is , @xmath197 randomly takes a value in @xmath198 depending on @xmath199 .",
    "when there is no jump along the trajectory , we set @xmath200 , @xmath201 , and the algorithm reduces to standard hmc . in the following , we first prove the detailed balance condition when a jump happens , and then use it to prove the convergence of the algorithm to the stationary distribution .    when a mode jumping occurs at some fixed step @xmath197 , we can divide the @xmath14 leapfrog steps into three parts : @xmath202 steps continuous movement according to standard hmc , 1 step discontinuous jump , and @xmath203 steps according to standard hmc in the opposite world .",
    "note that the metropolis acceptance probability in standard hmc can be written as @xmath204 each summand @xmath205 is small ( @xmath206 ) except for the @xmath197-th one , where there is an energy gap , @xmath207 .",
    "given that the jump happens at the @xmath197-th step , we should remove the @xmath197-th summand from the acceptance probability : @xmath208 that is , we only count the acceptance probability for the first @xmath202 and the last @xmath203 steps of continuous movement in standard hmc ; at @xmath209 step , we  reset \" the energy level by accounting for the energy gap @xmath207 .",
    "therefore , the following proposition is true .",
    "note that @xmath220 is either an accepted proposal or the current state after a rejection .",
    "therefore , @xmath221",
    "\\\\ = & \\iiint h(\\tilde{{\\boldsymbol{\\theta}}}^ * ) [ \\alpha(\\hat t_{1:l}^{-1}(\\tilde{\\bf z}^ * ) , \\tilde{\\bf z}^ * ) \\mathbb p(d\\hat t_{1:l}^{-1}(\\tilde{\\bf z}^ * ) ) \\mathbb p(d\\hat t_{1+\\ell':l}^{-1}(\\tilde{\\bf z}^ * ) ) + ( 1-\\alpha(\\tilde{\\bf z}^*,\\hat t_{1:l}(\\tilde{\\bf z}^ * ) ) ) \\mathbb p(d\\tilde{\\bf z}^ * ) \\mathbb p(d\\hat t_{1:\\ell'}(\\tilde{\\bf z}^ * ) ) ] d\\ell'\\\\ = & \\int h(\\tilde{{\\boldsymbol{\\theta}}}^ * ) \\mathbb p(d\\tilde{\\bf z}^ * ) + \\\\ & \\iiint h(\\tilde{{\\boldsymbol{\\theta}}}^*)[\\alpha(\\hat t_{1:l}^{-1}(\\tilde{\\bf z}^ * ) , \\tilde{\\bf z}^*)\\mathbb p(d \\hat t_{1:l}^{-1}(\\tilde{\\bf z}^ * ) ) \\mathbb p(d\\hat t_{1+\\ell':l}^{-1}(\\tilde{\\bf z}^ * ) ) - \\alpha(\\tilde{\\bf z}^*,\\hat t_{1:l}(\\tilde{\\bf z}^ * ) ) \\mathbb p(d\\tilde{\\bf z}^ * ) \\mathbb p(d\\hat t_{1:\\ell'}(\\tilde{\\bf z}^ * ) ) ] d\\ell ' \\end{split}\\ ] ] therefore , it suffices to prove that @xmath222    based on its construction , @xmath212 is time reversible for all @xmath156 .",
    "denote the involution @xmath223 .",
    "we have @xmath224 .",
    "further , because @xmath225 is quadratic in @xmath183 , we have @xmath226 . therefore @xmath227 .",
    "then the left hand side of becomes @xmath228 on the other hand , by the detailed balance condition , the right hand side of becomes @xmath229 note that @xmath230 since both @xmath231 and @xmath232 are leapfrog steps of standard hmc ( no jump ) .",
    "the difference in the numbers of leapfrog steps ( i.e. , @xmath203 and @xmath202 ) does not affect the stationarity since the number of leapfrog steps can be randomized in hmc @xcite .",
    "therefore , we have @xmath233 , which proves .",
    "prepare the modes @xmath234 set @xmath235 sample velocity @xmath236 calculate @xmath237 set @xmath238 , @xmath239 , @xmath240 .",
    "@xmath241 @xmath242 find the closest mode @xmath243 and build a network connecting it to all modes @xmath244 in the opposite world calculate @xmath245 sample @xmath246 set @xmath247 choose one of the @xmath111 wormholes according to probability @xmath248 and set @xmath249 @xmath250 $ ] @xmath251 @xmath252 if a jump has occurred , set @xmath253 and calculate energy gap @xmath207 .",
    "calculate @xmath254 @xmath255 accept or reject the proposal @xmath256 according to @xmath257    initially search modes @xmath258 sample @xmath259 as the current state according to whmc ( algorithm [ alg : whmc ] ) .",
    "fit a mixture of gaussians @xmath260 with known modes , hessians and relative weights .",
    "propose @xmath261 and accept it with probability @xmath262 .",
    "determine if @xmath263 is a regeneration using ( 9)-(12 ) with @xmath264 and @xmath265 .",
    "search new modes by minimizing @xmath266 ; if new modes are discovered , update the mode library , wormhole network , and @xmath260 .",
    "+ discard @xmath263 , sample @xmath267 as in ( 12 ) using rejection sampling .",
    "set @xmath268 .",
    "set @xmath269 ."
  ],
  "abstract_text": [
    "<S> _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ in machine learning and statistics , probabilistic inference involving multimodal distributions is quite difficult . </S>",
    "<S> this is especially true in high dimensional problems , where most existing algorithms can not easily move from one mode to another . to address this issue </S>",
    "<S> , we propose a novel bayesian inference approach based on markov chain monte carlo . </S>",
    "<S> our method can effectively sample from multimodal distributions , especially when the dimension is high and the modes are isolated . to this end , it exploits and modifies the riemannian geometric properties of the target distribution to create _ wormholes _ connecting modes in order to facilitate moving between them . </S>",
    "<S> further , our proposed method uses the regeneration technique in order to adapt the algorithm by identifying new modes and updating the network of wormholes without affecting the stationary distribution . to find new modes , as opposed to rediscovering those previously identified </S>",
    "<S> , we employ a novel mode searching algorithm that explores a _ residual energy _ function obtained by subtracting an approximate gaussian mixture density ( based on previously discovered modes ) from the target density function . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ </S>"
  ]
}