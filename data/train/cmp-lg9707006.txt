{
  "article_text": [
    "finite - state automata have been successfully applied in many areas of computational linguistics .",
    "this paper describes two algorithms which approximate a _ hidden markov model _ ( hmm ) used for part - of - speech tagging by a",
    "_ finite - state transducer _",
    ". these algorithms may be useful beyond the current description on any kind of analysis of written or spoken language based on both finite - state technology and hmms , such as corpus analysis , speech recognition , etc .",
    "both algorithms have been fully implemented .",
    "an hmm used for tagging encodes , like a transducer , a relation between two languages .",
    "one language contains sequences of ambiguity classes obtained by looking up in a lexicon all words of a sentence .",
    "the other language contains sequences of tags obtained by statistically disambiguating the class sequences . from the outside",
    ", an hmm tagger behaves like a sequential transducer that deterministically maps every class sequence to a tag sequence , e.g. : @xmath0 ~[adj , noun ] ~[adj , noun ] ~ ...... ~[end ] } }   { \\tt{{\\rule{5mm}{0mm}}det{\\rule{12mm}{0mm}}adj{\\rule{12mm}{0mm}}noun{\\rule{3mm}{0 mm } } ~ ...... {\\rule{3mm}{0mm}}end}}\\ ] ]    the aim of the conversion is not to generate fsts that behave in the same way , or in as similar a way as possible like hmms , but rather fsts that perform tagging in as accurate a way as possible .",
    "the motivation to derive these fsts from hmms is that hmms can be trained and converted with little manual effort .",
    "the tagging speed when using transducers is up to five times higher than when using the underlying hmms .",
    "the main advantage of transforming an hmm is that the resulting transducer can be handled by finite state calculus . among others ,",
    "it can be composed with transducers that encode :    * correction rules for the most frequent tagging errors which are automatically generated ( brill , 1992 ; roche and schabes , 1995 ) or manually written ( chanod and tapanainen , 1995 ) , in order to significantly improve tagging accuracy .",
    "these rules may include long - distance dependencies not handled by hmm taggers , and can conveniently be expressed by the replace operator ( kaplan and kay , 1994 ; karttunen , 1995 ; kempe and karttunen , 1996 ) . * further steps of text analysis , e.g. light parsing or extraction of noun phrases or other phrases ( at - mokhtar and chanod , 1997 ) .",
    "these compositions enable complex text analysis to be performed by a single transducer .",
    "an hmm transducer builds on the data ( probability matrices ) of the underlying hmm .",
    "the accuracy of this data has an impact on the tagging accuracy of both the hmm itself and the derived transducer .",
    "the training of the hmm can be done on either a tagged or untagged corpus , and is not a topic of this paper since it is exhaustively described in the literature ( bahl and mercer , 1976 ; church , 1988 ) .",
    "an hmm can be identically represented by a weighted fst in a straightforward way .",
    "we are , however , interested in non - weighted transducers .",
    "this section presents a method that approximates a ( 1st order ) hmm by a transducer , called _ n - type _ approximation .    like in an hmm , we take into account initial probabilities @xmath1 , transition probabilities @xmath2 and class ( i.e. observation symbol ) probabilities @xmath3 .",
    "we do , however , not estimate probabilities over paths .",
    "the tag of the first word is selected based on its initial and class probability .",
    "the next tag is selected on its transition probability given the first tag , and its class probability , etc . unlike in an hmm",
    ", once a decision on a tag has been made , it influences the following decisions but is itself irreversible .    a transducer encoding this behaviour",
    "can be generated as sketched in figure [ f - nfst ] . in this example",
    "we have a set of three classes , @xmath4 with the two tags @xmath5 and @xmath6 , @xmath7 with the three tags @xmath8 , @xmath9 and @xmath10 , and @xmath11 with one tag @xmath12 .",
    "different classes may contain the same tag , e.g. @xmath6 and @xmath10 may refer to the same tag .        for every possible pair of a class and a tag ( e.g. @xmath13 or : noun ) a state is created and labelled with this same pair ( fig .",
    "[ f - nfst ] ) .",
    "an initial state which does not correspond with any pair , is also created .",
    "all states are final , marked by double circles .    for every state , as many outgoing arcs are created as there are classes ( three in fig .",
    "[ f - nfst ] ) . each such arc for a particular class points to the most probable pair of this same class .",
    "if the arc comes from the initial state , the most probable pair of a class and a tag ( destination state ) is estimated by : @xmath14    if the arc comes from a state other than the initial state , the most probable pair is estimated by : @xmath15    in the example ( fig .",
    "[ f - nfst ] ) @xmath13 is the most likely pair of class @xmath4 , and @xmath16 the most likely pair of class @xmath7 when coming from the initial state , and @xmath17 the most likely pair of class @xmath7 when coming from the state of @xmath18 .",
    "every arc is labelled with the same symbol pair as its destination state , with the class symbol in the upper language and the tag symbol in the lower language .",
    "e.g. every arc leading to the state of @xmath13 is labelled with c@xmath19:t@xmath20 .",
    "finally , all state labels can be deleted since the behaviour described above is encoded in the arc labels and the network structure . the network can be minimized and determinized .",
    "+ we call the model an _ n1-type model _ , the resulting fst an _",
    "n1-type transducer _ and the algorithm leading from the hmm to this transducer , an _ n1-type approximation _ of a 1st order hmm .    adapted to a 2nd order hmm",
    ", this algorithm would give an _",
    "n2-type approximation_. adapted to a zero order hmm , which means only to use class probabilities @xmath3 , the algorithm would give an _",
    "n0-type approximation_.    n - type transducers have deterministic states only .",
    "this section presents a method that approximates an hmm by a transducer , called _ s - type _ approximation .    tagging a sentence based on a 1st order hmm includes finding the most probable tag sequence @xmath21 given the class sequence @xmath22 of the sentence .",
    "the joint probability of @xmath22 and @xmath21 can be estimated by : @xmath23    the decision on a tag of a particular word can not be made separately from the other tags .",
    "tags can influence each other over a long distance via transition probabilities .",
    "often , however , it is unnecessary to decide on the tags of the whole sentence at once . in the case of a 1st order hmm",
    ", unambiguous classes ( containing one tag only ) , plus the sentence beginning and end positions , constitute barriers to the propagation of hmm probabilities . two tags with one or more barriers inbetween do not influence each other s probability .      to tag a sentence",
    ", one can split its class sequence at the barriers into subsequences , then tag them separately and concatenate them again .",
    "the result is equivalent to the one obtained by tagging the sentence as a whole .",
    "we distinguish between initial and middle subsequences .",
    "the final subsequence of a sentence is equivalent to a middle one , if we assume that the sentence end symbol ( .  or !  or  ? ) always corresponds to an unambiguous class @xmath24 .",
    "this allows us to ignore the meaning of the sentence end position as an hmm barrier because this role is taken by the unambiguous class @xmath24 at the sentence end .",
    "an initial subsequence @xmath25 starts with the sentence initial position , has any number ( incl .",
    "zero ) of ambiguous classes @xmath26 and ends with the first unambiguous class @xmath24 of the sentence .",
    "it can be described by the regular expression ] : @xmath27    the joint probability of an initial class subsequence @xmath25 of length @xmath28 , together with an initial tag subsequence @xmath29 , can be estimated by : @xmath30    a middle subsequence @xmath31 starts immediately after an unambiguous class @xmath24 , has any number ( incl .",
    "zero ) of ambiguous classes @xmath26 and ends with the following unambiguous class @xmath24 : @xmath32    for correct probability estimation we have to include the immediately preceding unambiguous class @xmath24 , actually belonging to the preceding subsequence @xmath25 or @xmath31 .",
    "we thereby obtain an extended middle subsequence@xmath33 : @xmath34    the joint probability of an extended middle class subsequence @xmath35 of length @xmath36 , together with a tag subsequence @xmath37 , can be estimated by : @xmath38      to build an s - type transducer , a large number of initial class subsequences @xmath25 and extended middle class subsequences @xmath35 are generated in one of the following two ways : + * ( a ) extraction from a corpus *    based on a lexicon and a guesser , we annotate an untagged training corpus with class labels . from every sentence , we extract the initial class subsequence @xmath25 that ends with the first unambiguous class @xmath24 ( eq . [ e - init - cseq ] ) , and all extended middle subsequences @xmath35 ranging from any unambiguous class @xmath24 ( in the sentence ) to the following unambiguous class ( eq . [ e - emid - cseq ] ) .    a frequency constraint ( threshold ) may be imposed on the subsequence selection , so that the only subsequences retained are those that occur at least a certain number of times in the training corpus . + * ( b ) generation of possible subsequences *    based on the set of classes , we generate all possible initial and extended middle class subsequences , @xmath25 and @xmath35 ( eq . [ e - init - cseq ] , [ e - emid - cseq ] ) up to a defined length . + every class subsequence @xmath25 or @xmath35 is first disambiguated based on a 1st order hmm , using the viterbi algorithm ( viterbi , 1967 ; rabiner , 1990 ) for efficiency , and then linked to its most probable tag subsequence @xmath29 or @xmath37 by means of the cross product operation@xmath33 : @xmath39 @xmath40    in all extended middle subsequences @xmath41 , e.g. : @xmath42 @xmath43 ~[adj , noun ] ~[adj , noun ] ~[noun ] } }     { \\tt{det{\\rule{9mm}{0mm}}adj{\\rule{12mm}{0mm}}adj{\\rule{9mm}{0mm}}noun } }    \\nonumber\\end{aligned}\\ ] ]    the first class symbol on the upper side and the first tag symbol on the lower side , will be marked as an extension that does not really belong to the middle sequence but which is necessary to disambiguate it correctly .",
    "example ( [ exm - mark-1 ] ) becomes : @xmath44 @xmath45 ~[adj , noun ] ~[adj , noun ] ~[noun ] } }     { \\tt{0.det{\\rule{9mm}{0mm}}adj{\\rule{12mm}{0mm}}adj{\\rule{9mm}{0mm}}noun } }    \\nonumber\\end{aligned}\\ ] ]    we then build the union @xmath46 of all initial subsequences @xmath47 and the union @xmath48 of all extended middle subsequences @xmath41 , and formulate a preliminary sentence model : @xmath49    in which all middle subsequences @xmath50 are still marked and extended in the sense that all occurrences of all unambiguous classes are mentioned twice : once unmarked as @xmath24 at the end of every sequence @xmath25 or @xmath51 , and the second time marked as @xmath52 at the beginning of every following sequence @xmath51 .",
    "the upper side of the sentence model @xmath53 describes the complete ( but extended ) class sequences of possible sentences , and the lower side of @xmath53 describes the corresponding ( extended ) tag sequences .    to ensure a correct concatenation of initial and middle subsequences , we formulate a concatenation constraint for the classes : @xmath54 \\;]_j\\ ] ]    stating that every middle subsequence must begin with the same marked unambiguous class @xmath52 ( e.g.    0    ' '' ''    .    ' '' ''    [ det ]    ) which occurs unmarked as @xmath24 ( e.g. ) at the end of the preceding subsequence since both symbols refer to the same occurrence of this unambiguous class .",
    "having ensured correct concatenation , we delete all marked classes on the upper side of the relation by means of @xmath55 \\invrepl \\left[\\ ; \\bigcup\\limits_j \\ ; [ c_u^0]_j \\;\\right]\\ ] ]    and all marked tags on the lower side by means of @xmath56_j \\;\\right ] \\repl [ \\;]\\ ] ]    by composing the above relations with the preliminary sentence model , we obtain the final sentence model@xmath33 : @xmath57    we call the model an _ s - type model _ , the corresponding fst an _ s - type transducer _ , and the whole algorithm leading from the hmm to the transducer , an _ s - type approximation _ of an hmm .",
    "the s - type transducer tags any corpus which contains only known subsequences , in exactly the same way , i.e. with the same errors , as the corresponding hmm tagger does .",
    "however , since an s - type transducer is incomplete , it can not tag sentences with one or more class subsequences not contained in the union of the initial or middle subsequences .",
    "an incomplete s - type transducer @xmath58 can be completed with subsequences from an auxiliary , complete n - type transducer @xmath59 as follows :    first , we extract the union of initial and the union of extended middle subsequences , @xmath60 and @xmath61 from the primary s - type transducer @xmath58 , and the unions @xmath62 and @xmath63 from the auxiliary n - type transducer @xmath59 . to extract the union @xmath46 of initial subsequences",
    "we use the following filter : @xmath64 \\kleenestar        \\;\\ ; { \\langle c_u , t \\rangle } \\;\\ ; [ \\ ; ? \\symp [ \\ ; ] \\ ; ] \\;\\kleenestar   \\label{e - fsi}\\ ] ]    where @xmath65 is the 1-level format ] of the symbol pair @xmath66 .",
    "the extraction takes place by @xmath67.l\\twol\\ ] ]    where the transducer @xmath59 is first converted into 1-level format@xmath68 , then composed with the filter @xmath69 ( eq . [ e - fsi ] ) .",
    "we extract the lower side of this composition , where every sequence of @xmath70 remains unchanged from the beginning up to the first occurrence of an unambiguous class @xmath24 .",
    "every following symbol is mapped to the empty string by means of @xmath71 ] \\;\\kleenestar$ ] ( eq .",
    "[ e - fsi ] ) .",
    "finally , the extracted lower side is again converted into 2-level format@xmath68 .",
    "the extraction of the union @xmath48 of extended middle subsequences is performed in a similar way .",
    "we then make the joint unions of initial and extended middle subsequences@xmath33 : @xmath72      \\compose { \\rule{-2mm}{0mm}\\enspace_{n}^{\\cup}\\rule{-0.5mm}{0mm}}s_i \\ ; ]   \\label{e - ex - si}\\ ] ] @xmath73      \\compose { \\rule{-2mm}{0mm}\\enspace_{n}^{\\cup}\\rule{-0.5mm}{0mm}}s_m^e \\ ; ]   \\label{e - ex - sme}\\ ] ]    in both cases ( eq .",
    "[ e - ex - si ] and [ e - ex - sme ] ) we union all subsequences from the principal model @xmath58 , with all those subsequences from the auxiliary model @xmath59 that are not in @xmath58 .",
    "finally , we generate the completed _ s+n - type _ transducer from the joint unions of subsequences @xmath46 and @xmath48 , as decribed above ( eq . [ e - pre - sent - model]-[e - fin - sent - model ] ) .    a transducer completed in this way , disambiguates all subsequences known to the principal incomplete s - type model , exactly as the underlying hmm does , and all other subsequences as the auxiliary n - type model does .",
    "the implemented tagger requires three transducers which represent a lexicon , a guesser and any above mentioned approximation of an hmm .",
    "all three transducers are sequential , i.e. deterministic on the input side .",
    "both the lexicon and guesser unambiguously map a surface form of any word that they accept to the corresponding class of tags ( fig .",
    "[ f_tgrexm ] , col .  1 and  2 ) :",
    "first , the word is looked for in the lexicon .",
    "if this fails , it is looked for in the guesser .",
    "if this equally fails , it gets the label which associates the word with the tag class of unknown words .",
    "tag probabilities in this class are approximated by tags of words that appear only once in the training corpus .",
    "as soon as an input token gets labelled with the tag class of sentence end symbols ( fig .",
    "[ f_tgrexm ] : ) , the tagger stops reading words from the input . at this point ,",
    "the tagger has read and stored the words of a whole sentence ( fig .",
    "[ f_tgrexm ] , col .  1 ) and generated the corresponding sequence of classes ( fig .",
    "[ f_tgrexm ] , col .  2 ) .",
    "the class sequence is now deterministically mapped to a tag sequence ( fig .",
    "[ f_tgrexm ] , col .",
    "3 ) by means of the hmm transducer . the tagger outputs the stored word and tag sequence of the sentence , and continues in the same way with the remaining sentences of the corpus .",
    "....    the              [ at ]             at    share            [ nn , vb ]          nn    of               [ in ]             in     ...              ...             ...    tripled          [ vbd , vbn ]        vbd    within           [ in , rb ]          in    that             [ cs , dt , wps ]      dt    span             [ nn , vb , vbd ]      vbd    of               [ in ]             in    time             [ nn , vb ]          nn    .                [ sent ]           sent ....",
    "this section compares different n - type and s - type transducers with each other and with the underlying hmm .",
    "the fsts perform tagging faster than the hmms .",
    "since all transducers are approximations of hmms , they give a lower tagging accuracy than the corresponding hmms .",
    "however , improvement in accuracy can be expected since these transducers can be composed with transducers encoding correction rules for frequent errors ( sec . [ s - intro ] ) .     & in % & in words / sec & #  states & #  arcs & time + hmm & 96.77 & 4  590 & & & +    n0-fst & 83.53 & * 20  582 * & 1 & 297 & 16 sec + n1-fst & 94.19 & 17  244 & 71 & 21  087 & 17 sec +    s+n1-fst  ( 20k ,  f1 ) & 94.74 & 13  575 & 927 & 203  853 & 3 min + s+n1-fst  ( 50k ,  f1 ) & 94.92 & 12  760 & 2  675 & 564  887 & 10 min + s+n1-fst  ( 100k ,  f1 ) & 95.05 & 12  038 & 4  709 & 976  785 & 23 min +    s+n1-fst  ( 100k ,  f2 ) & 94.76 & 14  178 & 476 & 107  728 & 2 min + s+n1-fst  ( 100k ,  f4 ) & 94.60 & 14  178 & 211 & 52  624 & 76 sec + s+n1-fst  ( 100k ,  f8 ) & 94.49 & 13  870 & 154 & 41  598 & 62 sec + s+n1-fst  ( 1 m ,  f2 ) & 95.67 & 11  393 & 2  049 & 418  536 & 7 min + s+n1-fst  ( 1 m ,  f4 ) & 95.36 & 11  193 & 799 & 167  952 & 4 min + s+n1-fst  ( 1 m ,  f8 ) & 95.09 & 13  575 & 432 & 96  712 & 3 min + s+n1-fst ( @xmath74 ) & 95.06 & 8  180 & 9  796 & 1  311  962 & 39 min + s+n1-fst ( @xmath75 ) & * 95.95 * & 4  870 & 92  463 & 13  681  113 & 47 h +     + @xmath76 +    table [ t_size ] compares different transducers on an english test case .",
    "the s+n1-type transducer containing all possible subsequences up to a length of three classes is the most accurate ( table [ t_size ] , last line , s+n1-fst ( @xmath75 ) : 95.95  % ) but also the largest one .",
    "a similar rate of accuracy at a much lower size can be achieved with the s+n1-type , either with all subsequences up to a length of two classes ( s+n1-fst ( @xmath74 ) : 95.06  % ) or with subsequences occurring at least once in a training corpus of 100  000 words ( s+n1-fst  ( 100k ,  f1 ) : 95.05  % ) .",
    "increasing the size of the training corpus and the frequency limit , i.e. the number of times that a subsequence must at least occur in the training corpus in order to be selected ( sec .",
    "[ s - make - stype ]  a ) , improves the relation between tagging accuracy and the size of the transducer .",
    "e.g. the s+n1-type transducer that encodes subsequences from a training corpus of 20  000 words ( table [ t_size ] , s+n1-fst  ( 20k ,  f1 ) : 94.74  % , 927 states , 203  853 arcs ) , performs less accurate tagging and is bigger than the transducer that encodes subsequences occurring at least eight times in a corpus of 1  000  000 words ( table [ t_size ] , s+n1-fst  ( 1 m ,  f8 ) : 95.09  % , 432 states , 96  712 arcs ) .",
    "most transducers in table [ t_size ] are faster then the underlying hmm ; the n0-type transducer about five times .",
    "there is a large variation in speed between the different transducers due to their structure and size .     &",
    "english & dutch & french & german & portug . & spanish +    hmm & 96.77 & 94.76 & 98.65 & 97.62 & 97.12 & 97.60 +    n0-fst & 83.53 & 81.99 & 91.13 & 82.97 & 91.03 & 93.65 + n1-fst & 94.19 & 91.58 & 98.18 & 94.49 & 96.19 & 96.46 +    s+n1-fst  ( 20k ,  f1 ) & 94.74 & 92.17 & 98.35 & 95.23 & 96.33 & 96.71 + s+n1-fst  ( 50k ,  f1 ) & 94.92 & 92.24 & *",
    "98.37 * & 95.57 & 96.49 & 96.76 + s+n1-fst  ( 100k ,  f1 ) & * 95.05*&*92.36*&*98.37 * & 95.81 & * 96.56*&*96.87 * +    s+n1-fst  ( 100k ,  f2 ) & 94.76 & 92.17 & 98.34 & 95.51 & 96.42 & 96.74 + s+n1-fst  ( 100k ,  f4 ) & 94.60 & 92.02 & 98.30 & 95.29 & 96.27 & 96.64 + s+n1-fst  ( 100k ,  f8 ) & 94.49 & 91.84 & 98.32 & 95.02 & 96.23 & 96.54 +    s+n1-fst  ( @xmath74 ) & * 95.06 * & 92.25 & * 98.37*&*95.92 * & 96.50 & * 96.90 * +    hmm train.crp .",
    "( # wd ) & 19  944 & 26  386 & 22  622 & 91  060 & 20  956 & 16  221 + test corpus  ( # words ) & 19  934 & 10  468 & 6  368 & 39  560 & 15  536 & 15  443 + # tags & 74 & 47 & 45 & 66 & 67 & 55 + # classes & 297 & 230 & 287 & 389 & 303 & 254 +     + @xmath77 +    table [ t_aclang ] compares the tagging accuracy of different transducers and the underlying hmm for different languages . in these tests",
    "the highest accuracy was always obtained by s - type transducers , either with all subsequences up to a length of two classes or with subsequences occurring at least once in a corpus of 100  000 words .",
    "the two methods described in this paper allow the approximation of an hmm used for part - of - speech tagging , by a finite - state transducer .",
    "both methods have been fully implemented .",
    "the tagging speed of the transducers is up to five times higher than that of the underlying hmm .",
    "the main advantage of transforming an hmm is that the resulting fst can be handled by finite state calculus and thus be directly composed with other transducers which encode tag correction rules and/or perform further steps of text analysis .",
    "* future research * will mainly focus on this possibility and will include composition with , among others :    * transducers that encode correction rules ( possibly including long - distance dependencies ) for the most frequent tagging errors , in order to significantly improve tagging accuracy .",
    "these rules can be either extracted automatically from a corpus ( brill , 1992 ) or written manually ( chanod and tapanainen , 1995 ) . * transducers for light parsing , phrase extraction and other analysis ( at - mokhtar and chanod , 1997 ) .",
    "an hmm transducer can be composed with one or more of these transducers in order to perform complex text analysis using only a single transducer .",
    "we also hope to improve the n - type model by using look - ahead to the following tags .",
    "i wish to thank the anonymous reviewers of my paper for their valuable comments and suggestions .",
    "i am grateful to lauri karttunen and gregory grefenstette ( both rxrc grenoble ) for extensive and frequent discussion during the period of my work , as well as to julian kupiec ( xerox parc ) and mehryar mohri ( at&t research ) for sending me some interesting ideas before i started .",
    "many thanks to all my colleagues at rxrc grenoble who helped me in whatever respect , particularly to anne schiller , marc dymetman and jean - pierre chanod for discussing parts of the work , and to irene maxwell for correcting various versions of the paper .",
    "at - mokhtar , salah and chanod , jean - pierre ( 1997 ) . incremental finite - state parsing . in the _ proceedings of the 5th conference of applied natural language processing .",
    "_ acl , pp .",
    "washington , dc , usa .    bahl , lalit r. and mercer , robert l. ( 1976 ) .",
    "part of speech assignment by a statistical decision algorithm . in _",
    "ieee international symposium on information theory_. pp .",
    "ronneby .",
    "brill , eric ( 1992 ) . a simple rule - based part - of - speech tagger . in the _ proceedings of the 3rd conference on applied natural language processing _",
    "152 - 155 .",
    "trento , italy .",
    "chanod , jean - pierre and tapanainen , pasi ( 1995 ) .",
    "tagging french - comparing a statistical and a constraint based method . in the _ proceedings of the 7th conference of the eacl _ , pp .",
    "149 - 156 .",
    "dublin , ireland .",
    "church , kenneth w. ( 1988 ) . a stochastic parts program and noun phrase parser for unrestricted text . in _ proceedings of the 2nd conference on applied natural language processing_. acl , pp .",
    "136 - 143 .",
    "kaplan , ronald m. and kay , martin ( 1994 ) .",
    "regular models of phonological rule systems . in _ computational linguistics_. 20:3 , pp .",
    "331 - 378 .",
    "karttunen , lauri ( 1995 ) . the replace operator . in the _ proceedings of the 33rd annual meeting of the association for computational linguistics_. cambridge ,",
    "cmp - lg/9504032    kempe , andr and karttunen , lauri ( 1996 ) .",
    "parallel replacement in finite state calculus . in the _ proceedings of the 16th international conference on computational linguistics _ , pp .",
    "622 - 627 .",
    "copenhagen , denmark .",
    "cmp - lg/9607007    rabiner , lawrence r. ( 1990 ) . a tutorial on hidden markov models and selected applications in speech recognition . in",
    "_ readings in speech recognition _ ( eds .",
    "a. waibel , k.f .",
    "morgan kaufmann publishers , inc .",
    "san mateo , ca .",
    "roche , emmanuel and schabes , yves ( 1995 ) .",
    "deterministic part - of - speech tagging with finite - state transducers . in",
    "_ computational linguistics_. vol .",
    "227 - 253 .",
    "viterbi , a.j .",
    "error bounds for convolutional codes and an asymptotical optimal decoding algorithm . in _ proceedings of ieee",
    "268 - 278 .",
    "below , a and b designate symbols , a and b designate languages , and r and q designate relations between two languages .",
    "more details on the following operators and pointers to finite - state literature can be found in http://www.rxrc.xerox.com/research/mltt/fst"
  ],
  "abstract_text": [
    "<S> this paper describes the conversion of a hidden markov model into a sequential transducer that closely approximates the behavior of the stochastic model . </S>",
    "<S> this transformation is especially advantageous for part - of - speech tagging because the resulting transducer can be composed with other transducers that encode correction rules for the most frequent tagging errors . </S>",
    "<S> the speed of tagging is also improved . </S>",
    "<S> the described methods have been implemented and successfully tested on six languages . </S>"
  ]
}