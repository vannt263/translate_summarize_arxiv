{
  "article_text": [
    "researchers in many fields use markov chain monte carlo ( mcmc ) methods to translate data into inferences on high - dimensional parameter spaces @xcite . studies of the cosmic microwave background ( cmb ) anisotropy spectra are a perfect example of this practice @xcite .",
    "the concordance cosmology requires six or seven ( depending on whether or not one assumes spatial flatness ) parameters to theoretically describe the spectrum of cmb anisotropies .",
    "given the computational expense of converting just one point in this parameter space into an anisotropy spectrum ( 1.3 seconds using the boltzmann code camb @xcite on a 2.5 ghz dual - core machine ) and then comparing that spectrum to the data ( 2 seconds using the official wmap likelihood code @xcite ) , exhaustively exploring the parameter space in search of models that fit the data well is unfeasible .",
    "mcmc is an alternative to this costly process that randomly walks through parameter space such that exploration theoretically locates and confines itself to regions where the fit to the data is good , and thus integrates the distribution of parameter values only over that space where it has high support .",
    "integrations that might have taken months when done exhaustively can be accomplished in days ; those that might have taken days can be accomplished in hours .",
    "it is fair to say that mcmc has become an `` industry standard '' within some communities . in spite of this success ,",
    "serious questions remain .",
    "mcmc is a sampling method and its theoretical guarantees are mostly with respect to convergence rather than sampling efficiency @xcite .",
    "parameter fitting problems are search problems , not sampling problems .",
    "there is no fundamental reason that a sampling algorithm should be good for searching and certainly no guarantee that the performance of mcmc as a search algorithm with a finite number of samples will be particularly efficient .",
    "finally , mcmc methods generally force investigators to make bayesian assumptions which may or may not be desirable .",
    "we present an algorithm originally proposed and implemented by @xcite as a more efficient and flexible alternative to mcmc .",
    "we will refer to the algorithm as the active parameter search ( aps ) algorithm .",
    "we provide c++ code to implemement aps ( publically available at ` https://github.com/uwssg/aps ` ) and test it on the 7-year release of the wmap cmb data @xcite .",
    "section [ sec : mcmc ] presents an overview of both the mcmc and aps algorithms and discusses the shortcomings of the former and how the latter attempts to address them . section [ sec : user ] details the user - specified parameters necessary to aps and some of the attendant considerations .",
    "section [ sec : cartoons ] compares the performance of mcmc and aps on a toy model of a multi - modal likelihood function .",
    "section [ sec : wmap7 ] presents the results of the wmap 7 test .",
    "we find that aps achieves parameter constraints at comparable ( if not faster ) times to mcmc while simultaneously exploring the parameter space more broadly .",
    "we begin this section with a general discussion of the process of deriving parameter constraints from data .",
    "we then sketch an mcmc method called the metropolis - hastings algorithm @xcite .",
    "we will outline the aps algorithm with an aside discussing gaussian processes , a method of multi - dimensional function fitting which occupies some prominence in the algorithm .",
    "we end this section by directly comparing aps with mcmc on a toy model in a two - dimensional parameter space .",
    "the basic problem both mcmc and aps attempt to address is the following : suppose there is a theory described by some number of parameters @xmath0 .",
    "this theory makes a prediction about some function @xmath1 which is tested by an experiment resulting in @xmath2 data points @xmath3 . to quantify what these data say about the theory , an investigator wants to ask what combinations of values of @xmath4 result in predictions that are consistent with the data to some threshold probability @xmath5 .",
    "usually , this is quantified by assuming that the @xmath2 data points are independent and normally distributed so that the statistic @xmath6 ( where @xmath7 is the uncertainty associated with the @xmath8 datapoint ) is a random variable distributed according to the @xmath9 probability distribution @xmath10 with @xmath2 degrees of freedom . in that case , the @xmath5 confidence limit constraint on @xmath4 corresponds to the set of all values @xmath4 which result in values of @xmath9 such that @xmath11 more generally , one has a likelihood function @xmath12 ( above represented by @xmath10 ) quantifying the goodness - of - fit between data and theory .",
    "one uses mcmc or aps to find the values of @xmath4 corresponding to @xmath13 , where @xmath14 is some threshold set by statistics and the desired @xmath5 .",
    "bayesian inference approaches this problem indirectly by assuming that @xmath4 are random variables distributed according to the distribution function @xmath12 .",
    "constraining these parameters to @xmath5 is therefore a problem of integrating @xmath12 over @xmath4 until the integral contains @xmath5 of the total @xmath12 .",
    "mcmc assumes this approach , as will be shown below .",
    "a common criticism of this mode of thought is that , even if the theory is a poor description of the data , bayesian inference will still yield a constraint consisting of the @xmath5 least - poor region of parameter space .",
    "frequentist inference takes a more objective approach , defining the @xmath5 confidence limit to be all points in parameter space that satisfy @xmath13 .",
    "aps assumes this approach , as will also be shown below , and exploits it to derive parameter constraints with siginficantly fewer calculations of @xmath12 ( a presumably costly procedure ) than mcmc .",
    "the direct product of mcmc is a chain : a list of points in parameter space that mcmc sampled and the number of times it sampled them .    *",
    "( 1 m ) begin by sampling a point @xmath15 from parameter space at random .",
    "record this point in the chain and evaluate the likelihood function for that combination of parameters .",
    "the value of this likelihood call will be @xmath16 . + * ( 2 m ) select another point @xmath17 in parameter space .",
    "this point should be selected randomly , but according to some distribution that depends only on the step length @xmath18 and is tuned so that the algorithm explores away from the initial point but does not effectively jump to any other place in parameter space uniformly at random .",
    "evaluate the likelihood function at @xmath17 , giving you @xmath19 .",
    "+ * ( 3 m ) if @xmath20 ( i.e. , if the new point in parameter space is a better fit to the data than the old point ) , the step is accepted and the new point is recorded in the chain .",
    "set @xmath21 and @xmath22 . + * ( 4 m ) if @xmath23 , draw a random number @xmath24 between 0 and 1 .",
    "if @xmath25 , accept the step anyway .",
    "+ * ( 5 m ) if the step was ultimately rejected , increment the number of samples associated with @xmath15 by one .",
    "+ * ( 6 m ) return to step ( 2 m ) and repeat .",
    "there are heuristic statistics that can be performed on the chain to give an indication of whether it has adequately sampled parameter space @xcite , though they have issues we point out later .",
    "this is the most basic form an mcmc algorithm can take .",
    "in addition to chapter 1 of @xcite , one may wish to consult appendix a of @xcite for guidance writing a functional mcmc .",
    "the tests in step ( 3 m ) and ( 4 m ) mean that any mcmc run will gravitate towards the high - likelihood regions of parameter space and ignore regions of extremely low likelihood .",
    "one converts the chain into parameter inferences by gridding parameter space and treating the chain as a histogram of how many times the chain visited each point on the grid ( or its proximity ) .",
    "this histogram will closely match the bayesian posterior distribution over the parameters .",
    "@xmath5 confidence limits are derived by integrating this histogram from its peak out to whatever bounds contain @xmath5 of the sampled points .",
    "there are several shortcomings with this method that the aps addresses .",
    "the first shortcoming is efficiency . because mcmc seeks to take samples that match the distribution rather than learning specific information about the distribution , such as where the confidence boundaries are , it wastes samples in areas where it already has information .",
    "for example , once a high - likelihood parameter setting has been evaluated there is no need to evaluate there again .",
    "it is already known that this is a high - likelihood region of the space .",
    "however , mcmc specifically indicates that high - likelihood by putting more samples there . these are wasted evaluations .",
    "a second mcmc shortcoming , related to the first , involves the possibility that there are several disjoint regions of high likelihood in the parameter space under consideration",
    ". this can be problematic for mcmc because steps ( 2m)-(4 m ) ensure that , once the chain finds one of these high likelihood regions , it is unlikely to step out of it and find another ( unless the distribution in step ( 2 m ) is such that it allows very large steps , in which case the chain will take significantly longer to converge ) .",
    "an unexplored region of the space may remain unexplored for a long time ( though in theory not infinitely long ) .",
    "mcmc never determines whether it has explored the region and whether information could be learned by doing so .    when initially testing aps , bryan _",
    "et al_. found a separate region of high - likelihood parameter space in the 1-year wmap data release which had been totally ignored by mcmc analyses .",
    "this second peak in @xmath26 disappeared as the wmap signal - to - noise improved in subsequent data releases ( as we will see in section [ sec : wmap7 ] ) , but it does serve as an illustration of this particular peril of mcmc . dunkley _ et al_. ( 2005 ) and @xcite attempt to address this problem and the general inefficiency of mcmc as a search algorithm by optimizing the proposal density in step ( 2 m ) .",
    "they find no clear solution that guards against multiple high likelihood regions .",
    "the final shortcoming of mcmc involves its unequivocally bayesian nature . for a number of data points @xmath27 bayesian and frequentist confidence limits",
    "are approximately equivalent .",
    "they can , however , yield conflicting results when constraining a subset of parameter space .",
    "consider a six - dimensional parameter space like the space of cosmological parameters mentioned in section [ sec : intro ] and revisited in section [ sec : wmap7 ] .",
    "if one is really only interested in constraining @xmath28 , one still has to deal with the question of how to treat @xmath29 , since they presumably affect @xmath26 in a way that is non - trivially correlated with @xmath28 . in bayesian inference ,",
    "one deals with this question by integrating the probability distribution over the full range of the uninteresting parameters ( marginalizing over them ) , i.e. the @xmath5 confidence limit is the contour in @xmath28 space such that @xmath30 where @xmath31 represent the bounds of the confidence limit in the two - dimensional subspace of interest .",
    "this integration yields the two - dimensional ellipses of figures 4 and 6 of @xcite ( and most observational cosmology papers since ) . from a frequentist perspective",
    ", the integration in equation ( [ bayesint ] ) is problematic .",
    "it risks drawing a bound that is either too restrictive or too inclusive .",
    "an over - restrictive bound could arise because the integral ( [ bayesint ] ) weights points in @xmath28 space according to the density of corresponding points in @xmath32 space that are highly likely .",
    "if there are combinations of @xmath28 that are highly likely only for an extremely limited set of @xmath29 , those points will be excluded from the @xmath5 confidence limit even though they are strictly allowed according to the @xmath13 criterion .",
    "bayesian inference does not give much thought to these excluded points because , if we consider the parameters @xmath4 to be random ( which bayesians do ) , the excluded points correspond to highly improbable values of @xmath28 precisely because they only agree with the data for such a limited set @xmath29 .",
    "frequentist bounds , consisting of any and all points for which @xmath13 , do not care how many different points in the larger parameter space are acceptable for a given @xmath28 . if just one point satisfies @xmath13 , the corresponding point in subspace is within the bound .",
    "an over - inclusive bound could arise because the integral ( [ bayesint ] ) searches only for that bound which contains @xmath5 of the total probability .",
    "if the model chosen to describe the data is universally poor , mcmc will still return a @xmath5 consisting of the least - poor region in parameter space without any obvious warning that the model is probably wrong .",
    "a frequentist bound would , in that case , return no @xmath5 confidence bound , highlighting the model s deficiency .",
    "traditionally , bayesian assumptions are made because they do not require prior knowledge of the underlying likelihood function . in order to draw a frequentist confidence limit",
    ", one must be able to calculate @xmath14 before sampling any points in parameter space .",
    "bayesian confidence limits only require knowledge of the relative likelihood between sampled points and thus can be revised with each new sample .",
    "this represents one advantage mcmc retains over aps .      to address the shortcomings of mcmc , bryan _",
    "et al_. bryan _",
    "et al_. ( 2007b ) propose the following alternative algorithm for exploring parameter space .",
    "* ( 1a ) generate some number @xmath33 of starting points uniformly distributed across parameter space .",
    "evaluate @xmath12 at each of these points . store them in @xmath34 .",
    "+ * ( 2a ) use the points already evaluated to fit a surrogate function that estimates the likelihood and uncertainty in it for other , as yet unevaluated , candidate points .",
    "+ * ( 3a ) generate a large number @xmath35 of candidate points also uniformly distributed over parameter space .",
    "use the surrogate function to guess the value of @xmath12 at each of the candidate points .",
    "this guess will be @xmath36",
    ". the uncertainty in your guess will be @xmath37 .",
    "section [ sec : gp ] will describe one means of finding @xmath36 and @xmath37 .",
    "the assumption is that estimating @xmath36 and @xmath37 is orders of magnitude less computationally expensive than finding the true @xmath12 . + * ( 4a ) select the candidate point with the maximum value of the statistic @xmath38 where @xmath39 is a parameter that has a similar effect as the length parameter in mcmc s proposal distribution .",
    "small values will make it take more samples around already good ( high @xmath26 ) points while large values will make it explore more agressively .",
    "evaluate @xmath12 at the selected point and add it to the list of evaluated @xmath34 .",
    "+ * ( 5a ) repeat steps ( 2a ) through ( 4a ) .",
    "confidence bounds are found by gridding the parameter space ( though no integration or other accumulation is required ) .",
    "convergence can be estimated heuristically by observing the size of changes in confidence bounds .",
    "maximizing @xmath40 in equation ( [ sstat ] ) implies , to some extent , minimizing @xmath41 .",
    "this algorithm therefore seeks out the boundary of the @xmath5 confidence limit , rather than its interior .",
    "this yields some efficiency improvements over mcmc .    the dependence of @xmath40 on @xmath37  the uncertainty of the predicted candidate @xmath12 value  forces the algorithm to simultaneously pay attention to unexplored regions of parameter space .",
    "we will see in section [ sec : gp ] that , if a region of parameter space is unexplored , the value of @xmath37 for a candidate point chosen from that region will be very high .",
    "this algorithm therefore guards against the second shortcoming of mcmc ( ignorance of disjoint regions of high likelihood ) by explicitly stepping away from regions that are already known to be near the @xmath5 confidence limit and sampling points from poorly - sampled regions of parameter space .",
    "@xcite empirically compared this dependence on @xmath37 to other information - theoretic dependences and found it to perform best . following that reference",
    ", we set @xmath42 in equation ( [ sstat ] ) .",
    "step ( 4a ) chooses points solely based on their own merit , rather than the ratio of likelihoods used in steps ( 3 m ) and ( 4 m ) of mcmc .",
    "this algorithm can therefore apply a purely frequentist test to parameter space .",
    "bounds are drawn in parameter space by examining the full set of points sampled and making a scatter - plot of those points which meet the @xmath13 criterion , rather than by integrating over the relative frequency with which the algorithm visited different points in parameter space .",
    "this guards agains the final shortcoming of mcmc : the inherent subjectivity of bayesian @xmath5 confidence limits .",
    "step ( 3a ) of aps generates a random set of candidate points and uses @xmath34 data from the points in parameter space already sampled to predict the value of @xmath12 at each candidate point ( this prediction is @xmath36 in equation [ sstat ] ) and assign an uncertainty @xmath37 to that prediction .",
    "the algorithm is agnostic about how this prediction and uncertainty are derived .",
    "we follow @xcite and use the formalism of gaussian processes ( more specifically : kriging ) to make the predictions .",
    "there is a slight difference between our formalism and theirs .",
    "this will be explained below .",
    "the following discussion comes mostly from chapter 2 and appendix a of @xcite .",
    "gaussian processes use the sampled data @xmath34 to predict @xmath12 at the candidate point @xmath43 by assuming that the function @xmath44 represents a sample drawn from a random process distributed across parameter space . at each point in parameter space , @xmath12 is assumed to be distributed according to a normal distribution with mean @xmath45 and variance dictated by the covariance function @xmath46 .",
    "@xmath47 is the intrisic variance in the value of @xmath12 at a single point @xmath48 .",
    "@xmath49 encodes how variations in @xmath12 at one point in parameter space affect variations in @xmath12 at other points in parameter space .",
    "rasmussen and williams ( 2006 ) treat the special case @xmath50 and finds ( see their equations 2.19 , 2.25 and 2.26 ) @xmath51 where the sums over @xmath52 and @xmath53 are sums over the sampled points in parameter space .",
    "@xmath54 relates the sampled point @xmath52 to the candidate point @xmath55 . we do not wish to assume that the mean value of @xmath12 is zero everywhere .",
    "therefore , we modify the equation for @xmath36 to give @xmath56 where @xmath45 is the algebraic mean of the sampled @xmath57 . note the similarity to a multi - dimensional taylor series expansion with the covariance matrix playing the role of the derivatives .",
    "equation ( [ sig ] ) differs from equation ( 6 ) in @xcite because they used the semivariance @xmath58\\ ] ] in place of the covariance @xmath49 . in practice ,",
    "the two assumptions result in equivalently valid @xmath36 and @xmath37 .",
    "the form of the covariance function @xmath46 must be assumed .",
    "we choose to use @xmath59\\ ] ] where @xmath60 is the distance in parameter space between the points @xmath48 and @xmath61 .",
    "the exponential form of @xmath49 quantifies the assumption that distant points should not be very correlated .",
    "the normalization constant @xmath62 ( known as the `` kriging parameter '' for the geophysicist who pioneered the overall method ) also must be assumed .",
    "this is somewhat problematic because , examining equation ( [ mu ] ) , one sees that the factors of @xmath62 and @xmath63 completely factor out of the prediction @xmath36 , so that the assumed value of @xmath62 has no effect on the accuracy of the prediction .",
    "if the opposite had been true , one could heuristically set @xmath62 to maximize the accuracy of @xmath36 .",
    "given that the function we are trying to model ( @xmath12 as a function of set data and a specified theory ) is not a random process , we find no a priori way to set @xmath62 and instead set it according to heuristics that we believe are consistent with the behaviors we desire from the aps algorithm .",
    "we discuss this in more detail in section [ sec : user ] .",
    "figure [ fig : gp ] applies the gaussian process of equations ( [ sig ] ) and ( [ mu ] ) with assumption ( [ covraw ] ) to a toy one - dimensional function .",
    "inspection shows many desirable behaviors in @xmath36 and @xmath37 . as @xmath64 approaches the sampled points @xmath65 , @xmath36 approaches @xmath57 and @xmath37 approaches zero .",
    "closer to a sampled point , the gaussian process knows more about the true behavior of the function .",
    "far from the @xmath65 , @xmath37 is larger , and the @xmath40 statistic in equation ( [ sstat ] ) will induce the aps algorithm to examine the true value of @xmath12 .",
    "uncertainty bounds . for the purposes of this illustration",
    ", we assumed that the kriging parameter was @xmath66 . ]      figure [ fig : cartoon ] directly compares aps with mcmc by running both on a toy model in two - dimensional parameter space .",
    "the model likelihood function is a @xmath9-distribution with two degrees of freedom .",
    "we are interested in the 95% confidence limit , which corresponds to a criterion of @xmath67 the model is constructed so that there are two regions of parameter space that meet this criterion .",
    "these are the two ellipses in figure [ fig : wide ] .",
    "figures [ fig : zoom_mcmc ] and [ fig : zoom_krig ] zoom in on one of these regions .",
    "the open circles correspond to the points sampled by mcmc .",
    "each color represents an independent chain ( four in all ) .",
    "each of these chains was allowed to run until it had sampled 100 points in parameter space .",
    "the blue crosses represent the points sampled by aps after it had sampled a total of 400 points in parameter space .",
    "note that no single mcmc chain found both regions of high likelihood .",
    "a user would have to run multiple independent chains to be at all certain that she had discovered all of the regions of interest .",
    "it is now standard to address this by running several mcmc chains and aggregating their outputs , but it is concerning to leave coverage of the search space to the luck of multiple random restarts .",
    "conversely , aps sampled points from distant regions of parameter space .",
    "this is how aps ultimately found both regions of high likelihood . indeed , after only 85 calls to the likelihood functio , aps had sampled points from both regions of high likelihood .",
    "consider figures [ fig : zoom_krig ] and [ fig : zoom_mcmc ] .",
    "it is obvious that , even near the most densely - sampled high - likelihood region , aps is principally interested in points on the boundary of the confidence region , while mcmc samples the interior .",
    "this is another way in which aps is a more efficient allocation of computing resources than mcmc .    finally we can observe the perils of common mcmc convergence heuristics .",
    "they generally compare statistics from all the chains in aggregate against individual chains . at convergence",
    ", these statistics should be similar .",
    "we observe that in the toy example , convergence has not happened because some chains are near one local maximum while some are at the other .",
    "however , from a practial point of view all the high likelihood areas have been identified .",
    "conversely , if the four chains had been chosen unluckily , they would have all converged to the same local maximum and never reached the other .",
    "the convergence tests would have reported success while the reality would have been failure to find both local maxima .",
    "we will present a more detailed comparison of mcmc and aps in section [ sec : cartoons ] .",
    "aps as presented thus far contains parameters which must be set by the user . we describe them in this section .",
    "a summary list is presented at the end of the section .",
    "@xmath35 is the number of candidate points considered at each iteration .",
    "one consideration that should go into choosing @xmath35 is speed .",
    "if @xmath35 is too large , the evaluation of all @xmath35 values of @xmath36 and @xmath37 in step ( 3a ) will become comparably expensive to evaluating the likelihood function and the algorithm will lose some of its efficiency advantage over mcmc .",
    "@xmath35 can also affect the algorithm s propensity to explore unknown regions of parameter space . a small @xmath35 adds additional randomness because the selection will be affected more by the luck of choosing candidates than the metric used to evaluate them .",
    "@xmath33 is the number of purely random points on which to sample @xmath12 before proceeding to iterate steps ( 2a)-(4a ) .",
    "this number need only be sufficiently large to make the initial gaussian process reasonable .",
    "it should not be so large as to be equivalent to a fine - gridding of parameter space , which would defeat the purpose of having an efficient search algorithm .    in practice",
    ", one must specify bounds on each of the parameters beyond which it is not worth exploring .",
    "to prevent the relative sizes of each parameter s allowed range from affecting the performance of our gaussian process , we amend the covariance function ( [ covraw ] ) to read @xmath68\\ ] ] where @xmath52 and @xmath53 denote different points in parameter space and the sum over @xmath69 is a sum over the dimensionality of the parameter space .",
    "additionally , in order to prevent the gaussian process from becoming prohibitively slow as more points are added to the set of sampled points , we follow @xcite and only use the @xmath70 nearest sampled neighbors of each candidate point when predicting @xmath36 and @xmath37 .",
    "@xmath70 is another choice made by the user .    to set the kriging parameter @xmath62 , we sample an additional uniform set of @xmath71 points after the initial @xmath33 sample but before proceeding to step ( 2a ) . for each of these points",
    "we both predict @xmath36 using the gaussian process and sample the actual @xmath12 .",
    "we set @xmath62 equal to the value necessary that 68% of these @xmath71 points have @xmath72 .",
    "as the algorithm runs , we periodically adjust @xmath62 so that the search through parameter space strikes a balance between exploring unknown regions of parameter space and identifying regions of parameter space satisfying @xmath73 .",
    "note that one can just as easily assume the value of @xmath62 , this , however , runs the risk that the aps algorithm will either fail to explore outside of the discovered high - likelihood points ( if @xmath62 is too small and @xmath74 ) or will ignore the high - likelihood region altogether ( the opposite case ) .",
    "we make one final modification to the aps algorithm as currently presented . because of the absolute nature of frequentist parameter constraints , a point in parameter space with likelihood @xmath75 is still not within the confidence limit .",
    "if the code finds such a point and immediately resumes its random search through the broader parameter space , it has not learned anything about the allowed values of @xmath4 .",
    "we therefore modify the code so that , whenever it finds a point with @xmath76 , it pauses in its search and spends some number @xmath77 of evaluations trying to find a nearby point that is actually inside the confidence limit .",
    "to conduct this refined search , we borrow an idea from @xcite and note that equations ( [ mu ] ) and ( [ covariogram ] ) offer a straightforwardly differentiable function for @xmath36 in terms of our theoretical parameters .",
    "much as equation ( [ mu ] ) does a good job of characterizing the value of @xmath26 at an unknown point , the derivative of equation ( [ mu ] ) should do a good job of characterizing the derivative of @xmath26 at a known point .",
    "this allows us to use gradient descent to walk from a point near the likelihood threshold towards a point that is inside the likelihood threshold .",
    "the method is as follows .    *",
    "( 1 g ) starting from the already sampled point @xmath43 which is near the likelihood threshold , assemble a list of the @xmath70 nearest neighbors from the set of sampled points , this time including @xmath43 itself as the absolute nearest neighbor . + * ( 2 g ) differentiate equation ( [ mu ] ) to get @xmath78 and differentiate equation ( [ covariogram ] ) to get @xmath79 + * ( 3 g ) use this derivative to select a point in parameter space a small step away from @xmath80 along the direction that will maximize the change in @xmath26 .",
    "sample the likelihood function at this point .",
    "this point is now @xmath43 .",
    "+ * ( 4 g ) repeat steps ( 1 g ) through ( 3 g ) until you find some fiducial maximum likelihood @xmath81 or you have iterated @xmath77 times , whichever comes first .",
    "this modification to aps is referred to as the `` lingering modification . ''    to summarize , the aps parameters that must be tuned by the user are ( values in parentheses are the values used on the 6 dimensional parameter space in section [ sec : wmap7 ] )    * @xmath33 ( 1000 )  the number of random distributed starting points evaluated in step ( 1a ) of aps .",
    "+ * @xmath71 ( 1000 )  a number of randomly sampled points used to heuristically set the kriging parameter @xmath62 in equation ( [ covraw ] ) . + * @xmath35 ( 250 )  the number of candidate points randomly generated in step ( 3a ) of aps .",
    "+ * @xmath70 ( 15 )  the number of nearest neighbors used in the gaussian process . + * @xmath77 ( 100 )  the maximum number of iterations to be spent using gradient descent in the lingering modification .",
    "+ * @xmath82 , @xmath83 ( @xmath84 )  the confidence limit threshold value aps is trying to find .",
    "+ * @xmath85 , @xmath86 ( @xmath87 )  the value used as a target by gradient descent in the lingering modification .",
    "+ * the dimensionality of the parameter space and the maximum and minimum values each parameter is allowed to take .",
    "in this section , we will test aps s performance against that of mcmc on a toy likelihood function with two regions of high likelihood .",
    "the function exists in four dimensions .",
    "the high likelihood regions are centered on the points @xmath88 the function itself is characterized by a @xmath9 statistic with 1199 degrees of freedom , which depends on @xmath4 according to @xmath89 + 1200\\exp[-0.5d_2 ^ 2]\\nonumber\\end{aligned}\\ ] ] where @xmath90 are the distances in parameter space from @xmath91 . in this case , the 95% confidence limit threshold corresponds to a value of @xmath92 .",
    "figure [ fig : toy ] shows a one - dimensional slice of this function .",
    "the red dashed line corresponds to the threshold @xmath9 limit .",
    "when testing aps , we set the tunable parameters thus @xmath93     distribution with 1199 degrees of freedom . ]    before running either mcmc or aps , we generate three test grids each of 10,000 known points on this likelihood surface .",
    "we will use these grids to measure how long it takes mcmc and aps to learn the entire behavior of this likelihood surface . the first test grid is uniformly distributed across the entire likelihood surface ( all @xmath94 parameters are allowed to vary from @xmath95 to @xmath96 ) .",
    "the second grid is spherically distributed about the rightmost high likelihood region in figure [ fig : toy ] .",
    "the third grid is spherically distributed about the leftmost high likelihood region .",
    "the first grid contains no points whose @xmath9 values are below the threshold limit .",
    "the second and third grids are roughly half comprised of points that are below the threshold .    to compare the efficacy of aps and mcmc at characterizing the likelihood surface",
    ", we run each algorithm 200 times . in the case of",
    "mcmc an individual `` run '' consists of four independent chains run in parallel . during each run",
    ", we periodically stop and consider the points sampled by each algorithm thus far .",
    "we treat these points as the input to a gaussian process which we use to guess the @xmath9 values of the points on our three test grids .",
    "we quantify the efficacy of the algorithms in terms of the number of mischaracterizations made on each grid .",
    "we define a `` mischaracterization '' as a point on the test grid which satisfies @xmath97 but for which the guassian process predicts @xmath98 or vice - versa .",
    "figure [ fig : toytest ] shows the performance of the algorithms on this test averaged over all the 200 instantiations of each algorithm .",
    "you will note that while both algorithms were essentially perfect at characterizing the widely distributed test grid 1 ( on which no points satistfied @xmath97 ; note the logarithmic vertical axis in figure [ fig : toyg1 ] ) , only aps with lingering successfully and reliably characterized both test grids centered on the high - likelihood regions in figure [ fig : toy ] .",
    "this is an illustration of the second shortcoming of mcmc identified in section [ sec : mcmcsketch ] : once an mcmc chain has identified a high likelihood region , it is unlikely to step out of that region and consider the possibility that other high likelihood regions exist .",
    "this problem persists even though each mcmc instantiation consists of four independent chains , each with its own opportunity to fall into one or the other high likelihood region .",
    "either the chains all fell into one high likelihood region and not the other , or the chains became trapped at the local minimum in equation ( [ toyfn ] ) at @xmath99 . in section [ sec : wmap7 ] we perform a similar test using actual data from the wmap cmb experiment @xcite and the 6-dimensional parameter space of the flat concordance cosmology .",
    "in this section , we test the aps algorithm on the 7-year data release of the wmap satellite , which measured the temperature and polarization anisotropy spectrum in the cosmic microwave background @xcite . for simplicity ,",
    "we only consider anisotropies in the temperature - temperature correlation function and modify the likelihood function @xcite to work in the space of anisotropy @xmath100s , rather than working directly in the pixel space .",
    "this results in a likelihood function sampled from a @xmath9-distribution with 1199 degrees of freedom . in this case",
    ", the @xmath13 criterion for the 95% confidence limit corresponds to @xmath97 with @xmath101 .",
    "we take as our parameter space the six dimensional parameter space describing the set of spatially flat @xmath102cdm ( cosmological constant and cold dark matter ) concordance cosmologies .",
    "those parameters are @xmath103\\}$ ]",
    ". these parameters will be familiar to users of the mcmc code cosmomc @xcite as the relative density of baryons , the relative denstiy of dark matter , the present day hubble parameter , the optical depth to last scattering , the spectral index controlling the scale - dependence of primordial density fluctuations in the universe , and a normalization parameter controlling the amplitude of primordial density fluctuations in the universe .",
    "we use the boltzmann code camb to convert from parameter values to anisotropy spectra @xcite .    in this case , we test aps with @xmath104 our comparison mcmc is run by the publically available software cosmomc @xcite .",
    "we compare the results of the two algorithms  both in terms of derived constraints on the cosmological parameters and in terms of exploration of the full parameter space  below .      as discussed in section [ sec : mcmc ] , mcmc determines parameter constraints by integrating over the posterior probability distribution on parameter space while the aps algorithm determines constraints by listing found points which satisfy @xmath13 and determining the region of parameter space spanned by those points .",
    "figure [ fig : contours ] compares these two approaches by tracking the development of the 95% confidence limit contour in one two - dimensional slice of our full six - dimensional parameter space as a function of the number of points sampled by each algorithm . in each frame , the solid contours represent contours drawn by considering all of the points found by each algorithm which satisfy the frequentist @xmath105 requirement .",
    "the blue contour represents points found by aps .",
    "the red contour represents points found by mcmc .",
    "the black contour represents points found by mcmc after sampling a total of 1.2 million points .",
    "note : for this comparison , we consider all of the points visited by mcmc , not just those points accepted in steps ( 3 m ) and ( 4 m ) from section [ sec : mcmcsketch ] .",
    "this is , in some sense , a more complete set of information about the likelihood surface than mcmc usually returns .",
    "the dashed black contour is the contour drawn using the usual bayesian inference on the points acccepted by mcmc after the full 1.2 million points have been sampled .",
    "the green crosses are the pixels in this 2-dimensional parameter space that bayesian inference believes are inside of the 95% confidence limit after the specified number of points have been sampled .",
    "the salient features of this figure are as follows .",
    "the solid black contour is larger than the dashed black contour .",
    "this means that mcmc visited points that met the frequentist threshold @xmath105 but not with enough frequency to satisfy the 95% bayesian confidence limit .",
    "this is an example of mcmc being too restrictive as discussed in section [ sec : mcmcsketch ] .",
    "a similar conclusion can be drawn from the fact that the green crosses do not fill the red contour until 400,000 points have been sampled .",
    "the green crosses congregate in the center of the contours because mcmc is principally interested in the deep interior of the high likelihood region .",
    "this is a manifestation of the inefficiency of mcmc discussed in section [ sec : mcmcsketch ] .",
    "the blue and red contours track quite well at all points in the algorithms histories .",
    "this shows that aps is at least as good as mcmc at deriving parameter constraints when you treat the points visited by mcmc from a frequentist perspective . comparing the blue contour to the green crosses , in figure [ fig : con100 ]",
    ", one sees that aps derives accurate parameter constraints faster than mcmc treated from a bayesian perspective .",
    "figure [ fig : scatterquant ] recreates figure [ fig : con400 ] , except that the green crosses represent the 50% confidence limit according to bayesian mcmc after sampling 400,000 points . the fact that these pixels still occur outside of the solid black contour ( frequentist mcmc 95% confidence limit after sampling 1.2 million points ) indicates that the false positives in figure [ fig : con400 ] represent a significant fraction of the total posterior probability integrated by bayesian mcmc at this point in the algorithm . in contrast , the aps contour already covers 86% of the final area of the frequentist mcmc contour . because of how they are drawn ( @xmath13 )",
    ", aps contours will not include false positives .",
    "figure [ fig : tracks ] plots the one - dimensional 95% confidence limits on our six cosmological parameters .",
    "again , the solid red lines consider all of the points visited by mcmc ( not just those accepted by the chain ) and set the limit according to the frequentist threshold .",
    "the blue lines represent the results from aps .",
    "note that aps only sampled 420,000 points before we stopped it .",
    "mcmc sampled 1.2 million points .",
    "the dashed black lines are the confidence limits inferred from bayesian analysis on only those points accepted by the mcmc chains . as in figure [ fig : contours ] , we see that aps converges to the same answers as freqentist mcmc in comparable time , and that both frequentist analyses find allowed parameter values that are missed by the bayesian analysis",
    ". cases where the dashed black lines stry outside of the solid red lines indicate bayesian anaysis applying spurious weight to low - likelihood points .    at this point",
    ", we have made the case that aps gives more accurate parameter constraints in less time than the usual , bayesian mcmc analysis .",
    "however , even if one were simply to modify their mcmc analyses to adopt a frequentist perspective ( the red contours and lines in figures [ fig : contours ] and [ fig : tracks ] ) , we show in section [ sec : explore ] below that aps exhibits superior performance characterizing the entire likelihood surface , not just the high likelihood subsurface . in the language of section",
    "[ sec : cartoons ] , aps gives constraints as accurate as mcmc with greater confidence that you have not ignored any additional regions of high likelihood .     except that now the green crosses are the 50% bayesian mcmc confidence limit .",
    "the fact that these crosses still occur outside of the solid black contours indicates that false positives account for a large fraction of the total posterior probability integrated by mcmc , even after sampling 400,000 points in parameter space . ]",
    "section [ sec : constraints ] examined the performance of the aps algorithm within the high likelihood region of parameter space by comparing the derived parameter constraints to those found by mcmc .",
    "this section will consider the performance of the aps algorithm in the low likelihood region of parameter space , asking the question `` how certain can we be that the excluded regions of parameter space contain no points of interest ? '' recall the toy model in section [ sec : cartoons ] : mcmc is notoriously inefficient ( or even ineffective ) at exploring multi - modal likelihood functions . by selecting sample points based both on proximity to the confidence limit and on uncertainty in the gaussian process prediction @xmath37 , the aps algorithm ought to improve on that performance .    to test this hypothesis , we perform the test illustrated in figure [ fig : toytest ] on the wmap parameter space and likelihood surface .",
    "we generate two test grids each of 1,000,000 points .",
    "one grid is distributed uniformly across parameter space .",
    "this grid contains no good points that satisfy the @xmath105 criterion .",
    "the other grid is spherically distributed about the vicinity of the high likelihood region and contains 110,000 good points .",
    "we then take the points sampled by mcmc and aps ( again , we take all of the points sampled by mcmc ; not just those points accepted in the chains ) at different times in their history and use those points as the input for a gaussian process , which we use to predict the @xmath26 values of the points on our test grid .",
    "figure [ fig : grid ] shows the number of points that are thus misclassified ( @xmath105 points for which the gaussian process predicts @xmath106 and vice - versa ) as a function of the number of points fed into the gaussian process . because the wmap 7 likelihood function is so expensive , we only ran this test once . to find the confidence limit ( dashed ) curves , we consider the uncertainty implied by the gaussian process @xmath37 in equation ( [ sig ] ) .",
    "the dashed curves encompass points that are within @xmath37 of being misclassified ( i.e. points for which @xmath97 but @xmath107 and vice versa ) .",
    "as you can see from figure [ fig : grid1 ] , aps does a much better job at characterizing the uniform test grid than does mcmc .",
    "figure [ fig : grid2 ] shows that the two algorithms perform comparably poor on the high likelihood test grid .",
    "this is likely due to the compact nature of the high likelihood region of the wmap 7 likelihood surface .",
    "it is small both in terms of extent on parameter space and in terms of the difference in @xmath9 between likely and unlikely points .",
    "recall that the 95% confidence limit we are considering corresponds to @xmath92 .",
    "the smallest @xmath9 found by either algorithm was @xmath108 .",
    "this small difference between likely and unlikely points means that even a fraction of a percent error in the value of @xmath9 predicted by our test grid gaussian process will result in a mischaracterization .",
    "figure [ fig : err ] shows that this is indeed what happens .",
    "here we plot the fractional error in predicted @xmath9 as a function of actual @xmath9 for both algorithms .",
    "red curves are mcmc .",
    "blue curves are aps .",
    "dotted curves are results after sampling 50,000 points . dashed curves are results after sampling 100,000 points .",
    "solid curves are results after sampling 400,000 points .",
    "as you can see from figure [ fig : errzoom ] , there is indeed imprecision on the order of 1% when considering test points near the 95% confidence limit threshold . on a more forgiving likelihood surface with greater @xmath109 between likely and unlikely points",
    ", this would not result in the large number of mischaracterizations evident in figure [ fig : grid2 ] .",
    "the wmap 7 likelihood surface is anything but forgiving .",
    "readers interested in seeing how aps learns about the likelihood surface over time can consider figures [ fig : errwide ] , which recreates figure [ fig : errzoom ] for a broader expanse of @xmath9 , and figure [ fig : chi ] , which shows the fraction of mischaracterized points on both test grids as a function of @xmath9 values .",
    "as you can see , while aps learns rapidly about the unlikely regions of parameter space , mcmc remains largely oblivious to what is going on in the regions outside of its integral bounds .",
    "these results , combined with the parameter constraints illustrated in section [ sec : constraints ] cause us to conclude that aps is at least as effective as mcmc and locating regions of high likelihood parameter space , and is significantly more robust against anomalies in regions of low likelihood parameter space .",
    "aps as demonstrated here has three principal advantages over mcmc when deriving parameter constraints .    *",
    "( ia ) aps is more computationally efficient than mcmc in that it does not spend time exploring the interior of high likelihood regions when only their bounds are of interest . as a result it can yield comparable parameter constraints with significantly fewer calls made to expensive likelihood functions .",
    "+ * ( iia ) aps allows for simultaneous robust statements about both high and low likelihood regions of parameter space .",
    "mcmc is robust only in the high likelihood region it happens to discover . + * ( iiia ) aps allows investigators to apply frequentist assumptions to their parameter constraints .",
    "the shortcomings of aps are twofold .    *",
    "( ib ) aps has no framework for exploring a likelihood function whose form is not known ( this is the corrollary to ( iiia ) above ) .",
    "you must specify @xmath14 or @xmath82 before running aps .",
    "you can not use aps to discover @xmath14 .",
    "+ * ( iib ) there is no well - accepted stopping criterion for the algorithm equivalent to the convergence criteria usually applied to mcmc ( for example , @xcite ) . however , as we observed on the toy problem and as bryan et al .",
    "observed by finding a second high likelihood area in the wmap data , mcmc s convergence provides a false sense of security . + * ( iiib ) aps is much more complicated to implement than the most basic mcmc .",
    "we hope that by making our code publically available , we can help the community to overcome this hurdle .",
    "aps may be used as a more efficient alternative to mcmc . they may also be combined . often",
    ", the convergence of mcmc chains is dependent on the size of parameter space to be explored .",
    "the larger the region , the slower convergence .",
    "investigators can exploit advantage ( ia ) by pre - processing their data with aps and using the discovered high likelihood regions to set the prior bounds for their mcmc analyses .",
    "we make our code available at ` https://github.com/uwssg/aps ` .",
    "the code is presented as a series of c++ modules with directions indicating where the user can interface with her particular likelihood function .",
    "those with questions about the code or the algorithm should not hesitate to contact the authors .",
    "sfd would like to thank eric linder , arman shafieloo , and jacob vanderplas for useful conversations about gaussian processes .",
    "sfd would also like to acknowledge the hospitality of the institute for the early universe at ewha womans university in seoul , korea , who hosted him while some of this work was done .",
    "we acknowledge support from doe award grant number desc0002607 and nsf grant iis-0844580 ."
  ],
  "abstract_text": [
    "<S> we consider the problem of inferring constraints on a high - dimensional parameter space with a computationally expensive likelihood function . </S>",
    "<S> markov chain monte carlo ( mcmc ) methods offer significant improvements in efficiency over grid - based searches and are easy to implement in a wide range of cases . </S>",
    "<S> however , mcmcs offer few guarantees that all of the interesting regions of parameter space are explored . </S>",
    "<S> we propose a machine learning algorithm that improves upon the performance of mcmc by intelligently targeting likelihood evaluations so as to quickly and accurately characterize the likelihood surface in both low- and high - likelihood regions . </S>",
    "<S> we compare our algorithm to mcmc on toy examples and the 7-year wmap cosmic microwave background data release . </S>",
    "<S> our algorithm finds comparable parameter constraints to mcmc in fewer calls to the likelihood function and with greater certainty that all of the interesting regions of parameter space have been explored .    </S>",
    "<S> [ firstpage ] </S>"
  ]
}