{
  "article_text": [
    "let @xmath0 denote the output of node @xmath1 in a neural network .",
    "hebbian learning ( hebb 1949 ) is a type of unsupervised learning where the neural network connection strengths @xmath2 are reinforced whenever the products @xmath3 are large . if @xmath4 is the correlation matrix @xmath5 and the hebbian learning law is local , all the lines of the connection matrix @xmath2 will converge to the eigenvector of @xmath4 with the largest eigenvalue . to obtain other eigenvector directions",
    "requires non - local laws ( sanger 1989 , oja 1989 , 1992 , dente and vilela mendes 1996 ) . these principal component analysis ( pca )",
    "algorithms find the characteristic directions of the correlation matrix @xmath4 .",
    "if the data has zero mean ( @xmath6 ) they are the orthogonal directions along which the data has maximum variance .",
    "if the data is gaussian in each channel , it is distributed as a hyperellipsoid and the correlation matrix @xmath4 already contains all information about the statistical properties .",
    "this is because higher order moments of the data may be obtained from the second order moments .",
    "however , if the data is non - gaussian , the pca analysis is not complete and higher order correlations are needed to characterise the statistical properties .",
    "this led some authors ( softy and kammen 1991 , taylor and coombes 1993 ) to propose networks with higher order neurons to obtain the higher order statistical correlations of the data .",
    "an higher order neuron is one that is capable of accepting , in each of its input lines , data from two or more channels at once .",
    "there is then a set of adjustable strengths @xmath2 , @xmath7 , @xmath8 , @xmath9 , @xmath10 being the order of the neuron .",
    "networks with higher order neurons have interesting applications , for example in fitting data to a high - dimensional hypersurface .",
    "however there is a basic weakness in the characterisation of the statistical properties of non - gaussian data by higher order moments .",
    "existence of the moments of a distribution function depends on the behaviour of this function at infinity and it frequently happens that a distribution has moments up to a certain order , but no higher ones .",
    "a well - behaved probability distribution might even have no moments of order higher than one ( the mean ) .",
    "in addition a sequence of moments does not necessarily determine a probability distribution function uniquely ( lukacs 1970 ) .",
    "two different distributions may have the same set of moments .",
    "therefore , for non - gaussian data , the pca algorithms or higher order generalisations may lead to misleading results .    as an example consider the two - dimensional signal shown in fig.1 .",
    "2 shows the evolution of the connection strengths @xmath11 and @xmath12 when this signal is passed through a typical pca algorithm .",
    "large oscillations appear and finally the algorithm overflows .",
    "smaller learning rates do not introduce qualitative modifications in this evolution .",
    "the values may at times appear to stabilise , but large spikes do occur .",
    "the reason for this behaviour is that the seemingly harmless data in fig.1 is generated by a linear combination of a gaussian with the following distribution@xmath13 which has first moment , but no moments of higher order",
    ".    to be concerned with non - gaussian processes is not a pure academic exercise because , in many applications , adequate tools are needed to analyse such processes .",
    "for example , processes without higher order moments , in particular those associated with lvy statistics , are prominent in complex processes such as relaxation in glassy materials , chaotic phase diffusion in josephson junctions and turbulent diffusion ( shlesinger et al 1993 , zumofen and klafter 1993 , 1994 ) .    moments of an arbitrary probability distribution may not exist .",
    "however , because every bounded and measurable function is integrable with respect to any distribution , the existence of the characteristic function @xmath14 is always assured ( lukacs 1970 ) .",
    "@xmath15 where @xmath16 and @xmath17 are n - dimensional vectors , @xmath17 is the data vector and @xmath18 its distribution function .",
    "the characteristic function is a compact and complete characterisation of the probability distribution of the signal .",
    "if , in addition , one wishes to describe the time correlations of the stochastic process @xmath19 , the corresponding quantity is the characteristic functional ( hida 1980 ) @xmath20 where @xmath21 is a smooth function and the scalar product is @xmath22 where @xmath23 is the probability measure over the sample paths of the process .    in the following",
    "we develop an algorithm to compute the characteristic function from the data , by a learning process .",
    "the main idea is that in the end of the learning process we should have a neural network which is a representation of the characteristic function .",
    "this network is then available to provide all the required information on the probability distribution of the data being analysed . to obtain full information on the stochastic process",
    ", a similar algorithm might be used to construct the characteristic functional .",
    "however this turns out to be computationally very demanding .",
    "instead we develop a network to learn the transition function and from this the process may be characterised .",
    "suppose we want to learn the characteristic function @xmath14 ( eq . [ 1.2 ] ) of a one - dimensional signal @xmath19 in a domain @xmath24 $ ] . the @xmath16-domain is divided into n intervals by a sequence of values @xmath25 @xmath26 @xmath27 @xmath28 @xmath29 and a network is constructed with n+1 intermediate layer nodes and an output node ( fig.3 ) .",
    "the learning parameters in the network are the connection strengths @xmath30 and the node parameters @xmath31 .",
    "the existence of the node parameter means that the output of a node in the intermediate layer is @xmath32 , @xmath33 being a non - linear function .",
    "the use of both connection strengths and node parameters in neural networks makes them equivalent to a wide range of other connectionist systems ( doyne farmer 1990 ) and improves their performance in standard applications ( dente and vilela mendes 1996 ) .",
    "the learning laws for the network of fig.3 are : @xmath34 \\theta _ i\\left ( t\\right ) \\chi _ i\\left ( \\alpha _ j\\right )   \\end{array}\\ ] ] @xmath35 . the intermediate layer nodes are equipped with a radial basis function @xmath36 where in general we use @xmath37 for all @xmath1 .",
    "the output is a simple additive node .",
    "the learning constant @xmath38 should be sufficiently small to insure that the learning time is much smaller than the characteristic times of the data @xmath39 .",
    "if this condition is satisfied each node parameter @xmath31 tends to @xmath40 , the real part of the characteristic function @xmath14 for @xmath41 .",
    "the @xmath30 learning law was chosen to minimise the error function @xmath42 one sees that the learning scheme is an hybrid one , in the sense that the node parameter @xmath31 learns , in an unsupervised way , ( the real part of ) the characteristic function @xmath43 and then , by a supervised learning scheme , the @xmath30 s are adjusted to reproduce the @xmath31 value in the output whenever the input is @xmath44 . through the learning law ( [ 2.1 ] ) each node parameter @xmath31 converges to @xmath45 and the interpolating nature of the radial basis functions guarantees that , after training , the network will approximate the real part of the characteristic function for any @xmath16 in the domain @xmath46 $ ] .",
    "a similar network is constructed for the imaginary part of the characteristic function , where now @xmath47    for higher dimensional data the scheme is similar .",
    "the number of required nodes is @xmath48 for a @xmath49-dimensional data vector @xmath50 .",
    "for example for the 2-dimensional data of fig.1 we have used a set of @xmath51 nodes ( fig.4 )    each node in the square lattice has two inputs for the two components @xmath52 and @xmath27 of the vector argument of @xmath53 .",
    "the learning laws are , as before    @xmath54 \\theta _ { ( ij)}\\left ( t\\right ) \\chi _ { ( ij)}\\left ( \\overrightarrow{\\alpha _ { ( kl)}}% \\right )   \\end{array}\\ ] ]    the pair @xmath55 denotes the position of the node in the square lattice and the radial basis function is @xmath56 two networks are used , one for the real part of the characteristic function , another for the imaginary part with , in eqs.([2.5 ] ) , @xmath57 replaced by @xmath58 .    figs.5a - b shows the values computed by our algorithm for the real and imaginary parts of the characteristic function corresponding to the two - dimensional signal in fig.1 . on the left is a plot of the exact characteristic function and on the right the values learned by the network . in this case we show only the mesh corresponding to the @xmath31 values .",
    "one obtains a 2.0% accuracy for the real part and 4.5% accuracy for the imaginary part .",
    "the convergence of the learning process is fast and the approximation is reasonably good .",
    "notice in particular the slope discontinuity at the origin which reveals the non - existence of a second moment .",
    "the parameters used for the learning laws in this example were @xmath38=0.00036 , @xmath59=1.8 , @xmath60=0.25 .",
    "the number of points in the training set is 25000 .    for a second example",
    "the data was generated by a weierstrass random walk with probability distribution @xmath61 and b=1.31 , which is a process of the lvy flight type .",
    "the characteristic function , obtained by the network , is shown in fig . 6 .",
    "taking the @xmath62of the network output one obtains the scaling exponent 1.49 near @xmath16=0 , close to the expected fractal dimension of the random walk path ( 1.5 ) .",
    "the parameters used for the learning laws in this example were @xmath38=0.0005 , @xmath59=1.75 , @xmath63=0.1732 .",
    "the number of points in the training set is 80000 .",
    "these examples test the algorithm as a process identifier , in the sense that , after the learning process , the network is a dynamical representation of the characteristic function and may be used to perform all kinds of analysis of the statistics of the data .",
    "there are other ways to obtain the characteristic function of a probability distribution , which may be found in the statistical inference literature ( prakasa rao 1987 ) .",
    "our purpose in developing neural - like algorithms for this purpose was both to have a device that , after learning , is quick to evaluate and , at the same time , adjusts itself easily , through continuous learning , to changing statistics .",
    "as the pca algorithms that extract the full correlation matrix , our neural algorithm laws are also non - local . as a computation algorithm",
    "this is not a serious issue , but for hardware implementations it might raise some problems .",
    "as we have stated before the full characterisation of the time structure of a stochastic process requires the knowledge of its characteristic functional ( eq . [ 1.3 ] ) for a dense set of functions @xmath64 .    to construct an approximation to the characteristic functional we might discretize the time and the inner product in the exponential becomes a sum over the process sampled at a sequence of times .",
    "@xmath65 the problem would then be reduced to the construction of a multidimensional characteristic function as in section 2 . in practice",
    "we would have to limit the time - depth of the functional to a maximum of @xmath66 time steps , @xmath67 being the maximum time - delay over which time correlations may be obtained . if @xmath68 is the number of different @xmath17 values for each @xmath69 , the algorithm developed in section 2 requires @xmath70 nodes in the intermediate layer and , for any reasonably large @xmath66 , this method becomes computationally explosive .",
    "an alternative and computationally simpler method consist in , restricting ourselves to markov processes , to characterise the process by the construction of networks to represent the transition function for fixed time intervals . from these networks",
    "the differential chapman - kolmogorov equation may then be reconstructed .",
    "let @xmath19 be a one dimension markov process and @xmath71 its transition function , that is , the conditional probability of finding the value @xmath72 at time @xmath73 given @xmath74 at time @xmath75 .",
    "assume further that the process is stationary @xmath76    the network that configures itself to represent this function is similar to the one we used for the 2-dimensional characteristic function .",
    "it is sketched in fig.s 7a - b .",
    "it has a set of @xmath51 intermediate layer nodes with node parameters , the node with coordinates @xmath77 corresponding to the arguments @xmath78 in the transition function .",
    "the domain of both arguments is @xmath79 . for each pair @xmath80 in the data set ,",
    "the node parameters that are updated are those in the 4 columns containing the nearest neighbours of the point @xmath81 ( fig .",
    "the learning rule is @xmath82 @xmath83 where @xmath84 if @xmath55 is one of the nearest neighbours of the data point and zero otherwise .",
    "@xmath16 is a neighbourhood smoothing factor .",
    "@xmath85 is a column normalisation factor . in the limit of large learning times",
    "the node parameters approach the transition function @xmath86 as for the networks in section 2 , the intermediate layer nodes are equipped with a radial basis function ( eq . [ 2.6 ] ) and the connection strengths in the output additive node have a learning law identical to the second equation in ( [ 2.5 ] ) .",
    "the role of this part of the network is , as before , to obtain an interpolating effect .    what the algorithm of eqs.([3.3 ] ) and",
    "( [ 3.4 ] ) does is to compute recursively the average number of transitions between points in the configuration space of the process .",
    "the spatial smoothing effect of the algorithm automatically insures a good representation of a continuous function from a finite data set .",
    "furthermore its recursive nature would be appropriate for the case of drifting statistics .",
    "for a stationary process , once the learning process has been achieved and if @xmath87 is chosen to be sufficiently small , the network itself may be used to simulate the stationary markov process . a complete characterisation of the process may also be obtained by training a few similar networks for different ( small ) @xmath87 values and computing the coefficient functions in the differential chapman - kolmogorov equation ( gardiner 1983 ) .",
    "@xmath88 the coefficients are obtained from the transition probabilities , noticing that for all @xmath89@xmath90 @xmath91 @xmath92 @xmath93 is the jumping kernel , @xmath94 the drift and @xmath95 the diffusion coefficient .    as an example we have considered a markov process with jumping , drift and diffusion .",
    "a typical sample path is shown in fig .",
    "three networks were trained on this process , to learn the transition function for @xmath96 , @xmath97 and @xmath98 ( @xmath99ms ) .",
    "9 shows the transition function for @xmath96 and @xmath98 .",
    "10 shows two sections of the transition function for @xmath100 , that is , @xmath101 and @xmath102 .",
    "the networks were then used to extract the coefficient functions @xmath103 , @xmath104 and @xmath105 . to find the drift @xmath103 we use eq .",
    "( [ 3.8 ] ) .",
    "11 shows the computed drift function and a least square linear fit .",
    "also shown is a comparison with the exact drift function of the process .    to obtain the diffusion coefficient",
    "@xmath106 we use ( [ 3.9 ] ) .",
    "12 shows the diffusion coefficient for different @xmath87 values .",
    "@xmath87 is the smallest time step used in the process simulation .",
    "therefore @xmath107 is our estimate for the diffusion coefficient . in this case , because the diffusion coefficient is found to be a constant , the value of the jumping kernel @xmath105 is easily obtained by integration around the local maxima @xmath108 of @xmath109 with @xmath110 .",
    "@xmath111 with @xmath112 .",
    "we conclude @xmath113 .",
    "the parameters used for the learning laws in this example were @xmath114=0.48 , @xmath16=0.00021 .",
    "the number of points in the training set is 1000000 ."
  ],
  "abstract_text": [
    "<S> principal component analysis ( pca ) algorithms use neural networks to extract the eigenvectors of the correlation matrix from the data . </S>",
    "<S> however , if the process is non - gaussian , pca algorithms or their higher order generalisations provide only incomplete or misleading information on the statistical properties of the data . to handle such situations we propose neural network algorithms , with an hybrid ( supervised and unsupervised ) learning scheme , which constructs the characteristic function of the probability distribution and the transition functions of the stochastic process . </S>",
    "<S> illustrative examples are presented , which include cauchy and lvy - type processes . </S>"
  ]
}