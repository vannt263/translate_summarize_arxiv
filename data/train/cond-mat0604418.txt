{
  "article_text": [
    "percolation is a thoroughly studied model in statistical physics.@xcite the algorithm invented by hoshen and kopelman in 1976 allows for examing large percolation lattices using monte carlo methods,@xcite as for simulating a @xmath6 lattice only a hyperplane of @xmath7 sites has to be stored .",
    "the normal way to do a simulation using the hk algorithm is to choose an @xmath0 and an occupation probability @xmath8 , and then walk through the lattice in a linear fashion , calculating interesting observables ( like cluster numbers ) on the fly or at the end .",
    "when one is interested in the dependance of these observables on either @xmath8 or @xmath0 , one has to repeat these simulations with varying values of these parameters .",
    "in 2000 newman and ziff published an algorithm which allows to simulate percolation for all @xmath9 in one run , making it very easy to study percolation properties in dependance on @xmath8.@xcite unfortunately , their algorithm lacks a desirable property of the hk algorithm : they need to store the full lattice of @xmath6 sites , whereas for hk only memory for @xmath7 sites is needed .",
    "a slight modification of the hk algorithm allows to simulate percolation for many @xmath10 in one run , with only a small performance penalty ( depending on the number of intermediate @xmath0 which are chosen for investigation ) .",
    "the biggest advantage of the hk algorithm , small memory consumption , is preserved .",
    "the modified algorithm needs to store three times as much sites as the original one ( a constant factor ) , but this is still @xmath11 , meaning big advantages for low dimensions .",
    "this modified algorithm shall be presented in this paper , along with simulation results for site percolation on the cubic lattice , i.  e.  @xmath12 .",
    "the traditional hk algorithm works by dividing the @xmath6 lattice into @xmath0 hyperplanes of @xmath7 sites , then investigating one hyperplane after the other .",
    "one advantage of this simple scheme is that it allows for domain decomposition and thus parallelization.@xcite    another approach is to work recursively on growing lattices : first simulate a lattice of size @xmath6 , then advance to @xmath13 by adding a shell with a thickness of one site onto the old lattice .",
    "this way , when simulating an @xmath14 lattice , all sub - lattices of sizes @xmath15 can be investigated ; the investigation of intermediate lattices may take some additional time ( for counting clusters etc . ) , but the time spent on generating the full lattice minus time for investigation is the same , because the same number of sites is generated as with the traditional approach .",
    "this recursive approach was chosen for this paper .",
    "it works as follows : imagine a cube of size @xmath16 .",
    "it has six faces , labelled a , b , c , x , y , z ( cf .  fig .",
    "[ f1 ] ) . in order to increase the size @xmath0 of the cube by one , three new faces a ,",
    "b , c with a thickness of one are slapped onto the old faces a , b , c ; additionally , edges ab , bc , and ca , and a corner abc are added , forming a full @xmath17 cube .",
    "dimensions , only a @xmath18 hyperplane needs to be stored , here for the growing cube only its surface .",
    "when going from an @xmath0-cube to an @xmath19-cube , the surface grows . as edges and corners",
    "have to be treated specially , they are stored separately . for a @xmath20 cube , the faces ( a , b , c ) are @xmath21 , the edges ( ab , bc , ca ) @xmath22 , and the corner ( abc ) is @xmath23 .",
    "[ f1],width=453 ]    using the hk algorithm ( both the original and the modified version ) , each newly generated site has @xmath24 old neighbors . for @xmath12",
    ", we have top , left , and back neighbors . for the sake of simplicity , we assume here that top and left neighbors are within a new face a ( resp .",
    "b , c ) , while back neighbors are in the old face a. due to geometry , site numbering is shifted by one : the site a@xmath25 has a back neighbor of a@xmath26 .",
    "a@xmath27 has a back neighbor ca@xmath28 , a@xmath29 has a back neighbor ab@xmath30 , and a@xmath31 has abc .",
    "thus edges and the corner need to be treated specially ( cf .  fig .",
    "[ f2 ] for more clarity ) .     to @xmath19 , the surface which needs to be stored for the hoshen - kopelman algorithm , needs to grow , too .",
    "the faces of the cube , here a , grow from @xmath21 ( a , plain font in figure ) to @xmath32 ( a , bold italic in figure ) .",
    "for the largest part of a , a@xmath25 , the back neighbors are sites of a , but for the inner rim of a , a@xmath33 resp .",
    "a@xmath34 , the back neighbors are ca resp .  ab . for a@xmath31 , it is abc .",
    "[ f2],width=453 ]    the site a@xmath35 has the left neighbor a@xmath36 and the top neighbor a@xmath37 , when we start working inwards from a@xmath38 . this way",
    ", we do not need to store a and a , but can overwrite a with the values of a. the same trick is used in the original hk algorithm for storing only one hyperplane instead of two .    this way we need three faces of size @xmath39 , three edges of @xmath40 and one corner of size 1 , for a total of @xmath41 sites , in order to simulate a system of size @xmath16 .",
    "for the traditional approach , we need @xmath42 sites , meaning one third , a fixed factor .",
    "but we need an additional label array for both methods , so that memory consumption does not differ drastically .    as the new approach has about the same speed and only moderately higher memory consumption than the traditional one , it is a viable alternative",
    ".    one important thing to keep in mind is that for a single run , results for @xmath43 and @xmath44 are _ not _ statistically independent , e. g. results for @xmath0 and @xmath19 are naturally highly correlated .",
    "this problem can be alleviated by averaging over many runs with different random numbers , because results for different random numbers are suitably statistically independent , as long as the used random number generator has a sufficiently high quality .      for small lattices , open boundaries would mean a strong distortion of results , in the most cases proportional to @xmath45 ( surface divided by volume ) . in order to get rid of these influences , it is necessary to use periodic boundary conditions , i.  e. sites on an open border are connected to corresponding sites on the opposite border .",
    "this is also possible for the modified hk algorithm .",
    "it is necessary to store not only the front faces a , b , c , but also the rear faces x , y , z. when we choose to obtain observables for an intermediate lattice size @xmath0 , we connect sites on a to z , on b to y , and on c to x.    when during periodicization a cluster on one face connects to the same cluster on the opposite face , that cluster spans the whole systems and wraps around , and thus can be identified as the infinite cluster .",
    "when such a cluster is detected , the system percolates .",
    "it is possible that more than one such cluster forms ; the number of different spanning clusters is counted as @xmath46 .    as the label array is modified by periodicization , we need to save a copy first , because we need the original array to continue the simulation for larger @xmath0 afterwards .",
    "after the periodicization , the modified label array contains all cluster properties ; these can be easily extracted and written out .",
    "the modified array can be discarded afterwards .    using this method , it is possible to use fully periodic boundary conditions for intermediate @xmath0 , although the lattice grows afterwards .",
    "periodic b.  c. mean a performance penalty and a doubling of memory consumption ( as six faces instead of three need to be stored ) .",
    "but the same is also true of the traditional hk algorithm for periodic b.  c. , were two instead of one hyperplane need to be stored , still giving a factor of three in memory consumption .",
    "when simulating large lattices , the label array is filled with labels , to which sites in the lattice point .",
    "due to the way with which the hk algorithm ( both original and modified version ) works , lots of these labels are indirect ones , which point to other labels .",
    "other labels correspond to clusters that are hidden in the bulk of the lattice and no longer touch a surface .",
    "it is possible to get rid of these labels and thus free up precious space by using a method known as nakinishi recycling.@xcite    in order to support fully periodic b.  c. , for each recycling all labels represented in a , b , c , x , y , z , ab , bc , ca , abc , have to be taken into account .",
    "the method is fully analogous to that used for the traditional hk algorithm .",
    "all simulations were done using the r@xmath47 pseudo - random number generator.@xcite the value of @xmath48 is not known exactly for site percolation on the cubic lattice . for this paper",
    "it was chosen as @xmath49.@xcite simulations were done for @xmath50 , in order to investigate behavior below , at , and above the critical threshold .",
    "for each value of @xmath8 , 1000 runs with different random numbers were made and used for averaging .",
    "total required cpu time was 10000 hours on 2.2 ghz opteron processors .",
    "for the number @xmath51 of clusters of size @xmath52 in a lattice , we expect a distribution @xmath53 right at @xmath48 . to make analysis easier , we look at @xmath54 , the number of clusters with at least @xmath52 sites :    @xmath55    in a log - log - plot , we would expect a straight line with slope @xmath56 .",
    "deviations from the power law would be hard to detect due to the logarithmic scale , thus we plot @xmath57 linearly on the @xmath58-axis . by varying @xmath59 until a flat plateau forms",
    ", we can estimate @xmath60 .",
    "we can clearly see in fig .",
    "[ f3 ] the deviation for small @xmath52 , which are the same for different @xmath0 , and for large @xmath52 , which differ for various @xmath0 , as these are caused by the finite system size .",
    "for @xmath61 ( @xmath62 ) , @xmath63 ( @xmath64 ) , and @xmath1 ( @xmath65 ) . by plotting @xmath66 , all points should fall on the same line . for small @xmath52 , we can see corrections to scaling , for large @xmath52 the effects of the finite system size , even though the boundary conditions are fully periodic . for larger @xmath0 , the finite size effects occur for larger @xmath52 .",
    "[ f3],width=377 ]    for @xmath67 , @xmath54 drops off sharply for small @xmath52 , with @xmath52 slowly growing for growing @xmath0 ( not plotted ) .",
    "the same is true for @xmath68 , but there does an additional infinite cluster form ( not plotted , too ) .",
    "an infinite cluster is a cluster that spans the whole system . when using periodic b.  c. , this cluster needs to wrap around , i.  e. span the whole system and then periodically connect to itself .",
    "according to theory , for @xmath67 no infinite cluster is expected , for @xmath69 about one with fractal properties , and for @xmath68 one with bulk properties .",
    "the critical threshold @xmath48 is well - defined only for infinite lattices , where the number of infinite clusters ( when these clusters are really infinite ) @xmath46 is exactly zero below @xmath48 and exactly one above @xmath48 . for finite lattices ,",
    "it is reasonable to define @xmath48 so that the average number of infinite clusters is @xmath70 , i.  e. a spanning cluster forms as often as it does not .",
    "below @xmath48 , @xmath46 drops sharply to zero , above @xmath48 it goes up sharply to one .",
    "@xmath48 depends on the lattice size .    for simulations at @xmath48 , here a value of @xmath71",
    "was chosen . as can be seen form fig .",
    "[ f4 ] , this value is slightly too small , but still very near at the true value for @xmath48 .",
    "an interesting behavior for @xmath46 results : @xmath72 .     of infinite clusters , at @xmath73 , averaged over 1000 runs , depending on lattice size @xmath0 .",
    "right at @xmath48 we expect @xmath74 , i.  e. a spanning cluster forms as often as it does not . as here @xmath75 , the value @xmath71 chosen for this paper is slightly to small ; due to finite size effects , sufficiently large lattices have to be simulated in order to see this , even when using periodic b.  c. for the number of infinite clusters we get @xmath76 .",
    "[ f4],width=377 ]     of infinite clusters , at @xmath73 , averaged over 1000 runs , depending on lattice size @xmath0 .",
    "right at @xmath48 we expect @xmath74 , i.  e. a spanning cluster forms as often as it does not . as here @xmath75 , the value @xmath71 chosen for this paper is slightly to small ; due to finite size effects , sufficiently large lattices have to be simulated in order to see this , even when using periodic b.  c. for the number of infinite clusters we get @xmath76 .",
    "[ f4],width=377 ]    for @xmath68 , @xmath46 is expected to be exactly equal one .",
    "this is true for lattice sizes @xmath77 , but for smaller lattices sizes , an interesting behavior is visible ( cf .",
    "[ f7 ] ) : for very small lattices , less than one infinite cluster forms on average , for slightly larger lattices , sometimes more than one such cluster forms .     of infinite clusters ( i.  e. that wrap around the system ) for @xmath5 , depending on lattice size .",
    "theory predicts that only one such cluster forms .",
    "however , for small lattice sizes a complex finite size behavior can be seen : for @xmath78 , less than one infinite cluster forms on average , for @xmath79 more than one cluster forms , with a maximum at @xmath80 , afterwards @xmath46 drops to @xmath81 exponentially .",
    "the solid line corresponds to @xmath82 .",
    "for this plot , @xmath83 runs of up to @xmath61 have been averaged .",
    "[ f7],width=377 ]     of infinite clusters ( i.  e. that wrap around the system ) for @xmath5 , depending on lattice size .",
    "theory predicts that only one such cluster forms .",
    "however , for small lattice sizes a complex finite size behavior can be seen : for @xmath78 , less than one infinite cluster forms on average , for @xmath79 more than one cluster forms , with a maximum at @xmath80 , afterwards @xmath46 drops to @xmath81 exponentially .",
    "the solid line corresponds to @xmath82 . for this plot ,",
    "@xmath83 runs of up to @xmath61 have been averaged .",
    "[ f7],width=377 ]    for @xmath67 , no infinite cluster forms for even small @xmath0 . only for very small @xmath0 , sometimes an infinite cluster appears . for larger @xmath8",
    "this @xmath0 becomes larger ( not plotted ) .",
    "as mentioned above , the infinite cluster has fractal properties at @xmath69 , meaning that it grows @xmath84 , with @xmath85 a non - integer value . from fig .",
    "[ f5 ] we can extract a value of @xmath86 .     of the largest cluster ( which is the infinite cluster , if it forms ) , at @xmath69 , scaled by @xmath87 , depending on simulated lattice size .",
    "the cluster grows @xmath84 , with a fractal dimension of @xmath86 .",
    "thus , by scaling , we get a straight line ( the line visible in the plot serves as guide to the eye ) . for very small lattice sizes @xmath0 , corrections to scaling can be seen .",
    "[ f5],width=377 ]    for @xmath68 , the infinite cluster obtains bulk properties , i.  e.  it grows @xmath84 with @xmath88 ; it has no longer fractal properties ( not plotted ) .    for @xmath67",
    ", we expect the largest cluster ( which does not span the whole lattice ) to grow @xmath89 , cf .",
    "this is true for @xmath2 and @xmath3 ( not plotted ) , only slope and intercept are different . from fig .",
    "[ f6 ] another important effect can be seen : although it seems natural to determine the largest cluster @xmath90 by taking the largest cluster of all independent runs , this would mean that no averaging happens , and thus the values of @xmath90 are not statistically independent for different @xmath0 : a large cluster in one single run would dominate the results .",
    "thus , even here averaging is necessary to obtain meaningful results .     of the largest cluster , at p=0.25 , depending on simulated lattice size .",
    "@xmath65 are for @xmath90 averaged over 1000 runs , @xmath64 for the maximal @xmath90 of 1000 runs , which has stronger fluctuations ( as the maximum is determined by single clusters ; in fact , as no averaging is done , the results for @xmath0 and @xmath19 are _ not _ statistically independent ) .",
    "the solid line corresponds to @xmath91 , the dashed line to @xmath92 . thus ,",
    "for @xmath67 , the largest cluster grows @xmath93 .",
    "[ f6],width=377 ]",
    "it is possible to modify the hoshen - kopelman algorithm in order to obtain not only results for a @xmath14 lattice in one simulation run , but also results for intermediate @xmath0 with @xmath15 .",
    "compared to the original hk algorithm , there is a small performance penalty and a higher memory consumption ( a constant factor of @xmath24 for storing the lattices sites ) .",
    "the modified algorithm is very useful in order to study the dependance of percolation properties on the lattice size .    in this paper ,",
    "the new algorithm was used for site percolation on the cubic lattice , i.  e.  @xmath12 , lattice sizes up to @xmath94 , and occupation probabilities below , at , and above @xmath48 .",
    "results are presented for cluster size distribution , number of infinite clusters , and size of the largest cluster .",
    "a natural next step would be to move away from three dimensions and to adapt the algorithm to different @xmath24 .",
    "adapting it to @xmath95 is easy , while @xmath96 is not easy , but still straightforward .",
    "parallelization by using domain decomposition is apparently very hard , as the size of the domains would be changing .",
    "this would only be necessary in order to simulate huge lattices , where the @xmath0-dependence might not be very interesting .",
    "as several runs always need to be averaged , in order to obtain statistically independent data , replication is always a viable parallelization strategy .",
    "i would like to thank d.  stauffer for finally forcing me to publish this paper .",
    "furthermore , i would like to thank the computing centre of the university of cologne for computing time on their hpc - cluster clio ."
  ],
  "abstract_text": [
    "<S> in order to inverstigate the dependence on lattice size of several observables in percolation , the hoshen - kopelman algorithm was modified so that growing lattices could be simulated . by this way </S>",
    "<S> , when simulating a lattice of size @xmath0 , lattices of smaller sizes can be simulated in the same run for free , saving computing time .    here </S>",
    "<S> , site percolation in three dimensions was studied . </S>",
    "<S> lattices of up to @xmath1 , with many @xmath0-steps in between , have been simulated , for occupation probabilities of @xmath2 , @xmath3 , @xmath4 , and @xmath5 . </S>"
  ]
}