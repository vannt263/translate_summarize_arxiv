{
  "article_text": [
    "the molecular dynamics ( md ) simulation code _",
    "ls1 mardyn_(large systems 1 : molecular dynamics ) is presented here .",
    "ls1 mardyn_program is an interdisciplinary endeavor , whose contributors have backgrounds from engineering , computer science and physics , aiming at studying challenging scenarios with up to trillions of molecules . in the considered systems ,",
    "the spatial distribution of the molecules may be heterogeneous and subject to rapid unpredictable change .",
    "this is reflected by the algorithms and data structures as well as a highly modular software engineering approach .",
    "the source code of _",
    "ls1 mardyn_is made publicly available as free software under a two - clause bsd license.@xcite    molecular modelling and simulation has become a powerful computational method@xcite and is applied to a wide variety of areas such as thermodynamic properties of fluids,@xcite phase equilibria,@xcite interfacial properties,@xcite phase transitions,@xcite transport coefficients,@xcite adsorption , @xcite mechanical properties of solids,@xcite flow phenomena,@xcite polymer properties,@xcite protein folding , @xcite or self - assembly.@xcite the sound physical basis of the approach makes it extremely versatile . for a given force field , the phase space can be explored by molecular dynamics simulation under a variety of boundary conditions , which allows gathering information on all thermodynamic states and processes on the molecular level .",
    "if required , external forces ( e.g.  an electric field ) can be imposed in addition to the intermolecular interactions .",
    "md simulation has an extremely high temporal and spatial resolution of the order of @xmath0 seconds and @xmath1 meters , respectively .",
    "this resolution is useful for studying physical phenomena at small length scales , such as the structure of fluid interfaces . with a time discretization on the femtosecond scale , rapid processes are immediately accessible , while slower processes may require particularly devised sampling techniques such as metadynamics.@xcite the number of molecules is also a challenge for molecular simulation .",
    "while systems of practical interest contain extremely large numbers of molecules , e.g.  of the order of @xmath2 , the largest ensembles that can be handled today are of the order of @xmath3 molecules.@xcite this limitation is usually addressed by focusing on representative subvolumes , containing a limited number of molecules , to which an appropriate set of boundary conditions is applied .",
    "depending on the type of information that is determined , e.g.  transport properties @xcite or phase equilibria @xcite of bulk phases , a number of molecules of the order of @xmath4 @xmath5 may be sufficient . however , non - equilibrium scenarios such as condensation @xcite or mass transfer through nanoporous membranes@xcite require much larger simulation volumes .",
    "there are so many scalable md codes available that a comprehensive discussion would be beyond the scope of the present paper . for the development of md codes , as for any software ,",
    "there are trade - offs between generality and optimization for a single purpose , which no particular implementation can completely evade .",
    "several popular md simulation environments are tailored for studying biologically relevant systems , with typical application scenarios including conformational sampling of macromolecules in aqueous solution .",
    "the relaxation processes of such systems are often several orders of magnitude slower than for simple fluids , requiring an emphasis on sampling techniques and long simulation times , but not necessarily on large systems .",
    "the _ amber _",
    "package,@xcite for instance , scales well for systems containing up to @xmath6 @xmath5 molecules , facilitating md simulations that reach the microsecond time scale.@xcite similarly , _ _",
    "gromacs__@xcite and _ namd_,@xcite which also have a focus on biosystems , have been shown to perform quite efficiently on modern hpc architectures as well .",
    "_ tinker _ was optimized for biosystems with polarizable force fields,@xcite whereas _",
    "charmm_,@xcite which was co - developed by nobel prize winner martin karplus , is suitable for coupling classical md simulation of macromolecules with quantum mechanics.@xcite the _ lammps _ program @xcite as well as _ dl_poly_,@xcite which scales well for homogeneous fluid systems with up to tens of millions of molecules , and _",
    "espresso_,@xcite which emphasizes its versatility and covers both molecular and mesoscopic simulation approaches , are highly performant codes which aim at a high degree of generality , including many classes of pair potentials and methods .",
    "ms2 _ program performs well for the simulation of vapor - liquid equilibria and other thermodynamic properties,@xcite but is limited to relatively small numbers of molecules .",
    "the _ imd _",
    "code,@xcite which has twice before held the md simulation world record in terms of system size , has a focus on multi - body potentials for solids .    with _",
    "mardyn _ , which is presented here , a novel md code is made available to the public .",
    "it is more specialized than most of the molecular simulation programs mentioned above .",
    "in particular , it is restricted to rigid molecules , and only constant volume ensembles are supported , so that the pressure can not be specified in advance .",
    "electrostatic long - range interactions , beyond the cut - off radius , are considered by the reaction field method,@xcite which can not be applied to systems containing ions . however , _",
    "ls1 mardyn _ is highly performant and scalable . holding the present world record in simulated system size,@xcite",
    "it is furthermore characterized by a _ modular structure _ , facilitating a high degree of flexibility within a _ single code base_. thus , _",
    "ls1 mardyn_is not only a simulation engine , but also a framework for developing and evaluating simulation algorithms , e.g.  different thermostats or parallelization schemes .",
    "therefore , its software structure supports alternative implementations for methods in most parts of the program , including core parts such as the numerical integration of the equations of motion .",
    "the c++ programming language was used , including low level optimizations for particular hpc systems . in this way , _",
    "ls1 mardyn_has been proven to run efficiently on a variety of architectures , from ordinary workstations to massively - parallel supercomputers .    in a fluid system ,",
    "neighborhood relations between molecules are always subject to rapid change .",
    "thus , the neighbor molecules have to be redetermined throughout the simulation . for this purpose , _",
    "ls1 mardyn _ employs a linked - cell data structure,@xcite which is efficiently parallelized by spatial domain decomposition.@xcite thereby , the simulation volume is divided into subvolumes that are assigned to different processes . interactions with molecules in adjacent subvolumes are explicitly accounted for by synchronized halo regions.@xcite using _",
    "ls1 mardyn _",
    ", a wide range of simulation scenarios can be addressed , and pre - release versions of _",
    "ls1 mardyn _ have already been successfully applied to a variety of topics from chemical and process engineering : nucleation in supersaturated vapors @xcite was considered with a particular focus on systems with a large number of particles.@xcite on the supermuc , over four trillion molecules were simulated.@xcite the vapor - liquid surface tension and its dependence on size and curvature was characterized.@xcite the _",
    "ls1 mardyn_program was furthermore employed to investigate fluid flow through nanoporous membrane materials @xcite and adsorption phenomena such as the fluid - solid contact angle in dependence on the fluid - wall interaction.@xcite    scenario generators for _ ls1 mardyn _ are available both internally , i.e.  without hard disk input / output , and as external executables .",
    "the internal generators create the initial configuration directly in memory , which is distributed among the processes , facilitating a better scalability for massively - parallel execution .",
    "a generalized output plugin interface can be used to extract any kind of information during the simulation and to visualize the simulation trajectory with megamol @xcite and other compatible tools .",
    "this paper is organized as follows : section [ sec : models ] describes molecular models which are available in _",
    "mardyn_. section [ sec : md - methods ] introduces the underlying computational methods .",
    "the implemented load balancing approach is discussed in detail in section  [ sec : parallelization ] . a performance analysis of _",
    "ls1 mardyn _ is presented in section  [ sec : performance ] , including results obtained on two of the fastest hpc systems .",
    "molecular motion has two different aspects : external degrees of freedom , corresponding to the translation and rotation with respect to the molecular center of mass , as well as internal degrees of freedom that describe the conformation of the molecule . in _",
    "ls1 mardyn _ , molecules are modeled as rigid rotators , disregarding internal degrees of freedom and employing effective pair potentials for the intermolecular interaction .",
    "this modeling approach is suitable for all small molecules which do not exhibit significant conformational transitions .",
    "an extension of the code to internal degrees of freedom is the subject of a presently ongoing development , which is not discussed here .",
    "the microcanonical ( @xmath7 ) , canonical ( @xmath8 ) and grand - canonical ( @xmath9 ) ensembles are supported , whereby the temperature is ( for @xmath8 and @xmath9 ) kept constant by velocity rescaling .",
    "the lennard - jones ( lj ) potential @xmath10,\\ ] ] with the size parameter @xmath11 and the energy parameter @xmath12 , is used to account for repulsive and dispersive interactions .",
    "it can also be employed in a truncated and shifted ( ljts ) version.@xcite lj potential parameters for the unlike interaction , i.e.  the pair potential acting between molecules of different species , are determined by the lorentz and berthelot combination rules,@xcite which can be further adjusted by binary interaction parameters.@xcite    point charges and higher - order point polarities up to second order ( i.e.  dipoles and quadrupoles ) , are implemented to model electrostatic interactions in terms of a multipole expansion.@xcite this allows an efficient computational handling while sufficient accuracy is maintained for the full range of thermophysical properties.@xcite the tersoff potential  @xcite can be used within _",
    "ls1 mardyn_in order to accurately describe a variety of solid materials.@xcite as a multi - body potential , it is computationally more expensive than electrostatics and the lj potential .",
    "any system of units can be used in _",
    "mardyn_as long as it is algebraically consistent and includes the boltzmann constant @xmath13 as well as the coulomb constant @xmath14 among its basic units .",
    "thereby , expressions for quantities related to temperature and the electrostatic interactions are simplified .",
    "the units of size , energy and charge are related ( by coulomb s law and the coulomb constant unit ) and can not be specified independently of each other . a temperature is converted to energy units by using @xmath15 = @xmath4 , and vice versa . in this way",
    ", all other units are determined ; for an example , see [ tab : units ] .",
    ".a consistent set of atomic units ( used by the scenario generators ) .",
    "[ cols= \" < , < \" , ]      the compiler used to build the code has a large impact on its performance .",
    "[ fig : compiler - comparison ] shows results obtained with a serial version of _",
    "ls1 mardyn_employing different compilers on the sb and nh partitions of _ laki _ as well as on _ hermit_. the test scenarios were a lj vapor ( at @xmath16 = @xmath17 and @xmath18 = @xmath19 ) consisting of @xmath20 molecules and ethylene oxide in a liquid state ( at @xmath21 = @xmath22 k and @xmath23 = @xmath24 mol / l ) with @xmath25 molecules .",
    "as can be seen , the sequential program runs fastest on the sandy bridge based _ laki _ system and built with the gnu compiler .",
    "unless noted otherwise , the gnu  compiler was also used for all further studies discussed below .",
    "+    the computational complexity of the linked - cell algorithm and domain decomposition scheme used in _ ls1 mardyn_is @xmath26 . to evaluate the efficiency of the implementation , runs with different numbers of molecules were performed .",
    "the results in [ fig : complexity - study ] show that in the present case , the implementation scales almost perfectly with @xmath26 , as the execution time per molecule is approximately constant .     with different system sizes on _ laki _",
    "( sb ) . ]      for the scalability evaluation of _",
    "ls1 mardyn _ , different target scenarios with a varying degree of complexity were considered , produced by the internal scenario generators , cf .  [",
    "fig : scenarios ] .    *",
    "* homogeneous liquid : * ethylene oxide at a density of @xmath23 @xmath27",
    "@xmath28 mol / l and a temperature of @xmath29 k. the molecular model for ethylene oxide consists of three lj sites and one point dipole.@xcite * * droplet : * simulation scenario containing a ljts nanodroplet ( cut - off radius @xmath30 @xmath11 ) surrounded by a supersaturated vapor at a reduced temperature of . *",
    "* planar interface : * simulation of a planar vapor - liquid interface of the ljts fluid ( cut - off radius @xmath30 @xmath11 ) at a reduced temperature of .    in the scenarios , the number of molecules was varied .",
    "they were simulated on the platforms given in [ tab : benchmarking - platforms ] for @xmath31 time steps and with disabled final i / o .",
    "parallelization is associated with additional complexity due to communication and synchronization between the different execution paths of the program . in comparison with sequential execution on a single processing unit ,",
    "this introduces an overhead . to determine the magnitude of this overhead for",
    "ls1 mardyn _ , the _ planar interface _ scenario with @xmath32 lj sites was executed over @xmath31 time steps on the _ hermit _ system , both with the sequential and the mpi parallel version of the code , but using only a single process .",
    "execution of the sequential program took @xmath33 s , while the mpi parallel version took @xmath34 s. this indicates that the overhead due to imperfect concurrency amounts to around @xmath35 only .",
    "scaling studies were carried out with the _ homogeneous liquid _ scenario on the entire _ hermit _ system , using the standard domain decomposition method , i.e.  all processes were assigned equal volumes .",
    "the results presented in [ fig : ls1-scaling - animake - hermit ] show that _ ls1 mardyn_scales favorably in the present case .        as discussed above",
    ", load balancing is of major importance for inhomogeneous molecule distributions .",
    "strong scaling experiments were therefore carried out for the _ planar interface _ and _ droplet _ scenarios .",
    "the droplet was positioned slightly off the center of the simulation volume to avoid symmetry effects .",
    "the scenarios were run for @xmath31 time steps , and the decomposition was updated every @xmath36 time steps .",
    "the results are presented in [ fig : ls1-weak - scaling - inhomogeneous - hermit ] and show a clear advantage of the dynamic tree - based decomposition , making the simulation up to four times as fast as the static decomposition into subdomains with equal volume .     +    in addition to comparing the run times , the effectiveness of the dynamic load balancing implementation in _",
    "ls1 mardyn_is supported by traces revealing the load distribution between the processes .",
    "[ fig : ls - lb - mkesfera - np14 ] shows such traces , generated with _ vampirtrace _ , for @xmath37 processes of a _ droplet _ scenario simulation on the _ hermit _ system . for the trivial domain decomposition , @xmath38 out of @xmath37 processes are waiting in mpi routines most of the time , while the remaining three processes have to carry the bulk of the actual computation .",
    "in contrast , the @xmath39-d decomposition exhibits a more balanced distribution of computation and communication .      a version of _ ls1 mardyn_was optimized for simulating single - site lj particles on the _ supermuc _",
    "system,@xcite one of the largest x86 systems worldwide with @xmath40 cores and a theoretical peak performance of more than @xmath41  pflops .",
    "it is based on a high - performance fdr-10 infiniband interconnect by mellanox and composed of 18 so - called islands , each of which consists of 512 nodes with 16 intel sandy bridge ep cores at @xmath42 ghz clock speed ( turbo mode disabled ) sharing @xmath43 gb of main memory .",
    "main features of the optimized code version include a lightweight shared - memory parallelization and hand - coded intrinsics in single precision for the lj interactions within the kernel .",
    "the kernels were implemented in avx128 ( rather than avx256 ) , mainly for two reasons : first , the architecture of the intel sandy bridge processor is unbalanced with respect to load and store bandwidth , which may result in equal performance for both variants .",
    "second , avx128 code usually shows better performance on the amd bulldozer architecture , where two processor cores share one 256-bit floating - point unit .    to evaluate the performance with respect to strong scaling behavior",
    ", a scenario with  particles was studied , which fits into the memory of two nodes , as @xmath44 gb per node are needed .",
    "thereby , a cut - off radius of @xmath45 @xmath27 @xmath46 @xmath11 was employed .",
    "[ fig : scaling ] shows that a very good scaling was achieved for up to @xmath47 cores using @xmath25 threads .",
    "built with the intel compiler , the implementation delivered a sustained performance of @xmath48  gflops , corresponding to @xmath49 % single - precision peak performance at a parallel efficiency of @xmath50 compared to @xmath43  cores ( @xmath51  threads ) .",
    "in addition , a weak scaling analysis with @xmath52 @xmath53 @xmath54 particles per node was performed , where a peak performance of @xmath55 or @xmath56 tflops was achieved at a parallel efficiency of @xmath57 % when scaling from @xmath4 to @xmath43 @xmath58 cores .    as the kernel was implemented using avx128 , the same scenario was executed on the cray xe6 system _ hermit _ at hlrs , however , without shared - memory parallelization and built with the gnu compiler .",
    "a noteworthy feature of the cray xe6 machine is its 3d torus network with gemini interconnect , which directly plugs in to the hypertransport 3 host interface for fast mpi communication . on _ hermit _",
    ", the code achieved a parallel efficiency of @xmath59 and @xmath60 gflops in case of strong scaling and @xmath61 % and @xmath62 tflops or @xmath63 peak performance for weak scaling , respectively , on @xmath47 cores in comparison to @xmath51 cores , i.e.  two nodes .",
    "as can be seen in [ fig : scaling ] , the scalability on _ hermit _ is superior , particularly for strong scaling .",
    "the gemini interconnect allows for higher bandwidth and lower latency for mpi communications than the fdr-10 infiniband interconnect of _",
    "supermuc_. furthermore , a 3d torus network is more favorable for the communication pattern of _ ls1 mardyn_than the tree topology of _ supermuc _ , where the nodes belonging to each island ( @xmath64 cores ) communicate via a fully connected network , while for inter - island communication four nodes have to share a single uplink .",
    "this can also be seen in [ fig : scaling ] , where the scalability noticeably drops when going from @xmath64 to @xmath65 processes .     ) and strong scaling ( @xmath66 ) of _ ls1 mardyn_on _ hermit _ ( left ) and _ supermuc _ ( right ) , including the speedup ( top ) and the parallel efficiency ( bottom ) , i.e.  the speedup reduced by the number of processes .",
    "almost ideal scaling was achieved in case of weak scaling , whereas a parallel efficiency of @xmath67 % was obtained in the strong scaling tests on _ supermuc _ and @xmath68 % on _ hermit _ , compared to two nodes.,title=\"fig : \" ]    as described by eckhardt et al.,@xcite a larger weak scaling benchmark on the whole _",
    "supermuc _ was performed with that code version . simulating @xmath69 molecules , to our knowledge",
    "the largest md simulation to date , with a cut - off radius of @xmath45 @xmath27 @xmath70 @xmath11 , one time step took roughly @xmath71 . for this scenario ,",
    "a speedup of @xmath72 compared to a single core with an absolute performance of @xmath73 tflops was achieved , which corresponds to @xmath74  % peak performance efficiency .",
    "the massively parallel md simulation code _ ls1 mardyn_was introduced and presented .",
    "the _ ls1 mardyn_program is designed to simulate homogeneous and heterogeneous fluid systems containing very large numbers of molecules .",
    "fluid molecules are modeled as rigid rotators consisting of multiple interaction sites , enabling simulations of a wide variety of scenarios from noble gases to complex fluid systems under confinement .",
    "the code , which presently holds the world record for the largest md simulation , was evaluated on large - scale hpc architectures .",
    "it was found to scale almost perfectly on over @xmath75 @xmath5 cores for homogeneous scenarios .",
    "the dynamic load balancing capability of _",
    "ls1 mardyn_was tested with different scenarios , delivering a significantly improved scalability for challenging , highly heterogeneous systems .",
    "it can be concluded that _",
    "mardyn _ , which is made publicly available as free software,@xcite represents the state of the art in md simulation .",
    "it can be recommended for large - scale applications , and particularly for processes at fluid interfaces , where highly heterogeneous and time - dependent particle distributions may occur . due to the modularity of its code base , future",
    "work can adjust _",
    "ls1 mardyn _ to newly emerging hpc architectures and further extend the range of available molecular modeling approaches and simulation methods . in this way , _",
    "ls1 mardyn _ aims at driving the progress of molecular simulation in general , paving the way to the micrometer length scale and the microsecond time scale for computational molecular engineering .",
    "the authors would like to thank a.  bode and m.  brehm for their help in accessing the supercomputing infrastructure at the leibniz supercomputing center ( lrz ) of the bavarian academy of sciences and humanities .",
    "they thank d.  mader for his contribution to developing the very first version of the _ ls1 mardyn _ program , s.  grottel , m.  heinen , d.  jenz and g.  reina for their work on libraries and tools , as well as c.  avendao jimnez , s.  eckelsbach , k.  langenbach , r.  lustig , s.  k.  miroshnichenko , e.  a.  mller , g.  rutkai , f.  siperstein , r.  srivastava and n.  tchipev for fruitful discussions .",
    "the present work was conducted under the auspices of the boltzmann - zuse society for computational molecular engineering ( bzs ) , and the molecular simulations were carried out within the supercomputing project _",
    "pr83ri _ on the _ supermuc _ at the lrz , garching , and within mmhbf2 on _ hermit _ and _ laki _ at the hlrs , stuttgart .",
    "financial support is acknowledged due to the imemo and skasim grants of the german federal ministry of education and research ( bmbf ) , and the reinhart koselleck program as well as the collaborative research center micos ( sfb 926 ) of the german research foundation ( dfg ) .        _ large systems 1 : molecular dynamics _ ; http://www.ls1-mardyn.de/ , accessed august 19 , 2014 allen ,  m.  p. ; tildesley ,  d.  j. _ computer simulation of liquids _ ; clarendon : oxford , 1987 frenkel ,  d. ; smit ,  b. _ understanding molecular simulation _",
    ", 2nd ed . ; academic press : san diego , 2002 deublein ,  s. ; eckl ,  b. ; stoll ,  j. ; lishchuk ,  s.  v. ; guevara  carrin ,  g. ; glass ,  c.  w. ; merker ,  t. ; bernreuther ,  m. ; hasse ,  h. ; vrabec ,  j. _ comput .  phys .  comm . _ * 2011 * , _ 182 _ , 23502367 mller ,  d. ; fischer ,  j. _ mol",
    ".  phys . _ * 1990 * , _ 69 _ , 463473 vrabec ,  j. ; hasse ,  h. _ mol",
    ".  phys . _ * 2002 * , _ 100 _ , 33753383 rusanov ,  a.  i. ; brodskaya ,  e.  n. _ j.  colloid interf .",
    "sci . _ * 1977 * , _ 62 _ , 542555 rao ,  m. ; berne ,  b.  j. ; kalos ,  m.  h. _ j.  chem .",
    "phys . _ * 1978 * , _ 68 _ , 13251336 anglil ,  r. ; diemand ,  j. ; tanaka ,  k.  k. ; tanaka ,  h. _ j.  chem .",
    "phys . _ * 2014 * , _ 140 _ , 074303 chialvo ,  a.  a. ; debenedetti ,  p.  g. _ phys .",
    "rev .  a _",
    "* 1991 * , _ 43 _ , 42894295 sokoowski ,  s. ; fischer ,  j. _ phys .",
    "rev .  a _",
    "* 1990 * , _ 41 _ , 68666870 horsch ,  m. ; heitzig ,  m. ; dan ,  c. ; harting ,  j. ; hasse ,  h. ; vrabec ,  j. _ langmuir _ * 2010 * , _ 26 _ , 1091310917 rsch ,  f. ; trebin ,  h .-",
    "* 2009 * , _ 87 _ , 66004 thompson ,  p.  a. ; troian ,  s.  m. _ nature _ * 1997 * , _ 389 _ , 360362 frentrup ,  h. ; avendao ,  c. ; horsch ,  m. ; salih ,  a. ; mller ,  e.  a. _ mol .",
    "sim . _ * 2012 * , _ 38 _ , 540553 mller - plathe ,  f. _ chemphyschem _ * 2002 * , _ 3 _ , 754769 lee ,  e.  h. ; hsin ,  j. ; sotomayor ,  m. ; comellas ,  g. ; schulten ,  k. _ structure _ * 2009 * , _ 17 _ , 12951306 lindorff - larsen ,  k. ; piana ,  s. ; dror ,  r.  o. ; shaw ,  d.  e. _ science _ * 2011 * , _ 334 _ , 517520 engel ,  m. ; trebin ,  h .- r . _ phys .",
    "lett . _ * 2007 * , _ 98 _ , 225505 laio ,  a. ; parrinello ,  m. _ proc .",
    "sci . _ * 2002 * , _ 99 _ , 1256212566 eckhardt ,  w. ; heinecke ,  a. ; bader ,  r. ; brehm ,  m. ; hammer ,  n. ; huber ,  h. ; kleinhenz ,  h .-",
    "g . ; vrabec ,  j. ; hasse ,  h. ; horsch ,  m. ; bernreuther ,  m. ; glass ,  c.  w. ; niethammer ,  c. ; bode ,  a. ; bungartz ,  j. in _ supercomputing  proceedings of the xxviii .  international supercomputing conference ( isc ) _ ; kunkel ,  j.  m. , ludwig ,  t. , meuer ,  h.  w. , eds . ; lecture notes in computer science 7905 ; springer : heidelberg , 2013 ; pp 112 guevara  carrin ,  g. ; vrabec ,  j. ; hasse ,  h. _ j.  chem .",
    "phys . _ * 2011 * , _ 134 _ , 074508 horsch ,  m. ; vrabec ,  j. ; bernreuther ,  m. ; grottel ,  s. ; reina ,  g. ; wix ,  a. ; schaber ,  k. ; hasse ,  h. _ j.  chem .  phys . _ * 2008 * , _ 128 _ , 164510 horsch ,  m. ; vrabec ,  j. _ j.  chem .",
    "phys . _ * 2009 * , _ 131 _ , 184104ller ,  e.  a. _ curr .  opin .",
    "eng . _ * 2013 * , _ 2 _ , 223228 case ,  d.  a. ; cheatham ,  i. ,  t.  e. ; darden ,  t. ; gohlke ,  h. ; luo ,  r. ; merz ,  j. ,  k.  m. ; onufriev ,  a. ; simmerling ,  c. ; wang ,  b. ; woods ,  r. _ j. comput .",
    "chem . _ * 2005 * , _ 26 _ , 16681688 salomon  ferrer ,  r. ; gtz ,  a.  w. ; poole ,  d. ; le  grand ,  s. ; walker ,  r.  c. _ j.  chem .",
    "theory comput .",
    "_ * 2013 * , _ 9 _ , 38783888 berendsen ,  h. j.  c. ; van  der spoel ,  d. ; van drunen ,  r. _ comput .",
    "comm . _ * 1995 * , _ 91 _ , 4356 pronk ,  s. ; szilrd ,  p. ; schulz ,  r. ; larsson ,  p. ; bjelkmar ,  p. ; apostolov ,  r. ; shirts ,  m.  r. ; smith ,  j.  c. ; kasson ,  p.  m. ; van  der spoel ,  d. ; hess ,  b. ; lindahl ,  e. _ bioinformatics _ * 2013 * , _ 29 _ , 845854 phillips ,  j.  c. ; braun ,  r. ; wang ,  w. ; gumbart ,  j. ; tajkhorshid ,  e. ; villa ,  e. ; chipot ,  c. ; skeel ,  r.  d. ; kal ,  l. ; schulten ,  k. _ j.  comput .",
    "chem _ * 2005 * , _ 26 _ , 17811802 ren ,  p. ; wu ,  c. ; ponder ,  j.  w. _ j.  chem .",
    "theory comput . _ * 2011 * , _ 7 _ , 31433461 brooks ,  b.  r. ; bruccoleri ,  r.  e. ; olafson ,  b.  d. ; states ,  d.  j. ; swaminathan ,  s. ; karplus ,  m. _ j.  comput .  chem . _ * 1983 * , _ 4 _ , 187217 brooks ,  b.  r. ; brooks ,  i. ,  c.  l. ; mackerell ,  a.  d. ; nilsson ,  l. ; petrella ,  r.  j. ; roux ,  b. ; won ,  y. ; archontis ,  g. ; bartels ,  c. ; boresch ,  s. ; caflisch ,  a. ; caves ,  l. ; cui ,  q. ; dinner ,  a.  r. ; feig ,  m. ; fischer ,  s. ; gao ,  j. ; hodoscek ,  m. ; i m ,  w. ; kuczera ,  k. ; lazaridis ,  t. ; ma ,  j. ; ovchinnikov ,  v. ; paci ,  e. ; pastor ,  r.  w. ; post ,  c.  b. ; pu ,  j.  z. ; schaefer ,  m. ; tidor ,  b. ; venable ,  r.  m. ; woodcock ,  h.  l. ; wu ,  x. ; yang ,  w. ; york ,  d.  m. ; karplus ,  m. _ j.  comput .  chem . _ * 2009 * , _ 30 _ , 15451615 plimpton ,  s. _ j.  comput .  phys . _ * 1995 * , _ 117 _ , 119 brown ,  w.  m. ; wang ,  p. ; plimpton ,  s.  j. ; tharrington ,  a.  n. _ comput . phys .  comm . _ * 2011 * , _ 182 _ , 898911 plimpton ,  s.  j. ; thompson ,  a.  p. _ mrs bulletin _",
    "* 2012 * , _ 37 _ , 513521 diemand ,  j. ; anglil ,  r. ; tanaka ,  k.  k. ; tanaka ,  h. _ j.  chem .",
    "phys . _ * 2013 * , _ 139 _ , 074309 todorov ,  i.  t. ; smith ,  w. ; trachenko ,  k. ; dove ,  m.  t. _ j.  materials chem . _",
    "* 2006 * , _ 16 _ , 19111918 limbach ,  h .- j . ;",
    "arnold ,  a. ; mann ,  b.  a. ; holm ,  c. _ comput .",
    "_ * 2006 * , _ 174 _ , 704727 stadler ,  j. ; mikulla ,  r. ; trebin ,  h .-",
    "j.  mod .",
    "c _ * 1997 * , _ 8 _ , 11311140 roth ,  j. ; ghler ,  f. ; trebin ,  h .-",
    "j.  mod .",
    "c _ * 2000 * , _ 11 _ , 317322 saager ,  b. ; fischer ,  j. ; neumann ,  m. _ mol .",
    "sim . _ * 1991 * , _ 6 _ , 2749 quentrec ,  r. ; brot ,  c. _ j.  comput .",
    "_ * 1973 * , _ 13 _ , 430432 hockney ,  r.  w. ; eastwood ,  j.  w. _ computer simulation using particles _ ; mcgraw - hill : new york , 1981 schamberger ,  s. ; wierum ,  j .- m . in _ proceedings of the vii .  international conference on parallel computing technologies ( pact ) _ ; malyshkin ,  v. , ed . ;",
    "lecture notes in computer science 2763 ; springer : heidelberg , 2003 ; pp 165179 bernreuther ,  m. ; vrabec ,  j. in _ high performance computing on vector systems _ ; resch ,  m. , bnisch ,  t. , benkert ,  k. , bez ,  w. , furui ,  t. , seo ,  y. , eds . ;",
    "springer : heidelberg , 2006 ; pp 187195 bernreuther ,  m. ; buchholz ,  m. ; bungartz ,  h .- j . in _",
    "parallel computing : architectures , algorithms and applications  proceedings of the xii . international conference on parallel computing ( parco ) _ ; joubert ,  g. , bischof ,  c. , peters ,  f. , lippert ,  t. , bcker ,  m. , gibbon ,  p. , mohr ,  b. , eds . ; advances in parallel computing 15 ; ios : amsterdam , 2008 ; pp 5360 horsch ,  m. ; vrabec ,  j. ; hasse ,  h. _ phys .",
    "e _ * 2008 * , _ 78 _ , 011603 vrabec ,  j. ; horsch ,  m. ; hasse ,  h. _ j.  heat transfer ( asme ) _ * 2009 * , _ 131 _ , 043202 horsch ,  m. ; lin ,  z. ; windmann ,  t. ; hasse ,  h. ; vrabec ,  j. _ atmospher .",
    "res . _ * 2011 * , _ 101 _ , 519526 grottel ,  s. ; reina ,  g. ; vrabec ,  j. ; ertl ,  t. _ ieee transact .",
    "vis .  comp .",
    "graph . _ * 2007 * , _ 13 _ , 16241631 horsch ,  m. ; miroshnichenko ,  s. ; vrabec ,  j. _ j.  physical studies ( lviv ) _ * 2009 * , _ 13 _ , 4004 horsch ,  m. ; hasse ,  h. ; shchekin ,  a.  k. ; agarwal ,  a. ; eckelsbach ,  s. ; vrabec ,  j. ; mller ,  e.  a. ; jackson ,  g. _ phys .  rev .",
    "e _ * 2012 * , _ 85 _ , 031605 werth ,  s. ; lishchuk ,  s.  v. ; horsch ,  m. ; hasse ,  h. _ physica a _ * 2013 * , _ 392 _ , 23592367 horsch ,  m. ; hasse ,  h. _ chem .  eng .",
    "sci . _ * 2014 * , _ 107 _ , 235244 werth ,  s. ; rutkai ,  g. ; vrabec ,  j. ; horsch ,  m. ; hasse ,  h. _ mol .",
    "phys . _ * 2014 * , in press ( doi : 10.1080/00268976.2013.861086 ) horsch ,  m. ; vrabec ,  j. ; bernreuther ,  m. ; hasse ,  h. in _ proceedings of the 6th international symposium on turbulence , heat and mass transfer _ ; hanjali ,  k. , ed . ;",
    "begell house : new york , 2009 ; pp 8992 horsch ,  m. ; niethammer ,  c. ; vrabec ,  j. ; hasse ,  h. _ informat .",
    "technol . _ * 2013 * , _ 55 _ , 97101 grottel ,  s. ; reina ,  g. ; ertl ,  t. in _ proceedings of the ieee pacific visualization symposium _ ; eades ,  p. , ertl ,  t. , shen ,  h .- w . , eds . ; ieee computer society , 2009 ; pp 6572 grottel ,  s. ; reina ,  g. ; dachsbacher ,  c. ; ertl ,  t. _ comp .  graph .",
    "forum _ * 2010 * , _ 29 _ , 953962 lorentz ,  h.  a. _ ann .",
    "( leipzig ) _ * 1881 * , _ 12 _ , 127136 , 660661 schnabel ,  t. ; vrabec ,  j. ; hasse ,  h. _ j.  mol .",
    "liq . _ * 2007 * , _ 135 _ , 170178 berthelot ,  d. _ compt .",
    "sci . _ * 1898 * , _ 126 _ , 17031706 , 18571858 vrabec ,  j. ; huang ,  y .- l . ; hasse ,  h. _ fluid phase equilib . _ * 2009 * , _ 279 _ , 120135 stoll ,  j. ; vrabec ,  j. ; hasse ,  h. _ aiche j. _ * 2003 * , _ 49 _ , 21872198 vrabec ,  j. ; stoll ,  j. ; hasse ,  h. _ mol .",
    "sim . _ * 2005 * , _ 31 _ , 215221 stone ,  a.  j. _ science _ * 2008 * , _ 321 _ , 787789 gray ,  c.  g. ; gubbins ,  k.  e. _ theory of molecular fluids _ ; oxford university press , 1984 ; vol . 1 : fundamentals eckl ,  b. ; vrabec ,  j. ; hasse ,  h. _ fluid phase equilib . _ * 2008 * , _ 274 _ , 1626 tersoff ,  j. _ phys .  rev .",
    "lett . _ * 1988 * , _ 61 _ , 28792882 tersoff ,  j. _ phys .",
    "b _ * 1989 * , _ 39 _ , 55665568 ghiringhelli ,  l.  m. ; valeriani ,  c. ; los ,  j.  h. ; meijer ,  e.  j. ; fasolino ,  a. ; frenkel ,  d. _ mol",
    ".  phys . _ * 2008 * , _ 106 _ , 20112038 buchholz ,  m. framework zur parallelisierung von molekulardynamiksimulationen in verfahrenstechnischen anwendungen .",
    "dissertation , technische universitt mnchen , 2010 strmer ,  c. _ radium ( paris ) _ * 1912 * , _ 9 _ , 395399 verlet ,  l. _ phys .",
    "rev . _ * 1967 * , _ 159 _ , 98103 hockney ,  r.  w. _ methods comput .",
    "phys . _ * 1970 * , _ 9 _ , 136211 plimpton ,  s. ; hendrickson ,  b. in _ parallel computing in computational chemistry _ ; mattson ,  t.  g. , ed . ;",
    "acs : washington , d.c . , 1995 ;",
    "pp 114132 bentley ,  j.  l. _ comm .",
    "acm _ * 1975 * , _ 18 _ , 509517 simon ,  h.  d. ; teng ,  s .- h .",
    "_ siam j.  sci .",
    "comput . _ * 1995 * , _ 18 _ , 14361445 bernard ,  p .- e . ; gautier ,  t. ; trystram ,  d. _ parallel processing ",
    "proceedings of the xiii .  international conference on parallel and distributed processing ( ipps / spdp ) _ ; ieee : washington , d.c . , 1999 ; pp 638644 fleissner ,  f. ; eberhard ,  p. in _ parallel computing : architectures , algorithms and applications  proceedings of the xii",
    ".  international conference on parallel computing ( parco ) _ ; joubert ,  g. , bischof ,  c. , peters ,  f. , lippert ,  t. , bcker ,  m. , gibbon ,  p. , mohr ,  b. , eds . ; advances in parallel computing 15 ; ios : amsterdam , 2008 ; pp 3744 berger ,  m.  j. ; bokhari ,  s.  h. _ ieee transact .  comput . _",
    "* 1987 * , _ c-36 _ , 570580"
  ],
  "abstract_text": [
    "<S> the molecular dynamics simulation code _ </S>",
    "<S> mardyn_is presented . </S>",
    "<S> it is a highly scalable code , optimized for massively parallel execution on supercomputing architectures , and currently holds the world record for the largest molecular simulation with over four trillion particles . </S>",
    "<S> it enables the application of pair potentials to length and time scales which were previously out of scope for molecular dynamics simulation . with an efficient dynamic load balancing scheme </S>",
    "<S> , it delivers high scalability even for challenging heterogeneous configurations . </S>",
    "<S> presently , multi - center rigid potential models based on lennard - jones sites , point charges and higher - order polarities are supported . due to its modular design , _ </S>",
    "<S> ls1 mardyn_can be extended to new physical models , methods , and algorithms , allowing future users to tailor it to suit their respective needs . </S>",
    "<S> possible applications include scenarios with complex geometries , e.g.  for fluids at interfaces , as well as non - equilibrium molecular dynamics simulation of heat and mass transfer . </S>"
  ]
}