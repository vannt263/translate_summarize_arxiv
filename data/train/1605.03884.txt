{
  "article_text": [
    "bayesian networks ( bns ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) are a class of statistical models composed by a set of random variables @xmath0 and by a directed acyclic graph ( dag ) @xmath1 in which each node in @xmath2 is associated with one of the random variables in @xmath3 ( they are usually referred to interchangeably ) . the arcs in @xmath4",
    "express direct dependence relationships among the variables in @xmath3 ; graphical separation of two nodes implies the conditional independence of the corresponding random variables . in principle",
    ", there are many possible choices for the joint distribution of @xmath3 ; literature has focused mostly on discrete bns @xcite , in which both @xmath3 and the @xmath5 are multinomial random variables and the parameters of interest are the conditional probabilities associated with each variable , usually represented as conditional probability tables .",
    "other possibilities include gaussian bns @xcite and conditional linear gaussian bns @xcite .",
    "the task of learning a bn from data is performed in two steps in an inherently bayesian setting .",
    "consider a data set @xmath6 and a bn @xmath7 .",
    "if we denote the parameters of the joint distribution of @xmath3 with @xmath8 , we can assume without loss of generality that @xmath8 uniquely identifies @xmath3 in the family of distributions chosen to model @xmath6 and write @xmath9    _ structure learning _",
    "consists in finding the dag @xmath10 that encodes the dependence structure of the data .",
    "three general approaches to learn @xmath10 from @xmath6 have been explored in the literature : constraint - based , score - based and hybrid .",
    "constraint - based algorithms use conditional independence tests such as mutual information @xcite to assess the presence or absence of individual arcs in @xmath10 .",
    "score - based algorithms are typically heuristic search algorithms and use a goodness - of - fit score such as bic @xcite or the bayesian dirichlet equivalent uniform ( bdeu ) marginal likelihood @xcite to find an optimal @xmath10 . for the latter a uniform ( u ) prior over the space of dags is assumed for simplicity .",
    "hybrid algorithms combine the previous two approaches , using conditional independence tests to restrict the search space in which to perform a heuristic search for an optimal @xmath10 .",
    "for some examples , see @xcite , @xcite , @xcite and @xcite .",
    "_ parameter learning _ involves the estimation of the parameters @xmath8 given the dag @xmath10 learned in the first step . thanks to the markov property @xcite ,",
    "this step is computationally efficient because if the data are complete the _ global distribution _ of @xmath3 decomposes into @xmath11 and the _ local distribution _ associated with each node @xmath5 depends only on the configurations of the values of its parents @xmath12 .",
    "note that this decomposition does not uniquely identify a bn ; different dags can encode the same global distribution , thus grouping bns into equivalence classes @xcite characterised by the skeleton of @xmath10 ( its underlying undirected graph ) and its v - structures ( patterns of arcs of the type @xmath13 ) .",
    "in the remainder of this paper we will focus on discrete bn structure learning in a bayesian framework . in section [ sec : bdeu ]",
    "we will describe the canonical marginal likelihood used to identify _ maximum a posteriori _ ( map ) dags in score - based algorithms , bdeu , and the uniform prior u over the space of the dags .",
    "we will review and discuss their underlying assumptions and fundamental properties . in section [ sec : bds ]",
    "we will address some of their limitations by introducing a new set of assumptions and the corresponding modified posterior score , which we will call the _ bayesian dirichlet sparse _ ( bds ) marginal likelihood with a _ marginal uniform _ ( mu ) prior .",
    "based on the results of an extensive simulation study , in section [ sec : sim ] we will show that mu+bds is preferable to u+bdeu because it is more accurate in learning @xmath10 from the data ; and because the resulting bns provide better predictive power than those learned using u+bdeu .",
    "starting from ( [ eq : lproc ] ) , we can decompose @xmath14 into @xmath15 where @xmath16 is the prior distribution over the space of the dags and @xmath17 is the marginal likelihood of the data given @xmath10 averaged over all possible parameter sets @xmath8 . using ( [ eq : parents ] ) we can then decompose @xmath17 into one component for each node as follows : @xmath18.\\ ] ] in the case of discrete bns , we assume @xmath19 where the @xmath20 are the conditional probabilities @xmath21 .",
    "we then assume a conjugate prior @xmath22 , @xmath23 to obtain the posterior @xmath24 which we use to estimate the @xmath25 from the counts @xmath26 observed in @xmath6 .",
    "@xmath27 is known as the _ imaginary _ or _ equivalent sample size _ and determines how much weight is assigned to the prior in terms of the size of an imaginary sample supporting it .",
    "further assuming _ positivity _",
    "( @xmath28 ) , _ parameter independence _ ( @xmath25 for different parent configurations are independent ) , _ parameter modularity _",
    "( @xmath25 associated with different nodes are independent ) and _ complete data _ , @xcite derived a closed form expression for ( [ eq : structlearn ] ) , known as the _ bayesian dirichlet _ ( bd ) score : @xmath29\\ ] ] where @xmath30 is the number of states of @xmath5 ; @xmath31 is the number of configurations of @xmath12 ; @xmath32 ; and @xmath33 . for @xmath34",
    "we obtain the k2 score from @xcite ; and for @xmath35 we obtain the _ bayesian dirichlet equivalent uniform _ ( bdeu ) score from @xcite , which is the most common choice used in score - based algorithms to estimate @xmath14 .",
    "it can be shown that bdeu is score equivalent @xcite , that is , it takes the same value for dags that encode the same probability distribution .",
    "the uniform prior over the parameters associated with each @xmath36 was justified by the lack of prior knowledge and widely assumed to be non - informative .",
    "however , there is an increasing amount of evidence that such a set of assumptions leads to a prior that is far from non - informative and that has a strong impact on the quality of the learned dags .",
    "@xcite showed via simulation that the map dags selected using bdeu are highly sensitive to the choice of @xmath37 .",
    "even for `` reasonable '' values such as @xmath38 $ ] , they obtained dags with markedly different number of arcs , and they showed that large values of @xmath37 tend to produce dags with more arcs .",
    "this is counter - intuitive because larger @xmath37 would normally be expected to result in stronger regularisation and sparser bns .",
    "@xcite similarly showed that the number of arcs in the map network is determined by a complex interaction between @xmath37 and @xmath6 ; in the limits @xmath39 and @xmath40 it is possible to obtain both very sparse and very dense dags .",
    "furthermore , they argued that bdeu can be rather unstable for `` medium - sized '' data and small @xmath37 , which is a very common scenario .",
    "@xcite approached the problem from a different perspective and derived an analytic approximation for the `` optimal '' value of @xmath37 that maximises predictive accuracy , further suggesting that the interplay between @xmath37 and @xmath6 is controlled by the skewness of the @xmath41 and by the strength of the dependence relationships between the nodes .",
    "these results have been analytically confirmed more recently by @xcite .",
    "as far as @xmath16 is concerned , the most common choice is the uniform ( u ) distribution @xmath42 ; the space of the dags grows super - exponentially in @xmath43 @xcite and that makes it extremely difficult to specify informative priors @xcite . in our previous work @xcite , we explored the first- and second - order properties of u and we showed that for each possible pair of nodes @xmath44 @xmath45 where @xmath46 , @xmath47 and @xmath48 .",
    "this prior distribution is asymptotically ( marginally ) uniform over both arc presence and direction : each arc is present in @xmath10 with probability @xmath49 and , when present , it appears in each direction with probability @xmath49 .",
    "we also showed that two arcs are correlated if they are incident on a common node and uncorrelated otherwise through exhaustive enumeration of all possible dags for @xmath50 and through simulation for larger @xmath43 .",
    "this suggests that false positives and false negatives can potentially propagate through @xmath16 as well as @xmath17 and lead to further errors in learning @xmath10 .",
    "it is clear from the literature review in section [ sec : bdeu ] that assuming uniform priors for @xmath20 and @xmath10 can have a negative impact on the quality of the dags learned using bdeu .",
    "therefore , we propose an alternative set of assumptions ; we call the resulting score the _ bayesian dirichlet sparse _ ( bds ) marginal likelihood with a _ marginal uniform _ ( mu ) prior .",
    "firstly , we consider the marginal likelihood bdeu . starting from ( [ eq : bd ] ) , we can write it as @xmath51\\ ] ] where @xmath52 .",
    "if the positivity assumption is violated or the sample size @xmath53 is small , there may be configurations of some @xmath12 that are not observed in @xmath6 .",
    "in such cases @xmath54 and @xmath55      \\prod_{j : n_{ij } > 0 }      \\left [        \\frac{\\gamma(r _ i \\alpha_i^*)}{\\gamma(r_i \\alpha_i^ * + n_{ij } ) }        \\prod_{k=1}^{r_i } \\frac{\\gamma(\\alpha_i^ * + n_{ijk})}{\\gamma(\\alpha_i^ * ) }      \\right].\\ ] ] this implies that the effective imaginary sample size decreases as the number of unobserved parents configurations increases , since @xmath56 . in turn",
    ", the posterior estimates of @xmath25 gradually converge to the corresponding maximum likelihood estimates thus favouring overfitting and the inclusion of spurious arcs in @xmath10 .",
    "furthermore , the comparison between dags with very different number of arcs may be inconsistent because the respective effective imaginary sample sizes will be different . @xcite and @xcite observed both these phenomena , indeed linking them to the interplay between @xmath37 and @xmath6 .    to address these two undesirable features of bdeu",
    "we replace @xmath57 in ( [ eq : bdeu ] ) with @xmath58 note that ( [ eq : newprior ] ) is still piece - wise uniform , but now @xmath59 so the effective imaginary sample size is equal to @xmath37 even for sparse data .",
    "intuitively , we are defining a uniform prior just on the conditional distributions we can estimate from @xmath6 , thus moving from a fully bayesian to an empirical bayes score . plugging ( [ eq : newprior ] ) in ( [ eq : bd ] ) we obtain bds : @xmath60\\ ] ] if the positivity assumption holds , we will eventually observe all parents configurations in the data and thus @xmath61 as @xmath62 .",
    "note , however , that bds is not score equivalent for finite @xmath53 unless all @xmath63 .",
    "a numeric example is given below , which also highlights how bds can be computed in the same time as bdeu .",
    "consider two binary variables @xmath64 and @xmath65 with data @xmath6 comprising @xmath66 , @xmath67 , @xmath68 , @xmath69 where @xmath70 .",
    "if @xmath71 , @xmath72 and @xmath73 @xmath74    \\left[\\frac{\\gamma(1)}{\\gamma(1 + 7 ) }          \\frac{\\gamma(\\sfrac{1}{2 } + 2)\\gamma(\\sfrac{1}{2 } + 5)}{\\gamma(\\sfrac{1}{2})\\gamma(\\sfrac{1}{2})}\\right ]    = 0.0009 ,    \\end{gathered}\\ ] ] @xmath75 \\\\",
    "\\left[\\frac{\\gamma(\\sfrac{1}{2})\\gamma(\\sfrac{1}{2})}{\\gamma(\\sfrac{1}{2 } + 2)\\gamma(\\sfrac{1}{2 } + 5 ) }            \\frac{\\gamma(\\sfrac{1}{4 } + 0)\\gamma(\\sfrac{1}{4 } + 0)\\gamma(\\sfrac{1}{4 } + 2)\\gamma(\\sfrac{1}{4 } + 5)}{\\gamma(\\sfrac{1}{4})\\gamma(\\sfrac{1}{4})\\gamma(\\sfrac{1}{4})\\gamma(\\sfrac{1}{4})}\\right ]      = 0.0006 ;    \\end{gathered}\\ ] ] as a term of comparison the empty dag @xmath76 has @xmath77 .",
    "in the general case we have @xmath78 which breaks the score equivalence condition in @xcite because of the uneven imaginary sample size associated with each node ( like the k2 score ) .",
    "we can interpret @xmath79 as an adaptive regularisation hyperparameter that penalises @xmath36 that are not fully observed in @xmath6 , which typically correspond to @xmath5 with a large number of incoming arcs . since @xcite showed that bdeu favours the inclusion of spurious arcs for sparse @xmath36 , this adaptive regularisation should lead to sparser dags and reduce overfitting , in turn improving predictive accuracy as well .",
    "secondly , we propose a modified prior over for @xmath10 with the same aims .",
    "we start from the consideration that score - based structure learning algorithms typically generate new candidate dags by a single arc addition , deletion or reversal .",
    "so , for example @xmath80 when using the u prior we can rewrite ( [ eq : hcstep ] ) as @xmath81 the fact that u always simplifies is equivalent to assigning equal probabilities to all possible states of an arc ( subject to the acyclicity constraint ) , say @xmath82 using the notation in ( [ eq : ba ] ) .",
    "in other words , u favours the inclusion of new arcs in @xmath10 ( subject to the acyclicity constraint ) as @xmath83 . since @xcite also showed that arcs incident on a common node are correlated and may favour each other s inclusion , u",
    "may then contribute to overfitting @xmath10 .",
    "therefore , we introduce the _ marginal uniform _ ( mu ) prior , in which we assume an independent prior for each arc as in @xcite , with probabilities @xmath84 as in @xcite .",
    "these assumptions make mu computationally trivial to use : the ratio of the prior probabilities is @xmath49 for arc addition , @xmath85 for arc deletion and @xmath86 for arc reversal , for all arcs .",
    "furthermore , arc inclusion now has the same prior probability as arc exclusion ( @xmath87 ) and arcs incident on a common are no longer correlated , thus limiting overfitting and preventing the inclusion of spurious arcs to propagate .",
    "however , the marginal distribution for each arc is the same as in ( [ eq : ba ] ) for large @xmath43 , hence the name `` marginal uniform '' .",
    "@xmath88 & & network & @xmath43 & @xmath89 & @xmath88 + alarm & @xmath90 & @xmath91 & @xmath92 & & hailfinder & @xmath93 & @xmath94 & @xmath95 + andes & @xmath96 & @xmath97 & @xmath98 & & hepar 2 & @xmath99 & @xmath100 & @xmath101 + asia & @xmath102 & @xmath102 & @xmath103 & & insurance & @xmath104 & @xmath105 & @xmath106 + child & @xmath107 & @xmath108 & @xmath109 & & pathfinder & @xmath110 & @xmath111 & @xmath112 + diabetes & @xmath113 & @xmath114 & @xmath115 & & pigs & @xmath116 & @xmath117 & @xmath118 +    .average shd distance from @xmath119 ( lower is better , best in bold ) . [ cols= \" < , > , > , > , > , > , > , > , > , > , > , > \" , ]",
    "in this paper we proposed a new posterior score for discrete bn structure learning .",
    "we defined it as the combination of a new prior over the space of dags , the `` marginal uniform '' ( mu ) prior , and of a new empirical bayes marginal likelihood , which we call `` bayesian dirichlet sparse '' ( bds ) .",
    "both have been designed to address the inconsistent behaviour of the classic uniform ( u ) prior and of bdeu explored by @xcite , @xcite and @xcite among others . in particular , our aim was to prevent the inclusion of spurious arcs .    in an extensive simulation study using @xmath120 reference bns we find that mu+bds outperforms u+bdeu for all combinations of bn and sample sizes , both in the quality of the learned dags and in predictive accuracy .",
    "this is achieved without increasing the computational complexity of the posterior score , since mu+bds can be computed in the same time as u+bdeu . in this respect , the posterior score we propose is preferable to similar proposals in the literature .",
    "for instance , the nip - bic score from @xcite and the nip - bde / expected log - bde scores from @xcite outperform bdeu but at a significant computational cost .",
    "the same is true for the optimal @xmath37 proposed by @xcite for bdeu , whose estimation requires multiple runs of the structure learning algorithm to converge .",
    "the max - bde and min - bde scores in @xcite overcome in part the limitations of bdeu by optimising for either goodness of fit at the expense of predictive accuracy , or vice versa .",
    "as a further term of comparison , we also included bic in the simulation ; while it outperforms u+bdeu in some circumstances and it is computationally efficient , mu+bds is better overall in the dags it learns and in predictive accuracy ."
  ],
  "abstract_text": [
    "<S> bayesian network structure learning is often performed in a bayesian setting , by evaluating candidate structures using their posterior probabilities for a given data set . </S>",
    "<S> score - based algorithms then use those posterior probabilities as an objective function and return the _ maximum a posteriori _ network as the learned model . for discrete bayesian networks , </S>",
    "<S> the canonical choice for a posterior score is the bayesian dirichlet equivalent uniform ( bdeu ) marginal likelihood with a uniform ( u ) graph prior @xcite . </S>",
    "<S> its favourable theoretical properties descend from assuming a uniform prior both on the space of the network structures and on the space of the parameters of the network . in this paper </S>",
    "<S> , we revisit the limitations of these assumptions ; and we introduce an alternative set of assumptions and the resulting score : the bayesian dirichlet sparse ( bds ) empirical bayes marginal likelihood with a marginal uniform ( mu ) graph prior . </S>",
    "<S> we evaluate its performance in an extensive simulation study , showing that mu+bds is more accurate than u+bdeu both in learning the structure of the network and in predicting new observations , while not being computationally more complex to estimate .    </S>",
    "<S> bayesian networks , structure learning , graph prior , marginal likelihood , discrete data . </S>"
  ]
}