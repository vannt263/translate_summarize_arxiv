{
  "article_text": [
    "in this paper we study the strengths and limitations of lagrangian relaxation applied to the partial cover problem .",
    "let @xmath3 be collection of subsets of a universal set @xmath4 with cost @xmath5 and profit @xmath6 , and let @xmath7 be a target coverage parameter .",
    "a set @xmath8 is a _ partial cover _ if the overall profit of elements covered by @xmath9 is at least @xmath7 .",
    "the objective is to find a minimum cost partial cover .",
    "the high level idea behind lagrangian relaxation is as follows . in an ip formulation for partial cover , the constraint enforcing that at least @xmath7 profit is covered is _ relaxed _ : the constraint is multiplied by a parameter @xmath10 and lifted to the objective function .",
    "this relaxed ip corresponds , up to a constant factor , to the prize - collecting version of the underlying covering problem in which there is no requirement on how much profit to cover but a penalty of @xmath11 must be paid if we leave element @xmath12 uncovered .",
    "an approximation algorithm for the prize - collecting version having the lagrangian multiplier preserving ( lmp ) property is used to obtain values @xmath13 and @xmath14 that are close together for which the algorithm produces solutions @xmath15 and @xmath16 respectively .",
    "these solutions are such that @xmath15 is inexpensive but unfeasible ( covering less than @xmath7 profit ) , and @xmath16 is feasible ( covering at least @xmath7 profit ) but potentially very expensive . finally , these two solutions are combined to obtain a cover that is both inexpensive and feasible .",
    "broadly speaking there are two ways to combine @xmath15 and @xmath16 .",
    "one option is to treat the approximation algorithm for the prize - collecting version as a black box , only making use of the lmp property in the analysis .",
    "another option is to focus on a particular lmp algorithm and exploit additional structure that it may offer .",
    "not surprisingly , the latter approach has yielded better approximation guarantees .",
    "for example , for @xmath17-median compare the 6-approximation of @xcite to the 4-approximation of @xcite ; for @xmath17-mst compare the 5-factor to the 3-factor approximation due to @xcite .",
    "the results in this paper support the common belief regarding the inherent weakness of the black - box approach .",
    "first , we show a lower bound on the approximation factor achievable for partial cover in general using lagrangian relaxation and the black - box approach that matches the recent upper bound of @xcite . to overcome this obstacle",
    ", we concentrate on kolen s algorithm for prize - collecting totally balanced cover @xcite . by carefully analyzing the algorithm s inner workings we identify structural similarities between @xmath15 and @xmath16 , which we later exploit when combining the two solutions . as a result",
    "we derive an almost tight characterization of the integrality gap of the standard linear relaxation for partial totally balanced cover .",
    "this in turn implies improved approximation algorithms for a number of related problems .",
    "much work has been done on covering problems because of both their simple and elegant formulation , and their pervasiveness in different application areas . in its most general form",
    "the problem , also known as set cover , can not be approximated within @xmath18 unless @xmath19  @xcite . due to this hardness ,",
    "easier , special cases have been studied .    a general class of covering problems that can be solved efficiently",
    "are those whose element - set incidence matrix is balanced .",
    "a @xmath20 matrix is _ balanced _ if it does not contain a square submatrix of odd order with row and column sums equal to 2 .",
    "these matrices were introduced by berge @xcite who showed that if @xmath21 is balanced then the polyhedron @xmath22 is integral .",
    "a @xmath20 matrix is _ totally balanced _",
    "if it does not contain a square submatrix with row and column sums equal to 2 and no identical columns .",
    "kolen @xcite gave a simple primal - dual algorithm that solves optimally the covering problem defined by a totally balanced matrix .",
    "a @xmath23 matrix is _ totally unimodular _ if every square submatrix has determinant 0 or @xmath24 .",
    "although totally balanced and totally unimodular matrices are subclasses of balanced matrices , the two classes are neither disjoint nor one is included in the other . beyond this point",
    ", even minor generalizations can make the covering problem hard .",
    "for example , consider the _",
    "vertex cover _ problem : given a graph @xmath25 we are to choose a minimum size subset of vertices such that every edge is incident on at least one of the chosen vertices . if @xmath26 is bipartite , the element - set incidence matrix for the problem is totally unimodular ; however , if @xmath26 is a general graph the problem becomes np - hard @xcite .",
    "numerous approximation algorithms have been developed for vertex cover  @xcite .",
    "the best known approximation factor for general graphs is @xcite ; yet , after 25 years of study , the best constant factor approximation for vertex cover remains 2 @xcite .",
    "this lack of progress has led researchers to seek generalizations of vertex cover that can still be approximated within twice of optimum .",
    "one such generalization is the _ multicut _ problem on trees : given a tree @xmath27 and a collection of pairs of vertices , a cover is formed by a set of edges whose removal separates all pairs .",
    "the problem was first studied by @xcite who gave an elegant primal - dual 2-approximation .",
    "a notable shortcoming of the standard set cover formulation is that certain hard - to - cover elements , also known as _ outliers _",
    "@xcite , can render the optimal solution very expensive .",
    "motivated by the presence of outliers , the unit - profit partial version calls for a collection of sets covering not all but a specified number @xmath17 of elements .",
    "partial multicut , a.k.a .",
    "@xmath17-multicut , was recently studied independently by @xcite and by @xcite , who gave a @xmath28 approximation algorithm .",
    "this scheme was generalized by @xcite who showed how to design a @xmath29 approximation for any covering problem using lagrangian relaxation and an @xmath0-lmp approximation as a black box .",
    "( their algorithm runs in time polynomial on @xmath30 and the running time of the @xmath0-lmp approximation . )",
    "section  [ section : lowerbound ] shows that for partial cover in general no algorithm that uses lagrangian relaxation and an @xmath0-lmp approximation as a black box can yield an approximation factor better than @xmath1 . in section  [ section : p - tbc ]",
    "we give an almost tight characterization of the integrality gap of the standard lp for partial totally balanced cover , settling a question posed by @xcite .",
    "our approach is based on lagrangian relaxation and kolen s algorithm .",
    "we prove that @xmath31 for any @xmath32 , where @xmath33 and @xmath34 are the costs of the optimal integral and fractional solutions respectively and @xmath35 is the cost of the most expensive set in the instance . the trade - off between additive and multiplicative error is not an artifact of our analysis or a shortcoming of our approach . on the contrary",
    ", this is precisely how the integrality gap behaves .",
    "more specifically , we show a family of instances where @xmath36 . in other words",
    ", there is an unbounded additive gap in terms of @xmath35 but as it grows the multiplicative gap narrows exponentially fast .",
    "finally , we show how the above result can be applied , borrowing ideas , to get a @xmath37 approximation or a quasi - polynomial time @xmath2-approximation for covering problems that can be expressed with a suitable combination of @xmath2 totally - balanced matrices .",
    "this translates into improved approximations for a number of problems : a @xmath38 approximation for the partial multicut on trees @xcite , a @xmath39 approximation for partial path hitting on trees @xcite , a 2-approximation for partial rectangle stabbing @xcite , and a @xmath2 approximation for partial set - cover with @xmath2-blocks @xcite .",
    "in addition , the @xmath40 can be removed from the first two approximation guarantees if we allow quasi - polynomial time .",
    "it is worth noting that prior to this work , the best approximation ratio for all these problems could be achieved with the framework of @xcite . in each case",
    "our results improve the approximation ratio by a @xmath41 multiplicative factor .",
    "let @xmath42 be a collection of subsets of a universal set @xmath43 .",
    "each set has a cost specified by @xmath44 , and each element has a profit specified by @xmath45 .",
    "given a target coverage @xmath7 , the objective of the partial cover problem is to find a minimum cost solution @xmath8 such that @xmath46 , where the notation @xmath47 denotes the overall profit of elements covered by @xmath9 .",
    "the problem is captured by the ip below .",
    "matrix @xmath48 is an element - set incidence matrix , that is , @xmath49 if and only if element @xmath12 belongs to set @xmath50 ; variable @xmath51 indicates whether set @xmath52 is chosen in the solution @xmath9 ; variable @xmath53 indicates whether element @xmath54 is left uncovered .",
    "lagrangian relaxation is used to get rid of the constraint bounding the profit of uncovered elements to be at most @xmath55 .",
    "the constraint is multiplied by the parameter @xmath10 , called lagrange multiplier , and is lifted to the objective function .",
    "the resulting ip corresponds , up to the constant @xmath56 factor in the objective function , to the prize - collecting version of the covering problem , where the penalty for leaving element @xmath54 uncovered is @xmath57 .",
    "@xmath58 { { a } { x } } + { { i } { r } } \\geq & { 1 } \\hspace{0.5 cm }   \\\\[0.2 cm ] { { p } \\cdot { r } } \\leq & p(u)-p \\\\[0.2 cm ] r_i , x_j \\in & \\ { 0 , 1\\ } \\\\[0.4 cm ] \\end{array}\\end{gathered}\\ ] ]    ( 0,1.75)(2.5,1.75 ) ( 2.5,0 ) ( 1.25,-.4 )    @xmath59 \\hspace{4em}{{a } { x } } + { { i } { r } } \\geq & { 1 } \\\\[0.2 cm ] r_i , x_j \\in & \\{0,1\\ } \\end{array}\\end{gathered}\\ ] ]    let @xmath60 be the cost of an optimal partial cover and @xmath61 be the cost of an optimal prize - collecting cover for a given @xmath10 .",
    "let @xmath62 be an @xmath0-approximation for the prize - collecting variant of the problem .",
    "algorithm @xmath62 is said to have the lagrangian multiplier preserving ( lmp ) property if it produces a solution @xmath9 such that @xmath63    note that @xmath64 .",
    "thus , @xmath65    therefore , if we could find a value of @xmath10 such that @xmath9 covers exactly @xmath7 profit then @xmath9 is @xmath0-approximate .",
    "however , if @xmath66 , the solution is not feasible , and if @xmath67 , equation does not offer any guarantee on the cost of @xmath9 . unfortunately , there are cases where no value of @xmath10 produces a solution covering exactly @xmath7 profit .",
    "thus , the idea is to use binary search to find two values @xmath13 and @xmath14 that are close together and are such that @xmath68 covers less , and @xmath69 covers more than @xmath7 profit .",
    "the two solutions are then combined in some fashion to produce a feasible cover .",
    "a common way to combine the two solutions returned by the @xmath0-lmp is to treat the algorithm as a black box , solely relaying on the lmp property   in the analysis .",
    "more formally , an algorithm for partial cover that uses lagrangian relaxation and an @xmath0-lmp approximation @xmath62 as a black box is as follows .",
    "first , we are allowed to run @xmath62 with as many different values of @xmath10 as desired ; then , the solutions thus found are combined to produce a feasible partial cover .",
    "no computational restriction is placed on the second step , except that only sets returned by @xmath62 may be used .",
    "[ theorem : lowerbound ] in general , the partial cover problem can not be approximated better than @xmath1 using lagrangian relaxation and an @xmath0-lmp algorithm @xmath62 as a black box .",
    "# 1 ( -1,0)(14,2 ) ( 2,1 ) ( 5,1 ) ( 11,1 ) ( 13,1 ) ( 0,1 ) ( -2.5,1)@xmath70    # 1 ( 0,0)(2,13 ) ( 1,14.5)@xmath71    5 cm    ( -3,-3)(14,15 ) ( 0,9 ) ( 0,6 ) ( 0,0 ) ( 1,-1 ) ( 4,-1 ) ( 10,-1 ) ( 8,13.5)@xmath72 ( -2.6,4)@xmath72    let @xmath73 and @xmath74 be sets as depicted on the right . for each @xmath54 and @xmath52 the intersection @xmath75 consists of a cluster of @xmath76 elements .",
    "there are @xmath77 clusters .",
    "set @xmath78 is made up of @xmath76 clusters ; set @xmath79 is made up of @xmath76 clusters and two additional elements ( the leftmost and rightmost elements in the picture . )",
    "thus @xmath80 and @xmath81 .",
    "in addition , there are sets @xmath82 , which are not shown in the picture",
    ". set @xmath83 contains one element from each cluster and the leftmost element of @xmath79 .",
    "thus @xmath84 .",
    "the cost of @xmath83 is @xmath85 , the cost of @xmath78 is @xmath86 , and the cost of @xmath79 is @xmath87 .",
    "every element has unit profit and the target coverage is @xmath88 .",
    "it is not hard to see that @xmath82 is an optimal partial cover with a cost of 1 .",
    "the @xmath0-lmp approximation algorithm we use has the unfortunate property that it never returns sets from the optimal solution .",
    "[ lemma : naughty - lmp ] there exists an @xmath0-lmp approximation @xmath62 that for the above instance and any value of @xmath10 outputs either @xmath89 or @xmath90 or @xmath91 .",
    "hence , if we use @xmath62 as a black box we must build a partial cover with the sets @xmath90 and @xmath91 .",
    "note that in order to cover @xmath92 elements either all @xmath21-sets , or all @xmath93-sets must be used . in the first case @xmath94",
    "additional @xmath93-sets are needed to attain feasibility , and the solution has cost @xmath95 ; in the second case the solution is feasible but again has cost @xmath95 .",
    "theorem  [ theorem : lowerbound ] follows .",
    "one assumption usually made in the literature @xcite is that @xmath96 , for some constant @xmath97 , or more generally an additive error in terms of @xmath35 is allowed .",
    "this does not help in our construction as @xmath35 can be made arbitrarily small by increasing  @xmath76 .",
    "admittedly , our lower bound example belongs to a specific class of covering problem ( every element belongs to at most three sets ) and although the example can be embedded into a partial totally unimodular covering problem , it is not clear how to embed the example into other classes .",
    "nevertheless , the @xmath95 upper bound of koneman et el .",
    "@xcite makes no assumption about the underlying problem , only using the lmp property in the analysis .",
    "it was entirely conceivable that the @xmath1 factor could be improved using a different merging strategy ",
    "theorem  [ theorem : lowerbound ] precludes this possibility .",
    "in order to overcome the lower bound of theorem  [ theorem : lowerbound ] , one must concentrate on a specific class of covering problems or make additional assumptions about the @xmath0-lmp algorithm . in this section",
    "we focus on covering problems whose ip matrix @xmath21 is totally balanced .",
    "more specifically , we study the integrality gap of the standard linear relaxation for partial totally balanced cover ( p - tbc ) shown below .    [",
    "theorem : lp - gap ] let @xmath33 and @xmath34 be the cost of the optimal integral and fractional solutions of an instance of p - tbc .",
    "then @xmath98 for any @xmath99 .",
    "furthermore , for any large enough @xmath99 the exists an instance where @xmath100 .",
    "@xmath101 \\hspace{2ex } { { a } { x } } + { { i } { r } } \\geq & { 1 } \\\\[0.2 cm ] { { p } \\cdot { r } } \\leq & p(u ) - p   \\\\[0.2 cm ] r_i , x_e \\geq 0 \\end{array}\\end{gathered}\\ ] ]    ( 0,1.75)(2.5,1.75 ) ( 2.5,0 ) ( 1.25,-.4)lp duality    @xmath102 \\hspace{5ex}{{a^t } { y } }   & \\leq { c } \\hspace{0.5 cm } \\\\[0.2 cm ] { y } & \\leq   \\lambda { p } \\\\[0.2 cm ] y_i , \\lambda & \\geq 0 \\\\[0.4 cm ] \\end{array}\\end{gathered}\\ ] ]    our approach is based on lagrangian relaxation and kolen s algorithm for prize - collecting totally balanced cover ( pc - tbc ) .",
    "the latter exploits the fact that a totally balanced matrix can be put into greedy standard form by permuting the order of its rows and columns ; in fact , the converse is also true @xcite .",
    "a matrix is in standard greedy form if it does not contain as an induced submatrix @xmath103 \\label{eq : forbidden - matrix}\\ ] ] there are polynomial time algorithms that can transform a totally balanced matrix into greedy standard form @xcite by shuffling the rows and columns of @xmath21 .",
    "since this transformation does not affect the underlying covering problem , we assume that @xmath21 is given in standard greedy form .      for the sake of completeness",
    "we describe kolen s primal - dual algorithm for pc - tbc .",
    "the algorithm finds a dual solution @xmath104 and a primal solution @xmath105 , which is then pruned in a reverse - delete step to obtain the final solution @xmath106 .",
    "the linear and dual relaxations for pc - tbc appear below .",
    "@xmath107 \\hspace{2ex } { { a } { x } } + { { i } { r } } \\geq & { 1 } \\\\[0.2 cm ] r_i , x_e \\geq 0 \\end{array}\\end{gathered}\\ ] ]    ( 0,1.75)(2.5,1.75 ) ( 2.5,0 ) ( 1.25,-.4)lp duality    @xmath108 \\hspace{3ex}{{a^t } { y } }   & \\leq { c } \\hspace{0.5 cm } \\\\[0.2 cm ] { y } & \\leq   \\lambda { p } \\\\[0.2 cm ] y_i & \\geq 0 \\\\[0.4 cm ] \\end{array}\\end{gathered}\\ ] ]    the residual cost of the set @xmath52 w.r.t .",
    "@xmath104 is defined as @xmath109 .",
    "the algorithm starts from the trivial dual solution @xmath110 , and processes the elements in increasing column order of @xmath111 .",
    "let @xmath54 the index of the current element .",
    "its corresponding dual variable , @xmath112 , is increased until either the residual cost of some set @xmath52 containing @xmath54 equals 0 ( we say set @xmath52 becomes tight ) , or @xmath112 equals @xmath57 ( lines 3 - 5 ) .",
    "let @xmath113 be the set of tight sets after the dual update is completed .",
    "as it stands the cover @xmath105 may be too expensive to be accounted for using the lower bound provided by @xmath114 because a single element may belong to multiple sets in @xmath105 .",
    "the key insight is that some of the sets in @xmath105 are redundant and can be pruned .",
    "given sets @xmath115 we say that @xmath116 _ dominates _ @xmath117 in @xmath104 if @xmath118 and there exists an item @xmath54 such that @xmath119 and @xmath54 belongs to @xmath116 and @xmath117 , that is , @xmath120 .",
    "the reverse - delete step iteratively identifies the largest index @xmath52 in @xmath105 , adds @xmath52 to @xmath106 , and removes @xmath52 and all the sets it dominates .",
    "this is repeated until no set is left in @xmath105 ( lines 811 ) .",
    "notice that all sets @xmath121 are tight , thus we can pay for set @xmath52 by charging the dual variables of items that belong to @xmath52 . because of the reverse - delete step if @xmath119 then @xmath54 belongs to at most one set in @xmath106 ; thus in paying for @xmath106 we charge covered items at most once . using the fact @xmath21 is in standard greedy form ,",
    "it can be shown @xcite that if @xmath54 was left uncovered then we can afford its penalty , i.e. , @xmath122 .",
    "the solution @xmath106 is optimal for pc - tbc since @xmath123    if we could find a value of @xmath10 such that kolen@xmath124 returns a solution @xmath125 covering _ exactly _",
    "@xmath7 profit , we are done since from it follows that @xmath126 notice that @xmath127 is a feasible for the dual relaxation of p - tbc and its cost is precisely the right hand side of .",
    "therefore for this instance ip = dl = lp and theorem  [ theorem : lp - gap ] follows .",
    "unfortunately , there are cases where no such value of @xmath10 exists .",
    "nonetheless , we can always find a _ threshold value _",
    "@xmath10 such that for any infinitesimally small @xmath128 , @xmath129 and @xmath130 produce solutions covering less and more than @xmath7 profit respectively .",
    "a threshold value can be found using megiddo s parametric search @xcite by making @xmath131 calls to the procedure kolen .",
    "let @xmath104 ( @xmath132 ) be the dual solution and @xmath105 ( @xmath133 ) the set of tight sets when kolen is run on @xmath10 ( @xmath134 ) .",
    "without loss of generality assume @xmath106 covers more than @xmath7 profit .",
    "( the case where @xmath106 covers less than @xmath7 profit is symmetrical : we work with @xmath135 and @xmath136 instead of @xmath137 and @xmath133 . )",
    "our plan to prove theorem  [ theorem : lp - gap ] is to devise an algorithm to merge @xmath106 and @xmath138 in order to obtain a cheap solution covering at least @xmath7 profit .      before describing the algorithm we need to establish some important properties regarding these two solutions and their corresponding dual solutions .    for any @xmath54 ,",
    "the value of @xmath139 is a linear function of @xmath140 for all @xmath54 .",
    "this follows from the fact that @xmath140 is infinitesimally small .",
    "furthermore , the constant term in this linear function is @xmath112 .",
    "[ lemma : tiny - difference ] for each @xmath12 there exists @xmath141 , independent of @xmath140 , such that @xmath142 .    by induction on the number of iteration of the dual update step of kolen , using the fact that the same property holds for the residual cost of the sets .",
    "a useful corollary of lemma  [ lemma : tiny - difference ] is that @xmath143 , since if the residual cost of a set is non - zero in @xmath104 it must necessarily be non - zero in @xmath132 .",
    "the other way around may not hold .    at the heart of our approach",
    "is the notion of a merger graph @xmath144 .",
    "the vertex set of @xmath26 is made up of sets from the two solutions , i.e. , @xmath145 .",
    "the edges of @xmath26 are directed and given by @xmath146    this graph has a lot of structure that can be exploited when merging the solutions .    the merger graph @xmath25 of @xmath138 and @xmath106 is a forest of out - branchings .",
    "first note that @xmath26 is acyclic , since if @xmath147 then necessarily @xmath118 .",
    "thus , it is enough to show that the in - degree of every @xmath148 is at most one .",
    "suppose otherwise , that is , there exist @xmath149 such that @xmath150 .",
    "assume that @xmath151 and @xmath152 ( the remaining cases are symmetrical ) .",
    "4.5 cm    @xmath153    by definition , we know that @xmath154 and that there exists @xmath155 ( @xmath156 ) that belongs to @xmath52 and @xmath116 ( @xmath117 ) such that @xmath157 ( @xmath158 ) . since @xmath111 is in standard greedy form we can infer that @xmath156 belongs to @xmath116 if @xmath159 , or @xmath155 belongs to @xmath117 if @xmath160 : the diagram on the right shows how , using the fact that @xmath111 does not contain   as an induced submatrix , we can infer that the boxed entries must be 1 .",
    "in either case we get that @xmath117 dominates @xmath116 in @xmath137 , which contradicts the fact that both belong to @xmath138 .",
    "the procedure merge starts from the unfeasible solution @xmath161 and guided by the merger graph  @xmath26 , it modifies @xmath162 step by step until feasibility is attained .",
    "the operation used to update @xmath162 is to take the symmetric difference of @xmath162 and a subtree of @xmath26 rooted at a vertex @xmath163 , which we denote by @xmath164 . for each root @xmath165 of an out - branchings of @xmath26 we set @xmath166 , until @xmath167 .",
    "at this point we return the solution produced by increase@xmath168 .",
    "notice that after setting @xmath166 in line 5 , the solution @xmath162 `` looks like '' @xmath106 within @xmath164 . indeed , if all roots are processed then @xmath169 .",
    "therefore , at some point we are bound to have @xmath167 and to make the call increase@xmath168 in line 6 . before describing increase",
    "we need to define a few terms .",
    "absolute benefit _ of set @xmath52 , which we denote by @xmath170 , be the profit of elements uniquely covered by set @xmath52 , that is , @xmath171 let @xmath172 .",
    "note that if @xmath173 , the removal of @xmath52 decreases the profit covered by @xmath162 by at least @xmath170 ; on the other hand , if @xmath174 , its addition increases the profit covered by at least @xmath170 .",
    "this notion of benefit can be extended to subtrees , @xmath175 we call this quantity the _ relative benefit _ of @xmath176 with respect to @xmath162 .",
    "it shows how the profit of uniquely covered elements changes when we take @xmath177 .",
    "note that @xmath178 can positive or negative .",
    "everything is in place to explain increase@xmath179 .",
    "the algorithm assumes the input solution is unfeasible but can be made feasible by adding some sets in @xmath176 ; more precisely , we assume @xmath180 and @xmath181 .",
    "if adding @xmath52 to @xmath162 makes the solution feasible then return @xmath182 . if there exists a child @xmath183 of @xmath52 that can be used to propagate the call down the tree then do that",
    "otherwise , _ split _ the subtree @xmath176 : add @xmath52 to @xmath162 and process the children of @xmath183 , setting @xmath184 until @xmath162 becomes feasible ( lines 6 - 9 ) . at this point @xmath185 and @xmath186 . if @xmath187 then call increase@xmath188 else call decrease@xmath189 and let @xmath190 be the cover returned by the recursive call ( lines 10 - 12 ) . finally , return the cover with minimum cost between @xmath162 and @xmath190 .",
    "the twin procedure decrease@xmath179 is essentially symmetrical : initially the input is feasible but can be made unfeasible by removing some sets in @xmath176 ; more precisely @xmath191 and @xmath192 .    at a very high level ,",
    "the intuition behind the increase / decrease scheme is as follows . in each call one of three things",
    "must occur :    a feasible cover with a small coverage excess is found ( lines 2 - 3 ) , or    the call is propagated down the tree at no cost ( lines 4 - 5 ) , or    a subtree @xmath193 is split ( lines 6 - 9 ) . in this case , the cost @xmath194 can not be accounted for , but the offset in coverage @xmath195 is reduced at least by a factor of 3 .",
    "if the increase / decrease algorithms split many subtrees ( incurring a high extra cost ) then the offset in coverage must have been very high at the beginning , which means the cost of the dual solution is high and so the splitting cost can be charged to it . in order to flesh out these ideas into a formal proof we need to establish some crucial properties of the merger graph and the algorithms .",
    "[ lemma : white - coverage ] if @xmath196 then there exist @xmath197 and @xmath198 such that either @xmath199 or @xmath200 or @xmath201 .    [ lemma : alternating ] let @xmath179 be the input of increase / decrease .",
    "then at the beginning of each call we have @xmath202 or @xmath203 for all @xmath204 . furthermore , if @xmath202 and @xmath203 then @xmath205 or @xmath206 must have been split in a previous call .    [ lemma : precondition ] let @xmath179 be the input of increase / decrease .",
    "for increase we always have @xmath207 , and for decrease we have @xmath208 .    recall that @xmath104 is also a feasible solution for the dual relaxation of p - tbc and its cost",
    "is given by @xmath209 .",
    "the following lemma proves the upper bound of theorem  [ theorem : lp - gap ] .",
    "[ lemma : merge ] suppose merge outputs @xmath162 .",
    "then @xmath210 for all @xmath99 .",
    "let us digress for a moment for the sake of exposition .",
    "suppose that in line 6 of merge , instead of calling increase , we return @xmath211 .",
    "notice every arc in the merger graph has exactly one endpoint in @xmath190 . by lemma  [ lemma : white - coverage ]",
    ", any element @xmath54 not covered by @xmath190 must have @xmath212 .",
    "furthermore , if @xmath213 then there exists at most one set in @xmath190 that covers @xmath54 ; if two such sets exist , one must dominate the other in @xmath104 and @xmath137 , which is not possible . hence , @xmath214 in the fortunate case that @xmath215 , the lemma would follow .",
    "of course , this need not happen and this is why we make the call to increase instead of returning @xmath190 .",
    "let @xmath216 be the root of the @xmath217 subtree split by increase / decrease .",
    "also let @xmath218 the solution right before splitting @xmath219 , and @xmath220 and @xmath221 be the unfeasible / feasible pair of solutions after the splitting , which are used as parameters in the recursive calls ( lines 11 - 12 ) .",
    "suppose lines 7 - 9 processed only one child of @xmath216 , this can only happen in increase , in which case @xmath222 but @xmath223 .",
    "the same argument used to derive gives us @xmath224 the cost of the missing sets is @xmath225 , thus if @xmath226 the lemma follows . a similar bound",
    "can be derived if the recursive call ends in line 3 before splitting the @xmath227 subtree .",
    "finally , the last case to consider is when lines 7 - 9 process two or more children @xmath216 for all @xmath228 . in this case",
    "@xmath229 which implies @xmath230 .",
    "also , @xmath231 since all elements @xmath54 not covered by @xmath232 must be such that @xmath122 .",
    "hence , as before @xmath233 adding the cost of @xmath234 we get the lemma .",
    "the results in this paper suggest that lagrangian relaxation is a powerful technique for designing approximation algorithms for partial covering problems , even though the black - box approach may not be able to fully realize its potential",
    ". it would be interesting to extend this study on the strengths and limitation of lagrangian relaxation to other problems .",
    "the obvious candidate is the @xmath17-median problem . @xcite",
    "designed a @xmath235-approximation for @xmath17-median using as a black box an @xmath0-lmp approximation for facility location .",
    "later , @xcite gave a 2-lmp approximation for facility location .",
    "is the algorithm in @xcite optimal in the sense of theorem  [ theorem : lowerbound ] ? can the algorithm in @xcite be turned into a 2-approximation for @xmath17-median by exploiting structural similarities when combining the two solutions ?",
    "* acknowledgments : * i am indebted to danny segev for sharing an early draft of @xcite and for pointing out kolen s work .",
    "also thanks to mohit singh and arie tamir for helpful discussions and to elena zotenko for suggesting deriving the result of section  [ section : lowerbound ] ."
  ],
  "abstract_text": [
    "<S> lagrangian relaxation has been used extensively in the design of approximation algorithms . </S>",
    "<S> this paper studies its strengths and limitations when applied to partial cover .    </S>",
    "<S> we show that for partial cover in general no algorithm that uses lagrangian relaxation and a lagrangian multiplier preserving ( lmp ) @xmath0-approximation as a black box can yield an approximation factor better than  @xmath1 . </S>",
    "<S> this matches the upper bound given by knemann _ </S>",
    "<S> et al . _ </S>",
    "<S> ( _ esa 2006 _ , pages 468479 ) .    faced with this limitation </S>",
    "<S> we study a specific , yet broad class of covering problems : partial totally balanced cover . by carefully analyzing the inner workings of the lmp algorithm we are able to give an almost tight characterization of the integrality gap of the standard linear relaxation of the problem . as a consequence </S>",
    "<S> we obtain improved approximations for the partial version of multicut and path hitting on trees , rectangle stabbing , and set cover with @xmath2-blocks .    </S>",
    "<S> julin mestre </S>"
  ]
}