{
  "article_text": [
    "in many applications one fits a parametrized curve described by an implicit equation @xmath0 to experimental data @xmath1 , @xmath2 . here",
    "@xmath3 denotes the vector of unknown parameters to be estimated .",
    "typically , @xmath4 is a polynomial in @xmath5 and @xmath6 , and its coefficients are unknown parameters ( or functions of unknown parameters ) .",
    "for example , a number of recent publications @xcite are devoted to the problem of fitting quadrics @xmath7 , in which case @xmath8 is the parameter vector .",
    "the problem of fitting circles , given by equation @xmath9 with three parameters @xmath10 , also attracted attention @xcite .",
    "we consider here the problem of fitting general curves given by implicit equations @xmath0 with @xmath11 being the parameter vector .",
    "our goal is to investigate statistical properties of various fitting algorithms .",
    "we are interested in their biasedness , covariance matrices , and the cramer - rao lower bound .",
    "first , we specify our model .",
    "we denote by @xmath12 the true value of @xmath3 .",
    "let @xmath13 , @xmath2 , be some points lying on the true curve @xmath14 .",
    "experimentally observed data points @xmath15 , @xmath2 , are perceived as random perturbations of the true points @xmath13 .",
    "we use notation @xmath16 and @xmath17 , for brevity .",
    "the random vectors @xmath18 are assumed to be independent and have zero mean .",
    "two specific assumptions on their probability distribution can be made , see @xcite :    * _ cartesian model _ :",
    "each @xmath19 is a two - dimensional normal vector with covariance matrix @xmath20 , where @xmath21 is the identity matrix . * _ radial model _ :",
    "@xmath22 where @xmath23 is a normal random variable @xmath24 , and @xmath25 is a unit normal vector to the curve @xmath14 at the point @xmath26 .",
    "our analysis covers both models , cartesian and radial . for simplicity , we assume that @xmath27 for all @xmath28 , but note that our results can be easily generalized to arbitrary @xmath29",
    ".    concerning the true points @xmath30 , @xmath2 , two assumptions are possible .",
    "many researchers @xcite consider them as fixed , but unknown , points on the true curve . in this case",
    "their coordinates @xmath13 can be treated as additional parameters of the model ( nuisance parameters ) .",
    "chan @xcite and others @xcite call this assumption a _",
    "functional model_. alternatively , one can assume that the true points @xmath30 are sampled from the curve @xmath31 according to some probability distribution on it .",
    "this assumption is referred to as a _ structural model _ @xcite .",
    "we only consider the functional model here .",
    "it is easy to verify that maximum likelihood estimation of the parameter @xmath3 for the functional model is given by the orthogonal least squares fit ( olsf ) , which is based on minimization of the function _ 1 ( ) = _ i=1^n [ d_i()]^2 [ fmain1 ] where @xmath32 denotes the distance from the point @xmath26 to the curve @xmath0 .",
    "the olsf is the method of choice in practice , especially when one fits simple curves such as lines and circles .",
    "however , for more general curves the olsf becomes intractable , because the precise distance @xmath33 is hard to compute .",
    "for example , when @xmath4 is a generic quadric ( ellipse or hyperbola ) , the computation of @xmath33 is equivalent to solving a polynomial equation of degree four , and its direct solution is known to be numerically unstable , see @xcite for more detail .",
    "then one resorts to various approximations .",
    "it is often convenient to minimize _ 2 ( ) = _ i=1^n [ p(x_i , y_i;)]^2 [ fmain2 ] instead of ( [ fmain1 ] ) .",
    "this method is referred to as a ( simple ) _ algebraic fit _ ( af ) , in this case one calls @xmath34 the _ algebraic distance _",
    "@xcite from the point @xmath1 to the curve .",
    "the af is computationally cheaper than the olsf , but its accuracy is often unacceptable , see below .",
    "the simple af ( [ fmain2 ] ) can be generalized to a _ weighted algebraic fit _ , which is based on minimization of _ 3 ( ) = _",
    "i=1^n w_i [ p(x_i , y_i;)]^2 [ fmain3 ] where @xmath35 are some weights , which may balance ( [ fmain2 ] ) and improve its performance .",
    "one way to define weights @xmath36 results from a linear approximation to @xmath33 : @xmath37 where @xmath38 is the gradient vector , see @xcite .",
    "then one minimizes the function _ 4 ( ) = _ i=1^n [ fmain4 ] this method is called the _ gradient weighted algebraic fit _ ( graf ) .",
    "it is a particular case of ( [ fmain3 ] ) with @xmath39 .",
    "the graf is known since at least 1974 @xcite and recently became standard for polynomial curve fitting @xcite .",
    "the computational cost of graf depends on the function @xmath40 , but , generally , the graf is much faster than the olsf .",
    "it is also known from practice that the accuracy of graf is almost as good as that of the olsf , and our analysis below confirms this fact . the graf is often claimed to be a _ statistically optimal _ weighted algebraic fit , and we will prove this fact as well .    not much has been published on statistical properties of the olsf and algebraic fits , apart from the simplest case of fitting lines and hyperplanes @xcite .",
    "chan @xcite , berman and culpin @xcite investigated circle fitting by the olsf and the simple algebraic fit ( [ fmain2 ] ) assuming the structural model .",
    "kanatani @xcite used the cartesian functional model and considered a general curve fitting problem .",
    "he established an analogue of the rao - cramer lower bound for unbiased estimates of @xmath3 , which we call here kanatani - cramer - rao ( kcr ) lower bound .",
    "he also showed that the covariance matrices of the olsf and the graf attain , to the leading order in @xmath41 , his lower bound .",
    "we note , however , that in most cases the olsf and algebraic fits are _ biased _",
    "@xcite , hence the kcr lower bound , as it is derived in @xcite , does not immediately apply to these methods .    in this paper",
    "we extend the kcr lower bound to biased estimates , which include the olsf and all weighted algebraic fits .",
    "we prove the kcr bound for estimates satisfying the following mild assumption :    * precision assumption*. for precise observations ( when @xmath42 for all @xmath43 ) , the estimate @xmath44 is precise , i.e. ( |*x*_1 ,  , |*x*_n ) = | [ tass ] it is easy to check that the olsf and algebraic fits ( [ fmain3 ] ) satisfy this assumption .",
    "we will also show that all unbiased estimates of @xmath44 satisfy ( [ tass ] ) .",
    "we then prove that the graf is , indeed , a statistically efficient fit , in the sense that its covariance matrix attains , to the leading order in @xmath41 , the kcr lower bound . on the other hand ,",
    "rather surprisingly , we find that graf is not the only statistically efficient algebraic fit , and we describe all statistically efficient algebraic fits .",
    "finally , we show that kanatani s theory and our extension to it remain valid for the radial functional model .",
    "our conclusions are illustrated by numerical experiments on circle fitting algorithms .",
    "recall that we have adopted the functional model , in which the true points @xmath30 , @xmath43 , are fixed .",
    "this automatically makes the sample size @xmath45 fixed , hence , many classical concepts of statistics , such as consistency and asymptotic efficiency ( which require taking the limit @xmath46 ) lose their meaning .",
    "it is customary , in the studies of the functional model of the curve fitting problem , to take the limit @xmath47 instead of @xmath46 , cf.@xcite .",
    "this is , by the way , not unreasonable from the practical point of view : in many experiments , @xmath45 is rather small and can not be ( easily ) increased , so the limit @xmath48 is of little interest . on the other hand , when the accuracy of experimental observations is high ( thus , @xmath41 is small ) , the limit @xmath49 is quite appropriate .",
    "now , let @xmath50 be an arbitrary estimate of @xmath3 satisfying the precision assumption ( [ tass ] ) . in our analysis we will always assume that all the underlying functions are regular ( continuous , have finite derivatives , etc . ) , which is a standard assumption @xcite .",
    "the mean value of the estimate @xmath44 is e ( ) = ( * x*_1,  ,*x*_n ) _",
    "i=1^n f(*x*_i ) d*x*_1d*x*_n [ et ] where @xmath51 is the probability density function for the random point @xmath26 , as specified by a particular model ( cartesian or radial ) .",
    "we now expand the estimate @xmath52 into a taylor series about the true point @xmath53 remembering ( [ tass ] ) : ( * x*_1 ,  , * x*_n ) = | + _ i=1^n",
    "_ i ( * x*_i - |*x*_i ) + o(^2 ) [ texpand ] where _",
    "i = _ * x*_i ( |*x*_1 ,  ,    for the gradient with respect to the variables @xmath54 .",
    "in other words , @xmath55 is a @xmath56 matrix of partial derivatives of the @xmath57 components of the function @xmath44 with respect to the two variables @xmath58 and @xmath59 , and this derivative is taken at the point @xmath60 ,    substituting the expansion ( [ texpand ] ) into ( [ et ] ) gives e ( ) = | + o(^2 ) [ tbias ] since @xmath61 .",
    "hence , the bias of the estimate @xmath44 is of order @xmath62 .",
    "it easily follows from the expansion ( [ texpand ] ) that the covariance matrix of the estimate @xmath44 is given by @xmath63    \\theta_i^t + { \\cal o}(\\sigma^4)\\ ] ] ( it is not hard to see that the cubical terms @xmath64 vanish because the normal random variables with zero mean also have zero third moment , see also @xcite ) .",
    "now , for the cartesian model @xmath65       = \\sigma^2 i\\ ] ] and for the radial model @xmath65       = \\sigma^2 { \\bf n}_i { \\bf n}_i^t\\ ] ] where @xmath25 is a unit normal vector to the curve @xmath14 at the point @xmath30",
    ". then we obtain _",
    "= ^2 _ i=1^n _ i _ i _ i^t + o(^4 ) [ csig0 ] where @xmath66 for the cartesian model and @xmath67 for the radial model . +",
    "* lemma*. _ we have @xmath68 for each @xmath2 .",
    "hence , for both models , cartesian and radial , the matrix @xmath69 is given by the same expression : _",
    "_ = ^2 _ i=1^n _ i _ i^t + o(^4 ) [ csig ]    this lemma is proved in appendix",
    ".    our next goal is now to find a lower bound for the matrix _",
    "1:= _ i=1^n _ i_i^t [ calc1 ] following @xcite , we consider perturbations of the parameter vector @xmath70 and the true points @xmath71 satisfying two constraints .",
    "first , since the true points must belong to the true curve , @xmath72 , we obtain , by the chain rule , _ * x * p(|*x*_i;| ) ,    @xmath73 stands for the scalar product of vectors .",
    "second , since the identity ( [ tass ] ) holds for all @xmath3 , we get _ i=1^n _ i |*x*_i = [ tcon2 ] by using the notation ( [ ti ] ) .",
    "now we need to find a lower bound for the matrix ( [ calc1 ] ) subject to the constraints ( [ tcon1 ] ) and ( [ tcon2 ] ) .",
    "that bound follows from a general theorem in linear algebra : + * theorem ( linear algebra)*. _ let @xmath74 and @xmath75 .",
    "suppose @xmath45 nonzero vectors @xmath76 and @xmath45 nonzero vectors @xmath77 are given , @xmath43 .",
    "consider @xmath78 matrices @xmath79 for @xmath43 , and @xmath80 matrix @xmath81 assume that the vectors @xmath82 span @xmath83 ( hence @xmath84 is nonsingular ) .",
    "we say that a set of @xmath45 matrices @xmath85 ( each of size @xmath78 ) is * proper * if _",
    "a_i w_i = r [ propera1 ] for any vectors @xmath86 and @xmath87 such that u_i^tw_i + v_i^tr = 0 [ propera2 ] for all @xmath43 .",
    "then for any proper set of matrices @xmath85 the @xmath80 matrix @xmath88 is bounded from below by @xmath89 in the sense that @xmath90 is a positive semidefinite matrix .",
    "the equality @xmath91 holds if and only if @xmath92 for all @xmath2 .",
    "_ + this theorem is , probably , known , but we provide a full proof in appendix , for the sake of completeness .    as a direct consequence of the above theorem",
    "we obtain the lower bound for our matrix @xmath93 : + * theorem ( kanatani - cramer - rao lower bound)*. _ we have @xmath94 , in the sense that @xmath95 is a positive semidefinite matrix , where _ _ ^-1 = _ i=1^n [ dmin ]    in view of ( [ csig ] ) and ( [ calc1 ] ) , the above theorem says that the lower bound for the covariance matrix @xmath96 is , to the leading order , _ _",
    "= ^2 d _ [ rc ] the standard deviations of the components of the estimate @xmath44 are of order @xmath97 . therefore",
    ", the bias of @xmath44 , which is at most of order @xmath62 by ( [ tbias ] ) , is infinitesimally small , as @xmath47 , compared to the standard deviations .",
    "this means that the estimates satisfying ( [ tass ] ) are practically unbiased .",
    "the bound ( [ rc ] ) was first derived by kanatani @xcite for the cartesian functional model and strictly unbiased estimates of @xmath3 , i.e.  satisfying @xmath98",
    ". one can easily derive ( [ tass ] ) from @xmath99 by taking the limit @xmath100 , hence our results generalize those of kanatani .",
    "here we derive an explicit formula for the covariance matrix of the weighted algebraic fit ( [ fmain3 ] ) and describe the weights @xmath36 for which the fit is statistically efficient . for brevity ,",
    "we write @xmath101 .",
    "we assume that the weight function @xmath102 is regular , in particular has bounded derivatives with respect to @xmath3 , the next section will demonstrate the importance of this condition .",
    "the solution of the minimization problem ( [ fmain3 ] ) satisfies p_i^2 _ w_i + 2 w_i p_i _",
    "p_i = 0 [ weq ] observe that @xmath103 , so that the first sum in ( [ weq ] ) is @xmath104 and the second sum is @xmath105 .",
    "hence , to the leading order , the solution of ( [ weq ] ) can be found by discarding the first sum and solving the reduced equation w_i p_i _",
    "p_i = 0 [ weq1 ] more precisely , if @xmath106 and @xmath107 are solutions of ( [ weq ] ) and ( [ weq1 ] ) , respectively , then @xmath108 , @xmath109 , and @xmath110 .",
    "furthermore , the covariance matrices of @xmath106 and @xmath107 coincide , to the leading order , i.e.  @xmath111 as @xmath47 . therefore , in what follows , we only deal with the solution of equation ( [ weq1 ] ) .    to find the covariance matrix of @xmath44 satisfying ( [ weq1 ] ) we put @xmath112 and @xmath113 and obtain , working to the leading order , @xmath114 hence @xmath115^{-1 }     \\left [ \\sum w_i ( \\nabla_{\\bf x } p_i)^t \\ ,     ( \\delta { \\bf x}_i)\\ , ( \\nabla_{\\theta } p_i)\\right ]      + { \\cal o}(\\sigma^2)\\ ] ] the covariance matrix is then @xmath116\\\\     & = & \\sigma^2     \\left [ \\sum w_i ( \\nabla_{\\theta } p_i )     ( \\nabla_{\\theta } p_i)^t \\right ] ^{-1 }     \\left [ \\sum w_i^2 \\|\\nabla_{\\bf x } p_i\\|^2     ( \\nabla_{\\theta } p_i )     ( \\nabla_{\\theta } p_i)^t \\right ] \\\\     & & \\times \\left [ \\sum w_i ( \\nabla_{\\theta } p_i )     ( \\nabla_{\\theta } p_i)^t \\right ] ^{-1 }     + { \\cal o}(\\sigma^3)\\end{aligned}\\ ] ] denote by @xmath117 the principal factor here , i.e.@xmath118^{-1 }     \\left [ \\sum w_i^2 \\|\\nabla_{\\bf x } p_i\\|^2     ( \\nabla_{\\theta } p_i )     ( \\nabla_{\\theta } p_i)^t \\right ] \\",
    ",     \\left [ \\sum w_i ( \\nabla_{\\theta } p_i )     ( \\nabla_{\\theta } p_i)^t \\right ] ^{-1}\\ ] ] the following theorem establishes a lower bound for @xmath117 : + * theorem*. _ we have @xmath119 , in the sense that @xmath120 is a positive semidefinite matrix , where @xmath121 is given by ( [ dmin ] ) .",
    "the equality @xmath122 holds if and only if @xmath123 for all @xmath2 . in other words ,",
    "an algebraic fit ( [ fmain3 ] ) is * statistically efficient * if and only if the weight function @xmath124 satisfies w(x , y ; ) = [ wopt ] for all triples @xmath125 such that @xmath0 . here",
    "@xmath126 may be an arbitrary function of @xmath3 . _",
    "+ the bound @xmath127 here is a particular case of the previous theorem .",
    "it also can be obtained directly from the linear algebra theorem if one sets @xmath128 , @xmath129 , and @xmath130^{-1 }     ( \\nabla_{\\theta } p_i ) \\ ,     ( \\nabla_{\\bf x } p_i)^t\\ ] ] for @xmath43 .    the expression ( [ wopt ] )",
    "characterizing the efficiency , follows from the last claim in the linear algebra theorem .",
    "here we illustrate our conclusions by the relatively simple problem of fitting circles . the canonical equation of a circle is ( x - a)^2 + ( y - b)^2 -r^2=0 [ circ0 ] and we need to estimate three parameters @xmath10 . the simple algebraic fit ( [ fmain2 ] ) takes form _ 2(a , b , r ) = _ i=1^n [ ( x_i - a)^2 + ( y_i - b)^2 -r^2]^2    [ f2 ] and the weighted algebraic fit ( [ fmain3 ] ) takes form _ 3(a , b , r ) = _ i=1^n w_i [ ( x_i - a)^2 + ( y_i - b)^2 -r^2]^2    [ f3 ] in particular , the graf becomes _",
    "4(a , b , r ) = _",
    "i=1^n    [ f4 ] ( where the irrelevant constant factor of 4 in the denominator is dropped ) .    in terms of ( [ dmin ] )",
    ", we have @xmath131 and @xmath132 , hence @xmath133=4r^2\\ ] ] therefore , _ = (    ccc u_i^2 & u_iv_i & u_i + u_iv_i & v_i^2 & v_i + u_i & v_i & n +    ) ^-1 [ dmincir ] where we denote , for brevity , @xmath134 the above expression for @xmath121 was derived earlier in @xcite .",
    "now , our theorem in section  [ secse ] shows that the weighted algebraic fit ( [ f3 ] ) is statistically efficient if and only if the weight function satisfies @xmath135 .",
    "since @xmath136 may be an arbitrary function , then the denominator @xmath137 here is irrelevant .",
    "hence , statistically efficiency is achieved whenever @xmath138 is simply independent of @xmath5 and @xmath6 for all @xmath139 lying on the circle . in particular , the graf ( [ f4 ] ) is statistically efficient because @xmath140^{-1}=r^{-2}$ ] .",
    "the simple af ( [ f2 ] ) is also statistically efficient since @xmath141 .",
    "we note that the graf ( [ f4 ] ) is a highly nonlinear problem , and in its exact form ( [ f4 ] ) is not used in practice . instead",
    ", there are two modifications of graf popular among experimenters .",
    "one is due to chernov and ososkov @xcite and pratt @xcite : _",
    "4(a , b , r ) = r^-2_i=1^n [ ( x_i - a)^2 + ( y_i - b)^2 -r^2]^2    [ f4a ] ( it is based on the approximation @xmath142 ) , and the other due to agin @xcite and taubin @xcite : _ 4(a , b , r ) = _ i=1^n [ ( x_i - a)^2 + ( y_i - b)^2 -r^2]^2    [ f4b ] ( here one simply averages the denominator of ( [ f4 ] ) over @xmath143 ) .",
    "we refer the reader to @xcite for a detailed analysis of these and other circle fitting algorithms , including their numerical implementations .",
    "we have tested experimentally the efficiency of four circle fitting algorithms : the olsf ( [ fmain1 ] ) , the simple af ( [ f2 ] ) , the pratt method ( [ f4a ] ) , and the taubin method ( [ f4b ] ) .",
    "we have generated @xmath144 points equally spaced on a circle , added an isotropic gaussian noise with variance @xmath62 ( according to the cartesian model ) , and estimated the efficiency of the estimate of the center by e = [ e ] here @xmath145 is the true center , @xmath146 is its estimate , @xmath147 denotes averaging over many random samples , and @xmath148 , @xmath149 are the first two diagonal entries of the matrix ( [ dmincir ] ) .",
    "table  1 shows the efficiency of the above mentioned four algorithms for various values of @xmath150 .",
    "we see that they all perform very well , and indeed are efficient as @xmath49 .",
    "one might notice that the olsf slightly outperforms the other methods , and the af is the second best .",
    "[ cols=\">,^,^,^,^\",options=\"header \" , ]     table 3 .",
    "data are sampled along a quarter of a circle .",
    "it is interesting to test smaller circular arcs , too .",
    "figure 1 shows a color - coded diagram of the efficiency of the olsf and the af for arcs from @xmath151 to @xmath152 and variable @xmath41 ( we set @xmath153 , where @xmath154 is the height of the circular arc , see fig .  2 , and @xmath155 varies from 0 to 0.5 ) .",
    "the efficiency of the pratt and taubin is virtually identical to that of the olsf , so it is not shown here .",
    "we see that the olsf and af are efficient as @xmath49 ( both squares in the diagram get white at the bottom ) , but the af loses its efficiency at moderate levels of noise ( @xmath156 ) , while the olsf remains accurate up to @xmath157 after which it rather sharply breaks down .",
    "@xmath158 @xmath158    figure 1 : the efficiency of the simple olsf ( left ) and the af ( center ) .",
    "the bar on the right explains color codes .",
    "the following analysis sheds more light on the behavior of the circle fitting algorithms .",
    "when the curvature of the arc decreases , the center coordinates @xmath159 and the radius @xmath160 grow to infinity and their estimates become highly unreliable . in that case the circle equation ( [ circ0 ] ) can be converted to a more convenient algebraic form a(x^2+y^2 ) + bx + cy + d = 0 [ abcd ] with an additional constrain on the parameters : @xmath161 .",
    "this parametrization was used in @xcite , and analyzed in detail in @xcite .",
    "we note that the original parameters can be recovered via @xmath162 , @xmath163 , and @xmath164 .",
    "the new parametrization ( [ abcd ] ) is safe to use for arcs with arbitrary small curvature : the parameters @xmath165 remain bounded and never develop singularities , see @xcite .",
    "even as the curvature vanishes , we simply get @xmath166 , and the equation ( [ abcd ] ) represents a line @xmath167 .",
    "figure 2 : the height of an arc , @xmath154 , and our formula for @xmath41 .    in terms of the new parameters @xmath165 ,",
    "the weighted algebraic fit ( [ fmain3 ] ) takes form _ 3(a , b , c , d ) = _",
    "i=1^n w_i [ a(x^2+y^2 ) + bx + cy + d]^2    [ ff3 ] ( under the constraint @xmath161 ) . converting the af ( [ f2 ] ) to the new parameters gives _",
    "2(a , b , c , d ) = _",
    "i=1^n a^-2 [ a(x^2+y^2 ) + bx + cy + d]^2    [ ff2 ] which corresponds to the weight function @xmath168 .",
    "the pratt method ( [ f4a ] ) turns to _ 4(a , b , c , d ) = _ i=1^n [ a(x^2+y^2 ) + bx + cy + d]^2    [ ff4 ] we now see why the af is unstable and inaccurate for arcs with small curvature : its weight function @xmath168 develops a singularity ( it explodes ) in the limit @xmath169 .",
    "recall that , in our derivation of the statistical efficiency theorem ( section  3 ) , we assumed that the weight function was regular ( had bounded derivatives ) .",
    "this assumption is clearly violated by the af ( [ ff2 ] ) . on the contrary ,",
    "the pratt fit ( [ ff4 ] ) uses a safe choice @xmath170 and thus behaves decently on arcs with small curvature , see next .",
    "@xmath158 @xmath158    figure 3 : the efficiency of the simple af ( left ) and the pratt method ( center ) .",
    "the bar on the right explains color codes .",
    "figure 3 shows a color - coded diagram of the efficiency of the estimate of the parameter , hence the estimation of @xmath171 is equivalent to that of the curvature , an important geometric parameter of the arc .",
    "] @xmath171 by the af ( [ ff2 ] ) versus pratt ( [ ff4 ] ) for arcs from @xmath151 to @xmath152 and the noise level @xmath153 , where @xmath154 is the height of the circular arc and @xmath155 varies from 0 to 0.5 . the efficiency of the olsf and the taubin method is visually indistinguishable from that of pratt ( the central square in fig .",
    "3 ) , so we did not include it here .",
    "we see that the af performs significantly worse than the pratt method for all arcs and most of the values of @xmath155 ( i.e. , @xmath41 ) .",
    "the pratt s efficiency is close 100% , its lowest point is 89% for @xmath152 arcs and",
    "@xmath172 ( the top right corner of the central square barely gets grey ) .",
    "the af s efficiency is below 10% for all @xmath173 and almost zero for @xmath174 .",
    "still , the af remains efficient as @xmath49 ( as the tiny white strip at the bottom of the left square proves ) , but its efficiency can be only counted on when @xmath41 is extremely small .",
    "our analysis demonstrates that the choice of the weights @xmath36 in the weighted algebraic fit ( [ fmain3 ] ) should be made according to our theorem in section  3 , and , in addition , one should avoid singularities in the domain of parameters .",
    "here we prove the theorem of linear algebra stated in section  [ seckcr ] .",
    "for the sake of clarity , we divide our proof into small lemmas :        * lemma 2*. _ if a set of @xmath45 matrices @xmath85 is proper , then rank@xmath180 .",
    "furthermore , each @xmath181 is given by @xmath182 for some vector @xmath183 , and the vectors @xmath184 satisfy @xmath185 where @xmath21 is the @xmath80 identity matrix .",
    "the converse is also true . _",
    "_ proof_. let vectors @xmath186 and @xmath187 satisfy the requirements ( [ propera1 ] ) and ( [ propera2 ] ) of the theorem . consider the orthogonal decomposition @xmath188 where @xmath189 is perpendicular to @xmath190 , i.e.  @xmath191 . then the constraint ( [ propera2 ] )",
    "can be rewritten as c_i = - [ propera3 ] for all @xmath2 and ( [ propera1 ] ) takes form _",
    "i=1^n c_ia_iu_i + _",
    "i=1^n a_iw_i^= r [ propera4 ] we conclude that @xmath192 for every vector @xmath189 orthogonal to @xmath190 , hence @xmath181 has a @xmath193-dimensional kernel , so indeed its rank is zero or one .",
    "if we denote @xmath194 , we obtain @xmath195 .",
    "combining this with ( [ propera3])-([propera4 ] ) gives @xmath196 since this identity holds for any vector @xmath87 , the expression within parentheses is @xmath197 .",
    "the converse is obtained by straightforward calculations .",
    "lemma is proved .        *",
    "lemma 3*. _ the sets of proper matrices make a linear variety , in the following sense .",
    "let @xmath200 and @xmath201 be two proper sets of matrices , then the set @xmath85 defined by @xmath202 is proper for every @xmath203 .",
    "_            _",
    "proof_. for each @xmath2 consider the @xmath213 matrix @xmath214 .",
    "using the previous lemma gives @xmath215 by construction , this matrix is positive semidefinite .",
    "hence , the following matrix is also positive semidefinite : @xmath216 by sylvester s theorem , the matrix @xmath217 is positive semidefinite .          _",
    "proof_. assume that there is a proper set of matrices @xmath200 , different from @xmath221 , for which @xmath91 .",
    "denote @xmath222 . by lemma 3 ,",
    "the set of matrices @xmath223 is proper for every real @xmath224 . consider the variable matrix @xmath225 [ a_i(\\gamma)]^t\\\\     & = & \\sum_{i=1}^n a_i^{\\rm o}(a_i^{\\rm o})^t     + \\gamma\\left ( \\sum_{i=1}^n a_i^{\\rm o}(\\delta a_i)^t     + \\sum_{i=1}^n ( \\delta a_i)(a_i^{\\rm o})^t\\right )     + \\gamma^2\\sum_{i=1}^n ( \\delta a_i)(\\delta a_i)^t\\end{aligned}\\ ] ] note that the matrix @xmath226 is symmetric . by lemma  5",
    "we have @xmath227 for all @xmath224 , and by lemma  6 we have @xmath228 .",
    "it is then easy to derive that @xmath229 .",
    "next , the matrix @xmath230 is symmetric positive semidefinite . since we assumed that @xmath231 , it is easy to derive that @xmath232 as well .",
    "therefore , @xmath233 for every @xmath2 .",
    "the theorem is proved .",
    "g. taubin , estimation of planar curves , surfaces and nonplanar space curves defined by implicit equations , with applications to edge and range image segmentation , _ ieee transactions on pattern analysis and machine intelligence _ , * 13 * , 1991 , 11151138 ."
  ],
  "abstract_text": [
    "<S> we study the problem of fitting parametrized curves to noisy data . under certain assumptions ( known as cartesian and radial functional models ) </S>",
    "<S> , we derive asymptotic expressions for the bias and the covariance matrix of the parameter estimates . </S>",
    "<S> we also extend kanatani s version of the cramer - rao lower bound , which he proved for unbiased estimates only , to more general estimates that include many popular algorithms ( most notably , the orthogonal least squares and algebraic fits ) . </S>",
    "<S> we then show that the gradient - weighted algebraic fit is statistically efficient and describe all other statistically efficient algebraic fits .    </S>",
    "<S> keywords : least squares fit , curve fitting , circle fitting , algebraic fit , rao - cramer bound , efficiency , functional model . </S>"
  ]
}