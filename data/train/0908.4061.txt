{
  "article_text": [
    "the main technical contribution of this paper is an extension of matouek s range emptiness and reporting data structures  @xcite ( see also @xcite for a dynamic version of the problem ) to the case of general semi - algebraic ranges .    [ [ ray - shooting - amid - balls . ] ] ray shooting amid balls .",
    "+ + + + + + + + + + + + + + + + + + + + + + + +    a motivating application of this study is ray shooting amid balls in @xmath6 , where we want to construct a data structure of linear size with near - linear preprocessing , which supports ray shooting queries in sublinear time .",
    "typically , in problems of this sort , the bound on the query time is some fractional power of @xmath1 , the number of objects , and the goal is to make the exponent as small as possible . for example ,",
    "ray shooting amid a collection of @xmath1 arbitrary triangles can be performed in @xmath14 time ( with linear storage ) @xcite .",
    "better solutions are known for various special cases .",
    "for example , the authors have shown @xcite that the query time can be improved to @xmath13 , when the triangles are all _ fat _ , or are all stabbed by a common line .    at the other end of the spectrum ,",
    "one is interested in ray shooting algorithms and data structures where a ray shooting query can be performed in logarithmic or polylogarithmic time ( or even @xmath29 time , for any @xmath10 ; this is @xmath21 in our shorthand notation ) . in this case",
    ", the goal is to reduce the storage ( and preprocessing ) requirements as much as possible .",
    "for example , for arbitrary triangles ( and even for the special case of fat triangles ) , the best known bound for the storage requirement ( with logarithmic query time ) is @xmath30 @xcite . for balls , mohaban and sharir @xcite , gave an algorithm with @xmath31 storage and @xmath21 query time .",
    "however , when only linear storage is used , the previously best known query time ( for balls ) is @xmath14 ( as in the case of general triangles ) . in this paper",
    "we show , as an application of our general range emptiness machinery , that this can be improved to @xmath13 time .",
    "when answering a ray - shooting query for a set @xmath32 of input objects , one generally reduces the problem to that of answering _ segment emptiness _",
    "queries , following the parametric searching scheme proposed by agarwal and matouek  @xcite ( see also megiddo @xcite for the original underlying technique ) .",
    "a standard way of performing the latter kind of queries is to switch to a dual parametric space , where each object in the input set is represented by a _",
    "point_. a segment @xmath33 in @xmath6 is mapped to a surface @xmath34 , which is the locus of all the points representing the objects that @xmath33 touches ( without penetrating into their interior ) .",
    "usually , @xmath34 partitions the dual space into two portions , one , @xmath35 , consisting of points representing objects whose interior is intersected by @xmath33 , and the other , @xmath36 , consisting of points representing objects that @xmath33 avoids .",
    "the segment - emptiness problem thus transforms into a range - emptiness query : does @xmath35 contain any point representing an input object ?    [ [ range - reporting - and - emptiness - searching . ] ] range reporting and emptiness searching .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    range - emptiness queries of this kind have been studied by matouek  @xcite ( see also agarwal and matouek  @xcite ) , but only for the case where the ranges are halfspaces bounded by hyperplanes . for this case",
    ", matouek has established a so - called _ shallow - cutting lemma _ , that shows the existence of a @xmath37-cutting of the @xmath1 given surfaces ( hyperplanes in this case ) .",
    "see below for more details . ] that covers the complement of the union of any @xmath38 given halfspace ranges , whose size is significantly smaller than the size of a @xmath37-cutting that covers the entire space .",
    "this lemma provides the basic tool for partitioning a point set @xmath0 , in the style of @xcite , so that _ shallow _ hyperplanes ( those containing at most @xmath39 points of @xmath0 below them , say , for some given parameter @xmath40 ) cross only a small number of cells of the partition ( see below for more details ) .",
    "this in turn yields a data structure , known as a _ shallow partition tree _ , that stores a recursive partitioning of @xmath0 , which enables us to answer more efficiently halfspace range _ reporting _",
    "queries for shallow hyperplanes , and thus also halfspace range emptiness queries . using this approach",
    ", the query time ( for emptiness ) improves from the general halfspace range searching query cost of @xmath41 to @xmath42 .",
    "reporting takes @xmath43 , where @xmath27 is the output size .",
    "consequently , one way of applying this machinery for more general semi - algebraic ranges is to `` lift '' the set of points and the ranges into a higher - dimensional space by means of an appropriate _ linearization _ , as in @xcite , and then apply the above machinery .",
    "( for this , one needs to assume that the given ranges have _ constant description complexity _ , meaning that each range is a boolean combination of a constant number of polynomial equalities and inequalities of constant maximum degree .",
    "however , if the space in which the ranges are linearized has high dimension , the resulting range reporting or emptiness queries become significantly less efficient .",
    "moreover , in many applications , the ranges are boolean combinations of polynomial ( equalities and ) inequalities , which creates additional difficulties in linearizing the ranges , resulting in even worse running time .",
    "an alternative technique is to give up linearization , and instead work in the original space .",
    "as follows from the machinery of @xcite ( and further elaborated later in this paper ) , this requires , as a major tool , the ( existence and ) construction of a decomposition of the _ complement of the union _ of @xmath38 given ranges ( in the case of segment emptiness , these are the ranges @xmath35 , for an appropriate collection of segments @xmath33 ) , into a small number of `` elementary cells '' ( in the terminology of @xcite  see also below ) . here",
    "we face , especially in higher dimensions , a scarcity of sharp bounds on the complexity of the union itself , to begin with , and then on the complexity of a decomposition of its complement .",
    "often , the best one can do is to decompose the entire arrangement of the given ranges , which results in too many elementary cells , and consequently in an algorithm with poor performance .    to recap , in the key technical step in answering general semi - algebraic range reporting or emptiness queries , the best current approaches are either to construct a cutting of the _ entire _ arrangement of the range - bounding surfaces in the original space , or to construct a shallow cutting in another higher - dimensional space into which the ranges can be linearized .",
    "for many natural problems ( including the segment - emptiness problem ) , both approaches yield relatively poor performance .    as we will shortly note , in handling general semi - algebraic ranges , we face another major technical issue , having to do with the construction of efficient _ test sets _ of ranges ( in the terminology of @xcite , elaborated below ) . addressing",
    "this issue is a major component of the analysis in this paper , and is discussed in detail later on .",
    "[ [ our - results . ] ] our results .",
    "+ + + + + + + + + + + +    we propose a variant of the shallow - cutting machinery of  @xcite for the case of semi - algebraic ranges , which avoids the need for linearization , and works in the original space ( which , for the case of ray shooting amid balls , is a 4-dimensional parametric space in which the balls are represented as points ) . while the machinery used by our variant is similar in principle to that in @xcite ,",
    "there are several significant technical difficulties which require more careful treatment .",
    "matouek s technique @xcite , as well as ours , considers a finite set @xmath44 of shallow ranges ( called a _",
    "test set _ ) , and builds a data structure which caters only for ranges in @xmath44 .",
    "matouek shows how to build , for any given parameter @xmath40 , a set of halfspaces of size polynomial in @xmath40 , which represents well _ all _ @xmath45-shallow ranges , in the following sense : for any _ simplicial partition _ into @xmath46 subsets of roughly equal size , each enclosed by some simplex ( in the linear case ) or some elementary cell ( in the general semi - algebraic case ) ; see  @xcite and section  [ section : range_emptiness ] below .",
    "] @xmath47 with parameter @xmath40 , let @xmath48 denote the maximal number of cells of @xmath47 crossed by a halfspace in @xmath44 .",
    "then each @xmath45-shallow halfspace crosses at most @xmath49 cells of @xmath47 , where @xmath25 is a constant that depends on the dimension .",
    "unfortunately ( for the present analysis ) , the linear nature of the ranges is crucially needed for the proof , which therefore fails for non - linear ranges .",
    "being a good representative of all shallow ranges , in the above sense , is only one of the requirements from a good test set @xmath44 .",
    "the other requirements are that @xmath44 be small , so that , in particular , it can be constructed efficiently , and that the ( decomposition of the ) complement of the union of any subset of @xmath44 have small complexity .",
    "all these properties hold for the case of halfspaces bounded by hyperplanes , studied in @xcite .",
    "[ testset_for_empty_only ] as it turns out , and hinted above , obtaining a `` good '' test set @xmath44 for general semi - algebraic ranges , with the above properties , is not an easy task .",
    "we give a simple general recipe for constructing such a set @xmath44 , but it consists of more complex ranges than those in the original setup . a major problem with this recipe is that since the members of @xmath44 have a more complex shape , it becomes harder to establish good bounds on the complexity of ( the decomposition of ) the complement of the union of any subset of these generalized ranges .",
    "nevertheless , once a good test set has been shown to exist , and to be efficiently computable , it leads to a construction of an efficient elementary - cell partition with a small crossing number for any empty or shallow original range . using this construction recursively ,",
    "one obtains a _ partition tree _ , of linear size , so that any shallow original range @xmath50 visits only a small number of its nodes ( where @xmath50 visits a node if it _ crosses _ the elementary cell enclosing the subset of that node , meaning that it intersects this cell but does not fully contain it ) , which in turn leads to an efficient range reporting or emptiness - testing procedure .",
    "this part , of constructing and searching the tree , is almost identical to its counterparts in the earlier works @xcite , and we will not elaborate on it here , focusing only on the technicalities in the construction of a single `` shallow '' elementary - cell partition .    developing all this machinery , and then putting it into action , we obtain efficient data structures for the following applications , improving previous results or obtaining the first nontrivial solutions .",
    "these instances are :    [ [ ray - shooting - amid - balls - in-3-space . ] ] ray shooting amid balls in 3-space .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    given a set @xmath32 of @xmath1 balls in @xmath6 , we construct , in @xmath8 time , a data structure of @xmath7 size , which can determine , for a given query segment @xmath33 , whether @xmath33 is empty ( avoids all balls ) , in @xmath13 time . plugging this data structure into the parametric searching technique of agarwal and matouek  @xcite , we obtain a data structure for answering ray shooting queries amid the balls of @xmath32 , which has similar performance bounds .",
    "we represent balls in 3-space as points in @xmath51 , where a ball with center @xmath52 and radius @xmath40 is mapped to the point @xmath53 , and each object @xmath54 is mapped to the surface @xmath55 , which is the locus of all ( points representing ) balls tangent to @xmath56 ( i.e. , balls that touch @xmath56 , but do not penetrate into its interior ) . in this case",
    ", the range of an object @xmath56 is the upper halfspace @xmath57 consisting of all points lying above @xmath55 ( representing balls that intersect @xmath56 ) .",
    "the complement of the union of a subfamily of these ranges is the region below the lower envelope of the corresponding surfaces which are considerably more complex than just lines or segments , but are nevertheless still of constant description complexity . ]",
    "the _ minimization diagram _ of this envelope is the 3-dimensional euclidean voronoi diagram of the corresponding set of objects .",
    "thus we reveal ( what we regard as ) a somewhat surprising connection between the problem of ray shooting amid balls and the problem of analyzing the complexity of euclidean voronoi diagrams of ( simply - shaped ) objects in 3-space .    [",
    "[ farthest - point - from - a - line - or - from - any - convex - set - in - reals3 . ] ] farthest point from a line ( or from any convex set ) in @xmath6 .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    let @xmath0 be a set of @xmath1 points in @xmath6 .",
    "we wish to preprocess @xmath0 into a data structure of size @xmath7 , so that , for any query line @xmath15 , we can efficiently find the point of @xmath0 farthest from @xmath15 .",
    "this is a useful routine for approximating polygonal paths in three dimensions ; see @xcite .    as in the ray shooting problem",
    ", we can reduce such a query to a range emptiness query of the form : given a cylinder @xmath58 , does it contain all the points of @xmath0 ? ( that is , is the complement of the cylinder empty ? ) we prefer to regard this as an instance of the complementary _ range fullness _ problem , which seeks to determine whether a query range is _ full _",
    "( i.e. , contains all the input points ) .",
    "our machinery can handle this problem .",
    "in fact , we can solve the range fullness problem for any family of _ convex _ ranges in 3-space , of constant description complexity .",
    "our solution requires @xmath7 storage and near linear preprocessing , and answers a range fullness query in @xmath16 time , improving the query time @xmath13 given by agarwal and matouek @xcite .",
    "we then apply this result to solve the problem of finding the largest - area triangle spanned by a set of @xmath1 points in 3-space .",
    "the resulting algorithm requires @xmath17 time , which improves a previous bound of @xmath59 due to daescu and serfling @xcite .",
    "we also adapt our machinery to compute efficiently the largest - perimeter triangle and the largest - height triangle spanned by such a point set .    in both this , and the preceding ray - shooting applications",
    ", we use the general , more abstract recipe for constructing good test sets .",
    "[ [ fat - triangle - and - circular - cap - range - emptiness - searching - and - reporting . ] ] fat triangle and circular cap range emptiness searching and reporting . + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    finally , we consider two planar instances of the range emptiness and reporting problems , in which we are given a planar set @xmath0 of @xmath1 points , and the ranges are either _",
    "@xmath19-fat triangles _ or sufficiently large _ circular caps _ ( say , larger than a semidisk ) . the general technique of agarwal and matouek  @xcite yields , for any class of planar ranges with constant description complexity , a data structure with near linear preprocessing and linear storage , which answers such queries in time @xmath16 ( for emptiness ) or @xmath60 ( for reporting ) .",
    "we improve the query time to @xmath21 and @xmath23 , respectively , in both cases .    in these planar applications , we abandon the general recipe , and construct good test sets in an ad - hoc ( and simpler ) manner . for @xmath19-fat triangles ( i.e. , triangles with the property that each of their angles is at least @xmath19 , which is some fixed positive constant ) ,",
    "the test set consists of `` canonical '' @xmath61-fat triangles , and the fast query performance is a consequence of the fact that the complement of the union of @xmath38 @xmath62-fat triangles is @xmath63 , for any constant @xmath64 @xcite .",
    "it is quite likely that our machinery can also be applied to other classes of fat objects in the plane , for which near - linear bounds on the complexity of their union are known @xcite .",
    "however , constructing a good test set for each of these classes is not an obvious step .",
    "we leave these extensions as open problems for further research .    for circular caps ,",
    "the motivation for range emptiness searching comes from the problem of finding , for a query consisting of a point @xmath65 and a line @xmath15 , the point of @xmath0 which lies above @xmath15 and is nearest to @xmath65 ( we only consider the case where @xmath65 lies on or above @xmath15 ) .",
    "such a procedure was considered in @xcite .",
    "using parametric searching , the latter problem can be reduced to that of testing for emptiness of a circular cap centered at @xmath65 and bounded by @xmath15 ( the assumption on the location of @xmath65 ensures that this cap is at least a semidisk ) . here too we manage to construct a test set which consists of ( possibly slightly smaller ) circular caps , and we exploit the fact that the complexity of the union of @xmath38 such caps is @xmath66 , as long as the caps are not too small ( relative to their bounding circles ) , to obtain the fast performance stated above .    [ [ approximate - range - counting . ] ] approximate range counting .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + +    adapting the recent techniques of @xcite , we can turn our solutions into efficient algorithms for approximate range counting ( with small relative error ) for the cases mentioned above .",
    "that is , for a specified @xmath10 , we can preprocess the input point set @xmath0 into a data structure which can efficiently compute , for any query range @xmath50 , an approximate count @xmath67 , satisfying @xmath68 .",
    "the performance of the resulting algorithms is detailed in section  [ sec : apx ] .",
    "as observed in the papers just cited , approximate range counting is closely related to the range emptiness problem , which in fact is a special case of the former problem .",
    "the algorithm in @xcite performs approximate range counting by a randomized binary search over @xmath69 , where the search is guided by repeated calls to an emptiness testing routine on various random samples of @xmath0 .",
    "this algorithm uses emptiness searching as a black box , so , plugging our solutions for this latter problem into their algorithm , we obtain efficient approximate range counting algorithms for the ranges considered in this paper .",
    "see section  [ sec : apx ] for details .",
    "[ [ related - work . ] ] related work .",
    "+ + + + + + + + + + + + +    our study was originally motivated by work by daescu and others  @xcite on path approximations and related problems . in these applications",
    "one needs to compute efficiently the vertex of a subpath which is farthest from a given segment ( connecting the two endpoints of the subpath ) .",
    "these works used the standard range searching machinery of @xcite , and motivated us to look for faster implementations .",
    "the general range emptiness ( or reporting ) problem was studied by the authors a few years ago  @xcite . in this earlier version ,",
    "we did not manage to handle properly the issue of constructing a good test set , so the results presented there are somewhat incomplete .",
    "the present paper builds upon the previous one , but provides a thorough analysis of this aspect of the problem , and consequently obtains a complete and efficient solution to the problems listed above , and lays down the foundation for obtaining efficient solutions to many other similar problems ",
    "we believe indeed that the applications given here only scratch the surface of the wealth of potential future applications of this sort .",
    "we begin with a brief review of the main concepts and notations used in our analysis .",
    "[ [ range - spaces . ] ] range spaces .",
    "+ + + + + + + + + + + + +    a range space is a pair @xmath70 , where @xmath71 is a set and @xmath72 is a collection of subsets of @xmath71 , called _",
    "ranges_. in our applications , @xmath73 , and @xmath74 is a collection of semi - algebraic sets of some specific type , each having _ constant description complexity_. that is , each set in @xmath74 is given as a boolean combination of a constant number of polynomial equalities and inequalities of constant maximum degree . to simplify the analysis , we assume , as in @xcite , that all the ranges in @xmath74 are defined by a single boolean combination , so that each polynomial @xmath75 in this combination is @xmath76-variate , and each range @xmath50 has @xmath77 degrees of freedom , so that if we substitute the values of these @xmath77 parameters into the last @xmath77 variables of each @xmath75 , the resulting boolean combination defines the range @xmath50 .",
    "this allows us to represent the ranges of @xmath74 as points in an appropriate @xmath77-dimensional parametric space .    under these special assumptions ,",
    "the range space @xmath70 has _ finite vc - dimension _",
    ", a property formally defined in @xcite .",
    "informally , it ensures that , for any finite subset @xmath0 of @xmath71 , the number of distinct ranges of @xmath0 is @xmath78 , where @xmath79 is the vc - dimension .    as a matter of fact , we will consider range spaces of the form @xmath80 , where @xmath81 is a finite point set , and each range in @xmath82 is the intersection of @xmath0 with a range in @xmath74 .    [ [ cuttings . ] ] cuttings .",
    "+ + + + + + + + +    given a finite collection @xmath74 of @xmath1 semi - algebraic ranges in @xmath2 , as above , and a parameter @xmath83 , a _",
    "@xmath84-cutting _ for @xmath74 is a partition @xmath85 of @xmath2 ( or of some portion of @xmath2 ) into a finite number of relatively open cells of dimensions @xmath86 , so that each cell is _ crossed _ by at most @xmath39 ranges of @xmath74 , where a range @xmath87 is said to cross a cell @xmath3 if @xmath88 , but @xmath50 does not fully contain @xmath3 .",
    "we will also need to consider _ weighted _ @xmath84-cuttings , where each range @xmath87 has a positive weight @xmath89 , and each cell of @xmath85 is crossed by ranges whose total weight is at most @xmath90 , where @xmath91 is the overall weight of all the ranges in @xmath74 .",
    "[ [ shallow - ranges . ] ] shallow ranges .",
    "+ + + + + + + + + + + + + + +    a range @xmath87 is called _",
    "@xmath27-shallow _ with respect to a set @xmath0 of points in @xmath2 , if @xmath92 .",
    "[ [ elementary - cells . ] ] elementary cells .",
    "+ + + + + + + + + + + + + + + + +    define , as in @xcite , an _ elementary cell _ in @xmath2 to be a connected relatively open semi - algebraic set of some dimension @xmath93 , which is homeomorphic to a ball and has constant description complexity . as above , we assume , for simplicity , that the elementary cells are defined by a single boolean combination involving @xmath77 free variables , and each cell is determined by fixing the values of these @xmath77 parameters .    [ [ elementary - cell - partition . ] ] elementary cell partition .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + +    let @xmath0 be a set of @xmath1 points in @xmath2 .",
    "an _ elementary cell partition _ of @xmath0 is a collection @xmath94 , for some integer @xmath38 , such that ( i ) @xmath95 is a partition of @xmath0 ( into pairwise disjoint subsets ) , and ( ii ) each @xmath96 is an elementary cell that contains the respective subset @xmath97 . in general , the cells @xmath96 need not be disjoint . usually , one also specifies a parameter @xmath98 , and requires that @xmath99 for each @xmath100 , so @xmath101 .",
    "[ [ the - function - zetar . ] ] the function @xmath102 .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in lemma  [ lemma : shallow_cutting ] and theorem  [ theo : part ] , we use a function @xmath102 that bounds the number of elementary cells in a decomposition of the complement of the union of any @xmath40 ranges of @xmath74 .",
    "we assume that @xmath102 is `` well behaved '' , in the sense that for each @xmath103 there exists @xmath104 such that @xmath105 for every @xmath40 .",
    "we also assume that @xmath106 .",
    "[ [ nualpha - samples - and - shallow - eps - nets . ] ] @xmath107-samples and shallow @xmath12-nets .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we recall the result of li et al .",
    "@xcite , and adapt it , similar to the recent observations in @xcite , to obtain a useful extension of the notion of @xmath12-nets .",
    "let @xmath108 be a range space of finite vc - dimension @xmath79 , and let @xmath109 be two given parameters . consider the distance function @xmath110 a subset @xmath111 is called a _ @xmath107-sample _ if for each @xmath112 we have @xmath113    [ lls ] a random sample @xmath114 of @xmath115 elements of @xmath71 is a @xmath107-sample with probability at least @xmath116 .",
    "har - peled and sharir  @xcite show that , by appropriately choosing @xmath19 and @xmath117 , various standard constructs , such as @xmath12-nets and @xmath12-approximations , are special cases of @xmath107-samples .",
    "here we follow a similar approach , and show the existence of small - size _ shallow @xmath12-nets _ , a new notation introduced in this paper .",
    "let us first define this notion . let @xmath108 be a range space of finite vc - dimension @xmath79 , and let @xmath118 be a given parameter .",
    "a subset @xmath111 is a shallow @xmath12-net if it satisfies the following two properties , for some absolute constant @xmath25 .",
    "\\(i ) for each @xmath119 and for any parameter @xmath120 , if @xmath121 then @xmath122 .",
    "\\(ii ) for each @xmath119 and for any parameter @xmath120 , if @xmath123 then @xmath124 .",
    "note the difference between shallow and standard @xmath12-nets : property ( i ) ( with @xmath125 ) implies that a shallow @xmath12-net is also a standard @xmath12-net ( possibly with a recalibration of @xmath12 ) . property ( ii )",
    "has no parallel in the case of standard @xmath12-nets  there is no guarantee how a standard net interacts with small ranges .    [ epsshallow ]",
    "a random sample @xmath114 of @xmath126 elements of @xmath71 is a shallow @xmath12-net with probability at least @xmath116 .",
    "* proof : * take @xmath127 , say , and calibrate the constants in the size of @xmath114 to guarantee , with probability @xmath116 , that @xmath114 is an @xmath128-sample .",
    "assume that this is indeed the case .",
    "for a range @xmath112 , put @xmath129 and @xmath130 .",
    "we have @xmath131 that is , @xmath132 or @xmath133 this is easily seen to imply properties ( i ) and ( ii ) . for ( i ) , let @xmath134 be a range for which @xmath121 ; that is , @xmath135 , for some absolute constant @xmath136 ( proportional to the vc - dimension )",
    ". then @xmath137 for ( ii ) , let @xmath134 be a range for which @xmath123 ; that is , @xmath138 .",
    "then @xmath139 for another absolute constant @xmath50 ( again , proportional to the vc - dimension ) . @xmath140",
    "[ [ shallow - cutting - in - the - semi - algebraic - case . ] ] shallow cutting in the semi - algebraic case .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we begin by extending the shallow cutting lemma of matouek  @xcite to the more general setting of semi - algebraic ranges .",
    "this extension is fairly straightforward , although it involves several technical steps that deserve to be highlighted .",
    "[ app : not ]    [ lemma : shallow_cutting ]",
    "let @xmath74 be a collection of @xmath1 semi - algebraic ranges in @xmath2 .",
    "assume that the complement of the union of any subset of @xmath38 ranges in @xmath74 can be decomposed into at most @xmath141 elementary cells , for a well - behaved function @xmath142 as above .",
    "then , for any @xmath98 , there exists a ( @xmath143)-cutting @xmath85 with the following properties : + ( i ) the union of the cells of @xmath85 contains the complement of the union of @xmath74 .",
    "+ ( ii ) @xmath85 consists of @xmath144 elementary cells . + (",
    "iii ) the complement of the union of the cells of @xmath85 is contained in a union of @xmath46 ranges in @xmath74 .",
    "see figure [ fig : shallow - lemma ] for an illustration .",
    "_ the proof is a fairly routine adaptation of the proof in @xcite .",
    "we employ a variant of the method of chazelle and friedman @xcite for constructing the cutting .",
    "let @xmath145 be a random sample of @xmath46 ranges of @xmath74 , and let @xmath146 denote the complement of the union of @xmath145 .",
    "by assumption , @xmath146 can be decomposed into at most @xmath144 elementary cells .",
    "the resulting collection @xmath85 of these cells is such that their union clearly contains the complement of the union of @xmath74 .",
    "moreover , the complement of the union of @xmath85 is the union of the @xmath46 ranges of @xmath145 .",
    "hence , @xmath85 satisfies all three conditions ( i)(iii ) , but it may fail to be a @xmath84-cutting .",
    "shallow_lemma.pstex_t    this latter property is enforced as in @xcite , by further decomposing each cell @xmath147 of @xmath85 that is crossed by more than @xmath39 ranges of @xmath74 , using additional subsamples from the surfaces that cross @xmath147 .",
    "specifically , for each cell @xmath147 of @xmath85 , let @xmath148 denote the subset of those ranges in @xmath74 that cross @xmath147 , and put @xmath149 .",
    "if @xmath150 , we sample @xmath151 ranges from @xmath148 , construct the complement of the union of these ranges , decompose it into at most @xmath152 elementary cells , and clip them to within @xmath147 .",
    "the resulting collection @xmath153 of subcells , over all cells @xmath147 of the original @xmath85 , clearly satisfies ( i ) .",
    "the analysis of @xcite ( see also @xcite ) establishes an exponential decay property on the number of cells of @xmath85 that are crossed by more than @xmath154 ranges , as a function of @xmath155 .",
    "specifically , as in @xcite , the expected number of such cells is @xmath156 , where @xmath157 is another random sample of @xmath74 , where each member of @xmath74 is chosen with probability @xmath158 .",
    "this property implies , as usual @xcite , that @xmath153 is ( with high probability ) a @xmath84-cutting , and it also implies that the size of @xmath153 is still @xmath144 , assuming @xmath142 to be well behaved . since we have only refined the original cells of @xmath85 , the number of ranges that cover the complement of the union of the final cells is still @xmath46 .",
    "@xmath140    a special case that arises frequently is where each range in @xmath74 is an upper ( or lower ) halfspace bounded by the graph of some continuous @xmath159-variate function . in this case",
    "the complement @xmath56 of the union of @xmath40 ranges is the portion of space that lies below the _ lower envelope _ of the bounding graphs . in this case",
    ", it suffices to decompose the graph of the lower envelope itself into at most @xmath102 elementary cells .",
    "indeed , having done that , we can extend each cell @xmath147 within the envelope into the cell @xmath160 consisting of all points that lie vertically below @xmath147 .",
    "the new cells decompose @xmath56 and are also elementary .    as already discussed in the introduction , obtaining tight or nearly tight bounds for @xmath102",
    "is still a major open problem for many instances of the above setup .",
    "for example , decomposing an upper envelope of @xmath40 @xmath159-variate functions of constant description complexity into @xmath161 elementary cells is still open for any @xmath162 .",
    "( this bound is best possible in the worst case , since it is the worst - case tight bound on the complexity of such an undecomposed envelope  @xcite . )",
    "the cases @xmath163 ( upper envelope of curves in the plane ) and @xmath164 ( upper envelope of 2-dimensional surfaces in 3-space ) are easy . in these cases",
    "@xmath102 is proportional to the complexity of the envelope , which in the worst case is near - linear for @xmath163 and near - quadratic for @xmath164 @xcite . in higher dimensions ,",
    "the only general - purpose bound known to date is the upper bound obtained by computing the _ vertical decomposition _ of the _ entire _ arrangement of the given surfaces , and extracting from it the relevant cells that lie on or above the envelope . in particular , for @xmath165 the bound is @xmath166 , as follows from the results of @xcite .",
    "this leaves a gap of about a factor of @xmath40 between this bound and the bound @xmath167 on the complexity of the undecomposed envelope .",
    "of course , in certain special cases , most notably the case of hyperplanes , as studied in @xcite , both the envelope and its decomposition have ( considerably ) smaller complexity .    the situation with the complexity of the union of geometric objects is even worse . while considerable progress was recently made on many special cases in two and three dimensions ( see @xcite for a recent comprehensive survey ) , there are only very few sharp bounds on the complexity of unions in higher dimensions .",
    "worse still , even when a sharp bound on the complexity of the union is known , obtaining comparable bounds on the complexity of a decomposition of the complement of the union is a much harder problem ( in @xmath168 dimensions ) . as an example",
    ", the union of @xmath1 congruent infinite cylinders in 3-space is known to have near - quadratic complexity  @xcite , but it is still an open problem whether its complement can be decomposed into a near - quadratic number of elementary cells .",
    "[ [ partition - theorem - for - shallow - semi - algebraic - ranges . ] ] partition theorem for shallow semi - algebraic ranges .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we next apply the new shallow cutting lemma to construct an elementary cell partition of a given input point set @xmath0 , with respect to a specific set @xmath44 of ranges .",
    "this is done in a fairly similar way to that in  @xcite ( see also @xcite ) .",
    "a major difference in handling the semi - algebraic case is the construction of a set @xmath44 of ranges that will be ( a ) small enough , and ( b ) representative of all shallow ( or empty ) ranges , in a sense discussed in detail below .",
    "the method given in @xcite does not work in the general semi - algebraic case , and different , sometimes ad - hoc approaches need to be taken .",
    "the following theorem summarizes the main part of the construction ( except for the construction of @xmath44 ) .",
    "[ theo : part ] let @xmath0 be a set of @xmath1 points in @xmath2 , let @xmath74 be a family of semi - algebraic ranges of constant description complexity , and let @xmath40 be fixed .",
    "let @xmath44 be another finite collection ( not necessarily a subset of @xmath74 ) of semi - algebraic ranges of constant description complexity with the following properties : ( i ) the ranges in @xmath44 are all @xmath45-shallow .",
    "( ii ) the complement of the union of any @xmath38 ranges of @xmath44 can be decomposed into at most @xmath141 elementary cells , for any @xmath38 .",
    "( iii ) any @xmath45-shallow range @xmath169 can be covered by the union of at most @xmath79 ranges of @xmath44 , where @xmath79 is a constant .",
    "+ then there exists an elementary cell partition @xmath47 of @xmath0 , of size @xmath46 , into subsets of size roughly @xmath39 , such that the crossing number of any @xmath45-shallow range in @xmath74 with the cells of @xmath47 is either @xmath170 , if @xmath171 , for any fixed @xmath10 , or @xmath172 , otherwise .    the proof , which , again , is similar to those in @xcite , proceeds through the following steps .",
    "we first have :    [ lemma : half_partition ] let @xmath0 be a set of @xmath1 points in @xmath2 , and @xmath173 a parameter .",
    "let @xmath44 be a set of @xmath45-shallow ranges , with the property that the complement of the union of any subset of @xmath38 ranges of @xmath44 can be decomposed into at most @xmath141 elementary cells , for any @xmath38 .",
    "then there exists a subset @xmath174 of at least @xmath175 points and an elementary cell partition @xmath94 for @xmath176 with @xmath177 for all @xmath100 , such that each range of @xmath44 crosses at most @xmath178 cells @xmath96 of @xmath47 .",
    "_ we will inductively construct disjoint sets @xmath179 of size @xmath39 and elementary cells @xmath180 such that @xmath181 for each @xmath100 .",
    "the construction terminates when @xmath182 .",
    "suppose that @xmath183 have already been constructed , and set @xmath184 .",
    "we construct @xmath97 as follows : for a range @xmath185 , let @xmath186 denote the number of cells among @xmath187 crossed by @xmath3 .",
    "we define a weighted collection @xmath188 of ranges , so that each range @xmath185 appears with weight ( or multiplicity ) @xmath189 .",
    "we put @xmath190 . by lemma  [ lemma : shallow_cutting ] and by our assumption",
    "that the function @xmath102 is well behaved , there exists a @xmath191-cutting @xmath192 for the weighted collection @xmath188 of size at most @xmath193 , for an appropriate choice of @xmath194 , with the following properties : the union of @xmath192 contains the complement of the union of @xmath44 , and the complement of the union of @xmath192 is contained in the union of @xmath195 ranges of @xmath44 .",
    "since all these ranges are @xmath45-shallow , the number of points of @xmath0 not in the union of @xmath192 is at most @xmath196 , and our assumptions on @xmath102 imply that this is smaller than @xmath197 , if we choose @xmath77 appropriately . since we assume that @xmath198 , it follows that at least @xmath197 points of @xmath199 lie in the union of the at most @xmath193 cells of @xmath192 . by the pigeonhole principle ,",
    "there is a cell @xmath96 of @xmath192 containing at least @xmath39 points of @xmath199 .",
    "we take @xmath97 to be some subset of @xmath200 of size exactly @xmath39 , and make @xmath96 the cell in the partition which contains @xmath97 .",
    "we next establish the asserted bound on the crossing numbers between the ranges of @xmath44 and the elementary cells @xmath201 , in the following standard manner .",
    "the final weight @xmath202 of a range @xmath185 with crossing number @xmath48 ( with respect to the final partition ) is @xmath203 . on the other hand , each newly added cell @xmath96",
    "is crossed by ranges of @xmath44 of total weight @xmath204 , because @xmath96 is an elementary cell of the corresponding weighted @xmath191-cutting @xmath192 .",
    "the weight of each of these crossing ranges is doubled at the @xmath100-th step , and the weight of all the other ranges remains unchanged .",
    "thus @xmath205 .",
    "hence , for each range @xmath206 we have @xmath207 .",
    "@xmath140    [ [ discussion . ] ] discussion .",
    "+ + + + + + + + + + +    the limitation of lemma  [ lemma : half_partition ] is that the bound that it derives ( a ) applies only to ranges in @xmath44 , and ( b ) includes the term @xmath208 .",
    "an ingenious component of the analysis in @xcite overcomes both problems , by choosing a test set @xmath44 of ranges whose size is only polynomial in @xmath40 ( and , in particular , is independent of @xmath1 ) , which is nevertheless sufficiently representative of all shallow ranges , in the sense that the crossing number of any @xmath45-shallow range is @xmath209 .",
    "this implies that lemma  [ lemma : half_partition ] holds for _ all _ shallow ranges , with the stronger bound which does not involve @xmath208 .",
    "unfortunately , the technique of @xcite does not extend to the case of semi - algebraic ranges , as it crucially relies on the linearity of the ranges .",
    "the following lemma gives a sufficient condition for a test set @xmath44 to be representative of the relevant shallow ranges , in the sense that @xmath44 satisfies the assumptions made in theorem  [ theo : part ] .",
    "that is :    [ lemma : const_cover ] let @xmath0 be a set of @xmath1 points in @xmath2 , and let @xmath74 be a family of semi - algebraic ranges with constant description complexity . consider an elementary - cell partition @xmath210 of @xmath0 such that @xmath211 for each @xmath100",
    "let @xmath44 be a finite set of @xmath45-shallow ranges ( not necessarily ranges of @xmath74 ) , so that the maximal crossing number of a range @xmath212 with respect to @xmath47 is @xmath48 .",
    "then , for any range @xmath87 which is contained in the union of at most @xmath79 ranges of @xmath44 ( for some constant @xmath79 ) , the crossing number of @xmath50 is at most @xmath213 .",
    "_ let @xmath87 be a range for which there exist @xmath79 ranges @xmath214 of @xmath44 such that @xmath215 . then",
    ", if @xmath50 crosses a cell @xmath96 of @xmath47 , then at least one of the covering ranges @xmath216 must either cross @xmath96 or fully contain @xmath96 .",
    "the number of cells of @xmath47 that can be crossed by any single @xmath216 is at most @xmath48 , and each @xmath216 can fully contain at most one cell of @xmath47 ( because @xmath216 is @xmath45-shallow ) . in the construction of the partition , we can even rule out the possibility that a range @xmath216 fully contains a cell of @xmath47 .",
    "this however has no effect on the asymptotic bounds that the analysis derives . ] hence , the overall number of cells of @xmath47 that @xmath50 can cross is at most @xmath213 , as asserted .",
    "@xmath140    [ [ proof - of - theoremtheopart . ] ] proof of theorem  [ theo : part ] .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + +    apply lemma  [ lemma : half_partition ] to the input set @xmath217 , with parameter @xmath218 .",
    "this yields an elementary - cell partition @xmath219 for ( at least ) half of the points of @xmath220 , which satisfies the properties of that lemma .",
    "let @xmath221 denote the set of the remaining points of @xmath220 , and set @xmath222 .",
    "apply lemma  [ lemma : half_partition ] again to @xmath221 with parameter @xmath223 , obtaining an elementary cell partition @xmath224 for ( at least ) half of the points of @xmath221 .",
    "we iterate this process @xmath225 times , until the set @xmath226 has fewer than @xmath39 points .",
    "we take @xmath47 to be the union of all the elementary - cell partitions @xmath227 formed so far , together with one large cell containing all the remaining points of @xmath226 .",
    "the resulting elementary - cell partition of @xmath0 consists of at most @xmath228 subsets , each of size at most @xmath39 .",
    "the crossing number of a range in @xmath44 is , by lemma  [ lemma : half_partition ] , @xmath229 our assumptions on @xmath142 imply that if @xmath171 , for any fixed @xmath10 , the first terms add up to @xmath230 ; otherwise we can bound their sum by @xmath231 .",
    "hence , by the properties of @xmath44 and by lemma  [ lemma : const_cover ] , the crossing number of any empty range is also @xmath232 or @xmath233 , respectively .",
    "@xmath140    [ [ partition - trees - and - reporting - or - emptiness - searching . ] ] partition trees and reporting or emptiness searching . + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    as in the classical works on range searching  @xcite , we apply theorem  [ theo : part ] recursively , and obtain a _ partition tree _",
    "@xmath234 , where each node @xmath235 of @xmath234 stores a subset @xmath236 of @xmath0 and an elementary cell @xmath237 enclosing @xmath236 .",
    "the children of a node @xmath235 are obtained from an elementary cell partition of @xmath236each of them stores one of the resulting subsets of @xmath236 and its enclosing cell . at the leaves ,",
    "the size of the subset that is stored is @xmath46 .    testing a range @xmath50 for emptiness",
    "is done by searching with @xmath50 in @xmath234 . at each visited node @xmath235 , where @xmath238 , we test whether @xmath239 , in which case @xmath50 is not empty . otherwise , we find the children of @xmath235 whose cells are intersected by @xmath50 . if there are too many of them we know that @xmath50 is not empty .",
    "otherwise , we recurse at each child .    reporting is performed in a similar manner . if @xmath240 we output all of @xmath237 .",
    "otherwise , we find the children of @xmath235 whose cells are intersected by @xmath50 . if there are too many of them we know that @xmath50 is not @xmath241-shallow ( with respect to @xmath236 ) , so , if @xmath40 is a constant , we can afford to check every element of @xmath236 for containment in @xmath50 , and output those points that do lie in @xmath50 .",
    "if there are not too many children , we recurse in each of them .",
    "the efficiency of the search depends on the function @xmath141 .",
    "if @xmath242 then an emptiness query takes @xmath243 time , and a reporting query takes @xmath244 , where @xmath77 is the output size . thus making @xmath142 ( i.e. , @xmath27 ) small is the main challenge in this technique .    [ [ a - general - recipe - for - constructing - good - test - sets . ] ] a general recipe for constructing good test sets .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    let @xmath74 be the given collection of semi - algebraic ranges of constant description complexity .",
    "as above , we assume that each range @xmath87 has @xmath77 degrees of freedom , for some constant parameter @xmath77 , so it can be represented as a point @xmath245 in a @xmath77-dimensional parametric space , which , for convenience , we denote as @xmath246 .",
    "each input point @xmath247 is mapped to a region @xmath248 , which is the locus of all points representing ranges which contain @xmath75 .",
    "we fix a parameter @xmath249 , and choose a random sample @xmath114 of @xmath250 points of @xmath0 , where @xmath251 is a sufficiently large constant .",
    "we form the set @xmath252 , construct the arrangement @xmath253 , and let @xmath254 denote the region consisting of all points contained in at most @xmath27 ranges of @xmath255 , where @xmath256 and @xmath257 is an absolute constant that we will fix later .",
    "we decompose @xmath258 into elementary cells , using , e.g. , vertical decomposition @xcite . in the worst case",
    ", we get @xmath259 elementary cells @xcite .",
    "is sufficient for our purpose . ]",
    "let @xmath147 be one of these cells .",
    "we associate with @xmath147 a generalized range @xmath260 in @xmath2 , which is the union @xmath261 . since @xmath147 has constant description complexity , as do the ranges of @xmath74 , it is easy to show that @xmath260 is also a semi - algebraic set of constant description complexity ( see  @xcite ) .",
    "we define the test set @xmath44 to consist of all the generalized ranges @xmath260 , over all cells @xmath147 in the decomposition of @xmath258 , and claim that , with high probability ( and with an appropriate choice of @xmath257 ) , @xmath44 is a good test set , in the following three aspects .",
    "\\(i ) * compactness .",
    "* @xmath262 ; that is , the size of @xmath44 is polynomial in @xmath40 and independent of @xmath1 .",
    "\\(ii ) * shallowness . *",
    "each range @xmath260 in @xmath44 is @xmath263-shallow with respect to @xmath0 , for some constant parameter @xmath136 .",
    "\\(iii ) * containment .",
    "* every _ @xmath45-shallow _ range @xmath87 is contained in a single range @xmath260 of @xmath44 .",
    "property ( i ) is obvious .",
    "consider the range space @xmath264 , where @xmath265 consists of all generalized ranges @xmath260 , over all elementary cells @xmath147 of the form arising in the above vertical decomposition .",
    "it is a fairly easy exercise to show that @xmath264 also has finite vc - dimension .",
    "see , e.g. , @xcite . by theorem  [ epsshallow ] ,",
    "if @xmath251 is a sufficiently large constant ( proportional to the vc - dimension of @xmath264 ) then @xmath114 is a shallow @xmath84-net for both range spaces @xmath266 and @xmath264 , with high probability , so we assume that @xmath114 is indeed such a shallow @xmath84-net .",
    "let @xmath267 .",
    "note that any point @xmath268 in @xmath260 lies in a range @xmath87 with @xmath269 . by definition",
    ", @xmath245 also belongs to @xmath248 , and so @xmath248 crosses or fully contains @xmath147 . since @xmath147 is @xmath270-shallow in @xmath253",
    ", it is fully contained in at most @xmath271 regions @xmath248 , for @xmath272 ( and is not crossed by any such region ) .",
    "hence @xmath273 , so , since @xmath114 is a shallow @xmath84-net for @xmath264 , we have @xmath274 , so @xmath260 is @xmath275-shallow , which establishes ( ii ) .    for ( iii ) , let @xmath169 be an @xmath45-shallow range . since @xmath114 is a shallow @xmath84-net for @xmath266 , and @xmath276 , we have @xmath277 . hence , with @xmath278 , @xmath279",
    ", so there is a cell @xmath147 of the decomposition which contains @xmath50 , which , by construction , implies that @xmath280 , thus establishing ( iii ) .    to make @xmath44 a really good test set",
    ", we also need the following fourth property :    \\(iv ) * efficiency .",
    "* there exists a good bound on the associated function @xmath141 , bounding the size of a decomposition of the complement of the union of any @xmath38 ranges of @xmath44 .",
    "the potentially rather complex shape of these generalized ranges makes it harder to obtain , in general , a good bound on @xmath142 .    in what follows",
    ", we manage to use this general recipe in two of our four applications ( ray shooting amid balls and range fullness searching ) , with good bounds on the corresponding functions @xmath281 . in two other planar applications ( range emptiness searching with fat triangles and with circular caps ) , we abandon the general technique , and construct ad hoc good test sets .",
    "[ [ remark ] ] remark : + + + + + + +    in the preceding construction , we wanted to make sure that every @xmath45-shallow range @xmath169 is covered by a range of @xmath44 .",
    "if we only need this property for _ empty _ ranges @xmath50 ( which is the case for emptiness testing ) , it suffices to consider only the @xmath282-level of @xmath253 , i.e. , the complement of the union of @xmath255 .",
    "other than this simplification , the construction proceeds as above .",
    "let @xmath0 be a set of @xmath1 points in 3-space , and let @xmath74 be a set of convex ranges of constant description complexity .",
    "we wish to preprocess @xmath0 in near - linear time into a data structure of linear size , so that , given a query range @xmath169 , we can efficiently determine whether @xmath50 contains all the points of @xmath0 .",
    "alternatively , we want to report all the points of @xmath0 that lie outside @xmath50 .",
    "( this is clearly a special case of range emptiness searching or range reporting , if one considers the complements of the ranges in @xmath74 . ) for simplicity , we only focus on the range fullness problem ; the extension to reporting `` outliers '' is similar to the standard treatment of reporting queries , as discussed earlier .",
    "we present a solution to this problem , with @xmath16 query time , thereby improving over the best known general bound of @xmath13 , given in @xcite , which applies to any range searching ( e.g. , range counting ) with semi - algebraic sets ( of constant description complexity ) in @xmath6 .    to apply our technique to this problem we first need to build a good test set . since fullness searching is complementary to emptiness searching , we need a property complementary to that assumed in theorem  [ theo : part ] ( see also lemma  [ lemma : const_cover ] ) .",
    "in fact , we will enforce the property that every full range @xmath50 fully contains a single range of @xmath44 , which is `` almost full '' ( contains at least @xmath283 points of @xmath0 ) .",
    "as above , assuming the ranges of @xmath74 to have @xmath77 degrees of freedom , we map each range @xmath87 to a point @xmath245 in @xmath246 .",
    "a point @xmath284 is mapped to a region @xmath248 which is the locus of all the points @xmath245 that correspond to ranges @xmath50 which contain @xmath75 .",
    "we fix @xmath83 , take a random sample @xmath114 of @xmath285 points of @xmath0 ( with a sufficiently large constant of proportionality ) , construct the _ intersection _ @xmath286 , and decompose it into elementary cells . for each resulting cell @xmath3 , let @xmath287 denote the _ intersection _ @xmath288 . as above , since @xmath3 has constant description complexity , @xmath287 is a semi - algebraic set of constant description complexity .",
    "note that , since the ranges in @xmath74 are convex , each range @xmath287 is also convex ( albeit of potentially more complex shape than that of the original ranges ) .",
    "define the test set @xmath44 to consist of all the generalized ranges @xmath287 , over all cells @xmath3 in the decomposition of @xmath289 .",
    "we argue that @xmath44 satisfies all four properties required from a good test set : ( i ) compactness : as above , the size of @xmath44 is polynomial in @xmath40 ( it is at most @xmath259 ) .",
    "( ii ) shallowness ( or , rather , `` almost fullness '' ) : for each cell @xmath3 and any @xmath87 with @xmath290 , @xmath245 lies in all the sets @xmath248 , for @xmath291 , and thus @xmath292 . by construction , we also have @xmath293 .",
    "apply the @xmath12-net theory  @xcite to the range space @xmath294 , where the ranges of @xmath295 are complements of ranges of the same form as the ranges @xmath287 . since @xmath296 for each cell @xmath3 in the decomposition",
    ", we have , with high probability , the property that for each cell @xmath3 of @xmath289 , @xmath287 contains at least @xmath283 points of @xmath0 , so it is an `` almost full '' range .",
    "( iii ) containment : let @xmath87 be a full range . then , in particular , @xmath292 . then @xmath297 , and let @xmath3 be the cell of @xmath289 containing @xmath245 .",
    "then , by construction , @xmath298 .",
    "( iv ) efficiency : finally , we show that the complexity of a decomposition of the intersection of any @xmath38 ranges in @xmath44 , is @xmath299 , so @xmath300 .    [",
    "claim : decomposition_of_convex ] let @xmath44 be a set of convex `` almost full '' ranges , each containing at least @xmath301 points of @xmath0 . the intersection , @xmath56 , of any @xmath38 ranges @xmath302",
    "can be decomposed into @xmath299 elementary cells .",
    "_ since all ranges in @xmath44 are convex , @xmath56 is a convex set too .",
    "assume , for simplicity of presentation , that @xmath56 is nonempty and has nonempty interior , and fix a point @xmath303 in that interior .",
    "we can regard the boundary of each @xmath304 as the graph of a bivariate function @xmath305 in spherical coordinates about @xmath303 .",
    "then @xmath306 is the graph of the lower envelopes of these functions .",
    "since the @xmath304 s have constant description complexity , ( the graph of ) each @xmath307 is also a semi - algebraic set of constant description complexity .",
    "hence the combinatorial complexity of @xmath306 is @xmath299 @xcite .",
    "moreover , since @xmath306 is 2-dimensional , we can partition it into @xmath299 trapezoidal - like elementary cells , using a variant of the vertical decomposition technique , and then extend each such cell @xmath308 to a 3-dimensional cone - like cell @xmath147 , which is the union of all segments connecting @xmath303 to the points of @xmath308 .",
    "the resulting cells @xmath147 constitute a decomposition of @xmath56 into @xmath299 elementary cells , as claimed .",
    "@xmath140    using the machinery developed in the preceding section , we therefore obtain the following result .",
    "[ thm : full ] let @xmath0 be a set of @xmath1 points in @xmath6 , and let @xmath74 be a family of convex ranges of constant description complexity",
    ". then one can construct , in near linear time , a data structure of linear size so that , for any range @xmath87 , it can determine , in @xmath16 time , whether @xmath50 is full .    [ [ reporting - outliers . ] ] reporting outliers .",
    "+ + + + + + + + + + + + + + + + + + +    to extend the above approach to the problem of reporting outliers , we apply a construction similar to that in the `` general recipe '' presented above .",
    "that is , we take the @xmath271 deepest levels of @xmath309 , for an appropriate constant @xmath257 , decompose them into elementary cells , and construct a generalized range @xmath287 for each of these cells @xmath3 .",
    "the general machinery given above implies the following result :    [ thm : full_report ]",
    "let @xmath0 be a set of @xmath1 points in @xmath6 , and let @xmath74 be a family of convex ranges of constant description complexity .",
    "then one can construct , in near linear time , a data structure of linear size so that , for any range @xmath87 , it can report the points of @xmath0 in the complement of @xmath50 , in @xmath60 time , where @xmath27 is the query output size .      a useful application of the data structure of theorem  [ thm : full ] is to _ farthest point queries_. in such a problem we are given a set @xmath0 of @xmath1 points in @xmath6 , and wish to preprocess it , in near - linear time , into a data structure of linear size , so that , given a convex query object @xmath303 ( from some fixed class of objects with constant description complexity ) , we can efficiently find the point of @xmath0 farthest from @xmath303 .",
    "we solve this problem using parametric searching @xcite .",
    "the corresponding decision problem is : given the query object @xmath303 and a distance @xmath310 , determine whether the minkowski sum @xmath311 is full , where @xmath312 is the ball of radius @xmath310 centered at the origin .",
    "the smallest @xmath310 with this property is the distance to the farthest point from @xmath303 . with an appropriate small - depth parallel implementation of this decision problem ,",
    "the parametric searching also takes time @xmath16 . reporting the @xmath27 farthest points from @xmath303 , for any parameter @xmath27 ,",
    "can be done in @xmath60 time , using a simple variant of this technique .",
    "let @xmath0 be a set of @xmath1 points in @xmath2 .",
    "we wish to find the triangle whose vertices belong to @xmath0 and whose area ( respectively , perimeter , height ) is maximal .",
    "this problem is a useful subroutine in path approximation algorithms ; see @xcite .",
    "daescu and serfling @xcite gave an @xmath59-algorithm for the 3-dimensional largest - area triangle .",
    "in @xmath313 dimensions , the running time is @xmath314 .    in @xmath6 , our technique , without any additional enhancements , yields the improved bound @xmath315 , using the following straightforward procedure .",
    "for each pair of points @xmath316 , we find the farthest point @xmath317 from the line @xmath318 , compute the area of @xmath319 , and output the largest - area triangle among those triangles .",
    "the procedure performs farthest - point queries from @xmath320 lines , for a total cost of @xmath315 , as claimed .",
    "we can improve this solution , using the following standard decomposition technique , to an algorithm with running time @xmath17 .",
    "first , the approach just described performs @xmath321 farthest - point queries on a set of @xmath114 points in time @xmath322 , where the second term is the preprocessing cost of preparing the data structure .    before continuing , we note the following technical issue .",
    "recall that we find the farthest point from a query line @xmath15 by drawing a cylinder @xmath323 around @xmath15 , whose radius @xmath310 is the smallest ( unknown ) radius for which @xmath323 contains @xmath0 .",
    "the concrete value of @xmath310 is found using parametric searching . in the approach that we follow now",
    ", we will execute in parallel @xmath320 different queries , each with its own @xmath310 , so care has to be taken when running the parametric search with this multitude of different unknown values of @xmath310 .",
    "while there are several alternative solutions to this problem , we use the following one , which seems the cleanest .",
    "let @xmath324 be a fixed parameter .",
    "for each pair @xmath325 of distinct points of @xmath0 , let @xmath326 denote the cylinder whose axis passes through @xmath327 and @xmath328 and whose radius is @xmath329 . in the decision procedure ,",
    "we specify the value of @xmath330 , and perform @xmath320 range fullness queries with the cylinders @xmath326 .",
    "if all of them are found to be full , then @xmath331 , where @xmath332 is the ( unknown ) maximal area of a triangle spanned by @xmath0 ; otherwise @xmath333 .",
    "( with a somewhat finer implementation , we can also distinguish between the cases @xmath334 and @xmath335 ; we omit the details of this refinement . )    to implement the decision procedure , we apply a duality transform , where each cylinder @xmath58 in 3-space is mapped to a point @xmath336 , where @xmath337 is some parametrization of the axis of @xmath58 and @xmath310 is its radius . in this dual parametric 5-space , a point @xmath284 is mapped to a surface @xmath338 , which is the locus of all ( points representing ) cylinders which contain @xmath75 on their boundary . note that the portion of space below ( resp . , above ) @xmath338 , in the @xmath310-direction , consists of points dual to cylinders which do not contain ( resp . , contain ) @xmath75 .",
    "let @xmath339 .",
    "fix some sufficiently large but constant parameter @xmath340 , and construct a @xmath341-cutting @xmath85 of @xmath342 , using the vertical decomposition of a random sample of @xmath343 surfaces of @xmath344 ( see , e.g. ,  @xcite ) . as follows from @xcite , the combinatorial complexity of @xmath85",
    "is @xmath345 .",
    "we distribute the @xmath320 points dual to the query cylinders among the cells of @xmath85 , in brute force , and also find , in equally brute force , for each cell @xmath147 the subset @xmath346 of surfaces which cross @xmath147 .",
    "we ignore cells which fully lie below some surface of @xmath344 , because cylinders whose dual points fall in such a cell can not be full ( the decision algorithm stops as soon as such a point ( cylinder ) is detected ) . for each of the remaining cells @xmath147 , we repeat this procedure with the subset of the points dual to the surfaces in @xmath346 and with the subset of cylinders whose",
    "dual points lie in @xmath147 .",
    "we keep iterating in this manner until we reach cuttings whose cells are crossed by at most @xmath39 dual surfaces , where @xmath40 is some ( non - constant ) parameter that we will shortly fix .",
    "as is easily checked , the overall number of cells in these cuttings is @xmath347 .",
    "we then run the preceding weaker procedure on each of the resulting cells @xmath147 , with the set @xmath348 of points dual to the surfaces which cross @xmath147 and with the set @xmath349 of cylinders whose dual points lie in @xmath147 . letting @xmath350 denote the number of these cylinders , the overall cost of the second phase of the procedure is @xmath351 since @xmath340 is a constant , the overall cost of the first phase is easily seen to be proportional to the overall size of the resulting subproblems , which is @xmath352 .",
    "overall , the cost is thus @xmath353 choosing @xmath354 , this becomes @xmath17 .    running a generic version of this decision procedure in parallel",
    "is fairly straightforward .",
    "the cuttings themselves depend only on the dual surfaces , which do not depend on @xmath332 , so we can construct them in a concrete , non - parametric fashion . locating the points dual to the query cylinders can be done in parallel , and , since @xmath340 is a constant , this takes constant parallel depth for each of the logarithmically many levels of cuttings",
    ". the second phase can also be executed in parallel in an obvious manner . omitting the further easy details ,",
    "we conclude that the overall algorithm also takes @xmath17 time .",
    "[ [ largest - perimeter - triangle . ] ] largest - perimeter triangle .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + +    the above technique can be adapted to yield efficient solutions of several problems of a similar flavor .",
    "for example , consider the problem of computing the largest - perimeter triangle among those spanned by a set @xmath0 of @xmath1 points in @xmath6 .",
    "here , for each pair @xmath325 of points of @xmath0 , and for a specified perimeter @xmath355 , we construct the ellipsoid of revolution @xmath356 , whose boundary is the locus of all points @xmath65 satisfying @xmath357 .",
    "( here , of course , we only consider pairs @xmath325 with @xmath358 . )",
    "we now run @xmath320 range fullness queries with these ellipsoids , and report that @xmath359 if at least one of these ellipsoids in not full , or @xmath360 otherwise , where @xmath361 is the largest perimeter .",
    "the efficient implementation of this procedure is carried out similar to the preceding algorithm , except that here the dual representation of our ellipsoids require six degrees of freedom , to specify the foci @xmath327 and @xmath328 . unlike the previous case , the dual surfaces @xmath338 do depend on @xmath355 , so , in the generic implementation of the decision procedure we also need to construct the various @xmath341-cuttings in a generic , parallel manner . if we add @xmath355 as a seventh degree of freedom , but then the overall performance of the algorithm deteriorates .",
    "] however , since @xmath340 is a constant , this is easy to do in constant parallel depth per cutting .",
    "a @xmath341-cutting in @xmath362 has complexity @xmath363 @xcite .",
    "a modified version of the preceding analysis then yields :    the largest - perimeter triangle among those spanned by a set of @xmath1 points in @xmath6 can be computed in @xmath364 time .",
    "[ [ largest - height - triangle . ] ] largest - height triangle .",
    "+ + + + + + + + + + + + + + + + + + + + + + + +    in this variant , we wish to compute the triangle with largest height among those determined by a set @xmath0 of @xmath1 points in @xmath6 . here , for each pair @xmath325 of points of @xmath0 , and for a specified height @xmath365 , we construct the cylinder @xmath366 , whose axis passes through @xmath327 and @xmath328 and whose radius is @xmath365 .",
    "we run @xmath320 range fullness queries with these cylinders , and report that @xmath367 if at least one of these cylinders in not full , or @xmath368 otherwise , where @xmath369 is the desired largest height .",
    "the efficient implementation of this procedure is carried out as above , except that here the dual representation of these cylinders require only four degrees of freedom , once @xmath365 is specified . as in the preceding case , here too the surfaces of @xmath344 also depend on @xmath365 , so we need a generic parallel procedure for constructing @xmath341-cuttings for these surfaces , which however is not difficult to achieve , since @xmath340 is a constant .",
    "we omit the simple routine details .",
    "since a @xmath341-cutting in @xmath51 has complexity @xmath370 @xcite , a modified version of the preceding analysis then yields :    the largest - height triangle among those spanned by a set of @xmath1 points in @xmath6 can be computed in @xmath371 time .",
    "[ [ further - extensions . ] ] further extensions .",
    "+ + + + + + + + + + + + + + + + + + +    we can extend this machinery to higher dimensions , although its performance deteriorates as the dimension grows .",
    "the range fullness problem in @xmath2 , for @xmath162 , can be handled in much the same way as in the 3-dimensional case .",
    "when extending claim  [ claim : decomposition_of_convex ] , we have an intersection of @xmath38 convex sets of constant description complexity in @xmath2 , and we can regard the boundary of the intersection as the lower envelope of @xmath38 @xmath159-variate functions of constant description complexity , each representing the boundary of one of the input convex sets , in spherical coordinates about some fixed point in the intersection . the complexity of the lower envelope is @xmath372 @xcite .",
    "however , we need to decompose the region below the envelope into elementary cells , and , as already noted , the only known general - purpose technique for doing so is to decompose the entire arrangement of the graphs of the @xmath38 boundary functions , and select the cells below the lower envelope .",
    "the complexity of such a decomposition is @xmath373 @xcite .",
    "this implies that @xmath374 .",
    "the rest of the analysis , including the construction of a good test set , is done in essentially the same manner .",
    "hence , using the machinery of the previous section , we obtain :    [ fulld ] let @xmath0 be a set of @xmath1 points in @xmath2 , for @xmath162 , and let @xmath74 be a family of convex ranges of constant description complexity",
    ". then one can construct , in near linear time , a data structure of linear size so that , for any range @xmath87 , it can determine , in @xmath375 , whether @xmath50 is full .",
    "[ [ finding - the - largest - area - triangle - in - realsd . ] ] finding the largest - area triangle in @xmath2 .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    let @xmath0 be a set of @xmath1 points in @xmath2 , for @xmath162 , and consider the problem of finding the largest - area triangle spanned by @xmath0 .",
    "we apply the same method as in the 3-dimensional case , whose main component is a decision procedure which tests @xmath320 cylinders for fullness .",
    "a cylinder ( with a line as an axis ) in @xmath2 has @xmath376 degrees of freedom , so the dual representation of our @xmath320 cylinders is as points in @xmath377 .",
    "the best known bound on the complexity of a @xmath84-cutting in this space is @xmath378 . applying this bound and the bound in theorem  [ fulld ] ,",
    "the overall cost of the decision procedure is @xmath379 optimizing the value of @xmath40 , and applying parametric searching , we get an algorithm for the maximum - area triangle in @xmath2 with running time @xmath380 we can extend the other problems ( largest - perimeter or largest - height triangles ) in a similar manner , and can also obtain algorithms for solving higher - dimensional variants , such as computing the largest - volume tetrahedron or higher - dimensional simplices .",
    "we omit the straightforward but tedious analysis , and the resulting cumbersome - looking bounds .",
    "let @xmath381 be a set of @xmath1 balls in 3-space . we show how to preprocess @xmath381 in near - linear time into a data structure of linear size , so that , given a query ray @xmath310 , the first ball that @xmath310 hits can be computed in @xmath13 time , improving the general bound @xmath14 mentioned in the introduction .",
    "as already noted , we use the parametric - searching technique of agarwal and matouek  @xcite , which reduces the problem to that of efficiently testing whether a query segment @xmath382 intersects any ball in @xmath381 , where @xmath65 is the origin of @xmath310 and @xmath383 is a parametric point along @xmath310 .",
    "[ [ parametric - representation - of - balls - and - segments . ] ] parametric representation of balls and segments .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we move to a parametric 4-dimensional space , in which balls in 3-space are represented by points , so that a ball with center at @xmath52 and radius @xmath40 is mapped to the point @xmath53 . a segment @xmath33 , or for that matter , any closed nonempty set @xmath384 of constant description complexity , is mapped to a surface @xmath55 , which is the locus of all points representing balls that touch @xmath56 but are openly disjoint from @xmath56 . by construction",
    ", @xmath55 is the graph of a totally defined continuous trivariate function @xmath385 , which is semi - algebraic of constant description complexity .",
    "moreover , points below ( resp . , above ) @xmath55 represent balls which are disjoint from @xmath56 ( resp . , intersect @xmath56 ) .",
    "moreover , for any such set @xmath56 , @xmath386 is , by definition , the ( euclidean ) distance of @xmath65 from @xmath56 .",
    "hence , given a collection @xmath387 of @xmath38 sets , the minimization diagram of the surfaces @xmath388 ( that is , the projection onto the 3-space @xmath389 of the lower envelope of these surfaces ) is the nearest - neighbor voronoi diagram of @xmath390 .",
    "we use this property later on , in deriving a sharp bound on the resulting function @xmath281 .",
    "[ [ building - a - test - set - for - segment - emptiness . ] ] building a test set for segment emptiness .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    here we use the general recipe for constructing good test sets , which covers each empty segment @xmath33 by a fairly complex `` canonical '' empty region @xmath56 , which has nonetheless constant description complexity . in parametric 4-space , each such region @xmath56",
    "is mapped to the upper halfspace above the corresponding surface @xmath55 ; this is the set of all balls that intersect @xmath56 .",
    "the complement of the union of @xmath38 such ranges is the portion of 4-space below the lower envelope of the corresponding surfaces @xmath391 . using the connection between this envelope and the voronoi diagram of the @xmath392 s",
    ", we are able to decompose ( the diagram and thus ) the complement of the union into @xmath393 elementary cells .    in more detail",
    ", the construction proceeds as follows .",
    "we start by choosing a random sample @xmath114 of @xmath285 balls of @xmath381 , to construct a test set @xmath44 for empty segment ranges , with respect to @xmath114 . while we do not have a clean , explicit geometric definition of these ranges , they will satisfy , as above , all the four requirements from a good test set .",
    "also , we spell out the adaptation of the general recipe to the present scenario , to help the reader see through one concrete application of the general recipe .",
    "specifically , we move to a dual space , in which segments in 3-space are represented as points .",
    "segments in 3-space have six degrees of freedom ; for example , we can represent a segment by the coordinates of its two endpoints . the dual space is therefore 6-dimensional .",
    "each ball @xmath394 is mapped to a surface @xmath395 , which is the locus of all points representing segments which touch @xmath396 but do not penetrate into its interior ; that is , either they are tangent to @xmath397 , at a point in their relative interior , or they have an endpoint on @xmath396 but are openly disjoint from @xmath397 .",
    "let @xmath255 denote the collection of the surfaces dual to the balls of @xmath114 .",
    "construct a @xmath84-cutting of @xmath253 , which consists of @xmath398 elementary cells @xcite .",
    "each cell @xmath147 has the property that all points in @xmath147 represent segments which meet a fixed set of balls from among the balls in @xmath114 and avoid all other balls of @xmath114 ; the set depends only on @xmath147 .    for each cell",
    "@xmath147 whose corresponding set of balls is _ empty _ , we define @xmath399 to be the union , in 3-space , of all segments @xmath33 whose dual points lie in @xmath147 . since @xmath147 is an elementary cell , @xmath399 is a semi - algebraic set of constant description complexity ( see , e.g. , @xcite ) .",
    "moreover , @xmath399 is an _",
    "@xmath114-empty _ region , in the sense that it is openly disjoint from all the balls in @xmath114 .    since we have to work in parametric 4-space , we map each region @xmath399 of the above kind into a range @xmath260 in 4-space , which is the locus of all ( points representing ) balls which intersect @xmath399 . as discussed above , @xmath260 is the upper halfspace bounded by the graph @xmath400 of the distance function from points in @xmath6 to @xmath399 .",
    "we define the desired test set @xmath44 to consist of all the ranges @xmath260 , as just defined , and argue that @xmath44 indeed satisfies all four properties required from a good test set : ( a ) compactness : @xmath401 , so its size is small .",
    "( b ) shallowness : with high probability , each range in @xmath44 is @xmath45-shallow , since it does not contain any point representing a ball in @xmath114 ( and we assume that the sample @xmath114 does indeed have this property , which makes all the ranges in @xmath44 @xmath45-shallow ) .",
    "( c ) containment : each empty segment is also @xmath114-empty , so its dual point lies in some cell @xmath147 of the cutting , whose associated subset of balls is empty . by construction , we have @xmath402 .",
    "that is , any ball intersecting @xmath33 also intersects @xmath399 , so the range in 4-space that @xmath33 defines is contained in @xmath260 , i.e. , in a single range of @xmath44 .",
    "( d ) efficiency : the complement of the union of any @xmath38 ranges in @xmath44 can be decomposed into @xmath403 elementary cells .",
    "the proof of ( d ) proceeds as follows .",
    "the complement of the union of @xmath38 ranges , @xmath404 , is the region below the lower envelope of the corresponding surfaces @xmath405 . to decompose this region",
    ", it suffices to produce a decomposition of the 3-dimensional minimization diagram of these surfaces , and extend each of the resulting cells into a semi - unbounded vertical prism , whose `` ceiling '' lies on the envelope .",
    "the combinatorial complexity of the minimization diagram of a collection @xmath406 of @xmath38 trivariate functions of constant description complexity is are shaped , nor how `` wildly '' they can intersect one another , as long as each of them has constant description complexity . ] @xmath403  @xcite . moreover , as noted above , the minimization diagram is the euclidean nearest - neighbor voronoi diagram of @xmath390 .",
    "we can decompose each cell @xmath407 of the diagram ( or , more precisely , the portion of the cell outside the union of the @xmath408 s ) using its _ star - shapedness _ with respect to its `` site '' @xmath408 ; that is , for any point @xmath409 , the segment connecting @xmath75 to its nearest point on @xmath408 is fully contained in @xmath410 . as is easy to verify",
    ", this property holds regardless of the shape , or intersection pattern , of the regions in @xmath390 .",
    "we first decompose the 2-dimensional faces bounding @xmath411 into elementary cells , using , e.g. , an appropriate variant of 2-dimensional vertical decomposition , and then take each such cell @xmath412 and extend it to a cell @xmath413 , which is the union of all segments , each connecting a point in @xmath412 to its nearest point on @xmath408 .",
    "the resulting cells , obtained by applying this decomposition to all cells of the diagram , form a decomposition of the portion of the diagram outside the union of the @xmath408 s , into a total of @xmath403 elementary cells , as desired .",
    "the union of the @xmath408 s themselves , being a subcollection of cells of a 3-dimensional arrangement of @xmath38 regions of constant description complexity , can also be decomposed into @xmath403 cells , using standard results on vertical decomposition in three dimensions  @xcite .",
    "using lemma  [ lemma : const_cover ] and the machinery of section  [ section : range_emptiness ] , in conjunction with the parametric searching technique of @xcite , we thus obtain the following theorem .    [",
    "main_balls ] ray shooting amid @xmath1 balls in 3-space can be performed in @xmath13 time , using a data structure of @xmath7 size , which can be constructed in @xmath8 time .",
    "[ [ remark-1 ] ] remark : + + + + + + +    in the preceding description , we only considered _ empty _ ranges .",
    "if desired , we can extend the analysis to obtain a data structure which also supports `` reporting queries '' , in which we want to report the first @xmath27 balls hit by a query ray .",
    "we omit the details of this straightforward extension .",
    "[ [ fat - triangle - reporting - and - emptiness - searching . ] ] fat triangle reporting and emptiness searching .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    let @xmath414 be a fixed constant , and let @xmath0 be a set of @xmath1 points in the plane .",
    "we wish to preprocess @xmath0 , in @xmath8 time , into a data structure of size @xmath7 , which , given an @xmath19-fat query triangle @xmath50 ( which , as we recall , is a triangle all of whose angles are at least @xmath19 ) , can determine in @xmath21 time whether @xmath415 , or report in @xmath23 time the points of @xmath0 in @xmath50 , where @xmath416 .",
    "to do so , we need to construct a good test set @xmath44 .",
    "we use the following `` canonization '' process ( an ad - hoc process , not following the general recipe of section  [ section : range_emptiness ] ) . as above , we apply the construction to a random sample @xmath114 of @xmath285 points of @xmath0 . for simplicity ,",
    "we first show how to canonize _ empty _ triangles , and then extend the construction to shallow triangles .",
    "( as before , the first part suffices for emptiness searching , whereas the second part is needed for reporting queries . )",
    "let @xmath20 be an @xmath19-fat empty triangle , which is then also @xmath114-empty .",
    "we expand @xmath20 homothetically , keeping one vertex fixed and translating the opposite side away , until it hits a point @xmath417 of @xmath114 .",
    "we then expand the new triangle homothetically from a second vertex , until the opposite side hits a second point @xmath418 of @xmath114 , and then apply a similar expansion from the third vertex , making the third edge of the triangle touch a third point @xmath419 of @xmath114 .",
    "we end up with an @xmath114-empty triangle @xmath420 , homothetic to , and containing , @xmath20 , each of whose sides passes through one of the points @xmath421 .",
    "see figure  [ fig : tricanon1 ] .",
    "( it is possible that some of these expansions never hit a point of @xmath114 , so we may end up with an unbounded wedge or halfplane instead of a triangle .",
    "also , the points @xmath422 need not be distinct . )",
    "tricanon1.pstex_t    let @xmath423 be the set of orientations @xmath424 .",
    "we turn the side containing @xmath417 clockwise and counterclockwise about @xmath417 , keeping its endpoints on the lines containing the other two sides , until we reach an orientation in @xmath423 , or until we hit another point of @xmath114 ( which could also be one of the points @xmath425 ) , whichever comes first .",
    "each of the new sides forms , with the two lines containing the two other sides , a new ( openly ) @xmath114-empty @xmath426-fat triangle ; the union of these two triangles covers @xmath420 .",
    "see figure  [ fig : tricanon2 ] .",
    "tricanon2.pstex_t    for each of the two new triangles , @xmath427 , we apply the same construction , by rotating the side containing @xmath418 clockwise and counterclockwise , thereby obtaining two new triangles whose union covers @xmath427 .",
    "we then apply the same construction to each of the four new triangles , this time rotating about @xmath419 .",
    "overall , we get up to eight new triangles whose union covers @xmath20 .",
    "each of these new triangles is @xmath61-fat , openly @xmath114-empty , and each of its sides either passes through two points of @xmath114 , or passes through one point of @xmath114 and has orientation in @xmath423 . since @xmath428 , it follows that the overall number of these canonical covering triangles is @xmath429 .",
    "( we omit the easy extensions of this step to handle unbounded wedges or halfplanes , or the cases where the points @xmath304 , or some of the newly encountered points , lie at vertices of the respective triangles . )",
    "we take @xmath44 to be the collection of these canonical triangles , and argue that @xmath44 indeed satisfies the properties of a good test set : ( a ) compactness : @xmath430 , so its size is small .",
    "( b ) shallowness : with high probability , each range in @xmath44 is @xmath45-shallow ( and , as usual , we assume that this property does indeed hold ) .",
    "( c ) containment : by construction , each @xmath19-fat empty triangle is contained in the union of at most eight triangles in @xmath44 .",
    "( d ) efficiency : being @xmath61-fat , the union of any @xmath38 triangles in @xmath44 has complexity @xmath63 @xcite , so the associated function @xmath142 satisfies @xmath431 .",
    "this , combined with lemma  [ lemma : const_cover ] and the machinery of section  [ section : range_emptiness ] , lead to the following theorem .",
    "[ theorem : fat_triangles_emptiness ] one can preprocess a set @xmath0 of @xmath1 points in the plane , in near - linear time , into a data structure of linear size , so that , for any query @xmath19-fat triangle @xmath20 , one can determine , in @xmath21 time , whether @xmath432 .    [ [ reporting - points - in - fat - triangles . ] ] reporting points in fat triangles .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we can extend the technique given above to the problem of reporting the points of @xmath0 that lie inside any query fat triangle .",
    "for this , we need to construct a test set that will be good for shallow ranges and not just for empty ones . using theorem  [ epsshallow ] ,",
    "we construct ( by random sampling ) a shallow @xmath84-net @xmath433 of size @xmath285 .",
    "we next canonize every @xmath45-shallow @xmath19-fat triangle @xmath20 , by the same canonization process used above , with respect to the set @xmath114 .",
    "note that each of the resulting canonical triangles contains ( in its interior ) the same subset of @xmath114 as @xmath20 does . by the properties of shallow @xmath84-nets , since @xmath434 ,",
    "we have @xmath435 , so all the resulting canonical triangles are @xmath436-shallow with respect to @xmath114 , for some absolute constant @xmath25 . again , since @xmath114 is a shallow @xmath84-net , all the canonical triangles are @xmath437-shallow with respect to @xmath0 , for another absolute constant @xmath438 .",
    "hence , the resulting collection @xmath44 of canonical triangles is a good test set for all shallow fat triangles , and we can apply the machinery of section  [ section : range_emptiness ] to obtain a data structure of linear size , which can be constructed in near - linear time , and which can perform reporting queries in fat triangles in time @xmath23 , where @xmath27 is the output size of the query .    [ [ range - emptiness - searching - with - semidisks - and - circular - caps . ] ] range emptiness searching with semidisks and circular caps .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the motivation for studying this problem comes from the following problem , addressed in @xcite .",
    "we are given a set @xmath0 of @xmath1 points in the plane , and wish to preprocess it into a data structure of linear size , so that , given a query point @xmath65 and a query line @xmath15 , one can quickly find the point of @xmath0 closest to @xmath65 and lying above @xmath15 .",
    "in the original problem , as formulated in @xcite , one also assumes that @xmath65 lies on @xmath15 , but we will consider , and solve , the more general version of the problem , where @xmath65 also lies above @xmath15 .    the standard approach ( e.g. , as in @xcite ) yields a solution with linear storage and near - linear preprocessing , and query time @xmath16 .",
    "we present a solution with query time @xmath21 .    using parametric searching @xcite",
    ", the problem reduces to that of testing whether the intersection of a disk of radius @xmath310 centered at @xmath65 with the halfplane @xmath439 above @xmath15 is @xmath0-empty .",
    "the resulting range is a circular cap larger than a semidisk ( or exactly a semidisk if @xmath65 lies on @xmath15 ) .",
    "again , the main task is to construct a good test set @xmath44 for such ranges , which we do by using an ad - hoc canonization process , which covers each empty circular cap by @xmath440 canonical caps , which satisfy the properties of a good test set ; in particular , we will have @xmath441 .",
    "( as before , we consider here only the case of emptiness detection , and will consider the reporting problem later . )    to construct a test set @xmath44 we choose a random sample @xmath114 of @xmath285 points of @xmath0 and build a set of canonical empty ranges with respect to @xmath114 .",
    "let @xmath442 be a given circular cap ( larger than a semidisk ) with center @xmath25 , radius @xmath310 , and chord supported by a line @xmath15 .",
    "we first translate @xmath15 in the direction which enlarges the cap , until either its portion within the disk @xmath443 of the cap touches a point of @xmath114 , or @xmath15 leaves @xmath443 .",
    "see figure  [ fig : capcanon1](left ) . in the latter case , @xmath58",
    "is contained in a complete @xmath114-empty disk , and it is fairly easy to show that such a disk is contained in the union of at most three canonical @xmath114-empty disks , each passing through three points of @xmath114 or through two diametrically opposite points of @xmath114 ; there are at most @xmath444 such disks .",
    "capcanon1.pstex_t    suppose then that the new chord ( we continue to denote its line as @xmath15 ) passes through a point @xmath65 of @xmath114 , as in the figure .",
    "let @xmath423 be a set of @xmath440 canonical orientations , uniformly spaced and sufficiently dense along the unit circle , for some small constant value @xmath19 .",
    "rotate @xmath15 about @xmath65 in both clockwise and counterclockwise directions , until we reach one of the two following events : ( i ) the orientation of @xmath15 belongs to @xmath423 ; or ( ii ) the portion of @xmath15 within @xmath443 touches another point of @xmath114 . in either case",
    ", the two new lines , call them @xmath445 , become canonical  there are only @xmath446 such possible lines .",
    "note that our original cap @xmath58 is contained in the union @xmath447 , where @xmath448 and @xmath449 .",
    "moreover , although the new caps need no longer be larger than a semidisk , they are not much smaller  this is an easy exercise in elementary geometry .",
    "see figure  [ fig : capcanon1](right ) .",
    "we next canonize the disk of @xmath58 ( which is also the disk of @xmath450 and @xmath451 ) .",
    "fix one of the new caps , say @xmath450 .",
    "expand @xmath450 from the center @xmath25 , keeping the line @xmath452 fixed , until we hit a point @xmath417 of @xmath114 ( lying in @xmath453 ) .",
    "see figure  [ fig : capcanon2](left ) .",
    "if @xmath25 lies in @xmath453 then we move @xmath25 parallel to @xmath452 in both directions , again keeping @xmath452 itself fixed and keeping the circle pass through @xmath417 , until we obtain two circular caps , each passing through @xmath417 and through a second point of @xmath114 ( if we do not hit a second point , we reach a quadrant , bounded by @xmath15 and by the line orthogonal to @xmath15 through @xmath417 ) .",
    "the union of the two new circular caps covers @xmath450 .",
    "see figure  [ fig : capcanon2](right ) .",
    "capcanon2.pstex_t    if @xmath25 lies in @xmath454 , we move it along the two rays connecting it to the endpoints @xmath455 of the chord defined by @xmath452 . as before ,",
    "each of the motions stops when the circle hits another point of @xmath114 in @xmath453 , or when the motion reaches @xmath456 or @xmath457 .",
    "we claim that @xmath450 is contained in the union of the two resulting caps .",
    "indeed , let @xmath458 and @xmath235 denote the locations of the center at the two stopping placements .",
    "we need to show that , for any point @xmath459 we have either @xmath460 or @xmath461 .",
    "if both inequalities did not hold , then both @xmath458 and @xmath235 would have to lie on the side of the perpendicular bisector of @xmath462 containing @xmath417 .",
    "this is easily seen to imply that @xmath25 must also lie on that side , which however is impossible ( because @xmath463 ) .",
    "see figure  [ fig : c1lemma ] .",
    "c1lemma.pstex_t    next , we take one of these latter caps , @xmath464 , whose bounding circle passes through @xmath417 and through a second point @xmath418 of @xmath465 , and move its center along the bisector of @xmath466 in both directions , keeping the bounding circle touch @xmath417 and @xmath418 , and still keeping the line @xmath452 supporting the chord fixed .",
    "we stop when the first of these events takes place : ( i ) the center reaches @xmath452 , in which case the cap becomes a semidisk ( this can happen in only one of the moving directions ) .",
    "( ii ) the center reaches the midpoint of @xmath466 .",
    "( iii ) the bounding circle touches a third point of @xmath467 .",
    "( iv ) the central angle of the chord along @xmath452 is equal to some fixed positive angle @xmath468 .",
    "the union of the two new caps covers @xmath464 .",
    "( it is possible that during the motion the moving circle becomes tangent to @xmath452 , and then leaves it , in which case the corresponding final cap is a full disk . )    similarly , if the center of @xmath464 lies on @xmath452 ( which can happen when the motion in the preceding canonization step reaches @xmath457 or @xmath456 ) , then we canonize its disk by translating the center to the left and to the right along @xmath469 until the bounding circle touches another point of @xmath467 , exactly as in the preceding case ( shown in figure  [ fig : capcanon2](right ) ) .",
    "let @xmath470 be one of the four new caps . in all cases",
    "@xmath470 is canonical : for the first kind of caps , the stopping condition that defines @xmath470 is ( ii ) or ( iii ) then either the circle bounding @xmath470 passes through three points of @xmath114 or it passes through two diametrically opposite points of @xmath114 .",
    "there are a total of @xmath444 such circles , and since @xmath470 is obtained ( in a unique manner ) by the interaction of one of these circles and one of the @xmath446 canonical chord - lines , there is a total of @xmath471 such caps .",
    "if the stopping condition is ( i ) , the cap is also canonical , because the center of the containing disk is the intersection point of a bisector of two points of @xmath114 with one of the @xmath446 canonical chord - lines , so there is a total of @xmath472 such caps . in the case of condition ( iv )",
    ", there are only @xmath446 such disks , for a total of @xmath472 caps .",
    "similar reasoning shows that the caps resulting in the second case are also canonical , and their number is @xmath472 .",
    "we take the test set @xmath44 to consist of all the caps of the final forms , and argue that it satisfies the properties of a good test set : ( a ) compactness : @xmath473 , so its size is small .",
    "( b ) shallowness : with high probability , each range in @xmath44 is @xmath45-shallow ( and we assume that this property does indeed hold ) .",
    "( c ) containment : each empty cap is also @xmath114-empty , so , by the above canonization process , it is contained in the union of @xmath440 caps of @xmath44 .",
    "( d ) efficiency : each cap @xmath474 is _ @xmath475-covered _ , for appropriate fixed constants @xmath476 , in the terminology of @xcite , meaning that for each point @xmath477 there exists an @xmath19-fat triangle touching @xmath75 , contained in @xmath58 , and with diameter which is at least @xmath136 times the diameter of @xmath58 . in addition , the boundaries of any two ranges in @xmath44 ( or of any two circular caps , for that matter ) intersect in at most four points , as is easily checked . as follows from the recent analysis of de berg @xcite ,",
    "the complexity of the union of any @xmath38 ranges in @xmath44 is @xmath478 .",
    "hence , the complement of the union of any @xmath38 ranges in @xmath44 can be decomposed into @xmath66 elementary cells , making @xmath441 .    in conclusion ,",
    "we obtain :    [ thm : caps ] let @xmath0 be a set of @xmath1 points in the plane .",
    "we can preprocess @xmath0 , in near - linear time , into a data structure of linear size , so that , for any query circular cap @xmath58 , larger than a semidisk , we can test whether @xmath479 is empty , in @xmath21 time .    combining theorem  [ thm : caps ] with parametric searching ,",
    "we obtain :    [ cor : caps ] let @xmath0 be a set of @xmath1 points in the plane .",
    "we can preprocess @xmath0 , in near - linear time , into a data structure of linear size , so that , for any query halfplane @xmath439 and point @xmath480 , we can find , in @xmath21 time , the point in @xmath481 nearest to @xmath65 .",
    "* the machinery developed in this section also applies to smaller circular caps , as long as they are not too small .",
    "formally , if the central angle of each cap is at least some fixed constant @xmath482 , the same technique holds , so we can test emptiness of such ranges in @xmath21 time , using a data structure which requires @xmath7 storage and @xmath8 preprocessing . thus theorem  [ thm : caps ] carries over to this scenario , but corollary [ cor : caps ] does not ,",
    "because we have no control over the `` fatness '' of the cap , as the disk shrinks or expands , when the center of the disk lies in @xmath483 , and once the canonical caps become too thin , the complexity of their union may become quadratic .",
    "[ [ reporting - points - in - semidisks - and - circular - caps . ] ] reporting points in semidisks and circular caps .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    as in the case of fat triangles , we can extend the technique to answer efficiently range reporting queries in semidisks or in sufficiently large circular caps .",
    "we use the same canonization process , with respect to a random sample @xmath114 of size @xmath285 which is a shallow @xmath84-net , and argue , exactly as in the case of fat triangles , that the resulting collection of canonical caps is a good test set for shallow semidisk or larger cap ranges . applying the machinery of section  [ section : range_emptiness ] ,",
    "we obtain a data structure of linear size , which can be constructed in near - linear time , and which can perform reporting queries in semidisks or larger caps , in time @xmath23 , where @xmath27 is the output size of the query .",
    "given a set @xmath0 of @xmath1 points in @xmath2 , a set @xmath74 of semi - algebraic ranges of constant description complexity , and a parameter @xmath484 , the _ approximate range counting _ problem is to preprocess @xmath0 into a data structure such that , for any query range @xmath169 , we can efficiently compute an approximate count @xmath67 which satisfies @xmath485 as in most of the rest of the paper , we will only consider the case where the size of the data structure is to be ( almost ) linear , and the goal is to find solutions with small query time .",
    "the problem has been studied in several recent papers  @xcite , for the special case where @xmath0 is a set of points in @xmath2 and @xmath74 is the collection of halfspaces ( bounded by hyperplanes ) .",
    "a variety of solutions , with near - linear storage , were derived ; in all of them , the dependence of the query cost on @xmath1 is close to @xmath486 , which , as reviewed earlier , is roughly the same as the cost of halfspace range emptiness queries , or the overhead cost of halfspace range reporting queries  @xcite .",
    "the fact that the approximate range counting problem is closely related to range emptiness comes as no surprise , because , when @xmath487 , the approximate count @xmath77 must be @xmath282 , so range emptiness is a special case of approximate range counting .",
    "the goal is therefore to derive solutions that are comparable , in their dependence on @xmath1 , with those that solve emptiness ( or reporting ) queries . as just noted , this has been accomplished for the case of halfspaces . in this section we extend this technique to the general semi - algebraic case .",
    "the simplest solution is to adapt the technique of aronov and har - peled  @xcite , which uses a procedure for answering range emptiness queries as a `` black box '' .",
    "specifically , suppose we have a data structure , @xmath488 , for any set @xmath176 of @xmath489 points , which can be constructed in @xmath490 time , uses @xmath491 storage , and can determine whether a query range @xmath169 is empty in @xmath492 time .",
    "using such a black box , aronov and har - peled show how to construct a data structure for @xmath1 points using @xmath493 storage and @xmath494 preprocessing , where @xmath495 is some constant for which @xmath496 and @xmath497 , for any @xmath498 . given a range @xmath169 , the data structure of @xcite returns , in @xmath499 time , an approximate count @xmath67 , satisfying @xmath500 .",
    "the intuition behind this approach is that a range @xmath50 , containing @xmath38 points of @xmath0 , is expected to contain @xmath501 points in a random sample from @xmath0 of size @xmath40 , and no points in a sample of size smaller than @xmath502 .",
    "the algorithm of @xcite then guesses the value of @xmath251 ( up to a factor of @xmath503 ) , sets @xmath40 to be an appropriate multiple of @xmath502 , and draws many ( specifically , @xmath504 random samples of size @xmath40 . if @xmath50 is empty ( resp . , nonempty ) for many of the samples then , with high probability , the guess for @xmath38 is too large ( resp .",
    ", too small ) .",
    "when we can not decide either way , we are at the correct value of @xmath38 ( up to a relative error of @xmath79 ) .",
    "the actual details of the search are somewhat more contrived ; see @xcite for those details .    plugging our emptiness data structures into the machinery of @xcite , we therefore obtain the following results . in all these applications we can take @xmath505 , so , in the terminology used above , the overall data structure uses @xmath506 storage and @xmath507 preprocessing .",
    "let @xmath0 be a set of @xmath1 points in the plane , and let @xmath19 , @xmath79 be given positive parameters .",
    "then we can preprocess @xmath0 into a data structure of size @xmath508 , in time @xmath509 , for any @xmath10 , such that , for any @xmath19-fat query triangle @xmath20 , we can compute , in @xmath510 time , for any @xmath10 , an approximate count @xmath511 satisfying @xmath512 .",
    "let @xmath0 be a set of @xmath1 points in the plane , and let @xmath79 be a given positive parameter .",
    "then we can preprocess @xmath0 into a data structure of size @xmath508 , in time @xmath509 , for any @xmath10 , such that , for any line @xmath15 , point @xmath75 on @xmath15 or above @xmath15 , and distance @xmath313 , we can compute , in @xmath510 time , for any @xmath10 , an approximate count @xmath513 of the exact number @xmath514 of the points of @xmath0 which lie above @xmath15 and at distance at most @xmath313 from @xmath75 , so that @xmath515 .",
    "let @xmath0 be a set of @xmath1 points in @xmath6 , @xmath74 a collection of convex semi - algebraic ranges of constant description complexity , and @xmath79 a given positive parameter .",
    "then we can preprocess @xmath0 into a data structure of size @xmath508 , in time @xmath509 , for any @xmath10 , such that , for any query range @xmath87 , we can compute , in @xmath516 time , for any @xmath10 , an approximate count @xmath67 of the number of points of @xmath0 outside @xmath50 , satisfying @xmath517 .",
    "let @xmath381 be a set of @xmath1 balls in @xmath6 , and let @xmath79 be a given positive parameter .",
    "then we can preprocess @xmath381 into a data structure of size @xmath508 , in time @xmath509 , for any @xmath10 , such that , for any query ray @xmath310 , we can compute , in @xmath518 time , for any @xmath10 , an approximate count @xmath519 of the exact number @xmath520 of the balls of @xmath381 intersected by @xmath310 , satisfying @xmath521 .",
    "* remark : * another approach to approximate range counting has been presented in @xcite , in which , rather than using range emptiness searching as a black box , one modifies the partition tree of the range emptiness data structure , and augments each of its inner nodes with a so - called _ relative @xmath522 approximation _ sets , which are then used to obtain the approximate count of a range .",
    "this approach too can be adapted to yield efficient approximate range counting algorithms for semialgebraic ranges , with a slightly improved dependence of their performance on @xmath79 .",
    "we omit details of such an adaptation in this paper .",
    "in this paper we have presented a general approach to efficient range emptiness searching with semi - algebraic ranges , and have applied it to several specific emptiness searching and ray shooting problems .",
    "the present study resolves and overcomes the technical problems encountered in our earlier study  @xcite , and presents more applications of the technique .",
    "clearly , there are many other applications of the new machinery , and an obvious direction for further research is to `` dig them up '' . in each such problem ,",
    "the main step would be to design a good test set , with associated function @xmath281 as small as possible , using either the general recipe or an appropriate ad - hoc analysis .",
    "many specific instances of this step are likely to generate interesting ( and often hard ) combinatorial questions . for example , as already mentioned earlier , we still do not know whether the complement of the union of @xmath1 ( congruent ) cylinder in @xmath6 can be decomposed into @xmath523 elementary cells .            p.  k. agarwal and j. erickson , geometric range searching and its relatives , in : _ advances in discrete and computational geometry _ ( b.  chazelle , j.  e.  goodman and r.  pollack , eds . ) , ams press , providence , ri , 1998 , pp .",
    "156 .            p.",
    "k. agarwal , j. pach and m. sharir , state of the union ( of geometric objects ) , in _ proc .",
    "joint summer research conf . on discrete and computational geometry : 20 years later _ ,",
    "452 , ams , 2008 , pp .",
    "948 .",
    "b. chazelle , h. edelsbrunner , l.  j. guibas , and m. sharir , a singly exponential stratification scheme for real semi - algebraic varieties and its applications , _ theoret .",
    "_ , 84 ( 1991 ) , 77105 . also in _ proc .",
    "16th int . colloq . on automata , languages and programming _ ( 1989 ) , pp ."
  ],
  "abstract_text": [
    "<S> in a typical range emptiness searching ( resp . , reporting ) problem , we are given a set @xmath0 of @xmath1 points in @xmath2 , and wish to preprocess it into a data structure that supports efficient range emptiness ( resp . </S>",
    "<S> , reporting ) queries , in which we specify a range @xmath3 , which , in general , is a semi - algebraic set in @xmath2 of constant description complexity , and wish to determine whether @xmath4 , or to report all the points in @xmath5 . </S>",
    "<S> range emptiness searching and reporting arise in many applications , and have been treated by matouek  @xcite in the special case where the ranges are halfspaces bounded by hyperplanes . as shown in @xcite , </S>",
    "<S> the two problems are closely related , and have solutions ( for the case of halfspaces ) with similar performance bounds . in this paper </S>",
    "<S> we extend the analysis to general semi - algebraic ranges , and show how to adapt matouek s technique , without the need to _ linearize _ the ranges into a higher - dimensional space . </S>",
    "<S> this yields more efficient solutions to several useful problems , and we demonstrate the new technique in four applications , with the following results :    \\(i ) an algorithm for ray shooting amid balls in @xmath6 , which uses @xmath7 storage and @xmath8 preprocessing , to mean an upper bound of the form @xmath9 , which holds for any @xmath10 , where @xmath11 is a constant that depends on @xmath12 . ] and answers a query in @xmath13 time , improving the previous bound of @xmath14 .    </S>",
    "<S> \\(ii ) an algorithm that preprocesses , in @xmath8 time , a set @xmath0 of @xmath1 points in @xmath6 into a data structure with @xmath7 storage , so that , for any query line @xmath15 ( or , for that matter , any simply - shaped convex set ) , the point of @xmath0 farthest from @xmath15 can be computed in @xmath16 time . this in turn yields an algorithm that computes the largest - area triangle spanned by @xmath0 in time @xmath17 , as well as nontrivial algorithms for computing the largest - perimeter or largest - height triangle spanned by @xmath0 .    </S>",
    "<S> \\(iii ) an algorithm that preprocesses , in @xmath8 time , a set @xmath0 of @xmath1 points in @xmath18 into a data structure with @xmath7 storage , so that , for any query @xmath19-fat triangle @xmath20 , we can determine , in @xmath21 time , whether @xmath22 is empty . </S>",
    "<S> alternatively , we can report in @xmath23 time , the points of @xmath22 , where @xmath24 .    </S>",
    "<S> \\(iv ) an algorithm that preprocesses , in @xmath8 time , a set @xmath0 of @xmath1 points in @xmath18 into a data structure with @xmath7 storage , so that , given any query semidisk @xmath25 , or a circular cap larger than a semidisk , we can determine , in @xmath21 time , whether @xmath26 is empty , or report the @xmath27 points in @xmath26 in @xmath28 time .    </S>",
    "<S> adapting the recent techniques of @xcite , we can turn our solutions into efficient algorithms for approximate range counting ( with small relative error ) for the cases mentioned above .    </S>",
    "<S> our technique is closely related to the notions of nearest- or farthest - neighbor generalized voronoi diagrams , and of the union or intersection of geometric objects , where sharper bounds on the combinatorial complexity of these structures yield faster range emptiness searching or reporting algorithms .    </S>",
    "<S> h  </S>",
    "<S> s 1v_1    # 1hayim says : # 1 # 1micha says : # 1 </S>"
  ]
}