{
  "article_text": [
    "readers of wikipedia often notice that multiple articles contain highly - similar or even identical passages . in some cases",
    "these represent duplicate articles marked for merging , but content overlap arises in other cases as well . for example , the article about a hurricane and the article about the location where it made landfall might share the same content about the impact of the natural disaster .",
    "identical content is most likely the result of copy and paste between articles , but interestingly , readers occasionally come across highly - similar content that state contradictory facts . in a distributed environment where anyone can edit content , these observations are perhaps not surprising , and in this paper we attempt to rigorously characterize these phenomena by treating the problem as that of near - duplicate sentence detection .",
    "we adapt standard locality - sensitive hashing ( lsh ) techniques to identify clusters of near - duplicate sentences in wikipedia .",
    "we believe this problem is interesting in a few ways :   for duplicate sentences , our analyses quantify the extent to which wikipedia content is simply _ replicated _ , as opposed to _ written _ from scratch . in the case of _",
    "near _ duplicates , some differences represent minor copyediting that does not change the substance of the content , but in other cases the differences represent contradictory facts . quantifying these cases",
    "provides an indirect measure of the quality of wikipedia in terms of self consistency .",
    "this work makes no claims about the novelty of our techniques nor our implementation in mapreduce .",
    "rather , our contribution lies in the analysis and categorization of near - duplicate sentence types .",
    "we have not seen locality - sensitive hashing applied to wikipedia in this way before .",
    "the problem we tackle in this paper is related to a few other problems that have been studied before .",
    "near - duplicate detection of web pages  @xcite is important in search because web pages are often copied or mirrored with only minor differences ( e.g. , ads or navigation bars ) ; it would be desirable to return only the `` canonical '' versions in search results .",
    "in fact , the algorithm that we use in this paper , minhash  @xcite , was originally developed for exactly this purpose .",
    "another closely - related problem is plagiarism detection  @xcite , or more generally , `` text reuse ''  @xcite .",
    "in contrast to near - duplicate detection , the focus is usually on smaller segments of text as opposed to entire documents .",
    "however , similar approaches such as shingling are applicable to both problems .",
    "other similar formulations of the problem are what the data mining community calls pairwise similarity search or `` all pairs '' search  @xcite and what the database community calls set similarity join  @xcite .",
    "the task is the same :  given a potentially large collection of objects , identify all pairs whose similarity is above a threshold according to some similarity metric .",
    "there are two classes of solutions to the above problems :  in the _ index - based _ approach , an inverted index is constructed from objects in the collection and a traversal of the index allows the similar pairs to be extracted , e.g. ,  @xcite ; with the _ hash - based _ approach , the basic idea is to use locality - sensitive hashing ( lsh ) to identify similar pairs based on hash collisions , e.g. , minhash  @xcite . of course",
    ", hybrid solutions are also possible . scaling up these solutions",
    "has been accomplished by mapreduce  @xcite .",
    "similarly , our approach takes advantage of minhash using a mapreduce implementation in hadoop .",
    "because of its open nature , wikipedia has generated much controversy over its editorial quality and factual correctness .",
    "an early study found wikipedia s accuracy to rival that of traditional encyclopedias  @xcite , but subsequent investigations have arrived at conflicting conclusions .",
    "a thorough review is beyond the scope of this short paper , but somewhat ironically , the best summary of this ongoing debate is a wikipedia article . since wikipedia may be edited anonymously , information may be freely copied between web sources and even between wikipedia articles without verification  however , this is not to say that there are no quality assurance mechanisms in wikipedia  @xcite .",
    "although there are active communities of editors who contribute to the upkeep of various articles , much of wikipedia is edited and expanded in an ad hoc manner . in particular ,",
    "wilkinson and huberman   found the distribution of article edits on wikipedia to have a long tail , meaning that a small number of articles account for most of the edits , and that the number of edits is related to article quality .",
    "articles with few edits and low editorial attention are less likely to be updated , which is a source of contradictory information based on our analysis .",
    "for near - duplicate detection we use a well - known technique called minhash  @xcite .",
    "we begin with a parameterized family of @xmath0 hash functions @xmath1 , @xmath2 .",
    "each sentence in a wikipedia article is broken up into @xmath3-gram `` shingles '' ( at the character level ) and for the shingle set @xmath4 a set @xmath5 of minimum hashes over the hash family is generated .",
    "the signature of a document @xmath6 is represented as a vector of @xmath7 minhashes randomly selected from the set of @xmath0 . to increase recall",
    "we generate @xmath8 signatures for each sentence ( i.e. , @xmath8 draws of @xmath7 from @xmath0 ) .",
    "broder proves a straightforward relationship between minhash collisions ( i.e. , documents that share the same signature ) and their jaccard similarities , which forms the theoretical foundation of why and how minhash `` works '' ; we refer the reader to the original paper for the relevant proofs .",
    "we implemented minhash in mapreduce  @xcite using hadoop for this study .",
    "the algorithm is as follows : each mapper receives a wikipedia article identified by a unique docid . inside the mapper we break the article into sentences using a regular expression ; sentences that are shorter than 75 shingles or longer than 600 shingles are discarded . for each sentence ,",
    "the @xmath8 minhash signatures are then computed ( per above ) .",
    "the family of hash functions is implemented using a `` multiply shift '' hashing scheme and generated from a random seed . for our experiments we use a 60 bit hash and a hash family of size @xmath9 .",
    "each signature is emitted as the key of an intermediate key ",
    "value pair with the sentence i d as the value ( constructed from the docid and the sentence number ) .",
    "the mapreduce programming model guarantees that all values associated with the same key ( minhash signatures in our case ) are shuffled to the same reducer and grouped together for further processing  in effect , collecting the hash collisions for us . in the reducers",
    "we receive signatures as keys and all sentence ids that share a signature as values .",
    "if there is more than one value per key , we write out all sentence ids as a cluster .",
    "this serves as input to the final cluster generation stage ( more below ) .",
    "one complexity of applying minhash to real - world datasets is the myriad of parameters that must be selected  each setting manifests a tradeoff between precision , recall , and computational effort .",
    "our approach to parameter tuning relied on a combination of analytical calculations and hand - tuning based on examining the output .",
    "we began by fixing the hashing scheme and the size of the hash family ( @xmath9 ) .",
    "we then generated 10 signatures ( @xmath8 = 10 ) per input sentence .",
    "based on broder  , the probability of a match for sentences @xmath10 and @xmath11 can be expressed as follows : @xmath12 = 1 - ( 1 - s^k)^m\\ ] ] where @xmath13 . with the above settings , the effects of different @xmath7 s",
    "are shown in figure  [ clust ] .",
    "based on this analysis , we set @xmath14 .",
    "this means that if we choose 0.9 jaccard similarity as our goal ( 90% overlap in shingle sets ) , then there is a 99% chance of a match ( i.e. , the recall ) .",
    "finally , after some hand tuning , we settled on a shingle length of 12 characters .",
    "this setting means that we obtain shingles that cross word boundaries , which allows us to capture ( to some extent ) word order in our similarity computations .",
    "based on the parameter settings , it is possible to estimate the amount of data that is generated by our approach , which is the product of the length of each signature , the number of signatures per sentence , and the total number of sentences .",
    "this computation is useful because the amount of intermediate data provides a rough proxy for algorithm running time .",
    "finally , note that it is possible to improve precision by filtering results with a second pass on the output signature groups . in this second pass",
    "we can discard false positives or apply another similarity metric ( e.g. , edit distance ) .",
    "compared to the computational cost of minhash , such additional processing is cheap since it is applied to far less data .",
    "however , we did not implement second - pass filtering in our experiments and leave this for future work .",
    "the output of minhash is a set of clusters , where each cluster represents a signature collision .",
    "since we generate multiple signatures per sentence , it is possible that a sentence appears in multiple clusters .",
    "we adopt the standard practice of merging all clusters that share at least one common sentence .",
    "cluster merging is accomplished in one pass ( outside mapreduce ) with a union find data structure  @xcite . in union find each node maintains a pointer to a head node for the set .",
    "we maintain a lookup table from each sentence i d to its node in the union find structure .",
    "iterating over the input clusters , we look up the node for each sentence , merging clusters or creating new ones as needed . merging two clusters",
    "involves changing the head node of one cluster to point to the head of the other .",
    "once all clusters are processed we perform a pass over the lookup table to obtain the mapping from clusters to nodes .",
    "another mapreduce job reconstructs the sentences in each cluster .",
    "our analysis was conducted on an xml dump of english wikipedia from july 2013 .",
    "the entire corpus is approximately 42 gb and contains 10.2 m articles ( after excluding certain non - article pages ) .",
    "based on our simple sentence chunker , there are a total 135.8 m sentences .    running",
    "our hadoop implementation of minhash on the entire collection took approximately 25 minutes on our cluster consisting of 16 nodes ( 128 cores ) .",
    "code necessary for replicating these experiments have been open sourced .",
    "after the processing pipeline , we identified 1.15 m clusters , 3.50 m article / sentence pairs over 1.09 m articles .",
    "the clusters contain 2.36 m unique sentences .",
    "cluster sizes range from 2 to over 40k ; see figure  [ clust ] for a histogram .",
    "most clusters are small ; about 99% of the clusters have ten or fewer sentences , but about a quarter of the output sentence / article pairs fall into clusters with sizes greater than ten .",
    "+ of the agricultural land 40.4% is used for growing crops and 26.6% is pastures while 2.2% is used for orchards or vine crops .",
    "[ gondiswil ] + of the agricultural land 26.1% is used for growing crops and 30.2% is pastures while 3.0% is used for orchards or vine crops .",
    "[ kleindietwil ] + of the agricultural land 37.8% is used for growing crops and 35.5% is pastures while 2.2% is used for orchards or vine crops .",
    "[ leimiswil ] + * identical * + professional organizers help redirect paradigms into more useful cross - applications that ensure properly co - sustainable futures for their clients spaces and processes .",
    "[ professional organizing ] + professional organizers help redirect paradigms into more useful cross - applications that ensure properly co - sustainable futures for their clients spaces and processes .",
    "[ professional organizer ] + * copyediting * + in dry areas it may only emerge from its burrow for a few weeks when conditions are right and usually at night but in areas with permanent water bodies and abundant rain it may be active all day . [",
    "great plains toad ] + in dry areas it may only emerge from its burrow for a few weeks when conditions are right and only at night but in areas with permanent water bodies and abundant rain it may be active all day .",
    "[ list of amphibians and reptiles of montana ] + * factual drift * + bulgaria , a poor rural nation of 7 million people sought to acquire macedonia but when it tried it was defeated in 1913 in the second balkan war .",
    "[ history of the balkans ] + bulgaria a poor rural nation of 4.5 million people sought to acquire macedonia but when it tried it was defeated in 1913 in the second balkan war . [ home front during world war i ] + * references * + komjth pter and vilmos totik : problems and theorems in classical set theory , springer - verlag , berlin , 2006 .",
    "[ pter komjth ] + pter komjth , vilmos totik : problems and theorems in classical set theory , springer - verlag , berlin , 2006 .",
    "[ vilmos totik ] + * other * + army medical research institute of infectious diseases ( usamriid ) microbiologist bruce e. [ timeline of the 2001 anthrax attacks ] + army medical research institute of infectious diseases ( usamriid ) which transitioned from the previous u.s .",
    "[ fort detrick ]    based on manual inspection of the cluster output , we can categorize near - duplicate clusters into one of six types .",
    "examples are shown in figure  [ toparticlegroups ] and discussed below .",
    "_ templates _ describe sentences that have identical structure , but with different entities , facts , or figures for _ different _ topics ( and thus are not contradictory ) . they reflect conscious attempts to impose structure across groups of articles that may be related .",
    "since several of the largest template clusters contain tens of thousands of sentences ( the largest over 40,000 ) , it is likely that some template groups are automatically generated using bots . in many cases template sentences",
    "are found in stub articles .",
    "_ identical _ sentences are the result of copy and paste , and are often found in articles that cover similar topics or articles that are subtopics of other topics .",
    "non - identical but highly - similar sentences break down into two types : _ copyediting _ refers to nearly identical sentences that differ in stylistic or otherwise non - substantive ways .",
    "they arise with minor editing after a copy and paste .",
    "_ factual drift _ describes sentences about the _ same _ topic that provide contradictory facts .",
    "although without detailed research , there is no way to ascertain which version ( if any ) is correct , we can identify a common scenario .",
    "after a copy and paste , a fact becomes out of date ( e.g. , the tallest building or the death toll in a disaster ) and is corrected in one instance but not the others .",
    "_ references _ refer to citations , typically occurring at the end of articles . since wikipedia does not adhere to one single citation style , the same work may be cited differently , or multiple citations to the same venue may be similar .    finally , clusters that do not fit into any of the above categories are classified as _",
    "other_. these typically represent sentences that are highly similar , but otherwise bear no semantic relationship with each other .",
    "sentence chunking errors often contribute to these spurious results .",
    "to quantify the distribution of these six cases , we randomly sampled 2094 clusters and performed manual classification .",
    "the results are shown in table  [ counts ] .",
    "nearly three quarters of all clusters are either _ identical _ or _",
    "templates_. from this table , we obtain a rough characterization of the precision of our analysis based on minhash  except for `` other '' , all categories are `` correct '' identification of near - duplicate sentences .",
    "recall can be computed analytically , as we have shown in the previous section .",
    ".manual classification of sample clusters .",
    "[ cols=\"<,>,>\",options=\"header \" , ]     [ counts ]    finally , we manually examined a few of the large clusters and noticed that they often contain a mix of different phenomena .",
    "one common pattern is distinct groups of identical sentences , where each of the groups are near duplicates .",
    "since there are relatively few large clusters , these nuances do not have a significant impact on the figures in table  [ counts ] .",
    "in this work , we applied minhash to the problem of near - duplicate sentence detection on wikipedia .",
    "our mapreduce implementation is highly scalable and processes english wikipedia in a short amount of time on a modest cluster .",
    "we found that there is a substantial amount of duplicate content in wikipedia , and that near - duplicate sentences manifest a few phenomena , the most interesting of which is contradictory facts .",
    "how can we act upon this analysis ?",
    "we could imagine a robot that monitors wikipedia to flag inconsistencies and requests editors to intervene and resolve .",
    "such a service would be valuable in improving the internal consistency and quality of wikipedia .",
    "there are several directions in which our work can be extended .",
    "currently , our technique only identifies _",
    "clusters _ of near - duplicate sentences  missing from our analysis is the notion of information flow :  which was the source article and which was the target of copying and pasting ?",
    "are there copy `` chains '' where content was progressively copied from one article to the next , with possible `` branches '' ( and accumulation of errors along the way ) ?",
    "our analysis of the large near - duplicate clusters suggests that there are complex edit histories that form tree - like structures .",
    "furthermore , are there editor - specific effects ?",
    "for example , is copying and pasting more likely by anonymous editors ?    finally ,",
    "templates represent interesting cases of near - duplicate sentences because they , in effect , encode structured knowledge .",
    "we could formalized such knowledge in infoboxes if they are not already :  it might be possible to apply information extraction techniques to generate structured linked data from template sentences .",
    "going in the opposite direction , we might ( via a robot ) _ create _ templates sentences to impose consistency across pages in the same category to ensure that certain facts are captured in prose ."
  ],
  "abstract_text": [
    "<S> _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ our study identifies sentences in wikipedia articles that are either identical or highly similar by applying techniques for near - duplicate detection of web pages . </S>",
    "<S> this is accomplished with a mapreduce implementation of minhash to identify clusters of sentences with high jaccard similarity . </S>",
    "<S> we show that these clusters can be categorized into six different types , two of which are particularly interesting :  identical sentences quantify the extent to which content in wikipedia is copied and pasted , and near - duplicate sentences that state contradictory facts point to quality issues in wikipedia . _ </S>",
    "<S> _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ </S>"
  ]
}