{
  "article_text": [
    "the empirical best linear unbiased predictors ( eblup ) or empirical bayes estimators ( eb ) in the bayesian context have been used for providing reliable small - area estimates in the normal linear mixed models .",
    "the unconditional mean squared errors ( mse ) have been widely used as a measure for prediction error of eblup , and the asymptotic approximations of the mses and their approximated unbiased estimators have been studied in a lot of papers under the assumption that the number of small areas is large .",
    "for example , see prasad and rao ( 1990 ) , ghosh and rao ( 1994 ) , rao ( 2003 ) , datta , rao and smith ( 2005 ) and hall and maiti ( 2006 ) .",
    "when data from the small area of interest are observed , the practitioners want to know how large prediction errors the eblup based on the observed data have .",
    "concerning this issue , the conventional unconditional mses do not give us appropriate estimation errors , since it is an integrated measure . booth and hobert ( 1998 )",
    "suggested the conditional mse given the data of the small area of interest , and datta , kubokawa , molina and rao ( 2011 ) and torabi and rao ( 2013 ) derived second - order unbiased estimators of the conditional mse in the fay - herriot and nested error regression models which are well - known normal linear mixed models .",
    "as pointed out in both papers , the difference between the conditional and unconditional mses is small in the normal linear mixed models , since it appears in the second - order terms . in the generalized linear mixed models ( glmm )",
    ", however , booth and hobert ( 1998 ) showed that the difference is significant for distributions far from normality , since it appears in the first - order or leading terms .    although the glmms are useful for analyzing count data in small area estimation , it is computationally hard to derive the eblup and to evaluate their conditional mses , because the marginal likelihood and eblup in the glmm can not be expressed in closed forms .",
    "in fact , we need relatively high dimensional numerical integration to evaluate the conditional mses . another point is the assumption that sample sizes of small areas are large , under which the laplace approximation can be used to get asymptotically unbiased estimators of the conditional mses .",
    "however , this assumption is against the situation in small area estimation with small samples sizes .",
    "an alternative model is the mixed model based on the natural exponential families with quadratic variance functions ( nef - qvf ) suggested in ghosh and maiti ( 2004 , 2008 ) . in the nef - qvf mixed models , the blup or the bayes estimator",
    "can be expressed explicitly as the weighed average of a sample mean and a prior mean . moreover ,",
    "the mse of the empirical bayes estimator can be approximated analytically , and their asymptotically unbiased estimator can be obtained without assuming that samples of small areas are large .",
    "the nef - qvf mixed models include the binomial - beta mixed and the poisson - gamma mixed models , which are practically useful for analyzing mortality data in small areas .",
    "thus , in this paper , we treat the nef - qvf mixed models instead of the glmm and focus on the conditional prediction errors or the conditional mses ( cmse ) of the empirical bayes estimators ( eb ) . assuming that the number of small areas is large , but sample sizes in small areas are bounded",
    ", we not only derive second - order approximations of the conditional mses and their second - order unbiased estimators in closed forms , but also show that the difference between the conditional and unconditional mses is significant and appears in the first - order terms under distributions far from normality .",
    "the paper is organized as follows : in section [ sec : gmm ] , the cmse of eb is addressed in the general mixed models , and the second - order approximation of the cmse is derived under suitable conditions on estimators of model parameters and predictors .",
    "second - order unbiased estimators of the cmse are obtained in two ways of the analytical and parametric bootstrap methods .    in section [ sec : nef ] , the nef - qvf mixed models are investigated as an application of the general results in section [ sec : gmm ] .",
    "the second - order approximations of the cmses and their second - order unbiased estimators are obtained in analytical and closed forms without assuming that sample sizes of small areas tend to infinity .",
    "ghosh and maiti ( 2004 ) derived the unconditional mse of eb , and their estimation method and techniques for analysis are heavily used in section [ sec : nef ] .",
    "it is interesting to point out that the first - order term in the cmse is an increasing function of the direct estimate in the small area for the poisson - gamma mixed model , and it is a quadratic concave function for the binomial - beta mixed model , while the corresponding first - order terms in the unconditional mses are constants for both mixed models .",
    "simulation and empirical studies of the suggested procedures are given in section [ sec : ns ] .",
    "two data sets are used for the empirical studies .",
    "one is the stomach cancer mortality data in saitama prefecture in japan , and the poisson - gamma mixed model is applied .",
    "the other is the infant mortality data before world war ii in ishikawa prefecture in japan , and we use the binomial - beta mixed model . through these analysis , it is observed that the estimates of the conditional mses are more variable than those of the unconditional mses , since conditional mse depends on the data of the area of interest .",
    "for some areas , the conditional mse gives much higher risks than the unconditional mse , namely , the conventional mse seems to under - estimate the conditional mse .",
    "thus , we suggest providing estimates of the conditional mse .    finally , the concluding remarks are given in section [ sec : cr ] , and the technical proofs are given in the appendix .",
    "let @xmath0 be a vector of observable random variables , and let @xmath1 be a vector of unobservable random variables .",
    "let @xmath2 be a @xmath3-dimensional vector of unknown parameters . in this paper",
    ", we treat continuous or discrete cases for @xmath4 and @xmath5 . the conditional probability density ( or mass )",
    "function of @xmath4 given @xmath6 is denoted by @xmath7 , and the conditional probability density ( or mass ) function of @xmath8 given @xmath2 is denoted by @xmath9 , namely , @xmath10 this expresses the general parametric mixed models . since it can be interpreted as a bayesian model , we here use the terminology used in bayes statistics . in the continuous case ,",
    "the marginal density function of @xmath4 for given @xmath2 and the conditional ( or posterior ) density function of @xmath8 given @xmath11 are given by @xmath12 and we use the same notations in the discrete case .",
    "then , for @xmath13 , we consider the problem of predicting a scalar quantity @xmath14 of each small area .",
    "when @xmath15 is predicted with @xmath16 , the predictor @xmath17 can be evaluated with the unconditional and conditional mses , described as @xmath18 , \\\\",
    "\\cmse(\\bta , \\hxi_i |y_i)= & e\\bigl [ \\bigl\\ { \\hxi_i - \\xi_i(\\theta_i,\\bta ) \\bigr\\}^2|y_i\\bigr],\\end{aligned}\\ ] ] which are denoted by mse and cmse , respectively .",
    "the best predictors of @xmath15 in terms of the two kinds of mses are the conditional mean given by @xmath19,\\ ] ] which is the bayes estimator in the bayesian context .",
    "since @xmath2 is unknown , we need to estimate @xmath2 from observations @xmath20 . substituting an estimator @xmath21 into @xmath22 results in the empirical bayes ( eb ) estimator @xmath23 .    in this paper",
    ", we focus on asymptotic evaluations of the cmse . to this end",
    ", we assume the following conditions on the estimator @xmath21 and the predictor @xmath22 for large @xmath24 :    [ as : est ]    the dimension @xmath3 of @xmath2 is bounded and the estimator @xmath21 satisfies that @xmath25 , @xmath26=o_p(m^{-1})$ ] and @xmath27 for @xmath13 .    for @xmath28 , @xmath29 , @xmath30 , and",
    "the conditional variances of @xmath15 and @xmath31 exist . for @xmath32 ,",
    "the estimator @xmath22 is continuously differentiable with respect to @xmath33 , and @xmath34<\\infty.\\ ] ]    under assumption [ as : est ] , we get a second - order approximation of cmse of @xmath23 .",
    "let @xmath35 , \\label{eqn : g2}\\end{aligned}\\ ] ] where @xmath36 is the conditional or posterior variance of @xmath14 .",
    "it is noted that @xmath37 and @xmath38 under assumption [ as : est ] .",
    "[ thm : cmse ] under assumption [ as : est ] , the conditional mse of @xmath23 is approximated as @xmath39    _ proof . _   since @xmath40 = 0 $ ] , it is observed that @xmath41 \\non\\\\ = & e [ \\ { \\xi_i(\\theta_i,\\bta)- \\hxi_i(y_i,\\bta ) \\}^2|y_i ] + e[\\ { \\hxi_i(y_i,\\bta)-\\hxi_i(y_i,\\hbta ) \\}^2|y_i ] , \\label{decom}\\end{aligned}\\ ] ] and that @xmath42 = var(\\xi(\\theta_i,\\bta)|y_i)=t_{1i}(y_i,\\bta)$ ] .",
    "it is noted that @xmath43 where @xmath44 is between @xmath2 and @xmath21 . since @xmath45",
    ", we obtain @xmath46=e\\bigl [ \\bigl\\ { ( \\btah-\\bta)^t { \\partial \\hxi_i(y_i,\\bta)\\over \\partial \\bta}\\bigr\\}^2\\bigr|y_i\\bigr ] + o_p(m^{-1}),\\ ] ] which shows theorem [ thm : cmse ] .",
    "@xmath47    we next derive second - order unbiased estimators of @xmath48 and @xmath49 , which result in a second - order unbiased estimator of cmse . as seen from theorem [ thm : cmse ] , the order of @xmath50 is @xmath51 , so that we can estimate @xmath50 by @xmath52 unbiasedly up to second - order . for estimation of @xmath36 ,",
    "the naive estimator @xmath53 has a second - order bias because @xmath37 .",
    "it is observed that @xmath54 = t_{1i}(y_i,\\bta ) + t_{11i}(y_i,\\bta ) + t_{12i}(y_i,\\bta ) + o_p(m^{-1 } ) , \\label{t1}\\ ] ] where @xmath55\\ ] ] and @xmath56\\bigr].\\ ] ] it is noted that @xmath57 and @xmath58 under assumption [ as : est ] .",
    "+ * [ analytical method ] *  it follows from ( [ t1 ] ) that a second - order unbiased estimator of cmse is given by @xmath59    [ thm : cmseu1 ] under assumption [ as : est ] , the estimator @xmath60 is a second - order unbiased estimator of cmse , namely @xmath61=\\cmse(\\bta,\\hxi_i(y_i,\\hbta)|y_i)+o_p(m^{-1}).\\ ] ]    as explained in section [ sec : nef ] , in the mixed model based on nef - qvf , we can provide analytical expressions for @xmath62 and @xmath63 , whereby we obtain a second - order unbiased estimator in a closed form . in general , however , it is hard to get analytical expressions for @xmath62 and @xmath63 . in this case , as given below , the parametric bootstrap method helps us provide a feasible second - order unbiased estimator of cmse .",
    "+ * [ parametric bootstrap method ] *   since @xmath4 is fixed , a bootstrap sample is generated from @xmath64 where @xmath65 s are mutually independently distributed as @xmath66 .",
    "noting that @xmath4 is fixed , we construct the estimator @xmath67 from the bootstrap sample @xmath68 with the same technique as used to obtain the estimator @xmath69 .",
    "let @xmath70 $ ] be the expectation with regard to the bootstrap sample ( [ boots ] ) .",
    "a second - order unbiased estimator of @xmath36 is given by @xmath71.\\ ] ] then , it can be verified that @xmath72=t_{1i}(y_i,\\bta)+o_p(m^{-1})$ ] .",
    "in fact , from ( [ t1 ] ) , it is noted that @xmath54=t_{1i}(y_i,\\bta)+d_i(y_i,\\bta)+o_p(m^{-1}),\\ ] ] where @xmath73 .",
    "this implies that @xmath74=t_{1i}(y_i,\\hbta)+d_i(y_i,\\hbta)+o_p(m^{-1})$ ] .",
    "since @xmath75 is continuous in @xmath2 and @xmath76 , one gets @xmath72=t_{1i}(y_i,\\bta)+o_p(m^{-1})$ ] .    for @xmath50 , from ( [ decom ] )",
    ", it is estimated via parametric bootstrap method as @xmath77.\\ ] ] it is noted that the estimator @xmath78 is always available although an analytical expression of @xmath50 is not necessarily available .",
    "combining the above results yields the estimator @xmath79    [ thm : cmseu2 ] under assumption [ as : est ] , the estimator @xmath80 is a second - order unbiased estimator of cmse , namely @xmath81=\\cmse(\\bta , \\hxi_i(y_i,\\hbta)|y_i)+o_p(m^{-1}).\\ ] ]",
    "we now consider the mixed models based on natural exponential families with quadratic variance functions ( nef - qvf ) .",
    "the nef - qvf mixed models were used in context of small area estimation by ghosh and maiti ( 2004 ) , who evaluated asymptotically the unconditional mse for calibrating uncertainty of the empirical bayes estimator when @xmath24 is large . in this section ,",
    "we handle an area level model with a survey estimate from each area where the survey estimate has a distribution based on nef - qvf , and apply the results in the previous section to provide a second - order approximation and its unbiased estimator for the conditional mse of the eb .    in our settings",
    ", it is assumed that known parameters @xmath82 s , which correspond to sample sizes in small - areas in normal cases , are bounded and the number of areas @xmath24 is large .",
    "let @xmath83 be mutually independent random variables where the conditional distribution of @xmath4 given @xmath84 and the marginal distribution of @xmath84 belong to the the following natural exponential families : @xmath85 , \\\\ \\th_i",
    "| \\nu , m_i\\sim & \\pi(\\th_i|\\nu , m_i ) = \\exp[\\nu(m_i\\th_i-\\psi(\\th_i))]c(\\nu , m_i ) , \\end{split } \\label{model}\\ ] ] where @xmath82 is a known scalar parameter and @xmath86 is an unknown scalar hyperparameter .",
    "let @xmath87 and @xmath88 .",
    "the function @xmath89 is the regular one - parameter exponential family and the function @xmath90 is the conjugate prior distribution .",
    "define @xmath91 by @xmath92 = \\psi'(\\th_i),\\ ] ] which is the conditional expectation of @xmath4 given @xmath84 , where @xmath93 .",
    "assume that @xmath94 for @xmath95 , namely , @xmath96 where @xmath97 for known constants @xmath98 , @xmath99 and @xmath100 which are not simultaneously zero .",
    "this means that given @xmath84 , the conditional variance @xmath101 is a quadratic function of the conditional expectation @xmath102 $ ] .",
    "this is the natural exponential family with the quadratic variance function ( nef - qvf ) studied by morris ( 1982 , 1983 ) .",
    "similarly , the mean and variance of the prior distribution are given by @xmath103 = m_i , \\quad \\var(\\xi_i|m_i,\\nu)={q_i(m_i)\\over \\nu - v_2}. \\label{eqn : emv}\\ ] ] in our settings , we consider the link given by @xmath104 where @xmath105 is a @xmath106 vector of explanatory variables and @xmath107 is a @xmath106 unknown common vector of regression coefficients .",
    "then , the unknown parameters @xmath2 in the previous section correspond to @xmath108 . the joint probability density ( or mass )",
    "function of @xmath109 can be expressed as @xmath110 where @xmath111 is the conditional ( or posterior ) density function of @xmath84 given @xmath4 , and @xmath112 is the marginal density function of @xmath4 . these density ( or mass ) functions are written as @xmath113 c(n_i+\\nu , \\hxi_i ) , \\\\",
    "f_\\pi(y_i|\\nu , m_i ) = & { c(\\nu , m_i)\\over c(n_i+\\nu,\\hxi_i ) } \\exp [ c(y_i , n_i ) ] , \\end{split } \\label{eqn : ecmodel}\\ ] ] where @xmath17 is the posterior expectation of @xmath91 , namely , @xmath114 $ ] , given by @xmath115 which corresponds to the bayes estimator of @xmath91 in the bayesian context when @xmath86 and @xmath116 are known . as shown in ghosh and maiti ( 2004 ) , @xmath117=&e[\\psi'(\\th_i)]=m_i , \\\\",
    "\\var(y_i)=&\\var(e[y_i|\\th_i])+e[\\var(y_i|\\th_i)]=\\var(\\xi_i)+e[q_i(\\xi_i)/n_i]=q_i(m_i)\\phi_i , \\\\",
    "\\cov(y_i , \\xi_i)=&e[\\cov(y_i,\\xi_i)|\\th_i]+ \\cov(e[y_i|\\th_i ] , \\xi_i)=q_i(m_i)/(\\nu - v_2),\\end{aligned}\\ ] ] for @xmath118 . using these observations ,",
    "ghosh and maiti ( 2004 ) showed that the bayes estimator @xmath17 given in ( [ b ] ) is the best linear unbiased predictor ( blup ) of @xmath91 in terms of mse .",
    "since the hyperparameters @xmath2 are unknown , we need to estimate them from the joint marginal distribution of @xmath119 . for the purpose , ghosh and maiti ( 2004 ) suggested the estimating equations given in godambe and thompson ( 1989 ) .",
    "let @xmath120 for @xmath121 and @xmath122 .",
    "let @xmath123 and @xmath124 , where @xmath125 $ ] , @xmath126 , and exact expressions of @xmath127 , @xmath128 and @xmath129 are given below .",
    "then , ghosh and maiti ( 2004 ) derived the estimating equations given by @xmath130 , which are written as @xmath131 q_i(m_i ) \\x_i = \\zero , \\\\ \\sum_{i=1}^m { 1\\over |\\bsi_i| } & \\{\\mu_{2i}g_{2i}-\\mu_{3i}g_{1i}\\}q_i(m_i)(1+v_2/n_i)(\\nu - v_2)^{-2}=0 .",
    "\\end{split } \\label{ee}\\ ] ] the resulting estimator of @xmath2 is here called thel gt - estimator and denoted by @xmath132 .",
    "the equations can be solved numerically . in our numerical investigation",
    ", we used the optim function in ` r to solve the estimating equations by minimizing the sums of squares of the estimating functions .",
    "this approach may cause the problem in the presence of multiple roots , but fortunately we did not encounter this situation in our examples given in section [ sec : ee ] .",
    "the exact moments @xmath125 $ ] , @xmath133 , are obtain from theorem 1 of ghosh and maiti ( 2004 ) as @xmath134 and @xmath135 + { 6\\over n_i}q_i'(m_i ) ( d_i+1)(2 d_i+1)e[(\\xi_i - m_i)^3 ] \\\\ & + { d_i + 1\\over n_i^2}\\bigl [ 7 \\{q'(m_i)\\}^2 + 2n_i(4 d_i+3)q(m_i)\\bigr ] e[(\\xi_i - m_i)^2 ]   \\\\ & + { 1\\over n_i^3}q(m_i)\\bigl [ n_i(2 d_i+3)q(m_i)+\\{q'(m_i)\\}^2\\bigr],\\end{aligned}\\ ] ] for @xmath136 .",
    "the expressions of the moments of @xmath91 are obtained given in kubokawa , @xmath137 ( 2014 ) as @xmath138=q(m_i)/(\\nu - v_2)$ ] , @xmath139=2q(m_i)q'(m_i)/(\\nu - v_2)(\\nu-2v_2)$ ] and @xmath140=\\frac{3q(m_i)\\left[(\\nu - v_2)q(m_i)+2\\left\\{q'(m_i)\\right\\}^2\\right]}{(\\nu - v_2)(\\nu-2v_2)(\\nu-3v_2)}.\\ ] ] using these expressions , we obtain the gt - estimator @xmath141 .",
    "an alternative method for estimating @xmath2 is the maximum likelihood estimator ( ml ) . since a closed expression of the marginal distribution of @xmath119",
    "is given in ( [ eqn : ecmodel ] ) in the nef - qvf mixed model , the ml - estimator of @xmath2 is provided by @xmath142 since we do not have a closed expression of the maximizer , we resort to a numerical optimization .",
    "when the parameter @xmath2 is estimated by the gt - estimator @xmath143 or the ml - estimator @xmath144 , we can construct the estimator @xmath145 for @xmath116 . substituting @xmath146 and @xmath147 into ( [ b ] )",
    ", we finally get the empirical bayes estimator of @xmath91 , given by @xmath148 the eb estimator is often used as a predictor in small area estimation and its uncertainty is of great importance .",
    "our interest is in evaluation of the conditional mse of @xmath149 , which is investigated in the next subsection .      since the second - order approximation of the conditional mse",
    "is given in theorem [ thm : cmse ] , we need to evaluate the first and second order terms @xmath36 and @xmath50 in the cmse . for the first order term , it is easy to see that @xmath150 which is @xmath151 . for the second order term ,",
    "unfortunately , we do not have an analytical expression of @xmath50 when we use the ml - estimator @xmath152 for @xmath69 .",
    "but , the parametric bootstrap method given in theorem [ thm : cmseu2 ] enables us to construct the second - order unbiased estimator of the cmse .",
    "when the gt - estimator @xmath153 is used for @xmath2 , on the other hand , we can derive an analytical expression of @xmath50 , which yields closed forms of the second - order approximation of the cmse and the asymptotically unbiased estimator of the cmse .",
    "thus , in the rest of this subsection , we focus on derivation of analytical expressions for the cmse when the gt - estimator @xmath153 is used for @xmath2 .",
    "we begin by giving a stochastic expansion and conditional moments of @xmath153 which is the solution of the estimating equations ( [ ee ] ) .",
    "we use the notations given by @xmath154 where the detailed forms of @xmath155 and @xmath156 are given in ( [ a1 ] ) and ( [ a2 ] ) in the appendix , respectively .",
    "it is noted that @xmath157 and @xmath158 .",
    "the following lemma is useful for evaluating the conditional mse , where the proof is given in the appendix .",
    "[ lem:1 ] let @xmath153 be the solution of estimating equations in @xmath159 .",
    "then for @xmath160 , @xmath161=\\u(\\bta)^{-1}+o_p(m^{-1}),\\\\ & e[\\btah_{\\rm gt}-\\bta|y_i]=\\b ( y_i,\\bta)+o_p(m^{-1 } ) .",
    "\\end{split } \\label{eqn}\\ ] ]    lemma [ lem:1 ] means that the second - order approximations of the conditional moments @xmath162 $ ] and @xmath163 $ ] do not depend on @xmath4 , that is , they are equal to the unconditional moments given in ghosh and maiti ( 2004 ) .",
    "lemma [ lem:1 ] shows that the estimator @xmath153 satisfies assumption [ as : est ] .",
    "it is noted that the partial derivatives of @xmath164 and @xmath165 appear in the expressions ( [ a1 ] ) and ( [ a2 ] ) , and these can be numerically evaluated using the numerical derivatives , where the detailed procedure is given in the appendix .",
    "we now derive analytical expressions @xmath50 in theorem [ thm : cmse ] . in the following theorem",
    ", we can evaluate @xmath50 as @xmath166 , \\label{t2n}\\ ] ] which is @xmath51 , where @xmath167    [ thm : cnef ] the cmse of @xmath168 can be approximated up to @xmath51 as @xmath169 where @xmath36 and @xmath50 are given in @xmath170 and @xmath171 , respectively .    _ proof .",
    "_ from theorem [ thm : cmse ] , it is sufficient to calculate @xmath172 , which is written as @xmath173 & = \\tr e\\bigl[\\bigl(\\frac{\\pd\\hxi_i}{\\pd\\bta}\\bigr)\\bigl(\\frac{\\pd\\hxi_i}{\\pd\\bta}\\bigr)^t(\\hbta_{\\rm gt}-\\bta)(\\hbta_{\\rm gt}-\\bta)^t\\bigr|y_i\\bigr]\\\\ & = \\tr\\bigl[\\bigl(\\frac{\\pd\\hxi_i}{\\pd\\bta}\\bigr)\\bigl(\\frac{\\pd\\hxi_i}{\\pd\\bta}\\bigr)^te\\left[(\\hbta_{\\rm gt}-\\bta)(\\hbta_{\\rm gt}-\\bta)^t\\big|y_i\\right]\\bigr].\\end{aligned}\\ ] ] it is noted from ( [ b ] ) that @xmath174 then from lemma [ lem:1 ] , the last formula can be approximated as @xmath175+o_p(m^{-1}),\\ ] ] which completes the proof .",
    "@xmath47    taking the expectation of @xmath176 with respect to @xmath4 , one gets the unconditional mse given in theorem 1 of ghosh and maiti ( 2004 ) with @xmath177 .",
    "in fact , @xmath178=\\frac{\\nu}{(n_i+\\nu)(\\nu - v_2)}q(m_i ) , \\\\",
    "t_{2i}(\\bta)\\equiv & e[t_{2i}(y_i,\\bta)]\\\\ = & ( n_i+\\nu)^{-2}\\tr\\bigl[\\bigl(\\begin{array}{cc } \\nu^2q(m_i)^2\\x_i\\x_i^t&\\0\\\\ \\0^t & n_i(n_i+\\nu)^{-1}q(m_i)(\\nu - v_2)^{-1}\\end{array } \\bigr)\\u(\\bta)^{-1}\\bigr].\\end{aligned}\\ ] ]    the unconditional mse of @xmath179 is approximated as @xmath180    it is interesting to investigate the difference between the approximations of the cmse and the mse .",
    "when the underlying distribution of @xmath4 is a normal distribution , we have @xmath181 , or @xmath182 and @xmath183 , so that @xmath184 , namely the leading term in the cmse is identical to that in the mse .",
    "thus , the difference between the cmse and the mse appears in the second - order term with @xmath51 .",
    "when @xmath99 or @xmath100 is not zero , however , the leading term @xmath36 in the cmse is a function of @xmath4 and it is not equal to the leading term @xmath185 in the mse .",
    "thus , for distributions far from the normality , the difference between the cmse and the mse is significant even when @xmath24 is large .",
    "this tells us about the remark that one can not replace the conditional mse given @xmath4 with the corresponding unconditional mse except for the normal distribution .",
    "some examples including the poisson and binomial distributions are given in section [ sec : exm ] .",
    "we next derive an analytical form of a second - order unbiased estimator for the cmse . for the purpose",
    ", we need to calculate @xmath62 and @xmath63 given in ( [ t11 ] ) and ( [ t12 ] ) , respectively .",
    "note that @xmath186 where @xmath187 , and @xmath188,\\\\ \\t_{1i}^{12}&=\\frac{\\pd^2t_{1i}}{\\pd\\bbe\\pd\\nu}=q(m_i)\\la_i(n_i+\\nu)^{-2}\\left\\{q'(\\hxi_i)\\left(n_i-\\nu(n_i+\\nu)\\la_i\\right)-2v_2n_i\\nu g_{1i}(n_i+\\nu)^{-1}\\right\\}\\x_i,\\\\ t_{1i}^{22}&=\\frac{\\pd^2t_{1i}}{\\pd\\nu^2}=2\\la_i^3q(\\hxi_i)+2\\la_i^2 n_i(n_i+\\nu)^{-2}q'(\\hxi_i)g_{1i } \\\\ & \\hspace{3 cm }   + 2\\la_in_i(n_i+\\nu)^{-4}g_{1i}\\left[(n_i+\\nu)q'(\\hxi_i)+n_i v_2g_{1i}\\right].\\end{aligned}\\ ] ] using ( [ eqn ] ) in lemma [ lem:1 ] , we obtain the analytical expressions of @xmath62 and @xmath63 as @xmath189.\\end{aligned}\\ ] ] the estimator @xmath190 given in ( [ ana ] ) is expressed as @xmath191.\\end{aligned}\\ ] ]    [ thm : unef ] the estimator @xmath192 is a second - order unbiased estimator , namely , @xmath193=\\cmse_i(\\bta , \\hxi_i(y_i,\\hbta_{\\rm gt } ) \\mid y_i)+o_p(m^{-1}).\\ ] ]    it is noted that the results in theorems [ thm : cnef ] and [ thm : unef ] do not require the condition that @xmath194 .",
    "thus , the results in theorems [ thm : cnef ] and [ thm : unef ] are applicable in the context of small area estimation .",
    "we give some examples of the mixed models belonging to ( [ model ] ) and investigate the conditional mse .",
    "+ * [ 1 ] fay - herriot model*. the fay - herriot model is an area - level model often used in small area estimation , given by @xmath195 where @xmath24 is the number of small areas , and @xmath196 s and @xmath197 s are mutually independently distributed random errors such that @xmath198 and @xmath199 .",
    "the notations in ( [ model ] ) correspond to @xmath200 and @xmath201 . in this case , the estimating equations in ( [ ee ] ) reduce to @xmath202 which coincide with the likelihood equations for the maximum likelihood estimators of @xmath107 and @xmath203 , namely @xmath204 in fay - herriot model .",
    "the terms @xmath36 and @xmath50 in approximation ( [ cmse ] ) of the cmse are written as @xmath205 which were given in datta ( 2011 ) . in the fay - herriot model , @xmath206 ,",
    "namely , the leading terms in the conditional and unconditional mses are identical , and the difference between the cmse and mse is small for large @xmath24",
    ".     + * [ 2 ] poisson - gamma mixture model*. let @xmath207 be mutually independent random variables having @xmath208 where @xmath209 are mutually independent , po@xmath210 denotes the poisson distribution with mean @xmath211 , and ga@xmath212 denotes the gamma distribution with shape parameter @xmath213 and scale parameter @xmath214 .",
    "let @xmath215 and @xmath216 for @xmath13 .",
    "then , the notations in ( [ model ] ) correspond to @xmath217 and @xmath218 . the posterior distribution of @xmath219 is @xmath220 or @xmath221",
    ". then we have @xmath222 which increases in @xmath4 .",
    "thus , the difference between the conditional and unconditional mses increases in @xmath4 .",
    "when a large value of @xmath4 is observed , it should be remarked that the conditional mse of the empirical bayes estimator given @xmath4 is larger than the unconditional ( or integrated ) mse .",
    "hence , it is meaningful to provide to practitioners the information on the conditional mse as well as the unconditional mse .    for the poisson - gamma mixture model ,",
    "the marginal distribution of @xmath4 ( marginal likelihood ) is the negative binomial distribution given by @xmath223 where @xmath224 denotes a gamma function .",
    "thus it is noted that the maximum likelihood estimator can be obtained by maximizing @xmath225 .",
    "+ * [ 3 ] binomial - beta mixture model .",
    "* let @xmath207 be mutually independent random variables having @xmath226 where @xmath227 are mutually independent , bin@xmath228 denotes the binomial distribution and beta@xmath212 denotes the beta distribution .",
    "let @xmath215 and @xmath229 for @xmath13 .",
    "then the notations in ( [ model ] ) correspond to @xmath230 and @xmath231 and @xmath232 .",
    "the posterior distribution of @xmath233 is beta@xmath234 or beta@xmath235 , so that @xmath36 is written as @xmath236 which is a quadratic and concave function of @xmath4 .",
    "since @xmath237 , @xmath36 is always positive and attains the maximum when @xmath238 or @xmath239 , and @xmath240 when @xmath241 .",
    "thus , the value of @xmath36 is relatively small when @xmath4 is close to @xmath242 or @xmath243 .",
    "when @xmath4 is around @xmath244 , the value of @xmath36 tends to be larger . when a value around @xmath244 is observed for @xmath4 ,",
    "it should be remarked that the conditional mse of the eb given @xmath4 is larger than the unconditional ( or integrated ) mse .    for the binomial - beta mixture model ,",
    "the marginal likelihood is proportional to @xmath245 where @xmath246 denotes a beta function .",
    "then , the mle of the parameters can be obtained as a maximizer of the marginal likelihood .",
    "we here give some comparisons of the conditional and unconditional mses and investigate finite sample performances of the second - order unbiased estimator of the cmse .",
    "we also apply the suggested procedures to real mortality data .",
    "it is interesting to investigate how different the conditional mse is from the unconditional mse .",
    "the major difference between them appears in the leading terms , namely the terms with order @xmath151 in the cmse and mse .",
    "the ratio of the leading term of the cmse to that of the mse is defined by @xmath247,\\ ] ] which is a function of @xmath4 and @xmath2 .",
    "we consider the case that @xmath248 , @xmath249 , @xmath250 and @xmath251 for @xmath13 .",
    "then , the curves of the functions @xmath252 are illustrated figure [ fig : comp ] for the three mixed models : the fay - herriot , poisson - gamma and binomial - beta models . as mentioned before , in the fay - herriot ( or normal - normal mixture ) model , @xmath253 since @xmath254 $ ] . for the poisson - gamma and binomial - beta mixture models , figure [ fig : comp ] tells us about the interesting features of their leading terms in the cmse , namely , the ratio is an increasing function of @xmath4 for the poisson - gamma mixture model , and a concave and quadratic function of @xmath4 for the binomial - beta mixture model .",
    "[ fig1 ]   for the three mixed models   ( the solid , dashed and dotted lines correspond to the fay - herriot , poisson - gamma mixture and binomial - beta mixture models , respectively . ) , title=\"fig:\",width=340 ]    we next investigate the corresponding ratios based on the second - order approximations of the cmse and mse .",
    "let us define @xmath255 by @xmath256,\\ ] ] where @xmath257 and @xmath258 $ ] are given in ( [ cmse ] ) and ( [ umse ] ) , respectively .",
    "since the second - order terms depend on @xmath24 , we treat the three cases of @xmath248 , @xmath259 and @xmath260 for @xmath261 and @xmath262 .",
    "we used @xmath263 for estimation of @xmath2 .",
    "the performances of @xmath255 are illustrated in figure [ fig : comp2 ] for the three mixed models , where the values of @xmath264 are @xmath265 for the fay - herriot model , @xmath266 for the poisson - gamma mixture model , and @xmath267 for the binomial - beta mixture models .",
    "figure [ fig : comp2 ] demonstrates that the second - order terms for the three mixed models do not contribute so much to @xmath255 or the conditional mse .",
    "[ fig2 ]   for the fay - herriot model ( upper left ) , the binomial - beta mixture model ( upper right ) and the poisson - gamma mixture model ( lower )   ( the solid , dashed and dotted lines correspond to the cases of @xmath248 , @xmath259 and @xmath260 , respectively .",
    "the conditioning value denotes @xmath4 . ) , title=\"fig:\",width=226 ]   for the fay - herriot model ( upper left ) , the binomial - beta mixture model ( upper right ) and the poisson - gamma mixture model ( lower )   ( the solid , dashed and dotted lines correspond to the cases of @xmath248 , @xmath259 and @xmath260 , respectively .",
    "the conditioning value denotes @xmath4 . ) , title=\"fig:\",width=226 ]   for the fay - herriot model ( upper left ) , the binomial - beta mixture model ( upper right ) and the poisson - gamma mixture model ( lower )   ( the solid , dashed and dotted lines correspond to the cases of @xmath248 , @xmath259 and @xmath260 , respectively .",
    "the conditioning value denotes @xmath4 . ) , title=\"fig:\",width=226 ]      we investigate finite performances of the second - order unbiased estimator for the conditional mse by simulation .",
    "the mixed models we examine are the poisson - gamma mixture and binomial - beta mixture models where the simple case of @xmath268 without covariates is treated with @xmath269 , @xmath251 and @xmath270 .    in the experiment of simulation ,",
    "let us fix the index of the area of interest as @xmath271 , namely the first area is of interest , and the value of @xmath272 is conditioned .",
    "as seen from the discussion given in section [ sec : com ] , the performances of the conditional mse depend on the value of @xmath272 . in this simulation",
    ", we consider the @xmath273-quantile point , denoted by @xmath274 , of the distribution of @xmath272 and select the five quantiles @xmath274 for @xmath275 , @xmath276 , @xmath277 , @xmath278 and @xmath279 . for the poisson - gamma mixture model",
    ", the marginal distribution of @xmath272 is the negative binomial distribution @xmath280 , and we can obtain the five quantiles @xmath274 from the marginal distribution . for the binomial - beta mixture model , the marginal distribution of @xmath272 is not given as a typical distribution .",
    "thus , we need to calculate numerically @xmath281-quantile values of @xmath272 .    the true values of cmse can be provided based on the simulation with @xmath282 replications . for @xmath283 ,",
    "we generate random variables @xmath284 and @xmath285 , @xmath286 , which are distributed as @xmath287 and @xmath288 . in the @xmath289-th replication , from the sample @xmath290 , we calculate the values of @xmath291 and @xmath292 .",
    "then , the true value of the cmse of @xmath293 can be numerically calculated as @xmath294 for estimation of the hyperparameter @xmath2 , we consider two types of estimators , gt - estimators obtained from estimating equation ( [ ee ] ) and ml - estimators by maximizing the marginal likelihood . we used the parametric bootstrap method given in theorem [ thm : cmseu2 ] .    through the same manner as described above",
    ", we generate another simulated sample with size @xmath295 and calculate the cmse estimate @xmath296 from ( [ estcmse ] ) .",
    "then , we can obtain the relative bias ( rb ) and coefficients of variation ( cv ) for the cmse estimator , which are defined by @xmath297^{1/2}\\bigg/{\\rm cmse}_1,\\end{aligned}\\ ] ] where @xmath298 denotes the cmse estimate in the @xmath299-th replication for @xmath300 .    for @xmath301 , @xmath276 , @xmath302 , @xmath278 and @xmath279 , the values of @xmath274 , @xmath303 , rb and cv for both gt and ml",
    "are reported in table [ tab : sim ] for the two mixed models , where the values of @xmath303 are multiplied by @xmath304 .",
    "table [ tab : sim ] demonstrates that the estimator @xmath305 of the conditional mse performs well for various values of @xmath274 in both models . for @xmath306 , it is biased than @xmath305 , but the cv@xmath307 is smaller than cv@xmath308 , namely the @xmath306 gives stable estimates .",
    "the true value of @xmath309 has a general trend of increase in @xmath274 for the poisson - gamma mixture model , and this coincides with the analytical property discussed in section [ sec : com ] . for the binomial - beta mixture model ,",
    "the true values of @xmath309 are about the same and do not have a feature of concavity explained in section [ sec : com ] .",
    "the values of rb and cv show that the analytical cmse estimator based on the gt - estimator and bootstrap cmse estimator based on the ml - estimator are not bad as an estimator of @xmath309 .",
    "@xmath310    [ tab : sim ]      we now apply the suggested procedures to the two data sets : the stomach cancer mortality data and the infant mortality data before world war ii , both of which are data from prefectures in japan . in this subsection , we use the analytical cmse estimator based on the gt - estimator .    *",
    "( mortality rates estimates in the poisson - gamma mixture model ) .",
    "*   we begin by analyzing the stomach cancer mortality data in japan . the data set consists of the observed number of mortality @xmath311 and its expected number @xmath82 of stomach cancer for women who lived in the @xmath312-th city or town in saitama prefecture , japan , for five years from 1995 to 1999 .",
    "such area - level data @xmath313 , @xmath28 , are available for @xmath314 cities and towns , and the total number of mortality in the whole region is @xmath315 .",
    "the expected numbers are adjusted by age on the basis of the population so that @xmath316 .    for @xmath207",
    ", we use the poisson - gamma mixture model discussed in section [ sec : exm ] , namely @xmath317 and @xmath318 .",
    "since data of mortality rate of stomach cancer for men are also available , we can use them as a covariate .",
    "let @xmath319 be a log - transformed mortality rate for men for @xmath312-th area .",
    "then , we treat the regression model @xmath320 for @xmath13 .",
    "the unknown parameters @xmath321 are estimated as the roots of the estimating equations in ( [ ee ] ) .",
    "their estimates are @xmath322 and @xmath323 .",
    "to illustrate the difference between cmse and mse , we use the percentage relative difference ( rd ) defined by @xmath324 when @xmath325 is positive , @xmath326 is larger than @xmath327 . in figure",
    "[ fig : emp1 ] , the plots of the values @xmath328 multiplied by @xmath329 and the values of @xmath330 for @xmath28 are given in the left and right figures , respectively , where @xmath215 is the standard mortality rate ( smr ) . from figure [ fig : emp1 ] , it is revealed that the values of @xmath326 are larger than those of @xmath327 for some areas , and that the relative differences @xmath325 have great variability , which comes from non - normality of distribution as discussed in section [ sec : com ] .",
    "table [ tab : emp1 ] reports the values of @xmath82 , @xmath4 , @xmath331 , @xmath332 , @xmath333 and @xmath325 for ten selected municipalities in saitama prefecture , where the values of @xmath333 and @xmath332 are multiplied by @xmath329 .",
    "it is noted that kumagaya has the maximum rd value and yoshida has the minimum rd value in our result .",
    "the values of rd tell us about important information that the given empirical bayes estimate has a different prediction error from the usual unconditional mse .",
    "for instance , in yoshida , the estimate of the cmse is @xmath334 , while that of the unconditional mse is @xmath335 , and the resulting rd is @xmath336 .",
    "this means that the unconditional mse over - estimates the cmse . on the other hand , in kumagaya ,",
    "the estimate of the cmse is @xmath337 , while that of the unconditional mse is @xmath338 , and the resulting rd is @xmath339 .",
    "this means that the unconditional mse under - estimates the cmse .",
    "remember that the cmse is a function of both @xmath4 and @xmath82 increasing for @xmath4 and decreasing for @xmath82 in the poisson - gamma model , while the unconditional mse does not depend on @xmath4 and decreases for @xmath82 .",
    "thus , the cmse is not always small in areas with small @xmath82 such as yoshida and naguri , and the unconditional mse may over - estimates the cmse . on the contrary , in area with large @xmath82 such as kumagaya , the unconditional mse may under - estimates the cmse , which leads to a serious situation in real application .",
    "( left ) and plots of @xmath330 ( right ) for stomach cancer mortality data , title=\"fig:\",width=226 ]   ( left ) and plots of @xmath330 ( right ) for stomach cancer mortality data , title=\"fig:\",width=226 ]    @xmath340    [ tab : emp1 ]    * ( infant mortality rates estimates in the binomial - beta mixture model ) . *   we next handle the historical data of the infant mortality data before world war ii . the data set consists of the observed number of infant mortality @xmath311 and the number of birth @xmath82 in the @xmath312-th city or town in ishikawa prefecture , japan , before world war ii .",
    "such area - level data are available for @xmath341 cities , towns and villages , and the total number of infant mortality in the whole region is @xmath342 .",
    "it is noted that the infant mortality rates @xmath215 before world war ii are not small and distributed around @xmath343 .",
    "thus , we here apply the data to the binomial - beta model rather than the poisson - gamma model . for @xmath207 , @xmath344 and @xmath233 have the distributions @xmath345 and @xmath346 , where @xmath347 for @xmath13 , since we do not have any covariates . thus , the unknown parameters are @xmath348 and their estimates are @xmath349 , namely @xmath350 , and @xmath351 .",
    "the plots of the values @xmath328 multiplied by @xmath329 and the values of @xmath330 for @xmath28 are given in the left and right figures of figure [ fig : emp2 ] , respectively .",
    "figure [ fig : emp2 ] suggests that the values of the relative difference rd increases in @xmath4 .",
    "this is because the leading @xmath151 term is an increasing function of @xmath4 for fixed @xmath82 since @xmath4 is between @xmath242 and @xmath277 , as investigated in section [ sec : com ] .",
    "it is observed from figure [ fig : emp2 ] that the unconditional mse under - estimates the cmse in most areas .",
    "this gives us a warning message on the empirical bayes estimates in each area since the unconditional mse underestimates the estimation error of the empirical bayes estimate based on given area data .",
    "table [ tab : emp2 ] reports the values of @xmath82 , @xmath4 , @xmath331 , @xmath332 , @xmath333 and @xmath325 for fifteen selected municipalities in ishikawa prefecture , where the values of @xmath333 and @xmath332 are multiplied by @xmath329 .",
    "it is noted that area 175 has the maximum rd value and area 46 has the minimum rd value in our result . for area 176 ,",
    "the observed mortality rate @xmath352 is much shrunken to @xmath353 by the empirical bayes estimator since the number of birth is quite small as given by @xmath354 .",
    "the unconditional mse is estimated by @xmath355 , but the relative difference is @xmath356 , and the estimate of cmse is @xmath357 , which is higher than the mse estimate .",
    "this suggests that it should be good to provide estimates of cmse as well as estimates of mse .",
    "( left ) and plots of @xmath330 ( right ) for infant mortality data , title=\"fig:\",width=226 ]   ( left ) and plots of @xmath330 ( right ) for infant mortality data , title=\"fig:\",width=226 ]    @xmath358    [ tab : emp2 ]",
    "in this paper , we have derived the second - order approximation of the conditional mse of the empirical bayes estimator and its second - order unbiased estimator in the general mixed models .",
    "those results have been applied to the mixed models based on nef - qvf , and the second - order evaluations of the cmse have been provided in analytical and closed forms for the gt - estimator and the parametric bootstrap method for the ml - estimator without assuming that the sample size @xmath82 goes to infinity .",
    "it has been shown that the difference between the conditional and unconditional mses is small for the normal distribution , while it is significant for the poisson - gamma and the binomial - beta mixture models .",
    "we have also clarified how different the cmse is from the mse by comparing the leading terms in the cmse and mse .",
    "concerning the two measures for evaluating the estimation error , one important issue is which one should go for the conditional or the unconditional approach . in general , this issue depends on what one wants to know as the estimation error . when data of the small area of interest are observed and one wants to know the estimation error of the empirical bayes estimate based on these data , the cmse given the data should be used .",
    "when one wants to know the average estimation error of the estimator , on the other hand , the unconditional mse is employed .",
    "as illustrated in figure [ fig : comp2 ] , however , the ratio of the cmse over the unconditional mse is significant for the poisson - gamma and the binomial - beta mixture models , while it is close to one for the normal fay - herriot model .",
    "the similar discrepancy between the two methods is shown in figures [ fig : emp1 ] and [ fig : emp2 ] , and tables [ tab : emp1 ] and [ tab : emp2 ] for the two examples .",
    "for example , in table [ tab : emp2 ] , the estimated cmse of area 175 is 1.190 , while the estimated unconditional mse is 0.678 .",
    "this gives us a warning message on the value 0.237 of the empirical bayes estimate based on the data from area 175 .",
    "these observations reveal the risk that the unconditional mse sometimes underestimates the estimation error of the empirical bayes estimate based on given area data .",
    "thus , we suggest providing estimates of the cmse .",
    "we would like to thank the associate editor and the reviewer for some important comments which led to an improved version of this paper .",
    "the first author was supported in part by grant - in - aid for scientific research ( 15j10076 ) from japan society for the promotion of science ( jsps ) .",
    "the second author acknowledges support from grant - in - aid for scientific research ( 15h01943 and 26330036 ) , japan .",
    "for notational simplicity , we put @xmath359 and we use @xmath360 as @xmath361 . using the results in ghosh and maiti ( 2004 ) , we immediately have @xmath362 , which implies that @xmath363=\\u^{-1}e\\left[\\s_m\\s_m^t|y_i\\right]\\u^{-1}+o_p(m^{-1}),\\ ] ] where @xmath364&=\\sum_{j=1}^me\\left[\\r_j\\g_j\\g_j^t\\r_j^t|y_i\\right]=\\sum_{j\\neq i}^me\\left[\\r_j\\g_j\\g_j^t\\r_j^t\\right]+\\r_i\\g_i\\g_i^t\\r_i^t\\\\ & = \\u+\\r_i(\\g_i\\g_i^t-\\bsi_i)\\r_i^t,\\end{aligned}\\ ] ] since @xmath365 depends only on @xmath366 of @xmath367 and @xmath20 are mutually independent . since @xmath368 and @xmath369 , we have @xmath370=\\u+o_p(1)$ ] , so that @xmath371=\\u^{-1}+o_p(m^{-1}).\\ ] ]     + next , we evaluate asymptotically the conditional bias of @xmath69 , i.e. @xmath372 $ ] .",
    "expanding the equation ( [ ee ] ) up to second order , we have @xmath373 where @xmath374 noting that @xmath375 , and @xmath376 for @xmath377 with @xmath378 .",
    "it noted that @xmath379 for @xmath380 , where @xmath381 is the @xmath382-th row vector of @xmath383 .",
    "the notation @xmath384 for scalars @xmath385 s , @xmath386 is defined by @xmath387 let @xmath388 , then we have @xmath389 therefore , it follows that @xmath390 whereby @xmath391=\\u^{-1}\\r_i\\g_i+\\frac12\\u^{-1}e\\left[\\t|y_i\\right]+\\u^{-1}e\\left[\\w\\u^{-1}\\s_m|y_i\\right].\\end{aligned}\\ ] ] for the second term in ( [ cb ] ) , note that @xmath392&=\\col_{\\ell}\\bigl\\{e\\left[(\\hbta-\\bta)^t\\left(\\frac{\\pd^2 s_{m\\ell}}{\\pd\\bta\\pd\\bta^t}\\right)(\\hbta-\\bta)\\bigg|y_i\\right]\\bigr\\}\\\\ & = \\col_{\\ell}\\bigl\\{\\tr\\bigl\\ { \\left(\\frac{\\pd^2 s_{m\\ell}}{\\pd\\bta\\pd\\bta^t}\\right ) e\\left[(\\hbta-\\bta)^t(\\hbta-\\bta)\\bigg|y_i\\right]\\bigr\\}\\bigr\\}\\\\ & = \\col_{\\ell}\\bigl\\{\\tr\\bigl(e\\bigl[\\frac{\\pd^2 s_{m\\ell}}{\\pd\\bta\\pd\\bta^t}\\bigr]\\u^{-1}\\bigr)\\bigr\\}+o_p(1)\\equiv \\a_2(\\bta)+o_p(1).\\end{aligned}\\ ] ] the straightforward calculation shows that @xmath393 so that @xmath394=\\sum_{i=1}^m\\left\\{2\\left(\\frac{\\pd \\r_{i\\ell}}{\\pd\\bta}\\right)\\d_i+(\\i_p\\otimes \\r_{i\\ell})e\\left(\\frac{\\pd^2 \\g_i}{\\pd\\bta^t\\pd\\bta}\\right)\\right\\}.\\ ] ] since @xmath395 we obtain @xmath396 whereby @xmath397 then we have @xmath398 for the evaluation of the third term in ( [ cb ] ) , we get @xmath399=\\u^{-1}e\\left[\\w\\u^{-1}\\s_m\\right]+o_p(m^{-1}),\\ ] ] and @xmath400=e\\left[\\left(\\frac{\\pd \\s_m}{\\pd\\bta^t}\\right)\\u^{-1}\\s_m\\right]\\\\ & = \\sum_{i=1}^m\\left(\\frac{\\pd \\r_i}{\\pd\\bta^t}\\right)e\\left[(\\i_p\\otimes \\g_i)\\u^{-1}\\r_i\\g_i\\right]+\\sum_{i=1}^m\\r_{i}e\\left[\\left(\\frac{\\pd \\g_i}{\\pd\\bta^t}\\right)\\u^{-1}\\r_i\\g_i\\right].\\end{aligned}\\ ] ] using the expression ( [ dg ] ) , we finally have @xmath401\\\\ & = \\sum_{i=1}^m\\left(\\frac{\\pd \\r_i}{\\pd\\bta^t}\\right)\\vec(\\d_i\\u^{-1})+2\\sum_{i=1}^m\\r_{i}q(m_i ) \\bigl(\\begin{array}{cc } \\0^t & 0\\\\ \\x_i^t & 0 \\end{array}\\bigr ) \\u^{-1}\\r_i \\bigl(\\begin{array}{c } \\mu_{2i}\\\\   \\mu_{3i } \\end{array}\\bigr ) , \\end{split}\\ ] ] which completes the proof .",
    "the analytical expression of @xmath402 and @xmath403 are complex and not practical . however , the values of these derivatives at some value @xmath404 can be easily calculated .",
    "let @xmath405 be a positive number depending on @xmath24 , then the value of @xmath406 at @xmath407 is evaluated as @xmath408 where @xmath409 is a vector of @xmath242 s other than @xmath382-th element is @xmath243 . since the difference between @xmath410 and @xmath411 at @xmath407 is @xmath412 , the choice @xmath413 does not affect the second - order unbiasedness of the cmse estimator established in theorem [ thm : unef ] . in numerical studies given in this paper , we choose @xmath414 satisfying @xmath413 .",
    "the partial derivative @xmath403 can be numerically evaluated in the same way .",
    "ghosh , m. and maiti , t. ( 2008 ) .",
    "empirical bayes confidence intervals for means of natural exponential family - quadratic variance function distributions with application to small area estimation .",
    "j. statist .",
    "_ , * 35 * , 484 - 495 ."
  ],
  "abstract_text": [
    "<S> the empirical bayes estimators in mixed models are useful for small area estimation in the sense of increasing precision of prediction for small area means , and one wants to know the prediction errors of the empirical bayes estimators based on the data . </S>",
    "<S> this paper is concerned with conditional prediction errors in the mixed models instead of conventional unconditional prediction errors . in the mixed models based on natural exponential families with quadratic variance functions </S>",
    "<S> , it is shown that the difference between the conditional and unconditional prediction errors is significant under distributions far from normality . </S>",
    "<S> especially for the binomial - beta mixed and the poisson - gamma mixed models , the leading terms in the conditional prediction errors are , respectively , a quadratic concave function and an increasing function of the direct estimate in the small area , while the corresponding leading terms in the unconditional prediction errors are constants . </S>",
    "<S> second - order unbiased estimators of the conditional prediction errors are also derived and their performances are examined through simulation and empirical studies .    binomial - beta mixture model ; conditional mean squared error ; fay - herriot model ; mixed model ; natural exponential family with quadratic variance function ; poisson - gamma mixture model ; random effect ; small area estimation </S>"
  ]
}