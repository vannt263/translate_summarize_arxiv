{
  "article_text": [
    "the recent massive work in the area of compressed sensing , surveyed in @xcite , rigorously demonstrated that one can algorithmically recover sparse ( and , more generally , compressible ) signals from incomplete observations .",
    "the simplest model is a @xmath11-dimensional signal @xmath6 with a small number of nonzeros : @xmath12 such signals are called @xmath9-sparse .",
    "we collect @xmath13 nonadaptive linear measurements of @xmath6 , given as @xmath14 where @xmath2 is some @xmath15 by @xmath11 measurement matrix .",
    "we then wish to efficiently recover the signal @xmath6 from its measurements @xmath8 .",
    "a necessary and sufficient condition for exact recovery is that the map @xmath2 be one - to - one on the set of @xmath9-sparse vectors .",
    "cands and tao @xcite proved that under a stronger ( quantitative ) condition , the sparse recovery problem is equivalent to a convex program @xmath16 and therefore is computationally tractable .",
    "this condition is that the map @xmath2 is an almost isometry on the set of @xmath7-sparse vectors :    a measurement matrix @xmath2 satisfies the _ restricted isometry condition _ ( ric ) with parameters @xmath17 for @xmath18 if we have @xmath19    under the restricted isometry condition with parameters @xmath20 , the convex program exactly recovers an @xmath9-sparse signal @xmath6 from its measurements @xmath8 @xcite .",
    "the restricted isometry condition can be viewed as an abstract form of the uniform uncertainty principle of harmonic analysis ( @xcite , see also @xcite and @xcite ) .",
    "many natural ensembles of random matrices , such as partial fourier , bernoulli and gaussian , satisfy the restricted isometry condition with parameters @xmath21 , @xmath22 provided that @xmath23 see e.g. section 2 of @xcite and the references therein . therefore , a computationally tractable exact recovery of sparse signals is possible with the number of measurements @xmath15 roughly proportional to the sparsity level @xmath9 , which is usually much smaller than the dimension @xmath11 .",
    "an important alternative to convex programming is greedy algorithms , which have roots in approximation theory .",
    "a greedy algorithm computes the support of @xmath6 iteratively , at each step finding one or more new elements ( based on some `` greedy '' rule ) and subtracting their contribution from the measurement vector @xmath8 .",
    "the greedy rules vary .",
    "the simplest rule is to pick a coordinate of @xmath24 of the biggest magnitude ; this defines the well known greedy algorithm called orthogonal matching pursuit ( omp ) , known otherwise as orthogonal greedy algorithm ( oga ) @xcite .",
    "greedy methods are usually fast and easy to implement , which makes them popular with practitioners .",
    "for example , omp needs just @xmath9 iterations to find the support of an @xmath9-sparse signal @xmath6 , and each iteration amounts to solving one least - squares problem ; so its running time is always polynomial in @xmath9 , @xmath15 and @xmath11 .",
    "in contrast , no known bounds are known on the running time of as a linear program . future work on customization of convex programming solvers for sparse recovery problems may change this picture , of course . for more discussion ,",
    "see @xcite and @xcite .",
    "a variant of omp was recently found in @xcite that has guarantees essentially as strong as those of convex programming methods .",
    "this greedy algorithm is called regularized orthogonal matching pursuit ( romp ) ; we state it in section  [ s : stable ] below . under the restricted isometry condition with parameters @xmath25",
    ", romp exactly recovers an @xmath9-sparse signal @xmath6 from its measurements @xmath8 .    summarizing ,",
    "_ the uniform uncertainty principle is a guarantee for efficient sparse recovery ; one can provably use either convex programming methods or greedy algorithms ( romp ) . _",
    "a more realistic scenario is where the measurements are inaccurate ( e.g. contaminated by noise ) and the signals are not exactly sparse . in most situations that arise in practice , one can not hope to know the measurement vector @xmath14 with arbitrary precision .",
    "instead , it is perturbed by a small error vector : @xmath1 . here",
    "the vector @xmath5 has unknown coordinates as well as unknown magnitude , and it needs not be sparse ( as all coordinates may be affected by the noise ) . for a recovery algorithm to be stable ,",
    "it should be able to approximately recover the original signal @xmath6 from these perturbed measurements .",
    "the stability of convex optimization algorithms for sparse recovery was studied in @xcite , @xcite , @xcite , @xcite .",
    "assuming that one knows a bound on the magnitude of the error , @xmath26 , it was shown in @xcite that the solution @xmath27 of the convex program @xmath28 is a good approximation to the unknown signal : @xmath29 .",
    "in contrast , the stability of greedy algorithms for sparse recovery has not been well understood .",
    "numerical evidence @xcite suggests that omp should be less stable than the convex program , but no theoretical results have been known in either the positive or negative direction .",
    "the present paper seeks to remedy this situation .",
    "we prove that _ romp is as stable as the convex program .",
    "_ this result essentially closes a gap between convex programming and greedy approaches to sparse recovery .    regularized orthogonal matching pursuit ( romp )    [ t : stability ] assume a measurement matrix @xmath2 satisfies the restricted isometry condition with parameters @xmath30 for @xmath31 .",
    "let @xmath6 be an @xmath9-sparse vector in @xmath32 .",
    "suppose that the measurement vector @xmath33 becomes corrupted , so we consider @xmath1 where @xmath5 is some error vector .",
    "then romp produces a good approximation to @xmath6 : @xmath34    note that in the noiseless situation ( @xmath35 ) the reconstruction is exact : @xmath36 .",
    "this case of theorem  [ t : stability ] was proved in @xcite .",
    "our stability result extends naturally to the even more realistic scenario where the signals are only approximately sparse . here and henceforth ,",
    "denote by @xmath37 the vector of the @xmath38 biggest coefficients in absolute value of @xmath39 .",
    "[ t : stabsig ] assume a measurement matrix @xmath2 satisfies the restricted isometry condition with parameters @xmath40 for @xmath31 .",
    "consider an arbitrary vector @xmath6 in @xmath32 .",
    "suppose that the measurement vector @xmath33 becomes corrupted , so we consider @xmath1 where @xmath5 is some error vector .",
    "then romp produces a good approximation to @xmath41 : @xmath42    * 1 .",
    "* the term @xmath41 in the corollary can be replaced by @xmath43 for any @xmath44 .",
    "this change will only affect the constant terms in the corollary .",
    "* 2 . * by applying corollary  [ t : stabsig ] to the largest @xmath45 coordinates of @xmath6 and using lemma  [ l : ve ] below , we also have the error bound for the entire vector @xmath6 : @xmath46    * 3 .",
    "* for the convex programming method , the stability bound was proved in @xcite , and even without the logarithmic factor .",
    "we conjecture that this factor is also not needed in our results for romp .",
    "* 4 . * unlike the convex program , romp succeeds with absolutely no prior knowledge about the error @xmath5 ; its magnitude can be arbitrary . in the terminology of @xcite , the convex programming approach needs to be `` noise - aware '' while romp needs not .",
    "* one can use romp to approximately compute a @xmath45-sparse vector that is close to _ the best @xmath45-term approximation _",
    "@xmath41 of an arbitrary signal @xmath6 . to this end ,",
    "one just needs to retain the @xmath45 biggest coordinates of @xmath27 .",
    "indeed , corollary  [ c : napprox ] below shows that the best @xmath45-term approximations of the original and the reconstructed signals are close : @xmath47    * 6 . *",
    "an important special case of corollary  [ t : stabsig ] is for the class of compressible vectors , which is a common model in signal processing , see @xcite , @xcite .",
    "suppose @xmath6 is a compressible vector in the sense that its coefficients obey a power law : for some @xmath48 , the @xmath49-th largest coefficient in magnitude of @xmath6 is bounded by @xmath50 .",
    "then yields the following bound on the reconstructed signal : @xmath51 as observed in @xcite , this bound is optimal ( within the logarithmic factor ) ; no algorithm can perform fundamentally better .    the rest of the paper",
    "is organized as follows . in section  [",
    "s : proof ] , we prove our main result , theorem  [ t : stability ] . in section  [",
    "s : consequences ] , we deduce the extension for approximately sparse signals , corollary  [ t : stabsig ] , and a consequence for best @xmath9-term approximations , corollary  [ c : napprox ] . in section  [ s : implementation ] , we demonstrate some numerical experiments that illustrate the stability of romp .",
    "we shall prove a stronger version of theorem  [ t : stability ] , which states that _ at every iteration _ of romp , either at least @xmath52 of the newly selected coordinates are from the support of the signal @xmath6 , or the error bound already holds .",
    "[ t : it ] assume @xmath2 satisfies the restricted isometry condition with parameters @xmath30 for @xmath31 .",
    "let @xmath53 be an @xmath9-sparse vector with measurements @xmath1 . then at any iteration of romp , after the regularization step where @xmath54 is the current chosen index set",
    ", we have @xmath55 and ( at least ) one of the following :    1 .",
    "[ j support ] @xmath56 ; 2 .",
    "[ error ] @xmath57 .",
    "in other words , either at least @xmath52 of the coordinates in the newly selected set @xmath58 belong to the support of @xmath6 or the bound on the error already holds .",
    "we show that the iteration invariant implies theorem  [ t : stability ] by examining the three possible cases :    * case 1 : ( ii ) occurs at some iteration .",
    "* we first note that since @xmath59 is nondecreasing , if ( ii ) occurs at some iteration , then it holds for all subsequent iterations . to show that this would then imply theorem  [ t : stability ]",
    ", we observe that by the restricted isometry condition and since @xmath60 , @xmath61 then again by the restricted isometry condition and definition of @xmath27 , @xmath62 thus we have that @xmath63 thus ( ii ) of the iteration invariant would imply theorem  [ t : stability ] .",
    "* case 2 : ( i ) occurs at every iteration and @xmath58 is always non - empty . * in this case , by ( i ) and the fact that @xmath58 is always non - empty , the algorithm identifies at least one element of the support in every iteration .",
    "thus if the algorithm runs @xmath9 iterations or until @xmath64 , it must be that @xmath65 , meaning that @xmath66 .",
    "then by the argument above for case 1 , this implies theorem  [ t : stability ] .",
    "* case 3 : ( i ) occurs at each iteration and @xmath67 for some iteration .",
    "* by the definition of @xmath58 , if @xmath67 then @xmath68 for that iteration . by definition of @xmath69",
    ", this must mean that @xmath70 this combined with part 1 of proposition  [ p : cons ] below ( and its proof , see  @xcite ) applied with the set @xmath71 yields @xmath72 then combinining this with part 2 of the same proposition , we have @xmath73 since @xmath74 , this means that the error bound ( ii ) must hold , so by case 1 this implies theorem  [ t : stability ] .",
    "we now turn to the proof of the iteration invariant , theorem  [ t : it ] .",
    "we will use the following proposition from @xcite .",
    "[ p : cons ] assume a measurement matrix @xmath2 satisfies the restricted isometry condition with parameters @xmath75",
    ". then the following holds .    1 .",
    "_ ( local approximation ) _ for every @xmath9-sparse vector @xmath0 and every set @xmath76 , @xmath77 , the observation vector @xmath78 satisfies @xmath79 2 .",
    "_ ( spectral norm ) _ for any vector @xmath80 and every set @xmath76 , @xmath81 , we have @xmath82 3 .   _",
    "( almost orthogonality of columns ) _ consider two disjoint sets @xmath83 , @xmath84 .",
    "let @xmath85 denote the orthogonal projections in @xmath86 onto @xmath87 and @xmath88 , respectively . then @xmath89    the proof of theorem  [ t : it ] is by induction on the iteration of romp .",
    "the induction claim is that for all previous iterations , the set of newly chosen indices is disjoint from the set of previously chosen indices @xmath54 , and either ( i ) or ( ii ) holds . clearly if ( ii ) held in a previous iteration , it would hold in all future iterations .",
    "thus we may assume that ( ii ) has not yet held .",
    "since ( i ) has held at each previous iteration , we must have @xmath90    let @xmath91 be the residual at the start of this iteration , and let @xmath58 , @xmath92 be the sets found by romp in this iteration . as in @xcite",
    ", we consider the subspace @xmath93 and its complementary subspaces @xmath94 the restricted isometry condition in the form of part  3 of proposition  [ p : cons ] ensures that @xmath95 and @xmath96 are almost orthogonal . thus @xmath96 is close to the orthogonal complement of @xmath95 in @xmath97 , @xmath98    the residual @xmath69 thus still has a simple description :    [ residual ] here and thereafter , let @xmath99 denote the orthogonal projection in @xmath86 onto a linear subspace @xmath100 . then @xmath101    by definition of the residual in the algorithm , @xmath102 . to complete the proof",
    "we need that @xmath103 .",
    "this follows from the orthogonal decomposition @xmath104 and the fact that @xmath105 .",
    "now we consider the signal we seek to identify at the current iteration , and its measurements : @xmath106 to guarantee a correct identification of @xmath107 , we first state two approximation lemmas that reflect in two different ways the fact that subspaces @xmath96 and @xmath108 are close to each other .",
    "[ c : proj ] we have @xmath109    by definition of @xmath95 , we have @xmath110 .",
    "therefore , by lemma  [ residual ] , @xmath111 , and so @xmath112 now we use part 3 of proposition  [ p : cons ] for the sets @xmath54 and @xmath113 whose union has cardinality at most @xmath114 by .",
    "it follows that @xmath115 as desired .",
    "[ l : uj ] consider the observation vectors @xmath116 and @xmath117 . then for any set @xmath118 with @xmath119 , @xmath120    since @xmath121",
    ", we have by lemma  [ c : proj ] and the restricted isometry condition that @xmath122 to complete the proof , it remains to apply part 2 of proposition  [ p : cons ] , which yields @xmath123 .",
    "we next show that the energy ( norm ) of @xmath124 when restricted to @xmath92 , and furthermore to @xmath58 , is not too small . by the regularization step of romp , since all selected coefficients have comparable magnitudes , we will conclude that not only a portion of energy but also of the _ support _ is selected correctly , or else the error bound must already be attained .",
    "this will be the desired conclusion .",
    "[ c : uj ] we have @xmath125 .",
    "let @xmath126 = @xmath113 .",
    "since @xmath127 , the maximality property of @xmath92 in the algorithm implies that @xmath128 by lemma  [ l : uj ] , @xmath129 furthermore , since @xmath130 , by part 1 of proposition  [ p : cons ] we have @xmath131 putting these three inequalities together , we conclude that @xmath132 this proves the lemma .",
    "we next bound the norm of @xmath124 restricted to the smaller set @xmath58 , again using the general property of regularization . in our context , lemma  3.7 of @xcite applied to the vector @xmath133 yields @xmath134 along with lemma  [ c : uj ] this directly implies :    [ c : uj0 ] we have @xmath135",
    "we now finish the proof of theorem  [ t : it ] . the claim that @xmath55 follows by the same arguments as in @xcite .",
    "the nontrivial part of the theorem is its last claim , that either ( i ) or ( ii ) holds .",
    "suppose ( i ) in the theorem fails .",
    "namely , suppose that @xmath136 , and thus @xmath137 set @xmath138 . by the comparability property of the coordinates in @xmath58 and since @xmath139 , there is a fraction of energy in @xmath140 : @xmath141 where the last inequality holds by lemma  [ c : uj0 ] .    on the other hand , we can approximate @xmath124 by @xmath142 as @xmath143 since @xmath144 , @xmath145 , and using lemma  [ l : uj ] , we have @xmath146 furthermore , by definition of @xmath107 , we have @xmath147 .",
    "so , by part 1 of proposition  [ p : cons ] , @xmath148 using the last two inequalities and , we conclude that @xmath149 this is a contradiction to  ( [ e : ubig ] ) so long as @xmath150 if this is true , then indeed ( i ) in the theorem must hold .",
    "if it is not true , then by the choice of @xmath151 , this implies that @xmath152 this proves theorem  [ t : it ] .",
    "next we turn to the proof of corollary  [ t : stabsig ] .",
    "we first partition @xmath6 so that @xmath153 . then since @xmath2 satisfies the restricted isometry condition with parameters @xmath40 , by theorem  [ t : stability ] and the triangle inequality , @xmath154 the following lemma as in @xcite relates the @xmath155-norm of a vector s tail to its @xmath156-norm .",
    "an application of this lemma combined with will prove corollary  [ t : stabsig ] .",
    "[ l : ve ] let @xmath157 , and let @xmath158 be the vector of the @xmath38 largest coordinates in absolute value from @xmath159",
    ". then @xmath160    let @xmath161 denote the @xmath162 largest entry of @xmath159 . if @xmath163 then @xmath164 so the claim holds .",
    "thus we may assume this is not the case .",
    "then we have @xmath165 simplifying gives the desired result .    by lemma 29 of @xcite",
    ", we have @xmath166 applying lemma  [ l : ve ] to the vector @xmath167 we then have @xmath168 combined with , this proves the corollary .",
    "we now show that by truncating the reconstructed vector , we obtain a @xmath45-sparse vector very close to the original signal .",
    "[ c : napprox]assume a measurement matrix @xmath2 satisfies the restricted isometry condition with parameters @xmath40 for @xmath31 .",
    "let @xmath6 be an arbitrary vector in @xmath32 , let @xmath169 be the measurement vector , and @xmath27 the reconstructed vector output by the romp algorithm . then @xmath170 where @xmath171 denotes the best @xmath38-sparse approximation to @xmath172 ( i.e. the vector consisting of the largest @xmath38 coordinates in absolute value ) .",
    "let @xmath173 and @xmath174 , and let @xmath126 and @xmath175 denote the supports of @xmath176 and @xmath177 respectively . by corollary  [ t : stabsig ] , it suffices to show that @xmath178 .    applying the triangle inequality ,",
    "we have @xmath179 we then have @xmath180 and @xmath181 since @xmath182 , we have @xmath183 . by the definition of @xmath175 , every coordinate of @xmath27 in @xmath175 is greater than or equal to every coordinate of @xmath27 in @xmath184 in absolute value .",
    "thus we have , @xmath185 thus @xmath186 , and so @xmath187 this completes the proof .",
    "* corollary  [ c : napprox ] combined with corollary  [ t : stabsig ] and implies that we can also estimate a bound on the whole signal @xmath6 : @xmath188",
    "this section describes our experiments that illustrate the stability of romp .",
    "we experimentally examine the recovery error using romp for both perturbed measurements and signals .",
    "the empirical recovery error is actually much better than that given in the theorems .",
    "first we describe the setup of our experiments . for many values of the ambient dimension @xmath11 , the number of measurements @xmath15 , and the sparsity @xmath9 , we reconstruct random signals using romp . for each set of values , we perform @xmath189 trials . initially , we generate an @xmath3 gaussian measurement matrix @xmath2 . for each trial , independent of the matrix , we generate an @xmath9-sparse signal @xmath6 by choosing @xmath9 components uniformly at random and setting them to one . in the case of perturbed signals , we add to the signal a @xmath11-dimensional error vector with gaussian entries . in the case of perturbed measurements ,",
    "we add an @xmath15-dimensional error vector with gaussian entries to the measurement vector @xmath33 .",
    "we then execute romp with the measurement vector @xmath14 or @xmath190 in the perturbed measurement case .",
    "after romp terminates , we output the reconstructed vector @xmath27 obtained from the least squares calculation and calculate its distance from the original signal .",
    "figure  [ fig : meas2 ] depicts the recovery error @xmath191 when romp was run with perturbed measurements .",
    "this plot was generated with @xmath192 for various levels of sparsity @xmath9 .",
    "the horizontal axis represents the number of measurements @xmath15 , and the vertical axis represents the average normalized recovery error .",
    "figure  [ fig : meas2 ] confirms the results of theorem  [ t : stability ] , while also suggesting the bound may be improved by removing the @xmath193 factor .",
    "figure  [ fig : sig4 ] depicts the normalized recovery error when the signal was perturbed by a gaussian vector .",
    "the figure confirms the results of corollary  [ t : stabsig ] while also suggesting again that the logarithmic factor in the corollary is unnecessary ."
  ],
  "abstract_text": [
    "<S> we demonstrate a simple greedy algorithm that can reliably recover a vector @xmath0 from incomplete and inaccurate measurements @xmath1 . here </S>",
    "<S> @xmath2 is a @xmath3 measurement matrix with @xmath4 , and @xmath5 is an error vector . </S>",
    "<S> our algorithm , regularized orthogonal matching pursuit ( romp ) , seeks to close the gap between two major approaches to sparse recovery . </S>",
    "<S> it combines the speed and ease of implementation of the greedy methods with the strong guarantees of the convex programming methods .    for any measurement matrix @xmath2 that satisfies a uniform uncertainty principle </S>",
    "<S> , romp recovers a signal @xmath6 with @xmath7 nonzeros from its inaccurate measurements @xmath8 in at most @xmath9 iterations , where each iteration amounts to solving a least squares problem . </S>",
    "<S> the noise level of the recovery is proportional to @xmath10 . </S>",
    "<S> in particular , if the error term @xmath5 vanishes the reconstruction is exact .    </S>",
    "<S> this stability result extends naturally to the very accurate recovery of approximately sparse signals . </S>"
  ]
}