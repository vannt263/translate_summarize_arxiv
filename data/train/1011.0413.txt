{
  "article_text": [
    "cur decompositions are a recently - popular class of randomized algorithms that approximate a data matrix @xmath1 by using only a small number of actual columns of @xmath0  @xcite .",
    "cur decompositions are often described as svd - like low - rank decompositions that have the additional advantage of being easily interpretable to domain scientists . the motivation to produce a more interpretable low - rank decomposition is also shared by sparse pca ( spca ) methods , which are optimization - based procedures that have been of interest recently in statistics and machine learning .",
    "although cur and spca methods start with similar motivations , they proceed very differently .",
    "for example , most cur methods have been randomized , and they take a purely algorithmic approach . by contrast , most spca methods start with a combinatorial optimization problem , and they then solve a relaxation of this problem .",
    "thus far , it has not been clear to researchers how the cur and spca approaches are related .",
    "it is the purpose of this paper to understand cur decompositions from a sparse optimization viewpoint , thereby elucidating the connection between cur decompositions and the spca class of sparse optimization methods .",
    "to do so , we begin by putting forth a combinatorial optimization problem ( see below ) which cur is implicitly approximately optimizing .",
    "this formulation will highlight two interesting features of cur : first , cur attains a distinctive pattern of sparsity , which has practical implications from the spca viewpoint ; and second , cur is implicitly optimizing a regression - type objective .",
    "these two observations then lead to the three main contributions of this paper : ( a ) first , we formulate a non - randomized optimization - based version of cur ( see problem 1 : gl - reg   in section  [ sec : cur - optim - fram ] ) that is based on a convex relaxation of the cur combinatorial optimization problem ; ( b ) second , we show that , in contrast to the original pca - based motivation for cur , cur s implicit objective can not be directly expressed in terms of a pca - type objective ( see theorem  [ thm : cur - not - pca ] in section  [ sec : connections - pca ] ) ; and ( c ) third , we propose an spca approach ( see problem 2 : gl - spca   in section  [ sec : group - lasso - pca ] ) that achieves the sparsity structure of cur within the pca framework .",
    "we also provide a brief empirical evaluation of our two proposed objectives .",
    "while our proposed gl - reg   and gl - spca   methods are promising in and of themselves , our purpose in this paper is not to explore them as alternatives to cur ; instead , our goal is to use them to help clarify the connection between cur and spca methods .",
    "we conclude this introduction with some remarks on notation .",
    "given a matrix @xmath2 , we use @xmath3 to denote its @xmath4th row ( as a row - vector ) and @xmath5 its @xmath4th column . similarly , given a set of indices",
    "@xmath6 , @xmath7 and @xmath8 denote the submatrices of @xmath2 containing only these @xmath6 rows and columns , respectively . finally , we let @xmath9 denote the column space of @xmath2 .",
    "in this section , we provide a brief background on cur and spca methods , with a particular emphasis on topics to which we will return in subsequent sections . before doing so , recall that , given an input matrix @xmath0 , principal component analysis ( pca ) seeks the @xmath10-dimensional hyperplane with the lowest reconstruction error .",
    "that is , it computes a @xmath11 orthogonal matrix @xmath12 that minimizes @xmath13 writing the svd of @xmath0 as @xmath14 , the minimizer of is given by @xmath15 , the first @xmath10 columns of @xmath16 . in the data analysis",
    "setting , each column of @xmath16 provides a particular linear combination of the columns of @xmath0 .",
    "these linear combinations are often thought of as latent factors . in many applications , interpreting such factors is made much easier if they are comprised of only a small number of actual columns of @xmath0 , which is equivalent to @xmath15 only having a small number of nonzero elements .",
    "cur decompositions were proposed by drineas and mahoney  @xcite to provide a low - rank approximation to a data matrix @xmath0 by using only a small number of actual columns and/or rows of @xmath0 .",
    "fast randomized variants  @xcite , deterministic variants  @xcite , nystrm - based variants  @xcite , and heuristic variants  @xcite have also been considered . observing that the best rank-@xmath10 approximation to the svd provides the best set of @xmath10 linear combinations of all the columns",
    ", one can ask for the best set of @xmath10 _ actual _ columns .",
    "most formalizations of `` best '' lead to intractable combinatorial optimization problems  @xcite , but one can take advantage of oversampling ( choosing slightly more than @xmath10 columns ) and randomness as computational resources to obtain strong quality - of - approximation guarantees .",
    "[ thm : cur ] given an arbitrary matrix @xmath1 and an integer @xmath10 , there exists a randomized algorithm that chooses a random subset @xmath17 of size @xmath18 such that @xmath19 , the @xmath20 submatrix containing those @xmath21 columns of @xmath0 , satisfies @xmath22 with probability at least @xmath23 , where @xmath24 is the best rank @xmath10 approximation to @xmath0 .",
    "the algorithm referred to by theorem  [ thm : cur ] is very simple :    compute the _ normalized statistical leverage scores _ , defined below in ( [ eqn : col_probs ] ) .",
    "form @xmath6 by randomly sampling @xmath21 columns of @xmath0 , using these normalized statistical leverage scores as an importance sampling distribution .",
    "return the @xmath25 matrix @xmath26 consisting of these selected columns .",
    "the key issue here is the choice of the importance sampling distribution .",
    "let the @xmath27 matrix @xmath15 be the top-@xmath10 right singular vectors of @xmath0 .",
    "then the _ normalized statistical leverage scores _ are @xmath28 for all @xmath29 , where @xmath30 denotes the @xmath4-th row of @xmath15 .",
    "these scores , proportional to the euclidean norms of the _ rows _ of the top-@xmath10 right singular vectors , define the relevant nonuniformity structure to be used to identify good ( in the sense of theorem  [ thm : cur ] ) columns .",
    "in addition , these scores are proportional to the diagonal elements of the projection matrix onto the top-@xmath10 right singular subspace . thus , they generalize the so - called hat matrix  @xcite , and they have a natural interpretation as capturing the `` statistical leverage '' or `` influence '' of a given column on the best low - rank fit of the data matrix  @xcite .",
    "spca methods attempt to make pca easier to interpret for domain experts by finding sparse approximations to the _ columns _ of @xmath16 . and not in the left singular vectors @xmath31 .",
    "this is similar to considering only the choice of columns and not of both columns and rows in cur .",
    "] there are several variants of spca .",
    "for example , jolliffe _ et al . _",
    "@xcite and witten _ et al . _",
    "@xcite use the maximum variance interpretation of pca and provide an optimization problem which explicitly encourages sparsity in @xmath16 based on a lasso constraint  @xcite .",
    "et al . _",
    "@xcite take a similar approach , but instead formulate the problem as an sdp .",
    "zou _ et al . _",
    "@xcite use the minimum reconstruction error interpretation of pca to suggest a different approach to the spca problem ; this formulation will be most relevant to our present purpose .",
    "they begin by formulating pca as the solution to a regression - type problem .",
    "[ thm : pca - reg ] given an arbitrary matrix @xmath1 and an integer @xmath10 , let @xmath2 and @xmath12 be @xmath27 matrices .",
    "then , for any @xmath32 , let @xmath33 then , the minimizing matrices @xmath34 and @xmath35 satisfy @xmath36 and @xmath37 , where @xmath38 or @xmath39 .",
    "that is , up to signs , @xmath34 consists of the top-@xmath10 right singular vectors of @xmath0 , and @xmath35 consists of those same vectors `` shrunk '' by a factor depending on the corresponding singular value . given this regression - type characterization of pca , zou _ et al . _",
    "@xcite then `` sparsify '' the formulation by adding an @xmath40 penalty on  @xmath12 : @xmath41 where @xmath42 .",
    "this regularization tends to sparsify @xmath12 element - wise , so that the solution @xmath35 gives a sparse approximation of @xmath15 .",
    "in this section , we present an optimization formulation of cur .",
    "recall , from section  [ sec : cur ] , that cur takes a purely algorithmic approach to the problem of approximating a matrix in terms of a small number of its columns .",
    "that is , it achieves sparsity indirectly by randomly selecting @xmath21 columns , and it does so in such a way that the reconstruction error is small with high probability ( theorem [ thm : cur ] ) .",
    "by contrast , spca methods are generally formulated as the exact solution to an optimization problem .    from theorem [ thm :",
    "cur ] , it is clear that cur seeks a subset @xmath6 of size @xmath21 for which @xmath43 is small . in this sense",
    ", cur can be viewed as a randomized algorithm for approximately solving the following combinatorial optimization problem : @xmath44 in words , this objective asks for the subset of @xmath21 columns of @xmath0 which best describes the entire matrix @xmath0 . notice that relaxing @xmath45 to @xmath46 does not affect the optimum .",
    "this optimization problem is analogous to all - subsets multivariate regression @xcite , which is known to be np - hard .",
    "however , by using ideas from the optimization literature we can approximate this combinatorial problem as a regularized regression problem that is convex .",
    "first , notice that is equivalent to @xmath47 where we now optimize over a @xmath48 matrix @xmath49 . to see the equivalence between and ,",
    "note that the constraint in is the same as finding some subset @xmath6 with @xmath46 such that @xmath50 .",
    "the formulation in provides a natural entry point to proposing a convex optimization approach corresponding to cur .",
    "first notice that uses an @xmath51 norm on the rows of @xmath49 , which is not convex .",
    "however , we can approximate the @xmath51 constraint by a _ group lasso _ penalty , which uses a well - known convex heuristic proposed by yuan _ et al . _",
    "@xcite that encourages prespecified _ groups _ of parameters to be simultaneously sparse .",
    "thus , the combinatorial problem in can be approximated by the following convex ( and thus tractable ) problem :    [ prob : glreg ] given an arbitrary matrix @xmath1 , let @xmath52 and @xmath53 .",
    "the gl - reg   problem is to solve @xmath54 where @xmath55 is chosen to get @xmath21 nonzero rows in @xmath56 .",
    "since the rows of @xmath49 are grouped together in the penalty @xmath57 , the row vector @xmath58 will tend to be either dense or entirely zero .",
    "note also that the algorithm to solve problem  [ prob : glreg ] is a special case of algorithm  [ alg : grouplasso ] ( see below ) , which solves the gl - spca   problem , to be introduced later .",
    "( finally , as a side remark , note that our proposed gl - reg   is strikingly similar to a recently proposed method for sparse inverse covariance estimation  @xcite . )",
    "our original intention in casting cur in the optimization framework was to understand better whether cur could be seen as an spca - type method .",
    "so far , we have established cur s connection to regression by showing that cur can be thought of as an approximation algorithm for the sparse regression problem  . in this section ,",
    "we discuss the relationship between regression and pca , and we show that cur can not be directly cast as an spca method .    to do this , recall that regression , in particular `` self '' regression , finds a @xmath59 that minimizes @xmath60 on the other hand , pca - type methods find a set of directions @xmath12 that minimize @xmath61 here , unlike in",
    ", we do not assume that @xmath12 is orthogonal , since the minimizer produced from spca methods is often not required to be orthogonal ( recall section [ sec : regul - sparse - pca ] ) .",
    "clearly , with no constraints on @xmath49 or @xmath12 , we can trivially achieve zero reconstruction error in both cases by taking @xmath62 and @xmath12 any @xmath48 full - rank matrix .",
    "however , with additional constraints , these two problems can be very different .",
    "it is common to consider sparsity and/or rank constraints .",
    "we have seen in section [ sec : cur - optim - fram ] that cur effectively requires @xmath49 to be row - sparse ; in the standard pca setting , @xmath12 is taken to be rank @xmath10 ( with @xmath63 ) , in which case is minimized by @xmath15 and obtains the optimal value @xmath64 ; finally , for spca , @xmath12 is further required to be sparse .    to illustrate the difference between the reconstruction errors and when extra constraints are imposed , consider the 2-dimensional toy example in figure  [ fig : recon ] . in this example",
    ", we compare regression with a row - sparsity constraint to pca with both rank and sparsity constraints . with @xmath65 , we plot @xmath66 against @xmath67 as the solid points in both plots of figure  [ fig : recon ] .",
    "constraining @xmath68 ( giving row - sparsity , as with cur methods ) , becomes @xmath69 , which is a simple linear regression , represented by the black thick line and minimizing the sum of squared vertical errors as shown .",
    "the red line ( left plot ) shows the first principal component direction , which minimizes @xmath70 among all rank - one matrices @xmath12 . here , @xmath70 is the sum of squared projection distances ( red dotted lines ) . finally ,",
    "if @xmath12 is further required to be sparse in the @xmath66 direction ( as with spca methods ) , we get the rank - one , sparse projection represented by the green line in figure  [ fig : recon ] ( right ) .",
    "the two sets of dotted lines in each plot clearly differ , indicating that their corresponding reconstruction errors are different as well .",
    "since we have shown that cur is minimizing a regression - based objective , this toy example suggests that cur may not in fact be optimizing a pca - type objective such as .",
    "next , we will make this intuition more precise .",
    "[ c][c]regression [ c][c]pca [ c][c]spca [ c][c]@xmath67 [ c][c]@xmath66 [ c][c ]  error   [ c][c ]  error      the first step to showing that cur is an spca method would be to produce a matrix @xmath71 for which @xmath72 , _ i.e. _ to express cur s approximation in the form of an spca approximation",
    ". however , this equality implies @xmath73 , meaning that @xmath74 .",
    "if such a @xmath71 existed , then clearly latexmath:[${\\textsc{err}}({{\\mathbf v}_\\textsc{cur } } ) =    and so cur could be regarded as implicitly performing sparse pca in the sense that ( a ) @xmath71 is sparse ; and ( b ) by theorem [ thm : cur ] ( with high probability ) , @xmath76 .",
    "thus , the existence of such a @xmath71 would cast cur directly as a randomized approximation algorithm for spca .",
    "however , the following theorem states that unless an unrealistic constraint on @xmath0 holds , there does not exist a matrix @xmath71 for which @xmath77 .",
    "the larger implication of this theorem is that cur can not be directly viewed as an spca - type method .",
    "let @xmath17 be an index set and suppose @xmath78 satisfies @xmath79 .",
    "then , @xmath80 unless @xmath81 , in which case `` @xmath82 '' holds .",
    "[ thm : cur - not - pca ]    @xmath83    the last inequality is strict unless @xmath84 .",
    "although cur can not be directly cast as an spca - type method , in this section we propose a sparse pca approach ( which we call the group lasso spca or gl - spca ) that accomplishes something very close to cur .",
    "our proposal produces a @xmath85 that has rows that are entirely zero , and it is motivated by the following two observations about cur .",
    "first , following from the definition of the leverage scores , cur chooses columns of @xmath0 based on the norm of their corresponding rows of @xmath15 .",
    "thus , it essentially `` zeros - out '' the rows of @xmath15 with small norms ( in a probabilistic sense ) .",
    "second , as we have noted in section  [ sec : connections - pca ] , if cur could be expressed as a pca method , its principal directions matrix `` @xmath71 '' would have @xmath86 rows that are entirely zero , corresponding to removing those columns of @xmath0 .",
    "recall that zou _ et al . _",
    "@xcite obtain a sparse @xmath85 by including in  ( [ eq : zou2 ] ) an additional @xmath40 penalty from the optimization problem  .",
    "since the @xmath40 penalty is on the entire matrix viewed as a vector , it encourages only unstructured sparsity . to achieve the cur - type row sparsity , we propose the following modification of  :    [ prob : cur - spca ] given an arbitrary matrix @xmath1 and an integer @xmath10 , let @xmath2 and @xmath12 be @xmath27 matrices , and let @xmath87 .",
    "the gl - spca   problem is to solve @xmath88    thus , the lasso penalty @xmath89 in   is replaced in   by a group lasso penalty @xmath90 , where rows of @xmath12 are grouped together so that each row of @xmath85 will tend to be either dense or entirely zero .",
    "importantly , the gl - spca   problem is not convex in @xmath12 and @xmath2 together ; it is , however , convex in @xmath12 , and it is easy to solve in @xmath2 .",
    "thus , analogous to the treatment in zou _",
    "et al . _",
    "@xcite , we propose an iterative alternate - minimization algorithm to solve gl - spca .",
    "this is described in algorithm  [ alg : grouplasso ] ; and the justification of this algorithm is given in section [ sec : deriv - alg ] .",
    "note that if we fix @xmath2 to be @xmath91 throughout , then algorithm  [ alg : grouplasso ] can be used to solve the gl - reg   problem discussed in section  [ sec : cur - optim - fram ] .",
    "we remark that such row - sparsity in @xmath85 can have either advantages or disadvantages .",
    "consider , for example , when there are a small number of informative columns in @xmath0 and the rest are not important for the task at hand  @xcite .",
    "in such a case , we would expect that enforcing entire rows to be zero would lead to better identification of the signal columns ; and this has been empirically observed in the application of cur to dna snp analysis  @xcite .",
    "the unstructured @xmath85 , by contrast , would not be able to `` borrow strength '' across all columns of @xmath85 to differentiate the signal columns from the noise columns .",
    "on the other hand , requiring such structured sparsity is more restrictive and may not be desirable .",
    "for example , in microarray analysis in which we have measured @xmath92 genes on @xmath93 patients , our goal may be to find several underlying factors .",
    "biologists have identified `` pathways '' of interconnected genes  @xcite , and it would be desirable if each sparse factor could be identified with a different pathway ( that is , a different set of genes ) . requiring all factors of @xmath85 to exclude the same @xmath86 genes does not allow a different sparse subset of genes to be active in each factor .",
    "we finish this section by pointing out that while most spca methods only enforce unstructured zeros in @xmath85 , the idea of having a structured sparsity in the pca context has very recently been explored  @xcite .",
    "our gl - spca   problem falls within the broad framework of this idea .",
    "[ alg : grouplasso ]",
    "in this section , we evaluate the performance of the four methods discussed above on both synthetic and real data .",
    "in particular , we compare the randomized cur algorithm of mahoney and drineas  @xcite to our gl - reg   ( of problem  [ prob : glreg ] ) , and we compare the spca algorithm proposed by zou _",
    "_  @xcite to our gl - spca   ( of problem  [ prob : cur - spca ] ) .",
    "we have also compared against the spca algorithm of witten _ et al . _",
    "@xcite , and we found the results to be very similar to those of zou _ et al . _",
    "we first consider synthetic examples of the form @xmath94 where @xmath95 is the underlying signal matrix and @xmath96 is a matrix of noise . in all our simulations , @xmath96 has i.i.d .",
    "@xmath97 entries , while the signal @xmath95 has one of the following forms :    1 .   @xmath98 $ ] where the @xmath20 matrix @xmath99 is the nonzero part of @xmath95 . in other words",
    ", @xmath95 has @xmath21 nonzero columns and does not necessarily have a low - rank structure .",
    "2 .   @xmath100 where @xmath31 and @xmath16 each consist of @xmath63 orthogonal columns .",
    "in addition to being low - rank , @xmath16 has entire rows equal to zero ( _ i.e. _ it is row - sparse ) .",
    "3 .   @xmath100 where @xmath31 and @xmath16 each consist of @xmath63 orthogonal columns . here",
    "@xmath16 is low - rank and sparse , but the sparsity is not structured ( _ i.e. _ it is scattered - sparse ) .",
    "a successful method attains low reconstruction error of the true signal @xmath95 and has high precision in identifying correctly the zeros in the underlying model .",
    "as previously discussed , the four methods optimize for different types of reconstruction error .",
    "thus , in comparing cur and gl - reg , we use the regression - type reconstruction error @xmath101 , whereas for the comparison of spca and gl - spca , we use the pca - type error @xmath102    table  [ tab : simulation ] presents the simulation results from the three cases .",
    "all comparisons use @xmath103 and @xmath104 . in case ii and iii",
    ", the signal matrix has rank @xmath105 .",
    "the underlying sparsity level is @xmath106 , _",
    "i.e. _ @xmath107 of the entries of @xmath95 ( case i ) and @xmath16 ( case ii&iii ) are zeros .",
    "note that all methods except for gl - reg   require the rank @xmath10 as an input , and we always take it to be 10 even in case i. for easy comparison , we have tuned each method to have the correct total number of zeros .",
    "the results are averaged over 5 trials .",
    ".simulation results :  the reconstruction errors and the percentages of correctly identified zeros ( in parentheses ) . [ cols=\"<,<,<,<,<\",options=\"header \" , ]     we notice in table  [ tab : simulation ] that the two regression - type methods cur and gl - reg   have very similar performance . as we would expect , since cur only uses information in the top @xmath10 singular vectors , it does slightly worse than gl - reg   in terms of precision when the underlying signal is not low - rank ( case i ) .",
    "in addition , both methods perform poorly if the sparsity is not structured as in case iii .",
    "the two pca - type methods perform similarly as well . again , the group lasso method seems to work better in case i. we note that the precisions reported here are based on element - wise sparsity  if we were measuring row - sparsity , methods like spca would perform poorly since they do not encourage entire rows to be zero .",
    "we next consider a microarray dataset of soft tissue tumors studied by nielsen _",
    "et al . _",
    "mahoney and drineas  @xcite apply cur to this dataset of @xmath108 tissue samples and @xmath109 genes . as with the simulation results , we use two sets of comparisons",
    ": we compare cur with gl - reg , and we compare spca with gl - spca .",
    "since we do not observe the underlying truth @xmath95 , we take @xmath110 and @xmath111 also , since we do not observe the true sparsity , we can not measure the precision as we do in table [ tab : simulation ] .",
    "the left plot in figure [ fig : pnas_compare ] shows @xmath112 as a function of @xmath113 .",
    "we see that cur and gl - reg   perform similarly .",
    "( however , since cur is a randomized algorithm , on every run it gives a different result . from a practical standpoint",
    ", this feature of cur can be disconcerting to biologists wanting to report a single set of important genes . in this light , gl - reg",
    "may be thought of as an attractive non - randomized alternative to  cur . )",
    "the right plot of figure [ fig : pnas_compare ] compares gl - spca   to spca ( specifically , zou _ et al . _",
    "since spca does not explicitly enforce row - sparsity , for a gene to be not used in the model requires _ all _ of the ( @xmath114 ) columns of @xmath85 to exclude it .",
    "this likely explains the advantage of gl - spca   over spca seen in the figure .",
    "[ c][c]@xmath112 [ c][c]@xmath115 [ c][c]number of genes used [ c][c]*microarray dataset * [ c][c]gl - reg [ c][c]gl - spca [ c][c]cur [ c][c]spca",
    "the algorithm alternates between minimizing with respect to @xmath2 and @xmath49 until convergence .    * solving for @xmath2 given @xmath49 :  * if @xmath49 is fixed , then the regularization penalty in   can be ignored , in which case the optimization problem becomes @xmath116 subject to @xmath117 .",
    "this problem was considered by zou _",
    "et al . _",
    "@xcite , who showed that the solution is obtained by computing the svd of @xmath118 as @xmath119 and then setting @xmath120 .",
    "this explains step  [ step : agivenb ] in algorithm  [ alg : grouplasso ] .",
    "* solving for @xmath49 given @xmath2 : * if @xmath2 is fixed , then   becomes an unconstrained convex optimization problem in @xmath49 .",
    "the subgradient equations ( using that @xmath121 ) are @xmath122 where the subgradient vectors @xmath123 if @xmath124 , or @xmath125 if @xmath126 .",
    "let us define @xmath127 so that the subgradient equations can be written as @xmath128    the following claim explains step  [ step : if ] in algorithm  [ alg : grouplasso ] .",
    "[ claim1 ] @xmath126 if and only if @xmath129 .    first , if @xmath126 , the subgradient equations   become @xmath130 . since @xmath125 if @xmath126 , we have @xmath129 . to prove the other direction ,",
    "recall that @xmath124 implies @xmath123 . substituting this expression into , rearranging terms , and taking the norm on both sides , we get @xmath131    by claim  [ claim1 ]",
    ", @xmath132 implies that @xmath133 which further implies @xmath123 .",
    "substituting into gives step  [ step : else ] in algorithm  [ alg : grouplasso ] .",
    "in this paper , we have elucidated several connections between two recently - popular matrix decomposition methods that adopt very different perspectives on obtaining interpretable low - rank matrix decompositions . in doing so ,",
    "we have suggested two optimization problems , gl - reg   and gl - spca , that highlight similarities and differences between the two methods . in general ,",
    "spca methods obtain interpretability by modifying an existing intractable objective with a convex regularization term that encourages sparsity , and then _ exactly _ optimizing that modified objective . on the other hand ,",
    "cur methods operate by using randomness and approximation as computational resources to optimize _ approximately _ an intractable objective , thereby implicitly incorporating a form of regularization into the steps of the approximation algorithm . understanding this concept of _ implicit regularization via",
    "approximate computation _ is clearly of interest more generally , in particular for applications where the size scale of the data is expected to increase .",
    "we would like to thank art owen and robert tibshirani for encouragement and helpful suggestions .",
    "jacob bien was supported by the urbanek family stanford graduate fellowship , and ya xu was supported by the melvin and joan lane stanford graduate fellowship . in addition , support from the nsf and afosr is gratefully acknowledged .",
    "belabbas and p.j .",
    "fast low - rank approximation for covariance matrices . in",
    "_ second ieee international workshop on computational advances in multi - sensor adaptive processing _ ,",
    "pages 293296 , 2007 .",
    "t.  nielsen , r.b .",
    "west , s.c .",
    "linn , o.  alter , m.a .",
    "knowling , j.  oconnell , s.  zhu , m.  fero , g.  sherlock , j.r .",
    "pollack , p.o .",
    "brown , d.  botstein , and m.  van  de rijn .",
    "molecular characterisation of soft tissue tumours : a gene expression study . , 359(9314):13011307 , 2002 .",
    "a.  subramanian , p.  tamayo , v.  k. mootha , s.  mukherjee , b.  l. ebert , m.  a. gillette , a.  paulovich , s.  l. pomeroy , t.  r. golub , e.  s. lander , and j.  p. mesirov .",
    "gene set enrichment analysis : a knowledge - based approach for interpreting genome - wide expression profiles .",
    ", 102(43):1554515550 , 2005 ."
  ],
  "abstract_text": [
    "<S> the cur decomposition provides an approximation of a matrix @xmath0 that has low reconstruction error and that is sparse in the sense that the resulting approximation lies in the span of only a few columns of @xmath0 . in this regard </S>",
    "<S> , it appears to be similar to many sparse pca methods . </S>",
    "<S> however , cur takes a randomized algorithmic approach , whereas most sparse pca methods are framed as convex optimization problems . in this paper </S>",
    "<S> , we try to understand cur from a sparse optimization viewpoint . </S>",
    "<S> we show that cur is implicitly optimizing a sparse regression objective and , furthermore , can not be directly cast as a sparse pca method . </S>",
    "<S> we also observe that the sparsity attained by cur possesses an interesting structure , which leads us to formulate a sparse pca method that achieves a cur - like sparsity . </S>"
  ]
}