{
  "article_text": [
    "tensors have found application in a variety of fields , ranging from chemometrics to signal processing and beyond . in this paper , we consider the problem of multilinear modeling of _ sparse count _ data .",
    "for instance , we may consider data that encodes the number of papers published by each author at each conference per year for a given time frame @xcite , the number of packets sent from one ip address to another using a specific port @xcite , or to / from and term counts on emails @xcite .",
    "our goal is to develop a descriptive model of such data , along with appropriate algorithms and theory .",
    "let @xmath0 represent an @xmath1-way data tensor of size @xmath2 .",
    "we are interested in an @xmath3-component nonnegative candecomp / parafac @xcite factor model @xmath4 where @xmath5 represents outer product and @xmath6 represents the @xmath7th column of the nonnegative _ factor matrix _",
    "@xmath8 of size @xmath9 .",
    "we refer to each summand as a _ component_. assuming each factor matrix has been column - normalized to sum to one , we refer to the nonnegative @xmath10 s as _ weights_.    in many applications such as chemometrics @xcite , we fit the model to the data using a least squares criteria , implicitly assuming that the random variation in the tensor data follows a gaussian distribution . in the case of sparse count data , however , the random variation is better described via a poisson distribution @xcite , i.e. , @xmath11 rather than @xmath12 , where the subscript @xmath13 is shorthand for the multi - index @xmath14 .",
    "in fact , a poisson model is a much better explanation for the zero observations that we encounter in sparse data  these zeros just correspond to events that were very unlikely to be observed .",
    "thus , we propose that rather than using the least squares ( ls ) error function given by @xmath15 , for count data we should instead minimize the ( generalized ) kullback - leibler ( kl ) divergence @xmath16 which equals the negative log - likelihood of the observations up to an additive constant . unfortunately , minimizing kl  divergence is more difficult than ls error .",
    "although other authors have considered fitting tensor data using kl divergence @xcite , we offer the following contributions :    we develop alternating poisson regression for nonnegative cp model ( cp - apr ) .",
    "the subproblems are solved using a majorization - minimization ( mm ) approach .",
    "if the algorithm is restricted to a single inner iteration per subproblem , it reduces to the standard lee - seung multiplicative for kl updates @xcite as extended to tensors by welling and weber @xcite .",
    "however , using multiple inner iterations is shown to accelerate the method , similar to what has been observed for ls @xcite    it is known that the lee - seung multiplicative updates may converge to a non - stationary point @xcite , and lin @xcite has previously introduced a fix for the ls version of the lee - seung method .",
    "we introduce a different technique for avoiding _ inadmissible zeros _",
    "( i.e. , zeros that violate stationarity conditions ) that is only a trivial change to the basic algorithm and prevents convergence to non - stationary points .",
    "this technique is straightforward to adapt to the matrix and/or ls cases as well .    assuming the subproblems can be solved exactly , we prove convergence of the cp - apr algorithm .",
    "in particular , we can show convergence even for sparse input data and solutions on the boundary of the nonnegative orthant .",
    "we explain how to efficiently implement cp - apr for large - scale sparse data .",
    "although it is well - known how to do large - scale sparse tensor calculations for the ls fitting function @xcite , the poisson likelihood fitting algorithm requires new sparse tensor kernels . to the best of our knowledge ,",
    "ours is the first implementation of any kl - divergence - based method for large - scale sparse tensors .",
    "we present experimental results showing the effectiveness of the method on both real and simulated data .",
    "in fact , the poisson assumption leads quite naturally to a generative model for sparse data .",
    "much of the past work in nonnegative matrix and tensor analysis has focused on the ls error @xcite , which corresponds to an assumption of normal independently identically distributed ( i.i.d . )",
    "the focus of this paper is kl divergence , which corresponds to maximum likelihood estimation under an independent poisson assumption ; see  [ sec : poisson ] . the seminal work in this domain",
    "are the papers of lee and seung @xcite , which propose very simple _ multiplicative _ update formulas for both ls and kl divergence , resulting in a very low cost - per - iteration .",
    "welling and weber @xcite were the first to generalize the lee and seung algorithms to nonnegative tensor factorization ( ntf ) .",
    "applications of ntf based on kl - divergence include eeg analysis @xcite and sound source separation @xcite .",
    "we note that generalizations of kl divergence have also been proposed in the literature , including bregman divergence @xcite and beta divergence  @xcite .    in terms of convergence ,",
    "lin @xcite and gillis and glienur @xcite have shown convergence of two different modified versions of the lee - seung method for ls .",
    "finesso and spreij @xcite ( tensor extension in @xcite ) have shown convergence of the lee - seung method for kl divergence ; however , we show later that numerical issues arise if the iterates come near to the boundary .",
    "this is related to the problems demonstrated by gonzalez and zhang @xcite that show , in the case of ls loss , the lee and seung method can converge to non - kkt points ; we show a similar example for kl divergence in ",
    "[ sec : misconvergence ] .",
    "our convergence theory is not focused on the lee - seung algorithm but rather on a gauss - seidel approach .",
    "the closest work is that of lin @xcite in which he considers the matrix problem in the least squares sense ; in the same paper , he dismisses the kl divergence problem as ill - defined but we address that issue in this paper by showing that the convex hull of the level sets of the kl divergence problem are compact .",
    "throughout , scalars are denoted by lowercase letters ( @xmath17 ) , vectors by boldface lowercase letters ( @xmath18 ) , matrices by boldface capital letters ( @xmath19 ) , and higher - order tensors by boldface euler script letters ( @xmath0 ) .",
    "we let @xmath20 denotes the vector of all ones and @xmath21 denotes the matrix of all ones .",
    "the @xmath22th column of a matrix @xmath19 is denoted by @xmath23 .",
    "we use multi - index notation so that a boldface @xmath13 represents the index @xmath24 .",
    "we use subscripts to denote iteration index for infinite sequences , and the difference between its use for an entry and its use as an iteration index should be clear by context .",
    "the notation @xmath25 refers to the two - norm for vectors or frobenious norm for matrices , i.e. , the sum of the squares of the entries .",
    "the notation @xmath26 refers to the one - norm , i.e. , the sum of the absolute values of the entries .",
    "the outer product is denoted by @xmath5 .",
    "the symbols @xmath27 and @xmath28 represents elementwise multiplication and division , respectively .",
    "the symbol @xmath29 denotes khatri - rao matrix multiplication .",
    "the mode-@xmath30 matricization or unfolding of a tensor @xmath0 is denoted by @xmath31 .",
    "see appendix  [ sec : notation - details ] for further details on these operations .      in statistics ,",
    "count data is often best described as following a poisson distribution . for a general discussion of the poisson distribution ,",
    "see , e.g. , @xcite .",
    "we summarize key facts here .",
    "a random variable @xmath32 is said to have a poisson distribution with parameter @xmath33 if it takes integer values @xmath34 with probability @xmath35 the mean and variance of @xmath32 are both @xmath36 ; therefore , the variance increases along with the mean , which seems like a reasonable assumption for count data .",
    "it is also useful to note that the sum of independent poisson random variables is also poisson .",
    "this is important in our case since each poisson parameter is a multilinear combination of the model parameters .",
    "we contrast poisson and gaussian distributions in figure  [ fig : gaussian_and_poisson ] .",
    "observe that there is good agreement between the distributions for larger values of the mean , @xmath36 .",
    "for small values of @xmath36 , however , the match is not as strong and the gaussian random variable can take on negative values .    .",
    "]    we can determine the optimal poisson parameters by maximizing the likelihood of the observed data .",
    "let @xmath37 be a vector of observations and let @xmath38 be the vector of poisson parameters .",
    "( we assume that @xmath39 s are not independent , else the function would entirely decouple in the parameters to be estimated . ) then the negative of the log of the likelihood function for ( [ eq : poisson ] ) is the kl divergence @xmath40 excepting the addition of the constant term @xmath41 , which is omitted . because we are working with sparse data ,",
    "there are many instances for which we expect @xmath42 , which leads to some ambiguity in ( [ eq : log - likelihood ] ) if @xmath43 .",
    "we assume throughout that @xmath44 for all @xmath45 .",
    "this is for notational convenience ; else , we would write ( [ eq : log - likelihood ] ) as    _ i _ i - _ i : x_i 0 x_i _ i.",
    "in this section we introduce the cp - apr algorithm for fitting a nonnegative _ poisson tensor decomposition ( ptf ) _ to count data .",
    "the algorithm employs an alternating optimization scheme that sequentially optimizes one factor matrix while holding the others fixed ; this is nonlinear gauss - seidel applied to the ptf problem .",
    "the subproblems are solved via a majorization - minimization ( mm ) algorithm , as described in ",
    "[ sec : subproblem ] .",
    "our optimization problem is defined as @xmath46^{i_n \\times r } |      \\| { { { \\bm{\\mathbf{\\makelowercase{a}}}}}_{r } } \\|_1 = 1 \\text { for } r=1,\\dots , r } .",
    "\\end{gathered}\\end{gathered}\\ ] ] here @xmath47 is shorthand notation for ( [ eq : cp ] ) @xcite . depending on context , @xmath48 represents the tensor itself or its constituent parts . for example , when we say @xmath49 , it means that that the factor matrices have stochasticity constraints on the columns .",
    "the function @xmath50 is not finite on all of @xmath51 .",
    "for example , if there exists @xmath13 such that @xmath52 and @xmath53 , then @xmath54 . if @xmath55 for all @xmath13 such that @xmath56 , however , then we are guaranteed that @xmath57 is finite .",
    "consequently , we will generally wish to restrict ourselves to a domain for which @xmath57 is finite .",
    "we define @xmath58 where @xmath59 denotes the convex hull .",
    "we observe that @xmath60 ( strict subset ) since , for example , the all - zero model is not in @xmath61 .",
    "the following lemma states that @xmath61 is compact for any @xmath62 ; the proof is given in appendix  [ sec : proof - omegazeta ] .",
    "[ lem : omegazeta ] let @xmath50 be as defined in ( [ eq : nlp ] ) and @xmath61 be as defined in ( [ eq : omegazeta ] ) . for any @xmath62 , @xmath61 is compact .",
    "we solve problem ( [ eq : nlp ] ) via an alternating approach , holding all factor matrices constant except one .",
    "consider the problem for the @xmath30th factor matrix .",
    "we note that there is scaling ambiguity that allows us to express the same @xmath48 in different ways , i.e. , @xmath63 the weights in ( [ eq : m2 ] ) are omitted because they are absorbed into the @xmath30th mode . from @xcite , we can express @xmath48 as @xmath64 where @xmath65 is defined in ( [ eq : bn ] ) and @xmath66 thus , we can rewrite the objective function in ( [ eq : nlp ] ) as @xmath67 { { \\bm{\\mathbf{\\makelowercase{e}}}}},\\ ] ] where @xmath20 is the vector of all ones , @xmath27 denotes the elementwise product , and the @xmath68 function is applied elementwise .",
    "we note that it is convenient to update @xmath8 and @xmath69 simultaneously since the resulting constraint on @xmath65 is simply @xmath70 .",
    "thus , at each inner iteration of the gauss - seidel algorithm , we optimize @xmath57 restricted to the @xmath30th block , i.e. , @xmath71 { { \\bm{\\mathbf{\\makelowercase{e}}}}}.\\ ] ] the updates for @xmath69 and @xmath8 come directly from @xmath65 .",
    "note that some care must be taken if an entire column of @xmath65 is zero ; if the @xmath7th column is zero , then we can set @xmath72 and @xmath73 to an arbitrary nonnegative vector that sums to one .",
    "the full procedure is given in algorithm  [ alg : outer ] ; this is a variant ( because of the handling of @xmath69 ) of nonlinear gauss - seidel .",
    "we note that the scaling and unscaling of the factor matrices is common in alternating algorithms , though not always explicit in the algorithm statement .",
    "there are many variations of this basic device ; for instance , in the context of the ls version of ntf , ( * ? ? ?",
    "* algorithm 2 ) collects the scaling information into an explicit scaling vector that is `` amended '' after each inner iteration    let @xmath0 be a tensor of size @xmath74 .",
    "let @xmath75 be an initial guess for an @xmath3-component model such that @xmath76 for some @xmath62 .",
    "@xmath77 @xmath78 { { \\bm{\\mathbf{\\makelowercase{e}}}}}$ ] subproblem @xmath79 @xmath80",
    "we defer the proof of convergence until  [ sec : convergence ] , but we discuss how to check for convergence here .",
    "first , we mention an assumption that is important to the theory and also arguably practical .",
    "let @xmath81 denote the set of indices of columns for which the @xmath82th row of @xmath83 is non - zero .",
    "if @xmath84 , then @xmath85 corresponds to a vectorization of the @xmath82th horizontal slice of @xmath86 , @xmath87 to a vectorization of the @xmath82th lateral slice , and @xmath88 to a vectorization of the @xmath82th frontal slice .",
    "more generally , we can think of vectorizing `` hyperslices '' with respect to each mode .",
    "[ as : full_row_rank ] the rows of the submatrix @xmath89 ( i.e. , only the columns corresponding to nonzero rows in @xmath31 are considered ) are linearly independent for all @xmath90 and @xmath91 .",
    "implies that @xmath92 for all @xmath82 .",
    "thus , we need to observe at least @xmath93 counts in the data tensor @xmath0 , and the counts need to be sufficiently distributed across @xmath0 . consequently",
    ", the conditions appeal to our intuition that there are concrete limits on how sparse the data tensor can be with respect to how many parameters we wish to fit .",
    "if , for example , we had @xmath94 , it is clear that we can remove element @xmath82 from the first dimension entirely since it contributes nothing .",
    "we are making a stronger requirement : each element in each dimension must have at least @xmath3 nonzeros in its corresponding hyperslice .",
    "a potential problem is that depends on the current iterate , which we can not predict in advance .",
    "however , we observe that if @xmath95 and the factor matrices have random uniform [ 0,1 ] positive entries and @xmath96 , then this condition is satisfied with probability one then the condition is satisfied with probability one . ]",
    ". this condition can be checked as the iterates progress .",
    "the matrix @xmath97 \\mpintra,\\ ] ] with @xmath28 denoting elementwise division , will come up repeatedly in the remainder of the paper .",
    "for instance , we observe that the partial derivative of @xmath50 with respect to @xmath8 is @xmath98 , where @xmath21 is the matrix of all ones .",
    "consequently , the matrix @xmath99 plays a role in checking convergence as follows .",
    "[ thm : kkt ] if @xmath95 and @xmath100 for some @xmath62 , then @xmath48 is a karush - kuhn - tucker ( kkt ) point of ( [ eq : nlp ] ) if and only if @xmath101    since @xmath95 , we can assume that @xmath69 has been absorbed into @xmath102 for some @xmath103 .",
    "thus , we can replace the constraints @xmath104 and @xmath105 with @xmath106 .",
    "in this case , the partial derivatives are @xmath107 since @xmath108 for some @xmath62 , we know that not all elements of @xmath48 are zero ; thus , the set of active constraints are linearly independent .",
    "the following conditions define a kkt point @xcite : @xmath109 here @xmath110 are the lagrange multipliers for the nonnegativity constraints and @xmath111 are the lagrange multipliers for the stochasticity constraints .",
    "if @xmath112 is a kkt point , then from ( [ eq : kkt - full - conditions ] ) , we have that @xmath113 , @xmath114 , and @xmath115 .",
    "thus , @xmath116 . since @xmath95 and @xmath103 is arbitrary , ( [ eq : kkt - conditions ] )",
    "follows immediately .",
    "if , on the other hand , ( [ eq : kkt - conditions ] ) is satisfied , choosing @xmath117 , and @xmath118 and @xmath119 for @xmath120 satisfies the kkt conditions in ( [ eq : kkt - full - conditions ] ) .",
    "hence , @xmath48 must be a kkt point .",
    "observe that the condition @xmath121 makes @xmath69 moot in the kkt conditions  this reflects the scaling ambiguity that is inherent in the model .    from theorem  [ thm : kkt ] and",
    "because feasibility is always maintained , we can check for convergence by verifying @xmath122 for @xmath91 , where @xmath123 is some specified convergence tolerance .",
    "we require the strict convexity of @xmath50 in each of the block coordinates .",
    "this is ensured under .",
    "[ lem : strict - convexity ] let @xmath124 be the function @xmath50 restricted to the @xmath30th block as defined in ( [ eq : subproblem ] ) . if is satisfied , then @xmath125 is strictly convex over @xmath126 .    in the proof , we drop the @xmath30 s for convenience .",
    "first note that @xmath127 is convex .",
    ". we can rewrite ( [ eq : subproblem ] ) as @xmath129 subject to @xmath130 .",
    "hence , it is sufficient to show that the function @xmath131 is strictly convex over the convex set @xmath132 .",
    "fix @xmath133 such that @xmath134 .",
    "since the inner product is affine and @xmath68 is a strictly concave function , we need only show that there exists some @xmath82 and @xmath22 such that @xmath135 and @xmath136 .",
    "we know at least one column must differ since @xmath134 ; let @xmath82 correspond to that column and define @xmath137 . by , we know that @xmath138 has full row rank .",
    "thus , there exists a column @xmath22 of @xmath139 such that @xmath140 and @xmath141 .",
    "hence , the claim .    here",
    "we state our main convergence result .",
    "although this result assumes that the subproblems can be solved exactly ( which is not the case in practice ) , it gives some idea as to the convergence behavior of the method .",
    "we follow the reasoning of the proof of convergence of nonlinear gauss - seidel ( * ? ? ? * proposition 3.9 ) , adapted here for the way that @xmath69 is handled .",
    "[ thm : cp - apr - convergence ] suppose that @xmath57 is strictly convex with respect to each block component and that it is minimized exactly for each block component subproblem of cp - apr .",
    "let @xmath142 be a limit point of the sequence @xmath143 such that @xmath144 .",
    "then @xmath142 is a kkt point of ( [ eq : nlp ] ) .",
    "let @xmath145 be the @xmath146th iterate produced by the _",
    "outer _ iterations of algorithm  [ alg : outer ] .",
    "define @xmath147 to be the @xmath30th iterate in the inner loop of outer iteration @xmath146 with the @xmath69-vector absorbed into the @xmath30th factor , i.e. , @xmath148 where @xmath149 is the solution to the @xmath30th subproblem at iteration @xmath146 .",
    "this defines @xmath150 to be the column - normalized version of @xmath149 , i.e. , @xmath151 .",
    "taking advantage of the scaling ambiguity to shift the weights between factors yields @xmath152 observe that    ^(n)_k = ,  ,",
    "^(n-1)_k+1 , ^(n)_k+1 ( ) ,    so there is a correspondence between @xmath153 and @xmath154 such that @xmath155 . for convenience ,",
    "we define    ^(0)_k = ( ) , ^(2)_k ,  , ^(n)_k .",
    "since we assume the subproblem is solved exactly at each iteration , we have @xmath156    recall that @xmath61 is compact by lemma  [ lem : omegazeta ] .",
    "since the sequence @xmath143 is contained in the set @xmath61 , it must have a convergent subsequence .",
    "we let @xmath157 denote the indices of that convergent subsequence and @xmath158 denote its limit point . by continuity of @xmath50 ,",
    "f(_k _ ) f ( _ * ) .",
    "we first show that @xmath159 .",
    "assume the contrary , i.e. , that it does not converge to zero .",
    "let @xmath160 . by possibly restricting to a subsequence of @xmath157 , we may assume there exists some @xmath161 such that @xmath162 for all @xmath163 .",
    "let @xmath164 ; then @xmath165 , @xmath166 , and @xmath167 differs from zero only along the first block component .",
    "notice that @xmath168 belong to a compact set and therefore has a limit point @xmath169 . by restricting to a further subsequence of @xmath157",
    ", we assume that @xmath170    let us fix some @xmath171 $ ] .",
    "notice that @xmath172 .",
    "therefore , @xmath173 lies on the line segment joining @xmath174 and @xmath175 and belongs to @xmath61 because @xmath61 is convex . using the convexity of @xmath50 w.r.t .",
    "the first block component and the fact that @xmath176 minimizes @xmath50 over all @xmath177 that differ from @xmath176 in the first block component , we obtain @xmath178 since @xmath179 , equation ( [ eq : bt3.14 ] ) shows that @xmath180 also converges to @xmath181 .",
    "taking limits as @xmath163 tends to infinity , we obtain @xmath182 where @xmath183 is just @xmath184 with @xmath185 absorbed into the first component .",
    "we conclude that    f ( _ * ) = f(^(0 ) _ * + _ 0 ^(1 ) _ * )    for every @xmath171 $ ] .",
    "since @xmath186 , this contradicts the strict convexity of @xmath50 as a function of the first block component .",
    "this contradiction establishes that @xmath159 .",
    "in particular , @xmath176 converges to @xmath183 .    by definition of @xmath176 and the assumption that each subproblem is solved exactly ,",
    "we have @xmath187 taking limits as @xmath188 , we obtain @xmath189 in other words , @xmath190 is the minimizer of @xmath50 with respect to the first block components with the remaining components are fixed at @xmath191 through @xmath192 . from the kkt conditions",
    "@xcite , we have that @xmath193 in turn , since @xmath144 , we have @xmath194 .",
    "repeating the previous argument shows that @xmath195 and that @xmath196 . continuing inductively , @xmath197 for @xmath91 .",
    "thus , by theorem  [ thm : kkt ] , @xmath184 is a kkt point of @xmath57 .    before proceeding to the discussion solving the subproblem",
    ", we point out that remarkably very little is assumed about the objective function @xmath50 in theorem  [ thm : cp - apr - convergence ] .",
    "the proof required that @xmath50 is differentiable , strictly convex in each of its block components , and there is a @xmath198 such that the level set @xmath199 is compact .",
    "the upshot is that theorem  [ thm : cp - apr - convergence ] applies equally well to other choices of @xmath50 corresponding to other members in the family of beta distributions that are convex , namely the divergences that correspond to @xmath200 $ ]  @xcite .",
    "in fact , it was also observed in @xcite that `` rescaling does not interfere with the convergence of the gauss ",
    "seidel iterations '' ( in the context of the ls formulation of ntf ) .",
    "the basic idea of a majorization - minimization ( mm ) algorithm is to convert a hard optimization problem ( e.g. , non - convex and/or non - differentiable ) into a series of simpler ones ( e.g. , smooth convex ) that are easy to minimize and that majorize the original function , as follows .",
    "let @xmath50 and @xmath201 be real - valued functions on @xmath202 and @xmath203 , respectively .",
    "we say that @xmath201 _ majorizes _ @xmath50 at @xmath204 if @xmath205 for all @xmath206 and @xmath207 .",
    "if @xmath208 is the function to be optimized and @xmath209 majorizes @xmath50 at @xmath210 , the basic mm iteration is    _",
    "k+1 = _ g ( , _ k ) .",
    "it is easy to see that such iterates always take non - increasing steps with respect to @xmath50 since    f(_k+1 ) g(_k+1 , _ k ) g(_k , _ k ) = f(_k ) ,    where @xmath211 is the current iterate and @xmath212 is the optimum computed at that iterate .    consider the @xmath30th subproblem in ( [ eq : subproblem ] ) . here",
    "we drop the @xmath30 s for convenience so that ( [ eq : subproblem ] ) reduces to @xmath213{{\\bm{\\mathbf{\\makelowercase{e}}}}}.\\ ] ] recall that @xmath214 is the nonnegative data tensor reshaped to a matrix of size @xmath215 , @xmath139 is a nonnegative matrix of size @xmath216 with rows that sum to 1 , and @xmath217 is a nonnegative matrix of size @xmath218 . for clarity in the ensuing discussion , we also restate in terms of the local variables for this section as follows :    [ as : full_row_rank_mm ] the rows of the submatrix @xmath219 ( i.e. , only the columns corresponding to nonzero rows in @xmath214 are considered ) are linearly independent for all @xmath220 .    according to , for every @xmath82",
    "there is at least one @xmath22 such that @xmath221 .",
    "thus , we can assume that we have @xmath222 such that @xmath223 is finite .",
    "we now introduce the majorization used in our subproblem solver .",
    "this majorization is also a special case of the one derived in @xcite when @xmath224 and has a long history in image reconstruction that predates its use in nmf @xcite .",
    "the objective @xmath50 is majorized at @xmath225 by the function @xmath226    { \\quad\\text{where}\\quad }    \\alpha_{rij } = \\frac{\\bar b_{ir}\\pi_{rj}}{\\sum_r \\bar b_{ir } \\pi_{rj}}.    \\ ] ] the proof of this fact is straightforward and thus relegated to appendix  [ sec : proof - mm ] .",
    "the advantage of this majorization is that the problem is now completely separable in terms of @xmath227 , i.e. , the individual entries of @xmath228 .",
    "moreover , @xmath229 has a unique global minimum with an analytic expression , given by @xmath230 , where @xmath231 is as defined in ( [ eq : phi ] ) and depends on @xmath217 . a proof is provided in appendix  [ sec : proof - mm - unique - global - min ] .",
    "the mm algorithm iterations are then defined by @xmath232 \\mpi\\ ] ] and @xmath214 and @xmath139 come from ( [ eq : subproblem - simple ] ) . if @xmath233 , clearly @xmath234 for all @xmath146 .",
    "observe that @xmath235 .",
    "we discuss in  [ sec : stopping ] how to exploit this simple relationship to quickly compute stopping rules for the algorithm . the mm algorithm to solve the gauss - seidel subproblem of line 4 in algorithm",
    "[ alg : outer ] is given in algorithm  [ alg : cpapr ] .",
    "@xmath236 [ line : b ] [ line : a ] @xmath237 [ line : phi ] @xmath238 [ line : b - update ] [ line : b ]    the monotonic decrease in objective function does not guarantee that the mm iterates will converge to the desired global minimizer of the subproblem .",
    "nonetheless , the following theorem shows that , under mild conditions on the starting point @xmath239 ( discussed further in  [ sec : zeros ] ) , the mm iterates will converge to the unique global minimum of ( [ eq : subproblem - simple ] ) .",
    "the proof follows the reasoning of the convergence proof of an algorithm for fitting a regularized poisson regression problem given in @xcite and is given in appendix  [ sec : subproblem - convergence ] .",
    "[ thm : subproblem - convergence ] let @xmath50 be as defined in ( [ eq : subproblem - simple ] ) and assume holds , let @xmath239 be a nonnegative matrix such that @xmath240 is finite and @xmath241 for all @xmath242 such that @xmath243 , and let the sequence @xmath244 be defined as in ( [ eq : mm - iterate ] ) .",
    "then @xmath244 converges to the global minimizer of @xmath50 .",
    "note that we make a modest but very useful generalization of existing results by allowing iterates to be on ( or very close to ) the boundary .",
    "prior convergence results , including @xcite assume that all iterates are strictly positive .",
    "though true in exact arithmetic , in numerical computations it is not uncommon for some iterates to become zero numerically . in ",
    "[ sec : zeros ] , we show how to ensure the condition on @xmath239 holds in practice .",
    "the previous algorithms omit many details and numerical checks that are needed in any practical implementation .",
    "thus , algorithm  [ alg : cpapr - detailed ] provides a detailed version that can be directly implemented .",
    "a highlight of this implementation is the `` inadmissible zero '' avoidance , which fixes a the problem of getting stuck at a zero value with multiplicative updates .",
    "let @xmath0 be a tensor of size @xmath74 .",
    "let @xmath245 be an initial guess for an @xmath3-component model such that @xmath76 for some @xmath62 .",
    "+ choose the following parameters :    * @xmath246 = maximum number of outer iterations * @xmath247 = maximum number of inner iterations ( per outer iteration ) * @xmath248 = convergence tolerance on kkt conditions ( e.g. , @xmath249 ) * @xmath250 = inadmissible zero avoidance adjustment ( e.g. , @xmath251 ) * @xmath252 = tolerance for identifying a potential inadmissible zero ( e.g. , @xmath253 ) * @xmath254 = minimum divisor to prevent divide - by - zero ( e.g. , @xmath253 )    ` isconverged ` @xmath255 true [ line : scooch - a ] @xmath256 [ line : scooch - b ] @xmath257 @xmath77 [ line : update_pi ] @xmath258 [ line : multiplicative_update ] [ line : tau ] break ` isconverged ` @xmath255 false @xmath259 @xmath260 @xmath261 break      if we only take one iteration of the subproblem loop ( i.e. , setting @xmath262 ) , then cp - apr is the lee - seung multiplicative update algorithm for the kl divergence .",
    "thus , we can view the lee - seung algorithm as a special case of our algorithm where we do not solve the subproblems exactly ; quite the contrary , we only take one step towards the subproblem solution .",
    "a well - known problem with multiplicative updates is that some elements may get `` stuck '' at zero ; see , e.g. , @xcite .",
    "for example , if @xmath263 , then the multiplicative updates will never change it . in many cases",
    ", a zero entry may be the correct answer , so we want to allow it . in other cases ,",
    "though , the zero entry may be incorrect in the sense that it does not satisfy the kkt conditions , i.e. , @xmath264 but @xmath265 . we refer to these values as _",
    "inadmissible zeros_. we correct this problem before we enter into the multiplicative update phase of the algorithm . in , any inadmissible zeros ( or near - zeros )",
    "are `` scooched '' away from zero and into the interior .",
    "the amount of the scooch is controlled by the user - defined parameter @xmath250 .",
    "the condition in theorem  [ thm : subproblem - convergence ] is exactly that the starting point should not have any zeros that are ultimately inadmissible .",
    "if we discover that a sequence of iterates leads to an inadmissible zero ( or almost - zero ) , we restart the method by restarting the method with a new starting point .",
    "this adjustment prevents convergence to non - kkt points .",
    "note that all the quantities needed to perform the check are precomputed and that there is no change to the algorithm besides adjusting a few zero entries in the current factor matrix .",
    "the fix for the inadmissible zeros is compatible with the lee - seung algorithm for ls error as well .",
    "lin @xcite has made a similar observation in the ls case and applied changes to his gradient descent version of the lee - seung method .",
    "our correction is different and is directly incorporated into the multiplicative update scheme rather than requiring a different update formula .",
    "gillis and glineur @xcite proposed a more drastic fix by restricting the factor matrices to have entries in @xmath266 for some small positive @xmath254 . avoiding all zeros",
    "clearly rules out the possibility of getting stuck at an inadmissible zero , but does so at the expense of eliminating any hope of obtaining sparse factor matrices , a desirable property in many applications .",
    "the convergence conditions on the subproblem require that @xmath267 .",
    "we do not require the value to be exactly zero but instead check that it is smaller in magnitude than the user - defined parameter @xmath248 .",
    "we break out of the subproblem loop as soon as this condition is satisfied .    from theorem",
    "[ thm : kkt ] , we can check for overall convergence by verifying ( [ eq : kkt - conditions ] ) .",
    "we do not want to calculate this at the end of every @xmath30-loop because it is expensive .",
    "instead , we know that the iterates will stop changing once we have converged and so we can validate the convergence of all factor matrices by checking that no factor matrix has been modified and every subproblem has converged .",
    "consider a large - scale sparse tensor that is too large to be stored as a dense tensor requiring @xmath268 memory . in this case , we can store the tensor as a sparse tensor as described in @xcite , requiring only @xmath269 memory .",
    "the elementwise division in the update of @xmath231 requires that we divide the tensor ( in matricized form ) @xmath214 by the current model estimate ( in matricized form ) @xmath270 .",
    "unfortunately , we can not afford to store @xmath271 explicitly as a dense tensor because it is the same size as @xmath86 .",
    "in fact , we generally can not even form @xmath139 explicitly because it requires almost as much storage as @xmath48 .",
    "we observe , however , that we need only calculate the values of @xmath271 that correspond to nonzeros in @xmath214 .",
    ". then we can store the sparse tensor @xmath0 as a set of values and multi - indices , @xmath273 for @xmath274 . in order to avoid forming the current model estimate , @xmath275 , as a dense object ,",
    "we will store only selected rows of @xmath139 , one per nonzero in @xmath0 ; we denote these rows by @xmath276 for @xmath274 .",
    "the @xmath277th vector is given by the elementwise product of rows of the factor matrices , i.e. , @xmath278 in order to determine @xmath279 in the calculation of @xmath231 , we proceed as follows .",
    "the tensor @xmath280 will have the same nonzero pattern as @xmath0 , and we let @xmath281 denote its values .",
    "it can be determined that @xmath282 to calculate @xmath283 , we simply have @xmath284 the storage of the @xmath276 for @xmath274 vectors and the entries @xmath281 requires @xmath285 additional storage .",
    "we contend that , for sparse count data , kl divergence ( [ eq : nll ] ) is a better objective function .",
    "to support our claim , we consider simulated data where we know the correct answer .",
    "specifically , we consider a 3-way tensor ( @xmath84 ) of size @xmath286 and @xmath287 factors .",
    "it will be generated from a model @xmath288 .",
    "the entries of the vector @xmath69 are selected uniformly at random from @xmath289 $ ] .",
    "each factor matrix @xmath290 is generated as follows :    for each column in @xmath290 , randomly select 10% ( i.e. , @xmath291 ) of the entries uniformly at random from the interval @xmath292 $ ] .",
    "the remaining entries are selected uniformly at random from @xmath289 $ ] .",
    "each column is scaled so that its 1-norm is 1 ( i.e. , its sum is 1 ) .",
    "an  observed \" tensor can be thought of as the outcome of tossing @xmath293 balls into @xmath294 empty urns where each entry of the tensor corresponds to an urn . for each ball , we first draw a factor @xmath7 with probability @xmath295 . the indices @xmath296 are selected randomly proportional to @xmath6 for @xmath297 . in other words ,",
    "the ball is then tossed into the @xmath296th urn with probability @xmath298 . in this manner ,",
    "the balls are allocated across the urns independently of each other .",
    "this procedure generates entries @xmath299 that are each distributed as @xmath300 .",
    "we adjust the final @xmath301 so that the scale matches that of @xmath0 , i.e. , @xmath302 .",
    "we generate problems where the number of observations ranges from 480,000 ( 0.1% ) down to 24,000 ( 0.005% ) .",
    "recall that implies that the absolute minimum number of observations is @xmath303 .",
    "we have used very few observations , as real problems do indeed tend to be this sparse .",
    "r@c|*2c@c|*2c@c & & + & & & & + & fms & # cols & fms & & fms & # cols & fms & # cols + 480000 & ( 0.100% ) & 0.58 & 6.4 & 0.71 & 7.3 & 0.89 & 8.7 & 0.96 & 9.5 + 240000 & ( 0.050% ) & 0.51 & 5.4 & 0.72 & 7.4 & 0.83 & 8.2 & 0.91 & 9.2 + 48000 & ( 0.010% ) & 0.37 & 3.8 & 0.59 & 6.3 & 0.76 & 7.5 & 0.80 & 7.9 + 24000 & ( 0.005% ) & 0.33 & 3.5 & 0.51 & 5.7 & 0.72 & 6.6 & 0.74 & 6.9 +    table  [ tab : simulated ] shows comparisons of four methods .",
    "the first two are optimizing ls : lee - seung for ls and alternating ls with no nonnegativity constraints ( cp - als ) .",
    "the last two are optimizing kl divergence : lee - seung for kl divergence and our method ( cp - apr ) .",
    "we have also tested the modified lee - seung method of finesso and spreij @xcite , but it is only a scaled version of the lee - seung method for kl divergence and gave nearly identical results which are omitted .",
    "all implementations are from version 2.5 of tensor toolbox for matlab @xcite ; exact parameter settings are provided in appendix  [ sec : compare_objectives - app ] .",
    "we report the factor match score ( fms ) , a measure in @xmath289 $ ] of how close the computed solution is to the true solution .",
    "a value of 1 is ideal .",
    "since the fms measure is somewhat abstract , we also report the number of columns in the first factor matrix such that the cosine of the angle between the true solution and the computed solution is greater than 0.95 .",
    "a value of 10 is ideal since we have used @xmath287 .",
    "the reported values are averages over 10 problems .",
    "see appendix  [ sec : compare_objectives - app ] for precise formulas for both measures .",
    "although these problems are extremely sparse , all methods are able to correctly identify components in the data .",
    "overall , the methods optimizing kl divergence are superior to those optimizing least squares .",
    "we also observe that cp - apr is an improvement compared to lee - seung  kl ; we provide later evidence that this improvement is more likely due to the inadmissible zero fix than the extra inner iterations ( which provide a benefit of enhanced speed rather than accuracy ) .",
    "we demonstrate the effectiveness of our simple fix for avoiding inadmissible zeros , as described in  [ sec : zeros ] .",
    "our technique is based on the same observation on inadmissible zeros as in lin @xcite , but the change to the algorithm is different . as in @xcite , we consider fitting a rank-10 bilinear model for a @xmath304 dense positive matrix with entries drawn independently and uniformly from @xmath289 $ ] .",
    "we apply cp - apr using @xmath305 .",
    "we do two runs : one with @xmath306 , corresponding to the standard lee - seung ( kl version ) algorithm , and the other with @xmath307 to move away from inadmissible zeros . in both runs we use the same strictly positive initial guess .",
    "figure  [ fig : misconvergence ] shows the magnitude of the kkt residual over more than @xmath308 iterations .",
    "when @xmath309 , the sequence clearly convergences . on the other hand when @xmath306 the iterates appear to get stuck at a non - kkt point .",
    "closer inspection of the factor matrix iterates reveals a single offending inadmissible zero , i.e. , its partial derivative is @xmath310 but should be nonnegative .",
    "hence , we use positive values of @xmath250 in our experiments .",
    "we show that increasing the maximum number of inner iterations @xmath247 can accelerate the convergence in table  [ tab : timing_plus ] .",
    "recall that @xmath311 corresponds to the lee - seung algorithm @xcite .",
    "we consider a 3-way tensor ( @xmath312 ) of size @xmath313 and @xmath314 factors .",
    "we generate 100 problem instances from 100 randomly generated models @xmath315 as described in ",
    "[ sec : compare_objectives ] with 0.1% observations .",
    "we compare cp - apr with @xmath316 and @xmath317 . and the other parameters set as @xmath318 , @xmath319 , @xmath320 , @xmath321 , @xmath322 .",
    "we track both the number of multiplicative updates ( ) and the cpu time using the matlab command ` cputime ` .",
    "the experiments were performed on an imac computer with a 3.4 ghz intel core i7 processor and 8 gb of ram .",
    "table  [ tab : fms_timing ] reports the fms scores as we vary @xmath247 , and we observe that the value of @xmath247 does not significantly impact accuracy .",
    "however , we observe that increasing @xmath247 can decrease the overall work and runtime",
    ". tables  [ tab : counts ] and [ tab : timing ] present the average number of multiplicative updates and total run times respectively .",
    "the distribution of updates and times was highly skewed as some problems required a substantial number of iterations .",
    "nonetheless , we see a monotonic decrease in the number of updates and time as @xmath247 increases .",
    "the differences are more substantial when comparing wall clock time .",
    "the reason for the disproportionate decrease in wall - clock time compared to the tally of updates is that the cost of the calculation of @xmath323 ( in ) is amortized over all the subproblem iterations .",
    "+       we consider the application of cp - apr to email data from the infamous federal energy regulatory commission ( ferc ) investigation of enron corporation .",
    "we use the version of the dataset prepared by zhou et al . @xcite and further processed by perry and wolfe @xcite , which includes detailed profiles on the employees .",
    "the data is arranged as a three - way tensor @xmath0 arranged as sender @xmath324 receiver @xmath324 month , where entry @xmath296 indicates the number of messages from employee @xmath82 to employee @xmath22 in month @xmath146 .",
    "the original data set had 38,388 messages ( technically , there were only 21,635 messages but some messages were sent to multiple recipients and so are counted multiple times ) exchanged between 156 employees over 44 months ( november 1998  june 2002 ) .",
    "we preprocessed the data , removing months that had fewer than 300 messages and removing any employees that did not send and receive an average of at least one message per month .",
    "ultimately , our data set spanned 28 months ( december 1999  march 2002 ) , involved 105 employees , and a total of 33,079 messages .",
    "the data is arranged so that the senders are sorted by frequency ( greatest to least ) .",
    "the tensor representation has a total of 8,540 nonzeros ( many of the messages occur between the same sender / receiver pair in the same time period ) .",
    "the tensor is 2.7% dense .",
    "we apply cp - apr to find a model for the data .",
    "there is no ideal method for choosing the number of components .",
    "typically , this value is selected through trial and error , trading off accuracy ( as the number of components grows ) and model simplicity . here",
    "we show results for @xmath287 components .",
    "we use the same settings for cp - apr as specified in appendix  [ sec : compare_objectives - app ] .",
    "+   +     figure  [ fig : enron ] illustrates six components in the resulting factorization ; the other four are shown in appendix  [ sec : enron - app ] .",
    "for each component , the top two plots shows the activity of senders and receivers , with the employees ordered from left to right by frequency of sending emails .",
    "each employee has a symbol indicating their seniority ( junior or senior ) , gender ( male or female ) , and department ( legal , trading , other ) .",
    "the sender and receiver factors have been normalized to sum to one , so the height of the marker indicates each employee s relative activity within the component . the third component ( in the time dimension )",
    "is scaled so that it indicates total message volume explained by that component .",
    "the light gray line shows the total message volume .",
    "it is interesting to observe how the components break down into specific subgroups .",
    "for instance , component 1 in figure  [ fig : enron-01 ] consists of nearly all `` legal '' and is majority female .",
    "this can be contrasted to component 5 in figure  [ fig : enron-05 ] , which is nearly all `` other '' and also majority female .",
    "component 3 in figure  [ fig : enron-03 ] is a conversation among `` senior '' staff and mostly male ; on the other hand , `` junior '' staff are more prominent in component 4 in figure  [ fig : enron-04 ] .",
    "component 8 in figure  [ fig : enron-08 ] seems to be a conversation among `` senior '' staff after the sec investigation has begun .",
    "component 10 in figure  [ fig : enron-10 ] indicates that a couple of `` legal '' staff are communicating with many `` other '' staff immediately after the sec investigation is announced , perhaps advising the `` other '' staff on appropriate responses to investigators .      as another example",
    ", we consider five years ( 1999 - 2004 ) of siam publication metadata that has previously been used by dunlavy et al .",
    "@xcite . here",
    ", we build a three - way sparse tensor based on title terms ( ignoring common stop words ) , authors , and journals .",
    "the author names have been normalized to last name plus initial(s ) .",
    "the resulting tensor is of size 4,952 ( terms ) @xmath324 6,955 ( authors ) @xmath324 11 ( journals ) and has 64,133 nonzeros ( 0.017% dense ) .",
    "the highest count is 17 for the triad ( ` education ' , ` schnabel b ' , ` siam rev . ' ) , which is a result of prof .",
    "schnabel s writing brief introductions to the education column for _ siam review_. in fact , the next 4 highest counts correspond to the terms ` problems ' , ` review ' , ` survey ' , and ` techniques ' , and to authors ` flaherty j ' and ` trefethen n ' .     1 & graphs , problem",
    ", algorithms , approximation , algorithm , complexity , optimal , trees , problems , bounds & kao my , peleg d , motwani r , cole r , devroye l , goldberg la , buhrman h , makino k , he x , even g & siam j comput , siam j discrete math + 2 & method , equations , methods , problems , numerical , multigrid , finite , element , solution , systems & chan tf , saad y , golub gh , vassilevski ps , manteuffel ta , tuma m , mccormick sf , russo g , puppo g , benzi m & siam j sci comput + 3 & finite , methods , equations , method , element , problems , numerical , error , analysis , equation & du q , shen j , ainsworth m , mccormick sf , wang jp , manteuffel ta , schwab c , ewing re , widlund ob , babuska i & siam j numer anal + 4 & control , systems , optimal , problems , stochastic , linear , nonlinear , stabilization , equations , equation & zhou xy , kushner hj , kunisch k , ito k , tang sj , raymond jp , ulbrich s , borkar vs , altman e , budhiraja a & siam j control optim + 5 & equations , solutions , problem , equation , boundary , nonlinear , system , stability , model , systems & wei jc , chen xf , frid h , yang t , krauskopf b , hohage t , seo jk , krylov nv , nishihara k , friedman a & siam j math anal + 6 & matrices , matrix , problems , systems , algorithm , linear , method , symmetric , problem , sparse & higham nj , guo ch , tisseur f , zhang zy , johnson cr , lin ww , mehrmann v , gu m , zha hy , golub gh & siam j matrix anal a + 7 & optimization , problems , programming , methods , method , algorithm , nonlinear , point , semidefinite , convergence & qi lq , tseng p , roos c , sun df , kunisch k , ng kf , jeyakumar v , qi hd , fukushima m , kojima m & siam j optimiz + 8 & model , nonlinear , equations , solutions , dynamics , waves , diffusion , system , analysis , phase & venakides s , knessl c , sherratt ja , ermentrout gb , scherzer o , haider ma , kaper tj , ward mj , tier c , warne dp & siam j appl math + 9 & equations , flow , model , problem , theory , asymptotic , models , method , analysis , singular & klar a , ammari h , wegener r , schuss z , stevens a , velazquez jjl , miura rm , movchan ab , fannjiang a , ryzhik l & siam j appl math + 10 & education , introduction , health , analysis , problems , matrix , method , methods , control , programming & flaherty j , trefethen n , schnabel b , [ none ] , moon g , shor pw , babuska i m , sauter sa , van dooren p , adjei s & siam rev + * * * *    computing a ten - component factorization yields the results shown in table  [ tab : siam ] .",
    "we use the same settings for cp - apr as specified in appendix  [ sec : compare_objectives - app ] . in the table , for the term and author modes , we list any entry whose factor score is greater than @xmath325 , where @xmath326 is the size of the @xmath30th mode ; in the journal mode , we list any entry greater than 0.01 .",
    "the 10th component corresponds to introductions written by section editors for _ siam review_. the 1st component shows that there is overlap in both authors and title keyword between the _ siam j. computing _ and the _",
    "siam j. discrete math_. the 2nd and 3rd components have some overlap in topic and two overlapping authors , but different journals .",
    "both components 8 and 9 correspond to the same journal but reveal two subgroups of authors writing on slightly different topics .",
    "we have developed an alternating poisson regression fitting algorithm , cp - apr , for ptf for sparse count data .",
    "when such data is generated via a poisson process , we show that methods based on kl divergence such as cp - apr recovers the true cp model more reliably than methods based on ls .",
    "indeed , in classical statistics , it is well known that the randomness observed in sparse count data is better explained and analyzed by the poisson model ( kl divergence ) than a gaussian one ( ls error ) .    our algorithm can be considered an extension of the lee - seung method for kl divergence with multiple inner iterations ( similar to @xcite for ls ) .",
    "allowing for multiple inner iterations has the benefit of accelerating convergence .",
    "moreover , being very similar to an existing method , cp - apr is simple to implement with the exception of some details of the sparse implementation as described in  [ sec : sparse ] . to the best of our knowledge ,",
    "ours is the first implementation of any kl - divergence - based method for large - scale sparse tensors .    in  [ sec : convergence ] , we provide a general - purpose convergence proof for the alternating gauss - seidel approach .",
    "the regularity conditions imposed in our proofs make rigorous and concrete our intuition that in the context of sparse count data , cp - apr will converge provided that the data tensor meets a minimal density and that nonzeros are sufficiently spread throughout the data tensor with respect to the size of the factor matrices being fit .",
    "any subproblem solver can be substituted for the mm method without changing the theory .",
    "a benefit of the mm subproblem solver is that its multiplier matrix can be used to explicitly track convergence based on the kkt conditions .",
    "moreover , we observe that we can use the kkt information to identify and correct inadmissible zeros using a `` scooch . ''",
    "lin @xcite had a similar observation in the ls case but came up with a different correction technique .",
    "we analyze convergence of the mm subproblem with the `` scooch '' in order to show that it will always converge .",
    "our results are stronger than past results because they allow iterates with some zero entries .",
    "even though zero entries are possible to avoid in exact arithmetic , they often occur numerical computations and so are important to consider .",
    "there remains much room for future work .",
    "foremost among practical considerations is speed of convergence .",
    "although multiplicative updates are relatively simple to compute , cp - apr can require many iterates .",
    "one approach to accelerating convergence would be to replace the mm algorithm subproblem solver .",
    "for example , kim et al .",
    "@xcite present fast quasi - newton methods for minimizing box - constrained convex functions that can be used to solve a nonnegative ls or minimum kl - divergence subproblem in a nonlinear gauss - seidel solver .",
    "a second approach is to focus on the sequence of outer iterates .",
    "zhou et al .",
    "@xcite provide a general quasi - newton acceleration scheme for iterative methods based on a quadratic approximation of the iteration map instead of the loss .",
    "there has also been significant work in finding sparse factors via @xmath327-penalization for matrices @xcite and tensors @xcite .",
    "sparse factors often provide more easily interpreted models , and penalization may also accelerate the convergence . while the factor matrices generated by cp - apr may be naturally sparse without imposing an @xmath327-penalty ,",
    "the degree of sparsity is not currently tunable .",
    "one may also consider extensions of this work in the context of missing data @xcite and for alternative tensor factorizations such as tucker @xcite .    perhaps most challenging , however , are open questions related to rank and inference .",
    "questions about how to choose rank are not new ; but given the context of sparse count data , might that structure be exploited to derive a sensible heuristic or even rigorous criterion for choosing the rank ? we already see that imposes an upper bound on the rank to ensure algorithmic convergence . regarding inference , our focus in this work was in thoroughly developing the algorithmic groundwork for fitting a ptf model for sparse count data .",
    "cp - apr can be used to estimate latent structure .",
    "once an estimate is in hand , however , it is natural to ask how much uncertainty there is in that estimate .",
    "for example , is it possible to put a confidence interval around the entries in the fitted factor matrices , especially zero or near zero entries ? given that inference for the related but simpler case of poisson regression has been worked out , we suspect that a sensible solution is waiting to be found .",
    "the benefits of answering these questions warrant further investigation .",
    "we highlight them as important topics for future research .",
    "we thank our colleagues at sandia for numerous helpful conversations in the course of this work , especially grey ballard and todd plantenga .",
    "we also thank kenneth lange for pointing us to relevant references on emission tomography .",
    "finally , we thank the anonymous referees and associate editor for suggestions which greatly improved the quality of the manuscript .",
    "[ [ outer - product ] ] outer product + + + + + + + + + + + + +    the outer product of @xmath1 vectors is an @xmath1-way tensor .",
    "for example ,    ( ) _ ijk = _ i _ j _ k.    [ [ elementwise - multiplication - and - division ] ] elementwise multiplication and division + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    let @xmath328 and @xmath329 be two same - sized tensors ( or matrices ) .",
    "then @xmath330 yields a tensor that is the same size as @xmath328 ( and @xmath329 ) such that @xmath331 for all @xmath13 . likewise , @xmath332 yields a tensor that is the same size as @xmath328 ( and @xmath329 ) such that @xmath333 for all @xmath13 .",
    "[ [ khatri - rao - product ] ] khatri - rao product + + + + + + + + + + + + + + + + + +    give two matrices @xmath19 and @xmath228 of sizes @xmath334 and @xmath335 , then @xmath336 is a matrix of size @xmath337 such that @xmath338 where the kronecker product of two vectors of size @xmath339 and @xmath340 is a vector of length @xmath341 given by @xmath342    [ [ matricization - of - a - tensor ] ] matricization of a tensor + + + + + + + + + + + + + + + + + + + + + + + + +    the mode-@xmath30 matricization or unfolding of a tensor @xmath0 is denoted by @xmath31 and is of size @xmath343 where @xmath344 . in this case ,",
    "tensor element @xmath13 maps to matrix element @xmath345 where @xmath346",
    "in this section , we provide a proof for lemma  [ lem : omegazeta ] .",
    "we first establish two useful lemmas .",
    "[ lem : lambda - bound ] let @xmath0 be fixed , let @xmath347 , and let @xmath57 be the objective function as in ( [ eq : nlp ] ) . if @xmath348 for some constant @xmath62 , then there exists constants @xmath349 ( depending on @xmath0 and @xmath350 ) such that @xmath351 $ ] .",
    "because the factor matrices are column stochastic , we can observe that @xmath352 we have @xmath353 .",
    "let @xmath354 where @xmath355 .",
    "we show that @xmath356 implies there exists @xmath349 such that @xmath357 $ ] .",
    "first assume there is no such lower bound @xmath358 . then there is a sequence @xmath359 tending to zero such that @xmath360 . but for sufficiently large @xmath30 , we have that @xmath361 . since @xmath362 for all @xmath30",
    ", we have that for sufficiently large @xmath30 the function @xmath363 .",
    "therefore , there is such a lower bound @xmath358 .",
    "now suppose there is no such upper bound @xmath364 , and therefore there is an unbounded and increasing sequence @xmath359 tending to infinity such that @xmath360 for all @xmath30 .",
    "note that @xmath365 . since @xmath366 is convex",
    ", we have that @xmath367 this inequality , however , indicates that for sufficiently large @xmath30 , the right hand side is greater than @xmath350",
    ". therefore , there must be an upper bound @xmath364 .",
    "substituting @xmath368 completes the proof .",
    "[ lem : finiteonomegazeta ] let @xmath0 be fixed , and let @xmath57 be the objective function as in ( [ eq : nlp ] ) .",
    "let @xmath61 be the convex hull of the level set of @xmath50 as defined in ( [ eq : omegazeta ] ) .",
    "the function @xmath57 is bounded for all @xmath108 .",
    "let @xmath369 .",
    "define @xmath370 to be the convex combination @xmath371 note that the restriction on @xmath372 is arbitrary but makes the proof simpler later on .",
    "observe that @xmath373 on the one hand , by lemma  [ lem : lambda - bound ] , there exists @xmath198 such that @xmath374 on the other hand , @xmath375 thus , @xmath376 now consider @xmath377 thus , @xmath378    given these two lemmas , we are finally ready to provide the proof of lemma  [ lem : omegazeta ] .",
    "fix @xmath350 .",
    "if @xmath379 is empty , then @xmath61 is empty and there is nothing left to do .",
    "thus , assume @xmath379 is nonempty .",
    "since @xmath50 is continuous at all @xmath380 for which @xmath381 is finite , @xmath50 is obviously continuous on @xmath61 by lemma  [ lem : finiteonomegazeta ] .",
    "since @xmath50 is continuous , @xmath382 is closed because it is the preimage of the closed set @xmath383 $ ] under @xmath50 ; thus , @xmath61 is closed because it is a convex combination of closed sets .",
    "consequently , we only need to show that @xmath61 is bounded . assume the contrary .",
    "then there exists a sequence of models @xmath384 such that @xmath385 . by lemma  [ lem : finiteonomegazeta ]",
    ", @xmath381 is finite on @xmath61 , but this contradicts lemma  [ lem : lambda - bound ] .",
    "hence , the claim .",
    "in this section we derive the mm update rules used to solve the subproblem .",
    "we first verify that ( [ eq : majorization ] ) majorizes ( [ eq : subproblem - simple ] ) .",
    "for convenience let @xmath128 so that ( [ eq : subproblem - simple ] ) reduces to @xmath386    proofs of the next two lemmas are given by lee and seung in @xcite but their arguments do not carefully handle boundary points .",
    "the following two lemmas and their proofs treat with more rigor the existence and value of updates when anchor points lie on admissible regions of the boundary .",
    "[ lem : mm ] let @xmath387 be a scalar and @xmath388 , @xmath389 , be a vector of length @xmath3 . for a vector @xmath390 , @xmath391 , of length @xmath3 , let the function @xmath50 be defined by @xmath392 then @xmath50 is majorized at @xmath393 by @xmath394    [ sec : proof - mm ]    if @xmath395 , then @xmath396 for all @xmath397 , and @xmath201 trivially majorizes @xmath50 at @xmath398 . consider the case when @xmath399 .",
    "it is immediate that @xmath400 .",
    "the majorization follows from the fact that @xmath68 is strictly concave and that we can write @xmath401 as a convex combination of the elements @xmath402 .",
    "note that if any elements @xmath403 are zero , they do not contribute to the sum since we assume @xmath44 for all @xmath45 and @xmath404 .",
    "we now derive an expression for the unique global minimizer of majorization . the majorization defined in ( [ eq : majorization ] )",
    "can be expressed in terms of @xmath405 as    @xmath406    { \\quad\\text{where}\\quad }    \\alpha_{rij } = \\frac{\\bar c_{ri}\\pi_{rj}}{\\sum_r \\bar c_{ri } \\pi_{rj}}.    \\ ] ]    [ lem : mm - unique - global - min ] let @xmath50 and @xmath201 be as defined in ( [ eq : subproblem_redux ] ) and ( [ eq : majorization ] ) , respectively .",
    "then , for all @xmath407 such that @xmath408 is finite , the function @xmath409 has a unique global minimum @xmath410 which is given by @xmath411 where @xmath412 , for all @xmath413 , @xmath414 .    because @xmath415 separates in the elements of @xmath405 we focus on solving each elementwise minimization problem .",
    "dropping subscripts , the minimization problem with respect to @xmath416 can be rewritten as @xmath417 where we have used the fact that @xmath418 . it is sufficient to prove that this univariate problem has a unique global minimizer , @xmath419 .",
    "first , consider the case where the second term is nonzero",
    ". inspecting the stationarity condition reveals the solution .",
    "moreover , the function is strictly convex and so has a unique global minimum .",
    "second , consider the case where the second term is zero . then , it is immediate that the unique global minimum is @xmath420 . moreover",
    ", the second term can only vanish when @xmath421 , and so the formula applies .",
    "in this section , we prove the mm algorithm in algorithm  [ alg : cpapr ] solves ( [ eq : subproblem - simple ] ) .",
    "we first establish the following general result for algorithm maps .",
    "part ( [ part : limit_points ] ) is a simple version of zangwill s convergence theorem @xcite in the case where the objective function and algorithm map are both continuous .",
    "the proof of part ( [ part : successive_iterates ] ) follows arguments of part of a proof for a different but related property on mm iterates in @xcite .",
    "[ thm : mm_limit_points ] let @xmath50 be a continuous function on a domain @xmath422 , and let @xmath423 be a continuous iterative map from @xmath422 into @xmath422 such that @xmath424 for all @xmath425 with @xmath426 .",
    "suppose there is an @xmath427 such that the set @xmath428 is compact .",
    "define @xmath429 for @xmath430 .",
    "then    [ part : limit_points ] the sequence of iterates @xmath431 has at least one limit point and all its limit points are fixed points of @xmath423 , and    [ part : successive_iterates ] the distance between successive iterates converges to 0 , i.e. , @xmath432 .",
    "the proof of ( [ part : limit_points ] ) follows that of proposition 10.3.2 of @xcite .",
    "first note that the sequence of iterates must be in @xmath433 because @xmath434 for all @xmath146 .",
    "since @xmath433 is compact , @xmath431 has a convergent subsequence whose limit is in @xmath433 ; denote this as @xmath435 as @xmath188 . since @xmath50 is assumed to be continuous , @xmath436 . moreover , clearly @xmath437 for all @xmath438 .",
    "note that @xmath439 .",
    "taking the limit of both sides and applying the continuity of @xmath423 and @xmath50 , we must have that @xmath440 .",
    "but we also have that @xmath441 again taking limits we obtain @xmath442",
    ". therefore @xmath443 .",
    "but by assumption , this equality implies that @xmath444 is a fixed point of @xmath423 , and thus ( [ part : limit_points ] ) is proven .",
    "we now turn to the proof of ( [ part : successive_iterates ] ) , which follows the proof of proposition 10.3.3 in @xcite .",
    "recall @xmath431 denotes the iterate sequence . since @xmath445 is decreasing and @xmath50 is bounded below on @xmath433",
    ", we can assert that @xmath445 is a convergent sequence with a limit @xmath446 .",
    "assume the contrary of ( [ part : successive_iterates ] ) , i.e. , that there exists an @xmath447 and a subsequence @xmath157 of the indices such that @xmath448 note that this subsequence is different from the one discussed in proving part  ( [ part : limit_points ] ) . since @xmath449 , by possibly restricting @xmath157 to a further subsequence , we may assume that @xmath450 converges to a limit @xmath451 .",
    "by possibly restricting @xmath157 to yet a further subsequence , we may additionally assume that @xmath452 converges to a limit @xmath18 .",
    "by ( [ eq : xkl ] ) , we can conclude @xmath453 .",
    "note that @xmath454 .",
    "taking the limit of both sides and using the continuity of @xmath423 we obtain @xmath455 . additionally ,",
    "using the continuity of @xmath50 , @xmath456 since @xmath457 , we have that @xmath458 which by assumption occurs if and only if @xmath459 .",
    "this implies that @xmath460 , and we have arrived at a contradiction .",
    "we now prove a series of lemmas leading up to a proof of the desired convergence result .",
    "[ lem : strict - decrease ] let @xmath461 such that @xmath462 is finite and suppose @xmath463",
    ". then @xmath464 .    by lemma  [ lem : mm - unique - global - min ]",
    "@xmath465 is the unique global minimizer of @xmath466 which majorizes @xmath50 at @xmath467 .",
    "therefore , if @xmath468 , we must have @xmath469 .",
    "[ lem : level - set - compact ] let @xmath50 be as defined in ( [ eq : subproblem - simple ] ) . for any nonnegative matrix @xmath239 such that @xmath240 is finite , the level set @xmath470 is compact .",
    "the proof follows the same logic as the proof for lemma  [ lem : lambda - bound ] .",
    "[ lem : mm_iterates_converge ] let @xmath50 be as defined in ( [ eq : subproblem - simple ] ) and @xmath423 be as defined in ( [ eq : mm - iterate ] ) , and suppose is satisfied .",
    "for any nonnegative matrix @xmath471 such that @xmath240 is finite , the sequence @xmath472 converges .",
    "note that all limit points of @xmath423 are fixed points of @xmath50 by theorem  [ thm : mm_limit_points ] .",
    "first , we show that the set of fixed point is finite .",
    "suppose that @xmath217 is a fixed point of @xmath423 .",
    "then we must have    ( - ( ) ) = 0 .    by lemma",
    "[ lem : strict - convexity ] , it can be verified that @xmath217 is the _",
    "unique _ global minimizer of @xmath473 where @xmath50 is as defined in ( [ eq : subproblem - simple ] ) .",
    "therefore , any fixed point that has the same zero pattern of @xmath217 must be equal to @xmath217 . since there are only a finite number of possible zero patterns , the number of fixed points is finite .",
    "since every limit point is a fixed point by theorem  [ thm : mm_limit_points]([part : limit_points ] ) , there are only finitely many limit points .",
    "let @xmath474 denote a collection of arbitrarily small neighborhoods around each fixed point indexed by @xmath277 .",
    "only finitely many iterates @xmath471 are in @xmath475 .",
    "so , all but finitely many iterates @xmath471 will be in @xmath476 . but @xmath477 eventually becomes smaller than smallest distance between any two neighborhoods by theorem  [ thm : mm_limit_points]([part : successive_iterates ] ) .",
    "therefore the sequence @xmath471 must belong to one of the neighborhoods for all but finitely many @xmath146 .",
    "so , any sequence of iterates must eventually converge to exactly one of the fixed points of @xmath423 .",
    "we now argue that it is impossible for the mm iterate sequence to converge to a non - kkt point if it has been appropriately initialized .",
    "[ lem : no_convergence_to_non_kkt ] let @xmath50 be as defined in ( [ eq : subproblem - simple ] ) and suppose is satisfied .",
    "suppose @xmath478 is a convergent sequence of iterates defined by ( [ eq : mm - iterate ] ) with @xmath233 and @xmath240 finite .",
    "if @xmath241 for all @xmath242 such that @xmath243 , then @xmath479 .    we give a proof by contradiction .",
    "suppose there exists @xmath242 such that @xmath480 but @xmath481 .",
    "since @xmath482 is a fixed point of @xmath423 , we must have @xmath483 ( \\mb{_{*}})_{ir } = 0 $ ] . by our assumption",
    ", however @xmath484 < 0 $ ] .",
    "thus , we must have @xmath485 . on the other hand , @xmath486 for all @xmath146 ( proof left to reader ) . since @xmath487 is a continuous function of @xmath217 on @xmath488",
    ", we know that there exists some @xmath489 such that @xmath490 implies @xmath471 is close enough to @xmath482 such that @xmath491 <    0 $ ] .",
    "since @xmath486 , we have @xmath492 ( \\mb{_{k}})_{ir } < 0 $ ] , which implies @xmath493 for all @xmath490 .",
    "but this contradicts @xmath494 .",
    "hence , the claim .    we now prove theorem  [ thm : subproblem - convergence ] .",
    "[ of theorem  [ thm : subproblem - convergence ] ] by lemma  [ lem : mm_iterates_converge ] , the sequence @xmath244 converges ; we call the limit point @xmath482 . at this limit point",
    ", we have :    @xmath495 ,    @xmath496 by lemma  [ lem : no_convergence_to_non_kkt ] ,    and @xmath497 by virtue of @xmath482 being a fixed point of @xmath423 .",
    "thus , the point @xmath482 satisfies the kkt conditions with respect to ( [ eq : subproblem - simple ] ) .",
    "furthermore , since @xmath50 is strictly convex by lemma  [ lem : strict - convexity ] , we can conclude that @xmath482 is the global minimum of @xmath50 .",
    "all implementations are from version 2.5 of tensor toolbox for matlab @xcite .",
    "all methods use a common initial guess for the solution .    * * lee - seung ls * : implemented in ` cp_nmu ` as descibed in @xcite .",
    "we use the default parameters except that the maximum number of iterations ( ` maxiters ` ) is set to 200 and the tolerance on the change in the fit ( ` tol ` ) is set to @xmath498 . * * cp - als * : implemented in ` cp_als ` as described in @xcite .",
    "we use the default parameter settings except that the maximum number iterations ( ` maxiters ` ) is 200 and the tolerance on the changes in fit ( ` tol ` ) is @xmath498 . *",
    "* lee seung kl * : implemented in ` cp_apr ` as described in this paper .",
    "the parameters are set as follows : @xmath499 ( ` maxiters ` ) , @xmath500 ( ` maxinneriters ` ) , @xmath501 ( ` tol ` ) , @xmath502 ( ` kappa ` ) , @xmath322 ( ` epsilon ` ) . * * cp - apr * : implemented in ` cp_apr ` as described in this paper .",
    "the parameters are set as follows : @xmath499 ( ` maxiters ` ) , @xmath503 ( ` maxinneriters ` ) , @xmath504 ( ` tol ` ) , @xmath505 ( ` kappa ` ) , @xmath506 ( ` kappatol ` ) , @xmath322 ( ` epsilon ` ) .",
    "we compare the methods in terms of their `` factor match score , '' defined as follows .",
    "let @xmath315 be the true model and let @xmath507 be the computed solution .",
    "the score of @xmath508 is computed as @xmath509 the fms is a rather abstract measure , so we also give results for the number of columns in @xmath510 that are correctly identified . in other words ,",
    "we count the number of times that the cosine of the angle between the true solution and the computed solution is greater than 0.95 , mathematically , @xmath511 .",
    "we use the first mode , but the results are representative of the other modes .",
    "figure  [ fig : enron - app ] illustrates the four components omitted in figure  [ fig : enron ] .          ,",
    "http://dx.doi.org/10.1007/978-1-84800-046-9[_discussion tracking in enron email using parafac _ ] , in survey of text mining ii : clustering , classification , and retrieval , m.  w. berry and m.  castellanos , eds . ,",
    "springer , london , 2008 .",
    ", http://dx.doi.org/10.1109/cvpr.2005.118[_damped newton algorithms for matrix factorization with missing data _ ] , in cvpr05 : 2005 ieee computer society conference on computer vision and pattern recognition , vol .  2 , ieee computer society , 2005 , pp",
    ".  316322 .    , http://dx.doi.org/10.1007/bf02310791[_analysis of individual differences in multidimensional scaling via an n - way generalization of `` eckart - young '' decomposition _ ] , psychometrika , 35 ( 1970 ) , pp .",
    "283319 .    , http://dx.doi.org/10.1109/icassp.2007.367106[_non-negative tensor factorization using alpha and beta divergences _ ] , in icassp 07 : proceedings of the international conference on acoustics , speech , and signal processing , 2007 .        , _ multilinear algebra for analyzing data with multiple linkages _ , in graph algorithms in the language of linear algebra , j.  kepner and j.  gilbert , eds . ,",
    "fundamentals of algorithms , siam , philadelphia , 2011 , pp .  85114 .                    , _ foundations of the parafac procedure : models and conditions for an  explanatory \" multi - modal factor analysis _ , ucla working papers in phonetics , 16 ( 1970 ) , pp",
    "available at http://www.psychology.uwo.ca/faculty/harshman/wpppfac0.pdf .",
    ", http://dx.doi.org/10.1137/07069239x [ _ nonnegative matrix factorization based on alternating nonnegativity constrained least squares and active set method _ ] , siam journal on matrix analysis and applications , 30 ( 2008 ) , pp",
    ".  713730 .",
    ", http://dx.doi.org/10.1109/tnn.2007.895831[_on the convergence of multiplicative update algorithms for nonnegative matrix factorization _ ] , ieee transactions on neural networks , 18 ( 2007 ) , pp .  15891596 .        , http://dx.doi.org/10.1109/bibm.2010.5706574[_sparse nonnegative matrix factorization with the elastic net _ ] , in bibm2010 : proceedings of the ieee international conference on bioinformatics and biomedicine , dec . 2010 , pp .",
    "265268 .        , _ decomposing the time - frequency representation of eeg using nonnegative matrix and multi - way factorization_. available at http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/4144/pdf/imm4144.pdf , 2006 .        , http://dx.doi.org/10.1016/s0169-7439(97)00031-2[_a weighted non - negative least squares algorithm for three - way `` parafac '' factor analysis _ ] , chemometrics and intelligent laboratory systems , 38 ( 1997 ) , pp .",
    "223242 .    , http://dx.doi.org/10.1002/env.3170050203[_positive matrix factorization : a non - negative factor model with optimal utilization of error estimates of data values _ ] , environmetrics , 5 ( 1994 ) , pp .",
    "111126 .            , http://dx.doi.org/10.1145/1150402.1150445[_beyond streams and graphs : dynamic tensor analysis _ ] , in kdd 06 : proceedings of the 12th acm sigkdd international conference on knowledge discovery and data mining , acm press , 2006 , pp .",
    "374383 .          , http://dx.doi.org/10.1007/s10618-010-0196-4[_nonnegative tensor factorization as an alternative csiszar ",
    "tusnady procedure : algorithms , convergence , probabilistic interpretations and novel probabilistic tensor latent variable analysis algorithms _ ] , data mining and knowledge discovery , 22 ( 2011 ) , pp .",
    "419466 .        , _ strategies for cleaning organizational emails with an application to enron email dataset_. naacsos 07 : 5th conference of north american association for computational social and organizational science , june 2007"
  ],
  "abstract_text": [
    "<S> tensors have found application in a variety of fields , ranging from chemometrics to signal processing and beyond . in this paper , we consider the problem of multilinear modeling of _ sparse count _ data . </S>",
    "<S> our goal is to develop a descriptive tensor factorization model of such data , along with appropriate algorithms and theory . </S>",
    "<S> to do so , we propose that the random variation is best described via a poisson distribution , which better describes the zeros observed in the data as compared to the typical assumption of a gaussian distribution . under a poisson assumption , </S>",
    "<S> we fit a model to observed data using the negative log - likelihood score . </S>",
    "<S> we present a new algorithm for poisson tensor factorization called candecomp  parafac alternating poisson regression ( cp - apr ) that is based on a majorization - minimization approach . </S>",
    "<S> it can be shown that cp - apr is a generalization of the lee - seung multiplicative updates . </S>",
    "<S> we show how to prevent the algorithm from converging to non - kkt points and prove convergence of cp - apr under mild conditions . </S>",
    "<S> we also explain how to implement cp - apr for large - scale sparse tensors and present results on several data sets , both real and simulated .    </S>",
    "<S> nonnegative tensor factorization , nonnegative candecomp - parafac , poisson tensor factorization , lee - seung multiplicative updates , majorization - minimization algorithms </S>"
  ]
}