{
  "article_text": [
    "many machine learning applications involve inherently multi - relational domains in which entities of heterogeneous types engage in a variety of relations .",
    "the statistical relational learning ( srl ) @xcite community has introduced representations that provide principled support for learning and reasoning in such multi - relational data . in a nutshell ,",
    "srl models use an expressive relational language , such as first - order logic or sql , to define relational features capable of capturing salient aspects of the structure of the domain .",
    "these relational features come with a parameterization , such that , once instantiated , they define a graphical model over which probabilistic inference can be performed .",
    "srl techniques have been successfully applied in domains as diverse as biology , natural language processing , ontology alignment , social networks , and the web .",
    "the basic design cycle of many such applications follows a trial - and - error trajectory , where relational features are manually defined by a human engineer , parameters are learned for those features on the training data , the resulting model is validated , and the cycle repeats as the human engineer adjusts the set of features .",
    "for example , this is the strategy recommended in the markov logic network tutorial  @xcite , slide 108 , that comes with the widely used alchemy software @xcite . typically , as a result ,",
    "a relatively small number of relational features are identified and used .",
    "this approach is appealing because background knowledge about the domain can be easily encoded in the intuitive relational language used in the srl model at hand , and the relative strengths of these features can be learned through the parameterization .",
    "an alternative to this design cycle is to use structure learning , where both the relational features and their accompanying parameterization are induced automatically from data .",
    "the state - of - the - art in structure learning has been advanced significantly in recent years @xcite .",
    "this work has resulted in highly innovative approaches to identifying potentially promising structure candidates and efficiently navigating through the large and complex search space for structures .",
    "so far , less emphasis has been placed on how , once identified , structure candidates are evaluated .",
    "existing techniques employ different flavors of a batch evaluation procedure , where candidate structures are scored on the available data and retained in the model if they improve the score .",
    "crucially , all existing techniques presume that all of the data , or , at least joins of pairs of relational tables @xcite , can be stored in memory for the purposes of candidate structure evaluation .",
    "while the importance of existing structure learning approaches is undisputed , their focus on identifying promising candidate structures , coupled with the assumption of batch scoring from in - memory data , is not a good match to all application scenarios . in particular , the designers of many srl applications skipped structure learning entirely and instead preferred the trial - and - error design cycle outlined above .",
    "there are at least two plausible reasons underlying their chosen approach .",
    "first , the applications frequently involved data sets that were simply too large to allow for batch scoring of candidate structures . to overcome this , parameters over hand - coded relational features",
    "have been trained in parallel @xcite , or on a stream of examples @xcite . beyond the problem of storing large amounts of data in memory , in some domains",
    ", the data may actually arrive as a stream and not be available all at once .",
    "second , the designers of many applications often already have intuitions about good relational features , either from existing domain knowledge , such as in natural language or social network tasks , or from having worked with the `` raw '' data and developed a representation for it that is already biased by what they intuitively perceive as important aspects to capture . in such cases , where domain knowledge allows one to identify what variables are likely to influence each other",
    ", the design bottleneck is in pinning down the exact formulation of the influences among variables and eliminating features based on spurious intuitions . in other words ,",
    "the problem of evaluating candidate features efficiently is at least as important as suggesting them .",
    "this paper seeks to streamline application development in relational domains by introducing an approach that efficiently evaluates relational features on pieces of the relational graph that are streamed to it one at a time .",
    "we call our approach resolwe , for lational tructure election from nline ight - eight valuation .",
    "resolwe  is agnostic to where the relational features it evaluates come from .",
    "they could be discovered locally from each piece of the data that is being streamed using an existing structure discovery approach , or provided by an application developer . in this paper",
    "we take a semi - automated approach where the human designer specifies a declarative bias , e.g. , @xcite , by providing a grammar for the relational features , from which all possible features are generated .",
    "this corresponds to the scenario outlined above where general intuitions about the domain are available , but determining the precise formulation of features requires efficiently evaluating different versions of the model on a large relational data set .",
    "we implement our approach in the framework of markov logic networks ( mlns ) @xcite .",
    "this choice is motivated by the fact that mlns have been widely used for relational application development .",
    "we start by providing necessary background in section  [ sect : background ] .",
    "resolwe  is described in section  [ sect : resolve ] . in section  [",
    "sect : experiments ] , we flesh out our proposed technique by using it to develop two applications in relational social media domains .",
    "our results indicate that resolwe  leads to significantly faster and more accurate learning .",
    "we conclude with a discussion of related and future work .",
    "* first - order logic terminology . * in first - order logic , relations are represented as _ predicates , _ such as articleedit(article , editor ) , which are boolean functions with typed arguments .",
    "assuming that the domain contains no functions , a _ term _",
    "is defined as a variable or a constant .",
    "an _ atom _ is a predicate applied to terms , such as articleedit(a , e ) .",
    "a positive / negative _ literal _ is a non - negated / negated atom .",
    "a literal is _ grounded _ if all of its arguments are constants , or actual entities from the domain ; conversely , a literal is _ ungrounded _ if all of its arguments are variables .",
    "a formula consists of literals connected by conjunction and disjunction .",
    "formulas whose literals contain only constants are grounded , whereas formulas whose literals contain only variables are ungrounded .",
    "we will refer to grounded formulas as _",
    "groundings_.    * learning setting : * we assume fully observable training data that consists of a large relational graph @xmath0 whose hyperedges and nodes can correspond to different relations and entity types respectively . in practice , @xmath0 is typically too large to fit in memory all at once , and/or parts of it arrive as learning progresses .",
    "thus , we address a scenario where subgraphs of @xmath0 arrive in a data stream @xmath1 .",
    "second , we assume a discriminative setting where one or more relations in a set @xmath2 are designated as target predicates whose values are to be predicted at test time , and the remaining relations @xmath3 are the evidence predicates whose values are observed at test time .",
    "* markov logic networks . *",
    "a markov logic network ( mln ) @xcite consists of a set of first - order logic formulae @xmath4 , each of which has an associated weight .",
    "mlns can be viewed as relational analogs to markov networks , in which the potential functions over cliques are defined by the groundings of the formulae in @xmath4 .",
    "the role of first - order logic , therefore , is to provide a highly expressive language for specifying general relational features .",
    "grounding the first - order logic formulae with a given set of entities results in a markov network .",
    "in particular , an mln computes the conditional joint probability of a set of predicate groundings @xmath5 of the target predicates @xmath2 , given truth values for a set of evidence predicate groundings @xmath6 as follows : @xmath7 above , @xmath5 and @xmath6 are the sets of all target and evidence predicate groundings , respectively ; @xmath8 and @xmath9 are the sets of corresponding truth assignments ; @xmath10 is the weight associated with formula @xmath11 ; @xmath12 is the number of true groundings of formula @xmath11 on truth assignment @xmath13 ; and the denominator computes the normalizing partition function @xmath14 .    for ease of exposition , in this work",
    "we assume that all of the formulas in the mln are conjunctions .",
    "this is not a restrictive assumption for the following reason .",
    "the most mature implementation of mlns , alchemy @xcite , handles arbitrary formulas by converting them into conjunctive normal form , as a conjunction of disjunctions .",
    "each disjunction produced in this way is then treated as a separate formula in the mln , i.e. , by viewing the mln as a soft conjunction of disjunctions .",
    "each disjunction in the mln can then be converted to a conjunction by negating it , and also negating its weight .",
    "learning on the stream @xmath1 proceeds in a three - stage process :    1 .",
    "the first @xmath15 sub - graphs that arrive are used to generate a set of relational features ( in our case , first - order logic mln formulae ) @xmath4 .",
    "2 .   resolwe  uses the next @xmath16 sub - graphs from @xmath1 to evaluate the formulae in @xmath4 , outputting a subset @xmath17 consisting of the formulae that , together , give good predictive accuracy for the groundings of @xmath2 given as observations the groundings of @xmath3 .",
    "the remainder of @xmath1 is used to train parameters on the formulae in @xmath18 .    in this paper ,",
    "the set of formulas @xmath4 in step 1 above is generated without requiring any data from the stream ( i.e. , @xmath19 ) by using a declarative bias ( in the spirit of @xcite ) to specify templates , from which all possible formulas that comply with the bias are formed .",
    "details on the templates used in each of our domains are provided in section  [ sect : experiments ] .",
    "our goal in using this semi - automatic procedure to generate @xmath4 was to take advantage of available background knowledge , while using systematic rule generation to make sure we do not inadvertently place the baseline to which we compare at a disadvantage      the goal of resolwe  is to provide a light - weight formula evaluation strategy that can be carried out by considering sub - graphs of the training data graph @xmath0 that arrive in a stream one at a time .",
    "the key is to develop criteria for what makes an effective formula and ensure that these criteria can be evaluated efficiently on @xmath1 . for this purpose , it is useful to rewrite each formula @xmath20 as @xmath21 , where @xmath22 is the evidence sub - formula and consists of all literals of @xmath20 with predicates in @xmath3 , @xmath23 is the query sub - formula and consists of all literals of @xmath20 with predicates in @xmath2 .",
    ", @xmath20 is assumed to be a conjunction .",
    "] we can view the roles of @xmath22 and @xmath23 in @xmath20 as _ selector _ and _ enforcer _ respectively : groundings of @xmath20 in which the corresponding grounding of @xmath22 is satisfied are `` selected '' and a particular pattern , or configuration of truth values , specified by @xmath23 is `` enforced '' over the truth values of the corresponding grounding of @xmath23 .",
    "this is because the truth value of groundings for which the part corresponding to @xmath22 is @xmath24 can never change to @xmath25 , regardless of the assignments made to the part corresponding to @xmath23 , and thus do not affect the values assigned to the ground literals corresponding to @xmath23 and are safely ignored by the inference . in other words , because groundings only affect inference if their @xmath22 part is satisfied , @xmath22 can be viewed as `` selecting '' those groundings .",
    "this view of @xmath20 allows us to specify two criteria for its effectiveness : ( 1 ) among the groundings of @xmath23 selected by @xmath22 , how _ uniformly _ do we observe the pattern that @xmath23 enforces in the data ; and ( 2 ) how _ surprising _ is that pattern , i.e. , how likely is one to observe the pattern in a randomly selected set of groundings of @xmath23 .",
    "intuitively , the uniformity criterion measures the correctness of @xmath20 .",
    "however , in the case of srl models , we are equally interested in very correct formulas and very _ in_correct ones , the latter getting negative weights during parameter training .",
    "the motivation for the second criterion is that we would like to find relational features that capture aspects beyond those that can be captured by simply using a prior over truth values .",
    "next , we make these criteria precise .",
    "let @xmath26 be a set of ungrounded literals and let @xmath27 be a randomly chosen grounding of @xmath26 .",
    "the joint assignment of truth values to the grounded literals in @xmath27 is a random variable @xmath28 with @xmath29 possible outcomes , i.e. if @xmath30 contains a single literal , the possible outcomes are @xmath31 ; if it contains two literals , the possible outcomes are @xmath32 , etc .",
    ", some of the literals become identical . ]",
    "we are interested in the probability distribution that governs @xmath28 .",
    "this distribution can be estimated empirically given observed truth assignments for a set of groundings @xmath33 of @xmath26 by simple counting as the proportion of time a particular configuration of values is observed .",
    "+ * definition 1 .",
    "* let @xmath34 represent the empirical distribution over joint truth assignments to a randomly chosen grounding of @xmath26 , as estimated on a set of groundings @xmath35 .",
    "armed with this notation , we now go back to the view of a formula @xmath20 as consisting of a selector @xmath22 and an enforcer @xmath23 and consider the empirical distribution @xmath36 , where @xmath37 is the set of literals in @xmath23 , and @xmath38 is the set of groundings of @xmath37 selected by @xmath22 , which is , in general , a subset of all possible groundings of @xmath37 . the first , uniformity , criterion identified above states that in an effective formula , @xmath39 , the probability , according to @xmath36 , of observing the configuration of truth values enforced by @xmath23 should be as extreme as possible . + * criterion 1 . *",
    "effective formulas maximize @xmath40 .",
    "the second criterion states that @xmath36 should be significantly different from the `` default '' @xmath41 , where @xmath42 is the set of all possible groundings of @xmath37 .",
    "in other words , we are interested in formulas whose selector @xmath22 homes in on groundings of @xmath37 for which the distribution over observed truth values deviates significantly from the default distribution over all possible groundings of @xmath37 . + * criterion 2 . *",
    "effective formulas maximize @xmath43 .",
    "criterion 2 can be evaluated using standard measures of the distance between two distributions , such as kl divergence @xcite , or by methods that are specifically designed to detect significant deviations on a large scale , e.g. , @xcite .",
    "however , evaluating such measures may be expensive .",
    "in particular , to estimate @xmath41 , we need to enumerate the observed joint truth values for all possible @xmath44 groundings of @xmath37 , where @xmath45 is the number of entities in the domain and @xmath46 is the number of distinct variables appearing in @xmath37 . in general , this number is much higher than the number of groundings selected by @xmath22 .",
    "instead , we note that relational domains are typically very sparse , i.e. the number of relations actually observed to be true is typically much smaller than the total number of possible relations that can form .",
    "thus , rather than estimating @xmath41 for different sets @xmath37 from the data , we can make the approximately correct assumption that @xmath41 will be skewed towards configurations that involve @xmath24 assignments to the literals .",
    "in essence , this means that we can assume the same skewed default distribution @xmath47 for all sets @xmath37 that contain @xmath48 literals .",
    "this assumption allows us to significantly simply the evaluation of rules according to the two criteria .",
    "next , we describe how this is done for sets @xmath37 of different sizes .",
    "the simplest case is when @xmath49 , i.e. , where the formula @xmath20 contains a single literal @xmath50 of a target predicate .",
    "supposing that @xmath50 is non - negated , @xmath47 is a bernoulli distribution , which , because of the skew towards @xmath24 assignments , has a very small probability of success .",
    "thus , to satisfy criterion 2 and maximize @xmath43 , @xmath36 needs to have a large probability of success . combined with criterion 1",
    ", we note that the only way both criteria may be satisfied is if @xmath51 is maximized .",
    "thus , when @xmath52 , maximizing both criteria is as simple as requiring that the rule correctly identifies regions that contain high proportions of true positives .",
    "the case when @xmath50 is negated is symmetric .",
    "the situation is a bit less straightforward when @xmath53 .",
    "for ease of exposition , let us assume that @xmath54 contains two non - negated literals @xmath50 and @xmath55 .",
    "now , according to the sparsity assumption , @xmath47 is skewed towards truth assignments in which at least one of @xmath50 or @xmath55 is @xmath56 thus , in order to satisfy criterion 2 , we need formulas for which @xmath36 places significant mass on the case where @xmath50 and @xmath55 are both @xmath25 . combined with criterion 1 , this means that effective formulas are ones for which @xmath57 is large . due to the sparsity in relational domains , in practice formulas with such selectors",
    "@xmath22 are rare .",
    "a second way to address criterion 2 is to look for formulas in which the conditional probability of one of @xmath50 or @xmath55 being @xmath25 , given that the other one is @xmath58 is surprisingly high . in terms of criterion 1 , this translates into formulas for which @xmath59 is high .",
    "thus , when @xmath60 , resolwe   autonomously determines the precise formulation of the enforcer @xmath23 from the non - negated literals in @xmath37 by evaluating @xmath57 , @xmath59 , and @xmath61 and choosing ones that result in high values .",
    "we note that the formulation @xmath62 does not contradict our assumption that @xmath20 is a conjunction .",
    "@xmath62 can be expressed in conjunctive form as @xmath63 , thus @xmath64 . when all literals in @xmath22 are observed , the effect of @xmath20 is equivalent to that of a formula @xmath65 , for which a negative weight is learned , despite the fact that @xmath20 and @xmath66 are not logically equivalent .    to summarize , in the general case , when @xmath54 consists of @xmath67 literals @xmath68 , resolwe  evaluates @xmath69 and for each @xmath70 $ ] @xmath71 , i \\neq k } q_i = true)\\ ] ] and selects formulations that have high probabilities .",
    "this process is summarized in algorithm  [ algo : resolve ] .",
    "steps 6 - 8 and 15 - 19 are only evaluated if the formula has more than one literal of a target predicate .",
    "the algorithm reduces the evaluation of the candidate formulas to the collection of a few statistics for each formula that can be easily computed on a stream of examples .",
    "moreover , by taking advantage of the simplifying assumption , the algorithm avoids having to estimate @xmath41 .",
    "+   +   +   +   +   + @xmath72 sub - formula of @xmath20 consisting of literals with predicates in @xmath3 @xmath73 set of literals of @xmath20 with predicates in @xmath2 compute @xmath74 compute @xmath75 , i \\neq k } q_i)$ ] add @xmath76 to @xmath18 add @xmath77 to @xmath18 .",
    "in this section , we demonstrate the methodology proposed in section  [ sect : resolve ] by developing applications in two social media domains .",
    "we compare resolwe  to a system ( called skipselection ) that skips the second step outlined at the beginning of section  [ sect : resolve ] and directly trains weights on the formulas in the original set @xmath4 .",
    "because for formulas with @xmath78 resolwe  automatically determines the correct logical connectives and negations in the part of the formula that involves literals of the target predicate ( i.e. , lines 6 - 8 and 15 - 19 in algorithm  [ algo : resolve ] ) , the set @xmath4 over which weights were trained by skipselection  included all possible formulas considered by resolwe .",
    "the goal of our experiments was 1 ) to determine whether more accurate models are obtained with resolwe ; and 2 ) to evaluate the relative efficiency of resolwe  and skipselection .",
    "we implemented resolwe  as part of the alchemy system @xcite . for weight - training",
    ", we adapted contrastive divergence @xcite to learn from relational instances that arrive in a stream and used a gaussian penalty on the weights .",
    "we preferred this algorithm over other available methods because we are interested in an efficient , light - weight approach .",
    "for inference , we used the mc - sat algorithm @xcite .",
    "generation of formulas from provided templates was implemented as a separate module in python .",
    "the experiments were conducted in two social media domains  wikicollabs and delicious .",
    "the task in this domain is to predict project - specific collaborations in wikipedia .",
    "the data consists of all 3,538 wikipedia articles that appeared in the featured and controversial lists in the period oct . 7 - 21 , 2009 .",
    "these articles are interesting because they are richly connected , both by their hyperlinks and by their human network of editors @xcite . for each article",
    ", we collected the editors who contributed to it , either by directly editing the article , or by editing its `` talk , '' i.e. , discussion , page .",
    "only edits that were not marked as `` minor '' by the editor were considered . in this way",
    ", we obtained a set of 280,068 editors .",
    "in addition , we collected the hyperlinks among the articles .",
    "these articles are densely inter - linked , as indicated by the large number of hyperlinks ( 45,006 ) among them .",
    "wikipedia articles often refer to external resources on the web .",
    "thus , for each article , we looked up the categorizations of each of its external references in the dmoz open directory . because this information is not available for all urls",
    ", we considered both exact matches of urls , for which there were about 0.9 per article , as well as exact matches for just the domain name part of the url , for which there were about 77 per article .",
    "an editor @xmath79 on wikipedia can communicate with another editor @xmath80 by editing @xmath80 s `` talk '' page .",
    "there were a total of 7,874,985 instances of communication between pairs of editors .",
    "the set of evidence predicates @xmath3 included articleedit(article , user ) and articletalk(article , user ) for the two ways in which a user may contribute to an article ; usertalk(user , user ) ; hyperlink(article , article ) ; similar(article , article ) , indicating that the cosine similarity between the tf / idf - weighted bag - of - words representation of the stemmed text in the two articles is between 0.1 and 0.5 ; verysimilar(article , article ) , indicating the similarity is greater than 0.5 ; category(article , category ) , providing the category under which the article appeared on the featured or controversial list , levelnexact(article , externalcategory ) and levelninexact(article , externalcategory ) for the different levels @xmath81 in the dmoz hierarchy in which an external url from an article is filed , either for exact url matches or for matches of just the domain name of the url .",
    "the data from the wikicollabs network was streamed in subgraphs @xmath82 that were centered around one of the editors @xmath83 .",
    "@xmath82 contains all articles @xmath84 to which @xmath83 is related via the articleedit or articletalk predicates and all editors @xmath85 , in addition to @xmath83 that contributed to any of the articles in @xmath84 , as well as the other articles to which the editors in @xmath85 contributed .",
    "also included were any hyperlinks among the included articles , any instances of usertalk relationships among the editors in @xmath85 , any available category information on the included articles .",
    "the task was to learn to predict which editor in @xmath85 contributes to the articles in @xmath84 , given all other information .",
    "for convenience , we represented the relationship between articles in @xmath84 and the users in @xmath85 by the modifies(article , user ) predicate , which was the only target predicate in @xmath2 .",
    "subgraphs were formed for users @xmath83 who made edits to the encyclopedia on at least 30 distinct days , had at least 30 collaborators , and edited at most 15 different articles .",
    "these restrictions are motivated by the observation that collaborator suggestion is most needed by editors who are strongly engaged with the encyclopedia , and so contribute to it over extended periods , but at the same time are focused in their interests . in this way , we excluded users , such as the `` 60% of registered users [ who ] never make another edit after their first 24 hours '' @xcite , as well as users who help oversee the editing process and are therefore somewhat superficially involved in large numbers of edits , from having subgraphs @xmath82 formed around them . however , such users can still appear in the subgraph of another user .",
    "we obtained a total of 1785 subgraphs .",
    "formulas for the wikicollabs task were generated from the templates shown in table  [ wikicollabstemplates ] .",
    "predicates in all - caps are templates that get expanded in designer - specified ways , as shown in table  [ wikicollabsexpansions ] .",
    "@xmath86    @xmath87exact}(t1 , c ) \\wedge \\mathtt{level[1|2|3]exact}(t2 , c ) |\\\\ & \\mathtt{level[1|2|3]inexact}(t1 , c ) \\wedge \\mathtt{level[1|2|3]inexact}(t2 , c ) \\}\\\\",
    "user\\_rel(u1 , u2 ) = & \\{\\mathtt{usertalk}(u1 , u2 ) | \\\\ & \\mathtt{articleedit}(t , u1 ) \\wedge \\mathtt{articleedit}(t , u2)| \\\\ & \\mathtt{articletalk}(t , u1 ) \\wedge \\mathtt{articletalk}(t , u2)|   \\\\ & \\mathtt{articleedit}(t , u1 ) \\wedge \\mathtt{articletalk}(t , u2)| \\\\ & \\mathtt{articletalk}(t , u1 ) \\wedge \\mathtt{articleedit}(t , u2)\\}\\end{aligned}\\ ] ]    as can be seen from these expansions , the @xmath88 template captures the different ways in which an editor may be related to an article , the @xmath89 and @xmath90 expand to the different ways in which two articles may be related , and @xmath91 expands to the different ways in which two users may be related .",
    "the @xmath88 and @xmath89 predicate templates were declared to be _ compounders _ , which means that when they are expanded in a rule , they can be replaced by a conjunction of more than one of their possible expansions .",
    "for example , in rule 1 above , @xmath92 may be expanded to @xmath93 , or @xmath94 , or @xmath95 .",
    "we limited the length of compoundings to at most 2 .",
    "resolwe  received only one version of rules generated from templates  [ len2.1 ] and  [ len2.2 ] , as it determines the correct configuration of literals of target predicates automatically .",
    "the task in this data set is to predict user friendships on the delicious social bookmarking site @xcite .",
    "we used the data collected by the authors of @xcite , which includes 425,486 instances of the `` fan '' relationship , which indicates that one user is a fan of another one , 446,879 instances of the `` network '' relationship , which is the inverse of `` fan '' ( i.e. , if a is a fan of b , then b is in a s network ) , and 48,809,570 instances of the ternary `` tagging '' relationship between a user , a tag , and a url . although the `` fan '' and `` network '' relationships are inverses of one another , the observations in the data were not complete .",
    "we completed them by treating them as a single `` friendship '' relationship .    to stream this data , we formed subgraphs @xmath82 , each of which was centered at one of the users @xmath83 .",
    "the task was to predict all friendships of @xmath83 .",
    "each @xmath82 included @xmath83 s actual friends @xmath96 as true positives , and , as true negatives , a sampling of users who are friends with users from @xmath96 .",
    "we did not form subgraphs for users @xmath83 for which the number of true negative friends was not at least as large as the number of true positive friends .",
    "the friendships between @xmath83 and the other users were hidden at test time , and the goal was to predict their existence .",
    "however , friendships among users other than @xmath83 were observed .",
    "for convenience in distinguishing between these two cases , we included an observed and an unobserved version of the friendship relationship : an unobserved , i.e. , target , cfriends(user ) predicate indicating that the user is friends with the implicit @xmath83 , and an observed , i.e. , evidence , friends(user , user ) predicate indicating that the two users are friends . for all users in @xmath82 , we included observations about all urls they bookmarked , along with the tags used .",
    "those were captured via the following predicates : bkmark(page , user , tag ) ; bkmarkafter(page , user ) , which indicates that the user bookmarked the page at least one day after it was bookmarked by @xmath83 ; bkmarkbefore(page , user ) , bkmarksameday(page , user ) , which provide analogous information for pages bookmarked before or on the same day as bookmarked by @xmath83 ; usedtag(tag , user ) ; sametag(user , user ) and sameurl(user , user ) to indicate , respectively , that two users ( different from @xmath83 ) used the same tags and bookmarked the same urls .",
    "we used 656 subgraphs constructed in this way .",
    "formulas for the delicious task were generated using the templates shown in table  [ delicioustemplates ] .",
    "we used the expansions shown in table  [ deliciousexpansions ] .",
    "@xmath97    @xmath98    the @xmath99 and @xmath90 templates expand to predicates that relate users to the user @xmath83 around whom the graph @xmath82 is centered via various bookmarking and tagging activities , whereas @xmath100 expands to different ways of relating two users .",
    "@xmath99 was declared a compounder , and @xmath100 was declared an _ extender _ , which meant that one or more possible expansions could be chained together .",
    "for example , @xmath101 could be expanded in ways such as @xmath102 .",
    "we allowed extensions and compoundings of length at most 2 .",
    "as before , resolwe  only needs the expansion from only one of the templates in lines  [ len2.3 ] and  [ len2.4 ] , as it determines the specific formulation autonomously .",
    "we performed four - fold cross - validation by splitting the subgraphs in the data randomly into 4 sets and performing 4 train / test runs , in each run withholding one of the folds for testing and training on the remaining three .",
    "we used @xmath103 and @xmath104 in algorithm  [ algo : resolve ] . before training weights , both for resolwe  and skipselection , we included a clause consisting of a single literal of the target predicate",
    "this is standard practice in mln applications that enables the model to capture the bias towards @xmath24 assignments by learning a negative weight on this single - literal clause .",
    "the results are summarized using two standard metrics from the information retrieval literature @xcite :    * ( map ) ean verage recision , which is identical to the area under the precision - recall curve .",
    "the map score is computed over a set of test subgraphs @xmath1 as follows : @xmath105 above , @xmath106 is the set of all possible @xmath107 pairs , and the precision at @xmath108 is defined as @xmath109 * ( auc - roc ) area under the roc curve , which is identical to the mean average true negative rate .",
    "this score is computed as follows : @xmath110 where the true negative rate at @xmath108 is defined as @xmath111      the results of our experiments are shown in figure  [ fig : results ] .",
    "all differences in this figure are significant at the 0.001 level according to a paired t - test .",
    "as can be seen , selecting formulas with resolwe  before training weights leads to significant improvements in both domains according to both metrics . because the auc - roc performance of a random predictor would be 0.5 , we can see that , in fact , by using resolwe , we can go from near - random performance , to significantly higher accuracy levels .",
    "moreover , resolwe  learns significantly faster than skipselection .",
    "table  [ tbl : timing ] presents results for the number of minutes taken by resolwe  and by weight learning on dedicated xeon 2.67ghz cpus , averaged over the 4 folds in each domain . in both cases , using resolwe  leads to dramatic decrease in training time .",
    "we note that our results in the delicious domain are not comparable to those of zhou et al .",
    "@xcite because their system uses global computations over all available data to arrive at predictions , whereas here we focus on making predictions using information local to subgraphs of the original relational graph .",
    "[ cols=\"^,^,^ \" , ]",
    "structure learning and feature selection are important problems that have been widely studied in both relational and i.i.d . settings . most feature selection approaches , e.g. , @xcite ,",
    "have been developed for non - streaming classification settings .",
    "one recent exception is the work of wu et al .",
    "@xcite , who study a classification task where the features arrive in a stream , while the data set is fixed .",
    "in contrast , here we explore the setting where the pool of features is fixed , but the data arrives as a stream .    closely related to this paper",
    "is work on structure learning of statistical relational models @xcite .",
    "this literature has made important advances on focusing the search through the super - exponential space of candidate models , thus discovering more accurate candidates faster .",
    "less emphasis has been placed on how to evaluate candidate structures and , in most existing work , evaluation has been carried out by computing a probabilistic score over candidate structures that , crucially , assumes that the training data is available in memory .",
    "in contrast , this paper addresses the complementary setting common in many relational applications where sufficient background knowledge is available to generate candidate structures , and the challenge is in how to efficiently evaluate them on data that is presented to the learner in a stream .",
    "the set - up explored here is probably most similar to that assumed by huynh and mooney @xcite , where formula selection and parameter training are carried out in two separate stages .",
    "however , while that previous work also employs an accuracy - based measure ( that of aleph @xcite ) to evaluate rule candidates , it does not address the task of evaluating candidates that have more than a single literal of the target predicate and does not consider streaming the relational instances .",
    "a few authors have addressed learning of structure from data streams .",
    "dries and de raedt @xcite introduced an inductive logic programming technique that uses candidate elimination to learn theories from a stream of examples .",
    "their work applies to noise - free data .",
    "recently , kummerfeld and danks @xcite introduced a `` temporal difference structure learning '' algorithm that learns causal structure from a data stream .",
    "this algorithm targets causal discovery in graphical models and is not applicable to the relational setting assumed here .",
    "learning from data streams in relational settings has so far focused on training the parameters of a model for which structure is provided ( as done in skipselection , described in section  [ sect : experiments ] ) .",
    "this approach was adopted by mihalkova and mooney @xcite and in upcoming work by huynh and mooney @xcite .",
    "we proposed an approach to streamlining application development in relational domains by efficiently evaluating a set of candidate formulas on relational instances that are streamed one at a time .",
    "the evaluation algorithm is derived from two natural criteria and efficiency is achieved by exploiting the fact that typical relational domains are sparse .",
    "we fleshed out our approach to develop two applications in large and noisy social media tasks , demonstrating significant gains in the speed and accuracy of learning .",
    "avenues for future work include adapting this approach to tackle domains that experience gradual concept drift . one way to do this is to interleave steps 2 and 3 outlined at the beginning of section  [ sect : resolve ] and use a decaying average of the statistics calculated by algorithm  [ algo : resolve ] .",
    "as soon as step 2 determines a change in the structure over which weights are learned in step 3 , the change is implemented , keeping the weights of the remaining rules at their currently learned values , and the process continues .",
    "a second potential direction for future work is exploiting other ways in which relational data may be streamed to the learner .",
    "for example , one interesting setting arises when the learner is allowed to actively decide in what ways and how much to grow the subgraphs @xmath82 around entities of interest @xmath83 .",
    "we would like to thank tom chao zhou for providing us with the delicious data set .",
    "l. mihalkova is supported by a ci fellowship under nsf grant # 0937060 to the computing research association .",
    "biba , m. , ferilli , s. , esposito , f. : discriminative structure learning of markov logic networks . in : proceedings of the 18th international conference on inductive logic programming ( ilp-08 ) .",
    "prague , czech republic ( 2008 )                          khosravi , h. , schulte , o. , man , t. , xu , x. , bina , b. : structure learning for markov logic networks with many descriptive attributes . in : proceedings of the 24th conference on artificial intelligence ( aaai-10 ) ( 2010 )",
    "kok , s. , singla , p. , richardson , m. , domingos , p. : the alchemy system for statistical relational ai .",
    ", department of computer science and engineering , university of washington ( 2005 ) , http://www.cs.washington.edu/ai/alchemy      lowd , d. , domingos , p. : efficient weight learning for markov logic networks . in : proceedings of the 11th european conference on principles and practice of knowledge discovery in databases ( pkdd-07 ) ( 2007 )        mihalkova , l. , mooney , r.j . : learning to disambiguate search queries from short sessions . in : proceedings of the european conference on machine learning and principles and practice of knowledge discovery in databases ( ecml / pkdd-09 ) ( 2009 )    panciera , k. , halfaker , a. , terveen , l. :",
    "wikipedians are born , not made : a study of power editors on wikipedia . in : proceedings of the acm 2009 international conference on supporting group work ( group ) ( 2009 )"
  ],
  "abstract_text": [
    "<S> statistical relational learning techniques have been successfully applied in a wide range of relational domains . in most of these applications </S>",
    "<S> , the human designers capitalized on their background knowledge by following a trial - and - error trajectory , where relational features are manually defined by a human engineer , parameters are learned for those features on the training data , the resulting model is validated , and the cycle repeats as the engineer adjusts the set of features . </S>",
    "<S> this paper seeks to streamline application development in large relational domains by introducing a light - weight approach that efficiently evaluates relational features on pieces of the relational graph that are streamed to it one at a time . </S>",
    "<S> we evaluate our approach on two social media tasks and demonstrate that it leads to more accurate models that are learned faster . </S>"
  ]
}