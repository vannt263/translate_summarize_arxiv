{
  "article_text": [
    "nmf is a matrix approximation technique that factorizes a nonnegative matrix into a pair of other nonnegative matrices of much lower rank : @xmath0 where @xmath1 $ ] denotes the feature - by - item data matrix , @xmath2 $ ] denotes the basis matrix , @xmath3 $ ] denotes the coefficient matrix , and @xmath4 denotes the number of factors which usually chosen so that @xmath5 .",
    "there are also other variants of nmf like semi - nmf , convex nmf , and symmetric nmf .",
    "detailed discussions can be found in , e.g. , @xcite and @xcite .",
    "the nonnegativity constraints and the reduced dimensionality define the uniqueness and power of nmf .",
    "the nonnegativity constraints allow only nonsubstractive linear combinations of the basis vectors @xmath6 to construct the data vectors @xmath7 , thus providing the parts - based interpretations as shown in @xcite .",
    "and the reduced dimensionality provides nmf with the clustering aspect and data compression capabilities .",
    "the most important nmf s application is in the data clustering , as some works have shown that it is a superior method compared to the standard clustering methods like spectral methods and @xmath4-means algorithm . in particular , xu et al .",
    "@xcite showed that nmf outperforms standard spectral methods in finding the document clustering in two text corpora , tdt2 and reuters . and",
    "kim et al .",
    "@xcite showed that nmf and sparse nmf are much more superior methods compared to the @xmath4-means algorithm in both a synthetic dataset ( which is well separated ) and a real dataset ( tdt2 ) .",
    "if sparsity constraints are imposed to columns of @xmath8 , the clustering aspect of nmf is intuitive since in the extreme case where there is only one nonzero entry per column , nmf will be equivalent to the @xmath4-means algorithm employed to the data vectors @xmath7 @xcite , and the sparsity constraints can be thought as the relaxation to the strict orthogonality constraints on rows of @xmath8 ( an equivalent explanation can also be stated for imposing sparsity on rows of @xmath9 ) .",
    "however , as reported by xu et al .",
    "@xcite and kim et al .",
    "@xcite , even without imposing sparsity constraints , nmf still can give very promising clustering results .",
    "but the authors did nt give any theoretical analysis on why the standard nmf ",
    "nmf without sparsity nor orthogonality constraint",
    " can give such good results .",
    "so far the best explanation for this remarkable fact is only qualitative : the standard nmf produces non - orthogonal latent semantic directions ( the basis vectors ) that are more likely to correspond to each of the clusters than those produced by the spectral methods , thus the clustering induced from the latent semantic directions of the standard nmf are better than clustering by the spectral methods @xcite .",
    "therefore , this work attempts to provide a theoretical support for the clustering aspect of the standard nmf .",
    "to compute @xmath9 and @xmath8 , usually eq .",
    "[ eq1 ] is rewritten into a minimization problem in the frobenius norm criterion .",
    "@xmath10 in addition to the usual frobenius norm criterion , the family of bregman divergences  which frobenius norm and kullback - leibler divergence are part of it  can also be used as the affinity measures .",
    "detailed discussion on the bregman divergences for nmf can be found in @xcite .",
    "sometimes it is more practical and intuitive to decompose @xmath11 into a series of smaller objectives . @xmath12 . \\label{eq010}\\end{aligned}\\ ] ]    minimizing @xmath13 is known to be the nonnegative least square ( nls ) problem , and some fast nmf algorithms are developed based on solving the nls subproblems , e.g. , alternating nls with block principal pivoting algorithm @xcite , active set method @xcite , and projected quasi - newton algorithm @xcite .",
    "decomposing nmf problem into nls subproblems also transforms the non - convex optimization in eq .",
    "[ eq000 ] to the convex optimization subproblems in eq .",
    "[ eq010 ] .",
    "even though eq .",
    "[ eq010 ] is not strictly convex , for two - block case , any limit point of the sequence \\{@xmath14,@xmath15 } , where @xmath16 is the updating step , is a stationary point @xcite .",
    "the objective in eq .",
    "[ eq010 ] aims to simultaneously find the suitable basis vectors such that the latent factors are revealed , and the coefficient vector @xmath17 such that a linear combination of the basis vectors ( @xmath18 ) is close to @xmath7 . in clustering term",
    "this can be rephrased as : to simultaneously find the cluster centers and the cluster assignments .    to investigate the clustering aspect of nmf ,",
    "four possibilities of nmf settings are discussed : ( 1 ) imposing orthogonality constraints on both rows of @xmath8 and columns of @xmath9 , ( 2 ) imposing orthogonality constraints on rows of @xmath8 , ( 3 ) imposing orthogonality constraints on columns of @xmath9 , and ( 4 ) no orthogonality constraint is imposed . the last case is the standard nmf which its clustering aspect is the focus of this paper as many works reported that it is a very effective clustering method .",
    "the following theorems proves that imposing column - orthogonality constraints on @xmath9 and row - orthogonality constraints on @xmath8 lead to the simultaneous clustering of similar items and related features .",
    "[ theorem1 ] minimizing the following objective @xmath19 is equivalent to applying ratio association to @xmath20 and @xmath21 , where @xmath22 and @xmath23 are the item affinity matrix and the feature affinity matrix respectively , thus leads to simultaneous clustering of similar items and related features .",
    "@xmath24    the lagrangian function : @xmath25 where @xmath26 , @xmath27 , @xmath28 , and @xmath29 are the lagrange multipliers . by the karush - kuhn - tucker ( kkt ) optimality conditions we get : @xmath30 with complementary slackness : @xmath31 where @xmath32 denotes component - wise multiplications .",
    "assume @xmath33 , @xmath34 , @xmath35 , and @xmath34 ( at the stationary point these assumptions are reasonable since the complementary slackness conditions hold and the lagrange multipliers can be assigned to zeros ) , we get : @xmath36 substituting eq .",
    "[ eqee ] into eq .",
    "[ eqc ] , we get : @xmath37 similarly , substituting eq .",
    "[ eqff ] into eq .",
    "[ eqc ] , we get : @xmath38 therefore , minimizing @xmath39 is equivalent to simultaneously optimizing : @xmath40 eq .",
    "[ eqg ] and eq.[eq2s4 ] are the ratio association objectives ( see @xcite for details on various graph cuts objectives ) applied to @xmath20 and @xmath41 respectively . thus minimizing @xmath39",
    "leads to the simultaneous clustering of similar items and related features .      when the orthogonality constraints are imposed only on rows of @xmath8 , it is no longer clear whether columns of @xmath9 will lead to the feature clustering .",
    "the following theorem shows that without imposing the orthogonality constraints on @xmath6 , the resulting @xmath9 can still lead to the feature clustering .",
    "[ theorem2 ] minimizing the following objective @xmath42 is equivalent to applying ratio association to @xmath20 , and also leads to the feature clustering indicator matrix @xmath9 which is approximately column - orthogonal .",
    "@xmath43    the lagrangian function : @xmath44 by applying the kkt conditions , we get : @xmath45 by substituting eq .",
    "[ eq8a ] and eq .",
    "[ eq9a ] into eq .",
    "[ eq6a ] , minimizing @xmath46 is equivalent to simultaneously optimizing : @xmath47 note that the step in eq .",
    "[ eq12a ] is justifiable since @xmath48 is a constant matrix . by using the fact @xmath49 , eq .",
    "[ eq12a ] can be rewritten as : @xmath50    the objective in eq .",
    "[ eq10a ] is equivalent to eq .",
    "[ eqg ] and eventually leads to the clustering of similar items .",
    "so the remaining problem is how to prove that optimizing eq .",
    "[ eq11a ] and [ eq13abc ] simultaneously will lead to the feature clustering indicator matrix @xmath9 which is approximately column - orthogonal .",
    "[ eq11a ] resembles eq .",
    "[ eqe ] , but without orthogonality nor upper bound constraint , so one can easily optimizing eq .  [ eq11a ] by setting @xmath9 to an infinity matrix .",
    "however , this violates eq .",
    "[ eq13abc ] which favors small @xmath9 .",
    "conversely , one can optimizing eq .  [ eq13abc ] by setting @xmath9 to a null matrix , but again this violates eq .",
    "[ eq11a ] .",
    "therefore , these two objectives create implicit lower and upper bound constraints on @xmath9 , and eq .  [ eq11a ] and eq .",
    "[ eq13abc ] can be rewritten into : @xmath51 where @xmath52 denotes the feature affinity matrix and @xmath53 denotes the upperbound constraints on @xmath9 .",
    "now we have box - constraint objectives which are known to behave well and are guaranteed to converge to the stationary point @xcite .    even though the objectives are now transformed into box - constraint optimization problems , since there is no column - orthogonality constraint , maximizing eq .",
    "[ eq13aa ] can be easily done by setting each entry of @xmath9 to the corresponding largest possible value ( in graph term this means to only create one partition on @xmath54 ) . but",
    "this scenario results in the maximum value of eq .",
    "[ eq13ab ] , which violates the objective .",
    "conversely , minimizing eq .",
    "[ eq13ab ] to the smallest possible value ( minimizing @xmath55 implies minimizing @xmath56 , but not vice versa ) violates eq .",
    "[ eq13aa ] .",
    "thus , the most reasonable scenario is : setting @xmath56 as small as possible and balancing @xmath55 with eq .",
    "[ eq13aa ] .",
    "this scenario is the relaxed ratio association applied to @xmath54 , and as long as vertices of @xmath54 are clustered , simultaneous optimizing eq .",
    "[ eq13aa ] and eq .",
    "[ eq13ab ] leads to the clustering of related features .",
    "moreover , as @xmath56 is minimum , @xmath9 is approximately column - orthogonal .",
    "[ theorem3 ] minimizing the following objective @xmath57 is equivalent to applying ratio association to @xmath21 , and also leads to the item clustering indicator matrix @xmath8 which is approximately row - orthogonal .    by following the proof of theorem [ theorem2 ] , minimizing @xmath58",
    "is equivalent to simultaneously optimizing : @xmath59    eq .  [ eq14a ] is equivalent to eq .",
    "[ eq2s4 ] and leads to the clustering of related features . and",
    "optimizing eq .",
    "[ eq14b ] and eq .",
    "[ eq14c ] simultaneously is equivalent to : @xmath60 where @xmath61 denotes the item affinity matrix , @xmath62 denotes the @xmath63-th row of @xmath8 , and @xmath64 denotes the upperbound constraints on @xmath8 .",
    "as in the proof of theorem [ theorem2 ] , the most reasonable scenario in simultaneously optimizing eq .",
    "[ eq14aa ] and eq .",
    "[ eq14ab ] is by setting @xmath65 as small as possible and balancing @xmath66 with eq .",
    "[ eq14aa ] .",
    "this leads to the clustering of similar items , and as @xmath65 is minimum , @xmath8 is approximately row - orthogonal .      in this section",
    "we prove that applying the standard nmf to the feature - by - item data matrix eventually leads to the simultaneous feature and item clustering .",
    "[ theorem4 ] minimizing the following objective @xmath67 leads to the feature clustering indicator matrix @xmath9 and the item clustering indicator matrix @xmath8 which are approximately column- and row - orthogonal respectively .    by following",
    "the proof of theorem [ theorem2 ] , minimizing @xmath68 is equivalent to simultaneously optimizing : @xmath69 by substituting @xmath70 and @xmath71 into the above equations , we get : @xmath72 for feature clustering , and : @xmath73 for item clustering .",
    "therefore , minimizing @xmath68 is equivalent to simultaneously optimizing : @xmath74 which will lead to the feature clustering indicator matrix @xmath9 and the item clustering indicator matrix @xmath8 that are approximately column- and row - orthogonal respectively .",
    "the affinity matrix @xmath75 induced from a unipartite ( undirected ) graph is a symmetric matrix , which is a special case of the rectangular affinity matrix @xmath48 . therefore , by following the discussion in section [ clusteringnmf ] , it can be shown that the standard nmf applied to @xmath75 leads to the clustering indicator matrix which is almost orthogonal .",
    "the affinity matrix @xmath76 induced from a directed graph is an asymmetric square matrix .",
    "since columns and rows of @xmath76 correspond to the same set of vertices with the same order , as the clustering problem is concerned , @xmath76 can be replaced by @xmath77 which is a symmetric matrix .",
    "then the standard nmf can be applied to this matrix to get the clustering indicator matrix which is almost orthogonal .",
    "ding et al .",
    "@xcite provides the theoretical analysis on the equivalences between orthogonal nmf to @xmath4-means clustering for both rectangular data matrices and symmetric matrices .",
    "however as their proofs utilize the zero gradient conditions , the hidden assumptions ( setting the lagrange multipliers to zeros ) are not revealed there .",
    "actually it can be easily shown that their approach is the kkt conditions applied to the unconstrained version of eq .",
    "thus there is no guarantee that minimizing eq .",
    "[ eq2 ] by using the zero gradient conditions leads to the stationary point located on the nonnegative orthant as required by the objective .",
    "applying the standard nmf to the symmetric matrix leads to almost orthogonal matrix was previously proven by ding et al .",
    "but due to the used approach , the theorem can not be extended to the rectangular matrices which so far are the usual form of the data ( practical applications of nmf seemed exclusively for rectangular matrices ) .",
    "therefore , their results can not be used to explain the abundant experimental results that show the power of the standard nmf in clustering , latent factors identification , learning the parts of objects , and producing sparse matrices even without explicit sparsity constraint @xcite .",
    "by using the strict kkt optimality conditions , we showed that even without explicitly imposing orthogonality nor sparsity constraint nmf produces approximately column - orthogonal basis matrix and row - orthogonal coefficient matrix which lead to the simultaneous feature and item clustering .",
    "this result , therefore , gives the theoretical explanation on some experimental results that show the power of the standard nmf as a clustering tool which are reported to be better than the spectral methods @xcite and @xmath4-means algorithm @xcite .",
    "h.  kim and h.  park , `` nonnegative matrix factorization based on alternating nonnegativity constrained least squares and active set method , '' siam .",
    "j. matrix anal . & appl .",
    "30(2 ) , pp .  713 - 30 , 2008 .",
    "i.  s.  dhillon , y.  guan , and b.  kulis , `` weighted graph cuts without eigenvectors : a multilevel approach , '' ieee transactions on pattern analysis and machine intelligence , vol .",
    "29 , no .  11 , pp .  1944 - 57 , 2007"
  ],
  "abstract_text": [
    "<S> this paper provides a theoretical explanation on the clustering aspect of nonnegative matrix factorization ( nmf ) . we prove that even without imposing orthogonality nor sparsity constraint on the basis and/or coefficient matrix , nmf still can give clustering results , thus providing a theoretical support for many works , e.g. , xu et al .  </S>",
    "<S> @xcite and kim et al .  </S>",
    "<S> @xcite , that show the superiority of the standard nmf as a clustering method . </S>",
    "<S> +    _ bound - constrained optimization , clustering method , non - convex optimization , nonnegative matrix factorization _ </S>"
  ]
}