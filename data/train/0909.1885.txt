{
  "article_text": [
    "last years have witnessed a surge of research interests in the inverse ising problem @xcite , also known as boltzmann machine learning in statistical inference theory @xcite .",
    "this is due to on one hand the fact that a huge amount of data can be collected from many biological systems such as neural networks , gene regulatory networks and metabolic networks @xcite , on the other hand to the growing need for novel and efficient algorithms to reconstruct the network based on the huge amount of experimental data .",
    "individual elements ( e.g. , neurons , genes , even computers in the internet ) in the network usually interact with each other to yield the collective behavior emerging at the network level",
    ". to extract functional connectivity from the collective behavior of the network , we use @xmath0 to represent the activity ( e.g. , electric activity of single neuron , expression level of single gene ) of each element in a network with size @xmath1 , then the likelihood of each state @xmath2 is assumed to be @xmath3 $ ] , also named the second - order maximum entropy model studied in refs .",
    "@xmath4 serve as lagrange multipliers corresponding to the constraints given by @xmath5 where @xmath6 is the magnetization and @xmath7 two - point connected correlation between sites @xmath8 and @xmath9 in the statistical physics language . from the experimental data ( e.g. , microarray data or multi electrode recordings ) , one can measure both the mean activity ( @xmath6 ) of each individual and pairwise correlations ( @xmath7 ) among them .",
    "the inverse ising problem is to infer the underlying parameters @xmath4 from the knowledge of measured @xmath5 , such that the resulting ising distribution is able to provide an accurate description of the statistics of the experimental data , i.e. , @xmath10 .",
    "the pairwise ising model has been extensively studied as an inverse ising problem on retinal networks  @xcite , cortical networks  @xcite and gene regulatory networks  @xcite . in ref .",
    "@xcite , it was observed that the maximum entropy principle can be used to extract information about gene interactions and the result reproduces the observed transcript profiles with high fidelity .",
    "et al _ also showed that the pairwise ising model captures @xmath11 of the correlation structure of the retinal network activity . on top of the existing spatial correlations among neurons , the temporal dependencies",
    "were also suggested to be a common feature of cortical networks  @xcite .",
    "recently , marre _ et al _ employed the same maximum entropy principle with a markovian assumption to predict the occurrence probabilities of spatiotemporal patterns and the result is significantly better than that obtained by ising models only considering spatial correlations . in aspects of theoretical analysis ,",
    "et al _  @xcite have investigated the dependence of the fit quality of pairwise model upon the time bin size as well as the system size and found that the pairwise model always provides an accurate statistical description of spikes as long as the system size does not exceed the critical size determined by the mean population firing rate and the bin size . from the algorithmic perspective , broderick _",
    "@xcite combined a coordinate descent algorithm with an adaption of the histogram monte carlo method to solve the inverse problem efficiently up to @xmath12 neurons .",
    "subsequently , mzard and mora introduced the message passing ideas to the inverse ising problem and proposed the susceptibility propagation as a comprehensive network reconstruction algorithm for sparse network or the network with sufficiently weak interactions  @xcite .",
    "the susceptibility propagation together with sessak - monasson approximation ( sm ) recently put forward in ref .",
    "@xcite , has been tested in sherrington - kirkpatrick model  @xcite , and it was found that the message passing based method as well as sm outperforms other existing mean field schemes and sm was shown to be more efficient .",
    "the message passing technique also found application in the inference of gene regulatory networks , and a statistical mechanics analysis has been presented in ref .",
    "et al _ in a recent work  @xcite studied the inverse problem on a simulated network of spiking neurons and it was observed that as the network size increases , sm and the inversion of thouless - anderson - palmer ( tap ) equations outperform other mean field type algorithms to predict the network structure yet the fit quality degrades as the system size grows .    in this paper",
    "we use the same four mean field schemes employed in ref .",
    "@xcite to test their performances on both the fully - connected and finite connectivity hopfield network and investigate the behavior of these inference algorithms with respect to the system size , the memory load and particularly different temperatures .",
    "it is shown that the paramagnetic phase helps to extract the information about couplings in the network , although the recall phase is in turn useful during the retrieval process of one of embedded patterns , and in this case the system is prevented from entering the paramagnetic or spin glass phase .",
    "the naive mean field method ( nmf ) and tap exhibit very good performances while independent - pair approximation ( ind ) shows a relatively high inference error . in the paramagnetic phase",
    ", sm leads to the nearly identical performances with nmf and tap , and their performances deteriorate with increasing network size . given the fixed system size , all the algorithms show increasing inference errors as the memory load increases . in the low temperature region ,",
    "sm shows relatively high inference errors and is even inferior to ind for certain memory loads .",
    "the sample - to - sample fluctuations are also taken into account , and we found the inference error shows large fluctuations as the temperature decreases .",
    "the finite connectivity hopfield network is also analyzed , as expected , ind performs well especially for the small number of stored patterns or at low temperature .",
    "in addition , when the temperature decreases , the reconstruction algorithms show similar behaviors as observed in the fully - connected network .",
    "the remainder of this paper is organized as follows .",
    "the hopfield model and glauber dynamics ( gd ) are introduced in sec .",
    "[ sec_hopf ] . in sec .",
    "[ sec_mfs ] , four mean field schemes , nmf , ind , sm and tap , are presented briefly .",
    "the reconstruction performances of these algorithms are reported in sec .",
    "[ sec_result ] for the fully - connected network and the finite connectivity network respectively .",
    "we conclude with our results and future perspectives in sec .",
    "[ sec_con ] .",
    "the hopfield network , proposed in ref .",
    "@xcite , later thoroughly discussed in refs .",
    "@xcite , functions as an associative memory network .",
    "it is in essence a recurrent network , and its equilibrium properties are determined by the following hamiltonian : @xmath13 where @xmath14 represents the state of each neuron in the network .",
    "@xmath15 indicates the neuron @xmath8 generates a spike while @xmath16 keeps quiescent .",
    "interactions between neurons are constructed according to the hebb s rule , @xmath17 where @xmath18 taking @xmath19 with equal probability @xmath20 are @xmath21 stored patterns .",
    "the number of patterns @xmath21 scales as @xmath22 in the fully - connected case .",
    "the hebb s rule expresses the multiplicative interaction between presynaptic and postsynaptic activity and positively correlation ( both neurons are on or off ) causes an enhanced coupling while negatively correlation results in a decreased one . under the hebb s rule , the hamiltonian equation  ( [ hop - h ] )",
    "can be actually rewritten in terms of the overlap between the network configuration and the stored patterns .",
    "this guarantees that the energy function @xmath23 always decreases while the system evolves according to the following gd rule .",
    "the fully - connected hopfield network requires complete and symmetric connectivity , also no self - interactions .",
    "mean field behavior of finite connectivity hopfield network has been recently studied in refs .",
    "the difference is that in the sparse network , the coupling @xmath24 is constructed as follows , @xmath25 where @xmath26 is the mean degree of each neuron . when @xmath27 , @xmath28 .",
    "we also assume that no self - interactions are present in the finite connectivity hopfield model and the connectivity @xmath29 is symmetric and subject to the distribution @xmath30    taking the thermal fluctuation into account , the gd rule  @xcite is specified as @xmath31 where @xmath32 stays for the energy change due to such a flip .",
    "equivalently , the dynamics rule can be recast into  @xcite , @xmath33\\ ] ] where the inverse temperature @xmath34 serves as a measure of degree of stochasticity and @xmath35 is the local field acting on @xmath14 . under this dynamics rule ,",
    "if the initial configuration is close enough to one of embedded patterns , it will tend to evolve towards the nearest attractor represented by one single pattern in the configuration space .",
    "that is also the meaning of associative memory .    in this work",
    ", we use gd to detect the equilibrium properties of hopfield networks , then the numerical simulation data is used to reconstruct the network .",
    "we shall keep to the small size system up to @xmath36 due to the computational cost . in the thermodynamic limit , the phase diagram of fully - connected hopfield network has been studied in refs .",
    "@xcite while that of sparse network in refs .",
    "@xcite . in the numerical simulation , two types gd will be implemented .",
    "both types are run in a randomly asynchronous manner .",
    "the type @xmath37 is executed as follows .",
    "we do the standard gd as demonstrated in equation  ( [ gdrule ] ) totally @xmath38 steps ( each step corresponds to the process where the state of each neuron is updated on average one time ) , among which the first @xmath39 steps are run for the system to reach the equilibrium state and the other @xmath39 steps for calculating magnetizations and correlations .",
    "we sample the state of the network every @xmath40 steps . to get around the difficulty that the system is apt to get stuck in local minima of the free energy landscape at low temperature , the simulated annealing technique  @xcite",
    "is introduced in type @xmath41 gd where we set the initial temperature to be @xmath42 and the cooling rate @xmath43 . at each intermediate temperature",
    ", we run gd @xmath44 steps . when the temperature is decreased to the desired one",
    ", we run another @xmath39 steps to sample the system",
    ". the smaller cooling rate and larger steps run at each intermediate temperature are favored but the corresponding computational cost increases .",
    "boltzmann machine learning works also as a network reconstruction algorithm  @xcite .",
    "it adjusts couplings at each iteration step according to the difference between measured correlation and that produced by the prescribed ising distribution .",
    "the algorithm is iterated until the difference falls within the desired accuracy .",
    "the boltzmann machine learning is exact but also computationally expensive since a large amount of monte carlo sampling steps are required . in this section , we will briefly introduce for the hopfield network reconstruction four fast mean field approximations presented in ref .",
    "@xcite .",
    "the naive mean field theory indicates that @xmath45 where @xmath46 . to calculate the connected correlation @xmath47",
    ", we use the fluctuation - response relation , @xmath48\\ ] ] then obtain the nmf prediction of couplings , @xmath49 where @xmath50 .",
    "note that the predicted @xmath24 here has been multiplied by @xmath34 , therefore the actual predicted one @xmath51 .",
    "this approximation assumes that each pair of neurons are independent of the rest part of the system , i.e. , their joint probability @xmath52 $ ] where @xmath53 is the local field neuron @xmath54 feels when neuron @xmath55 is removed from the system .",
    "then the ind prediction is given by @xmath56\\ ] ] where @xmath57 .",
    "in a recent work by sessak and monasson  @xcite , the sm prediction of couplings is derived using a systematic small correlation expansion , @xmath58 where @xmath59 is given by equation  ( [ ind ] ) and @xmath60_{ij}$ ] where @xmath61 is an identity matrix , @xmath62 and @xmath63 .",
    "the usual tap equation reads @xmath64  @xcite .",
    "taking the derivative of the field @xmath65 with respect to the magnetization @xmath66 , one readily obtains the tap prediction equation , @xmath67 which has been introduced by kappen and rodriguez  @xcite and tanaka  @xcite .    to measure the reconstruction performance of these algorithms",
    ", we define the inference error as @xmath68^{1/2}\\ ] ] where @xmath69 is the prediction value of the coupling based on the above four reconstruction algorithms and @xmath70 the original coupling constructed through the hebb s rule equation  ( [ hop - j ] ) or equation  ( [ hop - j - sparse ] ) .",
    "obviously , the smaller @xmath71 is , the more precisely the pairwise ising model reproduces the statistics of the experimental data .    .",
    "results from two samples are shown respectively .",
    "the full line indicates equality and the network is fully - connected .",
    "( a ) nmf , ( b ) ind , ( c ) sm and ( d ) tap . ,",
    "title=\"fig:\",width=321 ] .5 cm .",
    "results from two samples are shown respectively .",
    "the full line indicates equality and the network is fully - connected .",
    "( a ) nmf , ( b ) ind , ( c ) sm and ( d ) tap . ,",
    "title=\"fig:\",width=321 ] .2 cm .",
    "results from two samples are shown respectively .",
    "the full line indicates equality and the network is fully - connected .",
    "( a ) nmf , ( b ) ind , ( c ) sm and ( d ) tap . ,",
    "title=\"fig:\",width=321 ] .5 cm .",
    "results from two samples are shown respectively .",
    "the full line indicates equality and the network is fully - connected .",
    "( a ) nmf , ( b ) ind , ( c ) sm and ( d ) tap . , title=\"fig:\",width=321].2 cm     samples . ( a ) the inference error against the system size for the paramagnetic phase @xmath72 .",
    "results are obtained based on type @xmath37 gd .",
    "the inset shows an enlarged view of performances for nmf , sm as well as tap .",
    "( b ) the inference error against the memory load with @xmath73 .",
    "results are obtained based on type @xmath41 gd .",
    "( c ) the inference error against the memory load with @xmath74 .",
    "results are obtained based on type @xmath37 gd .",
    "the inset shows an enlarged view of performances for nmf , sm and tap .",
    "( d ) the inference error versus temperature for @xmath75 . for the low temperature regime , type b gd is used to collect data .",
    ", title=\"fig:\",width=321 ] .5 cm   samples .",
    "( a ) the inference error against the system size for the paramagnetic phase @xmath72 .",
    "results are obtained based on type @xmath37 gd .",
    "the inset shows an enlarged view of performances for nmf , sm as well as tap .",
    "( b ) the inference error against the memory load with @xmath73 .",
    "results are obtained based on type @xmath41 gd .",
    "( c ) the inference error against the memory load with @xmath74 .",
    "results are obtained based on type @xmath37 gd .",
    "the inset shows an enlarged view of performances for nmf , sm and tap .",
    "( d ) the inference error versus temperature for @xmath75 . for the low temperature regime , type b gd",
    "is used to collect data .",
    ", title=\"fig:\",width=321 ] .2 cm   samples .",
    "( a ) the inference error against the system size for the paramagnetic phase @xmath72 .",
    "results are obtained based on type @xmath37 gd .",
    "the inset shows an enlarged view of performances for nmf , sm as well as tap .",
    "( b ) the inference error against the memory load with @xmath73 .",
    "results are obtained based on type @xmath41 gd .",
    "( c ) the inference error against the memory load with @xmath74 .",
    "results are obtained based on type @xmath37 gd .",
    "the inset shows an enlarged view of performances for nmf , sm and tap .",
    "( d ) the inference error versus temperature for @xmath75 . for the low temperature regime , type b gd",
    "is used to collect data . , title=\"fig:\",width=321 ] .5 cm   samples .",
    "( a ) the inference error against the system size for the paramagnetic phase @xmath72 .",
    "results are obtained based on type @xmath37 gd .",
    "the inset shows an enlarged view of performances for nmf , sm as well as tap .",
    "( b ) the inference error against the memory load with @xmath73 .",
    "results are obtained based on type @xmath41 gd .",
    "( c ) the inference error against the memory load with @xmath74 .",
    "results are obtained based on type @xmath37 gd .",
    "the inset shows an enlarged view of performances for nmf , sm and tap .",
    "( d ) the inference error versus temperature for @xmath75 . for the low temperature regime , type b gd is used to collect data .",
    ", title=\"fig:\",width=321].2 cm     samples . two ratios",
    "@xmath76 are considered with the same mean degree @xmath77 , system size @xmath78 .",
    "for the low temperature regime , type b gd is used to collect data . , width=321 ]",
    "we restrict our analysis to the small size system , although the phase diagram of the hopfield model was derived in the thermodynamic limit@xcite and the result for the small size system will be slightly different due to the finite size effects if @xmath1 is not very small , it is expected that the recall phase , paramagnetic or spin glass phase will still appear as @xmath79 and @xmath80 vary .",
    "they can be characterized by two order parameters , the overlap between the network configuration and the @xmath81-th pattern @xmath82 , and the mean squared magnetization @xmath83 .",
    "scatter plots comparing the inferred couplings with the true ones for the fully - connected network are presented in fig .",
    "[ sg_scatterplot ] .",
    "nmf , tap and sm give very good inferences of couplings while ind shows a relatively high error .",
    "the part below the line underestimates the true couplings and the part above overestimates the true ones . as shown in fig .",
    "[ sg_scatterplot ] , nmf and tap show a similar behavior of estimating the true couplings .",
    "they always underestimate the true negative couplings but overestimate the positive ones .",
    "however , sm behaves conversely .",
    "as expected , in this fully - connected network , the independent pair approximation fails to recover the couplings within a desired accuracy .",
    "the performances versus the system size are illustrated in fig .",
    "[ infer_fhop ] ( a ) .",
    "it turns out that the inference errors increase with the network size for nmf , sm and tap in the paramagnetic phase .",
    "additionally , sm performs better than nmf and tap .",
    "the bad predictor ind shows large fluctuations from sample to sample .",
    "[ infer_fhop ] ( b ) presents the reconstruction performance versus the memory load at the low temperature .",
    "sm behaves badly with large fluctuations , even inferior to ind for certain memory loads .",
    "however , it should be noted that when just one single pattern is stored , all mean field schemes perform relatively better and give @xmath84 .",
    "it is due to the fact that there exists only one attraction for the retrieval dynamics , therefore it is easy to detect the equilibrium state of the network which contains all the information about the couplings .",
    "the reconstruction performances against the memory load in the paramagnetic phase are shown in fig .",
    "[ infer_fhop ] ( c ) .",
    "as the memory load increases , the algorithms behave worse and their inference errors show large standard deviations . however , in the paramagnetic phase , all the algorithms but ind give very small inference errors especially at small memory loads . given @xmath80 and @xmath1 , we report the inference performance against temperature in fig .",
    "[ infer_fhop ] ( d ) . in our numerical simulations , we find that in the low temperature region , some approximations , e.g. , tap , fail due to the high magnetizations ( very close to @xmath85 or @xmath86 ) or extremely small correlations ( @xmath87 ) ) computed from gd . at the same time , the determinant of the correlation matrix @xmath88 is nearly equal to zero",
    ". therefore the algorithms are unable to extract the couplings .",
    "these results are not shown in fig .",
    "[ infer_fhop ] ( d ) .",
    "for instance , we do observe the final retrieval of the stored pattern in a simulation with @xmath89 , nevertheless , only nmf is successful but leads to a high inference error @xmath90 and all other methods fail . in this case , given many stored patterns , if the first pattern is assumed to be retrieved , i.e. , the overlap @xmath91 , then the energy function equation  ( [ hop - h ] ) can be decomposed into two terms as @xmath92 , and the second term is relevant thus @xmath93 where @xmath94 , which implies that in the recall phase , the information about couplings is lost and the local field contains only the information about the retrieved pattern .",
    "the same failure occurs also in the spin glass phase , for example , the best performance for one single sample is given by tap , @xmath95 with @xmath96 . as shown in fig .",
    "[ infer_fhop ] ( d ) , the inference performance becomes worse as the temperature decreases . with many embedded patterns",
    ", we will lose the information about couplings even if the retrieval successes , as explained above .",
    "on the other hand , the free energy landscape becomes complex at low temperatures and it is difficult to detect the equilibrium properties of the system , i.e. , the emergence of spurious minima will trap the glauber dynamics and it requires much longer gd steps to reduce the noise of estimates of the magnetizations and correlations which affects the inference results a lot .",
    "for the current system , ergodicity breaking sets in at very low temperature where the replica symmetric assumption was shown to be invalid  @xcite , then the magnetizations and correlations lose their physical meanings in that they are usually defined in a single state , therefore reconstruction algorithms capable of dealing with the case of multi - state are necessary .",
    "finally , it should be mentioned that in the low temperature region , all mean field schemes show large sample - to - sample fluctuations and their predictions are unreliable .",
    "inference performances of the reconstruction algorithms on the sparse hopfield network are also considered and shown in fig .",
    "[ infer_shop ] . at the low temperature ,",
    "some mean field schemes fail therefore the results are not presented .",
    "it should be noted that ind performs well to reconstruct the finite connectivity network , consistent with the fact that the independent pair approximation is reasonable in the sparse network . particularly in the low @xmath80 regime ind",
    "shows a very good performance , and at the low temperature , it even outperforms other existing mean field schemes although the inference errors are relatively large . in the high temperature region ,",
    "all mean field schemes perform better with low @xmath80 than high @xmath80 . at temperatures larger than @xmath42 ,",
    "sm does a better job than other algorithms , as also observed in the fully - connected case .",
    "when we decrease the temperature where the system stays , we are confronted with the failure of all mean field schemes as appears in the case of fully - connected network , however , when the algorithm works well , the predicted couplings between unconnected neurons are very small compared to those between neurons which are actually interconnected , therefore one can reconstruct the network using an appropriate cutoff .",
    "on the other hand , it shows clearly that the sample - to - sample fluctuations become larger and larger as the temperature decreases .",
    "in this work , we have tested performances of four fast mean field schemes for inferring couplings of hopfield networks .",
    "we find that nmf , sm and tap show better performances than ind in paramagnetic phase , and their performances degrade with the increasing network size and memory load .",
    "the inference error achieved by sm shows large sample - to - sample fluctuations and becomes inferior to ind in a certain range of memory load as the system is presented in the low temperature region . when there is one single pattern stored in the network , all mean field schemes are able to do a good job , whereas , as many patterns are embedded , the free energy landscape becomes complex and even if one of embedded patterns is found , the information about couplings of the network is inevitably lost and the algorithms fail to reconstruct the network .",
    "similarly , in the spin glass phase , due to the emerging spurious minima , it is difficult to detect the equilibrium states and the estimated magnetizations and correlations become unreliable . the message passing algorithms proposed in ref .",
    "@xcite may offer hints for improving the performance of network reconstruction .",
    "our work implies that as a direct problem , the recall phase is favored , while the unfavored paramagnetic phase is in turn most useful for the reconstruction of the network , which is particular for the hopfield network where couplings are constructed according to the hebb s rule and the stored patterns are in fact random and uncorrelated . for the sparse network ,",
    "when we decrease the temperature and this is necessary for the network to recall one of stored patterns , all reconstruction algorithms deteriorate and ind shows a relatively better performance . whether in the fully - connected case or in the sparse case , the performances achieved by the reconstruction algorithms show larger and larger sample - to - sample fluctuations with decreasing temperatures and the predictions become no more accurate .",
    "the analysis of different approximations on inferring couplings of hopfield network is kept to in our work , however , more analyses on the more realistic networks , such as neural networks  @xcite , gene regulatory networks  @xcite and other biological networks  @xcite , are urgently required .",
    "this subject of ongoing research will shed light not only on reasons why some algorithms fail to reconstruct the network but also on the further mechanisms we need to develop novel efficient network reconstruction algorithms for practical data analysis .",
    "the author is grateful to pan zhang and haijun zhou for valuable discussions .",
    "the present work was supported by the national science foundation of china ( grant no .",
    "10774150 ) and by the national basic research program ( 973-program ) of china ( grant no .",
    "2007cb935903 ) ."
  ],
  "abstract_text": [
    "<S> we test four fast mean field type algorithms on hopfield networks as an inverse ising problem . </S>",
    "<S> the equilibrium behavior of hopfield networks is simulated through glauber dynamics . in the low temperature regime , </S>",
    "<S> the simulated annealing technique is adopted . </S>",
    "<S> although performances of these network reconstruction algorithms on the simulated network of spiking neurons are extensively studied recently , the analysis of hopfield networks is lacking so far . </S>",
    "<S> for the hopfield network , we found that , in the retrieval phase favored when the network wants to memory one of stored patterns , all the reconstruction algorithms fail to extract interactions within a desired accuracy , and the same failure occurs in the spin glass phase where spurious minima show up , while in the paramagnetic phase , albeit unfavored during the retrieval dynamics , the algorithms work well to reconstruct the network itself . </S>",
    "<S> this implies that , as a inverse problem , the paramagnetic phase is conversely useful for reconstructing the network while the retrieval phase loses all the information about interactions in the network except for the case where only one pattern is stored . </S>",
    "<S> the performances of algorithms are studied with respect to the system size , memory load and temperature , sample - to - sample fluctuations are also considered . </S>"
  ]
}