{
  "article_text": [
    "to average @xmath70 in over the disorder , we use the replica trick in two separate instances .",
    "first , to represent the logarithm of the partition function in  , we use @xmath71 . the inverse of the partition function in boltzmann distribution   is represented with a second set of replicas based on @xmath72 taking the underlying couplings @xmath73 to be taken independently from a gaussian distribution with zero mean and variance @xmath29 the average of @xmath74 is @xmath75 where the replica indices @xmath76 and @xmath61 run from @xmath77 to @xmath78 and @xmath77 to @xmath79 respectively , and the limits @xmath80 and @xmath81 will be taken at the end of the calculation .",
    "the vector @xmath82 with elements @xmath83 is the first row of the matrix of inferred couplings .",
    "we have used a set of delta - functions to define the argument of the objective function , @xmath84 .",
    "partition functions of this form have been investigate extensively in the context of statistical learning  @xcite .",
    "the key difference here is that the samples @xmath85 are not generated from some ` teacher perceptron ' , but are taken from the boltzmann distribution  .",
    "the partition function   can be evaluated by standard techniques , except for the first step , the sum over the samples @xmath86 . picking out the terms involving the samples , the average factorises over the sample index @xmath87 ( which we drop in the following for convenience ) leaving @xmath88 the contribution from @xmath62 is @xmath89 where we have introduced the shorthand @xmath90 .",
    "expanding the exponent in a taylor series ( small couplings , _",
    "i.e. _ , small @xmath29 ) gives @xmath91 \\ .\\ ] ] the first - order term in this expansion sums to zero , to yield a non - zero result would require @xmath92 , which does not appear in the sum .",
    "for the second order expression , terms with @xmath93 , @xmath94 sum to @xmath95 , other terms either sum to zero or are smaller by a factor of @xmath96 . for higher - order terms , the dominant contributions come from terms where spin pairs are contracted in the same manner as in the second - order term .",
    "the dominant ( 2n)-th order term is @xmath97(\\sum_{i < j } g_{ij}^2)^n=\\frac{1}{n!2^nn^n } ( \\sum_{i < j } g_{ij}^2)^n $ ] , resumming the series to infinite order gives @xmath98 expanding the shorthand in this result we have @xmath99 an analogous calculation can be made for the terms with @xmath100 which gives @xmath101 in the limit @xmath102 this term cancels with the first term of  .",
    "note that the first term in   scales differently with @xmath0 from the remaining terms . in order to probe this result numerically at finite @xmath0 , we compute the averages on the left hand sides and for a single matrix numerically and compare the logarithm of their product ( so the first term in   cancels with  ) with the analytical result @xmath103 . for @xmath104 , figure [ fig_average ] compares the numerical average over @xmath105 samples with the analytical result .",
    "the remaining terms in   can be simplified by introducing the order parameters @xmath106 and @xmath107 via integrals over delta - functions .",
    "thus we obtain in the limit @xmath102 @xmath108 where the @xmath56-functions themselves were represented by integrals over the so - called conjugate order parameters @xmath109 and @xmath110 . in the next step ,",
    "we exploit that the integrals over couplings factorise over @xmath111 and those over variables @xmath112 and @xmath113 factorise over @xmath114 , giving @xmath115 with @xmath116 where @xmath117 denotes a gaussian measure with mean zero and variance @xmath41 . at this point",
    "we take a replica symmetric ansatz defined by @xmath118 which allows the evaluation of and the taking of the limit @xmath81 yielding @xmath119 \\right ] \\ .\\end{aligned}\\ ] ] the extremum over the conjugate order parameters @xmath120 can be evaluated easily yielding @xmath121 which gives @xmath122 \\right ] \\",
    ".\\end{aligned}\\ ] ] we are particularly interested in the low - temperature limit @xmath24 ; according to , this limits project out the couplings minimizing the objective function . in the low - temperature limit",
    "we find that the order parameters at the extremum   scale as @xmath123 , where @xmath36 and @xmath124 are of order one . with this scaling , the integral over @xmath39 in   can be evaluated by saddle - point integration . to",
    "leading order in @xmath125 we have @xmath126 =   -\\beta \\mbox{min}_{{k } } \\left [ \\frac{({k}-r-\\sqrt{q_0}t)^2}{2 v } + \\rho({k})\\right ] \\ .\\end{aligned}\\ ] ] the minimum over @xmath39 admits a simple interpretation : as a function of @xmath127 , this is a minimum of @xmath19 that is ` close ' to @xmath40 , where the trade - off between closeness and smallness is controlled by @xmath36 .",
    "this relationship plays a central role in convex optimisation  @xcite , where it is known as the moreau envelope @xmath128 of a function @xmath129 @xmath130(x ) = \\mbox{min}_y \\left [   \\frac{(y - x)^2}{2v}+f(y)\\right ] \\ .\\ ] ] with this we obtain @xmath131(r+\\sqrt{q}t ) \\right ] \\ .\\ ] ] setting the derivatives of this expression with respect to @xmath132 and @xmath42 to zero gives the three saddle - point equations @xmath133 where @xmath134 is value of @xmath39 attaining the minimum in . to derive these equations we used @xmath135(x)=\\frac{d\\rho}{d{k}}\\vert_{{k}={k}(x)}$ ] and @xmath136(x)=-\\frac{1}{2}(\\frac{d\\rho}{d{k}}\\vert_{{k}={k}(x)})^2 $ ] .",
    "we are interested in the particular objective function that when used to reconstruct couplings according to , yields reconstructed couplings that are closest to the underlying couplings .",
    "we use the relative mean - square error   to quantify the performance of a particular objective function .",
    "we thus seek the particular function @xmath19 which maximises @xmath137 , subject to the constraints specified by the saddle point equations .",
    "a similar calculation appears in the context of optimal regression  @xcite .",
    "we use lagrange multipliers and maximise @xmath138 + \\gamma_2 \\left [ \\frac{r}{q^{\\star}v}+ \\alpha \\int dt\\,\\rho ' \\right ] + \\gamma_3 \\left[\\frac{q - r^2/q^{\\star}}{v^2 } - \\alpha \\int dt\\ , ( \\rho')^2 \\right ] \\ , \\ ] ] where we use the shorthand @xmath139 . with @xmath140(x)$ ] we have @xmath141 , @xmath142 and @xmath143 we can write as @xmath144 with @xmath145 and @xmath146 a shorthand for a gaussian integral measure with mean @xmath42 and variance @xmath41 . to find the optimal @xmath19 we take the functional derivative of @xmath147 with respect to @xmath148 , solve the resulting euler - lagrange equation , and determine the corresponding @xmath19 by inverting the moreau envelope .",
    "the euler - lagrange equation @xmath149 gives @xmath150 where @xmath151 is a gaussian with mean @xmath42 and variance @xmath41 . inserting this result into ,",
    "gives @xmath152 the first term integrates to zero , the second involves @xmath153 which gives the lagrangian as @xmath154 extremization with respect to the lagrange parameters gives @xmath155 only the first two of these equations are required to evaluate , the third establishes a relationship between the overlaps @xmath42 and @xmath41 at the optimal objective function @xmath156 integrating now gives up to a constant @xmath157 ( x)=-\\frac{1}{\\alpha v } \\left ( \\frac{r}{q^{\\star } } x -\\frac{1}{2 } ( x - r)^2   \\right ) \\ , \\ ] ] from which the optimal objective function can be obtained easily based on the relation that for a convex function @xmath158 , @xmath159_v ( x)=g(x)$ ] implies @xmath160_v ( y)$ ] .",
    "inverting   gives again up to a constant @xmath161 the optimal value of @xmath42 is specified by extremizing the lagrangian  , giving @xmath162 . multiplying any objective function by a constant or adding a constant",
    "to it does not affect the reconstruction , so the optimal objective function can also be written as @xmath163 with @xmath55 .",
    "of course @xmath29 is not known when reconstructing the couplings .",
    "we set out considering @xmath56 in a free parameter , which needs to be determined .",
    "the free energy for this particular objective function is @xmath164\\ .",
    "\\end{aligned}\\ ] ] with saddle - point equations giving @xmath165 the last result is crucial as the overlap @xmath41 of reconstructed couplings can be computed without knowing the underlying couplings .",
    "@xmath29 can be determined from a simple linear fit of @xmath41 against @xmath59 , which determines the offset parameter in the optimal objective function @xmath166    there is a second way to determine the optimal value of @xmath56 , which does not require optimizing the objective function at different values of @xmath56 .",
    "if the variance of couplings @xmath167 of a spin glass model with gaussian couplings ( the sherrington - kirckpatrick model ) were fixed to be @xmath77 , @xmath29 would be the square of the inverse temperature .",
    "the task is thus to determine the temperature parameter at which an observed set of @xmath5 spin configurations were taken , not knowing the concrete realisation of the underlying couplings but only the distribution from which the couplings were taken .",
    "we sketch a simple way to do this , based on the mean square spin - spin correlation @xmath168 ^ 2\\ ] ] with @xmath169 .",
    "this quantity is closely related to the spin - glass susceptibility , and can be computed from a set of spin samples .",
    "( the spin - glass susceptibility includes also a contribution from the diagonal term with @xmath92 . ) at a given value of @xmath34 , can be computed easily from spin - samples generated at different values of @xmath29 .",
    "figure [ fig_chi ] shows @xmath170 , calculated numerically for @xmath171 , against @xmath29 , along with a fit to a 4th - order polynomial .",
    "given a set of samples for which we want to reconstruct the couplings , one can evaluate @xmath170 for these samples and read off the corresponding value of @xmath29 from figure [ fig_chi ] , or more specifically , solve the fitted polynomial for @xmath29 .",
    "we used this approach to determine @xmath29 in a single realisation of the couplings with @xmath68 and @xmath172 as in figure  [ fig1 ] and  [ fig2 ] .",
    "we produce @xmath173 samples , resulting in @xmath174 , for which the fit shown in figure  [ fig_chi ] gives an estimate for @xmath29 of @xmath175 .",
    "this yields a threshold of @xmath176 , compared to the optimal value of @xmath56 , which is approximately @xmath177 .",
    "how small the resulting difference in the reconstruction error is can be read off from figure  [ fig1 ] .",
    "this approach also works at even smaller values of @xmath61 . for @xmath178",
    "we observe in a single realisation of the couplings and a set of @xmath179 samples @xmath180 , for which the fit ( recomputed at the new value of @xmath61 ) gives @xmath181 and the estimated threshold of approximately @xmath182 , compared to the correct threshold of approximately @xmath183 .",
    "the resulting reconstruction error is @xmath184 , compared to the reconstruction error with the optimal value of @xmath56 of @xmath185 . at @xmath62",
    "the threshold reaches zero .",
    "the statistics of reconstructed couplings can be read off from the free energy   using standard arguments , giving the average fraction of couplings exceeding a threshold @xmath76 as @xmath186 for the low - temperature limit we use the scaling of the conjugate order parameters @xmath187 which turns the integrals over @xmath51 into saddle - point integrals with saddle - point equation @xmath188 since @xmath189 follows a gaussian distribution with mean zero and unit variance , this result means that the reconstructed coupling @xmath51 is on average @xmath49 ( specifying the bias ) and has a variance @xmath50 .",
    "similarly , the distribution of @xmath190 can be calculated , in the low - temperature limit their statistics is that of @xmath191 \\equiv { \\mathcal p}_v[\\rho ] ( r+\\sqrt{q}t ) \\ , \\ ] ] where @xmath189 is a univariate gaussian with zero mean .",
    "@xmath192(x)$ ] is called the proximal map  @xcite .",
    "[ [ the - optimal - objective - function - in - the - presence - of - a - regularizer ] ] 4 . the optimal objective function in the presence of a regularizer ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    we consider a quadratic regularizer ( an @xmath193-regularizer ) added to the objective function ,",
    "so the objective function becomes @xmath194 the motivation for this additional term is to penalize large couplings .",
    "such a term can arise , for instance , from a ( gaussian ) bayesian prior .",
    "the parameter @xmath64 needs to be adjusted such that the self - overlap @xmath195 equals @xmath29 , which we assume to be known .",
    "this additional term only leads to a small alteration in the evaluation of the partition function  .",
    "the integrals over couplings in   become @xmath196 and the replica - symmetric free energy   becomes @xmath197 \\right ] \\ , \\end{aligned}\\ ] ] eliminating the conjugate order parameters @xmath198 in the limit @xmath24 as before lead to a free energy   @xmath199(r+\\sqrt{q}t ) \\right ] \\",
    ".\\end{aligned}\\ ] ] the value of the regularization parameter @xmath64 is determined such that the value of the self - overlap @xmath41 at the saddle - point equals @xmath200 .",
    "other types of regularizing terms can be treated analogously .",
    "the evaluation of the optimal function @xmath19 in the presence of a regularizing term proceeds as in section 2 .",
    "we find the same quadratic form as in the absence of the regularizing term @xmath201 but with an optimal value of the threshold @xmath202 .",
    "the free energy for an arbitrary value of the threshold @xmath56 is @xmath203\\end{aligned}\\ ] ] with saddle - point equations @xmath204 which are solved by @xmath205 since the self - overlap @xmath162 is fixed , the minimal reconstruction error coincides with the maximum overlap @xmath42 .",
    "this maximum is reached for @xmath206 and @xmath207 with @xmath208 remaining constant .",
    "asymptotic analysis of the saddle point equations yields in this limit @xmath209 and solving @xmath162 gives @xmath210 .",
    "the result for @xmath211 is to be compared to the result without the regularizing term , @xmath212 .    for the numerical simulations in fig  [ fig3 ] , each row of the matrix of coupling matrices was computed individually .",
    "the regularisation parameter @xmath64 was set for each row such that the self - overlap of the resulting couplings was equal to @xmath29 .",
    "for the optimal objective function , we set the offset @xmath56 to @xmath213 , choosing larger values did not affect the results ."
  ],
  "abstract_text": [
    "<S> the inverse ising problem seeks to reconstruct the parameters of an ising hamiltonian on the basis of spin configurations sampled from the boltzmann measure . over the last decade , many applications of the inverse ising problem have arisen , driven by the advent of large - scale data across different scientific disciplines . </S>",
    "<S> recently , strategies to solve the inverse ising problem based on convex optimisation have proven to be very successful . </S>",
    "<S> these approaches maximise particular objective functions with respect to the model parameters . </S>",
    "<S> examples are the pseudolikelihood method and interaction screening . in this paper </S>",
    "<S> , we establish a link between approaches to the inverse ising problem based on convex optimisation and the statistical physics of disordered systems . </S>",
    "<S> we characterise the performance of an arbitrary objective function and calculate the objective function which optimally reconstructs the model parameters . </S>",
    "<S> we evaluate the optimal objective function within a replica - symmetric ansatz and compare the results of the optimal objective function with other reconstruction methods . apart from giving a theoretical underpinning to solving the inverse ising problem by convex optimisation </S>",
    "<S> , the optimal objective function outperforms state - of - the - art methods , albeit by a small margin .    </S>",
    "<S> the advent of large - scale data across different scientific disciplines , especially biology , has inspired many applications of the inverse ising problem . over the last decade </S>",
    "<S> , the inverse ising problem has been used to analyze neural firing patterns  @xcite and gene expression data  @xcite , to infer biological fitness landscapes  @xcite , and to analyze financial data  @xcite . </S>",
    "<S> a variant of the inverse ising model with more than two states for each spin has been used to determine the three - dimensional structure of proteins  @xcite . </S>",
    "<S> this versatility is not surprising : the inverse ising problem arises naturally when one wants to learn the interactions between discrete variables describing an equilibrium system .    </S>",
    "<S> conceptually , the inference of parameters of an ising model from data is a simple matter : consider an ising model with @xmath0 binary spin variables , @xmath1 . </S>",
    "<S> pairwise interactions between the spins lead to the well - known ising hamiltonian @xmath2 where @xmath3 quantifies the coupling strength between a pair of spins , which we seek to infer . </S>",
    "<S> we have inserted a constant @xmath4 for later convenience , magnetic fields can also be added without difficulty . </S>",
    "<S> @xmath5 spin configurations ( samples ) @xmath6 are drawn independently from the boltzmann distribution @xmath7 and the task is to find the couplings which produce these spin configurations . </S>",
    "<S> this can be done by maximizing the so - called log - likelihood @xmath8 with respect to the couplings , which yields the maximum - likelihood estimate of the couplings . </S>",
    "<S> alternatively , bayes theorem specifies a probability distribution over the reconstructed couplings @xmath9 called the posterior probability . </S>",
    "<S> one can reconstruct the couplings by maximizing this posterior probability with respect to the couplings , or by computing their expected value under the posterior . in the limit of a large number of samples @xmath10 , maximizing the bayesian posterior   yields the same couplings as maximizing the log - likelihood  .    in practice </S>",
    "<S> , however , the computation of either the likelihood or the bayesian posterior is a hard task : the boltzmann distribution   contains the partition function , whose computations requires a number of steps which scales exponentially with the system size . </S>",
    "<S> a large number of approaches to likelihood maximisation have been made using the tools of statistical physics , including monte carlo methods for small systems  @xcite , the mean field approximation  @xcite , a small - correlation expansion  @xcite , and others . however , </S>",
    "<S> one of the most successful methods to solve the inverse ising problem sidesteps the computation of the likelihood altogether . </S>",
    "<S> it originates from statistics and is called pseudolikelihood  @xcite . </S>",
    "<S> pseudolikelihood reconstruction proceeds by maximizing @xmath11 with respect to the couplings , or rather , with respect to a particular row @xmath12 of the matrix of couplings . in  </S>",
    "<S> , we have introduced a shorthand describing spins coupled their effective local field @xmath13 , as well as the pseudolikelihood objective function @xmath14 . </S>",
    "<S> this method can be interpreted as using a paramagnetic model to describe the statistics of one particular spin @xmath15 in an effective local field , which depends on the couplings between spins . </S>",
    "<S> pseudolikelihood reconstruction has a number of attractive features : the couplings can be determined row - by - row using a convex optimisation algorithm , and the reconstruction becomes exact in the limit @xmath10 , even at low temperatures where many other methods fail  @xcite .    </S>",
    "<S> recently , a different function has been proposed as an objective function , @xmath16 : @xmath17 is to be minimised over the row @xmath12 of the matrix of couplings  @xcite . </S>",
    "<S> this reconstruction method , termed interaction screening , outperforms pseudolikelihood when the underlying coupling matrix is sparse , and comes close to saturating bounds on reconstruction set by information theory  @xcite .    </S>",
    "<S> given these two objective functions , one can ask if there is an objective function @xmath18 , which reconstructs the parameters of the ising model optimally , that is , minimises the difference between the reconstructed and underlying couplings over all functions @xmath19 that one might use . in this paper , </S>",
    "<S> we build a statistical mechanics of the inverse ising problem based on the family of objective functions @xmath19 . </S>",
    "<S> this theory tells us how well a certain objective function reconstructs the underlying couplings . </S>",
    "<S> it can also be used to derive the objective function which performs best . </S>",
    "<S> the theory applies to typical realisations of the underlying couplings , which in the thermodynamic limit @xmath20 are realised in nearly all instances of the couplings drawn from a particular distribution . for simplicity , we restrict ourselves to coupling matrices whose entries are drawn independently from a gaussian distribution .    </S>",
    "<S> * the partition function for the inverse problem . </S>",
    "<S> * we start by considering an arbitrary ( convex ) objective function @xmath19 . for the first row of the coupling matrix @xmath21 ( and equivalently for all other rows ) we obtain the minimum of the objective function @xmath22\\\\ & = -\\lim_{\\beta \\to \\infty } \\partial_{\\beta}\\ln \\int d \\textbf{j}_1 e^{-\\beta \\sum_{\\mu=1}^m \\rho({k}_1^{\\mu } ) } \\nonumber\\\\ & = -\\lim_{\\beta \\to \\infty } \\partial_{\\beta }   \\ln z ( { \\bf{s}}^1,{\\bf{s}}^2,\\ldots,{\\bf{s}}^m ) \\nonumber \\ , \\end{aligned}\\ ] ] from the partition function for the inverse problem we define as @xmath23 . in this partition function </S>",
    "<S> , the exponential function plays the role of a boltzmann weight , from which the limit @xmath24 selects the ground state thus minimizing @xmath25 . </S>",
    "<S> the @xmath5 spins samples @xmath26 are taken independently from the boltzmann distribution  . </S>",
    "<S> they remain fixed while the minimum over @xmath27 is sought and can be considered as quenched disorder . </S>",
    "<S> conversely , the entries of the reconstructed matrix of couplings act as phase space variables and are integrated out while spin samples remain constant . </S>",
    "<S> the logarithm of the partition function averaged over the disorder , the so - called quenched average , is @xmath28 where the double pointed brackets indicate the average both over the underlying couplings and samples . </S>",
    "<S> this average partition function describes the parameter inference for a typical realisation of the couplings and the spin configurations the reconstruction is based on . </S>",
    "<S> @xmath29 denotes the variance of the underlying couplings . </S>",
    "<S> we use the replica - trick in two different places of the calculation : to represent the logarithm of the partition function @xmath30 and to compute one over the partition function @xmath31 in the boltzmann measure . </S>",
    "<S> we obtain the free energy @xmath32(r+\\sqrt{q}t ) \\right ] \\ , \\nonumber\\end{aligned}\\ ] ] with @xmath33 and @xmath34 . </S>",
    "<S> this free energy is evaluated by extremizing over the order - parameters @xmath35 and @xmath36 . </S>",
    "<S> the result is based on a small couplings ( low @xmath29 ) expansion summed to infinite order , a replica - symmetric ansatz , the low - temperature limit @xmath24 , and the thermodynamic limit @xmath37 . </S>",
    "<S> details can be found in the methods section . </S>",
    "<S> @xmath38(x ) = \\mbox{min}_{{k } } \\left [   \\frac{({k}-x)^2}{2v}+\\rho({k})\\right]\\ ] ] defines the so - called moreau envelope of @xmath19 , which plays an important role in convex optimisation and nonlinear analysis  @xcite . </S>",
    "<S> the minimum over @xmath39 in the definition of the moreau envelope   seeks to minimise @xmath19 while at the same time staying close to @xmath40 , with the relative weight of these two objectives being controlled by @xmath36 . </S>",
    "<S> the moreau envelope also appears in the context of optimal linear regression  @xcite , where it emerges in a statistical mechanics analysis as well  @xcite .    </S>",
    "<S> * order parameters . </S>",
    "<S> * the order parameters @xmath41 and @xmath42 appearing in the free energy describe the statistics of the reconstructed couplings . at the extremum </S>",
    "<S> , the order parameter @xmath43 describes the overlap between the reconstructed couplings and the underlying couplings . </S>",
    "<S> similarly , the order parameter @xmath44 gives the overlap between a row vector of reconstructed couplings and itself . </S>",
    "<S> these order parameters turn out to be self - averaging in the thermodynamic limit : although @xmath45 fluctuates between different realisations of the couplings @xmath46 and the samples , these fluctuations vanish with increasing system size , so for ( nearly ) all realisations of the disorder we have @xmath47 , and similarly for the overlap @xmath42 .    the distribution of the reconstructed couplings can also be calculated from the partition function  , see methods . </S>",
    "<S> collecting all spin pairs where the underlying coupling takes on a particular value @xmath48 , the corresponding reconstructed couplings turn out to follow a gaussian distribution with mean @xmath49 and a variance @xmath50 . for the reconstruction to have no bias , </S>",
    "<S> the overlap @xmath42 thus needs to equal the variance of the underlying couplings @xmath29 , for then @xmath51 is a random variable with mean @xmath48 . </S>",
    "<S> is finite , even objective functions like pseudolikelihood lead to a biased reconstruction , and become unbiased only in the limit @xmath52 . ]    </S>",
    "<S> * the optimal objective function . * the two order parameters @xmath42 and @xmath41 </S>",
    "<S> also specify the reconstruction error . </S>",
    "<S> we look at the relative mean - square error @xmath53 and seek the particular objective function @xmath18 , which minimises this error . using the calculus of variations applied to the free energy  , we find @xmath54 a square function with a non - trivial offset , whose value is @xmath55 , </S>",
    "<S> see methods for details . </S>",
    "<S> error measures different from   which also depend on the order parameters @xmath42 and @xmath41 yield the same quadratic form of the optimal objective function , but have different values of @xmath56 . finding the optimal objective function thus requires the variance @xmath29 of the unknown couplings . </S>",
    "<S> @xmath29 and hence the offset can be determined as follows : for the objective function , the free energy   can be calculated easily , giving the overlap parameters @xmath57 the overlap @xmath58 of reconstructed couplings can be calculated easily without knowing the underlying couplings . </S>",
    "<S> @xmath29 and thus the optimal value of the offset @xmath56 can thus be determined from a simple linear fit of @xmath41 against @xmath59 . an alternative way to determine @xmath29 based on spin - spin correlations in the @xmath5 spin configurations </S>",
    "<S> is discussed in the appendix . in figure </S>",
    "<S> [ fig1 ] , we treat the offset @xmath56 as a free parameter and show the reconstruction error as well as the overlaps @xmath42 and @xmath41 for different values of @xmath56 and compare them to numerical simulations .    </S>",
    "<S> figure  [ fig2 ] compares reconstructed and underlying couplings for different methods ; the optimal objective function  , pseudolikelihood  , and mean - field reconstruction  @xcite , showing that the optimal objective function   outperforms both of these methods . </S>",
    "<S> figure  [ fig3 ] ( top ) compares the reconstruction error @xmath60 for these three methods , as well as interaction screening  , at different values of @xmath34 . </S>",
    "<S> the optimal objective function performs best , with a particularly wide margin at low values of @xmath61 . </S>",
    "<S> the reconstruction error increases for all four methods as @xmath61 decreases , most rapidly for pseudolikelihood , interaction screening , and mean - field reconstruction . for mean - field reconstruction , </S>",
    "<S> the rapid increase of the reconstruction error with decreasing @xmath61 is connected to the matrix of two - point spin correlations becoming singular at @xmath62 . for mean - field reconstruction , but also for the reconstruction based on pseudolikelihood and interaction screening , we find that the self - overlap parameter @xmath41 diverges as @xmath61 approaches one from above . for @xmath61 below three , the convex optimisation algorithms fail for pseudolikelihood and interaction screening and also the numerical extremization of the free energy   fails .    * reconstruction with a regularizer . * for pseudolikelihood and interaction </S>",
    "<S> screening this divergence can be avoided by adding a regularizing term to the objective function . </S>",
    "<S> regularizing terms are often used to control the sparsity of the coupling matrix . here </S>",
    "<S> we use a quadratic regularizer leading to the objective function @xmath63 this regularisation term penalizes large values of the couplings , so the regularisation parameter @xmath64 can be used to control the self - overlap @xmath41 of the reconstructed couplings . </S>",
    "<S> the objective function   yields the same free energy as   above , except for an additional term @xmath65 , see methods . </S>",
    "<S> the value of @xmath64 can be determined from data in the same way the value of the offset @xmath56 was determined ; by re - calculating the overlap parameters @xmath41 and @xmath42 from the free energy , matching the dependence of @xmath41 on @xmath64 with numerical results to determine @xmath29 , and then solving @xmath66 for the regularisation parameter . </S>",
    "<S> similarly , we calculate the optimal @xmath19 in the presence of a regularizer and find again the quadratic function , but a different optimal value of @xmath56 ( see methods ) . </S>",
    "<S> figure  [ fig3 ] ( bottom ) compares the reconstruction error @xmath60 for the optimal @xmath19 , pseudolikelihood , and interaction screening in the presence of the regularizing term . </S>",
    "<S> it shows that adding the regularizing term allows the extension of all three methods to values of @xmath61 below one . </S>",
    "<S> the reconstruction errors of pseudolikelihood and interaction screening are very close to each other , and while the reconstruction error of the optimal @xmath19 is always smaller than that of the other methods , the difference is only in the range of @xmath67 in numerical simulations with @xmath68 . </S>",
    "<S> thus the performance of both pseudolikelihood and interaction screening with a regularizer is close to optimal in the regime probed here .    * conclusion . * </S>",
    "<S> a number of open issues remain , some of them of a technical nature like the validity of the replica - symmetric ansatz used to calculate the free energy  . </S>",
    "<S> the objective function   is a convex function of the couplings . as a result , </S>",
    "<S> any local minimum of the objective function is also a global minimum , so we do not expect a spontaneous breaking of replica symmetry ( describing a situation with multiple minima ) . </S>",
    "<S> however , we expect the small - coupling resummation used to derive the free energy to fail for large coupling strengths , certainly above the spin - glass transition at @xmath69 . other important points are of a practical nature : in most applications , the matrix of underlying couplings is sparse . </S>",
    "<S> the extension of our approach to sparse coupling matrices may lead to an optimal objective function different from  , possibly one close or even identical to interaction screening  , which performs very well in the regime of sparse couplings matrices  @xcite .    </S>",
    "<S> * acknowledgments : * many thanks to guy bunin , david gross , and chau nguyen for discussions .    </S>",
    "<S> 23ifxundefined [ 1 ] ifx#1 ifnum [ 1 ] # 1firstoftwo secondoftwo ifx [ 1 ] # 1firstoftwo secondoftwo `` `` # 1''''@noop [ 0]secondoftwosanitize@url [ 0 ] </S>",
    "<S>  + 12$12  & 12#1212_12%12@startlink[1]@endlink[0]@bib@innerbibempty @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) </S>",
    "<S> @noop * * ,   ( ) @noop * * ,   ( ) @noop ( ) @noop ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop ( ) @noop ( ) @noop * * ,   ( ) link:\\doibase 10.1561/2400000003 [ * * ,   ( ) ] @noop * * ,   ( ) @noop * * ,   ( ) @noop ( ) link:\\doibase 10.1103/physrevx.6.031034 [ * * ,   ( ) ] @noop * * ,   ( ) @noop _ _  ( ,  ) </S>"
  ]
}