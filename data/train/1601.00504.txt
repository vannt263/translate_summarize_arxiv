{
  "article_text": [
    "the big data phenomenon is made possible by the parallelisation of data acquisition .",
    "the data are not collected by a centralised organism , but by several contributors .",
    "this is a strength since it allows a huge variety and quantity of data to be made available .",
    "however , a necessary step for studying data from different contributors is _ merging _ these datasets .",
    "for instance , consider the classical problem of knowing which part @xmath0 of its income @xmath1 an individual spends on housing .",
    "formally , we model the relation between @xmath1 and @xmath0 by @xmath2 where @xmath3 is a noise , @xmath4 is ( in case they are provided ) some additional _ contextual variables _ ( like e.g.  age , sex , job , etc ) and @xmath5 is the dependence function between @xmath0 and @xmath1 given the contextual variables @xmath4 .",
    "the objective is to estimate the dependence relation @xmath6 .",
    "an obstacle for answering this question is that in most countries , the data on wages @xmath7 are collected by one kind of agent ( e.g.  the office for national statistics ) , and the data on housing transactions @xmath8 are collected by another kind of agent ( e.g.  financial institutions specialized in mortgage lending ) .",
    "the dependency @xmath6 between these variables can not be established immediately , since the two data sets have been collected independently : @xmath7 and @xmath8 are independent .",
    "standard results on regression do nt apply in this context . estimating @xmath6 , or _ merging the variables @xmath7 and @xmath8 _ , is then challenging .",
    "this kind of problem becomes more and more common with the growing importance of social networks such as twitter , facebook , etc .",
    "for instance , it is desirable to combine the data collected by these social networks by merging the user profiles , in the interest of a fuller analysis of their content .",
    "a very popular method for overcoming this problem is called _ matching _ , see  @xcite .",
    "suppose that in addition to collecting data on incomes and house prices , the agents also collect _ contextual data _ ( corresponding to the contextual variables @xmath4 ) such as the age , sex , job , etc .",
    "in other words , the two independent datasets are of the form @xmath9 and @xmath10 where @xmath11 and @xmath12 are the contextual data of respectively @xmath7 and @xmath8 .",
    "the matching procedure consists in associating the data in @xmath7 with the data in @xmath8 that are most similar in terms of the contextual data , i.e.  in associating the @xmath13 with the @xmath14 such that a given distance between @xmath15 and @xmath16 is minimised .",
    "once the matched dataset is available , @xmath6 can be inferred using a suitable regression method . also using this idea of cleverly taking advantage of contextual variables ,",
    "many other methods have been considered for dealing with the problems of fusing data coming from independent sources , as _ data fusion , linkage , data integration , etc _",
    "@xcite .",
    "these approaches are very reasonable , but they rely heavily on the contextual data @xmath11 and @xmath12 . if these contextual data are not very detailed , these methods can perform poorly . for instance , the most complete uk database on housing transactions which is collected by the land registry contains the house market transaction prices , some structural characteristics of the house and the geographical location , but no information on the buyer . on the other hand ,",
    "the micro databases that contain information on the wage of individuals have few if no information on the type of lodging that the individuals occupy , apart from sometimes an approximate geographic location . in this baseline problem ,",
    "the approximate geographic location is the only matching variable available . in some highly populated area",
    ", there will be many matches .",
    "in this situation , an additional noise , or even a model misspecification will be introduced by the imprecision of the matching . in the case of the social network example , the collected data are often partially anonymized , which makes the merging process difficult .    in this paper , we develop an alternative method for learning the relationships between two variables that have been collected independently . our approach _ does not rely on contextual variables _ and can therefore be used to improve on any given matching method - it is particularly interesting in the situation where many possible matches are available .",
    "it can even be used when _ no contextual variables are available _ ,",
    "i.e.  when the only available data are @xmath7 and @xmath8 and when they have been collected independently .",
    "+ the necessary assumptions in order for our method to work is that the dependence function @xmath6 given the contextual variables @xmath4 is monotone ( increasing or decreasing ) and that the noise distribution is known ( exactly or by an estimate ) . in the income and house price example , it is clear that @xmath6 is an increasing function given standard contextual variables @xmath4 such as sex , age , job , etc - the larger the income of an individual , the more it will pay for its house _ on average _ given the standard contextual variables . in the social network example , a high utilisation of a given social mainstream network is _ on average _ positively connected with a high utilisation of another mainstream social media .",
    "this situation - an increasing relation @xmath6 given the contextual variables @xmath4 - is in fact fairly common , since we only need _ averaged _ monotonicity ( through @xmath17 ) .",
    "+ the fact that our method can even be used when no contextual variables are available can seem very surprising , since it is counter intuitive that the relation @xmath17 ( when there are no contextual variables , @xmath18 ) between @xmath7 and @xmath8 can be deduced from data @xmath7 and @xmath8 that are independent .",
    "the reason why this is possible is the monotonicity of @xmath17 . in the simple case when the noise @xmath3 is null , if it is known that @xmath17 is e.g.  increasing , then the @xmath19 percentile of @xmath7 corresponds to the @xmath19 percentile of @xmath8 .",
    "the relation @xmath17 between @xmath7 and @xmath8 can then be inferred by matching the quantiles of the distributions of @xmath7 and @xmath8 . when there is noise , the problem is slightly more involved , and this is the situation that we develop in this paper .",
    "our approach mixes elements of _ quantile regression and quantile comparison _ , and of _ deconvolution _ , applied in a non - standard way to the problem of merging data sets .",
    "quantile regression and quantile comparison ( see e.g.  @xcite ) consists in fitting a distribution with other distributions , or comparing a distribution with other distributions , by using the quantiles of the distributions .",
    "distribution deconvolution ( see e.g.  @xcite ) consists in estimating the distribution of a random variable using noisy samples .",
    "this paper is structured as follows . in section  [",
    "s : setting ] , we describe the setting of the paper . in section  [ s : results ] , we provide the estimator of @xmath6 , and associated bounds on its performance . finally , we present numerical experiments on world bank data , and on census and land registry data in section  [ s : expes ] for assessing the practical impact of our method .",
    "the appendix contains the proofs of the main theorems and additional experiments .",
    "in this section , we present in a formal way the setting and the objective .",
    "let @xmath20 be the dimension of the contextual variables @xmath4 . in our approach , we allow @xmath21 , i.e.  no contextual variables are available .",
    "we do not make assumptions on the way the contextual variables are generated .",
    "we assume the following modelling for the explicative variables @xmath1 : @xmath22 where @xmath23 is a density on @xmath24 ( and @xmath25 is the associated distribution function ) .",
    "we now assume the following modelling for the explained variable @xmath0 : @xmath26 where @xmath27 is a noise that is independent of @xmath1 , where @xmath28 is a density on @xmath24 of mean @xmath29 , and where for any @xmath4 , @xmath30 is a function from @xmath24 to @xmath24 which is _",
    "monotone_. we assume that we know the distribution @xmath28 . we write @xmath31 for the density of @xmath32 ( and @xmath33 for the associated distribution function ) , and @xmath34 for the density of @xmath35 . given the model , we have @xmath36 i.e.  @xmath34 is the convolution of @xmath31 and @xmath28 .",
    "* remark on the monotonicity assumption : * assuming that for any @xmath4 , @xmath30 is monotone , e.g.  non - decreasing , means that given the contextual variables @xmath4 and _ on average _ , a larger @xmath1 corresponds to a larger @xmath0 . in the rent and wage example , it is a very reasonable assumption that richer people , given standard contextual variables as their sex , age , socio professional category , location , etc spend _ on average _ more on lodging .",
    "another example concerns the number of followers on two social media such as e.g.  twitter and facebook .",
    "it is reasonable to assume that someone who is very active on facebook is more likely to be active on twitter .",
    "this assumption is not very restrictive since it is made _ on average _ , and we do not make the assumption that there is a strict order relationship that holds for every individual , which is a much stronger assumption .",
    "we assume that we are given two databases : @xmath37 with respectively @xmath38 and @xmath39 individuals , where @xmath40 and @xmath41 and are _ totally independent _",
    ", as e.g.  in the case where they are collected from two independent sources .",
    "for instance , @xmath7 can be a dataset containing a sample of wages of individual in a geographical unit , and @xmath8 a dataset containing a sample of house prices in the same geographical unit , and @xmath42 can be standard categorical variables such age age and sex .",
    "the problem here is that these datasets have been collected independently of each other , and one does not know which individual , of a given wage , buys which house .",
    "* objective : infer the function @xmath17 from the data @xmath43 .",
    "*      matching procedures ( see e.g.  @xcite ) aim at merging @xmath9 and @xmath10 by finding good matches between the contextual variables @xmath11 and @xmath12 . the basic procedure can be summarized as follows .",
    "let @xmath44 be a distance function between the data points in @xmath45 and the data points in @xmath46 .",
    "a non - robust matching procedure aims at associating any @xmath13 of the first dataset with the point @xmath14 of the second dataset that minimises @xmath47 .",
    "a more robust generalisation of this method , related to nearest neighbours methods , is to match , for any @xmath48 , the points @xmath49 of the first dataset whose contextual variables @xmath11 are @xmath50close to @xmath48 , with points @xmath51 of the second dataset whose contextual variables @xmath12 are @xmath50close to @xmath48 , for @xmath52 .    particularly in the case of not very detailed contextual variables ( age , sex , etc ) ,",
    "a typical matching approach would then not return a one to one match from @xmath7 to @xmath8 , but would map subsets of @xmath7 to subsets of @xmath8 , in function of @xmath4 .",
    "so a matching procedure outputs , for values of @xmath4 that exist in the dataset , the following subsets of @xmath7 and @xmath8 that correspond to @xmath4 : @xmath53 this provides a first merging of the variables , but in the case where for given @xmath4 , the function @xmath30 is not constant ( which is often the case for not very detailed contextual variables ) , this does not allow for a reconstruction of @xmath17 , and therefore for the determination of the relation between @xmath1 and @xmath0 .    in this paper",
    "is to refine a such procedure ( or in the case where there are no contextual variables and therefore where no matching is possible , our aim is to link as well as we can @xmath1 and @xmath0 ) , and is thus to estimate , for any @xmath4 in the set of contextual variables , the relation between @xmath1 and @xmath0 given @xmath4 , i.e.  the function @xmath54 given the data @xmath55 .    *",
    "revisited objective : infer the function @xmath6 from the data @xmath56 . *    [ [ remark - on - the - model ] ] remark on the model + + + + + + + + + + + + + + + + + + +    the post - matching model is @xmath57 - so that if for a given context @xmath4 we observed a dataset of the form @xmath58 ( therefore with with cross - information ) , @xmath59 and @xmath60 would not be independent - and one could use standard techniques as e.g.  regression .",
    "but in our setting we observe the data @xmath59 and @xmath60 from different , independent sources - making de facto @xmath59 and @xmath60 independent ( and fully independent , not just knowing the order statistics ) .",
    "regression techniques are not applicable there since we do not have the information of which @xmath61 in dataset @xmath59 corresponds to which @xmath62 in dataset @xmath60 .",
    "we now present our main procedure and results .",
    "we assume that we dispose of a matching procedure based on the contextual variables @xmath4 as described in subsection  [ ss : match ] .",
    "we restrict to the case of discrete contextual variables for the theoretical results , and we consider exact matching according to these variables ( the subsets @xmath55 correspond to exactly the same @xmath4 ) but this can be easily generalised in practice . if no contextual variables @xmath4 are available ( @xmath21 ) , then we use as convention in the rest of this section @xmath63    let @xmath4 be a given value of the contextual variables such that @xmath64 are non - empty and let @xmath65 be the number of data in the smallest of these two sets .",
    "let @xmath66 be the empirical distribution estimator of @xmath25 defined over the samples @xmath67 .",
    "we assume that there is a _ deconvolution estimator _",
    "@xmath68 that estimates the distribution @xmath33 , based on @xmath69 ( separating its density @xmath70 form the noise @xmath28 ) , and satisfies the following assumption .",
    "[ ass : estfh ] let @xmath71 .",
    "let @xmath72 .",
    "there exists an estimator @xmath68 of @xmath33 computed using the @xmath69 and the knowledge of @xmath73 and that is such that with probability larger than @xmath74 @xmath75 is standard under some regularity conditions .",
    "a discussion on the existence of it is provided in the subsection  [ app : decon ] .",
    "theorems  [ th : cocobleu ] and  [ th : cocobleu2 ] below give some properties on the efficiency of the estimate @xmath76 of @xmath6 defined as @xmath77 where @xmath78 is the pseudo inverse of @xmath68 .",
    "the entire procedure for computing @xmath76 , which we call matchmerge , for constructing this estimator of @xmath17 is summarized in algorithm  [ algo1 ] .    @xmath43 a matching method with respect to @xmath4 a deconvolution method from the noise @xmath79 apply the matching method to the data and obtain for all @xmath4 @xmath56 compute the empirical estimator @xmath66 of @xmath25 on @xmath67 compute the deconvolution estimator @xmath68 of @xmath33 using @xmath69 and @xmath28 set @xmath80 return @xmath81    the following theorem provides a first theoretical guarantees for @xmath82 , as well as a confidence statement in any point @xmath83 .",
    "[ th : cocobleu ] let @xmath84 .",
    "let assumption  [ ass : estfh ] be satisfied for @xmath71 .",
    "we have with probability @xmath85 @xmath86    the proof of this result is in the appendix , subsection  [ pr : cocobleu ] .",
    "this theorem provides a bound on the accuracy of the estimate of @xmath87 if a rather mild condition ( assumption  [ ass : estfh ] is verified ) .",
    "it is important to note here that apart from the assumption that @xmath6 is a monotone function ( without loss of generality , let us say that @xmath6 is non - decreasing ) , no additional assumptions are made on @xmath6 . and",
    "even though the datasets @xmath67 , @xmath69 are independent , it is possible to recover the link function @xmath17 in this non - parametric model .",
    "the bound in this theorem is not explicit , it depends on @xmath88 .",
    "the next theorem gives an explicit bound ( depending on @xmath89 ) , provided that an additional assumption is made on @xmath90 .",
    "[ ass : holder ] let @xmath91 .",
    "a function @xmath92 is hlder continuous on @xmath93 if for any @xmath94 , we have @xmath95    the hlder assumption is mild if @xmath96 is small and @xmath97 is large . in particular , functions that are differentiable on a compact ( or lipschitz )",
    "verify it with @xmath98 and @xmath99 , but it is more general than this ( functions of the form @xmath100 verify it in @xmath29 for any @xmath101 with parameters @xmath96 and @xmath102 ) .",
    "this theorem provides a more specific theoretical guarantee in the case where the distributions are hlder smooth .",
    "[ th : cocobleu2 ] let @xmath84 .",
    "let assumption  [ ass : estfh ] be satisfied for @xmath71 and assume that for any @xmath4 , @xmath6 is monotone ( without loss of generality , non - decreasing ) .",
    "let @xmath103 .",
    "assume that @xmath6 is @xmath104hlder on @xmath105 $ ] , and that @xmath106 is @xmath107hlder on @xmath108 $ ] ( where @xmath106 is the pseudo - inverse of @xmath25 ) as in assumption  [ ass : holder ] .",
    "then with probability larger than @xmath85 @xmath109    the proof of this result is in the appendix , subsection  [ pr : cocobleu2 ] .",
    "the bound in theorem  [ th : cocobleu2 ] implies that it is possible to recover the link function between the data @xmath110 , @xmath111 with a good precision .",
    "our approach enables us to complement matching in this way . again , if no contextual variables @xmath4 are available ( @xmath112 ) , our approach is still applicable by just skipping the matching step .",
    "[ [ remark - on - the - noise - assumption ] ] remark on the noise assumption + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in order for our theoretical results to holds , we need to know the distribution of the noise to perform the deconvolution .",
    "let @xmath113 be the fourier transform of the noise , and @xmath114 be the fourier transform of a distribution .",
    "if @xmath115 , i.e.  if the distance defined as @xmath116 , between the dirac fourier transform and @xmath113 is larger than the distance between @xmath114 and @xmath113 , then deconvolution with @xmath114 is better than quantile matching .",
    "the distance relation stated before is quite sensitive to parameters such as variance or range , so if @xmath113 is closer to @xmath114 in terms of these quantities than to a dirac mass in @xmath29 , it is likely that our method applied using @xmath114 in the deconvolution step will be more efficient than simple quantile matching .",
    "the model we have been considering until now is quite general . in many cases ,",
    "however , the effect of the contextual variables @xmath4 can be separated from the effect of @xmath1 .",
    "the underlying model is then as follows .",
    "let @xmath20 be the dimension of the control variables @xmath4 .",
    "we assume the following model for the explicative variables @xmath1 : @xmath118 where @xmath119 belongs to some functional class @xmath120 , and where @xmath121 is independent of @xmath4 , where @xmath114 is a density on @xmath24 ( with corresponding distribution @xmath122 ) .",
    "we now assume the following modelling for the explained variable @xmath0 : @xmath123 where @xmath124 is independent of @xmath125 , where @xmath73 is a density on @xmath24 of mean @xmath29 , where @xmath126 belongs to some functional class @xmath120 , and where @xmath17 is an increasing function .",
    "typically , @xmath3 is a white noise ( e.g.  gaussian ) .",
    "we assume that we know its distribution @xmath73 .",
    "let us write @xmath127we write @xmath128 for the density of @xmath129 ( with corresponding distribution @xmath130 ) , and @xmath131 for the density of @xmath132 ( with corresponding distribution @xmath92 ) .",
    "given the model , we have @xmath133 i.e.  @xmath131 is the convolution of @xmath128 and @xmath73 .",
    "a classical model which is separable is the popular linear model .",
    "real data are in general not exactly separable , but it is often a useful approximation to assume that they are - at least locally .    the objective here is to find @xmath17 , knowing that it is increasing . the following methodology can be applied .",
    "first , estimate @xmath119 and @xmath126 using respectively @xmath9 and @xmath10 .",
    "these two problems are standard functional regression problems , and usual methods can be used here ( linear regression , kernel regression , regression on a base , svm , etc , depending on the set @xmath120 containing @xmath134 ) . using these estimates",
    ", one can deduce estimates @xmath135 and @xmath136 for @xmath137 and @xmath138 .",
    "one can the perform the method described in subsection  [ ss : mr ] on the data @xmath135 and @xmath136 with null contextual variables .",
    "the method is described in algorithm  [ algo2 ] .",
    "@xmath43 a regression method a deconvolution method from the noise @xmath73 compute the regression estimator @xmath139 of @xmath119 using @xmath9 compute the regression estimator @xmath140 of @xmath126 using @xmath10 set @xmath141 , and @xmath142 compute the empirical estimator @xmath143 of @xmath122 on @xmath135 compute the deconvolution estimator @xmath144 of @xmath145 using @xmath136 and @xmath73 set @xmath146 return @xmath147",
    "in this section , we apply our methods on two real datasets : the first dataset contains macro - economic data from the world bank on development in @xmath148 african countries , and the second contains micro - economic data from the uk land registry on house transaction prices in london . in order to assess the performance of our method , we have deliberately chosen datasets which contain simultaneously the two variables we want to merge and therefore their relationship - but we split in these two applications these datasets when we apply our method and consider the first variable on the first part and the second variable on the second part , so that the two variables are independent and so that we are in real condition for applying our method .",
    "the datasets we use are publicly available and more informations on them can be found in the description of the experiments .",
    "we also performed simulations and applied our method on synthetic data , this is to be found in the appendix  [ s : expes2 ] due to space constraint .      in the first example",
    "we look at the impact of urbanisation on life expectancy in @xmath148 african countries , in @xmath149 .",
    "we use world bank macro - economic data ( the africa development indicators ( adi ) dataset ) .",
    "we consider the two variables @xmath150average life expectancy \" and @xmath151urbanisation percentage \" .",
    "the life expectancy is clearly an increasing function of urbanisation since a more urbanised country implies that more infrastructure ( electricity , hospitals , etc ) is available , and that accessing these public services is much easier .    in this dataset",
    ", the cross - information is available ( the data - set provides , for each country , the average life expectancy and the urbanisation percentage ) .",
    "this cross information will be used for performance assessment only , and of course , not by our method .",
    "we do not provide our algorithm with the dependence structure @xmath152 , but we provide it with independent sub - samples of size @xmath153 of @xmath7 and @xmath8 .",
    "we plot the estimate @xmath154 obtained with our method using different deconvolution distributions in the deconvolution process ( the  true \" noise distribution @xmath73 is not available ) , as well as the points @xmath155 for illustrating how the estimator of the link function captures the dependence structure from the monotonicity constraint .",
    "the results are plotted in figure  [ fig : exp4 ] , each curve corresponds to an estimate @xmath154 for which the deconvolution is made using a different deconvolution distribution .",
    "we provide results for normal deconvolution distribution with various variances ( left plot ) , and uniform deconvolution distribution with various ranges ( right plot ) .",
    "as expected , the smaller the variance of the deconvolution distribution , the closer the estimator @xmath154 is to the points from which it is constructed .",
    "however , as in standard regression , this comes together with the problem of over - fitting , as illustrated in table  [ ta1 ] where the risk ( the square root of the mse ) is evaluated on an independent sample of 20 countries .",
    "this evaluation shows clearly that taking a deconvolution distribution of small variance does not provide an estimator that has good generalization properties , and it is therefore better to take this into account instead of doing a simple quantile matching .    from the curves in figure  [ fig : exp4 ]",
    ", it seems that urbanisation has a multiplier effect on the life expectancy : indeed , the function @xmath17 is convex until a threshold",
    ". however , there is clearly endogeneity in this model .",
    "in particular , the richer the country , the more urbanised it is ( in africa ) . and the richer the country , the higher the life expectancy .",
    "if we want to measure the true effect of urbanisation on life expectancy , we should get rid of this side effect .",
    "we should thus control for this effect using a control variate @xmath156gdp per head \" , as explained in subsection  [ sec : compl2 ] .",
    "we assume here that there is a linear underlying model , and that we have @xmath157 we estimate the @xmath158 and the @xmath159 using the control @xmath4 and doing a linear regression ( as in algorithm  [ algo2 ] ) . as explained before , urbanisation , even after the control should have a positive effect on the controlled life expectancy , because urbanisation enlarges the access to important public services .",
    "the results in this controlled setting are plotted in figure  [ fig : exp5 ] .",
    "the points are the estimates of the controlled @xmath160 , and each curve corresponds to a deconvolution with a different noise deconvolution .",
    "we observe here that the curve @xmath154 is now concave - controlling by the gdp per head has cancelled the multiplier effect , although the impact of urbanisation on life expectancy is still positive .",
    "it means that some urbanisation has a very positive effect on life expectancy ( because it implies better access to vital infrastructure ) , but that this positive effect is sub linear ( the multiplier effect , coming from the fact that the gdp is positively correlated with urbanisation and life expectancy , is suppressed ) .",
    "the mse ( again , evaluated on an independent sample ) is displayed in table  [ ta2 ]            [ cols=\"<,>,>,>,^ , > , > , > \" , ]      in the second example we look at the impact of the the neighborhood share of high skilled residents on local property prices .",
    "we use data from the 2011 uk census on the share of residents holding a university degree in electoral wards and prices of 2011 housing transactions available for download at the land registry website .",
    "we consider the two variables @xmath150average price \" and @xmath151percentage of high skilled residents \" .",
    "house prices in an area are clearly increasing in the local concentration of well educated workers as workers holding university degrees are paid the highest wages and subsequently spend more on housing .",
    "we are interested in reconstructing the map of house prices in london , using the percentages of degrees ( with geographical co - variate ) , and the house prices ( but without using the geographical information ) : we want to merge the percentages of degrees ( plus geographical location ) with the house prices .",
    "we believe that this example is interesting , because unlike the uk , most countries do not provide refined geographical data for house transaction , whereas geographical census data are usually available in developed countries .",
    "figure [ fig : exp6 ] a ) shows the distribution of average house prices in 2011 for 733 london wards .",
    "house prices are highest in the west of inner london and in the south - west of outer london . with our method",
    "we try to reconstruct the local price pattern on the basis of the degree variable without making use of the geographical information available for the house prices .",
    "we divide at random our sample in two datasets , one that will serve the purpose of constructing the estimate and the other the one of evaluating it through the mse ( 300 for construction of the estimates and 433 for evaluating their performances ) .",
    "figure [ fig : exp6 ] c ) and d ) show reconstructions of the original price map using respectively the distributions @xmath161)$ ] , and @xmath162)$ ] for the deconvolution ( the  true \" noise distribution of the noise ) .",
    "the choice of a uniform deconvolution distribution @xmath73 is reasonable in this case ( since the variables are bounded and rescaled between @xmath29 and @xmath163 ) , and we considered two ranges for @xmath73 ( 1 and 5 ) . as a comparison method",
    "we also reconstruct the map based on quantile matching , which is equivalent to using a dirac mass in @xmath29 in the deconvolution step ( figure [ fig : exp6 ] b ) ) .",
    "all three methods are able to generate a price pattern very similar to the original spatial distributing shown in a ) .",
    "the qualitative difference is that the larger the noise with which one deconvolves , the more contrasted the picture gets - removing the smoothing that is due to the noise - and d ) is the most contrasted picture .",
    "figure [ fig : exp6bis ] shows the estimate @xmath154 obtained with our method using the different deconvolution distributions , plotted with the second part of the sample for evaluation .",
    "we evaluate the performance under the different deconvolution distributions by calculating the mean squared error ( mse ) , using the distance of each point to the function @xmath154 .",
    "quantile matching results in a mse of * 8.62 * , whereas the uniform distributions yield an mse of * 8.54 * for @xmath164)$ ] , respectively * 8.23 * for @xmath165)$ ] .",
    "our method outperforms a simple quantile match , when using a reasonable deconvolution distribution ( here uniform , since the noise is bounded ) .",
    "[ [ remark - regarding - the - deconvolution - noise - in - the - empirical - results ] ] remark regarding the deconvolution noise in the empirical results + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    empirically , if the noise distribution is unknown , we found out that in all studied cases , deconvolving with some `` reasonable '' distribution - even if it is not the correct one - outperforms quantile matching .",
    "the `` reasonable '' distribution does not need to be the true noise distribution , which is unknown , but it can be a distribution deduced from some prior knowledge on the noise .",
    "this knowledge does not need to be precise .",
    "in the examples of subsection  [ s : expes1 ] , deconvolving with a distribution of very small variance is giving bad results with respect to the mse while bigger noises are performing significantly better , see table 1 and 2 ( improvement ranges between 30% to more than 50% in table 2 ) .",
    "we did not show results for pure quantile matching because they are slightly worse than deconvolution with @xmath166 .",
    "also , deconvolution with the uniform distribution or the gaussian distribution of variance ranging from @xmath167 to @xmath168 provides comparable results - although these distributions are significantly different and largely span the set of `` reasonable '' distributions .",
    "similar results can be seen in the mse in figure 4 for the second example and in the appendix on synthetic data .",
    "this highlights the fact that one does not need to have a precise knowledge of the noise distribution to gain something significant with respect to quantile matching .",
    "we developed in this paper a new method for merging variables .",
    "it can be used as a complement to matching - or also on its own when no contextual variables are available .",
    "it is easy to implement and provides good results , both in theory and in practice , provided that the dependence function @xmath17 is monotone .",
    "\\a ) +     \\b ) +     \\c ) +     \\d ) +                 [ [ acknowledgements ] ] acknowledgements + + + + + + + + + + + + + + + +    part of this work was produced when ac was in the statslab in the university of cambridge .",
    "ac s work is supported since 2015 by the dfg s emmy noether grant musyad ( ca 1488/1 - 1 ) .",
    "there is a huge literature on deconvolution , and many estimators are available , see e.g.  @xcite .",
    "a simple example of a such estimator is as follows .",
    "let @xmath169 be the fourier transform , and @xmath170 be the inverse fourier transform , and let @xmath171 be the empirical distribution of @xmath69 .",
    "an estimator of the density @xmath31 can be defined as follows @xmath172 where @xmath173 is the truncated fourier transform of @xmath174 where any coefficients smaller than @xmath175 in absolute value are set to @xmath29 .",
    "the estimator presented in equation   is a very simple estimate , and is not always optimal .",
    "many other estimators have been proposed in the previously quoted papers , are more effective , and should be preferred to this one in practice .",
    "the plug - in estimate of the distribution function associated to @xmath176 , i.e.  @xmath177 of @xmath33 will converge to @xmath33 at a rate @xmath178 which depends on the regularity @xmath179 of @xmath31 ( the more regular @xmath31 , the faster the rate ) , and of the decay of the fourier spectrum of @xmath28 ( the heavier the tails of @xmath180 , the faster the rate ) , see e.g.  @xcite . for instance , if the function @xmath31 is in a hlder ball of smoothness @xmath179 ( see assumption  [ ass : holder ] below ) , and if the noise @xmath28 is  ordinary smooth \" ( polynomial decay of @xmath180 ) , then @xmath181 , where @xmath182 depend on @xmath179 and on the polynomial decay rate of @xmath180 . in e.g.  gaussian noise ,",
    "the rate is worse because the fourier transform of a gaussian decays very quickly , and the rate is @xmath183 , where @xmath3 depends on @xmath179 .      in this proof , in order to avoid notational heaviness , we write for convenience @xmath7 instead of @xmath67 and @xmath8 instead of @xmath69 .",
    "we also write @xmath122 for @xmath25 and @xmath130 for @xmath33 ( and the same simplification for the estimators ) .",
    "we first state the two following lemma .",
    "[ lem : estf ] let @xmath71 .",
    "let @xmath72 .",
    "there exists an estimator @xmath143 of @xmath122 computed using the @xmath7 that is such that with probability larger than @xmath74 @xmath184    the samples @xmath7 are i.i.d .",
    "distributed according to @xmath122 .",
    "this implies that the @xmath185 are i.i.d .",
    "bernoulli random variables of parameter @xmath186 .",
    "consider the classic estimator @xmath187 applying bernstein s inequality to this estimate implies that with probability @xmath74 @xmath188 which implies the result .",
    "[ lem : quantconf ] let @xmath189 .",
    "let @xmath92 be a distribution , and @xmath190 be the an estimate of @xmath92 such that for any @xmath72 , on an event of probability @xmath74 , @xmath191 let @xmath192 . with probability @xmath85 , we have @xmath193 where @xmath194 and @xmath195 are the pseudo - inverses of @xmath92 and @xmath190 .",
    "let @xmath196 it holds that @xmath197,$ ] since @xmath92 is a distribution function and is thus increasing . since @xmath190 is increasing , it holds that @xmath198 moreover , since the inverse of @xmath92 exists in the neighbourhood @xmath199 $ ] , we have that @xmath200    it holds by an union bound that with probability @xmath85 , @xmath201 this implies that with probability @xmath85 , @xmath202 this implies that with probability @xmath85 , @xmath203 since @xmath190 is increasing as an estimate of a distributions function ( and @xmath195 too then ) , we have that with probability @xmath85 , @xmath204 this concludes the proof .    by lemma  [ lem : estf ]",
    ", we have with probability @xmath74 , @xmath205 let @xmath206 and @xmath207 .",
    "the previous equation implies that with probability @xmath74 , @xmath208    by lemma  [ lem : quantconf ] and assumption  [ ass : estfh ] , we have with probability @xmath74 , @xmath209 where @xmath210 is the pseudo - inverse of @xmath130 , which implies since @xmath211 , that with probability @xmath74 , @xmath212 where @xmath213 is the pseudo - inverse of @xmath122 .    the previous equations imply , since @xmath214 is increasing that with probability @xmath85 , @xmath215 which implies that with probability @xmath85 , @xmath216      in this proof , in order to avoid notational heaviness , we write for convenience @xmath7 instead of @xmath67 and @xmath8 instead of @xmath69 .",
    "we also write @xmath122 for @xmath25 and @xmath130 for @xmath33 ( and the same simplification for the estimators ) .",
    "we first state the following lemma .",
    "[ lem : graph ] let @xmath217 .",
    "assume that @xmath122 is strictly increasing in @xmath83 .",
    "then @xmath218 where @xmath213 is the pseudo - inverse of @xmath122 .",
    "let @xmath219 and @xmath213 be the pseudo - inverses of @xmath220 . by definition @xmath221 and @xmath222 moreover @xmath223 where @xmath219 is the pseudo - inverse of @xmath17 .",
    "we have @xmath224 since @xmath122 is strictly increasing in @xmath83 .",
    "this concludes the proof .",
    "lemma  [ lem : graph ] implies since @xmath122 is strictly increasing in @xmath83 ( since @xmath213 is increasing and hlder continuous ) that @xmath225 .    from theorem",
    "[ th : cocobleu ] , we have that with probability @xmath85 , @xmath216 since @xmath213 is @xmath107hlder on @xmath226 $ ] , this implies that with probability @xmath85 @xmath227 and since @xmath17 is @xmath104hlder on @xmath105 $ ] , this implies that with probability @xmath85 @xmath228 this concludes the proof .",
    "we simulate @xmath38 data @xmath229)$ ] that are i.i.d .",
    "( here @xmath114 is a uniform density on @xmath230 $ ] ) , and @xmath231 data @xmath232 , independently of the @xmath233 , for different distributions @xmath73 and @xmath234 .",
    "the functions @xmath17 we consider in these simulations is @xmath235 .",
    "we estimate the link function @xmath17 by @xmath236 using the distribution of @xmath73 to deconvolve @xmath131 from @xmath73 .",
    "the results of these simulations , i.e.  plots of @xmath17 and of a realisation of @xmath154 , are provided in figure  [ fig : exp1 ] , for various noises @xmath73 and sample sizes .    without surprises ,",
    "the larger @xmath234 , the better the estimator @xmath154 .",
    "a less intuitive fact , is that the more heavy tailed the distribution , the the better the deconvolution ( the performances when @xmath73 is a student distribution are better than for the gaussian and uniform cases ) .",
    "this comes from the fact that heavy tailed noise distributions are easier to deconvolve than light tailed noises , as we mentioned below assumption  [ ass : estfh ] .",
    "we now consider the same setting as before ( in subsubsection  [ sss : sim1 ] ) but now we consider a function @xmath17 that has a discontinuity .",
    "the results are displayed in figure  [ fig : exp2 ] .",
    "the function @xmath235 of figure  [ fig : exp1 ] is easier to reconstruct than the function @xmath17 of figure  [ fig : exp2 ] , which has a discontinuity and that thus does not satisfy the assumptions of theorem  [ th : cocobleu2 ] .",
    "the estimator is however not far from the original function in particular in student noise , or at some distance of the discontinuity .",
    "[ [ simulations - in - the - case - of - deconvolution - with - the - wrong - noise - xi ] ] simulations in the case of deconvolution with the wrong noise @xmath73 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^    a third aspect that we illustrate with simulations is the impact of not knowing precisely the noise distribution @xmath73 ( during the deconvolution process ) on the estimator @xmath154 .",
    "more precisely , what is the impact to use a deconvolution distribution @xmath237 that differs from @xmath73 in the deconvolution process for obtaining the estimate @xmath238 ?",
    "we did experiments for two  true \" noises @xmath73 , and four deconvolution distributions @xmath237 .",
    "the results are displayed in figure  [ fig : exp3 ] .    the impact of not having a precise knowledge of @xmath73 is existent , but is not very important in these two examples .",
    "it implies that when confronted with a such problem , making a small error on the distribution of the noise @xmath73 is not completely altering the efficiency of the procedure ."
  ],
  "abstract_text": [
    "<S> the aim of this paper is to provide a new method for learning the relationships between data that have been obtained independently . unlike existing methods like matching </S>",
    "<S> , the proposed technique does not require any contextual information , provided that the dependency between the variables of interest is monotone . </S>",
    "<S> it can therefore be easily combined with matching in order to exploit the advantages of both methods . </S>",
    "<S> this technique can be described as a mix between quantile matching , and deconvolution . </S>",
    "<S> we provide for it a theoretical and an empirical validation . </S>"
  ]
}