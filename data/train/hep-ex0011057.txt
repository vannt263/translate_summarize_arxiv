{
  "article_text": [
    "perhaps the most common practical duty of a particle physicist is to analyze various distributions from a set of data @xmath0 .",
    "the typical tool used in this analysis is the histogram .",
    "the role of the histogram is to serve as an approximation of the parent distribution , or probability density function ( pdf ) from which the data were drawn . while histograms are straightforward and computationally efficient , there are many more sophisticated techniques which have been developed in the last century .",
    "one such method , kernel estimation , grew out of a simple generalization of the histogram and has proved to be particularly well - suited for particle physics .    in order to produce continuous estimates @xmath1 of the parent distribution from the _ empirical probability density function _",
    "@xmath2 , several techniques have been developed .",
    "these techniques can be roughly classified as either parametric or non - parametric .",
    "essentially , a parametric method assumes a model @xmath3 dependent on the parameters @xmath4 .",
    "the specification of this model is `` entirely a matter for the practical [ physicist ] '' .",
    "the goal of a parametric estimate is to optimize the parameters @xmath5 with respect to some goodness - of - fit criterion ( _ e.g. _  @xmath6 , log - likelihood , _ etc ... _ ) .",
    "parametric models are powerful because they allow us to infuse our model with our knowledge of physics . while parametric methods are very powerful , they are highly dependent on the specification of the model .",
    "parametric methods are clearly not practical for estimating the distributions from a wide variety of physical phenomena .",
    "the goal of non - parametric methods is to remove the model - dependence of the estimator .",
    "non - parametric estimates are concerned directly with optimizing the estimate @xmath1 .",
    "the prototypical non - parametric density estimate is the histogram .",
    "somewhat counterintuitively , non - parametric methods typically involve a large - possibly infinite - number of `` parameters '' ( better thought of as degrees of freedom ) .",
    "scott and terrell supplied a more concrete definition of a non - parametric estimator , `` roughly speaking , non - parametric estimators are asymptotically local , while parametric estimators are not.''@xcite that is to say , the influence of a data point @xmath7 on the density at @xmath8 should vanish asymptotically ( in the limit of an infinite ammount of data ) for any @xmath9 in a non - parametric estimate .",
    "the purpose of this paper is to introduce the notion of a kernel estimator and the inherent advantages it offers over other parametric and non - parametric estimators .",
    "the notion of a kernel estimator grew out of the asymptotic limit of averaged shifted histograms ( ash ) .",
    "the ash is a simple device that reduces the binning effects of traditional histograms .",
    "the ash algorithm is as follows : first , create a family of @xmath10 histograms , @xmath11 , with bin - width @xmath12 , such that the first bin of the @xmath13 histogram is placed at @xmath14 . because @xmath15 is an artificial parameter ,",
    "each of the @xmath16 is an equally good approximation of the parent distribution .",
    "thus , an obvious estimate of the parent distribution is simply the average of the @xmath16 , hence the name ` average shifted histogram ' .",
    "note that resulting estimate ( with @xmath10 times more bins than the original ) is not a true histogram , because the height of a ` bin ' is not necessarily equal to the number of events falling in that bin .",
    "however , it is a superior estimate of the parent distribution , because the dependence of initial bin position is essentially removed . in the limit",
    "@xmath17 the ash is equivalent to placing a triangular shaped _ kernel _ of probability about each data point @xmath7  @xcite .      in the univariate case ,",
    "the general kernel estimate of the parent distribution is given by @xmath18 where @xmath0 represents the data and @xmath12 is the smoothing parameter ( also called the _ bandwidth _ ) .",
    "immediately we can see that our estimate @xmath19 is bin - independent regardless of our choice of @xmath20 .",
    "the role of @xmath20 is to spread out the contribution of each data point in our estimate of the parent distribution .",
    "an obvious and natural choice of @xmath20 is a gaussian with @xmath21 and @xmath22 : @xmath23 though there are many choices of @xmath20 , gaussian kernels enjoy the attributes of being positive definite , infinitely differentiable , and defined on an infinite support . for physicists",
    "this means that our estimate @xmath19 is smooth and well - behaved in the tails .",
    "now we concern ourselves with the choice of the bandwidth @xmath12 . in equation",
    "[ e:1dkeys ] , the bandwidth is constant for all @xmath24 .",
    "thus , @xmath19 is referred to as the _ fixed kernel estimate_. the role of @xmath12 is to set the scale for our kernels . because the kernel method is a non - parametric method , @xmath12 is completely specified by our data set @xmath0 . in the limit of a large amount ( @xmath25 ) of normally distributed data",
    "@xcite , the _ mean integrated squared error _ of @xmath19 is minimized when @xmath26 of course , we rarely deal with normally distributed data , and , unfortunately , the optimal bandwidth @xmath27 is not known in general . in the case of highly bimodal data ( _ e.g. _  the output of a neural network discriminate ) , the standard deviation of the data is not a good measure for the scale of the true structure of the distribution .",
    "an astute reader may object to the choice of @xmath27 given in equation  [ e : hopt ] on the grounds of self - consistency - non - parametric estimates should only depend on the data locally , and @xmath28 is a global quantity . in order for the estimate to handle a wide variety of distributions as well as",
    "depend on the data only locally , we must introduce _ adaptive kernel estimation_. the only difference in the adaptive kernel technique is that our bandwidth parameter is no longer a global quantity .",
    "we require a term that acts as @xmath29 in equation  [ e : hopt ] .",
    "abramson  @xcite proposed an adaptive bandwidth parameter given by the expression @xmath30 equation  [ e : adaptive ] reflects the fact that in regions of high density we can accurately estimate the parent distribution with narrow kernels , while in regions of low density we require wide kernels to smooth out statistical fluctuations in our empirical probability density function .",
    "technically we are left with two outstanding issues : @xmath24 ) the expression for @xmath31 given in equation  [ e : adaptive ] references the _ a priori _ density , which we do not know , and @xmath32 ) the optimal choice of @xmath12 has still not been specified .",
    "clearly , @xmath33 , because of dimensional analysis .",
    "additionally , @xmath19 is our best estimate of the true parent distribution .",
    "thus we obtain @xmath34 with @xmath35    the adaptive kernel estimate can be thought of as a `` second iteration '' of the general kernel estimation technique . in practice ,",
    "the adaptive kernel technique almost completely removes any dependence on the original choice of the bandwidth in the fixed kernel estimate @xmath19 .",
    "furthermore , the adaptive kernel deals very well with multi - modal distributions . in extreme situations",
    "( _ i.e. _  when the scale of the local structure of the data @xmath29 is more than about two orders of magnitude smaller than the standard deviation @xmath28 of the data ) the factor @xmath36 in equation  [ e : hoptadaptive ] should be adjusted from its typical value of unity . in that case",
    "@xmath37 we have concluded the construction of a non - parametric estimate @xmath38 of an univariate parent distribution based on the empirical probability density function .",
    "our estimate is bin - independent , scale invariant , continuously differentiable , positive definite , and everywhere defined .",
    "both the fixed and adaptive kernel estimates assume that the domain of the parent distribution is all of @xmath39 .",
    "however , the output of a neural network discriminant , for example , is usually bounded by @xmath40 , where @xmath41 . in order to avoid probability from `` spilling out '' of the boundaries",
    "we must introduce the notion of a _",
    "boundary kernel_. without boundary kernels , our estimate will not be properly normalized and underestimate the true parent distribution close to the boundaries .",
    "boundary kernels modify our traditional gaussian kernels so that the total probability in the allowed regions is unity . clearly , our kernel should smoothly vary back to our original gaussian kernels as we move far from the boundaries .",
    "this constraint quickly reduces the kinds of boundary kernels we need consider .",
    "though a large amount of work has been put forward to introduce kernels which preserve the criteria @xmath42 these methods do not suit themselves well for physics applications .",
    "the primary problem is that the parametrized family of boundary kernels may contain kernels that are not positive definite - which negates their applicability to physics .",
    "also , boundary kernels satisfying equation  [ e : criterea ] systematically underestimate the parent distribution at a moderate distance from the boundary and overestimate very near the boundary .",
    "[ f : boundary ]    an alternate solution to the boundary problem is to simply reflect the data set about the boundary  @xcite . in that case , the probability that spills out of the boundary is exactly compensated by its mirror .",
    "the general kernel estimation technique generalizes to _",
    "d_-dimensions  @xcite .",
    "one choice for the _ d_-dimensional kernel is simply a product of univariate kernels with independent smoothing parameters .",
    "the following discussion will be restricted to the context of such product kernels .",
    "when dealing with multivariate density estimation , the covariance structure of the data becomes an issue . because the covariance structure of the data may not match the diagonal covariance structure of our kernels",
    ", we must apply a linear transformation which will diagonalize the covariance matrix @xmath43 of the data .",
    "ideally , the transformation would remain a local object ; however , in practice such non - linear transformations may be very difficult to obtain . in the remainder of this paper , the transformation matrix",
    "will be referred to as @xmath44 , and the @xmath45 will be assumed to be transformed .      for product kernels ,",
    "the fixed kernel estimate is given by @xmath46.\\ ] ] in the asymptotic limit of normally distributed data , the _ mean integrated squared error _ of @xmath19 is minimized when @xmath47      the adaptive kernel estimate @xmath48 is constructed in a similar manner as the univariate case ; however , the scaling law is usually left in a general form . because most multivariate data actually lies on a lower dimensional manifold embedded in the input space , the effective dimensionality @xmath49 must be found by maximizing some measure of performance or making some assumption .",
    "thus the multivariate adaptive bandwidth is usually written @xmath50 though @xmath51 , the precise value depends on the problem . note that the form of @xmath31 given in equation  [ e : ddhoptadaptive ] is independent of @xmath52 , thus it produces spherically symmetric kernels .",
    "this is clearly not optimal .",
    "furthermore , when @xmath53 the optimal value of @xmath12 may vary wildly .",
    "this is because the units are no longer correct and @xmath54 powers of scale factors are introduced by @xmath55 .",
    "both of these problems may be remedied with the introduction of a natural length scale associated with the data : the geometric mean of the standard deviations of the transformed @xmath0 , @xmath56 . in the absence of local covariance information ,",
    "the best we can do is assume that the @xmath57 are proportional to @xmath58 and inversely proportional to @xmath59 .",
    "thus we arrive at @xmath60 which is produces estimates that are invariant under linear - transformation of the input space when the covariance matrix is diagonalized .      just as in the univariate case , it is possible that the physically realizable domain of our parent distribution is not all of @xmath61 , but instead a bounded subspace of @xmath61 .",
    "typically , this situation arises when one of the components of the sample vector is bounded in the univariate sense ( _ i.e. _  @xmath62 ) .",
    "however , once we diagonalize the covariance matrix of our data the boundary condition will take on a new form in the transformed coordinates . in general , any linear boundary in our original coordinates",
    "@xmath63 can be expressed as @xmath64 , where @xmath65 is the unit - normal to the ( @xmath66)-dimensional hyperplane in our @xmath67-dimensional domain and @xmath68 is the distance between the origin and the point - of - closest approach . after transforming to a set of coordinates @xmath69 , in which the @xmath45 have diagonal covariance , our boundary condition is given by @xmath70 .",
    "thus , for each boundary one must introduce a reflected sample @xmath71 with @xmath72 in order to rectify the probability that spilled into unphysical regions .      in high - energy physics",
    "it is often necessary to combine data from heterogeneous sources ( _ e.g. _  independently produced monte carlo data sets which together comprise the standard model expectation ) .",
    "in general one would like to estimate the parent distribution from a more general _ empirical probability density function _",
    "@xmath73 , where @xmath74 represents the weight or _ a posteriori _ probability of the @xmath75 event . in the case of combining various monte carlo samples",
    ", one must reweight all events of a sample to some common luminosity ( say , 1 pb@xmath76 ) before combining them .",
    "thus for a monte carlo sample with @xmath77 events and cross - section @xmath78 each event must be weighted with @xmath79    the covariance matrix of the weighted sample must be generalized as follows : @xmath80 where @xmath81 and @xmath82 .",
    "then our estimate is simply given by    @xmath83.\\ ] ]",
    "kernel estimation techniques are applicable to all situations in which it is necessary or useful to have an estimate of the parent distribution of a set of data . in high - energy physics",
    "the use of kernel estimation runs the gamut from event selection to confidence level calculation .",
    "the most wide - spread use of kernel estimation techniques has been in the context of confidence level calculations , primarily for the higgs searches at lep @xcite .",
    "each selected candidate event has associated with it one or several discriminant variables ( _ e.g. _  reconstructed higgs mass , neural network output , _ etc ... _ ) .",
    "estimates of the distribution of discriminant variables are constructed for the signal and background processes , @xmath84 and @xmath85 respectively .",
    "these estimates are used to further discriminate between candidate events via the log - likelihood ratio @xmath86 or some other ` signal estimator ' . with the log - likelihood in hand , confidence level calculations transcend pure number counting and allow for a more powerful statistical tool with which to interpret the data @xcite .",
    "another context in which kernel estimation has been applied is the measurement of physical constants via maximum likelihood fitting .",
    "traditionally , the log - likelihood @xmath87 is maximized with respect to the parameters @xmath88 . in this context",
    ", @xmath89 is a parametrized model of the physical situation . in practice",
    "not all of the @xmath90 are ` floated ' or varied in the maximization routine , but instead many parameters are ` fixed ' from some independent measurement .",
    "while this model incorporates empirical or theoretical information , it may make unwanted assumptions about our data .    for an example , let us consider the measurement of @xmath91 at a @xmath92 factory .",
    "the probability density of a cp decay recoiling from a tagged @xmath92 ( @xmath93 ) meson is given by @xmath94 where @xmath95 is the time difference between the decay of the cp state and the recoiling @xmath92 ( @xmath93 ) tagged meson with @xmath96 .",
    "however , in an experiment we must take into account the mistag rate @xmath97 and the resolution of @xmath98 .",
    "the standard prescription is to measure @xmath97 and parametrize the resolution distribution @xmath99 with a single ( or double ) gaussian with bias  @xmath100 and variance  @xmath28 .",
    "the final probability distribution is obtained via a convolution with the resolution function and is of the form @xmath101 . now with @xmath102 , and @xmath28 ` fixed ' we must ` float ' @xmath103 to make our measurement  @xcite . here",
    "the form of @xmath104 , while justified , will have a systematic influence on the measured value of @xmath91 .",
    "if , on the other hand , the resolution function @xmath104 was estimated via a non - parametric means ( _ i.e. _  kernel estimation techniques ) , then there would be no artificial influence on the measurement and non - trivial resolution effects would be taken into account automatically .",
    "knuteson proposed a non - parametric parameter estimation technique based on kernel estimation of the joint density of feature vectors and the parameters @xmath105 to be measured . via bayesian statistics",
    "the _ posterior density _ of the parameters given an observation is obtained .",
    "then one estimates the parameters by simply maximizing the _ posterior _ density  @xcite .",
    "the first uses of kernel estimation in high - energy physics were in the context of new particle searches . in order to create a discriminant analysis ,",
    "one constructs the discriminant @xmath106 according to : @xmath107 it is easily seen that signal - like events get mapped to 1 and background - like events are mapped to 0 . by placing a cut on @xmath106 a ( possibly disconnected ) region ( bounded by the corresponding @xmath108-dimensional contour ) in the input space is selected .",
    "because of the geometrical complexity of the selected region , analyses of this type are more efficient than linear discriminant analyses .",
    "furthermore , because of the intuitive nature of equation  [ e : d ] , many prefer analyses of this type over artificial neural networks .",
    "while these analyses may avoid the black - box properties of artificial neural networks , their usefulness is limited to @xmath109 due to the _ curse of dimensionality_. however , with more intelligently chosen variables , the practical restriction of @xmath109 is not much of a limitation .",
    "discriminant analyses such as these were met with much success in the search for the top quark at d@xmath110  @xcite .",
    "an obvious application of kernel estimation techniques is the improvement and automation of cut optimization .",
    "given signal and background samples , one typically varies cuts to find a region @xmath111 which maximizes some measure of performance ( _ e.g. _  @xmath112 , @xmath113 , _ etc ... _ ) .",
    "traditionally , the cuts are applied directly to the sample events , thus only discrete values of the performance measure are obtained .",
    "this leaves the experimentalist in a bit of a quandary because in the neighborhood of the optimal cut value the performance measure tends to have many local extrema and is constant between sample points .",
    "not surprisingly , the optimal cut value is often just chosen by eye .",
    "the above method of cut optimization may be generalized by estimating the performance not from the raw samples , but instead from the estimates of the signal and background parent distributions , @xmath114 and @xmath115 respectively .",
    "then instead of having discrete values of @xmath116 and @xmath92 we obtain continuous functions @xmath117 and @xmath118 .",
    "finally , we are left with the more well - defined problem of finding the global maxima of a continuously varying performance measure - in the case of @xmath113 we obtain @xmath119 in fact , it should be pointed out that with a finite sample size there is an inherent uncertainty in @xmath114 and @xmath120 . thus , there is an an inherent uncertainty in  @xmath111 . in that case , the optimal selection strategy becomes probabilistic - events on the boundary @xmath111 should be selected with some well defined probability .",
    "while this strategy can be carried out in a deterministic fashion ( _ e.g. _  generating pseudo - random numbers with event and run number as a random seed ) the gain in performance does not merit the added complexity of the selection .",
    "in order to implement univariate kernel estimation techniques to produce fortran functions of @xmath38 based on the data in hbook ntuples , _ keys _ was developed .",
    "_ keys _ has been implemented in a perl script which : 1 ) parses an input file specifying the input ntuples and output filenames of a set of shapes , 2 ) interfaces with paw to obtain the data set @xmath0 , 3 ) produces a fortran function to calculate @xmath121 , 4 ) compiles and evaluates the fortran function , 5 ) produces plots ( see figure  2 ) of the estimate and the kolmogorov - smirnov probability that the @xmath0 is compatible with @xmath38  @xcite , and 6 ) produces a second fortran function which linearly interpolates @xmath38 between 300 sample points in order to expedite the evaluation of the function .",
    "_ keys _ has been used extensively for the parametrization of discriminant variable distributions in higgs searches at lep @xcite .",
    "[ fig : keysoutput ]      another implementation of univariate kernel estimation has been developed by jeremiah mans of the l3 collaboration .",
    "this implementation is written in c@xmath122 .",
    "the _ hepukeys _ class executes a c - compiler , and uses the dynamic loading methods ( dlopen , dlclose , dlsym ) for interactive or embedded implementations of the kernel estimation technique .",
    "this embedded structure allows one to include confidence level calculation , cut optimization , or any other application of kernel estimation directly in the analysis code .",
    "the original implementation of multivariate kernel estimation in high - energy physics was developed at rice university by holmstrm , miettinen and sain with guidance from scott around 1994 .",
    "this vms - based fortran implementation , called _ pde _ , was applied to searches for the top quark at d@xmath110 @xcite .      in order to produce a more portable and flexible package for kernel estimation , the author developedan object oriented implementation written in c@xmath122 .",
    "the core numerical procedures were left in a very general form with polymorphic kernels and minimal user interface .",
    "the core procedures have been adopted by two complete packages : _ rootpde _ and _",
    "winpde_.    padley and askew interfaced the core numerical procedures described above into a shared root library called _",
    "rootpde_. this package was primarily intended for discriminant analyses as described in section [ ss : discriminantanal ] .",
    "however , this package was submitted to d@xmath110 s cvs repository prior to the development of equation  [ e : h_ij ] ( see section [ ss : multivariateadaptivekernel ] ) and does not support event - by - event weighting ( see section [ ss : event - by - event ] ) or boundary kernels ( see sections [ ss : univariateboundarykernel ] and [ ss : multivariateboundarykernel ] ) .    since the original release of _ rootpde _",
    ", the author has been including more advanced features and developed methods which allow a kernel estimate object to write out a c@xmath122  function for use in other applications .",
    "furthermore , the interface has been generalized from pure discriminant analysis to a form more amenable to any of the applications described in section [ s : applications ] .",
    "currently , this development is taking place at ba@xmath123ar ; however , it will be made publicly available in the near future .",
    "_ winpde _ is very similar to the original release of _ rootpde _ described in section [ ss : rootpde ] ; however , the interface is written in visualbasic and intended for microsoft windows platforms . furthermore , _ winpde _ can interface with microsoft excel spreadsheets and offers a very intuitive user - interface .",
    "it seems appropriate to put kernel estimation techniques in a proper setting before concluding with a discussion of their inherent benefits . while kernel estimation techniques may be applied to situations in which a parametric estimates are popular , that comparison has essentially been made by section  [ ss : overview ] .",
    "instead , let us consider perhaps the most widely used non - parametric density estimation technique in high - energy physics : paw s smooth utility .",
    "a full development of the hquadf function that is used by paw s smooth utility is beyond the scope of this paper .",
    "however , a brief outline of the algorithm is presented .",
    "first and foremost , it is important to realize that smooth operates on histograms and not on the original data set @xmath0 .",
    "thus , smooth is dependent on the original binning of the data .",
    "smooth was introduced by this journal in john allison s 1993 paper @xcite .",
    "we will restrict ourselves to the univariate case .",
    "essentially smooth works by finding the bins @xmath124 of _ significant variation _ in the histogram @xmath125 and then using those points to construct a smoothed linear interpolation .",
    "bins of significant variation are those which satisfy @xmath126 , where @xmath127 is a user - defined significance threshold and @xmath128 with the points of significant variation @xmath129 in hand , the smoothed shape is given by @xmath130 where @xmath131 are the radial basis functions .",
    "the @xmath132 are user - defined smoothness parameters ( radii of curvature ) .",
    "the @xmath133 are found by minimizing the @xmath6 between @xmath134 and the original histogram . as allison pointed out",
    "`` lower @xmath6 can be obtained by reducing the cut on @xmath135 at the expense of following more of what _ might _ only be statistical fluctuations . '' by a different choice of @xmath127 and @xmath136 , the user has the power to magnify or remove statistical fluctuation in the data .      despite the user - specified parameters @xmath127 and @xmath136 ,",
    "smooth is a non - parametric estimate of a probability density function based on a set of data .",
    "the primary differences between smooth and kernel estimates are their approach and their rigor .",
    "while kernel estimates are bin - independent _ constructions _ of the estimate , smooth is a parameter - dependent _ fit _ of the estimate to a user - provided histogram .",
    "practically speaking , kernel estimates are based on well defined statistical techniques and smooth s estimates are adjusted by eye allowing for user bias and large systematic uncertainty .      when kernel estimation techniques are applied to confidence level calculations or parameter estimation , systematic effects become of particular importance .",
    "one may loosely classify the systematic errors associated with probability density estimation as either inherent or user - related errors . in its pure form kernel estimation techniques are entirely deterministic and have no user - specified parameters .",
    "if one decides to free the value of @xmath36 from its nominal value of unity ( see equation  [ e : rho ] ) or allow @xmath53 , then user - related systematic error are introduced . for smooth , the user - related parameters @xmath127 and @xmath136 can not be avoided .",
    "in addition to the possible user - related systematic errors , there are inherent systematic errors introduced by any probability density estimation technique . for parametric estimates , this inherent systematic",
    "is related to the quality of the model ; while for non - parametric estimates , this inherent systematic is related to the flexibility of the technique .",
    "the development of kernel estimation techniques has been directly focused on flexibility and the minimization of a particular choice of inherent systematic error : the asymptotic mean integrated squared error @xcite .    in practice",
    ", an experimentalist will want to choose their own estimate of the inherent systematic error ( _ e.g. _  the effect on the measured value of a parameter or 95% confidence level limit ) .",
    "this can be done in a variety of ways that effectively reduce to producing a family of estimates from independent samples of the same parent distribution .",
    "this family may be obtained by simply splitting up the data or via toy monte carlo simulation . because the systematic error introduced by the estimation technique is a function(al ) of the sampled parent distribution ( which is unknown ) ,",
    "the estimate itself is the best available choice of the parent distribution to be sampled in a monte carlo study .",
    "obviously , kernel estimation techniques are very powerful and very relevant to high - energy physics .",
    "while these techniques have been applied to a wide range of analyses , they seem to by largely unknown by the community .",
    "it is the aim of this paper to describe kernel estimation techniques , their applications , and their current implementations in order to promote their use .",
    "it is to the author s personal gratification to be part of a cross - polination of ideas between fields - in this case statistics and high - energy physics .",
    "the author would like to thank hannu miettinen - without his original guidance this paper would not have been written .",
    "the development of _ keys _ was largely influenced by members of the lephiggs analysis working group , most notably chris tully , steve armstrong , peter mcnamara , jason nielsen , arnulf quadt , tom junk , peter igo - kemenes , tom greening , and yuanning gao .",
    "thanks to paul padley and andrew askew for the development of _ rootpde _ and stephen mccaul for aid in the design of its core numerical procedures .",
    "karl berkelman provided very useful discussion on the sensitive issues of systematic errors .",
    "finally , the author is grateful for the support of sau lan wu during a stay at cern in which the majority of this work was conducted ."
  ],
  "abstract_text": [
    "<S> kernel estimation provides an unbinned and non - parametric estimate of the probability density function from which a set of data is drawn . in the first section , after a brief discussion on parametric and non - parametric methods , </S>",
    "<S> the theory of kernel estimation is developed for univariate and multivariate settings . </S>",
    "<S> the second section discusses some of the applications of kernel estimation to high - energy physics . </S>",
    "<S> the third section provides an overview of the available univariate and multivariate packages . </S>",
    "<S> this paper concludes with a discussion of the inherent advantages of kernel estimation techniques and systematic errors associated with the estimation of parent distributions </S>",
    "<S> .    kernel estimation , multivariate probability density estimation , keys , rootpde , winpde , pde , hepukeys , unbinned , non - parametric </S>"
  ]
}