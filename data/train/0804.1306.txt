{
  "article_text": [
    "it is well known that the brain has a highly developed and complex self - generated dynamical neural activity .",
    "we are therefore confronted with a dichotomy when attempting to understand the overall functioning of the brain or when designing an artificial cognitive system : a highly developed cognitive system , such as the brain , is influenced by sensory input but is not driven directly by the input signals .",
    "the cognitive system needs however this sensory information vitally for adapting to a changing environment and survival .    in this context",
    "we then want to address a two - fold goal :    * can we formulate a meaningful paradigm for the self - sustained internal dynamics of an autonomous cognitive system ? * how is the internal activity process influenced by sensory signals , _",
    "viz _ which are the principles for the respective learning processes ?    with respect to the first problem we note that an increasing flux of experimental results from neurobiology supports the notion of quasi - stationary spontaneous neural activity in the cortex @xcite .",
    "it is therefore reasonable to investigate possible set - ups for artificial cognitive systems centrally based on the notion of internally generated spontaneous transient states , as we do in the present investigation .     and @xmath0 for the length of activity - plateau and of the transient period respectively.,scaledwidth=45.0% ]",
    "we will present the model and the issues being studied in two steps , starting with a general overview , discussing technical details in a second step .",
    "there are many notions of complex system theory characterizing the long - term behavior of a dynamical system  @xcite , such as regular vs.  chaotic behavior .",
    "the term ` transient - state dynamics ' refers , on the other hand , to the type of activity occurring on intermediate time scales , as illustrated in fig .",
    "[ figure_trans_activity ] .",
    "a time series of semi - stable activity patterns , also denoted transient attractors , is characterized by two time scales .",
    "the typical duration @xmath1 of the activity plateaus and the typical time @xmath0 needed to perform the transition from one semi - stable state to the subsequent one .",
    "the transient attractors turns into stable attractors in the limit @xmath2 .",
    "transient state dynamics is intrinsically competitive in nature .",
    "when the current transient attractor turns unstable the subsequent transient state is selected by a competitive process .",
    "transient - state dynamics is a form of ` multi - winners - take - all ' process , with the winning coalition of dynamical variables suppressing all other competing activities .",
    "similar processes have been proposed to be relevant for various neural functionalities .",
    "edelman and tononi @xcite argue that ` critical reentrant events ' constitute transient conscious states in the human brain .",
    "these ` states - of - mind ' are in their view semi - stable global activity states of a continuously changing ensemble of neurons , the ` dynamic core ' .",
    "this activity takes place in what dehaene and naccache @xcite denote the ` global workspace ' .",
    "the global workspace serves , in the view of baars and franklin @xcite , as an exchange platform for conscious experience and working memory .",
    "crick and koch @xcite and koch @xcite have suggested that the global workspace is made - up of ` essential nodes ' , i.e. ensembles of neurons responsible for the explicit representation of particular aspects of visual scenes or other sensory information .",
    "generating an internal transient state - dynamics via eq .",
    "( [ eq_x_dot ] ) .",
    "the two examples differ in the functional dependence of the @xmath3 on the @xmath4 .",
    "the top graph corresponds to a system having sensitive periods , the bottom graph to a system without distinctive sensitive periods.,scaledwidth=45.0% ]      within the setting of a generalized neural network we utilize a continuous - time formulation with rate - encoding neural activity centers , characterized by normalized activity levels @xmath5 $ ] .",
    "one can then define , quite generally , via @xmath6 the respective growth rates @xmath7 .",
    "representative time series of growth rates @xmath7 are illustrated in fig .",
    "[ figure_sensitive_periods ] . when the @xmath8 the respective neural activity @xmath9 increases , approaching rapidly the upper bound , as illustrated in fig .  [ figure_trans_activity ]",
    "; when @xmath10 it decays to zero .",
    "the model is specified @xcite , by providing the functional dependence of the growth rates with respect to the set of activity - states @xmath11 .    during the transition",
    "periods many , if not all , neurons will enter the competition to become a member of the new winning coalition .",
    "the competition is especially pronounced whenever most of the growth rates @xmath7 are small in magnitude , with no small subset of growth rates dominating the all the others . whether this does or does not happen depends on the specifics of the model set - up , in fig .  [ figure_sensitive_periods ] two cases are illustrated ( upper / lower graph in fig .",
    "[ figure_sensitive_periods ] ) . in the first case",
    "the competition for the next winning coalition is restricted to a subset of neurons , in the second case the competition is network - wide .",
    "when most neurons participate in the competition process for a new winning coalition the model will have ` sensitive periods ' during the transition times and it will able to react to eventual external signals .    , the growth rates @xmath3 and the input signals @xmath12 from a simulation of a system containing @xmath13 sites ( color coding ) . the time series of winning coalition is ( from left to right ) : ( 1,5 ) @xmath14\\atop\\longrightarrow}$ ] ( 2,9,12 ) @xmath15\\atop\\longrightarrow}$ ] ( 0,2,3 ) @xmath16\\atop\\longrightarrow}$ ] ( 8,10 ) @xmath17\\atop\\longrightarrow}$ ] ( 5,14 ) @xmath18\\atop\\longrightarrow}$ ] ( 4,9,12 ) @xmath19\\atop\\longrightarrow}$ ] ( 12,13 ) , where @xmath20 $ ] corresponds to the transition - label given in the graph.,scaledwidth=45.0% ]      so far we have discussed in general terms the properties of isolated models exhibiting a self - sustained dynamical behavior in terms of a never - ending time series of semi - stable transient states , as illustrated in fig .",
    "[ figure_sensitive_periods ] , using rate - encoding equations specified in previous work @xcite .",
    "the importance of sensitive periods comes when this model is coupled to a stream of sensory input signals .",
    "it is reasonable to assume , that external input signals will contribute to the growth rates @xmath7 via @xmath21 where the @xmath22 encode the influence of the input signals .",
    "let us furthermore assume that the input signals are suitably normalized , such that @xmath23 for the transient states the @xmath24 for all sites not forming part of the winning coalition and the input signal will therefore not destroy the transient state . with the normalization given by eq .",
    "( [ eq_delta_r_magnitude ] ) the total growth rate @xmath25 will remain negative for all inactive sites .",
    "the input signal will however enter into the competition for the next winning coalition during a sensitive period , providing an additional boost for the respective neurons .",
    "this situation is exemplified in fig .",
    "[ figure_arb_15 ] , where we present simulation - results for a system containing @xmath13 neurons subject to two sensory inputs @xmath12 .",
    "the self - generated time series of winning coalitions is both times redirected by the strongest component of the sensory input , the details depending on whether the input signal partially overlaps with the current winning coalition or not .",
    "we then have a model system in which there are well defined time - windows suitable for the learning of correlations between the input signal and the intrinsic dynamical activity , namely during and shortly after a transition period , that is the sensitive period .",
    "a possible concrete implementation for this type of learning algorithm will be given further below .",
    "this set - up would allow the system also to react to an occasional very strong input signal having @xmath26 .",
    "such a strong signal would suppress the current transient state altogether and impose itself .",
    "this possibility of rare strong input signals is evidently important for animals and would be , presumably , also helpful for an artificial cognitive system .",
    "let us return to the central problem inherent to all system reacting to input signals having at the same time a non - trivial intrinsic dynamical activity .",
    "namely when should learning occur , i.e.when should a distinct activity center become more sensitive to a specific input pattern and when should it suppress its sensibility to a sensory signal .",
    "the above developed framework of competitive dynamics allows for a straightforward solution of this central issue : learning should occur then and only then when the input signal makes a qualitative difference , _ viz _ when the input signal deviates the transient - state process . for illustration",
    "let us assume that the series of winning coalitions is @xmath27\\atop\\longrightarrow}\\ ( 2,9,12 )        \\ { [ a]\\atop\\longrightarrow}\\ ( 0,2,3)~,\\ ] ] where the index @xmath28 $ ] indicates that the transition is driven by the internal autonomous dynamics and that the series of winning coalitions take the form @xmath27\\atop\\longrightarrow}\\ ( 2,9,12 )        \\ { [ s]\\atop\\longrightarrow}\\ ( 0,4,10)\\ ] ] in the presence of a certain sensory signal @xmath29 $ ] .",
    "note , that a background of weak or noisy sensory input could be present in the first case , but learning should nevertheless occur only in the second case .",
    "technically this is achieved by defining a suitable diffusive learning signal @xmath30 .",
    "it is activated whenever any of the input signals @xmath22 changes the sign of the respective growth rates during the sensitive periods , @xmath31 _ viz _ when it makes a qualitative difference . here",
    "@xmath32 is the internal contribution to the growth rate , i.e.  the value it would have in the absence of the input signal @xmath22 and the @xmath33 are the growth and decay rates for the diffusive learning signal respectively .",
    "the diffusive learning signal @xmath30 is a global signal and a sum @xmath34 over all dynamical variables is therefore implicit on the right - hand side of eq .",
    "( [ eq_dot_s ] ) .      the general procedure for the learning of correlation between external signals and intrinsic dynamical state for a cognitive system presented here does not rule out other mechanisms .",
    "here we concentrate on the learning algorithm which occurs automatically , one could say sub - consciously .",
    "active attention focusing , which is well known in the brain to potentially shut off a sensory input pathway or to enhance sensibility to it , may very well work in parallel , to the continuously ongoing mechanism investigated here .",
    "connecting the input with the dhan layer ( denote ` v ' in the graph ) are adapted during the learning process ( illustrated selectively by respective thick / thin blue lines in the graph ) .",
    "the dhan layer consists of active and inactive neurons ( red / yellow circles ) connected by intra - layer synaptic weights ( denote ` w ' in the graph ) .",
    "the topology shows five cliques ( denoted c - i to c - v in the graph ) of which c - ii is active , as emphasized by the red - color neurons . ,",
    "so far we have described , in general terms , the system we are investigating , having sensitive periods during the transition periods of the continuously ongoing transient - state process , with learning of input signals regulated by a diffusive learning signal .",
    "our system consists of two components , as illustrated in fig .",
    "[ fig_twolayers ] . for the component generating an infinite time - series of transient state we employ a dense homogeneous associative network ( dhan ) . the dhan - model has been studied previously and shown to generated a time series of transient states characterized by high associative overlaps between subsequent winning coalitions  @xcite .",
    "the time series might therefore be interpreted as an associative thought process , it carries with it a dynamical attention field @xcite .",
    "the input signal acts via eq .",
    "( [ eq_delta_r ] ) on the dhan layer , with the contribution @xmath22 to the growth rate of the dhan neuron @xmath35 given by @xmath36 where we have denoted now with the superscripts @xmath37 the dhan- and input - layer respectively . for subsequent use",
    "we also define in eq .",
    "( [ eq_delta_r_v_pq ] ) an auxiliary variable @xmath38 which quantifies the influence of inactive input - neurons .",
    "the task is now to find a suitable learning algorithm which extracts the relevant information from the input - data stream by mapping distinct input - patterns onto selected winning coalitions of the dhan layer .",
    "this is the setup typical for an independent component analysis @xcite .",
    "the multi - winners - take - all dynamics in the dhan module implies individual neural activities to be close to 0/1 during the transient states and we can therefore define three types of inter - layer links @xmath39 ( see fig .",
    "[ fig_twolayers ] ) :    * ( _ ` act ' _ ) + links connecting active input neurons with the winning coalition of the dhan module . *",
    "( _ ` orth ' _ ) + links connecting inactive input neurons with the winning coalition of the dhan module .",
    "* ( _ ` ina ' _ ) + links connecting active input neurons with inactive neurons of the dhan module .",
    "the orthogonal links take their name from the circumstance that the receptive fields of the winning coalition of the target layer need to orthogonalize to all input - patters differing from the present one .",
    "note that it is not the receptive field of individual dhan - neurons which is relevant , but rather the cumulative receptive field of a given winning coalition .",
    "we can then formulate three simple rules for the respective link - plasticity .",
    "whenever the new winning coalition in the dhan - layer is activated by the input layer , _ viz _ whenever there is a substantial diffusive learning signal , i.e.  when @xmath40 exceeds a certain threshold @xmath41 , the following optimization procedures should take place :    *   + the sum over active links should take a large but finite value @xmath42 , @xmath43 *   + the sum over orthogonal links should take a small value @xmath44 , @xmath45 *   + the sum over inactive links should take a small but non - vanishing value @xmath46 , @xmath47    the @xmath42 , @xmath46 and @xmath44 are the target values for the respective optimization processes . in order to implement these three rules we define three corresponding contributions to the link plasticities : @xmath48 where the inputs @xmath49 and @xmath38 to the target layer are defined by eq .",
    "( [ eq_delta_r_v_pq ] ) . for",
    "the sign - function @xmath50 is valid for @xmath51 and @xmath52 respectively , @xmath53 denotes the heaviside - step function in eq .",
    "( [ eq_back_contributions ] ) .",
    "the inter - layer links @xmath39 cease to be modified whenever the total input is optimal , _ viz _ when no more  mistakes  are made @xcite .",
    "bars problem with a probability @xmath54 for the occurrence of the individual horizontal or vertical bars .",
    "the problem is non - linear since the pattern intensity is not enhanced when an elementary horizontal and vertical bar overlap.,scaledwidth=45.0% ]    .a possible set of parameters for the inter - layer links , with @xmath55 and @xmath56 .",
    "[ cols=\"^,^,^,^\",options=\"header \" , ]     [ tab_par_back ]    using these definitions , the link plasticity may be written as @xmath57~ ,   \\nonumber \\end{aligned}\\ ] ] where @xmath41 is an appropriate threshold for the diffusive learning signal @xmath42 and @xmath46 are the desired contributions to the growth rate for active / inactive postsynaptic neurons , @xmath58 and @xmath59 the respective rates and @xmath60 and @xmath61 the critical activity reservoirs defining an active / inactive postsynaptic center respectively .",
    "a suitable set of parameters , which has been used throughout this work , is given in table  [ tab_par_back ] .",
    "we note , that a given interlayer - link @xmath39 is in general subject to competitive optimization from the three processes ( act / orth / ina ) .",
    "averaging would occur if the respective learning rates @xmath58/@xmath62/@xmath59 would be of the same order of magnitude .",
    "it is therefore necessary , that @xmath63      it is desirable that the interlayer connections @xmath39 neither grow unbounded with time ( runaway - effect ) nor disappear into irrelevance .",
    "suitable normalization procedures are therefore normally included explicitly into the respective neural learning rules and are present implicitly in eqs .",
    "( [ eq_back_contributions ] ) and eq .",
    "( [ eq_vdot ] ) .",
    "the strength of the input - signal is optimized by eq .",
    "( [ eq_vdot ] ) both for active as well as for inactive @xmath64-layer neurons , a property referred to as fan - in normalization .",
    "( [ eq_back_contributions ] ) and ( [ eq_vdot ] ) also regulate the overall strength of inter - layer links emanating from a given @xmath65-layer neuron , a property called fan - out normalization .",
    "next we note , that the timescales for the intrinsic autonomous dynamics in the dhan - layer and for the input signal could in principle differ substantially .",
    "potential interference problems can be avoided when learning is switched / one very fast , _ viz",
    "_ when activation and when the decay rate @xmath66 are larger , i.e.when @xmath67 is smaller than both the typical time scales of the input and of the self - sustained dynamics .",
    "a cognitive system needs to autonomously extract meaningful information about its environment from its sensory input data stream via signal separation and features extraction .",
    "the identification of recurrently appearing patterns , i.e.  of objects , in the background of fluctuation combinations of distinct and noisy patterns constitutes a core demand in this context .",
    "this is the domain of the independent component analysis @xcite and blind source separation @xcite , which seeks to find distinct representations of statistically independent input patterns .    in order to test",
    "our system made - up by an input - layer coupled to a dhan layer , as illustrated in fig .",
    "[ fig_twolayers ] , we have selected the bars problem @xcite .",
    "the bars problem constitutes a standard non - linear reference task for the feature extraction via an independent component analysis for a @xmath68 input layer .",
    "basic patterns are the @xmath69 vertical and @xmath69 horizontal bars .",
    "the individual input patterns are made - up of a non - linear superposition of the @xmath70 basic bars , containing with probability @xmath54 any one of them , as shown in fig .",
    "[ figure_bars_pattern ] .      for the simulations we presented to the system about @xmath71 randomly generated @xmath72 input patterns of the type shown in fig .",
    "[ figure_bars_pattern ] , including a small noise - level of about @xmath73 .",
    "the individual patterns lasted @xmath74 with about @xmath75 for the time in between two successive input signals .",
    "these time - scales are to be compared with the time - scale of the autonomous dhan - dynamics illustrated in the figs .",
    "[ figure_sensitive_periods ] and [ figure_arb_15 ] , for which the typical stability - period for a transient state is about @xmath76 .",
    "this implies that we presented the system sparse ( in time ) sensory input .",
    "we also note that the number of training patterns in our simulation is exceedingly small , standard neural algorithms use routinely @xmath77 and more training patterns @xcite .",
    "the results for the simulations are presented in figs .",
    "[ figure_crp_graph ] and [ figure_crf ] . for the geometry of the dhan network we used a simple 15-site chain containing 14 potential winning coalitions , as illustrated in fig .",
    "[ figure_crf ] , namely @xmath78 , @xmath79 ,  , @xmath80 .    in fig .",
    "[ figure_crp_graph ] we present the response @xmath81 of the 14 potential winning coalitions @xmath82 to the 10 basic input patterns @xmath83 , the isolated bars . here",
    "@xmath82 denotes the set of sites of the winning - coalition @xmath84 and @xmath85 its size , here @xmath86 .",
    "the individual potential winning coalitions have acquired in the course of the simulation , via the learning rule eq .",
    "( [ eq_vdot ] ) , distinct susceptibilities to the 10 bars compare fig .",
    "[ figure_crp_graph ] .",
    "we note that the learning is unsupervised and quite fast .",
    "this example - problem is overcomplete , there are more potential winning coalitions than statistically independent basic pattern",
    ". perfect signal separation can therefore not be expected .",
    "e.g.  the second - last vertical bar in fig .  [ figure_crf ] is not resolved .    as the dhan dynamics is competitive in nature already relatively small differences in the response",
    ", eq .  ( [ eq_clique_rec_patt ] ) , may determine the outcome of the competition between two competing potential winning coalitions .",
    "this circumstance is born - out by the results presented in fig .",
    "[ figure_crp_graph ] .",
    "the receptive fields @xmath87 of the 14 potential winning coalitions are given in fig .",
    "[ figure_crf ] .",
    "the inter - layer synaptic weights @xmath39 can be both positive and negative and the orthogonalization procedure , eq .",
    "( [ eq_back_contributions ] ) , results in complex receptive fields .    physically separated target neurons , _ viz _ a ` single - winner - takes - all ' set - up , are normally used for standard analysis neural algorithms performing an independent component analysis @xcite .",
    "the winning coalitions of the dhan - layer are however overlapping and every link @xmath39 targets in general more than one potential winning coalition in the dhan layer , in our case two for @xmath88 ( and one for @xmath89 ) .",
    "the unsupervised learning procedure , eq .  ( [ eq_vdot ] ) , involves therefore a competition between the active contribution @xmath90 and the @xmath91 and @xmath92 , as given by eq .",
    "( [ eq_back_contributions ] ) .",
    "the inter - layer synaptic weights are therefore only changed to the point necessary for recognition , but not up to the saturation point , as evident from the data presented in fig .",
    "[ figure_crf ] .",
    "this behavior is consistent with the ` learning by mistakes ' paradigm @xcite , which states that a cognitive system needs to learn in general only when committing a mistake .    ) , for the 14 winning coalitions @xmath93, .. ,[5]$ ] ( first row ) @xmath94, .. ,[10]$ ] ( second row ) and @xmath95, .. ,[14]$ ] ( third row ) as a function of the @xmath96 input field , compare fig .",
    "[ figure_bars_pattern ] .",
    "for the response of the 14 winning coalitions with respect to the ten reference input patterns compare fig .",
    "[ figure_crp_graph ] .",
    "note that the receptive fields can be both positive as well as negative , see the color-coding.bottom : the geometry of the dhan layer as a linear chain .",
    "the winning coalitions @xmath97 $ ] ( @xmath98 ) are numerated and correspond here to two connected nearest - neighbor in a linear - chain layout .",
    ", scaledwidth=45.0% ]    ) , for the 14 winning coalitions @xmath93, .. ,[5]$ ] ( first row ) @xmath94, .. ,[10]$ ] ( second row ) and @xmath95, .. ,[14]$ ] ( third row ) as a function of the @xmath96 input field , compare fig .",
    "[ figure_bars_pattern ] . for the response of the 14 winning coalitions with respect to the ten reference input patterns",
    "compare fig .",
    "[ figure_crp_graph ] .",
    "note that the receptive fields can be both positive as well as negative , see the color-coding.bottom : the geometry of the dhan layer as a linear chain .",
    "the winning coalitions @xmath97 $ ] ( @xmath98 ) are numerated and correspond here to two connected nearest - neighbor in a linear - chain layout .",
    ", scaledwidth=45.0% ]",
    "a cognitive system has its own internal dynamics and we studied here the interplay of these self - generated activity states , the time - series of winning coalitions , with the sensory input for the purpose of unsupervised feature extraction .",
    "we proposed learning to be autonomously activated during the transition from one winning coalition to the subsequent one .",
    "this general principle may be implemented algorithmically in various fashion .",
    "here we used a generalized neural net ( dhan - dense homogeneous associative net ) for the autonomous generation of a time series of associatively connected winning coalitions and controlled the unsupervised extraction of input - features by an autonomously generated diffusive learning signal .",
    "we tested the algorithm for the bars problem and found good and fast learning for the case of sparse temporal input .",
    "preliminary results indicate that the learning algorithm retains functionality under a wide range of conditions .",
    "we plan to extend the simulations to various forms of temporal inputs , especially to quasi - continuous input and to natural scene analysis and to study the embedding of the here proposed concept within the framework of a full - fledged and autonomously active cognitive system .",
    "damoiseaux , s.a.r.b .",
    "rombouts , f. barkhof , p. scheltens , c.j .",
    "stam , s.m .",
    "smith and c.f .",
    "beckmann , consistent resting - state networks across healthy subjects , _ pnas _ , vol .",
    "103 , 2006 , pp 13848 - 13853 .                    c. gros , self - sustained thought processes in a dense associative network , _ in ki 2005 _ , u. furbach ( ed . ) , _ springer lecture notes in artificial intelligence _ ,",
    "vol . 3698 , 2005 , 366 - 379 ; also available as http://arxiv.org/abs/q-bio.nc/0508032 ."
  ],
  "abstract_text": [
    "<S> the activity patterns of highly developed cognitive systems like the human brain are dominated by autonomous dynamical processes , that is by a self - sustained activity which would be present even in the absence of external sensory stimuli .    during normal operation the continuous influx of external stimuli could therefore be completely unrelated to the patterns generated internally by the autonomous dynamical process . </S>",
    "<S> learning of spurious correlations between external stimuli and autonomously generated internal activity states needs therefore to be avoided .    </S>",
    "<S> we study this problem within the paradigm of transient state dynamics for the internal activity , that is for an autonomous activity characterized by a infinite time - series of transiently stable attractor states . </S>",
    "<S> we propose that external stimuli will be relevant during the sensitive periods , the transition period between one transient state and the subsequent semi - stable attractor . </S>",
    "<S> a diffusive learning signal is generated unsupervised whenever the stimulus influences the internal dynamics qualitatively .    </S>",
    "<S> for testing we have presented to the model system stimuli corresponding to the bar - stripes problem and found it capable to perform the required independent - component analysis on its own , all the time being continuously and autonomously active . </S>"
  ]
}