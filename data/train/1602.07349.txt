{
  "article_text": [
    "this paper addresses the following question : how can one construct , from a set of observations , the model that most meaningfully describes the underlying system ?    in the context of the present paper",
    "the ` model ' is the multivariate probability distribution that best describes the set of observations .",
    "the problem of finding such a distribution becomes particularly challenging when the number of variables , @xmath0 , is large and the number of observations , @xmath1 , is small . indeed , in such a multivariate problem , the model must take into account at least an order @xmath2 of interrelations between the variables and therefore the number of model - parameters scales at least quadratically with the number of variables .",
    "a parsimonious approach requires to discover the model that best reproduces the statistical properties of the observations while keeping the number of parameters as small as possible .",
    "using a maximum entropy approach , up to the second order in the moments of the distribution , the model becomes the multivariate normal distribution . in the multivariate normal case",
    "there is a simple relationship between the sparsity pattern of the inverse of the covariance matrix ( the precision matrix , henceforth denoted by @xmath3 ) and the underlying partial correlation structure ( referred to as ` graphical model ' in the literature @xcite ) : two nodes @xmath4 and @xmath5 are linked in the graphical model if and only if the corresponding precision matrix element @xmath6 is different from zero .",
    "therefore the problem of estimating a sparse precision matrix is equivalent to the problem of learning a sparse multivariate normal graphical model ( known in the literature as gaussian markov random field ( gmrf ) @xcite ) . once the sparse precision matrix has been estimated , a number of efficient tools  mostly based on research in sparse numerical linear algebra  can be used to sample from the distribution , calculate conditional probabilities , calculate conditional statistics and forecast @xcite .",
    "gmrfs are of great importance in many applications spanning computer vision @xcite , sparse sensing @xcite , finance @xcite , gene expression @xcite ; biological neural networks @xcite , climate networks @xcite ; geostatistics and spatial statistics @xcite . almost universally , applications require modelling a large number of variables with a relatively small number of observations and therefore the issue of the statistical significance of the model parameters is very important .",
    "the problem of finding meaningful and parsimonious models , sometimes referred as sparse structure learning @xcite , has been tackled by using a number of different approaches .",
    "let us hereafter briefly account for some of the most relevant in the present context .",
    "_ constraint based approaches _ recover the structure of the network by testing the local markov property .",
    "usually the algorithm starts from a complete model and adopts a _ backward selection _",
    "approach by testing the independence of nodes conditioned on subsets of the remaining nodes ( algorithms sgs and pc @xcite ) and removing edges associated to nodes that are conditionally independent ; the algorithm stops when some criteria are met ",
    "e.g. every node has less than a given number of neighbours .",
    "conversely _ forward selection _",
    "algorithms start from a sparse model and add edges associated to nodes that are discovered to be conditionally dependent .",
    "an hybrid model is the gs algorithm where candidate edges are added to the model ( the `` grow '' step ) in a forward selection phase and subsequently reduced using a backward selection step ( the `` shrinkage '' step ) @xcite .",
    "however , the complexity of checking a large number of conditional independence statements makes these methods unsuitable for moderately large graphs .",
    "furthermore , aside from the complexity of measuring conditional independence , these methods do not generally optimize a global function , such as likelihood or the akaike information criterion @xcite but they rather try to exhaustively test all the ( local ) conditional independence properties of a set of data and therefore are difficult to use in a probabilistic framework .",
    "_ score based approaches _ learn the inference structure trying to optimize some global function : likelihood , kullback - leibler divergence @xcite , bayesian information criterion ( bic ) @xcite , minimum description length @xcite or the likelihood ratio test statistics @xcite . in these approaches ,",
    "the main issue is that the optimization is generally computationally demanding and some sort of greedy approach is required . in the field of _ decomposable models",
    "_ there are a number of methods that efficiently explore the graphical structure ( directed , in the case of bayesian models , or undirected in the case of log - linear or multivariate gaussian models ) by using advanced graph structures such as junction tree or clique graph @xcite , with the goal of producing sparse models ( so - called `` thin junction trees '' @xcite ) .",
    "other approaches @xcite treat the problem as a constrained optimization problem to recover the sparse covariance matrix . within this line ,",
    "_ regression based approaches _ generally try to minimize some loss function which enforces parsimony and sparsity by using penalization to constrain the number and size of the regression parameters .",
    "specifically ridge regression uses a @xmath7-norm penalty ; instead the _ lasso _ method @xcite uses an @xmath8-norm penalty and the elastic - net approach uses a convex combination of @xmath7 and @xmath8 penalties on the regression coefficients @xcite .",
    "these approaches are among the best performing regularization methodologies presently available . the @xmath8-norm penalty term favours solutions with parameters with zero value leading to models with sparse inverse covariances .",
    "sparsity is controlled by regularization parameters @xmath9 ; the larger the value of the parameters the more sparse the solution becomes .",
    "this approach has become extremely popular and , around the original idea , a large body of literature has been published with several novel algorithmic techniques that are continuously advancing this method @xcite among these the popular implementation _ glasso _ ( graphical - lasso ) @xcite which uses lasso to compute sparse graphical models . however , glasso methods are computationally intensive and , although they are sparse , the non - zero parameters interaction structure tends to be noisy and not significantly related with the true underlying interactions between the variables .",
    "recently , a new family of information filtering networks , the triangulated maximal planar graph ( tmfg ) @xcite , was introduced . these are planar graphs , similar to the pmfg , but with the advantages to be generated in a computationally efficient way and , more importantly for this paper , they are decomposable graphs ( see example in fig.[fig : cliquesandspearator ] ) .",
    "a decomposable graph has the property that every cycle of length greater than three has a chord , an edge that connects two vertices of the cycle in a smaller cycle of length three .",
    "decomposable graphs , also called chordal or triangulated , are clique forests , made of k - cliques ( complete sub graphs of k vertices ) connected by separators .",
    "separators are also cliques of smaller sizes with the property that the graph becomes divided into two or more disconnected components when the vertices of the separator are disconnected .",
    "for example , in the schematic representation of the tmfg reported in fig.[fig :",
    "cliquesandspearator ] the cliques are the tetrahedra @xmath10 and @xmath11 whereas the separator is the triangle @xmath12 .    the novelty of the method presented in this paper is the combination of decomposable information filtering networks @xcite with gaussian markov random fields @xcite to produce parsimonious models associated with a meaningful structure of dependency between the variables .",
    "the strength of this methodology is that the global sparse inverse covariance matrix is produced from a simple sum of local inversions .",
    "this makes the method computationally very efficient and statistically robust .",
    "given the local / global nature of its construction , in the following we shall refer to this method as _",
    "logo_.    in this paper , we demonstrate that the structure provided by information filtering networks is also extremely effective to generate high - likelihood sparse probabilistic models . in the linear case",
    ", the logo sparse inverse covariance has only @xmath13 parameters but , despite its sparsity , the associated multivariate normal distribution can still retrieve high likelihood values yielding , , comparable or better results than state - of - the - art glasso penalized inversions .    the rest of the paper is organized as follows : in section [ sec : methods ] we describe our methodology providing algorithms for sparse covariance inversion for two graph topologies : mst and the tmfg .",
    "we then show in section [ sec : results ] that our method yields comparable or better results in maximum likelihood compared to lasso - type and ridge regression estimates of the inverse covariances .",
    "subsequently we discuss how our approach can be used for time series prediction , financial stress testing and risk allocation . with section [ sec : conclusions ]",
    "we end with possible extensions for future work and conclusive remarks .     and @xmath11 and the separator is the triangle @xmath12 .",
    "this is a chordal graph .",
    ", scaledwidth=70.0% ]     and @xmath14 with a separating set @xmath15 .",
    "variables within sets @xmath16 or @xmath14 are assumed conditionally dependent whereas variables belonging to the two separated sets @xmath17 and @xmath18 are assumed independent conditionally to @xmath15 . by combining the two formulas one obtains : @xmath19.,scaledwidth=40.0% ]",
    "let us start by demonstrating how a decomposable information filtering network can be associated with a convenient factorization of the multivariate probability distribution .",
    "let us consider , in general , two sets @xmath16 and @xmath14 of variables with non empty intersection @xmath20 .",
    "let us also assume that the variables are mutually dependent within their own ensemble @xmath16 or @xmath14 but when one variable belongs to set @xmath17 and the other variable belongs to set @xmath18 , then they are independent conditioned to @xmath21 .",
    "we can now use the bayes formula : @xmath22 where @xmath23 is the joint probability distribution function of all variable in @xmath16 and @xmath14 , @xmath24 is the conditional probability distribution function for the variables in @xmath16 minus the subset in common with @xmath14 conditioned to all variables in @xmath14 and @xmath25 is the marginal probability distribution function of all variables in @xmath14 ( see fig.[fig : ensambles ] ) . from the bayes formula",
    "we also have the following identity : @xmath26 that combined with the previous gives the following factorization for the joint probability distribution function of all variable in @xmath16 and @xmath14 @xcite : @xmath27 let us now apply this formula to a set of variables associated with a decomposable information filtering network @xmath28 made of @xmath29 cliques , @xmath30 , with @xmath31 and @xmath32 complete separators @xmath33 , with @xmath34 .",
    "in such a network , the vertices represent the @xmath0 variables @xmath35 and the edges represent couples of _ conditionally _ dependent variables ( condition being with respect to all other variables ) .",
    "conversely , variables which are not directly connected with a network edge are conditionally independent .",
    "given such a network , in the same way as for eq.[eq : bayes ] , one can write the joint probability density function for the set of @xmath0 variables @xmath36 in terms of the following factorization into cliques and separators @xcite : @xmath37 where @xmath38 and @xmath39 are respectively the marginal probability density functions of the variables constituting @xmath40 and @xmath41 @xcite . the term @xmath42 counts the number of disconnected components produced by removing the separator @xmath41 and it is therefore the degree of the separator in the clique tree . given the graph @xmath28 , eq.[eq : factorizing ] is exact , it is a direct consequence of the bayes formula and it is therefore very general .",
    "we search for the functional form of the multivariate probability distribution function , @xmath43 . to find the functional form of the distribution @xmath44 and the values of its parameters @xmath45 , we use the maximum entropy method @xcite which constrains the model to have some given expectation values while maximising the overall information entropy @xmath46 . at the second order , the model distribution that maximizes entropy",
    "while constraining moments at given values is : @xmath47 where @xmath48 is the vector of expectation values with coefficients @xmath49}$ ] and @xmath50 are the matrix elements of @xmath51 .",
    "they are the lagrange multipliers associated with the second moments of the distribution @xmath52}=\\sigma_{i , j}$ ] which are the coefficients of the covariance matrix @xmath53 of the set of @xmath0 variables @xmath54 .",
    "it is clear that eq.[maxent ] is a multivariate normal distribution with @xmath55 .",
    "if we require the model @xmath43 to reproduce exactly all second moments @xmath56 , then the solution for the distribution parameters is @xmath57 .",
    "therefore , in order to construct the model , one could estimate empirically the covariance matrix @xmath58 from a set of @xmath1 observations and then invert it in order to estimate the inverse covariance .",
    "however , in the case when the observation length @xmath1 is smaller than the number of variables @xmath0 the empirical estimate of the covariance matrix @xmath58 can not be inverted .",
    "furthermore , also in the case when @xmath59 , such a model has @xmath60 parameters and this might be an overfitting solution describing noise instead of the underlying relationships between the variables resulting in poor predictive power @xcite .",
    "indeed , we shall see in the following that , when uncertainty is large ( @xmath1 small ) , models with a smaller number of parameters can have stronger predictive power and can better describe the statistical variability of the data @xcite . here , we consider a parsimonious modelling that fixes only a selected number of second moments and leaves the others unconstrained .",
    "this corresponds to model the multivariate distribution by using a _ sparse inverse covariance _ where the unconstrained moments are associated with zero coefficients in the inverse .",
    "let us note that this in turns implies zero partial correlation between the corresponding couples of variables .      from eq.[eq",
    ": factorizing ] it follows that , in the case of the multivariate normal distribution , the network @xmath28 coincides with the structure of non - zero coefficients , @xmath61 in eq.[maxent ] and their values can be computed from the local inversions of the covariance matrices respectively associated with the cliques and separators @xcite : @xmath62 and @xmath63 if @xmath64 are not both part of a common clique . + this is a very simple formula that reduces the global problem of a @xmath65 matrix inversion into a sum of local inversions of matrices of the sizes of the cliques and separators ( no more than 3 and 4 in the case of tmfg graphs @xcite ) .",
    "this means that , for tmfg graphs , only @xmath66 observations would be enough to produce a non - singular global estimate of the inverse covariance .",
    "an example illustrating this inversion procedure is provided in fig.[gdecom ] .",
    "planar graph made of 4-cliques separated by 3-cliques : +   variables associated with a decomposable tmfg graph with @xmath67 cliques and @xmath68 separators .",
    "[ gdecom],title=\"fig : \" ]   variables associated with a decomposable tmfg graph with @xmath67 cliques and @xmath68 separators .",
    "[ gdecom],title=\"fig : \" ] + example for the computation of element @xmath69 of the inverse covariance : @xmath70      we are now facing two related problems : 1 ) how to choose the moments to retain ?",
    "2 ) how to verify that the parsimonious model is describing well the statistical properties of the system of variables ? the solutions of these two problems are related because we aim to develop a methodology that chooses the non - zero elements of the inverse covariance in such a way as to best model the statistical properties of the real system under observation . in order to construct a model that is closest to the real phenomenon we search for the set of parameters , @xmath45 , associated with the largest likelihood , i.e. with the largest probability of observing the actual observations : @xmath71 , @xmath72 .... @xmath73 .",
    "the logarithm of the likelihood from a model distribution function , @xmath74 ( eq.[maxent ] ) , with parameters @xmath45 , is associated to the empirical estimate of the covariance matrix , @xmath75 , by @xcite : @xmath76    the network @xmath28 that we aim to discover must be associated with largest log - likelihood and it can be constructed in a greedy way by adding in subsequent steps elements with maximal log - likelihood . in this paper",
    "we propose two constructions : 1 ) the maximum spanning tree ( mst ) @xcite , which builds a spanning tree which maximises the sum of edge weights ; 2 ) a variant of the tmfg @xcite , which builds a planar graph that aims to maximize the sum of edge weights . in both cases edge weights",
    "are associated with the log - likelihood .",
    "one can show that for all decomposable graphs , following eq .",
    "[ eq : locainversion ] , the middle term in eq .",
    "[ eq : gaussianloglikelihood ] is : @xmath77 .",
    "hence , to maximize log - likelihood , only @xmath78 must be maximized ; from eq.[eq : factorizing ] , this is @xcite :    @xmath79    for the logo - mst , the construction is simplified because in a tree the cliques are the edges @xmath80 , the separators are the non - leaf vertices @xmath81 and @xmath82 are the vertex degrees .",
    "in this case eq.[eq : detlikelihoodlogo ] becomes @xmath83 , with @xmath84 the sample variance of variable ` @xmath4 ' .",
    "given that @xmath85 , with @xmath86 the pearson s correlation matrix element @xmath87 , then @xmath88 .",
    "therefore , the mst can be built through the standard prim s @xcite or kruskal s @xcite algorithms from a matrix of weights @xmath89 with coefficients @xmath90 .",
    "the _ logo - mst _ inverse covariance estimation , @xmath3 , is then computed by the local inversion , eq.[eq : locainversion ] , on the mst structure .",
    "note that the mst structure depends only on the correlations not the covariance .    ) inside triangular faces ( e.g. @xmath91 ) maximising the ratio of the determinants between separator and clique @xmath92 .",
    "this move generates a new 4-clique @xmath93 and transforms the triangular face into a separator @xmath94.,scaledwidth=70.0% ]    the logo - tmfg construction requires a specifically designed procedure .",
    "also in this case , only correlations matter ; indeed , the structure of the inverse covariance network reflects the partial correlations i.e. the correlation between two variables given all others .",
    "logo - tmfg starts with a tetrahedron , @xmath95 , with smallest correlation determinant @xmath96 and then iteratively introduces inside existing triangular faces the vertex that maximizes @xmath97 where @xmath98 and @xmath99 are the new clique and separator created by the vertex insertion .",
    "the _ logo - tmfg _ procedure is schematically reported in algorithm [ alg:4cliquetree ] and in fig .  [",
    "fig : tmfg ] .",
    "the tmfg is a computationally efficient algorithm @xcite that produces a decomposable ( chordal ) graph , with @xmath100 edges , which is a clique - tree constituted by four - cliques connected with three - cliques separators .",
    "note that for tmfg @xmath101 always .",
    "@xmath102 initialize @xmath103 with zero elements @xmath104 tetrahedron , @xmath105 , with smallest @xmath96 @xmath106 assign to @xmath107 the four triangular faces in @xmath108 : @xmath91 , @xmath109 , @xmath110 , @xmath111 @xmath112 assign to the remaining @xmath113 vertices not in @xmath108 @xmath114    let us note that by expanding to the second order in the correlation coefficients , the logarithms of the determinants are well approximated by a constant minus the sum of the square correlation coefficients associated with the edges in the cliques or separators .",
    "this can simplify the algorithm and the tmfg could be simply computed from a set of weights given by the squared correlation coefficients matrix , as described in @xcite .",
    "further , let us note that , for simplicity , in this paper we only consider likelihood maximization",
    ". a natural , straightforward extension of the present work is to consider akaike information criterion @xcite instead .",
    "however , we have verified that , for the cases studied in this paper the two approaches give very similar results .",
    "off - sample test data - series of different lengths @xmath1 varying from 20 to 2000 .",
    "the inverse matrices are computed on training datasets of the same length .",
    "data are log - returns sampled from 342 stocks prices of equities traded on the us market during the period 1997 - 2012 .",
    "the statistics is made stationary by random shuffling the time order .",
    "symbols correspond to averages over 100 samples generated by picking at random 300 series over the 342 and assigning training and testing sets by choosing at random two non - overlapping time - windows of length @xmath1 , the shaded bands are the 95% quantiles .",
    "the line on the top , labelled with ` max ' , is the theoretical maximum which is the log - likelihood obtained from the inverse covariance of the testing set .",
    ", scaledwidth=65.0% ]     compared with log - likelihood from state - of - the - art glasso @xmath8 penalized sparse inverse covariance models ( cross validated , g - lasso - cv , and of the same sparsity of tmfg , g - lasso - sp ) and ridge @xmath7 penalized inverse model ( ridge ) .",
    ", scaledwidth=65.0% ]         for glasso and for logo - tmfg .",
    "the bottom plot reports the fraction of computational time vs. the fraction of non zero off - diagonal coefficients of @xmath3 for glasso and for logo - tmfg .",
    "the measures are on @xmath115 off - sample test data - series of length @xmath116 .",
    "inverse matrices are computed on training datasets of the same length .",
    "data are log - returns sampled from 342 stocks prices of equities traded on the us market during the period 1997 - 2012 .",
    "the statistics is made stationary by random shuffling the time order .",
    "symbols correspond to averages over 100 samples generated by picking at random 300 series over the 342 and assigning training and testing sets by choosing at random two non - overlapping time - windows of length @xmath1 , the shaded bands are the 95% quantiles .",
    ", scaledwidth=65.0% ]    [ fig.s1 ]",
    "we investigated stock prices time series from a us equity market computing the daily log - returns ( @xmath117 with @xmath118 and @xmath119 with @xmath120 days during a period of 15 years from 1997 to 2012 @xcite ) .",
    "we build 100 different datasets by creating stationary time series of different lengths selecting returns at random points in time and randomly picking @xmath115 series out of the 342 in total .",
    "each dataset was divided into two temporal non - overlapping windows with @xmath1 elements constituting the ` training set ' and other @xmath1 elements the ` testing set ' .    in order to quantify the goodness of the methodology we computed the log likelihood of the testing dataset using the inverse covariance estimates from the training set .",
    "larger log likelihood indicate models that better describe the testing data .",
    "figure [ fig : ll_comparisonwithinverse ] reports the results for time series of different lengths from @xmath121 to @xmath122 .",
    "note that , the green upward triangles in fig .  [ fig : ll_comparisonwithinverse ] , denoted with _ max _",
    ", are the theoretical maximum from the inverse sample covariance matrix calculated on the testing set which is reported as a reference indicating the upper value for the attainable likelihood .",
    "let us first observe from this figure that , _ logo - tmfg _ outperforms the likelihood from the inverse covariance solution @xmath123 ( denoted with ` inv . cov . ' in fig .",
    "[ fig : ll_comparisonwithinverse ] ) . for @xmath124",
    "the inverse covariance is not computable and therefore comparison can not be made ; when @xmath125 , the inverse covariance is computable but it performs very poorly for small sample sizes @xmath126 becoming comparable to _ logo - tmfg _ only after @xmath127 with both approaching the theoretical maxima at @xmath128 .",
    "note that also _ logo - mst _ outperforms the inverse covariance solution in most of the range of @xmath1 .",
    "we then compared the log - likelihood from _ logo - mst _ and _ logo - tmfg _ sparse inverse covariance with state - of - the - art glasso @xmath8-norm penalized sparse inverse covariance models ( using the implementation by @xcite ) and ridge @xmath7-norm penalized inverse model .",
    "glasso method depends on the regularization parameters which were estimated by using two standard methods : i ) _ g - lasso - cv _ uses a two - fold cross validation method @xcite ; ii ) _ g - lasso - sp _ fixes the regularization parameter to the value that creates in the training set a sparse inverse with sparsity equal to _ logo - tmfg _ network ( @xmath100 parameters ) .",
    "_ ridge _ inverse penalization parameter was also computed by cross validation method @xcite .",
    "fig.[fig : ll_comparisonwithglasso ] reports a comparison between these methods for various values of @xmath1 .",
    "we can observe that _ logo - tmfg _ outperforms the glasso methods achieving larger likelihood from @xmath129 .",
    "results are detailed in table  [ tab : realdatashuff ] where we compare also with the null model ( ` null ' ) which is a completely disconnected network corresponding to a diagonal @xmath45 .",
    "logo models can achieve better performances than glasso models with fewer coefficients and are computationally more efficient .",
    "this is shown in fig.[fig : ll_comparisonwithglasso_spersityandtime ] where we report the comparison between the number of non - zero off - diagonal coefficients in the precision matrix @xmath45 in glasso - cv and logo models .",
    "these results shows that the number of coefficients for g - lasso - cv is 3 to 6 times larger than for logo - tmfg while the computation time for logo - tmfg is about three order of magnitude smaller than the computation time for g - lasso - cv .",
    "note that logo - tmfg has a constant number of coefficients equal to @xmath130 corresponding to the number of edges in the tmfg network . a further comparison between performance , sparsity and execution time",
    "is provided in fig .",
    "[ fig : ll_comparisonwithglasso_sparsityandtime2 ] where the top plot reports the fraction of log - likelihood for glasso ( implemented by using @xcite ) and for logo - tmfg vs. the fraction of non zero off - diagonal coefficients of @xmath3 for data series lengths of @xmath116 . in the bottom plot",
    "we report instead the fraction of computational time vs. the fraction of non zero off - diagonal coefficients of @xmath3 for glasso and for logo - tmfg .",
    "we can observe that at the same sparsity ( value 1 in the x - axis indicated with the vertical line ) glasso underperforms logo by 10% and glasso is about 50 times slower than logo on the same machine .",
    "we verified that eventually the maximum performance of glasso can become 1.5% better than logo but with 10 times more parameters and computation time 2,000 times longer .",
    "let us note that , in this example , the best glasso performance are measured a - posteriori on the testing set , they are therefore hypothetical maxima which can not be reached with cross validation methods that instead result in average performances of a few percent inferior to logo ( see fig.[fig : ll_comparisonwithglasso ] ) .",
    "all these results refer to the same simulations as for the results in figs.[fig : ll_comparisonwithinverse ] .",
    "the computation time of logo can be decreased even further by parallelising the algorithm .",
    "the previous results are for time series made stationary by random selecting log - returns at different times . in practice , financial time series - and other real world signals - are non - stationary having statistical properties that change with time .",
    "table  [ tab : realdata ] reports the same analysis as in fig.[fig : ll_comparisonwithglasso ] and table  [ tab : realdatashuff ] but with datasets taken in temporal sequence with the training set being the @xmath1 data points preceding the testing set .",
    "let us note that , considering the time - period analysed , in the case of large @xmath1 , the training set has most data points in the period preceding the 2007 - 2008 financial crisis whereas the testing set has data in the period following the crisis . nonetheless , we see that the results are comparable with the one obtained for the stationary case .",
    "surprisingly , we observe that for relatively small time - series lengths the values of the log - likelihood achieved by the various models is larger than in the stationary case .",
    "this counter - intuitive fact can be explained by the larger temporal persistence of real data with respect to the randomized series .",
    "we also investigated artificial datasets of @xmath115 multivariate variables generated from factor models respectively with 3 and 30 common factors .",
    "results for the average log - likelihood and the standard deviations computed over 100 samples at different values of @xmath1 are reported in table  [ tab : factormodels ] .",
    "note that by increasing the number of factors performances of all models become worse .",
    ".[tab : realdatashuff ] * demonstration that logo sparse modelling has better or comparable performances that state - of - the - art models . * comparison between log - likelihood for logo , glasso , ridge , complete inverse and null models .",
    "measures are on @xmath115 off - sample test data - series of lengths @xmath131 .",
    "data are the same as in fig.[fig : ll_comparisonwithglasso ] : log - returns sampled from 342 stocks prices made stationary by random shuffling the time order in the time - series .",
    "the values reported are the averages of 100 samples and the standard deviations are reported between brackets . ` max ' , is the theoretical maximum log - likelihood obtained from the inverse covariance of the testing set . [ cols=\"<,<,<,<,<,<,<\",options=\"header \" , ]      +      logo estimates the joint probability distribution function yielding the set of parameters for the model system s dependency structure .",
    "the conditional expectation values @xmath132 can be calculated from the conditional joint distribution function , which , from the bayes theorem , is @xmath133 . from this expression one",
    "obtains : @xmath134 where we have written the precision matrix @xmath3 as a block matrix @xmath135 whith @xmath136 being the @xmath137 part of the precision matrix in the lower right and @xmath138 the @xmath139 part of the precision matrix in the lower left .",
    "different way to write the linear regression which , in a more conventional form , reads : @xmath140 with the coefficients @xmath141 and the residuals given by @xmath142 .    indeed , the expected conditional fluctuations of the variables @xmath143 are quantified by the conditional covariance : @xmath144 which involves the term @xmath136 only , which therefore describes propagation of uncertanty across variables .",
    "a typical stress test for financial institutions , required by regulatory bodies , consists in forecasting the effect of severe financial and economic shocks on the balance sheet of a financial institution . in this context",
    "let s reformulate the previous results by considering @xmath145 the set of economic and financial variables that can be shocked and @xmath143 the set of the securities held in an institution s portfolio . assuming that all the changes in the economic and financial variables and in the assets of the portfolio can be modelled as a gmrf , then eq.[j ] represents the distribution of the returns of the portfolio ( @xmath143 ) conditional on the realization of the economic and financial shocks ( @xmath145 ) .",
    "an approach along similar lines was proposed in @xcite .",
    "we note that with the logo approach we have a sparse relationship between the financial variables and the securities .",
    "this makes calibration more robust and it can be insightful to identify mechanisms of impact and vulnerabilities .",
    "a second application is the calculation of conditional statistics in the presence of linear constraints ( see @xcite ) . in this case",
    "we indicate with @xmath54 a set of @xmath0 random variables associated with the returns in a portfolio of @xmath0 assets and with @xmath3 the associated sparse inverse covariance matrix .",
    "let @xmath146 be the vector of holdings of the portfolio , then @xmath147 is the return of the portfolio .",
    "an important question in portfolio management is to allocate profits and losses to different assets conditional on a given level of profit or loss , which is equivalent to knowing the distribution of returns conditional on a given level of loss @xmath148 .",
    "more generally we want to estimate @xmath149 where @xmath150 is generally a low rank @xmath151 ( @xmath152 in our example ) matrix that specifies @xmath151 hard linear constraints . using the lagrange multipliers method ( see @xcite for an introduction ) the conditional mean is calculated as ( @xcite ) : @xmath153 and the conditional covariance is : @xmath154    in case @xmath3 is estimated using decomposable information filtering networks ( mst or tmfg ) then it can be written as a sum of smaller matrices ( as in algorithm [ alg:4cliquetree ] ) involving cliques and separators :    @xmath155    this decomposition allows for a sparse and potentially parallel evaluation of the matrix products in eqs .  [ eq : condmean ] and [ eq : condvariance ] .",
    "this framework can therefore be used to build the profit / loss ( p / l ) distribution of a portfolio , conditionally on a number of explanatory variables , and to allocate the p / l to the different assets conditional on the realization of a given level of profit and loss .",
    "the solution is analytical and therefore extremely quick .",
    "besides , given the decomposability of the portfolio , eq.[eq : j_decomposition ] allows to calculate important statistics in parallel , by applying the calculations locally to the cliques and to the separators .",
    "for instance , it is a simple exercise to show that the unconditional expected p / l and the unconditional volatility can be calculated in parallel by adding the contributions of the cliques and subtracting the contributions of the separators . in summary logo provides the possibility to build a basic risk management framework that allows risk aggregation , allocation , stress testing and scenario analysis in a multivariate gaussian framework in a quick and potentially parallel fashion .",
    "we have introduced a methodology , logo , that makes use of information filtering networks to    logo produces high - dimensional sparse inverse covariances by using low - dimensional local inversions only , making the procedure computationally very efficient and little sensitive to the curse of dimensionality . the construction through a sum of local inversion , which is at the basis of logo , makes this method very suitable for parallel computing and dynamical adaptation by local , partial updating .",
    "we discussed the wide potential applicability of logo for sparse inverse covariance estimation , for sparse forecasting models and for financial applications such as stress - testing and risk allocation .    by comparing the results with a state - of - the - art glasso procedure we , in a fraction of the computation time , equivalent or better results with a sparser network structure .",
    "the model introduced in this paper is a second - order solution of the maximum entropy problem , resulting in linear , normal multivariate modelling .",
    "it is however well known that many real systems follow non - linear probability distributions .",
    "linearity is a severe limitation which can however be overcome in several ways .",
    "for instance , logo can be extended to a much broader class of non - linear models by using the so - called kernel trick @xcite .",
    "other generalisations to non - linear transelliptical models @xcite can also be implemented .",
    "these would be however , the topics of future works .",
    "t.a . acknowledges support of the uk economic and social research council ( esrc ) in funding the systemic risk centre ( es / k002309/1 ) .",
    "tdm wishes to thank the cost action td1210 for partially supporting this work .",
    "we acknowledge bloomberg for providing the data .",
    "we investigated the comparison between logo and state - of - the - art sparse glasso model from @xcite for the same financial data used for figs.[fig : ll_comparisonwithglasso ] and tab.[tab : realdatashuff ] but using the real temporal sequence of the financial data .",
    "these sequences are non - stationary having varying statistical properties across the time windows .",
    "this unavoidably must affect the capability of the model to describe statistically test data from the study of the training data being the two associated with different temporal states where different events affect the market dynamics .",
    "results for the log - likelihood are reported in fig.[fig : ll_comparisonwithinversereal ] , these are the same results reported in tab.[tab : realdata ] . by comparing with fig.[fig : ll_comparisonwithglasso ] we observe a much greater overall noise with an interesting collapse of performances with similar values for all models .",
    "we also observe larger log - likelihoods for shorter time - window .",
    "this fact is commented in section [ sec : results ] .",
    "let us notice that these results are in agreement with the finding for the stationary case with logo still performing better or comparably well with respect to glasso - cv .",
    "computational times and sparsity value are very similar to what reported for the stationary case .",
    "plots for the log - likelihood vs. the time series length for factor models are reported in fig.[fig : ll_comparisonwithinversefactor ] which correspond to the results reported in the paper in tab.[tab : realdata ] .",
    "we observe that logo still performs well and relatively similar to the real data performances when the number of factors is small .",
    "conversely , when the number of factors increases logo underperforms even the inverse correlation for @xmath156 .",
    "let us note that logo is still over performing the glasso with the same number of parameters .",
    "off - sample test data - series of different lengths @xmath1 varying from 20 to 2000 .",
    "inverse matrices are computed on training datasets of the same length .",
    "data are log - returns sampled from 342 stocks prices of equities traded on the us market during the period 1997 - 2012 .",
    "symbols correspond to averages over 100 samples generated by picking at random 300 series over the 342 and assigning training and testing sets by choosing at random two consecutive non - overlapping time - windows of length @xmath1 , the shaded bands are the 95% quantiles . testing set is the time - window preceding the training set .",
    "the line on the top , labelled with ` max ' , is the theoretical maximum which is the log - likelihood obtained from the inverse covariance of the testing set .",
    ", scaledwidth=70.0% ]       off - sample test data - series of different lengths @xmath1 varying from 20 to 2000 .",
    "inverse matrices are computed on training datasets of the same length .",
    "plots refer to sparse factor models simulations with 3 ( top ) and 30 ( bottom ) factors respectively .",
    "symbols correspond to averages over 100 samples generated by picking at random 300 series over the 342 and assigning training and testing sets by choosing at random two consecutive non - overlapping time - windows of length @xmath1 , the shaded bands are the 95% quantiles . testing set is the time - window preceding the training set .",
    "the line on the top , labelled with ` max ' , is the theoretical maximum which is the log - likelihood obtained from the inverse covariance of the testing set . , title=\"fig:\",scaledwidth=70.0% ] +   off - sample test data - series of different lengths @xmath1 varying from 20 to 2000 .",
    "inverse matrices are computed on training datasets of the same length .",
    "plots refer to sparse factor models simulations with 3 ( top ) and 30 ( bottom ) factors respectively .",
    "symbols correspond to averages over 100 samples generated by picking at random 300 series over the 342 and assigning training and testing sets by choosing at random two consecutive non - overlapping time - windows of length @xmath1 , the shaded bands are the 95% quantiles . testing set is the time - window preceding the training set .",
    "the line on the top , labelled with ` max ' , is the theoretical maximum which is the log - likelihood obtained from the inverse covariance of the testing set .",
    ", title=\"fig:\",scaledwidth=70.0% ]"
  ],
  "abstract_text": [
    "<S> we introduce a methodology to construct models . </S>",
    "<S> this method the global sparse inverse covariance from a simple sum of local inverse covariances computed on small sub - parts of the network . </S>",
    "<S> being based on local and low - dimensional inversions , this method is computationally very efficient and statistically robust even for the estimation of inverse covariance of high - dimensional , noisy and short time - series . </S>",
    "<S> the local nature of this approach allows us to perform computations in parallel and provides a tool for dynamical adaptation by partial updating when the properties of some variables change without the need of recomputing the whole model . </S>",
    "<S> this makes this approach particularly suitable to handle big datasets with large numbers of variables . </S>",
    "<S> .5 cm * keywords : * complex systems , econophysics , information filtering , sparse inverse covariance , probabilistic modelling , graphical modelling , glasso </S>"
  ]
}