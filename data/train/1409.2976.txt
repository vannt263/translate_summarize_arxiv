{
  "article_text": [
    "controlling complex quantum dynamics is a recurring theme in many different areas of amo physics and physical chemistry .",
    "recent examples include quantum state preparation  @xcite , interferometry  @xcite and imaging  @xcite , or reaction control  @xcite .",
    "the central idea of quantum control is to employ external fields to steer the dynamics in a desired way  @xcite .",
    "the fields that realize the desired dynamics can be determined by optimal control theory ( oct ) @xcite .",
    "an expectation value that encodes the target is then taken to be a functional of the external field which is minimized or maximized .",
    "the target can be simply a desired final state  @xcite , or a unitary operator  @xcite , a prescribed value of energy or position  @xcite , or an experimental signal such as a pump - probe trace  @xcite .",
    "the algorithms that can be employed for optimizing the target functional broadly fall into two categories  those where changes in the field are determined solely by evaluating the functional , such as simplex algorithms  @xcite , and those that utilize derivative information , such as krotov s method  @xcite or gradient ascent pulse engineering ( grape )  @xcite , possibly combined with quasi - newton methods  @xcite .",
    "the solutions that one obtains typically do not only depend on the target functional but also on the specific algorithm that is employed and the initial guess field .",
    "this is due to the fact that numerical optimization is always a local search which may find one of possibly many optimal solutions or get stuck in a local extremum .",
    "it is thus important to understand which features of an optimal control solution are due to the optimization procedure and which reflect truly physical properties of the quantum system .    for example , when seeking to identify , by use of optimal control theory , the quantum speed limit , i.e. , the shortest possible time in which a quantum operation can be carried out  @xcite , the answer should be independent of the algorithm",
    "moreover , in view of employing calculated solutions in an experiment , conditions such as limited power , limited time resolution , or limited bandwidth need to be met .",
    "the way in which the various optimization approaches can accommodate such requirements differ greatly .    here , we study control of a bose - einstein condensate in a magnetic microtrap , comparing several variants of a grape - type algorithm  @xcite with krotov s method  @xcite .",
    "we consider two targets  splitting the condensate , which resides initially in the ground state of a single well , into a double well , without inducing excitation , and exciting the condensate from the ground to the first excited state of a single well .",
    "the latter is important for stimulated processes in matter waves , whereas the former presents a crucial step in interferometry  @xcite .",
    "a challenging aspect of controlling a condensate is the non - linearity of the equation of motion which can compromise or even prevent convergence of the optimization  @xcite .",
    "the two methods tackle this problem in different ways , grape by computing the search direction for new control fields within the framework of lagrange parameters and submitting the optimal control search to generic minimization routines  @xcite , krotov s method by accounting for the non - linearity of the equations of motion in the monotonicity conditions when constructing the algorithm  @xcite .",
    "furthermore , the methods differ in the way in which additional requirements such as smoothness of the control can be accounted for .",
    "we compare the two optimization approaches with respect to the solutions they yield as well as their performance , and ease of use .",
    "our study extends an earlier comparison of grape - type algorithms with krotov s method  @xcite that was concerned with the linear schrdinger equation and with finite - size ( spin - type ) quantum systems .",
    "our paper is organized as follows : after introducing the equation of motion for the condensate dynamics together with the control targets in sec .  [",
    "sec : theory ] , we briefly review the two optimization schemes in sec .  [ sec : octmethods ] .",
    "section  [ sec : results ] presents our results for wavefunction splitting and shaking .",
    "moreover , we investigate the influence of the nonlinearity , the performance of the two algorithms , and the smoothness of the optimized control in secs .",
    "[ subsec : nonlin ] to  [ subsec : smooth ] .",
    "our conclusions are presented in sec .",
    "[ sec : conclusions ] .",
    "in this paper we consider a quasi-1d condensate residing in a magnetic confinement potential @xmath0 that can be controlled by some external _ control parameter _",
    "@xmath1 @xcite .",
    "we describe the condensate dynamics within the mean - field framework of the gross - pitaevskii equation , where @xmath2 is the condensate wavefunction , normalized to one , whose time evolution is governed by @xcite ( @xmath3 ) @xmath4 the first term on the right - hand side is the operator for the kinetic energy , the second one is the confinement potential , and the last term is the nonlinear atom - atom interaction in the mean field approximation .",
    "@xmath5 is the atom mass and @xmath6 is the strength of the nonlinear atom - atom interactions , which is related to the effective one - dimensional interaction strength @xmath7 and the number of atoms @xmath8 through @xmath9 @xcite .",
    "we can now formulate our optimal control problem .",
    "suppose that the condensate is initially described by the wavefunction @xmath10 and the potential is varied in the time interval @xmath11 $ ] .",
    "we are now seeking for an optimal time variation of @xmath1 that brings the terminal wavefunction @xmath12 as close as possible to a _ desired _",
    "wavefunction @xmath13 . to rate the success for a given control , we introduce the cost function @xmath14\\,,\\ ] ] which becomes zero when the terminal wavefunction matches the desired one up to an arbitrary phase .",
    "optimal control theory aims at a @xmath15 that minimizes eq .  .",
    "[ cols=\"<,^,^,^,^,^,^ , < \" , ]     in this paper , we apply two different optimal control approaches , namely a gradient ascent pulse engineering ( grape ) scheme  @xcite and krotov s method  @xcite , which will be discussed separately below .",
    "an overview of the control approaches is given in table  [ tab : algos ] .",
    "the grape scheme for bose - einstein condensates has been presented in detail elsewhere  @xcite , for this reason we only briefly introduce the working equations .",
    "experimentally , strong variations of the control parameter are difficult to achieve . therefore ,",
    "we add to the cost function an additional term @xcite , @xmath16 ^ 2 dt\\,.\\ ] ] mathematically , the additional term penalizes strong variations of the control parameter and is needed to make the oct problem well - posed  @xcite . through @xmath17",
    "it is possible to weight the relative importance of wavefunction matching and control smoothness . below we will set @xmath18 such that @xmath19 is dominated by the terminal cost @xmath20 .    in order to bring the system from the initial state @xmath21 to the terminal state @xmath22 we have to fulfill the gross - pitaevskii equation , which enters as a _ constraint _ in our optimization problem .",
    "the constrained optimization problem can be turned into an unconstrained one by means of lagrange multipliers @xmath23 , whose time evolution is governed by  @xcite @xmath24 subject to the terminal condition @xmath25 .",
    "the optimal control problem is then composed of the gross - pitaevskii equation   and eq .  , which must be fulfilled simultaneously together with @xcite @xmath26 for the optimal control .",
    "this expression differs from standard grape  @xcite and results from minimizing changes in the control , cf .",
    ".    this set of equations can be also employed for a non - optimal control where eq .   is not fulfilled . in this case",
    ".   is solved forwards in time and eq .   backwards in time , and the search direction @xmath27 for an improved control is calculated from one of the equations @xcite @xmath28}\\!\\!&=&\\!\\ !    -\\gamma\\ddot\\lambda-    { \\mathfrak{re}}\\,\\bigl < p\\bigr|\\frac{\\partial v}{\\partial\\lambda}\\bigl|\\psi\\bigr > \\,\\,\\ ,    \\mbox{for $ l^2 $ norm}\\label{eq : l2}\\qquad\\\\      -\\frac{d^2 } { dt^2}\\bigl[\\nabla_\\lambda j\\bigr]\\!\\!&=&\\!\\ !    -\\gamma\\ddot\\lambda-    { \\mathfrak{re}}\\,\\bigl < p\\bigr|\\frac{\\partial v}{\\partial\\lambda}\\bigl|\\psi\\bigr > \\,\\,\\ ,    \\mbox{for $ h^1 $ norm.}\\label{eq : h1}\\qquad\\end{aligned}\\ ] ] these two expressions are obtained by interpreting , on the right hand side of eq .  , the integral @xmath29 ^ 2\\,dt=\\langle \\dot\\lambda,\\dot\\lambda\\rangle_{l^2}= \\langle \\lambda,\\lambda\\rangle_{h^1}$ ] in terms of an @xmath30 or @xmath31 norm  @xcite .",
    "the @xmath31 norm implies that one additionally has to solve a poisson equation , see the derivative operator on the left hand side of eq .  .",
    "this generally results in a much smoother time dependence of the control parameters while the additional numerical effort for solving the poisson equation is negligible .",
    "as for an optimal control , both eqs .   and yield @xmath32 .    here",
    "we solve the optimal control equations using the matlab toolbox octbec @xcite .",
    "the ground and desired states of the gross - pitaevskii equation are computed using the optimal damping algorithm @xcite .",
    "the control parameters are obtained iteratively using either a conjugate gradient method ( grape grad ) , which only uses first order information , or a quasi - newton bfgs scheme  @xcite ( grape bfgs ) , which also takes into account second - order information via an approximated hessian . in both cases ,",
    "the optimization employs a line search to determine the optimal step size in the direction of a given gradient .",
    "the pulse update is calculated for all time points simultaneously , making the grape schemes _",
    "krotov s method  @xcite provides an alternative optimal control implementation .",
    "the main idea is to add to eq .",
    "a vanishing term  @xcite , which is chosen such that the minimum of the new function is also a minimum of @xmath19 .",
    "however , for non - optimal @xmath1 one can devise a scheme that always gives a new control corresponding to a lower cost function .",
    "thus , krotov s method leads to a monotonically convergent optimization algorithm that is expected to exhibit much faster convergence .",
    "our implementation closely follows refs .",
    "@xcite . specifically , the cost reads @xmath33 ^ 2}{s(t ) }   dt\\,,\\ ] ] where the reference field @xmath34 is typically chosen to be the control from the previous iteration  @xcite . the second term in eq .",
    "penalizes changes in the control from one iteration to the next , and ensures that as an optimum is approached the value of the functional is increasingly determined by only @xmath20 .",
    "@xmath35 is a shape function that controls the turning on and off of the control fields , @xmath36 is a step size parameter , and @xmath37 $ ] is bound between @xmath38 and @xmath39 .",
    "let @xmath40 and @xmath41 denote the wavefunction and control parameter , respectively , in the @xmath42th iteration of the optimal control loop .",
    "to get started , we first solve for an initial guess @xmath43 the gross - pitaevskii equation and the adjoint equation for the co - state @xmath44 , which is backward - propagated in time with the same terminal condition as in grape , in order to obtain @xmath45 and @xmath46 . in the next step ,",
    "we solve the gross - pitaevskii equation _ simultaneously _ with the equation for the new control field    @xmath47_{\\lambda^{(i+1)}(t ) }                          | \\psi^{(i+1)}(t ) }      +          { \\mathfrak{re}}\\ , \\frac{\\sigma(t)}{2i }              \\braket{\\delta \\psi(t)|                      \\left [                      \\frac{\\partial v}{\\partial \\lambda }                      \\right]_{\\lambda^{(i+1)}(t ) }                      | \\psi^{(i+1)}(t)}\\ , , \\ ] ]    where @xmath48 is obtained by propagating @xmath49 forward in time using the updated pulse .  .",
    "with this definition the adjoint equation and the terminal condition @xmath50 are the same for grape and krotov . as consequence , the scalar products on the right hand side of eq .",
    "involve the real rather than the imaginary part .",
    "] the fact that @xmath48 appears on the right hand side of the update equation implies that the update at a given time @xmath51 depends on the updates at all earlier times , making krotov s method _",
    "sequential_. this type of update makes it non - straightforward to include a cost term on the derivative of the control as in eq .  , since the derivative at a given time @xmath51 requires knowledge of past _ and _ future values of @xmath52 .    the last term in eq .",
    "with @xmath53 is generally needed to ensure convergence in presence of the nonlinear mean - field term @xmath54 of the gross - pitaevskii equation .",
    "convergence is achieved through a proper choice of @xmath55 @xcite . in this work",
    "we neglect this additional contribution for simplicity , as it is of only minor importance for the moderate @xmath6 values of our present concern .     for the grape and krotov optimizations , respectively .",
    "the potential is held constant after the terminal time @xmath56 ms .",
    "the dashed line shows the shape function @xmath57 of eq .",
    "used in our version of krotov s method , scaled by a factor of @xmath58 for better visibility .",
    "( b , c ) density plots of the condensate density @xmath59 during the splitting .",
    "the solid lines show the confinement potentials at three selected times and the time variation of the potential minima .",
    "( d , e ) terminal ( solid lines ) and desired ( dashed lines ) densities , which are indistinguishable . in the optimization we set @xmath60 and @xmath61 . ]    .",
    "for krotov one optimization iteration consists of a gross - pitaevskii solution , subject to eq .  , and a subsequent solution of the adjoint equation . in our simulations we use @xmath61 .",
    "the dashed lines report results of simulations with a larger nonlinearity @xmath62 hz . in the legend",
    "we report the @xmath6 values in units used in our simulations , with @xmath3 and time measured in milliseconds @xcite . ]",
    "the derivative @xmath63 in eq",
    ".   has to be computed for @xmath64 , thus leading to an implicit equation for @xmath64 .",
    "when @xmath36 is chosen sufficiently small , such that the control parameter varies only moderately from one iteration to the next , one can obtain the new control fields approximately from @xmath65_{\\lambda^{(i)}(t ) }        | \\psi^{(i+1)}(t )      } \\ , .",
    "\\end{split}\\ ] ] otherwise one can employ an iterative newton scheme for the calculation of @xmath64 , as briefly described in appendix  [ sec : newtonkrotov ] . in all our simulations we found eq .   to provide sufficiently accurate results .",
    "once the new wavefunctions @xmath48 and control parameters @xmath64 are computed , we get the adjoint variables @xmath66 through the solution of eq .   and continue with the krotov optimization loop until the cost function @xmath19 is small enough or a certain number of iterations is exceeded .    as a variant",
    ", we also use a combination of krotov s method with the bfgs method ( kbfgs ) @xcite .",
    "it includes an approximated hessian via the krotov gradient as an additional term in the update equation  .",
    "however , for technical reasons and differently from the grape bfgs algorithm , no line search is employed .",
    "in this paper , we consider two control problems . the first one is _ condensate splitting _",
    ", where the condensate initially resides in one well which is subsequently split into a double well . in our simulations we employ the confinement potential of lesanovsky et al .",
    "@xcite where the control parameter @xmath1 is associated with a radio frequency magnetic field @xcite .",
    "the objective is to bring at the terminal time @xmath67 the condensate wavefunction to the ground state of the double well potential .    in the second control problem",
    "the condensate wavefunction is excited from the ground to the first excited state of a single well potential .",
    "the confinement potential is an anharmonic single - well potential , details and a parameterized form of @xmath68 can be found in  @xcite .",
    "the shakeup is achieved by displacing the potential origin according to @xmath69 , where @xmath1 now corresponds to the position of the potential minimum , i.e. , through _",
    "wavefunction shaking_. experimental realizations of such shaking protocols have been reported in @xcite .    in our simulations",
    "grape and krotov start with the same initial guess .",
    "the terminal time is set to @xmath56 ms throughout . unless stated differently , we use a nonlinearity @xmath70 hz ( @xmath71 for units with @xmath3 and time measured in milliseconds , as used in our simulations @xcite ) .       but for shaking process . ]",
    "figure  [ fig : splitting1 ] shows ( a ) the controls obtained from our grape and krotov optimizations for condensate splitting , together with ( b , c ) the density maps of the condensate wavefunction .",
    "the potential is held constant after the terminal time @xmath56 ms of the control process .",
    "[ fig : splitting1](d , e ) show the square moduli of the terminal ( solid lines ) and desired ( dashed lines ) condensate wavefunctions , which are almost indistinguishable , thus demonstrating the success of both control protocols .",
    "this can be also seen from the density maps which show no time variations at later times , when the potential is held constant , in accordance to the fact that the terminal wavefunction is the groundstate of the double well trap .",
    "figure  [ fig : splitting2 ] compares the efficiency of the grape and krotov optimizations .",
    "we plot the cost function @xmath20 versus the number @xmath72 of equations solved during optimization . for both grape and krotov ,",
    "@xmath72 counts the solutions of either the gross - pitaevskii or the adjoint equation .",
    "the actual computer run times depend on the details of the numerical implementation , but are comparable for both schemes . as can be seen in fig .",
    "[ fig : splitting2 ] , in the grape optimization the cost function decreases in large steps after a given number of solved equations , whereas in the krotov optimization @xmath20 decreases continuously .",
    "the cost evolution of grape can be attributed to the bfgs search algorithm , where a line search is performed along a given search direction . once the minimum is found , the step is accepted ( @xmath20 drops ) and a new search direction is obtained through the solution of the adjoint equation .",
    "in contrast , the krotov algorithm is constructed such that @xmath20 decreases monotonically in each iteration step .",
    "altogether , grape and krotov optimizations perform equally well .     but for shaking process .",
    "we use @xmath73 . ]    in comparison to condensate splitting , the shakeup process is a considerably more complicated control problem .",
    "figure  [ fig : shaking1 ] shows the optimized control parameters as well as the time evolution of the condensate densities .",
    "both grape and krotov succeed comparably well . regarding the control fields , the grape one is smoother than the krotov one , due to the penalty term on @xmath74 in eq .  .",
    "from fig .",
    "[ fig : shaking2 ] we observe that a much higher number of optimization iterations is needed , in comparison to wavefunction splitting , for both optimization methods to significantly reduce @xmath20 .",
    "initially , @xmath20 decreases more rapidly for the krotov optimization , but after a larger number @xmath72 of solved equations , say around @xmath75 , grape performs better .",
    "we investigate the influence of the nonlinear atom - atom interaction on the convergence of the optimization loop .",
    "the dashed lines in fig .",
    "[ fig : splitting2 ] report results for splitting simulations with a larger nonlinearity @xmath62 hz . while the grape convergence depends only weakly on @xmath6 , krotov converges significantly slower for larger @xmath6 values .",
    "things are different for the shaking shown in fig .",
    "[ fig : shaking2 ] .",
    "while the grape performance again depends only weakly on @xmath6 , krotov converges _ faster _ with increasing @xmath6 . because of the lack of a line search in the krotov algorithm , the convergence behavior is far more dependent on specific features of the control landscape which depend strongly on @xmath6 .",
    "next , we inquire into the details of the convergence properties for the optimization of the shakeup process . by comparing grape with krotov",
    ", we will identify the advantages and disadvantages of the respective optimization methods .    , [ eq : h1 ] ) with @xmath31 or @xmath30 norm , respectively .",
    "( b ) optimal control parameters @xmath1 for bfgs solutions . ]",
    "figure  [ fig : shakinggrape](a ) shows the terminal cost function @xmath20 versus the number of solved equations of motion @xmath72 for the different grape schemes .",
    "it is evident that the conjugate gradient solutions reach a plateau after a certain number of iterations .",
    "in contrast , the bfgs solutions decrease significantly even at later stages of the optimization .",
    "we attribute this behavior to the use of the second order derivative information .",
    "the grape bfgs scheme , which estimates the hessian of @xmath19 in addition to @xmath27 , can take larger steps to cross flat regions of @xmath19 , contrary to the ( first - order ) grape gradient scheme , which gets stuck .",
    "figure  [ fig : shakinggrape](b ) shows the control fields for the grape bfgs schemes .",
    "although both optimization strategies perform equally well , the solutions obtained with @xmath31 norm are smoother and probably better suited for experimental implementation .",
    "but for krotov optimization .",
    "we investigate the influence of different mixing parameters @xmath36 between the old and new control fields , see eq .  , as well as a scheme with an adaptive @xmath36 choice ( see text for details ) .",
    "kbfgs reports the optimization result for a combination of krotov s method with bfgs  @xcite . ]",
    "figure  [ fig : shakingkrotov ] presents ( a ) @xmath20 versus @xmath72 and ( b ) the control parameters for the krotov optimization .",
    "the solid line with @xmath76 in panel ( a ) is identical to the one shown in fig .",
    "[ fig : shaking2 ] .",
    "when we increase @xmath36 ( black line ) the cost function drops more rapidly . however , we found that larger @xmath36 values can lead to sharp variations in @xmath1 which might be problematic for experimental implementations , as will be discussed in more detail below .            in fig .",
    "[ fig : shakingkrotov ] we additionally display results for a simulation using a combination of krotov s method with the bfgs scheme ( kbfgs )  @xcite .",
    "the performance of kbfgs is similar to the simpler optimization procedure of eq .  , a finding in accordance with ref .",
    "we attribute this to the fact that within the krotov scheme only a small portion of the control landscape is explored , because the monotonic convergence enforces small control updates , in contrast to grape where larger regions are scanned by the line search . as consequence ,",
    "the improvement in the krotov search direction via the hessian is minimal .",
    "finally , the dashed line for adaptive @xmath36 shows results for an optimization that starts with a small @xmath36 value , which subsequently increases in each iteration until the cost decreases by a desired amount ( here 2.5 per cent ) within one iteration .",
    "this @xmath36 value is then kept constant for the rest of the optimization .",
    "the idea behind this strategy is that the choice of @xmath36 is crucial for convergence , but the optimal value is different for each problem .",
    "generally , finding a suitable value for @xmath36 requires some trial and error .      for many experimental implementations",
    "it is indispensable to use smooth control parameters . in the following",
    "we investigate the smoothness of the optimal controls obtained by the different optimization methods .",
    "figure  [ fig : performancegrape](a ) shows for grape bfgs h1 the evolution of the @xmath1 values during optimization .",
    "one observes that during the first few iterations the characteristic features of @xmath1 emerge , which then become refined in the course of further iterations .",
    "fig ,  [ fig : performancegrape](b ) reports the power spectra ( square moduli of fourier transforms ) of the @xmath1 history during optimization . during the first say 20 iterations the fourier - transformed control parameter @xmath77 spectrally broadens , indicating the emergence of sharp features during optimization . with increasing iterations",
    "the spectral width of @xmath77 remains approximately constant .",
    "results of the grape bfgs l2 optimization are shown in figs .",
    "[ fig : performancegrape](c , d ) .",
    "we observe that , in contrast to the h1 results , @xmath1 acquires sharp features during optimization , as also reflected by the broad power spectrum .",
    "this is because initially the gradient @xmath27 , which determines the search direction for improved control parameters , exhibits strong variations .",
    "these variations are washed out in the h1 optimization through the solution of the poisson equation , see eq .  , leading to significantly smoother control parameters .    in grape",
    ", the user must additionally provide the weighting factor @xmath17 of eq .   that determines the relative importance of terminal cost and control smoothness .",
    "for the problems under study , we found that the performance of grape does not depend sensitively on the value of @xmath17 , and we usually use a small value such that the cost is dominated by the terminal cost .",
    "[ fig : performancekrotov](a , c ) show the @xmath1 history during a krotov optimization for different step sizes @xmath36 , and panels ( b , d ) report the corresponding power spectra . in comparison to the grape bfgs h1 optimization ,",
    "the power spectra are significantly broader , in particular for the larger @xmath36 values .",
    "this is due to the fact that in the functional used for the krotov optimization there is no penalty term that enforces smoothness of the control ( and thus a narrow spectrum ) .",
    "the choice of the step size @xmath36 is rather critical for the krotov performance .",
    "with increasing @xmath36 the cost function decreases more rapidly during optimization .",
    "however , values of @xmath36 that are too large can lead to numerical instabilities .",
    "these instabilities result from the discretization of the update equation ; mathematically , krotov is only guaranteed to converge monotonically for any value of @xmath36 if the control problem is continuous .    , but for a combined grape ",
    "krotov scheme where one initially starts with the krotov method and switches to grape after a given number of iterations .",
    "]    one might wonder whether a combination of both approaches would give the best of two worlds . in fig .",
    "[ fig : grapekrotov ] we present results for simulations where we start with a krotov optimization and switch to grape after a given number of iterations . as can be seen",
    ", the performance of this combined optimization does not offer a particular advantage over genuine grape or krotov optimizations .",
    "this is probably due to differences between the optimal control fields @xmath1 obtained by the two approaches , such that @xmath1 needs to be significantly modified when changing from one scheme to the other .",
    "in addition , the bfgs search algorithm of grape uses the information of previous iterations in order to estimate the hessian of the control space , and this information is missing when changing schemes .",
    "based on the two examples investigated in the previous section , namely wavefunction splitting and shaking in a magnetic microtrap , we now set out to analyze the advantages and disadvantages of the grape and krotov optimization methods which are tied to the functional that is minimized in each case .",
    "first , when the optimization converges fast to an optimal solution , such as for wavefunction splitting investigated in sec .",
    "[ subsec : splitting ] , both optimization algorithms perform equally well , even without carefully tuning the free parameters @xmath17 or @xmath36 . for such problems ,",
    "the choice of algorithm is a matter of personal preference . on the other hand , for optimization problems with slow convergence , such as wavefunction shaking ,",
    "more care has to be taken .",
    "specifically , there are significant differences between the two algorithms in terms of free parameters vs.  speed of convergence as well as possible cost functionals vs.  features of the obtained optimal control .",
    "while grape bfgs utilizes a line search to ensure monotonic convergence and to obtain the optimal step size in each iteration , the speed of convergence in krotov s method is mainly determined by the free parameter @xmath36 . on the one hand",
    "this means that grape bfgs works better `` out of the box '' since it automatically determines the best step size in each step . on the other hand ,",
    "the convergence is slowed down due to the necessity of a line search .",
    "it is also evident from our results that both algorithms yield controls with features that can be understood in terms of additional costs introduced in the functional .",
    "for grape we use a cost that penalizes a large derivative of the control which results in smooth controls in the end . for krotov",
    "s method we employ a penalty on changes in the amplitude of the control in each iteration .",
    "correspondingly this leads to controls that have a smaller integrated intensity and come at the cost of a less smooth control .    in principle",
    "it is conceivable to modify the krotov algorithm to take into account an additional cost term on the derivative of the control . while we conjecture that this will lead to controls that are comparable with those obtained in the grape framework , the necessary modification of the krotov algorithm is beyond the scope of the current work .    in the context of controlling bose - einstein condensates with experimentally smooth controls ,",
    "the optimization with the grape bfgs method , a functional enforcing smoothness , and use of the @xmath31 norm appears to be the method of choice .",
    "it is a black  box scheme with practically no problem - dependent parameters , it gives the desired smooth control fields , and works for various nonlinearity parameters @xmath6 .",
    "in contrast , the krotov optimization without an appropriate penalty term in the functional can converge faster but usually also leads to sharp features in the control .",
    "a sensitive choice of the step size @xmath36 is indispensable to achieve a compromise between fast convergence and smoothness .",
    "if smoothness is not an issue or extremely fast convergence is needed , the krotov method is preferable .",
    "a combination of grape and krotov in the sense of switching from one method to the other during the optimization did not result in any significant gain .",
    "this is explained by the different control solutions that are found by the different methods which do not easily facilitate a transition between them .",
    "it points to the fact that many control solutions exist and which solution is identified by the optimization depends strongly on the additional constraints  @xcite as well as the optimization method .",
    "this work has been supported in part by the austrian science fund fwf under project p24248 , by nawi graz , and the european union under grant no .",
    "297861 ( quaint ) .          which differs from eq .   in that the potential derivative",
    "is evaluated for @xmath64 .",
    "things can be easily generalized for the additional @xmath55 term of eq .  .",
    "let @xmath79 denote an initial guess for the solution of eq .",
    ", e.g.  the solution of eq .  .",
    "we now set @xmath80 , where @xmath81 is assumed to be a small quantity .",
    "thus , we can expand the second term on the right hand side of eq .   in lowest order of @xmath81 to obtain @xmath82\\bigl|\\psi^{(i+1)}(t)\\bigr>\\,.\\ ] ] separating the contributions of @xmath83 from the rest",
    ", we get @xmath84\\bigl|\\psi^{(i+1)}(t)\\bigr>\\right)\\delta\\lambda(t)\\approx    -(\\lambda_0(t)-\\lambda^{(i)}(t))+    s(t)\\bigl < p^{(i)}(t)\\bigr|\\left[\\frac{\\partial v}{\\partial\\lambda}\\bigl|_{\\lambda_0(t)}\\right ]    \\bigl|\\psi^{(i+1)}(t)\\bigr>\\,,\\ ] ]    which can be solved for @xmath81 .",
    "if @xmath85 is smaller than some small tolerance @xmath86 , we set @xmath87 .",
    "otherwise we set @xmath88 and repeat the newton iteration until convergence .",
    "typically only few iterations are needed to reach tolerances of the order of @xmath89 ."
  ],
  "abstract_text": [
    "<S> we study optimal quantum control of the dynamics of trapped bose - einstein condensates : the targets are to split a condensate , residing initially in a single well , into a double well , without inducing excitation ; and to excite a condensate from the ground to the first excited state of a single well . </S>",
    "<S> the condensate is described in the mean - field approximation of the gross - pitaevskii equation . </S>",
    "<S> we compare two optimization approaches in terms of their performance and ease of use , namely gradient ascent pulse engineering ( grape ) and krotov s method . </S>",
    "<S> both approaches are derived from the variational principle but differ in the way the control is updated , additional costs are accounted for , and second order derivative information can be included . </S>",
    "<S> we find that grape produces smoother control fields and works in a black - box manner , whereas krotov with a suitably chosen step size parameter converges faster but can produce sharp features in the control fields . </S>"
  ]
}