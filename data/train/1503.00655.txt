{
  "article_text": [
    "the core objective of this paper is to design and implement an iterative method for the solution of a system where the coefficient matrix is large , sparse , and nonsymmetric .",
    "the proposed method should be more efficient and robust than existing methods for solving such systems .",
    "one application in which such a system arises is in the computation of the _ scattering amplitude_. the scattering amplitude , in quantum physics , is the amplitude of the outgoing spherical wave relative to that of the incoming plane wave @xcite .",
    "it is useful when it is of interest to know what is reflected when a radar wave is impinging on a certain object .",
    "the scattering amplitude can be computed by taking the inner product of the right hand side vector @xmath3 of the @xmath4 @xmath5 and the solution * x * of the @xmath6 @xmath7 applications of the scattering amplitude come up in nuclear physics @xcite , quantum mechanics @xcite , and computational fluid dynamics ( cfd ) @xcite .",
    "one particular application is in the design of stealth planes @xcite .    the scattering amplitude @xmath2 creates a relationship between the right hand side of the adjoint system and the solution to the forward system in signal processing .",
    "the field @xmath8 is determined from the signal @xmath9 in the system @xmath0 .",
    "then the signal is received on an antenna characterized by the vector @xmath3 which is the right hand side of the adjoint system @xmath1 , and it is expressed as @xmath10 @xcite .",
    "we are interested in efficiently approximating the scattering amplitude .",
    "it is informative to look at methods that other researchers have used to solve this problem , which will be discussed below .",
    "the solution of the linear system ( [ ax = b ] ) is important for many applications beyond the scattering amplitude , such as in the numerical solution of pde with non - self - adjoint spatial differential operators .",
    "this solution can be obtained in many different ways , depending on the properties of the matrix @xmath11 .",
    "the @xmath12 factorization can be used to solve some problems with a symmetric matrix or a _ cholesky factorization _ can be used if the matrix is also known to be positive definite @xcite . however",
    ", for large , sparse systems , an iterative method is preferred .",
    "the _ conjugate gradient _ method is the preferred iterative method for a symmetric positive definite matrix @xmath11 @xcite .",
    "however it is much more difficult to find this solution for a matrix that is not symmetric positive definite . in the case that we have a matrix that is not symmetric , we can use methods like the _ biconjugate gradient ( bicg ) _",
    "@xcite and _ generalized minimal residual ( gmres ) _ methods @xcite .",
    "if we have a matrix that is symmetric but indefinite , _ symmlq _ @xcite is the iterative method of choice .",
    "since the scattering amplitude depends on both the forward and adjoint problem , it makse sense to use methods that take both the forward and adjoint problems into account , like the _ quasi - minimal residual ( qmr ) _",
    "@xcite and _ generalized least squares residual ( glsqr ) _ methods@xcite . +      the method of this paper employs a conjugate gradient - like approach since , for large , sparse matrices , it is best to use an iterative approach , such as the conjugate gradient method @xcite which is particularly effective for symmetric positive definite matrices . in particular ,",
    "conjugate gradient has a very rapid convergence if @xmath11 is near the identity either in the sense of a low rank perturbation or in the sense of the norm . in @xcite",
    "it is stated that    if @xmath13 is an @xmath14 symmetric positive definite matrix and rank(@xmath15)=@xmath16 then the hestenes - stiefel conjugate gradient algorithm converges in at most @xmath17 steps .",
    "suppose @xmath18 is symmetric positive definite and @xmath19 . if the hestenes - stiefel algorithm produces iterates @xmath20 and @xmath21 then @xmath22 where @xmath23 .",
    "it is also stated in @xcite that the accuracy of @xmath20 is often better than this theorem predicts and that the conjugate gradient method converges very rapidly in the @xmath11-norm if @xmath24 , where @xmath25 is the @xmath26 of @xmath11 , defined by @xmath27 with @xmath28 and @xmath29 referring to the largest and smallest singular values , respectively .    multiplying both sides of @xmath30 by @xmath31 yields the normal equations with a symmetric matrix @xmath32 that is also positive definite when @xmath11 is invertible . however , this approach is not conducive to solving the forward and adjoint problems simultaneously .",
    "furthermore , a significant problem with using @xmath33 is that now the condition number in the two - norm is squared for @xmath33 .",
    "since this increases the sensitivity of the matrix , possibly making it ill - conditioned , this paper explores an alternative approach .",
    "the idea is to transform the problems @xmath30 and @xmath34 into an equivalent system in which the matrix can be guaranteed to have real , positive eigenvalues , as well as eigenvectors that are in some sense orthogonal , which is then conducive to solution using a conjugate gradient - like iteration .",
    "it is not necessarily symmetry that we seek , but we will have symmetry with respect to some inner product . to this end",
    ", we use an idea first proposed by gene golub in @xcite , and consider a nonsymmetric saddle point matrix that has the form @xmath35.\\ ] ]    as required by the definition of a nonsymmetric saddle point matrix , we assume that the matrix @xmath36 is symmetric positive definite .",
    "the goal is to choose @xmath36 so that we can guarantee @xmath37 has real , positive eigenvalues . in this paper",
    "we will introduce the _ nonsymmetric saddle point conjugate gradient _ ( nspcg ) method to solve a nonsymmetric , large , sparse linear system , which will then allow us to compute the scattering amplitude .",
    "we will also use ilu preconditioning with nspcg , which gives rapid convergence compared to existing methods for solving such systems .",
    "this paper is organized as follows . in section [ sec2 ]",
    "we discuss the known methods for solving a large linear system with iterative approaches to compute the scattering amplitude such as bidiagonalization or least squares qr ( lsqr ) , quasi minimum residual ( qmr ) , and block generalized lsqr ( glsqr ) . in section [ sec4 ] we will introduce the method of this paper , nspcg . section 4 will include an analysis of the numerical results .",
    "the preconditioning techniques and results can be found in section [ sec5b ] .",
    "the conclusions and discussion of possible future work will be given in section [ sec6 ] .",
    "the qmr approach @xcite is based on the spectral decomposition @xmath38 ; also the basis of the qmr approach is the unsymmetric lanczos @xcite process which generates two sequences @xmath39\\ ] ] @xmath40\\ ] ] that are biorthogonal , meaning @xmath41 .",
    "we have the following relations : @xmath42 where the tridiagonal matrices @xmath43= \\left [ \\begin{array}{c } t_{k , k}\\\\ \\beta_{k}{\\bf e}_{k}^{t}\\\\ \\end{array } \\right]\\ ] ] and @xmath44= \\left [ \\begin{array}{c } \\hat{t}_{k , k}\\\\ \\hat{\\beta}_{k}{\\bf",
    "e}_{k}^{t}\\\\ \\end{array } \\right]\\ ] ] have block structures in which @xmath45 and @xmath46 are not necessarily symmetric .",
    "the residual , @xmath47 , in each iteration can be expressed as @xmath48 with a choice of @xmath49 where @xmath50 and @xmath51 .",
    "we now have the quasi - residual @xmath52 then we choose @xmath53 , where @xmath54 and @xmath55",
    ". then the adjoint residual is @xmath56 .",
    "the vectors @xmath57 and @xmath58 are the solutions of the least squares problems for minimizing @xmath59 and @xmath60 .",
    "so now the solutions can be defined as @xmath61      in lsqr @xcite , a truncated bidiagonalization is used in order to solve the forward and adjoint problems approximately .",
    "the bidiagonal factorization of @xmath11 is given by @xmath62 where @xmath63 and @xmath64 are orthogonal and @xmath15 is bidiagonal .",
    "thus the forward and adjoint systems can be written as @xmath65 @xmath66 now we can solve ( [ 2.1 ] ) by solving the following two systems @xmath67 and we can solve ( [ 2.2 ] ) by solving @xmath68 we need to use the following recurrence relations in an iterative process to produce a bidiagonal matrix @xmath69 where @xmath70 and @xmath71 are matrices with orthonormal columns , and @xmath72.\\ ] ] also we have that @xmath73 and @xmath74    because @xmath75 is bidiagonal , it follows that @xmath76 is symmetric and tridiagonal .",
    "it can be seen from ( [ 2.9 ] ) that ( [ this ] ) and ( [ that ] ) implicitly apply lanczos iteration to @xmath33 .",
    "now this iterative process can be used to obtain the approximate solution to the forward and adjoint systems .",
    "we define the residuals at step @xmath77 as @xmath78 where @xmath79 the goal of the lsqr approach is to obtain an approximation that minimizes the norm of the residual .",
    "that is , the norm @xmath80 is minimized .",
    "when working with the forward and adjoint problems , this approach is limited due to the relationship between the starting vectors @xmath81 the above relationship does not allow @xmath82 to be chosen independently .      the gslqr method",
    "@xcite overcomes the disadvantages of the lsqr method by choosing starting vectors @xmath83 and @xmath84 independently where , for an initial guess of @xmath85 and @xmath86 , @xmath50 and @xmath87 it is based on the factorizations @xmath88 from the above we get that @xmath89 where the recursion coefficients @xmath90 , @xmath91 , @xmath92 , and @xmath93 are chosen to make @xmath71 and @xmath70 have orthonormal columns , which yields @xmath94 we can define @xmath95 and @xmath96 , where @xmath97 , and @xmath98 .",
    "now we have that @xmath99 \\quad   s_{k+1,k } = \\left [ \\begin{array}{cccc } \\delta_{1 } & \\theta_{1 } & \\ ,   & \\ ,   \\\\ \\eta_{2 } & \\delta_{2 } & \\ddots & \\ , \\\\ \\ , & \\ddots & \\ddots & \\theta_{k-1 } \\\\ \\ , & \\ , & \\eta_{k } & \\delta_{k } \\\\ \\ , & \\ , & \\ , & \\eta_{k+1 } \\\\",
    "\\end{array } \\right].\\ ] ] the residuals can be expressed as follows @xmath100 and @xmath101 the solutions @xmath20 and @xmath102 are @xmath103",
    "the matrix @xmath37 , defined as follows @xmath104,\\ ] ] where @xmath105 is invertible and @xmath36 is a symmetric positive definite matrix , is an example of a _ nonsymmetric saddle point matrix_. it can be shown that @xmath106 for all @xmath107 . to see this , we first let @xmath108.$ ]",
    "then @xmath109 can be written as @xmath110 \\left [ \\begin{array}{cc } a^{t}wa & a^{t }   \\\\ -a & 0 \\\\ \\end{array } \\right ] \\left [ \\begin{array}{cc } { \\bf y }   \\\\ { \\bf z } \\\\ \\end{array } \\right ] \\\\ & = & { \\bf y}^{t}(a^{t}wa ) { \\bf y}-{\\bf z}^{t}a{\\bf y}+{\\bf y}^{t}a^{t}{\\bf z } \\\\ & = & { \\bf y}^{t}(a^{t}wa ) { \\bf y}.\\end{aligned}\\ ] ] now , if we let @xmath111 for any nonzero vector @xmath112 , then , @xmath113 .",
    "since @xmath36 is symmetric positive definite , we have that @xmath114 , since * r * is nonzero due to @xmath11 being invertible . on the other hand , if we assume @xmath115 , then @xmath116 that is , whether @xmath112 is nonzero or not , @xmath117 .",
    "we want to choose @xmath36 so that the matrix @xmath37 has a real positive spectrum , so it is suitable for a conjugate gradient - like iteration @xcite . to make this choice we need to first define @xmath118,\\ ] ] where @xmath119 is a polynomial of degree one in the form @xmath120 for @xmath121 and @xmath122.\\ ] ] the goal here is to determine if there exists a symmetric positive definite matrix @xmath123 with respect to which @xmath124 is symmetric , meaning that @xmath37 is @xmath123-symmetric if @xmath125 .",
    "let us first define a generic nonsymmetric saddle point matrix @xmath126.\\ ] ] and then define @xmath127 .",
    "we can use the following results from @xcite to determine how to obtain a real positive spectrum :    [ lem2 ] let the matrix @xmath128\\ ] ] be conformally partitioned with @xmath129 .",
    "then + ( 1 ) @xmath129 is @xmath130-symmetric , i.e. , @xmath131 , and for any polynomial @xmath119 , + ( 2 ) @xmath132 is @xmath130-symmetric , i.e. , @xmath133 , and + ( 3 ) @xmath129 is @xmath134-symmetric , i.e. , @xmath135 .",
    "[ thm1 ] the symmetric matrix @xmath123 is positive definite if and only if @xmath136 where @xmath137 and @xmath138 denote the smallest and largest eigenvalues , respectively , and @xmath139    a sufficient condition that makes @xmath123 positive definite can be derived from the above theorem .",
    "[ cor1 ] the matrix @xmath123 is symmetric positive definite when ( [ 4.1 ] ) holds , and , in addition , @xmath140 for @xmath141 , the right hand side of ( [ 4.4 ] ) is maximal and ( [ 4.4 ] ) reduces to @xmath142    the preceding results lead to a simple approach to determining whether @xmath129 is suitable for a conjugate gradient - like iteration @xcite .",
    "[ cor2 ] if there exists a @xmath143 so that @xmath123 is positive definite , then @xmath129 has a nonnegative real spectrum and a complete set of eigenvectors that are orthonormal with respect to the inner product defined by @xmath123 . in case",
    "@xmath144 has full rank , the spectrum of @xmath129 is real and positive .    using the previous results from @xcite , we obtain a simple criterion for determining whether the matrix @xmath37 from ( [ m ] ) can be constructed in such a way as to satisfy the criterion in corollary [ cor2 ] .    [ wthm ]",
    "let @xmath11 be an invertible @xmath145 real matrix , and let @xmath36 be a symmetric positive definite @xmath145 matrix that satisfies @xmath146 then the matrix @xmath37 defined by @xmath147\\ ] ] has real positive eigenvalues and eigenvectors that are orthogonal with respect to the inner product defined by @xmath148 .",
    "that is , the above selection of @xmath36 makes the matrix @xmath37 suitable for a conjugate gradient - like iteration .",
    "proof : we need to satisfy ( [ 4.1 ] ) with a proper selection of @xmath149 .",
    "let @xmath150 based on corollary [ cor1 ] .",
    "because of how @xmath149 is defined , @xmath151 satisfies @xmath152 which means ( [ 4.1 ] ) is also satisfied .",
    "now we need to choose @xmath36 so that ( [ 4.5 ] ) from corollary [ cor1 ] holds .",
    "we require @xmath153 or @xmath154 where @xmath155 is equal to the largest singular value of @xmath11 , @xmath156 .",
    "from the fact that @xmath157 is symmetric positive definite , we obtain @xmath158 therefore , ( [ 4.8 ] ) is satisfied if @xmath159 or , equivalently , if ( [ wcond ] ) is satisfied . @xmath160",
    "it follows that the matrix @xmath36 satisfies the requirements to make @xmath123 be symmetric positive definite and that @xmath161 has a real , positive spectrum from corollary [ cor2 ] .",
    "this result makes the matrix suitable for a conjugate gradient - like iteration , as will be described below .",
    "let @xmath163 be the svd of @xmath11 , where @xmath164 , \\quad v = \\left [ \\begin{array}{ccc } { \\bf v}_1 & \\cdots & { \\bf v}_n \\end{array } \\right]\\ ] ] and @xmath165 .",
    "in the case @xmath162 for some scalar @xmath166 , the condition from theorem [ wthm ] reduces to @xmath167    we now study the eigensystem of @xmath37 .",
    "let @xmath168 for @xmath169 , where @xmath170^t$ ] .",
    "the form of @xmath37 from ( [ m ] ) , with @xmath162 , yields @xmath171 for @xmath169 . substituting ( [ eig2 ] ) into ( [ eig1 ] ) yields @xmath172 multiplying through by @xmath11 and applying ( [ eig2 ] ) , we obtain @xmath173 it follows that each @xmath174 is a multiple of a left singular vector of @xmath37 , and @xmath175 is the square of the corresponding singular value . furthermore , from ( [ eig2 ] ) ,",
    "we find that @xmath176 is a multiple of a right singular vector of @xmath37 .",
    "we conclude that the eigenvectors @xmath177 of @xmath37 are given by @xmath178 , \\quad { \\bf x}_{2j } = \\left [ \\begin{array}{c } -\\lambda_j^- { \\bf v}_j \\\\ \\sigma_j { \\bf u}_j \\end{array } \\right ] , \\quad j = 1 , 2,\\ldots , n,\\ ] ] with corresponding eigenvalues @xmath179 that satisfy the quadratic equation @xmath180 it can be shown directly from ( [ mevcts ] ) and ( [ mevals ] ) that these eigenvalues are real and positive , and the corresponding eigenvectors linearly independent , if and only if @xmath166 satisfies the weaker condition @xmath181 which is consistent with the necessary and sufficient condition for @xmath123 to be positive definite given in theorem [ thm1 ] .",
    "let @xmath182 be nonsymmetric .",
    "we will now introduce a conjugate gradient ( cg ) approach that solves the linear system @xmath183 by solving an equivalent system of the form @xmath184 , where @xmath185.\\ ] ] the matrix @xmath37 is also not symmetric ; however , the spectrum is entirely contained in the right half of the complex plane , due to the fact that @xmath186 for all @xmath8 . in the preceding discussion , we established that if @xmath36 was chosen so as to satisfy the assumptions of theorem [ wthm ] , then @xmath37 is diagonalizable with real , positive eigenvalues . furthermore , the bilinear form @xmath187 , where @xmath188 , is a proper inner product , as @xmath189 is symmetric positive definite .",
    "it follows that @xmath37 is @xmath189-symmetric and @xmath189-definite , meaning that @xmath190 for all @xmath191 , and @xmath192 for all @xmath193 .",
    "let the vectors @xmath194 and @xmath9 be defined by @xmath195 , \\quad   { \\bf p}= \\left [ \\begin{array}{c } { \\bf d } \\\\ { \\bf 0 } \\\\",
    "\\end{array } \\right],\\ ] ] where @xmath196 , @xmath184 , and @xmath197 is the scattering amplitude for given vectors @xmath198 and @xmath199 that represent the field and antenna , respectively .",
    "the following conjugate gradient method is based on a given inner product @xmath200 for solving the linear system @xmath201 .",
    "we have the inner product matrix @xmath202 suggested by @xcite . from @xcite",
    ", we see that this choice of @xmath189 gives a working cg from the following lemma .",
    "[ lem1 ] suppose that the symmetric matrix @xmath123 is positive definite .",
    "then algorithm 3.1 is well defined for @xmath37 and @xmath202 , and ( until convergence ) the scalars @xmath203 and @xmath204 can be computed as @xmath205 @xmath206    with this choice of inner product matrix , it can be shown that the residuals computed using the preceding algorithm are , in some sense , orthogonal .",
    "each residual @xmath207 as defined in algorithm 3.1 is orthogonal to all previous residuals with respect to @xmath123 , i.e. @xmath208 , where @xmath209 .",
    "proof : we know that @xmath210 .",
    "let @xmath203 be defined as in ( [ alpha ] ) . also , we know that all of the search directions are orthogonal , i.e. @xmath211 for @xmath209 .",
    "we want to show that @xmath212 .",
    "this will be shown by induction , where the base case that we need to establish is @xmath213 to show this we use the definition of @xmath203 and the expression for the search directions in the above algorithm , @xmath210 .",
    "now we have that @xmath214 reindexing the definition of the residual from the algorithm yields the following expression for @xmath215 @xmath216 substituting this into ( [ step1 ] ) gives @xmath217 rearranging the last term in ( [ step2 ] ) yields @xmath218 because @xmath123 is symmetric , and we already know that the search directions @xmath219 are orthogonal with respect to @xmath123 . now it is easy to see that the denominator in ( [ step2 ] ) and the last factor in the numerator cancel leaving @xmath220 now we need to show that each residual is orthogonal to all previous residuals .",
    "we will do this by showing @xmath221 , where @xmath222 .",
    "our induction hypothesis is @xmath223 to show this , first shift the indices to get the expression @xmath224 rearranging the recurrence relation for the search directions yields @xmath225 using this expression for @xmath215 and @xmath226 we obtain @xmath227 where @xmath228 by the induction hypothesis .",
    "now we are left with @xmath229 where both terms are 0 due to the orthogonality of the search directions . @xmath160",
    "in this section , we will analyze the results from the methods described in this paper .",
    "these methods include qmr from section 2.2 , glsqr from section 2.3 , and nspcg from section 3.1 .",
    "we have duplicated the results from @xcite for glsqr and qmr and will compare them against the results for our nspcg method .",
    "we need to first define the following matrix , @xmath37 is our nonsymmetric saddle point matrix @xmath230\\ ] ] where @xmath162 is defined from ( [ w ] ) .",
    "these examples are from @xcite .",
    "this example uses the matrix created by a = sprand(n , n,0.2)+speye(n ) in matlab where ` n=100 ` .",
    "this creates a random sparse @xmath145 matrix , where 0.2 is the density of uniformly distributed nonzero entries , and adds this to the identity .",
    ", width=377 ]    in figure [ ex1 ] we see that at the beginning of the iteration nspcg reaches a better approximation in fewer iterations than either qmr or glsqr .",
    "although glsqr eventually outperforms nspcg , it takes about 120 iterations before it shows any sign of convergence at all .",
    "then it converges rapidly .",
    "example 2 uses the orsirr_1 matrix from the matrix market collection , which represents a linear system used in oil reservoir modeling .",
    "this matrix can be obtained from http://math.nist.gov / matrixmarket/.        we see that nspcg starts out with the lowest error in the 2-norm of the residual . also we see that in both figure [ ex2 ] and figure [ ex1 ] that nspcg is more consistent than either glsqr or qmr .",
    "although qmr actually outperforms glsqr and nspcg , it takes about 400 iterations to do so .",
    "first define the circulant matrix @xmath231.\\ ] ] now the matrix used in this example a=1e-3*sprand(n , n,0.2)+j , where n=100 , can be constructed in matlab .",
    ", width=377 ]    nspcg starts out steady and consistent again in this figure [ ex3 ] as we see in figure [ ex2 ] and figure [ ex1 ] .",
    "eventually , glsqr converges , taking about 70 iterations to do so , while qmr fails to show any sign of convergence .      we need to first define @xmath232 \\in \\mathbb{r}^{p , p } \\quad \\quad d_{2}= \\left [ \\begin{array}{cccc } 1 & \\ , & \\ , & \\ , \\\\ \\ , & 2 & \\ , & \\ , \\\\ \\ , & \\ , & \\ddots & \\ , \\\\ \\ , & \\ , & \\ , &",
    "q \\\\ \\end{array } \\right ] \\in \\mathbb{r}^{q , q}\\ ] ] where @xmath233 and @xmath234 diag@xmath235 .",
    "now we can define @xmath236 , where @xmath63 and @xmath64 are orthogonal matrices .",
    "for this example we use @xmath237 and @xmath238 .    , width=377 ]    from [ ex4 ] we see that nspcg starts off with the best approximation , but only for about 15 iterations",
    ". then it is overtaken by glsqr .",
    "also , we can see that qmr fails to converge at all .",
    "this example uses the same definition of @xmath239 , @xmath240 , and @xmath11 from example 4 . in this example",
    "we will let @xmath237 again , and @xmath241 .",
    ", width=377 ]    figure [ ex5 ] shows the same trend we have been seeing , that nspcg is more consistent at the beginning than any other method . at",
    "about 65 iterations glsqr outperforms nspcg , and qmr fails to converge again .",
    "this example uses the same definition of @xmath239 , @xmath240 , and @xmath11 from example 4 . in this example",
    "we will let @xmath242 again , and @xmath243 .",
    ", width=377 ]    from figure [ ex6 ] we see that nspcg shows the best results for the first 600 iterations .",
    "glsqr takes many iterations to converge in this case , and qmr does not converge at all .",
    "according to @xcite conjugate gradient has very rapid convergence for a symmetric positive definite matrix @xmath11 that is nearly identity .",
    "we need to apply _ preconditioning _ techniques to make our matrix @xmath37 satisfy this criterion",
    ". the result will be that the original system is transformed into an equivalent system where the coefficient matrix is near identity .",
    "as we have seen previously with conjugate gradient , preconditioning techniques can be generalized to the nonsymmetric case .",
    "the goal is to apply @xmath244 preconditioning @xcite , while taking into account the structure of the nonsymmetric saddle point matrix @xmath245 .",
    "the matrix @xmath36 in the ( 1,1 ) block is assumed to be a symmetric positive definite matrix ; therefore it has a cholesky factorization @xmath246 .",
    "we can use the @xmath247 factorization @xmath248 to obtain the factorization @xmath249 , where @xmath250 , \\quad u= \\left [ \\begin{array}{cc } r & q^{t}g^{-1 } \\\\ 0 & q^{t}g^{-1 }       \\end{array } \\right].\\ ] ] let us define @xmath251 where an incomplete @xmath247 factorization @xcite is computed from the sparse matrix @xmath252 which gives @xmath253 . by finding @xmath254 , \\quad \\widetilde{u}^{-1}=",
    "\\left [ \\begin{array}{cc } \\widetilde{r}^{-1 } & -\\widetilde{r}^{-1 } \\\\ 0 & g\\widetilde{q}^{-t }   \\end{array } \\right]\\ ] ] it can be seen that the resulting preconditioned system matrix is given by @xmath255.\\ ] ] the above matrix has the structure similar to that of @xmath37 from ( [ m ] ) , therefore it is a nonsymmetric saddle point matrix that is near @xmath256 .",
    "the following is example 1 from the previous section with preconditioning .",
    "the following is example 2 from the previous section with preconditioning .        in figure [ ex2preconditioning ]",
    "nspcg converges very rapidly in only 10 iterations .",
    "qmr takes over 200 iterations , but still does nt reach the level of accuracy that nspcg achieves .",
    "glsqr does not converge at all .",
    "the results from this paper show that the nspcg method is much more consistent and reliable than glsqr or qmr .",
    "nspcg only takes a few iterations to make fairly significant progress while glsqr takes many iterations in most cases , and qmr rarely makes any progress .",
    "if preconditioning is used with nspcg , as is usually done with a conjugate gradient method , we have provided evidence that it will dramatically accelerate convergence , compared to state - of - the - art iterative methods such as gmres or bicg that are typically used to solve such systems .",
    "these results support our hypothesis that more rapid convergence can be achieved by solving a system that , while still nonsymmetric , shares essential properties with symmetric positive definite matrices and therefore is more suitable for conjugate gradient - like iteration .",
    "future work will include relating the nspcg method to a quadrature rule , as in @xcite , that can be used to compute the scattering amplitude without explicitly solving the forward or adjoint problem .",
    "this has been done in @xcite with the symmetric matrix @xmath257\\ ] ] in conjunction with block lanczos iteration @xcite , but our goal is to achieve more rapid convergence .",
    "furthermore , because the forward system @xmath30 is replaced with a system with twice as many unknowns and equations , it is essential to implement the iteration carefully so that the gain in convergence speed is not offset by the additional expense of each iteration . to that end , it is worthwhile to consider other choices for the matrix @xmath36 instead of just a multiple of identity .",
    "99 arnett , d. _ supernovae and nucleosynthesis : an investgation of the history of matter , from the big bang to the present _ , princeton university press , ( 1996 ) .",
    "bjrck , a. `` a bidiagonalization algorithm for solving ill - posed sytem of linear equations '' .",
    "_ bit _ , * 41 * ( 2001 ) , pp .",
    "659 - 670 .",
    "brezinski , c. , redivo - zaglia , m. `` look - ahead in bicgstab and other product - type methods for linear systems , '' _ bit _ , * 35 * ( 1995 ) , pp .",
    "275 - 285 .",
    "giles , m. b. , pierce , a. `` an introduction to the adjoint approach to design '' , _ flow , turbulence , and combustion _ , * 65 * ( 2000 ) , pp .",
    "393 - 415 .",
    "golub , g. h. , lambers , j. v. private communication , ( november 6 , 2007 ) .",
    "golub , g. h. , meurant , g. `` matrices , moments , and quadrature '' .",
    "_ proceedings of the 15th dundee conference _",
    ", june - july ( 1993 ) , _ longman scientific and technical _ , ( 1994 ) , pp . 105 - 156 .",
    "golub , g. h. , stoll , m. , wathen , a. `` approximation of the scattering amplitude and linear systems '' .",
    "_ etna _ , * 23 * ( 2008 ) , pp . 178 - 203 .",
    "golub , g. h. , underwood , r. `` the block lanczos method for computing eigenvalues '' , _ mathematical software iii _ , * 7 * ( 1977 ) , pp .",
    "361 - 377 .",
    "golub , g. h. , van loan , c.f . : _ matrix computations , _ the johns hopkins university press ( 1996 ) .",
    "golub , g. h. , welsch , j. `` calculation of gauss quadrature rules '' _ math .",
    "_ , * 23 * ( 1969 ) , pp .",
    "221 - 230 .",
    "hestenes , m. , stiefel , e. `` methods of conjugate gradients for solving linear systems '' _ journal of research of the national bureau of standards _ * 49*(6 ) ( 1952 ) .",
    "hntynkov , i. , strako , z. `` lanczos tridiagonalization and core problems '' .",
    "_ linear algebra appl .",
    "_ , * 421 * ( 2007 ) , pp .",
    "243 - 251 .",
    "lambers , j. v. `` matrices , moments , and quadrature '' .",
    "landau , l. d. , lifshitz , e. _ quantum mechanics _ , pergumon press , oxford , ( 1965 ) .",
    "liesen , j. , parlett , b. `` on nonsymmetric saddle point matrices that allow conjugate gradient iterations '' .",
    "_ numerische mathematik _ ,",
    "* 108 * ( 2008 ) , pp .",
    "605 - 624 .",
    "lu , j. , darmofal , l. `` a quasi - minimal residual method for simultaneous primal - dual solutions , and superconvergent functional estimates '' , _",
    "siam j. sci .",
    "_ , * 24 * ( 2003 ) , pp .",
    "1693 - 1709 .",
    "morgan , r.b .",
    "`` a restarted gmres method augmented with eigenvectors , '' _ siam j. matrix anal .",
    "_ , * 16 * ( 1995 ) , pp",
    ". 1154 - 1171 .",
    "morgan , r. b. `` on restarting the arnoldi method for large nonsymmetric egenvalue problems '' , _ math comp .",
    "_ , * 65 * ( 1996 ) , pp . 1213 - 1230 .",
    "paige , c. c. , saunders , m. a. `` algorithm 583 lsqr : sparse linear equations and least squares problems '' , _ acm trans .",
    "_ , * 8 * ( 1982b ) , pp .",
    "195 - 209 .",
    "papadopoulous , a.t . ,",
    "duff , i. s. , wathen , a. j. : incomplete orthogonoal factorization methods using givens rotations ii : implementation and results _ bit _ * 45*(1 ) ( 2005 ) 159 - 179 .",
    "parlett , b. n. , nour - omid , b. `` the use of a refined error bound when updating eigenvalues of tridiagonals '' , _ lin .",
    "it s applic _ , * 34 * ( 1980 ) , pp .",
    "parlett , b. n. , simon , h. , stringer , l. m. , `` on estimating the largest eigenvalue with the lanczos algorithm '' , _ math . comp .",
    "_ , * 38 * ( 1982 ) , pp .",
    "153 - 166 .",
    "saad , y. : _ iterative methods for sparse linear systems .",
    "_ psw ( 1996 ) .",
    "saunders , m. a. `` solution of sparse rectangular systems , '' _ bit _ , * 35 * ( 1995 ) , pp .",
    "588 - 604 .",
    "saunders , m. a. , simon , h.d .",
    ", yip , e. l. `` two conjugate - gradient - type methods for unsymmetric linear equations '' , _ siam j. numer",
    "_ , * 25 * ( 1988 ) , pp .",
    "927 - 940 ."
  ],
  "abstract_text": [
    "<S> in this paper we examine iterative methods for solving the forward ( @xmath0 ) and adjoint ( @xmath1 ) systems of linear equations used to approximate the scattering amplitude , defined by @xmath2 . based on an idea first proposed by gene golub , we use a conjugate gradient - like iteration for a nonsymmetric saddle point matrix that is constructed so as to have a real positive spectrum . </S>",
    "<S> numerical experiments show that this method is more consistent than known methods for computing the scattering amplitude such as glsqr or qmr . </S>",
    "<S> we then demonstrate that when combined with known preconditioning techniques , the proposed method exhibits more rapid convergence than state - of - the - art iterative methods for nonsymmetric systems .    </S>",
    "<S> nonsymmetric saddle point matrix , conjugate gradient method , scattering amplitude </S>"
  ]
}