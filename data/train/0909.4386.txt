{
  "article_text": [
    "inferring the causal relations that have generated statistical dependencies among a set of observed random variables is challenging if no controlled randomized studies can be made . here",
    ", causal relations are represented as arrows connecting the variables , and the structure to be inferred is a directed acyclic graph ( dag ) @xcite .",
    "the constraint - based approach to causal discovery , one of the best known methods , selects directed acyclic graphs that satisfy both the causal markov condition and faithfulness : one accepts only those causal hypotheses that explain the observed dependencies and demand that all the observed _ in_dependencies are imposed by the structure , i.e. , common to all distributions that can be generated by the respective causal dag .",
    "however , the methods are fundamentally unable to distinguish between dags that induce the same set of dependencies ( markov - equivalent graphs ) .",
    "moreover , causal faithfulness is known to be violated if some of the causal relations are deterministic @xcite .",
    "solving these problem requires reasonable prior assumptions , either implicitly or explicitly as priors on conditional probabilities , as in bayesian settings @xcite .",
    "however , the fact that deterministic dependencies exist in real - world settings shows that priors that are densities on the parameters of the bayesian networks , as it is usually assumed , are problematic , and the construction of good priors becomes difficult .",
    "recently , several methods have been proposed that are able to distinguish between markov - equivalent dags causes @xmath1 or @xmath1 causes @xmath2 '' has been part of the challenge at the nips 2008 workshop `` causality : objectives and assessment '' @xcite ] .",
    "linear causal relations among non - gaussian random variables can be inferred via independent - component - analysis ( ica ) methods @xcite .",
    "the method of @xcite is able to infer causal directions among real - valued variables if every effect is a ( possibly non - linear ) function of its causes up to an additive noise term that is independent of the causes .",
    "the work of @xcite augmented these models by applying a non - linear function after adding the noise term . if the noise term vanishes or if all of the variables are gaussian and the relation is linear , all these methods fail .",
    "moreover , if the data are high - dimensional , the non - linear regression involved in the methods becomes hard to estimate .",
    "here we present a method that also works for these cases provided that the variables are multi - dimensional with sufficiently anisotropic covariance matrices .",
    "the underlying idea is that the causal hypothesis @xmath3 is only acceptable if the shortest description of the joint distribution @xmath4 is given by separate descriptions of the input distribution @xmath5 and the conditional distribution @xmath6 @xcite , expressing the fact that they represent independent mechanisms of nature .",
    "@xcite shows toy examples where such an independent choice often leads to joint distributions where @xmath7 and @xmath8 satisfy non - generic relations indicating that @xmath9 is wrong .",
    "here we develop this idea for the case of multi - dimensional variables @xmath2 and @xmath1 with a linear causal relation .    we start with a motivating example .",
    "assume that @xmath2 is a multivariate gaussian variable with values in @xmath10 and the isotropic covariance matrix @xmath11 .",
    "let @xmath1 be another @xmath10-valued variable that is deterministically influenced by @xmath2 via the linear relation @xmath12 for some @xmath13-matrix @xmath14 .",
    "this induces the covariance matrix @xmath15 the converse causal hypothesis @xmath9 becomes unlikely because @xmath7 ( which is determined by the covariance matrix @xmath16 ) and @xmath8 ( which is given by @xmath17 with probability @xmath18 ) are related in a suspicious way , since the same matrix @xmath14 appears in both descriptions .",
    "this untypical relationship between @xmath7 and @xmath8 can also be considered from the point of view of symmetries : consider the set of covariance matrices @xmath19 with @xmath20 , where @xmath21 denotes the orthogonal group . among them",
    ", @xmath22 is special because it is the only one that is transformed into the isotropic covariance matrix @xmath23 .",
    "more generally speaking , in light of the fact of how anisotropic the matrices @xmath24 are for _ generic _ @xmath25 , the hypothetical effect variable is surprisingly isotropic for @xmath26 ( here we have used the short notation @xmath27 ) .",
    "we will show below that this remains true with high probability ( in high dimensions ) if we start with an arbitrary covariance matrix @xmath23 and apply a random linear transformation @xmath14 chosen independently of @xmath23 .    to understand why independent choices of @xmath23 and @xmath14 typically induce untypical relations between @xmath28 and @xmath22 we also discuss the simple case that @xmath23 and @xmath14 are simultaneously diagonal with @xmath29 and @xmath30 as corresponding diagonal entries .",
    "thus @xmath22 is also diagonal and its diagonal entries ( eigenvalues ) are @xmath31 .",
    "we now assume that `` nature has chosen '' the values @xmath29 with @xmath32 independently from some distribution and @xmath30 from some other distribution .",
    "we can then interpret the values @xmath29 as instances of @xmath33-fold sampling of the random variable @xmath34 with expectation @xmath35 and the same for @xmath30 .",
    "if we assume that @xmath36 and @xmath34 are independent , we have @xmath37 due to the law of large numbers , this equation will for large @xmath33 approximatively be satisfied by the empirical averages , i.e. , @xmath38 for the backward direction @xmath9 we observe that the diagonal entries @xmath39 of @xmath22 and the diagonal entries @xmath40 of @xmath41 have not been chosen independently because @xmath42 whereas @xmath43 the last inequality holds because the random variables @xmath44 and @xmath45 are always negatively correlated ( this follows easily from the cauchy - schwarz inequality @xmath46 ) except for the trivial case when they are constant .",
    "we thus observe a systematic violation of ( [ tracecomm ] ) in the backward direction .",
    "the proof for non - diagonal matrices in section  [ iden ] uses standard spectral theory , but is based upon the same idea .",
    "the paper is structured as follows . in section  [ iden ]",
    ", we define an expression with traces on covariance matrices and show that typical linear models induce backward models for which this expression attains values that would be untypical for the forward direction . in section  [ exp ]",
    "we describe an algorithm that is based upon this result and discuss experiments with simulated and real data .",
    "section  [ gen ] proposes possible generalizations .",
    "given a hypothetical causal model @xmath47 ( where @xmath2 and @xmath1 are @xmath33- and @xmath48-dimensional , respectively ) we want to check whether the pair @xmath49 satisfies some relation that typical pairs @xmath50 only satisfy with low probability if @xmath20 is randomly chosen .",
    "to this end , we introduce the renormalized trace @xmath51 for dimension @xmath33 and compare the values @xmath52 one shows easily that the expectation of both values coincide if @xmath23 is randomly drawn from a distribution that is invariant under transformations @xmath53 this is because averaging the matrices @xmath54 over all @xmath20 projects onto @xmath55 since the average @xmath56 commutes with all matrices and is therefore a multiple of the identity . for our purposes , it is decisive that the typical case is close to this average , i.e. , the two expressions in ( [ tracecomp ] ) almost coincide . to show this",
    ", we need the following result @xcite :    @xmath57 + [ lev ] let @xmath58 be a lipschitz continuous function on the @xmath33-dimensional sphere with @xmath59 if a point @xmath60 on @xmath61 is randomly chosen according to an @xmath21-invariant prior , it satisfies @xmath62 with probability at least @xmath63 for some constant @xmath64 , where @xmath65 can be interpreted as the median or the average of @xmath66 .    given the above lemma , we can prove the following theorem :    @xmath57 + [ ind ] let @xmath67 be a symmetric , positive definite @xmath13-matrix and @xmath14 an arbitrary @xmath68-matrix .",
    "let @xmath25 be randomly chosen from @xmath21 according to the unique @xmath21-invariant distribution ( i.e. the haar measure ) . introducing the operator norm @xmath69 we have @xmath70 with probability at least @xmath71 for some constant @xmath64 ( independent of @xmath72 ) .",
    "proof : for an arbitrary orthonormal system @xmath73 we have @xmath74 we define the unit vectors @xmath75 dropping the index @xmath76 , we introduce the function @xmath77 for a randomly chosen @xmath20 , @xmath60 is a randomly chosen unit vector according to a uniform prior on the @xmath33-dimensional sphere @xmath61 .",
    "the average of @xmath78 is given by @xmath79 .",
    "the lipschitz constant is given by the operator norm of @xmath67 , i.e. , @xmath80 an arbitrarily chosen @xmath76 satisfies @xmath81 with probability @xmath82 .",
    "this follows from lemma  [ lev ] after replacing @xmath83 with @xmath84 .",
    "hence @xmath85 due to @xmath86 we thus have @xmath87 @xmath88    it is convenient to introduce @xmath89 as a scale - invariant measure for the strength of the violation of the equality of the expressions ( [ tracecomp ] ) .",
    "we now restrict the attention to two special cases where we can show that @xmath90 is non - zero for the backward direction .",
    "first , we restrict the attention to deterministic models @xmath91 and the case that @xmath92 where @xmath14 has rank @xmath33 .",
    "this ensures that the backward model is also deterministic , i.e. , @xmath93 with @xmath94 denoting the pseudo inverse .",
    "the following theorem shows that @xmath95 implies @xmath96 :    @xmath57 + let @xmath33 and @xmath48 denote the dimensions of @xmath2 and @xmath1 , respectively . if @xmath12 and @xmath17 , the covariance matrices satisfy @xmath97 where @xmath98 is a real - valued random variable whose distribution is the empirical distribution of eigenvalues of @xmath16 , i.e. , @xmath99 for all @xmath100 .",
    "proof : we have @xmath101 using @xmath102 and taking the logarithm we obtain @xmath103 then the statement follows from @xmath104 @xmath88    note that the term @xmath105 in eq .",
    "( [ delta ] ) will not converge to zero for dimension to infinity if the random matrices @xmath14 are drawn in a way that ensures that the distribution of @xmath98 converges to some distribution on @xmath106 with non - zero variance . assuming this",
    ", @xmath107 tends to some negative value if @xmath108 tends to zero for @xmath109 .",
    "we should , however , mention a problem that occurs for @xmath110 in the noise - less case discussed here : since @xmath22 has only rank @xmath33 , we could equally well replace @xmath28 with some other matrix @xmath111 that coincides with @xmath28 on all of the observed @xmath112-values .",
    "for those matrices @xmath111 , the value @xmath90 can get closer to zero because the term @xmath113 expresses the fact that the image of @xmath22 is orthogonal to the kernel of @xmath28 , which is already untypical for a generic model .",
    "it turns out that the observed violation of the multiplicativity of traces can be interpreted in terms of relative entropy distances . to show this , we need the following result :    @xmath57 + let @xmath67 be the covariance matrix of a centralized non - degenerate multi - variate gaussian distribution @xmath114 in @xmath33 dimensions .",
    "let the anisotropy of @xmath67 be defined by the relative entropy distance to the closest isotropic gaussian @xmath115 then @xmath116    proof : the relative entropy distance of two centralized gaussians with covariance matrices @xmath117 in @xmath33 dimensions is given by @xmath118 setting @xmath119 , the distance is minimized for @xmath120 , which yields eq .",
    "( [ d ] ) .",
    "@xmath88    straightforward computations show :    @xmath57 + let @xmath67 and @xmath14 be @xmath13-matrices with @xmath67 positive definite .",
    "then @xmath121    hence , for independently chosen @xmath14 and @xmath67 , the anisotropy of the output covariance matrix @xmath122 is approximately given by the anisotropy of @xmath67 plus the anisotropy of @xmath16 , which is the anisotropy of the output that @xmath14 induces on an isotropic input .",
    "for the backward direction , the anisotropy is smaller than the typical value .",
    "we now discuss an example with a stochastic relation between @xmath2 and @xmath1 .",
    "we first consider the general linear model @xmath123 where @xmath14 is an @xmath124 matrix and @xmath125 is a noise term ( statistically independent of @xmath2 ) with covariance matrix @xmath126 .",
    "we obtain @xmath127 the corresponding backward model , this induces a joint distribution @xmath4 that does not admit a linear backward model with an _ independent _ noise @xmath128 , we can then only obtain _ uncorrelated _ noise .",
    "we could in principle already use this fact for causal inference @xcite",
    ". however , our method also works for the gaussian case and if the dimension is too high for testing higher - order statistical dependences reliably . ]",
    "reads @xmath129 with @xmath130 now we focus on the special case where @xmath14 is an orthogonal transformation and @xmath125 is isotropic , i.e. , @xmath131 with @xmath132 .",
    "we then obtain a case where @xmath22 and @xmath133 are related in a way that makes @xmath134 positive :    @xmath57 + let @xmath47 with @xmath135 and the covariance matrix of @xmath125 be given by @xmath136 .",
    "then we have @xmath137    proof : we have @xmath138 with @xmath139 . therefore , @xmath140 one checks easily that the orthogonal transformation @xmath14 is irrelevant for the traces and we thus have @xmath141 where @xmath98 is a random variable of which distribution reflects the distribution of eigenvalues of @xmath67 .",
    "the function @xmath142 is monotonously increasing for positive @xmath143 and @xmath144 and thus also @xmath145 .",
    "hence @xmath146 and @xmath147 are positively correlated , i.e. , @xmath148 for all distributions of @xmath98 with non - zero variance .",
    "hence the logarithm is positive and thus @xmath149 .",
    "@xmath88    since the violation of the equality of the terms in ( [ tracecomp ] ) can be in both directions , we propose to prefer the causal direction for which @xmath90 is closer to zero .",
    "motivated by the above theoretical results , we propose to infer the causal direction using alg .",
    "[ algtr ] .",
    "@xmath150    in light of the theoretical results , the following issues have to be clarified by experiments with simulated data :    1 .   is the limit for dimension to infinity already justified for moderate dimensions ? 2 .",
    "is the multiplicativity of traces sufficiently violated for noisy models ?",
    "furthermore , the following issue has to be clarified by experiments with real data :    1 .",
    "is the behaviour of real causal structures qualitatively sufficiently close to our model with independent choices of @xmath14 and @xmath23 according to a uniform prior ?    for the simulated data , we have generated random models @xmath47 as follows :",
    "we independently draw each element of the @xmath68 structure matrix @xmath14 from a standardized gaussian distribution .",
    "this implies that the distribution of column vectors as well as the distribution of row vectors is isotropic . to generate a random covariance matrix @xmath23",
    ", we similarly draw an @xmath13 matrix @xmath151 and set @xmath152 . due to the invariance of our decision rule with respect to the scaling of @xmath14 and @xmath23",
    ", the structure matrix and the covariance can have the same scale without loss of generality .",
    "the covariance @xmath126 of the noise is generated in the same way , although with an adjustable parameter @xmath153 governing the scaling of the noise with respect to the signal : @xmath154 yields the deterministic setting , while @xmath155 equates the power of the noise to that of the signal .",
    "first , we demonstrate the performance of the method in the close - to deterministic setting ( @xmath156 ) as a function of the dimensionality @xmath157 of the simulations , ranging from dimension 2 to 50 . to show that the method is feasible even with a relatively small number of samples , we choose the number of samples @xmath158 to scale with the dimension as @xmath159 .",
    "( note that we must have @xmath160 to obtain invertible estimates of the covariance matrices . )",
    "the resulting proportion of correct vs wrong decisions is given in fig .",
    "[ fig : simulations]a , with the corresponding values of @xmath90 in fig .",
    "[ fig : simulations]b . as can be seen , even at as few as 5 dimensions and 10 samples , the method is able to reliably identify the direction of causality in these simulations .     +    to illustrate the degree to which identifiability is hampered by noise , the solid line in fig .",
    "[ fig : simulations]c gives the performance of the method for a fixed dimension ( @xmath161 ) and fixed sample size ( @xmath162 ) as a function of the noise level @xmath153 .",
    "as can be seen , the performance drops markedly as @xmath153 is increased .",
    "as soon as there is significantly more noise than signal ( say , @xmath163 ) , the number of samples is not sufficient to reliably estimate the required covariance matrices and hence the direction of causality .",
    "this is clear from looking at the much better performance of the method when based on the exact , true covariance matrices , given by the dashed lines . in fig .",
    "[ fig : simulations]d we show the corresponding values of @xmath90 , from which it is clear that the estimate based on the samples is quite biased for the forward direction .    as experiments with real data with known ground truth ,",
    "we have chosen @xmath164 pixel images of handwritten digits @xcite .",
    "as the linear map @xmath14 we have used both random local translation - invariant linear filters and also standard blurring of the images .",
    "( we added a small amount of noise to both original and processed images , to avoid problems with very close - to singular covariances . )",
    "see fig .",
    "[ fig : digits ] for some example original and processed image pairs .",
    "the task is then : given a sample of pairs @xmath165 consisting of the picture @xmath166 and its processed counterpart @xmath167 infer which of the set of pictures @xmath168 or @xmath112 are the originals ( ` causes ' ) . by partitioning the image set by the digit class ( 0 - 9 ) , and by testing a variety of random filters ( and the standard blur ) , we obtained a number of test cases to run our algorithm on . out of the total of 100 tested cases ,",
    "the method was able to correctly identify the set of original images 94 times , with 4 unknowns ( i.e.  only two falsely classified cases ) .",
    "these simulations and experiments are quite preliminary and mainly serve to illustrate the theory developed in the paper .",
    "they point out at least one important issue for future work : the construction of unbiased estimators for the trace values or the @xmath90 .",
    "the systematic deviation of the sample - based experiments from the covariance - matrix based experiments in fig .",
    "[ fig : simulations]c  d suggest that this could be a major improvement .",
    "in this section , we want to rephrase our theoretical results in a more abstract way to show the general structure . we have rejected the causal hypothesis @xmath3 if we observe that @xmath169 attains values that are not typical among the set of transformed input covariance matrices @xmath54 . in principle",
    ", we could have any function @xmath170 that maps the output distribution @xmath7 to some value @xmath171 .",
    "moreover , we could have any group @xmath172 of transformations @xmath173 on the input variable @xmath2 that define transformed input distributions via @xmath174 applying the conditional @xmath6 to @xmath175 defines output distributions @xmath176 that we compare to @xmath7 .",
    "in particular , we check whether the value @xmath171 is typical for the set @xmath177 .    @xmath57 + let @xmath2 and @xmath1 be random variables with joint distribution @xmath4 and @xmath172 be a group of transformations of the value set of @xmath2 .",
    "let @xmath178 be some real - valued function on the probability distributions of @xmath1 .",
    "the causal hypothesis @xmath3 is unlikely if @xmath171 is smaller or greater than the big majority of all distributions @xmath179    our prior knowledge about the structure of the data set determines the appropriate choice of @xmath172 .",
    "the idea is that @xmath172 expresses a set of transformations that generate input distributions @xmath175 that we consider equally likely .",
    "the permutation of components of @xmath2 also defines an interesting transformation group . for time",
    "series , the translation group would be the most natural choice .    interpreting this approach in a bayesian way",
    ", we thus use symmetry properties of priors without the need to explicitly define the priors themselves .",
    "our experiments with simulated data suggest that the method performs quite well already for moderate dimensions provided that the noiselevel is not too high .",
    "certainly , the model of drawing @xmath23 according to a distribution that is invariant under @xmath180 may be inappropriate for many practical applications .",
    "however , as the example with diagonal matrices in section  [ mot ] shows , the statement @xmath95 holds for a much broader class of models . for this reason , the method could also be used as a sanity check for causal hypotheses among one - dimensional variables .",
    "assume , for instance , one has a causal dag @xmath172 connecting @xmath181 variables attaining values in @xmath106 .",
    "if @xmath182 is an ordering that is consistent with @xmath172 , we define @xmath183 and @xmath184 and check the hypothesis @xmath185 using our method .",
    "provided that the true causal relations are linear , such a hypothesis should be accepted for every possible ordering that is consistent with the true causal dag .",
    "this way one could , for instance , check the causal relation between genes by clustering their expression levels to vector - valued variables .",
    "d.  heckerman , c.  meek , and g.  cooper .",
    "a bayesian approach to causal discovery . in c.",
    "glymour and g.  cooper , editors , _ computation , causation , and discovery _ , pages 141165 , cambridge , ma , 1999 .",
    "mit press .",
    "y.  kano and s.  shimizu .",
    "causal inference using nonnormality . in _ proceedings of the international symposium on science of modeling , the 30th anniversary of the information criterion _ , pages 261270 , tokyo , japan , 2003 .",
    "p.  hoyer , d.  janzing , j.  mooij , j.  peters , and b  schlkopf .",
    "nonlinear causal discovery with additive noise models . in d.",
    "koller , d.  schuurmans , y.  bengio , and l.  bottou , editors , _ advances in neural information processing systems 21 _ , vancouver , canada , 2009 . mit press .",
    "y.  le cun , b.  boser , j.  s. denker , d.  henderson , r.  e. howard , w.  hubbard , and l.  d. jackel .",
    "handwritten digit recognition with a back - propagation network . in _ advances in neural information processing systems _",
    ", pages 396404 .",
    "morgan kaufmann , 1990 ."
  ],
  "abstract_text": [
    "<S> we describe a method for inferring linear causal relations among multi - dimensional variables . </S>",
    "<S> the idea is to use an asymmetry between the distributions of cause and effect that occurs if both the covariance matrix of the cause and the structure matrix mapping cause to the effect are independently chosen . </S>",
    "<S> the method works for both stochastic and deterministic causal relations , provided that the dimensionality is sufficiently high ( in some experiments , @xmath0 was enough ) . </S>",
    "<S> it is applicable to gaussian as well as non - gaussian data . </S>"
  ]
}