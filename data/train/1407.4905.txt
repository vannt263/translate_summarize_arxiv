{
  "article_text": [
    "random walks in random environments ( rwre ) form a subclass of canonical models in the more general framework of random motions in random media that is widely used in physics .",
    "these models go back to the pioneer works of  @xcite , who introduced them to describe dna replication and of  @xcite who used them in the field of metallurgy .",
    "a more complete list of application fields may be found in the introduction of @xcite as well as in the references therein .",
    "these models have been intensively studied in the last four decades , mostly in the physics and probability theory literature .",
    "some surveys on the topic include  @xcite .",
    "+ statistical issues raised by those processes have been overlooked in the literature until very recently , when new biophysics experiments produced data that can be modeled ( at least in an ideal - case setup ) by rwre  @xcite .",
    "consequently , a new series of works appeared on statistical procedures aiming at estimating parameters from rwre data .",
    "another motivation to these studies comes from the fact that rwre are closely linked to branching processes with immigration in random environments ( bpire ) and that the statistical issues raised in one context may potentially have an impact in the other .",
    "@xcite investigates the local time of the one dimensional recurrent rwre in order to estimate the trajectories of the underlying random potential . in  @xcite , the ideal - case model from  @xcite",
    "is considered : a ( finite length ) dna molecule is unzipped several times and some device translates these unzippings into random walks along the dna molecule , whose sequence of bases is the random environment . here , the goal is to reconstruct this environment and thus achieve sequencing of the molecule . @xcite",
    "prove that a bayesian estimator ( maximum a posteriori ) of this sequence of bases is consistent as the number of unzippings increases and they characterize the probability of reconstruction error . in a different setting , several authors have considered the information on the environment that is contained in one single trajectory of the walk with infinite length . in their pioneer work ,",
    "@xcite consider a very general rwre and provide equations linking the distribution of some statistics of the trajectory to some moments of the environment distribution . in the specific case of a one - dimensional nearest neighbor path , those equations give moment estimators for the environment distribution parameters .",
    "more recently , @xcite studied a maximum likelihood estimator ( mle ) in the specific context of a one - dimensional nearest neighbor path in transient ballistic regime .",
    "they prove the consistency of this estimator ( as the length of the trajectory increases ) . from a numerical point of view",
    ", this mle outperforms the moment estimator constructed from the work of @xcite . in a companion article  @xcite ,",
    "they have further studied the asymptotic normality of the mle ( still in the ballistic regime ) , showed its asymptotic efficiency and constructed confidence intervals for the parameters .",
    "this work has been extended to the transient sub - ballistic regime in  @xcite . in this body of work on maximum likelihood procedures ,",
    "the results rely on the branching structure of the sequence of the number of left steps performed by the walk , which was originally observed by @xcite . in the recurrent case ,",
    "as the walk visits every sites infinitely often , this branching process of left steps explodes and the same approach is useless there . in theory",
    ", it is possible in this case to estimate the environment itself at each site , and then show that the empirical measure converges to its distribution .",
    "the problem with such a `` naive '' approach is the localization phenomena of recurrent rwre , discovered by @xcite : most of the sites visited by the rwre will be extremely few visited , because the walk spends a majority of its time in the valleys of the potential @xcite .",
    "this non uniformity is automatically handled with the approach followed by @xcite and the authors establish consistency of two estimators , a mle and a maximum pseudo - likelihood estimator .",
    "+ we now stress the fact that all the previously mentioned statistical works but the one from @xcite are valid in the case of an environment composed of independent and identically distributed ( i.i.d . ) random variables . while very convenient , this assumption might be restrictive in some contexts , e.g. dna modeling .",
    "in the present work , we investigate the statistical estimation of a parametric markov environment from a single trajectory of a one - dimensional nearest - neighbor path , when its length increases to infinity .",
    "we consider the case of a transient walk in the ballistic regime .",
    "our contribution is twofold : first , we show how the problem is cast into the one of estimating the parameter of a hidden markov model ( hmm ) , or more specifically of a first - order autoregressive process with markov regime . indeed , the rwre itself is not a hmm but the branching process of the sequence of left steps performed by the walk , is .",
    "second , we prove that the bivariate markov chain that defines the underlying autoregressive process is harris positive and we exhibit its stationary distribution . as a consequence , we can rely on previously established results for these autoregressive processes with markov regime @xcite and thus obtain the consistency and asymptotic normality of the mle for the original nearest - neighbor path in markov environment . + roughly speaking , an autoregressive model with markov regime is a bivariate process where the first component forms a latent ( unobserved ) markov chain while conditionally on this first component , the second one has the structure of an autoregressive process . these processes",
    "form a generalization of hidden markov models ( hmm ) , in which the first component remains a latent markov chain , while the second forms a sequence of independent observations , conditionally on the first .",
    "hmm have been introduced by @xcite with finite - latent and observed - state spaces .",
    "statistical properties of the mle in hmm form a rich literature ; a non exhaustive list would start with the seminal work of @xcite , include the developments of @xcite and finish with the latest results from @xcite .",
    "a general introduction to hmm may be found in the survey by @xcite and the book by @xcite .    while it is often believed that autoregressive processes with markov regime are straightforward generalizations of hmm ( and this is indeed the case concerning e.g. algorithmic procedures ) , the statistical properties of these models are slightly more difficult to obtain , ( see e.g. * ? ? ? * for model selection issues ) .",
    "as for the convergence properties of the mle , only the article by @xcite considers the autoregressive case ( instead of hmm ) explaining why we focus on their results in our context .",
    "it is also worth noticing that many of the previous results  ( with exception of * ? ? ?",
    "* ) require uniform positivity of the transition density of the latent markov chain , which might not be satisfied in some applications ( particularly in the case of an unbounded latent state space ) . as in our case",
    ", the latent state space corresponds to the environment state space and is included in @xmath0 , we do not face such problems .",
    "moreover , we stress that the results in @xcite rely on rather weak assumptions ( compared to previous results in * ? ? ?",
    "* ; * ? ? ?",
    "* on which they are partly built ) . as a consequence ,",
    "the assumptions that we obtain on rwre are also rather weak and will be satisfied in many contexts . +",
    "this article is organized as follows .",
    "our one - dimensional nearest - neighbor path in markov environment is described in section  [ sec : rwmre ] .",
    "then we explain why the direct likelihood approach may not be followed ( section  [ sec : pbm ] ) and cast the estimation problem as the one of parameter estimation in a hidden markov model ( section  [ sec : hmm ] ) . after having set the scene , we state the assumptions ( on the rwre ) and results in section  [ sec :",
    "hyp_res ] .",
    "we prove that ( under classical assumptions ) the mle is consistent and asymptotically normal .",
    "section  [ sec : exsim ] illustrates our results : we start by explaining how the likelihood may be computed ( section  [ sec : comput ] ) , then we explore different examples and describe our assumptions in these cases ( section  [ sec : examples ] ) and close the section with synthetic experiments on a simple example ( section  [ sec : simus ] ) .",
    "the proofs of our results are presented in section  [ sec : proofs ] .",
    "the main point is to establish that the bivariate markov chain that underlies the hmm is positive harris recurrent ( section  [ sec : propr ] )",
    ". then consistency , asymptotic normality and efficiency ( i.e. the asymptotic variance is the inverse of the fisher information ) follow from  @xcite by proving that our assumptions on the rwre imply theirs on the hmm ( sections  [ sec : cons_proof ] and  [ sec : an_proof ] , respectively ) .",
    "we start this section by describing the random environment .",
    "let @xmath1 be a closed subset of @xmath0 either finite , discrete or continuous , and @xmath2 the associated borel @xmath3-field .",
    "the environment is given by @xmath4 , a positive harris recurrent , aperiodic and stationary markov chain with values in @xmath1 and transition kernel @xmath5 $ ] .",
    "we suppose that the transition kernel @xmath6 depends on some unknown parameter @xmath7 and that @xmath7 belongs to some compact space @xmath8 .",
    "moreover , @xmath9 is absolutely continuous either with respect to ( w.r.t . )",
    "the lebesgue measure on @xmath0 when @xmath1 is continuous or w.r.t .",
    "the counting measure when @xmath1 is discrete , with density denoted by @xmath10 .",
    "we denote by @xmath11 the density of its associated stationary distribution .",
    "let us denote by @xmath12 the law of the environment @xmath13 on @xmath14 and @xmath15 the corresponding expectation .",
    "now , conditionally on the environment , the law of the random walk @xmath16 is the one of the time homogeneous markov chain on @xmath17 starting at @xmath18 and with transition probabilities @xmath19 the measure @xmath20 on @xmath21 is usually referred to as the quenched law of walk @xmath22 .",
    "note that this conditional law does not depend on the parameter @xmath7 but only on the environment @xmath13 at the current site @xmath23 .",
    "we also denote by @xmath24 the corresponding transition density ( w.r.t . to counting measure ) , namely @xmath25 where @xmath26 denotes the indicator function .",
    "next we define the measure @xmath27 on @xmath28 through @xmath29 the second marginal of @xmath30 ( that on @xmath31 ) , denoted also @xmath32 when no confusion occurs , is called the annealed law of walk @xmath22 .",
    "we denote by @xmath33 the corresponding expectation .",
    "note that the first marginal of @xmath32 is the law of the markov chain @xmath34 denoted by @xmath35    for all @xmath36 , we let @xmath37 in the case of an i.i.d .",
    "environment @xmath13 , @xcite gives the classification of @xmath22 between transient or recurrent cases according to whether @xmath38 is different or not from zero .",
    "for stationary ergodic environments , which is the case here , @xcite establishes that this characterization remains valid .",
    "thus , if @xmath39 then the walk is transient to the right , namely @xmath40 let @xmath41 be the first hitting time of the positive integer @xmath42 , @xmath43 and define @xmath44 theorem 4.1 in @xcite shows that if the environment satisfies the condition @xmath45 then the speed of the walk is strictly positive .",
    "namely , @xmath46 , the ratio @xmath47 converges to a finite limit as @xmath42 increases .",
    "thus   gives the so - called ballistic condition on the random walk with markov environment .",
    "note that in the i.i.d .",
    "case , this condition reduces to @xmath48 .",
    "moreover , in the non independent case , when the distribution of the environment is _ uniquely ergodic _ , namely @xmath13 is not i.i.d . and admits a unique invariant distribution , @xcite establishes that transience ( namely @xmath49 ) automatically implies ballistic regime ( see lemma 6.1 in * ? ? ?",
    "since in our context we assume that the markov environment @xmath13 admits a unique invariant distribution , the ballistic assumption thus reduces to @xmath50 in the following , we consider a transient to the right ballistic process @xmath22 .",
    "we consider a finite trajectory @xmath51 from the process @xmath22 , stopped at the first hitting time of a positive integer @xmath52 .",
    "the apparently more general case of a sequence @xmath53 of observations is discussed in remark  [ rem : nonseq ] .",
    "we assume that this sequence of observations is generated under @xmath54 for a true parameter value @xmath55 belonging to the interior @xmath56 of @xmath57 .",
    "our goal is to estimate this parameter value @xmath55 from the sequence of observations @xmath58 using a maximum likelihood approach . to motivate the following developments",
    ", we will first explain why we can not directly rely on the likelihood of these observations .",
    "indeed , let @xmath59 be the set of sites @xmath60 visited by the process up to time @xmath41 , namely @xmath61 under the assumption of a transient ( to the right ) process , the random set @xmath59 is equal to @xmath62\\!]$}}$ ] , where @xmath63 and @xmath64\\!]$}}$ ] denotes the set of integers between @xmath65 and @xmath66 for any @xmath67 in @xmath17 . here , @xmath68 is the smallest integer value visited by the process @xmath58 .",
    "we also introduce @xmath69 which is the random environment restricted to the set of sites visited by @xmath58 .",
    "now , the likelihood of @xmath58 is given by the following expression @xmath70    computing the likelihood from the above expression would require to compute @xmath71 integral terms ( where @xmath72 denotes cardinality ) . as @xmath73 ,",
    "this means that using a discretization method over @xmath74 points for each integral ( or letting @xmath74 be the cardinality of @xmath1 ) would result in summing over at least @xmath75 different terms .",
    "this is unfeasible but for small values of @xmath42 .",
    "moreover , the above expression is not well suited for studying the convergence properties of this likelihood .",
    "following @xcite , instead of focusing on the observed process",
    "@xmath58 , we will rather consider the underlying sequence @xmath76 of the number of left steps of the process @xmath58 at the sequence of sites @xmath77 and construct our estimator from this latter sequence . though we do not need it , note that it is argued in @xcite that the latter is in fact a sufficient statistic ( at least asymptotically ) for the parameter @xmath7 . in the next section ,",
    "we show that in the case of a markov environment , this process exhibits a hidden markov structure .",
    "moreover for transient rwre , this process is recurrent , allowing us to study the convergence properties of mle .",
    "we define the sequence of left steps at each visited site from the ( positive part of the ) trajectory @xmath58 as follows .",
    "let @xmath78 it is observed by @xcite in the case of an i.i.d",
    ". random environment that the sequence @xmath79 is distributed as a branching process with immigration in a random environment ( bpire ) .",
    "we will first show that this remains true in the case of a markov environment . to this aim ,",
    "let us introduce the time reversed environment @xmath80 defined by @xmath81 for all @xmath60 .",
    "it is a markov chain on @xmath1 with stationary density @xmath11 and transition @xmath82 defined by @xmath83 now we recursively define a sequence of random variables @xmath84 with @xmath85 and @xmath86 where for all @xmath87 , the random variables @xmath88 , are defined on the same probability space as previously , are independent and their conditional distribution , given the environment @xmath89 is @xmath90 here , @xmath91 is defined similarly as @xmath20 for the environment @xmath89 replacing @xmath13 . then , conditionally on @xmath89 , the sequence @xmath92 is an inhomogeneous branching process with immigration , with identical offspring and immigration law , given by a geometric distribution ( whose parameter depends on the random environment @xmath89 ) .",
    "moreover , it is easily seen that the annealed distribution of the sequence @xmath93 and that of @xmath94 are the same .",
    "[ lem : eqinlaw ] for any fixed integer @xmath52 , the sequence of left steps @xmath93 has same distribution as @xmath94 under @xmath95 .    for any fixed integer @xmath52 , let @xmath96 denote the time reversed environment starting at @xmath97 .",
    "let also @xmath98 be defined similarly as @xmath20 for the environment @xmath99 replacing @xmath13 .",
    "then it is known that for any sequence @xmath100 , we have the equality @xmath101 ( see for instance section 4.1 in * ? ? ?",
    "now the right - hand side @xmath102 only depends on the environment @xmath103 through the first @xmath104 variables @xmath105 whose distribution under @xmath12 is the same as @xmath106 . as a consequence , using definition   of distribution @xmath95 we obtain the result .",
    "when the environment @xmath13 is composed of i.i.d .",
    "random variables , the resulting sequence @xmath107 is a homogeneous markov chain under @xmath95 ( see e.g. * ? ? ? * ) .",
    "now , when the environment @xmath13 is itself a markov chain , we observe that @xmath84 is distributed as a hidden markov chain , or more precisely as the second marginal of a first order autoregressive process with markov regime @xcite , where the latent sequence is given by @xmath89 .",
    "we state this result as a lemma ( namely lemma  [ lem : bivariate ] below ) even though its proof is obvious and thus omitted .",
    "let us recall that a first order autoregressive process with markov regime ( or markov - switching autoregression ) is a bivariate process @xmath108 such that @xmath109 is a markov chain and conditionally on @xmath89 , the sequence @xmath110 is an inhomogeneous markov chain whose transition from @xmath111 to @xmath112 only depends on @xmath111 and @xmath113 .",
    "for any @xmath114 and @xmath115 , denote @xmath116 and let @xmath117 be the dirac measure at @xmath23 .",
    "let us recall that the process @xmath118 is defined through   and  .    [ lem : bivariate ] under @xmath119 , the process @xmath120 is a first - order autoregressive process with markov regime .",
    "the first component @xmath121 is an homogenous markov chain with transition kernel density @xmath122 and initial distribution @xmath123 .",
    "conditionally on @xmath89 , the process @xmath124 is an inhomogeneous markov chain , starting from @xmath85 and with transitions @xmath125 as a consequence , @xmath120 is a markov chain with state space @xmath126 , starting from @xmath127 and with transition kernel density @xmath128 defined for all @xmath129 by @xmath130    the conditional autoregressive part of the distribution , given by  , is usually referred to as emission distribution .",
    "note that in our framework , this law does not depend on the parameter @xmath7 .    under @xmath95 , the process @xmath131 has initial distribution @xmath127 . in the sequel",
    ", we also need @xmath131 as well as the chain @xmath132 starting from any initial distribution . for any probability @xmath133 on @xmath134 , denote @xmath135 the law of @xmath131 starting from @xmath136 ( note that @xmath137 ) .",
    "denote @xmath138 the corresponding expectation .",
    "in particular , for @xmath139 , we let @xmath140 and @xmath141 be the probability and expectation if @xmath142 .",
    "moreover , when only the first component is concerned and when no confusion occurs , if the chain @xmath89 or @xmath13 starts from its stationary distribution @xmath143 , we still denote this marginal law by @xmath12 and the corresponding expectation by @xmath144 if @xmath89 or @xmath13 start from another initial law , for example @xmath145 , we denote their law by @xmath146 and corresponding expectation @xmath147 . for @xmath148 ,",
    "we let @xmath149 ( resp .",
    "@xmath150 ) be the @xmath3-field induced by the @xmath104 first random variables of the environment ( resp .",
    "of the time reversed environment ) . moreover ,",
    "we denote by @xmath151 and @xmath152 the shifted sequences .",
    "the family of shift operators @xmath153 where @xmath154 is defined by @xmath155 in section  [ sec : proofs ] , we show that under the ballistic assumption , the bivariate kernel @xmath156 is positive harris recurrent and admits a unique invariant distribution with density @xmath157 , for which we give an explicit formula ( see proposition  [ prop : biv_harris ] ) . in the following ,",
    "we let @xmath158 and @xmath159 be the probability and expectation induced when considering the chain @xmath160 under its stationary distribution @xmath157 .",
    "recall that our aim is to infer the unknown parameter @xmath161 , using the observation of a finite trajectory @xmath58 up to the first hitting time @xmath41 of site @xmath42 .",
    "the observed trajectory is transformed into the sequence @xmath162 of the number of left steps of the process @xmath58 at the sequence of sites @xmath77 .",
    "this trajectory is generated under the law @xmath163 ( recall that @xmath163 is a shorthand notation of @xmath164 ) . due to the equality in law given by lemma  [ lem : eqinlaw ] , we can consider that we observe a sequence of random variables @xmath165 which is the second component of an autoregressive process with markov regime described in lemma  [ lem : bivariate ] .",
    "thus under @xmath163 , the law of the mle of these observations is the same than the law of mle built from @xmath165 .    as a consequence , we can now rely on a set of well established techniques developed in the context of autoregressive processes with markov regime , both for computing efficiently the likelihood and for establishing its asymptotic properties . following  @xcite",
    ", we define a conditional log - likelihood , conditioned on an initial state of the environment @xmath166 .",
    "the reason for doing so is that the stationary distribution of @xmath167 and hence the true likelihood , is typically infeasible to compute .",
    "[ def : crit ] fix some @xmath168 and consider the conditional log - likelihood of the observations defined as @xmath169    note that the above expression of the ( conditional ) log - likelihood shares the computational problems mentioned for expression  .",
    "however , in the present context of autoregressive processes with markov regime , efficient computation of this expression is possible .",
    "the key ingredient for this computation ( that also serves to study the convergence properties of @xmath170 ) is to rely on the following additive form @xmath171 where @xmath172 denotes @xmath173 for any integers @xmath174 .",
    "we further develop this point in section  [ sec : comput ] and also refer to @xcite for more details .    the estimator @xmath175 is defined as a measurable choice@xmath176    note that we omit the dependence of @xmath175 on the initial state @xmath177 of the environment .",
    "[ rem : nonseq ] when considering a size-@xmath42 sample @xmath178 instead of a trajectory stopped at random time @xmath41 , we may consider @xmath179 and restrict our attention to the sub - sample @xmath180 . as we consider a transient random walk",
    ", @xmath181 increases to infinity with @xmath42 .",
    "consistency with respect to @xmath42 or @xmath181 is equivalent . now considering the rates , note that in the ballistic case we can easily obtain that @xmath182 for some @xmath183 so that rates of convergence as well as efficiency issues with respect to @xmath181 or @xmath42 are the same .",
    "note that information about @xmath184 has to be extracted first and then the data may be reduced to the sequence of left steps without loosing information .",
    "recall that @xmath185 and @xmath186 are respectively the transition and the invariant probability densities of the environment markov chain @xmath187 with values in @xmath1 , while @xmath188 and @xmath186 are the same quantities for the time reversed chain @xmath89 .",
    "moreover , @xmath1 is a closed subset of @xmath0 so that we can assume that there exists some @xmath189 such that @xmath190.\\ ] ] the above assumption is known as the uniform ellipticity condition .",
    "we also recall that the random variable @xmath191 is defined by  .",
    "( ballistic case ) .",
    "[ hyp : ballistic ] for any @xmath192 , inequality   is satisfied .",
    "[ hyp : noyau ] there exist some constants @xmath193 such that @xmath194    note that it easily follows from this assumption that the stationary density @xmath123 also satisfies @xmath195 moreover , we also get that the time reversed transition density @xmath82 satisfies @xmath196    assumptions  [ hyp : ballistic ] and  [ hyp : noyau ] are used to establish that the bivariate process @xmath197 is positive harris recurrent .",
    "note in particular that the weakest assumptions currently ensuring consistency of the mle in the hmm setting contain positive harris recurrence of the hidden chain @xcite and  [ hyp : noyau ] is further required in the less simple case of an autoregressive model with markov regime @xcite .",
    "the lower bound in  [ hyp : noyau ] may be restrictive in a general hmm setting as it prevents the support @xmath1 from being unbounded .",
    "however here we have @xmath198 and thus  [ hyp : noyau ] is satisfied in many examples ( see section  [ sec : exsim ] ) .",
    "next assumption is classical from a statistical perspective and requires smoothness of the underlying model .",
    "( regularity condition ) .",
    "[ hyp : regular ] for all @xmath199 , the map @xmath200 is continuous .    in order to ensure identifiability of the model , we naturally require identifiability of the parameter from the distribution of the environment .",
    "( identifiability condition ) .",
    "[ hyp : ident ] @xmath201    [ thm : consistency ] under assumptions  [ hyp : ballistic ] to  [ hyp : ident ] , the maximum likelihood estimator @xmath175 converges @xmath163-almost surely to the true parameter value @xmath55 as @xmath42 tends to infinity .",
    "we now introduce the conditions that will ensure asymptotic normality of @xmath202 under @xmath163 . in the following , for any function @xmath203 ,",
    "we let @xmath204 and @xmath205 denote gradient vector and hessian matrix , respectively .",
    "moreover , @xmath206 is the uniform norm ( of a vector or a matrix ) .",
    "again , next condition is classical and requires regularity of the mapping underlying the statistical model .",
    "[ hyp : regular2 ] for all @xmath207 , the map @xmath208 is twice continuously differentiable on @xmath209 .",
    "moreover , @xmath210    following the notation from section 6.1 in  @xcite , we now introduce the asymptotic fisher information matrix .",
    "we start by extending the chain @xmath211 with indexes in @xmath212 to a stationary markov chain @xmath213 indexed by @xmath17 .",
    "let us recall that @xmath158 and @xmath159 respectively denote probability and expectation under the stationary distribution @xmath157 of the chain @xmath214 .",
    "for any @xmath215 , we let @xmath216 note that this expression derives from fisher identity stated in @xcite . indeed , under general assumptions , the score function equals the conditional expectation of the complete score , given the observed data . as the emission distribution @xmath217",
    "does not depend on the parameter @xmath7 , the complete score reduces to a sum of terms involving @xmath122 only .",
    "lemma 10 in  @xcite establishes that for any @xmath218 , the sequence @xmath219 converges in @xmath220 to some limit @xmath221 . from this quantity",
    ", we may define @xmath222 where by convention @xmath223 is a row vector and @xmath224 is the transpose vector of @xmath225 .",
    "then , @xmath226 is the fisher information matrix of the model .",
    "we can now state the asymptotic normality result .",
    "[ thm : an ] under assumptions  [ hyp : ballistic ] to  [ hyp : regular2 ] , if the asymptotic fisher information matrix @xmath227 defined by   is invertible , we have that @xmath228    note that the definition of @xmath229 is not constructive .",
    "in particular , asymptotic normality of the mle requires that @xmath227 is invertible but this may not be ensured through more explicit conditions on the original process .",
    "however , the fisher information may be approximated through the hessian of the log - likelihood . indeed ,",
    "theorem  3 in  @xcite states that the normalized hessian of the log - likelihood converges to @xmath230 under stationary distribution @xmath231 .",
    "moreover , this result is generalized to obtain convergence under non stationary distribution @xmath163 ( see the proof of theorem 6 in that reference ) .",
    "thus we have @xmath232 in practice , this may be used to approximate the asymptotic variance of the estimator @xmath233 , as illustrated in section  [ sec : simus ] .",
    "the computation of the log - likelihood relies on the following set of equations .",
    "as already noted , we have @xmath234 in this expression , the quantity @xmath235 is the called the _",
    "prediction filter_. it is a probability distribution on @xmath1 and it is computed through recurrence relations .",
    "indeed , we have @xmath236 where @xmath237 means proportional to ( up to a normalizing constant ) .    when @xmath1 is discrete , the integral terms over @xmath1 reduce to sums and computing the prediction filter recursively enables to compute the log - likelihood of the observations , and then the mle .",
    "we illustrate these computations in the case of example  [ ex : markov_fini ] below as well as in section  [ sec : simus ] . when @xmath1 is continuous , approximation methods are required , e.g. particle filters or monte carlo expectation - maximisation ( ` em ` ) algorithms .",
    "we refer to section 8 in  @xcite for more details .",
    "note that in any case , optimisation of the log - likelihood is either done through ` em ` algorithm  @xcite or by direct optimisation procedures , as there is no analytical expression for its maximiser .",
    "thus , the computation of the gradient of this log - likelihood is often used ( e.g. in descent gradient optimisation methods ) .",
    "as soon as we can differentiate under the integral sign ( which is valid under assumption  [ hyp : regular2 ] ) , the gradient function @xmath238 writes @xmath239 db db ' \\big ) .",
    "\\label{eq : gradient}\\end{aligned}\\ ] ] note that the gradient of the prediction filter @xmath240 may be obtained through recurrence relations similar to  .",
    "however , these relations are more involved since the normalizing constant in   depends on @xmath241 and can not be neglected .    to conclude this section",
    ", we mention that computing the hessian of the log - likelihood can be done in a similar way .",
    "in this section , we provide some examples of environments @xmath13 and check the assumptions needed for consistency and asymptotic normality of the mle .",
    "( simple i.i.d .",
    "environment on two values . )",
    "let @xmath242 and @xmath243 with known values @xmath244 and unknown parameter @xmath245\\subseteq ( 0,1)$ ] .",
    "the support of the environment is reduced to @xmath246 .",
    "moreover , we assume that @xmath247 and @xmath57 are such that the process is transient to the right and ballistic . in the i.i.d .",
    "case , the ballistic assumption ( that also implies transience ) reduces to @xmath248 and thus to @xmath249 the log - likelihood of the observations has a very simple form in this setup @xmath250,\\ ] ] and its maximiser @xmath251 is obtained through numerical optimisation .",
    "we refer to  @xcite for previous results obtained in this setup .",
    "assumptions  [ hyp : noyau ] and [ hyp : ident ] are satisfied as soon as @xmath252 $ ] and @xmath253 , respectively .",
    "moreover , assumptions  [ hyp : regular ] and [ hyp : regular2 ] are automatically satisfied . indeed , for any @xmath254 and any @xmath114 , we have @xmath255 as a consequence , theorems  [ thm : consistency ] and [ thm : an ] are valid in this setup .    [",
    "ex : markov_fini ] ( finite markov chain environment . )",
    "let us assume that @xmath246 is fixed and known and the stationary markov chain @xmath13 is defined through its transition matrix @xmath256 where the parameter is @xmath257 ^ 2 $ ] for some @xmath258    note that assumption  [ hyp : noyau ] is satisfied as soon as @xmath259 .",
    "the stationary measure of the markov chain is given by @xmath260 this is automatically a reversible markov chain so that @xmath261 .",
    "the transience condition writes @xmath262 moreover , as soon as @xmath263 the sequence @xmath13 is non independent and the existence of a unique stationary measure for @xmath13 ensures the ballistic regime from transience assumption  ( lemma 6.1 in * ? ? ?",
    "let us now consider the log - likelihood expression in this setup .",
    "as already explained , the key point for computing the log - likelihood in the setup of an autoregressive process with markov regime is to rely on the following additive form @xmath264 where @xmath265 is the prediction filter defined by   and we used @xmath266 . relying on matrix notation ,",
    "we let @xmath267 be the row vector @xmath268 while @xmath269 is the row vector @xmath270 and @xmath224 the transpose vector of @xmath225 .",
    "then we obtain @xmath271.\\ ] ] moreover , the sequence of prediction filters @xmath272 is obtained through the recurrence relations   that in our context , write as @xmath273 now , the gradient function @xmath274 given by   satisfies the following equations @xmath275 \\big(f_{k-1}^{\\t , a_1 } q_\\t    g_{k}^\\intercal \\big)^{-1 } , \\\\      \\partial_\\beta \\ell_n(\\t , a ) = \\sum_{k=1}^n \\big[(\\partial_\\beta f_{k-1}^{\\t , a_1 }      q_\\t + f_{k-1}^{\\t , a_1 } q'_2 )     g_{k}^\\intercal \\big",
    "] \\big(f_{k-1}^{\\t , a_1 } q_\\t    g_{k}^\\intercal \\big)^{-1 } ,     \\end{array } \\right.\\ ] ] where @xmath276 is the row vector with entries @xmath277 and @xmath278 let us denote by @xmath279 the row vector @xmath280 . in the current setup , the derivative of the prediction filter is obtained through @xmath281 and for any @xmath282 , @xmath283 }   { \\big (   f_k^{\\t ,     a_1 } q_\\t \\text{diag}(g_{k+1 } )   \\boldsymbol 1 ^{\\intercal}\\big)^{2 } } \\times f_k^{\\t , a_1 } q_\\t \\text{diag}(g_{k+1 } ) , \\end{aligned}\\ ] ] and a similar equation holds for @xmath284 .    in section  [ sec : simus ] , we provide an illustration of the numerical performances of the maximum likelihood estimator in this setup .",
    "note that second order derivatives of the prediction filter and thus the log - likelihood are obtained similarly .",
    "these are used to estimate the asymptotic covariance matrix of the mle in section  [ sec : simus ] .    to conclude this section , note that the regularity assumptions  [ hyp : regular ] and  [ hyp : regular2 ] are satisfied , as well as the identifiability condition  [ hyp : ident ] , as soon as @xmath285 and @xmath286 . as a consequence ,",
    "theorems  [ thm : consistency ] and [ thm : an ] are valid in this setup .",
    "( dna unzipping . )",
    "we consider the context of dna unzipping studied in @xcite where the goal is the sequencing of a molecule ( see also * ? ? ?",
    "the physical experiment consists in observing many different unzippings of a dna molecule which , due to its physical properties , may naturally ( re)-zip . in this context",
    ", the random walk @xmath22 represents the position of the fork at each time @xmath287 of the experiment , or equivalently the number of currently unzipped bases of the molecule . in the previous works , the authors are interested in the observation of many finite trajectories of the random walk in this random environment",
    "here , we consider the different problem of a single unzipping of a sufficiently long molecule .",
    "let @xmath288 denote the finite nucleotide alphabet .",
    "the sequence of bases @xmath289 of the ( finite length ) molecule are unknown and induce a specific environment that will be considered as random .",
    "more precisely , the conditional transitions of the random walk are given by @xmath290 where @xmath291 .",
    "the known parameter @xmath292 is the work to stretch under a force @xmath293 the open part of the two strands , it can be adjusted but is constant during the unzipping .",
    "parameter @xmath294 is also known and proportional to the inverse of temperature . the quantity",
    "@xmath295 is the binding energy that takes into account additional stacking effects and therefore depends on the base values at the @xmath296-th and also at the @xmath23-th positions .",
    "table  [ tab1 ] gives these binding energies at room temperature ( see * ? ? ?",
    "* ) .    .binding free energies ( units of @xmath297 ) . [ cols=\"<,<,<,<,<\",options=\"header \" , ]",
    "in this section , we investigate the properties of the bivariate process @xmath298 , namely we show that it is positive harris recurrent and we exhibit its invariant distribution .",
    "let us first define @xmath299 for the time reversed environment @xmath89 similarly as @xmath191 from equation   by @xmath300 we first remark that condition   writes the same for @xmath13 and @xmath89 so that environment @xmath89 is ballistic under assumption  [ hyp : ballistic ] and thus @xmath301 .",
    "moreover , under assumptions  [ hyp : ballistic ] and  [ hyp : noyau ] , we obtain the following uniform ballistic condition on the time reversed environment @xmath302 for some positive and finite constant @xmath303 .",
    "indeed , the lower bound follows from @xmath304 , by definition of @xmath191 .",
    "now , for any @xmath305 , we let @xmath306 .",
    "the upper bound is obtained through @xmath307 = 1+\\int_s \\tilde b{\\mathbf{e}^{\\theta}}_b ( \\breve r ) \\breve q_\\t(a , b ) db\\\\ & \\leq 1+\\frac{(1-\\eps)\\sigma_+}{\\eps\\sigma_-}{\\mathbf{e}^{\\theta}}(\\breve r),\\end{aligned}\\ ] ] where the first equality above is the strong markov property and the inequality uses both   and the lower bound   on the stationary distribution @xmath123 .",
    "[ prop : biv_harris ] under assumptions  [ hyp : ballistic ] and  [ hyp : noyau ] , the markov chain @xmath309 whose transition kernel is given by   is positive harris recurrent and aperiodic with invariant density distribution @xmath157 given by @xmath310    note that @xmath157 is indeed a density .",
    "we first prove that it is the density of an invariant distribution .",
    "thus we want to establish that for any @xmath311 , we have @xmath312 we start by considering the right - hand side of the above equation where we input the expressions for density @xmath157 and kernel @xmath128 .",
    "we let @xmath313   \\binom{x+y}{x } \\breve q_\\t ( a , b ) b^{x+1}(1-b)^y da .",
    "\\end{aligned}\\ ] ] from the definition of @xmath82 and using fubini s theorem for positive functions , we get @xmath314 b^{x+1 } da \\\\ & = \\mu_\\t(b ) \\int_{s } q_\\t(b , a)(1-b)^y { \\mathbf{e}^{\\theta}}_a\\left [ \\frac { r^{-1}b } { [ 1-b(1-r^{-1})]^{y+1 } } \\right ] da\\\\ & = \\mu_\\t(b ) \\int _ { s } q_\\t(b , a ) { \\mathbf{e}^{\\theta}}_a\\left [ \\frac{1}{1 + \\tilde b      r }    \\times \\left ( \\frac{1-b}{1-b+br^{-1 } } \\right)^y \\right ] da.\\end{aligned}\\ ] ] now , applying markov s property and the definition of the shift operator , we obtain @xmath315\\right ) \\\\ & = \\mu_\\t(b){\\mathbf{e}^{\\theta}}_b\\left    ( { \\mathbf{e}^{\\theta}}_{b}\\left [   \\frac{1}{1 + \\tilde b      r }     \\times \\left ( \\frac{\\tilde   b r}{1+\\tilde b r } \\right)^y      \\circ \\tau^1 \\big|\\f_1 \\right]\\right ) \\\\ & = \\mu_\\t(b){\\mathbf{e}^{\\theta}}_b\\left   ( { \\mathbf{e}^{\\theta}}_{b}\\left [   \\frac{1}{1+\\tilde b+\\tilde   b        \\w_1+\\ldots }    \\times   \\left (    \\frac{\\tilde    b+\\tilde   b\\tilde          \\w_1+\\ldots}{1+\\tilde b+\\tilde   b\\tilde \\w_1+\\ldots } \\right)^y      \\big| \\f_1 \\right]\\right ) \\\\ & = \\mu_\\t(b){\\mathbf{e}^{\\theta}}_b(r^{-1}(1-r^{-1})^y).\\end{aligned}\\ ] ] this concludes the validity of  .",
    "as the marginal process @xmath316 is aperiodic , this is also the case for the time reversed marginal process @xmath317 and for the bivariate process @xmath308 .",
    "following theorem  9.1.8 in @xcite , we want to prove that the markov chain @xmath318 is @xmath319-irreducible for some probability measure @xmath319 and that there exists a petite set @xmath320 and a function @xmath321 such that      for all @xmath324 and @xmath325 let @xmath326 be the projection of @xmath327 onto @xmath1 when @xmath328 and onto @xmath212 when @xmath329 .",
    "we also let @xmath330 be the first hitting time of the set @xmath327 by the chain @xmath318 .",
    "thanks to assumptions  [ hyp : noyau ] and  , we can write @xmath331 hence the markov chain is @xmath332-irreducible ( see section 4.2 in * ? ? ?",
    "* ) , where the measure @xmath332 defined on @xmath333 by @xmath334 is a probability measure . from proposition 4.2.2 in @xcite ,",
    "the chain is also @xmath319-irreducible .",
    "thanks to assumption  [ hyp : noyau ] again , we can easily see that for all @xmath335 , the set @xmath336\\!]$}}$ ] is a small set and as a consequence a petite set .",
    "indeed , for any @xmath337\\!]$}}$ ] we have @xmath338 let @xmath339 by using  , function @xmath340 is finite .",
    "moreover , we get that if @xmath341 then @xmath342 which proves that for all @xmath343 , the set @xmath344 is a petite set .",
    "now , we consider @xmath345\\\\   & = ( x+1){\\mathbf{e}^{\\theta}}_a[\\tilde \\w_{-1 } { \\mathbf{e}^{\\theta}}_a[\\breve r\\circ\\tau^1|\\breve { \\f}_1]]\\\\ & = ( x+1){\\mathbf{e}^{\\theta}}_a[\\tilde \\w_{-1}(1+\\tilde \\w_{-2}+\\tilde \\w_{-2}\\tilde \\w_{-3}+\\dots ) ] \\\\ & = ( x+1){\\mathbf{e}^{\\theta}}_a(\\breve r-1 ) .",
    "\\end{aligned}\\ ] ] note also that @xmath346 . as a consequence , for all @xmath347\\!]$}}$ ]",
    "we have @xmath348 .",
    "this concludes the proof of the proposition .",
    "consistency of the maximum likelihood estimator is given by theorem 1 in @xcite for the observations generated under stationary distribution and then extended by theorem 5 in the same reference for a general initial distribution case .",
    "both results are established under some assumptions that we now investigate in our context .",
    "note that our process is not stationary since it starts from @xmath349 .",
    "thus , we rely on theorem 5 from @xcite to establish the properties of our estimator .",
    "we show that our assumptions on the rwre ensure the general assumptions on the autoregressive process with markov regime needed to establish the consistency of the mle  ( assumptions ( a1 ) to ( a5 ) in * ? ? ?",
    ".    first , assumption  [ hyp : noyau ] is sufficient to ensure assumption  ( a1 ) from  @xcite . indeed ,",
    "assumption  [ hyp : noyau ] implies inequalities   which correspond exactly to part ( a ) of ( a1 ) on transition @xmath82",
    ". moreover , statement ( b ) of ( a1 ) writes in our case as @xmath350 positive and finite , which is automatically satisfied here .",
    "+ assumption  ( a2 ) from  @xcite requires that the transition kernel density @xmath128 of the markov chain @xmath308 defined by   is positive harris recurrent and aperiodic . in proposition",
    "[ prop : biv_harris ] , we proved that this is satisfied as soon as this is the case for the environment kernel @xmath351 ( namely assumption  [ hyp : noyau ] ) and under the ballistic assumption  [ hyp : ballistic ] on the rwre .",
    "let us recall that @xmath158 and @xmath159 are the probability and expectation induced when considering the chain @xmath160 under its stationary distribution @xmath157 .",
    "the first condition is satisfied according to the definition of @xmath217 given in  .",
    "moreover , we have @xmath353 relying on stirling s approximation , we have @xmath354 where @xmath355 stands for a sequence that is bounded in probability . thus we can write @xmath356 moreover , under assumption  , we have @xmath357 where @xmath358 denotes either the lebesgue measure of @xmath1 or its cardinality when @xmath1 is discrete . as a conclusion , as soon as @xmath359 , the second statement in the proposition is satisfied .",
    "now , from the definition of @xmath157 given in proposition  [ prop : biv_harris ] , we get @xmath360 which is finite thanks to  [ hyp : ballistic ] .      now , we let @xmath362 denote the marginal of the distribution @xmath158 on the set @xmath363 ( corresponding to the second marginal ) . in order to ensure identifiability of the model ( assumption ( a5 ) in",
    "* ) , we require identifiability of the parameter from the distribution of the environment ( assumption  [ hyp : ident ] in our work ) .",
    "we prove that @xmath7 is uniquely defined from @xmath362 .",
    "the knowledge of the distribution @xmath362 means that for any @xmath365 , any sequence @xmath366 , we know the quantity @xmath367 since @xmath217 does not depend on @xmath7 and is positive , if we assume that @xmath368 we obtain from the above expression that @xmath369 almost surely ( w.r.t .",
    "the underlying measure on @xmath370 ) . noting that assumption  [ hyp : ident ] can be formulated on @xmath361 or on @xmath82 equivalently , this implies @xmath371 .",
    "now a direct application from theorem  5 in @xcite combined with our previous developments establishes that under assumptions  [ hyp : noyau ] to  [ hyp : ident ] , the maximum likelihood estimator @xmath175 converges @xmath163-almost surely to the true parameter value @xmath55 as @xmath42 tends to infinity .",
    "applying theorem  6 from @xcite and using that in our case , their assumptions ( a6 ) to ( a8 ) are satisfied for @xmath82 under our assumption  [ hyp : regular2 ] , we obtain the weak convergence of the conditional score to a gaussian distribution , as soon as the asymptotic variance is defined , which means as soon as the fisher information matrix is invertible ."
  ],
  "abstract_text": [
    "<S> we focus on the parametric estimation of the distribution of a markov environment from the observation of a single trajectory of a one - dimensional nearest - neighbor path evolving in this random environment . in the ballistic case , as the length of the path increases , we prove consistency , asymptotic normality and efficiency of the maximum likelihood estimator . </S>",
    "<S> our contribution is two - fold : we cast the problem into the one of parameter estimation in a hidden markov model ( hmm ) and establish that the bivariate markov chain underlying this hmm is positive harris recurrent . </S>",
    "<S> we provide different examples of setups in which our results apply , in particular that of dna unzipping model , and we give a simple synthetic experiment to illustrate those results . </S>"
  ]
}