{
  "article_text": [
    "among the methods used to update from a prior probability distribution to a posterior distribution when new information becomes available there are two that can claim the distinction of being systematic , objective , and of wide applicability :  one is based on bayes theorem ( for applications to physics see @xcite ) and the other is based on the maximization of ( relative ) entropy @xcite .",
    "the choice between the two methods is dictated by the nature of the information being processed .",
    "bayes theorem should be used when we want to update our beliefs about the values of quantities @xmath0 on the basis of observed values of data @xmath1 and of the known relation between them  the likelihood @xmath2 .",
    "the posterior distribution is @xmath3 .",
    "the previous knowledge about @xmath0 is codified both in the prior distribution @xmath4 and also in the likelihood  @xmath2 .",
    "the selection of the prior is a difficult problem @xcite because it is not always clear how to translate our previous beliefs about @xmath0 into a distribution @xmath4 in an objective way .",
    "one approach that seems to work , at least sometimes , is to rely on experience and physical intuition but this becomes unreliable in situations of increasing complexity .",
    "attempts to achieve objectivity include arguments invoking symmetry ",
    "generalized forms of the principle of insufficient reason  and arguments that seek to identify that state of knowledge that reflects complete ignorance .",
    "the latter suggest connections with the notion of entropy @xcite and have led to proposals for entropic priors  @xcite .",
    "this brings us to the second method of processing information , the method of maximum entropy , which is designed for processing information given in the form of constraints on the family of posterior distributions @xcite .    in this paper",
    "we use entropic arguments to translate information into a prior distribution @xcite . rather than seeking a totally non - informative prior , we translate information that we do in fact have : the knowledge of the likelihood function",
    ", @xmath2 , already constitutes valuable prior information .",
    "the prior thus obtained is an entropic prior .  the _ bare _ entropic priors discussed here apply to a situation where all we know about the quantities @xmath0 is that they appear as parameters in the likelihood @xmath2 .",
    "it is straightforward , however , to extend the method and incorporate additional relevant information beyond that contained in the likelihood .",
    "the first proposal of priors of this form is due to skilling @xcite for the case of discrete distributions . the second proposal , due to rodrguez @xcite , provided the generalization to the continuous case and further elaborations @xcite . in section 2",
    "we give a derivation that is closer in spirit to applications of me  to statistical mechanics .",
    "a difficulty with the case of experiments that can be indefinitely repeated , which had been identified in @xcite , is diagnosed and resolved with the introduction of a hyper - parameter @xmath5 in section 3 .",
    "the analogy to statistical mechanics is important : the interpretation of @xmath5 as a lagrange multiplier affects how @xmath5 should be estimated and is an important difference between the entropic prior proposed here and those of skilling and rodrguez .",
    "the example of a gaussian likelihood is given in section 4 . in section 5",
    "we collect our conclusions and some final comments .",
    "we use the me method @xcite to derive a prior @xmath4 for use in bayes theorem @xmath6 .",
    "as discussed in @xcite , since bayes theorem follows from the product rule we must focus our attention on @xmath7 rather than @xmath4 .",
    "thus , the relevant universe of discourse is the product @xmath8 of @xmath9 , the space of all @xmath0s , and the data space @xmath10 .",
    "this important point was first made by rodrguez @xcite but both our derivation and final results differ from his @xcite .    to rank distributions on the space @xmath8 we must first decide on a prior @xmath11 .",
    "when nothing is known about the variables @xmath0  in particular , no relation between @xmath1 and @xmath0 is yet known ",
    "the prior must be a product @xmath12 of the separate priors in the spaces @xmath10 and @xmath9 because maximizing the relative entropy @xmath13=-\\int dy\\,d\\theta\\,p(y,\\theta)\\,\\log\\frac{p(y,\\theta ) } { m(y)\\mu(\\theta)},\\ ] ] yields @xmath14.this distribution reflects our state of ignorance : the data about @xmath1 tells us absolutely nothing about @xmath0 .    in what follows",
    "we assume that @xmath15 is known because it is an important part of understanding what data it is that has been collected .",
    "furthermore , if the @xmath0s are parameters labeling some distributions @xmath2 , then for each particular choice of the functional form of @xmath2 there is a natural distance in the space @xmath9 given by the fisher - rao metric @xmath16 , @xcite @xmath17 therefore the prior on @xmath0 is @xmath18 where @xmath19 is the determinant of @xmath20 .",
    "next we incorporate the crucial piece of information : of all joint distributions @xmath21 we consider the subset where the likelihood @xmath2 has a fixed , known functional form .",
    "notice that this is an unusual constraint ; it is not an expectation value .",
    "note also that the only information we are using about the quantities @xmath0 is that they appear as parameters in the known likelihood @xmath2 , _",
    "nothing else_. but , of course , should additional relevant information ( _ i.e. _ , an additional constraint ) be known it should also be taken into account .",
    "the preferred distribution @xmath7 is chosen by varying @xmath4 to maximize @xmath22=-\\int dy\\,d\\theta\\,\\pi(\\theta)p(y|\\theta)\\,\\log\\frac { \\pi(\\theta)p(y|\\theta)}{g^{1/2}(\\theta)m(y)}~. \\label{sigma[pi]}%\\ ] ] @xmath23 assuming that both @xmath24 and @xmath2 are normalized the result is @xmath25 and @xmath26 is the entropy of the likelihood , @xmath27 the entropic prior eq.([main ] ) is our first important result : it gives the probability that the value of @xmath0 should lie within the small volume @xmath28 .",
    "the preferred value of @xmath0 is that which maximizes the entropy @xmath26 because this maximizes the scalar probability density @xmath29 .",
    "note that eq.([main ] ) manifestly invariant under changes of the coordinates @xmath0 .    to summarize : for the special case of a fixed data space @xmath10 , that is , for experiments that can not be repeated , we have succeeded in translating the information contained in the model ",
    "the space @xmath10 , its measure @xmath15 , and the conditional distribution @xmath2  into a prior @xmath4 .",
    "but for experiments that can be repeated indefinitely the prior ( [ main ] ) yields nonsense and we have a problem .",
    "indeed , let us assume that @xmath0 is not a random  variable , its value is fixed but unknown .",
    "for @xmath30 independent repetitions of an experiment , the joint distribution in the space @xmath31 is @xmath32 and maximization of the appropriate @xmath33 entropy gives @xcite @xmath34 which is clearly wrong .",
    "the dependence of @xmath35 on the amount @xmath30 of data would lead us to a perpetual revision of the prior as more data is collected . for large @xmath30",
    "the data becomes irrelevant .",
    "the problem , as we will see next , is not a failure of the me method but a failure to include all the relevant information . indeed , when an experiment can be repeated we actually know more than just @xmath36 .",
    "we also know that discarding the values of say @xmath37 , yields an experiment that is indistinguishable from the single , @xmath38 , experiment .",
    "this _ additional _ information , which is expressed by @xmath39 leads to @xmath40 for all @xmath30 .",
    "next we identify a constraint that codifies this information within each space @xmath31 .",
    "for large @xmath30 the prior @xmath41 in eq.([pi(n ) ] ) reflects an overwhelming preference for the value of @xmath0 that maximizes the entropy @xmath26 . indeed , as @xmath42 we have @xmath43 which is manifestly incorrect .",
    "this suggests that information about the actual numerical value @xmath44 of the expected entropy @xmath45 is very relevant ( because if @xmath44 were known the problem above would not arise ) and that we should maximize @xmath33 subject to an additional constraint on @xmath44 .",
    "naturally , additional steps will be needed to estimate the unknown @xmath44 . a similar argument justifying the introduction of constraints in statistical physics",
    "is explored in @xcite .",
    "we maximize the entropy @xmath46=-\\int\\,d\\theta\\,dy^{(n)}\\,\\pi(\\theta)p(y^{(n)}|\\theta ) \\log\\frac{\\pi(\\theta)p(y^{(n)}|\\theta)}{g^{1/2}(\\theta)\\,m(y^{(n)})}%\\ ] ] subject to constraints on @xmath45 and that @xmath47 be normalized .",
    "( an unimportant factor of @xmath48 has been dropped from the fisher - rao measure @xmath49 . )",
    "the result is @xmath50   ~.\\ ] ] the undesired dependence on @xmath30 is eliminated if the lagrange multipliers @xmath51 in each space @xmath31 are chosen so that @xmath52 is a constant independent of @xmath30 .",
    "the resulting entropic prior , @xmath53 is our second important result .",
    "the prior @xmath54 incorporates information contained in the likelihood plus information about @xmath55    the last step would be to estimate @xmath5 and @xmath0 from bayes theorem @xmath56 where @xmath57 is a prior for @xmath5 .",
    "however , if we are only interested in @xmath0 , we can just marginalize over @xmath5 to get @xmath58 where @xmath59 the averaged @xmath60 is our final expression for the entropic prior . it is independent of the actual data @xmath61 as it should .",
    "next we assign an entropic prior to @xmath5 .",
    "we start by pointing out that @xmath5 is not on the same footing and should not be treated like the other parameters @xmath0 because the relation between @xmath5 and the data @xmath1 is indirect : @xmath5 is related to @xmath0 through @xmath54 , and @xmath0 is related to @xmath1 through @xmath62 once @xmath0 is given , the data @xmath1 contains no further information about @xmath5 . since the whole significance of @xmath5 is derived purely from @xmath54 , eq.([main 2 ] ) , the relevant universe of discourse is @xmath63 with @xmath64 and not @xmath65 as in @xcite which requires the introduction of an endless chain of hyper - parameters .",
    "we therefore consider the joint distribution @xmath66 and obtain @xmath57 by maximizing the entropy @xmath67=-\\int d\\alpha\\,d\\theta\\,\\,\\pi(\\alpha,\\theta)\\log\\frac { \\pi(\\alpha,\\theta)}{\\gamma^{1/2}(\\alpha)\\,g^{1/2}(\\theta)}\\ , \\label{sigma prime}%\\ ] ] where @xmath68 is determined below .",
    "since no reference is made to repeatable experiments in @xmath69 there is no need for any further constraints  and no further hyper - parameters  except for normalization .",
    "the result is @xmath70 where using eqs.([main 2 ] ) and ( [ sbar ] ) the fisher - rao measure @xmath71 is @xmath72   ^{2}=\\frac{d^{2}\\log\\zeta(\\alpha)}{d\\alpha^{2}% } \\",
    ", , \\label{gamma(alpha)}%\\ ] ] and where @xmath73 is given by @xmath74 this completes our derivation of the actual prior for @xmath0 : the averaged @xmath60 in eq.([main 3 ] ) codifies information contained in the likelihood function , plus the insight that for repeatable experiments , information about the expected likelihood entropy , even if unavailable , is relevant .",
    "consider data @xmath75 that are scattered around an unknown value @xmath76 , @xmath77 with @xmath78 and @xmath79 the goal is to estimate @xmath80 on the basis of @xmath61 and the information implicit in the data space @xmath10 , its measure @xmath15 ( discussed below ) , and the gaussian likelihood , @xmath81   ~. \\label{gaussian likelihood}%\\ ] ]    we asserted earlier that knowing the measure @xmath15 is part of knowing what data has been collected .",
    "in many physical situations where the data happen to be distributed according to eq.([gaussian likelihood ] ) the underlying space @xmath10 is invariant under translations and we can assume @xmath82 .",
    "indeed , the gaussian distribution can be obtained by maximizing an entropy with an underlying constant measure and constraints on the relevant information the mean @xmath76 and the variance @xmath83 .    from eqs.([stheta ] ) and ( [ gaussian likelihood ] ) the entropy of the likelihood is @xmath84   \\quad \\text{where}\\quad\\sigma_{0}\\overset{\\operatorname*{def}}{=}\\left",
    "(   \\frac { e}{2\\pi}\\right )   ^{1/2}\\frac{1}{m}~,\\ ] ] and the corresponding fisher - rao measure , from eq.([fisher metric ] ) is @xmath85{cc}% 1/\\sigma^{2 } & 0\\\\ 0 & 2/\\sigma^{2}% \\end{array } \\right\\vert = \\frac{2}{\\sigma^{4}}~.\\ ] ]    note that both @xmath86 and @xmath87 are independent of @xmath76 .",
    "this means that if we were concerned with the simpler problem of estimating @xmath76  in a situation where @xmath88 happens to be known , the bayesian estimate of @xmath76 using entropic priors coincides with the maximum likelihood estimate .",
    "when @xmath88 is unknown the @xmath5-dependent entropic prior , eq.([main 2 ] ) , is @xmath89 since @xmath90 is improper in both @xmath76 and @xmath88 we must introduce high and low cutoffs for both @xmath76 and @xmath88 .",
    "the fact that without cutoffs the model is not well defined is interpreted as a request for additional relevant information , namely , the values of the cutoffs .",
    "we write the range of @xmath76 as @xmath91 and introduce dimensionless quantities @xmath92 and",
    "@xmath93 ; @xmath88 extends from @xmath94 to @xmath95 .",
    "then @xmath96 and @xmath90 are given by @xmath97 and @xmath98 note that @xmath99 reduces to @xmath100 which is the jeffreys prior usually introduced by imposing invariance under scale transformations , @xmath101 .",
    "writing @xmath102 , the prior for @xmath5 , is obtained from eq.([gamma(alpha ) ] ) , @xmath103 and from eqs.([main 3 ] ) and ( [ s(alpha ) ] ) , @xmath104   ~ , \\label{pi(alpha)gaussian}%\\ ] ] where the normalization @xmath105 has been suitably redefined .",
    "eqs.([gammagaussian ] ) and ( [ pi(alpha)gaussian ] ) simplify in the limit @xmath106 .",
    "note that the same result is obtained irrespective of the order in which we let @xmath107 and/or @xmath108 .",
    "the resulting @xmath71 and @xmath109 are@xmath110 and @xmath111{cc}% \\frac{1}{\\left (   1-\\alpha\\right )   ^{2}}\\exp\\left [   { \\frac{1}{\\alpha-1}}\\right ] & \\text{for\\quad}\\alpha<1\\\\ 0 & \\text{for\\quad}\\alpha\\geq1 \\end{array } \\right .   \\label{limit pigaussian}%\\ ] ] where @xmath57 is normalized and is shown in fig . [ fig . 1 ] .",
    "[ ptb ]    ep-me03-fig1.eps    @xmath57 reaches its maximum value at @xmath112 . since @xmath113 for @xmath114 the expected value of @xmath5 and all higher moments diverge .",
    "this suggests that replacing the unknown @xmath5 in the prior @xmath54 by any given numerical value @xmath115 is probably not a good approximation .     since @xmath5 is unknown the effective prior for @xmath80",
    "is obtained marginalizing @xmath116 over @xmath5 , eq.([main 3 ] ) .",
    "since @xmath117 for @xmath118 as @xmath106 we can safely take the limit @xmath107 or @xmath119 while keeping @xmath120 fixed , @xmath121{cc}% \\frac{1}{\\delta\\mu\\sigma_{l}}\\frac{\\exp\\left [   { \\frac{1}{\\alpha-1}}\\right ] } { 1-\\alpha}\\left (   \\frac{\\sigma}{\\sigma_{l}}\\right )   ^{\\alpha-2 } & \\text{for\\quad}\\alpha<1\\\\ 0 & \\text{for\\quad}\\alpha\\geq1 .",
    "\\end{array } \\right .",
    "\\label{pi(theta , alpha)g}%\\ ] ] ( however we can not take @xmath122 ) .",
    "the averaged prior for @xmath76 and @xmath88 is @xmath123   } { 1-\\alpha } \\left (   \\frac{\\sigma}{\\sigma_{l}}\\right )   ^{\\alpha}d\\alpha=\\frac{2}{\\delta \\mu\\sigma}k_{0}\\left (   2\\sqrt{\\log\\frac{\\sigma}{\\sigma_{l}}}\\right )   ~ , \\label{main 5}%\\ ] ] where @xmath124 is a modified bessel function of the second kind .",
    "this is the entropic prior for the gaussian model .",
    "the function @xmath125 is shown in fig .",
    "2 ] as a function of @xmath126 .",
    "the singularity as @xmath127 is integrable .",
    "[ tb ]    ep-me03-fig2.eps",
    "using the method of maximum relative entropy we have translated the information contained in the known form of the likelihood into a prior distribution .",
    "the argument follows closely the analogous application of the me  method to statistical mechanics . for experiments that can not be repeated the resulting entropic prior  is formally identical with the einstein fluctuation formula . for repeatable experiments ,",
    "however , additional relevant information  represented in terms of a lagrange multiplier @xmath5  must be included in the analysis .",
    "the important case of a gaussian likelihood was treated in detail .",
    "we have dealt with the simplest case where all we know about the quantities @xmath0 is that they appear as parameters in the likelihood @xmath2 .",
    "our argument can , however , be generalized to situations where we know of additional relevant information beyond what is contained in the likelihood .",
    "such information can be taken into account through additional constraints in the maximization of the entropy @xmath88 .    to conclude we comment briefly on the entropic priors proposed by skilling and by rodrguez .",
    "skilling s prior , unlike ours , is not restricted to probability distributions but is intended for generic positive additive distributions  @xcite . our argument , which consists in maximizing the entropy @xmath88 subject to a constraint @xmath21 , makes no sense for generic positive additive distributions for which there is no available product rule .",
    "another important difference arises from the fact that skilling s entropy is not , in general , dimensionless and his hyper - parameter @xmath5 is interpreted some sort of cutoff carrying the appropriate corrective units .",
    "difficulties with skilling s prior were identified in @xcite .",
    "rodrguez s approach is , like ours , derived from a maximum entropy principle @xcite .",
    "one ( minor ) difference is his treatment of the underlying measure @xmath15 . for us knowing @xmath15 is part of knowing what data has been collected ; for him @xmath15 is an initial guess and he suggests setting @xmath128 for some value @xmath129 .",
    "the more important difference , however , is that the number of observed data @xmath30 is left unspecified .",
    "the space @xmath31 over which distributions are defined , and therefore the distributions themselves , also remain unspecified .",
    "it is not clear what the maximization of an entropy over such unspecified spaces could possibly mean but a hyper - parameter @xmath5 is eventually introduced and it is interpreted as a virtual number of observations supporting the initial guess @xmath129 .",
    "a different interpretation is given in @xcite .",
    "since @xmath5 is treated on the same footing as the other parameters @xmath0,- rodrguez s approach requires an endless chain of hyper - parameters .",
    "* acknowledgments- * many of our comments and arguments have been inspired by carlos c. rodrguez , volker dose , and rainer fischer through insightful questions and discussions which we gratefully acknowledge .",
    "a. c. also acknowledges the hospitality of the max - planck - institut fr plasmaphysik during the two extended visits when most of this work was carried out .      for a recent review see v. _ _ dose , bayesian inference in physics : case studies , rep .",
    ".  phys . *",
    "66 * , 1421 ( 2003 ) ; for a pedagogical introduction see d. s. sivia , data analysis , a bayesian tutorial  ( oxford university press , oxford , 1996 ) .",
    "e. t. jaynes , ieee trans .",
    "ssc-4 * , 227 ( 1968 ) ; j. m. bernardo , j. roy .",
    "b * 41 * , 113 ( 1979 ) ; a. zellner , `` bayesian methods and entropy in economics and econometrics '' in _ maximum entropy and bayesian methods _ , edited by w. t. grandy jr . and",
    "l. h. schick ( kluwer , dordrecht , 1991 ) .",
    "j. skilling , `` classic maximum entropy '' in _ maximum entropy and bayesian methods _",
    ", j. skilling ( ed . ) ( kluwer , dordrecht , 1989 ) ; `` quantified maximum entropy '' in _ maximum entropy and bayesian methods _ , p. f. fougre ( ed . )",
    "( kluwer , dordrecht , 1990 ) .    c. c. rodrguez , the metrics generated by the kullback number  in _ maximum entropy and bayesian methods _ , j. skilling ( ed . ) ( kluwer , dordrecht , 1989 ) ; objective bayesianism and geometry  in _ maximum entropy and bayesian methods _ , p. f. fougre ( ed . ) ( kluwer , dordrecht , 1990 ) ; entropic priors  in _ maximum entropy and bayesian methods _ , edited by w. t. grandy jr . and l. h. schick ( kluwer , dordrecht , 1991 ) ; bayesian robustness : a new look from geometry  in _ maximum entropy and bayesian methods _ ,",
    "g. r. heidbreder ( ed . ) ( kluwer , dordrecht , 1996 ) .",
    "c. c. rodrguez : ` entropic priors for discrete probabilistic networks and for mixtures of gaussian models ' . in : _",
    "bayesian inference and maximum entropy methods in science and engineering _ , ed . by r. l. fry , aip conf .",
    "proc . * 617 * , 410 ( 2002 ) ( online at arxiv.org/abs/physics/0201016 ) .",
    "a. caticha , ` maximum entropy , fluctuations and priors ' , in _ bayesian methods and maximum entropy in science and engineering _ , ed . by a. mohammad - djafari , aip conf",
    ". proc . * 568 * , 94 ( 2001 ) ( online at arxiv.org/abs/math-ph/0008017 ) .",
    "s. amari , _ differential - geometrical methods in statistics _ ( springer - verlag , 1985 ) ; for a brief derivation see a. caticha , change , time and information geometry ,  in _ bayesian methods and maximum entropy in science and engineering _ , ed . by a. mohammad - djafari , aip conf .",
    "proc . * 568 * , 72 ( 2001 ) ( online at arxiv.org/abs/math-ph/0008018 ) .",
    "j. skilling and s. sibisi , priors on measures  in _ maximum - entropy and bayesian methods _ , k. m. hanson and r. n. silver ( eds . ) ( kluwer , dordrecht , 1996 ) ; j. skilling , massive inference and maximum entropy  in _ maximum - entropy and bayesian methods _ , g. j. erickson , j. t. ryckert and c. r. smith ( eds . ) ( kluwer , dordrecht , 1998 ) ."
  ],
  "abstract_text": [
    "<S> the method of maximum ( relative ) entropy ( me ) is used to translate the information contained in the known form of the likelihood into a prior distribution for bayesian inference . </S>",
    "<S> the argument is guided by intuition gained from the successful use of me methods in statistical mechanics . for experiments that can not be repeated the resulting entropic prior  is formally identical with the einstein fluctuation formula . </S>",
    "<S> for repeatable experiments , however , the expected value of the entropy of the likelihood turns out to be relevant information that must be included in the analysis . as an example </S>",
    "<S> the entropic prior for a gaussian likelihood is calculated . </S>"
  ]
}