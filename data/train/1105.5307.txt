{
  "article_text": [
    "learning representations that are invariant to irrelevant transformations of the input is an important step towards building recognition systems automatically .",
    "invariance is a key property of some cells in the mammalian visual cortex .",
    "cells in high - level areas of the visual cortex respond to objects categories , and are invariant to a wide range of variations on the object ( pose , illumination , confirmation , instance , etc ) .",
    "the simplest known example of invariant representations in visual cortex are the complex cells of v1 that respond to edges of a given orientation but are activated by a wide range of positions of the edge .",
    "many artificial object recognition systems have built - in invariances , such as the translational invariance of convolutional network  @xcite , or sift descriptors @xcite .",
    "an important question is how can useful invariant representations of the visual world be learned from unlabeled samples .    in this paper",
    "we introduce an algorithm for learning features that are invariant ( or robust ) to common image transformations that typically occur between successive frames of a video or statistically within a single frame . while the method is quite simple , it is also computationally efficient , and possesses provable bounds on the speed of inference .",
    "the first component of the model is a layer of sparse coding .",
    "sparse coding   @xcite constructs a dictionary matrix @xmath0 so that input vectors can be represented by a linear combination of a small number of columns of the dictionary matrix .",
    "inference of the feature vector @xmath1 representing an input vector @xmath2 is performed by finding the @xmath1 that minimizes the following energy function @xmath3 where @xmath4 is a positive constant .",
    "the dictionary matrix @xmath0 is learned by minimizing @xmath5 averaged over a set of training samples @xmath6 , while constraining the columns of @xmath0 to have norm 1 .",
    "the first idea of the proposed method is to accumulate sparse feature vectors representing successive frames in a video , or versions of an image that are distorted by transformations that do not affect the nature of its content .",
    "@xmath7 where the sum runs over the distorted images @xmath8 .",
    "the second idea is to connect a second sparse coding layer on top of the first one that will capture dependencies between components of the accumulated sparse code vector .",
    "this second layer models vector @xmath9 using an _ invariant code _",
    "@xmath10 , which is the minimum of the following energy function @xmath11 where @xmath12 denotes the @xmath13 norm of @xmath10 , @xmath14 is a matrix , and @xmath15 is a positive constant controlling the sparsity of @xmath10 . unlike with traditional sparse coding , in this method",
    "the dictionary matrix interacts _ multiplicatively _ with the input @xmath9 . as in traditional sparse coding , the matrix @xmath14 is trained by gradient descent to minimize the average energy for the optimal @xmath10 over a training set of vectors @xmath9 obtained as stated above .",
    "the columns of @xmath14 are constrained to be normalized to 1 .",
    "essentially , the matrix @xmath14 will connect a component of @xmath10 to a set of components of @xmath9 if these components of @xmath9 co - occur frequently .",
    "when a component of @xmath10 turns on , it has the effect of lowering the coefficients of the components of @xmath16 to which it is strongly connected through the @xmath14 matrix . to put it another way ,",
    "if a set of components of @xmath9 often turn on together , the matrix @xmath14 will connect them to a component of @xmath10 . turning on",
    "this component of @xmath10 will lower the overall energy ( if @xmath15 is small enough ) because the whole set of components of @xmath9 will see their coefficient being lowered ( the exponential terms ) .",
    "hence , each unit of @xmath10 will connect units of that often turn on together within a sequence of images .",
    "these units will typically represent distorted version of a feature .    the energies ( [ eq_sparse_coding ] ) and ( [ eq_fulle ] ) can be naturally combined into a single combined model of @xmath1 and @xmath10 as explained in section [ sec_model ] . there",
    "the second layer @xmath10 is essentially modulating sparsity of the first layer @xmath1",
    ". single model of the image is more natural . for the invariance properties we did nt find much qualitative difference and since the former has provable inference bounds we presented the results for separate training .",
    "however the a two layer model should capture the statistics of an image . to demonstrate this we compared the in - paining capability of one and two layer models and found that two layer model does better job .",
    "for these experiments , the combined two layer model is necessary .",
    "we also found that despite the assumptions of the fast inference are not satisfied for the two layer model , empirically the inference is fast in this case as well .",
    "the first way to implement invariance is to take a known invariance , such as translational invariance in images , in put it directly into the architecture .",
    "this has been highly successful in convolutional neural networks @xcite and sift descriptors @xcite and its derivatives .",
    "the major drawback of this approach is that it works for known invariances , but not unknown invariances such as invariance to instance of an object .",
    "a system that would discover invariance on its own would be desired .",
    "second type of invariance implementation is considered in the framework of sparse coding or independent component analysis .",
    "the idea is to change a cost function on hidden units in a way that would prefer co - occurring units to be close together in some space @xcite .",
    "this is achieved by pooling units close in space together .",
    "this groups different inputs together producing a form of invariance .",
    "the drawback of this approach is that it requires some sort of imbedding in space and that the filters have to arrange themselves .    in the third approach ,",
    "rather then forcing units to arrange themselves , we let them learn whatever representations they want to learn and instead figure out which to pool together . in @xcite ,",
    "this was achieved by modulating covariance of the simple units with complex units .",
    "the fourth approach to invariance uses the following idea : if the inputs follow one another in time they are likely a consequence of the same cause .",
    "we would like to discover that cause and therefore look for representations that are common for all frames .",
    "this was achieved in several ways .",
    "in slow feature analysis @xcite one forces the representation to change slowly . in temporal product network",
    "@xcite one breaks the input into two representations - one that is common to all frames and one that is complementary . in @xcite",
    "the idea is similar but in addition the complementary representation specifies movement . in the simplest instance of hierarchical temporal memory @xcite one forms groups based on transition matrix between states .",
    "the @xcite is a structured model of video .",
    "a lot of the approaches for learning invariance are inspired by the fact that the neo - cortex learns to create invariant representations .",
    "consequently these approaches are not focused on creating efficient algorithms . in this paper , we given an efficient learning algorithm that falls into the framework of third and fourth approaches .",
    "the basic idea is to modulate the sparsity of sparse coding units using higher level units that are also sparse .",
    "the fourth approach is implemented by using the same higher level representation for several consecutive time frames . in the form our model is similar to that of @xcite but a little simpler . in a sense comparing our model to @xcite",
    "is similar to comparing sparse coding to independent component analysis .",
    "independent component analysis is a probabilistic model , whereas sparse coding attempts to reconstruct input in terms of few active hidden units .",
    "the advantage of sparse coding is that it is simpler and easier to optimize .",
    "there exist several very efficient inference and learning algorithms @xcite and sparse coding has been applied to a large number problems .",
    "it is this simplicity that allows efficient training of our model .",
    "the inference algorithm is closely derived from the fast iterative shrinkage - thresholding algorithm ( fista ) @xcite and has a convergence rate of @xmath17 where @xmath18 is the number of iterations .",
    "the model described above comprises two separately trained modules , whose inference is performed separately .",
    "however , one can devise a unified model with a single energy function that is conceptually simpler : @xmath19 given a set of inputs @xmath20 , the goal of training is to minimize @xmath21 .",
    "we do this by choosing one input @xmath2 at a time , minimizing ( [ eq_fulle ] ) over @xmath1 and @xmath10 with @xmath0 and @xmath14 fixed , then fixing the resulting @xmath22,@xmath23 and taking step in a negative gradient direction of @xmath0 , @xmath14 ( stochastic gradient descent ) .",
    "an algorithm for finding @xmath22,@xmath23 is given in section [ sec_theoretical ] .",
    "it consists of taking step in @xmath1 and @xmath10 separately , each of which lowers the energy .",
    "note : the @xmath24 functions in ( [ eq_fulle ] ) is different from that of the simple ( split ) model .",
    "the reason is that , in our experiments , either @xmath10 units lower the sparsity of @xmath1 too much , not resulting in a sparse @xmath1 code or the units @xmath10 do not turn on at all .",
    "we now describe a toy example that illustrates the main idea of the model  @xcite .",
    "the input , with @xmath25 , is an image patch consisting of a subset of the set of parallel lines of four different orientations and ten different positions per orientation .",
    "however for any given input , only lines with the same orientation can be present , figure [ fig_lines]a ( different orientations have equal probability and for a given orientation a line of this orientation is present with probability 0.2 independently of others ) .",
    "this is a toy example of a texture .",
    "training sparse coding on this input results in filters similar to one in figure [ fig_lines]b .",
    "we see that a given simple unit responds to one particular line .",
    "the noisy filters correspond to simple units that are inactive - this happens because there are only 40 discrete inputs . in realistic data such as natural images",
    ", we have a continuum and typically all units are used .",
    "clearly , sparse coding can not capture all the statistics present in the data .",
    "the simple units are not independent .",
    "we would like to learn that that units corresponding to lines of a given orientation usually turn on simultaneously .",
    "we trained ( [ eq_fulle ] ) on this data resulting in the filters in the figure [ fig_lines]b , c .",
    "the filters of the simple units of this full model are similar to those obtained by training just the sparse coding .",
    "the invariant units pool together simple units with filters corresponding to lines of the same orientation .",
    "this makes the invariant units invariant to the pattern of lines and dependent only on the orientation .",
    "only four invariant units were active corresponding to the four groups .",
    "as in sparse coding , on a realistic data such as natural images , all invariant units become active and distribute themselves with overlapping filters as we will se below .    ) .",
    "a ) randomly selected input image patches .",
    "the input patches are generated as follows .",
    "pick one of the four orientations at random .",
    "consider all lines of this orientation .",
    "put any such line into the image independently with probability 0.2 .",
    "b ) learned sparse coding filters .",
    "a given active unit responds to a particular line .",
    "c ) learned filters of the invariant units .",
    "each row corresponds to an invariant unit .",
    "the sparse coding filters are ordered according to the strength of their connection to the invariant unit .",
    "there are only four active units ( arrows ) and each responds to a given orientation , invariant to which lines of a given orientation are present . ]",
    "let us now discuss the motivation behind introducing a sequence of inputs ( @xmath26 ) in ( [ eq_fulle ] ) .",
    "inputs that follow one another in time are usually a consequence of the same cause .",
    "we would like to discover that cause .",
    "this cause is something that is present at all frames and therefore we are looking for a single representation @xmath10 in ( [ eq_fulle ] ) that is common to all the frames .",
    "another interesting point about the model ( [ eq_fulle ] ) is a that nonzero @xmath10 lowers the sparsity coefficient of units of @xmath1 that belong to a group making them more likely to become activated .",
    "this means that the model can utilize higher level information ( which group is present ) to modulate the activity of the lower layer .",
    "this is a desirable property for multi - layer systems because different parts of the system should propagate their belief to other parts . in our",
    "invariance experiments the results for the unified model were very similar to the results of the simple ( split ) model .",
    "below we show the results of this simple model because it is simple and because we provably know an efficient inference algorithm .",
    "however in the section [ sec_theoretical ] we will revisit the full system , generalize it to an @xmath27-layer system , give an algorithm for training it , and prove that under some assumptions of convexity , the algorithm again has a provably efficient inference . in the final section we use the full system for in - paining and show that it generalizes better then a single layer system .",
    "here we discuss how to find @xmath10 efficiently and give the numerical results of the paper .",
    "the results for the full model ( [ eq_fulle ] ) were similar .",
    "the advantage of ( [ eq_ei_gh ] ) compared to ( [ eq_fulle ] ) is that the fast iterative shrinkage - thresholding algorithm ( fista ) @xcite applies to it directly .",
    "fista applies to problems of the form @xmath28 where :    * @xmath29 is continuously differentiable , convex and lipschitz , that is @xmath30 .",
    "the @xmath31 is the lipschitz constant of @xmath32 .",
    "* @xmath33 is continuous , convex and possibly non - smooth    the problem is assumed to have solution @xmath34 . in our case @xmath35 and @xmath36 which satisfies these assumptions ( @xmath14 is initialized with nonnegative entries which stay nonnegative during the algorithm without a need to force it ) .",
    "this solution converges with bound @xmath37 where @xmath38 is the value of @xmath10 at the @xmath18-@xmath39 iteration and @xmath4 is a constant .",
    "the cost of each iteration is @xmath40 where @xmath27 is the input size and @xmath41 is the output size .",
    "more precisely the cost is one matrix multiplications by @xmath14 and by @xmath42 plus @xmath43 cost .",
    "we used the back - tracking version of the algorithm to find @xmath44 which contains a fixed number of @xmath40 operations ( independent of desired error ) .",
    "it is a standard knowledge and easy to see that the algorithm applies to the sparse coding ( [ eq_sparse_coding ] ) as well .",
    "the input to the network was prepared as follows .",
    "we converted all the images of the berkeley data - set into gray - scale images .",
    "we locally removed the mean for each pixel by subtracting a gaussian - weighted average of the nearby pixels .",
    "the width of the gaussian was @xmath45 pixels .",
    "then , we locally normalized the contrast by dividing each pixel by gaussian - weighted standard deviation of the nearby pixels ( with a small cutoff to prevent blow - ups ) .",
    "the width of the gaussian was also @xmath45 pixels .",
    "then , we picked a @xmath46 window in the image and , for a randomly chosen direction and magnitude , we moved it for @xmath47 frames and extracted them .",
    "the magnitude of the displacement was random in the range of @xmath48 pixels .",
    "a very large collection of such triplets of frames was extracted .",
    "we trained the sparse coding algorithm with @xmath49 code units in @xmath1 on each individual frame ( not on the @xmath50 concatenated frames ) .",
    "after training we found the sparse codes for each frame .",
    "there were @xmath51 units in the layer of invariant units @xmath10 . for larger a system with @xmath52 simple units and @xmath49 invariant units ,",
    "see the supplementary material .",
    "are equivalent ) .",
    "we see that invariant units typically learn to group together units with similar orientation and frequency .",
    "there are few other types of filters as well .",
    "the units in the middle and right panel correspond to each other and correspond to the units in the left panel reading panels left to right and then down . see the supplementary material for all the filters the system : @xmath53 input patches , @xmath52 simple units , @xmath49 invariant units . ]",
    "the results are shown in the figure [ fig_rfu ] , see caption for description .",
    "we see that many invariant cells learn to group together filters of similar orientation and frequency but at several positions and thus learn invariance with respect to translations .",
    "however there are other types of filters as well .",
    "remember that the algorithm learns statistical co - occurrence between features , whether in time or in space .    ) .",
    "left panel are responses of simple units trained with sparsity @xmath54 in ( [ eq_sparse_coding ] ) .",
    "the right four panels are responses of invariant units trained with sparsities @xmath55 in ( 3 ) on the values of simple units .",
    "the x - axis of each panel is the distance of the edge from the center of the image - the @xmath56 in ( [ eq_edge ] ) .",
    "the y - axis is the orientation of each edge - the @xmath57 in ( [ eq_edge ] ) .",
    "30 cells were chosen at random in each panel .",
    "different colors correspond to different cells .",
    "the color intensity is proportional to the response of the unit .",
    "we see that sparse coding inputs respond to a small range of frequencies and positions ( the elongated shape is due to the fact that an edge of orientation somewhat different from the edge detector orientation sweeps the detector at different positions @xmath56 ) . on the other hand invariant cells",
    "respond to edges of at similar range of frequencies but larger range of positions . at high sparsities",
    "the response boundaries are sharp and response regions do nt overlap .",
    "as we lower the sparsity the boundaries become more blurry and regions start to overlap .",
    "@xmath58 was used in ( [ eq_edge ] ) .",
    "other frequencies produced similar effect . ]",
    "the values of the weights give us important information about the properties of the system .",
    "however ultimately we are interested in how the system responds to an input .",
    "we study the response of these units to a commonly occurring input - an edge .",
    "specifically the inputs are given by the following function .",
    "@xmath59 where @xmath60 is the position of a pixel from the center of a patch , @xmath56 a real number specifying distance of the edge from the center and @xmath57 is the orientation of the edge from the @xmath2 axis .",
    "this is not an edge function , but a function obtained on an edge after local mean subtraction .",
    "the responses of the simple units and the invariant units are shown in the figure .",
    "[ fig_responses ] , see caption for description . as expected the sparse coding units respond to edges in a narrow range of positions and relatively narrow range of orientations .",
    "invariant cells on the other hand are able to pool different sparse coding units together and become invariant to a larger range of positions .",
    "thus the invariant units do indeed have the desired invariance properties .",
    "note that for large sparsities the regions have clear boundaries and are quite sharp .",
    "this is similar to the standard implementation of convolutional net , where the pooling regions are squares ( with clear boundaries ) .",
    "it is probably more preferable to have regions that overlap as happens at lower sparsities since one would prefer smoother responses rather then jumps across boundaries .",
    "in this section we return to the full model ( [ eq_fulle ] ) .",
    "we generalize it to an @xmath27 layer system , give an inference algorithm and outline the proof that under certain assumptions of convexity the algorithm has the fast @xmath17 convergence of fista , there @xmath18 is the iteration number .",
    "the basic idea of minimizing over @xmath61 in ( [ eq_fulle ] ) is to alternate between taking energy - lowering step in @xmath1 while fixing @xmath10 and taking energy - lowering step in @xmath10 while fixing @xmath1 .",
    "note that both of the restricted problems ( problem in @xmath1 fixing @xmath10 and problem in @xmath10 fixing @xmath1 ) satisfy conditions of the fista algorithm .",
    "this will allow us to take steps of appropriate size that are guaranteed to lower the total energy .",
    "before that however , we generalize the problem , which will reveal its structure and which does not introduce any additional complexity .    consider system consisting of @xmath27 layers with units @xmath62 in the @xmath63-@xmath39 layer with @xmath64",
    "we define @xmath65 that is all the vectors @xmath62 concatenated .",
    "we define two sets of functions .",
    "let @xmath66 be continuously differentiable , convex and lipschitz functions .",
    "there can be several such functions per layer , which is denoted by index @xmath15 .",
    "let @xmath67 be continuous and convex functions , not necessarily smooth . for convenience",
    "we define @xmath68 , @xmath69 , @xmath70 and @xmath71 .",
    "we define the energy of the system to be @xmath72 where in the second equality we drop the @xmath15 from the notation for simplicity .",
    "we will omit writing the @xmath15 for the rest of the paper .",
    "the equation ( [ eq_fulle ] ) is a special case of ( [ eq_hierarchy ] ) with @xmath73 , @xmath70,@xmath74 , @xmath75 , @xmath76 , @xmath77 the problem in @xmath62 keeping other variables fixed satisfies the conditions of the fista algorithm .",
    "we can define a step in the variable @xmath62 to be ( in analogy to @xcite eq .",
    "( 2.6 ) ) : @xmath78 where the later equality holds if @xmath79 . here",
    "sh is the shrinkage function @xmath80 . in the case where @xmath62 is restricted to be",
    "nonnegative we need to use @xmath81 instead of the shrinkage function .",
    "let us describe the algorithm for minimizing ( [ eq_hierarchy ] ) with respect to @xmath1 ( we will write it explicitly below ) .",
    "in the order from @xmath82 to @xmath83 , take the step @xmath84 in ( @xmath85 ) .",
    "repeat until desired accuracy .",
    "the @xmath86 s have to be chosen so that @xmath87 this can be assured by taking @xmath88 where the later @xmath44 denotes the lipschitz constant of its argument .",
    "otherwise , as used in our simulations , it can be found by backtracking , see below .",
    "this will assure that each steps lowers the total energy ( [ eq_hierarchy ] ) and hence the overall procedure will keep lowering it . in fact",
    "the step @xmath89 with such chosen @xmath44 is in some sense a step with ideal step size .",
    "let us now write the algorithm explicitly :    * hierarchical ( f)ista . *",
    "take @xmath90 , some @xmath91 and @xmath92 .",
    "set @xmath93 , @xmath94 , @xmath95 .",
    "+ step k. ( @xmath96 ) .",
    "+ loop a=1:n \\ { backtracking \\ { + .",
    "find smallest nonnegative integer @xmath97 such that with @xmath98 @xmath99 set @xmath100 } + .",
    "compute @xmath101 @xmath102 @xmath103    the algorithm described above is this algorithm with the choice @xmath104 in the second last line .",
    "let us discuss the @xmath105 s . for single layer system ( @xmath106 )",
    "@xmath104 choice is called ista and has convergence bound of @xmath107 .",
    "the other choice of @xmath105 is the fista algorithm .",
    "the convergence rate of fista is much better than that of ista , having @xmath108 in the denominator .    for hierarchical system , the choice @xmath104 guarantees that each step lowers the energy .",
    "the question is whether introducing the other choice of @xmath105 would speed up convergence to the fista convergence rate .",
    "the trouble is that the in general the product @xmath109 is non - convex , which is the case for ( [ eq_fulle ] ) .",
    "for example we can readily see that if the function has more then one local minima , this convergence would certainly not be guaranteed ( imagine starting at a non - minimal point with zero derivative ) .",
    "the effect of @xmath105 is that of momentum and this momentum increases towards one as @xmath18 increases .",
    "with such a large momentum the system is in a danger of `` running around '' .",
    "it might be effective to introduce this momentum but to regularize it ( say bound it by a number smaller then one ) . in any case one can always use the algorithm with @xmath104 .    in the special cases when all @xmath109 s are convex however , we give an outline of the proof that the algorithm converges with the fista rate of convergence . for this purpose",
    "we define the full step in @xmath1 , @xmath110 , to be the result of the sequence of steps @xmath84 eq .",
    "( [ eq_stepa ] ) from @xmath82 to @xmath83 .",
    "that is we have @xmath111 : @xmath112 .",
    "we assume that all the @xmath86 s are the same ( this is always possible by making all @xmath86 s equal the largest value ) .",
    "the core of the proof is to show the lemma 2.3 of @xcite :    * lemma 2.3 : * assume that @xmath113 , @xmath114 is continuously differentiable , lipschitz , @xmath115 is continuous , @xmath116 is convex and @xmath111 is defined by the sequence of @xmath89 s of ( [ eq_stepa ] ) as described above .",
    "then for any @xmath117 @xmath118    the proof in @xcite shows that if the algorithm consists of applying the sequence of @xmath111 s and these @xmath111 s satisfy lemma 2.3 , then the algorithm converges with rate @xmath119 .",
    "thus we need to prove lemma 2.3 .",
    "we start with the analog of lemma 2.2 of ( @xcite ) .",
    "* lemma 2.2 : * for any @xmath1 , one has @xmath120 if and only if there exist @xmath121 , the subdifferential of @xmath122 , such that @xmath123 this lemma follows trivially from the definition of @xmath124 as in @xcite .",
    "* proof of lemma 2.3 : * define @xmath125 . from convexity",
    "we have @xmath126 next we have the property ( [ eq_pproperty ] )",
    ". however the @xmath127 should be primed ( @xmath128 ) because the @xmath127 has already been updated .",
    "due to space limitations we wo nt write out all the calculations but specify the sequence of operations . the details are written out in the supplementary material .",
    "we take the first term on the left side of ( [ eq_lemma23 ] ) , @xmath129 and express it in its terms ( [ eq_hierarchy ] ) .",
    "then , replace the terms using the convexity equations and substitute @xmath130 s using the lemma 2.2 .",
    "then we take the second term of the left side of ( [ eq_lemma23 ] ) , @xmath131 , again express it using ( [ eq_hierarchy ] ) , and use the inequalities ( [ eq_pproperty ] ) . putting it all together ,",
    "all the gradient terms cancel and the other terms combine to give lemma 2.3 .",
    "this completes the proof .",
    "we introduced simple and efficient algorithm from learning invariant representation from unlabelled data .",
    "the method takes advantage of temporal consistency in sequential image data . in the future we plan to use the invariant features discovered by the method to hierarchical vision architectures , and apply it to recognition problems",
    "\\1 ) we give details of the proof of lemma 2.3 .",
    "+ 2 ) we show all of the invariant filters for system of : 20x20 patches input patches , 1000 simple units , 400 invariant units .",
    "* lemma 2.3 : * assume that @xmath113 , @xmath114 is continuously differentiable , lipschitz , @xmath115 is continuous , @xmath132 is convex and @xmath111 is defined by the sequence of @xmath89 s in the paper . then for any @xmath117 @xmath118    * proof of lemma 2.3 : * define @xmath125 .",
    "we first collect the inequalities that we will need .",
    "+ from convexity we have @xmath133 next we have the property for step @xmath134 that guarantees that the energy is lowered in each step .",
    "@xmath135 finally we have the lemma 2.2 @xmath136 now we can put these equations together .",
    "the steps are : write out the left side of ( [ eq_lemma23 ] ) in terms of the definition of e. use inequalities ( [ eq_convex ] ) and ( [ eq_pproperty ] ) . eliminate @xmath137 s using ( [ eq_gamma ] ) . simplify . here",
    "are the details : @xmath138 which is the formula ( [ eq_pproperty ] ) .",
    "note that in the line 5 and in the first term of lines 6 we shifted @xmath63 by one .",
    "this completes the proof .",
    "[ [ simple - unit - and - invariant - unit - filters .- alpha0.5-beta0.3 ] ] simple unit and invariant unit filters .",
    "@xmath54 , @xmath139 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
  ],
  "abstract_text": [
    "<S> we propose a simple and efficient algorithm for learning sparse invariant representations from unlabeled data with fast inference . when trained on short movies sequences , the learned features are selective to a range of orientations and spatial frequencies , but robust to a wide range of positions , similar to complex cells in the primary visual cortex . </S>",
    "<S> we give a hierarchical version of the algorithm , and give guarantees of fast convergence under certain conditions . </S>"
  ]
}