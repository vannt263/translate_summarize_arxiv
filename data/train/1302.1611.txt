{
  "article_text": [
    "in this paper we investigate the classical stochastic multi - armed bandit problem introduced by @xcite and described as follows : an agent facing @xmath3 actions ( or bandit arms ) selects one arm at every time step until a finite time horizon @xmath4 .",
    "successive pulls of each arm @xmath5 yield a sequence of i.i.d rewards @xmath6 according to some unknown distribution @xmath7 with expected value @xmath8 . denote by @xmath9 any optimal arm defined such that @xmath10 .",
    "a _ policy _",
    "@xmath11 is a sequence of random variables @xmath12 indicating which arm to pull at each time @xmath13 and such that @xmath14 depends only on observations strictly anterior to @xmath15 .",
    "the performance of a policy @xmath16 is measured by its ( cumulative ) _ regret _ at time @xmath17 that is defined by @xmath18 observe that if we denote by @xmath19 the number of times arm @xmath20 was pulled ( strictly ) before time @xmath21 and by @xmath22 the gap between arm @xmath20 and the optimal arm , then one can rewrite the regret as @xmath23 .",
    "this formulation will be used hereafter .",
    "we refer the reader to @xcite for a survey of the extensive literature on this problem and its variations . in this paper",
    "we investigate a phenomenon that was first observed in @xcite : with some prior knowledge ( in the form of lower bounds ) on the maximal mean @xmath0 and the minimal gap @xmath24 , it is possible to obtain a regret that is _ bounded uniformly in @xmath17 _ , which implies in particular that the regret does not tend to infinity as the time horizon @xmath17 tends to infinity .",
    "note that this result is striking , as the seminal paper @xcite indicates that , if one has no prior knowledge on the distributions , then asymptotically ( in @xmath17 ) a regret of order @xmath25 is unavoidable .",
    "we describe in section  [ sec:2armed ] a simple algorithm for the two - armed bandit problem when one knows the largest expected reward @xmath0 and the gap @xmath1 . in this two - armed case , this amounts to knowing @xmath26 and @xmath27 up to a permutation .",
    "we show that the regret of this algorithm is bounded by @xmath28 , uniformly in @xmath17 .",
    "the optimality of this bound is assessed in section  [ sec : lb ] where we show that any agent knowing @xmath1 and @xmath0 must incur a regret of at least @xmath2 .",
    "this upper and lower bounds raise the following question : can such bounded regret be achieved without one of these two pieces of information ?",
    "it follows from theorems  [ th : lb2 ] and  [ th : lb3 ] that the answer to this question is negative .",
    "indeed , the sole knowledge of either @xmath1 or @xmath0 leads to a rescaled regret @xmath29 that is at least logarithmic in @xmath17 .",
    "interestingly , all these results are fully non - asymptotic , including lower bounds .    what if @xmath1 is not perfectly known but only @xmath30 such that @xmath31 ?",
    "we answer this question in section  [ sec : general ] in the context of the general @xmath3-armed bandit problem .",
    "there , we prove an upper bound on @xmath32 when one knows the maximal mean @xmath0 together with a positive lower bound @xmath33 on the smallest gap @xmath1 .",
    "specifically , we design a randomized policy for which @xmath34 moreover , it follows form our main lower bound in theorem  [ th : lb3 ] that this result can not be improved without further assumptions , since for @xmath33 of order of @xmath35 no information on the smallest gap a logarithmic growth in @xmath17 is unavoidable for the rescaled regret @xmath29 .",
    "however for @xmath33 of order @xmath1 one would expect no dependency on @xmath33 ( since at least for @xmath36 our policy of section [ sec:2armed ] attains a regret of order @xmath2 ) . to deal with this issue we propose an improvement of the basic policy that for which the term @xmath37 is replaced by @xmath38 . in particular if all the gaps @xmath39 and @xmath33 are of the same order , the logarithmic becomes a log - log term .",
    "the _ exploration - exploitation tradeoff _ is a preponderant paradigm in the bandit literature .",
    "the effects of this tradeoff already appear for the case @xmath36 in the form of the @xmath25 term derived in the original @xcite paper . indeed",
    ", there exist simple classes of ( two ! ) problems over which the regret is uniformly bounded with full information but can not be bounded uniformly with bandit feedback , see theorem  [ th : lb2 ] .",
    "clearly , this tradeoff should become more and more apparent as the number of arms increases but this is not our main focus .",
    "rather , the combination of our results sheds light on an interesting phenomenon : the effects of the tradeoff vanish when both @xmath1 and @xmath0 are known but can be seen already when @xmath36 and either @xmath1 or @xmath0 is unknown .",
    "the two - armed bandit problem when one knows the distributions of the arms up to a permutation was first investigated in @xcite .",
    "the authors observed that in that case , using a policy based on the sequential likelihood ratio test , one can obtain a regret uniformly bounded over @xmath17 .",
    "both upper and lower bounds were provided .",
    "this setting was generalized in @xcite , where the authors considered the general multi - armed bandit problem when one knows a separating value @xmath40 between the largest mean and the other means . in that case",
    "they proved the bounded regret property for a policy based on sequential likelihood ratio tests for @xmath41 vs. @xmath42 ( assuming exponential distributions to compute the likelihoods ) .",
    "they also designed a more subtle strategy for the case when only @xmath0 is known . in that case too they proved a bounded regret property .",
    "the main open problems left by these works are ( i ) to understand the limitations of bounded regret , and ( ii ) to characterize the exact dependence on the parameters in the regret ( when bounded regret is achievable ) . in this paper",
    "we make progress on both questions .    regarding the limitations of bounded regret , we prove three finite - time lower bounds , including a finite - time version of the seminal result of @xcite .",
    "ideas similar to the ones we develop in theorems  [ th : lb1 ] and  [ th : lb2 ] already appeared in @xcite but our results are fully non asymptotic with the exact dependence in the parameters involved .",
    "theorem  [ th : lb3 ] is more innovative .",
    "it shows that a logarithmic growth for the rescaled regret @xmath29 is unavoidable even if one knows @xmath0 .",
    "the proof of this result goes beyond any previous lower bound for the stochastic multi - armed bandit problem , including @xcite , since all of them required to distinguish problems with different values of @xmath0 ( such as the ones in theorem  [ th : lb2 ] for example ) . as a consequence of this theorem",
    ", we can deduce that the policies with bounded regret derived in @xcite with only the knowledge of @xmath0 must have a suboptimal dependency in @xmath2 .",
    "the knowledge of @xmath0 was also exploited in other works .",
    "for instance in @xcite , the authors showed that knowing @xmath0 allows for policies with provably better concentration properties .",
    "their policies are based on sequential likelihood ratio tests for @xmath43 vs. @xmath44 ( assuming gaussian distributions to compute the likelihoods ) .",
    "to some extent it was to be expected that the knowledge of @xmath0 leads to an improved regret as it partially removes the need for exploration : if one arm has empirical performances close to @xmath0 , one can be confident that this is the best arm without worrying that it could be the best arm only because we have not yet explored enough the other options .",
    "however note that the problem turns out to be more subtle than the above simple argument and underlines the fact that one needs more than the knowledge of @xmath0 in order to have a bounded regret with optimal scaling in @xmath2 .",
    "indeed , theorem  [ th : lb3 ] implies that the sole knowledge of @xmath0 does not warrant the bounded property for the rescaled regret @xmath29 .      throughout the paper ,",
    "we assume that the distributions @xmath7 are sub - gaussian that is @xmath45 for all @xmath46 .",
    "note that these include gaussian distributions with variance less than @xmath47 and distributions supported on an interval of length less than @xmath48 .",
    "we denote by @xmath49 the empirical mean of arm @xmath20 after @xmath50 pulls , for @xmath51 . together with a chernoff bound , it is not hard to see that the sub - gaussian assumption implies the following concentration inequality , valid for any @xmath52 , @xmath53",
    "in this section we investigate a toy example where @xmath36 and the agent knows exactly both @xmath54 ( without loss of generality ) and @xmath1 . while somewhat simplistic this example offers a convenient framework to lay the main ideas to build policies with bounded regret .",
    "[ th : alg1 ] policy  [ fig : alg1 ] has regret bounded as @xmath55 , uniformly in @xmath17 .    without loss of generality",
    "we assume that @xmath56 is the optimal arm . observe that @xmath57 summing over @xmath15 for the second event , we get @xmath58    for the third event we use the definition of the policy to obtain @xmath59 and conclude as in  .",
    "this policy has two weaknesses .",
    "first one may pay a big price for misspecifying the value of  @xmath1 .",
    "namely if one only knows a lower bound @xmath60 and substitutes @xmath33 to @xmath1 in policy  [ fig : alg1 ] , then it follows easily that the regret becomes of order @xmath61 . furthermore , for essentially the same reason , the trivial generalization of this algorithm to the @xmath3-armed case would give a regret bounded by @xmath62 .",
    "in the next section we show how to overcome these two issues using a new , randomized , policy .",
    "in this section we consider the general multi - armed case , when the agent knows @xmath54 ( without loss of generality ) and an @xmath30 such that @xmath63 .",
    "akin to policy  [ fig : alg1 ] , the policy analyzed here sets a threshold at @xmath64 and prescribes to pull a single arm above this threshold .",
    "however if all arms have their empirical mean below this threshold , then the policy is more subtle than what was described in the previous section ( where all arms were pulled in round robin fashion ) .",
    "here the policy picks an arm at random , where the probability of selecting arm @xmath20 is essentially proportional to @xmath65 , which is an empirical estimate of @xmath66 since @xmath54 .",
    "policy  [ fig : alg3 ] is slighly more general , as it uses a potential function @xmath67 , and selects arm @xmath20 with probability inversely proportional to @xmath68 .",
    "the natural choice is @xmath69 , but other choices can lead to improved performances , see theorem [ th : algpsi ] below . note that we also analyze the case where @xmath70 ( that is , when we have no information on the smallest gap ) .",
    "[ th : algpsi ] fix @xmath71 $ ] , then policy  [ fig : alg3 ] associated with the potential @xmath72 satisfies for all @xmath73 , @xmath74 furthermore for @xmath70 , let @xmath75 , then the regret is bounded as @xmath76 the dependency in @xmath33 can be reduced by using the potential @xmath77 since it yields @xmath78\\big\\}\\,.\\ ] ]    if @xmath79 is of the order of every @xmath39 , then equation upper bounds the regret in @xmath80 ; on the other hand , using the potential @xmath72 only guarantees , under the same assumptions , a bound in @xmath81 .",
    "the result for @xmath82 implies that when one has no information on the smallest gap , our policy does not obtain bounded regret but it recovers the performances of ucb , @xcite . as we shall see in section [ sec : lb ] it is in fact impossible to obtain bounded regret scaling in @xmath2 if one only knows @xmath0 .",
    "theorem [ th : algpsi ] is deduced from the following more general regret bound for policy  [ fig : alg3 ] expressed in terms of the properties of the potential @xmath83 .",
    "[ th : alg3 ] fix @xmath84 $ ] and let @xmath83 be a differentiable and increasing function @xmath85 . if @xmath86 , policy  [ fig : alg3 ] satisfies for all @xmath73 , @xmath87\\big\\}\\,.\\ ] ] furthermore for @xmath70 it satisfies @xmath88    without loss of generality we assume that @xmath56 is the optimal arm .",
    "we decompose the event of a wrong selection into three events : @xmath89    using one can easily prove that the cumulative probability of the first two events is smaller than @xmath90 . for the third event , it is convenient to define the random variable @xmath91 that indicates whether the agent plays according to ( 0 ) , ( 1 ) or ( 2 ) in policy  [ fig : alg3 ] .",
    "we write the following , using the definition of the algorithm and the fact that @xmath83 is non - decreasing , @xmath92 a simple rewriting of time then concludes the proof for the case of @xmath70 .",
    "we use the slight abuse of notation @xmath93 ^ 2 $ ] , and @xmath94 .",
    "for @xmath86 we have    @xmath95    making the change of variable @xmath96 concludes the proof of theorem  [ th : alg3 ] .",
    "theorem [ th : algpsi ] follows from theorem  [ th : alg3 ] with specific choices for @xmath83 .",
    "first , take @xmath69 , @xmath97 $ ] and observe that the integral in   can be computed as @xmath98 which gives  .",
    "when @xmath70 , since @xmath99 , equation directly gives .",
    "next , we turn to the the slightly more sophisticated potential function @xmath77 . observe that for any @xmath100 , @xmath101 therefore , for @xmath102 $ ] , the integral in   is bounded from above by @xmath103 } dx&\\le   \\int_{{\\varepsilon}/2}^{1 } \\frac{8}{x\\log(4x/{\\varepsilon } ) } dx+ \\int_{1}^{\\infty } 9e^{-\\frac{x^2}{2 } } dx\\\\   & \\le 8\\log\\log(4/{\\varepsilon})-8\\log\\log 2 + 4\\\\   & \\le 8\\log\\log(4/{\\varepsilon})+7\\,.\\end{aligned}\\ ] ] it concludes the proof of  .",
    "we conclude our study of bounded regret in stochastic multi - armed bandits with three different lower bounds . for simplicity , we phrase these results for the simple two - armed case .",
    "first we show with theorem [ th : lb1 ] that if one knows both @xmath0 and @xmath1 , then the best attainable regret is of order @xmath2 , which matches ( up to a numerical constant ) the result of theorem [ th : alg1 ] .",
    "next we show in theorem [ th : lb2 ] that the sole knowledge of @xmath1 leads to a lower bound of order @xmath104 .",
    "this theorem implies that the bounds of @xcite , @xcite and @xcite exhibit a tight dependence in @xmath1 ( for the two - armed case ) , unlike the famous result of @xcite .",
    "moreover , compared to the proof of @xcite , our approach is ( i ) much simpler , ( ii ) non - asymptotic and ( iii ) it is not limited to a certain class of policies . finally we show in theorem  [ th : lb3 ] that if one only knows @xmath0 then a regret of order @xmath105 is unavoidable ( for some value of @xmath1 ) .",
    "our proof strategy consists in rephrasing arm selection as a hypothesis testing problem , and then use well - known lower bounding techniques for the minimax risk of hypothesis testing .",
    "for instance , the proof of theorem [ th : lb1 ] and theorem [ th : lb2 ] builds upon the following result ; see ( * ? ? ?",
    "* chaper 2 ) for a proof , or lemma [ lem:2 ] below with @xmath106 chosen to be a dirac mass at @xmath47 .",
    "recall that the kullback - leibler divergence between two positive measures @xmath107 with @xmath108 absolutely continuous with respect to @xmath109 , is defined as @xmath110    [ lem:1 ] let @xmath111 be two probability distributions supported on some set @xmath112 , with @xmath113 absolutely continuous with respect to @xmath114 . then for any measurable function @xmath115 , one has @xmath116    in this section we denote by @xmath117 the product distribution that generates the rewards from @xmath118 when pulling arm @xmath119 .",
    "the regret of a policy that observes such rewards is denoted by @xmath120 .",
    "finally let @xmath121 denote the probability associated to @xmath122 and by @xmath123 the corresponding expectation .",
    "hereafter , we favor rewards that are normally distributed because they lead to simpler calculations of the kl - divergence . however , our lower bounds remain of the same order for all families of distributions @xmath124 with expected value @xmath125 and such that @xmath126 for some absolute constant @xmath127 .",
    "this is the case , for example , of the bernoulli distribution with parameter @xmath125 as long as @xmath125 remains bounded away from 0 and 1 ; see , e.g. , ( * ? ? ?",
    "* lemma  4.1 ) .",
    "the first lower bound illustrates that when one knows the distributions up to a permutation , the best one can hope for is a bounded regret of order @xmath2 .",
    "[ th : lb1 ] let @xmath128 and @xmath129 .",
    "then for any policy , and for every @xmath73 , @xmath130    in this proof we assume that the policy has access to @xmath15 rewards from each arm at time step @xmath15 .",
    "clearly this full information setting is simpler than the bandit setting , and thus a lower bound for the former implies one for the latter . using lemma",
    "[ lem:1 ] as well as straightforward computations one obtains @xmath131    the above theorem ensures that the regret bound of theorem  [ th : alg1 ] has the correct dependence in @xmath1 .",
    "this is quite surprising as the original bound of  @xcite indicates that without the knowledge of @xmath0 and @xmath1 , one can incur a regret that diverges to infinity at a logarithmic rate .",
    "the next result shows that this logarithmic regret already appears when one does not know the value of @xmath0 .",
    "thus the knowledge of @xmath1 without the knowledge of @xmath0 is not sufficient to obtain a bounded regret .",
    "moreover , the following lower bound matches the upper bounds ( for the two - armed case ) of  @xcite , @xcite and  @xcite , thus proving their optimality .",
    "[ th : lb2 ] let @xmath132 and @xmath133 .",
    "then for any policy , and any @xmath73 , @xmath134    first note that @xmath135 furthermore , denoting by @xmath136 ( respectively @xmath137 ) the law of the observed rewards up to time @xmath15 under @xmath122 ( respectively under @xmath138 ) , and following the same computations than in the previous proof , one also obtains @xmath139 since under @xmath122 , arm 1 is uninformative , it follows from basic calculation that @xmath140 the above three displays yield @xmath141 } \\frac{\\delta}{2 } \\left(x + \\frac{n}{4 } \\exp(- 2 \\delta^2 x ) \\right)\\\\ & \\geq   \\frac{\\log(n \\delta^2 / 2)}{4 \\delta } .\\end{aligned}\\ ] ]    finally we prove that the knowledge of @xmath0 without the knowledge of @xmath1 is not sufficient either to obtain a bounded rescaled regret @xmath29 .",
    "this result is more difficult , and falls within the more general topic of lower bounds for adaptive rates .",
    "first we need to generalize lemma [ lem:1 ] to deal with both a composite alternative , and a rescaled risk .",
    "the proof of this result is standard and postponed to the appendix .",
    "[ lem:2 ] let @xmath114 and @xmath142 be probability distributions supported on some set @xmath112 , with @xmath143 absolutely continuous with respect to @xmath114 .",
    "let @xmath106 be a finite positive measure on @xmath144 .",
    "then for any measurable function @xmath115 , one has @xmath145 where @xmath146 is the positive measure on @xmath112 defined by @xmath147 and @xmath148 .",
    "note that @xmath149 is not a probability distribution , however it is a positive measure thus the kullback - leibler divergence in the above lemma is well - defined .",
    "[ th : lb3 ] let @xmath150 , and @xmath151 , @xmath152 $ ] .",
    "then for any policy , and any @xmath73 , @xmath153 } \\delta r_n(\\nu_{\\delta})\\right ) \\geq \\frac{1}{2 } \\log(n / 139 ) .\\ ] ]    theorem  [ th : lb3 ] can be read as follows : for any policy , and any @xmath73 , there exists @xmath152 $ ] and a problem instance with gap @xmath1 and optimal value @xmath154 such that on this problem one has @xmath155    similarly to the previous proof we define @xmath156 and @xmath157 as the law of the observed rewards up to time @xmath15 . lemma  [ lem:2 ] yields @xmath158 } \\delta r_n(\\nu_{\\delta})\\right ) \\geq \\frac{1}{2c_\\lambda } \\sum_{t=1}^n \\exp\\left(- { \\mathrm{kl}}\\left(\\nu_{0,t } , \\int \\delta \\nu_{\\delta , t } d\\lambda(\\delta ) \\right ) \\right).\\ ] ] for @xmath159 , define the average rewards for arm @xmath160 by @xmath161 .",
    "therefore , @xmath162 , @xmath163 and @xmath164 .",
    "recall that a policy @xmath165 taking values in @xmath166 generates a sequence of rewards @xmath167 distributed according to @xmath168 . the joint density",
    "( with respect to the lebesgue measure ) @xmath169 of @xmath170 , where @xmath171 can be computed easily using the chain rule for conditional densities .",
    "it is given by @xmath172 choosing @xmath173 and @xmath174 respectively , it yields @xmath175\\big)\\\\ & = \\exp\\big(-\\frac{1}{2}\\sum_{\\substack{\\ell=1\\\\i_\\ell=1 } } ^t\\big[(y^{(1)}_\\ell+\\delta)^2 -(y^{(1)}_\\ell)^2 \\big]-\\frac{1}{2}\\sum_{\\substack{\\ell=1\\\\i_\\ell=2 } } ^t\\big[(y^{(2)}_\\ell)^2 -(y^{(2)}_\\ell+1)^2 \\big]\\big)\\\\ & = \\exp \\left ( - \\frac{t^{(1)}}{2 } ( 2 \\delta \\hat{\\mu}^{(1 ) } + \\delta^2 ) + \\frac{t^{(2)}}{2 } ( 2 \\hat{\\mu}^{(2 ) } + 1 ) \\right)\\,,\\end{aligned}\\ ] ] where we denote for simplicity @xmath176 dropping the dependency in @xmath177 from the notation , it yields @xmath178 and thus @xmath179 where the last line follows standard computations .",
    "next , it follows from the cauchy - schwarz inequality that the function @xmath180 is convex for any function @xmath181 .",
    "together with the jensen inequality , it yields @xmath182 define @xmath183 and let @xmath106 be the uniform distribution on @xmath184 $ ] .",
    "since @xmath185 for @xmath186 , it yields @xmath187    thus we have proved that @xmath188 plugging this into one obtains @xmath189 } \\delta r_n(\\nu_{\\delta})\\right ) & \\geq \\frac{\\sqrt{n}}{8c_\\lambda } \\exp\\left(- \\frac{1}{2 } { { \\rm i}\\kern-0.18em{\\rm e}}_{\\nu_0 } t_2(n ) \\right)\\\\ & \\ge \\frac{\\sqrt{n}}{16 } \\exp\\left(- \\frac{1}{2 } { { \\rm i}\\kern-0.18em{\\rm e}}_{\\nu_0 } t_2(n ) \\right)\\,,\\end{aligned}\\ ] ] where we use the fact that @xmath190 , which implies @xmath191 .",
    "on the other hand one also has @xmath192 therefore @xmath189 } \\delta r_n(\\nu_{\\delta})\\right ) & \\geq \\min_{x \\in [ 0,n ] } \\frac{1}{2}\\big(x+\\frac{\\sqrt{n}}{16}\\exp(-x/2 )   \\big)\\\\ & = \\frac{1}{2}\\log(n/139)\\,.\\end{aligned}\\ ] ]    theorem  [ th : lb2 ] and  [ th : lb3 ] have important consequences on the _ exploration - exploitation tradeoff _ mentioned in the introduction . indeed , consider the full information case where at each round , the agent observes the reward of both arms . in this case , it is not hard to see that the policy that indicates to pull the arm with the best average reward has bounded regret of order @xmath2 .",
    "therefore , the knowledge of @xmath1 or @xmath0 alone does not alleviate the price for exploration .",
    "however , when both are known , it vanishes ( see theorem  [ th : alg1 ] ) .",
    "* acknowledgments .",
    "* we are indebted to alexander goldenshluger for bringing the reference @xcite to our attention .",
    "throughout the proof , radon - nikodym derivatives over @xmath112 are taken with respect to a common but unspecified reference measure .",
    "it does not enter our final result .",
    "it follows from fubini s theorem that @xmath193 furthermore the last expression is clearly minimized for @xmath194 .",
    "it yields @xmath195 note that the latter quantity is often referred to as _",
    "hellinger affinity _ and does not depend on the reference measure on @xmath112 ; see , e.g. , @xcite , chapter 2 . now using the cauchy - schwarz inequality and the fact that @xmath196 we get @xmath197 the above three displays together yield @xmath198 to complete the proof , observe that the jensen inequality yields @xmath199 \\\\ & \\geq   \\exp\\big[2 \\int \\log \\big ( \\sqrt{\\frac{d\\bar{\\rho}}{d\\rho_0 } } \\big ) d\\rho_0 \\big]\\\\ & =   \\exp [ - { \\mathrm{kl}}(\\rho_0 , \\bar{\\rho } ) ] .\\end{aligned}\\ ] ]"
  ],
  "abstract_text": [
    "<S> we study the stochastic multi - armed bandit problem when one knows the value @xmath0 of an optimal arm , as a well as a positive lower bound on the smallest positive gap @xmath1 . </S>",
    "<S> we propose a new randomized policy that attains a regret _ uniformly bounded over time _ in this setting . </S>",
    "<S> we also prove several lower bounds , which show in particular that bounded regret is not possible if one only knows @xmath1 , and bounded regret of order @xmath2 is not possible if one only knows @xmath0 . </S>"
  ]
}