{
  "article_text": [
    "the problem of finding global extrema ( maxima and minima ) for a univariate real function is important for variety of real world applications .",
    "for example , it arises in electric engineering @xcite,@xcite , @xcite in computer science @xcite , @xcite , @xcite and in various other fields ( see @xcite for further references ) . in many industrial applications the global optimization algorithm",
    "is expected to operate in real time while simultaneously , finding the global extremum of non - convex functions exhibiting large number of local sub - extrema .    the problem of efficiently finding a function s global extremum has been historically challenging .",
    "one of the first solutions , zero derivative method ( zdm ) , was proposed by pierre de fermat ( 1601 - 1665 ) .",
    "his main idea was to look for the global extremum among critical points : the points where the derivative of the target function is zero . despite its theoretical significance",
    ", fermat s proposed method ( zdm ) is limited by the numerical difficulties imposed by finding critical points .",
    "one of the leading global optimization approaches adopted by many industrial applications is a brute - force search or exhaustive search for the global extremum ( brute - force search ( bfs ) ) .",
    "it is simple to implement but its performance linearly depends on the complexity of the target function and the size of the search area .",
    "a plethora of optimization methods have been developed for various types of target functions . among them",
    "piyavskii - shubert method ( psm ) occupies a special place @xcite , @xcite , @xcite .",
    "it is one of the few procedures that delivers the global extremum for a univariate function and at the same time it exhibits reasonable performance as long as the respective lipschitz constant is of a modest value . on the other hand",
    ", the method is very sensitive to the size of the lipschitz constant : its performance sharply diminishes for large lipschitz constants . for this reason ,",
    "accelerations and improvements of psm were developed in the following papers @xcite , @xcite , @xcite , @xcite .",
    "numerical experiments presented in this paper show that leap gradient algorithm ( referred as lga ) significantly outperforms psm together with its modifications and improvements ( from @xcite , @xcite , @xcite ) when finding global extrema of polynomials .",
    "the method of gradient descent ( see , e.g. @xcite , @xcite , @xcite ) is widely used to solve various practical optimization problems .",
    "the main advantage of the gradient descent algorithm is its simplicity and applicability to a wide range of practical problems . on the other hand",
    ", gradient descent has limitations imposed by the initial guess of a starting point and then its subsequent conversion to a suboptimal solution .",
    "this paper gives practical recipes on how to overcome those limitations for a univariate function and how to equip the gradient descent algorithm with abilities to converge to a global extremum .",
    "it is achieved via evolutionary leaps towards the global extremum .",
    "lga neither requires the knowledge of the lipschitz constant nor convexity conditions that are often imposed on the target function .",
    "moreover , lga naturally becomes the standard gradient descent procedure when the target function is convex or when the algorithm operates in the close proximity to the global extremum .",
    "the recursive application of lga yields an efficient algorithm for calculating global extrema for univariate polynomials .",
    "lga does not intend to locate any critical points ( like zdm ) instead it follows gradient descent ( ascent ) until a local extremum is reached and then performs an evolutionary leap towards the next extremum .",
    "as far as performance is concerned , numerical experiments conducted for univariate polynomials show that lga outperforms bfs , zdm and psm with all its modifications from @xcite , @xcite , @xcite .",
    "the layout of this publication is as follows .",
    "section [ intro ] is the introduction .",
    "section [ sec:2 ] introduces lga for univariate functions .",
    "section [ lgarecursion ] explores the recursive application of lga .",
    "section [ multiw ] describes a recursive implementation of lga for polynomials .",
    "section [ realanalytic ] presets an implementation of lga for real analytic univariate functions .",
    "section [ numerical_experiments ] reports on numerical experiments with lga .",
    "it compares lga with zdm , bfs , psm and accelerations of psm presented in @xcite , @xcite , @xcite .",
    "polynomial roots for zdm are calculated with the help of laguerre s method @xcite , @xcite .",
    "section [ acknowledgment ] expresses gratitude to professionals who spent their time contributing to the paper , reviewing various parts of the publication and making many valuable suggestions and improvements .",
    "section [ appendix ] finalizes the publication with a snapshot of the working and thoroughly tested source code for lga .",
    "let @xmath0 be known and well defined real function on @xmath1,$ ] a closed interval of real numbers . consider the optimization problem @xmath2}\\ ] ] lga can solve it with a given precision @xmath3 if its solution exists .    [ [ leap - gradient - algorithm - lga . ] ]   * leap gradient algorithm ( lga ) .",
    "* + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    * set @xmath4 * iterate @xmath5 as long as @xmath6 and @xmath7 if @xmath8 then * stop * and take @xmath9 as an estimate of the argument and the value for the global minimum .",
    "+ if @xmath10 then proceed with * step 2*. * let @xmath11 be the solution of the following optimization problem @xmath12}\\ ] ] if @xmath13 then * stop * and @xmath14 is an estimate of the argument and the value for the global minimum .",
    "+ if @xmath15 then @xmath16 ( lga performs _ an evolutionary leap _ ) and go to * step 1*.    lga is illustrated in fig .",
    "[ fig:1 ] .",
    "epsf    if the target function is twice continuously differentiable on @xmath17 then the number of inflection points from @xmath17 is related to the number of evolutionary leaps performed by lga ( see * step 2 * ) .    [ inflection_and_leaps ]",
    "let @xmath0 be twice continuously differentiable on @xmath17 .",
    "let @xmath18 be the value of the evolutionary leap ( * step 2 * of lga ) .    if @xmath19 then @xmath20 $ ] contains at least two points where @xmath21 is equal to zero .",
    "evolutionary leaps from @xmath22 ( on @xmath23 are solutions of the following equation . @xmath24 where @xmath25    indeed , if @xmath0 is continuously differentiable then the necessary condition for @xmath26 to be the solution for ( [ optimization_proc ] ) on @xmath27 is @xmath28    differentiating yields @xmath29 after multiplying the equation with @xmath30 we obtain ( [ leap_formula ] ) .",
    "since @xmath18 is the solution of the problem ( [ optimization_proc ] ) and @xmath0 is twice differentiable we conclude that @xmath31 differentiating yields @xmath32 when @xmath33 hence , making use of ( [ leap_formula ] ) we obtain @xmath34    according to * step 2 * of lga for the evolutionary leap @xmath18 we have @xmath35 that together with ( [ leap_formula ] ) implies @xmath36 on the other hand , ( [ gradient_decent ] ) yields the existence of @xmath37 such that @xmath38 that together with ( [ rt ] ) yield the existence of @xmath39 where @xmath40 taking into account that @xmath41 is obtained after * step 1 * of lga and @xmath42 we obtain the existence of @xmath43 where @xmath44 under the conditions of the theorem @xmath45 is continuous on @xmath17 and the statement follows from ( [ rightplus ] ) , ( [ midminus ] ) and ( [ leftplus ] ) .    that means ( in a generic situation )",
    "a non trivial evolutionary leap inside @xmath27 is only possible over two inflection points of a target function .    consider the optimization problem @xmath46 where @xmath47 and @xmath48 are real numbers .",
    "if @xmath49 then @xmath50 has two different zeroes",
    ". hence , by theorem [ inflection_and_leaps ] , lga might need to perform a single evolutionary leap ( over two inflection points ) in order to solve the optimization problem .",
    "otherwise , @xmath51 , lga coincides with the standard gradient decent .",
    "lga replaces the optimization problem @xmath52}\\ ] ] with @xmath53}\\ ] ] where @xmath22 is calculated at the previous step of lga .",
    "it leads us to the following recursive procedure executed at each iteration of lga . @xmath54 and",
    "@xmath55 in order to complete an iteration of lga for @xmath56 one needs to calculate @xmath57}\\ ] ] and lga finds the minimum of @xmath58 if finding the minimum for @xmath59 is obvious , then the iteration of lga is completed after @xmath60 recursive steps .",
    "the next theorem shows when an lga iteration is completed after a finite number of recursive steps .",
    "if a real function @xmath0 is at least @xmath61-times continuously differentiable on the segment @xmath1 $ ] and @xmath62\\ ] ] then each iteration of lga for @xmath52}\\ ] ] is completed after not more than @xmath63 recursive steps .",
    "the function can be represented by taylor expansion centered at @xmath64 @xmath65 for all @xmath66.$ ] in accordance with notations ( [ iter_1 ] ) and ( [ iter_2 ] ) we have @xmath67 and @xmath68 under the conditions of the theorem @xmath69 achieves its minimum on @xmath70 $ ] at @xmath71    recursive lga is an efficient numerical method for finding global extrema of univariate polynomials .    for any polynomial @xmath72 on a segment",
    "@xmath1 $ ] recursive lga delivers the global extremum for @xmath73 in a finite number of steps .",
    "the proof is conducted by mathematical induction with respect to the degree of a polynomial . as a basis of mathematical induction and for the purpose of illustrations let us consider finding the global minimum on @xmath1 $ ] for the quadratic polynomial @xmath74 in accordance with notations ( [ iter_1 ] ) , ( [ iter_2 ] ) @xmath75 if @xmath76 then @xmath77 has its minimum at @xmath78 if @xmath79 then according to lga the global minimum is reached ad @xmath80 and the procedure stops .",
    "otherwise , @xmath81 lga leads us to @xmath82 where the step size @xmath83 is dictated by the required precision of lga .",
    "we follow the standard gradient descent when calculating @xmath84 as long as @xmath85 the gradient descent either stops at @xmath86 and the global minimum is located at @xmath86 or , according to lga , it stops at the first @xmath22 such that @xmath87 and @xmath22 delivers the global minimum .    if @xmath88 then the minimum for @xmath77 is located at @xmath89 according to lga , if @xmath90 then the minimum is at @xmath91 otherwise , the minimum is at @xmath89 the basis of the mathematical induction is established .",
    "the step of mathematical induction follows directly from recursive lga . indeed , suppose that recursive lga delivers , in a finite number of steps , the global minimum for any polynomial of degree less than @xmath92 however , following notations ( [ iter_1 ] ) and ( [ iter_2 ] ) , in order to calculate the global minimum for a polynomial @xmath93 of @xmath61-th degree we need to calculate the global minimum for @xmath94 a polynomial of @xmath95-st degree .",
    "hence , the statement follows by mathematical induction .",
    "horner s algorithm ( see , e.g. , @xcite , @xcite ) plays the central role in implementation of recursive lga for polynomials .",
    "lga reduces the optimization problem @xmath52}\\ ] ] to @xmath96}\\ ] ] where @xmath22 is calculated at the previous step of lga . if @xmath97 is a polynomial with real coefficients then",
    "so is @xmath98 where coefficients @xmath99 are calculated as follows .",
    "[ [ horners - algorithm ] ] * horner s algorithm * + + + + + + + + + + + + + + + + + + + + +    * @xmath100 * @xmath101    recursive lga ( [ iter_1 ] ) , ( [ iter_2 ] ) described in section [ lgarecursion ] is reduced to a finite number of iterations for horner s algorithm until the resulted polynomial is either a linear or a quadratic function",
    ". then the solution of the optimization problem is trivial and therefore the recursive procedure delivers the global extremum .    performing an evolutionary leap is a numerically expensive operation .",
    "theorem [ inflection_and_leaps ] shows that each lga leap inside the interval is a jump over two zeroes of the second derivative of the target function .",
    "that allows to improve the performance of lga for polynomials by limiting the number of evolutionary jumps by at most @xmath102 where @xmath61 is the degree of the target polynomial .",
    "the respective modification of lga for @xmath103}\\ ] ] is as follows .",
    "[ [ lga - for - polynomials ] ] * lga for polynomials * + + + + + + + + + + + + + + + + + + + + +    * if the degree of the polynomial is @xmath104 then return @xmath105 otherwise , return @xmath106 as the argument , value pair for the global minimum .",
    "+ if the degree is equal to @xmath107 @xmath108 if @xmath109 then return @xmath110 for @xmath111 and return @xmath112 when @xmath113 otherwise , return @xmath114 where @xmath115 + if the degree of the polynomial is larger than @xmath107 then set @xmath116 @xmath4 * iterate @xmath5 as long as @xmath117 and @xmath118 where @xmath119 are calculated with horner s algorithm .",
    "* * @xmath120 * * @xmath121 + if @xmath8 then * stop * and return @xmath86 as the argument and @xmath122 as the value for the estimate of the global minimum .",
    "+ if @xmath123 then proceed with * step 2*. * if lga already performed @xmath124 evolutionary jumps , @xmath125 then * stop * and return @xmath22 and @xmath126 as the argument , value pair for the estimate of the global minimum .",
    "+ otherwise , recursively apply lga to @xmath127}\\ ] ] + let @xmath11 be the solution of ( [ recursive_lga ] ) . if @xmath128 and @xmath129 then * stop * and return @xmath64 @xmath126 as an argument , value estimate of the global minimum .",
    "+ if @xmath130 and @xmath131 then , by theorem [ inflection_and_leaps ] , increment @xmath132 by one if @xmath22 equals to @xmath133 otherwise by two .",
    "after that lga performs _ an evolutionary leap _ by setting @xmath134 and proceeding with * step 1*.    a polynomial @xmath126 in lga is evaluated with horner s algorithm as follows .    set @xmath135 then iterate @xmath136 and @xmath137 is the value of the polynomial at @xmath138    interested reader will find a snapshot of the working and thoroughly tested source code of lga in appendix of this paper .",
    "consider the optimization problem @xmath139}\\ ] ] where @xmath0 is a univariate real analytic function on @xmath140.$ ] that means @xmath141 and the series uniformly converges to @xmath0 on @xmath140.$ ] @xmath142 denotes the value of the derivative @xmath143 at @xmath144    this section proposes a numerical procedure for solving ( [ optimization_problem_analytic ] ) .",
    "the procedure is based on lga .",
    "namely , ( [ optimization_problem_analytic ] )   is replaced with @xmath145}\\ ] ] where @xmath146 and @xmath147 the numerical algorithm is based on the following statement .",
    "[ real_analytic_lga ] for any @xmath148 $ ] @xmath149    it follows from taylor expansion with the integral remainder term that @xmath150 in order to calculate @xmath151 let us justify the following statements with the help of mathematical induction . @xmath152 the basis of mathematical induction follows from @xmath153 the step of mathematical induction for each of the statements ( [ first ] ) , ( [ second ] ) , ( [ third ] ) is as follows .",
    "suppose that ( [ first ] ) is true for @xmath154 and @xmath155 to prove that it remains true for @xmath156 and @xmath157 consider @xmath158 and @xmath159 due to the assumption of the mathematical induction @xmath160 for @xmath161 the statement ( [ first ] ) follows .",
    "assume that ( [ second ] ) is true for @xmath162 consider @xmath163 and ( [ first ] ) together with the assumption of the mathematical induction yield @xmath164 the statement ( [ second ] ) is established .",
    "assume that ( [ third ] ) is valid for @xmath162 then @xmath165 by the assumption of the mathematical induction and taking into account ( [ first ] ) , ( [ second ] ) we have @xmath166 statement ( [ third ] ) is established .",
    "applying @xmath167 to the taylor expansion @xmath168 and taking into account ( [ first ] ) , ( [ second ] ) yields @xmath169 on the other hand , @xmath170 and so is @xmath171 therefore setting @xmath172 in ( [ fourth ] ) and making use of ( [ third ] ) we obtain @xmath173 that completes the calculation of ( [ difference ] ) and the proof .",
    "given the required margin of error @xmath174 theorem [ real_analytic_lga ] provides an effective recipe for finding the global minimum of a real analytic function on the interval @xmath140.$ ]    [ [ lga - for - real - analytic - functions ] ] * lga for real analytic functions * + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    * find a natural number @xmath61 so that @xmath175 * calculate a step size @xmath3 such that @xmath176 * use lga to solve optimization problem ( [ optimization_problem_lga ] ) with @xmath3 from * step 2*.    let @xmath177 be the @xmath26-argument of the global minimum calculated at * step 3*. let @xmath178 be the @xmath26-argument of the global minimum calculated for the original function @xmath0 from ( [ optimization_problem_analytic ] )",
    ". then @xmath179 and @xmath180",
    "this section presents the results of numerical experiments conducted in order to compare the performance of lga with brute - force search ( bfs ) , zero derivative method ( zdm ) , modifications of piyavskii - shubert method ( psm ) discussed in @xcite , @xcite , @xcite .",
    "all numerical experiments follow the same scenario :    * repeat 500 times * step 1 * and * step 2*. * * randomly generate a real polynomial @xmath73 of degree @xmath61 with roots uniformly distributed on @xmath181\\times [ -1,\\;1],$ ] where @xmath86 is a real number between @xmath182 and @xmath183 which remains fixed across all 500 trials .",
    "* * use lga and its competitor to solve the optimization problem @xmath184}\\ ] ] with precision @xmath185 for @xmath26-argument of the global minimum on @xmath140.$ ] record the processing time for lga and its competitor . * after repeating 500 times @xmath186 calculate @xmath187 and @xmath188 the average processing time for lga and its competitor respectively .",
    "* update the file with the experimental records by adding a new line @xmath189 where @xmath61 is the degree of the polynomial .",
    "the final result is presented in the form of two curves ( average time spent versus polynomial degree ) , one for lga and the other for its competitor .",
    "total time spent @xmath190 includes all necessary supplementary steps that are needed in order to successfully implement the tested algorithm .",
    "for example , zdm total time covers calculation of critical points with laguerre s method and the subsequent search for the minimum among critical values .",
    "psm time includes calculation of the lipschitz constant or its counterparts .",
    "bfs attacks @xmath191}\\ ] ] by transforming it into @xmath192 and then taking the smallest value in @xmath193 and its respective @xmath26-argument as an estimate for the solution of the optimization problem .",
    "lga outperforms bfs for polynomials with roots uniformly distributed on @xmath181\\times [ -1,\\;1]$ ] where @xmath194 lga considerably speeds up as the value of the parameter @xmath86 decreases ( fig .",
    "[ fig:2 ] , fig .",
    "[ fig:3 ] ) .",
    "epsf    epsf    if @xmath195 then lga works exactly so well as bfs ( fig .",
    "[ fig:4 ] , fig .",
    "[ fig:5 ] ) .",
    "a generic application corresponds to the situation with @xmath196 therefore employing lga instead of bfs will improve the performance of your application .",
    "epsf    epsf      zdm finds the global minimum @xmath191}\\ ] ] by calculating zeroes @xmath197 of the derivative @xmath198\\ ] ] and then finding the smallest value in @xmath199 if it is @xmath200 then zdm returns @xmath201 as an estimate for the solution of the optimization problem . in all numerical experiments reported in this paper",
    "the polynomial roots for zdm were calculated with laguerre s method @xcite , @xcite .",
    "the zdm processing time includes the invocation of laguerre s method .",
    "lga notably faster than zdm ( fig .",
    "[ fig:6 ] ) .",
    "epsf      piyavskii s type algorithms tackle the optimization problem @xmath202}\\ ] ] by constructing at each iteration either a piecewise linear @xmath203 is lipschitz @xcite , @xcite , @xcite , @xcite ) or a piecewise quadratic @xmath204 is lipschitz @xcite , @xcite , @xcite , @xcite ) auxiliary function @xmath205 such that @xmath206\\ ] ] then the original optimization problem is replaced with @xmath207}\\ ] ] based on its solution the algorithm either terminates or proceeds to the next step with the new refined auxiliary function @xmath208      lga is compared against psm with tuning of the local lipschitz constants ( referred as lt ) and its enhancement lt_li presented in @xcite . in view of the numerical simulations from @xcite",
    "lt and lt_li appear to be the fastest among pyavskii s type algorithms ( discussed in @xcite ) with piecewise linear auxiliary functions .",
    "lga is faster than lt li ( fig .",
    "[ fig:7 ] ) .",
    "epsf      modifications of psm with smooth piecewise quadratic auxiliary functions are discussed in @xcite .",
    "[ fig:8 ] presents the comparison results between lga and psm enriched by local tuning of the lipschitz constant for @xmath209 ( referred as dlt ) .",
    "lga outperforms dlt .",
    "epsf    dlt with some local improvement technique @xcite is addressed as dlt_li .",
    "its comparison with lga is presented by fig.[fig:9 ] .",
    "lga is faster than dlt_li .",
    "epsf      the paper @xcite introduces the modification of psm based on piecewise quadratic auxiliary functions that are not necessary smooth . the algorithm from @xcite",
    "is referred in fig.[fig:10 ] as eek .",
    "lga is faster than eek .",
    "the author is grateful to anonymous referees for comments and suggestions that helped to focus and improve the original manuscript . in particular",
    ", the comparison between lga and piyavskii - shubert method was added upon a referee remark .",
    "the author is thankful to blaiklen marx for the help with preparing the manuscript for publication and testing the java implementations of algorithms in i-oblako.org framework .",
    "the program is looking for the global minimum of a polynomial on the interval @xmath1.$ ] the polynomial is represented as an array              while(polynom[polynom.length - 1 ] = = 0 & & polynom.length > 1 ) {       pl= new double[polynom.length - 1 ] ;       for(int i = 0 ; i",
    "< polynom.length - 1;i++ )             pl[i]=polynom[i ] ;      polynom = pl ; } if(polynom.length = = 1 )    return a ; if(polynom.length = = 2 ) {    if(polynom[1]>= 0 )          return a ;    else          return b ; }                  pl = horner(polynom , rt_prev ) ;            rt = getmin(pl , rt_prev , b , step ) ;          if(rt - rt_prev<= step )                return rt_prev ;          else {          if(rt_prev = = a )            njumps++ ;          else            njumps = njumps + 2 ;           }              public double gradientdescent(double [ ] polynom ,                                double a , double b ,                                       double step ) {             double   rt = a ;                    while(evalderivative(polynom , rt)<0 & & rt < b )                               rt = rt+step ;               return rt ; } ....      d.a",
    ". adams , `` a stopping criterion for polynomial root finding . ''",
    "acm,10 , 655 - 658 , ( 1967 ) .",
    "l. armijo , minimization of functions having lipschitz continuous first partial derivatives , pacific j. math . , 16 , 1 - 3 ( 1966 ) j. m. calvin , an adaptive univariate global optimization algorithm and its convergence rate under the wiener measure , informatica , 22 , 471488 ( 2011 ) . j. m. calvin and a. zilinskas , one - dimensional global optimization for observations with noise , comput .",
    "appl . , 50 , 157169 ( 2005 ) .",
    "r. ellaia , m. z. es - sadek , h. kasbioui , modified piyavskii s global one - dimensional optimization of a differentiable function , applied mathematics , 3 , 1306 - 1320 ( 2012 ) n.j .",
    "higham , accuracy and stability of numerical algorithms , siam , philadelphia ( 2002 ) .",
    "k. hamacher , on stochastic global optimization of one - dimensional functions , phys . a , 354 , 547557 ( 2005 ) .",
    "o. gler , foundations of optimizations , springer , new york ( 2010 ) d. e. johnson , introduction to filter theory , prentice hall , new jersey , ( 1976 ) l. kantorovich and g. akilov , functional analysis in normed spaces , fizmatgiz , moscow ( 1959 ) , translated by d. brown and a. robertson , pergamon press , oxford ( 1964 ) d. kalra and a. h. barr , guaranteed ray intersections with implicit surface , comput .",
    ", 23 , 297306 ( 1989 ) d.e .",
    "knuth , art of computer programming , vol .",
    "2 : seminumerical algorithms , 3rd ed . , ma : addison - wesley , ( 1998 ) d.e .",
    "kvasov , y.d .",
    "sergeyev , a univariate global search working with a set of lipschitz constants for the first derivative , optim .",
    ", 303 - 318 ( 2009 ) h. y .- f .",
    "lam , analog and digital filters - design and realization , prentice hall , new jersey , ( 1979 ) yu . nesterov , introductory lectures on convex optimization , applied optimization , 87 , kluwer academic publishers , boston ( 2004 ) s. piyavskii , an algorithm for finding the absolute minimum of a function , theory of optimal solutions , ik akad .",
    "nauk ussr , kiev , 2 , 13 - 24 , ( 1967 ) s. piyavskii , an algorithm for finding the absolute extremum of a function , ussr comput .",
    ", 12 , 57 - 67 , ( 1972 ) b. shubert , a sequential method seeking the global maximum of a function , siam j. numer .",
    ", 9 , 379 - 388 ( 1972 ) d. lera and y. d. sergeyev , acceleration of univariate global optimization algorithms working with lipschitz functions and lipschitz first derivatives , siam j. optim . , 23 , 1 , 508 - 529 ( 2013 ) y. d. sergeyev , global one - dimensional optimization using smooth auxiliary functions , math .",
    "programming , 81 , 127 - 146 ( 1998 )"
  ],
  "abstract_text": [
    "<S> the paper proposes a new algorithm for solving global univariate optimization problems . </S>",
    "<S> the algorithm does not require convexity of the target function . for a broad variety of target functions </S>",
    "<S> after performing ( if necessary ) several evolutionary leaps the algorithm naturally becomes the standard descent ( or ascent ) procedure near the global extremum . </S>",
    "<S> moreover , it leads us to an efficient numerical method for calculating the global extrema of univariate real analytic functions . </S>"
  ]
}