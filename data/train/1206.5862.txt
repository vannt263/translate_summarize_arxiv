{
  "article_text": [
    "bayesian nonparametrics is the area of bayesian analysis in which the finite - dimensional prior distributions of classical bayesian analysis are replaced with stochastic processes .",
    "while the rationale for allowing infinite collections of random variables into bayesian inference is often taken to be that of diminishing the role of prior assumptions , it is also possible to view the move to nonparametrics as supplying the bayesian paradigm with a richer collection of distributions with which to express prior belief , thus in some sense emphasizing the role of the prior . in practice , however , the field has been dominated by two stochastic processes  the gaussian process and the dirichlet process  and thus the flexibility promised by the nonparametric approach has arguably not yet been delivered . in the current paper",
    "we aim to provide a broader perspective on the kinds of stochastic processes that can provide a useful toolbox for bayesian nonparametric analysis . specifically , we focus on _ combinatorial stochastic processes _ as embodying mathematical structure that is useful for both model specification and inference .",
    "the phrase `` combinatorial stochastic process''comes from probability theory @xcite , where it refers to connections between stochastic processes and the mathematical field of combinatorics .",
    "indeed , the focus in this area of probability theory is on random versions of classical combinatorial objects such as partitions , trees and graphs  and on the role of combinatorial analysis in establishing properties of these processes .",
    "as we wish to argue , this connection is also fruitful in a statistical setting .",
    "roughly speaking , in statistics it is often natural to model observed data as arising from a combination of underlying factors . in the bayesian setting , such models are often embodied as latent variable models in which the latent variable has a compositional structure . making explicit use of ideas from combinatorics in latent variable modeling",
    "can not only suggest new modeling ideas but can also provide essential help with calculations of marginal and conditional probability distributions .",
    "the dirichlet process already serves as one interesting exhibit of the connections between bayesian nonparametrics and combinatorial stochastic processes . on the one hand ,",
    "the dirichlet process is classically defined in terms of a partition of a probability space @xcite , and there are many well - known connections between the dirichlet process and urn models ( @xcite ; @xcite ) . in the current paper , we will review and expand upon some of these connections , beginning our treatment ( nontraditionally ) with the notion of an _ exchangeable partition probability function _ ( eppf ) and , from there , related urn models , stick - breaking representations , subordinators and random measures .    on the other hand , the dirichlet process is limited in terms of the statistical notion of a `` combination of underlying factors '' that we referred to above .",
    "indeed , the dirichlet process is generally used in a statistical setting to express the idea that each data point is associated with one and only one underlying factor .",
    "in contrast to such _ clustering models _ , we wish to also _ featural models _ , where each data point is associated with a set of underlying features and it is the interaction among these features that gives rise to an observed data point . focusing on the case in which these features are binary , we develop some of the combinatorial stochastic process machinery needed to specify featural priors .",
    "specifically , we develop a counterpart to the eppf , which we refer to as the _ exchangeable feature probability function _ ( efpf ) , that characterizes the combinatorial structure of certain featural models .",
    "we again develop connections between this combinatorial function and suite of related stochastic processes , including urn models , stick - breaking representations , subordinators and random measures .",
    "as we will discuss , a particular underlying random measure in this case is the _ beta process _",
    ", originally studied by @xcite as a model of random hazard functions in survival analysis , but adapted by @xcite for applications in featural modeling .    for statistical applications",
    "it is not enough to develop expressive prior specifications , but it is also essential that inferential computations involving the posterior distribution are tractable .",
    "one of the reasons for the popularity of the dirichlet process is that the associated urn models and stick - breaking representations yield a variety of useful inference algorithms @xcite .",
    "as we will see , analogous algorithms are available for featural models .",
    "thus , as we discuss each of the various representations associated with both the dirichlet process and the beta process , we will also ( briefly ) discuss some of the consequences of each for posterior inference .    the remainder of the paper is organized as follows .",
    "we start by reviewing partitions and introducing feature allocations in section  [ sec : partition_feature ] in order to define distributions over these models ( section  [ sec : epf ] ) via the eppf in the partition case ( section  [ sec : epf_eppf ] ) and the efpf in the feature allocation case ( section  [ sec : epf_efpf ] ) . illustrating these exchangeable probability functions with examples",
    ", we will see that the well - known _ chinese restaurant process _",
    "( crp ) @xcite corresponds to a particular eppf choice ( example  [ ex : epf_crp ] ) and the _ indian buffet process _",
    "( ibp ) ( griffiths and ghahramani , @xcite ) corresponds to a particular choice of efpf ( example  [ ex : epf_ibp ] ) . from here , we progressively build up richer models by first reviewing stick lengths ( section  [ sec : stick ] ) , which we will see represent limiting frequencies of certain clusters or features , and then subordinators ( section  [ sec : sub ] ) , which further associate a random label with each cluster or feature .",
    "we illustrate these progressive augmentations for both the crp ( examples  [ ex : epf_crp ] ,  [ ex : cond_crp ] ,  [ ex : stick_crp ] , [ ex : sub_crp ] and  [ ex : sub_crp_sticks ] ) and ibp examples ( examples  [ ex : epf_ibp ] ,  [ ex : cond_ibp ] , [ ex : stick_ibp ] and  [ ex : sub_ibp ] ) .",
    "we augment the model once more to obtain a random measure on a general space of cluster or feature parameters in section  [ sec : crm ] , and discuss how marginalization of this random measure yields the crp in the case of the dirichlet process ( example  [ ex : crm_dp ] ) and the ibp in the case of the beta process ( example  [ ex : crm_bp ] ) .",
    "finally , in section  [ sec : conclusion ] , we mention some of the other combinatorial stochastic processes , beyond the dirichlet process and the beta process , that have begun to be studied in the bayesian nonparametrics literature , and we provide suggestions for further developments .",
    "while we have some intuitive ideas about what constitutes a cluster or feature model , we want to formalize these ideas before proceeding .",
    "we begin with the underlying combinatorial structure on the data indices .",
    "we think of @xmath0 : = \\{1,\\ldots , n\\}$ ] as representing the indices of the first @xmath1 data points .",
    "there are different groupings that we apply in the cluster case ( _ partitions _ ) and feature case ( _ feature allocations _ ) ; we describe these below .",
    "first , we wish to describe the space of _ partitions _ over the indices @xmath0 $ ] . in particular ,",
    "a partition @xmath2 of @xmath0 $ ] is defined to be a collection of mutually exclusive , exhaustive , nonempty subsets of @xmath0 $ ] called _ blocks _ ; that is , @xmath3 for some number of partition blocks @xmath4 . an example partition of @xmath5 $ ] is @xmath6 .",
    "similarly , a partition of @xmath7 is a collection of mutually exclusive , exhaustive , nonempty subsets of @xmath8 . in this case",
    ", the number of blocks may be infinite , and we have @xmath9 .",
    "an example partition of @xmath8 into two blocks is @xmath10 .",
    "we introduce a generalization of a partition called a _ feature allocation _ that relaxes both the mutually exclusive and exhaustive restrictions .",
    "in particular , a feature allocation @xmath11 of @xmath0 $ ] is defined to be a multiset of nonempty subsets of @xmath0 $ ] , again called _ blocks _ , such that each index @xmath12 can belong to any finite number of blocks .",
    "note that the constraint that no index should belong to infinitely many blocks coincides with our intuition for the meaning of these blocks as groups to which the index belongs .",
    "consider an example where the data points are images expressed as pixel arrays , and the latent features represent animals that may or may not appear in each picture .",
    "it is impossible to display an infinite number of animals in a picture with finitely many pixels .",
    "we write @xmath13 for some number of feature allocation blocks @xmath4",
    ". an example feature allocation of @xmath5 $ ] is @xmath14 . just as the blocks of a partition",
    "are sometimes called _ clusters _ , so are the blocks of a feature allocation sometimes called _",
    "we note that a partition is always a feature allocation , but the converse statement does not hold in general ; for instance , @xmath15 given above is not a partition .    in the remainder of this section",
    "we continue our development in terms of feature allocations since partitions are a special case of the former object .",
    "we note that we can extend the idea of random partitions @xcite to consider _ random feature allocations_. if @xmath16 is the space of all feature allocations of @xmath0 $ ] , then a random feature allocation @xmath17 of @xmath0 $ ] is a random element of this space .",
    "we next introduce a few useful assumptions on our random feature allocation . just as exchangeability of observations",
    "is often a central assumption in statistical modeling , so will we make use of _ exchangeable feature allocations_. to rigorously define such feature allocations , we introduce the following notation .",
    "let @xmath18 be a finite permutation .",
    "that is , for some finite value @xmath19 , we have @xmath20 for all @xmath21 . further , for any block @xmath22 , denote the permutation applied to the block as follows : @xmath23 . for any feature allocation @xmath24 , denote the permutation applied to the feature allocation as follows : @xmath25 .",
    "finally , let @xmath17 be a random feature allocation of @xmath0 $ ] .",
    "then we say that @xmath17 is exchangeable if @xmath26 for every finite permutation @xmath27 .    our second assumption in",
    "what follows will be that we are dealing with a _ consistent _ feature allocation .",
    "we often implicitly imagine the indices arriving one at a time : first 1 , then 2 , up to @xmath1 or beyond",
    ". we will find it useful , similarly , in defining random feature allocations to suppose that the randomness at stage @xmath12 somehow agrees with the randomness at stage @xmath28 .",
    "more formally , we say that a feature allocation @xmath29 of @xmath30 $ ] is a _ restriction _ of a feature allocation @xmath11 of @xmath0 $ ] for @xmath31 if @xmath32\\dvtx { { a}}\\in { f}_{n}\\bigr\\}.\\ ] ] let @xmath33 be the set of all feature allocations of @xmath0 $ ] whose restriction to @xmath30 $ ] is @xmath29 .",
    "then we say that the sequence of random feature allocations @xmath34 is _ consistent _ if for all @xmath35 and @xmath1 such that @xmath31 , we have that @xmath36    with this consistency condition in hand , we can define a random feature allocation @xmath37 of @xmath8 .",
    "in particular , such a feature allocation is characterized by the sequence of consistent finite restrictions @xmath17 to @xmath0 $ ] : @xmath38\\dvtx a \\in{f}_{\\infty}\\}$ ]",
    ". then @xmath37 is equivalent to a consistent sequence of finite feature allocations and may be thought of as a random element of the space of such sequences : @xmath39 .",
    "we let @xmath40 denote the space of consistent feature allocations , of which each random feature allocation is a random element , and we see that the sigma - algebra associated with this space is generated by the finite - dimensional sigma - algebras of the restricted random feature allocations @xmath41 .",
    "we say that @xmath37 is exchangeable if @xmath42 for every finite permutation @xmath27 .",
    "that is , when the permutation @xmath27 changes no indices above @xmath1 ,",
    "we require @xmath26 , where @xmath17 is the restriction of @xmath37 to @xmath0 $ ] . a characterization of distributions for @xmath37 is provided by @xcite , where a similar treatment of the introductory ideas of this section also appears .        in what follows ,",
    "we consider particular useful ways of representing distributions for exchangeable , consistent random feature allocations with emphasis on partitions as a special case .",
    "once we know that we can construct ( exchangeable and consistent ) random partitions and feature allocations , it remains to find useful representations of distributions over these objects .",
    "consider first an exchangeable , consistent , random partition @xmath43 . by the exchangeability assumption",
    ", the distribution of the partition should depend only on the ( unordered ) sizes of the blocks .",
    "therefore , there exists a function @xmath44 that is symmetric in its arguments such that , for any specific partition assignment @xmath45 , we have @xmath46 the function @xmath44 is called the _ exchangeable partition probability function _ ( eppf ) @xcite .",
    "[ ex : epf_crp ] the chinese restaurant process ( crp ) ( blackwell and macqueen , @xcite ) is an iterative description of a partition via the conditional distributions of the partition blocks to which increasing data indices belong .",
    "the chinese restaurant metaphor forms an equivalence between customers entering a chinese restaurant and data indices ; customers who share a table at the restaurant represent indices belonging to the same partition block .    to generate the label for the first index , the first customer enters",
    "the restaurant and sits down at some table , necessarily unoccupied since no one else is in the restaurant .",
    "a `` dish '' is set out at the new table ; call the dish `` 1 '' since it is the first dish .",
    "the customer is assigned the label of the dish at her table : @xmath47 .",
    "recursively , for a restaurant with _ concentration parameter _",
    "@xmath48 , the @xmath12th customer sits at an occupied table with probability in proportion to the number of people at the table and at a new table with probability proportional to @xmath49 . in the former case",
    ", @xmath50 takes the value of the existing dish at the table , and , in the latter case , the next available dish @xmath51 ( equal to the number of existing tables plus one ) appears at the new table , and @xmath52 . by summing over all possibilities",
    "when the @xmath12th customer arrives , one obtains the normalizing constant for the distribution across potential occupied tables : @xmath53 .",
    "an example of the distribution over tables for the @xmath12th customer is shown in figure  [ fig : crp ] . to summarize ,",
    "if we let @xmath54 , then the distribution of table assignments for the @xmath12th customer is @xmath55 \\\\[-8pt ] \\nonumber   & & \\quad= ( n-1+{\\theta})^{-1 } \\cases{\\#\\{m\\dvtx m < n , { z}_{m } = j\\},\\vspace*{2pt}\\cr   \\quad\\hspace*{11pt } \\mbox{for } j \\le k_{n-1 } , \\vspace*{2pt}\\cr \\theta,\\quad\\mbox{for } k = k_{n-1}+1.}\\end{aligned}\\ ] ]    we note that an equivalent generative description follows a plya urn style in specifying that each incoming customer sits next to an existing customer with probability proportional to 1 and forms a new table with probability proportional to @xmath49 @xcite .",
    "next , we find the probability of the partition induced by considering the collection of indices sitting at each table as a block in the partition .",
    "suppose that @xmath56 individuals sit at table @xmath51 so that the set of cardinalities of nonzero table occupancies is @xmath57 with @xmath58 .",
    "that is , we are considering the case when @xmath1 customers have entered the restaurant and sat at @xmath4 different tables in the specified configuration .",
    "we can see from equation ( [ eq : crp ] ) that when the @xmath12th customer enters ( @xmath59 ) , we obtain a factor of @xmath60 in the denominator . using the following notation for the",
    "rising and falling factorial @xmath61 we find a factor of @xmath62 must occur in the denominator of the probability of the partition of  @xmath0 $ ] .",
    "similarly , each time a customer forms a new table except for the first table , we obtain a factor of @xmath49 in the numerator . combining these factors",
    ", we find a factor of @xmath63 in the numerator . finally , each time a customer sits at an existing table with @xmath12 occupants , we obtain a factor of @xmath12 in the numerator .",
    "thus , for each table @xmath51 , we have a factor of @xmath64 once all customers have entered the restaurant.=1    having collected all terms in the process , we see that the probability of the resulting configuration is @xmath65 we first note that equation ( [ eq : crp_eppf ] ) depends only on the block sizes and not on the order of arrival of the customers or dishes at the tables .",
    "we conclude that the partition generated according to the crp scheme is exchangeable .",
    "moreover , as the partition @xmath66 is the restriction of @xmath67 to @xmath30 $ ] for any @xmath68 by construction , we have that equation ( [ eq : crp_eppf ] ) satisfies the consistency condition .",
    "it follows that equation ( [ eq : crp_eppf ] ) is , in fact , an eppf .      just as we considered an exchangeable , consistent , random partition above",
    ", so we now turn to an exchangeable , consistent , random feature allocation @xmath34 .",
    "let @xmath69 be any particular feature allocation .",
    "in calculating @xmath70 , we start by demonstrating in the next example that this probability in some sense undercounts features when they contain exactly the same indices : for example , @xmath71 for some @xmath72 .",
    "for instance , consider the following example .",
    "[ ex : two_bern ] let @xmath73 represent the frequencies of features @xmath74 and @xmath75 . draw @xmath76 and @xmath77 , independently . construct the random feature allocation by collecting those indices with successful draws : @xmath78 then the probability of the feature allocation @xmath79 is @xmath80 but the probability of the feature allocation @xmath81 is @xmath82 the difference is that in the latter case the features can be distinguished , and so we must account for the two possible pairings of features to frequencies @xmath83 .",
    "now , instead , let @xmath84 be @xmath17 with a uniform random ordering on the features .",
    "there is just a single possible ordering of @xmath85 , so the probability of @xmath86 is again @xmath87 however , there are two orderings of @xmath88 , so the probability of @xmath89 is @xmath80 and the same holds for the other ordering .    for reasons suggested by the previous example , we will find it useful to work with the random feature allocation after uniform random ordering , @xmath84 .",
    "one way to achieve such an ordering and maintain consistency across different @xmath1 is to associate some independent , continuous random variable with each feature ; for example , assign a uniform random variable on @xmath90 $ ] to each feature and order the features according to the order of the assigned random variables .",
    "when we view feature allocations constructed as marginals of a _ subordinator _ in section  [ sec : sub ] , we will see that this construction is natural",
    ".    in general , given a probability of a random feature allocation , @xmath91 , we can find the probability of a _ random ordered feature allocation _ , @xmath92 as follows .",
    "let @xmath93 be the number of unique elements of @xmath17 , and let @xmath94 be the multiplicities of these unique elements in decreasing size .",
    "then @xmath95 where @xmath96    we will see in section  [ sec : sub ] that augmentation of an exchangeable partition with a random ordering is also natural .",
    "however , the probability of an ordered random partition is not substantively different from the probability of an unordered version since the factor contributed by ordering a partition is always @xmath97 , where @xmath4 here is the number of partition blocks .    with this framework in place",
    ", we can see that some ordered feature allocations have a probability function @xmath98 nearly as in equation ( [ eq : eppf ] ) , that is , moreover , symmetric in its block - size arguments .",
    "consider again the previous example .",
    "consider any @xmath17 with block sizes @xmath99 and @xmath100 constructed as in example  [ ex : two_bern ] .",
    "then @xmath101 where @xmath98 is some function of the number of indices @xmath1 and the block sizes @xmath102 that we note is symmetric in all arguments after the first .",
    "in particular , we see that the order of @xmath99 and @xmath100 was immaterial .",
    "we note that in the partition case , @xmath103 , so @xmath1 is implicitly an argument to the eppf . in the feature case ,",
    "this summation condition no longer holds , so we make the argument @xmath1 explicit in equation ( [ eq : efpf_two_bern ] ) .    however , it is not necessarily the case that such a function , much less a symmetric one , exists for exchangeable feature models  in contrast to the case of exchangeable partitions and the eppf .",
    "we here describe an exchangeable , consistent random feature allocation whose ( ordered ) distribution does not depend only on the number of indices @xmath1 and the sizes of the blocks of the feature allocation .",
    "let @xmath104 be fixed frequencies that sum to one .",
    "let @xmath105 represent the collection of features to which index @xmath12 belongs .",
    "for @xmath106 , choose @xmath105 independently and identically according to @xmath107 we form a feature allocation from these labels as follows .",
    "for each label ( @xmath74 or @xmath75 ) , collect those indices @xmath12 with the given label appearing in @xmath105 to form a feature .",
    "now consider two possible outcome feature allocations : @xmath108 and @xmath109 .",
    "the likelihood of any random ordering @xmath110 of @xmath111 under this model is @xmath112 the likelihood of any ordering @xmath113 of @xmath114 is @xmath115 it follows from these two likelihoods that we can choose values of @xmath116 such that @xmath117 . but @xmath110 and @xmath113 have the same block counts and @xmath1 value ( @xmath118 ) .",
    "so there can be no such symmetric function @xmath44 , as in equation ( [ eq : efpf_two_bern ] ) , for this model .",
    "when a function @xmath98 exists in the form @xmath119 for some random ordered feature allocation @xmath120 such that @xmath98 is symmetric in all arguments after the first , we call it the _ exchangeable feature probability function _ ( efpf )",
    ". note that the eppf is not a special case of the efpf .",
    "the eppf assigns zero probability to any multiset in which an index occurs in more than one element of the multiset ; only the sizes of the multiset blocks are relevant in the efpf case .",
    "we next consider a more complex example of an efpf .",
    "[ ex : epf_ibp ] the indian buffet process ( ibp ) ( griffiths and ghahramani,@xcite ) is a generative model for a random feature allocation that is specified recursively like the chinese restaurant process . also like the crp",
    ", this culinary metaphor forms an equivalence between customers and the indices @xmath12 that will be partitioned : @xmath121 .",
    "here , `` dishes '' again correspond to feature labels just as they corresponded to partition labels for the crp .",
    "but in the ibp case , a customer can sample multiple dishes .    in particular ,",
    "we start with a single customer , who enters the buffet and chooses @xmath122 dishes . here",
    ", @xmath123 is called the _ mass parameter _ , and we will also see the _ concentration parameter _",
    "@xmath124 below .",
    "none of the dishes have been sampled by any other customers since no other customers have yet entered the restaurant .",
    "we label the dishes @xmath125 if @xmath126 .",
    "recursively , the @xmath12th customer chooses which dishes to sample in two parts .",
    "first , for each dish @xmath51 that has previously been sampled by any customer in @xmath127 , customer @xmath12 samples dish @xmath51 with probability @xmath128 for @xmath129 equal to the number of customers indexed @xmath130 who have tried dish @xmath51 . as each dish represents a feature , and sampling a dish represents that the customer index @xmath12 belongs to that feature , @xmath129 is the size of the block of the feature labeled @xmath51 in the feature allocation of @xmath131 $ ] .",
    "next , customer @xmath12 chooses @xmath132 new dishes to try . if @xmath133 , then the dishes receive unique labels @xmath134 . here",
    ", @xmath135 represents the number of sampled dishes after @xmath12 customers : @xmath136 .",
    "an example of the first few steps in the indian buffet process is shown in figure  [ fig : ibp ]",
    ".     indicates customer @xmath12 has sampled dish @xmath51 , and a white box indicates the customer has not sampled the dish . in the example",
    ", the second customer has sampled exactly those dishes indexed by 2 , 4 and 5 : @xmath137 . ]    with this generative model in hand , we can find the probability of a particular feature allocation .",
    "we discover its form by enumeration as for the crp eppf in example  [ ex : epf_crp ] . at each round @xmath12",
    ", we have a poisson number of new features , @xmath138 , represented . the probability factor associated with these choices",
    "is a product of poisson densities : @xmath139 let @xmath140 be the round on which the @xmath51th dish , in order of appearance , is first chosen",
    ". then the denominators for future dish choice probabilities are the factors in the product @xmath141 .",
    "the numerators for the times when the dish is chosen are the factors in the product @xmath142 .",
    "the numerators for the times when the dish is not chosen yield @xmath143 .",
    "let @xmath144 represent the collection of indices in the feature with label @xmath51 after @xmath12 customers have entered the restaurant",
    ". then @xmath145 .",
    "finally , let @xmath146 be the multiplicities of unique feature blocks formed by this model .",
    "we note that there are @xmath147 \\bigg/ \\biggl [ \\prod _ { h=1}^{h } { \\tilde{k}}_{h } !",
    "\\biggr]\\ ] ] rearrangements of the features generated by this process that all yield the same feature allocation .",
    "since they all have the same generating probability , we simply multiply by this factor to find the feature allocation probability . multiplying all factors together and taking @xmath148 yields @xmath149 \\\\ & & \\qquad { } \\cdot \\biggl [ \\prod_{k=1}^{k_{n } } \\frac{\\gamma({{\\theta}}+ m_{k})}{\\gamma({{\\theta}}+ n ) } \\gamma(n_{n , k } ) \\frac{\\gamma({{\\theta}}+ n - n_{n , k})}{\\gamma({{\\theta}}+m_{k}-1 ) } \\biggr ] \\\\ & & \\quad= \\biggl ( \\prod_{h=1}^{h } { \\tilde{k}}_{h } !",
    "\\biggr)^{-1 } \\biggl [ \\prod _ { n=1}^{n } ( { { \\theta}}{\\gamma})^{k^{+}_{n } } \\exp \\biggl ( -\\frac { { { \\theta}}{\\gamma}}{{{\\theta}}+ n -1 } \\biggr ) \\biggr ] \\\\ & & \\qquad{}\\cdot \\biggl [ \\frac{\\prod_{k=1}^{k_{n } } ( \\theta+ m_{k } - 1)}{\\prod_{n=1}^{n } ( \\theta+ n - 1)^{k_{n}^{+ } } } \\biggr ] \\\\ & & \\qquad { } \\cdot \\biggl [ \\prod_{k=1}^{k_{n } } \\frac{\\gamma(n_{n , k } ) \\gamma({{\\theta}}+ n - n_{n , k})}{\\gamma({{\\theta}}+ n ) } \\biggr ] \\\\ & & \\quad= \\biggl ( \\prod_{h=1}^{h } { \\tilde{k}}_{h } ! \\biggr)^{-1 } ( { { \\theta}}{\\gamma})^{k_{n } } \\\\ & & \\qquad{}\\cdot\\exp \\biggl ( -{{\\theta}}{\\gamma}\\sum_{n=1}^{n } ( { { \\theta}}+ n - 1)^{-1 } \\biggr)\\\\ & & \\qquad{}\\cdot \\prod_{k=1}^{k_{n } } \\frac{\\gamma(n_{n , k } ) \\gamma ( n - n_{n , k}+{{\\theta}})}{\\gamma(n+{{\\theta}})}.\\end{aligned}\\ ] ]    it follows from equation ( [ eq : order_mult ] ) that the probability of a uniform random ordering of the feature allocation is @xmath150    the distribution of @xmath84 has no dependence on the ordering of the indices in @xmath0 $ ] . hence , the distribution of @xmath17 depends only on the same quantities  the number of indices and the feature block sizes  and the feature multiplicities .",
    "so we see that the ibp construction yields an exchangeable random feature allocation .",
    "consistency follows from the recursive construction and exchangeability",
    ". therefore , equation ( [ eq : ibp_efpf ] ) is seen to be in efpf form [ cf .",
    "equation  ( [ eq : efpf ] ) ] .",
    "above , we have seen two examples of how specifying a conditional distribution for the block membership of index @xmath12 given the block membership of indices in @xmath151 $ ] yields an exchangeable probability function , for example , the eppf in the crp case ( example [ ex : epf_crp ] ) and the efpf in the ibp case ( example  [ ex : epf_ibp ] ) .",
    "this conditional distribution is often called a _ prediction rule _ , and study of the prediction rule in the clustering case may be referred to as _ species sampling _ ( @xcite ; @xcite ; @xcite ) .",
    "we will see next that the prediction rule can conversely be recovered from the exchangeable probability function specification and , therefore , the two are equivalent .      in examples  [ ex :",
    "epf_crp ] and  [ ex : epf_ibp ] above , we formed partitions and feature allocations in the following way . for partitions , we assigned labels @xmath50 to each index @xmath12 .",
    "then we generated a partition of @xmath0 $ ] from the sequence @xmath152 by saying that indices @xmath153 and @xmath12 are in the same partition block ( @xmath154 ) if and only if @xmath155 . the resulting partition is called the _ induced partition _ given the labels @xmath152 .",
    "similarly , given labels @xmath156 , we can form an induced partition of @xmath8 .",
    "it is easy to check that , given a sequence @xmath157 , the induced partitions of the subsequences @xmath152 will be consistent .    in the feature case , we first assigned label collections @xmath105 to each index @xmath12",
    ". @xmath105 is interpreted as a set containing the labels of the features to which @xmath12 belongs .",
    "it must have finite cardinality by our definition of a feature allocation . in this case",
    ", we generate a feature allocation on @xmath0 $ ] from the sequence @xmath158 by first letting @xmath159 be the set of unique values in @xmath160 . then the features are the collections of indices with shared labels : @xmath161 . the resulting feature allocation @xmath11 is called the _ induced feature allocation _ given the labels @xmath158 .",
    "similarly , given label collections @xmath162 , where each @xmath105 has finite cardinality , we can form an induced feature allocation of @xmath8 .",
    "as in the partition case , given a sequence @xmath162 , we can see that the induced feature allocations of the subsequences @xmath158 will be consistent .    in reducing to a partition or feature allocation from a set of labels , we shed the information concerning the labels for each partition block or feature .",
    "conversely , we introduce _ order - of - appearance _ labeling schemes to give partition blocks or features labels when we have , respectively , a partition or feature allocation .    in the partition case",
    ", the order - of - appearance labeling scheme assigns the label 1 to the partition block containing index 1 .",
    "recursively , suppose we have seen @xmath12 indices in @xmath51 different blocks with labels @xmath163 . and suppose the @xmath28st index does not belong to an existing block .",
    "then we assign its block the label @xmath164 .",
    "in the feature allocation case , we note that index 1 belongs to @xmath165 features .",
    "if @xmath166 , there are no features to label yet . if @xmath167 , we assign these @xmath165 features labels in @xmath168 . unless otherwise specified , we suppose that the labels are chosen uniformly at random",
    "let @xmath169 .",
    "recursively , suppose we have seen @xmath12 indices and @xmath135 different features with labels @xmath170 .",
    "suppose the @xmath28st index belongs to @xmath171 features that have not yet been labeled .",
    "let @xmath172 .",
    "if @xmath173 , there are no new features to label .",
    "if @xmath174 , assign these @xmath171 features labels in @xmath175 , for example , uniformly at random .",
    "we can use these labeling schemes to find the prediction rule , which makes use of partition block and feature labels , from the eppf or efpf as appropriate .",
    "first , consider a partition with eppf @xmath44 .",
    "then , given labels @xmath152 with @xmath176 , we wish to find the distribution of the label @xmath177 . using an order - of - appearance labeling , we know that either @xmath178 or @xmath179 . let @xmath180 be the partition induced by @xmath152 .",
    "let @xmath181 .",
    "let @xmath182 be the indicator of event @xmath74 ; that is , @xmath182 equals 1 if @xmath74 holds and 0 otherwise .",
    "let @xmath183 for @xmath184 , and set @xmath185 for completeness .",
    "@xmath186 is the number of partition blocks in the partition of @xmath187 $ ] . then the conditional distribution satisfies @xmath188 but the probability of a certain labeling is just the probability of the underlying partition in this construction , so @xmath189    [ ex : cond_crp ] we continue our chinese restaurant process example by deriving the chinese restaurant table assignment scheme from the eppf in equation ( [ eq : crp_eppf ] ) . substituting in the eppf for the crp",
    ", we find @xmath190 just as in equation ( [ eq : crp ] ) .    to find the feature allocation prediction rule",
    ", we now imagine a feature allocation with efpf @xmath98 . here",
    "we must be slightly more careful about counting due to feature multiplicities .",
    "suppose that after @xmath1 indices have been seen , we have label collections @xmath158 , containing a total of @xmath191 features , labeled @xmath192 .",
    "we wish to find the distribution of @xmath193 .",
    "suppose @xmath194 belongs to @xmath195 features that do not contain any index in @xmath0 $ ] .",
    "using an order - of - appearance labeling , we know that , if @xmath196 , the @xmath195 new features have labels @xmath197 . let @xmath198 be the feature allocation induced by @xmath158 .",
    "let @xmath199 be the size of the @xmath51th feature .",
    "so @xmath200 , where we let @xmath201 for all of the features that are first exhibited by index @xmath194 : @xmath202 .",
    "further , let the number of features , including new ones , be written @xmath203 .",
    "then the conditional distribution satisfies @xmath204 as we assume that the labels @xmath205 are consistentacross  @xmath1 , the probability of a certain labeling is just the probability of the underlying ordered feature allocation times a combinatorial term .",
    "the combinatorial term accounts first for the uniform ordering of the new features among themselves for labeling and then for the uniform ordering of the new features among the old features in the overall uniform random ordering : @xmath206 \\nonumber\\\\ & & \\qquad { } \\cdot\\frac { { { p}}(n , n_{n+1,1},\\ldots , n_{n+1,k_{n+1 } } ) } { { { p}}(n , n_{n,1},\\ldots , n_{n , k_{n } } ) } \\nonumber\\\\   & & \\quad= \\frac{1}{k_{n+1}^{+ } ! }",
    "\\cdot \\frac{k_{n+1}!}{k_{n}!}\\nonumber\\\\ & & \\qquad { } \\cdot\\frac { { { p}}(n , n_{n+1,1},\\ldots , n_{n+1,k_{n+1 } } ) } { { { p}}(n , n_{n,1},\\ldots , n_{n , k_{n } } ) } .\\end{aligned}\\ ] ]    [ ex : cond_ibp ] just as we derived the chinese restaurant process prediction rule [ equation ( [ eq : pred_crp_derived ] ) ] from its eppf [ equation ( [ eq : crp_eppf ] ) ] in example  [ ex : cond_crp ] , so can we derive the indian buffet process prediction rule from its efpf [ equation ( [ eq : ibp_efpf ] ) ] by using equation ( [ eq : pred_from_efpf ] ) . substituting the ibp efpf into equation ( [ eq : pred_from_efpf ] )",
    ", we find @xmath207\\\\ & & \\qquad\\bigg/\\biggl\\{\\biggl(\\frac{1}{k_{n}!}\\biggr ) ( { { \\theta}}{\\gamma})^{k_{n } } \\\\ & & \\hspace*{30pt}\\quad{}\\cdot\\exp \\biggl ( -{{\\theta}}{\\gamma}\\sum_{n=1}^{n } ( { { \\theta}}+ n - 1)^{-1 }   \\biggr)\\\\ & & \\hspace*{32pt}\\quad{}\\cdot \\biggl[\\prod_{k=1}^{k_{n } } { \\gamma ( n_{n , k } ) \\gamma(n - n_{n , k}+{{\\theta}})}\\\\ & & \\hspace*{112pt}\\qquad{}/{\\bigl(\\gamma(n+{{\\theta}})\\bigr)}\\biggr]\\biggr\\ } \\\\ & & \\quad= \\biggl [ \\frac{1}{k_{n+1}^{+ } ! } \\exp \\biggl(- \\frac{{{\\theta}}{\\gamma } } { \\theta+ ( n+1 ) - 1 } \\biggr ) \\\\ & & \\hspace*{57pt}{}\\cdot\\biggl(\\frac{{{\\theta}}{\\gamma } } { \\theta+ ( n+1 ) - 1 } \\biggr)^{k_{n+1}^{+ } } \\biggr ] \\\\ & & \\qquad { } \\cdot\\bigl({{\\theta}}+ ( n+1 ) - 1\\bigr)^{k_{n+1}^{+}}\\\\ & & \\qquad { } \\cdot \\biggl [ \\prod _ { k = k_{n}+1}^{k_{n+1 } } \\bigl({{\\theta}}+ ( n+1 ) - 1 \\bigr)^{-1 } \\biggr ] \\\\ & & \\qquad { } \\cdot\\prod_{k=1}^{k_{n } } \\frac{n_{k}^{{\\mathbh{1}}\\{k \\in z\\ } } ( n - n_{n , k } + { { \\theta}})^{{\\mathbh{1}}\\{k \\notin z\\ } } } { n+{{\\theta } } } \\\\ & & \\quad= { \\operatorname{pois}}\\biggl ( k_{n+1}^{+ } \\big| \\frac{{{\\theta}}{\\gamma } } { \\theta+ ( n+1 ) - 1 } \\biggr)\\\\ & & \\qquad { } \\cdot\\prod_{k=1}^{k_{n } } { \\operatorname{bern}}\\biggl ( { \\mathbh{1}}\\{k \\in z\\}\\big | \\frac { n_{n , k}}{n + { { \\theta } } } \\biggr).\\end{aligned}\\ ] ] the final line is exactly the poisson distribution for the number of new features times the bernoulli distributions for the draws of existing features , as described in example  [ ex : epf_ibp ] .",
    "the prediction rule formulation of the eppf or efpf is particularly useful in providing a means of inferring partitions and feature allocations from a data set . in particular",
    ", we assume that we have data points @xmath208 generated in the following manner . in the partition case",
    ", we generate an exchangeable , consistent , random partition @xmath67 according to the distribution specified by some eppf @xmath44 .",
    "next , we assign each partition block a random parameter that characterizes that block . to be precise , for the @xmath51th partition block to appear according to an order - of - appearance labeling scheme , give this block a new _ random _",
    "label @xmath209 , for some continuous distribution @xmath210 . for each @xmath12 , let @xmath211 where @xmath51 is the order - of - appearance label of index @xmath12 .",
    "finally , let @xmath212 for some distribution @xmath213 with parameter @xmath50 .",
    "the choices of both @xmath210 and @xmath213 are specific to the problem domain .    without attempting to survey the vast literature on clustering ,",
    "we describe a stylized example to provide intuition for the preceding generative model . in this example , let @xmath12 index an animal observed in the wild ; @xmath155 indicates that animals @xmath12 and @xmath153 belong to the same ( latent , unobserved ) species ; @xmath214 is a vector describing the ( latent , unobserved ) height and weight for that species ; and @xmath215 is the observed height and weight of the @xmath12th animal .",
    "@xmath215 need not even be directly observed , but equation  ( [ eq : likelihood ] ) together with an eppf might be part of a larger generative model . in a generalization of the previous stylized example , @xmath50 indicates the dominant species in the @xmath12th geographical region ; @xmath211 indicates some overall species height and weight parameters ( for the @xmath51th species ) ; @xmath215 indicates the height and weight parameters for species @xmath51 in the @xmath12th region .",
    "that is , the height and weight for the species may vary by region .",
    "we measure and observe the height and weight @xmath216 of some @xmath217 animals in the @xmath12th region , believed to be i.i.d",
    ". draws from a distribution depending on @xmath215 .    note that the sequence @xmath152 is sufficient to describe the partition @xmath218",
    "since @xmath218 is the collection of blocks of @xmath0 $ ] with the same label values @xmath50 .",
    "the continuity of @xmath210 is necessary to guarantee the a.s .",
    "uniqueness of the block values .",
    "so , if we can describe the posterior distribution of @xmath152 , we can in principle describe the posterior distribution of @xmath218 .",
    "the posterior distribution of @xmath152 conditional on @xmath219 can not typically be solved for in closed form , so we turn to a method that approximates this posterior .",
    "we will see that prediction rules facilitate the design of a markov chain monte carlo ( mcmc ) sampler , in which we approximate the desired posterior distribution by a markov chain of random samples proven to have the true posterior as its equilibrium distribution .    in the gibbs sampler formulation of mcmc ( geman and geman , @xcite ) , we sample each parameter in turn and conditional on all other parameters in the model . in our case , we will sequentially sample each element of @xmath152 . the key observation here",
    "is that @xmath152 is an exchangeable sequence .",
    "this observation follows by noting that the partition is exchangeable by assumption , and the sequence @xmath220 is exchangeable since it is i.i.d . ; @xmath221 is an exchangeable sequence since it is a function of @xmath43 and @xmath220 .",
    "therefore , the distribution of @xmath50 , given the remaining elements @xmath222 , is the same as if we thought of @xmath50 as the final , @xmath1th element in a sequence with @xmath223 preceding values given by @xmath224 . and the distribution of @xmath225 given @xmath226 is provided by the prediction rule .",
    "the full details of the gibbs sampler for the crp in examples  [ ex : epf_crp ] and  [ ex : cond_crp ] were introduced by @xcite , @xcite , @xcite and are covered in fuller generality by @xcite .",
    "it is worth noting that the sequence of order - of - appearance labels is not exchangeable ; for instance , the first label is always 1 .",
    "however , the prediction rule for @xmath225 given @xmath227 breaks into two parts : ( 1 ) the probability of @xmath225 taking either a value in @xmath228 or a new value and ( 2 ) the distribution of @xmath225 when it takes a new value . when programming such a sampler , it is often useful to simply encode the sets of unique values , which may be done by retaining any set of labels that induce the correct partition ( e.g. , integer labels ) and separately retaining the set of unique parameter values .",
    "indeed , updating the parameter values and partition block assignments separately can lead to improved mixing of the sampler @xcite .",
    "similarly , in the feature case , we imagine the following generative model for our data .",
    "first , let @xmath17 be a random feature allocation generated according to the efpf @xmath98 .",
    "for the @xmath51th feature block in an order - of - appearance labeling scheme , assign a random label @xmath229 to this block for some continuous distribution  @xmath210 . for each @xmath12 , let @xmath230 , where @xmath231 is here the set of order - of - appearance labels of the features to which @xmath12 belongs . finally , as above , @xmath232 where the likelihood @xmath213 and parameter distribution @xmath210 are again application - specific and where now @xmath213 depends on the variable - size collection of parameters in  @xmath105 .",
    "@xcite provide a review of likelihoods used in practice for feature models . to motivate some of these modeling choices ,",
    "let us consider some stylized examples that provide helpful intuition .",
    "for example , let @xmath12 index customers at a book - selling website ; @xmath233 describes a book topic such as economics , modern art or science fiction .",
    "if @xmath233 describes science fiction books , @xmath234 indicates that the @xmath12th customer likes to buy science fiction books .",
    "but @xmath105 might have cardinality greater than one ( the customer is interested in multiple book topics ) or cardinality zero ( the customer never buys books ) .",
    "finally , @xmath235 is a set of book sales for customer @xmath12 on the book - selling site .    as a second example , let @xmath12 index pictures in a database ; @xmath233 describes a pictorial element such as a train or grass or a cow ; @xmath234 indicates that picture @xmath12 contains , for example , a train ; finally , the observed array of pixels @xmath235 that form the picture is generated to contain the pictorial elements in @xmath105 . as in the clustering case , @xmath235 might not even be directly observed but might serve as a random effect in a deeper hierarchical model .",
    "we observe that although the order - of - appearance label sets are not exchangeable , the sequence @xmath236 is .",
    "this fact allows the formulation of a gibbs sampler via the observation that the distribution of @xmath105 , given the remaining elements @xmath237 , is the same as if we thought of @xmath105 as the final , @xmath1th element in a sequence with @xmath223 preceding values given by @xmath238 .",
    "the full details of such a sampler for the case of the ibp ( examples  [ ex : epf_ibp ] and  [ ex : cond_ibp ] ) are given by @xcite .",
    "as in the partition case , in practice , when programming the sampler , it is useful to separate the feature allocation encoding from the feature parameter values .",
    "@xcite describe how _",
    "left order form _",
    "matrices give a convenient representation of the feature allocation in this context .",
    "not every symmetric function defined for an arbitrary number of arguments with values in the unit interval is an eppf @xcite , and not every symmetric function with an additional positive integer argument is an efpf .",
    "for instance , the consistency property in equation ( [ eq : strong_consistency ] ) implies certain additivity requirements for the function @xmath44 .",
    "consider the function  @xmath239 defined with @xmath240 from the information in equation ( [ eq : fake_eppf ] ) , @xmath239 may be further defined so as to be symmetric in its arguments for any number of arguments , but since it does not satisfy @xmath241 , it can not be an eppf .",
    "consider the function @xmath239 defined with @xmath242 \\\\[-8pt ] \\nonumber \\qquad p(n=1,1,1 ) & = & 0.9 , \\ldots\\end{aligned}\\ ] ] from the information in equation ( [ eq : fake_efpf ] ) , @xmath239 may be further defined so as to be symmetric in its arguments for any number of arguments after the initial @xmath1 argument , but since @xmath243 , it can not be an efpf .",
    "it therefore requires some care to define a suitable distribution over consistent , exchangeable random feature allocations or partitions using the exchangeable probability function framework .",
    "since we are working with exchangeable sequences of random variables , it is natural to turn to de finetti s theorem ( @xcite ; @xcite ) for clues as to how to proceed .",
    "de finetti s theorem tells us that any exchangeable sequence of random variables can be expressed as an independent and identically distributed sequence when conditioned on an underlying random _ mixing measure_. while this theorem may seem difficult to apply directly to , for example , exchangeable partitions , it may be applied more naturally to an exchangeable sequence of numbers derived from a sequence of partitions .",
    "the argument below is due to @xcite .",
    "suppose that @xmath43 is an exchangeable , consistent sequence of random partitions .",
    "consider the @xmath51th partition block to appear according to an order - of - appearance labeling scheme , and give this block a new _ random _ label , @xmath244)$ ] , such that each random label is drawn independently from the rest .",
    "this construction is the same as the one used for parameter generation in section  [ sec : epf_infer ] , and @xmath43 is exchangeable by the same arguments used there .",
    "let @xmath50 equal @xmath245 exactly when @xmath12 belongs to the partition with this label .",
    "if we apply de finetti s theorem to the sequence @xmath221 and note that @xmath221 has at most countably many different values , we see that there exists some random sequence @xmath246 such that @xmath247 $ ] for all @xmath51 and , conditioned on the frequencies @xmath246 , @xmath221 has the same distribution as i.i.d",
    ". draws from @xmath246 . in this description , we have brushed over technicalities associated with partition blocks that contain only one index even as @xmath248 ( which may imply @xmath249 ) .",
    "but if we assume that every partition block eventually contains at least two indices , we can achieve an exchangeable partition of @xmath0 $ ] as follows .",
    "let @xmath246 represent a sequence of values in @xmath250 $ ] such that @xmath251 .",
    "draw @xmath252 .",
    "let @xmath218 be the induced partition given @xmath152 .",
    "exchangeability follows from the i.i.d",
    ". draws , and consistency follows from the induced partition construction .",
    "when the frequencies @xmath246 are thought of as subintervals of the unit interval , that is , a partition of the unit interval , they are collectively called _",
    "kingman s paintbox _ @xcite . as another naming convention",
    ", we may think of the unit interval as a _",
    "we partition the unit interval by breaking it into various _ stick lengths _ , which represent the frequencies of each partition block .",
    "a similar construction can be seen to yield exchangeable , consistent random feature allocations . in this case , let @xmath253 represent a sequence of values in @xmath250 $ ] such that @xmath254 .",
    "we generate feature collections independently for each index as follows .",
    "start with @xmath255 . for each feature @xmath51 , add @xmath51 to the set @xmath105 , independently from all other features , with probability @xmath256 .",
    "let @xmath17 be the induced feature allocation given @xmath158 .",
    "exchangeability of @xmath17 follows from the i.i.d .",
    "draws of @xmath105 , and consistency follows from the induced feature allocation construction .",
    "the finite sum constraint ensures each index belongs to a finite number of features a.s .",
    "it remains to specify a distribution on the partition or feature frequencies .",
    "the frequencies can not be i.i.d .",
    "due to the finite summation constraint in both cases . in the partition case ,",
    "any infinite set of frequencies can not even be independent since the summation is fixed to one .",
    "one scheme to ensure summation to unity is called _ stick - breaking _ ( @xcite ; @xcite ; @xcite ; @xcite ) . in stick - breaking , the stick lengths are obtained by recursively breaking off parts of the unit interval to return as the atoms @xmath257 ( cf . figure  [ fig : stick_illus ] ) .",
    "in particular , we generate stick - breaking proportions @xmath258 as @xmath90$]-valued random variables .",
    "then @xmath259 is the first proportion @xmath260 times the initial stick length @xmath261 ; hence , @xmath262 .",
    "recursively , after @xmath51 breaks , the remaining length of the initial unit interval is @xmath263 .",
    "and @xmath264 is the proportion @xmath265 of the remaining stick ; hence , @xmath266 .    the stick - breaking construction yields @xmath267 such that @xmath268 $ ] for each @xmath51 and @xmath269 .",
    "if the @xmath270 do not decay too rapidly , we will have @xmath271 .",
    "in particular , the partition block proportions @xmath272 sum to unity a.s .",
    "iff there is no remaining stick mass : @xmath273 .",
    "we often make the additional , convenient assumption that the @xmath270 are independent . in this case",
    ", a necessary and sufficient condition for @xmath271 is @xmath274 = -\\infty$ ] @xcite . when the @xmath270 are independent and of a canonical distribution , they are easily simulated",
    ". moreover , if we assume that the @xmath270 are such that the @xmath272 decay sufficiently rapidly in @xmath51 , one strategy for simulating a stick - breaking model is to ignore all @xmath275 for some fixed , finite @xmath4 . this approximation is known as truncation @xcite .",
    "it is fortuitously the case that in some models of particular interest , such useful assumptions fall out naturally from the model construction ( e.g. , examples  [ ex : stick_crp ] and  [ ex : stick_ibp ] ) .",
    "[ ex : stick_crp ] in the original exchangeability result due to de finetti @xcite , the exchangeable random variables were zero / one - valued , and the mixing measure was a distribution on a single frequency so that the outcomes were conditionally bernoulli .",
    "we will find a similar result in obtaining the stick - breaking proportions associated with the chinese restaurant process .",
    "we can construct a sequence of binary - valued random variables by dividing the customers in the crp who are sitting at the first table from the rest ; color the former collection of customers gray and the latter collection of customers white .",
    "then , we see that the first customer must be colored gray .",
    "and thus we begin with a single gray customer and no white customers .",
    "this binary valuation for the first table in the crp is illustrated by the first column in the matrix in figure  [ fig : polya_dp ] .        at this point",
    ", it is useful to recall the plya urn construction ( @xcite ; @xcite),whereby an urn starts with @xmath276 gray balls and @xmath277 white balls . at each round @xmath1 , we draw a ball from the urn , replace it , and add @xmath278 of the same color of ball to the urn . at the end of the round",
    ", we have @xmath279 gray balls and @xmath280 white balls . despite the urn metaphor ,",
    "the number of balls need not be an integer at any time . by checking equation  ( [ eq : crp ] ) , which defines the crp , we can see that the coloring of the gray / white customer matrix assignments starting with the second customer has the same distributions as a sequence of balls from a plya urn as a plya urn with @xmath281 initial gray balls , @xmath282 initial white balls and @xmath283 replacement balls .",
    "let @xmath284 and @xmath285 represent the numbers of gray and white balls , respectively , in the urn after @xmath1 rounds .",
    "the important fact about the plya urn we use here is that there exists some @xmath286 such that @xmath287 for all @xmath1 . in this particular case of the crp , then , @xmath288 is one if a customer sits at the first table ( or zero otherwise ) , and @xmath289 with @xmath290 .",
    "we now look at the sequence of customers who sit at the second and subsequent tables .",
    "that is , we condition on customers not sitting at the first table or equivalently on the sequence with @xmath291 .",
    "again , we have that the first customer sits at the second table , by the crp construction",
    ". now let customers at the second table be colored gray and customers at the third and later tables be colored white .",
    "this valuation is illustrated in the second column in figure  [ fig : polya_dp ] ; each @xmath292 in the figure denotes a data point where the first partition block is chosen and , therefore , the current plya urn is not in play . as before , we begin with one gray customer and no white customers .",
    "we can check equation ( [ eq : crp ] ) to see that customer coloring once more proceeds according to a plya urn scheme with @xmath293 initial gray balls , @xmath294 initial white balls and @xmath295 replacement balls .",
    "thus , contingent on a customer not sitting at the first table , the @xmath1th customer sits at the second table with i.i.d .",
    "distribution @xmath296 with @xmath297 . since the sequence of individuals sitting at the second table has no other dependence on the sequence of individuals sitting at the first table , we have that @xmath298 is independent of @xmath260 .",
    "the argument just outlined proceeds recursively to show us that the @xmath1th customer , conditional on not sitting at the first @xmath299 tables for @xmath300 , sits at the @xmath4th table with i.i.d .",
    "distribution @xmath301 and @xmath302 with @xmath303 independent of the previous @xmath304 .        combining these results",
    ", we see that we have the following construction for the customer seating patterns .",
    "the @xmath270 are distributed independently and identically according to @xmath305 .",
    "the probability @xmath306 of sitting at the @xmath4th table is the probability of not sitting at the first @xmath299 tables , conditional on not sitting at the previous table , times the conditional probability of sitting at the @xmath4th table : @xmath307 \\cdot{v}_{k}$ ] .",
    "finally , with the vector of table frequencies @xmath246 , each customer sits independently and identically at the corresponding vector of tables according to these frequencies .",
    "this process is summarized here : @xmath308    to see that this process is well - defined , first note that @xmath309 $ ] exists , is negative and is the same for all @xmath51 values .",
    "it follows that @xmath310 = -\\infty$ ] , so by the discussion before this example , we must have @xmath311 .",
    "the feature case is easier .",
    "since it does not require the frequencies to sum to one , the random frequencies can be independent so long as they have an a.s .",
    "finite sum .",
    "[ ex : stick_ibp ] as in the case of the crp , we can recover the stick lengths for the indian buffet process using an argument based on an urn model .",
    "recall that on the first round of the indian buffet process , @xmath312 features are chosen to contain index @xmath261 .",
    "consider one of the features , labeled  @xmath51 . by construction",
    ", each future data point @xmath1 belongs to this feature with probability @xmath313 .",
    "thus , we can model the sequence after the first data point as a plya urn of the sort encountered in example [ ex : stick_crp ] with initially @xmath314 gray balls , @xmath315 white balls and @xmath316 replacement balls .",
    "as we have seen , there exists a random variable @xmath317 such that representation of this feature by data point @xmath1 is chosen , i.i.d . across all @xmath1 , as @xmath318 .",
    "since the bernoulli draws conditional on previous draws are independent across all @xmath51 , the @xmath270 are likewise independent of each other ; this fact is also true for @xmath51 in future rounds . draws according to such an urn",
    "are illustrated in each of the first four columns of the matrix in figure  [ fig : polya_ibp ] .",
    "now consider any round @xmath12 .",
    "according to the ibp construction , @xmath319 new features are chosen to include index @xmath12 .",
    "each future data point @xmath1 ( with @xmath320 ) represents feature @xmath51 among these features with probability @xmath321 . in this case",
    ", we can model the sequence after the @xmath12th data point as a plya urn with @xmath322 initial gray balls , @xmath323 initial white balls and @xmath316 replacement balls .",
    "so there exists a random variable @xmath324 such that representation of feature @xmath51 by data point @xmath1 is chosen , i.i.d . across all @xmath1 , as @xmath318 .    finally , then , we have the following generative model for the feature allocation by iterating across @xmath325 @xcite : @xmath326 \\\\[-8pt ] \\eqntext { k = k_{n-1 } + 1,\\ldots , k_{n } , } \\\\ \\nonumber { i}_{n , k } & \\stackrel{\\mathrm{indep } } { \\sim } & { \\operatorname{bern } } ( { v}_{k}),\\quad   k = 1 , \\ldots , k_{n}.\\end{aligned}\\ ] ] @xmath327 is an indicator random variable for whether feature @xmath51 contains index @xmath12 .",
    "the collection of features to which index @xmath12 belongs , @xmath105 , is the collection of features @xmath51 with @xmath328 .      as we have seen above ,",
    "the exchangeable probability functions of section  [ sec : epf ] are the marginal distributions of the partitions or feature allocations generated according to stick - length models with the stick lengths integrated out .",
    "it has been proposed that including the stick lengths in mcmc samplers of these models will improve mixing ( ishwaran and zarepour , @xcite ) . while it is impossible to sample the countably infinite set of partition block or feature frequencies in these models ( cf .",
    "examples [ ex : stick_crp ] and  [ ex : stick_ibp ] ) , a  number of ways of getting around this difficulty have been investigated .",
    "@xcite examine two separate finite approximations to the full crp stick - length model : one uses a parametric approximation to the full infinite model , and the other creates a truncation by setting the stick break at some fixed size @xmath4 to be  1 : @xmath329 .",
    "there also exist techniques that avoid any approximations and deal instead directly with the full model , in particular , retrospective sampling ( papaspiliopoulos and roberts , @xcite ) and slice sampling @xcite .",
    "while our discussion thus far has focused onmcmc sampling as a means of approximating the posterior distribution of either the block assignments or both the block assignments and stick lengths , including the stick lengths in a posterior analysis facilitates a different posterior approximation ; in particular , _ variational methods _ can also be used to approximate the posterior .",
    "these methods minimize some notion of distance to the posterior over a family of potential approximating distributions @xcite . the practicality and , indeed , speed of these methods in the case of stick - breaking for the crp ( example  [ ex : stick_crp ] )",
    "have been demonstrated by @xcite .",
    "a number of different models for the stick lengths corresponding to the features of an ibp ( example  [ ex : stick_ibp ] ) have been discovered .",
    "the distributions described in example  [ ex : stick_ibp ] are covered by @xcite , who build on work from @xcite , @xcite .",
    "a special case of the ibp is examined by @xcite , who detail a slice sampling algorithm for sampling from the posterior of the stick lengths and feature assignments . yet another stick - length model for the ibp",
    "is explored by @xcite , who show how to apply variational methods to approximate the posterior of their model .",
    "stick - length modeling has the further advantage of allowing inference in cases where it is not straightforward to integrate out the underlying stick lengths to obtain a tractable exchangeable probability function .",
    "an important point to reiterate about the labels @xmath50 and label collections @xmath105 is that when we use the order - of - appearance labeling scheme for partition or feature blocks described above , the random sequences @xmath221 and @xmath236 are not exchangeable . often , however , we would like to make use of special properties of exchangeability when dealing with these sequences .",
    "for instance , if we use markov chain monte carlo to sample from the posterior distribution of a partition ( cf .",
    "section  [ sec : epf_infer ] ) , we might want to gibbs sample the cluster assignment of data point @xmath12 given the assignments of the remaining data points : @xmath50 given @xmath330 .",
    "this sampling is particularly easy in some cases @xcite if we can treat @xmath50 as the last random variable in the sequence , but this treatment requires exchangeability .    a way to get around this dilemma",
    "was suggested by @xcite and appeared above in our motivation for using stick lengths .",
    "namely , we assign to the @xmath51th partition block a uniform random label @xmath331)$ ] ; analogously , we assign to the @xmath51th feature a uniform random label @xmath332)$ ] .",
    "we can see that in both cases , all of the labels are a.s .",
    "now , in the partition case , let @xmath50 be the uniform random label of the partition block to which @xmath12 belongs . and",
    "in the feature case , let @xmath105 be the ( finite ) set of uniform random feature labels for the features to which @xmath12 belongs .",
    "we can recover the partition or feature allocation as the induced partition or feature allocation by grouping indices assigned to the same label .",
    "moreover , as discussed above , we now have that each of @xmath221 and @xmath236 is an exchangeable sequence .",
    "if we form partitions or features according to the stick - length constructions detailed in section  [ sec : stick ] , we know that each unique partition or feature label @xmath333 is associated with a frequency @xmath334",
    ". we can use this association to form a random measure : @xmath335 where @xmath336 is a unit point mass located at @xmath333 . in the partition case , @xmath337",
    ", so the random measure is a random probability measure , and we may draw @xmath338 . in the feature case ,",
    "the weights have a finite sum but do not necessarily sum to one . in the feature case ,",
    "we draw @xmath105 by including each @xmath333 for which @xmath339 yields a draw of 1 .",
    "another way to codify the random measure in equation ( [ eq : rand_meas ] ) is as a monotone increasing stochastic process on @xmath90 $ ] .",
    "let @xmath340 then the atoms of @xmath341 are in one - to - one correspondence with the jumps of the process @xmath342 .",
    "this increasing random function construction gives us another means of choosing distributions for the weights @xmath334 .",
    "we have already seen that these can not be i.i.d .",
    "due to the finite summation condition .",
    "however , we will see that if we require that the _ increments _ of a monotone , increasing stochastic process are independent and stationary , then we can use the jumps of that function as the atoms in our random measure for partitions or features .",
    "@xmath343a _ subordinator _",
    "( @xcite ; @xcite @xcite ) is a stochastic process @xmath344 that has the following properties :    * nonnegative , nondecreasing paths ( a.s . ) , * paths that are right - continuous with left limits , and * stationary , independent increments .    for our purposes , wherein the subordinator values will ultimately correspond to (",
    "perhaps scaled ) probabilities , we will assume the subordinator takes values in @xmath345 , though alternative ranges with a sense of ordering are possible .",
    "subordinators are of interest to us because they not only exhibit the stationary independent increments property but they also can always be decomposed into two components : a deterministic _ drift _ component and a _ poisson point process_. recall that a poisson point process on space @xmath346 with rate measure @xmath347 , where @xmath348 , yields a countable subset of points of @xmath346 .",
    "let @xmath349 be the number of points of the process in set @xmath74 for @xmath350 .",
    "the process is characterized by the fact that , first , @xmath351 for any @xmath74 and , second , for any disjoint @xmath352 , we have that @xmath353 are independent random variables .",
    "see @xcite for a thorough treatment of these processes .",
    "an example subordinator with both drift and jump components is shown on the left - hand side of figure  [ fig : subordinator ] .",
    "the subordinator decomposition is detailed in the following result @xcite .",
    "[ thm : sub_ppp ] every subordinator @xmath344 can be written as @xmath354 for some constant @xmath355 and where @xmath356 is the countable set of points of a poisson point process with intensity @xmath357 , where @xmath358 is a lvy measure ; that is , @xmath359    in particular , then , if a subordinator is finite at time  @xmath360 , the jumps of the subordinator up to @xmath360 may be used as feature block frequencies if they have support in @xmath90 $ ] . or , in general , the normalized jumps may be used as partition block frequencies .",
    "we can see from the right - hand side of figure  [ fig : subordinator ] that the jumps of a subordinator partition intervals of the form @xmath361 , as long as the subordinator has no drift component . in either the feature or cluster case ,",
    "we have substituted the condition of independent and identical distribution for the partition or feature frequencies ( i.e. , the jumps ) with a more natural continuous - time analogue : independent , stationary intervals .",
    "just as the laplace transform of a positive random variable characterizes the distribution of that random variable , so does the laplace transform of the subordinator  which is a positive random variable at any fixed time point ",
    "describe this stochastic process ( @xcite @xcite ) .",
    "[ thm : lk ] if @xmath344 is a subordinator , then for @xmath362 we have @xmath363 with @xmath364 where @xmath355 is called the drift constant and @xmath358 is a nonnegative , lvy measure on @xmath365 .",
    "the function @xmath366 is called the _",
    "laplace exponent _ in this context .",
    "we note that a subordinator is characterized by its drift constant and lvy measure .",
    "using subordinators for feature allocation modeling is particularly easy ; since the jumps of the subordinators are formed by a poisson point process , we can use poisson process methodology to find the stick lengths and efpf . to set up this derivation ,",
    "suppose we generate feature membership from a subordinator by taking bernoulli draws at each of its jumps with success probability equal to the jump size .",
    "since every jump has strictly positive size , the feature associated with each jump will eventually score a bernoulli success for some index @xmath12 with probability one .",
    "therefore , we can enumerate all jumps of the process in order of appearance ; that is , we first enumerate all features in which index @xmath261 appears , then all features in which index @xmath367 appears but not index @xmath261 , and so on . at the @xmath12th iteration , we enumerate all features in which index @xmath12 appears but not previous indices .",
    "let @xmath368 represent the number of indices so chosen on the @xmath12th round .",
    "let @xmath369 so that recursively @xmath370 is the number of subordinator jumps seen by round @xmath12 , inclusive .",
    "let @xmath256 for @xmath371 be the distribution of a particular subordinator jump seen on round @xmath12 .",
    "we now turn to connecting the subordinator perspective to the earlier derivation of stick lengths in section  [ sec : stick ] .",
    "[ ex : sub_ibp ] in our earlier discussion , we found a collection of stick lengths to represent the featural frequencies for the ibp [ equation  ( [ eq : beta_proc_stick_lengths ] ) of example  [ ex : stick_ibp ] in section  [ sec : stick ] ] . to see the connection to subordinators , we start from the _ beta process subordinator _",
    "@xcite with zero drift ( @xmath372 ) and lvy measure @xmath373 we will see that the mass parameter @xmath123 and concentration parameter @xmath124 are the same as those introduced in example  [ ex : epf_ibp ] and continued in example  [ ex : stick_ibp ] .",
    "[ thm : sub_sticks ] generate a feature allocation from a beta process subordinator with lvy measure given by equation ( [ eq : beta_levy ] ) .",
    "then the sequence of subordinator jumps @xmath253 , indexed in order of appearance , has the same distribution as the sequence of ibp stick lengths @xmath374 described by equations ( [ eq : beta_proc_num_sticks ] ) and ( [ eq : beta_proc_stick_lengths ] ) .",
    "-axis values of the filled black circles , emphasized by dotted lines , are generated according to a poisson process .",
    "the @xmath90$]-valued function @xmath375 is arbitrary .",
    "the vertical axis values of the points are uniform draws in @xmath90 $ ] .",
    "the `` thinned '' points are the collection of @xmath376-axis values corresponding to vertical axis values below @xmath375 and are denoted with a @xmath292 symbol . ]",
    "recall the following fact about poisson thinning @xcite , illustrated in figure  [ fig : thinning ] .",
    "suppose that a poisson point process with rate measure @xmath377 generates points with values @xmath376 .",
    "then suppose that , for each such point @xmath376 , we keep it with probability @xmath378 $ ] .",
    "the resulting set of points is also a poisson point process , now with rate measure @xmath379 .",
    "we prove theorem  [ thm : sub_sticks ] recursively .",
    "define the measure @xmath380 so that @xmath381 is the beta process lvy measure @xmath358 in equation ( [ eq : beta_levy ] ) .",
    "we make the recursive assumption that @xmath382 is distributed as the beta process measure without atoms corresponding to features chosen on the first @xmath12 iterations .",
    "there are two parts to proving theorem  [ thm : sub_sticks ] .",
    "first , we show that , on the @xmath12th iteration , the number of features chosen and the distribution of the corresponding atom weights agree with equations ( [ eq : beta_proc_num_sticks ] ) and ( [ eq : beta_proc_stick_lengths ] ) , respectively .",
    "second , we check that the recursion assumption holds .    for the first part , note that on the @xmath12th round we choose features with probability equal to their atom weight .",
    "so we form a thinned poisson process with rate measure @xmath383 .",
    "this rate measure has total mass @xmath384 so the number of features chosen is poisson - distributed with mean @xmath385 , as desired [ cf .",
    "equation  ( [ eq : beta_proc_num_sticks ] ) ] . and",
    "the atom weights have distribution equal to the normalized rate measure @xmath386 as desired [ cf . equation ( [ eq : beta_proc_stick_lengths ] ) ] .    finally , to check the recursion assumption , we note that those sticks that remain were chosen for having bernoulli failure draws ; that is , they were chosen with probability equal to one minus their atom weight .",
    "so the thinned rate measure for the next round is @xmath387 which is just @xmath382 .",
    "the form of the efpf of the feature allocation generated from the beta process subordinator follows immediately from the stick - length distributions we have just derived by the discussion in example  [ ex : stick_ibp ] in section  [ sec : stick ] .",
    "we see from the previous example that feature allocation stick lengths and efpfs can be obtained in a straightforward manner using the poisson process representation of the jumps of the subordinator .",
    "partitions , however , are not as easy to analyze , principally due to the fact that the subordinator jumps must first be normalized to obtain a probability measure on @xmath90 $ ] ; a random measure with finite total mass is not sufficient in the partition case .",
    "hence , we must compute the stick lengths and eppf using partition block frequencies from these normalized jumps instead of directly from the subordinator jumps .    in the eppf case",
    ", we make use of a result that gives us the exchangeable probability function as a function of the laplace exponent .",
    "though we do not derive this formula here , its derivation can be found in @xcite ; the proof relies on , first , calculating the joint distribution of the subordinator jumps and partition generated from the normalized jumps and , second , integrating out the subordinator jumps to find the partition marginal .",
    "[ thm : sub_eppf ] form a probability measure @xmath341 by normalizing jumps of the subordinator with laplace exponent @xmath388 . let @xmath43 be a consistent set of exchangeable partitions induced by i.i.d",
    ". draws from @xmath341 . for each exchangeable partition @xmath389 of @xmath0 $ ] with @xmath390 for each @xmath51 , @xmath391 where @xmath392 is the @xmath56th derivative of the laplace exponent @xmath388 evaluated at @xmath377 .    [",
    "ex : sub_crp ] we start by introducing the _ gamma process _ , a subordinator that we will see below generates the chinese restaurant process eppf .",
    "the gamma process has laplace exponent @xmath366 [ equation ( [ eq : laplace_exp ] ) ] characterized by @xmath393 for @xmath124 and @xmath394 [ cf .",
    "equation ( [ eq : lk ] ) in theorem  [ thm : lk ] ] .",
    "we will see that @xmath395 corresponds to the crp concentration parameter and that @xmath396 is arbitrary and does not affect the partition model .",
    "we calculate the eppf using theorem  [ thm : sub_eppf ] .",
    "[ thm : eppf_gamma]@xmath397 the eppf for partition block membership chosen according to the normalized jumps @xmath246 of the gamma subordinator with parameter @xmath48 is the crp eppf [ equation ( [ eq : crp_eppf ] ) ] .    by theorem  [ thm : sub_eppf ] , if we can find all order derivatives of the laplace exponent @xmath388 , we can calculate the eppf for the partitions generated with frequencies equal to the normalized jumps of this subordinator .",
    "the derivatives of @xmath388 , which are known to always exist ( @xcite ; @xcite ) , are straightforward to calculate if we begin by noting that , from equation ( [ eq : lk ] ) in theorem  [ thm : lk ] , we have in general that @xmath398 hence , for the gamma process subordinator , @xmath399 then simple integration and differentiation yield @xmath400 since @xmath401 and @xmath402 we can substitute these quantities into the general eppf formula in equation ( [ eq : laplace_eppf ] ) of theorem  [ thm : sub_eppf ] to obtain @xmath403 { b}^{n-1-n-{{\\theta}}+1 } \\\\ & & \\qquad{}\\cdot\\int_{0}^{\\infty } x^{n-1 } ( x + 1)^{-n-{{\\theta } } } \\,dx\\quad\\mathrm{for}\\ x = \\lambda/ { b}\\\\ & & \\quad= \\frac{{{\\theta}}^{k}}{(n-1 ) ! } \\biggl [ \\prod_{k=1}^{k } ( n_{k } - 1 ) ! \\biggr ] \\frac{\\gamma(n ) \\gamma({{\\theta}})}{\\gamma(n+{{\\theta } } ) } \\\\ & & \\quad= \\theta^{k } \\biggl [ \\prod_{k=1}^{k } ( n_{k } - 1 ) ! \\biggr ] \\frac { 1}{{{\\theta}}({{\\theta}}+ 1)_{n-1 \\uparrow1}}.\\end{aligned}\\ ] ] the penultimate line follows from the form of the beta prime distribution",
    ". the final line is the crp eppf from equation ( [ eq : crp_eppf ] ) , as desired .",
    "we note in particular that the parameter @xmath396 does not appear in the final eppf .",
    "whenever the laplace exponent of a subordinator is known , theorem  [ thm : sub_eppf ] can similarly be applied to quickly find the eppf of the partition generated by sampling from the normalized subordinator jumps .    to find the distributions of the stick lengths",
    " that is , the partition block frequencies  from the subordinator representation for a partition , we must find the distributions of the normalized subordinator jumps .",
    "as in the feature case , we may enumerate the jumps of a subordinator used for partitioning in the order of their appearance .",
    "that is , let @xmath259 be the normalized subordinator jump size corresponding to the cluster of the first data point .",
    "recursively , suppose index @xmath12 joins a cluster to which none of the indices in @xmath151 $ ] belong , and suppose there are @xmath51 clusters among @xmath151 $ ] .",
    "then let @xmath264 be the normalized subordinator jump size corresponding to the cluster containing @xmath12 .",
    "[ ex : sub_crp_sticks ] we continue with the crp example .",
    "[ thm : stick_gamma]@xmath404 the normalized subordinator jumps @xmath246 in order of appearance of the gamma subordinator with concentration parameter @xmath395 ( and arbitrary parameter @xmath394 ) have the same distribution as the crp stick lengths [ equation ( [ eq : dp_stick ] ) of example  [ ex : stick_crp ] in section  [ sec : stick ] ] .",
    "first , we introduce some notation .",
    "let @xmath405 , the sum over all of the jumps of the subordinator .",
    "second , let @xmath406 , the total sum minus the first @xmath51 elements ( in order of appearance ) .",
    "note that @xmath407 . finally , let @xmath408 and @xmath409 .",
    "then a simple telescoping of factors shows that @xmath410 : @xmath411    it remains to show that the @xmath270 have the desired distribution . to that end",
    ", it is easier to work with the @xmath412 .",
    "we will find the following lemma @xcite useful .",
    "consider a subordinator with lvy measure @xmath358 , and suppose @xmath413 equals the sum of all jumps of the subordinator .",
    "let @xmath414 be the density of @xmath358 with respect to the lebesgue measure . and",
    "let @xmath415 be the density of the distribution of @xmath413 with respect to the lebesgue measure .",
    "then @xmath416    with this lemma in hand , the result follows from a change of variables calculation ; we use a bijection between @xmath417 and @xmath418 defined by @xmath419 .",
    "the determinant of the jacobian for the transformation to the former variables from the latter is @xmath420 = \\prod_{j=0}^{k-1 } { \\tau}_{j } ( \\tau , w_{1 } , \\ldots , w_{k}).\\ ] ] in the derivation that follows , we start by expressing results in terms of the @xmath421 terms with the dependence on @xmath422 suppressed to avoid notational clutter , for example , @xmath423 . at the end , we will evaluate the @xmath421 terms as functions of @xmath424 .",
    "for now , then , we have @xmath425    in the case of the gamma process , we can read @xmath426 from equation ( [ eq : gamma_levy ] ) .",
    "the function @xmath415 is determined by @xmath414 and in this case @xcite , @xmath427 so @xmath428 since the distribution factorizes , the @xmath429 are independent of each other and of @xmath413 .",
    "second , we can read off the distributional kernel of each @xmath412 to establish @xmath430 , from whence it follows that @xmath431 .      in some sense",
    ", we skipped ahead in describing inference in sections  [ sec : epf_infer ] and  [ sec : stick_infer ] .",
    "there , we made use of the fact that random labels for partitions and features imply exhangeability of the data partition block assignments @xmath221 and data feature assignments @xmath236 . in the discussion above",
    ", we study the object that associates random uniformly distributed labels with each partition or feature . assuming the labels come from a uniform distribution rather than a general continuous distribution is a special case of the discussion in section  [ sec : epf_infer ] , and we defer the general case to the next section ( section  [ sec : crm ] ) .",
    "we have seen above that it is particularly straightforward to obtain an eppf or efpf formulation , which yields gibbs sampling steps as described in section  [ sec : epf_infer ] , when the stick lengths are generated according to a normalized poisson process in the partition case or a poisson process in the feature case .",
    "examples [ ex : sub_ibp ] and  [ ex : sub_crp ] illustrate how to find such exchangeable probability functions .",
    "further , we have already seen the usefulness of the stick representation in inference , and examples  [ ex : sub_ibp ] and  [ ex : sub_crp_sticks ] illustrate how stick - length distributions may be recovered from the subordinator framework .",
    "in our discussion of subordinators , the jump sizes of the subordinator corresponded to the feature frequencies or unnormalized partition frequencies and were the quantities of interest .",
    "by contrast , the locations of the jumps mainly served as convenient labels for the frequencies .",
    "these locations were chosen uniformly at random from the unit interval .",
    "this choice guaranteed the a.s .",
    "uniqueness of the labels and the exchangeability of the sequence of index assignments : @xmath221 in the clustering case or @xmath236 in the feature case .    however , a labeling retains exchangeability and a.s .",
    "uniqueness as long as the labels are chosen i.i.d . from any continuous distribution ( not just the uniform distribution ) .",
    "moreover , in typical applications , we wish to associate some parameter , often referred to as a `` random effect , '' with each partition block or feature . in the partition case",
    ", we usually model the @xmath12th data point @xmath215 as being generated according to some likelihood depending on the parameter corresponding to its block assignment . for example , an individual animal s height and weight , @xmath215 , varies randomly around the height and weight of its species , @xmath50 . likewise , in the feature case , we typically model the observed data point @xmath215 as being generated according to some likelihood depending on the collection of parameters corresponding to its collection of feature block assignments [ cf . equation  ( [ eq : likelihood ] ) ] . for example , the book - buying pattern of an online consumer , @xmath215 , varies with some noise based on the topics this person likes to read about : @xmath105 is a collection , possibly empty , of such topics .    in these cases",
    ", it can be useful to suppose that the partition block labels ( or feature labels ) @xmath233 are not necessarily @xmath432-valued but rather are generated i.i.d . according to some continuous distribution @xmath210 on a general space @xmath433 .",
    "then , whenever @xmath51 is the order - of - appearance partition block label of index @xmath12 , we let @xmath211 .",
    "similarly , whenever @xmath51 is the order - of - appearance feature label for some feature to which index @xmath12 belongs , @xmath234 . finally , then , we complete the generative model in the partition case by letting @xmath434 for some distribution function @xmath213 depending on parameter @xmath50 . and in the feature case , @xmath435 , where now the distribution function @xmath213 depends on the collection of parameters  @xmath105 .",
    "when we take the jump sizes @xmath436 of a subordinator as the weights of atoms with locations @xmath437 drawn i.i.d . according to @xmath210 as described above , we find ourselves with a _ completely random measure _",
    "@xmath438 : @xmath439 a completely random measure is a random measure @xmath341 such that whenever @xmath74 and @xmath440 are disjoint sets , we have that @xmath441 and @xmath442 are independent random variables .    to see that associating these more general atom locations to the jumps of a subordinator yields a completely random measure , note that theorem  [ thm : sub_ppp ] tells us that the subordinator jump sizes are generated according to a poisson point process , with some intensity measure @xmath443 .",
    "the marking theorem for poisson point processes @xcite in turn yields that the tuples @xmath444 are generated according to a poisson point process with intensity measure @xmath445 .",
    "by @xcite , whenever the tuples @xmath446 are drawn according to a poisson point process , the measure in equation  ( [ eq : crm ] ) is completely random .",
    "[ ex : crm_dp ] we can form a completely random measure from the gamma process subordinator and a random labeling of the partition blocks . specifically , suppose that the labels come from a continuous measure @xmath447 .",
    "then we generate a completely random measure @xmath448 , called a _ gamma process _",
    "@xcite , in the following way : @xmath449 here , @xmath450 denotes a draw from a poisson point process with intensity measure @xmath451 .",
    "the parameters @xmath124 and @xmath394 are the same as for the gamma process subordinator .",
    "a gamma process draw , along with its generating poisson point process intensity measure , is illustrated in figure  [ fig : gamma_process ] .     in equation ( [ eq : gamma_ppp_intens ] ) for the choice @xmath452 $ ] and @xmath447 the uniform distribution on @xmath90 $ ] .",
    "the endpoints of the line segments are points drawn from the poisson point process as in equation ( [ eq : gamma_ppp_draw ] ) . taking the positive real - valued coordinate ( leftmost axis ) as the atom weights",
    ", we find the random measure @xmath448 ( a gamma process ) on @xmath433 from equation  ( [ eq : gamma_proc_crm ] ) in the bottom plane . ]",
    "the _ dirichlet process _ ( dp ) is the random measure formed by normalizing the gamma process @xcite .",
    "since the dirichlet process atom weights sum to one , it can not be completely random .",
    "we can write the dirichlet process @xmath453 generated from the gamma process @xmath454 above as @xmath455 the random variables @xmath272 have the same distribution as the dirichlet process sticks [ equation ( [ eq : dp_stick ] ) ] or normalized gamma process subordinator jump lengths , as we have seen above ( example  [ ex : sub_crp ] ) .",
    "consider sampling points from a dirichlet process and forming the induced partition of the data indices .",
    "theorem [ thm : eppf_gamma ] shows us that the distribution of the induced partition is the chinese restaurant process eppf .",
    "[ ex : crm_bp ] we can form a completely random measure from the beta process subordinator and a random labeling of the feature blocks . if the labels are generated i.i.d . from a continuous measure @xmath210",
    ", then we say the completely random measure @xmath75 , generated as follows , is called a _ beta process _ : @xmath456 the beta process , along with its generating intensity measure , is depicted in figure  [ fig : beta_process ] .",
    "the @xmath436 have the same distribution as the beta process sticks [ equation  ( [ eq : beta_proc_stick_lengths ] ) ] or the beta process subordinator jump lengths ( example  [ ex : sub_ibp ] ) .     in equation ( [ eq : beta_ppp_intens ] ) for the choice",
    "@xmath452 $ ] and @xmath447 the uniform distribution on @xmath90 $ ] .",
    "the endpoints of the line segments are points drawn from the poisson point process as in equation ( [ eq : beta_ppp_draw ] ) .",
    "taking the @xmath90$]-valued coordinate ( leftmost axis ) as the atom weights , we find the measure @xmath457 ( a  beta process ) on @xmath433 from equation ( [ eq : beta_ppp_crm ] ) in the bottom plane . ]    now consider sampling a collection of atom locations according to bernoulli draws from the atom weights of a beta process and forming the induced feature allocation of the data indices .",
    "theorem  [ thm : sub_sticks ] shows us that the distribution of the induced feature allocation is given by the indian buffet process efpf .      in this section we finally study the full model first outlined in the context of inference of partition and feature structures in section  [ sec : epf_infer ] .",
    "the partition or feature labels described in this section are the same as the block - specific parameters first described in section  [ sec : epf_infer ] .",
    "since this section focuses on a generalization of the partition or feature labeling scheme beyond the uniform distribution option encoded in subordinators , inference for the atom weights remains unchanged from sections  [ sec : epf_infer ] ,  [ sec : stick_infer ] and [ sec : sub_infer ] .",
    "however , we note that , in the course of inferring underlying partition or feature structures , we are often also interested in inferring the parameters of the generative model of the data given the partition block or the feature labels .",
    "conditional on the partition or feature structure , such inference is handled as in a normal hierarchical model with fixed dependencies .",
    "namely , the parameter within a particular block may be inferred from the data points that depend on this block as well as the prior distribution for the parameters .",
    "details for the dirichlet process example inferred via mcmc sampling are provided by @xcite , @xcite , @xcite ; @xcite work out details for the dirichlet process using variational methods . in the beta process case , @xcite , @xcite , @xcite describe mcmc sampling , and @xcite describe a variational approach .",
    "in the discussion above we have pursued a progressive augmentation from ( 1 ) simple distributions over partitions and feature allocations in the form of exchangeable probability functions to ( 2 ) the representation of stick lengths encoding frequencies of the partition block and feature occurrences to ( 3 ) subordinators , which associate random @xmath432-valued labels with each partition block or feature , and finally to ( 4 ) completely random measures , which associate a general class of labels with the stick lengths and whose labels we generally use as parameters in likelihood models built from the partition or feature allocation representation .    along the way",
    ", we have focused primarily on two vignettes .",
    "we have shown , via these successive augmentations , that the chinese restaurant process specifies the marginal distribution of the induced partition formed from i.i.d",
    ". draws from a dirichlet process , which is in turn a normalized completely random measure .",
    "and we have shown that the indian buffet process specifies the marginal distribution of the induced feature allocation formed by i.i.d .",
    "bernoulli draws across the weights of a beta process .",
    "there are many extensions of these ideas that lie beyond the scope of this paper .",
    "a number of extensions of the crp and dirichlet process exist  in either the eppf form ( @xcite ; @xcite ) , the stick - length form ( dunson and park , @xcite ) or the random measure form ( pitman and yor , @xcite ) . likewise , extensions of the ibp and beta process have been explored ( @xcite ; @xcite ; @xcite ) .",
    "more generally , the framework above demonstrates how alternative partition and feature allocation models may be constructed  either by introducing different eppfs ( @xcite ; @xcite ) or efpfs , different stick - length distributions @xcite or different random measures @xcite .    finally , we note that expanding the set of combinatorial structures with useful bayesian priors from partitions to the superset of feature allocations suggests that further such structures might be usefully .",
    "for instance , the _ beta negative binomial process _",
    "( @xcite ; @xcite ) provides a prior on a generalization of a feature allocation where we allow the features themselves to be multisets ; that is , each index may have nonnegative integer multiplicities of features . models on trees ( @xcite ; @xcite ; @xcite ) , graphs @xcite and permutations @xcite provide avenues for future exploration . and there",
    "likely remain further structures to be fitted out with useful bayesian priors .",
    "t. broderick s research was funded by a national science foundation graduate research fellowship .",
    "this material is supported in part by the national science foundation award 0806118 combinatorial stochastic processes and is based upon work supported in part by the office of naval research under contract / grant number n00014 - 11 - 1 - 0688 ."
  ],
  "abstract_text": [
    "<S> one of the focal points of the modern literature on bayesian nonparametrics has been the problem of _ clustering _ , or _ partitioning _ </S>",
    "<S> , where each data point is modeled as being associated with one and only one of some collection of groups called clusters or partition blocks . </S>",
    "<S> underlying these bayesian nonparametric models are a set of interrelated stochastic processes , most notably the dirichlet process and the chinese restaurant process . in this paper </S>",
    "<S> we provide a formal development of an analogous problem , called _ feature modeling _ , for associating data points with arbitrary nonnegative integer numbers of groups , now called features or topics . </S>",
    "<S> we review the existing combinatorial stochastic process representations for the clustering problem and develop analogous representations for the feature modeling problem . </S>",
    "<S> these representations include the beta process and the indian buffet process as well as new representations that provide insight into the connections between these processes . </S>",
    "<S> we thereby bring the same level of completeness to the treatment of bayesian nonparametric feature modeling that has previously been achieved for bayesian nonparametric clustering .    , </S>"
  ]
}