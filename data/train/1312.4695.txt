{
  "article_text": [
    "natural sounds such as speech , environmental noises or animal calls possess a complex statistical structure . to achieve a good performance in real - world hearing tasks , neuronal and artificial systems should utilize data representations which are adapted to regularities of the natural auditory environment , while making task relevant quantities explicit .    one particular statistical property of natural sounds , is _ sparsness _ @xcite . a typical sample of a sparse signal can be represented as a linear combination of only a few features belonging to a large dictionary .",
    "it has been suggested , that neural representations of sound , may take advantage of this regularity .",
    "indeed - learning sparse codes of natural sounds has successfully predicted shapes of cochlear filters @xcite and spectrotemporal receptive fields of the inferior colliculus neurons @xcite , suggesting adaptation of those systems to the natural environment .",
    "sparse coding methods have also proven to be useful in machine hearing applications such as speech and music classification @xcite .    however , in some cases linear sparse codes are not an appropriate representation of the data .",
    "linear coding models are not robust to temporal jitter , since even a small temporal stimulus shift changes the encoding @xcite . additionally , phase ( i.e. temporal displacement not larger than the waveform period ) is not explicitly represented .",
    "phase information is relevant for perceptual tasks such as spatial hearing @xcite or recognition of conspecific songs in songbirds @xcite . for performing such computations , a data representation different from linear sparse coding",
    "may be more useful .",
    "phase and amplitude information can be separated by representing the data using a combination of complex vectors .",
    "complex - valued sparse coding @xcite and independent component analysis ( ica ) @xcite have been applied to learn representations of natural image patches .",
    "real and imaginary parts of complex features emerging from natural images resemble gabor filters of equal frequency , position scale and orientation in quadrature phase relationship .",
    "such basis functions reveal phase invariance i.e. the value of the complex amplitude does not change with spatial stimulus shifts smaller than the oscillation period . suggested by results obtained on natural images",
    ", one could think that complex sparse codes learned from natural sounds will also reveal phase invariance , since real - valued sparse codes yield features localized in time and frequency @xcite .",
    "this is , however , not true .",
    "natural sounds possess highly non - local cross - frequency correlations @xcite . reflecting this structure ,",
    "sparse , complex codes of natural acoustic stimuli capture frequency and bandwidth invariances .",
    "only a small fraction is phase - shift ( or time ) invariant @xcite .    in order to learn from the statistics of the data a representation that preserves a desired property such as phase invariance",
    ", one could select a parametric form of basis functions and adapt the parameter set @xcite .",
    "this method has been applied before to audio data by adapting a gammatone dictionary @xcite . despite many advantages of this solution , there exists a possibility , that parametric form of dictionary elements is not flexible enough to efficiently span the data space .",
    "to alleviate this problem this paper proposes to learn a sparse and complex representation of natural sounds with the phase - invariance promoting priors . proposed priors induce temporal continuity , i.e. _ slowness _ @xcite of both phase and amplitude , which turns out to be a correct assumption for learning phase - invariant features .",
    "+ the goal of the present work is , therefore , twofold :    1 .   to analyse high - order statistics of natural sounds",
    "this is done by learning sparse , complex representations of a speech corpus and analysis of the obtained features .",
    "learned dictionaries differ in cardinality ( complete and two - times overcomplete ) and are learned with and without prior knowledge .",
    "2 .   to introduce priors useful in learning",
    "structured , phase - invariant dictionaries of any data , not only natural sounds .",
    "in addition to imposing a desired structure , priors improve convergence of learning algorithms and allow to use less data for learning .",
    "the paper is structured as follows . in section [ sec2 ] complex valued sparse coding model",
    "is introduced together with phase and amplitude continuity priors .",
    "section [ section3 ] discusses statistical properties of complex features learned via complete and overcomplete sparse coding on natural , speech sounds . in section [ section4 ] coding efficiency of learned representations",
    "is assessed by comparing their performance in a denoising task and estimating entropies of learned coefficients .",
    "in a sparse coding model utilizing complex basis functions each data vector @xmath0 is represented as : @xmath1 where @xmath2 are complex coefficients , @xmath3 denotes a complex conjugation , @xmath4 are complex basis functions and @xmath5 is an additive gaussian noise .",
    "complex coefficients in euler s form become @xmath6 ( where @xmath7 ) therefore equation can be rewritten to explicitely represent phase @xmath8 and amplitude @xmath9 as separate variables : @xmath10 real and imaginary parts @xmath11 and @xmath12 of basis functions @xmath13 - span a subspace within which the position of a data sample is determined by amplitude @xmath14 and phase @xmath15 .",
    "depending on a number of basis functions @xmath16 , the representation can be complete ( @xmath17 ) or overcomplete ( @xmath18 ) . if data vectors are chunks of a temporal signal , such as natural sounds , the basis functions @xmath19 are functions of time ( as opposed to basis functions of natural images which constitute a spatial representation ) .",
    "if one attempts to learn a representation of particular time - dependent properties , constraints should be placed on the basis function set .",
    "[ fig1 ]        in a probabilistic formulation , equations can be understood as a likelihood model of the data , given coefficients @xmath20 and basis functions @xmath19 : @xmath21 the prior over complex coefficients assumes independence between subspaces and promotes sparse solutions i.e. solutions with most amplitudes close to @xmath22 : @xmath23 where @xmath24 is a normalizing constant .",
    "function @xmath25 penalizes large amplitude values .",
    "since amplitudes are always non - negative , distribution @xmath26 is assumed to be exponential i.e. @xmath27 . due to this assumption , the model attempts to form a data representation keeping complex amplitudes maximally independent across subspaces , while still allowing dependence between coordinates @xmath28 which determine position within each subspace .",
    "the posterior over coefficients @xmath20 becomes @xmath29    this model was introduced in @xcite as a model of natural image patches .",
    "assuming @xmath30 and @xmath31 , it is equivalent to 2-dimensional independent subspace analysis(isa ) @xcite .",
    "figure [ fig1 ] a ) depicts four exemples of complex basis functions learned by from natural , speech sounds .",
    "in addition to temporal plots in cartesian ( first row ) and polar ( second row ) coordinates each basis function is also depicted in the frequency domain ( third row ) .",
    "real ( @xmath11 - black lines ) and imaginary ( @xmath12 - gray lines ) parts of basis functions do not resemble each other and are not temporally localized , capturing the non - local strucutre of speech sounds .",
    "the sparse coding model described in the previous subsection , does not constrain the basis functions in any way .",
    "they are allowed to vary freely during the learning process .",
    "as visible on figure [ fig1 ] a ) , an unconstrained adaptation to natural sound corpus yields complex basis functions invariant to numerous stimulus aspects such as frequency or time shifts , not necessarily phase .    learning a structured dictionary",
    "requires therefore placing priors over basis functions , which favour solutions of desired properties such as phase - invariance .",
    "real and imaginary parts of a phase - shift invariant basis function , have equal , unimodal frequency spectra and both span the same temporal interval .",
    "additionally , the imaginary part should be shifted in time a quarter of the cycle with respect to the real one .    before proposing a prior promoting such solutions",
    ", one should note that each basis function @xmath32 can be represented in polar coordinates as @xmath33 where @xmath34 and @xmath35 denote instantaneous phase and amplitude , respectively .",
    "angular frequency can be defined as a temporal derivative of instantaneous phase . if phase dynamics are highly variable and non - monotonic over time , real and imaginary components of this signal have non - identical spectra and/or their frequencies change in time ( see figure [ fig1 ] a ) , second and third rows ) . on the other hand , by enforcing phase @xmath35 to change smoothly and monotonically , one should obtain real and imaginary parts with matching frequency spectra . in the limiting case ,",
    "when phase is a linear function of time , real and imaginary parts oscillate in the same frequency and are in a quadrature phase relationship .",
    "furthermore , vectors which span a phase - shift invariant subspace should have the same temporal support , implying that the complex amplitude should also vary slowly in time .    in order to learn a phase - shift invariant representation of natural sounds ,",
    "the present work proposes a prior over basis functions of the following form : @xmath36 function @xmath37 introduces the penalty proportional to the variance of amplitude s temporal derivative : @xmath38 where @xmath39 .",
    "it promotes basis functions with a slowly - varying envelope , highly correlated between consecutive time steps .",
    "phase prior @xmath40 is defined by function @xmath41 of the following form : @xmath42 where @xmath43 and @xmath44 denotes the sign function . similarly to @xmath37 it promotes temporal slowness of phase .",
    "the additional factor @xmath45 enforces @xmath46 to be larger than @xmath47 . in this way",
    ", it prevents phase from changing direction and causes it to be a non - increasing function of time .",
    "one could also enforce this by bounding the phase derivative from above : @xmath48 .",
    "this method would however require the hand tuning of the @xmath49 parameter .",
    "the posterior over basis functions given a data sample @xmath50 and its representation @xmath20 becomes : @xmath51    where the likelihood model @xmath52 is defined by equation .",
    "taken together , prior @xmath53 biases the learning process towards temporally localized basis functions with real and imaginary parts of the same instantaneous frequency .",
    "exemplary complex features learned with introduced priors are depicted on figure [ fig1 ] b ) . compared with unconstrained subspaces from figure [ fig1 ] a ) , their amplitudes are smooth , and their phases change monotonically .",
    "moreover , frequency spectra of @xmath11 and @xmath12 align well .",
    "such bases form a phase invariant representation of the data .",
    "inference i.e. estimation of coefficients @xmath20 given a data sample @xmath50 is performed by finding a maximum a posteriori estimate ( map ) .",
    "this corresponds to finding a mode of the posterior distribution and can be computed via a gradient descent on the negative log - posterior ( for gradient derivations please refer to the supplementary material ) : @xmath54 + \\lambda   \\sum_{i=1}^{n/2 } s(a_i)\\ ] ]    learning of basis functions @xmath19 is performed in two steps as in@xcite .",
    "firstly , coefficients @xmath20 are inferred given the data sample @xmath50 . secondly ,",
    "using inferred @xmath20 values , a gradient update is performed on basis functions shifting solutions towards the mode of the posterior distribution .",
    "this is equivalent to minimization of the following energy function : @xmath55 + \\gamma \\sum_{i=1}^{n/2}s_{\\phi}(a_i)+ \\beta \\sum_{i=1}^{n/2}s_{a}(a_i)\\ ] ] where @xmath56 and @xmath57 are free parameters which control the strength of each prior .",
    "they are set to be smaller than one to prevent from dominating over the error term .",
    "this prevents the model from learning trivial or non - data matching solutions ( e.g. very low frequencies ) .",
    "the two steps are iterated until the algorithm converges . after every learning iteration",
    ", real and imaginary vectors spanning each subspace are orthogonalized by gram - schmidt orthogonalization .",
    "the error term in the equation as well as amplitude and phase penalty terms @xmath58 and @xmath40 usually have very different numerical values .",
    "for this reason , at every iteration of the learning process each term contributing to the gradient of function @xmath59 was normalized to the unit length .",
    "prior - related terms are then multiplied by smaller than @xmath60 , strength - controlling constants @xmath61 $ ] and @xmath62 $ ] . in this way",
    ", gradient information was used to find an appropriate direction in the search space while preventing terms with larger numerical values from dominating the search .",
    "natural sound statistics were analysed by learning complex dictionaries and analysing properties of obtained basis functions .",
    "complete and twice overcomplete representations were learned with and without basis function priors .",
    "this resulted in the total number of four dictionaries .",
    "due to space constraints , entire dictionaries are visualized in time and frequency domain in the supplementary material .    all models were trained using a speech corpus from the international phonetic database handbook @xcite .",
    "this dataset contains sounds of human speakers telling a story in @xmath63 different languages .",
    "speech comprises a variety of acoustic structures , both harmonic and non - harmonic .",
    "one should note that it may not perfectly reflect environmental sound statistics , however it has been used before as a proxy for natural sounds @xcite .",
    "all sound files were down sampled to @xmath64 hz from their original sampling rates . for training , @xmath65 intervals",
    "were randomly sampled from all recording files .",
    "each interval was @xmath66 samples long which corresponds to @xmath67 ms . prior to training",
    ", @xmath68 principal components of the data , explaining @xmath69 of total variance , were rejected .",
    "this corresponds to low - pass filtering the data with the @xmath70 hz cutoff frequency .",
    "the sparsity control parameter @xmath71 was set to @xmath72 value .",
    "after multiple experiments , the prior strengths @xmath73 and @xmath74 were set to @xmath75 and @xmath72 , respectively .    [ fig2 ]        spectra of real and imaginary parts",
    "were compared by plotting their peaks against each other , and computing correlations .",
    "spectral peaks of unconstrained basis functions do not match well ( figure [ fig2 ] a ) , black circles ) .",
    "they are broadly scattered around the diagonal in both - complete and overcomplete case .",
    "each such complex basis function , consists of two vectors of different frequency .",
    "this is in contrast to basis functions learned with priors ( gray triangles ) , which are concentrated around diagonals , with only a few exceptions .",
    "this means that spectra of their real and imaginary parts have the same frequency peak .    to go beyond peak comparison , and to test",
    "how well basis functions aligned in the frequency domain , correlations between their spectra were computed .",
    "correlation equal to @xmath60 means that real and imaginary parts covaried together , and strongly overlapped , while a low correlation value implies highly different spectra .",
    "normalized correlation values were histogramed and are depicted on figure [ fig2 ] d ) . a clear difference between prior based ( gray lines ) and unconstrained ( black lines )",
    "dictionaries is visible .",
    "correlation distributions of the latter ones are quite broad ( although in the overcomplete case a stronger peak close to @xmath60 is present ) and include all possible values .",
    "spectrum correlations of prior based basis functions , are in turn , strongly concentrated around the maximal value i.e. @xmath60 implying similarity of real and imaginary parts .",
    "spectral breadth of basis functions was assessed by computing the concentration index ( ci ) i.e. ratio between the peak value of the spectrum and the total spectral power .",
    "ci quantifies how well are the basis functions localized in the frequency domain .",
    "this measure was used instead of computing bandwidth , since spectra of some basis functions consisted of two or more isolated peaks and bandwidth is not well defined in such cases .",
    "cis of real and imaginary vectors are plotted against each other on figure [ fig2 ] b ) .",
    "unconstrained basis functions ( black circles ) tend to cluster along the diagonal for low ci values and for higher ones they diverge .",
    "this means , that if either - real or imaginary vector has a pronounced , strongly localized peak in the frequency spectrum other one will be more broad .",
    "prior - based basis functions lie along the diagonal , meaning that spectra of their components have similar degree of concentration .",
    "they are , however , much more broad than unconstrained basis functions , with cis not exceeding the @xmath75 value .",
    "it has been suggested that statistical models of natural sounds capture harmonic frequency relationships @xcite .",
    "the harmonic structure of natural sounds gives rise to non - local correlations in the frequency domain , which is different from local pixel correlations of natural image patches . to test whether cross - frequency couplings learned by the unconstrained model",
    "reflects harmonic strucutre of speech the following analysis was performed .",
    "firstly each real and imaginary pair of vectors was converted into frequency domain .",
    "then , for each pair spectral peaks were extracted ( a frequency was defined as a peak if it contained more than @xmath72 of the total spectral power ) .",
    "ratios of the minimal peak value to remaining values were computed .",
    "ratio histograms are depicted of figure [ fig3harm ]    [ fig3harm ]     are marked black . ]",
    "prior based features contained mostly a single peak equal for real and imaginary vectors which is reflected by concentration of nearly all peak ratios at @xmath60 .",
    "unconstrained features revealed different structure .",
    "sharp histogram peaks are visible directly at or very close to multiplcations of @xmath76 ( one should compare this figure to figure 4 b in @xcite ) .",
    "this is an indication that indeed , the unconstrained model has learned harmonic frequency relationships .",
    "this can be expected , since real and imaginary parts of complex valued features capture mutually dependent data aspects .    to evaluate properties of basis functions in the temporal domain , variability of normalized amplitudes",
    "was computed , according to the equation .",
    "results are plotted against peak frequency of the real part on figure [ fig2 ] c ) . in both - complete and overcomplete case ,",
    "the unconstrained basis functions ( black circles ) reveal a frequency dependence . for higher frequencies ,",
    "amplitudes vary more strongly , although around @xmath77 khz variability decreases again . amplitudes of features learned in the presence of priors is much lower , as expected . the slow amplitude prior quenches the variability almost to @xmath22 , with a slight raise observable in higher frequency regimes .",
    "[ fig3 ]        in order to understand how learned basis functions tile the time - frequency plane , wigner distributions of each vector were computed .",
    "wigner distributions describe spectrotemporal energy distribution of temporal signal . in the next step ,",
    "equiprobability contours corresponding to @xmath78 probability value for real and @xmath79 for imaginary parts were plotted ( figure [ fig3 ] , black and blue contours respectively ) .",
    "such representation of temporal basis functions on the time frequency plane was introduced by @xcite .",
    "if both vectors spanning each subspace were indeed phase invariant , their equiprobability contours should lie within each other .",
    "figure [ fig3 ] shows , that this is rarely the case for unconstrained basis functions ( first column ) . while they tile the time - frequency plane uniformly , corresponding real and imaginary parts often lie far apart , modelling different regions .",
    "prior - constrained basis functions can not vary their real and imaginary parts independently during the learning process .",
    "while unconstrained bases increase their temporal support with decreasing frequency , ones learned with priors are strongly localized in time , independent of their spectrum . in most cases ,",
    "corresponding real and imaginary vectors occupy the same area , however a tendency is visible among imaginary components of low - frequency features to be elevated with respect to their real counterparts along the frequency axis .",
    "an interesting effect is visible in the region between @xmath76 and @xmath80 khz .",
    "there , constrained basis functions become broadband and span large frequency intervals . in some cases",
    "the bandwidth reversal occurs .",
    "this is visible as  banana - like  shapes .",
    "the empty regions above and below , are covered by equiprobability contours corresponding to probabilities lower than @xmath78 , which are not visible on the plot .",
    "these structures reflect temporal frequency variation of the basis functions ( see figure 1 b , second row - phases are monotonic , but rather piecewise linear functions of time ) .",
    "a possible explanation of their emergence is that real and imaginary vectors tend to diverge from each other ( as in the unconstrained model ) , but the prior forces them to stay close on the time frequency plane .",
    "interestingly a qualitative change in wigner distribution shapes was observed in this frequency regime by abdallah and plumbey @xcite , who studied independent components of natural sounds .",
    "such behaviour may imply , that data drives real and imaginary parts of basis functions to span distant frequencies , while priors  keep them together  .",
    "figure [ fig3 ] a confirms that constrained and unconstrained representations differ strongly in their spectro - temporal structure .",
    "as mentioned before each complex basis function forms an invariant representation of some data feature i.e. varying phase within subspace spanned by vectors @xmath81 and @xmath82 generates features of different quality , while keeping the complex amplitude constant . to summarize stimulus - invariances captured by every dictionary ,",
    "basis functions were assigned to four classes of invariance , according to the following criteria :    1 .",
    "spectral invariance - frequency peaks of real and imaginary vectors differ by more than @xmath83  hz 2 .",
    "bandwidth invariance - real and imaginary parts have the same frequency peak , but different concentration index ci 3 .",
    "time invariance - frequency peaks and concentrations match , real and imaginary vectors are shifted in time more than a period of peak frequency 4 .",
    "phase invariance - same as above , with a shift smaller than the peak frequency period .",
    "figure [ fig3 ]",
    "b ) depicts how many basis functions from each dictionary fall within each invariance class .",
    "unconstrained representations capture mostly spectral invariances ( black color ) .",
    "temporal invariances ( white ) are the second largest class , while phase invariant features ( light gray ) constitute a minor fraction @xmath84 in complete and @xmath85 in the overcomplete case .",
    "in contrast , representations learned with phase and amplitude priors capture mostly phase invariances - @xmath86 and @xmath87 respectively .",
    "one should note , that spectral invariances learned by constrained models are much less diverse than ones captured by unconstrained ones .",
    "they result mostly from the slight misalignment of spectral peaks - see figure [ fig2 ] a ) .",
    "in order to reassure that the structure of the unconstrained basis functions does not constitute a learning artifact , two control experiments were performed . in the first one ,",
    "a complete dictionary of prior based basis functions was used as an initialization for unconstrained learning .",
    "if features revealing complex invariances were a robust data property not merely a local minimum , the resulting dictionary should differ from the original one .",
    "this is indeed what happened .",
    "figure [ fig5 ] a depicts four randomly selected , prior based basis functions used as initial conditions . figure [ fig5 ] b shows the same basis functions after @xmath88 learning iterations . structure induced by priors has vanished .",
    "spectra of single vectors are more localized , but not necessarily match each other ( see the last basis function ) .",
    "phases and amplitudes vary strongly in time .",
    "this observation confirms that structure of the unconstrained dictionary forms a representation better spanning the data space .",
    "secondly , unconstrained features were learned from natural image dataset ( taken from @xcite ) .",
    "eight randomly selected subspaces are depicted on figure [ fig5 ] c. as expected , they are localized in space and frequency and are in a quadrature phase relationship .",
    "[ fig5 ]     learning iterations .",
    "c ) complex basis functions learned without priors from the image data . ]",
    "in order to compare how well different dictionaries model the underlying data distribution , two different criteria were used .",
    "firstly , performance of a dictionary in a denoising task was measured and secondly relative coding efficiency was compared by estimating the entropy of linear coefficients .",
    "both tests were performed using a testing dataset of @xmath89 samples drawn from the ipa speech corpus .",
    "the test dataset was preprocessed in the same way as the training one .",
    "the ability of different dictionaries to match a typical structure in the data in the presensce of noise was quantified .",
    "this may be also understood as an indirect estimate of the likelihood , since it is known that models of high - likelihood perform well in denoising tasks @xcite .",
    "each vector @xmath50 from the test dataset was blurred with an i.i.d .",
    "gaussian noise with sd @xmath90 .",
    "coefficients @xmath20 were inferred from the noisy sample . in the next step peak signal to noise ratio ( psnr ) defined as @xmath91 ( where @xmath92 is the reconstruction of the data vector given inferred @xmath20 ) was computed .",
    "[ fig4 ]        average psnr for each dictionary is plotted on figure [ fig4 ] a ) . for comparison",
    ", denoising was also performed using a basis consisting of orthogonalized white noise vectors .",
    "surprisingly , all dictionaries adapted to the data showed very similar performance .",
    "priors presence and overcompletness did not affect results much .",
    "despite different basis function shapes dictionaries were able to reconstruct orginal sound chunks , while rejecting the noise .",
    "white noise bases gave , as expected , lower reconstruction quality , which has however risen in the overcomplete case .",
    "such behavior is expected , since the model had more degrees of freedom to match the data structure .",
    "one way to asess coding efficiency given a fixed dictionary , is to estimate entropies of linear coefficients @xmath93 and @xmath94 @xcite . by shannon s source coding theorem",
    ", the entropy of the data distribution @xmath95 constitutes a lower bound on code - word length : @xmath96 if the true probability distribution @xmath95 is unknown and is approximated by a proxy distribution @xmath97 , the average code length becomes : @xmath98 where @xmath99 is the kullblack - leibler divergence from distribution @xmath95 to @xmath97 .",
    "therefore , representation which yields lower code length should be closer to the true underlying distribution of the data .",
    "for details see @xcite .",
    "coefficient entropies were estimated by creating normalized histograms with @xmath100 bins .",
    "entropy was estimated as @xmath101 , where @xmath102 is a number of counts in @xmath103-th bin and @xmath104 is the number of samples .",
    "one should note , that even though the estimation was strongly biased , the goal was to use entropy as a relative measure of coding efficiency using different representations , not as an absolute one .",
    "average coefficient entropy for all learned dictionaries is plotted on figure [ fig4 ] b ) .",
    "overall , overcomplete representations yielded lower entropies .",
    "it means that models with more sparse causes than dimensions explain the data better and may be surprising when compared with natural image models , where overcomplete representations yield higher coefficient entropies @xcite .",
    "representing the data using phase - invariant basis functions required slightly more bits than using unconstrained ones .",
    "this indicates that even though both representations gave good denoising performance , the unconstrained model may be closer to the true , data generating distribution .",
    "as expected , white noise bases required more bits per coefficient to encode speech sounds and their overall performance was poor .",
    "this work has addressed two lines of research .",
    "firstly natural sound statistics were studied by learning sparse complex valued representations and analysis of obtained features .",
    "it has been demonstrated that in contrary to natural images learned features are invariant to many different stimulus aspects , not only phase .",
    "one should note that phase in short sound waveforms is a very different physical quantity than spatial phase of natural image patches .",
    "it can be expected that temporal structure of air pressure waveforms is going to have different statistical properties than spatial structure of reflected light . the present work shows that intuitions ( phase invariance of sparse , complex dictionaries ) gained from learning representations in one signal domain ( images ) may not transfer to others ( e.g. sound ) .",
    "present findings go along recent results by terashima and colleagues @xcite , who suggested that differences in spatial organization of visual and auditory cortices may reflect different correlation structures of natural sounds and images .    in parallel to analysis of sound statistics , priors promoting phase invariant sparse codes were proposed .",
    "the form of prior which penalizes temporal variability of the signal is long known @xcite .",
    "recently , it was used to learn complex , temporally persistent representations from sequences of natural image patches @xcite . here",
    "slow priors were placed on amplitudes and phases of basis functions , not coefficients .",
    "phase prior includes also an additional term promoting monotonic change of phase in time .",
    "complete and overcomplete speech representations were learned using unconstrained and prior - based models . obtained",
    "basis functions highly differ in shape and spectro - temporal properties . despite differences ,",
    "they perform equally well in a denoising task , and yield similar coefficient entropies .",
    "this implies that prior based dictionaries can be used without quality loss to represent natural sounds in tasks such spatial hearing , where phase information has to be made explicit .",
    "proposed approach to learn complex dictionaries may find applications outside natural scene statistics research .",
    "for instance , phase invariant dictionaries are useful in modelling time - epoched signal , where epochs can be misaligned @xcite .",
    "many other prior forms may be selected to learn structured signal representations .",
    "for instance penalizing variability of the second temporal phase derivative should yield basis functions which are well localized in frequency .",
    "applicability and usefulness of such prior in learning efficient representations of sensory data is a subject for future work .",
    "this work was funded by the dfg graduate college  interneuro ",
    "gradient of equation [ ebf2 ] can be decomposed into three terms : @xmath122 representing the reconstruction error term and phase and amplitude priors consecutively . in polar coordinates , for @xmath123 phase prior gradient is : @xmath124\\ ] ] for boundary conditions",
    "i.e. @xmath125 and @xmath126 , this gradient becomes consecutively : @xmath127            the residue term is most conveniently represented in cartesian coordinates for real and imaginary coefficients @xmath132 , where , as previously , @xmath114 , indicates whether coefficient is real or imaginary : @xmath133"
  ],
  "abstract_text": [
    "<S> complex - valued sparse coding is a data representation which employs a dictionary of two - dimensional subspaces , while imposing sparse , factorial prior on complex amplitudes . </S>",
    "<S> when trained on a dataset of natural image patches , it learns phase invariant features which closely resemble receptive fields of complex cells in the visual cortex . </S>",
    "<S> features trained on natural sounds however , rarely reveal phase invariance and capture other aspects of the data . </S>",
    "<S> this observation is a starting point for the present work . as its first contribution </S>",
    "<S> , it provides an analysis of natural sound statistics by means of learning sparse , complex representations of short speech intervals . </S>",
    "<S> secondly , it proposes priors over the basis function set , which bias them towards phase - invariant solutions . in this way , a dictionary of complex basis functions can be learned from the data statistics , while preserving the phase invariance property . </S>",
    "<S> finally , representations trained on speech sounds with and without priors are compared . </S>",
    "<S> prior - based basis functions reveal performance comparable to unconstrained sparse coding , while explicitely representing phase as a temporal shift . </S>",
    "<S> such representations can find applications in many perceptual and machine learning tasks . </S>"
  ]
}