{
  "article_text": [
    "a typical statistical pattern recognition system usually consists of four modules : a sensor module , a preprocessing module , a feature extraction module , and a classification module @xcite . among these four modules ,",
    "the feature extraction module plays a critical role in the success of the system .",
    "the objective of feature extraction is to find a specific representation which encodes relevant information from input data , so that not only is the computational complexity of subsequent classifiers reduced but also the useful features can be used to perform the desired tasks @xcite .",
    "usually , the real - world data can be represented as a high - dimensional vector @xcite .",
    "for instance , an image of size @xmath0 can be viewed as a point in a @xmath1 dimensional feature space .",
    "however , the high dimensionality of data prevents from direct usage of learning techniques in a high - dimensional space .",
    "a common way to deal with this problem is to make use of feature extraction techniques , or more specifically , use dimensionality reduction techniques @xcite to project the original high - dimensional data onto a low - dimensional space .",
    "recently , biometric recognition , which refers to the task of automatic identification of individuals based on their physiological and/or behavioral characteristics , has received much attention due to its wide range of applications , such as law enforcement , access control , and video surveillance @xcite .",
    "a number of biometrics have been proposed in recent years ( e.g. , @xcite ) .",
    "two kinds of biometric characteristics are usually used , i.e. , physiological characteristics ( such as face , palmprint , and ear ) and behavioral characteristics ( such as gait , signature ) . despite decade - long efforts , building an automatic and robust biometric recognition system",
    "remains a challenging problem due to variations in illumination , pose , occlusion , etc .    during the fast few decades",
    ", numerous feature extraction methods have been put forward to deal with the biometric recognition problems .",
    "for example , qian et al .",
    "@xcite proposed the discriminative histograms of local dominant orientation ( d - hldo ) method for biometric image feature extraction .",
    "shekhar et al .",
    "@xcite developed a joint sparse representation for robust multimodal biometrics recognition . beside feature extraction ,",
    "feature selection is also extensively investigated to discover the knowledge related to biometric data .",
    "different from feature extraction , which generates new features from functions of the original features , feature selection returns a subset of the features from a large feature pool . boosting @xcite and lasso @xcite have been successfully used to perform feature selection in face detection and recognition .",
    "sun et al .",
    "@xcite proposed an optimization formulation for ordinal feature selection for iris and palmprint recognition .",
    "guo et al .",
    "@xcite presented the feature band selection for the online multispectral palmprint recognition .",
    "ghoualmi et al .",
    "@xcite proposed a feature selection method based on the genetic algorithm for ear authentication .",
    "kumar et al .",
    "@xcite suggested to use feature selection and combination to improve the performance of bimodal biometric system .",
    "until now , a large number of feature extraction methods have been developed .",
    "however , many methods mainly consider the first order statistics of data , which are indeed non - linearly distributed .",
    "even though nonlinear feature extraction methods are introduced to handle the non - linearly distributions , the computational cost of these methods is high . on the other hand , high order statistics which capture the complex statistical relationship of the data",
    "can be beneficial for feature extraction and feature selection , potentially leading to superior performance .    in this paper",
    ", we propose a novel nonlinear feature extraction framework , which takes advantage of the quadratic projection technique . compared with the traditional linear projection technique",
    ", the quadratic projection technique exploits the second order statistics of data .",
    "it is well - known that the quadratic classifiers are optimal for the data under gaussian distributions .",
    "even when the data is not gaussian - distributed , we can still expect quadratic projection to perform better than linear projection under general conditions since more high - order information is taken into consideration in quadratic projection .    more specifically",
    ", we propose a novel nonlinear feature extraction framework based on the quadratic projection technique .",
    "different from the traditional linear projection technique ( which obtains a feature vector based on a linear form ) , the quadratic projection technique uses a quadratic form to extract a feature vector , where each feature is extracted by using the homogeneous polynomial of degree two in a number of original features . in the proposed framework , a set of quadratic matrices",
    "is learned to distinguish each class from all other classes .",
    "mathmatically , we formulate quadratic matrix learning ( qml ) as a standard semidefinite programming ( sdp ) problem .",
    "to solve the scalability of qml , we further develop an efficient algorithm which significantly reduces the computational complexity of the conventional interior - point sdp solvers @xcite .    in this paper",
    ", we will motivate and study this new framework within the context of biometric recognition .",
    "we use biometrics data as a case study to illustrate the effectiveness of the proposed framework .",
    "experimental results on three representative biometric recognition tasks ( including face , palmprint , and ear recognition ) show that the proposed algorithm achieves better performance than the linear projection based and kernel / tensor based feature extraction algorithms .    in summary ,",
    "the main contributions of our work are summarized as follows :    1 .   a novel feature extraction framework based on the quadratic projection technique",
    "is proposed to extract discriminative features , where a set of quadratic matrices is learned .",
    "experimental results on biometric recognition tasks show the effectiveness of the proposed framework .",
    "2 .   we develop an efficient algorithm for quadratic matrix learning ( qml ) via the lagrange duality theory .",
    "our proposed algorithm is much more scalable than the traditional sdp solvers .",
    "the importance of this improvement is that it thereby allows us to apply qml to high - dimensional data .",
    "the rest of this paper is organized as follows .",
    "section 2 describes related work .",
    "section 3 presents the details of the proposed quadratic projection based feature extraction framework , where a novel algorithm is developed for efficient qml .",
    "experimental results on three biometric recognition tasks are given in section 4 .",
    "finally , section 5 provides the concluding remarks .",
    "feature extraction can be performed in a linear or nonlinear way . the linear feature extraction based algorithms usually perform a linear mapping of input data onto a low - dimensional feature space . typical algorithms include principal component analysis ( pca ) @xcite , linear discriminant analysis ( lda ) @xcite , locality preserving projections ( lpp ) @xcite , margin fisher analysis ( mfa ) @xcite , class - dependence feature analysis ( cfa ) @xcite , local discriminative gaussians ( ldg ) @xcite , and low rank matrix factorization @xcite .",
    "recently , a large number of distance metric learning algorithms @xcite have been proposed to perform linear feature extraction .",
    "these algorithms are computationally efficient .",
    "however , their performance can degrade in cases with non - linearly distributed data existing in many real - world applications .",
    "nonlinear feature extraction algorithms are based on the intuition that input data lies on a nonlinear manifold in a high - dimensional space .",
    "a direct and natural way to extend the linear feature extraction algorithms to nonlinear cases is to take advantage of the kernel technique @xcite , which does not have to explicitly compute the nonlinear mapping between the input space and the feature space .",
    "the kernel - based nonlinear algorithms find nonlinear projections by nonlinearly mapping data onto a higher - dimensional feature space , but it still performs linear projections in the new feature space .",
    "other types of nonlinear algorithms include manifold learning techniques , such as isomap @xcite , locally linear embedding ( lle ) @xcite , and local tangent space alignment ( ltsa ) @xcite .",
    "nevertheless , many manifold learning algorithms suffer from the so - called out - of - sample problem @xcite , i.e. , these algorithms provide mapping only for training data but not for unseen test data .",
    "the multilinear subspace learning ( msl ) techniques @xcite have also been developed for finding a low - dimensional representation of high - dimensional tensor data through direct mapping .",
    "there are three types of multilinear projections according to the forms of input and output of a projection @xcite , i.e. , vector - to - vector projection ( vvp ) , tensor - to - tensor projection ( ttp ) , and tensor - to - vector projection ( tvp ) .",
    "although the msl algorithms preserve the structure in original data by operating on natural tensor representations , most of these algorithms are based on iterative schemes and usually converge to local solutions .",
    "generally speaking , the distributions of real - world data ( such as biometric data ) show highly non - linear and non - convex .",
    "therefore , the non - linear feature extraction is beneficial for the subsequent classification .",
    "however , the kernel extension is computationally expensive , while the multi - linear based algorithms often offer local optimal solutions . in this paper",
    ", we develop a novel nonlinear feature extraction framework , which leverages the quadratic projection technique to encode high order statistics of the biometric data .",
    "note that both the proposed algorithm and the msl algorithms ( using 2d matrix data ) @xcite aim to optimize a matrix .",
    "however , in the proposed framework , the optimized matrix is a quadratic matrix required to satisfy the positive semidefinite constraint ( usually not required in the msl algorithms ) .",
    "furthermore , compared with the msl algorithms which attempt to obtain one matrix to distinguish all classes , the proposed framework obtains multiple quadratic matrices , where each quadratic matrix is trained to separate one class from the other classes .",
    "in this section , we present a quadratic projection based feature extraction framework .",
    "we begin with an overview of the proposed framework in section 3.1 .",
    "the optimization problem of quadratic matrix learning ( qml ) is formulated in section 3.2 .",
    "an efficient algorithm , termed dualqml , to solve the problem of qml is derived in section 3.3 .",
    "we give the complete algorithm in section 3.4 .",
    "finally , we discuss some important issues about the proposed algorithm in section 3.5 .    before formally presenting the proposed algorithm , we describe some notations used in this paper .",
    "a column vector is represented by a bold lower case letter and a matrix is represented by a bold upper - case letter . for a positive semidefinite ( p.s.d . )",
    "matrix @xmath2 , we denote it as @xmath3 .",
    "given a symmetric matrix @xmath4 and its eigen - decomposition @xmath5 , where @xmath6 is an orthonormal matrix and @xmath7 is a diagonal matrix , we define the positive part of @xmath4 as @xcite : @xmath8\\textbf{u}^{\\rm{t } } , \\nonumber\\end{aligned}\\ ] ] and the negative part of @xmath4 as : @xmath9\\textbf{u}^{\\rm{t } } , \\nonumber\\end{aligned}\\ ] ] where , @xmath10 is a square matrix in which all elements are zeros . @xmath11 and @xmath12 compute the element - wise maximum and minimum of two matrices , respectively .",
    "therefore , the positive ( or negative ) part of * a * is composed of the positive ( or negative ) eigenvalues and the associate eigenvectors . obviously , @xmath13 holds .",
    "traditional linear feature extraction algorithms project high - dimensional data onto a lower - dimensional feature space by using a linear projection matrix , which computes the first order statistics of data .",
    "however , in many real - world applications , higher order statistics of data are more beneficial for feature extraction . in this subsection",
    ", we propose a quadratic projection based feature extraction framework , which exploits the homogeneous quadratic polynomials in the variables for feature extraction .",
    "inspired by cfa @xcite , where a correlation filter is designed for each class , we propose a feature extraction framework where a quadratic matrix is learned for each class . the proposed framework contains two main steps .",
    "first , a set of quadratic matrices is obtained , where each quadratic matrix is learned to separate a specific class from all other classes during the training stage .",
    "then , all the learned quadratic matrices are used to perform feature extraction .",
    "more specifically , each component of a feature vector is generated by applying a quadratic projection ( defined as the form of @xmath14 ) to an input sample image @xmath15 ( @xmath16 ) according to a specific quadratic matrix @xmath17 ( @xmath18 is a symmetric matrix ) .",
    "as we can see , the key step of the proposed feature extraction framework is qml by which a quadratic matrix can be learned . in the following subsections",
    ", we will describe the problem of qml in detail .",
    "suppose that we have a set of sample images @xmath19 , and given a class @xmath20 , the sample images can be classified as : @xmath21 where @xmath22 is the image set consisting of the intra - class sample images of the @xmath20-th class , while @xmath23 is the image set consisting of the extra - class sample images of the @xmath20-th class .",
    "let us write the quadratic matrix learned for the @xmath20-th class as @xmath24 .",
    "the objective of qml is to find a matrix so that the projected values of the data belonging to the @xmath20-th class and the other classes are well - separated after quadratic projections .",
    "therefore , a simple way to define a criterion for qml is to require that the quadratic projections of the samples in @xmath23 are minimized while at the same time , the quadratic projections of the samples in @xmath22 should be as large as possible .",
    "this yields the following optimization criterion : @xmath25 notice that @xmath24 is required to be a p.s.d matrix , which means the quadratic projections ( constituting the components in the extracted feature vector ) of samples are not less than 0 .",
    "this is consistent with the correlation operation in cfa , where the correlation outputs ( corresponding to the linear constraints during the design process of correlation filters ) are non - negative .",
    "in fact , non - negative constraints of feature vectors are also beneficial for metric comparison @xcite .",
    "note that the choice of the constant on the right hand side of ( 1 ) is arbitrary .",
    "this is due to the fact that changing the constant 1 to any other positive constant @xmath20 will result in @xmath26 being replaced by @xmath27 .",
    "however , one problem with ( [ eq:1 ] ) is that it may not be suitable to solve real - world biometric recognition tasks , where data could be noisy and include a limited number of training samples as well .    to enhance the generalization capability and robustness of qml",
    ", we propose a new objective function by considering the regularization principle .",
    "it is well - known that regularization plays a critical role in many machine learning algorithms to prevent overfitting @xcite .",
    "therefore , we propose a general regularization formulation of qml as follows : @xmath28 where @xmath29 represents the frobenius norm of @xmath24 , if @xmath30_{m\\times m}$ ]",
    ".    there are two items in the objective function of ( [ eq:2 ] ) .",
    "the first item serves as a regularization term which prevents the value of any element within the matrix @xmath24 from being too large .",
    "the second item stands for the summed projected values corresponding to the extra - class samples .",
    "@xmath31 is a regularized parameter to balance the two items .",
    "in addition , the first constraint in ( [ eq:2 ] ) makes sure that each sample from the @xmath20-th class yields an output whose value is at least larger than 1 .",
    "thus , a discriminative quadratic matrix is learned such that the projected values corresponding to the intra - class samples and extra - class samples are well - separated .    to solve the above optimization problem , the second item and the first constraint in the objective function of ( 2 ) can be respectively rewritten as : @xmath32 and @xmath33 where the product ` @xmath34 ' is a point - wise matrix multiplication operator , and @xmath35 represents a trace operator that computes the sum of the diagonal elements of a matrix . @xmath36 and @xmath37 can be represented as @xmath38 and @xmath39 , respectively .",
    "thus , problem ( 2 ) can be rewritten as : @xmath40    problem ( [ eq:5 ] ) is a convex optimization problem , since the objective function is convex ( this can be easily proved by using the second - order convexity conditions ) , the inequality constraints are linear , and the p.s.d .",
    "constraint is convex . as a matter of fact , problem ( [ eq:5 ] ) can be formulated as a standard sdp problem @xcite , using a standard trick which converts a quadratic objective function into a linear matrix inequality and a linear objective function .",
    "hence , it can be directly solved by using the off - the - shelf sdp solvers @xcite .",
    "however , the conventional interior - point sdp solvers suffer from a high computational complexity of @xmath41 , where @xmath42 is the dimensionality of data , and it can only deal with the problems involving up to a few hundreds of variables @xcite .",
    "this hampers the application of the conventional sdp solvers to high - dimensional data , such as data in biometric recognition ( usually involving thousands of variables ) .      in this section ,",
    "we propose to use the lagrange duality theory @xcite to make ( [ eq:5 ] ) applicable to high - dimensional data .",
    "we introduce a dual multiplier @xmath43 associated with the inequality constraints , and a matrix @xmath44 associated with the p.s.d .",
    "constraint in the primal problem ( 5 ) .",
    "according to the lagrange duality theory , a non - negative dual variable is associated with an inequality constraint in the primal problem .",
    "therefore , the dual variable @xmath43 should satisfy the non - negative property .",
    "in addition , due to the fact that the p.s.d .",
    "cone is self - dual , @xmath44 should be a p.s.d . matrix .",
    "hence , the lagrangian of ( [ eq:5 ] ) can be written as follows : @xmath45 with @xmath46 and @xmath47 . here",
    "@xmath46 denotes that all elements in @xmath43 are non - negative , and @xmath48 represents the @xmath49-th element of @xmath43 .",
    "the karush - kuhn - tucker ( kkt ) conditions @xcite are necessary and sufficient conditions for any pair of primal and dual optimal points of a convex problem .",
    "any points that satisfy the kkt conditions are primal and dual optimal , and thus have zero duality gap .",
    "one of the kkt conditions is that the gradient of the lagrangian with respect to the primal variable vanishes at the primal optimal point . therefore , we can minimize the lagrangian over @xmath24 by setting the first derivative of ( 6 ) with respect to @xmath50 to zero",
    "thus , we obtain @xmath51 where @xmath52 and @xmath53 are respectively the primal and dual optimal solutions .",
    "therefore , ( [ eq:9 ] ) is one kkt condition which enables us to recover the primal variable from the dual ones .",
    "based on the above expressions , the dual function which is defined as the minimum value of the lagrangian over the primal variable can be obtained as follows : @xmath54 therefore , we obtain the lagrange dual of ( [ eq:5 ] ) as : @xmath55 the lagrange dual problem ( [ eq:10 ] ) is always convex , since the objective function to be maximized is concave and the constraints are convex .",
    "so , both the primal and dual problems are convex . on the other hand , due to the convexity of the primal problem , and strict convexity of the lagrangian with respect to the primal variable @xmath50 ,",
    "the primal problem is strictly feasible ( i.e. , there exist @xmath56 which satisfies the linear inequalities in ( 5 ) ) .",
    "slater s condition @xcite is satisfied and thus strong duality between ( [ eq:5 ] ) and ( [ eq:10 ] ) holds .",
    "therefore , the objective values of the two problems meet at optimality and we can obtain the solution of the primal problem by solving the dual problem .",
    "problem ( [ eq:10 ] ) still has the p.s.d .",
    "constraint and it is not obvious to see how to solve it in an efficient way other than using off - the - shelf sdp solvers .",
    "however , by taking the idea of alternating optimization technique , we can derive an efficient solution . to be specific ,",
    "we first fix @xmath43 and solve the optimization problem with respect to @xmath44 . then",
    ", we fix @xmath44 and solve the optimization problem with respect to @xmath43 .    given a fixed @xmath43 , problem ( [ eq:10 ] ) can be rewritten as : @xmath57 the above optimization problem finds a p.s.d .",
    "matrix so that @xmath58 is minimized .",
    "this problem has a closed - form solution , which can be written as : @xmath59 where @xmath60 is the positive part of @xmath61 .",
    "thus , according to the definition of @xmath62 , @xmath63 is a p.s.d .",
    "matrix .    since the optimal @xmath64 is expressed as a function with respect to @xmath43 , the optimization problem ( [ eq:10 ] ) can be simplified into a problem where only @xmath43 needs to be optimized .",
    "therefore , we can simplify ( [ eq:10 ] ) as : @xmath65 problem ( [ eq:13 ] ) does not involve any matrix variables and it only has a simple constraint on @xmath43 .",
    "therefore , we can use the first - order newton algorithm , such as l - bfgs - b @xcite , to solve the problem . to use l - bfgs - b , we only need to compute the gradient of the objective function of ( [ eq:13 ] ) , which is @xmath66 finally , once the optimal @xmath67 is obtained , the optimal @xmath64 can be calculated accordingly .",
    "it is worth mentioning that the computational complexity of the proposed dualqml algorithm is much lower than the conventional sdp solvers during the training stage .",
    "this is because that at each iteration in the dualqml algorithm , the computation of ( 13 ) , which runs the full eigen - decomposition , is only implemented once to obtain all the gradients . in our case , since the number of constraints is much smaller than the dimensionality of data , eigen - decomposition dominates the computational cost during each iteration .",
    "hence , the overall computational complexity is only @xmath68 with @xmath69 being around 30@xmath7050 .",
    "recall that the complexity of the conventional sdp solvers is about @xmath41 .",
    "therefore , the computational cost of the proposed dualqml algorithm for training is significantly reduced , especially when the dimensionality of data is high .",
    "as we mention previously , the key step of the quadratic projection based feature extraction framework is to obtain a set of quadratic matrices by solving the problem of qml .",
    "we have shown the elements of the proposed dualqml based feature extraction algorithm in previous subsections . in algorithm 1 , we give the detailed outline of the proposed algorithm for image classification .",
    "next , we discuss a couple of important issues about the proposed algorithm . first , compared with the linear feature extraction algorithms ( such as pca , lda ) , the computational complexity of the proposed algorithm is higher since an iteration scheme is used to obtain the quadratic matrix during the training stage .",
    "nevertheless , the proposed algorithm allows for higher flexibility of the decision boundary due to the usage of a nonlinear feature extraction framework .",
    "second , qml is an asymmetric two - class problem since the number of extra - class samples is usually larger than that of intra - class samples .",
    "methods to tackle the asymmetry problem include the cascade classification structure @xcite , adaboost - based algorithms @xcite , and asymmetric weighting of covariance matrices @xcite .",
    "in contrast , during the formulation of qml , we minimize the sum of quadratic projections of the extra - class samples while constraining the quadratic projections of each intra - class sample to be larger than 1 , which alleviates the overemphasis on extra - class samples .",
    "third , regularization is critical to ensure excellent generalization performance for many algorithms .",
    "for instance , an effective eigenspectrum regularization framework @xcite was developed to extract discriminative features . in this paper",
    ", we use a frobenius norm based regularization term to enhance the generalization and robustness performance of feature extraction , which can lead to scalable and simple optimization by considering the dual formulation .",
    "finally , we note that both distance metric learning ( dml ) @xcite and qml attempt to learn a p.s.d .  matrix .",
    "however , their objective functions are intrinsically different : dml finds a metric for measuring similarity between samples , while qml learns a matrix for feature extraction .",
    "in this section , the performance of the proposed dualqml - based feature extraction algorithm is evaluated on three different biometric recognition tasks .",
    "experimental configurations are presented in section 4.1 .",
    "experiments on face recognition , palmprint recognition , and ear recognition are given in sections 4.2 , 4.3 and 4.4 , respectively .",
    "the computational complexity of different methods is analyzed in section 4.5 .",
    "finally , discussions between different algorithms are shown in section 4.6 .",
    "seven databases , including four face databases , two palmprint databases , and one ear database , are used for evaluation .",
    "we compare the proposed algorithm with several state - of - the - art linear feature extraction algorithms , including the lda @xcite , mfa @xcite , cfa with two correlation filters ( i.e. , otf @xcite , oeotf @xcite ) algorithms , and several nonlinear feature extraction algorithms , including the general tensor discriminant analysis ( gtda ) @xcite , kernel lda ( k - lda ) @xcite , maximal linear embedding ( mle ) @xcite , and the eigenspectrum regularization based kernel lda ( er - kda ) algorithms @xcite . besides",
    ", we also compare with the asymmetric principal component analysis ( apca ) @xcite and eigenfeature regularization and extraction ( ere ) algorithms @xcite , which address the asymmetric data distribution problem and the regularization problem , respectively .",
    "all the images are normalized and cropped to the size of @xmath71 . a series of experiments is designed to compare the performance of all the competing methods under conditions with different numbers of training samples .",
    "specifically , in all the experiments , a subset ( consisting of @xmath42 images per individual ) of each database is randomly taken from the database to form the training set , while the rest of the database is used as the test set . for a fixed value of @xmath42 ,",
    "the experiments with randomly sampled subsets are implemented 30 times .",
    "we report the average error recognition rate and the standard variance of the achieved error rates obtained by each competing algorithm as the final results , where the lowest error recognition rate for each case is formatted in the bold font .",
    "the regularization parameter @xmath31 is tuned by using 10-fold cross - validation , where we set the value of the regularization parameter to be within [ 0.1 , 10 ] . to be specific , the training set is randomly partitioned into @xmath72 equal sized non - overlapping subsets . among the 10 subsets , a single subset is retained as the validation data for testing the model , while the remaining 9 are used as the training data . the cross - validation process is then repeated 10 times . finally , the parameter with the lowest recognition accuracy is chosen ( similar to @xcite ) .",
    "note that the training process of a quadratic matrix is to produce a correlation peak only for the authentic samples from the class of interest , which means that the maximal value criterion , i.e. , the class index of the maximal component in the feature vector , can be used as the classification rule .",
    "thus , the label of a test sample can be given according to @xmath73),\\ ] ] where @xmath74,\\mathbf{p}[2],\\cdots,\\mathbf{p}[c])^{\\rm{t}}$ ] is the extracted feature vector corresponding to the test sample .",
    "the maximal value criterion , however , does not consider the features in the training set , which is beneficial for classification . in this paper , the nearest neighbor ( nn ) classifier with the cosine similarity is also employed .",
    "therefore , for the proposed dualqml algorithm , we respectively evaluate the method with the cosine similarity for the nn classifier ( denoted as dualqml ) and that with the maximal value criterion for classification ( denoted as dualqml ( max ) ) . for the other competing algorithms , the nn classifier with the cosine similarity",
    "is employed except for apca , where the mahalanobis distance is used ( apca with the mahalanobis distance performs better than that with other distances @xcite ) .      in this section ,",
    "we show the experimental results on face recognition .",
    "four public face databases , including the ar databasealeix / ardatabase.html ] , pie database , feret database , and frgc database , are used for evaluation .",
    "the ar database consists of over 4,000 face images from 126 individuals , including frontal views of faces with varying illumination conditions , facial expressions and occlusions .",
    "the images of most individuals were taken twice at a two - week interval . therefore , there are two sections on the ar database , where each section contains 13 face images and 120 individuals ( including 65 men and 55 women ) participated on both sessions .",
    "the images of these 120 individuals are used in our experiments and only the full facial images are selected here ( those facial images with occlusions are excluded since no attempt is made to handle occluded face recognition for all the competing methods ) . therefore , the selected ar subset contains 120 individuals ( each individual has 14 face images ) .",
    "the pie database contains a large number of pose and illumination conditions along with different facial expressions .",
    "the whole pie database has 41,368 images obtained from 68 individuals , where each individual were recorded under 43 illumination conditions , 13 poses and 3 facial expressions .",
    "because all the competing methods mainly focus on frontal / near - frontal face recognition , we use the frontal images ( with all illumination and facial expression changes ) for each individual .",
    "hence , the selected pie subset contains 68 individuals ( each individual has 46 face images ) .",
    "the ar and pie face databases are used to evaluate the performance of different methods under various illumination and facial expression changes .",
    "several examples on the ar and pie face databases are shown in fig .",
    "[ fig : face ] .",
    "the feret database is a result of the feret program sponsored by the us department of defense .",
    "it contains various facial expressions , illumination conditions , pose variations . to evaluate the performance of the method under small pose variations , we choose the pose subset of feret which contains 1,400 images of 200 subjects ( each subject has 7 images with pose angle ranging from @xmath75 to @xmath76 ) .",
    "the frgc version 2.0 is a large - scale face database established under uncontrolled indoor and outdoor settings . to evaluate the performance of the method under both indoor and outdoor environments , we use 6,000 images of 300 subjects ( 40 images for each subject ) .",
    "the face images in this subset are captured in controlled and uncontrolled conditions with severe illumination variations .",
    "several examples on the feret and frgc face databases are shown in fig .",
    "[ fig : face2 ] .",
    "for all the databases , the values of @xmath42 ( i.e. , the number of images for each individual ) to compose the training set are set to 2 , 4 , 6 , 8 , respectively ( expect for feret where the values of @xmath42 are set to 2 , 4 , 6 , respectively ) .",
    "[ tab : face ]    [ tab : face2 ]    the average error recognition rates and standard variances obtained by all the competing algorithms versus different values of @xmath42 on different face databases are shown in tables [ tab : face ] and [ tab : face2 ] . from the results , we can see that the proposed dualqml - based feature extraction algorithm achieves the best performance .",
    "compared with dualqml ( max ) , dualqml with the cosine similarity improves the error rates , which demonstrates the advantages of using the cosine similarity measure as a metric .",
    "due to the usage of the eigenspectrum regularization , er - kda and ere obtain lower error recognition rates compared with the linear - based algorithms , such as lda , mfa and cfa - otf .",
    "in contrast , by exploiting the quadratic form , dualqml makes feature extraction more effective and discriminative . in summary",
    ", dualqml shows more effectiveness for feature extraction in the application of face recognition than the other competing algorithms .",
    "note that several algorithms ( such as ere and dualqml ) also achieve the good performance on the ar , pie and feret databases when @xmath77 . however , these algorithms obtain higher error rates on the frgc database which is captured under uncontrolled conditions .",
    "therefore , how to further improve the performance of the feature extraction algorithms for databases under uncontrolled environments needs more investigation .",
    "both cfa ( based on correlation filters ) and the proposed algorithm ( based on quadratic matrices ) distinguish one specific class from all other classes for one projection axis .",
    "however , the design of correlation filter usually uses the equality constraints while the optimization problem of qml adopts the inequality constraints , which effectively improve the generalization ability of the learned quadratic matrix .      in this section ,",
    "we conduct experiments on palmprint recognition .",
    "the polyu palmprint database @xcite contains 7,752 gray - scale images of 386 different palms .",
    "the casia palmprint database @xcite contains 5,502 palmprint images captured from 312 subjects .",
    "we use the two databases for evaluation .",
    "the values of @xmath42 to compose the training set are set to 2 , 4 , 6 and 8 , respectively .",
    "several examples of the palmprint images in the database are shown in fig .",
    "[ fig : palmprint ] .        [ tab : palm ]    the experimental results are shown in table [ tab : palm ] .",
    "we can see that the dualqml - based feature extraction algorithm achieves the lowest error recognition rates among all the competing algorithms . in this experiment , lda achieves high error recognition rates .",
    "this is due to the fact that the linear projection technique extracts less discrimination information than the nonlinear projection one in dealing with variations of palmprints .",
    "specifically , er - kda and ere achieve lower error recognition rates compared with lda , mfa , and cfa - otf .",
    "the performance obtained by cfa - oeotf is better than that obtained by cfa - otf due to the fact that cfa - oeotf emphasizes the separation of intra - class and extra - class samples , while cfa - otf focuses on the minimization of the correlation energy .",
    "gtda considers an image as a tensor ( i.e. , a matrix ) , so that the internal geometric structure is kept . however , gtda is still based on linear projections of data .",
    "although apca addresses the issue of asymmetric data distribution , it might not be effective to extract a compact feature set for classification . in comparison",
    ", dualqml learns different quadratic matrices for different classes .",
    "even though the possibility of similar classes in the training set exists , the trained models of similar classes are largely different to each other .",
    "therefore , dualqml can extract more discriminative features than the other competing algorithms .      in this section ,",
    "we use the iit delhi ear database @xcite for ear recognition .",
    "the database consists of the images of 212 subjects with 754 ear images ( each subject has at least three ear images ) .",
    "the whole database is used for evaluation .",
    "since some subjects in this database only have three ear images per subject , the values of @xmath42 to compose the training set are set to 2 and 3 , respectively .",
    "several examples of the two ear images in the database are shown in fig .",
    "[ fig : ear ] .",
    "[ tab : ear ]    the experimental results are given in table [ tab : ear ] .",
    "the proposed dualqml algorithm achieves the best results , with at least @xmath702% improvements on the error rates than all the other algorithms .",
    "especially , apca and lda get the worst error recognition rates , which are much higher than the proposed dualqml .",
    "this validates that dualqml is more effective for feature extraction than apca and lda .",
    "the error recognition rates obtained by gtda and k - lda are higher than er - kda .",
    "this is because that er - kda considers the information in both the range space and the null space .",
    "ere achieves the error rates comparable to dualqml due to an effective eigenspectrum model to alleviate problems of instability and overfitting when the number of training samples is not large . both mle and dualqml are the nonlinear feature extraction methods .",
    "however , mle uses the combination of local linear models , which requires a large number of training samples .",
    "in contrast , dualqml considers the regularization principle to effectively handle the situation when data contain a limited number of training samples .",
    "we give the computational time comparisons between the proposed dualqml method and several representative feature extraction methods , including apca , lda , k - lda , er - kda .",
    "all the computational time is reported on a workstation with 2 intel xeon e5620 ( 2.40ghz ) cpus ( only one core is used ) on the matlab platform .",
    "table 5 shows the total time spent on the training and the average time for testing a single image on the ar database ( when @xmath78 ) .",
    "[ tab : time ]    the computational time of dualqml used for training is higher than that the of other methods .",
    "this is because the iterative procedure is used to obtain the quadratic matrices by considering the positive semidefinite constraint . however , the computational time of dualqml used for test is faster than the kernel - based nonlinear algorithms , such as k - lda , er - kda ( note that the time complexity of dualqml for test is @xmath79 , where @xmath80 is the number of classes and @xmath81 is the input dimensionality , while that of the kernel - based nonlinear projection based algorithms is @xmath82 , where @xmath83 is the reduced dimensionality and @xmath84 is the number of data points ) .",
    "the proposed dualqml achieves lower error rates compared with the competing algorithms on different biometric tasks . on the other hand ,",
    "the average test time of the proposed algorithm is about 1 seconds per image . as the training stage",
    "is usually performed offline , the computational complexity of the proposed method will not limit its applications to real - world tasks .",
    "there are two reasons to explain why the proposed dualqml algorithm shows a better performance than the state - of - the - art algorithms , such as lda , mfa , cfa , k - lda , mle , and er - kda :    \\1 ) the problem of dualqml is cast as a constrained optimization framework , which tries to optimize the separation between the extra - class samples and intra - class samples .",
    "lda and mfa try to find a global projection that can maximize the between - class scatter and minimize the within - class scatter simultaneously .",
    "cfa obtains a linear projection that can discriminate one class from the other classes .",
    "both k - lda and er - kda techniques extend lda to nonlinear projections based on the kernel technique .",
    "mle aligns local linear models in a global coordinate space .",
    "most methods attempt to learn a projection that shrinks distances between the same classes and expands distances between different classes in a global sense .",
    "however , the local structures in each class might not be well learned by these methods @xcite .",
    "in contrast , the proposed algorithm explicitly encourages unconstrained projected value for each sample of the class of interest , which can better adapt to different class distributions .",
    "\\2 ) dualqml extracts features in a class - specific manner while other algorithms extract features in a generic way . for each class in the training",
    "set , a class - specific model is learned to distinguish one class from the other classes . based on the design criterion of qml ,",
    "the features extracted from the same class are similar while those from different class are different",
    ". therefore , dualqml can better discriminate similar classes .",
    "in this paper , we have presented a novel quadratic projection based feature extraction framework and applied it to biometric recognition .",
    "the key step is to obtain a set of quadratic matrices by solving the problem of the quadratic matrix learning ( qml ) . to address the scalability of qml",
    ", we have developed an efficient dualqml algorithm .",
    "the key idea is that , rather than solving the primal problem , we solve the lagrange dual problem by exploiting the special structure of qml .",
    "the proposed algorithm is simple to implement and scalable to high - dimensional biometric data .",
    "experimental results on three types of biometric recognition tasks have shown the superiority performance of the proposed feature extraction algorithm .",
    "the authors would like to thank the associate editor and the anonymous reviewers for their constructive comments .",
    "this work was supported by the national natural science foundation of china under grants 61571379 , 61472334 and 61170179 and supported by the fundamental research funds for the central universities under grant 20720130720 .",
    "jain , a.  ross , s.  prabhakar , an introduction to biometric recognition , ieee trans .",
    "circuits syst .",
    "video technol .",
    "14 ( 1 ) ( 2004 ) 4 - 20 .",
    "jiang , linear subspace learning - based dimensionality reduction , ieee signal process .  mag .",
    "28 ( 2 ) ( 2011 ) 16 - 26 .",
    "jain , p. flynn , a. ross , handbook of biometrics , springer - verlag , 2007 .",
    "s. yan , d. xu , b. zhang , h. zhang , q. yang , s. lin , graph embedding and extensions : a general framework for dimensionality reduction , ieee trans .  pattern anal .",
    "29 ( 1 ) ( 2007 ) 40 - 51 .",
    "m. harandi , m. salzmann , r. hartley , from manifold to manifold : geometry - aware dimensionality reduction for spd matrices , in : proceedings of european conference on computer vision , 2014 , pp .",
    "s. boyd , l. vandenberghe , convex optimization , cambridge university press , 2004 .",
    "unar , w.c .",
    "seng , a.  abbasi , a review of biometric technology along with trends and prospects .",
    "pattern recognit .",
    "47 ( 8) ( 2014 ) 2673 - 2688 .",
    "d.  zhang , w.k .",
    "kong , j.  you , m.  wong , on - line palmprint identification , ieee trans .  pattern anal .",
    "intell .  25 ( 9 ) ( 2003 ) 1041 - 1050 .",
    "j.  yang , a.f .",
    "frangi , j.y .",
    "yang , d.  zhang , j.  zhong , kpca plus lda : a complete kernel fisher discriminant framework for feature extraction and recognition , ieee trans .  pattern anal .",
    "intell .  29 ( 8) ( 2005 ) 1297 - 1308 .",
    "p.  yan , k.w .",
    "bowyer , biometric recognition using 3d ear shape , ieee trans .  pattern anal .",
    "intell .  27 ( 2 ) ( 2007 ) 230 - 244 .",
    "d.  xu , s.  yan , d.  tao , h.  zhang , marginal fisher analysis and its variants for human gait recognition and content based image retrieval , ieee trans .  image process .",
    "16 ( 11 ) ( 2007 ) 2811 - 2821 .",
    "x.  jing , s.  li , d.  zhang , c.  lan , j.y .",
    "yang , optimal subset - division based discrimination and its kernelization for face and palmprint recognition , pattern recognit .  45 ( 10 ) ( 2012 ) 3590 - 3602 .",
    "r.  xiao , w.j .",
    "li , y.d .",
    "tian , x.  tang , joint boosting feature selection for robust face recognition , in : proceedings of international conference on computer vision and pattern recognition , 2006 , pp .",
    "1415 - 1422 .",
    "l.  ghoualmi , a.  draa , s.  chikhi , an efficient feature selection scheme based on genetic algorithm for ear biometrics authentication . in : proceedings of international symposium on programming and systems , 2015 , pp .  1 - 5",
    ".    a.  kumar , d .",
    "biometric recognition using feature selection and combination , in : proceedings of international conference on audio- and video - based biometric person authentication , 2005 , pp .",
    "813 - 822 .",
    "m.  turk , m.  pentland , eigenfaces for recognition , j.  cogn .",
    "3 ( 1 ) ( 1991 ) 71 - 86 .",
    "p.  belhumeur , j.  hespanha , d.  kriegman , eigenfaces vs.  fisherfaces : recognition using class specific linear projection , ieee trans .  pattern anal .",
    "intell .  19 ( 7 ) ( 1997 ) 711 - 720 .",
    "x.  he , s.  yan , y.  hu , p.  niyogi , h.j .  zhang , face recognition using laplacianfaces , ieee trans .",
    "pattern anal .",
    "intell .  27 ( 3 ) ( 2005 ) 328 - 340 .",
    "b.v.k.v .",
    "kumar , m.  savvides , c.  xie , correlation pattern recognition for face recognition , proc .",
    "ieee 94 ( 11 ) ( 2006 ) 1963 - 1976 .",
    "y.  yan , y.j .",
    "zhang , 1d correlation filter based class - dependence feature analysis for face recognition , pattern recognit .",
    "41(12 ) ( 2008 ) 3834 - 3841 .",
    "n.  parrish , m.r .",
    "gupta , dimensionality reduction by local discriminative gaussians , in : proceedings of international conference on machine learning , 2012 , pp .",
    "559 - 566 .",
    "e.  kim , m.  lee , s.  oh , elastic - net regularization of singular values for robust subspace learning , in : proceedings of international conference on computer vision and pattern recognition , 2015 , pp .  915 - 923 .",
    "davis , b.  kulis , p.  jain , s.  sra , i.s .",
    "dhillon , information - theoretic metric learning , in : proceedings of international conference on machine learning , 2007 , pp .  209 - 216 .",
    "weinberger , j.  blitzer , l.k .",
    "saul , distance metric learning for large margin classification , j.  mach .",
    "res .  10 ( 2009 ) 207 - 244",
    ". q.  wang , z.  wang , l.  zhang , p.  li , shrinkage expansion adaptive metric learning , in : proceedings of european conference on computer vision , 2014 , pp .",
    "456 - 471 .",
    "z.  huang , r.  wang , s.  shan , x.  chen , projection metric learning on grassmann manifold with application to video based face recognition , in : proceedings of international conference on computer vision and pattern recognition , 2015 , pp .",
    "140 - 149 .",
    "k.  muller , s.  mika , g.  riitsch , k.  tsuda , b.  sch@xmath85lkopf , an introduction to kernel - based learning algorithms , ieee trans .",
    ".  12 ( 2 ) ( 2001 ) 181 - 201 .",
    "s.  zafeiriou , g.  tzimiropoulos , m.  petrou , t.  stathaki , regularized kernel discriminant analysis with a robust kernel for face recognition and verification , ieee trans .",
    "neural netw .",
    "syst .  23 ( 3 ) ( 2012 ) 526 - 534 . j.b .",
    "tenenbaum , v.  silva , j.c .",
    "langford , a global geometric framework for nonlinear dimensionality reduction , science 290 ( 2000 ) 2319 - 2323 .",
    "roweis , l.k .",
    "saul , nonlinear dimensionality reduction by locally linear embedding , science 290 ( 2000 ) 2323 - 2326 .",
    "z.  zhang , h.  zha , principal manifolds and nonlinear dimension reduction via local tangent space alignment , siam j.  sci .",
    "26 ( 1 ) ( 2005 ) 313 - 338 .",
    "j.  ham , d.  lee , s.  mika , b.  sch@xmath85lkopf , a kernel view of the dimensionality reduction of manifolds , in : proceedings of international conference on machine learning , 2004 , pp .",
    ".                  x.d .",
    "jiang , asymmetric principal component and discriminant analyses for pattern classification , ieee trans .",
    "pattern anal .",
    "intell .  31 ( 5 ) ( 2009 ) 931 - 937",
    "jiang , b.  mandal , a.  kot , eigenfeature regularization and extraction in face recognition , ieee trans .  pattern anal .",
    "intell .  30 ( 3 ) ( 2008 ) 383 - 394 .",
    "r.  wang , s.  shan , j.  chen , w.  gao , maximal linear embedding for dimensionality reduction , ieee trans .  pattern anal .",
    "33 ( 9 ) ( 2011 ) 1776 - 1792 .",
    "huang , m.  ramesh , t.  berg , e.  learned - miller .",
    "labeled faces in the wild : a database for studying face recognition in unconstrained environments , technical report 07 - 49 , university of massachusetts , 2007 .",
    "c.  liu , h.  wechsler , gabor feature based classification using the enhanced fisher linear discriminant model for face recognition , ieee trans .  image process .  11 ( 4 ) ( 2012 ) 467 - 476 . z.n .",
    "sun , t.n .",
    "tan , y.h .",
    "wang , s.  li , ordinal palmprint representation for personal identification , in : proceedings of international conference on computer vision and pattern recognition , 2005 , pp .",
    "279 - 284 .",
    "a.  kumar , c.  wu , automated human identification using ear imaging , pattern recognit .  45 ( 3 ) ( 2012 ) 956 - 968"
  ],
  "abstract_text": [
    "<S> this paper presents a novel quadratic projection based feature extraction framework , where a set of quadratic matrices is learned to distinguish each class from all other classes . </S>",
    "<S> we formulate quadratic matrix learning ( qml ) as a standard semidefinite programming ( sdp ) problem . however , the conventional interior - point sdp solvers do not scale well to the problem of qml for high - dimensional data . to solve the scalability of qml </S>",
    "<S> , we develop an efficient algorithm , termed dualqml , based on the lagrange duality theory , to extract nonlinear features . </S>",
    "<S> to evaluate the feasibility and effectiveness of the proposed framework , we conduct extensive experiments on biometric recognition . </S>",
    "<S> experimental results on three representative biometric recognition tasks , including face , palmprint , and ear recognition , demonstrate the superiority of the dualqml - based feature extraction algorithm compared to the current state - of - the - art algorithms . </S>",
    "<S> +    biometric recognition , feature extraction , quadratic projection , semidefinite programming , lagrange duality </S>"
  ]
}