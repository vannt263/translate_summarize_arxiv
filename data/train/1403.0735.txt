{
  "article_text": [
    "consider estimation of a parameter @xmath0 in the linear regression model @xmath1 where @xmath2 is a given , deterministic @xmath3 matrix , and @xmath4 is an @xmath5-variate standard normal vector .",
    "the model is standard , but we are interested in the _ sparse _ setup , where @xmath6 , and possibly @xmath7 , and `` many '' or `` most '' of the coefficients @xmath8 of the parameter vector are zero , or close to zero .",
    "we study a bayesian approach based on priors that set a selection of coefficients @xmath8 a priori to zero ; equivalently , priors that distribute their mass over models that use only a ( small ) selection of the columns of @xmath2 .",
    "bayes s formula gives a posterior distribution as usual .",
    "we study this under the `` frequentist '' assumption that the data @xmath9 has in reality been generated according to a given ( sparse ) parameter @xmath10 .",
    "the expectation under the previous distribution is denoted @xmath11 .",
    "specifically , we consider a prior @xmath12 on @xmath13 that first selects a _ dimension _",
    "@xmath14 from a prior @xmath15 on the set @xmath16 , next a random subset @xmath17 of cardinality @xmath18 and finally a set of nonzero values @xmath19 from a prior density @xmath20 on @xmath21 .",
    "formally , the prior on @xmath22 can be expressed as @xmath23 where the term @xmath24 refers to the coordinates @xmath25 being zero .",
    "we focus on the situation where @xmath20 is a product @xmath26 of densities over the coordinates in  @xmath27 , for @xmath28 a fixed continuous density on @xmath29 , with the laplace density as an important special case .",
    "the prior @xmath15 is crucial for expressing the `` sparsity '' of the parameter .",
    "one of the main findings of this paper is that weights @xmath30 that decrease slightly faster than exponential in the dimension @xmath14 give good performance .",
    "priors of the type of ( [ defprior ] ) were considered by many authors , including @xcite .",
    "other related contributions include @xcite .",
    "the paper @xcite contains a theoretical analysis similar to the present paper , but restricted to the special case that the regression matrix @xmath2 is the identity and @xmath31 ; see example  [ examplesequencemodel ] .",
    "the general model ( [ model ] ) shares some features with this special case , but is different in that it must take account of the noninvertibility of @xmath2 and its interplay with the sparsity assumption , especially for the case of recovering the parameter @xmath13 , as opposed to estimating the mean @xmath32 . while the proofs in @xcite use a factorization of the model along the coordinate axes , exponential tests and entropy bounds , in the present paper we employ a direct and refined analysis of the posterior ratio ( [ bayes ] ) , exploiting the specific form of the prior laplace density @xmath28 . furthermore , even for the case that @xmath2 is the identity matrix , the present paper provides several new results of interest : distributional approximations to the posterior distribution , insight in the scaling of the prior on the nonzero coordinates and oracle formulations of the contraction rates .",
    "algorithms for the computation of the posterior distribution corresponding to  ( [ defprior ] ) , especially for the `` spike and slab '' prior described in example  [ example.slabspike ] below , are routine for small dimensions @xmath33 and @xmath5 ( e.g. , @xcite ) . for large dimensions the resulting computations are intensive , due to the large number of possible submodels @xmath27 .",
    "many authors are currently developing algorithms that can cope with larger numbers of covariates , in the sparse setup considered in the present paper . in section  [ sectioncomputationalaspects ]",
    "we review recent progress on various methods , of which some are feasible for values of @xmath33 up to hundreds or thousands @xcite .",
    "although this upper bound will increase in the coming years , clearly it falls far short of the dimensions attainable by ( point ) estimation methods based on convex programming , such as the lasso . other bayesian approaches to sparse regression that do not explicitly include model selection ( e.g. , @xcite )",
    "can cope with somewhat higher dimensions , but truly high - dimensional models are out of reach of fully bayesian methods at the present time .    not surprisingly to overcome the nonidentifiability of the full parameter vector @xmath13 in the overspecified model ( [ model ] ) , we borrow from the work on sparse regression within the non - bayesian framework ; see @xcite .",
    "good performance of the posterior distribution is shown under _ compatibility _ and _ smallest sparse eigenvalue _ conditions ; see section  [ sec.recovery ] . although the constants in these results are not as sharp as results for the lasso , the posterior contraction rates obtained are broadly comparable to convergence rates of the lasso .",
    "the lasso and its variants are important frequentist methods for sparse signal recovery . as the lasso is a posterior mode ( for an i.i.d .",
    "laplace prior on the @xmath8 ) , it may seem to give an immediate link between bayesian and non - bayesian methods .",
    "however , we show in section  [ sec.spars_and_lasso ] that the lasso is essentially non - bayesian , in the sense that the corresponding _ full _ posterior distribution is a useless object .",
    "in contrast , the posterior distribution resulting from the prior ( [ defprior ] ) gives both reasonable reconstruction of the parameter @xmath13 and a quantification of uncertainty through the spread in the posterior distribution .",
    "we infer this from combining results on the contraction rate of the full posterior distribution with distributional approximations .",
    "the latter show that the posterior distribution behaves asymptotically as a mixture of bernstein  von mises type approximations to submodels , where the location of the mixture components depends on the setting .",
    "the latter approximations are new , also for the special case that @xmath2 is the identity matrix .",
    "it is crucial for these results that the prior ( [ defprior ] ) models sparsity through the _ model selection _",
    "prior @xmath15 , and separates this from modeling the nonzero coordinates through the prior densities @xmath20 .",
    "for instance , in the case that @xmath20 is a product of laplace densities , this allows the scale parameter to be constant or even to tend to zero , thus making this prior uninformative .",
    "this is in stark contrast to the choice of the smoothing parameter in the ( bayesian ) lasso , which must tend to infinity in order to shrink parameters to zero , where it can not differentiate between truly small and nonzero parameters .",
    "technically this has the consequence that the essential part of the proofs is to show that the posterior distribution concentrates on sets of small dimension .",
    "this sets it apart from the frequentist literature on sparse regression , although , as mentioned , many essential ideas reappear here in a bayesian framework .",
    "the paper is organized as follows . in section  [ sec.recovery ]",
    "we present the main results of the paper .",
    "we specialize to laplace priors on the nonzero coefficients and investigate the ability of the posterior distribution to recover the parameter vector @xmath13 , the predictive vector @xmath32 and the set of nonzero coordinates .",
    "furthermore , we derive a distributional approximation to the posterior distribution , and apply this to construct and study credible sets . in section  [ sec.spars_and_lasso ]",
    "we present the negative result on the bayesian interpretation of the lasso .",
    "next in section  [ sectionarbitrarydesign ] we show that for recovery of only the predictive vector @xmath32 , significantly milder conditions than in section  [ sec.recovery ] suffice .",
    "proofs are deferred to section  [ sectionproofs ] and the supplementary material @xcite .      for a vector @xmath0 and a set @xmath34 of indices , @xmath35 is the vector @xmath36 , and @xmath37 is the cardinality of @xmath27",
    ". the _ support _ of the parameter @xmath13 is the set @xmath38 .",
    "the support of the true parameter @xmath10 is denoted @xmath39 , with cardinality @xmath40 .",
    "similarly , for a generic vector @xmath41 , we write @xmath42 and @xmath43 .",
    "we write @xmath44 if there is no ambiguity to which set @xmath27 is referred to . for @xmath45 and @xmath0 , let @xmath46 .",
    "we let @xmath47 be the @xmath48th column of @xmath2 , and @xmath49 for the prior @xmath12 defined above , bayes s formula gives the following expression for the posterior distribution @xmath50 $ ] . for any borel set @xmath51 of @xmath52 , @xmath53 = \\int_b e^{-\\|y - x{\\beta}\\|_2 ^ 2/2}\\,d\\pi ( { \\beta } ) \\big/ \\int e^{-\\|y - x{\\beta}\\|_2 ^ 2/2}\\,d\\pi({\\beta}).\\ ] ]",
    "in this section we consider the prior ( [ defprior ] ) , with @xmath20 the product of @xmath37 laplace densities @xmath54 .",
    "we allow the ( inverse ) scale parameter @xmath55 to change with @xmath33 , within the range , with @xmath56 defined in ( [ defnormx ] ) , @xmath57 the quantity @xmath58 in the upper bound is the usual value of the regularization parameter @xmath55 of the lasso [ as in ( [ eqlasso ] ) below ] .",
    "its large value causes the lasso to shrink many coordinates @xmath8 to zero , as is desired in the sparse situation .",
    "however , in our bayesian setup , sparsity should be induced by model selection , through the prior @xmath15 on the model dimension , and the laplace prior densities model only the nonzero coordinates .",
    "large values of @xmath55 would shrink the nonzero coordinates to zero , which is clearly undesirable and unnatural .",
    "thus it is natural to assume @xmath59 , and fixed values of @xmath55 , and even values decreasing to zero , may well be natural , depending on the regression setting .",
    "we shall see that small values of @xmath55 permit a distributional approximation to the posterior distribution centered at unbiased estimators .",
    "the results below hold for all @xmath60 in the range ( [ eq.lambda_cond ] ) , but they are meant to be read for a specific sequence of @xmath60 and are not suitable for optimization over  @xmath55 .",
    "the precise interpretation of the size of @xmath55 is confounded with the regression setting , the error variance ( which we have set to unity for simplicity of notation ) and the scaling of the regression matrix .",
    "the following three special cases shed some light on this .",
    "[ examplesequencemodel ] in the _ sequence model _ considered in @xcite and @xcite , the observation is a vector @xmath61 of independent coordinates @xmath62 .",
    "this corresponds to @xmath63 and @xmath64 in the present setting ( [ model ] ) , whence @xmath65 .",
    "condition ( [ eq.lambda_cond ] ) then reduces to @xmath66 .",
    "fixed values of @xmath55 , as considered in @xcite , are easily included .",
    "as there is only one observation per parameter , it may not be unreasonable to consider @xmath67 , in order to create noninformative priors for the nonzero coefficients .",
    "this is allowed easily also .",
    "[ examplesequencemodelmultiple ] in an extension of the sequence model of the preceding example , the @xmath5 observations are from normal distributions @xmath68 with variances @xmath69 . by defining the @xmath70 as @xmath71 times the original observations , we can fit this into model ( [ model ] ) , which has unit error variances .",
    "if we keep the original definition of the @xmath8 , then the regression matrix is @xmath72 , and hence @xmath73 .",
    "condition ( [ eq.lambda_cond ] ) then reduces to @xmath74 .",
    "fixed values of @xmath55 are included if @xmath75 , and values tending to zero if @xmath76 . by sufficiency of the sample",
    "mean in the normal location problem this corresponds to a sufficient number of replicate measurements on every parameter @xmath8 in the original problem .",
    "[ exampleregressionmodel ] if every row of the regression equation @xmath77 refers to a measurement of an instance of a fixed relationship between an input vector @xmath78 and the corresponding output @xmath70 , then the entry @xmath79 of @xmath2 is the value of individual @xmath48 on the @xmath80th covariable .",
    "it is then reasonable to think of these entries as being sampled from some fixed distribution , independent of @xmath5 and  @xmath33 , in which case @xmath56 will ( typically ) be of the order @xmath81 .",
    "a fundamental example is the case where the entries of @xmath2 are independent standard gaussian @xmath82 .",
    "condition  ( [ eq.lambda_cond ] ) then reduces to @xmath83 .",
    "fixed values of @xmath55 , as considered in @xcite , are included , provided @xmath84 .",
    "although condition ( [ eq.lambda_cond ] ) does not exclude shrinkage through large values of @xmath55 , as for the lasso , the most interesting situation is that sparsity is induced through model selection .",
    "the prior @xmath15 on model dimension is crucial ; it must downweight big models , but at the same time give sufficient mass to the true model",
    ". exponential decrease turns out to work .",
    "there are constants @xmath85 with @xmath86    assumption ( [ assump.on_the_dim_prior ] ) is met by the priors of the form , for constants @xmath87 , @xmath88 we refer to these priors as _ complexity priors _ , as their rate of decrease reflects the number of models @xmath89 of given size @xmath14 for @xmath90 ; cf .",
    "@xcite .",
    "[ example.slabspike ] modeling the coordinates @xmath91 as i.i.d .",
    "variables from a mixture @xmath92 , of a dirac measure @xmath93 at zero and a laplace distribution @xmath94 , is included in ( [ defprior ] ) with @xmath95 the binomial distribution with parameter @xmath33 and @xmath96 . the size @xmath96 of the point mass at zero controls the model selection .",
    "the overall prior obtained by choosing @xmath96 from a beta @xmath97 hyper prior with @xmath98 satisfies ( [ eq.classofdimpriors ] ) ; cf .",
    "example  2.2 in @xcite .",
    "this prior is universal in that it is free of unknown smoothing parameters .    to conclude the discussion on the prior , we briefly comment on the case that the noise vector has unknown variance @xmath99 ; that is",
    ", we observe @xmath100 . in this case",
    "one may use an empirical bayesian approach , which replaces the unknown parameter by an estimator , or a hierarchical bayesian approach , which puts a prior on @xmath101 , a common choice being an inverse gamma prior . since @xmath102",
    ", it is natural to apply the prior , as in this paper , to the parameter @xmath103 .",
    "thus given @xmath101 and a model @xmath27 , we choose the prior density on the nonzero values @xmath104 as the product of @xmath37 laplace densities @xmath105 .",
    "the parameter @xmath13 in model ( [ model ] ) is not estimable without conditions on the regression matrix . for the interesting case @xmath106 , it is even necessarily unidentifiable .",
    "if @xmath13 is known to be sparse , then `` local invertibility '' of the gram matrix @xmath107 is sufficient for estimability , even in the case @xmath106 .",
    "this is made precise in the following definitions , which are based on the literature , but with simplified notation suited to our bayesian setup .",
    "for accessibility we include short discussions on the relations between the various concepts .",
    "[ assump.comp ] the compatibility number of model @xmath108 is given by @xmath109    the compatibility number ( which is @xmath110 in the notation of @xcite , page 157 ) compares the @xmath111-norm of the predictive vector @xmath32 to the @xmath112-norm of the parameter @xmath35 . a model @xmath27 is considered `` compatible '' if @xmath113 .",
    "it then satisfies the nontrivial inequality @xmath114 .",
    "we shall see that true vectors @xmath10 with compatible support @xmath115 can be recovered from the data , uniformly in a lower bound on the size of their compatibility numbers",
    ".    the number 7 has no particular interest , but for simplicity we use a numerical value instead of an unspecified constant . since the vectors",
    "@xmath13 in the infimum satisfy @xmath116 , it would not be a great loss of generality to replace @xmath35 in the denominator of the quotient by @xmath13 .",
    "however , the factor @xmath117 in the numerator may be seen as resulting from the comparison of the @xmath112- and @xmath111-norms of @xmath35 through the cauchy ",
    "schwarz inequality : @xmath118 by @xmath119 would make the compatibility number smaller , and hence give a more restrictive condition .",
    "the compatibility number involves the full vectors @xmath13 ( also their coordinates outside of @xmath27 ) and allows to reduce the recovery problem to sparse vectors .",
    "the next two definitions concern sparse vectors only , but unlike the compatibility number , they are uniform in vectors up to a given dimension . in the notation of @xcite ( pages 156157 )",
    "the numbers in the definitions are the minima over @xmath120 of the numbers @xmath121 and @xmath122 , respectively .    [ assump.uniform_comp ]",
    "the compatibility number in vectors of dimension @xmath14 is defined as @xmath123    [ assump.smallest_sp_ev ] the smallest scaled singular value of dimension @xmath14 is defined as @xmath124    for recovery we shall impose that these numbers for @xmath14 equal to ( a multiple of ) the dimension of the true parameter vector are bounded away from zero . since @xmath125 by the cauchy ",
    "schwarz inequality , it follows that @xmath126 , for any @xmath127 .",
    "the stronger assumptions on the design matrix imposed through @xmath128 will be used for recovery with respect to the @xmath111-norm , whereas the numbers @xmath129 suffice for @xmath112-reconstruction . in definition",
    "[ assump.smallest_sp_ev ] , `` scaled '' refers to the scaling of the matrix @xmath2 by division by the maximum column length @xmath56 ; if the latter is unity , then @xmath128 is just the smallest scaled singular value of a submatrix of @xmath2 of dimension @xmath14 .",
    "the final and strongest invertibility condition is in terms of `` mutual coherence '' of the regression matrix , which is the maximum correlation between its columns .",
    "the _ mutual coherence number _ is @xmath130    the `` _ _",
    "@xmath131 mutual coherence condition _",
    "_ '' is that this number is bounded above by @xmath132 , in which case reconstruction is typically possible for true vectors @xmath13 of dimension up to @xmath14 .",
    "as correlations are easy to interpret , conditions of this type , which go back to @xcite , have been used by many authors .",
    "( notably , bunea , tsybakov and wegkamp @xcite show that for reconstructions using the @xmath112- and @xmath111-norms , taking the maximum over all correlations can be relaxed to a maximum over pairs that involve at least one `` active '' coordinate . )",
    "the following lemma shows that they are typically stronger than conditions in terms of compatibility numbers or sparse singular values .",
    "the lemma is embodied in lemma  2 in @xcite , and is closely related to the inequalities obtained in @xcite . for ease of reference",
    "we provide a proof in the supplementary material @xcite .",
    "[ lemmacoherence ] @xmath133 ; @xmath134 .    by evaluating the infimum in definition  [ assump.uniform_comp ] with @xmath13",
    "equal to unit vectors , we see that @xmath135 , which will typically be bounded away from zero .",
    "thus the lemma implies that compatibility numbers and sparse singular values are certainly bounded away from zero for models up to size a multiple of @xmath136 , that is , models of size satisfying the `` mutual coherence condition . ''",
    "this makes the mutual coherence the strongest of the three `` sparse invertibility '' indices introduced previously .",
    "we note that the reverse inequalities do not hold in general , and indeed the compatibility constant can easily be bounded away from zero , even if the mutual coherence number is much larger than @xmath137 .    for many other possible indices ( including `` restricted isometry '' and `` irrepresentability '' ) , and extensive discussion of their relationships",
    ", we refer to sections  6.13 and  7.5 of @xcite .",
    "in particular , the diagram on page 177 exhibits compatibility as the weakest condition that still allows oracle bounds for prediction and reconstruction by the lasso for the @xmath111- and @xmath138-norms .",
    "the results on posterior contraction and model selection presented below are in the same spirit .",
    "in addition we consider contraction with respect to the @xmath139-norm , and for ( only ) the latter we employ the more restrictive mutual coherence number , analogously to the study of @xcite of the lasso and the dantzig estimator under the supremum norm .",
    "thus mutual coherence is useful in two ways : it may provide a simple ( albeit crude ) way to bound the other indices , and it may allow to use stronger norms .",
    "direct verification of compatibility may be preferable , as this applies to a much broader set of regression matrices .",
    "the following well - studied examples may help appreciate the discussion :    in the sequence model of example  [ examplesequencemodel ] the regression matrix @xmath2 is the identity , and hence the compatibility numbers are 1 , and the mutual coherence number is zero .",
    "this is the optimal situation , under which all results below are valid .",
    "( the compatibility numbers are maximally 1 , as follows by evaluating them with a unit vector . )    regression with orthogonal design can be transformed to this situation .    in the response",
    "setting of example  [ exampleregressionmodel ] it is reasonable to assume that the entries of @xmath2 are i.i.d .",
    "random variables . under exponential moment conditions ,",
    "it is shown in @xcite that in this situation and for not extremely large @xmath33 the mutual coherence number is with high probability bounded by a multiple of @xmath140 .",
    "[ specifically , this is true for @xmath141 or @xmath142 if the entries are bounded or possess an exponential moment of order @xmath143 , resp .",
    "] in view of lemma  [ lemmacoherence ] the compatibility and sparse singular value indices of models up to dimension a multiple of @xmath144 are then bounded away from zero .",
    "this implies that the results on model selection and @xmath112- and @xmath111-contraction rates in the following certainly apply if the number of nonzero regression coefficients is smaller than this order . for a survey on more recent results on lower bounds of the compatibility number and the smallest sparse eigenvalue ,",
    "see section  6.2 of @xcite .    by scaling the columns of the design matrix",
    "it can be ensured that the @xmath145-matrix @xmath146 has unit diagonal .",
    "then @xmath147 , and the off - diagonal elements @xmath148 are the correlations between the columns .",
    "it is shown in @xcite that if @xmath148 is equal to a constant @xmath96 with @xmath149 , or @xmath150 , for every @xmath151 , then models up to dimension @xmath14 satisfy the `` strong irrepresentability condition '' and hence are consistently estimable .",
    "since these examples satisfy the mutual coherence condition , up to a constant , these examples are also covered in the present paper , for every norm and aspect considered .    as another example , zhao and yu @xcite",
    "consider correlations satisfying @xmath152 , for @xmath153 and @xmath31 . in this case",
    "_ all _ eigenvalues of @xmath154 are bounded away from zero by a margin that depends on @xmath155 only , whence the numbers @xmath156 are bounded away from zero , for every dimension @xmath14 .",
    "this implies that the results on dimensionality , model selection and @xmath112- and @xmath111-rates obtained below are valid . on the other hand ,",
    "the mutual coherence number is equal to @xmath155 , which excludes the @xmath139-results .    as a final example , the authors of @xcite consider matrices @xmath154 that vanish except in small blocks along the diagonal .",
    "such matrices can also not be handled in general through the mutual coherence number , but do cooperate with the other sparse invertibility indices .      for simplicity",
    "the main results are stated in limit form , for @xmath157 .",
    "more precise assertions , including precise values of `` large '' constants , can easily be deduced from the proofs .",
    "the results are obtained under the assumption of gaussian noise in model ( [ model ] ) .",
    "in fact , as indicated in remark  1 in the supplementary material @xcite , many of the assertions are robust under misspecification of the error distribution .",
    "the first theorem shows that the posterior distribution does not overshoot the true dimension of the parameter by more than a factor . in the interesting case that @xmath158 , this factor can be simplified to @xmath159 for any constant @xmath160 if the true parameter is compatible .",
    "the constant @xmath161 comes from condition ( [ assump.on_the_dim_prior ] ) . as a consequence",
    ", @xmath159 can be made arbitrarily close to one by choosing a suitable prior on the dimension .",
    "( although the convergence to zero in this and the following theorems is uniform , it can be read off from the proofs that the speed of convergence deteriorates for very small @xmath60 . also only the dominating terms in the dependence of the dimension or contraction rate are shown .",
    "thus the theorems as stated are not suitable for optimization over @xmath60 .",
    "in particular , it should not be concluded that the smallest possible @xmath60 is optimal . )    [ thmm.mod_sel ] if @xmath55 satisfies ( [ eq.lambda_cond ] ) and @xmath15 satisfies ( [ assump.on_the_dim_prior ] ) then , with @xmath162 and for any @xmath160 , @xmath163    the theorem is a special case of theorem  [ thmm.mod_sel_gen ] in section  [ sectionproofs ] . as all our results",
    ", the theorem concerns the full posterior distribution , not only a measure of its center .",
    "however , it may be compared to similar results for point estimators , as in chapter  7 of @xcite .",
    "the second theorem concerns the ability of the posterior distribution to recover the true model from the data .",
    "it gives rates of contraction of the posterior distribution both regarding _ prediction error _ @xmath164 and regarding the parameter @xmath13 relative to the @xmath112- and @xmath111- and @xmath139-distances .",
    "besides on the dimensionality , the rate depends on compatibility . set @xmath165 \\\\[-8pt ] \\nonumber \\widetilde\\psi(s)&=&\\widetilde\\phi \\biggl ( \\biggl ( 2+\\frac{3}{a_4}+ \\frac{33}{\\phi(s)^2}\\frac{{\\lambda } } { \\overline{\\lambda } } \\biggr ) |s| \\biggr ) . \\ ] ] in the interesting case that @xmath166 , these numbers are asymptotically bounded below by @xmath167 and @xmath168 if @xmath169 is bounded away from zero .",
    "thus the following theorem gives rates of recovery that are uniform in true vectors @xmath13 such that @xmath169 and @xmath167 or @xmath170 are bounded away from zero .",
    "[ again the theorem , even though uniform in @xmath60 satisfying ( [ eq.lambda_cond ] ) , is meant to be read for a given sequence of @xmath60 . ]    [ theoremrecovery ] if @xmath55 satisfies ( [ eq.lambda_cond ] ) , and @xmath15 satisfies ( [ assump.on_the_dim_prior ] ) , then for sufficiently large @xmath171 , with @xmath172 , @xmath173 furthermore , for every @xmath174 , any @xmath175 , and @xmath176 with @xmath177 , for sufficiently large @xmath171 , @xmath178    the first three assertions of the theorem are consequences of the following theorem of oracle type , upon choosing @xmath179 in this theorem .",
    "the fourth assertion is proved in section  [ sectionproofs ] under the conditions of theorem  [ thmm.bvm_type ] below . in the framework of example  [ exampleregressionmodel ] , for instance say for i.i.d .",
    "gaussian design and @xmath180 , the fourth assertion is true with large probability uniformly over sparse vectors such that @xmath181 .",
    "an _ oracle inequality _ for the prediction error of a point estimator @xmath182 is an assertion that with large probability , and some penalty function @xmath183 , @xmath184 see , for example , @xcite , theorem  6.2 , or @xcite for the lasso or the dantzig selector .",
    "few oracle - type results for _ posterior measures _ have been developed .",
    "( the results of @xcite , for projection estimators in white noise , are close relatives . )",
    "the following theorem is an example of such a statement .",
    "given compatibility it shows that the bulk of the vectors @xmath13 in the support of the posterior distribution satisfy an oracle inequality with penalty @xmath185 .",
    "[ thmm.pred_and_l1 ] if @xmath55 satisfies ( [ eq.lambda_cond ] ) , and @xmath15 satisfies ( [ assump.on_the_dim_prior ] ) , then , for @xmath186 and @xmath187 given in ( [ defpsis ] ) , there exists a constant @xmath171 such that uniformly over @xmath10 and @xmath41 with @xmath188 , where @xmath42 , @xmath189 \\big| y \\biggr ) { \\rightarrow}0 , \\\\ & & { \\mathbb{e}}_{{\\beta}^0}\\pi \\biggl({\\beta}:\\bigl \\|{\\beta}-{\\beta}^0 \\bigr\\|_1 > \\bigl\\|{\\beta}^*-{\\beta}^0\\bigr\\|_1\\\\ & & \\hspace*{32pt}{}+\\frac{m}{\\overline\\psi(s_{0})^2 } \\biggl[\\frac{\\|x({\\beta}^*-{\\beta}^0)\\|_2 ^ 2}{\\|x\\|\\sqrt{\\log p } } + \\frac{|s_{^*}| \\sqrt{\\log p}}{\\|x\\|\\phi(s_{*})^2 } \\biggr ] \\big| y \\biggr){\\rightarrow}0 , \\\\ & & { \\mathbb{e}}_{{\\beta}^0}\\pi \\biggl({\\beta } : \\bigl\\|{\\beta}-{\\beta}^0 \\bigr\\|_2 > \\frac{m}{\\|x\\|\\widetilde\\psi(s_0)^2 }",
    "\\biggl[\\bigl\\|x\\bigl({\\beta}^*-{\\beta}^0 \\bigr)\\bigr\\| _ 2+\\frac{\\sqrt{|s_{*}|\\log p}}{\\phi(s _ { * } ) } \\biggr ] \\big| y   \\biggr){\\rightarrow}0.\\end{aligned}\\ ] ]    besides the choice @xmath179 , which yields the first three assertions of theorem  [ theoremrecovery ] , other choices of @xmath41 also give interesting results .",
    "for instance , in the sequence model of example  [ examplesequencemodel ] , the choice @xmath190 gives that @xmath191 for @xmath192 smaller than @xmath193 , this improves on theorem  [ theoremrecovery ] , by quantifying the rate in the sizes and not only the number of nonzero coordinates in @xmath10 .",
    "the posterior distribution induces a distribution on the set of models @xmath17 , which updates the prior masses given to these models by ( [ defprior ] ) .",
    "it is desirable that this puts most of its mass on the true model @xmath115 . as the support of a vector @xmath10",
    "is defined only in a qualitative manner by its coordinates @xmath194 being zero or not , this will not be true in general .",
    "however , the following theorem shows , under ( only strong ) compatibility , that the posterior distribution will not charge models that are strict supersets of the true model , no matter the magnitudes of the nonzero coordinates in @xmath10 .",
    "this may be considered the effect of model selection through the prior @xmath15 , which under our assumptions prefers smaller models , enough so that it will not add unnecessary coordinates when all truly nonzero coordinates are present .",
    "[ ( selection : no supersets ) ] [ theoremselectionnosupersets ] if @xmath55 satisfies ( [ eq.lambda_cond ] ) , and @xmath15 satisfies  ( [ assump.on_the_dim_prior ] ) with @xmath195 , then for every @xmath174 and any @xmath196 with@xmath197 and @xmath198 , @xmath199    a nonzero coordinate of @xmath10 that is too close to zero can not be detected as being nonzero by any method .",
    "consequently , the posterior distribution may well charge models @xmath27 that contain only a subset of the true model @xmath115 and possibly other coordinates , which is not excluded by the preceding theorem .",
    "the following theorem gives thresholds for detection , which become smaller as the compatibility conditions become stronger .",
    "the theorem may be compared to results in terms of _ beta - min conditions _ for point estimators ; see , for example , @xcite , corollary  7.6 .",
    "[ ( selection ) ] [ theoremselection ] if @xmath55 satisfies ( [ eq.lambda_cond ] ) , and @xmath15 satisfies ( [ assump.on_the_dim_prior ] ) , then , for sufficiently large @xmath171 , @xmath200 furthermore , for every @xmath174 , any @xmath201 , and any @xmath176 with @xmath177 , @xmath202    by combining theorems  [ theoremselectionnosupersets ] and  [ theoremselection ] we see that under the assumptions of the theorems the posterior distribution _ consistently selects _ the correct model if _ all _ nonzero coordinates of @xmath10 are bounded away from 0 by the thresholds given in theorem  [ theoremselection ] . for @xmath171 as in the preceding theorem ,",
    "let @xmath203 define @xmath204 similarly with @xmath205 in the threshold replaced by @xmath206 and with @xmath186 instead of @xmath187 .",
    "[ cor.consistent_mod_selection ] if @xmath55 satisfies ( [ eq.lambda_cond ] ) , and @xmath15 satisfies ( [ assump.on_the_dim_prior ] ) with @xmath195 , and @xmath196 such that @xmath198 and@xmath207 , then , for every @xmath174 , @xmath208 the same is true with @xmath209 and @xmath210 replaced by @xmath211 and @xmath212 .",
    "consistent posterior model selection implies in particular , that the model with the largest posterior mass is model selection consistent in the frequentist sense . this can be established as in the proof of theorem  2.5 in @xcite .      in this section",
    "we show that the posterior distribution can be approximated by a mixture of normal distributions .",
    "moreover , given consistent selection of the true model , this mixture collapses to a single normal distribution .",
    "we restrict to what we shall refer to as the _ small lambda regime _ ,",
    "@xmath213 in this case the centering of the normal distributions does not depend on the size of scaling parameters @xmath55 .",
    "in contrast , in the `` large lambda regime , '' which includes the usual order of magnitude of the smoothing parameter in the lasso , the posterior distribution mimics the lasso , and gives a biased reconstruction of the true parameter ; see theorem  1 in the supplementary material @xcite .",
    "the small lambda regime includes a variety of possible choices within our general assumption ( [ eq.lambda_cond ] ) . a smaller value of @xmath55 corresponds to a noninformative prior on the nonzero coordinates of the parameter vector . here",
    "`` small '' is relative , depending on the model and the number of observations .",
    "[ ( small lambda regime ) ] for the minimal choice @xmath214 in  ( [ eq.lambda_cond ] ) the small lambda regime ( [ eq.cond_on_lambda_bvm ] ) simplifies to @xmath215 .",
    "thus the regime applies to a wide range of true parameters .    in the sequence model with multiple observations given in example  [ examplesequencemodelmultiple ] and the response model of example  [ exampleregressionmodel ] , we have @xmath216 and @xmath217 , respectively , and @xmath55 is in the small lambda regime if @xmath218 is much smaller than @xmath219 and @xmath144 , respectively .",
    "the second allows @xmath220 if @xmath221 .    for a given model @xmath222",
    "let @xmath223 be the latexmath:[$n\\times    the columns @xmath47 with @xmath225 , and let @xmath226 be a least square estimator in the restricted model @xmath227 , that is , @xmath228 in case the restricted model would be correctly specified , the least squares estimator would possess a @xmath229-distribution , and the posterior distribution ( in a setting where the data washes out the prior ) would be asymptotically equivalent to a @xmath230-distribution , by the bernstein ",
    "von mises theorem . in our present situation ,",
    "the posterior distribution is approximated by a random mixture of these normal distributions , of the form @xmath231 where @xmath232 denotes the dirac measure at @xmath233 , the weights @xmath234 satisfy @xmath235 and , for a sufficiently large @xmath171@xmath236 the weights @xmath237 are a data - dependent probability distribution on the collection of models @xmath238 .",
    "the latter collection can be considered a `` neighborhood '' of the support of the true parameter , both in terms of dimensionality and the ( lack of ) extension of the true parameter outside these models .",
    "a different way of writing the approximation @xmath239 is @xmath240 where @xmath241 is the intersection ( and not projection ) of @xmath242 with the subspace @xmath21 . to see this , decompose @xmath243 , and observe that the two summands are orthogonal .",
    "the lebesgue integral @xmath244 can be interpreted as an improper prior on the parameter @xmath35 of model @xmath27 , and the expression as a mixture of the corresponding posterior distributions , with model weights proportional to the prior weights times @xmath245 .",
    "it follows that the laplace priors @xmath20 on the nonzero coordinates wash out from the components of the posterior . on the other hand , they are still visible in the weights through the factors @xmath246 . in general , this influence is mild in the sense that these factors will not change the relative weights of the models much .",
    "[ thmm.bvm_type ] if @xmath55 satisfies ( [ eq.lambda_cond ] ) , and @xmath15 satisfies ( [ assump.on_the_dim_prior ] ) , then for every @xmath174 and any @xmath176 with @xmath247 , @xmath248    [ cor.strong_mod_selection ] under the combined assumptions of corollary  [ cor.consistent_mod_selection ] and theorem  [ thmm.bvm_type ] , @xmath249    the distributional results imply that the spread in the posterior distribution gives a correct ( conservative ) quantification of remaining uncertainty on the parameter .",
    "one way of making this precise is in terms of _ credible sets _ for the individual parameters @xmath250 .",
    "the marginal posterior distribution of @xmath250 is a mixture @xmath251 of a point mass at zero and a continuous component @xmath252 .",
    "thus a reasonable _ upper 0.975 credible limit _ for @xmath250 is equal to @xmath253 it is not difficult to see that under the conditions of corollary  [ cor.strong_mod_selection ] , @xmath254 if @xmath255 and @xmath256 if @xmath257 .",
    "the lasso ( cf . @xcite)@xmath258\\ ] ] is the posterior mode for the prior that models the coordinates @xmath8 as an i.i.d . sample from a laplace distribution with scale parameter @xmath55 , and thus also possesses a bayesian flavor . it is well known to have many desirable properties : it is computationally tractable ; with appropriately tuned smoothing parameter @xmath55 it attains good reconstruction rates ; it automatically leads to sparse solutions ; by small adaptations it can be made consistent for model selection under standard conditions .",
    "however , as a bayesian object it has a deficit : in the sparse setup the full posterior distribution corresponding to the lasso prior does not contract at the same speed as its mode .",
    "therefore the full posterior distribution is useless for uncertainty quantification , the central idea of bayesian inference .",
    "we prove this in the following theorem , which we restrict to the sequence model of example  [ examplesequencemodel ] , that is , model ( [ model ] ) with @xmath63 the identity matrix . in this setting the lasso estimator is known to attain the ( near ) minimax rate @xmath259 for the square euclidean loss over the `` nearly black bodies '' @xmath260 , and a near minimax rate over many other sparsity classes as well , if the regularity parameter @xmath55 is chosen of the order @xmath261 .",
    "the next theorem shows that for this choice the lasso posterior distribution @xmath262 puts no mass on balls of radius of the order @xmath263 , which is substantially bigger than the minimax rate @xmath264 ( except for extremely dense signals ) .    intuitively ,",
    "this is explained by the fact that the parameter @xmath55 in the laplace prior must be large in order to shrink coefficients @xmath8 to zero , but at the same time reasonable so that the laplace prior can model the nonzero coordinates . that these conflicting demands do not affect the good behavior of the lasso estimators",
    "must be due to the special geometric , sparsity - inducing form of the posterior mode , not to the bayesian connection .",
    "[ lemlb ] assume that we are in the setting of example  [ examplesequencemodel ] . for any @xmath265 such that @xmath266 , there exists @xmath267 such that , as @xmath268 , @xmath269",
    "the vector @xmath32 is the mean vector of the observation @xmath9 in ( [ model ] ) , and one might guess that this is estimable without identifiability conditions on the regression matrix @xmath2 . in this section",
    "we show that the posterior distribution based on the prior ( [ defprior ] ) can indeed solve this _ prediction problem _ at ( nearly ) optimal rates under no condition on the design matrix @xmath2 .",
    "these results are inspired by @xcite and theorem  [ thmm - orapred ] below can be seen as a full bayesian version of the results on the pac - bayesian point estimators in the latter paper ; see also @xcite for prediction results for mixtures of least - squares estimators .",
    "we are still interested in the sparse setting , and hence the regression matrix @xmath2 still intervenes by modeling the unknown mean vector @xmath270 as a linear combination of a small set of its columns .",
    "first , we consider the case of priors ( [ defprior ] ) that model the mean vector indirectly by modeling the set of columns and the coefficients of the linear combination .",
    "the prior @xmath30 comes in through the constant @xmath271 for the choice of prior on coordinates @xmath8 , the best results are obtained with heavy - tailed densities @xmath28 . in general",
    "the rate depends on the kullback ",
    "leibler divergence between the measure with distribution function @xmath272 ( corresponding to the prior density @xmath273 ) and the same measure shifted by @xmath274 .",
    "let @xmath275 be the kullback  leibler divergence , and set @xmath276    [ thmm - orapred ] for any prior @xmath15 and @xmath277 as in ( [ predcpi ] ) , any density @xmath28 that is symmetric about @xmath278 , any @xmath279 and @xmath280 , @xmath281    if the prior on the dimension satisfies ( [ assump.on_the_dim_prior ] ) with @xmath195 , then @xmath277 is bounded in  @xmath33 , and the rate for squared error loss is determined by @xmath282 this rate might be dominated by the kullback ",
    "leibler divergence for large signal  @xmath283 .",
    "however , for heavy tailed priors @xmath28 the induced constraints on the signal to achieve the good rate @xmath284 are quite mild .",
    "consider the prior distribution ( [ defprior ] ) with @xmath20 a product of @xmath37 univariate densities @xmath28 of the form @xmath285    [ corpredht ] if @xmath15 satisfies ( [ eq.classofdimpriors ] ) with @xmath286 , and @xmath28 is of the form ( [ def.ht ] ) with @xmath287 and @xmath288 , then for sufficiently large @xmath171 , @xmath289 for @xmath290 .    the constant @xmath291 in theorem  [ thmm - orapred ] can be improved to @xmath292 , for an arbitrary @xmath267 , by a slight adaptation of the argument .",
    "using pac - bayesian techniques dalalyan and tsybakov @xcite obtain an oracle inequality with leading constant @xmath293 for a so - called pseudo - posterior mean : the likelihood in ( [ bayes ] ) is raised to some power , which amounts to replacing the @xmath294 factor by @xmath295 .",
    "the `` inverse temperature '' @xmath296 must be taken large enough ; the case @xmath297 corresponding to the bayes posterior as considered here is not included ; see also @xcite .",
    "theorem  [ thmm - orapred ] and its corollary address the question of achieving prediction with no condition on @xmath2 , and the same rate is achieved as in section  [ sec.recovery ] with the same type of priors , up to some slight loss incurred only for true vectors @xmath10 with very large entries .",
    "as shown in the corollary , this slight dependence on @xmath10 can be made milder with flatter priors .",
    "we now consider a different approach specifically targeted at the prediction problem and which enables to remove dependency on the size of the coordinates of @xmath10 completely .",
    "because the prediction problem is concerned only with the mean vector , and the columns of @xmath2 will typically be linearly dependent , it is natural to define the prior distribution directly on the corresponding subspaces . for any @xmath108 ,",
    "let @xmath298 be the subspace of @xmath299 generated by the columns @xmath300 of  @xmath2 .",
    "let @xmath301 denote the collection of all _ distinct _ subspaces @xmath302 .",
    "define a ( improper ) prior @xmath303 on @xmath299 by first selecting an integer @xmath304 in @xmath305 according to a prior @xmath306 , next given @xmath304 selecting a subspace @xmath307 of dimension @xmath304 uniformly at random among subspaces in @xmath301 of dimension @xmath304 ; finally , let @xmath308 given @xmath309 be defined as lebesgue measure on @xmath309 if @xmath310 , and let @xmath303 be the dirac mass at @xmath311 for @xmath312 .",
    "note that the posterior distribution @xmath313 $ ] is a well - defined probability measure on @xmath299 .",
    "we choose , for a fixed @xmath314 ( the numerical constant @xmath315 is for simplicity ) , @xmath316 let @xmath317 and @xmath318 be the dimension of @xmath319 .",
    "[ thmm - improper ] let @xmath303 be the improper prior on @xmath299 defined above with @xmath320 as in  ( [ def - pn ] ) .",
    "for @xmath171 large enough , @xmath321 \\to0.\\ ] ]    the result is uniform in @xmath322 .",
    "also , note that @xmath323 if the true subspace @xmath319 is known , where @xmath324 denotes the orthogonal projection in @xmath299 into the subspace @xmath319 .",
    "in this section we survey computational methods to compute posterior distributions in the regression model ( [ model ] ) based on model selection priors ( [ defprior ] ) . in most cases , this is a `` spike and slab '' prior , as discussed in example  [ example.slabspike ] , implemented with auxiliary 01 variables that indicate whether a parameter @xmath250 is included in the model or not .",
    "the slab distribution is typically chosen a scale mixture of gaussian distributions , which may include the laplace law , which is an exponential mixture .",
    "most implementations also allow an unknown error variance ( which is taken to be unity in the present paper ) , with the inverse gamma distribution as the favorite prior",
    ".    for low - dimensional regression problems , computation of the posterior given mixture priors was studied by many authors , including @xcite .",
    "higher - dimensional settings have been considered recently : most of the following papers have appeared in the last five years , and a number of them are preprints .",
    "several authors @xcite have implemented _ mcmc _ schemes to simulate from the posterior distribution , coupled with _",
    "stochastic search _ algorithms that limit the model space , so as to alleviate the curse of dimensionality .",
    "besides computation time , monitoring the convergence of the samplers is an issue . for higher dimensions it is impossible to sample from the complete model space , but this should also not be necessary , as in sparse situations the posterior will concentrate on lower - dimensional spaces , as is also apparent from our theoretical results .",
    "bottolo et al .",
    "@xcite provide ready - made software , which runs on dimensions up to several thousands .",
    "the same authors have also exploited hardware solutions , such as graphical processing units , to speed up computations in genomic data analyses .",
    "_ sequential monte carlo methods _ or _ particle filters _ can be viewed as mcmc schemes that can more readily incorporate correct moves in the model space that ensure good approximation to the posterior distribution . in @xcite",
    "such methods are shown to perform well for model selection in regression models with up to hundreds of covariates .",
    "the _ shrinkage - thresholding metropolis adjusted langevin algorithm _ ( or stmala ) introduced in @xcite is another variation on earlier mcmc algorithms , targeted to work for @xmath106 , in , for instance , imaging applications .",
    "it jointly samples a model and a regression vector in this model , using proposals based on the gradient of the logarithm of the smooth part of the posterior distribution ( as in mala ) combined with applying a shrinkage - thresholding operator to set coordinates to zero .",
    "geometric convergence of the algorithm , which is capable of moving between rather distant models , is guaranteed for slab prior densities of the form @xmath325 , where @xmath326 .",
    "illustrations showing good practical performance are given in @xcite ( section  5.2 ) for values of @xmath327 equal to @xmath328 or @xmath329 .",
    "an alternative to simulation from the exact posterior is to compute an exact , analytic approximation to the posterior .",
    "a relatively simple and computationally efficient _ variational bayes approximation _ is proposed in @xcite and is shown to perform satisfactorily , but examples in the paper are limited to cases where @xmath330 .    by relaxing the spike at zero to a gaussian distribution with small variance , rokov and george @xcite succeeded in reducing computations of aspects of the posterior distribution , such as means and moments , to iterations of an efficient _ em - algorithm_. they show good performance with exponentially decreasing priors on model dimension , as considered in the present paper .    closely related to the spike and slab prior is _ exponential weighting _",
    ", where each of the @xmath331 models is given a prior weight , which is then updated with the likelihood function . a survey and numerical simulations in high - dimensional settings using the metropolis ",
    "hastings algorithm can be found in @xcite .",
    "stable reconstructions in dimensions up to @xmath332 , @xmath333 and sparsity level @xmath334 are shown to require usually no more than 2000 iterations .",
    "an ( empirical , pseudo- ) bayes approach with a spike and gaussian slabs centered at the least square solutions of the underlying model is implemented in @xcite .",
    "the algorithm , which can be initialized at the lasso estimator , is shown to perform well for @xmath5 up to 100 and @xmath33 up to 1000 . because the slabs are centered on data - based quantities , the target of this algorithm is different from the posterior distribution in the present paper . however , since the prior puts mass on all models , its computational complexity is comparable to the procedure in the present paper .    for the sequence model of example  [ examplesequencemodel ] ,",
    "an algorithm to compute posterior quantities such as modes and quantiles based on _ generating polynomials _ is implemented in @xcite .",
    "this is efficient in terms of computation time , but requires large memory .",
    "up to @xmath335 standard software and hardware suffice . the method may be extended to other designs by making suitable transformations @xcite .",
    "denote by @xmath336 the density of the @xmath337-distribution , and the corresponding log likelihood ratios by @xmath338    [ lem.expl_bd_for_denom ] for @xmath33 sufficiently large and any @xmath339 , with support @xmath340 and @xmath341 , and @xmath12 given by ( [ defprior ] ) with @xmath20 a product of laplace densities with scale  @xmath55 , we have , almost surely , @xmath342    for @xmath343 the right - hand side is @xmath344 , while the left - hand side is bounded below by @xmath345 , by ( [ defprior ] ) .",
    "thus we may assume that @xmath346 .",
    "first we prove that for any set @xmath27 and @xmath347 , @xmath348 if @xmath349 are i.i.d .",
    "random variables with the laplace distribution with scale parameter @xmath55 , then @xmath350 are i.i.d",
    ". exponential variables of the same scale .",
    "hence the left - hand side of the display , which is equal to@xmath351 , is the probability that the first @xmath14 events of a poisson process of intensity @xmath55 occur before time @xmath96 .",
    "this is identical to the probability that the poisson process has @xmath14 or more events in @xmath352 $ ] , which is the sum in the display .    by ( [ defprior ] )",
    ", the left - hand side of the lemma is bounded below by @xmath353 by ( [ eq.lr_representation ] ) , the change of variables @xmath354 and the inequality @xmath355 .",
    "the finite measure @xmath356 defined by the identity @xmath357 is symmetric about zero , and hence the mean of @xmath358 relative to @xmath356 is zero .",
    "let @xmath359 denote the normalized probability measure corresponding to @xmath356 , that is , @xmath360 .",
    "let @xmath361 denote the expectation operator with respect to @xmath359 .",
    "define @xmath362 . by jensen s inequality @xmath363 .",
    "however , @xmath364 , by the just mentioned symmetry of  @xmath356 .",
    "so the last display is bounded below by @xmath365 almost surely . using that @xmath366 , and then ( [ eq.int_explicit ] )",
    ", we find that the integral in the last display is bounded below by @xmath367 with ( [ eq.lambda_cond ] ) , @xmath368 is bounded from below by @xmath369 , if @xmath370 and by @xmath371 , if @xmath372 .",
    "since @xmath373 and @xmath374 decays to zero slower than any polynomial power of @xmath33 , we find @xmath375 in both cases , provided that @xmath33 is sufficiently large .",
    "the lemma follows upon substituting these bounds and the bound @xmath376 in the display .",
    "[ lem.com ] for any @xmath377 and random variable @xmath378 , @xmath379    write the left - hand side as @xmath380 $ ] , and use the cauchy ",
    "schwarz inequality ; see , for example , @xcite , lemma  6.1 .",
    "[ eq.mcal_t_def ] @xmath381    under the probability measure @xmath382 the vector @xmath383 possesses an @xmath5-dimensional standard normal distribution , whence the @xmath33 coordinates of the vector @xmath384 are normal with variances @xmath385 .",
    "now @xmath386 , which can be bounded by the tail bound for the normal distribution .",
    "[ thmm.mod_sel_gen ] if @xmath55 satisfies ( [ eq.lambda_cond ] ) and the prior @xmath15 satisfies ( [ assump.on_the_dim_prior ] ) , then for any @xmath160 , @xmath387    by the definition of @xmath58 in ( [ eq.lambda_cond ] ) and lemma  [ eq.mcal_t_def ] , the complement of the event @xmath388 has @xmath382-probability bounded by @xmath389 . by combining this with lemma  [ lem.com ]",
    "we see that for any @xmath41 and any measurable set @xmath242 , @xmath390 \\bigr)^{1/2 } + \\frac{2}p . \\label{eq.main_decomp_mod_sel_gen}\\ ] ] by bayes s formula followed by lemma  [ lem.expl_bd_for_denom ] , with @xmath391 the likelihood ratio given in ( [ eq.lr_representation ] ) , @xmath392 \\\\[-8pt ] \\nonumber & \\leq & \\frac{e p^{2s_*}}{\\pi_p(s _ * ) } e^{{\\lambda}\\|{\\beta}^*\\|_1 } \\int_b e^{-({1}/2 ) \\|x({\\beta}-{\\beta}^*)\\|_2 ^ 2 + ( y - x{\\beta}^*)^t x({\\beta}-{\\beta}^ * ) } \\,d\\pi({\\beta}).\\end{aligned}\\ ] ] using hlder s inequality @xmath393 and the cauchy ",
    "schwarz inequality , we see that on the event @xmath394 , @xmath395 therefore , on the event @xmath396 , the expected value under @xmath397 of the integrand on the right - hand side of ( [ eq.post_exp_bd_gen ] ) is bounded above by @xmath398 e^{({{\\lambda}}/{(2{\\overline{\\lambda } } ) } ) l({\\beta } ) } \\\\ & & \\qquad = e^{-({1}/2 ) ( 1-(1-{{\\lambda}}/{(2{\\overline{\\lambda}})})^2 ) \\|x({\\beta}-{\\beta}^*)\\|_2 ^ 2 } e^{({{\\lambda}}/{(2{\\overline{\\lambda } } ) } ) l({\\beta } ) } \\\\ & & \\qquad \\le e^{\\|x({\\beta}^0-{\\beta}^*)\\|_2 ^ 2 } e^{-({{\\lambda}}/{(8\\overline{\\lambda } ) } ) \\|x({\\beta}-{\\beta}^*)\\|_2 ^ 2+({{\\lambda}}/2 ) \\|{\\beta}-{\\beta}^*\\|_1},\\end{aligned}\\ ] ] where we use that @xmath399 .",
    "it follows that the expected value @xmath400 $ ] under @xmath41 of ( [ eq.post_exp_bd_gen ] ) over @xmath396 is bounded above by @xmath401 by the triangle inequality , @xmath402 for @xmath403 , as is seen by splitting the norms on the right - hand side over @xmath340 and @xmath404 .",
    "if @xmath405 , then we write @xmath406 and use the definition of the compatibility number @xmath407 to find that @xmath408 we combine the last three displays to see that ( [ eqexpectedoverto ] ) is bounded above by @xmath409 for the set @xmath410 and @xmath411 , the integral in this expression is bounded above by @xmath412 by assumption ( [ assump.on_the_dim_prior ] ) . combining the preceding with ( [ eq.main_decomp_mod_sel_gen ] ) , we see that @xmath413 using that @xmath414 , we can infer the theorem by choosing @xmath415 for fixed @xmath160 .",
    "proof of theorem  [ thmm.pred_and_l1 ] by theorem  [ thmm.mod_sel_gen ] the posterior distribution is asymptotically supported on the event @xmath416 , for @xmath417 and @xmath418 the same expression with @xmath41 replaced by @xmath10 .",
    "thus it suffices to prove that the intersections of the events in the theorem with the event @xmath419 tends to zero . by combining ( [ eq.post_exp_bd_gen ] ) ,",
    "( [ eqestimateinnerproduct ] ) and the inequality @xmath420 , we see that on the event @xmath388 , the variable @xmath421 is bounded above by @xmath422 by definition  [ assump.uniform_comp ] of the uniform compatibility number , @xmath423 since @xmath424 , on the event @xmath419 and @xmath425 by assumption , it follows from ( [ defpsis ] ) that for a set @xmath426 , @xmath427 \\\\[-8pt ] \\nonumber & & { } \\times\\int_b e^{-({1}/8 ) \\|x({\\beta}-{\\beta}^*)\\|_2 ^ 2 -{\\overline{\\lambda}}\\|{\\beta}-{\\beta}^*\\|_1+{\\lambda}\\|{\\beta}\\|_1 } \\,d\\pi({\\beta}).\\end{aligned}\\ ] ] since @xmath428 it suffices to show that the right - hand side tends to zero for the relevant event @xmath51 .",
    "_ proof of first assertion_. on the set @xmath429 , we have @xmath430 , by the triangle inequality .",
    "note that @xmath431 .",
    "it follows that for the set @xmath51 , the preceding display is bounded above by @xmath432 by ( [ assump.on_the_dim_prior ] ) and a calculation similar to the proof of theorem  [ thmm.mod_sel_gen ] .",
    "for @xmath433 this tends to zero .",
    "thus we have proved that for some sufficiently large constant  @xmath171 , @xmath434 _ proof of second assertion_. similar to ( [ eqinvokecompcond ] ) , @xmath435 the claim follows now from the first assertion .",
    "_ proof of third assertion_. note that @xmath436 .",
    "now , the proof follows from the first assertion .",
    "proof of theorem  [ thmm.bvm_type ] the total variation distance between a probability measure @xmath12 and its renormalized restriction @xmath437 to a set @xmath438 is bounded above by @xmath439 .",
    "we apply this to both the posterior measure @xmath440 and the approximation @xmath441 , with the set @xmath442 where @xmath171 is a sufficiently large constant . by theorem  [ theoremrecovery ]",
    "the probability @xmath443 tends to one under @xmath444 , and at the end of this proof we show that @xmath445 tends to one as well .",
    "hence it suffices to prove theorem  [ thmm.bvm_type ] with @xmath440 and @xmath441 replaced by their renormalized restrictions to @xmath438 .",
    "the measure @xmath446 is by its definition a mixture over measures corresponding to models @xmath447 . by theorems  [ thmm.mod_sel ] and  [ theoremrecovery ] the measure",
    "@xmath448 is asymptotically concentrated on these models .",
    "if @xmath449 is the renormalized restriction of a probability vector @xmath450 to a set @xmath451 , then , for any probability measures @xmath452 , @xmath453 by the preceding paragraph .",
    "we infer that we can make a further reduction by restricting and renormalizing the mixing weights of @xmath440 to @xmath451 .",
    "more precisely , define probability measures by @xmath454 then it suffices to show that @xmath455 .",
    "( the factor @xmath456 in the second formula cancels in the normalization , but is inserted to connect to the remainder of the proof . )    for any sequences of measures @xmath457 and @xmath458 , we have @xmath459 if @xmath460 is absolutely continuous with respect to @xmath461 with density @xmath462 , for every  @xmath27 .",
    "it follows that @xmath463 this tends to zero by the definition of @xmath438 and the assumptions on @xmath10 .",
    "finally we show that @xmath464 . for @xmath465 , the likelihood ratio given in  ( [ eq.lr_representation ] )",
    ", we have @xmath466 by ( [ eq.lr_representation ] ) the denominator in @xmath467 , and for the second inequality we use jensen s inequality similarly as in the proof of lemma  [ lem.expl_bd_for_denom ] .    using hlder s inequality @xmath468",
    ", we see that on the event @xmath469 , @xmath470 since @xmath471 for every @xmath472 , it follows that on @xmath396 the numerator in @xmath473 is bounded above by @xmath474 it follows that @xmath473 is bounded above by @xmath475 by jensen s inequality applied to the logarithm @xmath476 , and hence @xmath477 , by ( [ eq.lambda_cond ] ) .",
    "the prior mass @xmath30 can be bounded below by powers of @xmath478 by ( [ assump.on_the_dim_prior ] ) .",
    "this shows that the display tends to zero for sufficiently large @xmath171 .",
    "proof of theorem  [ theoremselectionnosupersets ]",
    "let @xmath479 be the collection of all sets @xmath447 such that @xmath480 and @xmath481 . in view of theorem  [ thmm.bvm_type ]",
    "it suffices to show that @xmath482 .",
    "note that due to @xmath195 , any set in @xmath447 has cardinality smaller @xmath483 . by ( [ eqdefweightsw ] ) , with @xmath484 ,",
    "@xmath485 we shall show below that the factors on the right - hand side can be bounded as follows : for any fixed @xmath486 , @xmath487 combining these estimates with assumption ( [ assump.on_the_dim_prior ] ) shows that for @xmath488 , the event in the second relation , @xmath489 for @xmath490 we have @xmath491 .",
    "thus the expression tends to zero if @xmath492 . since",
    "@xmath96 can be chosen arbitrarily close to @xmath493 , this translates into @xmath198 .",
    "to prove bound ( [ eqseparateinequalities ] ) , we apply the interlacing theorem to the principal submatrix @xmath494 of @xmath495 to see that @xmath496 , for @xmath497 , where @xmath498 denote the eigenvalues in decreasing order , whence @xmath499 assertion ( [ eqseparateinequalities ] ) follows upon combining this with ( [ eq.lambda_cond ] ) .    to bound the probability of the event @xmath488 in ( [ eqseparateinequalitiestwo ] ) , we note that by the projection property of the least squares estimator , for @xmath480 the difference @xmath500 is the square length of the projection of @xmath9 onto the orthocomplement of the range of @xmath501 within the range of @xmath223 , a subspace of dimension @xmath502 .",
    "because the mean @xmath503 of @xmath504 is inside the smaller of these ranges , it cancels under the projection , and we may use the projection of the standard normal vector @xmath4 instead . thus the square length possesses a chi - square distribution with @xmath502 degrees of freedom .",
    "there are @xmath505 models @xmath506 that give rise to such a chi - square distribution .",
    "since @xmath507 , we can apply lemma  [ lem - chisq ] with @xmath508 to give that @xmath509 is bounded above by @xmath510 .",
    "this tends to zero as @xmath511 , due to @xmath512 , where the last inequality follows from @xmath513 .",
    "[ lem - chisq ] for every @xmath486 , there exists a constant @xmath514 independent of @xmath515 and @xmath516 such that for any variables @xmath517 that are marginally @xmath518 distributed , @xmath519    by markov s inequality , for any @xmath520 , @xmath521 the results follows upon choosing @xmath522 , giving @xmath523 and @xmath524 .",
    "proof of theorem  [ theoremselection ] _ proof of first two assertions_. because @xmath525 , the posterior probability of the set @xmath526 tends to zero by theorem  [ thmm.pred_and_l1 ] .",
    "this implies the first assertion .",
    "the second assertion follows similarly from the second assertion of theorem  [ thmm.pred_and_l1 ] .",
    "_ proof of third assertion_. first we prove that the largest coefficient in absolute value , say @xmath527 , is selected by the posterior if this is above the threshold . by theorem  [ thmm.bvm_type ]",
    "it is enough to show that @xmath528 .",
    "for any given set @xmath27 with @xmath529 , let @xmath530 and @xmath44 .",
    "then @xmath531 we shall bound this further by showing that @xmath532 , for every @xmath27 in the sum .",
    "the quotient of these weights is equal to @xmath533 in view of ( [ assump.on_the_dim_prior ] ) . by the interlacing theorem ,",
    "the eigenvalues @xmath534 in increasing order of the matrices @xmath495 and @xmath535 satisfy @xmath536 , for any @xmath537 .",
    "this implies that @xmath538 . since @xmath539 , for any @xmath13 ,",
    "the largest eigenvalue @xmath540 is at most @xmath541 . combining this with ( [ eq.lambda_cond ] )",
    ", we conclude that the preceding display is bounded below by @xmath542 by definition of the least squares estimator , the difference of the square norms in the exponent is the square length of the projection of @xmath504 onto the orthocomplement @xmath543 of the range of @xmath223 in the range of @xmath544 , the one - dimensional space spanned by the vector @xmath545 , where @xmath546 denotes the projection onto the range of @xmath223 .",
    "if , with an abuse of notation , @xmath547 is the projection onto @xmath543 , then @xmath548 \\\\[-8pt ] \\nonumber & = & \\frac{\\langle x{\\beta}^0,x_m - p_sx_m\\rangle^2}{2\\|x_m - p_sx_m\\|_2 ^ 2 }   -\\frac{\\langle{\\varepsilon},x_m - p_sx_m\\rangle^2}{\\|x_m - p_sx_m\\|_2 ^ 2}.\\end{aligned}\\ ] ] we shall show that the first term on the right is large if @xmath549 is large , and the second is small with large probability .",
    "we start by noting that for @xmath550 and any @xmath27 , @xmath551 \\\\[-8pt ] \\nonumber & = & \\frac{1}{\\widetilde\\phi(s)^2\\|x\\|^2}\\sum_{i\\in s}\\bigl(x^tx \\bigr)_{i , j}^2 \\le\\frac{s{\\mathop{\\mathrm { mc } } } ( x)^2\\|x\\|^2}{\\widetilde\\phi(s)^2}.\\end{aligned}\\ ] ] it follows from the definitions that @xmath552 , for every @xmath80 .",
    "combined , this shows that @xmath553 if @xmath554 .",
    "we write @xmath555 , for @xmath556 the matrix obtained by removing the column @xmath557 from @xmath2 , and split the first inner product in ( [ eqsizeprojection ] ) in the two parts @xmath558 using that @xmath559 if @xmath560 , the definition of @xmath561 to bound @xmath562 , the cauchy  schwarz inequality on @xmath563 and ( [ eqprojnormestimate ] ) . putting the estimates together",
    "we find that for @xmath564 , @xmath565 we can split the random inner product in ( [ eqsizeprojection ] ) in the two parts @xmath566 and @xmath567 . for @xmath568 , @xmath569 each variable @xmath570 is normally distributed with mean zero and variance @xmath571 , for any @xmath572 .",
    "when @xmath573 varies over @xmath574 and @xmath27 over all subsets of size @xmath14 that do not contain @xmath573 , there are @xmath33 possible variables in the first term and @xmath575 possible variables in the second .",
    "for @xmath576 the variances of the variables in the two terms are of the orders @xmath577 and @xmath578 , respectively .",
    "therefore the means of the two suprema are of the orders @xmath579 and @xmath580 , respectively , if @xmath581 . with probability @xmath582",
    "these variables do not exceed a multiple of their means .",
    "we conclude that for @xmath583 and @xmath584 , the left - hand side of ( [ eqsizeprojection ] ) is , with probability tending to one , bounded below by @xmath585 , whence for @xmath586 for large @xmath171 , uniformly in @xmath587 , @xmath588 for @xmath326 as large as desired ( depending on @xmath171 ) and @xmath514 a suitable positive constant .",
    "so , with overwhelming probability , @xmath589 thus @xmath590 at the order @xmath591 .    next ,",
    "for @xmath592 the second largest coefficient , we consider @xmath593 . by reasoning similar to the preceding , we show that the index @xmath594 is included asymptotically , etc .",
    "we thank an associate editor and four referees for valuable comments .",
    "we are also grateful to amandine schreck for helpful discussions ."
  ],
  "abstract_text": [
    "<S> we study full bayesian procedures for high - dimensional linear regression under sparsity constraints . </S>",
    "<S> the prior is a mixture of point masses at zero and continuous distributions . under compatibility conditions on the design matrix </S>",
    "<S> , the posterior distribution is shown to contract at the optimal rate for recovery of the unknown sparse vector , and to give optimal prediction of the response vector . </S>",
    "<S> it is also shown to select the correct sparse model , or at least the coefficients that are significantly different from zero . </S>",
    "<S> the asymptotic shape of the posterior distribution is characterized and employed to the construction and study of credible sets for uncertainty quantification .    </S>",
    "<S> ./style / arxiv - general.cfg    ,     + </S>"
  ]
}