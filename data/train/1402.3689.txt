{
  "article_text": [
    "in order to naturally interact with objects and people , robots need robust and efficient perception capabilities .",
    "for example , human - robot interaction requires the recognition of gestures , actions , and facial expressions .",
    "there has been tremendous progress towards endowing robots with visual perception .",
    "nevertheless , the visual modality has its own limitations , e.g. , it can not operate in bad ( too dark or too bright ) lighting conditions , and the interaction is inherently limited to objects and people that are within the visual field . in parallel to visual information , _ sounds _ produced by objects , by humans , or human - object interactions convey rich cognitive information about the ongoing context , events , and communicative behaviors .    compared to visual analysis , audio analysis is complementary but it also has its own advantages .",
    "visual data are huge , visual information is complex to extract , and hence efficient visual routines may be difficult to embed into the robot s onboard hardware / software resources .",
    "in contrast , acoustic signal processing may be quite efficient , because the lower amount of data to be analyzed ( depending however on the complexity of the acoustic scene ) . by using hearing",
    ", a robot may be able to recognize the ongoing events , estimate their relevance , and take appropriate decisions , even if they are not within the range of the visual sensors .",
    "moreover , proper recognition and localization of sound events may be used to trigger visual attention mechanisms .",
    "therefore , audition is considered with increasing attention by robotic practitioners since hearing capabilities are likely to considerably improve the overall `` cognitive understanding '' of a scene as an extended catalogue of events , and improve the interactive capabilities of robots with humans , as well as with animals and objects , including other robots .",
    "this is related to computational auditory scene analysis ( casa ) which attempts to model the abilities of human audition , notably to segregate coherent auditory streams  @xcite .",
    "it covers a set of challenging problems some of which have already been successfully investigated in robotics : multiple - source localization @xcite and separation @xcite , speech recognition @xcite , speech / non - speech / music classification , detection / segmentation and recognition of elementary sounds ( possibly in background signal / noise ) , etc .",
    "some of these modules were successfully integrated in robotic platforms , e.g. , hark @xcite and asimo @xcite , to cite just a few .    in the framework of robot audition ,",
    "this paper addresses isolated recognition of `` domestic sounds '' .",
    "we address both audio - signal representation and classification .",
    "the audio recordings are collected with a nao robot manufactured by aldebaran - robotics .",
    "similar benchmarks can be found for example in @xcite ( for scene recognition ) , @xcite , @xcite and @xcite .",
    "this setup implies notable difficulties , the most notable one being the low microphone quality currently available with nao .",
    "the collected sounds are from a real - world scenario , e.g. , fig .",
    "[ nao_bip ] : there are different types of sound sources , located at different ( more or less distant ) positions relatively to the robot head .",
    "the recorded audio signals are perturbed by room reverberations and by various linear or non - linear filtering effects ( notably the robot s head - related transfer function which is difficult to estimate ) .",
    "the sounds are corrupted by the internal noise coming from the hardware inside the robot head . also , the robot has limited computational capabilities , and this is expected to have a strong influence on the choice of signal representation and classification algorithms , as detailed below .",
    "the experimental data and used in this paper stays in contrast with clean sound databases recorded with high - quality microphones in specially equipped rooms .",
    "moreover , automatic speech recognition ( asr ) techniques often use close - range microphones which is not the case here since the robot is at some distance from the audio sources .",
    "we consider short sounds , typically in the range 0.1 to 1.0 seconds , that result from such events as the opening / closing of a door , people dropping an object or clapping hands , as opposed to continuous sounds or continuous sound streams .",
    "many of these short sounds have an impulsive nature , and they are assumed to have well - defined start- and end - points . therefore , basic detection techniques based on signal energy or other statistics can be used to pre - segment the signal before classification @xcite , and we do not address the detection / segmentation problem in the present paper : we assume that a correct segmentation of these short sounds is available .",
    "we also assume that the sounds do not overlap in time .",
    "non - stationary sound streams such as continuous speech or music signals are not considered in the present study ( although our dataset contains isolated spoken words , see section  ii ) .",
    "continuous speech is usually processed with specific classifiers , e.g. , hidden markov models ( hmms ) , that model the dynamic evolution of the spectral patterns corresponding to the successive phonemes @xcite .",
    "music signals are particularly tricky to process because of the richness of their content .",
    "more stationary sound streams such as the flow of tap water , washing machine , fans , etc . , are not considered as well .",
    "the latter category can be considered either as long sound events or as background noise / context for overlapping short sound events .",
    "all these problems will be considered in future extensions of the present work which focuses on implementation of short sounds recognition in a robotic context .",
    "note that this task is not trivial in itself , even without the limitations of the robotic context , depending on the number and complexity of the sound categories .",
    "for example , different objects can produce similar sounds that should , or should not , be classified together depending on the application . on the opposite ,",
    "the same physical object can produce different types of sounds that may not belong to the same category .",
    "our dataset contains 42 sound categories , which is a quite substantial number of sound types , as compared to previous studies , e.g. , 10 as in @xcite , 15 - 16 as in @xcite or 22 categories @xcite .",
    "our main goal is to carry out a benchmark assessing different signal representations ( audio features ) and different classifiers , in the spirit of what was done in , e.g. ,  @xcite for environmental sound recognition .",
    "we selected several feature spaces to represent sounds , as well as a number of classification techniques .",
    "many possible combinations of features and classifiers were tested , possibly to reveal general trends and propose an optimal solution .    obviously , the accuracy score is the most important gauge for a classifier .",
    "the tested techniques are dedicated to be embedded in autonomous robots , hence other important indicators are analyzed and reported .",
    "first , robots have to work in ( quasi ) real - time , therefore execution has to be as fast as possible .",
    "three time statistics are provided : the _ feature computation time _",
    "( time to compute features from a raw signal of a given length ) , _ training time _ ( time to train all the models for classification ) , and _ recognition time _",
    "( time to classify a new incoming sound of a given length ) .",
    "secondly , memory requirement is also a valuable resource in an embedded system , and we estimate the _ training memory _ ( memory used to store the trained models ) . getting the accuracy score , the computation times and memory costs for",
    "each feature / classification method will allow us to find optimal solution(s ) or good trade - offs for reliable sound recognition with a consumer robot .",
    "the remainder of this paper is organized as follows .",
    "section  [ sec : data ] describes in detail the dataset recorded and used in this study .",
    "sections  [ sec : features ] and  [ sec : classifiers ] present respectively the different features and classifiers that were used .",
    "experiments and results are presented in section  [ sec : experiments ] . conclusions and",
    "the future work are expanded in section [ sec : futurework ] .",
    "the dataset must have the following characteristics : ( i ) to be recorded with low - quality sensors , ( ii ) to suffer from typical internal robot noise , ( iii ) to be recorded in realistic domestic environments , i.e. , in rooms with no special acoustic characteristics , presence of reverberations and of multiple sound - source randomly distributed across the room , and ( iv ) containing a substantial number of real - world sound types with only a few samples per class .",
    "up to our knowledge , no existing database that fulfills these requirements is available .",
    "therefore , we recorded a database by placing nao in both a home and an office , and by using its frontal 300hz  18khz bandpass microphone .",
    "the collected signals are sampled at 48khz and quantized at 16 bits per sample .",
    "the robot - head fan produces noise within the band from 0 to 4  khz , shading weak sounds . during recording",
    ", the robot stands still and hence is not affected by noise generated by its motion .",
    "the dataset is available online .",
    "four scenarios and 42 sound classes were considered , as summarized in table  [ tab : data_sets ] .    * * kitchen * : the first part contains a large variety of everyday sounds collected in a home kitchen .",
    "we recorded 12 sound categories with different temporal and spectral characteristics : impulsive sounds ( _ close the microwave _ , _ choking _ ) , harmonic sounds ( _ microwave alarm _ ) and transient sounds ( _ running the tap _ , _ eating _ ) .",
    "the sounds were recorded from three different positions , 1 to 5 meters range and at various angles from the sound source . at each position ,",
    "seven instances of each class were recorded , which sums up to 21 examples per class . * * office * : the second part is related to an office environment .",
    "we acquired seven sounds : _ door close , door open , door key , door knock , ripped paper , zip , ( another ) zip .",
    "_ they were randomly recorded from 0.3 to 5 meters range and from various angles .",
    "all the sound related to door actions were recorded using different doors . * * non - verbal * : the third part of the data contains non - verbal sounds , which are produced by humans , and can be seen as communication signals , but typically not taken into account in asr systems .",
    "there are three classes ( _ fingerclap , handclap , tongue clic _ ) recorded from 0.3 to 5 meters range and from various angles , with four different people . *",
    "* speech * : the fourth part of the dataset contains occurrences of isolated words .",
    "even if speech recognition is not in the scope of the present work , we judged of great interest to test methods designed for short sounds recognition on such speech samples .",
    "hence , we recorded twenty word classes from four different people placed in front of nao , roughly one meter away .",
    "except for the _ kitchen _ classes , each class has 20 instances which made a total number of 852 sounds recorded for the whole dataset .",
    "considering that detection step is not addressed in this study , each sound has been manually segmented using an audio editor . as an illustration of the signals `` quality '' ,",
    "[ snr_dataset ] shows the signal - to - noise ratio ( snr ) statistics for each class , the noise being here the internal noise , measured during absence of any external sound .",
    ".taxonomy of the recorded data set classes . [ cols=\"^,^,^\",options=\"header \" , ]",
    "in this section , we present the different signal representations that were tested in our classification benchmark .",
    "although quite short ( see introduction ) , the considered signals are generally non - stationary , hence most of the features are actually _ time sequences _ of _ feature vectors _ computed using the very usual short - term sliding window approach widely used in audio processing . except when specified , the window analysis is a 30ms hamming window with 50% overlap .",
    "all the features introduced in this section have been proposed in the audio processing literature @xcite .",
    "we compute the energy as the root mean square of the samples in an audio frame ( the rectangular window is used here ) .",
    "it can be seen a measure of amplitude variation over time .",
    "defined as the number of zero crossings in an audio frame .",
    "it can be used to classify voiced and unvoiced speech sounds , and it has also been used to differentiate speech , music and background noise  @xcite .      this feature is the total duration of the detected sound expressed in seconds . therefore , in addition to being a scalar value , it is the only feature that is not extracted on a short - time basis . it may help to distinguish short , e.g. percussive , sounds from longer ones .",
    "all these features are computed using the short - term fourier tranform ( stft ) of the signal .",
    "@xmath0 denotes the @xmath1-th _ magnitude _ coefficient of the @xmath2-point stft frame at time @xmath3 .",
    "the spectral roll - off is the cut - off frequency below which 99% the spectral energy is contained .",
    "it is used in speech recognition to classify voiced and unvoiced speech  @xcite .",
    "those features characterize the overall shape of the spectrum using @xmath4-order moments of frequency bin weighted by spectral magnitude : @xmath5 the first moment , or spectral centroid or brightness , corresponds to the mean value of the weighted frequency .",
    "the second order moment measures the spread of the frequency distribution around the mean .",
    "the third order moment , or skewness , is a measure of the asymmetry of the distribution .",
    "the kurtosis ( fourth order moment ) is a measure of the `` peakedness '' of the distribution .",
    "the two features represents the global amount of decreasing of the spectral amplitude .",
    "the spectral slope is estimated by linear regression .",
    "@xmath6 where @xmath7 represents the value of the linear regression at bin @xmath1 ( and at time @xmath3 ) .",
    "the formulation of the spectral decrease comes from perceptual studies and tries to be coherent with human hearing  @xcite . @xmath8",
    "an estimation of the flatness of the magnitude spectrum is obtained by the ratio between its arithmetic and geometric mean ( flat if @xmath9 or peaky if @xmath10 ) : @xmath11      the two features measure the average variation of spectral coefficients between two consecutive frames : @xmath12 @xmath13      widely used in speech and speaker recognition @xcite , mfccs are cepstral coefficients that represent the spectrum envelope on a perceptive mel - frequency scale .",
    "those coefficients are computed as the discrete cosine transform ( dct ) of the logarithm of fft power coefficients passed through a mel - filter bank ( 40 log - spaced bands in the range 300hz-10000hz according to the following mel - scale @xmath14 ) .",
    "usually , the first coefficient is omitted and the first and second derivatives of the remaining coefficients can be added .",
    "the wavelet transform @xcite transpose a signal from time domain to time - frequency domain like the stft although the different family of basis functions , allowing multi - resolution analysis to get a variable time and frequency resolution .",
    "the discrete version of the transform @xcite uses a @xmath15-stage cascade of a downsampling by 2 and a high - pass and low - pass filter .",
    "thus , a signal @xmath16 can be decomposed on @xmath17 and @xmath18 with @xmath19 called respectively the approximation coefficients and the details coefficients .",
    "inspired from @xcite and @xcite , the feature vector is the concatenation of the mean and the standard deviation of the coefficients @xmath20 and @xmath21 with @xmath22 .",
    "the experiments use an 8th order decomposition on a 8-coefficient daubechies family .      based on modelling of the human cochlea ,",
    "the auditory image model ( aim ) of @xcite produces stabilized auditory images ( sai ) , which are a time delay - frequency sound representation close to a correlogram . the process chains three main stages , multi - channel gammatone filter bank , half - wave rectification and triggered time integration , and leads to a representation with high dimensionality .",
    "a technique was proposed in @xcite to reduce the dimensionality of the sai features .",
    "this procedure consists of three steps : create patches from the sai , compute a low - dimensional vector representation of each patch , and concatenate these patch feature vectors to form the final feature vector .",
    "depending on the feature nature , the successive feature vectors @xmath23 of a given sound can be further processed to produce different final features , which will feed the classifiers :    * the _ sequencing _ i.e. simple concatenation , of the ( original ) successive vectors @xmath24 $ ] . *",
    "the _ mean _ of the vectors over the entire acoustic event .",
    "the concatenation of the mean and standard deviation can also be used .",
    "* the _ bag - of - words _ ( bow ) approach .",
    "the features of all sounds are first clustered using the @xmath25-means algorithm .",
    "then , each sound has its feature vectors quantized using the resulting centroids , and is then represented as the _ normalized histogram _ of centroid occurrences . * the _ interpolation _ of the feature vector sequence to the mean duration @xmath26 of all vector sequences in the database .",
    "each sound is thus represented by @xmath26 interpolated feature vectors sequenced into @xmath27 $ ] .",
    "the interpolation enables to normalize the vector sequence along the time axis , so that the new representation can be used by `` fixed data length '' classifiers .",
    "it amounts to a simplified dynamic time warping ( dtw ) applied `` blindly '' , i.e. without inspecting the fine structural organization of the sounds .",
    "the bag - of - words also intrinsically enables ( temporal ) normalization but without taking into account the timeline ordering of the vector sequence .",
    "finally , it can also be noted that the final feature representation may also consist of the ( row - wise ) concatenation of several different features .",
    "this is a particular ( straightforward ) case of information fusion for classification , a vast domain which deepened investigation in the context of sound recognition by a robot is out of the scope of the present paper .",
    "the computation of the wavelets has been done using the matlab wavelet toolbox .",
    "sai features are available at @xcite .",
    "all other features have been computed with the python / c++ toolbox yaafe @xcite .",
    "in this section all the tested classifiers are described .",
    "a multiclass classifier consists of a mapping @xmath28 , where @xmath29 is the feature space , @xmath30 is the set of labels and @xmath31 is the number of classes",
    ". the dimension of @xmath29 may be fixed or varying with the sound , depending on the feature used . given a feature vector ( or sequence of feature vectors ) @xmath32 , @xmath33 is the score of classifying @xmath34 as @xmath35 .",
    "the higher the score is , the more likely @xmath35 is the class of @xmath34 . hence , a new unlabelled observation @xmath32 is classified as : @xmath36 in the following , @xmath37 will denote the training set , i.e. a set of feature vectors @xmath38 which class is known , and that is used to train the classifiers .      the @xmath1-nearest neighbors ( @xmath1-nn ) classifier",
    "is based on the well - known @xmath1-nn algorithm which returns the subset of @xmath39 , containing the @xmath1 closest points to a given vector @xmath34 .",
    "the mapping of the @xmath1-nn classifier is : @xmath40 where @xmath41 means the class of @xmath42 .",
    "@xmath43 is the number of feature vectors among the @xmath1-nearest neighbors of @xmath34 that belong to the class @xmath35 . in other words ,",
    "each of the @xmath1 neighbors votes for its own class , and the class with more votes is assigned to @xmath34 .      the previous method needs to keep in memory all the training data during the recognition stage .",
    "qnn is able to circumvent this issue by quantizing the features previously to the nearest neighbor search .",
    "more precisely , the vectors are first divided in @xmath44 parts , leading to @xmath44 feature subspaces .",
    "if @xmath45 denotes the @xmath46-th part of the @xmath4-th training vector , we define @xmath47 , the training set of the @xmath46-th feature subspace .",
    "a @xmath25-means algorithm @xcite is ran for every @xmath48 , providing for a set of centroids .",
    "the quantization function , that assigns the @xmath46-th subvector @xmath49 of @xmath34 to its closest centroid is denoted by @xmath50 . in that case",
    "the mapping @xmath51 is : @xmath52 where @xmath53 .",
    "this corresponds to finding the quantized vector in the training set closest to the quantized test vector , and assigning its class to @xmath34 .",
    "see @xcite for more details on this technique .",
    "the method is parametrized by @xmath25 and @xmath44 .",
    "the higher @xmath25 and @xmath44 are , the more costly the method is , and the higher the recognition rate is . increasing @xmath44 may allow us to reduce @xmath25 with no negative effects on the recognition rate .",
    "the support vector machines ( svm ) is a discriminative binary classification method @xcite .",
    "it has been used in sound recognition in multiple situations as in @xcite and @xcite with hierarchical structures or in @xcite with 1-class svms .",
    "svms provides a discriminative function @xmath54 , learnt form a set of positive examples and a set of negative examples .",
    "the points satisfying @xmath55 form a hyperplane in the space induced by the kernel function @xmath56 .",
    "@xmath57 means that @xmath34 should be classified as positive and @xmath58 as negative .",
    "we refer the reader to @xcite for details on the formulation .",
    "importantly , a parameter @xmath59 regulates the amount of allowed misclassification in the training set , such that svms deal with overlapping classes .",
    "since svms are binary classifiers , two strategies have been developed to use them in the multiclass task .",
    "on one hand the _ one - versus - rest _ ( 1vr ) , in which @xmath31 different svms are trained , one per class . in that case the mapping @xmath51 is defined as @xmath60 where @xmath61 is the discriminant function trained with @xmath62 and @xmath63 . on the other hand the _ one - versus - one _ ( 1v1 ) strategy , which corresponds to evaluate all possible binary classification problems with @xmath31 classes .",
    "the classification mapping is then : @xmath64 where @xmath65 is the discriminant function trained with @xmath62 and @xmath66 . as for @xmath1-nns , this is equivalent to say that each svm is voting for one class and @xmath34 is classified to the class with more votes . in our experiments ,",
    "the 1v1 approach always outperformed 1vr both in terms of accuracy and speed , and we only consider 1v1 in the following .",
    "five different kernels are tested , namely : linear @xmath67 , polynomial @xmath68 , radial basis @xmath69 , sigmoid @xmath70 , and @xmath71 , @xmath15 being the dimension of the features .",
    "the parameters of the svms are the misclassification regulation parameter @xmath59 , the multiclass strategy , the kernel used and , if any , the kernel parameters .",
    "the gaussian mixture model ( gmm ) is a probabilistic generative model widely used in classification tasks . in our case",
    ", we use one gmm per sound class .",
    "each gmm is a weighted sum of @xmath15 gaussian components ( in this model , each observation is assumed to be generated by one of these components ) , which parameter set denoted by @xmath72 is composed of @xmath15 weights , mean vectors and covariance matrices .",
    "we learn @xmath31 sets of parameters @xmath72 , @xmath31 being the number of classes using the well - known expectation - maximization ( em ) algorithm .",
    "the mapping @xmath51 corresponds to the likelihood of the observed data given the model parameters . for a sequence of feature vectors",
    "@xmath24 $ ] , which are assumed to be independent , we have : @xmath73 this method is parametrized by the number @xmath15 of gaussians in the mixture , the maximum number of em iterations and the shape of the covariance matrices ( full or diagonal ) .",
    "we refer the reader to @xcite for more details about gmm .",
    "the hidden markov models ( hmm ) also belong to the family of generative models @xcite . in a hmm",
    "the observations depend on a hidden discrete random variable usually called state , taking values from 1 to @xmath74 .",
    "the probability of the observations given the state value is called emission probability .",
    "the state is assumed to be markovian , that is , the state at time @xmath3 only depends on the state at time @xmath75 .",
    "in addition , the states are constrained to happen in order , i.e. state @xmath76 before the state @xmath77 ; this is usually known as _ left - to - right _ hmm .",
    "the emission probability is usually gaussian or gmm . as in the case of gmm , one model @xmath78 per class is learnt ( through an em algorithm ) .",
    "the model consists of the parameters of the emission probability and the parameters modeling the markovian dynamics .",
    "the function @xmath51 is also the likelihood of the observations given the model : @xmath79 the parameters of the hmm are the parameters of the emission probability , the number of states @xmath74 .",
    "we refer the reader to @xcite for more details about hmm .",
    "the @xmath1-nn , gmm algorithms comes from the matlab toolboxes .",
    "the qnn algorithm is our own matlab code inspired from @xcite .",
    "the hmms are developed using the machine learning pmtk3 library @xcite .",
    "the svms are implemented using libsvm @xcite .",
    "[ sec : setup ]    given the database described in section  [ sec : data ] , a large set of combinations of feature types , features post - processing , and classifiers have been tested ( note that all combinations do not make sense , e.g. , some features are not appropriate for time interpolation ; we implemented only relevant combinations ) . in order to be able to statistically compare the different sound recognition methods , we perform @xmath1-fold cross - validation repeated on @xmath4 different runs .",
    "the results are averaged on these @xmath4 runs , with @xmath1 and @xmath4 being set to @xmath80 .",
    "tables  [ results : accuracy ] to [ results : trainingcost ] gather the different statistics on the different combinations of features ( rows ) + post - processing , and classifiers ( columns ) .",
    "_ gmm-1 _ stands for the gmm method ( section [ subsec : gmm ] ) applied when @xmath81 , while _ gmm - t _ corresponds to @xmath82 .",
    "it is important to note that gmm - t and hmm methods are fed with sounds represented by the original ( variable - length ) sequence of feature vectors , whereas all the other classifiers are fed with a single fixed - size vector representation issued from post - processing by either mean ( rows 14 ) , bag - of - words ( rows 5 and 6 ) , or fixed - sized interpolation ( rows 7 and 8) .",
    "the latter still represents a vector time - sequence but of fixed length , and hence can be reshaped in a single vector .",
    "_ ttff _ stands for time and time - frequency features ( corresponding to the features of section [ subsec : timefeatures ] and [ subsec : freqfeatures ] ) .",
    "cells filled with gray correspond to irrelevant combinations .      note first that the best results using _ ttff _ or by concatenating ttff+mfcc have been found using the features _ energy , zcr , spectral decrease , spectral flatness , spectral slope_. therefore these features have been used in the presented results .",
    "adding the _ roll - off _ and the _ spectral moments _ gives similar results .",
    "the _ sound duration _ is not a reliable feature in the present context , since it lead to drop in scores .    as for accuracy ,",
    "the best results are obtained using svm classifiers on interpolated mfcc+ttff coefficients ( 97% accuracy ) , followed by _",
    "k_-nn with interpolated mfcc ( 96.2% ) .",
    "hmm on mfcc coefficients , which is a very usual combination in the literature , provides a very good baseline at 92.6% good accuracy .",
    "therefore , a major result here is that , for short pre - segmented domestic sound recognition , a quite simple technique such as _ k_-nn , that requires no training , can perform better than asr reference methods such as hmms .",
    "the latter requires both training and much longer decoding time ( see table  [ results : recognitiontime ] ) and may be more appropriate for long and complex sound sequences such as speech signals . as could be predicted ,",
    "the preservation of dynamic information is important for accurate recognition : see the 96.2% good accuracy for _ k_-nn with mfcc + interpolation vs. 87.4% for _ k_-nn with mfcc + mean ; see also the difference between gmm-1 and gmm - t .",
    "this is confirmed by the poor results obtained with the bag - of - words approach which has not proven being relevant in these experiments ( remind that bow histograms cumulate information over frames but loose the temporal structure ; also , the histogram codebook can not be large because the training time grows up exponentially with @xmath25 : for the experiments , we used @xmath83 ) .",
    "however , accurate vector alignment using advanced dtw as used in hmms do not seem as crucial as for asr : here basic fixed - size interpolation seems efficient enough for the task at hand .",
    "this rises many questions about the ( temporal and spectral ) structure of domestic sounds , that go beyond the scope of the present study . waiting for further investigations ,",
    "the fact that _ k_-nn with simple feature sequence interpolation outperforms hmms ( and gmm - t ) can be partly explained by the fact that _ k_-nns use original data in the recognition task while hmms ( and gmm - t ) use data models . in addition _",
    "k_-nn is a discriminative technique , whereas hmms ( and gmm - t ) are generative models .",
    "a consequence is that _ k_-nns have a very large memory requirement to store the prototypes ( see table  [ results : trainingcost ] ) , which a major drawback for autonomous robotics .",
    "obviously , svm is an interesting alternative , modestly increasing the recognition time over _ k_-nn for a much smaller memory cost . and",
    "so is qnn which has a larger recognition time but an even smaller memory cost .",
    "therefore , the choice between _",
    "k_-nns , svm and qnn should depend on the specifications of the autonomous robot in terms of computation and memory resources .",
    "gmm - t with mfcc has good accuracy performance but the recognition time is quite high , making it less interesting than the above - mentioned methods .",
    "it remains unclear why svms perform significantly better with mfcc+ttff+interpolation than with mfcc+interpolation , whereas the difference is not so pronounced for _ k_-nn and qnn ( and for some other settings , adding ttff even decreases the accuracy scores ; this is difficult to explain , except appealing to the redundancy between some ttff features and mfcc information ) .",
    "anyway , a major point that arises from this study is that , for short domestic sounds recognition , the three methods _ k_-nns , svm and qnn , combined with simple time interpolation of features , seem preferable to the ( more complex ) hmms widely used for speech recognition and recently extended to the more general problem of sound scene analysis .    to complement those results , we present in table [ results : featuretime ] , the time to compute feature vector(s ) from a sound ( mean or sequence ; column _ feature _ ) .",
    "in the column _ bow _ , _ k - means _ is the training time of the codebook , and _ histo _ the time to transform the feature vector(s ) of one sound into a histogram . _",
    "interpolation _ is the time to perform the fixed - length time interpolation on the feature vector(s ) of one sound .",
    "we can see that the time to compute the feature vector ( sequence ) is reasonable but not negligible : for example , it is an order of magnitude larger than the recognition time of _",
    "k_-nn , but it is also more than an order of magnitude lower than the recognition time for hmms . for mfcc coefficients ,",
    "the time needed to interpolate the mfcc sequence is comparable to the time needed to calculate the coefficients .",
    "note that the memory cost for training the models from data are not considered in the present study , since this can be processed offline .",
    "we addressed the problem of sound recognition by an autonomous humanoid robot , by benchmarking a large set of audio feature representations , post - processing , and classification techniques .",
    "a major result of this work is that , for the 42 classes of kitchen / office / voice sounds that we considered , very good accuracy scores ( larger than 92% and up to 97% ) were obtained for three techniques of very reasonable complexity ( at least for decoding ) , namely _",
    "k_-nn , svm and qnn .",
    "moreover these methods were applied successfully on fixed - size sequences of mfcc vectors obtained with very simple dtw ( fixed - size interpolation ) .",
    "the performance in accuracy is of the same order and even outperforms the performance of hmms applied on the original vector sequences , whereas the decoding time ( hence computational cost ) is much lower .",
    "therefore , these three methods seem to be appropriate within the context a robotic implementation .",
    "a more thorough analysis of the nature of domestic sounds must be carried out to reveal if they are characterized by an inner structure , in a similar way as , e.g. speech signals are characterized by successive phonemes ( and transitions between them ) .",
    "domestic / environmental sounds can also be analyzed in terms of taxonomy , nature ( matter of the object that generated the sound : metal , wood , glass , etc . ) ,",
    "interactions or dynamics ( friction , shock , etc . ) . to reach this goal ,",
    "the number of classes must be increased radically to reach several hundreds .",
    "the introduction of a `` garbage class '' is absolutely necessary , since , it is impossible to consider all the possible sound categories .",
    "future work will also consider the processing of continuous audio streams , e.g. , taking into account stationary and less stationary background noise , or `` longer '' sounds indicating a specific activity ( e.g. , tap water flushing ) .",
    "in addition to external noise , we will address the problem of ego - noise ( generated by robot joints in motion ) detection and removal , as in @xcite . in the long run",
    ", we aim at merging the sound recognition system in a complete framework for acoustic scene analysis including source localization and separation , embedded in the robot nao .",
    "j.  huang , t.  supaongprapa , i.  terakura , f.  wang , n.  ohnishi , and n.  sugie , `` a model - based sound localization system and its application to robot navigation , '' _ robotics and autonomous systems _",
    "27 , no .  4 , pp . 199209 , 1999 .",
    "s.  yamamoto , k.  nakadai , m.  nakano , h.  tsujino , j .-",
    "valin , k.  komatani , t.  ogata , and h.  g. okuno , `` real - time robot audition system that recognizes simultaneous speech in the real world , '' in _ int .",
    "conf . on intell .",
    "rob . and syst .",
    "_ , 2006 , pp . 53335338",
    ".      y.  sakagami , r.  watanabe , c.  aoyama , s.  matsunaga , n.  higaki , and k.  fujimura , `` the intelligent asimo : system overview and integration , '' in _ int .",
    "conf . on intell .",
    "syst . _ , 2002 , pp .",
    "24782483 .",
    "y.  sasaki , m.  kaneyoshi , s.  kagami , h.  mizoguchi , and t.  enomoto , `` daily sound recognition using pitch - cluster - maps for mobile robot audition , '' in _ int .",
    "conf . on intell .",
    "rob . and syst .",
    "_ , 2009 , pp",
    ". 27242729 .",
    "n.  yamakawa , t.  takahashi , t.  kitahara , t.  ogata , and h.  g. okuno , `` environmental sound recognition for robot audition using matching - pursuit , '' in _ modern approaches in applied intelligence _ , ser .",
    "lecture notes in computer science.1em plus 0.5em minus 0.4em springer , 2011 , pp .",
    "j.  stork , l.  spinello , j.  silva , and k.  arras , `` audio - based human activity recognition using non - markovian ensemble voting , '' in _ international symp . on robots and human interactive communications _",
    ", 2012 , pp . 509514 .                          c.  lin , s.  chen , t.  truong , and y.  chang , `` audio classification and categorization based on wavelets and support vector machine , '' _ ieee transactions on speech and audio processing _ , vol .  13 , no .  5 , pp .",
    "644651 , 2005 ."
  ],
  "abstract_text": [
    "<S> we address the problem of sound representation and classification and present results of a comparative study in the context of a domestic robotic scenario . </S>",
    "<S> a dataset of sounds was recorded in realistic conditions ( background noise , presence of several sound sources , reverberations , etc . ) using the humanoid robot nao . </S>",
    "<S> an extended benchmark is carried out to test a variety of representations combined with several classifiers . </S>",
    "<S> we provide results obtained with the annotated dataset and we assess the methods quantitatively on the basis of their classification scores , computation times and memory requirements . </S>",
    "<S> the annotated dataset is publicly available at https://team.inria.fr / perception / nard/. </S>"
  ]
}