{
  "article_text": [
    "many natural phenomena decompose into latent features . for example",
    ", visual scenes can be decomposed into objects ; genetic regulatory networks can be decomposed into transcription factors ; music can be decomposed into spectral components . in these examples ,",
    "multiple latent features can be simultaneously active , and each can influence the observed data .",
    "dimensionality reduction methods , such as principal component analysis , factor analysis , and probabilistic matrix factorization , provide a statistical approach to inferring latent features @xcite .",
    "these methods characterize a small set of dimensions , or features , and model each data point as a weighted combination of these features .",
    "dimensionality reduction can improve predictions and identify hidden structures in observed data .",
    "dimensionality reduction methods typically require that the number of latent features ( i.e. , the number of dimensions ) be fixed in advance .",
    "researchers have recently proposed a more flexible approach based on bayesian nonparametric models , where the number of features is inferred from the data through a posterior distribution .",
    "these models are usually based on the indian buffet process ( ibp ; @xcite ) , a prior over binary matrices with a finite number of rows ( corresponding to data points ) and an infinite number of columns ( corresponding to latent features ) . using the ibp as a building block ,",
    "bayesian nonparametric latent feature models have been applied to several statistical problems ( e.g. , @xcite ) .",
    "since the number of features is effectively unbounded , these models are sometimes known as `` infinite '' latent feature models .",
    "the ibp assumes that data are _ exchangeable _ : permuting the order of rows leaves the probability of a feature matrix unchanged .",
    "this assumption may be appropriate for some data sets , but for many others we expect dependencies between data points and , consequently , between their latent representations .",
    "as examples , the latent features describing human motion are autocorrelated over time ; the latent features describing environmental risk factors are autocorrelated over space . in this paper",
    ", we present a generalization of the ibp  the _ distance dependent ibp _",
    "( dd - ibp)that addresses this limitation .",
    "the dd - ibp allows infinite latent feature models to capture non - exchangeable structure .",
    "the problem of adapting nonparametric models to non - exchangeable data has been studied extensively in the mixture - modeling literature . in particular , variants of the dirichlet process mixture model allow dependencies between data points ( e.g. , @xcite ) . these dependencies may be spatial , temporal or more generally covariate - dependent ; the effect of such dependencies is to induce sharing of features between nearby data points .    among these methods",
    "is the _ distance dependent chinese restaurant process _",
    "( dd - crp ; @xcite ) .",
    "the dd - crp is a non - exchangeable generalization of the chinese restaurant process ( crp ) , the prior over partitions of data that emerges in bayesian nonparametric mixture modeling @xcite .",
    "the dd - crp models non - exchangeability by using using distances between data points  nearby data points ( e.g. , in time or space ) are more likely to be assigned to the same mixture component .",
    "the dd - ibp extends these ideas to infinite latent feature models , where distances between data points influence feature - sharing , and nearby data points are more likely to share latent features .",
    "we review the ibp in section  [ sec : ibp ] and develop the dd - ibp in section  [ sec : ddibp ] . like the dd - crp ,",
    "the dd - ibp lacks _ marginal invariance _ , which means that removing one observation changes the distribution over the other observations .",
    "we discuss this property further in section [ sec : invariance ] .",
    "although many bayesian nonparametric models have this property , we view it as a particular modeling choice that may be appropriate for some problems but not for others .",
    "several other infinite latent feature models have been developed to capture dependencies between data in different ways , for example using phylogenetic trees @xcite or latent gaussian processes @xcite .",
    "of particular relevance to this work is the model of zhou et al .",
    "@xcite , which uses a hierarchical beta process to couple data . these and other related models are discussed further in section [ sec : related ] . in section [ sec :",
    "feat ] , we characterize the feature - sharing properties of the dd - ibp and compare it to those of the model proposed by zhou et al . @xcite .",
    "we find that the different models capture qualitatively distinct dependency structures .",
    "exact posterior inference in the dd - ibp is intractable .",
    "we present an approximate inference algorithm based on markov chain monte carlo ( mcmc ; @xcite ) in section [ sec : mcmc ] , and we apply this algorithm in section  [ sec : lin ] to infer the latent features in a linear - gaussian model .",
    "the experimental results presented in section [ sec : results ] suggest that the dd - ibp is an effective tool for modeling latent structure in data with dependencies between observations .",
    "we first review the definition of the ibp and its role in defining infinite latent feature models .",
    "we then introduce the dd - ibp .",
    "the ibp is a prior over binary matrices @xmath0 with an infinite number of columns @xcite . in the indian buffet metaphor ,",
    "rows of @xmath0 correspond to customers and columns correspond to dishes . in data",
    "analysis , the customers represent data points and the dishes represent features .",
    "let @xmath1 denote the entry of @xmath0 at row @xmath2 and column @xmath3 .",
    "whether customer @xmath2 has decided to sample dish @xmath3 ( that is , whether @xmath4 ) corresponds to whether data point @xmath2 possesses feature @xmath3 .",
    "the ibp is defined as a sequential process .",
    "the first customer enters the restaurant and samples the first @xmath5 number of dishes , where the hyperparameter @xmath6 is a scalar . in the binary matrix",
    ", this corresponds to the first row being a contiguous block of ones , whose length is the number of dishes sampled ( @xmath7 ) , followed by an infinite block of zeros .",
    "subsequent customers @xmath8 enter , sampling each previously sampled dish according to its popularity , @xmath9 where @xmath10 is the number of customers that sampled dish @xmath3 prior to customer @xmath2 .",
    "( we emphasize that eq .",
    "[ eq : ibp ] applies only to dishes @xmath3 that were previously sampled , i.e. , for which @xmath11 . )",
    "then , each customer samples @xmath12 new dishes .",
    "again these are represented as a contiguous block of ones in the columns beyond the last dish sampled by a previous customer .    though described sequentially , griffiths and ghahramani @xcite showed that the resulting rows of the binary matrix are _ exchangeable _ ( up to a permutation of the columns ) .",
    "this means that the order of the customers does not affect the probability of the resulting binary matrix .",
    "this is seen in the beta - bernoulli perspective , which we review in section  [ sec : related ] . in the next section ,",
    "we develop a generalization of the ibp that relaxes this assumption .      like the ibp , the dd - ibp is a distribution over binary latent feature matrices with a finite number of rows and an infinite number of columns .",
    "each pair of customers has an associated distance , e.g. , distance in time or space , or based on a covariate .",
    "two customers that are close together in this distance will be more likely to share the same dishes ( that is , features ) than two customers that are far apart .",
    "the dd - ibp can be understood in terms of the following sequential construction .",
    "first , each customer selects a poisson - distributed number of dishes ( feature columns ) .",
    "the dishes selected by a customer in this phase of the construction are said to be `` owned '' by this customer .",
    "a dish is either unowned , or is owned by exactly one customer .",
    "this step is akin to the selection of new dishes in the ibp .",
    "then , for each owned dish , customers connect to one another .",
    "the probability that one customer connects to another decreases in the distance between them .",
    "note that customers do not sample each dish , as in the ibp , but rather connect to other customers .",
    "finally , dish inheritance is computed : a customer inherits a dish if its owner ( from the first step ) is reachable in the connectivity graph for that dish .",
    "this inheritance is computed deterministically from the connections generated in the previous step .",
    "the dishes that each customer samples are those that he inherits or owns .",
    "thus , similarity of sampled dishes between nearby customers is induced via distance - dependent connection probabilities .",
    "we now more formally describe the probabilistic generative process of the binary matrix @xmath13 .",
    "first , we introduce some notation and terminology .",
    "* dishes ( columns of @xmath0 ) are identified with the natural numbers @xmath14 .",
    "the set of dishes owned by customer @xmath2 is @xmath15 .",
    "the cardinality of this set is @xmath16 .",
    "these sets are disjoint , so @xmath17 for @xmath18 .",
    "the total number of owned dishes is @xmath19 .",
    "the set of dishes owned by customers excluding @xmath2 is @xmath20 . * each dish",
    "is associated with a set of customer - to - customer assignments , specified by the @xmath21 _ connectivity matrix _ @xmath22 , where @xmath23 indicates that customer @xmath2 connects to customer @xmath24 for dish @xmath3 . given @xmath22 ,",
    "the customers form a set of ( possibly cyclic ) directed graphs , one for each dish .",
    "ownership vector _ is @xmath25 , where @xmath26 indicates the customer who owns dish @xmath3 , so @xmath27 . *",
    "the @xmath28 distance matrix between customers is @xmath29 , where the distance between customers @xmath2 and @xmath24 is @xmath30 .",
    "a customer s self - distance is 0 : @xmath31 .",
    "we call the distance matrix _ sequential _ when @xmath32 for @xmath33 . in this special case",
    ", customers can only connect to previous customers .",
    "* the _ decay function _",
    "$ ] maps distance to a quantity , which we call _ proximity _ , that controls the probabilities of customer links .",
    "we require that @xmath35 and @xmath36 .",
    "we obtain the _ normalized proximity matrix _ @xmath37 by applying the decay function to each customer pair and normalizing by customer .",
    "that is , @xmath38 , where @xmath39 .    using this notation",
    ", we generate the feature indicator matrix @xmath0 as follows :    1 .   *",
    "assign dish ownership*. for each customer @xmath2 , allocate @xmath40 unowned dishes to the customer s set of owned dishes , @xmath41 . for each @xmath42 , set the ownership @xmath43 .",
    "2 .   * assign customer connections*. for each customer @xmath2 and dish @xmath44 , draw a customer assignment according to @xmath45 .",
    "note that customers can connect to themselves . in this case",
    ", they do not inherit a dish unless they own it ( see the next step ) .",
    "3 .   * compute dish inheritance*. we say that customer @xmath24 _ inherits _ dish @xmath3 if there exists a path along the directed graph for dish @xmath3 from customer @xmath24 to the dish s owner @xmath46 ( i.e. , if @xmath46 is reachable from @xmath24 ) , where the directed graph is defined by column @xmath3 of @xmath22 . the owner of a dish automatically inherits it . can link to other customers for dish @xmath3 even if @xmath42 , these connections are ignored in determining dish inheritance when @xmath42 . ]",
    "we encode reachability with @xmath47 . if customer @xmath24 is reachable from customer @xmath2 for dish @xmath3 then @xmath48 .",
    "otherwise @xmath49 .",
    "4 .   * compute the feature indicator matrix*. for each customer @xmath2 and dish @xmath3 we set @xmath4 if @xmath2 inherits @xmath3 , otherwise @xmath50 .",
    "an example of customer assignments sampled from the dd - ibp is shown in figure [ fig : ddibp_schematic ] . in this example",
    ", customer 1 owns dish 1 ; customers 2 - 4 all reach customer 1 , either directly or through a chain , and thereby inherit the dish ( indicated by gray shading ) .",
    "consequently , feature 1 is active for customers 1 - 4 .",
    "dish 2 is owned by customer 2 ; only customer 1 reaches customer 2 , and hence feature 2 is active for customers 1 and 2 .",
    "dish 3 is owned by customer 2 , but no other customers reach customer 2 , and hence feature 3 is active only for that customer .",
    "the generative process of the dd - ibp defines the following joint distribution of the ownership vector and connectivity matrix , @xmath51 consider the first term .",
    "recall that the set of dishes each customer owns @xmath41 and the total number of owned dishes @xmath52 are both functions of the ownership vector @xmath53 .",
    "thus , the probability of the ownership vector is @xmath54 where @xmath25 is a deterministic function of @xmath55 .",
    "consider the second term .",
    "the conditional distribution of the connectivity matrix @xmath22 depends on the total number of owned dishes and the normalized proximity matrix @xmath37 ( derived from the distances and decay function ) , @xmath56 the dependence on @xmath53 comes from @xmath52 , which is determined by @xmath53 .",
    "random feature models ( and the traditional ibp ) operate with a random binary matrix @xmath13 . in the dd - ibp",
    ", @xmath13 is a ( deterministic ) many - to - one function of the random variables @xmath22 and @xmath25 , which we denote by @xmath57 .",
    "we compute the probability of a binary matrix by marginalizing out the appropriate configurations of these variables @xmath58 the dd - ibp reduces to the standard ibp in the special case when @xmath59 for all @xmath60 and the distance matrix is sequential .",
    "( recall : @xmath61 is sequential if @xmath32 for @xmath62 . ) to see this , consider the probability that the @xmath3th dish is sampled by the @xmath2th customer ( that is , @xmath4 ) .",
    "this probability is the proportion of previous customers that already reach @xmath46 because the probabiity of connecting to each customer is proportional to one .",
    "this probability is @xmath63 , which is the same as in the ibp .",
    "this is akin to the relationship between the dd - crp and the traditional crp under the same condition .",
    "many different decay functions are possible within this framework .",
    "figure [ fig : decayfun ] shows samples of @xmath0 using four decay functions and a sequential distance defined by absolute temporal distance ( @xmath64 for @xmath65 and @xmath66 for @xmath62 ) .",
    "* the _ constant _ , @xmath59 .",
    "this is the standard ibp . *",
    "the _ exponential _",
    ", @xmath67 . * the _ logistic _ , @xmath68 . * the _ window _ , @xmath69 $ ] .",
    "each decay function encourages the sharing of features across nearby rows in a different way .    when combined with an observation model ( which specifies how the latent features give rise to observed data ) , the dd - ibp functions as a prior over latent feature representations of a data set . in section [ sec : lin ] , we consider a specific example of how the dd - ibp can be used to analyze data .",
    "unlike the traditional ibp , the dd - ibp is not ( in general ) _ marginally invariant _ , the property that removing a customer leaves the distribution over latent features for the remaining customers unchanged .",
    "( the dd - ibp builds on the dd - crp , which is not marginally invariant either . ) in some circumstances , marginal invariance is desirable for computational reasons .",
    "for example , the conditional distributions over missing data for models lacking marginal invariance require computing ratios of normalization constants .",
    "in contrast , marginally invariant models , due to their factorized structure , require less computation for conditional distributions over missing data .",
    "in other circumstances , such as exploratory analysis of fully observed datasets , this computational concern is less important . beyond computational considerations , while marginal invariance may be an appropriate modeling assumption in some data sets",
    ", it may be inappropriate in others .    also unlike the traditional ibp , the dd - ibp is not exchangeable in general . to state this formally ,",
    "let @xmath70 be a permutation of the integers @xmath71 , and for a given @xmath72 binary matrix @xmath0 , let @xmath73 be the matrix created by permuting its rows according to @xmath70 .",
    "let @xmath0 be drawn from the dd - ibp with distance matrix @xmath61 , mass parameter @xmath6 and decay function @xmath74 . then , except in certain special cases ( such as when @xmath61 recovers the traditional ibp ) , @xmath75 permuting the data changes its distribution , and so the dd - ibp is not exchangeable in general .",
    "although the dd - ibp is not exchangeable , it does have a related symmetry .",
    "let @xmath76 be the @xmath77 matrix @xmath61 with both its rows and its columns permuted according to @xmath70 , and let @xmath73 be drawn from the dd - ibp with distance matrix @xmath76 rather than @xmath61 .",
    "( we retain the same values for @xmath6 and @xmath74 . ) then , in general , @xmath78 thus , if we permute both the data and the distance matrix , probabilities remain unchanged .",
    "permuting both the data and the distance matrix is like first relabeling the data , and then explicitly altering the probability distribution to account for this relabeling . if the dd - ibp were exchangeable , one would not need to alter the probability distribution to account for relabeling .",
    "in this section we describe related work on infinite latent feature models that capture external dependence between the data .",
    "we focus on the most closely related model , which is the _ dependent hierarchical beta process _",
    "( dhbp ; @xcite ) . as a prelude to describing the dhbp",
    ", we review the connection between the ibp and the beta process .",
    "recall that the ibp is exchangeable .",
    "consequently , by de finetti s theorem @xcite , the rows of @xmath0 , considered as binary vectors @xmath79 , are conditionally independent , @xmath80 in this marginal distribution , @xmath81 is a random measure on the feature space @xmath82 and @xmath83 is the de finetti mixing distribution ( see @xcite ) .",
    "thibaux and jordan @xcite showed that the de finetti mixing distribution underlying the ibp is the _ beta process _",
    "( bp ) , parameterized by a positive _ concentration parameter _ @xmath84 and a _ base measure _",
    "@xmath85 on @xmath82 .",
    "a draw @xmath86 is defined by a countably infinite collection of weighted atoms , @xmath87 where @xmath88 is a probability distribution that places a single atom at @xmath89 , and the @xmath90 $ ] are independent random variables whose distribution is described as follows .",
    "if @xmath85 is continuous , then the atoms and their weights are drawn from a nonhomogeneous poisson process defined on the space @xmath91 $ ] with rate measure @xmath92 if @xmath85 is discrete and of the form @xmath93 , @xmath94 $ ] , then @xmath81 has atoms at the same locations as @xmath85 , with @xmath95 .",
    "following thibaux and jordan @xcite , we define the _ mass parameter _ as @xmath96 .",
    "note that @xmath85 is not necessarily a probability measure , and hence @xmath97 can take on non - negative values different from 1 .",
    "conditional on a draw from the beta process , the feature representation @xmath98 of data point @xmath2 is generated by drawing from the _ bernoulli process _",
    "( bep ) with base measure @xmath81 : @xmath99 .",
    "if @xmath81 is discrete , then @xmath100 , where @xmath101 . in other words ,",
    "feature @xmath3 is activated with probability @xmath102 independently for all data points .",
    "sampling @xmath0 from the compound beta - bernoulli process is equivalent to sampling @xmath0 directly from the ibp when @xmath103 and @xmath104 @xcite .",
    "the dhbp @xcite builds external dependence between data points using the above bp construction .",
    "the dependencies are induced by mixing independent bp random measures , weighted by their proximities @xmath37 .",
    "the dhbp is based on the following generative process , @xmath105 this is equivalent to drawing @xmath98 from a bernoulli process whose base measure is a linear combination of bp random measures , @xmath106 dependencies between data points are captured in the dhbp by the proximity matrix @xmath37 , as in the dd - ibp .",
    "this allows proximal data points ( e.g. , in time or space ) to share more latent features than distant ones .    in section [ sec : feat ]",
    ", we compare the feature - sharing properties of the dhbp and dd - ibp . using an asymptotic analysis",
    ", we show that the dd - ibp offers more flexibility in modeling the proportion of features shared between data points , but less flexibility in modeling uncertainty about these proportions .      although still a nascent area of research , several other non - exchangeable priors for infinite latent feature models have been proposed .",
    "williamson , orbanz and ghahramani @xcite used a hierarchical gaussian process to couple the latent features of data in a covariate - dependent manner .",
    "they named this model the _ dependent indian buffet process _",
    "their framework is flexible : it can couple columns of @xmath0 in addition to rows , while the dd - ibp can not",
    ". however , this flexibility comes at a computational cost during inference : their algorithm requires sampling an extra layer of variables .",
    "miller , griffiths and jordan @xcite proposed a `` phylogenetic ibp '' that encodes tree - structured dependencies between data .",
    "doshi - velez and ghahramani @xcite proposed a `` correlated ibp '' that couples data points and features through a set of latent clusters .",
    "both of these models relax exchangeability , but they do not allow dependencies to be specified directly in terms of distances between data . furthermore , inference for these models requires more intensive computation than does the standard ibp . the mcmc algorithm presented by miller et al .",
    "@xcite for the phylogenetic ibp involves both dynamic programming and auxiliary variable sampling .",
    "similarly , the mcmc algorithm for the correlated ibp involves sampling latent clusters in addition to latent features .",
    "our model also incurs extra computational cost relative to the traditional ibp due to the computation of reachability ( quadratic in the number of observations ) ; however , it permits a richer specification of the dependency structure between observations than either the phylogenetic or the correlated ibp .",
    "recently , ren et al .",
    "@xcite presented a novel way of introducing dependency into latent feature models based on the beta process . instead of defining distances between customers , each dish",
    "is associated with a latent covariate vector , and distances are defined between each customer s ( observed ) covariates and the dish - specific covariates .",
    "customers then choose dishes with probability proportional to the customer - dish proximity .",
    "this construction comes with a significant computational advantage for data sets where the time complexity is tied predominantly to the number of observations .",
    "the downside of this construction is that the mcmc algorithm used for inference must sample a separate covariate vector for each dish , which may scale poorly if the covariate dimensionality is high .",
    "in this section , we compare the feature - sharing properties of the dhbp and dd - ibp .",
    "two data points share a feature if that feature is active for both ( i.e. , @xmath107 for @xmath18 and a given feature @xmath3 ) .",
    "this analysis is useful for understanding the types of dependencies induced by the different models , and can help guide the choice of model and hyperparameter settings for particular data analysis problems .",
    "we consider an asymptotic regime in which the mass parameter is large ( @xmath6 for the dd - ibp and @xmath97 for the dhbp ) , which simplifies feature - sharing properties .",
    "proofs of all propositions in this section may be found in the appendix .",
    "we first characterize the limiting distributional properties of feature - sharing in the dd - ibp as @xmath108 .",
    "we drop the feature index @xmath3 in the reachability indicator @xmath109 , writing it @xmath110 .",
    "we do this because features ( that is , columns of the binary matrix @xmath0 ) are exchangeable under the dd - ibp and , consequently , the distribution of the random vector @xmath111 is invariant across @xmath3 .",
    "let @xmath112 denote the number of features held by data point @xmath2 , and let @xmath113 denote the number of features shared by data points @xmath2 and @xmath24 , where @xmath114 .    under the dd - ibp , @xmath115    the probabilities @xmath116 and @xmath117",
    "depend strongly on the distribution of the connectivity matrix @xmath22 , but do not depend on the ownership vector @xmath25 , since @xmath47 is independent of dish ownership .",
    "we derive the limiting properties of @xmath118 and @xmath119 from properties of the poisson distribution . in this and",
    "following results , @xmath120 indicates convergence in distribution .",
    "[ cor : ddibp ] let @xmath114 .",
    "@xmath118 and @xmath119 converge in distribution under the dd - ibp to the following constants as @xmath108 : @xmath121",
    "this corollary shows that the limiting fraction of shared features @xmath122 in the dd - ibp is a constant that may be different for each pair of data points @xmath2 and @xmath24 .",
    "in contrast , we show below that the same limiting fraction under the dhbp is random , and takes one of two values .",
    "these two values are fixed , and do not depend upon the data points @xmath2 and @xmath24 .      here",
    "we characterize the limiting distributional properties of feature sharing in the dhbp as @xmath85 becomes infinitely concentrated ( i.e. , @xmath123 , analogous to @xmath124 ) . in this limit",
    ", feature - sharing is primarily attributable to dependency induced by the proximity matrix @xmath37 .    if @xmath85 is continuous , then under the dhbp , @xmath125    we derive the limiting properties of @xmath118 and @xmath119 from properties of the poisson distribution .",
    "let @xmath114 .",
    "conditional on @xmath126 , @xmath118 and @xmath119 converge in distribution under the dhbp to the following constants as @xmath127 : @xmath128 [ eq : r ]    thus , the expected fraction of object @xmath2 s features shared with object @xmath24 , @xmath122 , is a factor of @xmath129 bigger when @xmath130 . as @xmath131 , this fraction goes to @xmath132 . as @xmath133",
    ", it goes to 1 .",
    "we can obtain the unconditional fraction by marginalizing over @xmath134 and @xmath135 :    [ cor : dhbp ] let @xmath114 .",
    "@xmath122 converges in distribution under the dhbp as @xmath136 to a random variable @xmath137 defined by @xmath138 where @xmath139 .",
    "this corollary shows that as @xmath97 grows large , the fraction of shared features becomes one of two values ( determined by @xmath140 and @xmath141 ) , with a mixing probability determined by the dependency structure .",
    "thus , the dhbp affords substantial flexibility in specifying the mixing probability ( via @xmath37 ) , but is constrained to two possible values of the limiting fraction .      for comparison",
    ", we briefly describe the feature - sharing properties under the traditional ibp .    under the traditional ibp , by exchangeability , @xmath118 and @xmath119",
    "are equal in distribution to @xmath142 and @xmath143 .",
    "the first customer draws a @xmath144 number of dishes .",
    "the second customer then chooses whether to sample each of these dishes independently and with probability @xmath145 .",
    "thus , the number of dishes sampled by both the first and second customers is @xmath146 .",
    "this shows that , under the traditional ibp , as @xmath147 with @xmath114 , @xmath148      using an asymptotic analysis , the preceding theoretical results show that the dd - ibp and dhbp provide different forms of flexibility in specifying the way in which features are shared between data points .",
    "this asymptotic analysis takes the limit as the mass parameters @xmath6 and @xmath97 become large .",
    "this limit is taken for theoretical tractability , and removes much of the uncertainty that is otherwise present in these models . while such limiting dd - ibp and dhbp models",
    "are not intended for practical use , their simplicity provides insight into behavior in non - asymptotic regimes .    under the dd - ibp , corollary  [ cor : ddibp ]",
    "shows that the modeler is allowed a great deal of flexibility in specifying the proportions of features shared by data points . given a matrix specifying the proportion of features that are believed to be shared by pairs of data points",
    ", one can ( if this matrix is sufficiently well - behaved ) design a distance matrix that causes the dd - ibp to concentrate on the desired proportions .",
    "while the dd - ibp can not model an arbitrary modeler - specified matrix of proportions , the set of matrices that can be modeled is very large .",
    "in contrast , under the dhbp , corollary  [ cor : dhbp ] shows that the modeler has less flexibility in specifying the proportions of features shared . under the dhbp ,",
    "the modeler chooses two values , @xmath149 and @xmath150 , and the proportion of features shared by any pair of data points in the asymptotic regime must be one of these two values .",
    "section  [ sec : ibp - theory ] shows that the traditional ibp has the least flexibility . in the asymptotic regime ,",
    "the proportion of features shared by each pair of data points is a constant .",
    "while the dd - ibp has more flexibility in specifying values of the feature - sharing - proportions than the dhbp , it has less flexibility ( at least in this asymptotic regime ) in modeling uncertainty about these feature - sharing proportions . under the dd - ibp",
    ", the proportion of features shared by a pair of data points in the asymptotic regime is a deterministic quantity . under the dhbp ,",
    "the proportion of features shared is a random quantity , even in the asymptotic regime .",
    "a modeler using the dhbp has full flexibility in choosing the joint probability distribution governing these proportions .",
    "one could extend the dd - ibp to allow uncertainty about the feature - sharing - proportions by specifying a hyperprior over distance matrices , but we do not consider this extension further .    figure  [ fig : featureshare ] illustrates the difference in asymptotic feature - sharing behavior between the dhbp and dd - ibp .",
    "subfigures in the upper row are draws from the dhbp , and subfigures in the bottom row are draws from the dd - ibp . within a single subfigure ,",
    "the shade in the cell @xmath151 is the fraction @xmath122 .",
    "( the diagonals @xmath152 have been set to @xmath153 to bring out other aspects of the matrix . )",
    "each of the four columns represents a pair of independent draws . to approximate the asymptotic regime considered by the theory , the mass parameters for the two models are set to large values of @xmath154 .",
    "the figure shows that , in draws from the dhbp , off - diagonal cells have one of two shades , corresponding to the two possible limiting values for @xmath122 . in the different columns , correspoindng to different independent draws , the patterns are different , showing that @xmath122 remains random under the dhbp , even in the asymptotic regime .",
    "in contrast , in draws from the dd - ibp , off - diagonal cells take a variety of different values , but remain unchanged across independent draws .",
    "figure  [ fig : featureshare_poisson ] illustrates non - asymptotic feature - sharing behavior in a simple setting with only two data points .",
    "the figure shows the feature - sharing behavior of the dhbp ( top ) and dd - ibp ( bottom ) at two values for the mass parameter : @xmath155 ( top row ) and @xmath156 ( bottom row ) .",
    "each subfigure shows the probability mass function @xmath157 as a function the proximity @xmath158 , where @xmath159 for the dd - ibp .",
    "because there are only two data points , with @xmath160 and @xmath161 , specifying @xmath158 is sufficient for specifying the full proximity matrix @xmath37 .",
    "for the dhbp , we set @xmath162 and @xmath163 . also facilitating comparison ,",
    "@xmath164 $ ] is the same between both models ( when @xmath165 ) .",
    "figure  [ fig : featureshare_poisson ] shows that as the proximity @xmath158 increases to @xmath166 , the number of shared features @xmath119 tends to increase under both models .",
    "more precisely , @xmath157 concentrates on larger values of @xmath119 as @xmath158 increases . however , the way in which the probability mass functions change with @xmath158 differs between the two models . in the dd - ibp , the most likely value of @xmath119 increases smoothly , while under the dhbp it remains roughly constant and then jumps .",
    "as one varies @xmath158 across its full range , the set of most likely values for @xmath119 under the dd - ibp spans its full range from @xmath153 to @xmath167 , while under the dhbp the most likely value for @xmath119 takes only a few values . instead",
    ", varying @xmath158 under the dhbp allows a variety of bimodal distributions centered near the values from the asymptotic analysis .",
    "this difference in non - asymptotic behaviors mirrors the difference between the two models in the asymptotic regime , where the dd - ibp allows feature - sharing - proportions to be specified almost arbitrarily but allows little flexibility in modeling uncertainty about them , and the dhbp limits the number of possible values for the feature - sharing proportions , but allows uncertainty over these values .",
    "given a dataset @xmath168 and a latent feature model @xmath169 with parameter @xmath170 , the goal of inference is to compute the joint posterior over the customer assignment matrix @xmath22 , the dd - ibp hyperparameter @xmath6 , and likelihood parameter @xmath170 , as given by bayes rule : @xmath171 where the first term is the likelihood ( recall that @xmath0 is a deterministic function of @xmath22 and @xmath25 ) , the second term is the prior over parameters , the third term is the dd - ibp prior over the connectivity matrix @xmath22 , the fourth term is the prior over the ownership vector @xmath25 , and the last term is the prior over @xmath6 . in",
    "what follows , we assume that @xmath172 is conditionally independent of @xmath173 and @xmath174 for @xmath175 given @xmath176 and @xmath170 .    exact inference in this model is computationally intractable .",
    "we therefore use mcmc sampling @xcite to approximate the posterior with @xmath177 samples .",
    "the algorithm can be adapted to different datasets by choosing an appropriate likelihood function . in the next section ,",
    "we show how to adapt this algorithm to a simple linear - gaussian model .",
    "our algorithm combines gibbs and metropolis updates . for gibbs updates , we sample a variable from its conditional distribution given the current states of all the other variables .",
    "conjugacy allows simple gibbs updates for @xmath170 and @xmath6 .",
    "because the dd - ibp prior is not conjugate to the likelihood , we use the metropolis algorithm to sample @xmath22 and @xmath25 .",
    "we generate proposals for @xmath22 and @xmath25 , and then accept or reject them based on the likelihood ratio .",
    "we further divide these updates into two cases : updates for `` owned '' ( active ) dishes and updates of dish ownership .",
    "* sampling @xmath170*. to sample the likelihood parameter @xmath170 , we draw from the following conditional distribution : @xmath178 where the prior and likelihood are problem - specific . to obtain a closed - form expression for this conditional distribution",
    ", the prior and likelihood must be conjugate . for non - conjugate priors",
    ", one can use alternative updates , such as metropolis - hastings or slice sampling @xcite .",
    "generally , updates for @xmath170 will be decomposed into separate updates for each component of @xmath170 . in some cases",
    ", @xmath170 can be marginalized analytically ; an example is presented in the next section .",
    "* sampling @xmath6*. to sample the hyperparameter @xmath6 , we draw from the following conditional distribution : @xmath179 where @xmath180 is determined by @xmath25 and the prior on @xmath6 is a gamma distribution with shape @xmath181 and inverse scale @xmath182 . using the conjugacy of the gamma and poisson distributions , the conditional distribution over @xmath6",
    "is given by : @xmath183    * sampling assignments for owned dishes*. we update customer assignments for owned dishes ( corresponding to `` active '' features ) using gibbs sampling .",
    "for @xmath184 , @xmath185 , and @xmath186 , we draw a sample from the conditional distribution over @xmath187 given the current state of all the other variables : @xmath188 where @xmath172 is the @xmath2th row of @xmath189 , @xmath190 is the @xmath2th row of @xmath22 , and @xmath191 is @xmath22 excluding row @xmath2 .",
    "is conditionally independent of @xmath192 given @xmath193 , and @xmath170 . ]",
    "the first factor in eq .",
    "[ eq : cond ] is the likelihood , ( i.e. , those for which @xmath194 ) . ] and the second factor is the prior , given by @xmath195 . in considering possible assignments of @xmath187 , one of two scenarios will occur : either data point @xmath2 reaches the owner of @xmath3 ( in which case feature @xmath3 becomes active for @xmath2 as well as for all other data points that reach @xmath2 ) , or it does not ( in which case feature @xmath3 becomes inactive for @xmath2 as well as for all other data points that reach @xmath2 ) .",
    "this means we only need to consider two different likelihoods when updating @xmath187 .",
    "* sampling dish ownership*. we update dish ownership and customer assignments for newly owned dishes ( corresponding to features going from inactive to active in the sampling step ) using metropolis sampling .",
    "both a new connectivity matrix @xmath196 and ownership vector @xmath197 are proposed by drawing from the prior , and then accepted or rejected according to a likelihood ratio . in more detail ,",
    "the update proceeds as follows .    1 .",
    "propose @xmath198 for each data point @xmath199 .",
    "2 .   set @xmath200 . then populate or depopulate it by performing , for each @xmath199 , 1 .   if @xmath201 , insert @xmath202 new dishes into @xmath41 .",
    "then , for all @xmath203 $ ] and @xmath204 , sample @xmath205 according to @xmath206 .",
    "2 .   if @xmath207 , remove @xmath202 randomly selected dishes from @xmath41 . +",
    "this reallocation of dishes induces a new ownership vector @xmath197 .",
    "3 .   compute the acceptance ratio @xmath208 . because the prior ( conditional on the current state of the markov chain ) is being used as the proposal distribution",
    ", the acceptance ratio reduces to a likelihood ratio ( the prior and proposal terms cancel out ) : @xmath209.\\end{aligned}\\ ] ] 4 .",
    "draw @xmath210 . set @xmath211 and @xmath212",
    "if @xmath213 , otherwise leave @xmath22 and @xmath25 unchanged .    iteratively applying these updates , the sampler will ( after a burn - in period ) draw samples from a distribution that approaches the posterior ( eq . [ eq : post ] ) as the burn - in period grows large .",
    "the time complexity of this algorithm is dominated by the reachability computation , @xmath214 , and the likelihood computation , which is @xmath215 if coded naively ( see @xcite for a more efficient implementation using rank - one updates ) .",
    "as an example of how the dd - ibp can be used in data analysis , we incorporate it into a linear - gaussian latent feature model ( figure [ fig : schematic_linear ] ) .",
    "this model was originally studied for the ibp by griffiths and ghahramani @xcite .",
    "the observed data @xmath216 consist of @xmath217 objects , each of which is a @xmath218-dimensional vector of real - valued object properties .",
    "we model @xmath189 as a linear combination of binary latent features corrupted by gaussian noise : @xmath219 where @xmath220 is a @xmath221 matrix of real - valued weights , and @xmath222 is a @xmath223 matrix of independent , zero - mean gaussian noise terms with standard deviation @xmath224 .",
    "we place a zero - mean gaussian prior on @xmath220 with covariance @xmath225 .",
    "intuitively , the weights capture how the latent features interact to produce the observed data .",
    "for example , if each latent feature corresponds to a person in an image , then the weight @xmath226 captures the contribution of person @xmath3 to pixel @xmath227 .    within the algorithm of the previous section , @xmath228 . as a consequence of our gaussian assumptions",
    ", @xmath220 can be marginalized analytically , yielding the likelihood : @xmath229 where tr(@xmath230 ) is the matrix trace and @xmath231 . in calculating the likelihood",
    ", we only include the `` active '' columns of @xmath0 ( i.e. , those for which @xmath232 ) , and @xmath52 is the number of active columns .",
    "in this section we report experimental investigations of the dd - ibp and comparisons with alternative models .",
    "we first show how the dd - ibp can be used as a dimensionality reduction pre - processing technique for classification tasks when the data points are non - exchangeable .",
    "we then show how the dd - ibp can be applied to missing data problems .",
    "the performance of supervised learning algorithms is often enhanced by pre - processing the data to reduce its dimensionality @xcite .",
    "classical techniques for dimensionality reduction , such as principal components analysis and factor analysis , assume exchangeability , as does the infinite latent feature model based on the ibp @xcite .",
    "for this reason , these techniques may not work as well for pre - processing non - exchangeable data , and this may adversely affect their performance on supervised learning tasks .",
    "we investigated this hypothesis using a magnetic resonance imaging ( mri ) data set collected from 27 patients with alzheimer s disease and 35 healthy controls @xcite .",
    "the observed features consist of 4 structural summary statistics measured in 56 brain regions of interest : ( 1 ) surface area ; ( 2 ) shape index ; ( 3 ) curvedness ; ( 4 ) fractal dimension .",
    "the classification task is to sort individuals into alzheimer s or control classes based on their observed features .",
    "age - related changes in brain structure produce natural declines in cognitive function that make diagnosis of alzheimer s disease difficult @xcite .",
    "thus , it is important to take age into account when designing predictive models . for the dd - ibp and dhbp",
    ", age is naturally incorporated as a covariate over which we constructed a distance matrix .",
    "specifically , we defined @xmath30 as the absolute age difference between subjects @xmath2 and @xmath24 , with @xmath32 for @xmath62 ( i.e. , the distance matrix is sequential ) .",
    "this induces a prior belief that individuals with similar ages tend to share more latent features . in the mri data set",
    ", ages ranged from 60 to 90 ( median : @xmath233 ) .        in detail",
    ", we ran 1500 iterations of mcmc sampling on the entire data set using the linear - gaussian observation model , and then selected the latent features of the _ maximum a posteriori _ sample as input to a supervised learning algorithm ( @xmath234-regularized logistic regression , with the regularization constant set to @xmath235 ) .",
    "training was performed on half of the data , and testing on the other half .",
    "the noise hyperparameters of the dd - ibp and dhbp ( @xmath224 and @xmath236 ) were updated using metropolis - hastings proposals .",
    "we monitored the log of the joint distribution @xmath237 .",
    "visual inspection of the log joint probability traces suggested that the sampler reaches a local maximum within 400 - 500 iterations ( figure [ fig : ad_trace ] ) .",
    "this process was repeated for a range of decay parameter ( @xmath238 ) values , using the exponential decay function .",
    "the same proximity matrix , @xmath37 , was used for both the dd - ibp and dhbp .",
    "we performed 5 random restarts of the sampler and recomputed the classification measure for each restart , averaging the resulting measures to reduce sampling variability . for comparison",
    ", we also made predictions using the standard ibp , the dibp @xcite , and the raw observed features ( i.e. , no pre - processing ) .",
    "the dibp was fit using the mcmc algorithm described in williamson et al .",
    "@xcite , which adaptively samples the parameters controlling dependencies between observations ( thus the results do not depend on @xmath238 ) .",
    "classification results are shown in figure [ fig : ad_results ] ( left ) , where performance is measured as the area under the receiver operating characteristic curve ( auc ) .",
    "chance performance corresponds to an auc of 0.5 , perfect performance to an auc of 1 . for a range of @xmath238 values",
    ", the dd - ibp produces superior classification performance to the alternative models , with performance increasing as a function of @xmath238 .",
    "the dhbp performs worse relative to the raw data for low @xmath238 values .",
    "the magnitude of the standard error ( across random restarts ) is roughly @xmath239 that of the means .",
    "we also ran the dd - ibp sampler with @xmath240 ( in which case the dd - ibp and ibp are equivalent ) and found no significant difference between it and the standard ibp sampler with respect to performance on the alzheimer s classification and the eeg reconstruction ( see next section ) .          as an example of a missing data problem , we use latent feature models to reconstruct missing observations in electroencephalography ( eeg ) time series .",
    "the eeg data are from a visual detection experiment in which human subjects were asked to count how many times a particular image appeared on the screen @xcite .",
    "the data were collected as part of a larger effort to design brain - computer interfaces to assist physically disabled subjects .",
    "distance between data points was defined using the absolute time - difference .",
    "data were z - scored prior to analysis . for 10 of the data points , we removed 2 of the observed features at random .",
    "we then ran the mcmc sampler for 1500 iterations , adding gibbs updates for the missing data by sampling from the observation distribution ( eq . [ eq : lik ] ) conditional on the current values of the latent features and hyperparameters .",
    "we then used the map sample for reconstruction .",
    "we measured performance by the squared reconstruction error on the missing data .",
    "figure [ fig : eeg ] shows the reconstruction results , demonstrating that the dd - ibp is effective for reconstructing missing data in this dataset and achieves lower reconstruction error than the alternative models we consider .",
    "by relaxing the exchangeability assumption for infinite latent feature models , the dd - ibp extends their applicability to a richer class of data .",
    "we have shown empirically that this innovation fares better than the standard ibp on non - exchangeable data ( e.g. , timeseries ) .",
    "we note that the dd - ibp is not a standard bayesian nonparametric distribution , in the sense of arising from a de finetti mixing distribution . for the standard ibp , the de",
    "finetti mixing distribution has been identified as the beta process @xcite , but this result does not generalize to the dd - ibp due to its non - exchangeability ( a consequence of de finetti s theorem ) .",
    "nonetheless , this does not detract from our model s ability to let the data infer the number of latent features , a property that it shares with other infinite latent feature models .",
    "a number of future directions are possible .",
    "first , it may be possible to exploit distance dependence to derive more efficient samplers .",
    "in particular , doshi - velez and ghahramani @xcite have shown that partitioning the data into subsets enables faster gibbs sampling for the traditional ibp ; the window decay function imposes a natural partition of the data into conditionally independent subsets .",
    "second , application of the dd - ibp to other likelihood functions is straightforward .",
    "for example , it could be applied to relational data @xcite or text data @xcite . as pointed out by miller et al .",
    "@xcite , covariates like age or location often play an important role in link prediction . whereas miller et al .",
    "@xcite incorporated covariates into the likelihood function , one could instead incorporate them into the prior by defining covariate - based distances between data points ( e.g. , the age difference between two people ) .",
    "a distinction of the latter approach is that it would allow one to model dependencies in terms of latent features .",
    "for instance , two people close in age or geographic location may be more likely to share latent interests , a pattern naturally captured by the dd - ibp .",
    "third , modeling shared dependency structure across groups is important for several applications . in fmri and eeg studies , for example ,",
    "similar spatial and temporal dependencies are frequently observed across subjects .",
    "modeling shared structure without sacrificing intersubject variability has been addressed with hierarchical models @xcite .",
    "one way to extend the dd - ibp hierarchically would be to allow the parameters of the decay function to vary across individuals while being coupled together by higher - level variables .",
    "sjg was supported by a nsf graduate research fellowship .",
    "pif acknowledges support from nsf award @xmath241 .",
    "we thank matt hoffman , chong wang , gungor polatkan , sean gerrish and john paisley for helpful discussions .",
    "we are also grateful to sinead williamson and mingyuan zhou for sharing their code .",
    "10    a.  ahmed and e.p .",
    "timeline : a dynamic hierarchical dirichlet process model for recovering birth / death and evolution of topics in text stream . in _ proc . of uai _ , 2010 .",
    "beckmann , m.  jenkinson , and s.m .",
    ", 20(2):10521063 , 2003 .",
    "jm  bernardo and a.f.m .",
    "john wiley & son ltd , 1994 .",
    "c.m . bishop . .",
    "springer , 2006 .",
    "d.  blackwell and j.  macqueen .",
    "ferguson distributions via plya urn schemes .",
    ", 1(2):353355 , 1973 .",
    "d.  blei and p.i .",
    "frazier . . , 2010 .",
    "f.  caron , m.  davy , and a.  doucet . .",
    "uncertainty in artificial intelligence _",
    ", 2007 .",
    "n.  christou and i.d .",
    "dinov . confidence interval based parameter estimationa new socr applet and activity .",
    ", 6(5):e19178 , 2011 .    y.  chung and d.b .",
    ", 63(1):5980 , 2011 .",
    "f.  doshi - velez and z.  ghahramani . .",
    "in _ international conference on machine learning _ , pages 273280 , 2009 .",
    "f.  doshi - velez and z.  ghahramani . . , 2009",
    "duan , m.  guindani , and a.e .",
    "gelfand . .",
    ", 94(4):809825 , 2007 .",
    "t.  erkinjuntti , r.  laaksonen , r.  sulkava , r.  syrjlinen , and j.  palo .",
    "neuropsychological differentiation between normal aging , alzheimer s disease and vascular dementia .",
    ", 74(5):393403 , 1986 .",
    "m.d . escobar and m.  west .",
    ", 90(430):577588 , 1995 .",
    "griffin and m.f.j .",
    "101(473):179194 , 2006 .    t.l .",
    "griffiths and z.  ghahramani . .",
    ", 18 , 2005 .",
    "griffiths and z.  ghahramani . .",
    ", 12:11851224 , 2011 .",
    "u.  hoffmann , j.m .",
    "vesin , t.  ebrahimi , and k.  diserens . .",
    ", 167(1):115125 , 2008 .",
    "d.  knowles and z.  ghahramani . .",
    ", pages 381388 , 2007 .",
    "e.  meeds , z.  ghahramani , r.m .",
    "neal , and s.t .",
    "roweis . . , 19 , 2007 .    k.t",
    "miller , t.l .",
    "griffiths , and m.i .",
    "miller , t.l .",
    "griffiths , and m.i .",
    "jordan . . , 2009 .",
    "navarro and t.l .",
    "griffiths . .",
    ", 20(11):25972628 , 2008 .",
    "v.  rao and y.w .",
    "teh . spatial normalized gamma processes . , 22:15541562 , 2009 .",
    "rasmussen . .",
    ", 12:554560 , 2000 .",
    "l.  ren , y.  wang , d.  dunson , and l.  carin . the kernel beta process . , 2011 .",
    "robert and g.  casella . .",
    "springer verlag , 2004 .",
    "r.  thibaux and m.i .",
    "in _ international conference on artificial intelligence and statistics _ , volume  11 , pages 564571 .",
    "citeseer , 2007 .",
    "s.  williamson , p.  orbanz , and z.  ghahramani . .",
    ", 2010 .",
    "woolrich , t.e.j .",
    "behrens , c.f .",
    "beckmann , m.  jenkinson , and s.m .",
    ", 21(4):17321747 , 2004 .",
    "m.  zhou , h.  yang , g.  sapiro , d.  dunson , and l.  carin .",
    "dependent hierarchical beta process for image interpolation and denoising .",
    "recall that @xmath112 is the number of features held by data point @xmath2 , and @xmath113 is the number of features shared by data points @xmath2 and @xmath24 , where @xmath114 .        for each feature",
    ", there is some probability @xmath242 that it is turned on for data point @xmath2 , and some probability @xmath243 that it is shared by @xmath2 and @xmath24 ( note that features are exchangeable in the dd - ibp ) .",
    "the total number of features across all data points is distributed according to @xmath244 , where @xmath245 . since each feature",
    "is turned on independently , the total number of active features for a single data point @xmath2 is distributed according to @xmath246 .",
    "similarly , the total number of features shared by data points @xmath2 and @xmath24 is @xmath247 .",
    "the activation and co - activation probabilities are given by : @xmath248 where we have used that @xmath249 .",
    "we write the random measures @xmath81 and @xmath250 in the generative model defining the dhbp in section 3.2 as the following mixtures over point masses . @xmath251",
    "recall that @xmath252 where @xmath253 .",
    "let @xmath1 be the random variable that is @xmath166 if the bernoulli process draw @xmath98 has atom @xmath254 , and @xmath153 if not .",
    "we have @xmath255 . because @xmath85 is continuous , @xmath256 for @xmath257 and the random variables @xmath118 and @xmath119",
    "satisfy @xmath258 we first show that @xmath118 is poisson distributed with mean @xmath97 .      for a given @xmath222 , the density of @xmath102 conditioned on @xmath262 is : @xmath263 we can use this density to calculate the success probability @xmath259 : @xmath264     = \\mathbb{e}[p^\\ast_{g_ik}|p_k > \\epsilon ]   = \\mathbb{e}[p_k|p_k > \\epsilon ]   = \\frac{\\int_{\\epsilon}^1 p c_0 p^{-1}(1-p)^{c_0 - 1 } dp}{\\int_{\\epsilon}^1 c_0 p^{-1}(1-p)^{c_0 - 1 } dp},\\ ] ] where we have used the tower property of conditional expectation in the second and third equalities .",
    "let @xmath268 be the number of such atoms that are also in @xmath98 .",
    "because @xmath268 is the sum of @xmath266 independent bernoulli trials that each have success probability @xmath259 , it follows that @xmath269 and @xmath270 because @xmath271 , it follows that @xmath272 , where @xmath273 \\left[\\frac{\\int_{\\epsilon}^1 p c_0 p^{-1}(1-p)^{c_0 - 1 } dp}{\\int_{\\epsilon}^1 c_0 p^{-1}(1-p)^{c_0 - 1 } dp } \\right ] \\nonumber \\\\ & = \\lim_{\\epsilon \\rightarrow 0 } \\gamma \\int_{\\epsilon}^1 p c_0",
    "p^{-1}(1-p)^{c_0 - 1 } dp \\nonumber   = \\gamma c_0 \\int_{0}^1 ( 1-p)^{c_0 - 1 } dp \\nonumber   = \\gamma,\\end{aligned}\\ ] ] where we have used that @xmath274 . thus @xmath275 .",
    "we perform a similar analysis to show the distribution of @xmath119 .",
    "let @xmath276 denote the probability that @xmath98 and @xmath277 share atom @xmath254 conditional on @xmath260 , @xmath134 and @xmath135 .",
    "that is , @xmath278 although only @xmath222 appears in the argument of @xmath276 , this quantity also implicitly depends on @xmath134 and @xmath135 .",
    "we calculate @xmath276 explicitly below .",
    "to calculate @xmath284 , we consider two cases . in each case",
    ", we first calculate @xmath276 and then calculate the limit , showing that it is the same as the mean of @xmath119 claimed in the statement of the proposition .    *",
    "* case 1 * : @xmath285 @xmath286 \\nonumber   = \\mathbb{e}[(p^\\ast_{g_ik})^2|p_k > \\epsilon , g_i , g_j ] \\nonumber \\\\ & = \\mathbb{e}[\\mathbb{e}[(p^\\ast_{g_ik})^2|p_k , g_i , g_j]|p_k > \\epsilon , g_i , g_j ] \\nonumber   = \\mathbb{e}[p_k ( c_1 p_k + 1)/(c_1 + 1)|p_k > \\epsilon , g_i , g_j ] \\nonumber \\\\ & = \\frac{\\int_{\\epsilon}^1 \\frac{c_1 p + 1}{c_1 + 1 } p c_0 p^{-1}(1-p)^{c_0 - 1 } dp}{\\int_{\\epsilon}^1 c_0 p^{-1}(1-p)^{c_0 - 1 } dp }   = \\gamma \\frac{c_0}{c_1 + 1 } \\frac{\\int_{\\epsilon}^1 ( c_1p+1 ) ( 1-p)^{c_0 - 1 } dp}{\\lambda_\\epsilon}.\\end{aligned}\\ ] ] then the limit @xmath287 can be written @xmath288 \\nonumber   \\\\ & = \\gamma \\frac{c_0}{c_1 + 1 } \\left [ \\frac{c_1}{c_0(c_0 + 1 ) } + \\frac{1}{c_0 } \\right ] \\nonumber   = \\gamma \\frac{c_0 + c_1 + 1}{(c_0 + 1)(c_1 + 1)},\\end{aligned}\\ ] ] where we have used that @xmath274 and @xmath289 . *",
    "* case 2 * : @xmath290 @xmath291 \\nonumber   = \\mathbb{e}[\\mathbb{e}[p^\\ast_{g_ik } p^\\ast_{g_jk } |p_k , g_i , g_j]|p_k > \\epsilon , g_i , g_j ] \\nonumber \\\\ & = \\mathbb{e}[p_k^2|p_k > \\epsilon , g_i , g_j ] \\nonumber   = \\gamma \\frac{\\int_{\\epsilon}^1 p^2 c_0 p^{-1}(1-p)^{c_0 - 1 } dp}{\\lambda_\\epsilon}.\\end{aligned}\\ ] ] then the limit @xmath287 can be written @xmath292 where we have used that @xmath289 ."
  ],
  "abstract_text": [
    "<S> latent feature models are widely used to decompose data into a small number of components . </S>",
    "<S> bayesian nonparametric variants of these models , which use the indian buffet process ( ibp ) as a prior over latent features , allow the number of features to be determined from the data . </S>",
    "<S> we present a generalization of the ibp , the _ distance dependent indian buffet process _ </S>",
    "<S> ( dd - ibp ) , for modeling non - exchangeable data . </S>",
    "<S> it relies on distances defined between data points , biasing nearby data to share more features . </S>",
    "<S> the choice of distance measure allows for many kinds of dependencies , including temporal and spatial . </S>",
    "<S> further , the original ibp is a special case of the dd - ibp . in this paper </S>",
    "<S> , we develop the dd - ibp and theoretically characterize its feature - sharing properties . </S>",
    "<S> we derive a markov chain monte carlo sampler for a linear gaussian model with a dd - ibp prior and study its performance on several non - exchangeable data sets . </S>",
    "<S> keywords : bayesian nonparametrics , dimensionality reduction , matrix factorization </S>"
  ]
}