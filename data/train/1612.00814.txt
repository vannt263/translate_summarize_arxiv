{
  "article_text": [
    "understanding the 3d world is at the heart of successful computer vision applications in robotics , rendering and modeling  @xcite .",
    "it is especially important to solve this problem using the most convenient visual sensory data : 2d images . in this paper",
    ", we propose an end - to - end solution to the challenging problem of predicting the underlying true shape of an object given an arbitrary single image observation of it .",
    "this problem definition embodies a fundamental challenge : imagery observations of 3d shapes are interleaved representations of intrinsic properties of the shape itself ( e.g. , geometry , material ) , as well as its extrinsic properties that depend on its interaction with the observer and the environment ( e.g. , orientation , position , and illumination ) .",
    "physically principled shape understanding should be able to efficiently disentangle such interleaved factors .",
    "this observation leads to insight that an end - to - end solution to this problem from the perspective of learning agents ( neural networks ) should involve the following properties : 1 ) the agent should understand the physical meaning of how a 2d observation is generated from the 3d shape , and 2 ) the agent should be conscious about the outcome of its interaction with the object ; more specifically , by moving around the object , the agent should be able to correspond the observations to the viewpoint change .",
    "if such properties are embodied in a learning agent , it will be able to disentangle the shape from the extrinsic factors because these factors are trivial to understand in the 3d world . to enable the agent with these capabilities , we introduce a built - in camera system that can transform the 3d object into 2d images in - network . additionally , we architect the network such that the latent representation disentangles the shape from view changes .",
    "more specifically , our network takes as input an object image and predicts its volumetric 3d shape so that the perspective transformations of predicted shape match well with corresponding 2d observations .",
    "we implement this neural network based on a combination of image encoder , volume decoder and perspective transformer ( similar to spatial transformer as introduced by jaderberg et al .",
    "@xcite ) . during training",
    ", the volumetric 3d shape is gradually learned from single - view input and the feedback of other views through back - propagation .",
    "thus at test time , the 3d shape can be directly generated from a single image .",
    "we conduct experimental evaluations using a subset of 3d models from shapenetcore  @xcite .",
    "results from single - class and multi - class training demonstrate excellent performance of our network for volumetric 3d reconstruction .",
    "our main contributions are summarized below .    *",
    "we show that neural networks are able to predict 3d shape from single - view without using the ground truth 3d volumetric data for training .",
    "this is made possible by introducing a 2d silhouette loss function based on perspective transformations . *",
    "we train a single network for multi - class 3d object volumetric reconstruction and show its generalization potential to unseen categories . * compared to training with full azimuth angles , we demonstrate comparatively similar results when training with partial views .",
    "[ [ representation - learning - for-3d - objects . ] ] representation learning for 3d objects .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    recently , advances have been made in learning deep neural networks for 3d objects using large - scale cad databases  @xcite .",
    "wu et al .",
    "@xcite proposed a deep generative model that extends the convolutional deep belief network @xcite to model volumetric 3d shapes .",
    "different from @xcite that uses volumetric 3d representation , su et al .",
    "@xcite proposed a multi - view convolutional network for 3d shape categorization with a view - pooling mechanism .",
    "these methods focus more on 3d shape recognition instead of 3d shape reconstruction .",
    "recent work  @xcite attempt to learn a joint representation for both 2d images and 3d shapes .",
    "tatarchenko et al .",
    "@xcite developed a convolutional network to synthesize unseen 3d views from a single image and demonstrated the synthesized images can be used them to reconstruct 3d shape .",
    "qi et al .",
    "@xcite introduced a joint embedding by combining volumetric representation and multi - view representation together to improve 3d shape recognition performance .",
    "girdhar et al .",
    "@xcite proposed a generative model for 3d volumetric data and combined it with a 2d image embedding network for single - view 3d shape generation .",
    "choy et al .",
    "@xcite introduce a 3d recurrent neural network ( 3d - r2n2 ) based on long - short term memory ( lstm ) to predict the 3d shape of an object from a single view or multiple views .",
    "compared to these single - view methods , our 3d reconstruction network is learned end - to - end and the network can be even trained without ground truth volumes .",
    "concurrent to our work , renzede et al .",
    "@xcite introduced a general framework to learn 3d structures from 2d observations with 3d-2d projection mechanism .",
    "their 3d-2d projection mechanism either has learnable parameters or adopts non - differentiable component using mcmc , while our perspective projection network is both differentiable and parameter - free .    [ [ representation - learning - by - transformations . ] ] representation learning by transformations .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    learning from transformed sensory data has gained attention   @xcite in recent years .",
    "memisevic and hinton  @xcite introduced a gated boltzmann machine that models the transformations between image pairs using multiplicative interaction .",
    "reed et al .",
    "@xcite showed that a disentangled hidden unit representations of boltzmann machines ( disbm ) could be learned based on the transformations on data manifold .",
    "yang et al .",
    "@xcite learned out - of - plane rotation of rendered images to obtain disentangled identity and viewpoint units by curriculum learning .",
    "kulkarni et al .",
    "@xcite proposed to learn a semantically interpretable latent representation from 3d rendered images using variational auto - encoders @xcite by including specific transformations in mini - batches .",
    "complimentary to convolutional networks , jaderberg et al .",
    "@xcite introduced a differentiable sampling layer that directly incorporates geometric transformations into representation learning .",
    "concurrent to our work , wu et al .",
    "@xcite proposed a 3d-2d projection layer that enables the learning of 3d object structures using 2d keypoints as annotation .",
    "in this section , we develop neural networks for reconstructing 3d objects . from the perspective of a learning agent ( e.g. , neural network ) , a natural way to understand one 3d object @xmath0 is from its 2d views by transformations . by moving around the 3d object",
    ", the agent should be able to recognize its unique features and eventually build a 3d mental model of it as illustrated in figure  [ fig : intro](a ) .",
    "assume that @xmath1 is the 2d image from the @xmath2-th viewpoint @xmath3 by projection @xmath4 , or rendering in graphics .",
    "an object @xmath0 in a certain scene is the entanglement of shape , color and texture ( its intrinsic properties ) and the image @xmath1 is the further entanglement with viewpoint and illumination ( extrinsic parameters ) .",
    "the general goal of understanding 3d objects can be viewed as disentangling intrinsic properties and extrinsic parameters from a single image .     and @xmath5 .",
    "]    in this paper , we focus on the 3d shape learning by ignoring the color and texture factors , and we further simplify the problem by making the following assumptions : 1 ) the scene is clean white background ; 2 ) the illumination is constant natural lighting .",
    "we use the volumetric representation of 3d shape @xmath6 where each voxel @xmath7 is a binary unit .",
    "in other words , the voxel equals to one , i.e. , @xmath8 , if the @xmath9-th voxel sapce is occupied by the shape ; otherwise @xmath10 . assuming the 2d silhouette @xmath11 is obtained from the @xmath2-th image @xmath1 , we can specify the 3d-2d projection @xmath12 .",
    "note that 2d silhouette estimation is typically solved by object segmentation in real - world but it becomes trivial in our case due to the white background . in the following sub - sections ,",
    "we propose a formulation for learning to predict the volumetric 3d shape @xmath6 from an image @xmath1 with and without the 3d volume supervision .",
    "we consider single - view volumetric 3d reconstruction as a dense prediction problem and develop a convolutional encoder - decoder network for this learning task denoted by @xmath13 .",
    "the encoder network @xmath14 learns a _ viewpoint - invariant _ latent representation @xmath15 which is then used by the decoder @xmath16 to generate the volume @xmath17 . in case the ground truth volumetric shapes @xmath6 are available , the problem can be easily considered as learning volumetric 3d shapes with a regular reconstruction objective in 3d space : @xmath18 . in practice , however , _ the ground truth volumetric 3d shapes may not be available for training_. for example , the agent observes the 2d silhouette via its built - in camera without accessing the volumetric 3d shape .",
    "inspired by the space carving theory  @xcite , we propose a silhouette - based volumetric loss function .",
    "in particular , we build on the premise that a 2d silhouette @xmath19 projected from the generated volume @xmath20 under certain camera viewpoint @xmath21 should match the ground truth 2d silhouette @xmath22 from image observations . in other words ,",
    "if all the generated silhouettes @xmath19 match well with their corresponding ground truth silhouettes @xmath22 for all @xmath23 s , then we hypothesize that the generated volume @xmath20 should be as good as one instance of _ visual hull _",
    "equivalent class of the ground truth volume @xmath6  @xcite .",
    "therefore , we formulate the learning objective for the @xmath2-th image as @xmath24 where @xmath23 is the index of output 2d silhouettes , @xmath25 is the number of silhouettes used for each input image and @xmath26 is the 3d-2d projection function . note that the above training objective eq .",
    "enables training without using ground - truth volumes .",
    "the network diagram is illustrated in figure  [ fig : intro](b ) .",
    "a more general learning objective is given by a combination of both objectives : @xmath27 where @xmath28 and @xmath29 are constants that control the tradeoff between the two losses .      as defined previously ,",
    "2d silhouette @xmath11 is obtained via perspective projection given input 3d volume @xmath6 and specific camera viewpoint @xmath3 . in this work ,",
    "we implement the perspective projection ( see figure  [ fig : intro](c ) ) with a 4-by-4 transformation matrix @xmath30 , where @xmath31 is camera calibration matrix and @xmath32 is extrinsic parameters .",
    "@xmath33 for each point @xmath34 in 3d world coordinates , we compute the corresponding point @xmath35 in screen coordinates ( plus disparity @xmath36 ) using the perspective transformation : @xmath37 .",
    "similar to the spatial transformer network introduced in  @xcite , we propose a 2-step procedure : ( 1 ) performing dense sampling from input volume ( in 3d world coordinates ) to output volume ( in screen coordinates ) , and ( 2 ) flattening the 3d spatial output across disparity dimension . in the experiment",
    ", we assume that transformation matrix is always given as input , parametrized by the viewpoint @xmath38 .",
    "again , the 3d point @xmath39 in input volume @xmath40 and corresponding point @xmath41 in output volume @xmath42 is linked by perspective transformation matrix @xmath30 . here ,",
    "@xmath43 and @xmath44 are the width , height and depth of input and output volume , respectively .",
    "we summarize the dense sampling step and channel - wise flattening step as follows .",
    "@xmath45 here , @xmath46 is the @xmath9-th voxel value corresponding to the point @xmath41 ( where @xmath47 .",
    "note that we use the @xmath48 operator for projection instead of summation along one dimension since the volume is represented as a binary cube where the solid voxels have value 1 and empty voxels have value 0 .",
    "intuitively , we have the following two observations : ( 1 ) each empty voxel will not contribute to the foreground pixel of @xmath49 from any viewpoint ; ( 2 ) each solid voxel can contribute to the foreground pixel of @xmath49 only if it is visible from a specific viewpoint .      as the same volumetric 3d shape",
    "is expected to be generated from different images of the object , the encoder network is required to learn a 3d view - invariant latent representation @xmath50 this sub - problem itself is a challenging task in computer vision  @xcite .",
    "thus , we adopt a two - stage training procedure : first , we learn the encoder network for a 3d view - invariant latent representation @xmath51 and then train the volumetric decoder with perspective transformer networks . as shown in  @xcite , a disentangled representation of 2d synthetic images can be learned from consecutive rotations with a recurrent network , we pre - train the encoder of our network using a similar curriculum strategy so that the latent representation only contains 3d view - invariant identity information of the object .",
    "once we obtain an encoder network that recognizes the identity of single - view images , we next learn the volume generator regularized by the perspective transformer networks . to encourage the volume decoder to learn a consistent 3d volume from different viewpoints , we include the projections from neighboring viewpoints in each mini - batch so that the network has relatively sufficient information to reconstruct the 3d shape .",
    "[ [ shapenetcore . ] ] shapenetcore .",
    "+ + + + + + + + + + + + +    this dataset contains about 51,300 unique 3d models from 55 common object categories @xcite .",
    "each 3d model is rendered from 24 azimuth angles ( with steps of 15@xmath52 ) with fixed elevation angles ( 30@xmath52 ) under the same camera and lighting setup .",
    "we then crop and rescale the centering region of each image to @xmath53 pixels . for each ground truth",
    "3d shape , we create a volume of @xmath54 voxels from its canonical orientation ( @xmath55 ) .            as shown in figure  [ fig : net_arch ] , our encoder - decoder network has three components : a 2d convolutional encoder , a 3d up - convolutional decoder and a perspective transformer networks .",
    "the 2d convolutional encoder consists of 3 convolution layers , followed by 3 fully - connected layers ( convolution layers have 64 , 128 and 256 channels with fixed filter size of @xmath56 ; the three fully - connected layers have 1024 , 1024 and 512 neurons , respectively ) .",
    "the 3d convolutional decoder consists of one fully - connected layer , followed by 3 convolution layers ( the fully - connected layer have @xmath57 neurons ; convolution layers have 256 , 96 and 1 channels with filter size of @xmath58 , @xmath59 and @xmath60 ) . for perspective transformer networks , we used perspective transformation to project 3d volume to 2d silhouette where the transformation matrix is parametrized by 16 variables and sampling grid is set to @xmath54 .",
    "we use the same network architecture for all the experiments .",
    "we used the adam @xcite solver for stochastic optimization in all the experiments . during the pre - training stage ( for encoder ) , we used mini - batch of size 32 , 32 , 8 , 4 , 3 and 2 for training the rnn-1 , rnn-2 , rnn-4 , rnn-8 , rnn-12 and rnn-16 as used in yang et al .",
    "we used the learning rate @xmath61 for rnn-1 , and @xmath62 for the rest of recurrent neural networks . during the fine - tuning stage ( for volume decoder ) , we used mini - batch of size 6 and learning rate @xmath61 . for each object in a mini - batch , we include projections from all 24 views as supervision .",
    "the models including the perspective transformer nets are implemented using torch  @xcite . to download the code",
    ", please refer to the project webpage : http://goo.gl/yej2h6 .      as mentioned in the formulation ,",
    "there are several variants of the model depending on the hyper - parameters of learning objectives @xmath28 and @xmath29 . in the experimental section ,",
    "we denote the model trained with projection loss only , volume loss only , and combined loss as * ptn - proj * ( pr ) , * cnn - vol * ( vo ) , and * ptn - comb * ( co ) , respectively . in the experiments ,",
    "we address the following questions : ( 1 ) will the model trained with combined loss achieve better single - view 3d reconstruction performance over model trained on volume loss only ( vs. ) ?",
    "( 2 ) what is the performance gap between the models with and without ground - truth volumes ( vs. ) ? ( 3 ) how do the three models generalize to instances from unseen categories which are not present in the training set ? to answer the questions , we trained the three models under two experimental settings : single category and multiple categories ."
  ],
  "abstract_text": [
    "<S> understanding the 3d world is a fundamental problem in computer vision . however , learning a good representation of 3d objects is still an open problem due to the high dimensionality of the data and many factors of variation involved . in this work </S>",
    "<S> , we investigate the task of single - view 3d object reconstruction from a learning agent s perspective . </S>",
    "<S> we formulate the learning process as an interaction between 3d and 2d representations and propose an encoder - decoder network with a novel projection loss defined by the perspective transformation . </S>",
    "<S> more importantly , the projection loss enables the unsupervised learning using 2d observation without explicit 3d supervision . </S>",
    "<S> we demonstrate the ability of the model in generating 3d volume from a single 2d image with three sets of experiments : ( 1 ) learning from single - class objects ; ( 2 ) learning from multi - class objects and ( 3 ) testing on novel object classes . </S>",
    "<S> results show superior performance and better generalization ability for 3d object reconstruction when the projection loss is involved . </S>"
  ]
}