{
  "article_text": [
    "shannon s _ differential entropy _  @xcite @xmath0 of a _ continuous random variable _",
    "@xmath1 following a probability density function @xmath2 ( denoted by @xmath3 ) on the support @xmath4 quantifies the amount of uncertainty  @xcite of @xmath1 by the following celebrated formula :    @xmath5    when the logarithm is expressed in basis @xmath6 , the entropy is measured in _",
    "bits_. when using the natural logarithm ( basis @xmath7 ) , the entropy is measured in _",
    "nats_. the entropy functional @xmath8 is _ concave _",
    "@xcite , may be _ negative _ is a gaussian distribution of mean @xmath9 and standard deviation @xmath10 , then @xmath11 , and is therefore negative when @xmath12 . ] , and may be _ infinite _ with @xmath13 for @xmath14 ( with support @xmath15 )",
    ". then @xmath16 .",
    "this result is to contrast with the fact that the discrete entropy on a finite alphabet @xmath17 is bounded by @xmath18 .",
    "] when the integral of eq .",
    "[ eq : h ] diverges .",
    "although closed - form formula for the differential entropy are available for many common statistical distributions ( see the devoted book  @xcite and  @xcite ) , the _ differential entropy of mixtures _ usually does not admit closed - form expressions  @xcite because the log term in eq .",
    "[ eq : h ] transforms into an untractable _ log - sum term _ when dealing with mixture densities .",
    "let us denote by @xmath19 the density of a mixture .",
    "the probability density of a weighted sum of random variables is obtained by convolution of the densities . ]",
    "@xmath20 with @xmath21 components @xmath22 , where @xmath23 denotes the @xmath21-dimensional open probability simplex .",
    "that is , a mixture is a _",
    "convex combination _ of component distributions @xmath24 .",
    "we shall consider mixtures of gaussians with component probability density functions @xmath25 such that : @xmath26 where @xmath27\\in\\bbr$ ] and @xmath28}>0 $ ] denote the mean parameter and the standard deviation of @xmath29 , respectively .",
    "statistical mixtures allow flexible fine modeling of _ arbitrary _ smooth densities : they are provably universal smooth density estimators .",
    "the most common mixtures are the gaussian mixture models ( gmms ) that are frequenty met in applications . to tackle the differential entropy of continuous mixtures , various _",
    "approximation techniques _ have been designed ( see  @xcite and references therein for a state - of - the - art ) . in practice , to estimate @xmath0 with @xmath3 , one uses the following _ monte - carlo ( mc ) stochastic integration _ : @xmath30 where @xmath31 is an independent and identically distributed ( iid ) set of variates sampled from @xmath3 .",
    "this mc estimator @xmath32 is consistent ( ie . , @xmath33 , convergence in probability ) .",
    "moshksar and khandani  @xcite recently considered the special case of isotropic spherical gaussian mixture models ( ie .",
    ", gmms with identical standard deviation ) , and used taylor expansions to arbitrarily finely approximate the differential entropy of those isotropic gmms .",
    "interestingly , they mentioned in their paper  @xcite the so - called _ maximum entropy upper bound _ ( meub ) that relies on the fact that the continuous distribution with prescribed variance maximizing the entropy is the gaussian distribution of same variance .",
    "since the entropy of a univariate gaussian @xmath34 is @xmath35 , we end up with the following maximum entropy upper bound for an arbitrary random variable @xmath1 : @xmath36\\right),\\ ] ] where @xmath37=e[(x - e[x])^2]=e[x^2]-e[x]^2 $ ] denotes the variance of @xmath1 .",
    "since the variance @xmath37 $ ] of an arbitrary gaussian mixture can be easily calculated in closed - form  @xcite : @xmath38=\\sum_{c=1}^k w_c\\left((\\mu_c-\\bar\\mu)^2+\\sigma_c^2)\\right),\\ ] ] with @xmath39 $ ] ) , eq .",
    "[ eq : meub ] yields the _ gaussian maxent upper bound _ : @xmath40    in this work , we propose to further use the maximum entropy upper bound principle to derive an _ infinite series _ of maxent upper bounds .",
    "although our bounds will be instantiated for gmms , they apply more broadly to univariate continous mixtures . for example , our maxent upper bounds also hold for mixtures of exponential families  @xcite that generalize the gmms ( and have always guaranteed finite entropy ) . for gmms",
    ", we shall show that the gaussian maxent upper bound is not necessarily the best maxent upper bound in closed form , and report instead a series of upper bounds .",
    "the paper is organized as follows : section  [ sec : maxent ] introduces the general principle for building maximum entropy ( maxent ) upper bounds .",
    "it is followed by section  [ sec : rgm ] that construct a series of maxent upper bounds derived from a special family of maxent distributions that we termed absolute monomial exponential families .",
    "those generic bounds are instantiated for gaussian mixture models ( gmms ) in section  [ sec : mm ] .",
    "section  [ sec : exp ] report on our experiments and discusses the tightness of the bounds .",
    "finally , section  [ sec : concl ] wrap ups the results and conclude the work .",
    "besides , an appendix provides the detailed calculation of the raw absolute moment of a non - centered normal distribution that is used in section  [ sec : mm ] to get closed - form maxent upper bounds for gmms .",
    "the maxent distribution principle was investigated by jaynes  @xcite to infer a distribution given several `` moment constraints . ''",
    "maxent asks to solve the following constrained optimization problem : @xmath41=\\eta_i,\\quad i\\in [ d]=\\{1,\\ldots , d\\}.\\ ] ] when an iid sample set @xmath31 is given , we may choose , for example , the raw geometric _ sample moments _",
    "@xmath42 for setting up the constraint @xmath43=\\eta_i$ ] ( ie . , taking @xmath44 in eq .",
    "[ eq : maxent ] ) .",
    "the distribution @xmath2 maximizing the entropy under those moment constraints is unique and termed the _ maxent distribution_. the constrained optimization of eq .",
    "[ eq : maxent ] is solved by means of lagrangian multipliers  @xcite .",
    "it is well - known  @xcite that the maxent distribution @xmath2 belongs to a _",
    "parametric family _ of distributions called an _ exponential family _  @xcite . an exponential family ( ef )",
    "admits the following _ canonical probability density function _ :    @xmath45    where @xmath46 denotes the scalar product , and @xmath47 the natural parameter vector belonging to the natural parameter space @xmath48 . for maxent distributions ,",
    "the exponential family @xmath49 is generated by the sufficient statistics @xmath50 s .",
    "the natural parameter of the maxent distribution is given by the lagrangian multipliers  @xcite .",
    "it follows that we have @xmath51=\\eta$ ] for some unique random variable @xmath52 with @xmath53 , see  @xcite .",
    "the function @xmath54 is called the log - normalizer  @xcite since it allows to normalize the density to a probability : @xmath55 . in statistical physics ,",
    "the partition function @xmath56 is rather used to normalize the distributions .",
    "the key observation is to notice that by construction , _ any other distribution _ with density @xmath57 different from the maxent distribution @xmath2 and satisfying all the @xmath58 moment constraints @xmath59=\\eta_i$ ] will have necessarily smaller entropy : @xmath60 with @xmath61 .",
    "however , depending on the choosing sufficient statistics @xmath62 s , neither @xmath63 nor @xmath64 may be available in closed - forms , and thus need to be approximated numerically  @xcite .",
    "in the remainder , we upper bound the differential entropy of a continuous random variable ( eg . , finite mixtures ) by building a _ collection _ of upper bounds derived from maxent distributions which admit closed - form expressions for their differential entropy .",
    "those bounds proves handy in practice for gmms since the differential entropy of a gmm is not available in closed - form  @xcite . besides",
    ", we report closed - form formula for calculating the arbitrary raw absolute moments of a univariate gaussian mixture model that may prove useful in other areas of statistical machine learning and information theory .",
    "consider the univariate uni - order ( @xmath65 ) family of _ absolute monomial exponential family _ ( amef ) induced by the absolute value of a _ monomial _ of degree @xmath66 defined over the full support @xmath67 :    @xmath68    for @xmath69 .",
    "the natural parameter space is @xmath70 ) .",
    "the log - normalizer is : @xmath71 where the gamma function @xmath72 generalizes the factorial ( @xmath73 for @xmath74 ) .",
    "the gamma function can be approximated finely in a few constant operations .",
    "in fact , even better , it is the function @xmath75 that can be calculated quickly ( see the numerical receipe in  @xcite ) , so that the log - normalizer of eq .",
    "[ eq : fl ] can be calculated fast for any @xmath66 and @xmath69 .",
    "note that those amef distributions are _ unimodal _ distributions with the unique mode located at @xmath76 .",
    "since @xmath77 , the mean @xmath78 $ ] of an amef is always zero .",
    "now , the key element is to notice that the differential entropy @xmath79 of an amef admits the following closed - form formula : @xmath80 where @xmath81 is a constant independent of @xmath63 .",
    "the entropy can be expressed _",
    "equivalently _ using the legendre convex conjugate  @xcite @xmath82 as : @xmath83 with @xmath84 and @xmath85 .",
    "therefore the entropy formula expressed using the @xmath86-parameter is : @xmath87 with @xmath88 a constant at prescribed @xmath89 , independent of @xmath86 .",
    "we readily check young - fenchel equality : @xmath90 since @xmath91 , and observe that @xmath92 .",
    "let @xmath93 denote the _ expectation parameter space_. for amefs , the dual natural / expectation parameter spaces are thus @xmath70 and @xmath94 . to avoid confusion ,",
    "let us denote by @xmath95 and by @xmath96 the entropy formula of eq .",
    "[ eq : htheta ] and eq.[eq : heta ] with respect to the natural and expectation parameters , respectively .    to upper bound the entropy of _ any arbitrary univariate continuous random variable",
    "_ @xmath1 ( let it be a mixture or not ) , we simply calculate the @xmath89-th raw absolute geometric moment @xmath97 $ ] , and deduce the following maxent entropy upper bound ( meub ) @xmath98 : @xmath99\\right)\\ ] ]    we thus obtain an infinite countable series of maxent upper bounds ( meubs ) that we summarize in the following theorem :    [ theo1 ] let @xmath1 be a continuous random variable with support @xmath100 .",
    "then the differential entropy @xmath0 of @xmath1 is upper bounded by the following series of maxent upper bounds : @xmath101 , \\quad\\forall l\\in\\bbn,\\ ] ] where @xmath102 .",
    "note that for even integer @xmath89 , @xmath97=e_{x}[x^l]$ ] .",
    "that is , the absolute geometric moments coincide with the geometric moments for even integer @xmath89 .",
    "let us give two well - known maxent distributions that are amef maxent distributions in disguise , with their corresponding differential entropies :    * consider @xmath103 . since @xmath104",
    ", we get @xmath105 .",
    "thus @xmath106 . by setting @xmath107",
    ", we get the usual canonical _ standard gaussian density _ : @xmath108 .",
    "since @xmath107 , we recover the usual entropy of a gaussian : @xmath109 .",
    "it follows that : @xmath110 since @xmath107 , and considering the location - scale family , we get back the usual entropy expression of a gaussian @xmath111 : @xmath109 . *",
    "consider @xmath112 .",
    "the maxent distribution is the _ standard laplacian distribution _",
    "@xcite with density written canonically as @xmath113 with @xmath114 .",
    "the differential entropy can be expressed in _ either _ the natural or expectation coordinate system as @xmath115 or @xmath116 with @xmath117 , respectively .    in general , the differential entropy of an amef distribution of degree @xmath89 is negative when : @xmath118 and non - negative otherwise .",
    "note that when @xmath103 , the amef is the gaussian family , and since @xmath119 , we recover the negative entropy condition @xmath120 ; and conclude that @xmath12 , as already claimed above .    finally , we can extend the amef differential entropy formula to _ location - scale amef distributions _ with @xmath9 a _ location parameter _ and @xmath121 a _",
    "dispersion parameter_. let @xmath122 , and @xmath123 ( where @xmath124 denotes the standard amef distribution ) .",
    "we have @xmath125 and by making a change of variable in the integral of eq .",
    "[ eq : h ] ( see appendix ) , it follows that @xmath126 ( thus always independent of the location parameter ) .",
    "the differential entropy of a location - scale absolute monomial exponential family of degree @xmath89 and location parameter @xmath9 and dispersion parameter @xmath10 is available in closed - form as : @xmath127 where @xmath81 and @xmath88 .",
    "note that scaling an amef amounts to scale its natural parameter since @xmath128 with @xmath129 ( see eq .",
    "[ eq : amefpdf ] ) .",
    "when the support is restricted to @xmath130 instead of @xmath131 , we subtract the @xmath132 from @xmath133 and @xmath134 formula .",
    "for example , this is useful when considering mixtures of rayleigh distributions instead of gaussian distributions .",
    "in order to apply the maxent upper bounds @xmath98 for a gmm with probability density function @xmath135 , we need to compute its absolute raw moment and plug this value into formula eq .",
    "[ eq : ul ] . by linearity of the expectation operator",
    ", we have : @xmath136= \\sum_{i=1}^k w_i e_{p(x;\\mu_i,\\sigma_i)}\\left[|x|^r\\right].\\ ] ] the raw geometric moments and absolute raw geometric moments for a _ centered _ gaussian distribution ( ie .",
    "@xmath137 ) are reported in  @xcite : closed - form formula are reported for the ( absolute ) moments for real - valued @xmath138 using the kummer s confluent hypergeometric functions  @xcite .    for @xmath103 , we can thus recover the well - known _",
    "maxent variance gmm upper bound _",
    "@xcite : @xmath139 with @xmath140 .",
    "surprisingly , we did not find the general formula for the raw absolute moments of a non - centered gaussian .",
    "we carried out the calculations reported in the appendix .",
    "fortunately , the raw absolute moments of a gaussian admit closed - form formula expressed equivalently either using the cumulative distribution function ( cdf ) @xmath141 , the error function @xmath142 or the complementary error function @xmath143 . those basic cdf , erf and",
    "erfc functions are related to each other by the following identities : @xmath144 based on our calculations , we state the series of maxent upper bounds for the differential entropy of a gmm @xmath145 in the following corollary of theorem  [ theo1 ] :    the differential entropy @xmath0 of a gaussian mixture model @xmath146 is upper bounded by : @xmath147 where @xmath102 and @xmath148 where @xmath149 with the terminal recursion cases : @xmath150    in particular , the first two maxent upper bounds ( corresponding to the laplacian and gaussian maxent distributions , respectively ) are given as :    the differential entropy of a gaussian mixture model @xmath151 is upper bounded by : @xmath152    the differential entropy of a gmm @xmath151 is upper bounded by : @xmath153 with @xmath154 .",
    "thus we can bound the differential entropy by @xmath155 , and for our series of upper bounds by : @xmath156    since the differential entropy does not change by changing the location parameter , we may consider without loss of generality that the gmm is centered to zero ( that is , its expectation @xmath157 $ ] is zero ) .",
    "if not , we simply translate the gmm by setting the component means to @xmath158 so that the expectation of the gmm matches the expectation of the amef .",
    "this alignment of the gmm to the amef preserves the maxent upper bounds .    in general , we may shift the gmm @xmath1 by @xmath159 by setting @xmath160 .",
    "let @xmath161 denotes this shifted gmm , @xmath162 $ ] and @xmath163 .",
    "we can further refine the meubs by minimizing the maxent upper bounds :    @xmath164 where @xmath102 .",
    "when @xmath103 , the optimal shift is obtained for @xmath165 .",
    "however , when @xmath112 , the optimization problem is non - trivial and requires numerical optimization procedures .",
    "( in the remainder , we consider @xmath165 when carrying experiments . )",
    "first , we consider the following experiment repeated @xmath166 times : we draw of a gmm @xmath167 with two components with @xmath168 and @xmath169 chosen as uniform weights renormalized to @xmath170 , we recenter the gmm so that @xmath171 ( setting @xmath165 ) , and compute the stochastic approximation @xmath172 of @xmath0 ( for @xmath173 samples ) , and the first order and second order maximum entropy upper bounds @xmath174)$ ] and @xmath175)$ ] .",
    "we report average approximations @xmath176 , and the percentage of times meub @xmath177 : gaussian meub is on average @xmath178 above @xmath172 and laplacian meub is on average @xmath179 above @xmath172 .",
    "laplacian meub bound beats the gaussian meub @xmath180 on average .",
    "thus we recommend practitioners to upper bound the differential entropy of a gmm @xmath1 by @xmath181 .",
    "a question one may ponder is whether all meubs @xmath182 are useful of not",
    "? we performed an experiment by drawing at random gmms @xmath1 with @xmath183 components and checking among the first @xmath184 bounds @xmath185 .",
    "we found experimentally that most of the time the bounds @xmath186 and @xmath187 suffices , but sometimes the tightest bound could be @xmath188 .",
    "this is the case when one component is almost a dirac ( @xmath189 ) while the other component has significant standard deviation , and the two gaussian components far apart .",
    "for example , let us choose @xmath190",
    ". then @xmath191 for @xmath192 .",
    "we get nan numerical errors when computing @xmath193 using the closed - form formula .",
    "we shall make more precise those arguments in the following section .      first , let us show that bound @xmath194 ( the laplacian meub ) may be better than @xmath195 ( the gaussian meub ) . to derive analytic conditions",
    ", we consider the _ restricted case _ of zero - centered gmms  @xcite .",
    "we have @xmath196 , and therefore get the upper bound @xmath194 on the differential entropy of the mixture as :    @xmath197    this bound is strictly better than the traditional gaussian bound  @xcite : @xmath198 provided that @xmath199 .",
    "note that when @xmath103 and @xmath200 , the @xmath195 bound matches precisely the entropy of the single - component gaussian mixture .",
    "let @xmath201 be the _ arithmetic weighted mean _ and @xmath202 be the _",
    "quadratic mean _ of the weighted standard deviations , respectively .",
    "then @xmath203 if and only if : @xmath204 that is , we need to have @xmath205 .",
    "observe that the _ weighted quadratic mean _ dominates the quasi - arithmetic weighted mean by @xmath206 , see  @xcite . ]",
    "the _ weighted arithmetic mean _ , and therefore @xmath207 .",
    "equality of arithmetic / quadratic means only happens when all the @xmath208 s coincide ( since we have zero - centered gmms , that means that the gmm collapses to a gaussian ) . to summarize our illustrating example , bound @xmath194 may be better or worse than @xmath195 depending on the set of @xmath208 s .",
    "for the degenerate case @xmath200 ( single component @xmath209 ) , the condition of @xmath210 ( @xmath194 tighter than @xmath195 ) writes as @xmath211 ( that is , @xmath212 ) .",
    "now , for @xmath66 , we built a maxent upper bound on the differential entropy of a gmm @xmath213 .",
    "how does this infinite sequence of bounds @xmath214 relate to each others ? for a prescribed value @xmath215 , @xmath216 decreases as @xmath89 increases , but the absolute raw moment @xmath217 also varies .",
    "does there always exist a gmm @xmath1 so that there exists @xmath218 such that @xmath219 , or not ?    for general gmms , we explained in the former section that by taking a gmm with two components with one component almost a dirac , we could establish experimentally that all bounds could yield the tightest one . here , to answer negatively this question when considering the family of zero - centered gaussian mixtures , we shall consider _ even _",
    "integers @xmath89 and @xmath220 .",
    "let @xmath221 .",
    "then the geometric raw moments coincide with the central geometric moments , and by the linearity of the expectation operator , we have  @xcite : @xmath222= \\underbrace{2^{\\frac{l}{2}}\\frac{\\gamma(\\frac{1+l}{2})}{\\sqrt{\\pi}}}_{z_l }   \\left(\\sum_{i=1}^k w_i \\sigma_i^l \\right ) = a_l(x ) .",
    "\\label{eq : gmm0}\\ ] ]    then we have the following maxent upper bound @xmath98 :    the differential entropy of a zero - centered gmm @xmath223 is upper bounded by : @xmath224 where @xmath225 is the _",
    "@xmath89-th power mean _ : @xmath226    when @xmath227 , we have @xmath228 .",
    "thus for a tighter bound @xmath229 , we need to find a zero - centered gmm so that :    @xmath230    since the @xmath231-power mean dominates dominates another mean @xmath232 ( that is , @xmath233 ) when @xmath234 . ]",
    "the @xmath89-power mean ( ie . ,",
    "@xmath235 ) , we have @xmath236 and therefore @xmath237 .",
    "it turns out that @xmath238 when @xmath239 with @xmath240 .",
    "so we conclude that only @xmath194 and @xmath195 are necessary for zero - centered gmms ( and we define the meub as @xmath241 ) .    when considering arbitrary gmms , the situation is analytically more complex to decide .",
    "last but not least , whether bound @xmath98 proves useful or not depends on the mixture family ( eg .",
    ", mixtures of pareto distributions  @xcite ) .",
    "a pareto distribution has density @xmath242 for @xmath243 and @xmath244 the shape parameter .",
    "the raw moments of a pareto distribution is @xmath245 for @xmath246 and @xmath247 otherwise .",
    "we considered the novel parametric family of absolute monomial exponential families ( amefs ) , and reported a closed - form differential entropy formula for these amefs .",
    "we then considered a collection of maximum entropy upper bounds ( meubs ) for an arbitrary continuous random variable based on its raw geometric absolute moments ( theorem  [ theo1 ] ) , and show how to apply those generic bounds to the specific case of gaussian mixture models ( gmms ) .",
    "interestingly , we showed that the laplacian maxent upper bound may potentially be tighter than the traditionally used gaussian maxent upper bound .",
    "therefore , we recommend in practice to take the minimum of these laplacian and gaussian meubs . this new series of maxent upper bounds proves useful in practice since the differential entropy of mixtures does not admit a closed - form formula  @xcite . besides , we report in the appendix closed - form formula for calculating the raw absolute moments of a univariate gaussian mixture model .",
    "the method can be extended to any location - scale univariate continuous distribution .",
    "a java source code for reproducible research with test experiments is available at : +    https://www.lix.polytechnique.fr/~nielsen/meub/",
    "let @xmath248 denote the density of a _ location - scale distribution _ on the full support @xmath131 , where @xmath249 denotes the _ location parameter _ and",
    "@xmath10 the _ dispersion parameter_. for example , a normal distribution has location parameter its mean and dispersion parameter its standard deviation .",
    "let us prove that the entropy @xmath0 is @xmath250 with @xmath251 and @xmath252 , a quantity always independent of the location parameter @xmath9 .",
    "we shall make use of a change of variable @xmath253 ( with @xmath254 ) in the integral to get : @xmath255",
    "let @xmath256 be a normal random variable of mean @xmath249 and standard deviation @xmath10 .",
    "let us express the density of the normal distribution as a location - scale density : @xmath248 , where @xmath257 denotes the density of the standard normal distribution @xmath258 : @xmath259    define the raw ( uncentered ) @xmath89-th absolute moment @xmath260 $ ] for a continuous univariate location - scale family with standard density @xmath261 : @xmath262 = \\int_{-\\infty}^{+\\infty } |x|^l \\frac{1}{\\sigma } p_0\\left(\\frac{x-\\mu}{\\sigma}\\right ) \\dx .\\ ] ]    we first consider the calculation of @xmath263=e[x^l]$ ] for even integer @xmath89 , and then proceed with the computation of @xmath260 $ ] for odd @xmath89 .",
    "the raw absolute geometric moment amounts to the raw geometric moment for even integer @xmath89 : @xmath264=e[x^l]$ ] .",
    "it follows after a change of variable @xmath253 ( so that @xmath265 ) with @xmath266 ( and @xmath267 ) that : @xmath268    performing the _ binomial expansion _",
    "@xmath269 , we get : @xmath270 where @xmath271 is the @xmath272-th raw moment of the standard normal distribution @xmath258 : @xmath273=\\left\\ { \\begin{array}{ll } ( l-1)!!\\sigma^l & \\mbox{even $ l$},\\\\ \\sqrt{\\frac{2}{\\pi } } 2^{\\frac{l-1}{2 } } \\left(\\frac{l-1}{2}\\right ) ! \\sigma^l & \\mbox{odd $ l$}. \\end{array } \\right.,\\ ] ] with @xmath274 the double factorial : @xmath275 .",
    "we end - up with the following raw moment _ direct formula _ for an even integer @xmath89 :    @xmath276    in particular , we recover the second ( absolute ) moment : @xmath277 = \\mu^2+\\sigma^2.\\ ] ]      we get rid of the cumbersome absolute value by splitting the integral onto the positive and negative support as follows :    @xmath278    consider the change of variable @xmath253 .",
    "we get :    @xmath279    by performing binomial expansion and sliding the integral inside the binomial sum , we get : @xmath280 with @xmath281    by a change of variable @xmath282 ( with @xmath283 ) , we find that :    @xmath284    thus it is enough to consider the computation of @xmath285 for all non - negative integers @xmath286 , and get the raw absolute moment as : @xmath287    consider the _ integration by parts__a^b-\\int_a^b u'(x)v(x)\\dx$ ] . ] for calculating integral : @xmath288 with @xmath289 ( and antiderivative @xmath290 , where @xmath291 is a constant ) and @xmath292 ( and derivative @xmath293 ) :    @xmath294    thus we end up with the following recursive formula : @xmath295    with the terminal recursion cases :    @xmath296    equivalent expressions may be obtained using the error function or the complementary error function by using the following identities : @xmath144      we checked experimentally the numerical robustness of those formula by comparing the exact moment formula with the monte - carlo estimated ones . in general , the moment @xmath297=\\int g(x)p(x;\\mu,\\sigma)\\dx$ ] can be approximated stochastically using monte - carlo sampling as : @xmath298 with @xmath299 .",
    "mc estimators are consistent ( ie .",
    ", tend to the true value when @xmath300 ) .",
    "we can also discretize the moment integral using various quadratic rules to approximate the moment values .",
    "the table below reports the first ten ( 10 ) raw absolute moments @xmath260 $ ] of a gaussian random variable @xmath301 :    [ cols=\"<,<,<,<\",options=\"header \" , ]      for univariate mixtures of natural efs with a polynomial sufficient statistic @xmath302 , we may easily calculate moments using the moment generating function ( mgf )  @xcite :    @xmath303=\\exp(f(\\theta+u)-f(\\theta)).\\ ] ]    thus for uni - order efs , the geometric moments are given by the higher - order derivatives @xmath304=m^{(l)}(0)$ ] . for uni - order exponential families , it follows that @xmath305=f'(\\theta)=\\eta$ ] , and @xmath306=f''(\\theta)>0 $ ] ( since @xmath133 is strictly convex ) .",
    "it follows that efs have always all finite order moments expressed using the higher - order derivatives of the mgf .",
    "thus we can always explicitly calculate the geometric moments of the sufficient statistic @xmath302 from the mgf provided that the log - normalizer @xmath64 is available in closed - form .",
    "for example , we may consider mixtures of rayleigh distributions ( with @xmath307 , see  @xcite ) instead of gmms , and get closed - form maxent upper bounds .",
    "the geometric raw moments of a rayleigh mixture @xmath1 is @xmath308 .",
    "the examples below show how definite integration is performed using the computer algebra system ( cas ) maxima :    .... / * example of a density that has infinite   shannon entropy * / p(x ) : = log(2)/(x*log(x)**2 ) ; / * check it integrates to 1 * / integrate(p(x),x,2,inf ) ; / * check integral diverges * / integrate(-p(x)*log(p(x)),x,2,inf ) ; ....                                                                          lidija trailovic and lucy  y pao . variance estimation and ranking of gaussian mixture distributions in target tracking applications . in _",
    "decision and control , 2002 , proceedings of the 41st ieee conference on _ , volume  2 , pages 21952201 .",
    "ieee , 2002 ."
  ],
  "abstract_text": [
    "<S> we present a series of closed - form maximum entropy upper bounds for the differential entropy of a continuous univariate random variable and study the properties of that series . </S>",
    "<S> we then show how to use those generic bounds for upper bounding the differential entropy of gaussian mixture models . </S>",
    "<S> this requires to calculate the raw moments and raw absolute moments of gaussian mixtures in closed - form that may also be handy in statistical machine learning and information theory . </S>",
    "<S> we report on our experiments and discuss on the tightness of those bounds .    </S>",
    "<S> * keywords * : differential entropy , gaussian mixture models , maximum entropy , absolute monomial exponential families , absolute moments .    </S>",
    "<S> # 1 # 1 # 1 # 1#2#1,#2 </S>"
  ]
}