{
  "article_text": [
    "robustly recovering the intrinsic low - dimensional structure of high - dimensional visual data , which is known as robust principal component analysis ( rpca ) , plays a fundamental role in various computer vision tasks , such as face image alignment and processing , video denoising , structure from motion , background modeling , photometric stereo and texture representation ( see e.g. , @xcite , @xcite , @xcite , @xcite , @xcite , and @xcite , to name just a few ) . through the years",
    ", a large number of approaches have been proposed for solving this problem .",
    "the representative works include @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , and @xcite .",
    "the main limitation of above mentioned methods is that there is no theoretical guarantee for their performance .",
    "recently , the advances in compressive sensing have led to increasingly interests in considering rpca as a problem of exactly recovering a low - rank matrix @xmath6 from corrupted observations @xmath7 , where @xmath8 is known to be sparse ( @xcite , @xcite ) . its mathematical model is as follows : @xmath9 where @xmath10 is the @xmath11 norm of a matrix , i.e. , the number of nonzero entries in the matrix .    unfortunately , problem ( [ eq : rpca ] ) is known to be np - hard .",
    "so @xcite proposed using principal component pursuit ( pcp ) to solve ( [ eq : rpca ] ) , which is to replace the rank function and @xmath11 norm with the nuclear norm ( which is the sum of the singular values of a matrix , denoted as @xmath12 ) and @xmath0 norm ( which is the sum of the absolute values of the entries ) , respectively . more specifically , pcp is to solve the following convex problem instead : @xmath13 they also _ rigorously proved _ that under fairly general conditions and @xmath14 , pcp can _ exactly recover _ the low - rank matrix @xmath6 ( namely the underlying low - dimensional structure ) with an overwhelming probability , i.e. , the difference of the probability from 1 decays exponentially when the matrix size increases .",
    "this theoretical analysis makes pcp distinct from previous methods for rpca .",
    "all the existing algorithms for rpca need to compute either svd or matrix - matrix multiplications on the whole data matrix .",
    "so their computation complexities are all at least quadratic w.r.t .",
    "the data size , preventing the applications of rpca to large - scale problems when the time is critical . in this paper",
    ", we address the large - scale rpca problem and propose a truly linear cost method to solve the pcp model ( [ eq : pcp ] ) when the data size is very large while the target rank is relatively small .",
    "such kind of data is ubiquitous in computer vision .",
    "our algorithm fully utilizes the properties of low - rankness .",
    "the main idea is to apply pcp to a randomly selected submatrix of the original noisy matrix and compute a low rank submatrix . using this low rank submatrix ,",
    "the true low rank matrix can be estimated efficiently , where the low rank submatrix is part of it .    specifically , our method consists of two steps ( illustrated in figure  [ fig : lena ] ) .",
    "the first step is to recover a submatrix . ]",
    "@xmath15 ( figure  [ fig : lena ] ( e ) ) of @xmath6 .",
    "we call this submatrix the seed matrix because all other entries of @xmath6 can be further calculated by this submatrix .",
    "the second step is to use the seed matrix to recover two submatrices @xmath16 and @xmath17 ( figures  [ fig : lena ] ( f)-(g ) ) , which are on the same rows and columns as @xmath15 in @xmath6 , respectively .",
    "they are recovered by minimizing the @xmath0 distance from the subspaces spanned by the columns and rows of @xmath15 , respectively .",
    "hence we call this step @xmath0 filtering . the remaining part @xmath18 ( figure  [ fig : lena ] ( h ) ) of @xmath6",
    "can be represented by @xmath15 , @xmath16 and @xmath17 , using the generalized nystrm method ( @xcite ) . as analyzed in section",
    "[ sec : comp ] , our method is of linear cost with respect to the data size . besides the advantage of linear time cost , the proposed algorithm is also highly parallel : the columns of @xmath16 and the rows of @xmath17 can be recovered fully independently .",
    "we also prove that under suitable conditions , our method can _ exactly _ recover the underling low - rank matrix @xmath6 with an overwhelming probability . to our best knowledge , this is the first algorithm that can _ exactly _ solve a nuclear norm minimization problem in _",
    "linear time_.    [ cols=\"^,^,^,^ \" , ]",
    "in this paper , we propose the first _ linear time _ algorithm , named the @xmath0 filtering method , for _ exactly _ solving very large pcp problems , whose ranks are supposed to be very small compared to the data size .",
    "it first recovers a seed matrix and then uses the seed matrix to filter some rows and columns of the data matrix .",
    "it avoids svd on the original data matrix , and the @xmath0 filtering step can be done in full parallelism . as a result ,",
    "the time cost of our @xmath0 filtering method is only linear with respect to the data size , making applications of rpca to extremely large scale problems possible .",
    "the experiments on both synthetic and real world data demonstrate the high accuracy and efficiency of our method .",
    "it is possible that the proposed technique can be applied to other large scale nuclear norm minimization problems , e.g. , matrix completion ( @xcite ) and low - rank representation ( @xcite ) .",
    "this will be our future work .",
    "the authors would like to thank prof .",
    "zaiwen wen and dr .",
    "yadong mu for sharing us their codes for lmafit ( @xcite ) and random projection ( @xcite ) , respectively .",
    "this work is partially supported by the grants of the national nature science foundation of china - guangdong joint fund ( no .",
    "u0935004 ) , the national nature science foundation of china fund ( no . 60873181 , 61173103 ) and the fundamental research funds for the central universities .",
    "the first author would also like to thank the support from china scholarship council .",
    "ganesh a , lin z , wright j , wu l , chen m , ma y ( 2009 ) fast algorithms for recovering a corrupted low - rank matrix . in : proceedings of international workshop on computational advances in multi - sensor adaptive processing"
  ],
  "abstract_text": [
    "<S> in the past decades , exactly recovering the intrinsic data structure from corrupted observations , which is known as robust principal component analysis ( rpca ) , has attracted tremendous interests and found many applications in computer vision . recently , this problem has been formulated as recovering a low - rank component and a sparse component from the observed data matrix . </S>",
    "<S> it is proved that under some suitable conditions , this problem can be exactly solved by principal component pursuit ( pcp ) , i.e. , minimizing a combination of nuclear norm and @xmath0 norm . </S>",
    "<S> most of the existing methods for solving pcp require singular value decompositions ( svd ) of the data matrix , resulting in a high computational complexity , hence preventing the applications of rpca to very large scale computer vision problems . in this paper </S>",
    "<S> , we propose a novel algorithm , called @xmath0 filtering , for _ exactly _ solving pcp with an @xmath1 complexity , where @xmath2 is the size of data matrix and @xmath3 is the rank of the matrix to recover , which is supposed to be much smaller than @xmath4 and @xmath5 . </S>",
    "<S> moreover , @xmath0 filtering is _ </S>",
    "<S> highly parallelizable_. it is the first algorithm that can _ exactly _ solve a nuclear norm minimization problem in _ linear time _ </S>",
    "<S> ( with respect to the data size ) . </S>",
    "<S> experiments on both synthetic data and real applications testify to the great advantage of @xmath0 filtering in speed over state - of - the - art algorithms . </S>"
  ]
}