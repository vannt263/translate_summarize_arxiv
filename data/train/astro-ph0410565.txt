{
  "article_text": [
    "a new breed of techniques for the diagnostics of solar magnetic fields has been emerging during the last few years .",
    "the fundamental idea that inspired them is the recognition that the observable polarization ( stokes ) line profiles exhibit conspicuous _",
    "patterns _ of features associated with the underlying physical conditions in the atmosphere where they form .",
    "this has recently motivated a considerable interest in the field of pattern recognition .",
    "potential applications to solar magnetic field diagnostics have been explored in a number of recent papers by @xcite ( @xcite ) ; @xcite ( @xcite ) ; @xcite ( @xcite ) ; @xcite ( @xcite ) ; @xcite ( @xcite ) ; @xcite ( @xcite ) ; @xcite ( @xcite ) ; @xcite ( @xcite ) .",
    "pattern recognition techniques were originally developed in the fields of artificial intelligence and machine vision , but have found a vast number of applications over a broad spectrum of disciplines ( see , e.g. , the review by @xcite @xcite ) .",
    "solar physicists have relied on least - squares fitting procedures to invert the observed spectra since the late 70s , although this only became a generalized practice in the early 90s ( for more details see the reviews by @xcite @xcite ; @xcite @xcite ) .",
    "the least - squares approach is still as useful as ever and is not likely to be replaced by the newer pattern recognition in the near future .",
    "the reason is simply that one can obtain much more detailed information from a given set of profiles by using a least - squares inversion .",
    "it also offers more flexibility in the physical model used for the inversion .",
    "research on new inversion techniques is intended to develop alternative algorithms that will _ complement _ , and not _ replace _ previous least - square procedures .",
    "nevertheless , it is justified to devote significant efforts towards the development of alternative inversion algorithms . without going into much detail here ( see ,",
    "e.g. , @xcite @xcite for a more thorough discussion ) , it should be noted that the next generation spectro - polarimeters ( both ground - based and space - borne ) will deliver enormous dataflows that simply can not be analyzed with traditional inversion codes .",
    "the need for larger fields of view , higher spatial resolution and better time cadence is driving new instrumental developments .",
    "these , in turn , demand new analysis techniques that are fast , ideally operating in real - time , and robust , in the sense of not requiring any human supervision .",
    "other important considerations that motivate research on pattern recognition are related to the need for establishing long - term databases of observations ( properly inverted ) , or the desire to obtain real - time maps of the solar magnetic field at the observing site .    as an intermediate step in this process ,",
    "some authors have sought a suitable set of features in the stokes profiles of spectral lines and an appropriate algorithm to extract such features .",
    "feature extraction is of great potential interest , e.g. for data compression ( which would help circumvent limited spacecraft telemetry ) , dimensionality reduction , profile classification , inversion pre - processing , etc .",
    "the most promising approaches developed so far seem to be the use of principal component analysis ( pca ) , introduced by @xcite ( @xcite ) , and the expansion in hermitian functions ( ehf ) of @xcite ( @xcite ) .",
    "in this paper i present a new method which makes use of artificial neural networks ( anns ) , and compare it to pca and ehf in terms of performance and potential usefulness .",
    "the organization of the present paper is as follows .",
    "section  [ sec : expansions ] below reviews the pca and ehf methods and discusses some important ideas that are relevant for our purposes here .",
    "section  [ sec : ann ] gives a detailed introduction to anns and puts forward the concept of auto - associative anns , which is the base for the feature extraction algorithm .",
    "the comparison among the three methods is presented in  [ sec : performance ] .",
    "finally , the most important conclusions are discussed in  [ sec : conc ] .",
    "mathematically , the problem considered in this paper can be summarized as follows .",
    "given a set of one or more stokes profiles @xmath0 ( where @xmath1 denotes the stokes parameter and @xmath2 are the @xmath3 wavelengths of the spectral samples ) , we wish to extract a suitable set of @xmath4 parameters @xmath5 ( features ) that contain as much information from the profiles as possible : @xmath6 with @xmath7 being a @xmath4-dimensional function that characterizes the feature - extraction technique .",
    "we can then reconstruct the original profile approximately by doing the inverse transformation : @xmath8    three different strategies are explored in the next sections .",
    "the first two , pca and ehf , are series expansions in terms of orthonormal basis functions .",
    "the third method , aann , is somewhat more complex and allows for non - linear relations between the features and the profiles .",
    "pca has been used in a broad variety of fields ( e.g. , @xcite @xcite ) and was recently introduced in the radiative transfer literature by @xcite ( @xcite ) . at its core , pca is simply a series expansion in terms of some orthonormal basis functions . if @xmath9 is a discretized spectral profile , we have :    @xmath10    where @xmath11 is the number of wavelength samples , @xmath12 are the basis functions ( sometimes referred to as eigenprofiles or eigenvectors ) , and @xmath13 are the coefficients of the expansion . these coefficients are sometimes referred to as features or eigenfeatures .    the basic premise that makes pca so special is that the basis is cleverly chosen for each particular problem that one is dealing with .",
    "instead of using a predefined set of basis functions , pca seeks an _ optimum _ ( in the sense defined below ) set of eigenprofiles @xmath14 .",
    "the first obvious condition that the @xmath14 must meet in order to form a basis of the profile space is orthonormality :    @xmath15    where @xmath16 is the kronecker delta .",
    "this condition implies that the expansion coefficients @xmath13 can be determined simply by the following scalar product :    @xmath17    equation  ( [ eq : pca ] ) is exact because the sum extends to as many as @xmath11 expansion terms .",
    "however , the whole point of feature extraction is precisely to reduce the dimensionality of the problem .",
    "this goal is achieved by truncating the expansion at a given order @xmath4 ( @xmath18 ) , retaining only those terms that are deemed relevant .",
    "consider a statistical sample of @xmath19 profiles , denoted by @xmath20 ( with @xmath21 ) .",
    "the individual components of a vector @xmath20 are the observable spectral samples ( @xmath22 ) .",
    "let us approximate these profiles by :    @xmath23    where @xmath24 is a constant vector .",
    "when the expansion is truncated at order @xmath4 , the total quadratic error accumulated over the entire sample is :    @xmath25    notice that , so far , we have not particularized to pca or given an explicit form for the @xmath26 . the discussion above applies to any expansion in basis functions .",
    "the distinctive feature of pca is that it provides a basis that minimizes the truncation error for any order @xmath4 .",
    "minimization of @xmath27 with respect to @xmath28 straighforwardly leads to :    @xmath29    the truncation error @xmath27 can also be minimized with respect to the basis functions @xmath26 .",
    "one then obtains :    @xmath30    where @xmath31 is the covariance matrix , defined as :    @xmath32    in eq  ( [ eq : covmatrix ] ) above , @xmath33 is the average profile over the statistical sample and @xmath34 denotes the usual matrix transposition operation .",
    "equation  ( [ eq : eigen ] ) shows that the pca basis functions are simply the eigenvectors of the covariance matrix .",
    "hence the denominations of eigenfunctions or eigenprofiles that they sometimes receive .",
    "the pca expansion procedure can be summarized as follows .",
    "first , one picks a suitable database of profiles that are deemed representative of the physical problem being tackled .",
    "this database should ideally span a broad range of profiles , covering all the possible realizations that we might expect to find in the observations .",
    "the next step is to construct the covariance matrix @xmath31 by using eq  ( [ eq : covmatrix ] ) , and solve the corresponding eigenvalue problem ( eq  [ [ eq : eigen ] ] ) .",
    "the eigenvectors of @xmath31 are a basis of orthonormal profiles @xmath26 ( @xmath35 ) with some interesting properties .",
    "in particular , it provides the _ optimum _ linear coordinate transformation for our problem ( assuming that the original database is a good representation ) , in the sense that the information preserved upon truncation at any order @xmath4 is maximum .",
    "the hermite functions constitute an orthonormal basis of @xmath36 , the space of integrable functions .",
    "they can be calculated as the product of a gaussian by the hermite polynomials ( with some normalization constants ) :    @xmath37    the hermite polynomials that appear in eq  ( [ eq : herm1 ] ) are given by the following recursive relations :    @xmath38    the @xmath39 defined by in eqs  ( [ eq : herm1 ] ) and  ( [ eq : herm2 ] ) are orthonormal .",
    "as with pca above , any given profile may be expanded as linear combination of this basis :    @xmath40    obviously , the optimal expansion is attained when @xmath41 is the wavelength relative to line center , and normalized to the width of the line .",
    "before going into details on the particular physical problem of spectral line analysis , a general introduction to anns is probably in order .",
    "an ann is an abstract structure that mimics to some extent the functioning of a biological neural system .",
    "consider a number of memory cells ( neurons ) , each one having the ability to store a datum .",
    "the neurons are inter - connected ( _ synaptic connections _ ) in such a way that the number they store can be modified according to the values stored in other neighboring neurons that is is connected to ( see fig  [ fig : ann ] ) .",
    "ann investigations usually employ some simplifying restrictions on the structure of neurons and synaptic connections .",
    "one of them is to consider _ forward - propagating _ networks , in which a signal propagation direction is defined .",
    "the synaptic connections seen by a neuron can be inward- or outward - directed , depending on whether it is used to receive or send a signal from or to a neighbor .",
    "some neurons are taken as inputs in which the signal originates and have no inward connections .",
    "the neurons connected to the inputs are then modified according to the values in the input neurons .",
    "such modifications in turn trigger a new set of neurons to alter their values , and the process continues until the output neurons ( which have no further outwards connections ) are reached .",
    "forward - propagating networks are not allowed to have loopback connections .    in order to fix ideas",
    "let us consider a special type of forward - propagating ann , namely the _ multi - layer perceptron _",
    "this is perhaps the most widely used ann geometry .",
    "a mlp is arranged in successive layers , as sketched in fig  [ fig : ann ] .",
    "every neuron in a given layer is connected with all neurons in the previous layer .",
    "no conexions are allowed between neurons in the same layer , or between layers that are not successive .",
    "let us discuss in some more detail the actual process of signal propagation .",
    "we start with a set of inputs @xmath42 ( with @xmath43 ) . denoting by @xmath44 the datum stored in neuron @xmath45 belonging to layer @xmath46 , and defining @xmath47 to be the input layer , we have that @xmath48    the basic equation for signal propagation in a mlp is :    @xmath49    with @xmath50 and @xmath51 .",
    "the @xmath52 and @xmath53 are the synaptic weights and biases , respectively .",
    "they are parameters of the network that define its behavior .",
    "the function @xmath54 is the so - called _ activation function_. a mlp is said to be linear when @xmath55 . in this case",
    "the value in a given neuron is simply a linear combination of the data in its neighbors . for many interesting applications ,",
    "however , non - linear activation functions are chosen .",
    "one of the most widely employed functions is the hyperbolic tangent , which shall be used in this paper as well .    from a mathematical point of view",
    ", a forward - propagating ann may be viewed as a multi - dimensional non - linear function @xmath56 that transforms a vector of inputs @xmath42 ( with @xmath43 ) onto a vector of outputs @xmath57 ( with @xmath58 ) .",
    "the input and output spaces are not necessarily of the same dimension ( @xmath59 ) . formally :    @xmath60    the idea is to use @xmath56 as an approximation to the physical model under consideration . what is distinctive about anns is their ability to `` learn '' .",
    "this means that the actual form of @xmath56 is altered during a _ training process _ , making it a better approximation .",
    "the usual procedure to train a mlp is by generating a set of training data , consisting of known inputs @xmath61 and outputs ( sometimes referred to as targets ) @xmath62 .",
    "the training inputs are propagated through the network , producing some outputs @xmath63 , which in general differ from the targets by an error @xmath64 .",
    "one then modifies the network parameters @xmath65 and @xmath66 to minimize the sum of quadratic errors .",
    "several algorithms exist in the literature to perform this task .",
    "perhaps the most widely used is the _ back - propagation _ algorithm ( @xcite @xcite ) , which has been employed for the calculations in the present paper . computer routines in fortran  90 to create and train mlps are available from the author upon request .",
    "dimensionality reduction ( and feature extraction ) can be achieved by using _",
    "auto - associative _ anns ( hereafter aanns ) .",
    "an aann is trained with targets that are equal to the inputs of the training set ( @xmath67 ) .",
    "this would not be very useful without another distinctive feature of aanns , namely a reduced number of neurons in one ( or more ) of the hidden layers ( the `` bottleneck '' layer ) . the resulting structure for a simple aann is shown in fig  [ fig : aann ] .",
    "when the aann in fig  [ fig : aann ] is trained with @xmath68 , it needs to find a way to compress the input data into a smaller subset of numbers ( in the particular example of the figure , going from 5 to 3 ) from which the input profile can be later reconstructed .",
    "when the aann has only one hidden layer ( as the one in the figure ) and the activation function is linear , it can be shown that the aann training has one global solution ( @xcite @xcite ; @xcite @xcite ) .",
    "this solution is precisely the same that one wold obtain with a pca decomposition .",
    "in other words , a properly trained linear aann with one hidden layer is able to extract pca coefficients from an input profile and then reconstruct the original profile from these coefficients .",
    "using aanns to do pca decomposition is not very useful .",
    "there are simpler methods that are guaranteed to work and do not require a cumbersome , time - consuming training process .",
    "however , there may be instances in which it is convenient to use multi - layer non - linear aanns , which are effectively some form of non - linear generalization of pca .",
    "extreme examples are the academic problems represented in fig  [ fig : circle ] .    in the @xmath69 coordinate system ,",
    "one needs two values to entirely determine the position of the points in the figure . a pca analysis of the dataset would result in another orthogonal reference system @xmath70 , which is simply a rotation of @xmath69 thus requiring also two numbers for each datapoint",
    "obviously , the optimal coordinate choice for these problems would be a system that has one coordinate @xmath71 along the curve and another @xmath72 perpendicular to it . in this new sytem ,",
    "specifying @xmath71 alone practially determines the location of the datapoints .",
    "we can say in this case that the `` intrinsic dimensionality '' of the problem is @xmath73 , which is smaller than that of the `` measurement '' space @xmath74 .",
    "a similar reasoning may be applied to the analysis of spectral stokes profiles .",
    "typical observations of a line profile consist of @xmath75100 spectral samples times 4  stokes parameters . however , the intrinsic dimensionality of the physical problem is usually much smaller .",
    "for instance , a simple milne - eddington model is specified by @xmath7610  atmospheric parameters .",
    "the mapping from @xmath69 to @xmath77 in the examples above is non - linear and therefore can not be picked up by the pca decomposition .",
    "however , a properly trained aann may be able to perform this mapping .",
    "this is an interesting property with potential interest for spectral line diagnostics .",
    "in this section i present some results from application of the three methods ( pca , ehf and aann ) to the analysis of simulated solar profiles .",
    "a word of caution is in order , since these results depend on the particular conditions of the problem under study , such as the set of profiles used for the simulation , the model atmosphere considered , etc .",
    "therefore , the performances reported here are to be taken as merely orientative .    for a sake of computational simplicity ,",
    "the simulations have been carried out with milne - eddington models that have fixed thermodynamical parameters , corresponding to typical solar conditions .",
    "the magnetic field vector varies randomly , with the field strength ranging between 0 and 4000  g , and both the inclination and azimuth between 0 and 180 degrees .",
    "the spectral lines simulated are the well - known pair of lines at 6302   .",
    "random noise at the level of @xmath78 was added to all profiles .",
    "several sets of 5000 profiles were computed from random models .",
    "one of such sets was used to calculate the pca @xmath79 eigenprofiles ( see the discussion preceding eq  [ [ eq : eigen ] ] ) .",
    "the rest were used to train an aann with a total of 4 non - linear layers .",
    "the spectral ranges corresponding to the two telluric lines in this region have been removed from the input profiles , but the network is still required to provide the full profile on output .",
    "the `` bottleneck '' layer of the aann contains 10 neurons , whereas the input and output layers have 320 each .",
    "the reason for chosing 10 neurons in the bottleneck layer is that this is the intrinsic dimensionality of the profiles ( given here by the number of free parameters in the milne - eddington model atmosphere ) .",
    "after a given training batch has been processed the ann is presented with a set of 500 test profiles ( not included in the training ) to measure its performance .",
    "the training process was stopped when two successive batches yield no significant improvement in this test .",
    "the total cpu time for the training process was over 30 hours running on a 2.4  ghz intel pentium  4 processor .",
    "fig  [ fig : profs ] shows the final reconstruction accuracy of the aann for a sample set of stokes profiles .",
    "a reference sample of 500 spectra was analyzed and reconstructed using pca , the ehf and the aann .",
    "for each spectrum , the difference between the simulated and the reconstructed profiles is quantified as : @xmath80 where @xmath81 is the number of spectral samples multiplied by the number of stokes parameters considered , @xmath82 is the reference profile , @xmath83 is the reconstruction and @xmath84 is the measurement noise . using this definition ,",
    "the error in the reconstruction shown in fig  [ fig : profs ] is @xmath85 .",
    "figure  [ fig : chi2 ] shows the @xmath86 , averaged over the reference sample , as a function of the coefficients used in the pca and ehf . in the case of the aann the number of parameters is fixed and determined by the structure of the network .",
    "the vertical line marks the number of neurons in the bottleneck layer .",
    "not surprisingly , the ehf exhibits the lowest accuracy .",
    "however , this method still has merit as it is the only one that has analytical basis functions .",
    "moreover , it does not require any `` a priori '' computations , unlike the aann ( which needs training ) or pca ( which needs a database ) .",
    "the aann works well when high accuracy is not required and the model atmospheres are relatively simple ( see the reconstruction in fig  [ fig : profs ] ) .",
    "unfortunately , the computational expense increases rapidly for more complex models or higher accuracy demands , rendering this technique impractical for some problems .",
    "both pca and the ehf ultimately converge to zero error when a sufficiently large number of expansion coefficients is employed .",
    "this is probably a desirable feature for some applications , depending on the reconstruction error tolerance .",
    "once it has been trained , the aann forward - propagation is very fast because it only requires the calculation of @xmath44 for each neuron using eq  ( [ anneq ] ) .",
    "notice that we only need to propagate the signal from the inputs to the bottleneck layer ( in the case of feature extraction ) , or from the bottleneck to the outputs ( in the case of profile reconstruction ) . in terms of computational requirements",
    ", none of the three techniques considered here presents a clear advantage over the others .",
    "the development of new instrumentations and the need to process ever increasing dataflows is turning the attention of solar physicists to pattern recognition techniques ( see references in  [ sec : intro ] ) .",
    "the present work can be considered a follow up to the papers by @xcite ( @xcite ) and @xcite ( @xcite ) , introducing a new technique for feature extraction and comparing it to the proposed pca and ehf proposed in those papers .",
    "the aann expansion is based on a mlp which is trained to reproduce the profile supplied in the input neurons .",
    "however , one ( or more ) of the mlp hidden layers has fewer neurons than the inputs / outputs layers . upon successful training",
    ", the aann is able to extract a small set of features from a profile .",
    "these features are numbers stored in the bottleneck layer , from which the entire profile can be reconstructed .",
    "this works effectively as a decomposition / expansion scheme in which the number of coefficients is set by the number of neurons in the bottleneck layer .",
    "the comparison among the three feature extraction techniques does not reveal a `` winner '' . instead , each method has its own advantages and disadvantages .",
    "depending on the particular problem it may be more interesting to use one or another .",
    "ultimately , the goal is to use the expansion coefficients from whichever method one might choose for the implementation of some inversion procedure .",
    "the most straightforward way to do this would be a look - up table , as @xcite ( @xcite ) did for pca .",
    "other alternatives include neural networks or support vector machines , which could benefit from the dimensionality reduction offered by these techniques as a pre - processing layer .                            , h. 2002 , in solmag 2002 .",
    "proceedings of the magnetic coupling of the solar atmosphere euroconference and iau colloquium 188 , 11 - 15 june 2002 , santorini , greece . ed . h. sawaya - lacoste .",
    "esa sp-505 .",
    "noordwijk , netherlands : esa publications division , isbn 92 - 9092 - 815 - 8 , 2002 , p. 45"
  ],
  "abstract_text": [
    "<S> this paper introduces a novel feature extraction technique for the analysis of spectral line stokes profiles . </S>",
    "<S> the procedure is based on the use of an auto - associative artificial neural network containing non - linear hidden layers . </S>",
    "<S> the neural network extracts a small subset of parameters from the profiles ( features ) , from which it is then able to reconstruct the original profile . </S>",
    "<S> this new approach is compared to two other procedures that have been proposed in previous works , namely principal component analysis and hermitian function expansions . </S>",
    "<S> depending on the target application , each one of these three techniques has some advantages and disadvantages , which are discussed here . </S>"
  ]
}