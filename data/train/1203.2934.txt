{
  "article_text": [
    "the authors thank m.  genovese and i.  p.  degiovanni for useful discussions . this work has been supported by miur ( firb `` lichis '' - rbfr10yq3h ) , mae ( inquest ) , and the university of trieste ( fra 2009 ) .",
    "50 bipm , iec , ifcc , ilac , iso , iupac , iupap and oiml 2008 _ evaluation of measurement data  supplement 1 to the guide to the expression of uncertainty in measurement  propagation of distributions using a monte carlo method _ joint committee for guides in metrology ,",
    "jcgm 101 http://www.bipm.org/utils/common/documents/jcgm/jcgm_101_2008_e.pdf jaynes e t 1957 phys . rev . * 106 * 620 ; jaynes e t 1957 phys",
    ". rev . * 108 * 171 wger w 1987 ieee trans .",
    "measurement * im-36 * 655658 lira i 2009 metrologia * 46 * l27 kullback s _ information theory and statistics _",
    "( wiley , new york , 1959 ) jaynes e t 1968 ieee trans .",
    "systems science and cybernetics * ssc-4 * 227 olivares s and paris m g a 2007 phys .",
    "a * 76 * 042120 hyvarinen a 1998 adv .",
    "neural inf .",
    ". syst . * 10 * 273 ; hyvarinen a and oja e 2000 neural networks * 13 * 411"
  ],
  "abstract_text": [
    "<S> supplement 1 to gum ( gum - s1 ) recommends the use of maximum entropy principle ( maxent ) in determining the probability distribution of a quantity having specified properties , e.g. , specified central moments . </S>",
    "<S> when we only know the mean value and the variance of a variable , gum - s1 prescribes a gaussian probability distribution for that variable . </S>",
    "<S> when further information is available , in the form of a finite interval in which the variable is known to lie , we indicate how the distribution for the variable in this case can be obtained . </S>",
    "<S> a gaussian distribution should only be used in this case when the standard deviation is small compared to the range of variation ( the length of the interval ) . in general , </S>",
    "<S> when the interval is finite , the parameters of the distribution should be evaluated numerically , as suggested by i.  lira [ _ metrologia _ , 2009 , * 46 * , l27 ] . here </S>",
    "<S> we note that the knowledge of the range of variation is equivalent to a bias of the distribution toward a flat distribution in that range , and the principle of minimum kullback entropy ( mke ) should be used in the derivation of the probability distribution rather than the maxent , thus leading to an exponential distribution with non gaussian features . </S>",
    "<S> furthermore , up to evaluating the distribution negentropy , we quantify the deviation of mke distributions from maxent ones and , thus , we rigorously justify the use of gum - s1 recommendation also if we have further information on the range of variation of a quantity , namely , provided that its standard uncertainty is sufficiently small compared to the range .    </S>",
    "<S> supplement 1 to gum ( gum - s1 ) @xcite provides assignments of probability density functions for some common circumstances . </S>",
    "<S> in particular , it is stated that if we know only the mean value @xmath0 and the variance @xmath1 of a certain quantity @xmath2 , we should assign a gaussian probability distribution to that quantity , according to the principle of maximum entropy ( maxent ) @xcite . </S>",
    "<S> the derivation is quite simple , as one has to look for the distribution @xmath3 maximizing the shannon entropy : @xmath4    = -\\int_{\\mathbbm r } \\!\\ ! </S>",
    "<S> dx\\ , p(x ) \\log p(x)\\ , , \\end{aligned}\\ ] ] which is given by : @xmath5 where the values of the coefficients @xmath6 should be determined to satisfy the constraints : @xmath7 with : @xmath8    however , sometimes we also know the range of the possible values of the quantity @xmath2 . </S>",
    "<S> two relevant examples are given by the phase - shift in interferometry , which is topologically confined in a @xmath9-window , and by the displacement amplitude of a harmonic oscillator , whose range of variation is dictated by energy constraints . in this case , it has been noticed by i.  lira in @xcite that a gaussian probability distribution with support on the real axis can be rigorously justified only if the standard uncertainty is sufficiently small with respect to the range of variation of the quantity . </S>",
    "<S> more in details , if we have any information about the range of variation , then this information should be employed in deriving the distribution maximizing the entropy as well as in evaluating the values of the coefficients @xmath10 of the distribution .    </S>",
    "<S> let us denote @xmath11 the range of the quantity @xmath2 , i.e. , the subset of the real line where the values of @xmath2 have nonzero probability to occur . </S>",
    "<S> the functional form of the distribution is still given by the exponential function in eq .  </S>",
    "<S> ( [ expf ] ) , however with nonzero support only in @xmath12 , whereas the coefficients are to be determined by formulas like those in eq .  </S>",
    "<S> ( [ constraints ] ) , again with @xmath13 replaced by @xmath14 . </S>",
    "<S> it then follows , e.g. , that for a variable which is known _ a priori _ to lie in a given interval , the maximum entropy distribution is not gaussian , and the gaussian approximation may be employed only if the standard deviation is small compared to range of the possible values of the quantity .    here </S>",
    "<S> we point out that having information about the range of variation may be expressed as a bias of the distribution toward a flat distribution in that range and the reasoning presented in @xcite may be subsumed by the minimum kullback entropy principle ( mke ) @xcite . </S>",
    "<S> the kullback entropy , or relative entropy , or kullback - leibler divergence , of two distributions @xmath3 and @xmath15 reads : @xmath16 = \\int_{\\mathbbm r } \\!\\ !    </S>",
    "<S> dx\\ , p(x ) \\log\\left [ p(x)/q(x)\\right].\\ ] ] according to the mke , in order to find the distribution @xmath3 given a bias toward @xmath15 , we should minimize the function : @xmath17    = k[p|q ] + \\sum_{k=0}^{2 } \\lambda_{k}\\left [ \\int_{\\mathbbm r } \\!\\ !      </S>",
    "<S> dx\\ , p(x)\\,x^k - m_k \\right],\\ ] ] with respect to the function @xmath3 , obtaining : @xmath18 where the parameters @xmath6 can be still ( numerically ) computed by using eq .  </S>",
    "<S> ( [ constraints ] ) . eq .  </S>",
    "<S> ( [ mke : sol ] ) represents the probability distribution satisfying the given constraints , but with a bias toward the distribution @xmath15 , which , for instance , may contains the information about the range of the variable @xmath19 . </S>",
    "<S> this information , which in the case of the maxent is not explicitly taken into account , now it is naturally considered from the beginning . remarkably , this is a different scenario from that covered in gum - s1 , i.e. , when further information on the quantity is available , namely , the interval of values within which the quantity is known to lie is finite .    indeed , as mentioned above , </S>",
    "<S> if the standard uncertainty is sufficiently small with respect to the range of variation of the quantity , we can adopt a gaussian probability distribution over the whole real axis and , thus , use the gum - s1 recommendation . in order to rigorously justify this statement , which has been qualitatively addressed in @xcite </S>",
    "<S> , we assess quantitatively how the knowledge of the range of variation influences the assignment of a probability distribution by considering the deviation of the mke distribution from a gaussian distribution , which would represents the maxent solution in the absence of any information about the range of variation . the deviation from normality of the mke distribution ( [ mke : sol ] ) </S>",
    "<S> may be quantified by its negentropy @xcite : @xmath20 = \\mbox{$\\frac12 $ } \\left[1+\\log \\left(2\\pi \\sigma^2_{{\\scriptstyle x}}\\right)\\right ]    -s[p]\\,,\\ ] ] where @xmath21 $ ] is the shannon entropy ( [ shan ] ) of the distribution ( [ mke : sol ] ) . as for example , for a variable known to lie in a given interval @xmath22 \\subset { \\mathbbm r}$ ] , @xmath23 , that corresponds to a bias of @xmath3 toward the flat distribution : @xmath24 $ } \\\\        0          & \\mbox{otherwise }      \\end{array }    </S>",
    "<S> \\right . \\,,\\ ] ] the negentropy ( [ nege ] ) reads : @xmath25 = \\mbox{$\\frac12 $ } \\left[1+\\log \\left(2\\pi \\sigma^2_{{\\scriptstyle x}}\\right)\\right ]    -\\log \\left(b - a\\right ) -\\lambda_0 - \\lambda_1 \\bar{x } -\\lambda_2    ( \\sigma_{{\\scriptstyle x}}^2 + \\bar{x}^2)\\,.\\ ] ] in the simplest case , namely when @xmath26 and @xmath27 $ ] , the dependence of the coefficients @xmath28 and @xmath29 is such that we have a scaling law for negentropy , which depends only on the ratio @xmath30 . </S>",
    "<S> this is illustrated in fig .  </S>",
    "<S> [ f : f1 ] , where we report the negentropy as a function of @xmath30 for different values of @xmath31 .    </S>",
    "<S> $ ] . </S>",
    "<S> we report the negentropy of the distribution as a function of the ratio @xmath30 for different values of the variance : @xmath32 ( green squares ) , @xmath33 ( red circles ) , @xmath34 ( blue triangles ) . </S>",
    "<S> [ f : f1],width=264 ]    in conclusion , we have shown that the determination of the probability distribution of a variable for which we know the first two moments and its range of variation may be effectively pursued by using the mke . </S>",
    "<S> furthermore , the negentropy of the distribution may be used to quantify how much the mke solution differs from the maxent one , i.e. to assess how the knowledge of the range of variation influences the assignment of a probability distribution . </S>",
    "<S> our analysis quantitatively supports the conclusions of ref .  </S>",
    "<S> @xcite and rigorously justifies the use of gum - s1 recommendation also in the presence of further information on the range of variation of a quantity , namely , provided that its standard uncertainty is sufficiently small compared to the range . </S>"
  ]
}