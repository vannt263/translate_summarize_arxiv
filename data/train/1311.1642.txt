{
  "article_text": [
    "compressed sensing addresses the problem of recovering nearly - sparse signals from vastly incomplete measurements  @xcite . by using the prior assumptions on the signal , the number of measurements can be well below the shannon sampling rate and effective reconstruction algorithms are available .",
    "the standard compressed sensing approach deals with _ linear _ measurements .",
    "the success of signal recovery algorithms often relies on the so - called _ restricted isometry property _ ( rip ) @xcite , which is a near - identity spectral property of small submatrices of the measurement gramian .",
    "the rip condition is satisfied with high probability and nearly optimal number of measurements for a large class of random measurements @xcite , which explains the popularity of all sorts of random sensing approaches .",
    "the most effective recovery algorithms are based either on a greedy approach or on variational models , such as @xmath0-norm minimization , leading to suitable iterative thresholded gradient descent methods . in the literature of mathematical signal processing , greedy algorithms for sparse recovery",
    "originate from the so - called matching pursuit @xcite , although several predecessors were well - known in other communities . among astronomers and asteroseismologists , for instance ,",
    "_ orthogonal least squares _ ( ols )",
    "@xcite was already in use in the 60s for the detection of significant frequencies of star light - spectra ( the so - called pre - whitening ) @xcite .",
    "we refer to @xcite for more recent developments on greedy approaches .",
    "iterative thresholding algorithms have instead a variational nature and they are designed to minimize the discrepancy with respect to the measurements and simultaneously to promote sparsity by iterated thresholding operations .",
    "we refer to @xcite and references therein for more details on such iterative schemes for sparse recovery from linear measurements",
    ".    often models of physical measurements in the applied sciences and engineering , however , are not linear and it is of utmost interest to investigate to which extent the theory of compressed sensing can be generalized to nonlinear measurements .",
    "two relevant real - life applications in physics can be mentioned , asteroseismic light measurements @xcite to determine the shape of pulsating stars and magnitude measurements in phase retrieval problems important to diffraction imaging and x - ray crystallography @xcite .",
    "there are already several recent attempts towards nonlinear compressed sensing , for instance the work by blumensath et al .",
    "@xcite , quadratic measurements are considered in @xcite , and further nonlinear inverse problems are analyzed in @xcite .",
    "phase retrieval is an active field of research nowadays and has been addressed by related approaches @xcite .",
    "+ in the present paper we provide a more unified view , by restricting the possible nonlinearity of the measurements to quasi - linear maps , which are sufficiently smooth , at least lipschitz , and they fulfill generalized versions of the classical rip .",
    "in contrast to the situation of linear measurements , the nonlinearity of the measurements actually plays in a differing manner within different recovery algorithms .",
    "therefore it is necessary to design corresponding forms of rip depending on the recovery strategies used , see conditions , for a greedy algorithm and , for iterative thresholding algorithms . in particular , we show that for certain randomized quasi - linear measurements , including lipschitz perturbations of classical rip matrices and phase retrieval from random projections , the proposed restricted isometry properties hold with high probability .",
    "while for the phase retrieval problem the stability results in @xcite are restricted to the real setting , we additionally extend them to the complex case .",
    "algorithmically we first focus on a generalized _ orthogonal least squares _ ( ols ) .",
    "such a greedy approach was already proposed in @xcite , although there no analysis of convergence was yet provided .",
    "we show within the framework of quasi - linear compressed sensing problems the recovery guarantees of this algorithm , by taking inspiration from @xcite , where a similar analysis is performed for linear measurements . the greedy algorithm we propose works for both types of applied problems mentioned above , i.e. , asteroseismology and phase retrieval .",
    "let us stress that for the latter and for signals which have rapidly decaying nonincreasing rearrangement , few iterations of this greedy algorithm are sufficient to obtain a good recovery accuracy .",
    "hence , our approach seems very competitive compared to the semi - definite program used in @xcite for phase retrieval , by recasting the problem into a demanding optimization on matrices .",
    "the greedy strategy as derived here , however , also inherits two drawbacks : ( 1 ) the original signal is required to satisfy the mentioned decay conditions , and ( 2 ) the approach needs careful implementations of multivariate global optimization to derive high accuracy for signal recovery .    to possibly circumvent those drawbacks ,",
    "we then explore alternative strategies , generalizing iterative hard- and soft - thresholding methods , which allow us to recover nearly - sparse signals not satisfying the decay assumptions .",
    "the results we present for hard - thresholding are mainly based on blumensath s findings in @xcite . for iterative soft - thresholding",
    ", we prove in an original fashion the convergence of the algorithm towards a limit point and we bound its distance to the original signal . while iterative thresholding algorithms are rarely successful for phase retrieval problems , we show their strong recovery guarantees for quasi - linear measurements which are lipschitz perturbations of rip matrices .",
    "we further emphasize in our numerical experiments that different iterative algorithms based on contractive principles do provide rather diverse success recovery results for the same problem , especially when nonlinearities are involved : this is due to the fact that the basins of attraction towards fixed points of the corresponding iterations can be significantly different . in our view , this is certainly sufficient motivation to explore several algorithmic approaches and not restricting ourselves just to a favorite one .",
    "+ as we clarified above , each algorithmic approach requires a different treatment of the nonlinearity of the measurements with the consequent need of defining corresponding generalizations of the rip .",
    "hence , we develop the presentation of our results according to the different algorithms , starting first with the generalized orthogonal least squares , and later continuing with the iterative thresholding algorithms . along the way , we present examples of applications and we show how to fulfill the required deterministic conditions of convergence by randomized quasi - linear measurements .",
    "the outline of the paper is as follows : in section [ sec:2 ] , we introduce the nonlinear compressed sensing problem , and in section 3 we derive a greedy scheme for nearly sparse signal reconstruction .",
    "we show applications of this algorithm in section [ sec : quasi linear ] to analyze simulated asteroseismic data towards the detection of frequency pulsation of stars . in section",
    "[ sec : phase ] , we discuss refinements and changes needed for the phase retrieval problem and also provide numerical experiments . for signals not satisfying the decay assumptions needed for the greedy algorithm to converge , iterative",
    "thresholding algorithms are discussed in section [ sec : thresholding ] .",
    "in the by now classical compressed sensing framework , an unknown nearly sparse signal @xmath1 is to be reconstructed from @xmath2 linear measurements , with @xmath3 , and one models this setting as the solution of a linear system @xmath4 where @xmath5 is some noise term and the @xmath6-th row of @xmath7 corresponds to the @xmath6-th linear measurement on the unknown signal @xmath8 with outcome @xmath9 .",
    "we say that @xmath10 satisfies the restricted isometry property ( rip ) of order @xmath11 with @xmath12 if @xmath13 for all @xmath14 with at most @xmath11 nonzero entries .",
    "we call such vectors @xmath11-sparse . if @xmath10 satisfies the rip of order @xmath15 and @xmath16 , then signal recovery is possible up to noise level and @xmath11-term approximation error",
    ". it should be mentioned that large classes of random matrices @xmath7 satisfy the rip with high probability for the ( nearly-)optimal dimensionality scaling @xmath17 .",
    "we refer to  @xcite for the early results and @xcite for a recent extended treatise .",
    "many real - life applications in physics and biomedical sciences , however , carry some strongly nonlinear structure , so that the linear model is not suited anymore , even as an approximation . towards the definition of a nonlinear framework for compressed sensing",
    ", we shall consider for @xmath3 a map @xmath18 , which is not anymore necessarily linear , and aim at reconstructing @xmath1 from the measurements @xmath19 given by @xmath20 similarly to linear problems , also the unique and stable solution of the equation is in general an impossible task , unless we require certain a priori assumptions on @xmath8 , and some stability properties similar to for the nonlinear map @xmath10 . as the variety of possible nonlinearities is extremely vast ,",
    "it is perhaps too ambitious to expect that generalized rip properties can be verified for any type of nonlinearity . as a matter of fact , and as we shall show in details below , most of the nonlinear models with stability properties which allow for nearly sparse signal recovery , have a smooth quasi - linear nature . with this",
    "we mean that there exists a lipschitz map @xmath21 such that @xmath22 , for all @xmath14 .",
    "however , in the following we will use and explicitly highlight this quasi - linear structure only when necessary .    : + initialize @xmath23 , @xmath24 +    our first approach towards the solution of will be based on a greedy principle , since it is also perhaps the most intuitive one : we search first for the best @xmath25-sparse signal which is minimizing the discrepancy with respect to the measurements and then we seek for a next best matching @xmath26-sparse signal having as one of the active entries the one previously detected , and so on .",
    "this method is formally summarized in the @xmath27-norm matching greedy algorithm [ algo:1 ] . for the sake of clarity , we mention that @xmath28 denotes the standard euclidean norm of any vector @xmath29 , while @xmath30 is its @xmath27-norm for @xmath31 .",
    "moreover , when dealing with matrices , we denote with @xmath32 the spectral norm of the matrix @xmath10 and with @xmath33 its hilbert - schmidt norm .",
    "greedy algorithms have already proven useful and efficient for many sparse signal reconstruction problems in a linear setting , cf .",
    "@xcite , and we refer to @xcite for a more recent treatise . before we can state our reconstruction result here , we still need some preparation . the nonincreasing rearrangement of @xmath34 is defined as @xmath35 for @xmath36 , we define the class of @xmath37-rapidly decaying vectors in @xmath38 by @xmath39 given @xmath34 , the vector @xmath40 is the best @xmath41-sparse approximation of @xmath42 , i.e. , it consists of the @xmath41 largest entries of @xmath42 in absolute value and zeros elsewhere .",
    "signal recovery is possible under decay and stability conditions using the @xmath27-greedy algorithm [ algo:1 ] , which is a generalized orthogonal least squares @xcite :    [ th : rec result 1 ] let @xmath43 , where @xmath1 is the signal to be recovered and @xmath44 is a noise term .",
    "suppose further that @xmath45 , @xmath46 , and @xmath47 .",
    "if the following conditions hold ,    * there are @xmath48 such that , for all @xmath11-sparse @xmath49 , @xmath50 * @xmath51 such that @xmath52 , where @xmath53 and @xmath54 with @xmath55 ,    then the @xmath27-greedy algorithm [ algo:1 ] yields a sequence @xmath56 satisfying @xmath57 and @xmath58 if @xmath59 is @xmath11-sparse , then @xmath60 .    according to @xmath53 , the noise term @xmath5 must be small and we implicitly suppose that @xmath46 .",
    "otherwise , we can simply choose a smaller @xmath11 .",
    "if the signal @xmath59 is @xmath11-sparse and the noise term @xmath5 equals zero , then the @xmath27-greedy algorithm [ algo:1 ] in theorem [ th : rec result 1 ] yields @xmath61 .",
    "a similar greedy algorithm was proposed in @xcite for nonlinear problems , and our main contribution here is the careful analysis of its signal recovery capabilities .",
    "conditions of the type have also been used in @xcite , but with additional restrictions , so that the constants @xmath62 and @xmath63 must be close to each other . in our theorem",
    "[ th : rec result 1 ] , we do not need any constraints on such constants , because the decay conditions on the signal can compensate for this .",
    "a similar relaxation using decaying signals was proposed in @xcite for linear operators @xmath10 , but even there the authors still assume @xmath64 .",
    "we do not require here any of such conditions .",
    "the proof of theorem [ th : rec result 1 ] extends the preliminary results obtained in @xcite , which we split and generalize in the following two lemmas :    [ lemma:-1 ] if @xmath1 is contained in @xmath65 , then @xmath66    the proof of lemma [ lemma:-1 ] is a straightforward calculation , in which the geometric series is used , so we omit the details .",
    "[ lemma:0 ] fix @xmath45 and suppose that @xmath46 . if @xmath1 is contained in @xmath65 with @xmath67 , where @xmath68 , for some @xmath69 , then , for @xmath70 , @xmath71    it is sufficient to consider @xmath59 , which are not @xmath41-sparse .",
    "a short calculation reveals that the condition on @xmath37 implies @xmath72 we multiply the above inequality by @xmath73 , so that @xmath74 yields @xmath75 lemma [ lemma:-1 ] now implies .    we can now turn to the proof of theorem [ th : rec result 1 ]",
    ":    we must check that the index set selected by the @xmath27-greedy algorithm matches the location of the nonzero entries of @xmath76 .",
    "we use induction and observe that nothing needs to be checked for @xmath77 . in the induction step , we suppose that @xmath78 .",
    "let us choose @xmath79 .",
    "the lower inequality in yields @xmath80 where the last inequality is a crude estimate based on @xmath79 .",
    "thus , lemma [ lemma:0 ] implies @xmath81 on the other hand , the minimizing property of @xmath82 and the upper inequality in yield @xmath83 the last line and are contradictory if @xmath84 , so that we must have @xmath84 , for some @xmath85 , which concludes the part about the support .",
    "next , we shall derive the error bound .",
    "standard computations yield @xmath86 few rather rough estimates yield @xmath87 which concludes the proof .      in this subsection",
    "we present examples where algorithm [ algo:1 ] can be successfully used .",
    "we start with an abstract example of a nonlinear lipschitz perturbation of a linear model and then we consider a relevant real - life example from asteroseismology .      as an explicit example of @xmath10 matching the requirements of theorem [ th : rec result 1 ] with high probability , we propose lipschitz perturbations of rip matrices :    [ th : example ]",
    "if @xmath10 is chosen as @xmath88 where @xmath89 satisfies the rip of order @xmath11 and constant @xmath90 , @xmath91 is some reference vector , @xmath92 is a bounded lipschitz continuous function , @xmath93 is a sufficiently small scaling factor , and @xmath94 arbitrarily fixed , then there are constants @xmath48 , such that the assumptions in theorem [ th : rec result 1 ] hold for @xmath95 .",
    "we first check on @xmath63 .",
    "if @xmath96 denotes the lipschitz constant of @xmath97 , then we obtain @xmath98 \\|\\\\ & \\leq   ( 1+\\delta_k)\\|\\hat{x}-y\\|+ \\epsilon l   \\big|\\|\\hat{x}-x_0\\| - \\|y - x_0\\|\\big|\\|a_2 \\|_{2}\\|\\hat{x}\\|+\\epsilon b\\|a_2\\|_{2 } \\|\\hat{x}-y\\|\\\\ & \\leq ( 1+\\delta_k+\\epsilon l    \\|a_2 \\|_{2}\\|\\hat{x}\\| + \\epsilon b\\|a_2\\|_2)\\|\\hat{x}-y\\|,\\end{aligned}\\ ] ] where we have used the reverse triangular inequality and @xmath99 .",
    "thus , we can choose @xmath100 , where @xmath101 .",
    "next , we derive a suitable @xmath62 . for @xmath11-sparse",
    "@xmath49 , we derive similarly to the above calculations @xmath102 if @xmath93 is sufficiently small , we can choose @xmath103 .",
    "any matrix satisfying the rip of order @xmath11 with high probability , for instance being within certain classes of random matrices @xcite , induces maps @xmath10 via proposition [ th : example ] that satisfy the assumptions of theorem [ th : rec result 1 ] .",
    "notice that the form of nonlinearity considered in is actually quasi - linear , i.e. , @xmath22 , where @xmath104 .",
    "asteroseismology studies the oscillation occurring inside variable pulsating stars as seismic waves @xcite . some regions of the stellar surface contract and heat up while others expand and cool down in a regular pattern causing observable changes in the light intensity .",
    "this also means that areas of different temperature correspond to locations of different expansion of the star and characterize its shape . through the analysis of the frequency spectra it is possible to determine the internal stellar structure .",
    "often complex pulsation patterns with multiperiodic oscillations are observed and their identification is needed .",
    "we refer to @xcite for a detailed mathematical formulation of the model connecting the instantaneous star shape and its actual light intensity at different frequencies . here",
    "we limit ourselves to a schematic description where we assume the star being a two dimensional object with a pulsating shape contour .",
    "let the function @xmath105 describe the star shape contour , for a parameter @xmath106 , which also simultaneously represents the temperature ( or emitted wavelength ) on the stellar surface at some fixed point in time",
    ". its oscillatory behavior yields @xmath107 for some coefficient vector @xmath108 and some inclination angle @xmath109 .",
    "this vector @xmath42 needs to be reconstructed from the instantaneous light measurements @xmath110 , modeled in @xcite by the formula @xmath111 so that we suppose that @xmath112 is sampled at @xmath113 . here",
    ", @xmath97 is a correction factor modeling _ limb darkening _",
    ", i.e. , the fading intensity of the light of the star towards its limb , and @xmath114 is some partition of unity modeling the wavelength range of each telescope sensor , see @xcite for details .",
    "notice that the light intensity data at different frequency bands ( corresponding to different @xmath115 ) are obtained through the quasi - linear measurements @xmath116 with @xmath117 and one wants to reconstruct a vector @xmath42 matching the data with few nonzero entries .",
    "in fact it is rather accepted in the asteroseismology community that only few low frequencies of the star shape ( when its contour shape is expanded in spherical harmonics ) are relevant @xcite .",
    "we do not claim that the model matches all of the assumptions in theorem [ th : rec result 1 ] , but we shall observe that algorithm [ algo:1 ] for @xmath95 can be used to identify the instantaneous pulsation patterns of simulated light intensity data , when low frequencies are activated .",
    "this is a consequence of the fact that different low frequency activations result in sufficiently uncorrelated measurements in the data to be distinguished by the greedy algorithm towards recovery . for the numerical experiments , the ambient dimension is @xmath118 and we make @xmath119 measurements , see @xcite for details on the choice of @xmath115 , @xmath97 , and the used multivariate optimization routines .",
    "we generate @xmath26- and @xmath120-sparse signals and apply algorithm [ algo:1 ] in figs .  [",
    "fig : pulsation ] and [ fig : pulsation 2 ] to reconstruct the signal .",
    "the greedy strategy identifies one additional location of the solution s support at each iteration step and finds the correct signal after @xmath26 and @xmath120 steps , respectively . in fig .",
    "[ fig : noise ] , we generated a signal whose entries decay rapidly , so that higher frequencies have lower magnitudes .",
    "we show the reconstruction of the shape after @xmath120 iterations of the greedy algorithm .",
    "as expected , higher frequencies are suppressed and we obtain a low - pass filter approximation of the original shape .",
    "experiments in x - ray crystallography and diffraction imaging require signal reconstruction from magnitude measurements , usually in terms of light intensities .",
    "we do not present the explicit physical models , which would go beyond the scope of the present paper , but refer to the literature instead .",
    "it seems impossible to provide a comprehensive list of references , so we only mention @xcite for some classical algorithms .",
    "let @xmath1 be some signal that we need to reconstruct from measurements @xmath121 , where we selected a set of measurement vectors @xmath122 .",
    "in other words , we have phaseless measurements and need to reconstruct @xmath123 .",
    "it turns out surprisingly that the above framework of reconstruction from nonlinear sensing can be modified to fit the phase retrieval problem .",
    "let us stress that so far the most efficient and stable recovery procedures are based on semi - definite programming , as used in @xcite , by recasting the problem into a perhaps demanding optimization on matrices .",
    "in this section we show that there is no need to linearize the problem by lifting the dimensionality towards low - rank matrix recovery , but is is sufficient to address a plain sparse vector recovery in the fully nonlinear setting .    in models relevant to optical measurements like diffraction imaging and x - ray crystallography",
    ", we must deal with the complex setting , in which @xmath124 is at most determined up to multiplication by a complex unit .",
    "we shall state our findings for the real case first and afterwards discuss the extensions to complex vector spaces .",
    "let @xmath125 be a collection of measurement matrices .",
    "we consider the map @xmath126 and we aim at reconstructing a signal vector @xmath1 from measurements @xmath127 . since @xmath128 , the vector @xmath59 can at best be determined up to its sign , and the lower bound in can not hold , not allowing us to use directly theorem [ th : rec result 1 ] .",
    "however , we notice that for special classes of @xmath10 , for instance , when @xmath129 are independent gaussian matrices , the lower bound in holds with high probability as long as @xmath130 stays away from @xmath131 , see the heuristic probability transitions of validity of shown in fig .  [ fig:1 ] .",
    "hence , there is the hope that the greedy algorithm can nevertheless be successful , because it proceeds by selecting first the largest components of the expected solution , hence orienting the reconstruction precisely towards the direction within the space where actually holds with high probability , in a certain sense realizing a self - fulfilling prophecy : the algorithm goes only where it is supposed to work . in order to make this geometric intuition more explicit",
    "we shall modify the deterministic conditions of theorem [ th : rec result 1 ] accordingly , so that we cover the above setting as well .",
    "under slightly different deterministic conditions , we derive a recovery result very similar to theorem [ th : rec result 1 ] :    [ th : new ] let @xmath10 be given by and @xmath43 , where @xmath1 is the signal to be recovered and @xmath44 is a noise term .",
    "suppose further that @xmath45 , @xmath46 , and @xmath47 . if the following conditions are satisfied ,    * there are constants @xmath48 , such that , for all @xmath11-sparse @xmath49 , @xmath132 * @xmath51 with @xmath133 , where @xmath53 and @xmath54 with @xmath134 ,    then the @xmath27-greedy algorithm [ algo:1 ] yields a sequence @xmath56 satisfying @xmath57 and @xmath135 if @xmath59 is @xmath11-sparse , then @xmath60 .",
    "note that resembles the restricted isometry property for rank minimization problems , in which @xmath62 and @xmath63 are required to be close to each other , see @xcite and references therein . in theorem",
    "[ th : new ] , we can allow for any pair of constants and compensate deviation between @xmath62 and @xmath63 by adding the decay condition on the signal .",
    "in other words , we shift conditions on the measurements towards conditions on the signal .",
    "the structure of the proof of theorem [ th : new ] is almost the same as the one for theorem [ th : rec result 1 ] , so that we first derive results similar to the lemmas [ lemma:-1 ] and [ lemma:0 ] :    [ lemma:-1b ] if @xmath1 is contained in @xmath65 , then @xmath136    we omit the straight - forward proof and state the second lemma that is needed :    [ lemma:1 ] fix @xmath45 and suppose that @xmath46 . if @xmath1 is contained in @xmath65 with @xmath137 , where @xmath68 , for some @xmath69 , then , for @xmath70 , @xmath138    as in the proof of lemma [ lemma:0 ] , the conditions on @xmath37 imply @xmath139 multiplying by @xmath140 and applying lemma [ lemma:-1b ] yield @xmath141 we can further estimate @xmath142 which concludes the proof .    as in the proof of theorem [ th :",
    "rec result 1 ] , we must check that the index set selected by the @xmath27-greedy algorithm [ algo:1 ] matches the location of the nonzero entries of @xmath76 .",
    "again , we use induction and the initialization @xmath77 is trivial .",
    "now , we suppose that @xmath78 and choose @xmath79 . the lower bound in yields @xmath143 which is due to @xmath79 , so that one row and one column of @xmath144 corresponding to one of the @xmath41-largest entries of @xmath59 are zero .",
    "lemma [ lemma:1 ] implies @xmath145 on the other hand , the minimizing property of @xmath82 and the condition imply @xmath146 the latter inequality implies with that @xmath147 , for all @xmath85 , which concludes the part about the support .",
    "next , we shall verify the error bound .",
    "we obtain @xmath148 some rough estimates yield @xmath149 which concludes the proof .",
    "the greedy algorithm [ algo:1 ] can also be performed in the complex setting .",
    "the complex version of theorem [ th : new ] also holds when recovery of @xmath123 is replaced by a complex unit vector times @xmath59 .",
    "we aim at choosing @xmath129 in a suitable random way , so that the conditions in theorem [ th : new ] are satisfied with high probability . indeed , the upper bound in is always satisfied for some @xmath150 , because we are in a finite - dimensional regime . in this section",
    "we are interested in one rank matrices @xmath151 , for some vector @xmath152 , @xmath153 , because then @xmath154 models the phase retrieval problem .      to check on the assumptions in theorem [ th : new ]",
    ", we shall draw at random the measurement vectors @xmath155 from probability distributions to be characterized next .",
    "we say that a random vector @xmath156 satisfies the _ small - ball assumption _ if there is a constant @xmath157 , such that , for all @xmath158 and @xmath159 , @xmath160 moreover , we say that @xmath161 is _ isotropic _ if @xmath162 , for all @xmath158 .",
    "the vector @xmath161 is said to be _",
    "@xmath96-subgaussian _ if , for all @xmath158 and @xmath163 , @xmath164 eldar and mendelson derived the following result :    [ th : eldar ] let @xmath155 be a set of independent copies of a random vector @xmath156 that is isotropic , @xmath96-subgaussian , and satisfies the small - ball assumption .",
    "then there are positive constants @xmath165 such that , for all @xmath166 with @xmath167 , and for all @xmath11-sparse @xmath168 , @xmath169 with probability of failure at most @xmath170 .",
    "the uniform distribution on the sphere and the gaussian distribution on @xmath38 induce random vectors satisfying the assumptions of theorem [ th : eldar ] .",
    "however , at first glance , the above theorem does not help us directly , because we are seeking for an estimate involving the hilbert - schmidt norm .",
    "it is remarkable though that @xmath171 the relation then yields that also the lower bound in is satisfied for any constant @xmath172 , so that theorem [ th : new ] can be used for @xmath173 .",
    "thus , if the signal @xmath59 is sparse and satisfies decay conditions matching the constants in , then algorithm [ algo:1 ] for @xmath173 recovers @xmath174 .      in the following and at least for the uniform distribution on the sphere , we shall generalize theorem [ th : eldar ] to the complex setting , so that the assumptions of theorem [ th : new ] hold with high probability .    [ th : complex and new ] if @xmath155 are independent uniformly distributed vectors on the unit sphere , then there is a constant @xmath175 such that , for all @xmath11-sparse @xmath176 and @xmath177 , @xmath178 with probability of failure at most @xmath179 .    for fixed @xmath168 , the results in @xcite imply that there are constants @xmath180 such that , for all @xmath181 , @xmath182 with probability of failure at most @xmath183",
    "if both @xmath42 and @xmath130 are @xmath11-sparse , then the union of their supports induces an at most @xmath15-dimensional coordinate subspace , so that also @xmath184 can be reduced to a @xmath185 matrix , by eliminating rows and columns that do not belong to the indices of the subspace .",
    "results in @xcite can be used to derive that the estimate holds uniformly for elements in this subspace when @xmath186 , for some constant @xmath187 .",
    "there are at most @xmath188 many of such coordinate subspaces , see also @xcite",
    ". therefore , by a union bound , the probability of failure is at most @xmath189 thus , if also @xmath190 with @xmath191 , then we have the desired result .",
    "we want to point out that the use of the term @xmath192 limits theorem [ th : eldar ] to the real setting .",
    "our observation was the key to derive the analog result in the complex setting .",
    "a slightly more general phase retrieval problem was discussed in @xcite , where the measurement @xmath10 in is given by @xmath194 , @xmath153 , and each @xmath195 is an orthogonal projector onto an @xmath193-dimensional linear subspace @xmath196 of @xmath38 .",
    "the set of all @xmath193-dimensional linear subspaces @xmath197 is a manifold endowed with the standard normalized haar measure @xmath198 :    [ th : not sparse bachoc ] there is a constant @xmath199 , only depending on @xmath193 , such that the following holds : for @xmath200 fixed , there exist constants @xmath201 , such that , for all @xmath202 and @xmath203 independently chosen random subspaces with identical distribution @xmath198 , the inequality @xmath204 for all @xmath168 , holds with probability of failure at most @xmath205 .",
    "since the rank of @xmath184 is at most @xmath26 , its hilbert - schmidt norm is bounded by @xmath206 times the operator norm .",
    "thus , we have the lower @xmath0-bound in for @xmath207 when @xmath202 , and the random choice of subspaces ( and hence orthogonal projectors ) enables us to apply theorem [ th : new ] . the result for @xmath11-sparse signals",
    "is a consequence of theorem [ th : not sparse bachoc ] :    [ cor : new ] there is a constant @xmath199 , only depending on @xmath193 , such that the following holds : for @xmath200 fixed , there exist constants @xmath201 , such that , for all @xmath208 and @xmath209 independently chosen random subspaces with identical distribution @xmath198 , the inequality @xmath210 for all @xmath11-sparse @xmath168 , holds with probability of failure at most @xmath211 .",
    "the lower bound on @xmath2 in theorem [ th : not sparse bachoc ] is not needed when the vectors @xmath42 and @xmath130 are fixed in , cf .",
    "if both @xmath212 are supported in one fixed coordinate subspace of dimension @xmath15 , then the proof of theorem [ th : not sparse bachoc ] in @xcite , see also @xcite , yields that holds uniformly for this subspace provided @xmath213 .",
    "similar to the proof of theorem [ th : complex and new ] , we shall use theorem [ th : not sparse bachoc ] with a @xmath15 coordinate subspace and then apply a union bound by counting the number of such subspaces .",
    "indeed , since @xmath184 can be treated as a @xmath185 matrix by removing zero rows and columns , theorem [ th : not sparse bachoc ] implies for all @xmath168 supported in a fixed coordinate subspace of dimension @xmath15 with probability of failure at most @xmath211 when @xmath213 .",
    "again , we have used that @xmath184 has rank at most two , so that the hilbert - schmidt norm is bounded by @xmath206 times the operator norm .",
    "the remaining part can be copied from the end of the proof of theorem [ th : complex and new ] .",
    "it is mentioned in @xcite already that theorem [ th : not sparse bachoc ] also holds in the complex setting .",
    "therefore , corollary [ cor : new ] has a complex version too , and our present results hold for complex rank-@xmath193 projectors .      to conclude the discussion on random measurements for phase retrieval , we shall generalize some results from @xcite to sparse vectors .",
    "also , we want to present a framework , in which @xmath129 in can be chosen as a set of independent random matrices with independent gaussian entries .",
    "let @xmath214 be a random map that takes values in linear maps from @xmath215 to @xmath216 .",
    "then @xmath214 is called nearly isometrically distributed if , for all @xmath217 , @xmath218 and , for all @xmath219 , we have @xmath220 with probability of failure at most @xmath221 , where @xmath222 is an increasing function .",
    "note that the definition of nearly isometries in @xcite is more restrictive , but we can find several examples there .",
    "for instance , if @xmath129 in are independent matrices with independent standard gaussian entries , then the map @xmath223 is nearly isometrically distributed , see @xcite .",
    "the following theorem fits into our setting , and it should be mentioned that we will only use and for symmetric matrices @xmath224 of rank at most @xmath26 .",
    "so , we could even further weaken the notion of nearly isometric distributions accordingly .",
    "[ th : again ] fix @xmath45",
    ". if @xmath214 is a nearly isometric random map from @xmath215 to @xmath216 and @xmath225 , then there are constants @xmath226 , such that , uniformly for all @xmath227 and all @xmath11-sparse @xmath168 , @xmath228 with probability of failure at most @xmath229 .    as a consequence of theorem [ th : again ] ,",
    "we can fix @xmath230 and derive two constants @xmath226 depending only on @xmath230 and @xmath231 , such that holds for all @xmath11-sparse @xmath168 in a uniform fashion with probability of failure at most @xmath232 whenever @xmath233 . the analogous result for not necessarily sparse vectors",
    "is derived in @xcite .",
    "we fix an index set @xmath234 of @xmath15 coordinates in @xmath38 , denote the underlying coordinate subspace by @xmath235 , and define @xmath236 any element @xmath237 can be written as @xmath238 such that @xmath239 and @xmath240 are orthogonal unit norm vectors . in order to build a covering of @xmath241",
    ", we start with a covering of the @xmath15-dimensional unit sphere @xmath242 in @xmath243 . indeed ,",
    "there is a finite set @xmath244 of cardinality at",
    "most @xmath245 , such that , for every @xmath246 , there is @xmath247 with @xmath248 , see , for instance , ( * ? ? ?",
    "* lemma 5.2 )",
    ". we can also uniformly cover @xmath249 $ ] with a finite set of cardinality @xmath250 , so that the error is bounded by @xmath251 .",
    "thus , we can cover @xmath241 with a set @xmath252 of cardinality at most @xmath253 , such that , for every @xmath254 , there is @xmath255 with @xmath256 , so that holds uniformly on @xmath252 with probability of failure at most @xmath257 .",
    "taking the square root yields with at most the same probability of failure that @xmath258 holds uniformly for all @xmath259 .",
    "we now define the random variable @xmath260 and consider an arbitrary @xmath261 .",
    "then there is @xmath262 such that is satisfied , so that we can further estimate @xmath263 for the last inequality , we have used . by choosing @xmath237 with @xmath264 , we derive @xmath265 , which implies @xmath266 .",
    "thus , we obtain @xmath267 .",
    "the lower bound is similarly derived by @xmath268    so far , we have the estimate in a uniform fashion for all @xmath212 in some fixed coordinate subspace of dimension @xmath15 with probability of failure at most @xmath257 .",
    "again , we derive the union bound by counting subspaces as in the proof of theorem [ th : complex and new ] .",
    "there are at most @xmath269 many subspaces , so that we can conclude the proof .",
    "we shall follow the model in and study signal reconstruction rates for random choices of measurement vectors @xmath270 chosen as independent standard gaussian vectors . for a fixed number of measurements @xmath2",
    ", we shall study the signal recovery rate depending on the sparsity @xmath11 .",
    "we expect to have high success rates for small @xmath11 and decreased rates when @xmath11 grows .",
    "figure [ fig : phase ] shows results of numerical experiments consistent with our theoretical findings . we must point out though that each step of the greedy algorithm requires solving a nonconvex global optimization problem . here , we used standard optimization routines that may yield results that are not optimal .",
    "better outcomes can be expected when applying more sophisticated global optimization methods , for instance , based on adaptive grids and more elaborate analysis of functions of few parameters in high dimensions , cf .",
    "although the quasi - linear structure of the measurements does not play an explicit role in the formulation of algorithm [ algo:1 ] , in the examples we showed in the previous sections it was nevertheless a crucial aspect to obtain generalized rip conditions such as and .",
    "when using iterative thresholding algorithms , as we shall show below , the quasi - linear structure of the measurements gets into the formulation of the algorithms as well .",
    "hence , from now on we shall be a bit more explicit on the form of nonlinearity and we consider a map @xmath271 , and aim to reconstruct @xmath1 from measurements @xmath19 given by @xmath272 as guiding examples , we keep as references the maps @xmath10 in proposition [ th : example ] , which can be written as @xmath273 , where @xmath274 .",
    "+ the study of iterative thresholding algorithms that we propose below is motivated by the intrinsic limitations of algorithm [ algo:1 ] , in particular , its restriction to recovering only signals which have a rapid decay of their nonincreasing rearrangement , and the potential complexity explosion due to the need of performing several high - dimensional global optimizations at each greedy step .",
    "we follow ideas in @xcite and aim to use the iterative scheme @xmath275 where @xmath276 is some parameter and , as before , @xmath277 denotes the best @xmath11-sparse approximation of @xmath49 .",
    "the following theorem is a reformulation of a result by blumensath in @xcite :    [ th : blumi ] let @xmath43 , where @xmath1 is the signal to be recovered and @xmath44 is a noise term .",
    "suppose @xmath45 is fixed . if @xmath278 satisfies the following assumptions ,    * there is @xmath157 such that @xmath279 , * there are @xmath48 such that , for all @xmath11-sparse @xmath280 , @xmath281 * there is @xmath282 such that , for all @xmath11-sparse @xmath49 , @xmath283 * there is @xmath284 such that @xmath285 , * the constants satisfy @xmath286 ,    then the iterative scheme converges towards @xmath287 satisfying @xmath288 where @xmath289 .",
    "note that is again a rip condition for each @xmath290 .",
    "the proof of theorem [ th : blumi ] is based on blumensath s findings in @xcite , where the nonlinear operator @xmath10 is replaced by its first order approximation at @xmath82 within the iterative scheme .",
    "when dealing with the quasi - linear setting , it is natural to use @xmath291 , so we formulated the iteration in this way already .",
    "we first verify that the assumptions in ( * ? ? ?",
    "* corollary 2 ) are satisfied . by using ,",
    "we derive , for @xmath11-sparse @xmath49 , @xmath292 the assumptions of ( * ? ? ?",
    "* corollary 2 ) are satisfied , which implies that the iterative scheme converges to some @xmath11-sparse @xmath293 satisfying @xmath294 where @xmath289 . we still need to estimate the left term on the right - hand side . a zero addition yields @xmath295 which concludes the proof .",
    "we shall verify that the map in proposition [ th : example ] satisfies the assumptions of theorem [ th : blumi ] at least when @xmath59 is @xmath11-sparse :    [ ex:1 ] let @xmath274 , so that @xmath296 with @xmath10 as in proposition [ th : example ] . by a similar proof and under the same notations we derive that an upper bound in can be chosen as @xmath297 , where @xmath230 is the rip constant for @xmath298 . for the lower bound , we compute @xmath299 . in other words , @xmath300 satisfies the rip of order @xmath11 with constant @xmath301 . in",
    ", we can choose @xmath302 .",
    "thus , if @xmath303 are sufficiently small , then the assumptions of theorem [ th : blumi ] are satisfied .",
    "the type of thresholding in the scheme of the previous section is one among many potential choices . here",
    ", we shall discuss soft - thresholding , which is widely applied when dealing with linear compressed sensing problems .",
    "our findings in this section are based on preliminary results in @xcite .",
    "we suppose that the original signal is sparse and , in fact , we aim to reconstruct the sparsest @xmath59 that matches the data @xmath304 . in other words , we intend to solve for @xmath1 with @xmath305 subject to @xmath306 .",
    "theorem [ th : blumi ] yields that we can use iterative hard - thresholding to reconstruct the sparsest solution . here",
    ", we shall follow a slightly different strategy .",
    "as @xmath307-minimization is a combinatorial optimization problem and computationally cumbersome in principle , even np - hard under many circumstances , it is common practice in compressed sensing to replace the @xmath307-pseudo norm with the @xmath0-norm , so that we consider the problem @xmath308 it is also somewhat standard to work with an additional relaxation of it and instead solve for @xmath309 given by @xmath310 where @xmath175 is sometimes called the relaxation parameter .",
    "the optimization allows for @xmath311 , hence , is particularly beneficial when we deal with measurement noise , so that @xmath312 and @xmath313 can be suitably chosen to compensate for the magnitude of @xmath5 .",
    "if there is no noise term , then approximates when @xmath313 tends to zero .",
    "the latter is a standard result but we explicitly state this and prove it for the sake of completeness :    [ prop:1 ] let the map @xmath314 be continuous and suppose that @xmath315 is a sequence of nonnegative numbers that converge towards @xmath316 . if @xmath317 is any sequence of minimizers of @xmath318 , then it contains a subsequence that converges towards a minimizer of .",
    "if the minimizer of is unique , then the entire sequence @xmath317 converges towards this minimizer .",
    "let @xmath59 be a minimizer of .",
    "direct computations yield @xmath319 thus , there is a convergent subsequence @xmath320 , for @xmath321 . since @xmath322 and hold",
    ", @xmath323 must be a minimizer of .    now",
    ", suppose that the minimizer @xmath59 of is unique .",
    "if @xmath324 is an accumulation point of @xmath325 , then there is a subsequence converging towards @xmath324 .",
    "the same arguments as above with the uniqueness assumption yield @xmath326 , so that @xmath325 is a bounded sequence with only one single accumulation point .",
    "hence , the entire sequence converges towards @xmath59 .    from here on , we shall focus on , which we aim to solve at least approximately using some iterative scheme .",
    "first , we define the map @xmath327 to develop the iterative scheme , we present some conditions , so that @xmath328 is contractive :    [ th : th elast one maybe ] given @xmath19 , fix @xmath175 and suppose that there are constants @xmath329 such that , for all @xmath168 ,    a.   [ it : i ] @xmath330 b.   [ it : ii ] there is @xmath331 such that @xmath332 and @xmath333 , c.   [ it : iii ] @xmath334 d.   [ it : iv ] if @xmath130 is @xmath335-sparse , then @xmath336 e.   [ it : v ] the constants satisfy @xmath337 ,    then @xmath328 is a bounded contraction , so that the recursive scheme @xmath338 converges for any initial vector towards a point @xmath339 satisfying the fixed point relationship @xmath340    we believe that the fixed point of in theorem [ th : th elast one maybe ] is close to the actual minimizer of . to support this point of view , we shall later investigate on the distance @xmath341 in theorem [ theorem : very last now ] and also provide some numerical experiments in section [ sec : soft numerics ] .",
    "a few more comments are in order before we take care of the proof : note that the constant @xmath342 must hold for the operator norm in , and @xmath343 in the rip of covers only sparse vectors . therefore , @xmath344 can be much smaller than @xmath342 .",
    "condition is a standard lipschitz property .",
    "if @xmath343 is indeed less than @xmath25 , then small data @xmath345 can make up for larger other constants , so that can hold .",
    "the requirement is more delicate though and a rough derivation goes as follows : the data @xmath345 are supposed to lie in the range of @xmath300 , which is satisfied , for instance , if @xmath300 is onto .",
    "the pseudo - inverse of @xmath300 then yields a vector @xmath346 with minimal @xmath347-norm .",
    "we can then ask for boundedness of all operator norms of the pseudo - inverses .",
    "however , we still need to bound the @xmath0-norm by using the @xmath347-norm , which introduces an additional factor @xmath348 .",
    "we introduce the soft - thresholding operator @xmath349 , @xmath350 given by @xmath351 which we shall use in the following proof :    for @xmath34 , we can apply ( ii ) , so that @xmath333 and @xmath332 , implying @xmath352 thus , we have @xmath353 .    the conditions and imply @xmath354 it is well - known that @xmath355 and @xmath356 is the soft - thresholding operator in , cf .",
    "note that @xmath357 can be bounded by @xmath358 it is known , cf .",
    "@xcite and ( * ? ? ?",
    "* lemma 4.15 ) , that the bound on @xmath357 implies @xmath359 the condition can be rewritten as @xmath360 , for suitably sparse @xmath130 . since soft - thresholding",
    "is nonexpansive , i.e. , @xmath361 the identity then yields with @xmath362 which implies @xmath363 thus , @xmath328 is contractive and @xmath364 converges towards a fixed point .",
    "if @xmath365 is large , then the conditions in theorem [ th : th elast one maybe ] are extremely strong . for smaller @xmath365 , on the other hand , we can find examples matching the requirements .",
    "note also that the above proof reveals that @xmath339 is at most @xmath366-sparse .",
    "[ ex:2 ] as in example [ ex:1 ] , let @xmath274 , so that @xmath296 with @xmath10 as in proposition [ th : example ]",
    ". we additionally suppose that @xmath367 , that @xmath298 is onto , and denote the smallest eigenvalue of @xmath368 by @xmath369",
    ". then we can choose @xmath370 and @xmath371 .",
    "if @xmath372 are sufficiently small , then the smallest eigenvalue of @xmath373 is almost given by the smallest eigenvalue of @xmath368 and denoted by @xmath369 .",
    "if @xmath374 denotes the pseudo - inverse of @xmath300 , then @xmath375 . for , we can define @xmath376 , so that @xmath377 and @xmath378 .",
    "still , suppose that @xmath372 are sufficiently small and also assume that @xmath298 satisfies the rip with constant @xmath227 for sufficiently large sparsity requirements in .",
    "thus , the conditions of theorem [ th : th elast one maybe ] are satisfied if @xmath365 is sufficiently small .    : + initialize @xmath379 as an arbitrary vector +    the recursive scheme in theorem [ th : th elast one maybe ] involving @xmath328 requires a minimization in each iteration step . to derive a more efficient scheme , we consider the surrogate functional @xmath380 we have @xmath381 and propose the iterative algorithm [ algo:2 ] . in each iteration step , we minimize the surrogate functional in the first variable having the second one fixed with the previous iteration , which only requires a simple soft - thresholding .",
    "indeed , iterative soft - thresholding converges towards the fixed point @xmath339 :    [ th : very last ] suppose that the assumptions of theorem [ th : th elast one maybe ] are satisfied and let @xmath339 be the @xmath11-sparse fixed point in .",
    "we define @xmath382 and @xmath383 , where @xmath384 and @xmath157 sufficiently large .",
    "additionally assume that    a.   [ item : a ] there is @xmath385 such that , for all @xmath386-sparse vectors @xmath49 , @xmath387 b.   [ item : b ] the constants satisfy @xmath388 .    then by using @xmath389 as initial vector , the iterative algorithm [ algo:2 ] converges towards @xmath339 with @xmath390    note that the above @xmath11 is at most @xmath391 .",
    "also , it may be possible to choose @xmath343 a little bigger than necessary to ensure @xmath392 .",
    "condition ( b ) can then be satisfied when the magnitude of the data @xmath345 is sufficiently small",
    ". moreover , if constants are suitably chosen , example [ ex:2 ] also provides a map @xmath278 that satisfies the assumptions of theorem [ th : very last ] when @xmath365 is small .",
    "we use induction and observe that the case @xmath77 is trivially verified .",
    "next , we suppose that @xmath364 satisfies @xmath393 and that it has at most @xmath394 nonzero entries . our aim is now to verify that @xmath395 also satisfies the support condition and @xmath396 . to simplify notation let @xmath397 so that @xmath398 .",
    "it will be useful later to derive bounds for both terms @xmath399 and @xmath400 .",
    "therefore , we start to estimate @xmath401 where we have used in the form @xmath402 and the induction hypothesis .",
    "next , we take care of @xmath403 and derive @xmath404 by using and the minimizing property of @xmath339 , we derive @xmath405 the triangular inequality then yields @xmath406 .",
    "thus , we obtain the estimate @xmath407    according to and , the condition yields @xmath408 results in ( * ? ? ?",
    "* lemma 4.15 ) with @xmath409 imply that there is a constant @xmath157 such that @xmath410 where @xmath411 . since the above right - hand side is smaller than @xmath394 , we have the desired support estimate .    next , we take care of the error bounds . since @xmath339 is a fixed point of",
    ", we have @xmath412 , which we have already used in .",
    "the nonexpansiveness of @xmath356 yields with @xmath398 @xmath413 the same way as for , we use the bounds in and with to derive @xmath414 so that we can conclude the proof .",
    "it remains to verify that the output @xmath339 of the iterative soft - thresholding scheme is close to the minimizer @xmath309 of :    [ theorem : very last now ] suppose that the assumptions of theorem [ th : very last ] hold , that there is a @xmath394-sparse minimizer @xmath309 of , and that @xmath415 holds , then we have @xmath416 where @xmath59 satisfies @xmath417 and @xmath418 .",
    "note that proposition [ prop:1 ] yields that the minimizer @xmath309 can approximate @xmath59 , so that @xmath419 can become small and , hence , @xmath420 must be small .",
    "it should be mentioned though that the assumptions of theorem [ th : very last ] depend on @xmath313 because its magnitude steers the sparsity of @xmath339 .",
    "therefore , letting @xmath313 tend to zero is quite delicate because the assumptions become stronger . indeed , taking the limit requires that condition in theorem [ th : th elast one maybe ] holds for all @xmath49 , not just for sparse vectors , and the same is required for condition in theorem [ th : very last ] .",
    "we first bound @xmath309 by @xmath421 therefore , we have @xmath422 . since @xmath309 is @xmath394-sparse , we derive @xmath423 these computations and a zero addition imply @xmath424 we shall now bound both terms on the right - hand side separately . the minimizing property and in theorem [ th : th elast one maybe ] yield @xmath425 the second term is bounded by @xmath426 so that we can conclude the proof .",
    "alternatively , we can also bound the distance between @xmath339 and @xmath59 :    suppose that the assumptions of theorem [ th : very last ] hold , that we can replace @xmath339 in condition ( a ) of the latter theorem with some @xmath394-sparse @xmath1 satisfying @xmath417 , and that @xmath415 holds , then we have @xmath427    we can estimate @xmath428 from here on , some straight - forward calculations yield the required statement .      theorem [ th :",
    "very last ] provides a simple thresholding algorithm to compute the fixed point @xmath339 in that is more efficient than the recursive scheme in theorem [ th : th elast one maybe ] . to support theorem [ theorem : very last now ] , we shall check numerically that @xmath339 is indeed close to a minimizer @xmath309 of .",
    "the quasi - linear measurements are taken from the examples [ ex:1 ] and [ ex:2 ] .",
    "the recovery rates from iterative hard- and soft - thresholding are plotted in figure [ sub:2 ] and show a phase transition . for soft - thresholding ,",
    "this transition depends on both , the sparsity level @xmath11 and the measurement magnitude .",
    "those observations are consistent with the theoretical results in theorem [ th : very last ] und suggest that the original signal can be recovered by iterative soft - thresholding .",
    "we also use hard - thresholding but for comparable parameter choices the signal was only recovered when @xmath429 , cf .",
    "[ sub:1 ] .",
    "the assumptions of the theorems [ th : blumi ] and [ th : very last ] can not be satisfied within the phase retrieval setting , and the initial vectors @xmath430 in and in algorithm [ algo:2 ] , respectively , would lead to a sequence of zero vectors .",
    "we observed numerically , that other choices of initial vectors do not yield acceptable recovery rates either , so that we did not pursue this direction .",
    "martin ehler  acknowledges the financial support by the vienna science and technology fund ( wwtf ) through project vrg12 - 009 and the research career transition awards program eh 405/1 - 1/575910 of the national institutes of health and the german science foundation .",
    "massimo fornasier  is supported by the erc - starting grant for the project `` high - dimensional sparse optimal control '' .",
    "juliane sigl acknowledges the partial financial support of the start - project `` sparse approximation and optimization in high - dimensions '' and the hospitality of the johann radon institute for computational and applied mathematics , austrian academy of sciences , linz , during the early preparation of this work .",
    "height 2pt depth -1.6pt width 23pt , _ an iterative algorithm for nonlinear inverse problems with joint sparsity constraints in vector valued regimes and an application to color image inpainting _ ,",
    "inverse problems , 23 ( 2007 ) , pp .  18511870 .                  , _ introduction to the non - asymptotic analysis of random matrices _ , in compressed sensing , theory and applications , y.  eldar and g.  kutyniok , eds . , cambridge university press , 2012 , ch .  5 , pp ."
  ],
  "abstract_text": [
    "<S> inspired by significant real - life applications , in particular , sparse phase retrieval and sparse pulsation frequency detection in asteroseismology , we investigate a general framework for _ compressed sensing _ </S>",
    "<S> , where the measurements are _ quasi - linear_. we formulate natural generalizations of the well - known _ restricted isometry property _ ( rip ) towards nonlinear measurements , which allow us to prove both unique identifiability of sparse signals as well as the convergence of recovery algorithms to compute them efficiently . </S>",
    "<S> we show that for certain randomized quasi - linear measurements , including lipschitz perturbations of classical rip matrices and phase retrieval from random projections , the proposed restricted isometry properties hold with high probability . we analyze a generalized _ orthogonal least squares _ ( ols ) under the assumption that magnitudes of signal entries to be recovered decay fast . </S>",
    "<S> greed is good again , as we show that this algorithm performs efficiently in phase retrieval and asteroseismology . for situations where the decay assumption on the signal does not necessarily hold , </S>",
    "<S> we propose two alternative algorithms , which are natural generalizations of the well - known _ iterative hard and soft - thresholding_. while these algorithms are rarely successful for the mentioned applications , we show their strong recovery guarantees for quasi - linear measurements which are lipschitz perturbations of rip matrices .    </S>",
    "<S> compressed sensing , restricted isometry property , greedy algorithm , quasi - linear , iterative thresholding    94a20 , 47j25 , 15b52 </S>"
  ]
}