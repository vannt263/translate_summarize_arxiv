{
  "article_text": [
    "semi - supervised learning is a class of machine learning methods that stand in the middle ground between supervised learning in which all training data are labeled , and unsupervised learning in which no training data are labeled .",
    "specifically , in addition to the labeled training data @xmath0 , there exist unlabeled inputs @xmath1 . under certain assumptions on the geometric structure of the input data , such as the cluster assumption or the low - dimensional manifold assumption @xcite , the use of both labeled and unlabeled data can achieve better prediction accuracy than supervised learning that only uses labeled inputs @xmath0 .",
    "semi - supervised learning has become popular since the acquisition of unlabeled data is relatively inexpensive .",
    "a large number of methods were developed under the framework of semi - supervised learning .",
    "for example , @xcite proposed that the combination of labeled and unlabeled data will improve the prediction accuracy under the assumption of mixture models .",
    "the self - training method @xcite and the co - training method @xcite were soon applied to semi - supervised learning when mixture models are not assumed .",
    "@xcite described an approach to semi - supervised clustering based on hidden markov random fields ( hmrfs ) that can combine multiple approaches in a unified probabilistic framework .",
    "@xcite proposed a probabilistic framework for semi - supervised learning incorporating a k - means type hard partition clustering algorithm ( hmrf - kmeans ) .",
    "@xcite proposed the transductive support vector machines ( tsvms ) that used the idea of transductive learning by including unlabeled data in the computation of the margin .",
    "transductive learning is a variant of semi - supervised learning which focuses on the inference of the correct labels for the given unlabeled data other than the inference of the general rule .",
    "@xcite used a convex relaxation of the optimization problem called semi - definite programming as a different approaches to the tsvms .    in this article , we focus on a particular semi - supervised method ",
    "graph - based semi - supervised learning . in this method , the geometric structure of the input data are represented by a graph @xmath2 , where nodes @xmath3 represent the inputs and edges @xmath4 represent the similarities between them .",
    "the similarities are given in an @xmath5 by @xmath5 symmetric similarity matrix ( or called _ kernel _ matrix ) , @xmath6 $ ] , where @xmath7 .",
    "the larger @xmath8 implies that @xmath9 and @xmath10 are more similar .",
    "further , let @xmath11 be the responses of the labeled data .",
    "@xcite proposed the following graph - based learning method , @xmath12    subject to @xmath13 .",
    "its solution is called the estimated scores .",
    "the objective function ( named `` hard criterion '' thereafter ) , requires all the estimated score to be exactly the same as the responses for the labeled data .",
    "@xcite relaxed this requirement by proposing a soft version ( named `` soft criterion '' thereafter ) .",
    "we follow an equivalent form given in @xcite , @xmath14 the soft criterion belongs to the `` loss+penalty '' paradigm : it searches for the minimizer @xmath15 which achieves a small training error , and in the meanwhile imposes the smoothness on @xmath15 by a penalty based on similarity matrix . it can be easily seen that when @xmath16 the soft criterion is equivalent to the hard criterion .",
    "the tuning parameter @xmath17 being 0 in the soft criterion is understood in the following sense : the squared loss has infinite weight and thereby @xmath18 for all labeled data . but @xmath19 still plays a crucial role when it has no conflict with the hard constraints on the labeled data , that is , it provides links between @xmath20 s on the labeled and unlabeled data .",
    "therefore , the soft criterion at @xmath16 becomes the hard criterion .",
    "researchers have proposed different variants of graph - based learning methods , such as @xcite and @xcite .",
    "we only focus on and in this article",
    ".    the theoretical properties of graph - based learning have been studied in computer science and statistics literatures .",
    "@xcite derived the limit of the laplacian regularizer when the sample size of unlabeled data goes to infinity .",
    "@xcite considered the convergence of laplacian regularizer on riemannian manifolds .",
    "@xcite reinterpretted the graph laplacian as a measure of intrinsic distances between inputs on a manifold and reformulated the problem as a functional optimization in a reproducing kernel hilbert space .",
    "@xcite pointed out that the hard criterion can yield completely noninformative solution when the size of unlabeled data goes to infinity and labeled data are finite , that is , the solution can give a perfect fit on the labeled data but remains as 0 on the unlabeled data .",
    "@xcite obtained the asymptotic mean squared error of a different version of graph - based learning criterion .",
    "@xcite gave a bound of the generalization error for a slightly different version of .",
    "but to the best of our knowledge , no result is available in literature on a very fundamental question  the consistency of graph - based learning , which is the main focus of this article .",
    "specifically , we want to answer the question that under what conditions @xmath21 will converge to @xmath22 $ ] on unlabeled data , where @xmath22 $ ] is the true probability of a positive label given @xmath9 if responses are binary , and @xmath22 $ ] is the regression function on @xmath9 if responses are continuous .",
    "we will always call @xmath22 $ ] as regression function for simplicity .",
    "most of the literatures discussed above considered a `` functional version '' of and .",
    "they used a functional optimization problem with the optimizer @xmath23 being a function , as an approximation of the original problem with the optimizer @xmath15 being a vector . and",
    "they studied the behavior of the limit of graph laplacian and the solution @xmath23 .",
    "we do not adopt this framework but use a more direct approach .",
    "we focus on the original problem and study the relations of @xmath21 and @xmath22 $ ] under the general non - parametric setting .",
    "our approach essentially belongs to the framework of transductive learning , which focuses on the prediction on the given unlabeled data @xmath24 , not the general mapping from inputs to responses . by establishing a link between the optimizer of and the nadaraya - watson estimator @xcite for kernel regression",
    ", we will prove the consistency of the hard criterion .",
    "the theorem allows both @xmath25 and @xmath26 to grow . on the other hand ,",
    "we show that the soft criterion is inconsistent for sufficiently large @xmath17 . to the best of our knowledge ,",
    "this is the first result that explicitly distinguishes the hard criterion and the soft criterion of graph - based learning from a theoretical perspective and shows that they have very different asymptotic behaviors .",
    "the rest of the article is organized as follows . in section [ sec : main ] , we state the consistency result for the hard criterion and give the counterexample for the soft criterion .",
    "we prove the consistency result in section [ sec : proof ] .",
    "numerical studies in section [ sec : sim ] support our theoretical findings .",
    "section [ sec : summary ] concludes with a summary and discussion of future research directions .",
    "we begin with basic notation and setup .",
    "let @xmath27 be independently and identically distributed pairs . here each @xmath9 is a @xmath28-dimensional vector and @xmath29 are binary responses labeled as 1 and 0 ( the classification case ) or continuous responses ( the regression case ) .",
    "the last @xmath25 responses are unobserved .",
    "@xcite used a fixed point algorithm to solve the hard criterion , which is @xmath30 note that is not a closed - form solution but an updating formula for the iterative algorithm , since its right - hand side depends on unknown quantities .    in order to obtain a closed - form solution for ,",
    "we begin by solving the soft version and then let @xmath16 . recall that @xmath31 is the similarity matrix .",
    "let @xmath32 where @xmath33 , and @xmath34 being the unnormalized graph laplacian ( see @xcite for more details ) .",
    "soft criterion can be written in matrix form @xmath35 where @xmath36 .",
    "further , let @xmath37 be an @xmath5 by @xmath5 matrix defined as @xmath38 then by taking the derivative of with respect to @xmath39 and setting equal to zero , we obtain the solution as follows , @xmath40 what we are interested in are the estimated scores on the unlabeled data , i.e. @xmath41 . in order to obtain an explicit form for @xmath42",
    ", we use a formula for inverse of a block matrix ( see standard textbooks on matrix algebra such as @xcite for more details ) : for any non - singular square matrix @xmath43 @xmath44 @xmath45 write @xmath46 and @xmath31 as @xmath47 block matrices , @xmath48 by the formula above , @xmath49    letting @xmath16 , we obtain the solution for the hard criterion , @xmath50 @xcite obtained a similar formula for a slightly different objective function .    clearly , the form of is closely related to the nadaraya - watson estimator @xcite,@xcite for kernel regression , which is @xmath51 the nadaraya - watson estimator is well studied under the non - parametric framework .",
    "we can construct @xmath31 by a kernel function , that is , let @xmath52 , where @xmath53 is a nonnegative function on @xmath54 , and @xmath55 is a positive constant controlling the bandwidth of the kernel .",
    "let @xmath56 $ ] be the true regression function .",
    "the consistency of nadaraya - watson estimator was first proved by @xcite and @xcite .",
    "and many other researchers such as @xcite and @xcite studied its asymptotic properties under different assumptions . here",
    "we follow the result in @xcite .",
    "if @xmath57 , @xmath58 as @xmath59 , and @xmath53 satisfies :    * @xmath53 is bounded by @xmath60 ; * the support of @xmath53 is compact ; * @xmath61 for some @xmath62 and some closed ball @xmath63 centered at the origin and having positive radius @xmath64 ,    then @xmath65 converges to @xmath66 in probability for @xmath67 .    by establishing a connection between the solution of the hard criterion and nadaraya - watson estimator",
    ", we prove the following main theorem :    suppose that @xmath68 are independently and identically distributed with @xmath69 being bounded ; @xmath55 and @xmath53 satisfy the above conditions .",
    "further , let @xmath70 be the difference of two independent @xmath71 s , i.e. @xmath72 , with probability density function @xmath73 .",
    "assume that there exists @xmath74 , such that for any @xmath75 , @xmath76 then , for @xmath77 , @xmath78 given in converges to @xmath66 in probability , for @xmath67 .    the proof will be given in section [ sec : proof ] .",
    "theorem 1 established the consistency of the hard criterion under the standard non - parametric framework with two additional assumptions .",
    "firstly , both labeled data and unlabeled data are allowed to grow but the size of unlabeled data @xmath25 grows slower than the size of labeled data @xmath26 .",
    "we conjecture that when @xmath25 grows faster than @xmath26 , the graph - based semi - supervised learning may not be consistent based on the simulation studies in section [ sec : sim ] .",
    "@xcite also suggested that the method may not work when @xmath25 grows too fast .",
    "secondly , we assume that density function of the difference of two independent inputs is strictly positive near the origin , which is a mild technical condition valid for commonly used density functions .",
    "theorem 1 provides some surprising insights about the hard criterion of graph - based learning . at a first glance",
    ", the hard criterion makes an impractical assumption that requires the responses to be noiseless , while the soft criterion seems to be a more natural choice . but",
    "according to our theoretical analysis , the hard criterion is consistent under the standard non - parametric framework where the responses on training data are of course allowed to be random and noisy .",
    "we now consider the soft criterion with @xmath79 .",
    "suppose that @xmath68 are independently and identically distributed with @xmath80 .",
    "further , suppose that @xmath31 represents a connected graph .",
    "then for sufficiently large @xmath17 , the soft criterion is inconsistent .",
    "consider another extreme case of the soft criterion , @xmath81 .",
    "when @xmath31 represents a connected graph , the objective function becomes @xmath82    subject to @xmath83 .",
    "it is easy to check that the solution of , denoted by @xmath84 , is given by @xmath85 by the law of large numbers , @xmath86 \\,\\ , \\mbox{almost surely}.\\end{aligned}\\ ] ] clearly , @xmath87",
    "\\neq q(x_{n+a})$ ] since the right - hand side is a random variable .",
    "this implies that for sufficiently large @xmath17 , the soft criterion is inconsistent .",
    "we give the proof of theorem 1 in this section .",
    "recall that @xmath88 we first focus on @xmath89 .",
    "clearly , @xmath90 for any positive integer @xmath91 , define @xmath92 our goal is to prove that the limit of @xmath93 exists with probability approaching 1 , and thus we can have @xmath94 with probability approaching 1 @xcite .",
    "+ by definition , + @xmath95 @xmath96 + where @xmath97 for @xmath98 .",
    "thus we have    @xmath99    since @xmath57 , there exist @xmath100 , such that @xmath101 holds for every @xmath102 .",
    "thus by the assumption in , for @xmath103 and @xmath104 , @xmath105 where @xmath106 denotes the volume of a @xmath28-dimensional ball with radius @xmath55 , and @xmath107 is a constant independent with @xmath26 .",
    "since @xmath108 , the above inequality implies @xmath109 . on the other side , @xmath110 since @xmath57 .",
    "further , @xmath111 by chebyshev s inequality , for any @xmath112 , since @xmath58 , @xmath113 this further implies @xmath114    we now continue to study the property of @xmath115 .",
    "consider each element @xmath116 of this matrix .",
    "for @xmath117 , @xmath118 by condition ( i ) and ( iii ) .",
    "for simplicity of notation , let @xmath119 where @xmath120 is a nonnegative function depending on @xmath26 .",
    "thus @xmath121 is an upper bound of every element in the matrix @xmath115 . by",
    ", we have @xmath122 which implies @xmath123 and @xmath124 since @xmath125 , we have @xmath126 where @xmath127 . note that @xmath128 is a constant independent with @xmath26 and @xmath25 .    for the sake of simplicity ,",
    "we say a matrix @xmath43 has _ tiny elements _ , if @xmath129 with probability approaching 1 , where @xmath130 .",
    "and @xmath131 denotes the @xmath132-th row of @xmath43 .",
    "then @xmath115 has tiny elements by .",
    "moreover , @xmath133 holds with probability approaching 1 . by induction , @xmath134 with probability approaching 1 .",
    "therefore , @xmath135 @xmath136 thus @xmath137 exists with probability approaching 1 since @xmath138 , and @xmath139 also has tiny elements",
    ". therefore , @xmath140 with probability approaching 1 .",
    "we now go back to the solution of the hard criterion of graph - based semi - supervised learning , @xmath141 with probability approaching 1 . for @xmath142",
    ", @xmath143 equals to the @xmath144th row of @xmath145 , i.e. , @xmath146 with probability approaching 1 .    by assumption ,",
    "@xmath69 s are bounded . without loss of generality ,",
    "assume @xmath147 . for @xmath142 , define @xmath148 we have @xmath149 with probability approaching 1 as @xmath59 .",
    "this implies @xmath150 since for any @xmath151 we can find @xmath152 such that @xmath153 and @xmath154    finally , for each @xmath142 , @xmath155 since @xmath139 has tiny elements , @xmath156 which implies @xmath157 in probability .",
    "the theorem then holds by the consistency of nadaraya - watson estimator .",
    "in this section , we compare the performance of the hard criterion and the soft criterion with different tuning parameters under a linear and non - linear model .    the inputs @xmath158 are generated independently from a truncated multivariate normal distribution .",
    "specifically , let @xmath159 follow a @xmath160-dimensional multivariate normal with the mean @xmath161 , and the variance - covariance matrix @xmath162 we set @xmath163 .",
    "for @xmath164 and @xmath165 , let @xmath166 if @xmath167 $ ] and @xmath168 otherwise , where @xmath169 and @xmath170 are the @xmath171-th component of @xmath9 and @xmath159 , respectively .",
    "let @xmath31 be the gaussian radial basis function ( rbf ) kernel , that is , @xmath172 where @xmath173 .",
    "note that @xmath31 has compact support since @xmath9 s are truncated , and the choice of @xmath55 satisfies the condition in theorem 1 .",
    "we consider two models in simulation studies . in model 1 , the responses @xmath69 s follow a logistic regression with @xmath174 for @xmath175 . model 2 uses a non - linear logit function , @xmath176 for @xmath175 .",
    "we compare the performance of graph - based learning methods with four different tuning parameters , @xmath177 and 5 .",
    "the performance is measured by the root mean squared error ( rmse ) on the unlabeled data , that is , @xmath178 each simulation is repeated 1000 times and the average rmses are reported .",
    "figure 1 shows the rmses under model 1 when the sample size of unlabeled data @xmath25 is fixed as 30 and the sample size of labeled data @xmath179 , 30 , 50 , 100 , 200 , 300 , 500 , 800 , 1000 and 1500 . as @xmath26 increases , the rmses of all methods decrease as expected .",
    "more importantly , the rmse increases as @xmath17 increases .",
    "in particular , the hard criterion always outperforms the soft criterion , which is consistent with our theoretical results .",
    "figure 2 shows the rmses under model 1 when @xmath26 is fixed as 100 and @xmath180 , 60 , 100 , 300 , 500 and 1000 . as before",
    ", the rmse always increases as @xmath17 increases .",
    "moreover , the rmses of all methods increase as @xmath25 increases , which suggests that the hard criterion may not be consistent when @xmath25 grows faster than @xmath26 . for the non - linear logit function ,",
    "figure 3 and 4 show the same patterns as in figure 1 and 2 , respectively , which also support our theoretical results .",
    "in this article , we proved the consistency of graph - based semi - supervised learning when the tuning parameter of the graph laplacian is zero ( the hard criterion ) and showed that the method can be inconsistent when the tuning parameter is nonzero ( the soft criterion ) . moreover , the numerical studies also suggest that the hard criterion outperforms the soft criterion in terms of the rmse .",
    "these results provide a better understanding about the statistical properties of graph - based semi - supervised learning .",
    "of course , the accuracy of prediction can be measured by other indicators such as the area under the receiver operating characteristic curve ( auc ) .",
    "the hard criterion may not always be the best choice in term of these indicators .",
    "further theoretical properties such as rank consistency will be explored in future research .",
    "moreover , we would also like to investigate the behavior of these methods when the unlabeled data grow faster than the label data ."
  ],
  "abstract_text": [
    "<S> graph - based semi - supervised learning is one of the most popular methods in machine learning . </S>",
    "<S> some of its theoretical properties such as bounds for the generalization error and the convergence of the graph laplacian regularizer have been studied in computer science and statistics literatures . however , a fundamental statistical property , the consistency of the estimator from this method has not been proved . in this article , we study the consistency problem under a non - parametric framework . </S>",
    "<S> we prove the consistency of graph - based learning in the case that the estimated scores are enforced to be equal to the observed responses for the labeled data . </S>",
    "<S> the sample sizes of both labeled and unlabeled data are allowed to grow in this result . when the estimated scores are not required to be equal to the observed responses , </S>",
    "<S> a tuning parameter is used to balance the loss function and the graph laplacian regularizer . </S>",
    "<S> we give a counterexample demonstrating that the estimator for this case can be inconsistent . </S>",
    "<S> the theoretical findings are supported by numerical studies . </S>"
  ]
}