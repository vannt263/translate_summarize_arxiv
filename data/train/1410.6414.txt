{
  "article_text": [
    "many application tasks can be formalized as matching between objects in two heterogeneous domains , in which the association between some objects and information on those objects are given .",
    "we refer to the objects from one domain as queries and those from the other as targets , with the distinction usually clear from the context . for example , in collaborative filtering , given some items , one manages to find the users who have best match to the items , by using the preference of some users on some items as well as the features of users and items .",
    "another example is image tagging , in which one wants to associate tags ( keywords ) with images based on some tagged images as well as the features of tags and images .",
    "recent years have observed a great success of employing machine learning techniques , referred to as learning - to - match in this paper , to solve the matching problems .    among existing approaches , a family of factorization models that make use of feature spaces to encode additional information , stand out as state - of - the - art in matching tasks .",
    "examples include factorization machines  @xcite , feature - based latent factor models for link prediction  @xcite , and regression - based latent factor models  @xcite .",
    "we refer to this class of methods as feature - based matrix factorization ( fmf ) in this paper .",
    "the basic idea of fmf is to formalize the task as extension to plain matrix factorization for incorporating the features of objects into the model . in this way",
    ", one can make a full use of available information in the task to improve the accuracies .",
    "in fact , fmf is the best performer on many real world matching tasks . in collaborative filtering ,",
    "fmf models using user feedback  @xcite , attribute  @xcite , and content  @xcite have outperformed other models including plain matrix factorization . in web search ,",
    "fmf models for calculating matching scores ( relevance ) between queries and documents have significantly enhanced relevance ranking  @xcite .",
    "fmf models have also been successfully employed in link prediction  @xcite , and have been adopted by the champion teams in kdd cup 2012  @xcite .",
    "the learning of the fmf model can be conducted with a coordinate descent algorithm or a stochastic gradient descent algorithm .",
    "since a matching problem is usually of a very large scale , with hundreds of millions of objects and features or more , it can easily become hard for fmf to manage .",
    "it is therefore necessary to develop a parallel and efficient algorithm for fmf .",
    "this is exactly the problem we attempt to address in this paper .",
    "making fmf scalable and efficient is much more difficult than it appears , due to the following two challenges .",
    "first , training requires simultaneous access to all the features , and thus the existing techniques for parallelization of matrix factorization  @xcite are not directly applicable .",
    "second , the computation complexity of the coordinate descent algorithm is still too high , and it can easily fail to run on a single machine when the scale of problem becomes large , calling for techniques to significantly accelerate the computation . by making use of repeating patterns , the least - squares and probit losses can be scaled up for coordinate descent  @xcite , but it does not provide guarantee for any general convex loss functions",
    ". existed parallel coordinate descent algorithms , such as  @xcite  and  @xcite , due to the complex feature dependencies , can not be directly applied here . the hogwild !",
    "@xcite algorithm for parallel stochastic gradient descent can be applied here , but it is a generic algorithm and thus is still inefficient for fmf .    in this paper",
    ", we try to tackle the two challenges by developing a parallel and efficient algorithm tailored for learning - to - match .",
    "the algorithm , referred to as parallel and efficient algorithm for learning - to - match ( pl2 m ) , parallelizes and accelerates the coordinate descent algorithm through ( 1 ) iteratively relaxing the objective to facilitate parallel updates of parameters , and ( 2 ) avoiding repeated calculations caused by features .",
    "the main contributions of this paper are as follows .",
    "* we propose the parallel and efficient algorithm for feature - based matrix factorization , which iteratively relaxes the objective for parallel updates of parameters , and neatly avoids repeated calculations caused by features , for any general convex loss functions . * we theoretically prove the convergence of the proposed algorithms on minimizing the original objective function , which is further verified by our extensive experiments .",
    "the parallel algorithm can automatically adjust the rate of parallel updates according to the conditions in learning .",
    "* we empirically demonstrate the effectiveness and efficiency of the proposed algorithm on four benchmark datasets .",
    "the parallel algorithm achieves nearly linear speedup and the proposed acceleration helps the parallel algorithm run about @xmath0 times faster than the hogwild !",
    "@xcite algorithm on average , using @xmath1 threads .",
    "given the importance of the fmf models and difficulty of their parallelization , the work in this paper represents a significant contribution to the study of learning to match .",
    "to our best knowledge , this is the first effort on the scalability of the general fmf models .",
    "the rest of the paper is organized as follows .",
    "section  [ sec : l2 m ] gives a formal description of the generalized matrix factorization and section  [ sec : ecd ] explains the efficient coordinate descent algorithm .",
    "section  [ sec : parallel - update ] describes parallelization of the coordinate descent algorithm .",
    "related work is introduced in section  [ sec : rel ] .",
    "experimental results are provided in section  [ sec : exp ] . finally , the paper is concluded in section  [ sec : con ] .",
    "in this section , we give a formal definition of learning - to - match and a formulation of a feature - based matrix factorization .",
    "we also present our motivation of parallelizing this learning task .",
    "learning - to - match can be formally defined as follows .",
    "let @xmath2 $ ] be the instances in the query domain and @xmath3 $ ] be the instances in the target domain , where @xmath4 and @xmath5 are query and target instances ( feature vectors ) respectively . for some query - target pairs ,",
    "the corresponding matching scores @xmath6 are given as training data , where @xmath7 is the set of indices for all observed query - target pairs .",
    "our problem is to learn to predict the matching score @xmath8 between any pair of query @xmath9 and target @xmath10 .",
    "the setting is rather general and it subsumes many application problems . for example , in collaborative filtering , a user s preference over an item can be interpreted as the matching score between the user and the item . in social link prediction",
    ", the likelihood of link between nodes on the network can be as regarded as the matching score between the nodes .",
    "web search , in general document retrieval , can also be formalized as a problem of first matching between a given query and documents and then ranking of documents based on the matching scores .",
    "the goal of learning - to - match is to make accurate prediction by effectively using the information on the given relations between instances ( e.g. , similar users may prefer similar items ) , as well as the information on the features of instances ( e.g. , users may prefer items with similar properties )",
    ".          the query and target instances ( feature vectors ) are in two heterogeneous feature spaces , and a direct match between them is generally impossible .",
    "instead , we map the feature vectors in the two domains into a latent space and perform matching on the images of the feature vectors in the latent space .",
    "we calculate the matching score of a query - target pair as @xmath11 where @xmath12 and @xmath13 are transformation matrices that map feature vectors from the feature spaces into the latent space . @xmath14 and @xmath15 are latent factors of query instance @xmath9 and target instance @xmath10 . in this paper , we use @xmath14 to denote the @xmath9th column of matrix @xmath16 and @xmath15 to denote the @xmath10th column of matrix @xmath17 .",
    "we refer to the model in equation  ( [ eq : model ] ) as the model of feature - based matrix factorization .",
    "the model can be also interpreted as linear matching function of latent factors , in which the latent factor of each instance is also linearly constructed from feature vectors .",
    "the query latent factor @xmath14 can be expressed as @xmath18 .",
    "the target latent factor @xmath15 can be expressed similarly .",
    "the model , as shown in figure  [ fig : fmf ] , contains many existing models of feature - based matrix factorization as special cases  @xcite . when no  informative \" features are available for objects of both domains , the feature matrices contain only the indices of the objects .",
    "clearly , in such cases @xmath19 and @xmath20 become identity matrices of sizes @xmath21 and @xmath22 , the feature - based matrix factorization model naturally degenerates to the plain matrix factorization  @xcite .",
    "the objective of the learning task then becomes   and @xmath17 are not model parameters ; they are only auxiliary variables determined by @xmath23 and @xmath24 . ] : @xmath25    here @xmath26 is a strongly convex loss function that measures the difference between the prediction @xmath27 and the ground truth @xmath28",
    ". the loss function @xmath29 can be square loss for regression @xmath30 or logistic loss for classification @xmath31 where @xmath32 $ ] .",
    "@xmath33 is a regularization term based on elastic net  @xcite which includes @xmath34 ( ridge ) and @xmath35 ( lasso ) regularization as its special cases . @xmath36",
    "@xmath37 @xmath38 , @xmath39 , @xmath40 \\{calculate @xmath16 , @xmath17 , @xmath41 } +      the use of features in learning - to - match is crucial for the accuracy of the task .",
    "usually an fmf model , when properly optimized , can produce a higher accuracy in prediction than a model of plan matrix factorization ( mf ) .",
    "this is because the fmf model can leverage more information for the prediction , particularly the feature information , while the mf model can only rely on relations between instances which are usually very sparse .",
    "for example , in the task of recommendation  @xcite , only about @xmath42 entries are observed . in tencent",
    "weibo dataset at kdd cup 2012 , about @xmath43 of users in the test set have no following records in the training set  @xcite .",
    "as a result , mf can not achieve satisfactory results in the tasks , while fmf models give the best results .",
    "in fact , it has been observed the fmf models ( with different types of features used ) achieve state - of - the - art results on many different tasks , outperforming the models of mf with big margin .",
    "for example , in collaborative filtering , user feedback ( svd++ )  @xcite , user attribute  @xcite , and product attribute  @xcite are incorporated into models to further improve the accuracies in prediction . in web search",
    "@xcite , term vectors of queries and documents are used as features to significantly improve relevance ranking .",
    "fmf models also give the best results in link prediction in kdd cup 2012  @xcite .",
    "@xmath37 + @xmath38 , @xmath39 , @xmath40 \\{calculate @xmath16 , @xmath17 , @xmath41 } +      the success of the fmf models strongly indicates the necessity of scaling up the corresponding learning algorithms , given that the existing algorithms still can not easily handle large datasets . by making use of repeating patterns , the least - squares and probit losses",
    "can be scaled up for coordinate descent  @xcite , but it does not guarantee for any general convex loss functions .",
    "other algorithms of parallel coordinate descent , such as  @xcite  and  @xcite can not be directly applied to fmf , because it is difficult for them to handle complex feature dependencies in fmf .",
    "the hogwild !  @xcite algorithm for parallel stochastic gradient descent can be applied here , but it is a generic algorithm and thus is still inefficient for fmf . to our best knowledge , our work in this paper is the first effort on scalability of learning - to - match , i.e. , feature - based matrix factorization .",
    "in this section , we propose an acceleration of the coordinate descent algorithm for solving the feature - based matrix factorization problem .",
    "we prove the convergence of the accelerated algorithm , and we also give its time complexity at section  [ sec : timecomplextiy ] .",
    "let @xmath44 be the gradient of each instance over prediction and @xmath45 be constant of @xmath29 , such that , @xmath46 that is , @xmath47 .",
    "we can exploit the standard technique to learn the model using coordinate descent ( cd ) , as shown in algorithm  [ alg : cd - old ] . here",
    ", @xmath48 is defined using the following thresholding function to handle optimization of @xmath35 norm @xmath49    the regularization term only affects the result through function @xmath48 , and thus makes most part of the algorithm independent of regularization .",
    "note that we implicitly assume @xmath27 is buffered and kept up to date , when @xmath50 is needed in the algorithm .",
    "the time complexity of one update in algorithm  [ alg : cd - old ] is @xmath51 , where @xmath52 and @xmath53 denote numbers of nonzero entries in feature matrices @xmath19 and @xmath20 , @xmath54 and @xmath55 denote numbers of query and target instances , and @xmath56 denotes average number of nonzero features for each pair @xmath57 .",
    "we note that the time complexity is the same as the time complexity of stochastic gradient optimization for equation  ( [ eq : obj ] ) . from the analysis",
    ", we can see that the time complexity of algorithm is increased by order of @xmath58 when average number of nonzero features increases .",
    "this can greatly hamper the learning of matching model , when a large number of features are used .",
    "we give an efficient algorithm for learning - to - match by avoiding the repeated calculations caused by features .",
    "there is only a little works focused on the acceleration of the fmf models .",
    "the most relevant one is that scaling up some specific coordinate descent by making use of repeating patterns of features  @xcite .",
    "however , it is specialized for the least - squares and probit losses .",
    "although the idea is similar to the avoiding repeated calculations in our work , we extend the idea to any general convex loss functions .",
    "one can see that there exist repeated calculations of summations for the same query or target when calculating @xmath59 and @xmath60 in algorithm  [ alg : cd - old ] , which gives us a chance to speed up the algorithm .",
    "we introduce two auxiliary variables @xmath61 and @xmath62 calculated by @xmath63 where @xmath64 is the set of observed target instances associated with query instance @xmath9 .",
    "the key idea of efficient cd is to make use of @xmath61 and @xmath62 to save duplicated summations in algorithm  [ alg : cd - old ] . since the gradient value @xmath50 is changed after each update , it is not trivial to let @xmath61 unchanged .",
    "our algorithm keeps making @xmath61 updated to ensure the convergence of the algorithm . the efficient algorithm for learning - to - match",
    "is shown in algorithm  [ alg : cd - group ] .",
    "next , we prove the convergence of algorithm  [ alg : cd - group ] , which greatly reduces the time complexity of learning .",
    "suppose that the @xmath65th row of @xmath23 is changed by @xmath66 .",
    "after the change , the loss function can be bounded by @xmath67 intuitively , updating @xmath61 corresponds to minimizing the quadratic upper bound @xmath68 of the original convex loss which is re - estimated each round .",
    "formally , all the values of @xmath66 are zero in the beginning .",
    "we need to sequentially update @xmath69 for different @xmath70 s to minimize @xmath68 . assuming that we have already updated @xmath71",
    "@xmath72 @xmath73 and need to decide @xmath74 , we can calculate the upper bound @xmath75 as follows @xmath76 the first order term of this equation is exactly the update rule in algorithm  [ alg : cd - group ] . using @xmath77 to denote the change on the @xmath65th row after carrying out the update , we arrive at the following inequality @xmath78",
    "note that we start from @xmath79 , and we have @xmath80 after the update .",
    "the inequality in equation  ( [ eq:17 ] ) shows that the original loss function decreases after each round of update , and hence this proves the convergence of algorithm  [ alg : cd - group ] for any differentiable convex loss function .",
    "we propose a parallel and efficient learning - to - match algorithm ( pl2 m ) to further improve the scalability and efficiency by deriving an adaptive estimation of the conflicts caused by parallel updates .",
    "specifically , we consider parallelizing and accelerating algorithm  [ alg : cd - group ] . the statistics calculation and preprocessing steps in algorithm  [ alg : cd - group ] can be naturally separated into several independent tasks and thus fully parallelized .",
    "however , there is strong dependency within @xmath23 update steps , making the parallelization of it a difficult task . we will discuss how we solve the problem next .",
    "let @xmath81 be a set of feature indices to be updated in parallel .",
    "assume that the statistics of @xmath61 is up to date as in algorithm  [ alg : cd - group ] and we want to change @xmath69 for @xmath82 in parallel . for simplicity of notation , we use @xmath66 to represent the change in @xmath83 .",
    "the value of @xmath75 after this change will be @xmath84 in a specific case in which @xmath85 for @xmath86 , the third line in equation  ( [ eq : boundpupdate ] ) becomes zero .",
    "this means that the features in the selected set do not appear in the same instance . in such case",
    ", the loss can be separated into @xmath87 independent parts and the original update rule can be applied in parallel . not surprisingly , such condition does not hold in many real world scenarios . we need to remove these troublesome cross terms in the second line , by deriving an adaptive estimation of the conflicts caused by parallel updates , more specifically , by the inequality : @xmath88 with the inequality , we can bound @xmath75 as follows @xmath89    @xmath37 + @xmath38 , @xmath39 , @xmath40 \\{calculate @xmath16 , @xmath17 , @xmath41 } +    obviously this new upper bound @xmath90 can be separated into @xmath87 independent parts and optimized in parallel .",
    "moreover , the sum @xmath91 is common for all features in set @xmath81 and is only needed to be calculated once . with this result , we give a parallel and efficient algorithm for learning - to - match , shown in algorithm  [ alg : pcd ] .",
    "the relaxation of @xmath92 into @xmath90 is performed iteratively in the optimization , and it still attempts to optimize the original objective as in equation  ( [ eq : obj ] ) , which is a case much analogous to expectation - maximization algorithm in finding a maximum - likelihood solution .",
    "let @xmath77 be the change in @xmath83 after each parallel update .",
    "since each parallel update optimizes @xmath90 , we have the following inequality @xmath93 it indicates that @xmath75 decreases after each parallel update .",
    "it then follows that the parallel procedure for optimizing the original loss function in algorithm  [ alg : pcd ] always converges .",
    "the update rule depends on the statistics @xmath94 . with the following notation @xmath95",
    "it can be shown that the parallel update of @xmath96 is shrunken by @xmath97 compared to sequential update .",
    "intuitively @xmath97 depends on the co - occurrence between features @xmath98 .",
    "when features in @xmath81 rarely co - occur , @xmath97 will be close to one , which means that we can update `` aggressively '' .",
    "when features in @xmath81 co - occur frequently , @xmath97 will get small and we need to update more `` conservatively '' . in an extreme case in which no feature co - occurs with each other , @xmath99 and",
    "we get perfect parallelization without any loss of update efficiency . in another extreme case in which we have",
    "@xmath87 duplicated features ( @xmath100 ) , @xmath101 , which is extremely conservative given the size of @xmath81 .",
    "the advantage of our algorithm is that it _ automatically adjusts _ its `` level of conservativeness '' by the condition in learning , and thus it always ensures the convergence of the algorithm regardless of the number of threads and the nature of dataset .",
    "the changes in loss function can be analyzed accordingly .",
    "let us consider the simple case in which @xmath102 and only @xmath34 regularization is involved .",
    "the change of loss after parallel update can be bounded by @xmath103 as this inequality indicates , compared to the ideal case in which features do not co - occur , each parallel update s contribution to the loss change is scaled by @xmath97 .",
    "the above analysis also intuitively justifies that @xmath97 controls the efficiency of the update .",
    "the time complexity of the efficient algorithm ( algorithm  [ alg : cd - group ] ) is only of @xmath104 .",
    "it is linear to numbers of nonzero entries of feature matrices and number of observed entries of @xmath105 .",
    "recall the time complexity of the coordinate descent algorithm ( algorithm  [ alg : cd - old ] ) , which is @xmath51 .",
    "the _ speedup _ on @xmath23 updates in algorithm  [ alg : cd - group ] is as follows : @xmath106 this corresponds to average number of observed target instances per query instance .",
    "similarly , on the @xmath24 updates , the _ speedup _ is about @xmath107 times .",
    "therefore , the overall _ speedup _ of algorithm  [ alg : cd - group ] over algorithm  [ alg : cd - old ] is at least , @xmath108    in application tasks , this can be at level of @xmath109 to @xmath110 such as collaborative filtering and link prediction . when @xmath111 is close to  ( or smaller than ) @xmath112  ( datasets like yahoo !  music , tencent weibo and movielens-10 m ) , our algorithm runs as fast as the algorithm of plain matrix factorization even though it uses extra features .    for the complexity of the parallel and efficient",
    "learning - to - match algorithm ( pl2 m ) described in algorithm  [ alg : pcd ] , using @xmath113 threads to run the algorithm , the computation cost for one round update is @xmath114 .",
    "it is due to the fact that all parts of the algorithm are parallelized .",
    "this analysis does not consider the synchronization cost .",
    "in real world settings , we need to take synchronization cost into consideration , the corresponding time complexity becomes @xmath115 , where @xmath116 denotes variance of computation costs by parallel tasks .",
    "assume that we have @xmath113 tasks and the time costs of the tasks are @xmath117 .",
    "we define @xmath118 , since the training is delayed by the slowest task . to achieve maximum speedup",
    ", we need to schedule the tasks well such that the load of each task is average , which is always feasible when @xmath87 , @xmath55 , and @xmath54 are large .",
    "therefore , our algorithm can gain almost @xmath113 times speedup .    in real world applications ,",
    "there is a trade - off between the size of _ parallel coordinate set _ @xmath87 and the parameter @xmath116 , especially when different features have different levels of sparsity in the dataset .",
    "when we increase the size of _ parallel coordinate set _ @xmath87 , we can divide the task into @xmath113 threads in a more balanced way . on the other hand",
    ", @xmath97 will decrease as we increase @xmath87 , making the update more conservative .",
    "thus a parallel coordinate set needs to be chosen to balance convergence and acceleration .",
    "in fact , we need to empirically choose @xmath81 such that each instance is covered by only a few nonzero features and the task size is large enough to run in a fairly balanced way .    in this paper , we fix @xmath87 and randomly partition elements from the feature indices to generate a set of disjoint subsets @xmath81 in each round .",
    "we note that there can be more sophisticated scheduling strategies to select @xmath81 , which is beyond the scope of this paper and can be an interesting topic for future research .    [",
    "cols=\"<,^,^,^,^,<,<\",options=\"header \" , ]      finally , we evaluate the scalability of the parallel learning - to - match algorithm ( pl2 m ) .",
    "we test the average running time of pl2m-5k on yahoo !",
    "music dataset , pl2m-500 on tencent weibo dataset , pl2m-50 on flickr dataset , pl2m-50 on movielens-10 m with varying numbers of threads and evaluate the improvement in efficiency .",
    "as shown in figure  [ fig : speedup ] , the speedup curves are similar on yahoo !  music , tencent weibo , and flickr datasets , but the curve converges earlier on the movielens-10 m dataset .",
    "this is because that movielens-10 m is relatively smaller than the others and pl2m-50 runs really fast on movielens-10 m , which only needs about 18 seconds when 8 threads are used .",
    "although the speedup gained by parallelization is not as much as that on other datasets , the parallel algorithm can also provide accelerations .    on the first 3 datasets , pl2 m can achieve almost linear speedup with less than @xmath1 threads , but the speedup gain slows down with more threads",
    "we observe that the working threads are still fully occupied with more than 8 .",
    "we conjecture that this turning point is due to the fact that the number of physical cores of the machine is only 8 . from the results",
    ", we can find that pl2 m is able to gain about @xmath119 times speedup using @xmath109 threads , confirming the scalability of the parallel algorithm .    in summary ,",
    "the speedup gained by the parallel algorithm is significant , and thus it can easily handle hundreds millions of instances and features on a single machine .",
    "we have proposed a parallel and efficient algorithm for learning - to - match , more specifically feature - based matrix factorization , a general and state - of - the - art approach .",
    "our algorithm employs ( 1 ) iterative relaxations to solve the conflicts caused by parallel updates , with provable convergence guarantee on minimizing the original objective function , and ( 2 ) accelerate the computation by avoiding the repeated calculations caused by features , for any general convex loss functions . as a result",
    ", our algorithm can easily handle data with hundreds of millions of objects and features on a single machine .",
    "extensive experimental results show that our algorithm is both effective and efficient when compared to the baselines .    as future work",
    ", we plan to ( 1 ) extend the algorithm to a distributed setting instead of the current multi - threading , ( 2 ) find better scheduling strategies for making parallel updates with a guaranteed bound of speedup , and ( 3 ) apply the technique developed in this paper to the parallelization of other learning methods , such as markov chain monte carlo  ( mcmc ) learning methods for learning - to - match problem .",
    "j.  bradley , a.  kyrola , d.  bickson , and c.  guestrin .",
    "parallel coordinate descent for l1-regularized loss minimization . in l.",
    "getoor and t.  scheffer , editors , _ proceedings of the 28th international conference on machine learning _ , icml 11 , pages 321328 , new york , ny , usa , june 2011 .",
    "acm .",
    "a.  s. das , m.  datar , a.  garg , and s.  rajaram .",
    "google news personalization : scalable online collaborative filtering . in _ proceedings of the 16th international conference on world wide web _ , www 07 , pages 271280 , new york , ny , usa , 2007 .",
    "acm .",
    "c.  liu , h .- c .",
    "yang , j.  fan , l .- w .",
    "he , and y .- m .",
    "distributed nonnegative matrix factorization for web - scale dyadic data analysis on mapreduce . in _ proceedings of the 19th international conference on world wide web _ , www 10 , pages 681690 , new york , ny , usa , 2010 .",
    "acm .",
    "b.  recht , c.  re , s.  wright , and f.  niu .",
    "hogwild : a lock - free approach to parallelizing stochastic gradient descent . in j.  shawe - taylor , r.  zemel , p.  bartlett , f.  pereira , and k.  weinberger , editors , _ advances in neural information processing systems 24 _ , pages 693701 .",
    "2011 .          c.  scherrer , m.  halappanavar , a.  tewari , and d.  haglin .",
    "scaling up coordinate descent algorithms for large @xmath120 regularization problems .",
    "in icml 12 , pages 14071414 , new york , ny , usa , july 2012 . omnipress .    c.  scherrer , a.  tewari , m.  halappanavar , and d.  haglin .",
    "feature clustering for accelerating parallel coordinate descent . in p.",
    "bartlett , f.  pereira , c.  burges , l.  bottou , and k.  weinberger , editors , _ advances in neural information processing systems 25 _ , pages 2836 .",
    "2012 .",
    "d.  h. stern , r.  herbrich , and t.  graepel .",
    "matchbox : large scale online bayesian recommendations . in _ proceedings of the 18th international conference on world wide web _ , www 09 , pages 111120 , new york , ny , usa , 2009 .",
    "j.  weston , c.  wang , r.  weiss , and a.  berenzweig . latent collaborative retrieval . in j.  langford and j.  pineau , editors , _ proceedings of the 29th international conference on machine learning _ , icml 12 , pages 916 , new york , ny , usa , july 2012 . omnipress .",
    "w.  wu , h.  li , and j.  xu .",
    "learning query and document similarities from click - through bipartite graph with metadata . in",
    "_ proceedings of the sixth acm international conference on web search and data mining _ , wsdm 13 , pages 687696 , new york , ny , usa , 2013 .",
    "yu , c .- j .",
    "hsieh , s.  si , and i.  dhillon .",
    "scalable coordinate descent approaches to parallel matrix factorization for recommender systems .",
    "icdm 12 , pages 765774 , washington , dc , usa , 2012 .",
    "ieee computer society .",
    "y.  zhou , d.  wilkinson , r.  schreiber , and r.  pan .",
    "large - scale parallel collaborative filtering for the netflix prize . in _ proceedings of the 4th international conference on algorithmic aspects in information and management _ , aaim 08 , pages 337348 , berlin , heidelberg , 2008 .",
    "springer - verlag ."
  ],
  "abstract_text": [
    "<S> many tasks in data mining and related fields can be formalized as matching between objects in two heterogeneous domains , including collaborative filtering , link prediction , image tagging , and web search . </S>",
    "<S> machine learning techniques , referred to as learning - to - match in this paper , have been successfully applied to the problems . among them , a class of state - of - the - art methods , named feature - based matrix factorization , formalize the task as an extension to matrix factorization by incorporating auxiliary features into the model . </S>",
    "<S> unfortunately , making those algorithms scale to real world problems is challenging , and simple parallelization strategies fail due to the complex cross talking patterns between sub - tasks . in this paper </S>",
    "<S> , we tackle this challenge with a novel parallel and efficient algorithm for feature - based matrix factorization . our algorithm , based on coordinate descent , can easily handle hundreds of millions of instances and features on a single machine . </S>",
    "<S> the key recipe of this algorithm is an iterative relaxation of the objective to facilitate parallel updates of parameters , with guaranteed convergence on minimizing the original objective function . </S>",
    "<S> experimental results demonstrate that the proposed method is effective on a wide range of matching problems , with efficiency significantly improved upon the baselines while accuracy retained unchanged . </S>"
  ]
}