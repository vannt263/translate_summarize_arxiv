{
  "article_text": [
    "the set of @xmath0 matrices @xmath1 for which @xmath2 is called the @xmath0 stiefel manifold and is denoted @xmath3 .",
    "the densities of a quadratic exponential family on this manifold ( with respect to the uniform measure ) are given by @xmath4 where @xmath5 , @xmath6 is an @xmath7 diagonal matrix and @xmath8 is a symmetric matrix .",
    "since @xmath9 , the density is unchanged under transformations of the form @xmath10 or @xmath11 .",
    "additionally , it is convenient to restrict the diagonal entries of @xmath6 to be in decreasing order . if @xmath6 is not ordered in this way , there exists a reparameterization @xmath12 giving the same distribution as @xmath13 but where @xmath14 has ordered diagonal entries .",
    "more details on the stiefel manifold and these distributions can be found in @xcite , @xcite , @xcite and the references therein .",
    "distributions of this form were originally studied in the case @xmath15 , so that the manifold was just the surface of the @xmath16-sphere . in this case",
    ", @xmath6 reduces to a scalar and can be absorbed into the matrix @xmath8 .",
    "the quadratic exponential family then has densities of the form @xmath17 the case that @xmath18 was studied by von mises , fisher and langevin , and so a distribution with density proportional to @xmath19 is often called a von mises - fisher or langevin distribution on the sphere .",
    "the case that @xmath20 and @xmath21 was studied by @xcite , and is called the bingham distribution .",
    "this distribution has `` antipodal symmetry '' in that @xmath22 , and so may be appropriate as a model for random axes , rather than random directions .    in recognition of the work of the above mentioned authors , we refer to distributions with densities given by ( [ eq : bmfvec ] ) and ( [ eq : bmfmat ] ) as vector - variate and matrix - variate bingham - von mises - fisher distributions , respectively .",
    "this is a rather long name , however , so in this vignette i will refer to them as bmf distributions .",
    "the case that @xmath8 ( or @xmath6 ) is the zero matrix will be referred to as an mf distribution , and the case that @xmath23 is zero will be referred to as a bingham distribution .",
    "more descriptive names might be l , q and lq to replace the names mf , bingham , and bmf , respectively , the idea being that the `` l '' and `` q '' refer to the presence of linear and quadratic components of the density .",
    "it is often useful to model an @xmath24 rectangular matrix - variate dataset @xmath25 as being equal to some reduced rank matrix @xmath26 plus i.i.d .",
    "noise , so that @xmath27 , with the elements @xmath28 of @xmath29 assumed to be i.i.d .  with zero mean and some unknown variance @xmath30 .",
    "the singular value decomposition states that any rank-@xmath31 matrix @xmath26 can be expressed as @xmath32 , where @xmath33 , @xmath34 and @xmath35 is an @xmath36 diagonal matrix .",
    "if we are willing to assume normality of the errors , the model can then be written as @xmath37 bayesian rank selection for this model was considered in @xcite . in this vignette",
    "we consider estimation for a specified rank @xmath31 , in which case the unknown parameters in the model are @xmath38 .",
    "given a suitable prior distribution over these parameters , bayesian inference can proceed via construction of a markov chain with stationary distribution equal to the conditional distribution of the parameters given @xmath25 , i.e.  the distribution with density @xmath39 . in particular , conjugate prior distributions allow the construction of a markov chain via the gibbs sampler , which iteratively simulates each parameter from its full conditional distribution .",
    "if the prior distribution for @xmath1 is uniform on @xmath3 , then its full conditional density is given by @xmath40^t [ { \\mathbf{y}}-{\\mathbf{u}}{\\mathbf{d}}{\\mathbf{v}}^t ] /(2\\sigma^2 ) )   \\\\ & \\propto &   { { \\rm etr } } ( [ { \\mathbf{y } } { \\mathbf{v } } { \\mathbf{d}}/\\sigma^2 ] ^t { \\mathbf{u } } ) , \\end{aligned}\\ ] ] which is the density of an mf@xmath41 distribution .",
    "similarly , the full conditional distribution of @xmath42 under a uniform prior is mf@xmath43 . for this vignette",
    ", we will use the following prior distributions for @xmath44 : @xmath45 the corresponding full conditional distributions are @xmath46,\\tau^2\\sigma^2/[\\tau^2 + \\sigma^2 ] ) \\\\",
    "\\{1/\\tau^2 |   { \\mathbf{u } } , { \\mathbf{d } } , { \\mathbf{v } } , { \\mathbf{y } } , \\sigma^2 \\ } & \\sim &   { \\rm gamma } (   [ \\eta_0+r]/2 , [ \\eta_0\\tau^2_0 + \\sum d_j^2 ] /2 )   \\\\   \\{1/\\sigma^2 | { \\mathbf{u } } , { \\mathbf{d } } , { \\mathbf{v } } , { \\mathbf{y } } , \\tau^2 \\ } & \\sim &     { \\rm gamma } ( [ \\nu_0+m n]/2 , [ \\nu_0\\sigma^2_0 + || { \\mathbf{y } } -{\\mathbf{u}}{\\mathbf{d}}{\\mathbf{v}}^t||^2]/2 ) . \\end{aligned}\\ ] ]      we now randomly generate some parameters and data according to the model above :    > library(rstiefel ) >",
    "set.seed(1 ) > m<-60 ; n<-40 ; r0<-4 > u0<-rustiefel(m , r0 ) > v0<-rustiefel(n , r0 ) > d0<-diag(sort(rexp(r0),decreasing = true))*sqrt(m*n ) > m0<-u0 >",
    "y<-m0 + matrix(rnorm(n*m),m , n )    the only command from the rstiefel package used here is rustiefel , which generates a uniformly distributed random orthonormal matrix . note that rustiefel(m , r ) gives a matrix with @xmath16 rows and @xmath31 columns , and so the arguments are in the reverse of their order in the symbolic representation of the manifold @xmath3 .",
    "now we try to recover the true values of the parameters @xmath47 from the observed data @xmath25 . just for fun ,",
    "let s estimate these parameters with a presumed rank @xmath48 that is larger than the actual rank .",
    "equivalently , we can think of @xmath49 as having dimension @xmath0 , @xmath50 and @xmath7 , but with the last @xmath51 diagonal entries of @xmath52 being zero .",
    "the prior distributions for @xmath1 and @xmath42 are uniform on their respective manifolds .",
    "we set our hyperparameters for the other priors as follows :    > nu0<-1 ; s20<-1 # inverse - gamma prior for the error variance s2 > eta0<-1 ; t20<-1 # inverse - gamma prior for the variance t2 of the sing vals    construction of a gibbs sampler requires starting values for all ( but one ) of the unknown parameters .",
    "an natural choice is the mle :    > r<-6 > tmp<-svd(y ) ; u<-tmp@xmath53 ; v<-tmp$]v[,1:r ] ; d<-diag(tmp@xmath54 )   > s2<-var(c(y - u > t2<-mean(diag(d^2 ) ) \\end{sinput } \\end{schunk } let 's compare the mle of $ ] d@xmath55 40.05172 25.00226 19.70827 13.43382",
    "13.10381 12.64942 \\end{soutput } \\begin{sinput } > diag(d0 ) \\end{sinput } \\begin{soutput } [ 1 ] 38.514216 24.015791 17.352783   1.169442 \\end{soutput } \\end{schunk } the values of the mle are , as expected , larger than the   true values , especially for the smaller values of $ ] d_0@xmath56d@xmath57d@xmath58p(d | y)@xmath59m_0=u_0d_0 v_0^t@xmath60udv^t@xmath61r@xmath62r@xmath63r@xmath64m_0@xmath65r@xmath66u[,1:r ] > m.b1<-mps / dim(dps)[1 ] > tmp<-svd(m.b1 ) ; m.b2<-tmp@xmath53$]d[1:r ] ) > mean ( ( m0-m.ml)^2 )    [ 1 ] 0.3563462    > mean ( ( m0-m.b1)^2 )    [ 1 ] 0.1315899    > mean ( ( m0-m.b2)^2 )    [ 1 ] 0.1311898    not surprisingly , the mle has a much larger loss than the bayes estimates .",
    "the squared error for the two bayes estimates are nearly identical .",
    "this is because although the posterior mean has full rank @xmath67 , it is very close to its rank-@xmath31 approximation .",
    "finally , let s make some plots based on the output of the gibbs sampler .",
    "the left - most plot of figure [ fig : svd ] gives simulated values of @xmath35 , with the values of @xmath52 given in thick lines .",
    "the mixing of the markov chain looks pretty reasonable .",
    "the center plot gives @xmath68 versus its posterior expectation , approximated from the mcmc sample average of @xmath69 .",
    "the right plot gives the mles of @xmath52 in pink , the posterior expectations of @xmath52 in light blue , and the true values in thin black lines .",
    "the posterior estimates are very accurate for the large singular values of @xmath52 , but are overestimates for the smallest values ( the last @xmath51 of which are zero ) .",
    "however , these bayes estimates are much better than the unregularized mles .",
    "the package rstiefel includes a dataset on the social network and some health behaviors of a group of @xmath70 scottish teenage girls .",
    "these data were derived from the data available at http://www.stats.ox.ac.uk/~snijders/siena/s50_data.htm and described in @xcite .",
    "let @xmath25 be the @xmath71 symmetric adjacency matrix corresponding to this network , with off - diagonal entry @xmath72 equal to the binary indicator of a friendship between actors @xmath73 and @xmath74 , as reported by one or both actors . in this vignette we will derive a model - based representation of these data using the following reduced - rank probit model : @xmath75 where @xmath76 i.i.d .",
    "normal@xmath77 , @xmath78 and the matrix @xmath1 with row vectors @xmath79 lies in the stiefel manifold @xmath80 .",
    "this model is a type of two - way latent factor model in which the relationship between actors @xmath73 and @xmath74 is modeled in terms of their unobserved latent factors @xmath81 and @xmath82 .",
    "this model and its relationship to other latent variable network models are described more fully in @xcite .",
    "convenient prior distributions for @xmath83 are as follows : @xmath84 conditional on the observed network @xmath25 , posterior inference can proceed via a gibbs sampling scheme for the unknown quantities @xmath85 . under model ( [ eq : eigenmodel ] ) , observing @xmath86 or @xmath87 implies that @xmath88 is less than or greater than zero , respectively . thus conditional on @xmath89 , the distribution of @xmath90 is that of a random symmetric normal matrix with mean @xmath91 and independent entries that are constrained to be positive or negative depending on the entries of @xmath25",
    ". given @xmath90 , the full conditional distributions of @xmath92 do not depend on @xmath25 , and can be obtained from the corresponding prior distributions and the density for the matrix @xmath90 , given by @xmath93^t [ { \\mathbf{z } } - \\theta { \\mathbf{1 } } { \\mathbf{1}}^t - { \\mathbf{u } } \\lambda { \\mathbf{u}}^t ] /4 ) \\nonumber   \\\\ & = & { \\rm { { \\rm etr } } } ( -{\\mathbf{e}}^t{\\mathbf{e}}/4 )   \\times      { \\rm { { \\rm etr } } } ( \\lambda{\\mathbf{u}}^t { \\mathbf{e } } { \\mathbf{u } } /2   )   \\times    { \\rm { { \\rm etr } } } ( -\\lambda^2/4 ) , \\label{eq : llik}\\end{aligned}\\ ] ] where @xmath94 has mean @xmath95 and off - diagonal variances of 1 .",
    "the diagonal elements of @xmath29 ( and @xmath90 ) have variance 2 , but do not correspond to any observed data as the diagonal of @xmath25 is undefined .",
    "these diagonal elements are integrated over in the markov chain monte carlo estimation scheme described below . from ( [ eq : llik ] ) , the full conditional distribution of @xmath1 is easily seen to be a bingham@xmath96 distribution .",
    "full conditional distributions for the other quantities are available via standard calculations , and are given in @xcite and in the code below .",
    "the @xmath98 matrix @xmath99 provides a binary indicator of drug use and smoking behavior for each actor during the period of the study . understanding the relationship between these health behaviors and the social network",
    "can be facilitated by examining the relationship between @xmath99 and the latent factors @xmath1 that represent the network via the model given in ( [ eq : eigenmodel ] ) .",
    "a value of @xmath100 allows the prior magnitude of the latent factor effects to increase with @xmath101 , but not as fast as the residual variance : letting @xmath102 be the first column of @xmath1 , we have @xmath103   =    { \\rm   e } [ \\lambda_1 ^ 2 ]   = n$ ] . on the other hand , letting @xmath104",
    "be the matrix of residuals @xmath105 , we have @xmath106 = ( n+1)n $ ] .",
    "we are now ready to run the gibbs sampler",
    ". we will store simulated values of @xmath107 and @xmath108 in the objects ` lps ` and ` tps ` , respectively . instead of saving values of @xmath1",
    ", we will just compute the sum of @xmath109 across iterations of the markov chain . dividing by the number of iterations",
    ", this sum provides an approximation to the posterior mean of @xmath110 .",
    "a rank-@xmath31 eigendecomposition of the posterior mean can be used to provide an estimate of @xmath1 .",
    "> # # mcmc > lps<-tps<-null ; mps<-matrix(0,dim(y),dim(y ) ) > for(s in 1:10000 ) + + + z<-rz_fc(y , theta+u+ + e<-z - u+ v.theta<-1/(1/t2.theta + choose(dim(y)[1],2 ) ) + e.theta<-v.theta*sum(e[upper.tri(e ) ] ) + theta<-rnorm(1,e.theta , sqrt(v.theta ) ) + + e<-z - theta + v.lambda<-2*t2.lambda/(2+t2.lambda ) + e.lambda<-v.lambda*diag(t(u)+ l<-diag(rnorm(r , e.lambda , sqrt(v.lambda ) ) ) + + u<-rbing.matrix.gibbs(e/2,l , u ) + + # # output + if(s>100 & s+ + lps<-rbind(lps , sort(diag(l ) ) ) ; tps<-c(tps , theta ) ; mps<-mps+u+ +    note that this code uses a function ` rz_fc ` , which simulates from the full conditional distribution of @xmath90 given @xmath111 , which is that of independent constrained normal random variables .",
    "the code for this function can be obtained from the latex   source file for this document .",
    "a summary of the posterior distribution is provided in figure [ fig : sna ] .",
    "the first panel plots the posterior density of @xmath108 , and the second plots the ( marginal ) posterior densities of the ordered values of @xmath112 .",
    "this plot strongly suggests that the values of @xmath113 and @xmath114 are both positive .",
    "since the probability of a friendship between @xmath73 and @xmath74 is increasing in @xmath115 , the results posit that friendships are more likely between individuals with similar values for their latent factors ( this effect is sometimes referred to as homophily ) . the third panel plots the observed network with the node positions obtained from the estimates of @xmath79 based on the rank-2 approximation of the posterior mean of @xmath110 .",
    "the plotting colors and characters for the nodes are determined by the drug and smoking behaviors : non - smokers are plotted in green and smokers in red , non - drug users are plotted as circles and drug users as triangles . the plot indicates a separation between students with no drug or tobacco use ( green circles ) from the other students in terms of their latent factors , suggesting a relationship between these health behaviors and the social network ."
  ],
  "abstract_text": [
    "<S> we illustrate the use of the r - package * rstiefel * for matrix - variate data analysis in the context of two examples . the first example considers estimation of a reduced - rank mean matrix in the presence of normally distributed noise . </S>",
    "<S> the second example considers the modeling of a social network of friendships among teenagers . </S>",
    "<S> bayesian estimation for these models requires the ability to simulate from the matrix - variate von mises - fisher distributions and the matrix - variate bingham distributions on the stiefel manifold . </S>"
  ]
}