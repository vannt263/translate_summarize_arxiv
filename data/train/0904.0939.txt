{
  "article_text": [
    "solving the 3d schrdinger equation given an arbitrary potential @xmath2 is of great practical use in modern quantum physics ; however , there are only a handful of potentials for which analytic solution is possible . in addition",
    ", any potential that does not have a high degree of symmetry , e.g. radial symmetry , requires solution in full 3d , making standard `` point - and - shoot '' methods @xcite for solving one - dimensional partial differential equations of little use . in this paper",
    "we discuss a parallel algorithm for solving the 3d schrdinger equation given an arbitrary potential @xmath2 using the finite difference time domain ( fdtd ) method .",
    "the fdtd method has a long history of application to computational electromagnetics @xcite . in the area of computational electromagnetics parallel versions of the algorithms",
    "have been developed and tested @xcite . in this paper",
    ", we discuss the application of parallelized fdtd to the 3d schrdinger equation .",
    "the standard fdtd method has been applied to the 3d schrdinger equation by several authors in the past  @xcite . here",
    "we show how to efficiently parallelize the algorithm .",
    "we describe our parallel algorithm for finding ground and excited state wavefunctions and observables such as energy eigenvalues , and root - mean - squared radii .",
    "additionally , we introduce a way to use symmetry constraints for determining excited state wavefunctions / energies and introduce a multi - resolution technique that dramatically decreases compute time on large lattices .",
    "this paper is accompanied by an open - source release of a code that implements the algorithm detailed in this paper .",
    "the code uses the message passing interface ( mpi ) protocol for message passing between computational nodes .",
    "we note that another popular method for numerical solution of the 3d schrdinger equation is the diffusion monte carlo ( dmc ) technique , see and references therein .",
    "the starting point for this method is the same as the fdtd method applied here , namely transformation of the schrdinger equation to imaginary time .",
    "however , in the dmc algorithm the resulting `` dynamical '' equations are transformed into an integral green s function form and then the resulting integral equation is computed using stochastic sampling .",
    "the method is highly inefficient unless importance sampling @xcite is used .",
    "dmc is efficiently parallelized and there are several codes which implement parallelized dmc @xcite .",
    "the method is similar in many ways to the one presented herein ; however , the method we use does not suffer from the fermion sign problem which forces dmc to use the so - called `` fixed - node approximation '' @xcite .",
    "in addition , although the dmc algorithm can , in principle , be applied to extract properties of the excited states of the system most applications to date only calculate the ground state wavefunction and its associated expectation values .",
    "the fdtd method described herein can extract both ground and excited state wavefunctions .",
    "the organization of the paper is as follows . in secs .",
    "[ sec : setup ] and [ sec : fdtdmethod ] we briefly review the basics of the fdtd method applied to the 3d schrdinger equation and derive the equations necessary to evolve the quantum - mechanical wavefunction . in sec .",
    "[ sec : initsymmetry ] we discuss the possibility of imposing a symmetry constraint on the fdtd initial condition in order to pick out different quantum - mechanical states . in sec .",
    "[ sec : strategy ] we describe our strategy for parallelizing the fdtd evolution equations and the measurement of observables . in sec .",
    "[ sec : multiresolution ] we introduce an efficient method of using lower - resolution fdtd wavefunctions as initial conditions for higher - resolution fdtd runs that greatly speeds up determination of high - accuracy wavefunctions and their associated observables . in sec .",
    "[ sec : results ] we give results for a few potentials including benchmarks showing how the code scales as the number of computational nodes is increased . finally , in sec .",
    "[ sec : conclusions ] we conclude and give an outlook for future work .",
    "in this section we introduce the theory necessary to understand the fdtd approach for solving the time - independent schrdinger equation . here",
    "we will briefly review the basic idea of the fdtd method and in the next section we will describe how to obtain the discretized `` equations of motion '' .",
    "we are interested in solving the time - independent schrdinger equation with a static potential @xmath3 and a particle of mass @xmath4 @xmath5 where @xmath6 is a quantum - mechanical wavefunction that solves this equation , @xmath7 is the energy eigenvalue corresponding to @xmath6 , and @xmath8 is the hamiltonian operator . in order to solve this",
    "time - independent ( static ) problem it is efficacious to consider the time - dependent schrdinger equation @xmath9\\psi(\\vec{r},t ) \\ , .\\ ] ] a solution to ( [ schrod ] ) can be expanded in terms of the basis functions of the time - independent problem , i.e. @xmath10 where @xmath11 are expansion coefficients which are fixed by initial conditions ( @xmath12 represents the ground state , @xmath13 the first excited state , etc . ) and @xmath7 is the energy associated with each state . is understood to represent the full set of quantum numbers of a given state of energy @xmath7 . in the degenerate case @xmath6",
    "is an admixture of the different degenerate states . ]    by performing a wick rotation to imaginary time , @xmath14 , and setting @xmath15 and @xmath16 in order to simplify the notation , we can rewrite eq .",
    "( [ schrod ] ) as @xmath17 which has a general solution of the form @xmath18    since @xmath19 , for large imaginary time @xmath20 the wavefunction @xmath21 will be dominated by the ground state wavefunction @xmath22 . in the limit @xmath20",
    "goes to infinity we have @xmath23 therefore , if one evolves eq .",
    "( [ wickeq ] ) to large imaginary times one will obtain a good approximation to the ground state wavefunction . ;",
    "therefore , one must evolve to imaginary times much larger than @xmath24 . ]",
    "this allows one to determine the ground state energy by numerically solving equation ( [ wickeq ] ) for large imaginary time , and then use this wavefunction to find the energy expectation value @xmath25 : @xmath26 however , the method is not limited to extraction of only the ground state wavefunction and expectation values . in the next sections we will describe two different methods that can be used to extract , in addition , excited state wavefunctions .",
    "to numerically solve the wick - rotated schrdinger equation ( [ wickeq ] ) one can approximate the derivatives by using discrete finite differences . for the application at hand we can , without loss of generality , assume that the wavefunction is real - valued as long as the potential is real - valued . the imaginary time derivative becomes @xmath27 where @xmath28 is some finite change in imaginary time .",
    "similarly , the right hand side of equation ( [ wickeq ] ) becomes @xmath29\\nonumber\\\\ & & \\hspace{-3cm}+\\frac{1}{2\\delta y^{2}}[\\psi(x , y+\\delta y , z,\\tau)-2\\psi(x , y , z,\\tau)+\\psi(x , y-\\delta y , z,\\tau)]\\nonumber\\\\ & & \\hspace{-3cm}+\\frac{1}{2\\delta z^{2}}[\\psi(x , y , z+\\delta z,\\tau)-2\\psi(x , y , z,\\tau)+\\psi(x , y , z-\\delta z,\\tau)]\\nonumber\\\\ & & \\hspace{-3cm}-\\frac{1}{2}v(x , y , z)[\\psi(x , y , z,\\tau)+\\psi(x , y , z,\\tau+\\delta\\tau ) ] \\ , , \\end{aligned}\\ ] ] + where , in the last term , we have averaged the wavefunction in imaginary time in order to improve the stability of the algorithm following taflove @xcite and sudiarta and geldart @xcite .",
    "note that if the potential @xmath30 has singular points these have to be regulated in some way , e.g. by ensuring that none of the lattice points coincides with a singular point .",
    "assuming , for simplicity , that the lattice spacing in each direction is the same so that @xmath31 this equation can be rewritten more compactly by defining a difference vector @xmath32 \\ , , \\end{aligned}\\ ] ] together with a matrix - valued @xmath33 field @xmath34 , \\ ] ] giving @xmath35 \\ , , \\end{aligned}\\ ] ]    rewriting equation ( [ wickeq ] ) with equations ( [ time ] ) and ( [ spacesimple ] ) gives the following update equation for @xmath36 in imaginary time : @xmath37 where @xmath38 and @xmath39 are @xmath40 and we have reintroduced the mass , @xmath4 , for generality .",
    "evolution begins by choosing a random 3d wavefunction as the initial condition . in practice ,",
    "we use gaussian distributed random numbers with an amplitude of one .",
    "the boundary values of the wavefunction are set to zero ; however , other boundary conditions are easily implemented .",
    "achieved using the we - pml scheme @xcite . to the best of our knowledge",
    "the pml method has only been applied to unparallelized solution of the schrdinger equation @xcite . ]",
    "note that during the imaginary time evolution the norm of the wavefunction decreases ( see eq .",
    "[ imagdecom ] ) , so we additionally renormalize the wavefunction during the evolution in order to avoid numerical underflow",
    ". this does not affect physical observables .",
    "we solve eq .",
    "( [ updateeq ] ) on a three - dimensional lattice with lattice spacing @xmath41 and @xmath42 lattice sites in each direction .",
    "note that the lattice spacing @xmath41 and size @xmath43 should be chosen so that the states one is trying to determine ( i ) fit inside of the lattice volume , i.e. @xmath44 , and ( ii ) are described with a sufficiently fine resolution , i.e. @xmath45 . also note that since we use an explicit method for solving the resulting partial differential equation for the wavefunction , the numerical evolution in imaginary time is subject to numerical instability if the time step is taken too large . performing the standard von neumann stability analysis @xcite",
    "one finds that @xmath46 in order achieve stability . for a fixed lattice volume @xmath47 , therefore , @xmath48 when keeping the lattice volume fixed .",
    "the total compute time scales as @xmath49 and assuming @xmath50 , we find that the total compute time scales as @xmath51 .    at any imaginary time @xmath20 the energy of the state , @xmath52 , can be computed via a discretized form of equation ( [ energy0 ] ) @xmath53 = \\frac{\\sum_{x , y , z}\\psi(x , y , z,\\tau )   \\biggl [ \\frac{1}{2 } \\sum_{i=1}^3\\;\\left({\\bf d\\cdot\\hat{\\psi}}\\right)_i -v(x , y , z)\\psi(x , y , z,\\tau ) \\biggr ] } { \\sum_{x , y , z}\\psi(x , y , z,\\tau)^{2 } } \\ ; . \\label{energysum}\\ ] ]    excited states are extracted by saving the full 3d wavefunction to local memory periodically , which we will call taking a `` snapshot '' of the wavefunction .",
    "after convergence of the ground state wavefunction these snapshots can be used , one by one , to extract states with higher - energy eigenvalues by projecting out the ground state wavefunction , then the first excited state wavefunction , and so on  @xcite . in principle , one can extract as many states as the number of snapshots of the wavefunction saved during the evolution .",
    "for example , assume that we have converged to the ground state @xmath54 and that we also have a snapshot version of the wavefunction @xmath55 taken during the evolution . to extract",
    "the first excited state @xmath56 we can project out the ground state using @xmath57 for this operation to give a reliable approximation to @xmath56 the snapshot time should obey @xmath58 .",
    "one can use another snapshot wavefunction that was saved and obtain the second excited state by projecting out both the ground state and the first excited state .",
    "finally we mention that one can extract the binding energy of a state by computing its energy and subtracting the value of the potential at infinity @xmath59 = e[\\psi ] - \\frac{<\\!\\psi|v_\\infty|\\psi\\!>}{<\\!\\psi|\\psi\\ ! > } \\ , , \\label{bindingenergy}\\ ] ] where @xmath60 with @xmath61 as usual .",
    "note that if @xmath62 is a constant , then eq .  ( [ bindingenergy ] ) simplifies to @xmath63 =   e[\\psi ] - v_\\infty$ ] .",
    "another way to calculate the energies of the excited states is to impose a symmetry constraint on the initial conditions used for the fdtd evolution .",
    "the standard evolution calls for a random initial wavefunction ; however , if we are solving a problem that has a potential with sufficient symmetry we can impose a symmetry condition on the wavefunction in order to pick out the different states required .",
    "for example , if we were considering a spherically symmetric coulomb potential then we could select only the @xmath64 , @xmath65 , @xmath66 , etc .",
    "states by requiring the initial condition to be reflection symmetric about the @xmath67 , @xmath68 , and @xmath69 axes .",
    "this would preclude the algorithm finding any anti - symmetric states such as the @xmath70 state since evolution under the hamiltonian operator can not break the symmetry of the wavefunction . likewise to directly determine the @xmath70 excited state one can start by making the fdtd initial state wavefunction anti - symmetric about one of the axes , e.g. the @xmath69-axis . as we will show below this provides for a fast and accurate method for determining the low - lying excited states .",
    "notationally , we will introduce two symbols , the symmetrization operator @xmath71 and the anti - symmetrization operator @xmath72 . here",
    "@xmath73 labels the spatial direction about which we are ( anti-)symmetrizing , i.e. @xmath74 . although not required , it is implicit that we perform the symmetrization about a plane with @xmath75 , @xmath76 , or @xmath77 , respectively . in practice",
    "these are implemented by initializing the lattice and then simply copying , or copying plus flipping the sign , elements from one half of the lattice to the other . in practice ,",
    "we find that due to round - off error one should reimpose the symmetry condition periodically in order to guarantee that lower - energy eigenstates do not reappear during the evolution .",
    "parallelizing the fdtd algorithm described above is relatively straightforward .",
    "ideally , one would segment the volume into @xmath78 equal subvolumes and distribute them equally across all computational nodes ; however , in this paper we will assume a somewhat simpler possibility of dividing the lattice into `` slices '' .",
    "our method here will be to start with a @xmath79 lattice and slice it along one direction in space , e.g. the @xmath67 direction , into @xmath78 pieces where @xmath42 is divisible by @xmath78 .",
    "we then send each slice of @xmath80 lattice to a separate computational node and have each computational node communicate boundary information between nodes which are evolving the sub - lattices to its right and/or left .",
    "the partitioning of the lattice is indicated via a 2d sketch in fig .",
    "[ fig : parallelizationsketch ] . in practice , in order to implement boundary conditions and synchronization of boundaries between computation nodes compactly in the code , we add `` padding elements '' to the overall lattice so that the actual lattice size is @xmath81 .",
    "the outside elements of the physical lattice hold the boundary value for the wavefunction . in all examples below the boundary value of the wavefunction",
    "will be assumed to be zero ; however , different types of boundary conditions are easily accomodated . when slicing the lattice in order to distribute the job to multiple computational nodes we keep padding elements on each slice so that the actual size of the slices is @xmath82 .",
    "padding elements on nodes that share a boundary are used to keep them synchronized , while padding elements on nodes that are at the edges of the lattice hold the wavefunction boundary condition .    in fig .",
    "[ fig : programflow ] we show a flow chart that outlines the basic method we use to evolve each node s sub - lattice in imaginary time . in the figure each column corresponds to a separate computational node .",
    "solid lines indicate the process flow between tasks and dashed lines indicate data flow between computational nodes .",
    "shaded boxes indicate non - blocking communications calls that allow the process flow to continue while communications take place . as can be seen from fig .",
    "[ fig : programflow ] we have optimized each lattice update by making the first step in each update iteration a non - blocking send / receive between nodes .",
    "while this send / receive is happening each node can then update the interior of its sub - lattice .",
    "for example , in the two node case show in fig .",
    "[ fig : parallelizationsketch ] this means that node 1 would update all sites with an @xmath67-index between 1 and 3 while node 2 would update sites with @xmath67-index between 6 and 8 .",
    "once these interior updates are complete each node then waits for the boundary communication initiated previously to complete , if it has not already done so .",
    "once the boundaries have been synchronized , the boundary elements themselves can be updated .",
    "going back to our example shown in fig .",
    "[ fig : parallelizationsketch ] this would mean that node 1 would update all sites with @xmath67-index of 4 and node 2 would update all sites with an @xmath67-index of 5 .",
    "convergence is determined by checking the ground state binding energy periodically , e.g. every one hundred time steps , to see if it has changed by more than a given tolerance . in the code ,",
    "the frequency of this check is an adjustable parameter and should be tuned based on the expected energy of the state , e.g. if the energy is very close to zero then convergence can proceed very slowly and the check frequency should be correspondingly larger . parametrically",
    "the check frequency should scale as @xmath83 .    for computation of observables",
    "each computational node computes its contribution to the observable .",
    "then a parallel call is placed that collects the local values computed into a central value stored in computational node 1 .",
    "then node 1 broadcasts the value to the other nodes so that all nodes are then aware of the value of the particular observable .",
    "for example , to compute the energy of the state as indicated in eq .",
    "( [ energysum ] ) each computational node computes the portion of the sum corresponding to its sub - lattice and then these values are collected via a parallel sum operation to node 1 and then broadcast out to each node .",
    "each node can then use this information to determine if the wavefunction evolution is complete .",
    "we note that the normalization of the wavefunction is done in a similar way with each node computing its piece of the norm , collecting the total norm to node 1 , broadcasting the total norm to all nodes , and then each node normalizes the values contained on its sub - lattice . in this way computation of observables and wavefunction normalization",
    "is also parallelized in our approach .      in most settings computational clusters",
    "are limited by their communication speed rather than by cpu speed . in order to understand how",
    "things scale we introduce two time scales : @xmath84 which is the amount of time needed to update one lattice site and @xmath85 which is the amount of time needed to communicate ( send and receive ) the information contained on one lattice site .",
    "typically @xmath86 unless the cluster being used is on an extremely fast network .",
    "therefore , the algorithm should be optimized to reduce the amount of communications required .    for the one - dimensional partitions employed here @xmath87 where @xmath88 is the number of 1d slices distibuted across the cluster and the factor of 2 comes from the 2 surfaces which must be communicated by the internal partitions . for the calculation not to have a communications bottleneck we should have @xmath89",
    ". using ( [ 1dscaling ] ) we find that this constraint requires @xmath90    in the benchmarks section below we will present measurements of @xmath91 and @xmath92 using our test cluster .",
    "we find that @xmath93 .",
    "using this , and assuming , as a concrete example , a lattice size of @xmath94 we find @xmath95 . for clusters with more than 102 nodes",
    "it would be more efficient to perform a fully 3d partitioning . in the case of a fully 3d partitioning one",
    "finds that the limit due to communications overhead is @xmath96 .",
    "if one is interested in high - precision wavefunctions for low - lying excited states , an efficient way to do this is to use a multi - resolution technique .",
    "this simply means that we start with a random wavefunction on small lattice , e.g. @xmath97 , and use the fdtd technique to determine the ground state and first few excited states and save the wavefunctions , either in local memory or disk .",
    "we can then use a linear combination of the coarse versions of each state as the initial condition on a larger lattice , e.g. @xmath98 , while keeping the lattice volume fixed .",
    "we can then `` bootstrap '' our way up to extremely large lattices , e.g. on the order of @xmath99 , by proceeding from low resolution to high resolution . in the results section",
    "we will present quantitative measurements of the speed improvement that is realized using this technique .",
    "in this section we present results obtained for various 3d potentials and benchmarks that show how the code scales with the number of computational nodes .",
    "our benchmarks were performed on a small cluster of 4 servers , each with two quad - core 2 ghz amd opteron processors .",
    "each server can therefore efficiently run eight computational processes simultaneously , allowing a maximum of 32 computational nodes .",
    "the servers were networked with commercial 1 gbit / s tcp / ip networking . for the operating system we used 64 bit ubuntu server edition 8.10 linux .      in order to implement the parallel algorithm we use a mixture of c / c++ and the message passing interface ( mpi ) library for message passing between computational nodes @xcite .",
    "the servers used the openmpi implementation of the mpi api .",
    "the code itself is open - sourced under the gnu general public license ( gpl ) and is available for internet download via the url in ref .",
    "@xcite .",
    "@xmath100     \\(a ) ( b )    in this section we present data for the scaling of the time of one iteration and the time for communication on a @xmath101 lattice . as discussed in sec .",
    "[ sec:1dvs3d ] we expect to see ideal scaling of the code as long as communication time is shorter than the update time , i.e. @xmath89 . in fig .",
    "[ fig : itertimes]a we show the time to complete one iteration as a function of the number of computational nodes on a log - log axis along with a linear fit .",
    "the linear fit obtained gives @xmath102 .",
    "in addition , in fig .  [",
    "fig : itertimes]b we show a comparison of the full time for each iteration with the amount of time needed to communicate a lattice site s information ( in this case the local value of the wavefunction ) . in both fig .",
    "[ fig : itertimes]a and [ fig : itertimes]b the error bars are the standard error determined by averaging over 10 runs , @xmath103 , where @xmath104 is the standard deviation across the sampled set of runs and @xmath42 is the number of runs .    as can be seen from fig .",
    "[ fig : itertimes]b using a @xmath105 lattice the algorithm performs well up to @xmath106 at which point the communication time becomes equal to the iteration time .",
    "for @xmath107 we would see a violation of the scaling above due to communication overhead . note that this is rough agreement with our estimate from sec .",
    "[ sec:1dvs3d ] which , for @xmath105 lattice predicts the point where communications and update times to be equal to be @xmath108 .",
    "note that in fig .",
    "[ fig : itertimes]b the increase in communication times as @xmath109 increases is due to the architecture of the cluster used for the benchmarks which has eight cores per server .",
    "if @xmath110 then all jobs run on one server , thereby decreasing the communications overhead . in the next section , we will present benchmarks for different potentials in order to ( a ) confirm the scaling obtained above in specific cases and ( b ) to verify that the code converges to the physically expected values for cases which are analytically solvable .",
    "we use the following potential for finding the coulomb wavefunctions @xmath111 where @xmath41 is the lattice spacing in units of the bohr radius and @xmath112 is the distance from the center of the 3d lattice . the constant of @xmath113 is added for @xmath114 in order to ensure that the potential is continuous at @xmath115 .",
    "this is equivalent to making the potential constant for @xmath116 and shifting the entire potential by a constant which does not affect the binding energy .",
    "analytically , in our natural units the binding energy of the @xmath117th state is @xmath118 where @xmath119 is the principal quantum number labeling each state .",
    "the ground state therefore has a binding energy of @xmath120 and the first excited state has @xmath121 , etc .",
    "note that to convert these to electron volts you should multiply by 27.2 ev .    in fig .",
    "[ fig : coulombscaling ] we show the amount of time needed in seconds to achieve convergence of the ground state binding energy to a part in @xmath122 as a function of the number of computational nodes for @xmath123 on a log - log plot . for this benchmark we used a lattice with @xmath101 , a constant lattice spacing of @xmath124 , a constant imaginary time step of @xmath125 , and the particle mass",
    "was also set to @xmath16 . in order to remove run - by - run fluctuations due to the random initial conditions we used the same initial condition in all cases . in fig .",
    "[ fig : coulombscaling ] the error bars are the standard error determined by averaging over 10 runs , @xmath103 , where @xmath104 is the standard deviation across the sampled set of runs and @xmath42 is the number of runs . in all cases shown the first two energy levels obtained were @xmath126 and @xmath127 .",
    "this corresponds to an accuracy of 0.2% and 2.4% , respectively . in fig .",
    "[ fig : coulombscaling ] the extracted scaling slope is close to 1 indicating that the compute time in this case scales almost ideally , i.e. inversely proportional to the number of computing nodes . note that the fit obtained in fig .",
    "[ fig : coulombscaling ] has a slope with magnitude greater than 1 indicating scaling which is better than ideal ; however , as one can see from the figure there is some uncertainty associated with this fit .",
    "we use the following potential for finding the 3d harmonic oscillator wavefunctions @xmath128 where @xmath112 is the distance from the center of the 3d lattice .    in fig .",
    "[ fig : hoscaling ] we show the amount of time needed in seconds to achieve convergence of the ground state binding energy to a part in @xmath122 as a function of the number of computational nodes for @xmath123 . for this benchmark",
    "we used a constant lattice spacing of @xmath129 , a constant imaginary time step of @xmath130 , and a @xmath101 dimension lattice so that the box dimension was @xmath131 . in fig .",
    "[ fig : hoscaling ] the error bars are the standard error determined by averaging over 10 runs , @xmath103 , where @xmath104 is the standard deviation across the sampled set of runs and @xmath42 is the number of runs .",
    "the particle mass was also set to @xmath16 . in order to remove run - by - run fluctuations due to the random initial conditions we used the same initial condition in all cases . in all cases the ground state energy obtained was @xmath132 corresponding to an accuracy of 0.0026% . in fig .",
    "[ fig : hoscaling ] the extracted scaling slope is 0.91 meaning that the compute time scales as @xmath133 in this case .",
    "this is a slightly different slope than in the coulomb potential case .",
    "this is due to fluctuations in compute time due to server load and sporadic network delays .",
    "the scaling coefficient reported in the conclusions will be the average of all scaling coefficients extracted from the different potentials detailed in this paper .",
    "the previous two examples have spherical symmetry and hence it is not necessary to apply a fully 3d schrdinger equation solver to them .",
    "we do so only in order to show scaling with computational nodes and percent error compared to analytically available solutions . as a nontrivial example of the broad applicability of the fdtd technique we apply it to a potential that is a constant negative value of @xmath134 inside a surface defined by a regular dodecahedron with the following 20 vertices @xmath135 where @xmath136 is the golden ratio .",
    "the value -1 is mapped to the point @xmath13 and the value 1 is mapped to the point @xmath137 in all three dimensions . as a result",
    ", the containing sphere has a radius of @xmath138 .    in fig .",
    "[ fig : dodecahedron ] we show the ground and first excited states extracted from a run on a @xmath98 lattice with a lattice spacing of @xmath139 , an imaginary time step of @xmath140 and particle mass of @xmath141 . on the left",
    "we show the ground state and on the right the first excited state .",
    "we find that the energies of these two levels are @xmath142 and @xmath143 .",
    "note that for the first excited state the position of the node surface can change during each run due to the random initial conditions used . in practice",
    ", the node surface seems to align along one randomly chosen edge of one of the pentagons that make up the surface of the dodecahedron .    in fig .",
    "[ fig : dodecahedronscaling ] we show the amount of time needed in seconds to achieve convergence of the dodecahedron ground state binding energy to a part in @xmath122 as a function of the number of computational nodes for @xmath123 . for this benchmark we used a @xmath105 lattice with a constant lattice spacing of @xmath139 , an imaginary time step of @xmath140 and particle mass of @xmath141 . in fig .",
    "[ fig : dodecahedronscaling ] the error bars are the standard error determined by averaging over 10 runs , @xmath103 , where @xmath104 is the standard deviation across the sampled set of runs and @xmath42 is the number of runs . in all cases",
    "the ground state energy obtained was @xmath144 . in fig .",
    "[ fig : dodecahedronscaling ] the extracted scaling slope is 0.91 meaning that the compute time scales as @xmath133 in this case .",
    "@xmath145          one of the fundamental problems associated with using a single fdtd run to determine both the ground state and excited states is that typically the excited states are much more extended in space than the ground state , particularly for potentials with a `` long range tail '' like the coulomb potential .",
    "for this reason it is usually difficult to obtain accurate energy eigenvalues for both ground and excited states unless the lattice has an extremely fine lattice spacing and a large number of points in each direction so that the dimension of the box is also large . in sec .",
    "[ sec : coulombbench ] we presented benchmarks for the coulomb potential on a @xmath105 lattice that had a dimension of 25.6 bohr radii . as we found in that section , we were able to determine the ground and first excited states to 0.2% and 2.4% .",
    "improving the accuracy of the first excited state would require going to a lattice with dimensions larger than @xmath105 .    while this is possible with the parallelized code , there is a more efficient way to find excited states by applying symmetry constraints to the initial wavefunction .",
    "for example , to find the 1p state of the coulomb problem we can initialize the wavefunction as @xmath146 as discussed in sec .",
    "[ sec : initsymmetry ] . in this case",
    "we explicitly project out the ground state wavefunction since it is symmetric about the @xmath69-axis . applying this method on a @xmath147 lattice with lattice spacing @xmath148 and imaginary time step @xmath149",
    "we find the first excited state energy to be @xmath150 which is accurate to 0.06% . at the same time we can extract the next excited state which is anti - symmetric about the @xmath69-axis ( @xmath151 state ) finding in this case , @xmath152 , corresponding to an accuracy of 0.4% .",
    "the application of symmetry constraints can also allow one to pick out states with different orientations in a 3d potential that breaks spherical symmetry .",
    "in ref  @xcite this technique was used to accurately determine the different heavy quarkonium p - wave states corresponding to angular momentum @xmath153 and @xmath154 .",
    "therefore , the ability to constrain the symmetry of the initial fdtd wavefunction is a powerful technique .      in this section",
    "we present benchmarks for the application of the multi - resolution technique to the coulomb potential problem .",
    "the current version of the code supports this feature by allowing users the option of saving the wavefunction at the end of the run .",
    "the saved wavefunctions can then be read in and used as the initial condition for a subsequent run .",
    "the saved wavefunctions can have a different resolution than the resolution of the new run and the code automatically adjusts by sampling / spreading out the wavefunction appropriately .    by using this technique",
    "we can accelerate the determination of the high accuracy energy eigenvalues and wavefunctions . in sec .",
    "[ sec : coulombbench ] we found that using 32 computational nodes and a random initial wavefunction a @xmath105 run took approximately 1.3 hours . scaling naively to a @xmath155 lattice , while keeping the lattice volume fixed , would take approximately 42 hours . using the multi - resolution technique and bootstrapping from @xmath98 up to @xmath155 a high resolution ground state and energy eigenvalue",
    "can be computed in approximately 45 minutes using the same 32 computational nodes . at the final resolution of @xmath156 and a lattice size of 25.6 bohr radii the @xmath155 run gives @xmath157 which is accurate to 0.07% .",
    "therefore , the multi - resolution technique provides a performance increase of a factor of 50 compared to using random initial wavefunctions for all runs .",
    "in this paper we have described a parallel fdtd algorithm for solving the 3d schrdinger equation .",
    "we have shown that for large 3d lattices the method gives a compute time that scales as @xmath158 .",
    "this final scaling coefficient and associated error were obtained by averaging the three different scaling coefficients extracted for the coulomb , harmonic oscillator , and dodecahedron potentials .",
    "the crucial optimization that allowed us to achieve nearly ideal scaling was the use of non - blocking sends / receives of the boundary data so that update of each node s sub - lattice can proceed while communication of the boundary information is taking place , providing for an `` inside - out '' update algorithm .",
    "additionally we introduced two novel techniques that can be used in conjunction with the fdtd method .",
    "first , we discussed the possibility of imposing a symmetry constraint on the initial wavefunction used for the fdtd evolution .",
    "the imposed symmetry constraint allows us to easily construct states that are orthogonal to the ground state and/or some other excited states . using this technique we can select states that have a certain symmetry , thereby allowing for extremely accurate determination of the particular states we are interested in .",
    "second , we introduced the `` multi - resolution technique '' which simply means that we use the fdtd output wavefunctions from lower - resolution runs as the initial condition for higher - resolution runs . using this method we showed that we can efficiently `` bootstrap '' our way from small to large lattices , thereby obtaining high - accuracy wavefunctions and eigenvalues in a fraction of the time required when using random initial wavefunctions on large lattices .",
    "the code developed for this paper has been released under an open - source gpl license @xcite .",
    "an obvious next step will be to extend the code to fully 3d partitions , which is in progress .",
    "other areas of improvement include adding support for different types of boundary conditions and complex potentials @xcite .",
    "we thank v. antocheviz dexheimer , a. dumitru , j. groff , and j. milingo for helpful comments . m.s .",
    "thanks a. dumitru , y. guo , and a. mocsy for collaboration in the original project that prompted the development of this code . m.s .",
    "also thanks sharon stephenson for support of d.y . during the preparation of this manuscript .",
    "d.y . was supported by the national science foundation award # 0555652 ."
  ],
  "abstract_text": [
    "<S> we describe a parallel algorithm for solving the time - independent 3d schrdinger equation using the finite difference time domain ( fdtd ) method . </S>",
    "<S> we introduce an optimized parallelization scheme that reduces communication overhead between computational nodes . </S>",
    "<S> we demonstrate that the compute time , @xmath0 , scales inversely with the number of computational nodes as @xmath1 . </S>",
    "<S> this makes it possible to solve the 3d schrdinger equation on extremely large spatial lattices using a small computing cluster . </S>",
    "<S> in addition , we present a new method for precisely determining the energy eigenvalues and wavefunctions of quantum states based on a symmetry constraint on the fdtd initial condition . </S>",
    "<S> finally , we discuss the usage of multi - resolution techniques in order to speed up convergence on extremely large lattices . </S>"
  ]
}