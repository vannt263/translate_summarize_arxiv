{
  "article_text": [
    "we consider the numerical resolution of filtering problems and the estimation of the associated normalizing constants for state - space models .",
    "in particular , the data is modelled by a discrete time process @xmath5 , @xmath6 , associated to a hidden signal modelled by a markov chain @xmath7 , @xmath8 ; we concerned with high dimensions , i.e.  @xmath1 large . for simplicity , we assume that the location of the signal at time 0 is fixed and known , but the algorithm can easily be extended to the general case being a random variable . in this case",
    "we require a mechanism through which we can produce a sample from its distribution with a polynomial computational effort in the dimension of the state space . ] .",
    "we will write the joint density ( with respect to an appropriate dominating measure ) of @xmath9 as @xmath10 for kernel functions @xmath11 and @xmath12 so that , given the hidden states @xmath13 , the data @xmath14 consist of independent entries with @xmath15 only depending on @xmath16 .",
    "the objective is to approximate the filtering distribution @xmath17 .",
    "this filtering problem when @xmath1 is large is notoriously difficult , in many scenarios .    in general",
    ", the filter can not be computed exactly and one often has to resort to numerical methods , for example by using particle filters ( see e.g.  @xcite ) .",
    "particle filters make use of a sequence of proposal densities and sequentially simulate from these a collection of @xmath18 samples , termed particles . in most scenarios",
    "it is not possible to use the distribution of interest as a proposal .",
    "therefore , one must correct for the discrepancy between proposal and target via importance weights . in the majority of cases of practical interest , the variance of these importance weights increases with algorithmic time .",
    "this can , to some extent , be dealt with via a resampling procedure consisted of sampling with replacement from the current weighted samples and resetting them to @xmath19 .",
    "the variability of the weights is often measured by the effective sample size ( ess ) .",
    "if @xmath1 is small to moderate , then particle filters can many times perform very well in the time parameter @xmath3 ( e.g.  @xcite ) .",
    "for instance , under conditions the monte carlo error of the estimate of the filter can be uniform with respect to the time parameter .    for some state - space models , with specific structures , particle algorithms can work well in high dimensions , or at least can be appropriately modified to do so .",
    "we note for instance that one can set - up an effective particle filter even when @xmath20 provided one assumes a finite ( and small , relatively to @xmath1 ) amount of information in the likelihood ( see e.g. @xcite for details ) .",
    "this is _ not _ the class of problems for which we are interested in here . in general",
    ", it is mainly the amount of information in the likelihood @xmath21 that determines the algorithmic challenge rather than the dimension @xmath1 of the hidden space per - se ( this is related to what is called ` effective dimension ' in @xcite ) .",
    "the function @xmath22 can convey a lot of information about the hidden state , especially so in high dimensions . if this is the case , using the prior transition kernel @xmath23 as proposal will be ineffective .",
    "we concentrate here on the challenging class of problems with large state space dimension @xmath1 and an amount of information in the likelihood that increases with @xmath1 .",
    "it is then known that the standard particle filter will typically perform poorly in this context , often requiring that @xmath24 , for some @xmath25 , see for instance @xcite .",
    "the results of @xcite , amongst others , has motivated substantial research in the literature on particle filters in high - dimensions , such as the recent work in @xcite which attempts an approximate split of the @xmath1-dimensional state vector to confront the curse - of - dimensionality for importance sampling , at the cost of introducing difficult to quantify bias with magnitude that depends on the position along the @xmath1 co - ordinates . see @xcite and the references therein for some algorithms designed for high - dimensional filtering . to - date",
    ", there are few particle filtering algorithms that are :    1 .",
    "asymptotically consistent ( as @xmath4 grows ) , 2 .",
    "of fixed computational cost per time step ( ` online ' ) , 3 .   supported by theoretical analysis demonstrating a sub - exponential cost in @xmath1",
    ".    in this article we attempt to provide an algorithm which has the above properties .",
    "our method develops as follows . in a general setting , we assume there exists an increasing sequence of sets @xmath26 , with @xmath27 , for some integer @xmath28 , such that we can factorize : @xmath29 for appropriate functions @xmath30 , where we denote @xmath31 .",
    "as we will remark later on , this structure is not an absolutely necessary requirement for the subsequent algorithm , but will clarify the ideas in the development of the method . within a sequential monte carlo context , one can think of augmenting the sequence of distributions of increasing dimension @xmath32 , @xmath33 , moving from @xmath34 to @xmath35 , with intermediate laws on @xmath36 ) is not uncommon .",
    "for instance one should typically be able to obtain such a factorization for the prior term @xmath37 by marginalising over subsets of co - ordinates .",
    "then , for the likelihood component @xmath21 this could for instance be implied when the model assumes a local dependence structure for the observations .",
    "critically , for this approach to be effective it is necessary that the factorisation is such that will allow for a gradual introduction of the ` full ' likelihood term @xmath38 along the @xmath39 steps .",
    "for instance , trivial choices like @xmath40 , @xmath41 , and @xmath42 will be ineffective , as they only introduce the complete likelihood term in the last step .",
    "our contribution is based upon the idea that particle filters in general work well with regards to the time parameter ( they are sequential ) .",
    "thus , we will exploit the structure in ( [ eq : hmm_struc ] ) to build up a particle filter in space - time moving vertically along the space index ; for this reason , we call the new algorithm the space - time particle filter ( stpf ) . we break the @xmath43-th time - step of the particle filter into @xmath39 space - steps and run a system of @xmath4 independent particle filters for these steps .",
    "this is similar to a tempering approach as the one in @xcite , in the context of sequential monte carlo algorithms @xcite for a single target probability of dimension @xmath1 .",
    "there , the idea is to use annealing steps , interpolating between an easy to sample distribution and the target with an @xmath44 number of steps . in the context of filtering , for the filter ,",
    "say , at time 1 we break the problem of trying to perform importance sampling in one step for a @xmath1-dimensional object ( which typically does not perform well , as noted by @xcite ) into @xmath45 easier steps via the particle filter along space ; as the particle filter on low to moderate dimensions is typically well behaved , one expects the proposed procedure to work well even if @xmath1 is large .",
    "a similar idea is used at subsequent time steps of the filter .    in the main part of the paper and in all theoretical derivations",
    ", we work under the easier to present scenario @xmath46 and @xmath47 .",
    "we establish that our algorithm is consistent as @xmath4 grows ( for fixed @xmath1 ) , i.e.  that one can estimate the filter with enough computational power , in a manner that is online .",
    "the we look at two simple models : a ) an i.i.d .",
    "scenario both in space and time , b ) a markovian model along space . in both cases , we present results indicating that the algorithm is stable at a cost of @xmath2 .",
    "as we remark later on , we expect this cost to be optimistic , but , we conjecture that the cost in general is no worse than polynomial in @xmath1 .",
    "these claims are further supported by numerical simulations .",
    "we stress here that there is a lot more to be investigated in terms of the analytical properties of the proposed algorithm to fully explore its potential , certainly in more complex model structures than the above .",
    "this work aims to make an important first contribution in an very significant and challenging problem and open up several directions for future investigation .",
    "this article is structured as follows . in section [ sec : algo ] the stpf algorithm is given . in section [ sec : theory ] our mathematical results are given ; some proofs are housed in the appendix . in section [ sec : numerics ] our algorithm is implemented and compared to existing methodology . in section [ sec : summary ] the article is concluded with several remarks for future work .",
    "we develop an algorithm that combines a local filter running @xmath1 space - step using @xmath48 particles , with a global filter making time - steps and uses @xmath4 particles .",
    "we will establish in section [ sec : theory ] , that for any fixed @xmath49 , the algorithm is consistent , with respect to some estimates of interest , as @xmath4 grows .",
    "a motivation for using such an approach is that it can potentially provide good estimates for expectations over the complete @xmath1-dimensional filtering density @xmath17 , whereas a standard filter with @xmath50 could exhibit path degeneracy even within a single time - step ( for large @xmath1 ) , thus providing unreliable estimates for @xmath17 .",
    "this approach has been motivated by the island particle model of @xcite , where a related method for standard particle filters ( and not related with confronting the dimensionality issue ) was developed , but is not a trivial extension of it , so some extra effort is required to ensure correctness of the algorithm .",
    "we will also explain how to set @xmath48 as a function of @xmath1 to ensure some stability properties with respect to @xmath1 in some specific modelling scenarios .",
    "the notation @xmath51 is adopted , with @xmath52 , denoting the particle , @xmath53 the discrete observation time , @xmath54 denoting dimensions @xmath55 and @xmath56 the particle in the local system .      for each @xmath52",
    ", the following algorithm is run .",
    "we introduce a sequence of proposal densities @xmath57 and will run a particle filter in space - direction that builds up the dimension towards @xmath58 . at space - step 1 ,",
    "one generates @xmath48-samples from @xmath59 in @xmath60 and computes the weights @xmath61 the @xmath48-samples are resampled , according to their corresponding weights . for simplicity",
    ", we will assume we use multinomial resampling .",
    "the resampled particles are written as @xmath62 . at subsequent points",
    "@xmath63 one generates @xmath48-samples from @xmath64 in @xmath60 and computes @xmath65 the @xmath48-samples are resampled according to the weights . at the end of the 1st time - step , all the last particles are resampled , thus giving @xmath66 ( so that we have @xmath4 independent particle systems of @xmath48 particles ) .",
    "the @xmath4 particle systems are assigned weights @xmath67 we then resample the @xmath4-particle systems according to these weights .",
    "the normalizing constant @xmath68 can be estimated by @xmath69 for @xmath70 , the filter at time 1 , @xmath71 can be estimated by @xmath72 where , with some abuse of notation , we assume that @xmath66 have been resampled according to the weights of the global filter in",
    ". we will remark on these estimates later on .      for each @xmath52 ,",
    "the following algorithm is run .",
    "introduce a sequence of proposal densities @xmath74 . at step 1 , one produces @xmath48-samples from @xmath75 in @xmath60 and computes the weights @xmath76 the @xmath48-samples are resampled , according to the weights inclusive of the @xmath77 , which are denoted @xmath78 at step @xmath79 . at subsequent points @xmath63 ,",
    "one produces @xmath48-samples from @xmath80 in @xmath60 and computes the weights , for @xmath56 @xmath81 the @xmath48-samples are resampled according to the weights . at the end of the time step ,",
    "the @xmath4 particle systems are assigned weights @xmath82 we then resample the @xmath4-particle systems according to the weights .",
    "the normalizing constant @xmath83 can be estimated by @xmath84 for @xmath70 , the filter at time @xmath3 , @xmath85 can be estimated by ( assuming again that @xmath86 denote the values after resampling according to the global weights in ( [ eq : abc ] ) ) @xmath87      in terms of the estimate of the filter , , we expect there to be a _ path degeneracy _ effect for the local filters ( see @xcite ) , especially for @xmath1 large , due to resampling forcing common ancestries for different particles",
    ". for instance , in a worst case scenario , for a given @xmath52 , only one of the @xmath48 samples will be a good representation of the target filtering distribution at current time - step .",
    "however , one can still average over all @xmath48-samples as we have done ; one can also select a single sample for estimation , if preferred .",
    "in addition , in a general setting the form of the weights @xmath88 , @xmath73 , depends upon @xmath77 ; there may be an additional path degeneracy effect with these samples .",
    "to an extent , this can be alleviated using dynamic resampling ( e.g.  @xcite and the references therein ) ; we will discuss how path degeneracy could be potentially dealt with in section [ sec : path_degen ] below .",
    "in addition , in some scenarios ( see e.g.  @xcite ) the path degeneracy can betaken care of if the number of samples is quadratic in the time parameter ; i.e.  @xmath89 .",
    "note that we have assumed that @xmath90 however , this need not be the case .",
    "all one needs is a collection of functions @xmath91 , such that the variance ( w.r.t .",
    "the simulated algorithm ) of @xmath92 is reasonable , especially as @xmath1 grows .",
    "then , the particles obtained at the end of the @xmath43-th time - step under @xmath93 can be used as proposals with an importance sampler targeting @xmath94 , with the above ratio giving the relevant weights . in such a scenario",
    ", we expect the algorithm to perform reasonably well , even for large @xmath1 ; however , the construction of such functions @xmath91 may not be trivial in general .",
    "the algorithm is easily parallelized over @xmath4 , at least in - between global resampling times .",
    "we also note that the idea of using a particle filter within a particle filter has been used , for example , in @xcite .",
    "the algorithm can also be thought of as a novel generalization of the island particle filter @xcite . in our algorithm",
    ", one runs an entire particle filter for @xmath1 time steps , as the local filter , whereas , it is only one step in @xcite ; as we shall see in section [ sec : theory ] , this appears to be critical in the high - dimensional filtering context .",
    "we also remark that , unlike the method described in @xcite , the algorithm in this is article is consistent as @xmath4 grows .      as mentioned above",
    ", the path degeneracy effect may limit the success of the proposed algorithm .",
    "we expect it to be of use when @xmath1 is maybe too large for the standard particle filter , but not overly large .",
    "path degeneracy can in principle be dealt with , at an increased computational cost , in the following way ; in such cases one can run the algorithm simply with @xmath50 . at time 1",
    ", one may apply an markov chain monte carlo ( mcmc ) ` mutation ' kernel for each local particle at each dimension step , where the invariant target density is proportional to ( @xmath95 ) @xmath96 at subsequent time steps @xmath3 , one uses the marginal particle filter ( e.g.  @xcite ) and targets , up - to proportionality for each local particle at each space - step @xmath97 also using mcmc steps with the above invariant density .",
    "notice that the above expression is a monte carlo estimator the ( unnormalised ) marginal distribution of @xmath98 under the model specified by the @xmath99 functionals . assuming an effective design of the mcmc step",
    ", the path degeneracy effect can be overcome , and each time - step @xmath3 will still has fixed ( but increased ) computational complexity .",
    "the cost of this modified algorithm , assuming the cost of computing @xmath99 is @xmath100 for each @xmath101 , is @xmath102 ; so long as @xmath48 is polynomial in @xmath1 , this is still a reasonable algorithm for high - dimensional problems .",
    "we note that , even though we do not analyze this algorithm mathematically , we will implement it .",
    "we will now establish that if @xmath103 are fixed then stpf will provide consistent estimates of quantities of interest of the true filter as @xmath4 grows .",
    "indeed , one can prove many results about the algorithm in this setting , such as finite-@xmath4 bounds and central limit theorems ; however , this is not the focus of this work and the consistency result is provided to validate the use of the algorithm . throughout , we condition on a fixed data record and we will suppose that @xmath104 below @xmath105 denotes convergence in probability as @xmath4 grows , where @xmath106 denotes the law under the simulated algorithm .",
    "we denote by @xmath107 the class of bounded and measurable real - valued functions on @xmath0 .",
    "we will write , for @xmath53 @xmath108 and @xmath109 so that @xmath110 corresponds to the filtering density of @xmath111 .",
    "the proof of the following theorem is given in appendix [ sec : prf_consis ] .",
    "it ensures that the @xmath4 particle systems correspond to a standard particle filter on an enlarged state space ; once this is established standard consistency results for particle filters on general state spaces ( e.g.  @xcite ) will complete the proof .",
    "we denote by @xmath105 convergence in probability .",
    "[ theo : consis ] let @xmath103 be fixed and let @xmath112 .",
    "then we have for any @xmath73 @xmath113    the proof establishes that also @xmath114 can be used as an estimator for the filter ; this may be more effective than the estimator given in the statement of the theorem , due to the path degeneracy effect mentioned earlier .",
    "in addition , one can assume the context described in ( [ eq : correct_normal ] ) with the target not having a product structure , but the weights in ( [ eq : correct_normal ] ) have controlled variance .",
    "even in this more general case one can the follow the arguments in the proof , to obtain consistency in that case ( assuming the expression in is upper - bounded ) .",
    "we now come to the main objective of our theoretical analysis",
    ". we will set @xmath4 as fixed and consider the algorithm as @xmath1 grows . in order to facilitate our analysis",
    ", we will consider approximating a probability , with density proportional to @xmath115 we will use the stpf with proposals @xmath116 . in the case of a state - space model , this would correspond to @xmath117 which would seldom occur in a real scenario .",
    "however , analysis in this context is expected to be informative for more complex scenarios as in the work of @xcite .",
    "note that , because of the loss of dependence on subsequent observation times , we expect that any complexity analysis with respect to @xmath1 to be slightly over - optimistic ; as noted the path degeneracy effect is expected to play a role in this algorithm in general .",
    "we will consider the relative variance of the standard estimate of the normalizing constant @xmath118 , given for instance in theorem [ theo : consis ] which now writes as @xmath119 the proof of the following result is given in appendix [ sec : rv_res ] .",
    "note that due to the i.i.d .",
    "structure along time and space , all variables @xmath120 can be assumed i.i.d .  from @xmath121 .",
    "[ prop : rv_res ] assume that @xmath122 then @xmath123 =   \\big(\\frac{1}{n}\\big(\\frac{1}{m_d}\\frac{\\int\\alpha(x)^2/q(x)dx}{(\\int\\alpha(x)dx)^2 } + \\frac{m_d-1}{m_d}\\big)^d + \\frac{n-1}{n}\\big)^n - 1.\\ ] ]    the case @xmath124 corresponds , in some sense , to the standard particle filter . in this case , by jensen s inequality , the right hand side of the above identity will diverge as @xmath1 grows , unless @xmath4 is of exponential order in @xmath1 . as a result , we can stabilize the algorithm with an @xmath125 cost , where @xmath25 .",
    "however , if one sets @xmath126 , then the right hand side of the above identity will stabilize and the cost of the algorithm is @xmath2 .",
    "this provides some intuition about why our approach may be effective in high dimensions .",
    "in fact , one can say a bit more . we suppose that @xmath127 is upper and lower bounded",
    "; this typically implies that @xmath128 lies only on some compact subset of @xmath60 .",
    "denoting by @xmath129 weak convergence as @xmath130 and @xmath131 the log - normal distribution of location @xmath132 , scale @xmath133 , we have the following .    [ prop : conv_weights ] let @xmath134 , for some @xmath135 and @xmath136 fixed .",
    "suppose that @xmath137 then we have that @xmath138 , and subsequently @xmath139 where @xmath140 .",
    "the result follows from ( * ? ? ?",
    "* theorem 1.1 ) and elementary calculations , which we omit .",
    "the result suggests that the algorithm stabilizes as @xmath1 grows at a @xmath2 cost . using the continuous mapping theorem , for @xmath18",
    ", one can show that the effective sample size ( ess ) will also converge to a non - trivial random variable ; see e.g.  ( * ? ? ? * proof of theorem 3.2 ) . moreover , based upon personal communication with pierre del moral",
    ", we conjecture that setting @xmath141 , for some @xmath142 , the ess converges to @xmath4 ; hence suggesting that @xmath143 is an optimal computational effort in this case .",
    "an intuition behind the results is that for a standard particle filter , when run for @xmath3 steps with @xmath4 particles , the relative variance of the estimate for the normalizing constant grows at most linearly in the number of steps @xmath3 provided @xmath144 ( see @xcite for details ) . in the algorithm ,",
    "the weights @xmath145 are estimates of normalizing constants for the local filter , so one expects that if @xmath143 , then the algorithm should work well for large @xmath1 .",
    "there is , however , an important point to be made .",
    "the result above assumes an i.i.d .",
    "structure which removes any path degeneracy effect , both within a local filter , and in the time - dependence between observations .",
    "however , in general contexts one still expects that setting @xmath48 to be a polynomial function of @xmath1 should allow reasonable empirical performance .",
    "this is because the relative variance of the normalizing constant can be controlled in such path dependent cases , with polynomial cost ; see @xcite for example .    in the case of no global resampling , one would typically use the estimate , for @xmath118 @xmath146 a weak convergence result also holds in this case .",
    "we now adopt a context of no global resampling and consider the monte carlo error of the following two estimates , for @xmath53 , @xmath56 fixed and @xmath147 , @xmath148 and @xmath149 we remark that this is the simplest case in terms of analysis , as for example the case of when global resampling is considered is seemingly more complex .",
    "we now give our result ; the technical results for the proof can be found in appendix [ app : mc_avg ] .",
    "we set @xmath150 for @xmath151 , we denote @xmath152 .",
    "also @xmath153 are the continuous and real - valued functions on @xmath60 .",
    "let @xmath134 , for some @xmath135 and @xmath53 , @xmath18 fixed . then we have , for any @xmath147 , @xmath154    1 .",
    "@xmath155^{1/p } = 0\\ ] ] 2 .",
    "there exists an @xmath156 , depending upon @xmath157 only , such that @xmath158^{1/p } \\leq\\ ] ] @xmath159^{1/p}\\ ] ] where @xmath160 is as in .    for case 1 .",
    "we have that by proposition [ prop : conv_weights ] , and the continuous mapping theorem that ( after scaling the numerator and denominator by @xmath161 ) , for each @xmath162 @xmath163 where @xmath164 , for @xmath160 as in proposition [ prop : conv_weights ] . by standard importance sampling and resampling results ( see for instance @xcite ) ) , we have that @xmath165 by lemma [ lem : asymp_ind ] 2 .",
    ", these two terms are asymptotically independent .",
    "thus we have @xmath166 the proof of 1 .",
    "is complete on noting the boundedness of the associated quantities .    for case 2 .  by proposition [ prop : conv_weights ] , the fact that @xmath167 ( see e.g. @xcite ) and lemma  [ lem : asymp_ind ] 1 .",
    "we have @xmath168 where the @xmath169 are independent of the @xmath170 and have a distribution that has density @xmath171 .",
    "then , by the boundedness of the associated quantities we have @xmath158^{1/p}\\ ] ] @xmath172^{1/p}.\\ ] ] the proof can now be completed by the same calculations as in the proof of ( * ? ? ?",
    "* theorem 3.3 ) and are hence omitted .",
    "the main points are , first , that the error in estimation of fixed - dimensional marginals is independent of @xmath1 and , second , that averaging over the local particle cloud seems to help in high dimensions .",
    "we repeat that the scaling for @xmath48 that stabilises the weights for the global filter may be over - optimistic for more general models , due to the loss of a path - degeneracy effect over the observation times in the i.i.d .",
    "case .",
    "we now consider a more realistic scenario for our analysis in high - dimensions . in order to read this section , one will need to consult appendices [ sec : prf_consis ] and [ app : prf_nc ] ; this section can be skipped with no loss in continuity .",
    "we consider the interaction of the dimension and the time parameter in the behaviour of the algorithm .",
    "we will now list some assumptions and notations needed to describe the result .",
    "[ hyp : pstruc ] for every @xmath53 we have @xmath173 where @xmath174 , @xmath175 and for every @xmath176 , @xmath177 .",
    "it is noted that even under ( a[hyp : pstruc ] ) a standard particle filter which propagates all @xmath1 co - ordinates together may degenerate as @xmath1 grows .",
    "however , as we will remark , the stpf can stabilize under assumptions , even if @xmath50 .",
    "our algorithm will use the markov kernels @xmath178 as the proposals .",
    "define the semigroup , for @xmath179 : @xmath180 where @xmath181 . for @xmath112",
    "define @xmath182    [ hyp : semigroup ] there exists a @xmath183 , such that for every @xmath184 and @xmath185 @xmath186    note ( a[hyp : semigroup ] ) is fairly standard in the literature ( e.g.  @xcite ) and given ( a[hyp : pstruc ] ) it will hold under some simple assumptions on @xmath187 and @xmath43 .",
    "now , we will consider the global filter with @xmath4 particles , as standard results in the literature can provide immediately clts and sllns for quantities of interest .",
    "we will then investigate the effect of the dimensionality @xmath1 on the involved terms .",
    "consider the standard estimate for the normalising constant for the global filter @xmath188 when @xmath189 simply denotes monte - carlo averages over the @xmath4 particle systems at time @xmath157 , see appendix [ sec : prf_consis ] for analytic definitions . from standard particle filtering theory",
    ", we have that @xmath189 is an unbiased estimator of the corresponding limiting quantity , denoted @xmath190 , see e.g. ( * ? ? ?",
    "* theorem 7.4.2 ) .",
    "also , under our assumptions , one has the following clt as @xmath191 ( see ( * ? ? ?",
    "* proposition 9.4.2 ) ) @xmath192 where @xmath193 is the one dimensional normal distribution with zero mean and variance @xmath160 , and @xmath194 all bold terms correspond to standard feynman - kac quantities and are defined in appendix [ sec : prf_consis ] .",
    "we also show in appendix [ sec : prf_consis ] that the normalising constant of the global filter coincides with the one of the original filter of interest , that is @xmath195 thus , ( [ eq : clt ] ) provides in fact a clt for the estimate of stpf for @xmath196 proposed in theorem [ theo : consis ] .",
    "we have the following result , whose proof is in appendix [ app : prf_nc ] :    [ theo : nc_is_ok ] assume ( a[hyp : pstruc]-[hyp : semigroup ] ) .",
    "then there exist a @xmath197 such that for any @xmath198 and any @xmath199 @xmath200    our result establishes that the asymptotic in @xmath4 variance of the relative value of the normalizing constant estimate grows at most linearly in @xmath3 and , if @xmath143 does not grow with the dimension .",
    "the cost of the algorithm is @xmath2 .",
    "the linear growth in time is a standard result in the literature ( see @xcite ) and one does not expect to do better than this .",
    "note , that a particular model structure is chosen and one expects a higher cost in more general problems .",
    "we expect that to show that the error in estimation of the filter is time uniform , under ( a[hyp : pstruc ] ) , that one will need to set @xmath89 .",
    "this is because one is performing estimation on the path of the algorithm ; see ( * ? ? ?",
    "* theorem 15.2.1 and corollary 15.2.2 ) .",
    "indeed , one can be even more specific ; if @xmath50 , then one can show that , under ( a[hyp : pstruc]-[hyp : semigroup ] ) that the @xmath201-error associated to the estimate of the filter ( applied to a bounded test function in @xmath0 ) at time @xmath3 is upper - bounded by @xmath202 ( via ( * ? ? ?",
    "* theorem 15.2.1 , corollary 15.2.2 ) ) with @xmath203 independent of @xmath1 and @xmath3 .",
    "thus setting @xmath89 , the upper - bound depends on @xmath1 only through @xmath204 .",
    "we consider the following simple model .",
    "let @xmath8 be such that we have @xmath205 ( the @xmath1-dimensional vector of zeros ) and @xmath206 where @xmath207 and @xmath208 are some known static parameters .",
    "for the observations , we set @xmath209 where @xmath210 , @xmath95 . it is easily shown that this linear gaussian model has the structure .",
    "we consider the standard particle filter and the stpf .",
    "the data are simulated from the model with @xmath211 and @xmath212 @xmath1-dimensional observations .",
    "these parameters are also used within the filters .",
    "both filters use the model transitions as the proposal and the likelihood function as the potential .",
    "for stpf we use @xmath213 and @xmath214 , and for the particle filter algorithm we use @xmath215 particles .",
    "adaptive resampling is used in all situations ( with appropriate adjustment to the formula of calculating the weights for each of the @xmath4 particles , as well as the estimates ) .",
    "some results for @xmath216 are presented in figures  [ fig : exam1 mean ] to  [ fig : exam1 var ] .",
    "for example  [ sec : exam1 ] across 100 runs . ]     from a single run . ]     for example  [ sec : exam1 ] across 100 runs . ]",
    "the averages of estimators per time step ( for the posterior mean of the first co - ordinate @xmath217 given all date up to time @xmath3 ) across 100 separate algorithmic runs are illustrated in figure  [ fig : exam1 mean ] .",
    "for stpf , the estimator corresponds to the double average over @xmath48 , @xmath4 as shown in section [ sec : algo ] .",
    "the figure shows that the particle filter collapses when the dimension become moderate or large .",
    "it is unable to provide meaningful estimates when @xmath218 ( as the estimates completely lose track of the observations ) .",
    "in contrast , the stpf performs reasonably well in all three cases . in figure",
    "[ fig : exam1 ess ] we can observe the ess ( scaled by the number of particles ) for each time step of the two algorithms .",
    "the standard filter struggles significantly even in the case @xmath219 and it collapses when @xmath218 .",
    "the performance of the new algorithm is deteriorating ( but not collapsing ) when the dimension increases .",
    "this is inevitably due to the path degeneracy effect that we have mentioned .",
    "these conclusions are further supported in figure  [ fig : exam1 var ] where the variance per time step for the estimators of the posterior mean of the first co - ordinate @xmath217 ( given the data up to time @xmath3 ) across 100 runs is displayed .",
    "we consider the following model on a two - dimensional graph , which follows that described in @xcite .",
    "let the components of state @xmath220 be indexed by vertices @xmath221 , where @xmath222 .",
    "the dimension of the model is thus @xmath223 .",
    "the distance between two vertices , @xmath224 and @xmath225 , is calculated in the usual euclidean sense , @xmath226 . at time @xmath3",
    ", @xmath227 follows a mixture distribution , @xmath228 where @xmath229 for @xmath230 is the neighborhood of vertex @xmath231 . for observations , @xmath209 where @xmath232 , @xmath221 are i.i.d .  @xmath233-distributed random variables with degree of freedom @xmath234 .",
    "in this example , we use a gaussian mixture with component mean @xmath235 and unity variance .",
    "the weights are set to be @xmath236 and @xmath237 .",
    "in other words , when @xmath238 , each vertex evolves as a gaussian random walk itself . we simulated data from model @xmath239 , @xmath240 , @xmath241 and @xmath242 .",
    "it results in a 1024 dimensional model .",
    "these parameters are also used in the filters .",
    "we will compare the standard particle filter , the stpf , the marginal stpf algorithm ( as described in section  [ sec : path_degen ] ) and the block particle filter ( bpf ) in @xcite ( notice that the block particle filter is characterised by space varying bias , by construction ) .",
    "the simulations for the stpf versions are done with @xmath243 .",
    "the number of particles for the standard particle filter and bpf are @xmath215 . for the marginal algorithm",
    ", we also simulated with @xmath244 and @xmath245 .",
    "the block size of bpf is set to be @xmath246 , @xmath247 , and it is partitioned such that each block is itself a square .",
    "the mcmc moves of the marginal algorithm are simple gaussian random walks with standard deviation ( the scale ) being @xmath248 .",
    "the optimal block size in @xcite is about @xmath249 for ten thousand particles and a two - dimensional graph .",
    "thus , we considered the cases @xmath250 and  @xmath251 , the two nearest integers such that @xmath1 is divisible by @xmath252 .",
    "a single run takes around 2 minutes for the standard particle filter and the block filter on an intel xeon w3550 cpu , with four cores and eight threads .",
    "it takes around 10 minutes for the stpf .",
    "it takes about 40 minutes for the marginal algorithm with @xmath244 and @xmath245 , and about 7 hours for @xmath253 .",
    "the standard particle filter performs poorly and can not provide adequate estimates ( similar to the @xmath254 case in the previous example ) . in figure",
    "[ fig : exam2 var ] , we observe the variance per time step of the estimators for two vertices , across 30 runs .",
    "the first vertex , @xmath255 is not on the boundary of either block size and the second , @xmath256 is on the boundary of both block sizes . in either case , the stpf significantly outperforms the block filter , albeit under slightly longer run times .",
    "the stpf does not collapse in high - dimensions , but perhaps does not have excellent performance .",
    "the marginal stpf performs very well , but the computational time is substantially higher than all of the other algorithms .",
    "however , with @xmath244 and @xmath143 , the marginal stpf provides a good balance between performance and computational cost in challenging situations where the path degeneracy may hinder successful application of the new algorithm .",
    "the block filter variance for @xmath256 ( boundary vertex ) is about twice that of @xmath255 while the new algorithm performs equally well for both cases .     and @xmath256 for example  [ sec : exam2 ] .",
    "the variances are estimated from 100 simulations for each algorithm . ]",
    "in this article we have considered a novel class of particle algorithms for high - dimensional filtering problems and investigated both theoretical and practical aspects of the algorithm .",
    "we believe the article opens new directions in an important and challenging monte - carlo problem , and several aspects of the method remain to be investigated in future research .",
    "there are indeed several possible extensions to the work in this article .",
    "in particular , an analysis of the algorithm when the structure of the state - space model is more complex than the structures considered in this article . we expect that in such scenarios , that the cost of the algorithm should increase , but only by a polynomial factor in @xmath1 .",
    "in addition , the interaction of dimension and time behaviour is of particular interest .",
    "ajay jasra and yan zhou were supported by acrf tier 2 grant r-155 - 000 - 143 - 112 .",
    "we thank pierre del moral for many useful conversations on this work .",
    "we set @xmath257 notice that @xmath258=\\mathbb{e}[x]=1 $ ] , so that due to the i.i.d .",
    "structure along @xmath79 we have that @xmath259 = \\frac{1}{n}\\big ( \\mathbb{e}[x^2 ]   \\big)^d + \\frac{n-1}{n}\\end{aligned}\\ ] ] also , due to the i.i.d .",
    "structure along @xmath260 we have @xmath261=   \\frac{1}{m_d}\\frac{\\int a^2(x)/q(x)dx}{\\big ( \\int a(x)dx \\big)^2 } + \\frac{m_d-1}{m_d}.\\end{aligned}\\ ] ] finally , we have that , due to i.i.d .",
    "structure along @xmath3 , @xmath262   & = \\mathbb{e}\\big[\\big(\\frac{p^{n , m_d}(y_{1:n})}{\\big(\\int\\alpha(x)dx\\big)^{nd}}\\big)^2\\big ] - 1 \\\\    & = ( \\mathbb{e}[i^2])^n - 1.\\end{aligned}\\ ] ] a synthesis of the above three equations gives the required result .",
    "in order to prove theorem [ theo : consis ] , we will first introduce another round of notations .",
    "let @xmath263 be a sequence of measurable spaces endowed with a countably generated @xmath133-field @xmath264 .",
    "the set @xmath265 denotes the class of bounded @xmath266-measurable functions on @xmath267 where @xmath268 is the borel @xmath133-algebra on @xmath60 .",
    "we will consider non - negative operators @xmath269 such that for each @xmath270 the mapping @xmath271 is a finite non - negative measure on @xmath264 and for each @xmath272 the function @xmath273 is @xmath274-measurable ; the kernel @xmath275 is markovian if @xmath276 is a probability measure for every @xmath270 .",
    "for a finite measure @xmath132 on @xmath277 and borel test function @xmath278 we define @xmath279      we will define a feynman - kac model on an appropriate enlarged space .",
    "that is , one markov transition on the enlarged space will correspond to one observation time and will collect all @xmath1 space - steps of the local filter for this time - step .",
    "some care is needed with the notation , as we need to keep track of the development of the co - ordinates at time @xmath3 , together with the states at time @xmath280 as the latter are involved in the proposal .    *",
    "time - step 1 * : consider observation time 1 of the algorithm .",
    "we define a sequence of random variables @xmath281 with @xmath282 , @xmath283 , such that @xmath284 , for @xmath95 , and @xmath285 .",
    "for @xmath95 we will write the co - ordinates of @xmath281 as @xmath286 , with the obvious extension for the case @xmath287 . as @xmath288 is fixed , we will drop it from our notations , as will become clear below .",
    "also , for simplicity we simply write @xmath121 instead of the analytical @xmath289 as the subscripts are implied by those of @xmath290 .",
    "we follow this convention throughout appendix [ sec : prf_consis ] .",
    "we define the following sequence of markov kernels corresponding to the proposal for the co - ordinates at the first time step : @xmath291 next , we will take under consideration the weights and the resampling . for @xmath95 and",
    "a probability measure @xmath132 on @xmath292 define @xmath293 for the local particle filter in observation time 1 , write the un - weighted empirical measure @xmath294 we also consider all random variables involved at time - step 1 and set @xmath295 the joint law of the samples required by the local filter is @xmath296 notice , that in the notation we have established herein , the potential @xmath297 defined in the main text can now equivalently be expressed as @xmath298 we also set @xmath299 .",
    "* time - step @xmath300 * : at subsequent observation times , @xmath73 , we again work with variables denoted @xmath301 , with @xmath282 , but this time we have to keep track of the corresponding paths at time @xmath280 , thus we will use the notation @xmath302 , with @xmath303 , @xmath304 , with the latter component referring to the ` tail ' at time @xmath280 of the path found at @xmath305 at time @xmath3 and space position @xmath79 .",
    "so , we have @xmath306 , @xmath95 and @xmath307 .",
    "we define the following sequence of kernels : @xmath308 for @xmath63 and a probability measure @xmath132 on @xmath309 define the measure on @xmath310 @xmath311 for the local particle filter at space - step @xmath79 , we write the empirical measure @xmath312 set @xmath313 .",
    "the transition law of all involved samples in the local particle filter is @xmath314 then , we will work with the potential @xmath315 the algorithm described in section [ sec : algo ] corresponds to a standard particle filter approximation ( with @xmath4 particles ) of a feynman - kac model specified by the initial distribution ( [ eq : eta_1_def ] ) , the markovian transitions ( [ eq : m_n_def ] ) and the potentials in ( [ eq : g_1_def ] ) , ( [ eq : g_n_def ] ) .",
    "thus , for the monte - carlo algorithm with @xmath4 particles , set @xmath316 for the @xmath4-empirical measure of @xmath317 and set , for @xmath132 a probability measure , @xmath73 @xmath318 then our global filter samples from the path measure , up - to observation time @xmath3 @xmath319 not including resampling at observation time @xmath3 .",
    "we use the standard definition of the normalising constant for any @xmath53 @xmath320 and set @xmath321 thus @xmath322 corresponds to the predictive distribution at time @xmath3 for the global filter .",
    "notice , that from ( [ eq : island_nc ] ) , we can equivalently write for the unnormalised measure @xmath323      we consider functions of the particular form @xmath324 for functions of the above type , we write @xmath325 .",
    "we will illustrate that upon application on this family , several feynman - kac quantities of the global model ( with signal dynamics @xmath326,@xmath327 and potentials @xmath328 ) coincide with those of the original model of interest ( with signal dynamics @xmath329 and potentials @xmath330 ) . in particular we calculate @xmath331 as , from ( [ eq : ante ] ) , it is the building block for other expressions .",
    "notice we can write @xmath332 so , the integral concerns now the local particle filter with weights @xmath333 and markov kernels @xmath334 .",
    "in particular , the integral corresponds to the expected value of the particle approximation of the standard feynamn - kac unnormalised estimator with standard unbiasedness properties ( * ? ? ? * theorem 7.4.2 ) .",
    "that is , the integral is equal to ( here , for each @xmath335 , the process @xmath336 is a markov chain evolving via @xmath337 @xmath338 respectively ) @xmath339.\\ ] ] from the analytical definition of the kernels and the weights , this latter quantity is easily seen to be equal to @xmath340 so , we have obtained that @xmath341 thus , applying the above result recursively , we obtain from ( [ eq : ante ] ) that @xmath342 using the standard feynman - kac notation , this latter integral can be denoted as @xmath343 for the unnormalised measure @xmath344 .",
    "thus , for instance , for the normalising constants , we have that @xmath345      we have established that the algorithm is a standard particle filter approximation of a feynman - kac formula on an extended space",
    ". thus , standard results , e.g.  in @xcite , will give consistency for monte - carlo estimates on the enlarged state - space .",
    "in only remains to show that indeed the quantities in the statement of theorem [ theo : consis ] correspond to monte - carlo averages of the global filter in the enlarged space . we look directly at the last two quantities in the statement of the theorem , as the derivation for the first two ones is similar and simpler . for",
    "the first we set @xmath346 and we immediately have that ( denoting by @xmath347 the resampled islands , under the weights @xmath348 ) @xmath349 notice now that the quantity on the left is precisely the double average in the statement of the theorem and the quantity on the right , from ( [ eq : aa ] ) , is equal to @xmath350 . for the last statement in the theorem ,",
    "the quantity on the left is @xmath351 which , from standard particle filter theory converges in probability to @xmath352 .",
    "recall the notation for the global filter from appendix [ sec : prf_consis ] .",
    "we define the semi - group @xmath374 and we also set @xmath375 recall from the main result in ( [ eq : important ] ) in appendix [ sec : prf_consis ] , connecting the global with the local filter , that @xmath376 , and upon an iterative application of this result @xmath377 we also have that @xmath378 and , finally , that @xmath379 . using all these expressions , simple calculations will give @xmath380 where we have defined @xmath381 the main thing to notice now , is that @xmath382 corresponds to the standard estimate of the normalising constant for the @xmath157-th local filter divided with its expected value , and we can use standard results from the literature to control its second moment . indeed , by assumptions ( a[hyp : pstruc]-[hyp : semigroup ] ) and ( * ? ? ?",
    "* theorem 16.4.1 ) ( see remark [ rem : nc_rev_var ] ) , there exists @xmath383 ( which does not depend on @xmath157 or @xmath384 ) such that for any @xmath185 and any @xmath385 @xmath386 where the upper - bound only depends on @xmath1 via the term @xmath387 .",
    "notice also that @xmath388 , so by ( a[hyp : semigroup ] ) and jensen s inequality ( so that @xmath389 for positive @xmath390 ) , we have @xmath391 thus , returning in ( [ eq : bb ] ) , and using the last two equations , we get , starting with the @xmath392-inequality @xmath393 from here , one can easily complete the proof and hence we conclude .",
    "[ rem : nc_rev_var ] in the proof of theorem [ theo : nc_is_ok ] we have used ( * ? ? ? * theorem 16.4.1 ) .",
    "this is a result on the relative variance of the particle estimate of the normalizing constant , and as stated in @xcite does not include a function , i.e.  an estimate of the form @xmath394 for some @xmath112 .",
    "based on personal communication with pierre del moral , ( * ? ? ?",
    "* theorem 16.4.1 ) can be extended to include a function , by modification of the potential functions and the use of the final formula in @xcite .            ,",
    "p. , li , b. & bengtsson , t.  ( 2008 ) .",
    "sharp failure rates for the bootstrap particle filter in high dimensions . in _ pushing the limits of contemporary statistics _ , b. clarke & s. ghosal , eds , 318329 , ims .",
    "below let @xmath353 be a random variable with probability density @xmath354 . recall that @xmath355 is particle @xmath162 , local particle @xmath335 at observation time @xmath3 , dimension @xmath1 and it has just been locally resampled using the weights @xmath356 .",
    "recall that there is no global resampling . throughout @xmath134 (",
    "assumed to be integer , for notational convenience ) .",
    "we first consider statement 1 .",
    "set @xmath361 and consider the standardised quantity @xmath362 , then we have that for @xmath363 fixed , @xmath364 = \\\\",
    "\\mathbb{e}\\bigg[\\exp\\big\\{rt_1\\overline{\\mathbf{g}}_n(x_n^{i,1:m_d}(1:d))\\big\\ }   \\frac{\\sum_{l=1}^{m_d}g_{n , d}(x_{n}^{i , l}(d))e^{rt_2\\varphi(x^{i , l}_n(d))}}{\\sum_{l=1}^{m_d}g_{n , d}(x_{n}^{i , l}(d ) ) } \\bigg ] .",
    "\\label{eq : nono}\\end{gathered}\\ ] ] by standard slln , we have that @xmath365 also , proposition [ prop : conv_weights ] implies that @xmath366 where @xmath367 for @xmath160 defined therein . hence , from slutsky s lemmas we have @xmath368 the proof of 1 .",
    "is concluded on noting the boundedness of the functions .    for the proof of 2 .",
    "we have @xmath369 = \\nonumber \\\\ \\mathbb{e}\\big[e^{rt_1\\overline{\\mathbf{g}}_n(x_n^{i,1:m_d}(1:d))}\\big[e^ { rt_2 \\frac{1}{m_d}\\sum_{l=1}^{m_d}\\varphi(\\check{x}^{i , l}_n(d ) ) } -e^ { rt_2\\pi(\\varphi ) } \\big]\\big ] + e^ { rt_2\\pi(\\varphi)}\\mathbb{e}\\big[e^{rt_1\\overline{\\mathbf{g}}_n(x_n^{i,1:m_d}(1:d))}\\big]\\nonumber \\\\ = : a_d + b_d   \\label{eq : an}\\end{gathered}\\ ] ] where we have used the short - hand @xmath370 . from standard importance sampling and resampling results ( see e.g. @xcite )",
    ", we have that @xmath371 so , returning to ( [ eq : an ] ) , we have obtained that @xmath372 , thus @xmath373 = \\exp\\ { rt_2\\pi(\\varphi)\\}\\mathbb{e}[\\exp\\{rt_1v_{n}^i\\}]\\ ] ] which concludes the proof of 2 .."
  ],
  "abstract_text": [
    "<S> we consider the numerical approximation of the filtering problem in high dimensions , that is , when the hidden state lies in @xmath0 with @xmath1 large . for low dimensional problems , </S>",
    "<S> one of the most popular numerical procedures for consistent inference is the class of approximations termed particle filters or sequential monte carlo methods . </S>",
    "<S> however , in high dimensions , standard particle filters ( e.g. the bootstrap particle filter ) can have a cost that is exponential in @xmath1 for the algorithm to be stable in an appropriate sense . </S>",
    "<S> we develop a new particle filter , called the _ space - time particle filter _ , for a specific family of state - space models in discrete time . </S>",
    "<S> this new class of particle filters provide consistent monte carlo estimates for any fixed @xmath1 , as do standard particle filters . </S>",
    "<S> moreover , we expect that the state - space particle filter will scale much better with @xmath1 than the standard filter . </S>",
    "<S> we illustrate this analytically for a model of a simple i.i.d . structure and one of a markovian structure in the @xmath1-dimensional space - direction , </S>",
    "<S> when we show that the algorithm exhibits certain stability properties as @xmath1 increases at a cost @xmath2 , where @xmath3 is the time parameter and @xmath4 is the number of monte carlo samples , that are fixed and independent of @xmath1 . </S>",
    "<S> similar results are expected to hold , under a more general structure than the i.i.d .  </S>",
    "<S> one . </S>",
    "<S> our theoretical results are also supported by numerical simulations on practical models of complex structures . </S>",
    "<S> the results suggest that it is indeed possible to tackle some high dimensional filtering problems using the space - time particle filter that standard particle filters can not handle . </S>",
    "<S> + * keywords * : state - space models ; high - dimensions ; particle filters .    * a stable particle filter in high - dimensions *    by alexandros beskos , dan crisan , ajay jasra , kengo kamatani , & yan zhou    department of statistical science , university college london , london , wc1e 6bt , uk . e-mail:`a.beskos@ucl.ac.uk ` + department of mathematics , imperial college london , london , sw7 2az , uk . </S>",
    "<S> + e-mail:`d.crisan@ic.ac.uk ` + department of statistics & applied probability , national university of singapore , singapore , 117546 , sg . </S>",
    "<S> + e-mail:`staja@nus.edu.sg , stazhou@nus.edu.sg ` + graduate school of engineering science , osaka university , osaka , 565 - 0871 , jp . e-mail:`kamatani@sigmath.es.osaka-u.ac.jp ` + </S>"
  ]
}