{
  "article_text": [
    "we consider the problem of building a diffusion operator on directed networks and graphs with adjacency matrix @xmath0 that parallels diffusion maps on undirected networks @xcite . a natural approach to this is to build the magnetic eigenmaps laplacian @xcite , @xmath1 where @xmath2 and @xmath3 .",
    "this method assigns a complex rotation to asymmetric links .",
    "this approach generalizes the notion of building a diffusion map or graph embedding to work on directed graphs .    in this note",
    ", we observe that markov normalization of the adjacency matrix , @xmath4 allows us to introduce the notion of diffusion time steps , which is capable of capturing the evolution of the process .",
    "markov normalization of the graph adjacency matrix yields several benefits :    1 .   normalizing the complex rotation matrix @xmath5 by the density of the node , which emphasizes absorbing states and converges to the stationary distribution of the process , 2 .",
    "stabilizing the phase embedding with respect to choice of the rotation parameter @xmath6 , 3 .   separating the long time trend of the process into the first eigenvector and recurrent states into subsequent eigenvectors , and 4 .   introducing diffusion time , which allows for the study of multi - step neighborhoods .",
    "we further note that it is possible to augment the adjacency matrix by a transportation factor , as is done in the pagerank algorithm , when there exists an absorbing state @xcite . for a small @xmath7",
    ", we replace @xmath8 .",
    "this creates a small probability of jumping from any node to any other node , and thus escaping a sink in the process and making the process ergodic .",
    "for the rest of the paper we will assume that the adjacency matrix @xmath9 is ergodic , under the pretense that if there is an absorbing state , we simply add a transportation factor .",
    "an important aspect of diffusion maps is the ability to separate the statistics of the sampling distribution from the geometry of the underlying manifold @xcite .",
    "this is accomplished by normalizing the affinity between points by the overall transition probability ,",
    "@xmath10    this also allows us to consider the properties of the multiple time step diffusion process @xmath11 for @xmath12 . for a symmetric process ( i.e. undirected graph ) ,",
    "the embedding of this process remains stable .",
    "this is because , if the eigendecomposition is @xmath13 , then @xmath14 however , for a non - symmetric graph , this property no longer holds .",
    "thus the eigenvectors of @xmath11 will exhibit time dependence beyond a simple multiplicity factor .",
    "the markov normalized magnetic laplacian then takes the form of @xmath15    adjusting the weight matrix to be markov normalized has several implications on the magnetic laplacian . in diffusion on a symmetric graph , normalizing by the node density causes the principal eigenvector @xmath16 to be a constant across all nodes and is thus trivial .",
    "however , when the network is not symmetric , the principal eigenvector takes on a much more important role .",
    "namely , the principal eigenvector recovers the degree of asymmetry between nodes .",
    "this implies that , as @xmath17 increases , the phase of the eigenvectors converges to the page rank of the network .",
    "suppose there exists @xmath18 such that @xmath19 for all @xmath20 .",
    "let @xmath21 be the normalized magnetic laplacian .",
    "then in the limit as @xmath22 , the principal eigenvector of the normalized laplacian @xmath23 ( denoted @xmath24 ) converge to @xmath25 where @xmath26 ( i.e. the stationary distribution of @xmath9 ) , and @xmath27 .    from theorem 1 in @xcite , we know that the magnetic laplacian @xmath28 has a zero eigenvalue iff @xmath29 such that @xmath30 . furthermore , we know that , in this case , @xmath31 . we also know that , since @xmath9 is ergodic by assumption , @xmath32 as @xmath22 , where @xmath26 .",
    "that means @xmath33 .",
    "let @xmath34 be the principal eigenvector of @xmath35 , where @xmath36 and @xmath37 .",
    "then @xmath38 where @xmath39 . because @xmath40 is row stochastic , we know @xmath41 , so @xmath42 .",
    "since @xmath31 , we know @xmath43    this only applies in the limit as @xmath22 .",
    "the embeddings of @xmath44 for small @xmath17 , and even @xmath45 , yield valuable properties due to the markov normalization .",
    "we show this empirically in section [ examples ] .",
    "in all examples of the markov normalized process , we will refer to a value of @xmath46 that is on the same scale as for the unnormalized laplacian .",
    "however , due to the weights being normalized to sum to 1 , they are actually on a different scale than the unnormalized weights matrix . for that reason , we set @xmath47 for the markov normalized weights so that the degree of rotation is on the same order of magnitude for both methods .",
    "we begin with the three cluster example from @xcite .",
    "this example is particularly simple given that the complex rotation of three clusters will , more often than not , keep the clusters separated .",
    "however , for small values of @xmath6 , rotation of the unnormalized asymmetric weights matrix yields insufficient separation between the clusters , as shown in figure [ fig : theeclusters ] .",
    "in fact , figure [ fig : threeclustersrandg ] shows the percentage of points that cluster correctly across 100 uniform random small values of @xmath6 ( @xmath48 ) , where clustering was done by k - means with @xmath49 ."
  ],
  "abstract_text": [
    "<S> we note that building a magnetic laplacian from the markov transition matrix , rather than the graph adjacency matrix , yields several benefits for the magnetic eigenmaps algorithm . </S>",
    "<S> the two largest benefits are that the embedding becomes more stable as a function of the rotation parameter g , and the principal eigenvector of the magnetic laplacian now converges to the page rank of the network as a function of diffusion time . </S>",
    "<S> we show empirically that this normalization improves the phase and real / imaginary embeddings of the low - frequency eigenvectors of the magnetic laplacian . </S>"
  ]
}