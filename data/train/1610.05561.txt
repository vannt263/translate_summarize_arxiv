{
  "article_text": [
    "optimal experiment design ( oed ) or shortly optimal design @xcite is a sub - field of optimal control theory which concentrates design of an optimal control law aiming at the maximization of the information content in the response of a dynamical system related to its parameters .",
    "the statistical advantage brought by information maximization helps the researchers to generate the best input to their target plant / system that can be used in a system identification experiment producing estimates with minimum variance @xcite . with the utilization of mathematical models in theoretical neuroscience research ,",
    "the application of optimal experiment design in adaptive stimuli generation should be beneficial as it is expected to have better evaluations of the model specific parameters from the collected stimulus - response data .",
    "though these benefits , the optimal experiment design have not found its place among theoretical or computational neuroscience research due to the nature of the models . as the stimulus - response relationship is naturally quite non - linear , computational complexity of the optimization algorithms utilized for an optimal experiment design will typically be very high and thus oed has not gained enough attraction during the past decades .",
    "however , thanks to the today s computational powers of new microprocessors , it will be much easier to talk about a real optimal experiment design in neuroscience research ( @xcite,@xcite ) . in the past decades",
    ", some researchers had stimulated their models by gaussian white noise stimuli @xcite , @xcite and performed an estimation of input - output relationships of their model ( @xcite @xcite and @xcite ) .",
    "this algorithmically simpler approach is theoretically proven to be efficient in the estimation of models based on linear filters and their cascades . however , in @xcite , it is suggested that white noise stimuli may not be successful as a stimuli in the parametric identification of non - linear response models due to high level of parameter confounding ( refer to @xcite for a detailed description of the confounding phenomenon in non - linear models ) .    concerning the applications of optimal experiment design to biological neural network models ,",
    "there exist a limited amount of research .",
    "one such example is @xcite where a static non - linear input output mapping is utilized as a neural stimulus - response model .",
    "the optimal design of the stimuli is performed by the maximization of the d - optimal metric of the fisher information matrix ( fim ) @xcite which reflects a minimization of the variance of the total parametric error of the model network . in the last research ,",
    "the parameter estimation is based on the maximum a posteriori ( map ) estimation methodology @xcite which is linked to the maximum likelihood estimation ( ml ) @xcite approach .",
    "two other successful mainly experimental work on applications of optimal experiment design to adaptive data collection are @xcite and @xcite .",
    "the experimental works successfully proven the efficiency of optimal designs for certain models in theoretical neuroscience . however , none of those studies explore fully dynamical non - linear models explicitly . because of this deficiency , this research will concentrate on an application of the optimal experiment design to a fully dynamical non - linear model .",
    "the final goal is almost similar to that of @xcite .",
    "the proposed model is a continuous time dynamical recurrent neural network ( ctrnn ) @xcite in general and it also represents the excitatory and inhibitory behaviours @xcite of the realistic biological neurons .",
    "like in that of @xcite and its derivatives , the ctrnn describes the dynamics of the membrane potentials of the constituent neurons .",
    "however , the channel activation dynamics is not directly represented .",
    "instead it constitutes , a more generic model which can be applied to a network having any number of neurons .",
    "the dynamic properties of the neuron membrane is represented by time constants and the synaptic excitation and inhibition are represented as network weights ( scalar gains ) . though not the same , a similar excitatory - inhibitory structure is utilized in numerous studies such as @xcite .",
    "as there is nt sufficient amount of research on the application of oed to dynamical neural network models , it will be convenient to start with a basic network model having two neurons representing the average of excitatory and inhibitory populations respectively .",
    "the final goal is to estimate the time constants and weight parameters .",
    "the optimal experiment design will be performed by maximizing a certain metric of the fim .",
    "the fim is a function of stimulus input and network parameters .",
    "as the true network parameters are not known in the actual problem , the information matrix should depend on the estimated values of the parameters in the current step .",
    "an optimization on a time dependent variable like stimulus will not be easy and often its parametrization is required . in auditory neuroscience point of view , that can be done by representing the stimuli by a sum of phased cosine elements .",
    "if periodic stimulation is allowed , these can be formed as harmonics based on a base stimulation frequency .",
    "the optimally designed stimulus will be the driving force of a joint maximum likelihood estimation ( jmle ) process which involves all the recorded response data .",
    "unfortunately , the recorded response data will not be continuous .",
    "the reason for this is that , in vivo measurements of the membrane potentials are often very difficult and dangerous as the direct manipulation with the neuron in vivo may trigger the death of a neuron .",
    "thus , in the real experimental set - up , the peaks of the membrane potentials are collected as firing instants . as a result",
    ", one will only have a neural spike train with the exact neural spiking times ( timings of the membrane potential peaks ) but no other data .",
    "this outcome prevents one to apply traditional parameter estimation techniques such as minimum mean square estimation ( mmse ) as it will require continuous firing rate ( is based on the membrane potential ) data .",
    "researches like @xcite , suggests that the neural spiking profile of sensory neurons obey the famous inhomogeneous poisson distribution @xcite . under this assumption",
    ", the fisher information matrix @xcite and likelihood functions @xcite can be derived based on poisson statistical distribution .",
    "the optimization of a certain measure of fisher information matrix and the likelihood can be performed by readily available packages such as matlab^^ optimization toolbox ( like well known _ fmincon _",
    "algorithm ) .",
    "there are certain challenges in this research .",
    "first of all , the limited availability of similar studies lead to the fact that this work is one of the first contributions on the applications of optimal experiment design to the dynamical neural network modelling .",
    "secondly , we will most probably not be able to have a reasonable estimate just from a single spiking response data set as we do not have a continuous response data .",
    "this is also demonstrated in the related kernel density estimation research such as @xcite . from these sources",
    ", one will easily note that repeated trials and superimposed spike sequences are required to obtain a meaningfully accurate firing rate information from the neural response data . in a real experiment environment , repeating the trials with the same stimulus profile will not be appropriate as the repeated responses of the same stimulus are found to be attenuated . because of this issue , a new stimulus should be designed each time based on the currently estimated parameters of the model and then it should be used in an updated estimation .",
    "these updated parameters are used in the next step to generate the new optimal stimulus . as a result one",
    "will have a new stimulus in each step and thus the risk of response attenuation is largely reduced . in a maximum likelihood estimation ,",
    "the likelihood function will depend on the whole spiking data obtained throughout the experiment ( or simulation ) .",
    "the parallel processing capabilities of matlab^^ ( i.e. _ parfor _ ) on multiple processor / core computers will help in resolving of those issues .",
    "the continuous time recurrent neural networks have a similar structure to that of the discrete time counterparts that are often met in artificial intelligence studies . in * figure",
    "[ fig : generic - ctrnn ] * , one can see a general continuous time network that may have any number of neurons .",
    "the mathematical representation of this generic model can be written as shown below @xcite : @xmath0 where @xmath1 is the time constant , @xmath2 is the membrane potential of the @xmath3 neuron , @xmath4 is the synaptic connection weight between the @xmath3 and @xmath5 neurons @xmath6 is the connection weight from @xmath7 input to the @xmath3 neuron and @xmath8 is the @xmath7 input .",
    "the term @xmath9 is a membrane potential dependent function which acts as a variable gain on the synaptic inputs to from the @xmath5 neuron to the @xmath3 one .",
    "it can be shown by a logistic sigmoid function which can be shown as : @xmath10 where @xmath11 is the maximum rate at which the @xmath5 neuron can fire , @xmath12 is a soft threshold parameter of the @xmath5 neuron and @xmath13 is a slope constant .",
    "this is the only source of non - linearity in .",
    "in addition it also models the activation - inactivation behaviour in more specific models of the neuron ( like @xcite ) .",
    "the work by @xcite shows that gives a relationship between the firing rate @xmath14 and membrane potential @xmath15 of the @xmath5 neuron . in",
    "sensory nervous system , some of neurons have excitatory synaptic connections while some have inhibitory ones .",
    "this fact is reflected to the model in by assigning negative values to the weight parameters which are originating from neurons with inhibitory synaptic connections . in the introduction of this research",
    ", it is stated that it would be convenient to apply the theory to a basic network first of all due to the lack of related research and computational complexity .",
    "so a basic excitatory and inhibitory continuous time recurrent dynamical network can be written as shown in the following : @xmath16 where the subscripts @xmath17 and @xmath18 stands for excitatory and inhibitory neurons respectively .",
    "starting from now on , we will have a single stimulus and it will be represented by the term @xmath19 which will be generated by the optimal design algorithm .",
    "in addition in order to suit the model equations to the estimation theory formalism the time constant may be moved to the right hand side as shown below :    @xmath20=\\left[\\begin{array}{cc }          \\beta_{e } & 0\\\\          0 & \\beta_{i }          \\end{array}\\right]\\left\\ { -\\left[\\begin{array}{c }          v_{e}\\\\          v_{i }          \\end{array}\\right]+\\left[\\begin{array}{cc }          w_{ee } & -w_{ei}\\\\          w_{ie } & -w_{ii }          \\end{array}\\right]\\left[\\begin{array}{c }          g_{e}\\left(v_{e}\\right)\\\\          g_{i}\\left(v_{i}\\right )          \\end{array}\\right]+\\left[\\begin{array}{c }          w_{e}\\\\          w_{i }          \\end{array}\\right]i\\right\\ } \\label{eq : our - model - matrix - form}\\ ] ]    note that this equation is written in matrix form to be conformed to the formal non - linear system forms . a descriptive illustration related to",
    "is presented in * figure [ fig : generic - ctrnn]b*. it should also be noted that , in and the weights are all assumed as positive coefficients and they have signs in the equation .",
    "so negative signs indicate that originating neuron is inhibitory ( tend to hyper - polarize the other neurons in the network )",
    ".      the theoretical response of the network in will be the firing rate of the excitatory neuron as @xmath21 . in the actual environment ,",
    "the neural spiking due to the firing rate @xmath22 is available instead . while introducing this research ,",
    "it is stated that this spiking events conform to an inhomogeneous poisson process which is defined below : @xmath23=\\frac{e^{-\\lambda } \\lambda^{k}}{k ! }      \\label{eq : inhomogeneous - poisson}\\ ] ] where @xmath24 is the mean number of spikes based on the firing rate @xmath25 which varies with time , and @xmath26 indicates the cumulative total number of spikes up to time @xmath27 , so that @xmath28 is the number of spikes within the time interval @xmath29 . in other words , the probability of having @xmath30 number of spikes in the interval @xmath31",
    "is given by the poisson distribution above .",
    "consider a spike train @xmath32 in the time interval @xmath33 . here",
    "the spike train is described by a list of the time stamps for the @xmath34 spikes .",
    "the probability density function for a given spiking train @xmath32 can be derived from the inhomogeneous poisson process @xcite .",
    "the result reads : @xmath35 this probability density describes how likely a particular spike train @xmath32 is generated by the inhomogeneous poisson process with the rate function @xmath36 .",
    "of course , this rate function depends implicitly on the network parameters and the stimulus used .",
    "the network parameters to be estimated are listed below as a vector : @xmath37   = \\left[\\beta_e,\\beta_i , w_e , w_i , w_{ee},w_{ei},w_{ie},w_{ii}\\right]\\label{eq : theta - ctrnn - param}\\ ] ] which includes the time constants and all the connection weights in the e - i network .",
    "our maximum - likelihood estimation of the network parameters is based on the likelihood function given by , which takes the individual spike timings into account .",
    "it is well known from estimation theory is that maximum likelihood estimation is asymptotically efficient , i.e. , reaching the cramr - rao bound in the limit of large data size .    to extend the likelihood function in to the situation where there are multiple spike trains elicited by multiple stimuli , consider a sequence of @xmath38 stimuli .",
    "suppose the @xmath39-th stimulus ( @xmath40 ) elicits a spike trains with a total of @xmath41 spikes in the time window @xmath42 $ ] , and the spike timings are given by @xmath43 . by ,",
    "the likelihood function for the spike train @xmath44 is @xmath45 where @xmath46 is the firing rate in response to the @xmath39-th stimulus .",
    "note that the rate function @xmath46 depends implicitly on the network parameters @xmath47 and on the stimulus parameters .",
    "the left - hand side of emphasizes the dependence on network parameters @xmath47 , which is convenient for parameter estimation .",
    "the dependence on the stimulus parameters will be discussed in the next section .",
    "we assume that the responses to different stimuli are independent , which is a reasonable assumption when the inter - stimulus intervals are sufficiently large . under this assumption",
    ", the overall likelihood function for the collection of all @xmath38 spike trains can be written as @xmath48 by taking natural logarithm , we obtain the log likelihood function : @xmath49 maximum - likelihood estimation of the parameter set is given formally by @xmath50 numerical issues related to this optimization problem will be discussed in * sections [ sub : grad - computation ] * and * [ sub : other - numerical]*. in addition , some discussion on the local maxima problems is provided in * section [ sub : local - maxima - discussion]*.      the optimal design method generates the stimuli by maximizing a utility function , or an objective function .",
    "the basic idea is that these stimuli are designed so as to elicit responses that are most informative about the network parameters . in optimal design method ,",
    "the utility function @xmath51 depends on the stimulus parameters @xmath52 , but typically also on the model parameters @xmath47 .",
    "an intuitive explanation of the dependence on the model parameter is best illustrated with an example .",
    "suppose we want to estimate a gaussian tuning curve model with unknown parameters although we may have some idea about the sensible ranges of these parameters . to estimate the height of the tuning curve accurately , we should place a probing stimulus around the likely location of the peak . to estimate the width , the probing stimulus should go to where the tuning curve is likely to have the steepest slope . for the baseline",
    ", we should go for the lowest response .",
    "this simple example illustrates two facts : first , optimal design depends on our knowledge of possible parameter values ; second , the elicited responses in an optimally design experiment are expected to vary over a wide dynamic range as different parameters are estimated .",
    "once the utility function @xmath51 is chosen , the optimally designed stimulus may be written formally as : @xmath53 where the network parameters @xmath47 can be obtained by maximum - likelihood estimation from the existing spike data as described in the preceding section . here",
    "the stimulus is specified by vector @xmath52 , which is a set of parameters rather than the actual stimulus itself .",
    "direct computation of the actual time - varying stimulus is not easy because no closed analytical form of the objective function is available and furthermore the computation of the optimal control input generally requires a backward integration or recursion . instead of struggling with this difficulty",
    ", one can restrict the stimulus @xmath54 to a well known natural form such as sum of phased cosines as shown below : @xmath55 where @xmath56 is the amplitude , @xmath57 is the frequency of the @xmath58-th fourier component , and @xmath59 is the phase of the component .",
    "we choose a base frequency @xmath60 and set the frequencies of all other components as the harmonics : @xmath61 for @xmath62 .",
    "now the stimulus parameters can be summarized by the stimulus parameter vector : @xmath63      \\label{eq : x}\\ ] ] we sometimes refer to @xmath52 as the stimulus , with it understood that it really means a set of parameters that uniquely specify the actual stimulus @xmath54 .",
    "some popular choices of the objective function are based on the fisher information matrix , which is generally defined as : @xmath64 where @xmath65 is the probability distribution of the response @xmath66 to a given stimulus @xmath52 .",
    "the fisher information matrix reflects the amount of information contained in the noisy response @xmath67 about the model parameters @xmath68 , assuming a generative model given by the conditional probability @xmath65 .",
    "so the stimulus designed by maximizing a certain measure of the fisher information matrix is expected to decrease the error of the estimation of the parameters @xmath68 .",
    "the utility function @xmath69 can be chosen as a scalar function of the fisher information matrix @xmath70 .",
    "one theoretically well founded popular choice is the d - optimal design : @xmath71 although the determinant of the fisher information matrix is not always easy to optimize .",
    "the a - optimal design is based on the trace of the fisher information matrix and is easier to optimize : @xmath72 another alternative is the e - optimal design where the objective function is the smallest eigenvalue of the fisher information matrix . in this paper",
    "the a - optimality measure of the information matrix is preferred .",
    "there is an obvious reason for this preference . as the computational complexity of the optimization algorithms",
    "are expected to be high , the necessity of numerical derivative computation should be avoided as much as possible .",
    "since it is not easy to evaluate the derivatives of the eigenvalues and determinants by any means other than numerical approximations it will be convenient to apply a criterion like a - optimality which has a direct relationship like the sums of the diagonal elements .",
    "in the beginning of this section we mentioned that an optimally design stimulus is expected to depend on which parameter is supposed to be . since a scalar utility function in or depends on all the parameters @xmath73 $ ] , optimizing a single scalar function is sufficient to recover all the parameters .",
    "when a sequence of stimuli are generated by optimal design , the stimuli may sometimes alternate spontaneously as if the optimization is performed with respect to each of the parameters one by one @xcite .    in our network model",
    ", the recorded spike train has an inhomogeneous poisson distribution with the rate function @xmath22 .",
    "we write this rate as @xmath36 to emphasize that it is a time - varying function that depends on both the stimulus @xmath52 and the network parameters @xmath52 . for a small time window of duration @xmath74 and centered at time @xmath75 , the fisher information matrix entry in is reduced to : @xmath76 since the poisson rate function @xmath77 varies with time , the a - optimal utility function in should be modified by including integration over time : @xmath78 here the time window @xmath74 is ignored because it is a constant coefficient that does not affect the result of the optimization .    for convenience , we can also define the objective function with respect to a single parameter @xmath79 as follows : @xmath80 the objective function in is identical to @xmath81 .",
    "the optimization of the d - optimal criterion in is not affected by parameter rescaling , or changing the units of parameters .",
    "for example , changing the unit of parameter 1 ( say , from msec@xmath82 to sec@xmath82 ) is equivalent to rescaling the parameter by a constant coefficient : @xmath83 .",
    "the effect of this transformation is equivalent to a rescaling of the determinant of the fisher information matrix by a constant , namely , @xmath84 , which does not affect the location of the maximum of .",
    "by contrast , the criterion function in or are affected by parameter rescaling .",
    "a parameter with a smaller unit would tend to have larger derivative value and therefore contribute more to than a parameter with a large unit . to alleviate this problem",
    ", we use @xmath85 one by one to generate the stimuli .",
    "that is , stimulus 1 is generated by maximizing @xmath86 , and stimulus 2 is generated by maximizing @xmath87 , and so on .",
    "once the 8th stimulus is generated by maximizing @xmath88 , we go back and use @xmath86 to generate the next stimulus , and so on .",
    "finally , an alternative way to get rid of scale dependence is to introduce logarithm and use @xmath89 as the criterion , which , however , may become degenerate when @xmath85 approaches 0 .",
    "as just explained in the previous section about the computational issues in this research , the gradient computation decreases the computation durations considerably .",
    "the main issue with this fact is the lack of closed form expressions like in the case of static non - linear mappings as the model . in researches such as @xcite , @xcite and @xcite",
    "the gradients are computed as a self contained differential equation which is formed by taking the derivatives of the model equations and from both sides .",
    "compiling all the information in this section one can write the gradient of the fisher information measure ( i.e. the fisher information matrix with a certain optimality criterion such as a - optimality ) . in the beginning of this section , it is stated that the sensitivity levels of the firing rate w.r.to different network parameters are different and thus it would be convenient to maximize the fisher information for a single parameter at a time .",
    "the optimization as expressed in the optimal design problem is converted into a parameter optimization problem to optimize the amplitudes @xmath90 s and @xmath91 s of the stimulus . for the sake of simplicity and modularity in programming",
    ", these equations can be written using their shorthand notations .",
    "let @xmath92=[a_1,\\ldots , a_{n } , \\phi_1,\\ldots , \\phi_{n } ]      \\label{eq : param - stimulus}\\ ] ]    we write the state of the network as a vector : @xmath93^{\\rm t } $ ]    and we need derivatives such as @xmath94 . here , the idea is to use these derivatives as variables that can be solved directly from differential equations derived from the original dynamical equations in .",
    "for the optimal design , one can write the following : @xmath95      \\left\\ { -\\frac{\\partial \\bf v}{\\partial \\bf x}+\\left[\\begin{array}{cc }      w_{ee } & -w_{ei}\\\\      w_{ie } & -w_{ii }      \\end{array}\\right ]      \\left[\\begin{array}{cc }       g'_{e}\\left(v_{e}\\right ) & 0\\\\      0 &   g'_{i}\\left(v_{i}\\right )      \\end{array}\\right]\\frac{\\partial \\bf v}{\\partial \\bf x}+\\left[\\begin{array}{c }      w_{e}\\\\      w_{i }      \\end{array}\\right]\\frac{\\partial i}{\\partial \\bf x}\\right\\ }       \\label{eq : intermediate - step - for - dvu}\\ ] ] where @xmath96 and @xmath97 are the derivatives of the gain functions @xmath98 and @xmath99 respectively , and the matrices derivatives are defined in the usual manner : @xmath100\\ ] ] and @xmath101\\label{eq : potential - gradient - stimulus - parameters}\\ ] ]    can be written equivalently in the shorthand form : @xmath102 where @xmath103}$ ] , @xmath104 } $ ] , @xmath105 $ ] , and @xmath106}$ ] .    in the evaluation of the fisher information matrix and the gradients in estimation one",
    "will need the derivatives with respect to the parameters @xmath107=\\left[\\beta_e,\\beta_i , w_e , w_i , w_{ee},w_{ei},w_{ie},w_{ii}\\right]$ ] .",
    "it follows from the original dynamical equation in that @xmath108 where @xmath109^{\\rm t}}$ ] and the last term @xmath110 refers to the extra components resulting from the chain rule of differentiation .",
    "these extra terms are presented in * table [ tab : ext - comp - derivat]*.",
    ".the extra components @xmath110 in equation [ cols=\"^,^,^\",options=\"header \" , ]     our model in has two more important components which are the gain functions @xmath111 and @xmath112 .",
    "these are obtained by setting @xmath113 in by either @xmath114 or @xmath115. so one has @xmath116 additional parameters @xmath117 $ ] which have direct effect on the neural model behaviour .",
    "this research targeted the estimation of the network parameters only . because of",
    "that , the parameters of the gain functions are kept as fixed and they have the values @xmath118 .",
    "this set of parameters ( gain functions and * table [ tab : true - values - and - statistics ] * ) allows the network to have a unique equilibrium state for each stationary input . to demonstrate the excitatory and inhibitory characteristics of our model",
    ", we can stimulate the model with a square wave ( pulse ) stimulus as shown in * figure [ fig : pulse - response - ctrnn]a*. the resultant excitatory and inhibitory neural membrane potential responses ( @xmath119 and @xmath120 ) are shown in * figure [ fig : pulse - response - ctrnn]b * and * figure [ fig : pulse - response - ctrnn]c*. it can be said that , the network has shown both transient and sustained responses . in * figure [ fig : pulse - response - ctrnn]d * , the excitatory firing rate response @xmath121 which is related to excitatory potential as @xmath122 is shown . the response @xmath120 is slightly delayed which leads to the depolarization of excitatory unit until @xmath123ms .",
    "this delay is also responsible from the subsequent re - polarization and plateau formation in the membrane potential of excitatory neuron .",
    "the firing rate @xmath121 is higher during excitation and lower in subsequent plateau and repolarization phases ( * figure [ fig : pulse - response - ctrnn]d * ) .",
    "b * in response to a square - wave stimulus .",
    "the states of the excitatory and inhibitory units , @xmath124 and @xmath125 , are shown , together with the continuous firing rate of the excitatory unit , @xmath126 .",
    "the firing rate of the excitatory unit ( bottom panel ) has a transient component with higher firing rates , followed by a plateau or sustained component with lower firing rates . ]",
    "the optimisation of the stimuli requires that the maximum power level in a single stimulus is bounded .",
    "this is a precaution to protect the model from potential instabilities due to over - stimulation .",
    "in addition , if applied in a real environment the experiment subject will also be protected from such over - stimulations . as the amplitude parameter is assumed positive , assigning an upper bound defined as @xmath127 should be enough .",
    "this is applied to all stimulus amplitudes ( i.e. @xmath56 ) . in this research ,",
    "a fixed setting of @xmath128a is chosen .",
    "the lower bound is obviously @xmath129 . for the phase @xmath59 , no lower or upper bounds are necessary as the cosine function itself has already been bounded as @xmath130 .",
    "the frequencies of the stimulus components are @xmath131 harmonics of a base frequency @xmath132 @xmath133 .",
    "since we have a simulation time of @xmath134 seconds , we have a reasonable choice of @xmath135 hz which is in fact equal to @xmath136 .",
    "so we have chosen an integer relationship between the stimulation frequency and simulation time .",
    "the number of stimulus components @xmath137 is chosen as @xmath138 which is found to be reasonable concerning speed and performance balance .",
    "it is well known that optimization algorithms such as _ fmincon _ requires an initial guess of the optimum solution .",
    "a suitable choice for the initial guesses can be their assignment from a set of initial conditions generated randomly between the optimization bounds . in the optimization of stimuli , the initial amplitudes can be uniformly distributed between @xmath139 $ ] and phases can be uniformly distributed between @xmath140 $ ] .",
    "although we do not have any constraints on the phase parameter , we limit the initial phase values to a safe assumed range .    we follow a similar strategy for the parameter estimation based on maximum likelihood method .",
    "the multiple initial guesses will be chosen from a set of values uniformly distributed between the lower and upper bounds defined in * table [ tab : true - values - and - statistics]*.    in * section [ sub : jmle - theory ] * , one recall from that the likelihood estimation should produce better results when the number of samples ( i.e. @xmath38 in ) increases .",
    "because of this fact , the likelihood function is based on data having all spikes generated since the beginning of the simulation .",
    "the number of repeats determines @xmath38 .",
    "if simulation is repeated @xmath141 times ( @xmath141 iterations ) , one will have an @xmath38 value of @xmath142 due to the fact that each iteration has @xmath143 optimal designs sub - steps ( with respect to each parameter @xmath79 .",
    "read * section [ sub : oed - theory ] * ) .",
    "so , if one has 15 iterations @xmath144 , @xmath145 which means likelihood has @xmath146 samples .",
    "this also means that optimal design and subsequent parameter estimation will also be repeated @xmath146 times .      having all necessary information from * section [ sub : example - details ] * , one can perform an optimal design and obtain a sample optimal stimulus and associated neural responses * figure [ fig : stimexamplederivatives]*. it is noted that the optimal stimulus in * figure [ fig : stimexamplederivatives ] * top panel has a periodic variation as it is modelled as a phased cosines form ( as it is equivalent to real valued fourier series ) .    .",
    "driven by the same stimulus , the response of eight derivatives variables , namely , the derivatives of the firing rate @xmath77 with respect to all the network parameters , are shown as red curves .",
    "these derivatives were solved directly from differential equations . ]    in addition to the fundamental responses , the second half of * figure [ fig : stimexamplederivatives ] * displays the variation of the parametric sensitivity derivatives @xmath147 which are generated by integrating and then substituting to .",
    "the variation of the sensitivity derivatives support the idea of optimization of the fisher information metric with respect to a single parameter ( see ) .",
    "it appears that , the weight parameters have a very high sensitivity which are more than @xmath148 times that of the @xmath149 parameters .",
    "also some of the parameters affecting the behaviour of inhibitory ( i ) unit @xmath150 and also the e - i interconnection weights @xmath151 have a reverse behaviour .",
    "this fact appears as a negative values variation of the sensitivity derivatives .",
    "self inhibition coefficient @xmath152 does not show this behaviour as it represent an inhibitory effect on the inhibitory neuron potential ( which favours excitation ) .",
    "one of the most distinguishing result related to optimal design is the statistics of the optimal stimuli ( i.e. the amplitudes @xmath56 and phases @xmath59 ) .",
    "this analysis can be performed by generating the histograms of @xmath56 and @xmath59 from available data as shown in * figure [ fig : stimulus - amplitude - histogram]*. the * figure [ fig : stimulus - amplitude - histogram]a * shows the flat uniform distribution of the random stimuli amplitudes and phases .",
    "this is expected as the stimuli is generated directly from a uniform distribution . concerning the optimal stimuli , the histograms shown in * figure [ fig : stimulus - amplitude - histogram]b * reveals that optimal design has a tendency to maximize the amplitude of the stimuli towards the upper bound .",
    "this reassures that optimal design tends to maximize the stimulus power which is expected increase the efficiency of the parameter estimation process and it distinguishes optimal stimuli from their randomly generated counterparts .     and phases @xmath153 .",
    "( * a * ) random stimuli were generated by choosing their fourier amplitudes and phases randomly from uniform distributions .",
    "a total of 12,000 random stimuli were used in each plot .",
    "( * b * ) the optimally designed stimuli showed some structures in the distributions of their fourier amplitudes and phases , which differ radically from a uniform distribution .",
    "a total of 12,000 optimally designed stimuli were used in each plot . ]",
    "given a dataset consisting of stimulus - response pairs , we can always use maximum - likelihood estimation to fit a model to the data to recover the parameters .",
    "maximum - likelihood estimation is known to be asymptotically efficient in the limit of large data size , in the sense that the estimation is asymptotically unbiased ( i .. e , average of the estimates approaches the true value ) and has minimal variance ( i.e. , the variance of the estimates approaches the cramr - rao lower bound ) .",
    "we found that maximum likelihood obtained from the optimally design stimuli was always much better than that obtained from the random stimuli ( see * figure [ fig : likelihood - traces - box ] * ) .",
    "it also reveals that , the likelihood value increases as the number of stimuli increases . for",
    "any given number of stimuli , the optimally designed stimuli always yielded much greater likelihood value than the random stimuli .",
    "the minimum difference between the likelihood values ( the minimum from the optimal design and the maximum from the random stimuli based test ) was typically about two times greater than the standard deviation of either estimates except for the case with @xmath154 samples @xmath155 .",
    "even in this case , this violation appear only on one sample .",
    "in addition , it can be easily deduced from the box diagram in * figure [ fig : likelihood - traces - box ] * that the difference between @xmath156^th^ and @xmath157^th^ percentiles ( @xmath158^st^ and @xmath159^rd^ quartiles ) yield a value which is larger than three times the standard deviation of either estimate .",
    "the standard deviations of the maximum likelihood values are also larger in the random stimuli based tests .",
    "those results are certain evidences of the superiority of an optimal design over the random stimuli based tests .",
    "the difference between the maximum values of the two likelihoods ( from optimal and random stimuli ) becomes more significant as the number of samples @xmath38 increases .",
    "it would also be convenient to stress the fact that the greater the likelihood , the better the fitting of the data to the model . this fact is demonstrated by two regression lines imposed on the box diagram * figure [ fig : likelihood - traces - box]*. one of those lines correspond to the optimal design and the other correspond to the random stimuli based tests .",
    "both regression lines path through the origin point ( 0 , 0 ) approximately .",
    "this means that the ratio of the log likelihood values in the two cases is approximately a constant , regardless of the number of stimuli .",
    "the regression lines are represented by equations @xmath160 for optimal and @xmath161 for the random stimuli .",
    "so the two lines have a slope ratio of approximately @xmath162 .",
    "the significance of this number can be explained by a simple example . if one desires to attain the same level of likelihood with @xmath145 optimally designed stimuli , the required number of random stimuli to be generated is equal to a value about @xmath163 .",
    "another statistical comparison of the maximized likelihoods can be performed by wilcoxon rank - sum tests @xcite .",
    "when applied , one will be able to see the difference which is highly significant .",
    "regardless of the number of samples the p - values remained at least @xmath164 times smaller than the widely accepted probability significance threshold of @xmath165 or @xmath166 .",
    "the likelihood function provides an overall measure of how well a model fits the data .",
    "we have also tested the mean errors of individual parameters relative to their true values .",
    "the main finding is that , for each individual parameter , the error is typically smaller for the optimally designed stimuli than the error for the random stimuli .",
    "this result can be observed from the bar charts presented in * figure [ fig : mean - error - bars]*. the heights of the bars show the mean error levels for randomly and optimally generated stimuli respectively .",
    "one can get the benefits of the rank - sum test on the statistical properties of the parameter estimates .",
    "computation of rank - sum p - values for each individual parameter corresponding to the case of @xmath146 samples @xmath167 yields :    @xmath168    the above result showed that for @xmath145 the differences are statistically significant for @xmath169 parameters . for",
    "different values one can refer to * figure [ fig : mean - error - bars]*. in this illustration , the statistical significance of the difference between optimal and random stimuli based tests are indicated as an asterisk placed above the bars .",
    "any of the cases with asterisk means that , the associated sample size led to a result where optimal design is significantly better .    ) at the top of an optimal design bar",
    "indicates that the difference from the neighboring random bar is statistically significant at the level @xmath170 in the ranksum test . ]    here an interesting result is the statistical non - significance of the third parameter @xmath171 regardless of the sample size @xmath38 .",
    "this might be associated with the parameter confounding phenomenon that is discussed in * section [ sub : parameter - confo]*.    for convenience , one can see the mean values and standard deviations of the estimates obtained from @xmath172 optimal and random stimuli in * table [ tab : true - values - and - statistics]*. these are obtained with @xmath145 samples . in general ,",
    "mean values seem to be comparable for both stimuli however the standard deviations of estimates from optimal stimuli are smaller then that obtained from random stimuli .",
    "the mean values of parameters @xmath173 and @xmath152 are also closer to the true values when obtained from optimal stimuli .",
    "we need optimization in two places : maximum likelihood estimation of parameters , and optimal design of stimuli .",
    "due to speed and computational complexity considerations one needs to utilize gradient base local optimizers such as matlab^^ _ fmincon_.",
    "these algorithms often needs initial guesses and not all of the initial guesses will converge to a true value .",
    "this issue may especially appear in the cases where the objective function involves dynamical model ( differential or difference equations ) .",
    "in such cases , the problem of multiple local maxima might occur in the objective function which often requires multiple initial guesses to be provided to the solver .",
    "so an analysis on this issue may reveal useful information .",
    "suppose there are @xmath58 repeats or starts from different initial values .",
    "let @xmath174 be the probability of finding the `` correct '' solution in an individual run with random initial guess .",
    "then in @xmath58 repeated runs , the probably that at least one run will lead to the `` correct '' solution is    @xmath175    the probability @xmath174 can be estimated from a pairwise test or directly from the values of the likelihood and fisher information metric . in the test of the likelihood function , one can achieve the goal by starting the optimization from @xmath34 different initial guesses and checking the number of solutions which stay in an error bound @xmath176 for each individual parameter with respect to the solution leading to the highest likelihood value .",
    "in other words , to pass the test the following criterion should be satisfied for each individual parameter @xmath177 in :    @xmath178    where @xmath179 is the local optimum solution having the highest objective ( likelihood ) value and @xmath180 is the estimated value of @xmath47 .",
    "if the above is satisfied for all @xmath181 , this result is counted as one pass .",
    "so for maximum likelihood estimation with @xmath145 , the data suggests a probability of @xmath182 and to get a @xmath183 correct rate we will only need @xmath184 repeats .",
    "this is a result obtained from @xmath148 multiple initial guesses per @xmath185 different stimuli configurations ( total of @xmath186 occurrences ) . here",
    ", we have a high probability of obtaining a global maxima and thus we may get rid of multiple initial guesses requirement in the estimation of @xmath47 .    for the optimal design part the problem is expected to be harder as the stimulus amplitudes tend to the upper bound @xmath127 . .",
    "it should also be remembered that , the fisher information measure is computed with respect to a single parameter @xmath181 as shown in .",
    "thus it will here be convenient to analyse the respective fisher information metric @xmath187 as defined w.r.to each parameter @xmath181 . like in the case of likelihood analysis",
    ", we do the analysis on @xmath185 samples ( i.e. @xmath185 stimuli ) separately for amplitudes @xmath188 and phases @xmath189 .",
    "the criterion for passing the test is similar to that of after replacing @xmath180 by @xmath52 from and @xmath179 by @xmath190 with @xmath190 being the stimulus parameter yielding the largest value of @xmath187 . note that , since we do not have any concept of `` true stimulus parameters '' we will use @xmath190 in the denominator of .    after doing the analysis for amplitudes @xmath56",
    ", one can see that highest probability value @xmath191 is obtained for @xmath86 whereas the smallest value is obtained for @xmath192 as @xmath193 .",
    "the second and third smallest values are @xmath194 and @xmath195 having @xmath196 and @xmath197 respectively .",
    "the indices @xmath159,@xmath158,@xmath198 and @xmath116 correspond to the @xmath199,@xmath171,@xmath200 and @xmath201 .",
    "this means that the lowest probability values occur at the parameters @xmath171,@xmath200 and @xmath201 .",
    "this result might be interesting as those three parameters have strong correlations at least with one other network parameter ( see * section [ sub : parameter - confo ] * ) .",
    "the required number of repeats appears to be @xmath202 for the worst case ( @xmath193 for @xmath192 ) .    for the phase parameters",
    "@xmath59 , different initial conditions lead to different values .",
    "this is an expected situation because of this outcome , the solution yielding the largest value of fisher information metric @xmath203 among all runs with different initial conditions should be preferred in the actual application .",
    "modern parallel computing facilities will ease the implementation of optimization with multiple random guesses .",
    "the errors of some parameters tend to be correlated ( see * figure [ fig : confounding - plots ] * ) .",
    "parameter confounding may explain some of the correlations .",
    "the idea is that different parameter may compensate each other such that the network behaves in similar ways , even though the parameter values are different .",
    "it is known that in individual neurons , different ion channels may be regulated such that diverse configurations may lead to neurons with similar neuronal behaviours in their electrical activation patterns @xcite .",
    "similar kind of effect also exists at the network level @xcite",
    ".    here we will consider the original dynamical equations and demonstrate how parameter confounding might arise .",
    "we first emphasize that different parameters in our model are distinct and there is no strict confounding at all .",
    "the confounding is approximate in nature .    from the correlation analysis on the optimal design data ( * figures [ fig : confounding - plots]a * and * [ fig : confounding - plots]b * ) , three pairs of parameters stand out with the strongest correlations .",
    "these pairs are @xmath204 , @xmath205 , @xmath206 and @xmath207",
    ".    we will offer an intuitive heuristic explanation based on the idea of parameter confounding .",
    "we here rewrite the dynamical equations for convenience as shown below : @xmath208      the external stimulus @xmath54 drives the first equation through the weight @xmath209 .",
    "if this product is the same , the drive would be the same , even though the individual parameters are different .",
    "for example , if @xmath199 is increased by 10% from its true value while @xmath171 is decreased by 10% from its true value , then the product stays the same , so that the external input provides the same drive to .",
    "of course , any deviation from the true parameter values also leads to other differences elsewhere in the system .",
    "therefore , the confounding relation is only approximate and not strict .",
    "this heuristic argument gives an empirical formula : @xmath210 where @xmath199 and @xmath171 refer to the true values of these parameters , whereas @xmath211 and @xmath212 refer to the estimated values .",
    "these two parameters appear separately in different equations , namely , @xmath201 appearing only in while @xmath214 appearing only in . to combine them",
    ", we need to consider the interaction of these two equations . to simplify the problem",
    ", we consider a linearised system around the equilibrium state : @xmath215 where @xmath216 and @xmath217 are the slopes of the gain functions , and @xmath218 and @xmath219 are extra terms that depend on the equilibrium state and other parameters .",
    "note that @xmath125 appears in only once , in the second term in the curly brackets .",
    "since @xmath125 also satisfies , we solve for @xmath125 in terms of @xmath220 from and find a solution of the form : @xmath221 where @xmath222 is a constant .",
    "substitution into shows that the parameter combination @xmath223 scales how strongly @xmath220 influences this equation .",
    "thus we have a heuristic confounding relation : @xmath224      these two parameters both appear in the curly brackets in .",
    "we have a heuristic confounding relation : @xmath225 where @xmath226 and @xmath227 are the equilibrium states .",
    "if this equation is satisfied , we expect that the term in the curly brackets in would be close to a constant ( the right - hand side of ) whenever the state @xmath124 and @xmath125 are close to the equilibrium values .",
    "when the state variables vary freely , we expect this relation to hold only as a very crude approximation .",
    "simulation results show that these three confounding relation can qualitatively account for the data scattering .",
    "the random data follow the same pattern ( see * * figure [ fig : confounding - plots]**c ) although they appear to have more scattering compared to the optimal design based data .",
    "although the confounding relations are not strictly valid , their offer useful approximate explanations that are based on intuitive argument and are supported by the data as shown in * figure [ fig : confounding - plots]*.    the theoretical slopes are always smaller , suggesting that the heuristic theory only accounts for a portion of the correlation .",
    "it is likely that there are approximate confounding among more than those three pairs of parameters namely @xmath204 , @xmath205 , @xmath206 and @xmath207 .",
    "1 .   we have implemented an optimal design algorithm for generating stimuli that can efficiently probe the e - i network , which describes the dynamic interaction between an excitatory neuronal population and an inhibitory neuronal population .",
    "the data has been used to model the auditory system etc .",
    "2 .   the dynamical network allows both transient and sustained response components ( * figure [ fig : pulse - response - ctrnn ] * ) 3 .",
    "derivatives are computed directly by differential equations derived from the original system ( * figure [ fig : stimexamplederivatives ] * ) 4 .",
    "optimally designed stimuli have patterns .",
    "the amplitude tend to be saturated , which can lead to faster search because only boundary values need to be checked ( * figure [ fig : stimulus - amplitude - histogram ] * ) 5 .",
    "the optimally designed stimuli elicited responses that have more dynamic range ( more variance in the histogram of amplitude of the stimulus waveform @xmath228 ) 6 .",
    "the optimal design yield much better parameter estimation in term of likelihood ( * figure [ fig : likelihood - traces - box ] * ) , and in the errors of the individual parameters ( * figure [ fig : mean - error - bars ] * ) . 7 .",
    "we studied parameter confounding ( * figure [ fig : confounding - plots ] * ) .",
    "significance of the results can readily lead to practical applications .",
    "for example in the modelling of the auditory networks known works all used stationary sound stimuli .",
    "however , it is beneficial to include time dependency as realistic neurons and their networks have several dynamic features .",
    "the chosen model is an appropriate and simple model which has only two neurons with one being excitatory and one being inhibitory .",
    "this choice is reasonable in the context of this research as the availability of previous work targeting a similar goal is quite limited .",
    "in addition , the known ones mostly based on the static feed - forward network designs . in order to verify our optimal design strategy we have to start from a simpler model . the theoretical development and results of this work can very easily be adapted to alternative and/or more complicated models . here",
    "the most critical parameter is the number of data generating neurons ( neurons where the spiking events are recorded by electrode implantations ) and the computational complexity .",
    "the former is a procedural aspect of the experiment whereas the latter is directly related to the instrumentation .",
    "in addition , the universal approximating nature of the ctrnn s is an advantage on this manner .      in this work",
    ", we aim at investigation of the computational principles without trying the maximizing the computation speed . right now on a single pc , having a intel^^ corei7 processor with @xmath116 cores , the computational duration for optimization of a single stimulus is about @xmath229 minutes .",
    "surely , this is an average value as the number of steps required to converge to an optimum depends on certain conditions such as the value of objective value , gradient , constraint violation and step size .",
    "a similar situation exist for the optimization of the likelihood .",
    "however in this case , the optimization times of the likelihood will vary due to its increasing size as all the historical spiking is taken into account ( see ) leading to a function with gradually increasing complexity .",
    "although that is not the only fact contributing to the computational times , the average duration of optimization tend to increase with the size of likelihood @xmath38 .",
    "an average value for the observed duration of likelihood optimization is @xmath230 minutes . as a result optimization of one stimulus and subsequent likelihood estimation requires a duration of about @xmath231 minutes .",
    "this is approximately one hour .",
    "so one complete run with a sample size of @xmath145 is completed in a duration about @xmath232 hours .",
    "changing the value of the sample size @xmath38 will have a direct influence on the system computation time .",
    "for example , the duration of one complete run will reduce to a value about @xmath233 hours when @xmath234 .",
    "this is an expected situation as there will be a reduced number of the summation terms in the likelihood function .",
    "the reduction in the duration of the optimal design algorithm is only based on the reduction in the number of trials which is just equal to the sample size @xmath38 .",
    "so based on these findings , one will need a speed - up in order to adapt this work to an experiment .",
    "there are several ways to speed up the computation :    1 .   using a large time step",
    ": in this work we have integrated the equations using a time step of @xmath159-millisecond or @xmath235-seconds .",
    "this value may be increased to levels as high as @xmath236-seconds .",
    "this modification will have a little contribution to the speed of computation .",
    "the benefits will most likely be from the optimal design part due to the manipulation of a single interval ( no consideration of past / historical spike trains ) .",
    "however , the higher the time step the lower the accuracy of the estimates and the optimality of the stimuli .",
    "this main contributor to this fact is the spike generation algorithm where the accuracy of the locations of some spikes are lost when a coarse integration interval is applied .",
    "2 .   using and/or developing streamlined optimization algorithms : this can be a subject of a new project on the same field .",
    "this development is expected to have a considerable contribution to the computation time without any trade - offs over performance .",
    "3 .   generating the stimuli as a block rather than one by one : this is also a potential topic for a new project .",
    "this is expected to reduce the optimal design time without losing performance .",
    "4 .   employment of larger cluster computing systems ( or high performance computing systems ( hpc ) ) having more than @xmath172 cpu cores : though the most sophisticated and expensive solution , it is the best approach to cure the overall computational burdens and transform the theoretical only study to an experiment adaptable one .",
    "5 .   porting the algorithms to a lower level programming language such as c / c++ or fortran may help in speeding up the computation . if an efficient and stable numerical differentiation algorithm can be employed in this set - up , another optimality criterion such as _ d _ or _",
    "e _ optimality can be used in the computation of the fisher information metric which might help in reducing the number of steps ( i.e @xmath38 and/or @xmath141 values ) .",
    "knowing the fact that the optimal stimulus amplitudes @xmath56 tend to the upper boundary @xmath127 ( remember from * figure [ fig : stimulus - amplitude - histogram]b * ) .",
    "all the amplitudes can be set to same value as @xmath237 and only @xmath238 is computed from optimization .",
    "this setting can be helpful for speeding up the optimization time during an actual experiment .",
    "however , it should be verified by simulation whether this choice is meaningful for an actual experiment . in this case",
    ", one may not need many repeats as only the statistics of the stimuli is required .",
    "this occurrence is quite common in the simulation results thus we do not expect a performance degradation when this change is applied to the stimuli characterization .    with the above adaptations , it is expected that we are within reach to reduce the computation time for each 3-second stimuli to less than 3 seconds . in addition one can has the following options which are related to tuning of the algorithms used in this research :    1 .",
    "the parameter related to the sample size @xmath239 might be reduced .",
    "that is an examined situation in this study .",
    "optimal design seems to yield a better estimation performance with a reduced @xmath38 compared to random stimuli case with same @xmath38 .",
    "however , the former will lead to a slightly increased computational time . however , the trade - off is not very direct .",
    "for example , a random stimuli based simulation with @xmath145 samples requires a longer run than a simulation based on an optimally designed stimulus with @xmath154 samples .",
    "this is fairly a good trade - off .",
    "2 .   another option to increase the computational performance might be the reduction of the cut - off points in the optimization algorithm such as the first order optimality measure ( tolerance of the gradient ) and the step - size .",
    "this will result in a faster computation but this approach may bring out questions on the accurate detection of the local minimums among which the best one is chosen ( both in oed and likelihood optimization ) . for the ` fmincon ` algorithm in matlab the first order optimality tolerance and step - size",
    "might both be shifted from @xmath240 to @xmath241 .",
    "this tuning brings improvement in the computational duration about 10% without a considerable performance loss .",
    "however , if one has a hpc supported computational environment it is strongly recommended not to modify these settings .",
    "the above network is rather a simpler example to demonstrate the optimal design approach and its computational challenges .",
    "however more features can be brought to this research concerning efficiency and applicability to an actual experiment .    1 .",
    "speed up issues : the tasks related to speed of computation discussed in * section [ sub : computational - speed - issues ] * may be a separate project to be developed on top this research .",
    "large number of neurons may be considered together with multiple stimulus inputs and response data collection from multiple neurons ( both excitatory and inhibitory groups of neurons ) .",
    "more complex stimulus structures may be utilized .",
    "this can be achieved by increasing @xmath137 in or considering different stimulus representations other than phase cosines .",
    "4 .   in this research ,",
    "the primary goal was the estimation of the network weights @xmath242 and time constants @xmath243 .",
    "however , it will be interesting to test the methodology for its performance in estimation of firing thresholds and slopes .",
    "( i.e. the parameters @xmath244 and @xmath12 in ) 5",
    ".   some more realistic details like plasticity can be included to obtain a model describing the synaptic adaptation .",
    "although it is expected to be a harder problem , the method takes fewer stimuli and should be faster .",
    "[ it : optimality ] . however , it is stated in @xcite that , d - optimality brings an advantage that the optimization will be immune to the scales of the variables . on the contrary ,",
    "it is also stated in the same source that this mentioned fact is not true for a- and e - optimality criteria in general .",
    "the sensitivity to scaling of the variables lead to another issue that the confounding of the parameters brings certain problems about the bias and efficiency of the estimates .",
    "so it will be quite beneficial to see the results obtained from the same research with the optimal designs performed by d - optimal and other measures of fisher information metric such as e- and f - optimality .",
    "as these will require a new set of computations it will be better to include them in a future study .",
    "similar to the discussion in * article [ it : optimality ] * above , the methodology of optimization in optimal designs and likelihood optimization should be considered .",
    "current work involves evaluation of gradients for the sake of faster computation .",
    "however , this requires larger efforts in the preparation as one should develop a specific algorithm to compute the evolution of the gradients satisfactorily .",
    "this is also required for a speed - up . with the availability of a high performance computing system ,",
    "other optimization methods such as simulated - annealing , genetic algorithms and pattern search might be employed instead of the local minimizers such as _ fmincon _ of matlab .",
    "these algorithms may help in searching for a better optimal stimulus .",
    "after a sufficiently fast simulation is obtained an experiment can be performed involving a living experimental subject .",
    "the mapping of the actual sound heard by the animal during the course of experiment to the optimally designed stimulus is a critical issue here and will also be a part of the future related research ."
  ],
  "abstract_text": [
    "<S> we present a theoretical application of an optimal experiment design ( oed ) methodology to the development of mathematical models to describe the stimulus - response relationship of sensory neurons . </S>",
    "<S> although there are a few related studies in the computational neuroscience literature on this topic , most of them are either involving non - linear static maps or simple linear filters cascaded to a static non - linearity . </S>",
    "<S> although the linear filters might be appropriate to demonstrate some aspects of neural processes , the high level of non - linearity in the nature of the stimulus - response data may render them inadequate . </S>",
    "<S> in addition , modelling by a static non - linear input - output map may mask important dynamical ( time - dependent ) features in the response data . due to all those facts a non - linear continuous time </S>",
    "<S> dynamic recurrent neural network that models the excitatory and inhibitory membrane potential dynamics is preferred . </S>",
    "<S> the main goal of this research is to estimate the parametric details of this model from the available stimulus - response data . in order to design an efficient estimator </S>",
    "<S> an optimal experiment design scheme is proposed which computes a pre - shaped stimulus to maximize a certain measure of fisher information matrix . </S>",
    "<S> this measure depends on the estimated values of the parameters in the current step and the optimal stimuli are used in a maximum likelihood estimation procedure to find an estimate of the network parameters . </S>",
    "<S> this process works as a loop until a reasonable convergence occurs . </S>",
    "<S> the response data is discontinuous as it is composed of the neural spiking instants which is assumed to obey the poisson statistical distribution . </S>",
    "<S> thus the likelihood functions depend on the poisson statistics . </S>",
    "<S> the model considered in this research has universal approximation capability and thus can be used in the modelling of any non - linear processes . in order to validate the approach and </S>",
    "<S> evaluate its performance , a comparison with another approach on estimation based on randomly generated stimuli is also presented .    </S>",
    "<S> optimal design , sensory neurons , recurrent neural network , excitatory neuron , inhibitory neuron , maximum likelihood estimation </S>"
  ]
}