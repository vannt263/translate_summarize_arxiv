{
  "article_text": [
    "partially observed point processes provide a rich class of models to describe real data . for example , such models are used for stochastic volatility ( barndorff - nielsen & shephard , 2001 ) in finance , descriptions of queuing data in operations research ( fearnhead , 2004 ) , important seismological models ( daley & vere - jones , 1988 ) and applications in nuclear physics ( snyder & miller , 1998 ) . for complex dynamic models , that is , when data arrive sequentially in time , studies date back to at least snyder ( 1972 ) .",
    "however , fitting bayesian models requires smc ( e.g.  doucet et al .",
    "( 2000 ) ) and markov chain monte carlo ( mcmc ) methods .",
    "the main developments in this field include the work of : centanni & minozzo ( 2006a , b ) ; green ( 1995 ) ; del moral et al .",
    "( 2006,2007 ) ; doucet et al .",
    "( 2006 ) ; roberts et al .",
    "( 2004 ) , rydberg & shephard ( 2000 ) , see also whiteley et al .",
    "as we describe below , the smc methodology may fail in some scenarios and we will describe methodology to deal with the problems that will be outlined .    informally , the problem of interest is as follows .",
    "a process is observed discretely upon a given time - interval @xmath2 $ ] .",
    "the objective is to draw inference at time - points @xmath3 , on the unobserved marked pp @xmath4 , where @xmath5 are the ordered event times ( constrained to @xmath6 $ ] ) and @xmath7 are marks , given the observations @xmath8 .",
    "in other words to compute , for @xmath9 , at time @xmath10 @xmath11 in addition , there are static parameters specifying the probability model and these parameters will be estimated in a bayesian manner . at this stage a convention in our terminology is established .",
    "an algorithm is said to be _ sequential _ if it is able to process data as it arrives over time .",
    "an algorithm is said to be _ on - line _ if it is sequential and has a fixed computational cost per iteration / time - step .",
    "one of the first works applying computational methods to pp models , was rydberg & shephard ( 2000 ) .",
    "they focus upon a cox model where the unobserved pp parameterizes the intensity of the observations .",
    "rydberg & shephard ( 2000 ) used the auxiliary particle filter ( pitt & shephard , 1997 ) to simulate from the posterior density of the intensity at a given time point .",
    "this was superseded by centanni & minozzo ( 2006a , b ) , which allows one to infer the intensity at any given time , up to the current observation .",
    "centanni & minozzo ( 2006a , b ) perform an mcmc - type filtering algorithm , estimating static parameters using stochastic em .",
    "the methodology can not easily be adapted to the case where the static parameters are given a prior distribution .",
    "in addition , the theoretical validity of the approach has not been established ; this is verified in proposition [ prop : cmjustification ] of this article .",
    "smc samplers ( del moral et al .",
    "2006 ) are the focus of this paper and can be applied to all the problems stated above .",
    "smc methods simulate a set of @xmath12 weighted samples , termed particles , in order to approximate a sequence of distributions , which may be chosen by the user , but which include ( or closely related to ) the distributions in and .",
    "such methods are provably convergent as @xmath13 ( del moral , 2004 ) .",
    "a key feature of the approach is that the user must select :    1 .   the sequence of distributions [ enum : point1 ] 2 .   the mechanism by which particles are propagated [ enum : point2 ] .    if points 1 .  and 2 .",
    "are not properly addressed , there can be a substantial discrepancy between the proposal and target ; thus the variance of the weights will be large and estimation inaccurate .",
    "this issue is particularly relevant when the targets are defined on a sequence of nested spaces , as is the case for the pp models  the space of the point process trajectories becomes larger with the time - parameter @xmath14 .",
    "thus , in choosing the sequence of target distributions , we are faced with the question of how much the space should be enlarged at each iteration of the smc algorithm and how to choose a mechanism to propose particles in the new region of the space .",
    "this issue is referred to as _ the difficulty of extending the space_.    two solutions are proposed .",
    "the first is to _ saturate _ the state - space ; it is supposed that the observation interval , @xmath2 $ ] , of the pp is known _ a priori_. the sequence of target distributions",
    "is then defined on the whole interval and one sequentially introduces likelihood terms .",
    "this idea circumvents the problem of extending the space , at an extra computational cost .",
    "inference for the original density of interest can be achieved by importance sampling ( is ) .",
    "this approach can not be used if @xmath15 is unkown . in the second approach ,",
    "entitled _ data - point tempering _ , the sequence of target distributions are defined by sequentially introducing likelihood terms .",
    "this is achieved as follows : given that the pp has been sampled on @xmath6 $ ] the target is extended onto @xmath16 $ ] by sampling the missing part of the pp .",
    "then one introduces likelihood terms into the target that correspond to the data ( as in chopin ( 2002 ) ) .",
    "once all of the data have been introduced , the target density is ( [ eq : intro : filtering ] ) .",
    "it should be noted that neither of the methods are online , but some simple fixes are detailed .    section [ sec : motivate ] introduces a doubly stochastic pp model from finance which serves as a running example . in section [ sec : previous ] the ideas of centanni & minozzo ( 2006a , b ) are discussed ; it is established that the method is theoretically valid under some assumptions .",
    "the difficulty of extending the state space is also demonstrated . in section [ sec : lpf ] we introduce our smc methods . in section [ sec : financefinal ] our methods are illustrated on the running example . in section [ sec : summ ] we detail extensions to our work .",
    "some notations are introduced .",
    "we consider a sequence of probability measures @xmath17 on spaces @xmath18 , with dominating @xmath19finite measures",
    ". bounded and measurable functions on @xmath20 , @xmath21 , are written @xmath22 and @xmath23 .",
    "@xmath24 will refer to either the probability measure @xmath25 or density @xmath26 .",
    "the model we use to illustrate our ideas is from statistical finance .",
    "an important type of financial data is ultra high frequency data which consists of the irregularly spaced times of financial transactions and their corresponding monetary value .",
    "standard models for the fitting of such data have relied upon stochastic differential equations driven by wiener dynamics ; a debatable assumption due to the continuity of the sample paths . as noted in centanni & minozzo ( 2006b ) ,",
    "it is more appropriate to model the data as a cox process . due to the high frequency of the data , it is important to be able to perform sequential / on - line inference .",
    "data are observed in @xmath2 $ ] . in the context of finance , the assumption that @xmath15 be fixed is entirely reasonable .",
    "for example , when the model is used in the context of equities , the model is run for the trading day ; indeed due to different ( deterministic ) patterns in financial trading , it is likely that the fixed parameters below are varied according to the day .    a marked pp , of @xmath27 points ,",
    "is observed in time - period @xmath2 $ ] .",
    "this is written @xmath28 with @xmath29 , @xmath30 . here",
    "the @xmath31 are the transaction times and @xmath32 are the log - returns on the financial transactions . an appropriate model for such data , as in centanni & minozzo ( 2006b ) , is @xmath33 with @xmath34 a generic density ,",
    "@xmath35 are assumed to be @xmath36-distributed on 1 degree of freedom , location @xmath37 , scale @xmath38 and @xmath39 is the intensity .",
    "the unobserved intensity process is assumed to follow the dynamics @xmath40 with @xmath41 a compound poisson process : @xmath42 with @xmath43 a poisson process with rate parameter @xmath44 and i.i.d .",
    "jumps @xmath45 , @xmath46 is the exponential distribution . that is , for @xmath47 $ ] , @xmath48 with @xmath49 the jump times of the unobserved poisson process and @xmath50 fixed throughout ( using a short preliminary time series that is available in practice ) .",
    "we define the following notation : @xmath51 here @xmath52 ( respectively @xmath53 ) is the the restriction of the hidden ( observed ) pp to events in @xmath6 $ ] .",
    "similarly @xmath54 ( respectively @xmath55 ) is the the restriction of the hidden ( observed ) pp to events in @xmath56 $ ] .",
    "the objective is to perform inference at times @xmath57 , that is , to update the posterior distribution conditional on the data arriving in @xmath58 $ ] . to summarize , the posterior distribution at time @xmath10 is @xmath59 @xmath60}(\\bar{y}_n;\\bar{x}_n,\\mu,\\sigma)\\times\\\\ \\mathsf{p}(\\bar{x}_n)\\times \\tilde{p}(\\mu,\\sigma)\\label{eq : posterior}\\ ] ] with @xmath61}$ ] corresponding to the first part of the equation above , @xmath62 , @xmath63 , @xmath64 , @xmath65 and where @xmath66 is the uniform distribution on the set @xmath67",
    ", @xmath68 is the normal distribution of mean @xmath37 and variance @xmath69 , @xmath70 the gamma distribution of mean @xmath71 and @xmath72 is the poisson distribution .",
    "@xmath73 is the notation for the prior on the marked point - process and @xmath74 is the notation for the prior on @xmath75 . later",
    "a @xmath76 is introduced which will refer to an initial distribution .",
    "note it is possible to perform inference on @xmath75 independently of the unobserved pp ; it will not significantly complicate the simulation methods to include them .",
    "it is of interest to compute expectations w.r.t .",
    "the @xmath77 , and this is possible , using the smc methods below ( section [ sec : smcmethods ] ) .",
    "however , such algorithms are not of fixed computational cost ; the sequence of spaces over which the @xmath77 lie is increasing .",
    "these methods can also be used to draw inference from the marginal posterior of the process , over @xmath78 $ ] ; such algorithms can be designed to be of fixed computational complexity , for example by constraining any simulation to a fixed - size state - space .",
    "this idea is considered further in section [ sec : online_disc ] .",
    "one of the approaches for performing filtering for partially observed pp s is from centanni & minozzo ( 2006a ) . in this section",
    "the parameters @xmath75 are assumed known .",
    "let @xmath79 this is the support of the target densities for this method .",
    "the following decomposition is adopted @xmath80}(\\bar{y}_{n,1};\\bar{x}_n ) } { p_n(\\bar{y}_{n,1}|\\bar{y}_{n-1 } ) } \\mathsf{p}(\\bar{x}_{n,1 } ) \\pi_{n-1}(\\bar{x}_{n-1}|\\bar{y}_{n-1})\\\\ \\label{eq : centrec } \\tilde{p}_n(\\bar{y}_{n,1}|\\bar{y}_{n-1 } ) & = & \\int l_{(t_{n-1 } , t_n]}(\\bar{y}_{n,1};\\bar{x}_n ) \\mathsf{p}(\\bar{x}_{n,1 } ) \\pi_{n-1}(\\bar{x}_{n-1}|\\bar{y}_{n-1})d\\bar{x}_n . \\nonumber\\end{aligned}\\ ] ] at time @xmath81 of the algorithm , a reversible jump mcmc kernel ( although the analysis below is not restricted to such scenarios ) is used for @xmath82 steps to sample from the approximated density @xmath83}(\\bar{y}_{n,1};\\bar{x}_n ) \\mathsf{p}(\\bar{x}_{n,1 } ) s_{x , n-1}^{n}(\\bar{x}_{n-1})\\end{aligned}\\ ] ] where @xmath84 with @xmath85 obtained from a reversible jump mcmc algorithm of invariant measure @xmath86 .",
    "the algorithm for @xmath87 targets @xmath88 exactly ; there is no empirical density @xmath89 . at time",
    "@xmath87 the algorithm starts from an arbitrary point @xmath90 and subsequent steps are initialized by a draw from the empirical @xmath91 and the prior @xmath92 ( this can be modified ) ; @xmath93 additional samples are simulated .",
    "the above algorithm can be justified , theoretically , by using the poisson equation ( e.g.  glynn & meyn ( 1996 ) ) and induction arguments . below the assumption ( a )",
    "is made ; see the appendix for the assumption ( a ) as well as the proof .",
    "also , the expectation below is w.r.t .",
    "the process discussed above , given the observed data .",
    "[ prop : cmjustification ] assume ( a ) .",
    "then for any @xmath9 , @xmath53 , @xmath94 there exists @xmath95 such that for any @xmath96 @xmath97^{1/p } \\leq \\frac{b_{p , n}(\\bar{y}_n)\\|f_n\\|}{\\sqrt{n}}. \\label{eq : lpbound}\\ ] ]    this result helps to establish the theoretical validity of the method in centanni & minozzo ( 2006a ) , which to our knowledge , had not been established in that paper or elsewhere .",
    "in addition , it allows us to understand where and when the method may be of use ; this is discussed in section [ sec : smccm_comparison ] .",
    "smc samplers aim to approximate a sequence of related probability measures @xmath98 defined upon a common space @xmath99 .",
    "note that @xmath100 can depend upon the data and may not be known prior to simulation . for partially observed pps",
    "the probability measures are defined upon nested state - spaces : this case can be similarly handled with minor modification .",
    "smc samplers introduces a sequence of auxiliary probability measures @xmath101 on state - spaces of increasing dimension @xmath102}:=e_0\\times\\cdots\\times e_n,\\mathcal{e}_{[0,n]}:=\\mathcal{e}_0\\otimes\\cdots\\otimes\\mathcal{e}_n)$ ] , such that they admit the @xmath103 as marginals .",
    "the following sequence of auxiliary densities is used : @xmath104 where @xmath105 are backward markov kernels . in our application",
    "@xmath76 is the prior , on @xmath106 ( as defined below ) .",
    "it is clear that ( [ eq : smcsampaux ] ) admit the @xmath107 as marginals , and hence these distributions can be targeted using precisely the same mechanism as in sequential importance sampling / resampling ; the algorithm is given in figure [ fig : smcsampleralgo ] .",
    "the ess in figure [ fig : smcsampleralgo ] refers to the effective sample size ( liu , 2001 ) .",
    "this measures the weight degeneracy of the algorithm ; if the ess is close to @xmath82 , then this indicates that all of the samples are approximately independent .",
    "this is a standard metric by which to assess the performance the algorithm .",
    "the resampling method used throughout the paper is systematic resampling .",
    "one generic approach is to set @xmath108 as an mcmc kernel of invariant distribution @xmath109 and @xmath110 as the reversal kernel @xmath111 which we term the _ standard reversal kernel_. one can iterate the mcmc kernels , by which we use the positive integer @xmath112 to denote the number of iterates .",
    "it is also possible to apply the algorithm when @xmath108 is a mixture of kernels ; see del moral et al .",
    "( 2006 ) for details on the algorithm .    *",
    "\\0 . set @xmath113 ; for @xmath114 sample @xmath115 and compute @xmath116 .",
    "* 1.compute the normalized importance weights , @xmath117 if the @xmath118 then resample the particles and set the importance weights to uniform . set @xmath119 , if @xmath120 stop . * \\2 . for @xmath114 sample @xmath121 , and compute : @xmath122 @xmath123 and return to the start of 1 .      as described in section [ sec : intro ] , in complex problems it is often difficult to design efficient smc algorithms . in the example in section [ sec : motivate ] , the state - spaces of the subsequent densities are not common .",
    "the objective is to sample from a sequence of densities on the space , at time @xmath14 , @xmath124 with @xmath125 .",
    "that is , for any @xmath126 , @xmath127 .",
    "two standard methods for extending the space , as in del moral et al .  ( 2006 ) are to propagate particles by application of ` birth ' and the ` extend ' moves .",
    "consider the model in section [ sec : motivate ] .",
    "the following smc steps are used to extend the space at time @xmath14 of the algorithm .    *",
    "* birth*. a new jump is sampled uniformly in @xmath128 $ ] and a new mark from the prior .",
    "the incremental weight is @xmath129 * * extend*. a new jump is generated according to a markov kernel that corresponds to the random walk : @xmath130 with @xmath131 , @xmath132 .",
    "the new mark is sampled from the prior .",
    "the backward kernel and incremental weight are discussed in del moral et al .",
    "( 2007 ) section 4.3 .",
    "note , as remarked in whiteley et al .",
    "( 2011 ) , we need to be able to sample any number of births . with an extremely small probability , a proposal from the prior is included to form a mixture kernel .",
    "in addition to the above steps an mcmc sweep is included after the decision of whether or not to resample the particles is taken ( see step 1 .  of figure",
    "[ fig : smcsampleralgo ] ) : an mcmc kernel of invariant measure @xmath109 is applied .",
    "the kernel is much the same as in green ( 1995 ) .",
    "we applied the benchmark sampler , as detailed above , to some synthetic data in order to monitor the performance of the algorithm .",
    "standard practice in the reporting of financial data is to represent the time of a trade as a positive real number , with the integer part representing the number of days passed since january @xmath133 1900 and the non - integer part representing the fraction of 24 hours that has passed during that day ; thus , one minute corresponds to an interval of length @xmath134 .",
    "therefore we use a synthetic data set with intensity of order of magnitude @xmath135 .",
    "the ticks @xmath136 were generated from a specified intensity process @xmath137 that varied smoothly between three levels of constant intensity at @xmath138 , @xmath139 and @xmath140 .",
    "the log returns @xmath141 were sampled from the cauchy - distribution , location @xmath142 and scale @xmath143 .",
    "the entire data set was of size @xmath144 , @xmath2=[0,0.9]$ ] with @xmath145 .",
    "the intensity from which they were generated had constant levels at 6000 in the interval [ 0.05,0.18 ] ; at 4000 in the interval [ 0.51,0.68 ] ; and at 2000 in the intervals [ 0.28,0.42 ] and [ 0.78,0.90 ] .",
    "the sampler was implemented with all permutations @xmath146 for @xmath147 and @xmath148 , resampling whenever the effective sample size fell below @xmath149 ( recall @xmath82 is the number of particles and @xmath112 the mcmc iterations ) . when performing statistical inference , the intensity used parameters @xmath150 , @xmath151 and @xmath152 .",
    "it was found that for this smc sampler , the system consistently collapses to a single particle representation of the distribution of interest within an extremely short time period .",
    "that is , resampling is needed at almost every time step , which leads to an extremely poor representation of the target density .",
    "figure [ ess1 ] shows the ess at each time step for a paricular implementation .",
    "as can be seen , the algorithm behaves extremely poorly for this model .    , implemented with @xmath153 particles and with @xmath154 mcmc sweeps at each iteration .",
    "the dashed line indicates the resampling threshold at @xmath155 particles ; resampling is needed at 94.4% of the time steps.,height=302 ]      we have reviewed two existing techniques for the bayesian analysis of partially observed pp s .",
    "it should be noted that there are other methods , for example in varini ( 2007 ) . in that paper , the intensity has a finite number of functional forms and the uncertainty is related to the type of form at each inference time @xmath10 .",
    "the relative advantage of the approach of centanni & minozzo ( 2006a ) is the fact that the state - space need not be extended .",
    "on page 1586 of centanni & minozzo ( 2006a ) the authors describe the filtering / smoothing algorithm , for the process on the entire interval @xmath6 $ ] at time @xmath14 ; the theory discussed in proposition [ prop : cmjustification ] suggests that this method is not likely to work well as @xmath14 grows .",
    "the bound , which is perhaps a little loose is , for @xmath81 @xmath156 + \\hat{k}_n b_{p , n-1}(\\bar{y}_{n-1})\\ ] ] with @xmath157 $ ] , @xmath158 a constant related to the brkholder / davis inequalities ( e.g.  shiryaev ( 1996 ) ) , @xmath159 and @xmath160 a constant that is model / data dependent which is possibly bigger than 1 .",
    "the bound indicates that the error can increase over time , even under the exceptionally strong assumption ( a ) in the appendix .",
    "this is opposed to smc methods which are provably stable , under similar assumptions ( and that the entire state is updated ) , as @xmath161 ( del moral , 2004 ) .",
    "in other words , whilst the approach of centanni & minozzo is useful in difficult problems , it is less general with potentially slower convergence rate than smc . intuitively , it seems that the method of centanni & minozzo ( 2006a ) is perhaps only useful when considering the process on @xmath78 $ ] , as the process further back in time is not rejuvenated in any way . as a result , parameter estimation may not be very accurate .",
    "in addition , the method can not be extended to a sequential algorithm such that fully bayesian inference is possible . as noted above , smc samplers can be used in such contexts , but requires a computational budget that grows with the time parameter @xmath14 .    as mentioned above ,",
    "smc methods are provably stable under some conditions as the time parameter grows .",
    "however , some remarks related to the method in figure [ fig : smcsampleralgo ] can help to shed some light on the poor behaviour in section [ sec : simosfinance1 ] .",
    "consider the scenario when one is interested in statistical inference on @xmath162 $ ] .",
    "suppose for simplicity , one can write the posterior on this region as @xmath163 for fixed @xmath164 .",
    "if one considers just pure importance sampling , then conditioning upon the data , one can easily show that for any @xmath165(square ) integrable @xmath166 with @xmath167 , the asymptotic variance in the associated gaussian central limit theorem is lower - bounded by : @xmath168 then , for any mixing type sequence of data the asymptotic variance will for some @xmath166 and in _ some _ scenarios , grow without bound as @xmath169 grows - this is a very heuristic observation , that requires further investigation .",
    "hence , given this discussion and our empirical experience , it seems that we require a new methodology , especially for complex problems .      an important remark associated to the simulations in section [ sec : simosfinance1 ] ,",
    "is that it can not be expected that simply increasing the number of particles will necessarily a significantly better estimation procedure .",
    "the algorithm completely crashes to a single particle and it seems that naively increasing computation will not improve the simulations .    as discussed above ,",
    "the inherent difficulty of sampling from the given sequence of distributions is that of extending the state - space .",
    "it is known that conditional on all parameters except the final jump , the optimal importance distribution is the full conditional density ( del moral et al .",
    "2006 ) . in practice , for many problems it is either not possible to sample from this density , or to evaluate it exactly ( which is required ) . in the case that it is possible to sample from the full conditional , but the normalizing constant is unknown , the normalizing constant problem can be dealt with via the random weight idea ( rousset & doucet , 2006 ) . in the context of this problem we found that the simulation from the full conditional density of @xmath170 was difficult , to the extent that sensible rejection algorithms and approximations for the random weight technique were extremely poor .",
    "another solution , in del moral et al .",
    "( 2007 ) , consists of stopping the algorithm when the effective sample size ( ess ) drops and using an additional smc sampler to facilitate the extension of the state - space .",
    "however , in this example , the ess is so low , that it can not be expected to help . due to",
    "above discussion , it is clear that a new technique is required to sample from the sequence of distributions ; two ideas are presented below .",
    "one idea , in the context of estimating static parameters , that could be adopted is smc@xmath171 ( chopin et al .",
    "2011 ) which has appeared after the first versions of this article .",
    "in the following section , two approaches are presented to deal with the problems in section [ sec : simosfinance1 ] .",
    "first , a state - space saturation approach , where sampling of pp trajectories is performed over a state space corresponding to a fixed observation interval .",
    "second , a data - point tempering approach . in this approach ,",
    "as the time parameter increases , the ( artificial ) target in the new region is simply the prior and the data are then sequentially added to the likelihood , softening the state - space extension problem . both of these procedures use the basic structure of figure [ fig : smcsampleralgo ] , with some refinements , that are mentioned in the text . as for the algorithms in figure [ fig : smcsampleralgo ]",
    "we add dynamic resampling steps ; when mcmc kernels are used , one can resample before sampling - see del moral et al .",
    "( 2006 ) for details .",
    "a simple idea , which has been used in the context of reversible jump , is to saturate the state - space .",
    "the idea relies upon knowing the observation period of the pp ( @xmath2 $ ] ) _ a priori _ to the simulation .",
    "this is realistic in a variety of applications .",
    "for example , in section [ sec : motivate ] , often we may only be interested in performing inference for a day of trading and thus can set @xmath2 $ ] .    in details",
    ", it is proposed to sample , in the case of the example in section [ sec : motivate ] , from the sequence of target densities defined on the space @xmath172 the ( marginal , that is in the sense of ) target densities are now , denoted with a @xmath173 as a super - script : @xmath174 where the prior on the point process is @xmath175 , @xmath176 .",
    "we then use , for @xmath108 , an mcmc kernel of invariant measure @xmath177 and the standard reversal kernel discussed in section [ sec : smcmethods ] for the backward kernel .",
    "the initial distribution is the prior and the weight at time 0 is proportional to 1 for each particle .",
    "the incremental weights at subsequent time - points are simply : @xmath178 inference w.r.t .  the original @xmath179 can be performed via is as the supports of the targets of interest are contained within the proposals ( i.e.  via the targets of the saturated algorithm ) .",
    "a simple solution to the state - space extension problem , which allows data to be incorporated sequentially , albeit not being of fixed computational complexity is as follows .",
    "when the time parameter increases , the new part of the process is simulated according to the prior .",
    "then each new data point is added to the likelihood in a sequential manner . in other words",
    "if there are @xmath14 data points , then there are @xmath180 time - steps of the algorithm .    to illustrate , consider only the scenario of the data in @xmath162 $ ] , with @xmath181 .",
    "then our sequence of ( marginal ) targets are : @xmath182 and for @xmath183 @xmath184 then , when considering the extension of the point - process onto @xmath185 $ ] , one has a ( marginal ) target that is : @xmath186 when one extends the state - space , we sample from the prior on the new segment , which leads to a unit incremental weight ( up - to proportionality ) - no backward kernel is required here . then , when adding data , we simply use mcmc kernels to move the particles ( the kernels as in section [ sec : nested : spaces ] ) and the standard reversal kernel discussed in section [ sec : smcmethods ] for the backward kernel .",
    "this leads to an incremental weight that is the ratio of the consecutive densities at the previous state .",
    "the potential advantage of this idea is that , when extending the state - space , there is no extra data , to potentially complicate the likelihood .",
    "thus , it is expected that if the prior does not propose a significant number of new jumps , that the incremental weights should be of relatively low variance .",
    "the subsequent steps , when considering the jumps in @xmath187 are performed on a common state - space and hence should not be subject to as substantial variability as when the state - space changes .",
    "this idea could also be adapted to the case that the likelihood on the new interval are tempered instead ( e.g.  jasra et al .",
    "( 2007 ) ) .    as a theoretical investigation of this idea , we return to the discussion of section [ sec : smccm_comparison ] and in particular , where the joint target density is .",
    "we consider the data - point tempering which starts with a draw from the prior and sequentially adds data points . in otherwords",
    "runs for @xmath188 time - steps with @xmath189 with a @xmath190 such that for each @xmath191 , @xmath192 and all @xmath193 , @xmath194 .",
    "the algorithm resamples at every time - step and uses mcmc kernels , which are assumed to satisfy , for some @xmath195 , and each @xmath196 , @xmath169 , @xmath197 @xmath198 at the very final time - step one also resamples after the final weighting of the particles .",
    "write @xmath199 as the samples that approximate target .",
    "suppose @xmath200 , then there is a gaussian central limit theorem for @xmath201 writing the asymptotic variance as @xmath202 , we have the following result whose proof is in the appendix .",
    "[ prop : data_point ] for smc sampler described above , with final target then we have for any @xmath200 that there exists a @xmath203 such that for any @xmath204 , @xmath205 @xmath206    the upper - bound does not grow with the number of data .",
    "that is , by increasing the computational complexity linearly in the number of data , one has an algorithm whose error does not grow as more data ( and regions ) are added .",
    "this is similar to the observation of beskos et al .",
    "( 2011 ) , when increasing the dimension of the target density .",
    "we note that the result is derived under exceptionally strong assumptions . in general ,",
    "when one considers @xmath169 growing , one requires sharper tools than the dobrushin coefficients used here ( e.g.  eberle & marinelli ( 2011 ) ) ; this is beyond the scope of the current article and our result above is illustrative ( and hence potentially over - optimistic ) .",
    "a key characteristic that has not yet been addressed is the fact that each has a computational complexity that is increasing with time . in a procedure that would otherwise be well suited to providing online inference ,",
    "this is an unattractive feature .",
    "a large contribution to this increasing computational budget derives from the mcmc sweeps at the end of each iteration . as the space",
    "over which the invariant mcmc kernel is being applied is increased , so does the expense of the algorithm .",
    "an improvement to the computational demand of the samplers can therefore be made by keeping the space over which the mcmc kernel is applied constant .",
    "the _ reduced computational complexity ( rcc ) _ alternative to each of the samplers is also designed by amending the algorithms such that , at time @xmath10 , the mcmc sweep operates over , at most , 20 changepoints , i.e. over the interval @xmath207 . due to the well - known path degeneracy problem in smc ( see kantas et al .  ( 2011 ) ) , the estimates will be poor approximations of the true values , when including static parameters and extending the space of the point process for a long time .",
    "we note , at least for our application , it is reasonable to consider @xmath15 fixed and thus , this is less problematic .",
    "we now return to the example from section [ sec : motivate ] and the settings as in section [ sec : simosfinance1 ] .",
    "the saturated and tempered samplers , as well as their rcc alternatives , were implemented using the simulated data set ( in section [ sec : simosfinance1 ] ) , in order to compare their respective performances against the benchmark sampler and to compare the accuracy of the resulting intensity estimates against an observed intensity process .",
    "all of the alternative samplers were implemented under the same conditions , using the algorithm and model parameters as described for the implementation of the benchmark sampler .",
    "all results are averaged over 10 runs of the algorithm .    in assessing the performance of the sampler , quantities of interest are , once again , the resampling rate and the processing time , as well as the minimum ess recorded throughout the execution of the sampler .",
    "the resampling rates for all three samplers and their rcc alternatives are presented in table [ table_resamplingratecomparisons ] , with the corresponding minimum ess s attained recorded in table [ table_minimumesscomparisons ] and the corresponding processing times in table [ table_processingtimecomparisons ] .",
    "figure [ ess2 ] displays the evolution of the ess over a particular run of the algorithm .",
    "figure [ ess3 ] shows the estimated intensity at each time @xmath10 , given data up to time @xmath10 . from table [ table_resamplingratecomparisons ] , it is clear to see that , for the saturated and tempered samplers , an increase in @xmath112 results in a decrease in the resampling rates , i.e. a decrease in sampler degeneracy , as expected .",
    "it is also plain to see from table [ table_minimumesscomparisons ] that , as @xmath82 increases , so does the minimum ess , and thus the reliability of the estimates . from tables [ table_resamplingratecomparisons ] and [ table_minimumesscomparisons ] , figure [ ess3 ] and comparing figure [ ess2 ] to figure [ ess1 ]",
    "it is clear that the saturated and tempered samplers significantly outperformed the benchmark sampler .",
    ".table showing the resampling rates of each of the three smc samplers and their reduced computational complexity alternatives , for the six algorithm parameterisations that were tested .",
    "the ess plots for the saturated and tempered samplers with @xmath153 , @xmath154 are given in figure [ ess2 ] for comparison with the corresponding ess plot for the benchmark sampler given in figure [ ess1 ] [ cols= \" > , > , > , > , > , > , > \" , ]     table [ table_rmspecomparisons ] presents the rmspes for the intensity estimates resulting from the samplers and the rcc alternatives .",
    "it was observed that , in calculating the rmspes for lag indices @xmath208 using each sampler , both the saturated and tempered samplers displayed the smallest error at @xmath209 , i.e.  their respective one - step - ahead predictions were more accurate than those made for lags up to 2.64 hours ( each observation interval corresponds to 0.0264 days = 1.584 minutes ) .",
    "the rcc samplers provide significant computational savings and do not seem to degrade substantially , w.r.t .  the error criteria .",
    "again , we remark that in general one should not trust the estimates of the rcc , but as seen here , they can provide a guideline for the intensity values .",
    "in this paper we have considered smc simulation for partially observed point processes and implemented them for a particular doubly stochastic pp .",
    "two solutions were given , one based upon saturating the state - space , which is suitable in a wide variety of applications and data - point tempering which can be used in sequential problems .",
    "we also discussed rcc versions of these algorithms , which reduce computation , but will be subject to the path degeneracy problem when including static parameters and considering the smoothing distribution .",
    "we saw that the methods can be successful , in terms of weight degeneracy versus the benchmark approach detailed in del moral et al .",
    "in addition , for real data it was observed that predictions using the rcc could be reasonable ( relative to the normal versions of the algorithms ) , but caution on using these estimates should be used .",
    "the methodology we have presented is not online .",
    "as we have seen , when one modifies the approaches to have fixed computational complexity , the path degeneracy problem occurs and one can not deal with scenario with static parameters . in this case , we are working with dr .",
    "n. whiteley on a technique based upon fixed window filtering .",
    "this is an on - line algorithm which allows data to be incorporated as they arrive with computational cost which is non - increasing over time , but is biased .",
    "the approach involves sampling from a sequence of distributions which are constructed such that , at time @xmath10 , previously sampled events in @xmath210 $ ] can be discarded . in order to be exact ( in the sense of targeting the true posterior distributions ) , this scheme would involve point - wise evaluation of an intractable density .",
    "we are working on a sensible approximation of this density , at the cost of introducing a small bias .",
    "we thank nick whiteley for conversations on this work .",
    "the second author was supported by an moe grant .",
    "we thank two referees for their comments , which have vastly improved the article .",
    "in this appendix we give a proof of proposition [ prop : cmjustification ] . for probability meaure @xmath211 and function @xmath166 , @xmath212 . for any collection of points",
    "@xmath213 write @xmath214 the transition kernels are written @xmath215 ( which is not to be confused with the @xmath215 from the smc samplers algorithm ) and for any @xmath81 , @xmath12 , @xmath216empirical density @xmath217 , @xmath218 is the kernel of invariant distribution @xmath219}(\\bar{y}_{n,1};\\bar{x}_n ) } { p_n(\\bar{y}_{n,1}|\\bar{y}_{n-1 } ) } \\mathsf{p}(\\bar{x}_{n,1 } ) s_{n-1}^n(\\bar{x}_{n-1}).\\ ] ] recall the generic notation @xmath220 .",
    "we drop the dependence upon the data and denote @xmath221}(\\bar{y}_{n,1};\\bar{x}_n ) } { p_n(\\bar{y}_{n,1}|\\bar{y}_{n-1 } ) } \\mathsf{p}(\\bar{x}_{n,1 } ) \\label{eq : filteringweight}.\\ ] ] the @xmath216empirical measure of points generated up to time @xmath222 is written @xmath223 . for a given @xmath9 , @xmath224 we have the notation @xmath225 and @xmath226 , @xmath227 , @xmath228 the dirac measure .",
    "the @xmath19finite measure @xmath229 is defined on the space @xmath230 ; in practice it is the product of an appropriate version of lebesgue and counting measures .      _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ assumption _ * ( a)*_. there exist an @xmath231 and probability measure @xmath232 on @xmath233 such that for any @xmath234 @xmath235 for any @xmath81 , there exist an @xmath236 and probability measure @xmath237 on @xmath238 such that for any @xmath239 and any collection of points @xmath240 @xmath241 for any @xmath81 @xmath242 where @xmath243 is as in ( [ eq : filteringweight ] ) . _",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    it should be noted that the uniform ergodicity assumption on @xmath244 is quite strong . if the kernel @xmath218 were an metropolis - hastings independence sampler of proposal @xmath245 @xmath246",
    ", then @xmath247 satisfies the assumption if @xmath248 is uniformly lower - bounded .",
    "note also , due to the suppression of the data from the notation , it is typical that @xmath249 would depend upon @xmath250 .",
    "the proof is inductive on @xmath14 .",
    "some details are omitted as the proof is quite similar to the control of adaptive mcmc chains , e.g.  andrieu et al .",
    "it should be noted the proof for this algorithm differs as the kernel possesses an invariant measure that does not change with the iteration @xmath251 .",
    "let @xmath87 then , by ( a ) @xmath215 , is a uniformly ergodic markov kernel of invariant measure @xmath88 .",
    "it is simple to use the poisson equation to prove the proposition , which is given to establish the induction .",
    "let @xmath252 $ ] be the solution to the poisson equation ; @xmath253",
    ". then @xmath254 & = & \\sum_{i=1}^n[\\hat{f}_1(\\bar{x}_1^{(i)})-k_1(\\hat{f}_1)(\\bar{x}_1^{(i)})]\\\\ & = & \\sum_{i=1}^{n-1}[\\hat{f}_1(\\bar{x}_1^{(i+1)})-k_1(\\hat{f}_1)(\\bar{x}_1^{(i ) } ) ] + \\hat{f}_1(\\bar{x}_1^{(1 ) } ) - k_1(\\hat{f}_1)(\\bar{x}_1^{(n)})\\end{aligned}\\ ] ] the first quantity on the r.h.s . is a martingale , @xmath255 , w.r.t .",
    "the filtration @xmath256 ( i.e.  the @xmath19algebra generated by markov chain ) .",
    "then , using the minkowski inequality @xmath257\\bigg|^p\\bigg]^{1/p } \\leq \\frac{1}{n } \\bigg\\ { \\mathbb{e}_{\\bar{x}_1^{(1)}}\\bigg[\\big|m_n^1\\big|^p\\bigg]^{1/p } + |\\hat{f}_1(\\bar{x}_1^{(1)})| + \\mathbb{e}_{\\bar{x}_1^{(1)}}\\bigg [ \\bigg|k_1(\\hat{f}_1)(\\bar{x}_1^{(n)})\\big|^p\\bigg]^{1/p } \\bigg\\}.\\ ] ] the last term can be dealt with as follows .",
    "@xmath258^{1/p } & \\leq & \\mathbb{e}_{\\bar{x}_1^{(1)}}\\bigg[\\bigg|\\sum_{i=0}^{\\infty}[k_1^i(f_1)(\\bar{x}_1^{(n+1 ) } ) -\\pi_1(f_1)]\\bigg|^{p}\\bigg]^{1/p}\\\\ & \\leq & \\|f_1\\| \\sum_{i=0}^{\\infty}\\mathbb{e}_{\\bar{x}_1^{(1)}}\\bigg[\\bigg|[k_1^i-\\pi_1]\\bigg(\\frac{f_1}{\\|f_1\\|}\\bigg)(\\bar{x}_1^{(n+1)})\\bigg|^{p}\\bigg]^{1/p}\\\\ & \\leq & \\frac{\\|f_1\\|}{\\epsilon_1}\\end{aligned}\\ ] ] here we have applied the conditional jensen inequality and the bound on the total variation distance for uniformly ergodic markov chains : @xmath259 , @xmath260}|k_1^i(f)(x)-\\pi_1(f)|\\leq ( 1-\\epsilon_1)^i$ ] .",
    "note that this bound holds for any @xmath234 .",
    "the martingale term is bounded using the brkholder and davis inequalities ( i.e.  the inequality below holds for any @xmath94 ) : @xmath261^{1/p } \\leq b_p \\mathbb{e}_{\\bar{x}_1^{(1)}}\\bigg[\\bigg|\\sum_{i=1}^{n-1}[\\hat{f}_1(\\bar{x}_1^{(i)})- k_1(\\hat{f}_1)(\\bar{x}_1^{(i)})]^2\\bigg|^{p/2}\\bigg]^{1/p}.\\ ] ] when @xmath262 the minkowski inequality and the above manipulations yield a bound @xmath263 , with @xmath264 a constant only depending upon @xmath265 and @xmath266 . when @xmath267 the inequality @xmath268 for @xmath269 is applied then jensen to yield a similar bound ; see andrieu et al .",
    "( 2011 ) and the references therein .",
    "thus , for @xmath87 it follows @xmath270^{1/p } \\leq \\sqrt{n}b(p,\\epsilon_1)\\|f_1\\| $ ] ; note that @xmath264 depends only on @xmath266 and @xmath265 - this is important in the sequel . putting these bounds together and noting that , by the above arguments , the solution to the poisson equation is uniformly bounded in @xmath271 the proof at rank @xmath87 is completed .",
    "now assume the result at @xmath222 and consider @xmath14 .",
    "note that via fubini @xmath272 where @xmath273 .",
    "then application of the minkowski inequality yields : @xmath274^{1/p } & \\leq   & \\mathbb{e}_{\\bar{x}_{1}^{(1)}}\\bigg[\\bigg|\\frac{1}{n}\\sum_{i=1}^nf_n(\\bar{x}_{n}^{(i)})-s_{\\bar{x},n-1}^n(i(f_n\\times g_n))\\bigg|^p\\bigg]^{1/p } \\nonumber\\\\ & & + \\mathbb{e}_{\\bar{x}_{1}^{(1)}}\\bigg[\\bigg|[s_{\\bar{x},n-1}^n-\\pi_{n-1}](i(f_n\\times g_n))\\bigg|^p\\bigg]^{1/p } \\label{eq : induction}.\\end{aligned}\\ ] ] due to the induction hypothesis and ( a ) , the second term on the r.h.s of the inequality is upper - bounded by @xmath275 for some @xmath276 ; if the data were not suppressed , then there is an explicit dependence upon this quantity . then considering the first term on the r.h.s of ( [ eq : induction ] ) , conditioning upon the @xmath19algebra @xmath277 generated by the process at time @xmath14 is a uniformly ergodic markov chain of invariant distribution @xmath278",
    "thus , for example : @xmath279^{1/p } \\leq \\mathbb{e}_{\\bar{x}_1^{(1)}}\\bigg[\\bigg(\\frac{\\|f_n\\|}{\\epsilon_n}\\bigg)^p\\bigg]^{1/p}\\ ] ] adopting exactly the above arguments . noting that the bound on the conditional expectation is deterministic , i.e.  does not depend upon @xmath277 , the induction is easily completed .",
    "for the proof of proposition [ prop : data_point ] , we require a round of notations .",
    "we write @xmath280 and define the following quantities : @xmath281 with @xmath282 .",
    "in addition , set @xmath283 and @xmath284 we add an extra markov kernel to allow us to use directly formulae in del moral ( 2004 ) ; @xmath285",
    ". then we define @xmath286 in addition @xmath287 , @xmath288 , with @xmath289 with the convention that @xmath290 is the identity operator . also define @xmath291 and finally @xmath292    we have from proposition 9.4.2 of del moral ( 2004 ) that : @xmath293 the objective is to re - write the summand in terms of a difference @xmath294 and use the mixing conditions to control the dobrushin coefficient of the kernel @xmath295 ; see e.g.  del moral et al .",
    "( 2012 ) section 4 . to that end",
    ", we can only consider the first @xmath296 terms , for which the dobrushin coeffient will satisfy : @xmath297/2 \\rfloor } \\quad r_1-p \\geq 1 \\label{eq : dobrushin_bd}\\ ] ] for some @xmath298 that does not depend upon @xmath169 and @xmath299 the total variation distance ( again see del moral et al .",
    "( 2012 ) , as the condition @xmath300 of that paper is satisfied ) .",
    "the reminder of the terms in the sum are easily bounded , independently of @xmath169 , and we omit these calculations .    by using standard properties of feynman - kac formula , we have that each summand in is equal to @xmath301)^2}{\\eta_p(q_{p , r_1 + 1}(1))^2}\\bigg)\\ ] ] by using jensen s inequality , it follows that @xmath301)^2}{\\eta_p(q_{p , r_1 + 1}(1))^2}\\bigg)\\ ] ] @xmath302 using the fact that ( see e.g.  section 4 of del moral et al .  ( 2012 ) ) @xmath303 for a @xmath203 that does not depend on @xmath169 and using the bound in we can conclude .",
    "del moral , p. , doucet , a. & jasra , a.  ( 2007 ) .",
    "sequential monte carlo for bayesian computation ( with discussion ) . _ bayesian statistics 8 _ , ed .",
    "bayarri , s. , berger , j. o. , bernardo , j. m. , dawid , a. p. , heckerman , d. smith , a. f. m. and west , m. 115 - 149 , oup : oxford .",
    "rydberg , t. h. , & shephard , n.  ( 2000 ) . a modelling framework for the prices and times of trades made on the new york stock exchange .",
    "_ non - linear and non - stationary signal processing _ , ed .",
    "fitzgerald , w. j. , smith , r. l. , walden , a. t. & young , p. c. , 217246 , cup : cambridge ."
  ],
  "abstract_text": [
    "<S> this paper presents a simulation - based framework for sequential inference from partially and discretely observed point process ( pp s ) models with static parameters . taking on a bayesian perspective for the static parameters , we build upon sequential monte carlo ( smc ) methods , investigating the problems of performing sequential filtering and smoothing in complex examples , where current methods often fail . </S>",
    "<S> we consider various approaches for approximating posterior distributions using smc . </S>",
    "<S> our approaches , with some theoretical discussion are illustrated on a doubly stochastic point process applied in the context of finance . </S>",
    "<S> +    _ some key words _ : point processes , sequential monte carlo , intensity estimation    * inference for a class of partially observed point process models *    by james s. martin@xmath0 , ajay jasra@xmath1 & emma mccoy@xmath0    @xmath0department of mathematics , imperial college london , london , sw7 2az , uk . </S>",
    "<S> + e-mail:`james.martin04@ic.ac.uk , e.mccoy@ic.ac.uk ` + @xmath1department of statistics & applied probability , national university of singapore , singapore , 117546 , sg . </S>",
    "<S> + e-mail:`staja@nus.edu.sg ` </S>"
  ]
}