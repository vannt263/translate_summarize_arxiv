{
  "article_text": [
    "in recent years a number of models have emerged for describing higher - order structure in images ( i.e. , beyond sparse , gabor - like decompositions ) .",
    "these models utilize distributed representations of covariance matrices to form an infinite or combinatorial mixture of gaussians model of the data  @xcite .",
    "these models have been shown to effectively capture the non - stationary variance structure of natural images .",
    "a variety of related models have focused on the local radial ( in vectorized image space ) structure of natural images  @xcite . while these models represent a significant step forwards in modeling higher - order natural image structure , they only implicitly model local phase alignments across space and scale .",
    "such local phase alignments are implicated as being a hallmark of edges , contours , and other shape structure in natural images  @xcite . the model proposed in this paper attempts to extend these models to capture both amplitude and phase in natural images .    in this paper",
    ", we first extend the recent factorized , third - order boltzmann machine model of ranzato & hinton  @xcite to the case of @xmath0-spherically symmetric distributions . in order to directly model the dependencies among local amplitude and phase variables , we consider the restricted case of two - dimensional subspaces with @xmath1-norm . when adapted to natural images ,",
    "the subspace filters converge to quadrature - pair gabor - like functions , similar to previous work  @xcite .",
    "the dependencies among amplitudes are modeled using a set of hidden units , similar to ranzato & hinton  @xcite .",
    "phase dependencies between subspaces are modeled using another set of hidden units as a mixture of phase coupling `` covariance '' matrices : conditioned on the hidden units , phases are modeled via a phase - coupled distribution  @xcite .",
    "our model may be viewed within the same framework as a number of recent models that attempt to capture higher - order structure in images by factorizing the coefficients of oriented , bandpass filters  @xcite .",
    "these models are currently the best probabilistic models of natural image structure : they produce state - of - the - art denoising  @xcite , and achieve lower entropy encodings of natural images  @xcite .",
    "they can be viewed as sharing a common mathematical form in which the filter coefficients , @xmath2 , are factored into a non - negative component @xmath3 and a scalar component @xmath4 , where @xmath5 .",
    "the non - negative factors , @xmath3 , are modeled as either an independent set of variables each shared by a pair of linear components  @xcite , a set of variables with learned dependencies to the linear components  @xcite , or as a single radial component  @xcite .",
    "the scalar factor , @xmath4 , is modeled in a number of ways : as an independent angular unit vector  @xcite , as a correlated noise process  @xcite , as the phase angle of paired filters  @xcite , or as a sparse decomposition of latent variables  @xcite .    by separating the filter coefficients into two sets of variables , it is possible for higher levels of analysis to model higher - order statistical structure that was previously entangled in the filter coefficients themselves .",
    "for example , the non - negative variables @xmath3 are usually related to the local contrast or power within an image region , and karklin & lewicki have shown that it is possible to train a second layer to learn the structure in these variables via sparse coding  @xcite .",
    "similarly , lyu & simoncelli learn an mrf model on these variables and show that the resulting model achieves state - of - the - art denoising  @xcite .",
    "it is generally less clear what structure is represented in the scalar variables @xmath4 .",
    "here we take inspiration from zetzsche s observations regarding the circular symmetry of joint distributions of related filter coefficients and conjecture that this quantity should be modeled as the sine or cosine of an underlying phase variable , i.e. , @xmath6 or @xmath7  @xcite .",
    "one of the main contributions of this paper is a model of the joint structure of local phase in natural images . for the case of phases in a complex pyramid , the empirical marginal distribution of phases is always uniform across a corpus of natural images ( data not shown ) ; however , the empirical distribution of pairwise phase differences is often concentrated around a preferred phase .",
    "we can write the joint distribution of a pair of phase variables with a dependency in the difference of the phase variables as a von mises with concentration parameter , @xmath8 , and mean , @xmath9 , is defined as @xmath10 , where @xmath11 is the zeroth order modified bessel function . ] distribution in the difference of the phases : @xmath12 this distribution is parameterized by a concentration @xmath13 and a phase offset @xmath9 .",
    "using trigonometric identities we can re - express the cosine of the difference as a sum of bivariate terms of the form @xmath14 .",
    "one may view these terms as the pairwise statistics for angular variables , just as the the bivariate terms in the covariance matrix are the pairwise statistics for a gaussian .",
    "this logic extends to multivariate distributions with @xmath15 .    in the next section",
    "we describe an extension to the mean and covariance restricted boltzmann machine ( mcrbm ) of ranzato & hinton  @xcite .",
    "our extension represents local amplitude and phase in its factors .",
    "we model the amplitude dependencies with a set of hidden variables , and we model the phase dependencies among factors as a combinatorial mixture of phase - coupling distributions  @xcite .",
    "this model is shown schematically in fig .",
    "[ fig : model ] .",
    "@xmath16{figures / model_schematic.pdf}\\ ] ]",
    "we first review the factorized third - order boltzmann machine of ranzato and hinton  @xcite named mcrbm because it models both the mean and covariance structure of the data .",
    "we then describe an extension that models pairs of factors as two dimensional subspaces , which we call the mprbm .",
    "the mprbm provides a phase angle between pairs .",
    "the joint statistics between phase angles are not explicitly modeled by the mprbm .",
    "thus we propose additional hidden factors that model the pair - wise phase dependencies as a product of phase coupling distributions . for convenience",
    "we name this model mpkrbm , where k references the phase coupling matrix @xmath17 that is generated by conditioning on the phase - coupling hidden units .",
    "the mcrbm defines a probability distribution by specifying an energy function _ e_. the probability distribution is then given as @xmath18 , where @xmath19 is the vectorized input image patch with @xmath20 pixels .",
    "the mcrbm has two major parts of the energy , the covariance contributions , @xmath21 , and the mean contributions , @xmath22 .",
    "the covariance terms are defined as : @xmath23 where @xmath24 is a matrix with positive entries , @xmath25 is the number of hidden units and @xmath26 is a vector of biases .",
    "the columns of the matrix @xmath27 are the image domain filters and their squared outputs are weighted by each row in @xmath28 .",
    "the @xmath28 matrix can be considered as a weighting on the squared outputs of the image filters .",
    "we normalize the visible units ( @xmath29 ) following the procedure in  @xcite .",
    "each term in the first sum includes two visible units and one hidden unit and is a third - order boltzmann machine .",
    "however , the third - order interactions are restricted by the form of the model into factors . each factor , @xmath30 is a deterministic mapping from the image domain .",
    "the hidden units combine combinatorially to produce a zero - mean gaussian distribution with an inverse covariance matrix that is a function of the hidden units : @xmath31 because the representation in the hidden units is distributed , the model can describe a combinatorial number of covariance matrices .    the mean contribution to the energy , @xmath22",
    "is given as : @xmath32 with @xmath33 binary hidden units @xmath34 that connect directly to the visible units through the matrix @xmath35 .",
    "the @xmath36 terms are the mean hidden biases .",
    "the form of the mean contribution is a standard rbm @xcite .",
    "note that the conditional over both sets of hidden units is factorial .",
    "the conditional distribution over the visible units given the hidden units is a gaussian distribution , which is a function of the hidden variable states : @xmath37 where @xmath38 is given as in eq .",
    "[ eq : sigma ] .",
    "the mean of the specified gaussian is a function of both the mean @xmath39 and covariance @xmath40 hidden units .",
    "the total energy is given by : @xmath41 with the last two terms a penalty on the variance of the visible units introduced because @xmath21 is invariant to the norm of the visible units and biases @xmath42 on the visible units .",
    "a number of recent results indicate that the local structure of image patches is well modeled by @xmath0-spherically symmetric subspaces  @xcite . to produce @xmath0-spherically symmetric subspaces",
    "we impose a pairing of factors into an @xmath0 subspace . the covariance energy term in the mcrbm",
    "is thus altered to give : @xmath43^{1/\\alpha } - \\sum_{n=1}^n b_n^ch_n^c\\ ] ] now the tensor @xmath44 is a set of filters for each factor , @xmath45 spanning the @xmath46 dimensional subspace over the index @xmath47 .",
    "the distribution over the visible conditioned on the hidden units can be expressed as a mixture of @xmath0 distributions .",
    "note that the hidden units remain independent conditioned on the visible units .",
    "the optimal choice of @xmath46 and @xmath48 is an interesting project related to recent models  @xcite but is beyond the scope of this paper . here",
    ", we have chosen to focus on modeling the structure in the space complementary to the norms of the subspaces .",
    "to achieve a tractable form of the subspace structure we select the special case of @xmath49 and @xmath50 .",
    "the choice of @xmath50 is motivated by subspace - ica models  @xcite and sparse coding with complex basis functions  @xcite where the amplitude within each complex basis function subspace is modeled as a sparse component .",
    "while the formulation of @xmath0-spherically symmetric subspace models the spherically symmetric distributions of natural images , there are likely to be residual dependencies between the subspaces in the non - radial directions . for example , elongated edge structure will produce dependencies in the phase alignments across space and through spatial scale  @xcite .",
    "such dependencies are not captured , or are at least only implicitly captured in the mprbm . by formulating the mprbm with @xmath49 and @xmath50 we can define a phase angle within each subspace .",
    "the dependencies between these phase angles will capture image structure such as phase alignments due to edges .",
    "we define a new variable , @xmath51 , which is a deterministic function of the visible units : @xmath52 and @xmath53 , where @xmath54 , and @xmath55 is the imaginary unit and @xmath56 is the complex argument or phase .",
    "we now use a mathematical form that is similar to the covariance model contribution in the mcrbm to model the joint distribution of phases .",
    "we define the energy of the phase coupling contribution , denoted @xmath57 , as , @xmath58 with @xmath59 binary hidden units @xmath60 that modulate the columns of the matrix @xmath61 .",
    "the rows of r then modulate the squared projections of the vector @xmath2 through the matrix @xmath62 .",
    "the term @xmath63 is a vector of biases for the @xmath60 hidden units .",
    "similar to the @xmath40 terms in the mcrbm , the @xmath64 units in the mpkrbm contribute pair - wise dependencies in the sine - cosine space of the phases .",
    "pair - wise dependencies in the sine - cosine space can be re - expressed using trigonometric identities as terms in the sums and differences of the phase pairs , identical to the phase coupling described in eq .",
    "[ eq : vonmises ] .",
    "such explicit dependencies may be important to model because edges in images exhibit structured dependencies in the differences of local spatial phase .    because the phase coupling energy is additive in each @xmath65 term the hidden unit distribution conditioned on the hidden units is factorial .",
    "the probability of a given @xmath65 is given as : @xmath66 where the sigmoid , or logistic , function is @xmath67 .",
    "we can see the dependency structure imposed by the @xmath68 units by considering the conditional distribution in the space of the phases , @xmath69 : @xmath70 therefore , the @xmath68 units provide a combinatorial code of phase - coupling dependencies .",
    "the number of phase - coupling matrices that the model can generate is exponential in the number of @xmath68 hidden units because the hidden unit representation is binary and combinatorial .",
    "again , instead of allowing arbitrary three way interactions between the @xmath2 variables and the hidden units , we have chosen a specific factorization where the squared factors are @xmath71 . because there are no direct interactions between the hidden units , @xmath68",
    ", the model still has the form of a conventional restricted boltzmann machine .",
    "we call this model a mpkrbm because it builds upon the mprbm and the k references the coupling matrix in the pair - wise phase distribution produced by conditioning on @xmath68 .    combining the three types of hidden units , @xmath72 , @xmath39 , and @xmath68 , allows each type of hidden unit to model structure captured by the corresponding functional form .",
    "for example , the @xmath72 hidden units will generate phase dependencies implicitly through their activations .",
    "however , if the phase structure of the data contains additional structure not captured implicitly by the @xmath72 and @xmath39 hidden units , there will be a learning signal for the @xmath68 units .",
    "conversely , the phase statistics that are produced implicitly by the @xmath72 and @xmath39 units will be ignored by the @xmath68 terms because the learning signal is driven by the differences in the data and model distributions .",
    "we learn the parameters of the model by stochastic gradient ascent of the log - likelihood .",
    "we express the likelihood in terms of the energy with the hidden units integrated out ( omitting the visible squared term and biases ) : @xmath73^{1/\\alpha } + b_n^c ) ) \\\\\\nonumber & -&\\sum_{t=1}^t \\log ( 1 + \\exp ( \\frac{1}{2 } \\sum_{g=1}^g r_{gt } ( \\sum_{l=1}^l\\sum_{f=1}^f q_{flg } x_{fl})^2 + b_t^k ) ) \\\\ \\nonumber & - & \\sum_{j=1}^m \\log ( 1 + \\exp ( \\sum_{i=1}^d w_{ij } v_i + b_j^m))\\end{aligned}\\ ] ] it is not possible to efficiently sample the distribution over the visible units conditioned on the hidden units exactly ( in contrast , sampling from the visible units conditioned on the hidden units in a standard rbm is efficient and exact ) .",
    "we choose to integrate out the hidden variables , instead of taking the conditional distribution , to achieve better estimates of the model statistics .    maximizing the log - likelihood",
    "the gradient update for the model parameters ( denoted as @xmath74 is given as : @xmath75 where @xmath76 indicates the expectation taken over the distribution @xmath77 .",
    "calculating the expectation over the data distribution is straightforward .",
    "however , calculating the expectation over the model distribution requires computationally expensive sampling from the equilibrium distribution .",
    "therefore , we use standard techniques to approximate the expectation of the gradients under the model distribution following the procedure in  @xcite . to summarize , in contrastive divergence learning  @xcite the model distribution is approximated by running a dynamic sampler starting at the data for only one step .",
    "given the energy function with the hidden units integrated out , we run hybrid monte carlo sampling  @xcite starting at the data for one dynamical simulation to produce an approximate sample from the model distribution .",
    "for each dynamical simulation we draw a random momentum and run 20 leap - frog steps while adapting the step size to achieve a rejection rate of about  10% .",
    "@xmath78{figures / mpkrbm_c.pdf}\\ ] ]      we trained the models on image patches selected randomly from the berkeley segmentation database .",
    "we subtracted the image mean , and whitened 16x16 color image patches preserving 99% of the image variance .",
    "this resulted in @xmath79 visible units .",
    "we examined a model with 256 @xmath40 covariance units , 256 @xmath68 phase - coupling units , and 100 @xmath39 mean units .",
    "we initialized the values of the matrix @xmath80 to random values and normalized each image domain vector to have unit length .",
    "we initialized the matrices @xmath81 and @xmath82 to small random values with variances equal to 0.05 , and 0.1 respectively .",
    "we initialized the biases , @xmath26 , @xmath83 , @xmath84 , and @xmath85 to 2.0 , -2.0 , 0.0 , and 0.0 respectively .",
    "the learning rates for @xmath86 , @xmath82 , @xmath28 , @xmath80 , @xmath81 , @xmath87 , @xmath88 , @xmath89 , @xmath90 , were set to 0.0015 , 0.1 , 0.0015 , 0.15 , 0.015 , 0.0005 , 0.0015 , 0.0075 , and 0.0015 , respectively .",
    "after each learning update we normalized the lengths of the @xmath80 vectors to have the average of the lengths .",
    "this allowed the lengths of the @xmath80 vectors to grow or shrink to match the data distribution , but prevented any relative scaling between the subspaces .",
    "after each update we also set any positive values in @xmath28 to zero and normalized the columns to have unit @xmath1-norm .",
    "finally , we normalized the lengths of the columns of @xmath86 to have unit @xmath1-norm .",
    "we learned on mini - batches of 128 image patches and learned the various parts of the model sequentially .",
    "we adapted the parameters of a mprbm model with @xmath49 and @xmath50 and fixed the matrix @xmath28 to the negative identity for 10,000 iterations .",
    "we then adapted the parameters , including @xmath28 , for another 30,000 iterations .",
    "we then added the @xmath68 units to this learned model and adapted the values in @xmath82 for 20,000 iterations while holding the matrix @xmath86 fixed to the identity .",
    "next we adapted @xmath86 for 20,000 iterations .",
    "finally , we allowed all of the parameters in the model to adapt for 40,000 iterations .",
    "@xmath91{figures / mpkrbm_w.pdf}\\ ] ]",
    "here we examine the structure represented in the model parameters @xmath86 , @xmath82 , @xmath28 , and @xmath80 after training the mpkrbm on natural images . the subspace filters in the @xmath80",
    "learn localized oriented band - pass filters roughly in quadrature , see fig .",
    "[ fig : c_mpkrbm ] . we have observed that the filters in the matrix @xmath80 appear to learn more textured patterns than those in the mcrbm , but a more rigorous analysis is needed to verify such an observation .",
    "the weights in the matrix @xmath28 adapt to group subspaces with similar spatial position and spatial frequency .",
    "see fig .",
    "[ fig : p ] for a depiction of the image filters with the highest weights to each hidden unit @xmath72 .",
    "the values in the learned matrix @xmath81 are similar to those learned by the mcrbm and are shown in fig .",
    "[ fig : w ] .    @xmath78{figures / mpkrbm_p_subset.pdf}\\ ] ]    the learned @xmath86 and @xmath82 weights are harder to visualize as they express dependencies in a layer removed from the image domain .",
    "however , we can view the subspaces that are weighted highest by each column of @xmath82 . for each column in fig .",
    "[ fig : q ] we depict the image domain filters ( @xmath80 ) that are weighted highest by the corresponding column in @xmath82 .",
    "we similarly show the image domain filters that are weighted highest by each column in @xmath86 in fig .",
    "[ fig : r ] .    @xmath78{figures / mpkrbm_q_subset.pdf}\\ ] ]    @xmath78{figures / mpkrbm_r_subset.pdf}\\ ] ]",
    "the mprbm and mpkrbm suggest a number of interesting future directions . for example",
    ", it should be possible to learn the dimensionality of the subspaces by introducing a weighting matrix in the @xmath46 dimensional space of the tensor @xmath80 .",
    "however , it is not clear how to define an appropriate angle within these subspaces for the phase - coupling factors .",
    "although , it would be reasonable to learn a separate set of factors , @xmath80 , for the @xmath72 units and the @xmath68 units , thus freeing the constraint of @xmath49 for the @xmath72 units .",
    "it may also be possible to extend the mprbm to a nested form of @xmath0 distributions suggested in  @xcite .",
    "it is also worth exploring the behavior of the phase - coupling hidden units as the number of hidden units is varied . as we had little prior expectation as to the structure of the learned @xmath86 and @xmath82 matrices , the choice of 100 @xmath68 hidden units was rather arbitrary .",
    "in this paper we have introduced two new factorized boltzmann machines : the mprbm and the mpkrbm , which each extend the factorized third - order boltzmann machine ( the mcrbm ) of ranzato and hinton  @xcite .",
    "the form of these additional hidden unit factors are motivated by image models of subspace structure  @xcite and phase alignments due to edges in natural images  @xcite .",
    "focusing on the mpkrbm , we have shown that such a model learns phase structure in natural images .",
    "we would like to thank marcaurelio ranzato for helpful comments and discussions .",
    "we would also like to thank bruno a. olshausen for his contributions to early drafts of this document .",
    "this work has been supported by nsf grant iis-0917342 ( kk ) and nsf grant iis-0705939 to bruno a. olshausen .                  c.  f. cadieu and b.  a. olshausen .",
    "learning transformational invariants from natural movies . in d.",
    "koller , d.  schuurmans , y.  bengio , and l.  bottou , editors , _ advances in neural information processing systems 21 _ , pages 209216 . mit press , 2009 .",
    "f.  sinz , e.  p. simoncelli , and m.  bethge .",
    "hierarchical modeling of local image features through lp - nested symmetric distributions . in _ adv .",
    "neural information processing systems 22 _ , volume  22 , pages 16961704 ."
  ],
  "abstract_text": [
    "<S> we describe a model for capturing the statistical structure of local amplitude and local spatial phase in natural images . </S>",
    "<S> the model is based on a recently developed , factorized third - order boltzmann machine that was shown to be effective at capturing higher - order structure in images by modeling dependencies among squared filter outputs  @xcite . here </S>",
    "<S> , we extend this model to @xmath0-spherically symmetric subspaces . in order to model local amplitude and phase structure in images , we focus on the case of two dimensional subspaces , and the @xmath1-norm . </S>",
    "<S> when trained on natural images the model learns subspaces resembling quadrature - pair gabor filters . </S>",
    "<S> we then introduce an additional set of hidden units that model the dependencies among subspace phases . </S>",
    "<S> these hidden units form a combinatorial mixture of phase coupling distributions , concentrated in the sum and difference of phase pairs . when adapted to natural images , </S>",
    "<S> these distributions capture local spatial phase structure in natural images . </S>"
  ]
}