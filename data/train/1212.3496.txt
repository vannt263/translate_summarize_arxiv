{
  "article_text": [
    "during the rising phase of the solar cycle , it is becoming more important to understand the physics of the near - earth space . the dynamical phenomena caused by the constant flow of magnetized collisionless plasma from the sun",
    "creates space weather that may have harmful effects on space - borne or ground - based technological systems or on humans in space . while the physics of space weather is being studied with in situ instruments ( e.g. nasa s radiation belt storm probes launched in 2012 - 08 - 30 ) and by means of remote sensing , it is also important to model the near - earth space with numerical simulations .",
    "the simulations can be used both as a context to the one - dimensional data sets from obsevations , as well as a source to discover new physical mechanisms behind observed variations .",
    "present large scale ( global ) simulations are based on computationally light - weight simplified descriptions of plasma , such as magnetohydrodynamics ( mhd , @xcite , @xcite , @xcite and @xcite ) . on the other hand",
    "the complexity and range of spatial scales ( from less than @xmath1 to over @xmath0 km ) in space weather physics signifies the need to incorporate particle kinetic effects in the modeled equation set in order to better model , for example , magnetic reconnection , wave - particle interactions , shock acceleration of particles , ring current , radiation belt dynamics and charge exchange ( see e.g. @xcite for an overview ) .",
    "however , as one goes from mhd towards the full kinetic description of plasma ( from hybrid pic @xcite and vlasov @xcite to full pic @xcite , @xcite ) , the computational demands increase rapidly , indicating that the latest high performance computing techniques need to be incorporated in the design of new simulation architectures .",
    "as the number of cores in the fastest supercomputers increases exponentially the parallel performance of simulations on distributed memory machines is becoming crucial . on the other hand , utilizing a large number of cores efficiently in parallel is challenging especially in simulations using run - time adaptive mesh refinement ( amr ) .",
    "this is largely a data structure and an algorithm problem albeit specific to massively parallel physical simulations running on distributed memory machines .    in computer simulations dealing with , for example , continuous matter ( a fluid )",
    "the simulated domain is discretized into a set of points or finite volumes which we will refer to as cells . at any given cell",
    "the numerical solution of a differential equation describing the problem often depends only on data within a ( small ) part of the simulated volume .",
    "this is true for a single time step in a solver for a hyperbolic problem or a single iteration in a solver for an elliptic problem .",
    "this spatial data dependency can be implemented implicitly in the solver function(s ) or explicitly as a separate grid library used by the application .    in a simple case",
    "the number of cells in the simulation stays constant and the data dependency of each cell is identical allowing cell data to be stored in an array whose size is determined at grid creation and the spatial neighbors to be represented as indices into this array .",
    "a straightforward amr extension of this concept is to create additional nested grids in specific parts of the simulation domain with higher resolution . by solving each grid separately and interpolating the results from finer grids into coarser grids",
    "one does not have to modify the solver functions .",
    "this technique is used extensively for example by berger ( see @xcite for some of the earliest work ) and by @xcite and @xcite . in the rest of this work",
    "however we will concentrate on amr implementations in which additional overlapping grids are not created but instead cells of the initial grid are refined , i.e. replaced with multiple smaller cells",
    ".    a generic unstructured grid ( as provided for example by libmesh @xcite ) does not admit as simple a description as above and is generally described by a directed graph in which vertices represent simulation cells and directed edges represent the data dependencies between cells .",
    "unfortunately the nomenclature of graph theory and geometry overlap to some extent and discussing both topics simulataneously can lead to confusion .",
    "figure [ fig : nomenclature ] shows the nomenclature we use from this point forward , the standard graph theoretical terms are given in parentheses for reference . a cell is a natural unit in simulations using the finite volume method ( fvm ) and hereinafter we will use the term cell instead of vertex when discussing graphs .",
    "also an edge in fvm simulations usually refers to the edges of a cube representing the physical volume of a cell , and hence we will use the term arrow to refer to a directed edge in a graph . furthermore we note that each cell in the grid can also represent , for example , a block of cells similarly to @xcite , but for the purposes of this work the actual data stored in grid cells is largely irrelevant .",
    "since a graph can also be used to represent the cells and arrows of grids simpler than an unstructured mesh , the question arises how does a particular program implement its graph representation of the simulated system , e.g.  what simplifying assumptions have been made and how is the graph represented in memory .",
    "a popular representation in ( m)hd amr simulations is to have a fixed number of arrows directed away from each source cell and to store the arrows as native pointers to the destination cells . in case",
    "a cell does not exist all arrows pointing to it are invalidated in neighboring cells .",
    "this technique has been used with different variations by @xcite , @xcite , @xcite and @xcite , for example .",
    "there are several possibilities for representing the cells and arrows of a graph , for example an adjacency list or an adjacency matrix @xcite . in physical simulations",
    "the number of arrows in the graph is usually of the same order as the number of cells in which case a suitable representation is an adjacency list . in an adjacency list",
    "the cells of the graph are separate objects and each cell stores the arrows pointing to and/or from that cell .",
    "the cells of the graph and the arrows of each cell can be stored in different types of data structures .",
    "for example the cells are stored in a contiguous array ( representing a linear octree ) in @xcite , @xcite , @xcite and @xcite , a hash table in @xcite and a ( doubly ) linked list in @xcite . on the other hand the arrows of each cell are stored in a fixed size array of native pointers in @xcite and as single bits in @xcite .    in this work",
    "we introduce the distributed cartesian cell - refinable grid ( dccrg ) for rapid development of parallel simulations using , for example , finite volume or finite element methods ( fem ) . in dccrg the graph is represented by an adjacency list in which cells are stored into a hash table , while the arrow lists of cells are stored into contiguous arrays .",
    "we describe the details of the graph representation in section [ sec : hash_table ] . in section",
    "[ sec : implementation ] we describe the c++ implementation of dccrg and present its unique features with respect to other published grid codes : arbitrary data in grid cells , transparent updates of remote neighbor data , user - selectable neighborhood size for cells and ease of use . in section [ sec : tests ] we test the scalability of dccrg using a variety tests in one , two and three dimensions and draw our conclusions in section [ sec : discussion ] .",
    "dccrg represents the grid as an adjacency list in which cells are stored into a hash table .",
    "a hash table has one clear advantage over a tree when used to store the grid : cells can be accessed , inserted and deleted in constant amortized time regardless of the number of cells and their physical size and location .",
    "thus neither the total number of cells nor the number of refinement levels affect the simulating performance of a single core .",
    "each cell is associated with a unique i d which we use as a key into the hash table .",
    "a potential drawback of a hash table is the computational cost of the hash function , but according to our tests the cost is usually not important .",
    "the time to solve one flux between two cells in the mhd tests presented in section [ sec : mhd_tests ] is about four times larger than accessing one random cell in the hash table .",
    "the cell access time can be optimized further , for example , by storing and solving blocks of cells instead of single cells as is done in @xcite and discussed further in section [ sec : arbitrary_data ] .",
    "our cell ids are globally unique integers which offers several advantages : 1 ) the cell i d can be calculated locally , i.e. without communication with other processes , 2 ) the neighbors of a cell can be stored as cell ids instead of pointers that are not consistent across computing nodes , 3 ) the cost of computing the hash function values is minimized , 4 ) the memory required for storing the cell ids is small .",
    "figure [ fig : example_grid ] shows an example of mapping cells to unique ids which is done as follows : cell ids within each refinement level increase monotonically first in x coordinate , then in y and then in z , with cells at refinement level 0 represented by numbers from 1 to @xmath2 , refinement level 1 by numbers from @xmath3 to @xmath4 , etc .",
    "cells at refinement level @xmath5 are half the size of cells at refinement level @xmath6 in each dimension",
    ". cells at equal refinement level are identical in size . hence in three dimensions @xmath7 , where @xmath8 and @xmath9 are the grid size in cells of refinement level 0 in the x , y and z dimensions respectively .",
    "hereinafter cell size refers to the logical size of cells assuming a homogeneous and isotropic cartesian geometry .",
    "when searching for the neighbors of a cell in the hash table ( see section [ sec : neigh_search ] ) it is convenient to use the concept of cell indices : the location of each cell in the grid is represented by one number per dimension in the interval @xmath10 $ ] where l is the maximum refinement level of the grid and @xmath11 is @xmath12 , @xmath13 or @xmath9 respectively .",
    "figure [ fig : example_grid ] shows the possible cell indices for an example grid with @xmath14 and @xmath15 .",
    "the size of a single cell of refinement level @xmath6 is @xmath16 indices in each dimension .",
    "a cell spanning more than one index is considered to be located at indices closest to the origin of the grid , for example cell # 3 in figure [ fig : example_grid ] is located at indices ( 2 , 0 , 0 ) .",
    "similarly to @xcite there is a one - to - one mapping between cell ids and cell indices plus refinement levels , e.g.  in addition to its i d a cell can be uniquely identified by its indices and refinement level .    in the current implementation of dccrg a cell",
    "is refined by creating all of its children ; in the example grid of figure [ fig : example_grid ] refining cell # 1 would create cells # 2 ... #9 . in principle this is not required and more complex grid structures are possible in which , for example , the grid in figure [ fig : example_grid ] would consist of cells # 1 and # 3 alone .",
    "such an approach has been found useful by @xcite .",
    "complete refinement of cells in our case was a practical decision based on our current simulation needs and it also simplifies the neighbor searching code and enables optimizations described in the next section .      in dccrg all cells existing within a certain minimum distance from local cells ( cells owned by the process )",
    "are stored in a hash table with the cell i d as the key and the process owning the cell as the value . since the mapping of cell ids to a location is unique ,",
    "finding the neighbors of a cell in the hash table is straightforward : for all indices neighboring a given cell the hash table is searched for cells of all applicable refinement levels .",
    "figure [ fig : neighbors ] shows an example of neighbor searching in a grid with @xmath17 and a neighborhood size of one .",
    "the siblings of cell # 4 ( # 3 , # 7 , # 8 , # 11 , # 12 , # 15 and # 16 ) are not shown for clarity and some potential neighbors of cell # 4 in the positive x direction have also been omitted . in dccrg",
    "the cells neighborhoods are measured in multiples of their own size , e.g. for cell # 4 all cells that are ( at least partially ) between indices 0 and 11 inclusive in the x direction would be considered as neighbors . in order to find the neighbor(s ) of cell # 4 in positive x direction the hash table is searched for the smallest cell at indices ( 8 , 0 , 0 ) which in this case could correspond to any of the following cells : # 2 , # 5 , # 23 or # 155 . if cell # 23 is the smallest cell found in the hash table the search can stop since it is known that the siblings of cell # 23 also exist ( not shown , cells # 24 , # 31 , # 32 , # 55 , # 56 , # 63 and # 64 ) because all children of a cell are created when a cell is refined .",
    "on the other hand if cell # 155 is the smallest cell found at indices ( 8 , 0 , 0 ) then the search would continue at indices outside of cell # 155 and its siblings , for example at indices ( 10 , 0 , 0 ) or ( 8 , 2 , 0 ) .",
    "the concept of hanging nodes or faces used in unstructured mesh codes is not directly applicable to dccrg because a single cell is the smallest unit that dccrg deals with .",
    "since the user is responsible for defining what is stored in each cell he / she must also define , if required , the data stored at the faces , edges and vertices of cells . hanging nodes , which are the result of cells of different size sharing an edge or a face , must be handled by the solvers used for a particular application .",
    "for example in gumics-4 @xcite where the magnetic field is separated into background and perturbed components the face average background magnetic field is required when solving the flux through a face . with amr a face average value of the background field",
    "is required for every face of every cell because , for example , if only one face average is stored per dimension in cells it would not be possible to solve the flux between cells ( # 4 , # 2 ) and ( # 1 , # 5 ) in figure [ fig : neighbors ] .",
    "this is due to the fact that the face average value of the smaller cell must be used in both cases and it is only available if every face of every cell stores the face average field .",
    "currently dccrg enforces a maximum refinement level difference of 1 between neighboring cells .",
    "hence it is sufficient to search for cells of three refinement levels @xmath18 when finding the neighbors of a cell of refinement level @xmath6 . in principle",
    "the enforcement of maximum refinement level difference is not required .",
    "for example in figure [ fig : neighbors ] cell # 4 ( refinement level 1 ) has cell # 155 ( refinement level 3 ) as a neighbor but in the current version of dccrg such a situation is not permitted and searching the hash table for cells # 2 , # 5 and # 23 is sufficient for finding the neighbors of cell # 4 .",
    "this was a practical decision based on our experience with global mhd modeling of the earth s magnetosphere using gumics-4 . in future",
    "this restriction might be removed .",
    "a similar one is also used in @xcite .    even though a maximum refinement level difference of one is enforced between neighboring cells and searching for cells in the hash table",
    "is a quick operation , in practice the ids of neighbors of local cells are also stored explicitly . as mentioned in section [ sec : introduction ] , this neighbor information corresponds to arrows between cells in a graph and hence we will use the term arrow list to refer to the neighbor list of a cell .",
    "dccrg stores both the arrows pointing away from and the arrows pointing to local cells . with amr",
    "it is possible that there exists only one arrow between two cells because cells neighborhoods are measured in units of their own size .",
    "for example in figure [ fig : example_grid ] with a neighborhood size of 1 , cell # 13 would be considered a neighbor of cell # 2 but cell # 2 would not be considered a neighbor of cell # 13 .",
    "explicitly storing the arrows to and from local cells enables fast iteration for example by user code ( solvers , reconstruction functions , etc ) . in dccrg",
    "the arrow lists of local cells are stored as contiguous arrays .",
    "the most important advantage that globally unique cell ids have over a traditional graph implementation using native pointers between cells is that the arrows between cells are not required to be consistent during amr or load balancing .",
    "for example when doing amr , a pointer - based implementation has to be careful not to leave any dangling pointers and to update the pointers of all nearby cells in the correct order so as not to lose access to any cell . on the other hand with unique cell ids the arrow lists of cells",
    "can simply be emptied when needed and new neighbors searched in the hash table as described in section [ sec : neigh_search ] .",
    "in addition to being easy to implement this method also admits simple thread - based parallelization inside dccrg due to different threads modifying only the arrow lists of different cells .",
    "it is also advantageous to use unique cell ids in arrow lists instead of pointers in a parallel program running on distributed memory hardware . in this environment",
    "a pointer to a neighboring cell is only valid on one process and can not be used to refer to the same neighbor on other processes . on the other hand",
    "the same unique cell i d can be used by all processes to refer to the same neighbor regardless of its actual location in memory .",
    "a separate grid library is a natural abstraction probably for any physical simulation but especially for simulations using fvm where the concept of a grid and its cells data dependencies are easy to define and implement . thus following good software development practice dccrg",
    "is implemented independently of any specific physical problem or its solver , while still providing the flexibility required for various types of simulations , for example ( m)hd , advection ( e.g.  vlasov ) and kinetic .",
    "dccrg is written in c++ which allows us to easily separate low level functionality of dccrg into subclasses which higher - level classes can use thus also benefiting from a modular internal implementation , a technique also used in @xcite .",
    "for example the physical geometry of the grid is handled by a separate class which is also given to dccrg as a template parameter .",
    "this allows one to easily extend the grid geometries supported by dccrg . in the default",
    "homogeneous and cartesian geometry cells of the same refinement level are identical in size in each dimension .    here",
    "we describe the unique user - visible features of dccrg with respect to other grid codes and also present important features of the serial and parallel implementation .",
    "the most important feature distinguishing dccrg from other grid codes is the possibility of trivially storing data of arbitrary type and size in the grid s cells by simply giving the class which is stored in the cells as a template parameter to dccrg when creating an instance of the grid .",
    "this also allows a single simulation to have several independent parallel grids with different geometries and different types of data stored in the grids cells. the amount of data can also vary between different cells of the same grid and in the same cell as a function of time .",
    "this is required for example in kinetic simulations where not only does the total number of particles change but also the number of particles in each grid cell varies . in dccrg",
    "this is handled by each instance of the user s cell data class providing a mpi datatype corresponding to the data to be sent from or received by that particular cell .",
    "an example of this is presented in section [ sec : example_program ] .",
    "completely arbitrary cell data can also be transferred between processes if the cell data class provides a serialize function which the mpi bindings of boost library will use for transferring cell data between processes .",
    "although this method of transferring data between processes the most general it is also the slowest since data is first copied into a contiguous buffer by serialization and subsequently transferred by mpi resulting in at least one additional copy the data being created compared to pure a mpi transfer .",
    "this is also the case in the samrai framework @xcite which supports transferring arbitrary patch data using the same technique .",
    "dccrg can automatically transfer cell data between processes both for remote neighbor data updates and load balancing using simple function calls .",
    "furthermore whenever cell data is sent between processes either one mpi message per cell can be used or , similarly to @xcite , all cells being sent to another process can be transferred using a single mpi message .",
    "updating the remote neighbor data between processes is possible using several methods .",
    "the simplest one is the synchronous update function that updates the remote neighbor data between processes and returns once transfers have completed ( see section [ sec : example_program ] ) .",
    "the most fine - grained communication currently supported can be used by calling a separate function for initiating transfers and functions that wait for the sends and receives to complete respectively .",
    "a typical usage scenario would consist of the following :    1 .",
    "start transferring remote neighbor data 2 .",
    "solve the inner cells of the simulation ( cells without remote neighbors ) 3 .",
    "wait for the data from other processes to arrive 4 .",
    "solve the outer cells of the simulation ( cells with at least one neighbor on another process ) 5 .",
    "wait for the data from this process to be sent    the mhd scalability tests we present in section [ sec : mhd_tests ] use this procedure with the exception that step 5 is executed before step 4 due to the technical implementation of the gumics-4 mhd solver .      as mentioned in section [ sec : introduction ] the size of cells neighborhood in simulations is highly problem / solver dependent .",
    "specifically the problem / solver used in the simulation dictates the distance from which data is required at a cell in order to advance the simulation for one time step or one iteration in that cell . in many previously published grid codes",
    "the size of cells neighborhood is restricted to 1 either explicitly or implicitly .",
    "for example in @xcite in three dimensions a block ( a single cell from the point of view of the grid ) has 6 neighbors and it is assumed that a block consist of such a number of simulation cells that , for example , a solver needing data from a distance of 3 simulation cells can obtain that data from the neighboring block .",
    "other examples are @xcite , @xcite , @xcite , @xcite , @xcite and @xcite .",
    "dccrg supports an arbitrarily large neighborhood chosen by the user when the grid is initialized .",
    "the size of the neighborhood can be any unsigned integer and all other cells within that distance of a cell ( in units of size of the cell itself ) will be considered as a neighbors of the cell .",
    "this enables the use of high - order solvers with the added possibility of refining each neighboring cell individually .",
    "naturally one can also store a sufficiently large block of simulation cells in one dccrg cell allowing one to use a small 6 cell neighborhood as done in @xcite .",
    "zero neighborhood size is a special case in dccrg signifying that only face neighbors of equal size are considered neighbors ( with amr all of the refined neighbor s children are considered instead ) .",
    "for example in a periodic grid without amr neighborhood sizes of 1 and 2 would result in 26 and 124 neighbors per cell respectively .",
    "naturally the size of the neighborhood affects the amount of data that must be transferred between processes during remote neighbor data updates regardless of the grid implementation thus affecting parallel scalability .",
    "additionally since in dccrg a maximum refinement level difference of one is enforced between neighboring cells the size of the neighborhood does affect for example the amount of induced cell refinement ( see section [ sec : amr ] ) .",
    "even though initially dccrg was developed only for in - house use , it was nevertheless designed to be simple to use for the kinds of simulations it is targeted for . figure [ fig : gol ] shows an example of a complete parallel program playing conway s game of life using dccrg written in less that 60 lines of code ( loc ) including whitespace and comments .",
    "lines 10 ... 14 of the program define the class to be stored in every cell of dccrg along with member functions at and mpi_datatype which dccrg calls when querying the information required for transferring cell data between processes .",
    "the current state of a cell is saved into data[0 ] and the number of its live neighbors is saved into data[1 ] . on line 23 an instance of the grid",
    "is created with the class defined above as cell data . on line 24 the geometry of the grid",
    "is set to 10x10x1 cells at refinement level 0 with minimum coordinate at ( 0 , 0 , 0 ) and cells of size 1 in each dimension . on line 25 the grid is initialized by setting the load balancing function to use in zoltan , the neighborhood size and the maximum refinement level of cells .",
    "lines 26 and 27 balance the load using zoltan and collect the local cells . in this example",
    "the load is balanced only once and the local cell list does not change afterwards . on line 46 the non - existing neighbors of local cells are skipped .",
    "this is because the grid is initialized with non - periodic boundaries and neighbors that would be outside of the grid do not exist .",
    "figure [ fig : particles ] shows relevant excerpts from a simple kinetic simulation showing the use of dccrg in the case of variable amount of cell data , the full program can be viewed in the dccrg git repository .",
    "the remote neighbor update logic in the main simulation loop consists of the following steps :    1 .",
    "the total number of particles in each cell is transferred between processes ( lines 43 and 44 ) 2 .",
    "space for receiving particle data is allocated in local copies of remote cells based on their received total number of particles in step 1 ( lines 47 ... 52 ) 3 .",
    "the particle coordinates are transferred between processes ( lines 55 and 56 )    the cell data class of the example kinetic simulation must provide the correct information to dccrg when updating remote neighbor data : the at and mpi_datatype functions now return a different address and number of bytes respectively depending on whether the total number of particles or the particle coordinates are transferred between processes .",
    "this is decided by the user in the main simulation loop .",
    "additionally the resize function of the cell data class allocates memory for as many particles as is there are in number_of_particles . a similar approach to the one described above",
    "is also used in our vlasov simulation ( further developed from @xcite ) where each real space cell has a separate adaptable velocity grid for ions consisting of a variable number of @xmath19 cell velocity blocks .        in the previous example two",
    "communications are required per time step because processes receiving particle data do not know the number of incoming particles in advance . since the mpi standard requires that the maximum amount of data to be received is known before calling the receive function the number of particles has to be communicated separately .",
    "this guarantees that processes receiving particles can specify the size of the data to mpi and allocate the memory required for received particles .",
    "load balancing is also accomplished easily with dccrg .",
    "a user can call the balance_load function to let the zoltan @xcite library create a new partition , and single cells can also be moved manually between processes using the pin and unpin functions .",
    "most of zoltan s load balancing methods can be used , namely : none , random , block , rcb , rib , hsfc , graph , hypergraph and hier . in any case dccrg",
    "will transparently execute the new partition by transferring the necessary cell data between processes using mpi .",
    "the structure of the grid in dccrg includes the owner of a cell in addition to the unique i d of the cell ( i d is the key and owner is the value in a hash table ) .",
    "thus the cell ids themselves are not used for partitioning cells between processes and any cell can be moved to any process ( for example by using the random partitioner of zoltan which we have found to be very useful for testing ) .      due to the unique mapping of cells ids and their physical location and size it is straightforward to refine",
    "any given cell in the grid , i.e. to calculate the ids of the children of any cell , and can be done locally ( see section [ sec : hash_table ] ) . in order to enforce a maximum refinement level difference of one between neighboring cells whenever a cell is refined the refinement level of all neighbors",
    "is checked .",
    "if the refinement level of any neighbor is less than that of the cell being refined that neighbor is also refined .",
    "this is continued recursively until no more cells need to be refined .",
    "the size of the cells neighborhood affects induced refinement indirectly by changing the number of neighbors a cell has and hence the potential number of cells whose refinement will be induced . in dccrg",
    "a few simplifications have been made to amr : 1 ) any set of cells can only be refined once before calculating induced refinement ( by stop_refining ) , e.g. induced refinement can only increase the refinement level of cells by one , and 2 ) unrefining a cell does not induce unrefinement , e.g. any cell which has at least one neighbor with refinement level larger than the cell being unrefined ( i.e. it has a smaller neighbor ) can not be unrefined .    in a parallel setting the only difference to the above is that whenever a process refines or unrefines a cell that information has to be given to all processes which have cells within a certain distance of the cell that was refined or unrefined . currently this distance is equal to infinity , e.g. all changes to the structure of the grid ( refines and unrefines ) are communicated globally . this has a significant impact on the parallel scalability of amr in dccrg and is discussed in section [ sec : tests ] and a method for making this minimum distance finite is discussed in section [ sec : discussion ] .",
    "the changes in grid structure are exchanged between processes after each recursive step of induced refinement which are continued until no more cells need to be refined .",
    "good scalability in distributed memory machines requires asynchronous point - to - point mpi communication between processes with minimal total amount of communication , and especially minimal amount of global communication .",
    "the number of mpi messages should also be minimized in order not to burden the network with unnecessary traffic .",
    "therefore any process must know which processes require data from local cells and from which remote cells data is required during remote neighbor updates without querying that information from other processes beforehand .",
    "thus in dccrg every process knows the structure of the grid ( e.g.  which cells exist and which processes own them ) to at least a certain distance from any of its cells so it can calculate locally which cells data to send and receive from other processes . due to this dccrg",
    "does not require global communication during ordinary time stepping , e.g.  remote neighbor data updates , which enables excellent scalability when not doing load balancing or amr as shown in section [ sec : mhd_tests ] .",
    "internally dccrg precalculates these send and receive lists whenever the structure of the grid changes due to amr or cells being moved between processes .    even though the replicated mesh metadata of dccrg does not include the data stored in each cell it nevertheless limits the size of dccrg grids in practice to less than 100 m existing cells .",
    "this number does not depend on the refinement level or physical location of the cells and has so far been more than sufficient for our needs . to our knowledge",
    "the only parallel grid library that does not have any persistent global data structures is @xcite . in @xcite and @xcite",
    "only the macrostructure of the grid ( i.e. cells of refinement level 0 ) is known by all processes . according to the authors this limits the size of the grid to the order of @xmath20  @xmath0 cells of refinement level 0 but",
    "does not limit the number of smaller cells .",
    "the time stepping scalability of dccrg ( e.g.  without amr or load balancing ) depends mostly on the hardware running the simulation and on three parameters specific to each simulated system : 1 ) the time required to solve the inner cells of a process , 2 ) the amount of data transferred during the remote neighbor data update of a process and 3 ) the time required to solve the outer cells of a process .",
    "we show the dependency of dccrg scalability on these parameters by varying the total number of cells and processes and by using a mhd solver in one , two and three dimensions .",
    "the run - time amr scalability of dccrg is also presented .",
    "the non - amr scalability tests were carried out on three different supercomputers : 1 ) a 2 k core cray xt5 m system with 12 core nodes connected by seastar2 installed at the finnish meteorological institute which we will refer to as meteo , 2 ) a 295 k core ibm blue gene / p system with 128 core nodes ( nodeboards ) installed at the jlich supercomputing centre which we will refer to as jugene , and 3 ) a 12 k core bullx system with 32 core nodes connected by infiniband installed at the trs grand centre de calcul which we will refer to as curie .",
    "first we show the time stepping scalability of dccrg in several mhd problems with a solver developed for the global mhd model gumics-4 ( @xcite ) which solves ideal mhd equations in conservative form .",
    "specifically the solver is a first order roe s approximate riemann solver for a godunov type problem @xcite . in the test results we present here",
    "only the roe solver from gumics-4 is used as we do not experience problems with negative pressures or densities in the presented tests .",
    "the nature of the solver is such that when solving the flux through a face data is only required from cells adjacent to the face , irrespective of the size of cells involved .",
    "thus interpolation of data is not required at any point in the solution and if a cell has more than one face neighbor in any direction the flux through each common face is solved in the usual way . in the tests presented here",
    "we do not include a background magnetic field which is used in gumics-4 to represent the earth s dipole field .",
    "the problems we use are the one - dimensional shock tube presented for example in @xcite , the two - dimensional circularly polarized alfvn wave presented by @xcite and further elaborated on by @xcite , and the three - dimensional blast wave presented by @xcite .",
    "periodic boundary conditions are used in all tests , except for the shock tube test in the direction of the tube where initial conditions are enforced after every time step . in mhd tests",
    "every cell contains the cell - averaged values of the conservative mhd variables ( density , momentum density , total energy density and magnetic field ) giving a total of 128 bytes which must be transferred when updating the data of one cell between two processes . since only the face neighbors of a cell",
    "are required for calculating the next time step we use a neighborhood size of zero in dccrg . in these tests processes",
    "execute one collective mpi communication per time step in order to dynamically calculate the maximum physical length of the time step .",
    "no other global communication is done . since the grid is static in these tests the computational load is balanced only once at the start of the simulation by using a hilbert space - filling curve instead of zoltan .",
    "figure [ fig : mhd - meteo ] shows the results of strong scalability tests using mhd with a static grid in meteo in one , two and three dimensions . the total number of cells ( 10 k , 50 k , 0.1 m",
    ", 0.5 m and 1 m ) is kept constant while the number of mpi processes is increased from 12 to 1536 . in each test case scalability",
    "improves with the total number of cells because processes have more inner cells to solve while remote neighbor data is being transferred .",
    "for example in the shock tube test every process requires the data of two remote neighbors at most while the number of inner cells with 1.5 k processes increases from about 4 ( 10 k total cells ) to 649 ( 1 m total cells ) . with 1 m cells",
    "the one and two dimensional tests scale almost ideally in meteo and the three dimensional test is also quite close to ideal .",
    "the overall decrease in scalability with increasing number of dimensions is due to more data being transferred between processes for the same number of local cells .        as suggested by the scalability results above most of the simulation time",
    "is spent solving mhd which is shown in figure [ fig : meteo - profile ] for the three dimensional blast wave test using 1 m cells .",
    "the only global communication executed per time step while simulating is the calculation of the maximum length of the physical time step and is obtained using mpi_allreduce .",
    "this is labeled as allreduce in figure [ fig : meteo - profile ] and basically shows the computational and mpi imbalance between processes due to load balancing .",
    "initialization and file i / o are not included in the profile and other parts of the simulation take an insignificant fraction of the total run time .",
    "the non - amr scalability tests were also carried out in jugene and curie and the results for the three - dimensional blast wave are shown in figures [ fig : blast - wave - jugene ] and [ fig : blast - wave - curie ] respectively .",
    "similarly to meteo the one and two dimensional tests ( not shown ) scale better than the three - dimensional test in both jugene and curie .",
    "the overall results are similar in all tested machines , e.g.  scalability improves with increasing number of total cells and decreasing number of dimensions . in jugene very good",
    "scalability up to 32 k processes is obtained for a total number of cells of 1 m and above . the total simulation speed in jugene",
    "is only slightly above that of meteo mostly due to the relatively small single - core performance of jugene .",
    "additionally the average number of cells per process is more than 20 times larger in meteo than in jugene for the maximum number of processes used but this has only a small effect on scalability in jugene .            in curie",
    "good scalability up to 8 k processes is obtained only with 64 m total cells but with a maximum solution speed of nearly 400 m solved cells per second which is over twice of that in jugene .",
    "we attribute this to the relatively low node interconnect and high single - core and performance of curie respectively when compared to jugene .",
    "figure [ fig : amr_speed ] shows the speed of pure adaptive mesh refinement in dccrg . initially the grid is @xmath21 or @xmath22 cells and every process refines all local cells until the total size of the grid is @xmath22 or @xmath23 cells .",
    "initially the cells were partitioned using a space - filling curve and this is not included in the timings .",
    "cells also were not transferred between processes during amr .",
    "as can be seen in figure [ fig : amr_speed ] the maximum cell refining speed of dccrg is of the order of 1 m cells per second .",
    "the linear scalability of amr up to some 32 mpi processes is explained by the fact that after changes in the structure of the grid the arrow lists are recalculated only for local cells and mpi communication has not yet become a bottleneck . at 256 processes",
    "the amount of global communication required for updating the structure of the grid between all processes starts to significantly affect the speed of amr .",
    "this is discussed further in section [ sec : amr_blast_wave ] .     or @xmath22 cells .",
    "every process refines each local cell until the total size of the grid is @xmath22 or @xmath23 cells . ]      here we present the scalability of dccrg with amr in the three - dimensional blast wave test used in section [ sec : mhd_tests ] . in this test a procedure similar to the one in gumics-4 ( eq .  2 in @xcite )",
    "is used to decide whether to refine or unrefine a cell : a refinement index is calculated for each cell based on the relative difference of several variables between a cell and its face neighbors . here",
    "the calculation of refinement index @xmath24 additionally includes velocity shear relative to the maximum wave velocity from the cells interface .",
    "the full equation for the refinement index @xmath24 is : @xmath25 where @xmath26 denotes the difference in a variable between two cells , the hat denotes a minimum of the two values ( as it actually does also in @xcite ) , @xmath27 and @xmath28 is the maximum wave velocity from the cells interface . in this test",
    "a cell is refined if @xmath29 , where @xmath6 is the cell s current refinement level and @xmath30 is the maximum refinement level of the grid .",
    "in other words a cell is refined to the maximum refinement level if its refinement index exceeds 0.02 .",
    "a cell is unrefined if @xmath31 , e.g.  a cell is kept at refinement level 0 if @xmath32 and none of the cell s neighbors refinement levels exceed 1 ( due to dccrg enforcing a maximum refinement level difference of one between neighbors ) .",
    "we use a maximum refinement level of 4 in this test with an initial grid of @xmath33 cells which results in an effective resolution of @xmath34 m cells .",
    "the computational load is balanced using zoltan s recursive coordinate bisection ( rcb ) algorithm whenever the fraction of local cells ( @xmath35 , where max and min are the maximum and minimum number of local cells among all processes respectively ) exceeded a specified limit .",
    "animation 1 ( figure [ fig : amr_result ] in the print version ) shows from left to right , top to bottom the grid , pressure , magnetic and kinetic energy density during the simulation ( at the end of the simulation in print version ) in the y = 0 plane when grid is adapted at every time step .",
    "at the end of the simulation the fraction of maximum to minimum values are 15 for density ( not shown ) , 43 for pressure and 2.3 for magnetic energy density . even though the mhd solver we use is simpler than the one in @xcite the results are still close due to the high effective resolution achieved by using run - time amr .",
    "figure [ fig : amr_scalability ] shows the total solution speed during the simulations as a function of the number of mpi processes used . in the reference run a cfl @xcite of 0.4 is used , amr is done at every time step and the load is balanced whenever the local cell fraction @xmath36 .",
    "the @xmath37 runs are otherwise identical to the reference run but cfl is set to @xmath38 and amr is done every nth time step , essentially multiplying the amount of non - amr work in these runs by n. the results between different amr runs are identical by visual inspection except for increased diffusion in runs with low cfl .",
    "the ratio of work required by amr and the rest of the simulation has a significant effect on the total solution speed .",
    "the solution speed is a factor of 5 higher in the @xmath39 run than in the reference run when using about 500 mpi processes . in the reference",
    "amr run the total solution speed is close to @xmath40 of the non - amr version with up to 144 processes and in the @xmath39 the speed is close to @xmath41 with up to 288 processes after which both fractions start to decrease .",
    "we define these as the regions of excellent amr scalability . on the other hand in all of",
    "the amr runs the total solution speed increases up to about 500 to 600 processes after which it starts to decrease .",
    "we define this as the region where amr is scalable . the total number of cells in the amr runs averages to 4.5 m which is about @xmath42 of the non - amr version .",
    "consequently in the region of excellent scalability the time to solution when using amr is about 67 % to 22 % of the non - amr time for the reference and @xmath39 runs respectively .",
    "even with a higher number of mpi processes it can still be advantagous to use amr because the number of simulation cells is over a magnitude lower than without amr . at the end of the amr runs 9.9 m cells",
    "exist in the grid and the total number of cells created and removed is between 40.2 m and 40.7 m , depending mostly on the diffusion , and averages to about 91 k added + removed cells per time step",
    ".     exceeded 2 , where @xmath35 and max and min are the maximum and minimum number of local cells among processes respectively .",
    "the @xmath37 runs are otherwise identical to the reference run but cfl is set to @xmath38 and amr is done every nth time step , essentially multiplying the amount of non - amr work in these runs by n. ]    figure [ fig : meteo - profile - amr ] shows which parts of the amr blast wave test require the most time . as the number of processes is increased the largest fraction of simulation time is spend in global communication related to amr and load balancing .",
    "the allreduce label again indicates global calculation of the physical time step and the load balancing label indicates the simulation time spent in load balancing related functions . at about 300 mpi processes and above the largest fraction of simulation time",
    "is spent communicating changes in the structure of the grid between all processes .",
    "this includes both refining and unrefining cells as well as load balancing and in each case the mpi_allgatherv function is used for distributing the changes in grid structure between all processes .",
    "only a small fraction of the time spent in load balancing related functions is taken by zoltan .",
    "while the ease of use of a software library is subjective it can be quantified by the number of lines of code required for usage and compared against other libraries when using the same programming language . with dccrg a complete parallel program playing conway s game of life",
    "can be implemented in less than 60 loc including whitespace and comments .",
    "even though the required loc is a crude estimate for a software library s ease of use it is nevertheless telling that such a short parallel program does not seem to be possible with other grid libraries .",
    "the flexibility of dccrg also stands out since as far as we know only @xcite allows one to easily exchange arbitrary cell data between mpi processes .",
    "additionally dccrg supports transferring user - defined mpi datatypes which is critical for performance in some applications .",
    "for example when solving the 6 dimensional vlasov equation in the earth s magnetosphere ( @xcite ) the simulation is heavily memory bound and using mpi datatypes directly for exchanging remote neighbor data is significantly faster than serializing said data into an additional buffer(s ) before transfer .",
    "dccrg also provides automatic neighbor data updates between processes with the ability of easily overlapping computation with communication .",
    "currently the largest drawback of dccrg is the fact that the entire structure of the grid is known by every process , i.e. a part of the mesh metadata is replicated by all processes .",
    "the global data structure prevents grids larger than about 100 m cells but this has not been a problem for us and can be worked around by storing blocks instead of single cells into dccrg ( similarly to @xcite , for example ) .",
    "the global grid data structure of dccrg also reduces the scalability of amr in the worst case to about 200 and overall to about 600 mpi processes .",
    "nevertheless using amr can lead to significant savings in the required memory as the number of cells can be one or even two orders of magnitude lower . depending on the problem the required cpu time can also be significantly reduced when using amr especially when the number of mpi processes used is of the order of 300 or less .",
    "it should also be noted that using threads to parallelize solvers within a shared memory node could effectively multiply the scalability range of simulations by the number of cores within one node , but this was not investigated .    removing or significantly reducing the global data structure ( as done in @xcite , @xcite and @xcite ) should improve both the largest attainable grid size and scalability of amr considerably .",
    "intuitively this is straightforward since with the exception of load balancing every process only needs to know the structure of the grid up to some finite distance from local cells . in order to be able to arbitrarily refine and unrefine grid cells without global communication local changes in the structure of the grid",
    "must be communicated between all neighboring processes .",
    "a neighboring process is defined as any process that has one or more of its cells inside the neighborhood of any cell of refinement level 0 that overlaps a local cell . in other words",
    "if only level 0 cells exist in the grid then the owners of all remote neighbors of local cells are considered as neighboring processes ; and this holds no matter how the grid is subsequently refined and unrefined assuming that cells are not transferred between processes ( load balancing ) .",
    "global communication can also be avoided during load balancing if , for example , cells can be transferred only between neighboring processes . even in this case",
    "new neighbor processes have to be recalculated but global communication is not required because cells could only have been transferred to / from a subset of all processes . implementing this completely distributed mesh metadata",
    "is left to a subsequent study .",
    "we presented the distributed cartesian cell - refinable grid ( dccrg ) : an easy to use parallel structured grid library supporting adaptive mesh refinement and arbitrary c++ classe as cell data .",
    "various mhd scalability results were presented and depending on the problem , hardware and whether amr is used excellent to average scalability is achieved .",
    "dccrg is freely available for anyone to use , study and modify under version 3 of the gnu lesser general public license and can be downloaded from https://gitorious.org/dccrg .",
    "this work is a part of the project 200141-quespace , funded by the european research council under the european community s seventh framework programme .",
    "the research leading to these results has also received funding under grant agreement no 260330 of the european community s seventh framework programme .",
    "ih and mp are supported by project 218165 and as is supported by project 251797 of the academy of finland .",
    "results in this paper have in part been achieved using the prace research infrastructure resource curie based in france at tgcc and jugene based in germany at jsc .",
    "ih thanks daldorff , l.k.s . and pomoell , j. for insightful discussions .",
    "p. janhunen , m. palmroth , t. laitinen , i. honkonen , l. juusola , g. facsk , t.i .",
    "pulkkinen , the gumics-4 global mhd magnetosphere - ionosphere coupling simulation , j. atmos .",
    ".- terr . phys .",
    "80 ( 2012 ) 48 - 59 doi:10.1016/j.jastp.2012.03.006              m. palmroth , i. honkonen , a. sandroos , y. kempf , s. von alfthan , d. pokhotelov , preliminary testing of global hybrid - vlasov simulation : magnetosheath and cusps under northward interplanetary magnetic field , j. atmos .",
    "sol .- terr .",
    "phys . accepted ( 2012 )",
    "henshaw , d.w .",
    "schwendeman , parallel computation of three - dimensional flows using overlapping grids with adaptive mesh refinement , j. comput .",
    "( 2008 ) 7469 - 7502 doi:10.1016/j.jcp.2008.04.033            p. macneice , k.m .",
    "olson , c. mobarry , r. de fainchtein , c. packer , paramesh : a parallel adaptive mesh refinement community toolkit , comput .",
    "commun . , 126 ( 2000 ) 330 - 354 doi:10.1016/s0010 - 4655(99)00501 - 9    q.f .",
    "stout , d.l .",
    "de zeeuw , t.i .",
    "gombosi , c.p.t .",
    "groth , h.g .",
    "marshall , k.g .",
    "powell , adaptive blocks : a high performance data structure , proc .",
    "1997 acm / ieee conf .",
    "57 ( 1997 ) doi:10.1109/sc.1997.10027            w. bangerth , c. burstedde , t. heister , m. kronbichler , algorithms and data structures for massively parallel generic finite element codes , acm trans .",
    "38 ( 2011 ) 14/1 - 28 doi:10.1145/2049673.2049678      r. keppens , z. meliani , a.j .",
    "van marle , p. delmont , a. vlasis , b. van der holst , parallel , grid - adaptive approaches for relativistic hydro and magnetohydrodynamics , j. comput .",
    "( 2012 ) 718 - 744 doi:10.1016/j.jcp.2011.01.020    a.m. wissink , r.d .",
    "hornung , s.r .",
    "kohn , s.s .",
    "smith , n. elliott , large scale parallel structured amr calculations using the samrai framework , proc .",
    "2001 acm / ieee conf .",
    "( 2001 ) doi:10.1145/582034.582040"
  ],
  "abstract_text": [
    "<S> we present an easy to use and flexible grid library for developing highly scalable parallel simulations . </S>",
    "<S> the distributed cartesian cell - refinable grid ( dccrg ) supports adaptive mesh refinement and allows an arbitrary c++ class to be used as cell data . </S>",
    "<S> the amount of data in grid cells can vary both in space and time allowing dccrg to be used in very different types of simulations , for example in fluid and particle codes . </S>",
    "<S> dccrg transfers the data between neighboring cells on different processes transparently and asynchronously allowing one to overlap computation and communication . </S>",
    "<S> this enables excellent scalability at least up to 32 k cores in magnetohydrodynamic tests depending on the problem and hardware . in the version of dccrg presented here part of the mesh metadata </S>",
    "<S> is replicated between mpi processes reducing the scalability of adaptive mesh refinement ( amr ) to between 200 and 600 processes . </S>",
    "<S> dccrg is free software that anyone can use , study and modify and is available at https://gitorious.org/dccrg . </S>",
    "<S> users are also kindly requested to cite this work when publishing results obtained with dccrg .    </S>",
    "<S> parallel grid , adaptive mesh refinement , free open source software    * program summary *    _ manuscript title : _ parallel grid library for rapid and flexible simulation development + _ authors : _ honkonen , i. , von alfthan , s. , sandroos , a. , janhunen , p. , palmroth , m. + _ program title : _ </S>",
    "<S> dccrg + _ journal reference : _ </S>",
    "<S> + _ catalogue identifier : _ </S>",
    "<S> + _ licensing provisions : _ gnu lgpl v3 + _ programming language : _ </S>",
    "<S> c++ + _ computer : _ pc , cluster , supercomputer + _ operating system : _ posix + _ ram : _ 10 mb - 10 gb per process + _ number of processors used : _ 1 - 32768 cores + _ supplementary material : _ </S>",
    "<S> + _ keywords : _ dccrg , parallel , grid , amr , mpi , fvm , fem + _ classification : _ 4.12 , 4.14 , 6.5 , 19.3 , 19.10 , 20 + _ external routines / libraries : _ mpi-2 [ 1 ] , boost [ 2 ] , zoltan [ 3 ] , sfc++ [ 4 ] + _ nature of problem : _ + grid library supporting arbitrary data in grid cells , parallel adaptive mesh refinement , transparent remote neighbor data updates and load balancing . + _ solution method : _ + the simulation grid is represented by an adjacency list ( graph ) with vertices stored into a hash table and edges into contiguous arrays . </S>",
    "<S> message passing interface standard is used for parallelization . </S>",
    "<S> cell data is given as a template parameter when instantiating the grid . </S>",
    "<S> +   +   + _ restrictions : _ </S>",
    "<S> + logically cartesian grid . </S>",
    "<S> + _ additional comments : _ +   + _ running time : _ + running time depends on the hardware , problem and the solution method . </S>",
    "<S> small problems can be solved in under a minute and very large problems can take weeks . the examples and tests provided with the package </S>",
    "<S> take less than about one minute using default options . in the version of dccrg presented here </S>",
    "<S> the speed of adaptive mesh refinement is at most of the order of @xmath0 total created cells per second . </S>",
    "<S> +    0 http://www.mpi-forum.org/ http://www.boost.org/    k. devine , e. boman , r. heaphy , b. hendrickson , c. vaughan , zoltan data management services for parallel dynamic applications , comput . </S>",
    "<S> sci . </S>",
    "<S> eng . 4 ( 2002 ) 90 - 97 doi:10.1109/5992.988653    https://gitorious.org / sfc++ </S>"
  ]
}